Based on the given code from a commit, please generate supplementary code files according to the commit message.
####commit message
Add trusted_image_certificates to REST API

This change adds a trusted_image_certificates parameter to the
REST API for server create, rebuild, list, and show. The new
parameter may contain an array of strings, each string representing
the ID of a trusted certificate. The set of certificate IDs will be
stored in the trusted_certs field of the instance InstanceExtra
and will be used to verify the validity of the signing certificate
of a signed instance image.

APIImpact

Implements blueprint: nova-validate-certificates
Change-Id: Iedd3fea0e86648fae364f075915555dcb2c4f199

####code 
1 # Copyright 2010 United States Government as represented by the
2 # Administrator of the National Aeronautics and Space Administration.
3 # Copyright 2011 Piston Cloud Computing, Inc.
4 # Copyright 2012-2013 Red Hat, Inc.
5 # All Rights Reserved.
6 #
7 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
8 #    not use this file except in compliance with the License. You may obtain
9 #    a copy of the License at
10 #
11 #         http://www.apache.org/licenses/LICENSE-2.0
12 #
13 #    Unless required by applicable law or agreed to in writing, software
14 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
15 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
16 #    License for the specific language governing permissions and limitations
17 #    under the License.
18 
19 """Handles all requests relating to compute resources (e.g. guest VMs,
20 networking and storage of VMs, and compute hosts on which they run)."""
21 
22 import collections
23 import copy
24 import functools
25 import re
26 import string
27 
28 from castellan import key_manager
29 from oslo_log import log as logging
30 from oslo_messaging import exceptions as oslo_exceptions
31 from oslo_serialization import base64 as base64utils
32 from oslo_utils import excutils
33 from oslo_utils import strutils
34 from oslo_utils import timeutils
35 from oslo_utils import units
36 from oslo_utils import uuidutils
37 import six
38 from six.moves import range
39 
40 from nova import availability_zones
41 from nova import block_device
42 from nova.cells import opts as cells_opts
43 from nova.compute import flavors
44 from nova.compute import instance_actions
45 from nova.compute import instance_list
46 from nova.compute import migration_list
47 from nova.compute import power_state
48 from nova.compute import rpcapi as compute_rpcapi
49 from nova.compute import task_states
50 from nova.compute import utils as compute_utils
51 from nova.compute.utils import wrap_instance_event
52 from nova.compute import vm_states
53 from nova import conductor
54 import nova.conf
55 from nova.consoleauth import rpcapi as consoleauth_rpcapi
56 from nova import context as nova_context
57 from nova import crypto
58 from nova.db import base
59 from nova import exception
60 from nova import exception_wrapper
61 from nova import hooks
62 from nova.i18n import _
63 from nova import image
64 from nova import network
65 from nova.network import model as network_model
66 from nova.network.security_group import openstack_driver
67 from nova.network.security_group import security_group_base
68 from nova import objects
69 from nova.objects import base as obj_base
70 from nova.objects import block_device as block_device_obj
71 from nova.objects import fields as fields_obj
72 from nova.objects import keypair as keypair_obj
73 from nova.objects import quotas as quotas_obj
74 from nova.pci import request as pci_request
75 import nova.policy
76 from nova import profiler
77 from nova import rpc
78 from nova.scheduler import client as scheduler_client
79 from nova.scheduler import utils as scheduler_utils
80 from nova import servicegroup
81 from nova import utils
82 from nova.virt import hardware
83 from nova.volume import cinder
84 
85 LOG = logging.getLogger(__name__)
86 
87 get_notifier = functools.partial(rpc.get_notifier, service='compute')
88 # NOTE(gibi): legacy notification used compute as a service but these
89 # calls still run on the client side of the compute service which is
90 # nova-api. By setting the binary to nova-api below, we can make sure
91 # that the new versioned notifications has the right publisher_id but the
92 # legacy notifications does not change.
93 wrap_exception = functools.partial(exception_wrapper.wrap_exception,
94                                    get_notifier=get_notifier,
95                                    binary='nova-api')
96 CONF = nova.conf.CONF
97 
98 RO_SECURITY_GROUPS = ['default']
99 
100 AGGREGATE_ACTION_UPDATE = 'Update'
101 AGGREGATE_ACTION_UPDATE_META = 'UpdateMeta'
102 AGGREGATE_ACTION_DELETE = 'Delete'
103 AGGREGATE_ACTION_ADD = 'Add'
104 BFV_RESERVE_MIN_COMPUTE_VERSION = 17
105 CINDER_V3_ATTACH_MIN_COMPUTE_VERSION = 24
106 MIN_COMPUTE_MULTIATTACH = 27
107 
108 # FIXME(danms): Keep a global cache of the cells we find the
109 # first time we look. This needs to be refreshed on a timer or
110 # trigger.
111 CELLS = []
112 
113 
114 def check_instance_state(vm_state=None, task_state=(None,),
115                          must_have_launched=True):
116     """Decorator to check VM and/or task state before entry to API functions.
117 
118     If the instance is in the wrong state, or has not been successfully
119     started at least once the wrapper will raise an exception.
120     """
121 
122     if vm_state is not None and not isinstance(vm_state, set):
123         vm_state = set(vm_state)
124     if task_state is not None and not isinstance(task_state, set):
125         task_state = set(task_state)
126 
127     def outer(f):
128         @six.wraps(f)
129         def inner(self, context, instance, *args, **kw):
130             if vm_state is not None and instance.vm_state not in vm_state:
131                 raise exception.InstanceInvalidState(
132                     attr='vm_state',
133                     instance_uuid=instance.uuid,
134                     state=instance.vm_state,
135                     method=f.__name__)
136             if (task_state is not None and
137                     instance.task_state not in task_state):
138                 raise exception.InstanceInvalidState(
139                     attr='task_state',
140                     instance_uuid=instance.uuid,
141                     state=instance.task_state,
142                     method=f.__name__)
143             if must_have_launched and not instance.launched_at:
144                 raise exception.InstanceInvalidState(
145                     attr='launched_at',
146                     instance_uuid=instance.uuid,
147                     state=instance.launched_at,
148                     method=f.__name__)
149 
150             return f(self, context, instance, *args, **kw)
151         return inner
152     return outer
153 
154 
155 def _set_or_none(q):
156     return q if q is None or isinstance(q, set) else set(q)
157 
158 
159 def reject_instance_state(vm_state=None, task_state=None):
160     """Decorator.  Raise InstanceInvalidState if instance is in any of the
161     given states.
162     """
163 
164     vm_state = _set_or_none(vm_state)
165     task_state = _set_or_none(task_state)
166 
167     def outer(f):
168         @six.wraps(f)
169         def inner(self, context, instance, *args, **kw):
170             _InstanceInvalidState = functools.partial(
171                 exception.InstanceInvalidState,
172                 instance_uuid=instance.uuid,
173                 method=f.__name__)
174 
175             if vm_state is not None and instance.vm_state in vm_state:
176                 raise _InstanceInvalidState(
177                     attr='vm_state', state=instance.vm_state)
178 
179             if task_state is not None and instance.task_state in task_state:
180                 raise _InstanceInvalidState(
181                     attr='task_state', state=instance.task_state)
182 
183             return f(self, context, instance, *args, **kw)
184         return inner
185     return outer
186 
187 
188 def check_instance_host(function):
189     @six.wraps(function)
190     def wrapped(self, context, instance, *args, **kwargs):
191         if not instance.host:
192             raise exception.InstanceNotReady(instance_id=instance.uuid)
193         return function(self, context, instance, *args, **kwargs)
194     return wrapped
195 
196 
197 def check_instance_lock(function):
198     @six.wraps(function)
199     def inner(self, context, instance, *args, **kwargs):
200         if instance.locked and not context.is_admin:
201             raise exception.InstanceIsLocked(instance_uuid=instance.uuid)
202         return function(self, context, instance, *args, **kwargs)
203     return inner
204 
205 
206 def check_instance_cell(fn):
207     @six.wraps(fn)
208     def _wrapped(self, context, instance, *args, **kwargs):
209         self._validate_cell(instance)
210         return fn(self, context, instance, *args, **kwargs)
211     return _wrapped
212 
213 
214 def _diff_dict(orig, new):
215     """Return a dict describing how to change orig to new.  The keys
216     correspond to values that have changed; the value will be a list
217     of one or two elements.  The first element of the list will be
218     either '+' or '-', indicating whether the key was updated or
219     deleted; if the key was updated, the list will contain a second
220     element, giving the updated value.
221     """
222     # Figure out what keys went away
223     result = {k: ['-'] for k in set(orig.keys()) - set(new.keys())}
224     # Compute the updates
225     for key, value in new.items():
226         if key not in orig or value != orig[key]:
227             result[key] = ['+', value]
228     return result
229 
230 
231 def load_cells():
232     global CELLS
233     if not CELLS:
234         CELLS = objects.CellMappingList.get_all(
235             nova_context.get_admin_context())
236         LOG.debug('Found %(count)i cells: %(cells)s',
237                   dict(count=len(CELLS),
238                        cells=','.join([c.identity for c in CELLS])))
239 
240     if not CELLS:
241         LOG.error('No cells are configured, unable to continue')
242 
243 
244 @profiler.trace_cls("compute_api")
245 class API(base.Base):
246     """API for interacting with the compute manager."""
247 
248     def __init__(self, image_api=None, network_api=None, volume_api=None,
249                  security_group_api=None, **kwargs):
250         self.image_api = image_api or image.API()
251         self.network_api = network_api or network.API()
252         self.volume_api = volume_api or cinder.API()
253         self.security_group_api = (security_group_api or
254             openstack_driver.get_openstack_security_group_driver())
255         self.consoleauth_rpcapi = consoleauth_rpcapi.ConsoleAuthAPI()
256         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
257         self.compute_task_api = conductor.ComputeTaskAPI()
258         self.servicegroup_api = servicegroup.API()
259         self.notifier = rpc.get_notifier('compute', CONF.host)
260         if CONF.ephemeral_storage_encryption.enabled:
261             self.key_manager = key_manager.API()
262         # Help us to record host in EventReporter
263         self.host = CONF.host
264         super(API, self).__init__(**kwargs)
265 
266     @property
267     def cell_type(self):
268         try:
269             return getattr(self, '_cell_type')
270         except AttributeError:
271             self._cell_type = cells_opts.get_cell_type()
272             return self._cell_type
273 
274     def _validate_cell(self, instance):
275         if self.cell_type != 'api':
276             return
277         cell_name = instance.cell_name
278         if not cell_name:
279             raise exception.InstanceUnknownCell(
280                     instance_uuid=instance.uuid)
281 
282     def _record_action_start(self, context, instance, action):
283         objects.InstanceAction.action_start(context, instance.uuid,
284                                             action, want_result=False)
285 
286     def _check_injected_file_quota(self, context, injected_files):
287         """Enforce quota limits on injected files.
288 
289         Raises a QuotaError if any limit is exceeded.
290         """
291         if injected_files is None:
292             return
293 
294         # Check number of files first
295         try:
296             objects.Quotas.limit_check(context,
297                                        injected_files=len(injected_files))
298         except exception.OverQuota:
299             raise exception.OnsetFileLimitExceeded()
300 
301         # OK, now count path and content lengths; we're looking for
302         # the max...
303         max_path = 0
304         max_content = 0
305         for path, content in injected_files:
306             max_path = max(max_path, len(path))
307             max_content = max(max_content, len(content))
308 
309         try:
310             objects.Quotas.limit_check(context,
311                                        injected_file_path_bytes=max_path,
312                                        injected_file_content_bytes=max_content)
313         except exception.OverQuota as exc:
314             # Favor path limit over content limit for reporting
315             # purposes
316             if 'injected_file_path_bytes' in exc.kwargs['overs']:
317                 raise exception.OnsetFilePathLimitExceeded(
318                       allowed=exc.kwargs['quotas']['injected_file_path_bytes'])
319             else:
320                 raise exception.OnsetFileContentLimitExceeded(
321                    allowed=exc.kwargs['quotas']['injected_file_content_bytes'])
322 
323     def _check_metadata_properties_quota(self, context, metadata=None):
324         """Enforce quota limits on metadata properties."""
325         if not metadata:
326             metadata = {}
327         if not isinstance(metadata, dict):
328             msg = (_("Metadata type should be dict."))
329             raise exception.InvalidMetadata(reason=msg)
330         num_metadata = len(metadata)
331         try:
332             objects.Quotas.limit_check(context, metadata_items=num_metadata)
333         except exception.OverQuota as exc:
334             quota_metadata = exc.kwargs['quotas']['metadata_items']
335             raise exception.MetadataLimitExceeded(allowed=quota_metadata)
336 
337         # Because metadata is stored in the DB, we hard-code the size limits
338         # In future, we may support more variable length strings, so we act
339         #  as if this is quota-controlled for forwards compatibility.
340         # Those are only used in V2 API, from V2.1 API, those checks are
341         # validated at API layer schema validation.
342         for k, v in metadata.items():
343             try:
344                 utils.check_string_length(v)
345                 utils.check_string_length(k, min_length=1)
346             except exception.InvalidInput as e:
347                 raise exception.InvalidMetadata(reason=e.format_message())
348 
349             if len(k) > 255:
350                 msg = _("Metadata property key greater than 255 characters")
351                 raise exception.InvalidMetadataSize(reason=msg)
352             if len(v) > 255:
353                 msg = _("Metadata property value greater than 255 characters")
354                 raise exception.InvalidMetadataSize(reason=msg)
355 
356     def _check_requested_secgroups(self, context, secgroups):
357         """Check if the security group requested exists and belongs to
358         the project.
359 
360         :param context: The nova request context.
361         :type context: nova.context.RequestContext
362         :param secgroups: list of requested security group names, or uuids in
363             the case of Neutron.
364         :type secgroups: list
365         :returns: list of requested security group names unmodified if using
366             nova-network. If using Neutron, the list returned is all uuids.
367             Note that 'default' is a special case and will be unmodified if
368             it's requested.
369         """
370         security_groups = []
371         for secgroup in secgroups:
372             # NOTE(sdague): default is handled special
373             if secgroup == "default":
374                 security_groups.append(secgroup)
375                 continue
376             secgroup_dict = self.security_group_api.get(context, secgroup)
377             if not secgroup_dict:
378                 raise exception.SecurityGroupNotFoundForProject(
379                     project_id=context.project_id, security_group_id=secgroup)
380 
381             # Check to see if it's a nova-network or neutron type.
382             if isinstance(secgroup_dict['id'], int):
383                 # This is nova-network so just return the requested name.
384                 security_groups.append(secgroup)
385             else:
386                 # The id for neutron is a uuid, so we return the id (uuid).
387                 security_groups.append(secgroup_dict['id'])
388 
389         return security_groups
390 
391     def _check_requested_networks(self, context, requested_networks,
392                                   max_count):
393         """Check if the networks requested belongs to the project
394         and the fixed IP address for each network provided is within
395         same the network block
396         """
397         if requested_networks is not None:
398             if requested_networks.no_allocate:
399                 # If the network request was specifically 'none' meaning don't
400                 # allocate any networks, we just return the number of requested
401                 # instances since quotas don't change at all.
402                 return max_count
403 
404             # NOTE(danms): Temporary transition
405             requested_networks = requested_networks.as_tuples()
406 
407         return self.network_api.validate_networks(context, requested_networks,
408                                                   max_count)
409 
410     def _handle_kernel_and_ramdisk(self, context, kernel_id, ramdisk_id,
411                                    image):
412         """Choose kernel and ramdisk appropriate for the instance.
413 
414         The kernel and ramdisk can be chosen in one of two ways:
415 
416             1. Passed in with create-instance request.
417 
418             2. Inherited from image metadata.
419 
420         If inherited from image metadata, and if that image metadata value is
421         set to 'nokernel', both kernel and ramdisk will default to None.
422         """
423         # Inherit from image if not specified
424         image_properties = image.get('properties', {})
425 
426         if kernel_id is None:
427             kernel_id = image_properties.get('kernel_id')
428 
429         if ramdisk_id is None:
430             ramdisk_id = image_properties.get('ramdisk_id')
431 
432         # Force to None if kernel_id indicates that a kernel is not to be used
433         if kernel_id == 'nokernel':
434             kernel_id = None
435             ramdisk_id = None
436 
437         # Verify kernel and ramdisk exist (fail-fast)
438         if kernel_id is not None:
439             kernel_image = self.image_api.get(context, kernel_id)
440             # kernel_id could have been a URI, not a UUID, so to keep behaviour
441             # from before, which leaked that implementation detail out to the
442             # caller, we return the image UUID of the kernel image and ramdisk
443             # image (below) and not any image URIs that might have been
444             # supplied.
445             # TODO(jaypipes): Get rid of this silliness once we move to a real
446             # Image object and hide all of that stuff within nova.image.api.
447             kernel_id = kernel_image['id']
448 
449         if ramdisk_id is not None:
450             ramdisk_image = self.image_api.get(context, ramdisk_id)
451             ramdisk_id = ramdisk_image['id']
452 
453         return kernel_id, ramdisk_id
454 
455     @staticmethod
456     def parse_availability_zone(context, availability_zone):
457         # NOTE(vish): We have a legacy hack to allow admins to specify hosts
458         #             via az using az:host:node. It might be nice to expose an
459         #             api to specify specific hosts to force onto, but for
460         #             now it just supports this legacy hack.
461         # NOTE(deva): It is also possible to specify az::node, in which case
462         #             the host manager will determine the correct host.
463         forced_host = None
464         forced_node = None
465         if availability_zone and ':' in availability_zone:
466             c = availability_zone.count(':')
467             if c == 1:
468                 availability_zone, forced_host = availability_zone.split(':')
469             elif c == 2:
470                 if '::' in availability_zone:
471                     availability_zone, forced_node = \
472                             availability_zone.split('::')
473                 else:
474                     availability_zone, forced_host, forced_node = \
475                             availability_zone.split(':')
476             else:
477                 raise exception.InvalidInput(
478                         reason="Unable to parse availability_zone")
479 
480         if not availability_zone:
481             availability_zone = CONF.default_schedule_zone
482 
483         return availability_zone, forced_host, forced_node
484 
485     def _ensure_auto_disk_config_is_valid(self, auto_disk_config_img,
486                                           auto_disk_config, image):
487         auto_disk_config_disabled = \
488                 utils.is_auto_disk_config_disabled(auto_disk_config_img)
489         if auto_disk_config_disabled and auto_disk_config:
490             raise exception.AutoDiskConfigDisabledByImage(image=image)
491 
492     def _inherit_properties_from_image(self, image, auto_disk_config):
493         image_properties = image.get('properties', {})
494         auto_disk_config_img = \
495                 utils.get_auto_disk_config_from_image_props(image_properties)
496         self._ensure_auto_disk_config_is_valid(auto_disk_config_img,
497                                                auto_disk_config,
498                                                image.get("id"))
499         if auto_disk_config is None:
500             auto_disk_config = strutils.bool_from_string(auto_disk_config_img)
501 
502         return {
503             'os_type': image_properties.get('os_type'),
504             'architecture': image_properties.get('architecture'),
505             'vm_mode': image_properties.get('vm_mode'),
506             'auto_disk_config': auto_disk_config
507         }
508 
509     def _new_instance_name_from_template(self, uuid, display_name, index):
510         params = {
511             'uuid': uuid,
512             'name': display_name,
513             'count': index + 1,
514         }
515         try:
516             new_name = (CONF.multi_instance_display_name_template %
517                         params)
518         except (KeyError, TypeError):
519             LOG.exception('Failed to set instance name using '
520                           'multi_instance_display_name_template.')
521             new_name = display_name
522         return new_name
523 
524     def _apply_instance_name_template(self, context, instance, index):
525         original_name = instance.display_name
526         new_name = self._new_instance_name_from_template(instance.uuid,
527                 instance.display_name, index)
528         instance.display_name = new_name
529         if not instance.get('hostname', None):
530             if utils.sanitize_hostname(original_name) == "":
531                 instance.hostname = self._default_host_name(instance.uuid)
532             else:
533                 instance.hostname = utils.sanitize_hostname(new_name)
534         return instance
535 
536     def _check_config_drive(self, config_drive):
537         if config_drive:
538             try:
539                 bool_val = strutils.bool_from_string(config_drive,
540                                                      strict=True)
541             except ValueError:
542                 raise exception.ConfigDriveInvalidValue(option=config_drive)
543         else:
544             bool_val = False
545         # FIXME(comstud):  Bug ID 1193438 filed for this. This looks silly,
546         # but this is because the config drive column is a String.  False
547         # is represented by using an empty string.  And for whatever
548         # reason, we rely on the DB to cast True to a String.
549         return True if bool_val else ''
550 
551     def _check_requested_image(self, context, image_id, image,
552                                instance_type, root_bdm):
553         if not image:
554             return
555 
556         if image['status'] != 'active':
557             raise exception.ImageNotActive(image_id=image_id)
558 
559         image_properties = image.get('properties', {})
560         config_drive_option = image_properties.get(
561             'img_config_drive', 'optional')
562         if config_drive_option not in ['optional', 'mandatory']:
563             raise exception.InvalidImageConfigDrive(
564                 config_drive=config_drive_option)
565 
566         if instance_type['memory_mb'] < int(image.get('min_ram') or 0):
567             raise exception.FlavorMemoryTooSmall()
568 
569         # Image min_disk is in gb, size is in bytes. For sanity, have them both
570         # in bytes.
571         image_min_disk = int(image.get('min_disk') or 0) * units.Gi
572         image_size = int(image.get('size') or 0)
573 
574         # Target disk is a volume. Don't check flavor disk size because it
575         # doesn't make sense, and check min_disk against the volume size.
576         if (root_bdm is not None and root_bdm.is_volume):
577             # There are 2 possibilities here: either the target volume already
578             # exists, or it doesn't, in which case the bdm will contain the
579             # intended volume size.
580             #
581             # Cinder does its own check against min_disk, so if the target
582             # volume already exists this has already been done and we don't
583             # need to check it again here. In this case, volume_size may not be
584             # set on the bdm.
585             #
586             # If we're going to create the volume, the bdm will contain
587             # volume_size. Therefore we should check it if it exists. This will
588             # still be checked again by cinder when the volume is created, but
589             # that will not happen until the request reaches a host. By
590             # checking it here, the user gets an immediate and useful failure
591             # indication.
592             #
593             # The third possibility is that we have failed to consider
594             # something, and there are actually more than 2 possibilities. In
595             # this case cinder will still do the check at volume creation time.
596             # The behaviour will still be correct, but the user will not get an
597             # immediate failure from the api, and will instead have to
598             # determine why the instance is in an error state with a task of
599             # block_device_mapping.
600             #
601             # We could reasonably refactor this check into _validate_bdm at
602             # some future date, as the various size logic is already split out
603             # in there.
604             dest_size = root_bdm.volume_size
605             if dest_size is not None:
606                 dest_size *= units.Gi
607 
608                 if image_min_disk > dest_size:
609                     raise exception.VolumeSmallerThanMinDisk(
610                         volume_size=dest_size, image_min_disk=image_min_disk)
611 
612         # Target disk is a local disk whose size is taken from the flavor
613         else:
614             dest_size = instance_type['root_gb'] * units.Gi
615 
616             # NOTE(johannes): root_gb is allowed to be 0 for legacy reasons
617             # since libvirt interpreted the value differently than other
618             # drivers. A value of 0 means don't check size.
619             if dest_size != 0:
620                 if image_size > dest_size:
621                     raise exception.FlavorDiskSmallerThanImage(
622                         flavor_size=dest_size, image_size=image_size)
623 
624                 if image_min_disk > dest_size:
625                     raise exception.FlavorDiskSmallerThanMinDisk(
626                         flavor_size=dest_size, image_min_disk=image_min_disk)
627 
628     def _get_image_defined_bdms(self, instance_type, image_meta,
629                                 root_device_name):
630         image_properties = image_meta.get('properties', {})
631 
632         # Get the block device mappings defined by the image.
633         image_defined_bdms = image_properties.get('block_device_mapping', [])
634         legacy_image_defined = not image_properties.get('bdm_v2', False)
635 
636         image_mapping = image_properties.get('mappings', [])
637 
638         if legacy_image_defined:
639             image_defined_bdms = block_device.from_legacy_mapping(
640                 image_defined_bdms, None, root_device_name)
641         else:
642             image_defined_bdms = list(map(block_device.BlockDeviceDict,
643                                           image_defined_bdms))
644 
645         if image_mapping:
646             image_mapping = self._prepare_image_mapping(instance_type,
647                                                         image_mapping)
648             image_defined_bdms = self._merge_bdms_lists(
649                 image_mapping, image_defined_bdms)
650 
651         return image_defined_bdms
652 
653     def _get_flavor_defined_bdms(self, instance_type, block_device_mapping):
654         flavor_defined_bdms = []
655 
656         have_ephemeral_bdms = any(filter(
657             block_device.new_format_is_ephemeral, block_device_mapping))
658         have_swap_bdms = any(filter(
659             block_device.new_format_is_swap, block_device_mapping))
660 
661         if instance_type.get('ephemeral_gb') and not have_ephemeral_bdms:
662             flavor_defined_bdms.append(
663                 block_device.create_blank_bdm(instance_type['ephemeral_gb']))
664         if instance_type.get('swap') and not have_swap_bdms:
665             flavor_defined_bdms.append(
666                 block_device.create_blank_bdm(instance_type['swap'], 'swap'))
667 
668         return flavor_defined_bdms
669 
670     def _merge_bdms_lists(self, overridable_mappings, overrider_mappings):
671         """Override any block devices from the first list by device name
672 
673         :param overridable_mappings: list which items are overridden
674         :param overrider_mappings: list which items override
675 
676         :returns: A merged list of bdms
677         """
678         device_names = set(bdm['device_name'] for bdm in overrider_mappings
679                            if bdm['device_name'])
680         return (overrider_mappings +
681                 [bdm for bdm in overridable_mappings
682                  if bdm['device_name'] not in device_names])
683 
684     def _check_and_transform_bdm(self, context, base_options, instance_type,
685                                  image_meta, min_count, max_count,
686                                  block_device_mapping, legacy_bdm):
687         # NOTE (ndipanov): Assume root dev name is 'vda' if not supplied.
688         #                  It's needed for legacy conversion to work.
689         root_device_name = (base_options.get('root_device_name') or 'vda')
690         image_ref = base_options.get('image_ref', '')
691         # If the instance is booted by image and has a volume attached,
692         # the volume cannot have the same device name as root_device_name
693         if image_ref:
694             for bdm in block_device_mapping:
695                 if (bdm.get('destination_type') == 'volume' and
696                     block_device.strip_dev(bdm.get(
697                     'device_name')) == root_device_name):
698                     msg = _('The volume cannot be assigned the same device'
699                             ' name as the root device %s') % root_device_name
700                     raise exception.InvalidRequest(msg)
701 
702         image_defined_bdms = self._get_image_defined_bdms(
703             instance_type, image_meta, root_device_name)
704         root_in_image_bdms = (
705             block_device.get_root_bdm(image_defined_bdms) is not None)
706 
707         if legacy_bdm:
708             block_device_mapping = block_device.from_legacy_mapping(
709                 block_device_mapping, image_ref, root_device_name,
710                 no_root=root_in_image_bdms)
711         elif root_in_image_bdms:
712             # NOTE (ndipanov): client will insert an image mapping into the v2
713             # block_device_mapping, but if there is a bootable device in image
714             # mappings - we need to get rid of the inserted image
715             # NOTE (gibi): another case is when a server is booted with an
716             # image to bdm mapping where the image only contains a bdm to a
717             # snapshot. In this case the other image to bdm mapping
718             # contains an unnecessary device with boot_index == 0.
719             # Also in this case the image_ref is None as we are booting from
720             # an image to volume bdm.
721             def not_image_and_root_bdm(bdm):
722                 return not (bdm.get('boot_index') == 0 and
723                             bdm.get('source_type') == 'image')
724 
725             block_device_mapping = list(
726                 filter(not_image_and_root_bdm, block_device_mapping))
727 
728         block_device_mapping = self._merge_bdms_lists(
729             image_defined_bdms, block_device_mapping)
730 
731         if min_count > 1 or max_count > 1:
732             if any(map(lambda bdm: bdm['source_type'] == 'volume',
733                        block_device_mapping)):
734                 msg = _('Cannot attach one or more volumes to multiple'
735                         ' instances')
736                 raise exception.InvalidRequest(msg)
737 
738         block_device_mapping += self._get_flavor_defined_bdms(
739             instance_type, block_device_mapping)
740 
741         return block_device_obj.block_device_make_list_from_dicts(
742                 context, block_device_mapping)
743 
744     def _get_image(self, context, image_href):
745         if not image_href:
746             return None, {}
747 
748         image = self.image_api.get(context, image_href)
749         return image['id'], image
750 
751     def _checks_for_create_and_rebuild(self, context, image_id, image,
752                                        instance_type, metadata,
753                                        files_to_inject, root_bdm):
754         self._check_metadata_properties_quota(context, metadata)
755         self._check_injected_file_quota(context, files_to_inject)
756         self._check_requested_image(context, image_id, image,
757                                     instance_type, root_bdm)
758 
759     def _validate_and_build_base_options(self, context, instance_type,
760                                          boot_meta, image_href, image_id,
761                                          kernel_id, ramdisk_id, display_name,
762                                          display_description, key_name,
763                                          key_data, security_groups,
764                                          availability_zone, user_data,
765                                          metadata, access_ip_v4, access_ip_v6,
766                                          requested_networks, config_drive,
767                                          auto_disk_config, reservation_id,
768                                          max_count):
769         """Verify all the input parameters regardless of the provisioning
770         strategy being performed.
771         """
772         if instance_type['disabled']:
773             raise exception.FlavorNotFound(flavor_id=instance_type['id'])
774 
775         if user_data:
776             try:
777                 base64utils.decode_as_bytes(user_data)
778             except TypeError:
779                 raise exception.InstanceUserDataMalformed()
780 
781         # When using Neutron, _check_requested_secgroups will translate and
782         # return any requested security group names to uuids.
783         security_groups = (
784             self._check_requested_secgroups(context, security_groups))
785 
786         # Note:  max_count is the number of instances requested by the user,
787         # max_network_count is the maximum number of instances taking into
788         # account any network quotas
789         max_network_count = self._check_requested_networks(context,
790                                      requested_networks, max_count)
791 
792         kernel_id, ramdisk_id = self._handle_kernel_and_ramdisk(
793                 context, kernel_id, ramdisk_id, boot_meta)
794 
795         config_drive = self._check_config_drive(config_drive)
796 
797         if key_data is None and key_name is not None:
798             key_pair = objects.KeyPair.get_by_name(context,
799                                                    context.user_id,
800                                                    key_name)
801             key_data = key_pair.public_key
802         else:
803             key_pair = None
804 
805         root_device_name = block_device.prepend_dev(
806                 block_device.properties_root_device_name(
807                     boot_meta.get('properties', {})))
808 
809         try:
810             image_meta = objects.ImageMeta.from_dict(boot_meta)
811         except ValueError as e:
812             # there must be invalid values in the image meta properties so
813             # consider this an invalid request
814             msg = _('Invalid image metadata. Error: %s') % six.text_type(e)
815             raise exception.InvalidRequest(msg)
816         numa_topology = hardware.numa_get_constraints(
817                 instance_type, image_meta)
818 
819         system_metadata = {}
820 
821         # PCI requests come from two sources: instance flavor and
822         # requested_networks. The first call in below returns an
823         # InstancePCIRequests object which is a list of InstancePCIRequest
824         # objects. The second call in below creates an InstancePCIRequest
825         # object for each SR-IOV port, and append it to the list in the
826         # InstancePCIRequests object
827         pci_request_info = pci_request.get_pci_requests_from_flavor(
828             instance_type)
829         self.network_api.create_pci_requests_for_sriov_ports(context,
830             pci_request_info, requested_networks)
831 
832         base_options = {
833             'reservation_id': reservation_id,
834             'image_ref': image_href,
835             'kernel_id': kernel_id or '',
836             'ramdisk_id': ramdisk_id or '',
837             'power_state': power_state.NOSTATE,
838             'vm_state': vm_states.BUILDING,
839             'config_drive': config_drive,
840             'user_id': context.user_id,
841             'project_id': context.project_id,
842             'instance_type_id': instance_type['id'],
843             'memory_mb': instance_type['memory_mb'],
844             'vcpus': instance_type['vcpus'],
845             'root_gb': instance_type['root_gb'],
846             'ephemeral_gb': instance_type['ephemeral_gb'],
847             'display_name': display_name,
848             'display_description': display_description,
849             'user_data': user_data,
850             'key_name': key_name,
851             'key_data': key_data,
852             'locked': False,
853             'metadata': metadata or {},
854             'access_ip_v4': access_ip_v4,
855             'access_ip_v6': access_ip_v6,
856             'availability_zone': availability_zone,
857             'root_device_name': root_device_name,
858             'progress': 0,
859             'pci_requests': pci_request_info,
860             'numa_topology': numa_topology,
861             'system_metadata': system_metadata}
862 
863         options_from_image = self._inherit_properties_from_image(
864                 boot_meta, auto_disk_config)
865 
866         base_options.update(options_from_image)
867 
868         # return the validated options and maximum number of instances allowed
869         # by the network quotas
870         return base_options, max_network_count, key_pair, security_groups
871 
872     def _provision_instances(self, context, instance_type, min_count,
873             max_count, base_options, boot_meta, security_groups,
874             block_device_mapping, shutdown_terminate,
875             instance_group, check_server_group_quota, filter_properties,
876             key_pair, tags, trusted_certs, supports_multiattach=False):
877         # Check quotas
878         num_instances = compute_utils.check_num_instances_quota(
879                 context, instance_type, min_count, max_count)
880         security_groups = self.security_group_api.populate_security_groups(
881                 security_groups)
882         self.security_group_api.ensure_default(context)
883         LOG.debug("Going to run %s instances...", num_instances)
884         instances_to_build = []
885         try:
886             for i in range(num_instances):
887                 # Create a uuid for the instance so we can store the
888                 # RequestSpec before the instance is created.
889                 instance_uuid = uuidutils.generate_uuid()
890                 # Store the RequestSpec that will be used for scheduling.
891                 req_spec = objects.RequestSpec.from_components(context,
892                         instance_uuid, boot_meta, instance_type,
893                         base_options['numa_topology'],
894                         base_options['pci_requests'], filter_properties,
895                         instance_group, base_options['availability_zone'],
896                         security_groups=security_groups)
897                 # NOTE(danms): We need to record num_instances on the request
898                 # spec as this is how the conductor knows how many were in this
899                 # batch.
900                 req_spec.num_instances = num_instances
901                 req_spec.create()
902 
903                 # Create an instance object, but do not store in db yet.
904                 instance = objects.Instance(context=context)
905                 instance.uuid = instance_uuid
906                 instance.update(base_options)
907                 instance.keypairs = objects.KeyPairList(objects=[])
908                 if key_pair:
909                     instance.keypairs.objects.append(key_pair)
910 
911                 instance.trusted_certs = self._retrieve_trusted_certs_object(
912                     trusted_certs)
913 
914                 instance = self.create_db_entry_for_new_instance(context,
915                         instance_type, boot_meta, instance, security_groups,
916                         block_device_mapping, num_instances, i,
917                         shutdown_terminate, create_instance=False)
918                 block_device_mapping = (
919                     self._bdm_validate_set_size_and_instance(context,
920                         instance, instance_type, block_device_mapping,
921                         supports_multiattach))
922                 instance_tags = self._transform_tags(tags, instance.uuid)
923 
924                 build_request = objects.BuildRequest(context,
925                         instance=instance, instance_uuid=instance.uuid,
926                         project_id=instance.project_id,
927                         block_device_mappings=block_device_mapping,
928                         tags=instance_tags)
929                 build_request.create()
930 
931                 # Create an instance_mapping.  The null cell_mapping indicates
932                 # that the instance doesn't yet exist in a cell, and lookups
933                 # for it need to instead look for the RequestSpec.
934                 # cell_mapping will be populated after scheduling, with a
935                 # scheduling failure using the cell_mapping for the special
936                 # cell0.
937                 inst_mapping = objects.InstanceMapping(context=context)
938                 inst_mapping.instance_uuid = instance_uuid
939                 inst_mapping.project_id = context.project_id
940                 inst_mapping.cell_mapping = None
941                 inst_mapping.create()
942 
943                 instances_to_build.append(
944                     (req_spec, build_request, inst_mapping))
945 
946                 if instance_group:
947                     if check_server_group_quota:
948                         try:
949                             objects.Quotas.check_deltas(
950                                 context, {'server_group_members': 1},
951                                 instance_group, context.user_id)
952                         except exception.OverQuota:
953                             msg = _("Quota exceeded, too many servers in "
954                                     "group")
955                             raise exception.QuotaError(msg)
956 
957                     members = objects.InstanceGroup.add_members(
958                         context, instance_group.uuid, [instance.uuid])
959 
960                     # NOTE(melwitt): We recheck the quota after creating the
961                     # object to prevent users from allocating more resources
962                     # than their allowed quota in the event of a race. This is
963                     # configurable because it can be expensive if strict quota
964                     # limits are not required in a deployment.
965                     if CONF.quota.recheck_quota and check_server_group_quota:
966                         try:
967                             objects.Quotas.check_deltas(
968                                 context, {'server_group_members': 0},
969                                 instance_group, context.user_id)
970                         except exception.OverQuota:
971                             objects.InstanceGroup._remove_members_in_db(
972                                 context, instance_group.id, [instance.uuid])
973                             msg = _("Quota exceeded, too many servers in "
974                                     "group")
975                             raise exception.QuotaError(msg)
976                     # list of members added to servers group in this iteration
977                     # is needed to check quota of server group during add next
978                     # instance
979                     instance_group.members.extend(members)
980 
981         # In the case of any exceptions, attempt DB cleanup
982         except Exception:
983             with excutils.save_and_reraise_exception():
984                 self._cleanup_build_artifacts(None, instances_to_build)
985 
986         return instances_to_build
987 
988     @staticmethod
989     def _retrieve_trusted_certs_object(trusted_certs):
990         # Retrieve trusted_certs parameter, or use CONF value if certificate
991         # validation is enabled
992         if trusted_certs:
993             return objects.TrustedCerts(ids=trusted_certs)
994         elif (CONF.glance.verify_glance_signatures and
995               CONF.glance.enable_certificate_validation and
996               CONF.glance.default_trusted_certificate_ids):
997             return objects.TrustedCerts(
998                 ids=CONF.glance.default_trusted_certificate_ids)
999         else:
1000             return None
1001 
1002     def _get_bdm_image_metadata(self, context, block_device_mapping,
1003                                 legacy_bdm=True):
1004         """If we are booting from a volume, we need to get the
1005         volume details from Cinder and make sure we pass the
1006         metadata back accordingly.
1007         """
1008         if not block_device_mapping:
1009             return {}
1010 
1011         for bdm in block_device_mapping:
1012             if (legacy_bdm and
1013                     block_device.get_device_letter(
1014                        bdm.get('device_name', '')) != 'a'):
1015                 continue
1016             elif not legacy_bdm and bdm.get('boot_index') != 0:
1017                 continue
1018 
1019             volume_id = bdm.get('volume_id')
1020             snapshot_id = bdm.get('snapshot_id')
1021             if snapshot_id:
1022                 # NOTE(alaski): A volume snapshot inherits metadata from the
1023                 # originating volume, but the API does not expose metadata
1024                 # on the snapshot itself.  So we query the volume for it below.
1025                 snapshot = self.volume_api.get_snapshot(context, snapshot_id)
1026                 volume_id = snapshot['volume_id']
1027 
1028             if bdm.get('image_id'):
1029                 try:
1030                     image_id = bdm['image_id']
1031                     image_meta = self.image_api.get(context, image_id)
1032                     return image_meta
1033                 except Exception:
1034                     raise exception.InvalidBDMImage(id=image_id)
1035             elif volume_id:
1036                 try:
1037                     volume = self.volume_api.get(context, volume_id)
1038                 except exception.CinderConnectionFailed:
1039                     raise
1040                 except Exception:
1041                     raise exception.InvalidBDMVolume(id=volume_id)
1042 
1043                 if not volume.get('bootable', True):
1044                     raise exception.InvalidBDMVolumeNotBootable(id=volume_id)
1045 
1046                 return utils.get_image_metadata_from_volume(volume)
1047         return {}
1048 
1049     @staticmethod
1050     def _get_requested_instance_group(context, filter_properties):
1051         if (not filter_properties or
1052                 not filter_properties.get('scheduler_hints')):
1053             return
1054 
1055         group_hint = filter_properties.get('scheduler_hints').get('group')
1056         if not group_hint:
1057             return
1058 
1059         return objects.InstanceGroup.get_by_uuid(context, group_hint)
1060 
1061     def _create_instance(self, context, instance_type,
1062                image_href, kernel_id, ramdisk_id,
1063                min_count, max_count,
1064                display_name, display_description,
1065                key_name, key_data, security_groups,
1066                availability_zone, user_data, metadata, injected_files,
1067                admin_password, access_ip_v4, access_ip_v6,
1068                requested_networks, config_drive,
1069                block_device_mapping, auto_disk_config, filter_properties,
1070                reservation_id=None, legacy_bdm=True, shutdown_terminate=False,
1071                check_server_group_quota=False, tags=None,
1072                supports_multiattach=False, trusted_certs=None):
1073         """Verify all the input parameters regardless of the provisioning
1074         strategy being performed and schedule the instance(s) for
1075         creation.
1076         """
1077 
1078         # Normalize and setup some parameters
1079         if reservation_id is None:
1080             reservation_id = utils.generate_uid('r')
1081         security_groups = security_groups or ['default']
1082         min_count = min_count or 1
1083         max_count = max_count or min_count
1084         block_device_mapping = block_device_mapping or []
1085         tags = tags or []
1086 
1087         if image_href:
1088             image_id, boot_meta = self._get_image(context, image_href)
1089         else:
1090             image_id = None
1091             boot_meta = self._get_bdm_image_metadata(
1092                 context, block_device_mapping, legacy_bdm)
1093 
1094         self._check_auto_disk_config(image=boot_meta,
1095                                      auto_disk_config=auto_disk_config)
1096 
1097         base_options, max_net_count, key_pair, security_groups = \
1098                 self._validate_and_build_base_options(
1099                     context, instance_type, boot_meta, image_href, image_id,
1100                     kernel_id, ramdisk_id, display_name, display_description,
1101                     key_name, key_data, security_groups, availability_zone,
1102                     user_data, metadata, access_ip_v4, access_ip_v6,
1103                     requested_networks, config_drive, auto_disk_config,
1104                     reservation_id, max_count)
1105 
1106         # max_net_count is the maximum number of instances requested by the
1107         # user adjusted for any network quota constraints, including
1108         # consideration of connections to each requested network
1109         if max_net_count < min_count:
1110             raise exception.PortLimitExceeded()
1111         elif max_net_count < max_count:
1112             LOG.info("max count reduced from %(max_count)d to "
1113                      "%(max_net_count)d due to network port quota",
1114                      {'max_count': max_count,
1115                       'max_net_count': max_net_count})
1116             max_count = max_net_count
1117 
1118         block_device_mapping = self._check_and_transform_bdm(context,
1119             base_options, instance_type, boot_meta, min_count, max_count,
1120             block_device_mapping, legacy_bdm)
1121 
1122         # We can't do this check earlier because we need bdms from all sources
1123         # to have been merged in order to get the root bdm.
1124         self._checks_for_create_and_rebuild(context, image_id, boot_meta,
1125                 instance_type, metadata, injected_files,
1126                 block_device_mapping.root_bdm())
1127 
1128         instance_group = self._get_requested_instance_group(context,
1129                                    filter_properties)
1130 
1131         tags = self._create_tag_list_obj(context, tags)
1132 
1133         instances_to_build = self._provision_instances(
1134             context, instance_type, min_count, max_count, base_options,
1135             boot_meta, security_groups, block_device_mapping,
1136             shutdown_terminate, instance_group, check_server_group_quota,
1137             filter_properties, key_pair, tags, trusted_certs,
1138             supports_multiattach)
1139 
1140         instances = []
1141         request_specs = []
1142         build_requests = []
1143         for rs, build_request, im in instances_to_build:
1144             build_requests.append(build_request)
1145             instance = build_request.get_new_instance(context)
1146             instances.append(instance)
1147             request_specs.append(rs)
1148 
1149         if CONF.cells.enable:
1150             # NOTE(danms): CellsV1 can't do the new thing, so we
1151             # do the old thing here. We can remove this path once
1152             # we stop supporting v1.
1153             for instance in instances:
1154                 instance.create()
1155             # NOTE(melwitt): We recheck the quota after creating the objects
1156             # to prevent users from allocating more resources than their
1157             # allowed quota in the event of a race. This is configurable
1158             # because it can be expensive if strict quota limits are not
1159             # required in a deployment.
1160             if CONF.quota.recheck_quota:
1161                 try:
1162                     compute_utils.check_num_instances_quota(
1163                         context, instance_type, 0, 0,
1164                         orig_num_req=len(instances))
1165                 except exception.TooManyInstances:
1166                     with excutils.save_and_reraise_exception():
1167                         # Need to clean up all the instances we created
1168                         # along with the build requests, request specs,
1169                         # and instance mappings.
1170                         self._cleanup_build_artifacts(instances,
1171                                                       instances_to_build)
1172 
1173             self.compute_task_api.build_instances(context,
1174                 instances=instances, image=boot_meta,
1175                 filter_properties=filter_properties,
1176                 admin_password=admin_password,
1177                 injected_files=injected_files,
1178                 requested_networks=requested_networks,
1179                 security_groups=security_groups,
1180                 block_device_mapping=block_device_mapping,
1181                 legacy_bdm=False)
1182         else:
1183             self.compute_task_api.schedule_and_build_instances(
1184                 context,
1185                 build_requests=build_requests,
1186                 request_spec=request_specs,
1187                 image=boot_meta,
1188                 admin_password=admin_password,
1189                 injected_files=injected_files,
1190                 requested_networks=requested_networks,
1191                 block_device_mapping=block_device_mapping,
1192                 tags=tags)
1193 
1194         return instances, reservation_id
1195 
1196     @staticmethod
1197     def _cleanup_build_artifacts(instances, instances_to_build):
1198         # instances_to_build is a list of tuples:
1199         # (RequestSpec, BuildRequest, InstanceMapping)
1200 
1201         # Be paranoid about artifacts being deleted underneath us.
1202         for instance in instances or []:
1203             try:
1204                 instance.destroy()
1205             except exception.InstanceNotFound:
1206                 pass
1207         for rs, build_request, im in instances_to_build or []:
1208             try:
1209                 rs.destroy()
1210             except exception.RequestSpecNotFound:
1211                 pass
1212             try:
1213                 build_request.destroy()
1214             except exception.BuildRequestNotFound:
1215                 pass
1216             try:
1217                 im.destroy()
1218             except exception.InstanceMappingNotFound:
1219                 pass
1220 
1221     @staticmethod
1222     def _volume_size(instance_type, bdm):
1223         size = bdm.get('volume_size')
1224         # NOTE (ndipanov): inherit flavor size only for swap and ephemeral
1225         if (size is None and bdm.get('source_type') == 'blank' and
1226                 bdm.get('destination_type') == 'local'):
1227             if bdm.get('guest_format') == 'swap':
1228                 size = instance_type.get('swap', 0)
1229             else:
1230                 size = instance_type.get('ephemeral_gb', 0)
1231         return size
1232 
1233     def _prepare_image_mapping(self, instance_type, mappings):
1234         """Extract and format blank devices from image mappings."""
1235 
1236         prepared_mappings = []
1237 
1238         for bdm in block_device.mappings_prepend_dev(mappings):
1239             LOG.debug("Image bdm %s", bdm)
1240 
1241             virtual_name = bdm['virtual']
1242             if virtual_name == 'ami' or virtual_name == 'root':
1243                 continue
1244 
1245             if not block_device.is_swap_or_ephemeral(virtual_name):
1246                 continue
1247 
1248             guest_format = bdm.get('guest_format')
1249             if virtual_name == 'swap':
1250                 guest_format = 'swap'
1251             if not guest_format:
1252                 guest_format = CONF.default_ephemeral_format
1253 
1254             values = block_device.BlockDeviceDict({
1255                 'device_name': bdm['device'],
1256                 'source_type': 'blank',
1257                 'destination_type': 'local',
1258                 'device_type': 'disk',
1259                 'guest_format': guest_format,
1260                 'delete_on_termination': True,
1261                 'boot_index': -1})
1262 
1263             values['volume_size'] = self._volume_size(
1264                 instance_type, values)
1265             if values['volume_size'] == 0:
1266                 continue
1267 
1268             prepared_mappings.append(values)
1269 
1270         return prepared_mappings
1271 
1272     def _bdm_validate_set_size_and_instance(self, context, instance,
1273                                             instance_type,
1274                                             block_device_mapping,
1275                                             supports_multiattach=False):
1276         """Ensure the bdms are valid, then set size and associate with instance
1277 
1278         Because this method can be called multiple times when more than one
1279         instance is booted in a single request it makes a copy of the bdm list.
1280         """
1281         LOG.debug("block_device_mapping %s", list(block_device_mapping),
1282                   instance_uuid=instance.uuid)
1283         self._validate_bdm(
1284             context, instance, instance_type, block_device_mapping,
1285             supports_multiattach)
1286         instance_block_device_mapping = block_device_mapping.obj_clone()
1287         for bdm in instance_block_device_mapping:
1288             bdm.volume_size = self._volume_size(instance_type, bdm)
1289             bdm.instance_uuid = instance.uuid
1290         return instance_block_device_mapping
1291 
1292     def _create_block_device_mapping(self, block_device_mapping):
1293         # Copy the block_device_mapping because this method can be called
1294         # multiple times when more than one instance is booted in a single
1295         # request. This avoids 'id' being set and triggering the object dupe
1296         # detection
1297         db_block_device_mapping = copy.deepcopy(block_device_mapping)
1298         # Create the BlockDeviceMapping objects in the db.
1299         for bdm in db_block_device_mapping:
1300             # TODO(alaski): Why is this done?
1301             if bdm.volume_size == 0:
1302                 continue
1303 
1304             bdm.update_or_create()
1305 
1306     def _validate_bdm(self, context, instance, instance_type,
1307                       block_device_mappings, supports_multiattach=False):
1308         # Make sure that the boot indexes make sense.
1309         # Setting a negative value or None indicates that the device should not
1310         # be used for booting.
1311         boot_indexes = sorted([bdm.boot_index
1312                                for bdm in block_device_mappings
1313                                if bdm.boot_index is not None
1314                                and bdm.boot_index >= 0])
1315 
1316         # Each device which is capable of being used as boot device should
1317         # be given a unique boot index, starting from 0 in ascending order.
1318         if any(i != v for i, v in enumerate(boot_indexes)):
1319             # Convert the BlockDeviceMappingList to a list for repr details.
1320             LOG.debug('Invalid block device mapping boot sequence for '
1321                       'instance: %s', list(block_device_mappings),
1322                       instance=instance)
1323             raise exception.InvalidBDMBootSequence()
1324 
1325         for bdm in block_device_mappings:
1326             # NOTE(vish): For now, just make sure the volumes are accessible.
1327             # Additionally, check that the volume can be attached to this
1328             # instance.
1329             snapshot_id = bdm.snapshot_id
1330             volume_id = bdm.volume_id
1331             image_id = bdm.image_id
1332             if (image_id is not None and
1333                     image_id != instance.get('image_ref')):
1334                 try:
1335                     self._get_image(context, image_id)
1336                 except Exception:
1337                     raise exception.InvalidBDMImage(id=image_id)
1338                 if (bdm.source_type == 'image' and
1339                         bdm.destination_type == 'volume' and
1340                         not bdm.volume_size):
1341                     raise exception.InvalidBDM(message=_("Images with "
1342                         "destination_type 'volume' need to have a non-zero "
1343                         "size specified"))
1344             elif volume_id is not None:
1345                 # The instance is being created and we don't know which
1346                 # cell it's going to land in, so check all cells.
1347                 min_compute_version = \
1348                     objects.service.get_minimum_version_all_cells(
1349                         context, ['nova-compute'])
1350                 try:
1351                     # NOTE(ildikov): The boot from volume operation did not
1352                     # reserve the volume before Pike and as the older computes
1353                     # are running 'check_attach' which will fail if the volume
1354                     # is in 'attaching' state; if the compute service version
1355                     # is not high enough we will just perform the old check as
1356                     # opposed to reserving the volume here.
1357                     volume = self.volume_api.get(context, volume_id)
1358                     if (min_compute_version >=
1359                         BFV_RESERVE_MIN_COMPUTE_VERSION):
1360                         self._check_attach_and_reserve_volume(
1361                             context, volume, instance, bdm,
1362                             supports_multiattach)
1363                     else:
1364                         # NOTE(ildikov): This call is here only for backward
1365                         # compatibility can be removed after Ocata EOL.
1366                         self._check_attach(context, volume, instance)
1367                     bdm.volume_size = volume.get('size')
1368 
1369                     # NOTE(mnaser): If we end up reserving the volume, it will
1370                     #               not have an attachment_id which is needed
1371                     #               for cleanups.  This can be removed once
1372                     #               all calls to reserve_volume are gone.
1373                     if 'attachment_id' not in bdm:
1374                         bdm.attachment_id = None
1375                 except (exception.CinderConnectionFailed,
1376                         exception.InvalidVolume,
1377                         exception.MultiattachNotSupportedOldMicroversion,
1378                         exception.MultiattachSupportNotYetAvailable):
1379                     raise
1380                 except exception.InvalidInput as exc:
1381                     raise exception.InvalidVolume(reason=exc.format_message())
1382                 except Exception:
1383                     raise exception.InvalidBDMVolume(id=volume_id)
1384             elif snapshot_id is not None:
1385                 try:
1386                     snap = self.volume_api.get_snapshot(context, snapshot_id)
1387                     bdm.volume_size = bdm.volume_size or snap.get('size')
1388                 except exception.CinderConnectionFailed:
1389                     raise
1390                 except Exception:
1391                     raise exception.InvalidBDMSnapshot(id=snapshot_id)
1392             elif (bdm.source_type == 'blank' and
1393                     bdm.destination_type == 'volume' and
1394                     not bdm.volume_size):
1395                 raise exception.InvalidBDM(message=_("Blank volumes "
1396                     "(source: 'blank', dest: 'volume') need to have non-zero "
1397                     "size"))
1398 
1399         ephemeral_size = sum(bdm.volume_size or instance_type['ephemeral_gb']
1400                 for bdm in block_device_mappings
1401                 if block_device.new_format_is_ephemeral(bdm))
1402         if ephemeral_size > instance_type['ephemeral_gb']:
1403             raise exception.InvalidBDMEphemeralSize()
1404 
1405         # There should be only one swap
1406         swap_list = block_device.get_bdm_swap_list(block_device_mappings)
1407         if len(swap_list) > 1:
1408             msg = _("More than one swap drive requested.")
1409             raise exception.InvalidBDMFormat(details=msg)
1410 
1411         if swap_list:
1412             swap_size = swap_list[0].volume_size or 0
1413             if swap_size > instance_type['swap']:
1414                 raise exception.InvalidBDMSwapSize()
1415 
1416         max_local = CONF.max_local_block_devices
1417         if max_local >= 0:
1418             num_local = len([bdm for bdm in block_device_mappings
1419                              if bdm.destination_type == 'local'])
1420             if num_local > max_local:
1421                 raise exception.InvalidBDMLocalsLimit()
1422 
1423     def _check_attach(self, context, volume, instance):
1424         # TODO(ildikov): This check_attach code is kept only for backward
1425         # compatibility and should be removed after Ocata EOL.
1426         if volume['status'] != 'available':
1427             msg = _("volume '%(vol)s' status must be 'available'. Currently "
1428                     "in '%(status)s'") % {'vol': volume['id'],
1429                                           'status': volume['status']}
1430             raise exception.InvalidVolume(reason=msg)
1431         if volume['attach_status'] == 'attached':
1432             msg = _("volume %s already attached") % volume['id']
1433             raise exception.InvalidVolume(reason=msg)
1434         self.volume_api.check_availability_zone(context, volume,
1435                                                 instance=instance)
1436 
1437     def _populate_instance_names(self, instance, num_instances):
1438         """Populate instance display_name and hostname."""
1439         display_name = instance.get('display_name')
1440         if instance.obj_attr_is_set('hostname'):
1441             hostname = instance.get('hostname')
1442         else:
1443             hostname = None
1444 
1445         # NOTE(mriedem): This is only here for test simplicity since a server
1446         # name is required in the REST API.
1447         if display_name is None:
1448             display_name = self._default_display_name(instance.uuid)
1449             instance.display_name = display_name
1450 
1451         if hostname is None and num_instances == 1:
1452             # NOTE(russellb) In the multi-instance case, we're going to
1453             # overwrite the display_name using the
1454             # multi_instance_display_name_template.  We need the default
1455             # display_name set so that it can be used in the template, though.
1456             # Only set the hostname here if we're only creating one instance.
1457             # Otherwise, it will be built after the template based
1458             # display_name.
1459             hostname = display_name
1460             default_hostname = self._default_host_name(instance.uuid)
1461             instance.hostname = utils.sanitize_hostname(hostname,
1462                                                         default_hostname)
1463 
1464     def _default_display_name(self, instance_uuid):
1465         return "Server %s" % instance_uuid
1466 
1467     def _default_host_name(self, instance_uuid):
1468         return "Server-%s" % instance_uuid
1469 
1470     def _populate_instance_for_create(self, context, instance, image,
1471                                       index, security_groups, instance_type,
1472                                       num_instances, shutdown_terminate):
1473         """Build the beginning of a new instance."""
1474 
1475         instance.launch_index = index
1476         instance.vm_state = vm_states.BUILDING
1477         instance.task_state = task_states.SCHEDULING
1478         info_cache = objects.InstanceInfoCache()
1479         info_cache.instance_uuid = instance.uuid
1480         info_cache.network_info = network_model.NetworkInfo()
1481         instance.info_cache = info_cache
1482         instance.flavor = instance_type
1483         instance.old_flavor = None
1484         instance.new_flavor = None
1485         if CONF.ephemeral_storage_encryption.enabled:
1486             # NOTE(kfarr): dm-crypt expects the cipher in a
1487             # hyphenated format: cipher-chainmode-ivmode
1488             # (ex: aes-xts-plain64). The algorithm needs
1489             # to be parsed out to pass to the key manager (ex: aes).
1490             cipher = CONF.ephemeral_storage_encryption.cipher
1491             algorithm = cipher.split('-')[0] if cipher else None
1492             instance.ephemeral_key_uuid = self.key_manager.create_key(
1493                 context,
1494                 algorithm=algorithm,
1495                 length=CONF.ephemeral_storage_encryption.key_size)
1496         else:
1497             instance.ephemeral_key_uuid = None
1498 
1499         # Store image properties so we can use them later
1500         # (for notifications, etc).  Only store what we can.
1501         if not instance.obj_attr_is_set('system_metadata'):
1502             instance.system_metadata = {}
1503         # Make sure we have the dict form that we need for instance_update.
1504         instance.system_metadata = utils.instance_sys_meta(instance)
1505 
1506         system_meta = utils.get_system_metadata_from_image(
1507             image, instance_type)
1508 
1509         # In case we couldn't find any suitable base_image
1510         system_meta.setdefault('image_base_image_ref', instance.image_ref)
1511 
1512         system_meta['owner_user_name'] = context.user_name
1513         system_meta['owner_project_name'] = context.project_name
1514 
1515         instance.system_metadata.update(system_meta)
1516 
1517         if CONF.use_neutron:
1518             # For Neutron we don't actually store anything in the database, we
1519             # proxy the security groups on the instance from the ports
1520             # attached to the instance.
1521             instance.security_groups = objects.SecurityGroupList()
1522         else:
1523             instance.security_groups = security_groups
1524 
1525         self._populate_instance_names(instance, num_instances)
1526         instance.shutdown_terminate = shutdown_terminate
1527         if num_instances > 1 and self.cell_type != 'api':
1528             instance = self._apply_instance_name_template(context, instance,
1529                                                           index)
1530 
1531         return instance
1532 
1533     def _create_tag_list_obj(self, context, tags):
1534         """Create TagList objects from simple string tags.
1535 
1536         :param context: security context.
1537         :param tags: simple string tags from API request.
1538         :returns: TagList object.
1539         """
1540         tag_list = [objects.Tag(context=context, tag=t) for t in tags]
1541         tag_list_obj = objects.TagList(objects=tag_list)
1542         return tag_list_obj
1543 
1544     def _transform_tags(self, tags, resource_id):
1545         """Change the resource_id of the tags according to the input param.
1546 
1547         Because this method can be called multiple times when more than one
1548         instance is booted in a single request it makes a copy of the tags
1549         list.
1550 
1551         :param tags: TagList object.
1552         :param resource_id: string.
1553         :returns: TagList object.
1554         """
1555         instance_tags = tags.obj_clone()
1556         for tag in instance_tags:
1557             tag.resource_id = resource_id
1558         return instance_tags
1559 
1560     # This method remains because cellsv1 uses it in the scheduler
1561     def create_db_entry_for_new_instance(self, context, instance_type, image,
1562             instance, security_group, block_device_mapping, num_instances,
1563             index, shutdown_terminate=False, create_instance=True):
1564         """Create an entry in the DB for this new instance,
1565         including any related table updates (such as security group,
1566         etc).
1567 
1568         This is called by the scheduler after a location for the
1569         instance has been determined.
1570 
1571         :param create_instance: Determines if the instance is created here or
1572             just populated for later creation. This is done so that this code
1573             can be shared with cellsv1 which needs the instance creation to
1574             happen here. It should be removed and this method cleaned up when
1575             cellsv1 is a distant memory.
1576         """
1577         self._populate_instance_for_create(context, instance, image, index,
1578                                            security_group, instance_type,
1579                                            num_instances, shutdown_terminate)
1580 
1581         if create_instance:
1582             instance.create()
1583 
1584         return instance
1585 
1586     def _check_multiple_instances_with_neutron_ports(self,
1587                                                      requested_networks):
1588         """Check whether multiple instances are created from port id(s)."""
1589         for requested_net in requested_networks:
1590             if requested_net.port_id:
1591                 msg = _("Unable to launch multiple instances with"
1592                         " a single configured port ID. Please launch your"
1593                         " instance one by one with different ports.")
1594                 raise exception.MultiplePortsNotApplicable(reason=msg)
1595 
1596     def _check_multiple_instances_with_specified_ip(self, requested_networks):
1597         """Check whether multiple instances are created with specified ip."""
1598 
1599         for requested_net in requested_networks:
1600             if requested_net.network_id and requested_net.address:
1601                 msg = _("max_count cannot be greater than 1 if an fixed_ip "
1602                         "is specified.")
1603                 raise exception.InvalidFixedIpAndMaxCountRequest(reason=msg)
1604 
1605     @hooks.add_hook("create_instance")
1606     def create(self, context, instance_type,
1607                image_href, kernel_id=None, ramdisk_id=None,
1608                min_count=None, max_count=None,
1609                display_name=None, display_description=None,
1610                key_name=None, key_data=None, security_groups=None,
1611                availability_zone=None, forced_host=None, forced_node=None,
1612                user_data=None, metadata=None, injected_files=None,
1613                admin_password=None, block_device_mapping=None,
1614                access_ip_v4=None, access_ip_v6=None, requested_networks=None,
1615                config_drive=None, auto_disk_config=None, scheduler_hints=None,
1616                legacy_bdm=True, shutdown_terminate=False,
1617                check_server_group_quota=False, tags=None,
1618                supports_multiattach=False, trusted_certs=None):
1619         """Provision instances, sending instance information to the
1620         scheduler.  The scheduler will determine where the instance(s)
1621         go and will handle creating the DB entries.
1622 
1623         Returns a tuple of (instances, reservation_id)
1624         """
1625         if requested_networks and max_count is not None and max_count > 1:
1626             self._check_multiple_instances_with_specified_ip(
1627                 requested_networks)
1628             if utils.is_neutron():
1629                 self._check_multiple_instances_with_neutron_ports(
1630                     requested_networks)
1631 
1632         if availability_zone:
1633             available_zones = availability_zones.\
1634                 get_availability_zones(context.elevated(), True)
1635             if forced_host is None and availability_zone not in \
1636                     available_zones:
1637                 msg = _('The requested availability zone is not available')
1638                 raise exception.InvalidRequest(msg)
1639 
1640         filter_properties = scheduler_utils.build_filter_properties(
1641                 scheduler_hints, forced_host, forced_node, instance_type)
1642 
1643         return self._create_instance(
1644                        context, instance_type,
1645                        image_href, kernel_id, ramdisk_id,
1646                        min_count, max_count,
1647                        display_name, display_description,
1648                        key_name, key_data, security_groups,
1649                        availability_zone, user_data, metadata,
1650                        injected_files, admin_password,
1651                        access_ip_v4, access_ip_v6,
1652                        requested_networks, config_drive,
1653                        block_device_mapping, auto_disk_config,
1654                        filter_properties=filter_properties,
1655                        legacy_bdm=legacy_bdm,
1656                        shutdown_terminate=shutdown_terminate,
1657                        check_server_group_quota=check_server_group_quota,
1658                        tags=tags, supports_multiattach=supports_multiattach,
1659                        trusted_certs=trusted_certs)
1660 
1661     def _check_auto_disk_config(self, instance=None, image=None,
1662                                 **extra_instance_updates):
1663         auto_disk_config = extra_instance_updates.get("auto_disk_config")
1664         if auto_disk_config is None:
1665             return
1666         if not image and not instance:
1667             return
1668 
1669         if image:
1670             image_props = image.get("properties", {})
1671             auto_disk_config_img = \
1672                 utils.get_auto_disk_config_from_image_props(image_props)
1673             image_ref = image.get("id")
1674         else:
1675             sys_meta = utils.instance_sys_meta(instance)
1676             image_ref = sys_meta.get('image_base_image_ref')
1677             auto_disk_config_img = \
1678                 utils.get_auto_disk_config_from_instance(sys_meta=sys_meta)
1679 
1680         self._ensure_auto_disk_config_is_valid(auto_disk_config_img,
1681                                                auto_disk_config,
1682                                                image_ref)
1683 
1684     def _lookup_instance(self, context, uuid):
1685         '''Helper method for pulling an instance object from a database.
1686 
1687         During the transition to cellsv2 there is some complexity around
1688         retrieving an instance from the database which this method hides. If
1689         there is an instance mapping then query the cell for the instance, if
1690         no mapping exists then query the configured nova database.
1691 
1692         Once we are past the point that all deployments can be assumed to be
1693         migrated to cellsv2 this method can go away.
1694         '''
1695         inst_map = None
1696         try:
1697             inst_map = objects.InstanceMapping.get_by_instance_uuid(
1698                 context, uuid)
1699         except exception.InstanceMappingNotFound:
1700             # TODO(alaski): This exception block can be removed once we're
1701             # guaranteed everyone is using cellsv2.
1702             pass
1703 
1704         if (inst_map is None or inst_map.cell_mapping is None or
1705                 CONF.cells.enable):
1706             # If inst_map is None then the deployment has not migrated to
1707             # cellsv2 yet.
1708             # If inst_map.cell_mapping is None then the instance is not in a
1709             # cell yet. Until instance creation moves to the conductor the
1710             # instance can be found in the configured database, so attempt
1711             # to look it up.
1712             # If we're on cellsv1, we can't yet short-circuit the cells
1713             # messaging path
1714             cell = None
1715             try:
1716                 instance = objects.Instance.get_by_uuid(context, uuid)
1717             except exception.InstanceNotFound:
1718                 # If we get here then the conductor is in charge of writing the
1719                 # instance to the database and hasn't done that yet. It's up to
1720                 # the caller of this method to determine what to do with that
1721                 # information.
1722                 return None, None
1723         else:
1724             cell = inst_map.cell_mapping
1725             with nova_context.target_cell(context, cell) as cctxt:
1726                 try:
1727                     instance = objects.Instance.get_by_uuid(cctxt, uuid)
1728                 except exception.InstanceNotFound:
1729                     # Since the cell_mapping exists we know the instance is in
1730                     # the cell, however InstanceNotFound means it's already
1731                     # deleted.
1732                     return None, None
1733         return cell, instance
1734 
1735     def _delete_while_booting(self, context, instance):
1736         """Handle deletion if the instance has not reached a cell yet
1737 
1738         Deletion before an instance reaches a cell needs to be handled
1739         differently. What we're attempting to do is delete the BuildRequest
1740         before the api level conductor does.  If we succeed here then the boot
1741         request stops before reaching a cell.  If not then the instance will
1742         need to be looked up in a cell db and the normal delete path taken.
1743         """
1744         deleted = self._attempt_delete_of_buildrequest(context, instance)
1745 
1746         # After service version 15 deletion of the BuildRequest will halt the
1747         # build process in the conductor. In that case run the rest of this
1748         # method and consider the instance deleted. If we have not yet reached
1749         # service version 15 then just return False so the rest of the delete
1750         # process will proceed usually.
1751         service_version = objects.Service.get_minimum_version(
1752             context, 'nova-osapi_compute')
1753         if service_version < 15:
1754             return False
1755 
1756         if deleted:
1757             # If we've reached this block the successful deletion of the
1758             # buildrequest indicates that the build process should be halted by
1759             # the conductor.
1760 
1761             # NOTE(alaski): Though the conductor halts the build process it
1762             # does not currently delete the instance record. This is
1763             # because in the near future the instance record will not be
1764             # created if the buildrequest has been deleted here. For now we
1765             # ensure the instance has been set to deleted at this point.
1766             # Yes this directly contradicts the comment earlier in this
1767             # method, but this is a temporary measure.
1768             # Look up the instance because the current instance object was
1769             # stashed on the buildrequest and therefore not complete enough
1770             # to run .destroy().
1771             try:
1772                 instance_uuid = instance.uuid
1773                 cell, instance = self._lookup_instance(context, instance_uuid)
1774                 if instance is not None:
1775                     # If instance is None it has already been deleted.
1776                     if cell:
1777                         with nova_context.target_cell(context, cell) as cctxt:
1778                             # FIXME: When the instance context is targeted,
1779                             # we can remove this
1780                             with compute_utils.notify_about_instance_delete(
1781                                     self.notifier, cctxt, instance):
1782                                 instance.destroy()
1783                     else:
1784                         instance.destroy()
1785             except exception.InstanceNotFound:
1786                 pass
1787 
1788             return True
1789         return False
1790 
1791     def _attempt_delete_of_buildrequest(self, context, instance):
1792         # If there is a BuildRequest then the instance may not have been
1793         # written to a cell db yet. Delete the BuildRequest here, which
1794         # will indicate that the Instance build should not proceed.
1795         try:
1796             build_req = objects.BuildRequest.get_by_instance_uuid(
1797                 context, instance.uuid)
1798             build_req.destroy()
1799         except exception.BuildRequestNotFound:
1800             # This means that conductor has deleted the BuildRequest so the
1801             # instance is now in a cell and the delete needs to proceed
1802             # normally.
1803             return False
1804 
1805         # We need to detach from any volumes so they aren't orphaned.
1806         self._local_cleanup_bdm_volumes(
1807             build_req.block_device_mappings, instance, context)
1808 
1809         return True
1810 
1811     def _delete(self, context, instance, delete_type, cb, **instance_attrs):
1812         if instance.disable_terminate:
1813             LOG.info('instance termination disabled', instance=instance)
1814             return
1815 
1816         cell = None
1817         # If there is an instance.host (or the instance is shelved-offloaded or
1818         # in error state), the instance has been scheduled and sent to a
1819         # cell/compute which means it was pulled from the cell db.
1820         # Normal delete should be attempted.
1821         may_have_ports_or_volumes = compute_utils.may_have_ports_or_volumes(
1822             instance)
1823         if not instance.host and not may_have_ports_or_volumes:
1824             try:
1825                 if self._delete_while_booting(context, instance):
1826                     return
1827                 # If instance.host was not set it's possible that the Instance
1828                 # object here was pulled from a BuildRequest object and is not
1829                 # fully populated. Notably it will be missing an 'id' field
1830                 # which will prevent instance.destroy from functioning
1831                 # properly. A lookup is attempted which will either return a
1832                 # full Instance or None if not found. If not found then it's
1833                 # acceptable to skip the rest of the delete processing.
1834                 cell, instance = self._lookup_instance(context, instance.uuid)
1835                 if cell and instance:
1836                     try:
1837                         # Now destroy the instance from the cell it lives in.
1838                         with compute_utils.notify_about_instance_delete(
1839                                 self.notifier, context, instance):
1840                             instance.destroy()
1841                     except exception.InstanceNotFound:
1842                         pass
1843                     # The instance was deleted or is already gone.
1844                     return
1845                 if not instance:
1846                     # Instance is already deleted.
1847                     return
1848             except exception.ObjectActionError:
1849                 # NOTE(melwitt): This means the instance.host changed
1850                 # under us indicating the instance became scheduled
1851                 # during the destroy(). Refresh the instance from the DB and
1852                 # continue on with the delete logic for a scheduled instance.
1853                 # NOTE(danms): If instance.host is set, we should be able to
1854                 # do the following lookup. If not, there's not much we can
1855                 # do to recover.
1856                 cell, instance = self._lookup_instance(context, instance.uuid)
1857                 if not instance:
1858                     # Instance is already deleted
1859                     return
1860 
1861         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
1862                 context, instance.uuid)
1863 
1864         # At these states an instance has a snapshot associate.
1865         if instance.vm_state in (vm_states.SHELVED,
1866                                  vm_states.SHELVED_OFFLOADED):
1867             snapshot_id = instance.system_metadata.get('shelved_image_id')
1868             LOG.info("Working on deleting snapshot %s "
1869                      "from shelved instance...",
1870                      snapshot_id, instance=instance)
1871             try:
1872                 self.image_api.delete(context, snapshot_id)
1873             except (exception.ImageNotFound,
1874                     exception.ImageNotAuthorized) as exc:
1875                 LOG.warning("Failed to delete snapshot "
1876                             "from shelved instance (%s).",
1877                             exc.format_message(), instance=instance)
1878             except Exception:
1879                 LOG.exception("Something wrong happened when trying to "
1880                               "delete snapshot from shelved instance.",
1881                               instance=instance)
1882 
1883         original_task_state = instance.task_state
1884         try:
1885             # NOTE(maoy): no expected_task_state needs to be set
1886             instance.update(instance_attrs)
1887             instance.progress = 0
1888             instance.save()
1889 
1890             # NOTE(dtp): cells.enable = False means "use cells v2".
1891             # Run everywhere except v1 compute cells.
1892             if not CONF.cells.enable or self.cell_type == 'api':
1893                 self.consoleauth_rpcapi.delete_tokens_for_instance(
1894                     context, instance.uuid)
1895 
1896             if self.cell_type == 'api':
1897                 # NOTE(comstud): If we're in the API cell, we need to
1898                 # skip all remaining logic and just call the callback,
1899                 # which will cause a cast to the child cell.
1900                 cb(context, instance, bdms)
1901                 return
1902             if not instance.host and not may_have_ports_or_volumes:
1903                 try:
1904                     with compute_utils.notify_about_instance_delete(
1905                             self.notifier, context, instance,
1906                             delete_type
1907                             if delete_type != 'soft_delete'
1908                             else 'delete'):
1909                         instance.destroy()
1910                     LOG.info('Instance deleted and does not have host '
1911                              'field, its vm_state is %(state)s.',
1912                              {'state': instance.vm_state},
1913                               instance=instance)
1914                     return
1915                 except exception.ObjectActionError as ex:
1916                     # The instance's host likely changed under us as
1917                     # this instance could be building and has since been
1918                     # scheduled. Continue with attempts to delete it.
1919                     LOG.debug('Refreshing instance because: %s', ex,
1920                               instance=instance)
1921                     instance.refresh()
1922 
1923             if instance.vm_state == vm_states.RESIZED:
1924                 self._confirm_resize_on_deleting(context, instance)
1925 
1926             is_local_delete = True
1927             try:
1928                 # instance.host must be set in order to look up the service.
1929                 if instance.host is not None:
1930                     service = objects.Service.get_by_compute_host(
1931                         context.elevated(), instance.host)
1932                     is_local_delete = not self.servicegroup_api.service_is_up(
1933                         service)
1934                 if not is_local_delete:
1935                     if original_task_state in (task_states.DELETING,
1936                                                   task_states.SOFT_DELETING):
1937                         LOG.info('Instance is already in deleting state, '
1938                                  'ignoring this request',
1939                                  instance=instance)
1940                         return
1941                     self._record_action_start(context, instance,
1942                                               instance_actions.DELETE)
1943 
1944                     cb(context, instance, bdms)
1945             except exception.ComputeHostNotFound:
1946                 LOG.debug('Compute host %s not found during service up check, '
1947                           'going to local delete instance', instance.host,
1948                           instance=instance)
1949 
1950             if is_local_delete:
1951                 # If instance is in shelved_offloaded state or compute node
1952                 # isn't up, delete instance from db and clean bdms info and
1953                 # network info
1954                 if cell is None:
1955                     # NOTE(danms): If we didn't get our cell from one of the
1956                     # paths above, look it up now.
1957                     try:
1958                         im = objects.InstanceMapping.get_by_instance_uuid(
1959                             context, instance.uuid)
1960                         cell = im.cell_mapping
1961                     except exception.InstanceMappingNotFound:
1962                         LOG.warning('During local delete, failed to find '
1963                                     'instance mapping', instance=instance)
1964                         return
1965 
1966                 LOG.debug('Doing local delete in cell %s', cell.identity,
1967                           instance=instance)
1968                 with nova_context.target_cell(context, cell) as cctxt:
1969                     self._local_delete(cctxt, instance, bdms, delete_type, cb)
1970 
1971         except exception.InstanceNotFound:
1972             # NOTE(comstud): Race condition. Instance already gone.
1973             pass
1974 
1975     def _confirm_resize_on_deleting(self, context, instance):
1976         # If in the middle of a resize, use confirm_resize to
1977         # ensure the original instance is cleaned up too
1978         migration = None
1979         for status in ('finished', 'confirming'):
1980             try:
1981                 migration = objects.Migration.get_by_instance_and_status(
1982                         context.elevated(), instance.uuid, status)
1983                 LOG.info('Found an unconfirmed migration during delete, '
1984                          'id: %(id)s, status: %(status)s',
1985                          {'id': migration.id,
1986                           'status': migration.status},
1987                          instance=instance)
1988                 break
1989             except exception.MigrationNotFoundByStatus:
1990                 pass
1991 
1992         if not migration:
1993             LOG.info('Instance may have been confirmed during delete',
1994                      instance=instance)
1995             return
1996 
1997         src_host = migration.source_compute
1998 
1999         self._record_action_start(context, instance,
2000                                   instance_actions.CONFIRM_RESIZE)
2001 
2002         self.compute_rpcapi.confirm_resize(context,
2003                 instance, migration, src_host, cast=False)
2004 
2005     def _local_cleanup_bdm_volumes(self, bdms, instance, context):
2006         """The method deletes the bdm records and, if a bdm is a volume, call
2007         the terminate connection and the detach volume via the Volume API.
2008         """
2009         elevated = context.elevated()
2010         for bdm in bdms:
2011             if bdm.is_volume:
2012                 try:
2013                     if bdm.attachment_id:
2014                         self.volume_api.attachment_delete(context,
2015                                                           bdm.attachment_id)
2016                     else:
2017                         connector = compute_utils.get_stashed_volume_connector(
2018                             bdm, instance)
2019                         if connector:
2020                             self.volume_api.terminate_connection(context,
2021                                                                  bdm.volume_id,
2022                                                                  connector)
2023                         else:
2024                             LOG.debug('Unable to find connector for volume %s,'
2025                                       ' not attempting terminate_connection.',
2026                                       bdm.volume_id, instance=instance)
2027                         # Attempt to detach the volume. If there was no
2028                         # connection made in the first place this is just
2029                         # cleaning up the volume state in the Cinder DB.
2030                         self.volume_api.detach(elevated, bdm.volume_id,
2031                                                instance.uuid)
2032 
2033                     if bdm.delete_on_termination:
2034                         self.volume_api.delete(context, bdm.volume_id)
2035                 except Exception as exc:
2036                     LOG.warning("Ignoring volume cleanup failure due to %s",
2037                                 exc, instance=instance)
2038             # If we're cleaning up volumes from an instance that wasn't yet
2039             # created in a cell, i.e. the user deleted the server while
2040             # the BuildRequest still existed, then the BDM doesn't actually
2041             # exist in the DB to destroy it.
2042             if 'id' in bdm:
2043                 bdm.destroy()
2044 
2045     def _local_delete(self, context, instance, bdms, delete_type, cb):
2046         if instance.vm_state == vm_states.SHELVED_OFFLOADED:
2047             LOG.info("instance is in SHELVED_OFFLOADED state, cleanup"
2048                      " the instance's info from database.",
2049                      instance=instance)
2050         else:
2051             LOG.warning("instance's host %s is down, deleting from "
2052                         "database", instance.host, instance=instance)
2053         with compute_utils.notify_about_instance_delete(
2054                 self.notifier, context, instance,
2055                 delete_type if delete_type != 'soft_delete' else 'delete'):
2056 
2057             elevated = context.elevated()
2058             if self.cell_type != 'api':
2059                 # NOTE(liusheng): In nova-network multi_host scenario,deleting
2060                 # network info of the instance may need instance['host'] as
2061                 # destination host of RPC call. If instance in
2062                 # SHELVED_OFFLOADED state, instance['host'] is None, here, use
2063                 # shelved_host as host to deallocate network info and reset
2064                 # instance['host'] after that. Here we shouldn't use
2065                 # instance.save(), because this will mislead user who may think
2066                 # the instance's host has been changed, and actually, the
2067                 # instance.host is always None.
2068                 orig_host = instance.host
2069                 try:
2070                     if instance.vm_state == vm_states.SHELVED_OFFLOADED:
2071                         sysmeta = getattr(instance,
2072                                           obj_base.get_attrname(
2073                                               'system_metadata'))
2074                         instance.host = sysmeta.get('shelved_host')
2075                     self.network_api.deallocate_for_instance(elevated,
2076                                                              instance)
2077                 finally:
2078                     instance.host = orig_host
2079 
2080             # cleanup volumes
2081             self._local_cleanup_bdm_volumes(bdms, instance, context)
2082             cb(context, instance, bdms, local=True)
2083             instance.destroy()
2084 
2085     def _do_delete(self, context, instance, bdms, local=False):
2086         if local:
2087             instance.vm_state = vm_states.DELETED
2088             instance.task_state = None
2089             instance.terminated_at = timeutils.utcnow()
2090             instance.save()
2091         else:
2092             self.compute_rpcapi.terminate_instance(context, instance, bdms,
2093                                                    delete_type='delete')
2094 
2095     def _do_force_delete(self, context, instance, bdms, local=False):
2096         if local:
2097             instance.vm_state = vm_states.DELETED
2098             instance.task_state = None
2099             instance.terminated_at = timeutils.utcnow()
2100             instance.save()
2101         else:
2102             self.compute_rpcapi.terminate_instance(context, instance, bdms,
2103                                                    delete_type='force_delete')
2104 
2105     def _do_soft_delete(self, context, instance, bdms, local=False):
2106         if local:
2107             instance.vm_state = vm_states.SOFT_DELETED
2108             instance.task_state = None
2109             instance.terminated_at = timeutils.utcnow()
2110             instance.save()
2111         else:
2112             self.compute_rpcapi.soft_delete_instance(context, instance)
2113 
2114     # NOTE(maoy): we allow delete to be called no matter what vm_state says.
2115     @check_instance_lock
2116     @check_instance_cell
2117     @check_instance_state(vm_state=None, task_state=None,
2118                           must_have_launched=True)
2119     def soft_delete(self, context, instance):
2120         """Terminate an instance."""
2121         LOG.debug('Going to try to soft delete instance',
2122                   instance=instance)
2123 
2124         self._delete(context, instance, 'soft_delete', self._do_soft_delete,
2125                      task_state=task_states.SOFT_DELETING,
2126                      deleted_at=timeutils.utcnow())
2127 
2128     def _delete_instance(self, context, instance):
2129         self._delete(context, instance, 'delete', self._do_delete,
2130                      task_state=task_states.DELETING)
2131 
2132     @check_instance_lock
2133     @check_instance_cell
2134     @check_instance_state(vm_state=None, task_state=None,
2135                           must_have_launched=False)
2136     def delete(self, context, instance):
2137         """Terminate an instance."""
2138         LOG.debug("Going to try to terminate instance", instance=instance)
2139         self._delete_instance(context, instance)
2140 
2141     @check_instance_lock
2142     @check_instance_state(vm_state=[vm_states.SOFT_DELETED])
2143     def restore(self, context, instance):
2144         """Restore a previously deleted (but not reclaimed) instance."""
2145         # Check quotas
2146         flavor = instance.get_flavor()
2147         project_id, user_id = quotas_obj.ids_from_instance(context, instance)
2148         compute_utils.check_num_instances_quota(context, flavor, 1, 1,
2149                 project_id=project_id, user_id=user_id)
2150 
2151         self._record_action_start(context, instance, instance_actions.RESTORE)
2152 
2153         if instance.host:
2154             instance.task_state = task_states.RESTORING
2155             instance.deleted_at = None
2156             instance.save(expected_task_state=[None])
2157             # TODO(melwitt): We're not rechecking for strict quota here to
2158             # guard against going over quota during a race at this time because
2159             # the resource consumption for this operation is written to the
2160             # database by compute.
2161             self.compute_rpcapi.restore_instance(context, instance)
2162         else:
2163             instance.vm_state = vm_states.ACTIVE
2164             instance.task_state = None
2165             instance.deleted_at = None
2166             instance.save(expected_task_state=[None])
2167 
2168     @check_instance_lock
2169     @check_instance_state(task_state=None,
2170                           must_have_launched=False)
2171     def force_delete(self, context, instance):
2172         """Force delete an instance in any vm_state/task_state."""
2173         self._delete(context, instance, 'force_delete', self._do_force_delete,
2174                      task_state=task_states.DELETING)
2175 
2176     def force_stop(self, context, instance, do_cast=True, clean_shutdown=True):
2177         LOG.debug("Going to try to stop instance", instance=instance)
2178 
2179         instance.task_state = task_states.POWERING_OFF
2180         instance.progress = 0
2181         instance.save(expected_task_state=[None])
2182 
2183         self._record_action_start(context, instance, instance_actions.STOP)
2184 
2185         self.compute_rpcapi.stop_instance(context, instance, do_cast=do_cast,
2186                                           clean_shutdown=clean_shutdown)
2187 
2188     @check_instance_lock
2189     @check_instance_host
2190     @check_instance_cell
2191     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.ERROR])
2192     def stop(self, context, instance, do_cast=True, clean_shutdown=True):
2193         """Stop an instance."""
2194         self.force_stop(context, instance, do_cast, clean_shutdown)
2195 
2196     @check_instance_lock
2197     @check_instance_host
2198     @check_instance_cell
2199     @check_instance_state(vm_state=[vm_states.STOPPED])
2200     def start(self, context, instance):
2201         """Start an instance."""
2202         LOG.debug("Going to try to start instance", instance=instance)
2203 
2204         instance.task_state = task_states.POWERING_ON
2205         instance.save(expected_task_state=[None])
2206 
2207         self._record_action_start(context, instance, instance_actions.START)
2208         # TODO(yamahata): injected_files isn't supported right now.
2209         #                 It is used only for osapi. not for ec2 api.
2210         #                 availability_zone isn't used by run_instance.
2211         self.compute_rpcapi.start_instance(context, instance)
2212 
2213     @check_instance_lock
2214     @check_instance_host
2215     @check_instance_cell
2216     @check_instance_state(vm_state=vm_states.ALLOW_TRIGGER_CRASH_DUMP)
2217     def trigger_crash_dump(self, context, instance):
2218         """Trigger crash dump in an instance."""
2219         LOG.debug("Try to trigger crash dump", instance=instance)
2220 
2221         self._record_action_start(context, instance,
2222                                   instance_actions.TRIGGER_CRASH_DUMP)
2223 
2224         self.compute_rpcapi.trigger_crash_dump(context, instance)
2225 
2226     def _get_instance_map_or_none(self, context, instance_uuid):
2227         try:
2228             inst_map = objects.InstanceMapping.get_by_instance_uuid(
2229                     context, instance_uuid)
2230         except exception.InstanceMappingNotFound:
2231             # InstanceMapping should always be found generally. This exception
2232             # may be raised if a deployment has partially migrated the nova-api
2233             # services.
2234             inst_map = None
2235         return inst_map
2236 
2237     def _get_instance(self, context, instance_uuid, expected_attrs):
2238         # Before service version 15 the BuildRequest is not cleaned up during
2239         # a delete request so there is no reason to look it up here as we can't
2240         # trust that it's not referencing a deleted instance. Also even if
2241         # there is an instance mapping we don't need to honor it for older
2242         # service versions.
2243         service_version = objects.Service.get_minimum_version(
2244             context, 'nova-osapi_compute')
2245         # If we're on cellsv1, we also need to consult the top-level
2246         # merged replica instead of the cell directly, so fall through
2247         # here in that case as well.
2248         if service_version < 15 or CONF.cells.enable:
2249             return objects.Instance.get_by_uuid(context, instance_uuid,
2250                                                 expected_attrs=expected_attrs)
2251         inst_map = self._get_instance_map_or_none(context, instance_uuid)
2252         if inst_map and (inst_map.cell_mapping is not None):
2253             nova_context.set_target_cell(context, inst_map.cell_mapping)
2254             instance = objects.Instance.get_by_uuid(
2255                 context, instance_uuid, expected_attrs=expected_attrs)
2256         elif inst_map and (inst_map.cell_mapping is None):
2257             # This means the instance has not been scheduled and put in
2258             # a cell yet. For now it also may mean that the deployer
2259             # has not created their cell(s) yet.
2260             try:
2261                 build_req = objects.BuildRequest.get_by_instance_uuid(
2262                         context, instance_uuid)
2263                 instance = build_req.instance
2264             except exception.BuildRequestNotFound:
2265                 # Instance was mapped and the BuildRequest was deleted
2266                 # while fetching. Try again.
2267                 inst_map = self._get_instance_map_or_none(context,
2268                                                           instance_uuid)
2269                 if inst_map and (inst_map.cell_mapping is not None):
2270                     nova_context.set_target_cell(context,
2271                                                  inst_map.cell_mapping)
2272                     instance = objects.Instance.get_by_uuid(
2273                         context, instance_uuid,
2274                         expected_attrs=expected_attrs)
2275                 else:
2276                     raise exception.InstanceNotFound(instance_id=instance_uuid)
2277         else:
2278             raise exception.InstanceNotFound(instance_id=instance_uuid)
2279 
2280         return instance
2281 
2282     def get(self, context, instance_id, expected_attrs=None):
2283         """Get a single instance with the given instance_id."""
2284         if not expected_attrs:
2285             expected_attrs = []
2286         expected_attrs.extend(['metadata', 'system_metadata',
2287                                'security_groups', 'info_cache'])
2288         # NOTE(ameade): we still need to support integer ids for ec2
2289         try:
2290             if uuidutils.is_uuid_like(instance_id):
2291                 LOG.debug("Fetching instance by UUID",
2292                            instance_uuid=instance_id)
2293 
2294                 instance = self._get_instance(context, instance_id,
2295                                               expected_attrs)
2296             else:
2297                 LOG.debug("Failed to fetch instance by id %s", instance_id)
2298                 raise exception.InstanceNotFound(instance_id=instance_id)
2299         except exception.InvalidID:
2300             LOG.debug("Invalid instance id %s", instance_id)
2301             raise exception.InstanceNotFound(instance_id=instance_id)
2302 
2303         return instance
2304 
2305     def get_all(self, context, search_opts=None, limit=None, marker=None,
2306                 expected_attrs=None, sort_keys=None, sort_dirs=None):
2307         """Get all instances filtered by one of the given parameters.
2308 
2309         If there is no filter and the context is an admin, it will retrieve
2310         all instances in the system.
2311 
2312         Deleted instances will be returned by default, unless there is a
2313         search option that says otherwise.
2314 
2315         The results will be sorted based on the list of sort keys in the
2316         'sort_keys' parameter (first value is primary sort key, second value is
2317         secondary sort ket, etc.). For each sort key, the associated sort
2318         direction is based on the list of sort directions in the 'sort_dirs'
2319         parameter.
2320         """
2321         if search_opts is None:
2322             search_opts = {}
2323 
2324         LOG.debug("Searching by: %s", str(search_opts))
2325 
2326         # Fixups for the DB call
2327         filters = {}
2328 
2329         def _remap_flavor_filter(flavor_id):
2330             flavor = objects.Flavor.get_by_flavor_id(context, flavor_id)
2331             filters['instance_type_id'] = flavor.id
2332 
2333         def _remap_fixed_ip_filter(fixed_ip):
2334             # Turn fixed_ip into a regexp match. Since '.' matches
2335             # any character, we need to use regexp escaping for it.
2336             filters['ip'] = '^%s$' % fixed_ip.replace('.', '\\.')
2337 
2338         # search_option to filter_name mapping.
2339         filter_mapping = {
2340                 'image': 'image_ref',
2341                 'name': 'display_name',
2342                 'tenant_id': 'project_id',
2343                 'flavor': _remap_flavor_filter,
2344                 'fixed_ip': _remap_fixed_ip_filter}
2345 
2346         # copy from search_opts, doing various remappings as necessary
2347         for opt, value in search_opts.items():
2348             # Do remappings.
2349             # Values not in the filter_mapping table are copied as-is.
2350             # If remapping is None, option is not copied
2351             # If the remapping is a string, it is the filter_name to use
2352             try:
2353                 remap_object = filter_mapping[opt]
2354             except KeyError:
2355                 filters[opt] = value
2356             else:
2357                 # Remaps are strings to translate to, or functions to call
2358                 # to do the translating as defined by the table above.
2359                 if isinstance(remap_object, six.string_types):
2360                     filters[remap_object] = value
2361                 else:
2362                     try:
2363                         remap_object(value)
2364 
2365                     # We already know we can't match the filter, so
2366                     # return an empty list
2367                     except ValueError:
2368                         return objects.InstanceList()
2369 
2370         # IP address filtering cannot be applied at the DB layer, remove any DB
2371         # limit so that it can be applied after the IP filter.
2372         filter_ip = 'ip6' in filters or 'ip' in filters
2373         orig_limit = limit
2374         if filter_ip:
2375             if self.network_api.has_substr_port_filtering_extension(context):
2376                 # We're going to filter by IP using Neutron so set filter_ip
2377                 # to False so we don't attempt post-DB query filtering in
2378                 # memory below.
2379                 filter_ip = False
2380                 instance_uuids = self._ip_filter_using_neutron(context,
2381                                                                filters)
2382                 if instance_uuids:
2383                     # Note that 'uuid' is not in the 2.1 GET /servers query
2384                     # parameter schema, however, we allow additionalProperties
2385                     # so someone could filter instances by uuid, which doesn't
2386                     # make a lot of sense but we have to account for it.
2387                     if 'uuid' in filters and filters['uuid']:
2388                         filter_uuids = filters['uuid']
2389                         if isinstance(filter_uuids, list):
2390                             instance_uuids.extend(filter_uuids)
2391                         else:
2392                             # Assume a string. If it's a dict or tuple or
2393                             # something, well...that's too bad. This is why
2394                             # we have query parameter schema definitions.
2395                             if filter_uuids not in instance_uuids:
2396                                 instance_uuids.append(filter_uuids)
2397                     filters['uuid'] = instance_uuids
2398                 else:
2399                     # No matches on the ip filter(s), return an empty list.
2400                     return objects.InstanceList()
2401             elif limit:
2402                 LOG.debug('Removing limit for DB query due to IP filter')
2403                 limit = None
2404 
2405         # The ordering of instances will be
2406         # [sorted instances with no host] + [sorted instances with host].
2407         # This means BuildRequest and cell0 instances first, then cell
2408         # instances
2409         try:
2410             build_requests = objects.BuildRequestList.get_by_filters(
2411                 context, filters, limit=limit, marker=marker,
2412                 sort_keys=sort_keys, sort_dirs=sort_dirs)
2413             # If we found the marker in we need to set it to None
2414             # so we don't expect to find it in the cells below.
2415             marker = None
2416         except exception.MarkerNotFound:
2417             # If we didn't find the marker in the build requests then keep
2418             # looking for it in the cells.
2419             build_requests = objects.BuildRequestList()
2420         build_req_instances = objects.InstanceList(
2421             objects=[build_req.instance for build_req in build_requests])
2422         # Only subtract from limit if it is not None
2423         limit = (limit - len(build_req_instances)) if limit else limit
2424 
2425         # We could arguably avoid joining on security_groups if we're using
2426         # neutron (which is the default) but if you're using neutron then the
2427         # security_group_instance_association table should be empty anyway
2428         # and the DB should optimize out that join, making it insignificant.
2429         fields = ['metadata', 'info_cache', 'security_groups']
2430         if expected_attrs:
2431             fields.extend(expected_attrs)
2432 
2433         if CONF.cells.enable:
2434             insts = self._do_old_style_instance_list_for_poor_cellsv1_users(
2435                 context, filters, limit, marker, fields, sort_keys,
2436                 sort_dirs)
2437         else:
2438             insts = instance_list.get_instance_objects_sorted(
2439                 context, filters, limit, marker, fields, sort_keys, sort_dirs)
2440 
2441         def _get_unique_filter_method():
2442             seen_uuids = set()
2443 
2444             def _filter(instance):
2445                 if instance.uuid in seen_uuids:
2446                     return False
2447                 seen_uuids.add(instance.uuid)
2448                 return True
2449 
2450             return _filter
2451 
2452         filter_method = _get_unique_filter_method()
2453         # Only subtract from limit if it is not None
2454         limit = (limit - len(insts)) if limit else limit
2455         # TODO(alaski): Clean up the objects concatenation when List objects
2456         # support it natively.
2457         instances = objects.InstanceList(
2458             objects=list(filter(filter_method,
2459                            build_req_instances.objects +
2460                            insts.objects)))
2461 
2462         if filter_ip:
2463             instances = self._ip_filter(instances, filters, orig_limit)
2464 
2465         return instances
2466 
2467     def _do_old_style_instance_list_for_poor_cellsv1_users(self,
2468                                                            context, filters,
2469                                                            limit, marker,
2470                                                            fields,
2471                                                            sort_keys,
2472                                                            sort_dirs):
2473         try:
2474             cell0_mapping = objects.CellMapping.get_by_uuid(context,
2475                 objects.CellMapping.CELL0_UUID)
2476         except exception.CellMappingNotFound:
2477             cell0_instances = objects.InstanceList(objects=[])
2478         else:
2479             with nova_context.target_cell(context, cell0_mapping) as cctxt:
2480                 try:
2481                     cell0_instances = self._get_instances_by_filters(
2482                         cctxt, filters, limit=limit, marker=marker,
2483                         fields=fields, sort_keys=sort_keys,
2484                         sort_dirs=sort_dirs)
2485                     # If we found the marker in cell0 we need to set it to None
2486                     # so we don't expect to find it in the cells below.
2487                     marker = None
2488                 except exception.MarkerNotFound:
2489                     # We can ignore this since we need to look in the cell DB
2490                     cell0_instances = objects.InstanceList(objects=[])
2491         # Only subtract from limit if it is not None
2492         limit = (limit - len(cell0_instances)) if limit else limit
2493 
2494         # There is only planned support for a single cell here. Multiple cell
2495         # instance lists should be proxied to project Searchlight, or a similar
2496         # alternative.
2497         if limit is None or limit > 0:
2498             # NOTE(melwitt): If we're on cells v1, we need to read
2499             # instances from the top-level database because reading from
2500             # cells results in changed behavior, because of the syncing.
2501             # We can remove this path once we stop supporting cells v1.
2502             cell_instances = self._get_instances_by_filters(
2503                 context, filters, limit=limit, marker=marker,
2504                 fields=fields, sort_keys=sort_keys,
2505                 sort_dirs=sort_dirs)
2506         else:
2507             LOG.debug('Limit excludes any results from real cells')
2508             cell_instances = objects.InstanceList(objects=[])
2509 
2510         return cell0_instances + cell_instances
2511 
2512     @staticmethod
2513     def _ip_filter(inst_models, filters, limit):
2514         ipv4_f = re.compile(str(filters.get('ip')))
2515         ipv6_f = re.compile(str(filters.get('ip6')))
2516 
2517         def _match_instance(instance):
2518             nw_info = instance.get_network_info()
2519             for vif in nw_info:
2520                 for fixed_ip in vif.fixed_ips():
2521                     address = fixed_ip.get('address')
2522                     if not address:
2523                         continue
2524                     version = fixed_ip.get('version')
2525                     if ((version == 4 and ipv4_f.match(address)) or
2526                         (version == 6 and ipv6_f.match(address))):
2527                         return True
2528             return False
2529 
2530         result_objs = []
2531         for instance in inst_models:
2532             if _match_instance(instance):
2533                 result_objs.append(instance)
2534                 if limit and len(result_objs) == limit:
2535                     break
2536         return objects.InstanceList(objects=result_objs)
2537 
2538     def _ip_filter_using_neutron(self, context, filters):
2539         ip4_address = filters.get('ip')
2540         ip6_address = filters.get('ip6')
2541         addresses = [ip4_address, ip6_address]
2542         uuids = []
2543         for address in addresses:
2544             if address:
2545                 try:
2546                     ports = self.network_api.list_ports(
2547                         context, fixed_ips='ip_address_substr=' + address,
2548                         fields=['device_id'])['ports']
2549                     for port in ports:
2550                         uuids.append(port['device_id'])
2551                 except Exception as e:
2552                     LOG.error('An error occurred while listing ports '
2553                               'with an ip_address filter value of "%s". '
2554                               'Error: %s',
2555                               address, six.text_type(e))
2556         return uuids
2557 
2558     def _get_instances_by_filters(self, context, filters,
2559                                   limit=None, marker=None, fields=None,
2560                                   sort_keys=None, sort_dirs=None):
2561         return objects.InstanceList.get_by_filters(
2562             context, filters=filters, limit=limit, marker=marker,
2563             expected_attrs=fields, sort_keys=sort_keys, sort_dirs=sort_dirs)
2564 
2565     def update_instance(self, context, instance, updates):
2566         """Updates a single Instance object with some updates dict.
2567 
2568         Returns the updated instance.
2569         """
2570 
2571         # NOTE(sbauza): Given we only persist the Instance object after we
2572         # create the BuildRequest, we are sure that if the Instance object
2573         # has an ID field set, then it was persisted in the right Cell DB.
2574         if instance.obj_attr_is_set('id'):
2575             instance.update(updates)
2576             # Instance has been scheduled and the BuildRequest has been deleted
2577             # we can directly write the update down to the right cell.
2578             inst_map = self._get_instance_map_or_none(context, instance.uuid)
2579             # If we have a cell_mapping and we're not on cells v1, then
2580             # look up the instance in the cell database
2581             if inst_map and (inst_map.cell_mapping is not None) and (
2582                     not CONF.cells.enable):
2583                 with nova_context.target_cell(context,
2584                                               inst_map.cell_mapping) as cctxt:
2585                     with instance.obj_alternate_context(cctxt):
2586                         instance.save()
2587             else:
2588                 # If inst_map.cell_mapping does not point at a cell then cell
2589                 # migration has not happened yet.
2590                 # TODO(alaski): Make this a failure case after we put in
2591                 # a block that requires migrating to cellsv2.
2592                 instance.save()
2593         else:
2594             # Instance is not yet mapped to a cell, so we need to update
2595             # BuildRequest instead
2596             # TODO(sbauza): Fix the possible race conditions where BuildRequest
2597             # could be deleted because of either a concurrent instance delete
2598             # or because the scheduler just returned a destination right
2599             # after we called the instance in the API.
2600             try:
2601                 build_req = objects.BuildRequest.get_by_instance_uuid(
2602                     context, instance.uuid)
2603                 instance = build_req.instance
2604                 instance.update(updates)
2605                 # FIXME(sbauza): Here we are updating the current
2606                 # thread-related BuildRequest object. Given that another worker
2607                 # could have looking up at that BuildRequest in the API, it
2608                 # means that it could pass it down to the conductor without
2609                 # making sure that it's not updated, we could have some race
2610                 # condition where it would missing the updated fields, but
2611                 # that's something we could discuss once the instance record
2612                 # is persisted by the conductor.
2613                 build_req.save()
2614             except exception.BuildRequestNotFound:
2615                 # Instance was mapped and the BuildRequest was deleted
2616                 # while fetching (and possibly the instance could have been
2617                 # deleted as well). We need to lookup again the Instance object
2618                 # in order to correctly update it.
2619                 # TODO(sbauza): Figure out a good way to know the expected
2620                 # attributes by checking which fields are set or not.
2621                 expected_attrs = ['flavor', 'pci_devices', 'numa_topology',
2622                                   'tags', 'metadata', 'system_metadata',
2623                                   'security_groups', 'info_cache']
2624                 inst_map = self._get_instance_map_or_none(context,
2625                                                           instance.uuid)
2626                 if inst_map and (inst_map.cell_mapping is not None):
2627                     with nova_context.target_cell(
2628                             context,
2629                             inst_map.cell_mapping) as cctxt:
2630                         instance = objects.Instance.get_by_uuid(
2631                             cctxt, instance.uuid,
2632                             expected_attrs=expected_attrs)
2633                         instance.update(updates)
2634                         instance.save()
2635                 else:
2636                     # If inst_map.cell_mapping does not point at a cell then
2637                     # cell migration has not happened yet.
2638                     # TODO(alaski): Make this a failure case after we put in
2639                     # a block that requires migrating to cellsv2.
2640                     instance = objects.Instance.get_by_uuid(
2641                         context, instance.uuid, expected_attrs=expected_attrs)
2642                     instance.update(updates)
2643                     instance.save()
2644         return instance
2645 
2646     # NOTE(melwitt): We don't check instance lock for backup because lock is
2647     #                intended to prevent accidental change/delete of instances
2648     @check_instance_cell
2649     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
2650                                     vm_states.PAUSED, vm_states.SUSPENDED])
2651     def backup(self, context, instance, name, backup_type, rotation,
2652                extra_properties=None):
2653         """Backup the given instance
2654 
2655         :param instance: nova.objects.instance.Instance object
2656         :param name: name of the backup
2657         :param backup_type: 'daily' or 'weekly'
2658         :param rotation: int representing how many backups to keep around;
2659             None if rotation shouldn't be used (as in the case of snapshots)
2660         :param extra_properties: dict of extra image properties to include
2661                                  when creating the image.
2662         :returns: A dict containing image metadata
2663         """
2664         props_copy = dict(extra_properties, backup_type=backup_type)
2665 
2666         if compute_utils.is_volume_backed_instance(context, instance):
2667             LOG.info("It's not supported to backup volume backed "
2668                      "instance.", instance=instance)
2669             raise exception.InvalidRequest(
2670                 _('Backup is not supported for volume-backed instances.'))
2671         else:
2672             image_meta = self._create_image(context, instance,
2673                                             name, 'backup',
2674                                             extra_properties=props_copy)
2675 
2676         # NOTE(comstud): Any changes to this method should also be made
2677         # to the backup_instance() method in nova/cells/messaging.py
2678 
2679         instance.task_state = task_states.IMAGE_BACKUP
2680         instance.save(expected_task_state=[None])
2681 
2682         self._record_action_start(context, instance,
2683                                   instance_actions.BACKUP)
2684 
2685         self.compute_rpcapi.backup_instance(context, instance,
2686                                             image_meta['id'],
2687                                             backup_type,
2688                                             rotation)
2689         return image_meta
2690 
2691     # NOTE(melwitt): We don't check instance lock for snapshot because lock is
2692     #                intended to prevent accidental change/delete of instances
2693     @check_instance_cell
2694     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
2695                                     vm_states.PAUSED, vm_states.SUSPENDED])
2696     def snapshot(self, context, instance, name, extra_properties=None):
2697         """Snapshot the given instance.
2698 
2699         :param instance: nova.objects.instance.Instance object
2700         :param name: name of the snapshot
2701         :param extra_properties: dict of extra image properties to include
2702                                  when creating the image.
2703         :returns: A dict containing image metadata
2704         """
2705         image_meta = self._create_image(context, instance, name,
2706                                         'snapshot',
2707                                         extra_properties=extra_properties)
2708 
2709         # NOTE(comstud): Any changes to this method should also be made
2710         # to the snapshot_instance() method in nova/cells/messaging.py
2711         instance.task_state = task_states.IMAGE_SNAPSHOT_PENDING
2712         try:
2713             instance.save(expected_task_state=[None])
2714         except (exception.InstanceNotFound,
2715                 exception.UnexpectedDeletingTaskStateError) as ex:
2716             # Changing the instance task state to use in raising the
2717             # InstanceInvalidException below
2718             LOG.debug('Instance disappeared during snapshot.',
2719                       instance=instance)
2720             try:
2721                 image_id = image_meta['id']
2722                 self.image_api.delete(context, image_id)
2723                 LOG.info('Image %s deleted because instance '
2724                          'deleted before snapshot started.',
2725                          image_id, instance=instance)
2726             except exception.ImageNotFound:
2727                 pass
2728             except Exception as exc:
2729                 LOG.warning("Error while trying to clean up image %(img_id)s: "
2730                             "%(error_msg)s",
2731                             {"img_id": image_meta['id'],
2732                              "error_msg": six.text_type(exc)})
2733             attr = 'task_state'
2734             state = task_states.DELETING
2735             if type(ex) == exception.InstanceNotFound:
2736                 attr = 'vm_state'
2737                 state = vm_states.DELETED
2738             raise exception.InstanceInvalidState(attr=attr,
2739                                            instance_uuid=instance.uuid,
2740                                            state=state,
2741                                            method='snapshot')
2742 
2743         self._record_action_start(context, instance,
2744                                   instance_actions.CREATE_IMAGE)
2745 
2746         self.compute_rpcapi.snapshot_instance(context, instance,
2747                                               image_meta['id'])
2748 
2749         return image_meta
2750 
2751     def _create_image(self, context, instance, name, image_type,
2752                       extra_properties=None):
2753         """Create new image entry in the image service.  This new image
2754         will be reserved for the compute manager to upload a snapshot
2755         or backup.
2756 
2757         :param context: security context
2758         :param instance: nova.objects.instance.Instance object
2759         :param name: string for name of the snapshot
2760         :param image_type: snapshot | backup
2761         :param extra_properties: dict of extra image properties to include
2762 
2763         """
2764         properties = {
2765             'instance_uuid': instance.uuid,
2766             'user_id': str(context.user_id),
2767             'image_type': image_type,
2768         }
2769         properties.update(extra_properties or {})
2770 
2771         image_meta = self._initialize_instance_snapshot_metadata(
2772             instance, name, properties)
2773         # if we're making a snapshot, omit the disk and container formats,
2774         # since the image may have been converted to another format, and the
2775         # original values won't be accurate.  The driver will populate these
2776         # with the correct values later, on image upload.
2777         if image_type == 'snapshot':
2778             image_meta.pop('disk_format', None)
2779             image_meta.pop('container_format', None)
2780         return self.image_api.create(context, image_meta)
2781 
2782     def _initialize_instance_snapshot_metadata(self, instance, name,
2783                                                extra_properties=None):
2784         """Initialize new metadata for a snapshot of the given instance.
2785 
2786         :param instance: nova.objects.instance.Instance object
2787         :param name: string for name of the snapshot
2788         :param extra_properties: dict of extra metadata properties to include
2789 
2790         :returns: the new instance snapshot metadata
2791         """
2792         image_meta = utils.get_image_from_system_metadata(
2793             instance.system_metadata)
2794         image_meta.update({'name': name,
2795                            'is_public': False})
2796 
2797         # Delete properties that are non-inheritable
2798         properties = image_meta['properties']
2799         for key in CONF.non_inheritable_image_properties:
2800             properties.pop(key, None)
2801 
2802         # The properties in extra_properties have precedence
2803         properties.update(extra_properties or {})
2804 
2805         return image_meta
2806 
2807     # NOTE(melwitt): We don't check instance lock for snapshot because lock is
2808     #                intended to prevent accidental change/delete of instances
2809     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
2810                                     vm_states.SUSPENDED])
2811     def snapshot_volume_backed(self, context, instance, name,
2812                                extra_properties=None):
2813         """Snapshot the given volume-backed instance.
2814 
2815         :param instance: nova.objects.instance.Instance object
2816         :param name: name of the backup or snapshot
2817         :param extra_properties: dict of extra image properties to include
2818 
2819         :returns: the new image metadata
2820         """
2821         image_meta = self._initialize_instance_snapshot_metadata(
2822             instance, name, extra_properties)
2823         # the new image is simply a bucket of properties (particularly the
2824         # block device mapping, kernel and ramdisk IDs) with no image data,
2825         # hence the zero size
2826         image_meta['size'] = 0
2827         for attr in ('container_format', 'disk_format'):
2828             image_meta.pop(attr, None)
2829         properties = image_meta['properties']
2830         # clean properties before filling
2831         for key in ('block_device_mapping', 'bdm_v2', 'root_device_name'):
2832             properties.pop(key, None)
2833         if instance.root_device_name:
2834             properties['root_device_name'] = instance.root_device_name
2835 
2836         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
2837                 context, instance.uuid)
2838 
2839         mapping = []  # list of BDM dicts that can go into the image properties
2840         # Do some up-front filtering of the list of BDMs from
2841         # which we are going to create snapshots.
2842         volume_bdms = []
2843         for bdm in bdms:
2844             if bdm.no_device:
2845                 continue
2846             if bdm.is_volume:
2847                 # These will be handled below.
2848                 volume_bdms.append(bdm)
2849             else:
2850                 mapping.append(bdm.get_image_mapping())
2851 
2852         # Check limits in Cinder before creating snapshots to avoid going over
2853         # quota in the middle of a list of volumes. This is a best-effort check
2854         # but concurrently running snapshot requests from the same project
2855         # could still fail to create volume snapshots if they go over limit.
2856         if volume_bdms:
2857             limits = self.volume_api.get_absolute_limits(context)
2858             total_snapshots_used = limits['totalSnapshotsUsed']
2859             max_snapshots = limits['maxTotalSnapshots']
2860             # -1 means there is unlimited quota for snapshots
2861             if (max_snapshots > -1 and
2862                     len(volume_bdms) + total_snapshots_used > max_snapshots):
2863                 LOG.debug('Unable to create volume snapshots for instance. '
2864                           'Currently has %s snapshots, requesting %s new '
2865                           'snapshots, with a limit of %s.',
2866                           total_snapshots_used, len(volume_bdms),
2867                           max_snapshots, instance=instance)
2868                 raise exception.OverQuota(overs='snapshots')
2869 
2870         quiesced = False
2871         if instance.vm_state == vm_states.ACTIVE:
2872             try:
2873                 LOG.info("Attempting to quiesce instance before volume "
2874                          "snapshot.", instance=instance)
2875                 self.compute_rpcapi.quiesce_instance(context, instance)
2876                 quiesced = True
2877             except (exception.InstanceQuiesceNotSupported,
2878                     exception.QemuGuestAgentNotEnabled,
2879                     exception.NovaException, NotImplementedError) as err:
2880                 if strutils.bool_from_string(instance.system_metadata.get(
2881                         'image_os_require_quiesce')):
2882                     raise
2883                 else:
2884                     LOG.info('Skipping quiescing instance: %(reason)s.',
2885                              {'reason': err},
2886                              instance=instance)
2887 
2888         @wrap_instance_event(prefix='api')
2889         def snapshot_instance(self, context, instance, bdms):
2890             try:
2891                 for bdm in volume_bdms:
2892                     # create snapshot based on volume_id
2893                     volume = self.volume_api.get(context, bdm.volume_id)
2894                     # NOTE(yamahata): Should we wait for snapshot creation?
2895                     #                 Linux LVM snapshot creation completes in
2896                     #                 short time, it doesn't matter for now.
2897                     name = _('snapshot for %s') % image_meta['name']
2898                     LOG.debug('Creating snapshot from volume %s.',
2899                               volume['id'], instance=instance)
2900                     snapshot = self.volume_api.create_snapshot_force(
2901                         context, volume['id'],
2902                         name, volume['display_description'])
2903                     mapping_dict = block_device.snapshot_from_bdm(
2904                         snapshot['id'], bdm)
2905                     mapping_dict = mapping_dict.get_image_mapping()
2906                     mapping.append(mapping_dict)
2907                 return mapping
2908             # NOTE(tasker): No error handling is done in the above for loop.
2909             # This means that if the snapshot fails and throws an exception
2910             # the traceback will skip right over the unquiesce needed below.
2911             # Here, catch any exception, unquiesce the instance, and raise the
2912             # error so that the calling function can do what it needs to in
2913             # order to properly treat a failed snap.
2914             except Exception:
2915                 with excutils.save_and_reraise_exception():
2916                     if quiesced:
2917                         LOG.info("Unquiescing instance after volume snapshot "
2918                                  "failure.", instance=instance)
2919                         self.compute_rpcapi.unquiesce_instance(
2920                             context, instance, mapping)
2921 
2922         self._record_action_start(context, instance,
2923                                   instance_actions.CREATE_IMAGE)
2924         mapping = snapshot_instance(self, context, instance, bdms)
2925 
2926         if quiesced:
2927             self.compute_rpcapi.unquiesce_instance(context, instance, mapping)
2928 
2929         if mapping:
2930             properties['block_device_mapping'] = mapping
2931             properties['bdm_v2'] = True
2932 
2933         return self.image_api.create(context, image_meta)
2934 
2935     @check_instance_lock
2936     def reboot(self, context, instance, reboot_type):
2937         """Reboot the given instance."""
2938         if reboot_type == 'SOFT':
2939             self._soft_reboot(context, instance)
2940         else:
2941             self._hard_reboot(context, instance)
2942 
2943     @check_instance_state(vm_state=set(vm_states.ALLOW_SOFT_REBOOT),
2944                           task_state=[None])
2945     def _soft_reboot(self, context, instance):
2946         expected_task_state = [None]
2947         instance.task_state = task_states.REBOOTING
2948         instance.save(expected_task_state=expected_task_state)
2949 
2950         self._record_action_start(context, instance, instance_actions.REBOOT)
2951 
2952         self.compute_rpcapi.reboot_instance(context, instance=instance,
2953                                             block_device_info=None,
2954                                             reboot_type='SOFT')
2955 
2956     @check_instance_state(vm_state=set(vm_states.ALLOW_HARD_REBOOT),
2957                           task_state=task_states.ALLOW_REBOOT)
2958     def _hard_reboot(self, context, instance):
2959         instance.task_state = task_states.REBOOTING_HARD
2960         expected_task_state = [None,
2961                                task_states.REBOOTING,
2962                                task_states.REBOOT_PENDING,
2963                                task_states.REBOOT_STARTED,
2964                                task_states.REBOOTING_HARD,
2965                                task_states.RESUMING,
2966                                task_states.UNPAUSING,
2967                                task_states.SUSPENDING]
2968         instance.save(expected_task_state = expected_task_state)
2969 
2970         self._record_action_start(context, instance, instance_actions.REBOOT)
2971 
2972         self.compute_rpcapi.reboot_instance(context, instance=instance,
2973                                             block_device_info=None,
2974                                             reboot_type='HARD')
2975 
2976     @check_instance_lock
2977     @check_instance_cell
2978     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
2979                                     vm_states.ERROR])
2980     def rebuild(self, context, instance, image_href, admin_password,
2981                 files_to_inject=None, **kwargs):
2982         """Rebuild the given instance with the provided attributes."""
2983         files_to_inject = files_to_inject or []
2984         metadata = kwargs.get('metadata', {})
2985         preserve_ephemeral = kwargs.get('preserve_ephemeral', False)
2986         auto_disk_config = kwargs.get('auto_disk_config')
2987 
2988         if 'key_name' in kwargs:
2989             key_name = kwargs.pop('key_name')
2990             if key_name:
2991                 # NOTE(liuyulong): we are intentionally using the user_id from
2992                 # the request context rather than the instance.user_id because
2993                 # users own keys but instances are owned by projects, and
2994                 # another user in the same project can rebuild an instance
2995                 # even if they didn't create it.
2996                 key_pair = objects.KeyPair.get_by_name(context,
2997                                                        context.user_id,
2998                                                        key_name)
2999                 instance.key_name = key_pair.name
3000                 instance.key_data = key_pair.public_key
3001                 instance.keypairs = objects.KeyPairList(objects=[key_pair])
3002             else:
3003                 instance.key_name = None
3004                 instance.key_data = None
3005                 instance.keypairs = objects.KeyPairList(objects=[])
3006 
3007         image_id, image = self._get_image(context, image_href)
3008         self._check_auto_disk_config(image=image, **kwargs)
3009 
3010         flavor = instance.get_flavor()
3011         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
3012             context, instance.uuid)
3013         root_bdm = compute_utils.get_root_bdm(context, instance, bdms)
3014 
3015         # Check to see if the image is changing and we have a volume-backed
3016         # server. The compute doesn't support changing the image in the
3017         # root disk of a volume-backed server, so we need to just fail fast.
3018         is_volume_backed = compute_utils.is_volume_backed_instance(
3019             context, instance, bdms)
3020         if is_volume_backed:
3021             # For boot from volume, instance.image_ref is empty, so we need to
3022             # query the image from the volume.
3023             if root_bdm is None:
3024                 # This shouldn't happen and is an error, we need to fail. This
3025                 # is not the users fault, it's an internal error. Without a
3026                 # root BDM we have no way of knowing the backing volume (or
3027                 # image in that volume) for this instance.
3028                 raise exception.NovaException(
3029                     _('Unable to find root block device mapping for '
3030                       'volume-backed instance.'))
3031 
3032             volume = self.volume_api.get(context, root_bdm.volume_id)
3033             volume_image_metadata = volume.get('volume_image_metadata', {})
3034             orig_image_ref = volume_image_metadata.get('image_id')
3035 
3036             if orig_image_ref != image_href:
3037                 # Leave a breadcrumb.
3038                 LOG.debug('Requested to rebuild instance with a new image %s '
3039                           'for a volume-backed server with image %s in its '
3040                           'root volume which is not supported.', image_href,
3041                           orig_image_ref, instance=instance)
3042                 msg = _('Unable to rebuild with a different image for a '
3043                         'volume-backed server.')
3044                 raise exception.ImageUnacceptable(
3045                     image_id=image_href, reason=msg)
3046         else:
3047             orig_image_ref = instance.image_ref
3048 
3049         self._checks_for_create_and_rebuild(context, image_id, image,
3050                 flavor, metadata, files_to_inject, root_bdm)
3051 
3052         kernel_id, ramdisk_id = self._handle_kernel_and_ramdisk(
3053                 context, None, None, image)
3054 
3055         def _reset_image_metadata():
3056             """Remove old image properties that we're storing as instance
3057             system metadata.  These properties start with 'image_'.
3058             Then add the properties for the new image.
3059             """
3060             # FIXME(comstud): There's a race condition here in that if
3061             # the system_metadata for this instance is updated after
3062             # we do the previous save() and before we update.. those
3063             # other updates will be lost. Since this problem exists in
3064             # a lot of other places, I think it should be addressed in
3065             # a DB layer overhaul.
3066 
3067             orig_sys_metadata = dict(instance.system_metadata)
3068             # Remove the old keys
3069             for key in list(instance.system_metadata.keys()):
3070                 if key.startswith(utils.SM_IMAGE_PROP_PREFIX):
3071                     del instance.system_metadata[key]
3072 
3073             # Add the new ones
3074             new_sys_metadata = utils.get_system_metadata_from_image(
3075                 image, flavor)
3076 
3077             instance.system_metadata.update(new_sys_metadata)
3078             instance.save()
3079             return orig_sys_metadata
3080 
3081         # Since image might have changed, we may have new values for
3082         # os_type, vm_mode, etc
3083         options_from_image = self._inherit_properties_from_image(
3084                 image, auto_disk_config)
3085         instance.update(options_from_image)
3086 
3087         # Use trusted_certs value from kwargs to create TrustedCerts object
3088         retrieved_certs = self._retrieve_trusted_certs_object(
3089             kwargs.get('trusted_certs', None))
3090         if retrieved_certs:
3091             kwargs['trusted_certs'] = retrieved_certs
3092 
3093         instance.task_state = task_states.REBUILDING
3094         # An empty instance.image_ref is currently used as an indication
3095         # of BFV.  Preserve that over a rebuild to not break users.
3096         if not is_volume_backed:
3097             instance.image_ref = image_href
3098         instance.kernel_id = kernel_id or ""
3099         instance.ramdisk_id = ramdisk_id or ""
3100         instance.progress = 0
3101         instance.update(kwargs)
3102         instance.save(expected_task_state=[None])
3103 
3104         # On a rebuild, since we're potentially changing images, we need to
3105         # wipe out the old image properties that we're storing as instance
3106         # system metadata... and copy in the properties for the new image.
3107         orig_sys_metadata = _reset_image_metadata()
3108 
3109         self._record_action_start(context, instance, instance_actions.REBUILD)
3110 
3111         # NOTE(sbauza): The migration script we provided in Newton should make
3112         # sure that all our instances are currently migrated to have an
3113         # attached RequestSpec object but let's consider that the operator only
3114         # half migrated all their instances in the meantime.
3115         host = instance.host
3116         try:
3117             request_spec = objects.RequestSpec.get_by_instance_uuid(
3118                 context, instance.uuid)
3119             # If a new image is provided on rebuild, we will need to run
3120             # through the scheduler again, but we want the instance to be
3121             # rebuilt on the same host it's already on.
3122             if orig_image_ref != image_href:
3123                 # We have to modify the request spec that goes to the scheduler
3124                 # to contain the new image. We persist this since we've already
3125                 # changed the instance.image_ref above so we're being
3126                 # consistent.
3127                 request_spec.image = objects.ImageMeta.from_dict(image)
3128                 request_spec.save()
3129                 if 'scheduler_hints' not in request_spec:
3130                     request_spec.scheduler_hints = {}
3131                 # Nuke the id on this so we can't accidentally save
3132                 # this hint hack later
3133                 del request_spec.id
3134 
3135                 # NOTE(danms): Passing host=None tells conductor to
3136                 # call the scheduler. The _nova_check_type hint
3137                 # requires that the scheduler returns only the same
3138                 # host that we are currently on and only checks
3139                 # rebuild-related filters.
3140                 request_spec.scheduler_hints['_nova_check_type'] = ['rebuild']
3141                 request_spec.force_hosts = [instance.host]
3142                 request_spec.force_nodes = [instance.node]
3143                 host = None
3144         except exception.RequestSpecNotFound:
3145             # Some old instances can still have no RequestSpec object attached
3146             # to them, we need to support the old way
3147             request_spec = None
3148 
3149         self.compute_task_api.rebuild_instance(context, instance=instance,
3150                 new_pass=admin_password, injected_files=files_to_inject,
3151                 image_ref=image_href, orig_image_ref=orig_image_ref,
3152                 orig_sys_metadata=orig_sys_metadata, bdms=bdms,
3153                 preserve_ephemeral=preserve_ephemeral, host=host,
3154                 request_spec=request_spec,
3155                 kwargs=kwargs)
3156 
3157     @staticmethod
3158     def _check_quota_for_upsize(context, instance, current_flavor, new_flavor):
3159         project_id, user_id = quotas_obj.ids_from_instance(context,
3160                                                            instance)
3161         # Deltas will be empty if the resize is not an upsize.
3162         deltas = compute_utils.upsize_quota_delta(context, new_flavor,
3163                                                   current_flavor)
3164         if deltas:
3165             try:
3166                 res_deltas = {'cores': deltas.get('cores', 0),
3167                               'ram': deltas.get('ram', 0)}
3168                 objects.Quotas.check_deltas(context, res_deltas,
3169                                             project_id, user_id=user_id,
3170                                             check_project_id=project_id,
3171                                             check_user_id=user_id)
3172             except exception.OverQuota as exc:
3173                 quotas = exc.kwargs['quotas']
3174                 overs = exc.kwargs['overs']
3175                 usages = exc.kwargs['usages']
3176                 headroom = compute_utils.get_headroom(quotas, usages,
3177                                                       deltas)
3178                 (overs, reqs, total_alloweds,
3179                  useds) = compute_utils.get_over_quota_detail(headroom,
3180                                                               overs,
3181                                                               quotas,
3182                                                               deltas)
3183                 LOG.warning("%(overs)s quota exceeded for %(pid)s,"
3184                             " tried to resize instance.",
3185                             {'overs': overs, 'pid': context.project_id})
3186                 raise exception.TooManyInstances(overs=overs,
3187                                                  req=reqs,
3188                                                  used=useds,
3189                                                  allowed=total_alloweds)
3190 
3191     @check_instance_lock
3192     @check_instance_cell
3193     @check_instance_state(vm_state=[vm_states.RESIZED])
3194     def revert_resize(self, context, instance):
3195         """Reverts a resize, deleting the 'new' instance in the process."""
3196         elevated = context.elevated()
3197         migration = objects.Migration.get_by_instance_and_status(
3198             elevated, instance.uuid, 'finished')
3199 
3200         # If this is a resize down, a revert might go over quota.
3201         self._check_quota_for_upsize(context, instance, instance.flavor,
3202                                      instance.old_flavor)
3203 
3204         instance.task_state = task_states.RESIZE_REVERTING
3205         instance.save(expected_task_state=[None])
3206 
3207         migration.status = 'reverting'
3208         migration.save()
3209 
3210         self._record_action_start(context, instance,
3211                                   instance_actions.REVERT_RESIZE)
3212 
3213         # TODO(melwitt): We're not rechecking for strict quota here to guard
3214         # against going over quota during a race at this time because the
3215         # resource consumption for this operation is written to the database
3216         # by compute.
3217         self.compute_rpcapi.revert_resize(context, instance,
3218                                           migration,
3219                                           migration.dest_compute)
3220 
3221     @check_instance_lock
3222     @check_instance_cell
3223     @check_instance_state(vm_state=[vm_states.RESIZED])
3224     def confirm_resize(self, context, instance, migration=None):
3225         """Confirms a migration/resize and deletes the 'old' instance."""
3226         elevated = context.elevated()
3227         # NOTE(melwitt): We're not checking quota here because there isn't a
3228         # change in resource usage when confirming a resize. Resource
3229         # consumption for resizes are written to the database by compute, so
3230         # a confirm resize is just a clean up of the migration objects and a
3231         # state change in compute.
3232         if migration is None:
3233             migration = objects.Migration.get_by_instance_and_status(
3234                 elevated, instance.uuid, 'finished')
3235 
3236         migration.status = 'confirming'
3237         migration.save()
3238 
3239         self._record_action_start(context, instance,
3240                                   instance_actions.CONFIRM_RESIZE)
3241 
3242         self.compute_rpcapi.confirm_resize(context,
3243                                            instance,
3244                                            migration,
3245                                            migration.source_compute)
3246 
3247     @staticmethod
3248     def _resize_cells_support(context, instance,
3249                               current_instance_type, new_instance_type):
3250         """Special API cell logic for resize."""
3251         # NOTE(johannes/comstud): The API cell needs a local migration
3252         # record for later resize_confirm and resize_reverts.
3253         # We don't need source and/or destination
3254         # information, just the old and new flavors. Status is set to
3255         # 'finished' since nothing else will update the status along
3256         # the way.
3257         mig = objects.Migration(context=context.elevated())
3258         mig.instance_uuid = instance.uuid
3259         mig.old_instance_type_id = current_instance_type['id']
3260         mig.new_instance_type_id = new_instance_type['id']
3261         mig.status = 'finished'
3262         mig.migration_type = (
3263             mig.old_instance_type_id != mig.new_instance_type_id and
3264             'resize' or 'migration')
3265         mig.create()
3266 
3267     @check_instance_lock
3268     @check_instance_cell
3269     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED])
3270     def resize(self, context, instance, flavor_id=None, clean_shutdown=True,
3271                host_name=None, **extra_instance_updates):
3272         """Resize (ie, migrate) a running instance.
3273 
3274         If flavor_id is None, the process is considered a migration, keeping
3275         the original flavor_id. If flavor_id is not None, the instance should
3276         be migrated to a new host and resized to the new flavor_id.
3277         host_name is always None in the resize case.
3278         host_name can be set in the cold migration case only.
3279         """
3280         if host_name is not None:
3281             # Cannot migrate to the host where the instance exists
3282             # because it is useless.
3283             if host_name == instance.host:
3284                 raise exception.CannotMigrateToSameHost()
3285 
3286             # Check whether host exists or not.
3287             node = objects.ComputeNode.get_first_node_by_host_for_old_compat(
3288                 context, host_name, use_slave=True)
3289 
3290         self._check_auto_disk_config(instance, **extra_instance_updates)
3291 
3292         current_instance_type = instance.get_flavor()
3293 
3294         # If flavor_id is not provided, only migrate the instance.
3295         if not flavor_id:
3296             LOG.debug("flavor_id is None. Assuming migration.",
3297                       instance=instance)
3298             new_instance_type = current_instance_type
3299         else:
3300             new_instance_type = flavors.get_flavor_by_flavor_id(
3301                     flavor_id, read_deleted="no")
3302             if (new_instance_type.get('root_gb') == 0 and
3303                 current_instance_type.get('root_gb') != 0 and
3304                 not compute_utils.is_volume_backed_instance(context,
3305                     instance)):
3306                 reason = _('Resize to zero disk flavor is not allowed.')
3307                 raise exception.CannotResizeDisk(reason=reason)
3308 
3309         if not new_instance_type:
3310             raise exception.FlavorNotFound(flavor_id=flavor_id)
3311 
3312         current_instance_type_name = current_instance_type['name']
3313         new_instance_type_name = new_instance_type['name']
3314         LOG.debug("Old instance type %(current_instance_type_name)s, "
3315                   "new instance type %(new_instance_type_name)s",
3316                   {'current_instance_type_name': current_instance_type_name,
3317                    'new_instance_type_name': new_instance_type_name},
3318                   instance=instance)
3319 
3320         same_instance_type = (current_instance_type['id'] ==
3321                               new_instance_type['id'])
3322 
3323         # NOTE(sirp): We don't want to force a customer to change their flavor
3324         # when Ops is migrating off of a failed host.
3325         if not same_instance_type and new_instance_type.get('disabled'):
3326             raise exception.FlavorNotFound(flavor_id=flavor_id)
3327 
3328         if same_instance_type and flavor_id and self.cell_type != 'compute':
3329             raise exception.CannotResizeToSameFlavor()
3330 
3331         # ensure there is sufficient headroom for upsizes
3332         if flavor_id:
3333             self._check_quota_for_upsize(context, instance,
3334                                          current_instance_type,
3335                                          new_instance_type)
3336 
3337         instance.task_state = task_states.RESIZE_PREP
3338         instance.progress = 0
3339         instance.update(extra_instance_updates)
3340         instance.save(expected_task_state=[None])
3341 
3342         filter_properties = {'ignore_hosts': []}
3343 
3344         if not CONF.allow_resize_to_same_host:
3345             filter_properties['ignore_hosts'].append(instance.host)
3346 
3347         if self.cell_type == 'api':
3348             # Create migration record.
3349             self._resize_cells_support(context, instance,
3350                                        current_instance_type,
3351                                        new_instance_type)
3352 
3353         if not flavor_id:
3354             self._record_action_start(context, instance,
3355                                       instance_actions.MIGRATE)
3356         else:
3357             self._record_action_start(context, instance,
3358                                       instance_actions.RESIZE)
3359 
3360         # NOTE(sbauza): The migration script we provided in Newton should make
3361         # sure that all our instances are currently migrated to have an
3362         # attached RequestSpec object but let's consider that the operator only
3363         # half migrated all their instances in the meantime.
3364         try:
3365             request_spec = objects.RequestSpec.get_by_instance_uuid(
3366                 context, instance.uuid)
3367             request_spec.ignore_hosts = filter_properties['ignore_hosts']
3368         except exception.RequestSpecNotFound:
3369             # Some old instances can still have no RequestSpec object attached
3370             # to them, we need to support the old way
3371             if host_name is not None:
3372                 # If there is no request spec we cannot honor the request
3373                 # and we need to fail.
3374                 raise exception.CannotMigrateWithTargetHost()
3375             request_spec = None
3376 
3377         # TODO(melwitt): We're not rechecking for strict quota here to guard
3378         # against going over quota during a race at this time because the
3379         # resource consumption for this operation is written to the database
3380         # by compute.
3381         scheduler_hint = {'filter_properties': filter_properties}
3382 
3383         if request_spec:
3384             if host_name is None:
3385                 # If 'host_name' is not specified,
3386                 # clear the 'requested_destination' field of the RequestSpec.
3387                 request_spec.requested_destination = None
3388             else:
3389                 # Set the host and the node so that the scheduler will
3390                 # validate them.
3391                 # TODO(takashin): It will be added to check whether
3392                 # the specified host is within the same cell as
3393                 # the instance or not. If not, raise specific error message
3394                 # that is clear to the caller.
3395                 request_spec.requested_destination = objects.Destination(
3396                     host=node.host, node=node.hypervisor_hostname)
3397 
3398         self.compute_task_api.resize_instance(context, instance,
3399                 extra_instance_updates, scheduler_hint=scheduler_hint,
3400                 flavor=new_instance_type,
3401                 clean_shutdown=clean_shutdown,
3402                 request_spec=request_spec)
3403 
3404     @check_instance_lock
3405     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
3406                                     vm_states.PAUSED, vm_states.SUSPENDED])
3407     def shelve(self, context, instance, clean_shutdown=True):
3408         """Shelve an instance.
3409 
3410         Shuts down an instance and frees it up to be removed from the
3411         hypervisor.
3412         """
3413         instance.task_state = task_states.SHELVING
3414         instance.save(expected_task_state=[None])
3415 
3416         self._record_action_start(context, instance, instance_actions.SHELVE)
3417 
3418         if not compute_utils.is_volume_backed_instance(context, instance):
3419             name = '%s-shelved' % instance.display_name
3420             image_meta = self._create_image(context, instance, name,
3421                     'snapshot')
3422             image_id = image_meta['id']
3423             self.compute_rpcapi.shelve_instance(context, instance=instance,
3424                     image_id=image_id, clean_shutdown=clean_shutdown)
3425         else:
3426             self.compute_rpcapi.shelve_offload_instance(context,
3427                     instance=instance, clean_shutdown=clean_shutdown)
3428 
3429     @check_instance_lock
3430     @check_instance_state(vm_state=[vm_states.SHELVED])
3431     def shelve_offload(self, context, instance, clean_shutdown=True):
3432         """Remove a shelved instance from the hypervisor."""
3433         instance.task_state = task_states.SHELVING_OFFLOADING
3434         instance.save(expected_task_state=[None])
3435 
3436         self._record_action_start(context, instance,
3437                                   instance_actions.SHELVE_OFFLOAD)
3438 
3439         self.compute_rpcapi.shelve_offload_instance(context, instance=instance,
3440             clean_shutdown=clean_shutdown)
3441 
3442     @check_instance_lock
3443     @check_instance_state(vm_state=[vm_states.SHELVED,
3444         vm_states.SHELVED_OFFLOADED])
3445     def unshelve(self, context, instance):
3446         """Restore a shelved instance."""
3447         instance.task_state = task_states.UNSHELVING
3448         instance.save(expected_task_state=[None])
3449 
3450         self._record_action_start(context, instance, instance_actions.UNSHELVE)
3451 
3452         try:
3453             request_spec = objects.RequestSpec.get_by_instance_uuid(
3454                 context, instance.uuid)
3455         except exception.RequestSpecNotFound:
3456             # Some old instances can still have no RequestSpec object attached
3457             # to them, we need to support the old way
3458             request_spec = None
3459         self.compute_task_api.unshelve_instance(context, instance,
3460                                                 request_spec)
3461 
3462     @check_instance_lock
3463     def add_fixed_ip(self, context, instance, network_id):
3464         """Add fixed_ip from specified network to given instance."""
3465         self.compute_rpcapi.add_fixed_ip_to_instance(context,
3466                 instance=instance, network_id=network_id)
3467 
3468     @check_instance_lock
3469     def remove_fixed_ip(self, context, instance, address):
3470         """Remove fixed_ip from specified network to given instance."""
3471         self.compute_rpcapi.remove_fixed_ip_from_instance(context,
3472                 instance=instance, address=address)
3473 
3474     @check_instance_lock
3475     @check_instance_cell
3476     @check_instance_state(vm_state=[vm_states.ACTIVE])
3477     def pause(self, context, instance):
3478         """Pause the given instance."""
3479         instance.task_state = task_states.PAUSING
3480         instance.save(expected_task_state=[None])
3481         self._record_action_start(context, instance, instance_actions.PAUSE)
3482         self.compute_rpcapi.pause_instance(context, instance)
3483 
3484     @check_instance_lock
3485     @check_instance_cell
3486     @check_instance_state(vm_state=[vm_states.PAUSED])
3487     def unpause(self, context, instance):
3488         """Unpause the given instance."""
3489         instance.task_state = task_states.UNPAUSING
3490         instance.save(expected_task_state=[None])
3491         self._record_action_start(context, instance, instance_actions.UNPAUSE)
3492         self.compute_rpcapi.unpause_instance(context, instance)
3493 
3494     @check_instance_host
3495     def get_diagnostics(self, context, instance):
3496         """Retrieve diagnostics for the given instance."""
3497         return self.compute_rpcapi.get_diagnostics(context, instance=instance)
3498 
3499     @check_instance_host
3500     def get_instance_diagnostics(self, context, instance):
3501         """Retrieve diagnostics for the given instance."""
3502         return self.compute_rpcapi.get_instance_diagnostics(context,
3503                                                             instance=instance)
3504 
3505     @check_instance_lock
3506     @check_instance_cell
3507     @check_instance_state(vm_state=[vm_states.ACTIVE])
3508     def suspend(self, context, instance):
3509         """Suspend the given instance."""
3510         instance.task_state = task_states.SUSPENDING
3511         instance.save(expected_task_state=[None])
3512         self._record_action_start(context, instance, instance_actions.SUSPEND)
3513         self.compute_rpcapi.suspend_instance(context, instance)
3514 
3515     @check_instance_lock
3516     @check_instance_cell
3517     @check_instance_state(vm_state=[vm_states.SUSPENDED])
3518     def resume(self, context, instance):
3519         """Resume the given instance."""
3520         instance.task_state = task_states.RESUMING
3521         instance.save(expected_task_state=[None])
3522         self._record_action_start(context, instance, instance_actions.RESUME)
3523         self.compute_rpcapi.resume_instance(context, instance)
3524 
3525     @check_instance_lock
3526     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
3527                                     vm_states.ERROR])
3528     def rescue(self, context, instance, rescue_password=None,
3529                rescue_image_ref=None, clean_shutdown=True):
3530         """Rescue the given instance."""
3531 
3532         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
3533                     context, instance.uuid)
3534         for bdm in bdms:
3535             if bdm.volume_id:
3536                 vol = self.volume_api.get(context, bdm.volume_id)
3537                 self.volume_api.check_attached(context, vol)
3538         if compute_utils.is_volume_backed_instance(context, instance, bdms):
3539             reason = _("Cannot rescue a volume-backed instance")
3540             raise exception.InstanceNotRescuable(instance_id=instance.uuid,
3541                                                  reason=reason)
3542 
3543         instance.task_state = task_states.RESCUING
3544         instance.save(expected_task_state=[None])
3545 
3546         self._record_action_start(context, instance, instance_actions.RESCUE)
3547 
3548         self.compute_rpcapi.rescue_instance(context, instance=instance,
3549             rescue_password=rescue_password, rescue_image_ref=rescue_image_ref,
3550             clean_shutdown=clean_shutdown)
3551 
3552     @check_instance_lock
3553     @check_instance_state(vm_state=[vm_states.RESCUED])
3554     def unrescue(self, context, instance):
3555         """Unrescue the given instance."""
3556         instance.task_state = task_states.UNRESCUING
3557         instance.save(expected_task_state=[None])
3558 
3559         self._record_action_start(context, instance, instance_actions.UNRESCUE)
3560 
3561         self.compute_rpcapi.unrescue_instance(context, instance=instance)
3562 
3563     @check_instance_lock
3564     @check_instance_cell
3565     @check_instance_state(vm_state=[vm_states.ACTIVE])
3566     def set_admin_password(self, context, instance, password=None):
3567         """Set the root/admin password for the given instance.
3568 
3569         @param context: Nova auth context.
3570         @param instance: Nova instance object.
3571         @param password: The admin password for the instance.
3572         """
3573         instance.task_state = task_states.UPDATING_PASSWORD
3574         instance.save(expected_task_state=[None])
3575 
3576         self._record_action_start(context, instance,
3577                                   instance_actions.CHANGE_PASSWORD)
3578 
3579         self.compute_rpcapi.set_admin_password(context,
3580                                                instance=instance,
3581                                                new_pass=password)
3582 
3583     @check_instance_host
3584     @reject_instance_state(
3585         task_state=[task_states.DELETING, task_states.MIGRATING])
3586     def get_vnc_console(self, context, instance, console_type):
3587         """Get a url to an instance Console."""
3588         connect_info = self.compute_rpcapi.get_vnc_console(context,
3589                 instance=instance, console_type=console_type)
3590 
3591         self.consoleauth_rpcapi.authorize_console(context,
3592                 connect_info['token'], console_type,
3593                 connect_info['host'], connect_info['port'],
3594                 connect_info['internal_access_path'], instance.uuid,
3595                 access_url=connect_info['access_url'])
3596 
3597         return {'url': connect_info['access_url']}
3598 
3599     @check_instance_host
3600     def get_vnc_connect_info(self, context, instance, console_type):
3601         """Used in a child cell to get console info."""
3602         connect_info = self.compute_rpcapi.get_vnc_console(context,
3603                 instance=instance, console_type=console_type)
3604         return connect_info
3605 
3606     @check_instance_host
3607     @reject_instance_state(
3608         task_state=[task_states.DELETING, task_states.MIGRATING])
3609     def get_spice_console(self, context, instance, console_type):
3610         """Get a url to an instance Console."""
3611         connect_info = self.compute_rpcapi.get_spice_console(context,
3612                 instance=instance, console_type=console_type)
3613         self.consoleauth_rpcapi.authorize_console(context,
3614                 connect_info['token'], console_type,
3615                 connect_info['host'], connect_info['port'],
3616                 connect_info['internal_access_path'], instance.uuid,
3617                 access_url=connect_info['access_url'])
3618 
3619         return {'url': connect_info['access_url']}
3620 
3621     @check_instance_host
3622     def get_spice_connect_info(self, context, instance, console_type):
3623         """Used in a child cell to get console info."""
3624         connect_info = self.compute_rpcapi.get_spice_console(context,
3625                 instance=instance, console_type=console_type)
3626         return connect_info
3627 
3628     @check_instance_host
3629     @reject_instance_state(
3630         task_state=[task_states.DELETING, task_states.MIGRATING])
3631     def get_rdp_console(self, context, instance, console_type):
3632         """Get a url to an instance Console."""
3633         connect_info = self.compute_rpcapi.get_rdp_console(context,
3634                 instance=instance, console_type=console_type)
3635         self.consoleauth_rpcapi.authorize_console(context,
3636                 connect_info['token'], console_type,
3637                 connect_info['host'], connect_info['port'],
3638                 connect_info['internal_access_path'], instance.uuid,
3639                 access_url=connect_info['access_url'])
3640 
3641         return {'url': connect_info['access_url']}
3642 
3643     @check_instance_host
3644     def get_rdp_connect_info(self, context, instance, console_type):
3645         """Used in a child cell to get console info."""
3646         connect_info = self.compute_rpcapi.get_rdp_console(context,
3647                 instance=instance, console_type=console_type)
3648         return connect_info
3649 
3650     @check_instance_host
3651     @reject_instance_state(
3652         task_state=[task_states.DELETING, task_states.MIGRATING])
3653     def get_serial_console(self, context, instance, console_type):
3654         """Get a url to a serial console."""
3655         connect_info = self.compute_rpcapi.get_serial_console(context,
3656                 instance=instance, console_type=console_type)
3657 
3658         self.consoleauth_rpcapi.authorize_console(context,
3659                 connect_info['token'], console_type,
3660                 connect_info['host'], connect_info['port'],
3661                 connect_info['internal_access_path'], instance.uuid,
3662                 access_url=connect_info['access_url'])
3663         return {'url': connect_info['access_url']}
3664 
3665     @check_instance_host
3666     def get_serial_console_connect_info(self, context, instance, console_type):
3667         """Used in a child cell to get serial console."""
3668         connect_info = self.compute_rpcapi.get_serial_console(context,
3669                 instance=instance, console_type=console_type)
3670         return connect_info
3671 
3672     @check_instance_host
3673     @reject_instance_state(
3674         task_state=[task_states.DELETING, task_states.MIGRATING])
3675     def get_mks_console(self, context, instance, console_type):
3676         """Get a url to a MKS console."""
3677         connect_info = self.compute_rpcapi.get_mks_console(context,
3678                 instance=instance, console_type=console_type)
3679         self.consoleauth_rpcapi.authorize_console(context,
3680                 connect_info['token'], console_type,
3681                 connect_info['host'], connect_info['port'],
3682                 connect_info['internal_access_path'], instance.uuid,
3683                 access_url=connect_info['access_url'])
3684         return {'url': connect_info['access_url']}
3685 
3686     @check_instance_host
3687     def get_console_output(self, context, instance, tail_length=None):
3688         """Get console output for an instance."""
3689         return self.compute_rpcapi.get_console_output(context,
3690                 instance=instance, tail_length=tail_length)
3691 
3692     def lock(self, context, instance):
3693         """Lock the given instance."""
3694         # Only update the lock if we are an admin (non-owner)
3695         is_owner = instance.project_id == context.project_id
3696         if instance.locked and is_owner:
3697             return
3698 
3699         context = context.elevated()
3700         self._record_action_start(context, instance,
3701                                   instance_actions.LOCK)
3702 
3703         @wrap_instance_event(prefix='api')
3704         def lock(self, context, instance):
3705             LOG.debug('Locking', instance=instance)
3706             instance.locked = True
3707             instance.locked_by = 'owner' if is_owner else 'admin'
3708             instance.save()
3709 
3710         lock(self, context, instance)
3711 
3712     def is_expected_locked_by(self, context, instance):
3713         is_owner = instance.project_id == context.project_id
3714         expect_locked_by = 'owner' if is_owner else 'admin'
3715         locked_by = instance.locked_by
3716         if locked_by and locked_by != expect_locked_by:
3717             return False
3718         return True
3719 
3720     def unlock(self, context, instance):
3721         """Unlock the given instance."""
3722         context = context.elevated()
3723         self._record_action_start(context, instance,
3724                                   instance_actions.UNLOCK)
3725 
3726         @wrap_instance_event(prefix='api')
3727         def unlock(self, context, instance):
3728             LOG.debug('Unlocking', instance=instance)
3729             instance.locked = False
3730             instance.locked_by = None
3731             instance.save()
3732 
3733         unlock(self, context, instance)
3734 
3735     @check_instance_lock
3736     @check_instance_cell
3737     def reset_network(self, context, instance):
3738         """Reset networking on the instance."""
3739         self.compute_rpcapi.reset_network(context, instance=instance)
3740 
3741     @check_instance_lock
3742     @check_instance_cell
3743     def inject_network_info(self, context, instance):
3744         """Inject network info for the instance."""
3745         self.compute_rpcapi.inject_network_info(context, instance=instance)
3746 
3747     def _create_volume_bdm(self, context, instance, device, volume,
3748                            disk_bus, device_type, is_local_creation=False,
3749                            tag=None):
3750         volume_id = volume['id']
3751         if is_local_creation:
3752             # when the creation is done locally we can't specify the device
3753             # name as we do not have a way to check that the name specified is
3754             # a valid one.
3755             # We leave the setting of that value when the actual attach
3756             # happens on the compute manager
3757             # NOTE(artom) Local attach (to a shelved-offload instance) cannot
3758             # support device tagging because we have no way to call the compute
3759             # manager to check that it supports device tagging. In fact, we
3760             # don't even know which computer manager the instance will
3761             # eventually end up on when it's unshelved.
3762             volume_bdm = objects.BlockDeviceMapping(
3763                 context=context,
3764                 source_type='volume', destination_type='volume',
3765                 instance_uuid=instance.uuid, boot_index=None,
3766                 volume_id=volume_id,
3767                 device_name=None, guest_format=None,
3768                 disk_bus=disk_bus, device_type=device_type)
3769             volume_bdm.create()
3770         else:
3771             # NOTE(vish): This is done on the compute host because we want
3772             #             to avoid a race where two devices are requested at
3773             #             the same time. When db access is removed from
3774             #             compute, the bdm will be created here and we will
3775             #             have to make sure that they are assigned atomically.
3776             volume_bdm = self.compute_rpcapi.reserve_block_device_name(
3777                 context, instance, device, volume_id, disk_bus=disk_bus,
3778                 device_type=device_type, tag=tag,
3779                 multiattach=volume['multiattach'])
3780         return volume_bdm
3781 
3782     def _check_volume_already_attached_to_instance(self, context, instance,
3783                                                    volume_id):
3784         """Avoid attaching the same volume to the same instance twice.
3785 
3786            As the new Cinder flow (microversion 3.44) is handling the checks
3787            differently and allows to attach the same volume to the same
3788            instance twice to enable live_migrate we are checking whether the
3789            BDM already exists for this combination for the new flow and fail
3790            if it does.
3791         """
3792 
3793         try:
3794             objects.BlockDeviceMapping.get_by_volume_and_instance(
3795                 context, volume_id, instance.uuid)
3796 
3797             msg = _("volume %s already attached") % volume_id
3798             raise exception.InvalidVolume(reason=msg)
3799         except exception.VolumeBDMNotFound:
3800             pass
3801 
3802     def _check_attach_and_reserve_volume(self, context, volume, instance,
3803                                          bdm, supports_multiattach=False):
3804         volume_id = volume['id']
3805         self.volume_api.check_availability_zone(context, volume,
3806                                                 instance=instance)
3807         # If volume.multiattach=True and the microversion to
3808         # support multiattach is not used, fail the request.
3809         if volume['multiattach'] and not supports_multiattach:
3810             raise exception.MultiattachNotSupportedOldMicroversion()
3811 
3812         if 'id' in instance:
3813             # This is a volume attach to an existing instance, so
3814             # we only care about the cell the instance is in.
3815             min_compute_version = objects.Service.get_minimum_version(
3816                 context, 'nova-compute')
3817         else:
3818             # The instance is being created and we don't know which
3819             # cell it's going to land in, so check all cells.
3820             min_compute_version = \
3821                 objects.service.get_minimum_version_all_cells(
3822                     context, ['nova-compute'])
3823             # Check to see if the computes have been upgraded to support
3824             # booting from a multiattach volume.
3825             if (volume['multiattach'] and
3826                     min_compute_version < MIN_COMPUTE_MULTIATTACH):
3827                 raise exception.MultiattachSupportNotYetAvailable()
3828 
3829         if min_compute_version >= CINDER_V3_ATTACH_MIN_COMPUTE_VERSION:
3830             # Attempt a new style volume attachment, but fallback to old-style
3831             # in case Cinder API 3.44 isn't available.
3832             try:
3833                 attachment_id = self.volume_api.attachment_create(
3834                     context, volume_id, instance.uuid)['id']
3835                 bdm.attachment_id = attachment_id
3836                 # NOTE(ildikov): In case of boot from volume the BDM at this
3837                 # point is not yet created in a cell database, so we can't
3838                 # call save().  When attaching a volume to an existing
3839                 # instance, the instance is already in a cell and the BDM has
3840                 # been created in that same cell so updating here in that case
3841                 # is "ok".
3842                 if bdm.obj_attr_is_set('id'):
3843                     bdm.save()
3844             except exception.CinderAPIVersionNotAvailable:
3845                 LOG.debug('The available Cinder microversion is not high '
3846                           'enough to create new style volume attachment.')
3847                 self.volume_api.reserve_volume(context, volume_id)
3848         else:
3849             LOG.debug('The compute service version is not high enough to '
3850                       'create a new style volume attachment.')
3851             self.volume_api.reserve_volume(context, volume_id)
3852 
3853     def _attach_volume(self, context, instance, volume, device,
3854                        disk_bus, device_type, tag=None,
3855                        supports_multiattach=False):
3856         """Attach an existing volume to an existing instance.
3857 
3858         This method is separated to make it possible for cells version
3859         to override it.
3860         """
3861         volume_bdm = self._create_volume_bdm(
3862             context, instance, device, volume, disk_bus=disk_bus,
3863             device_type=device_type, tag=tag)
3864         try:
3865             self._check_attach_and_reserve_volume(context, volume, instance,
3866                                                   volume_bdm,
3867                                                   supports_multiattach)
3868             self._record_action_start(
3869                 context, instance, instance_actions.ATTACH_VOLUME)
3870             self.compute_rpcapi.attach_volume(context, instance, volume_bdm)
3871         except Exception:
3872             with excutils.save_and_reraise_exception():
3873                 volume_bdm.destroy()
3874 
3875         return volume_bdm.device_name
3876 
3877     def _attach_volume_shelved_offloaded(self, context, instance, volume,
3878                                          device, disk_bus, device_type):
3879         """Attach an existing volume to an instance in shelved offloaded state.
3880 
3881         Attaching a volume for an instance in shelved offloaded state requires
3882         to perform the regular check to see if we can attach and reserve the
3883         volume then we need to call the attach method on the volume API
3884         to mark the volume as 'in-use'.
3885         The instance at this stage is not managed by a compute manager
3886         therefore the actual attachment will be performed once the
3887         instance will be unshelved.
3888         """
3889         volume_id = volume['id']
3890 
3891         @wrap_instance_event(prefix='api')
3892         def attach_volume(self, context, v_id, instance, dev, attachment_id):
3893             if attachment_id:
3894                 # Normally we wouldn't complete an attachment without a host
3895                 # connector, but we do this to make the volume status change
3896                 # to "in-use" to maintain the API semantics with the old flow.
3897                 # When unshelving the instance, the compute service will deal
3898                 # with this disconnected attachment.
3899                 self.volume_api.attachment_complete(context, attachment_id)
3900             else:
3901                 self.volume_api.attach(context,
3902                                        v_id,
3903                                        instance.uuid,
3904                                        dev)
3905 
3906         volume_bdm = self._create_volume_bdm(
3907             context, instance, device, volume, disk_bus=disk_bus,
3908             device_type=device_type, is_local_creation=True)
3909         try:
3910             self._check_attach_and_reserve_volume(context, volume, instance,
3911                                                   volume_bdm)
3912             self._record_action_start(
3913                 context, instance,
3914                 instance_actions.ATTACH_VOLUME)
3915             attach_volume(self, context, volume_id, instance, device,
3916                           volume_bdm.attachment_id)
3917         except Exception:
3918             with excutils.save_and_reraise_exception():
3919                 volume_bdm.destroy()
3920 
3921         return volume_bdm.device_name
3922 
3923     @check_instance_lock
3924     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
3925                                     vm_states.STOPPED, vm_states.RESIZED,
3926                                     vm_states.SOFT_DELETED, vm_states.SHELVED,
3927                                     vm_states.SHELVED_OFFLOADED])
3928     def attach_volume(self, context, instance, volume_id, device=None,
3929                       disk_bus=None, device_type=None, tag=None,
3930                       supports_multiattach=False):
3931         """Attach an existing volume to an existing instance."""
3932         # NOTE(vish): Fail fast if the device is not going to pass. This
3933         #             will need to be removed along with the test if we
3934         #             change the logic in the manager for what constitutes
3935         #             a valid device.
3936         if device and not block_device.match_device(device):
3937             raise exception.InvalidDevicePath(path=device)
3938 
3939         # Check to see if the computes in this cell can support new-style
3940         # volume attachments.
3941         min_compute_version = objects.Service.get_minimum_version(
3942             context, 'nova-compute')
3943         if min_compute_version >= CINDER_V3_ATTACH_MIN_COMPUTE_VERSION:
3944             try:
3945                 # Check to see if Cinder is new enough to create new-style
3946                 # attachments.
3947                 cinder.is_microversion_supported(context, '3.44')
3948             except exception.CinderAPIVersionNotAvailable:
3949                 pass
3950             else:
3951                 # Make sure the volume isn't already attached to this instance
3952                 # because based on the above checks, we'll use the new style
3953                 # attachment flow in _check_attach_and_reserve_volume and
3954                 # Cinder will allow multiple attachments between the same
3955                 # volume and instance but the old flow API semantics don't
3956                 # allow that so we enforce it here.
3957                 self._check_volume_already_attached_to_instance(context,
3958                                                                 instance,
3959                                                                 volume_id)
3960 
3961         volume = self.volume_api.get(context, volume_id)
3962         is_shelved_offloaded = instance.vm_state == vm_states.SHELVED_OFFLOADED
3963         if is_shelved_offloaded:
3964             if tag:
3965                 # NOTE(artom) Local attach (to a shelved-offload instance)
3966                 # cannot support device tagging because we have no way to call
3967                 # the compute manager to check that it supports device tagging.
3968                 # In fact, we don't even know which computer manager the
3969                 # instance will eventually end up on when it's unshelved.
3970                 raise exception.VolumeTaggedAttachToShelvedNotSupported()
3971             if volume['multiattach']:
3972                 # NOTE(mriedem): Similar to tagged attach, we don't support
3973                 # attaching a multiattach volume to shelved offloaded instances
3974                 # because we can't tell if the compute host (since there isn't
3975                 # one) supports it. This could possibly be supported in the
3976                 # future if the scheduler was made aware of which computes
3977                 # support multiattach volumes.
3978                 raise exception.MultiattachToShelvedNotSupported()
3979             return self._attach_volume_shelved_offloaded(context,
3980                                                          instance,
3981                                                          volume,
3982                                                          device,
3983                                                          disk_bus,
3984                                                          device_type)
3985 
3986         return self._attach_volume(context, instance, volume, device,
3987                                    disk_bus, device_type, tag=tag,
3988                                    supports_multiattach=supports_multiattach)
3989 
3990     def _detach_volume(self, context, instance, volume):
3991         """Detach volume from instance.
3992 
3993         This method is separated to make it easier for cells version
3994         to override.
3995         """
3996         try:
3997             self.volume_api.begin_detaching(context, volume['id'])
3998         except exception.InvalidInput as exc:
3999             raise exception.InvalidVolume(reason=exc.format_message())
4000         attachments = volume.get('attachments', {})
4001         attachment_id = None
4002         if attachments and instance.uuid in attachments:
4003             attachment_id = attachments[instance.uuid]['attachment_id']
4004         self._record_action_start(
4005             context, instance, instance_actions.DETACH_VOLUME)
4006         self.compute_rpcapi.detach_volume(context, instance=instance,
4007                 volume_id=volume['id'], attachment_id=attachment_id)
4008 
4009     def _detach_volume_shelved_offloaded(self, context, instance, volume):
4010         """Detach a volume from an instance in shelved offloaded state.
4011 
4012         If the instance is shelved offloaded we just need to cleanup volume
4013         calling the volume api detach, the volume api terminate_connection
4014         and delete the bdm record.
4015         If the volume has delete_on_termination option set then we call the
4016         volume api delete as well.
4017         """
4018         @wrap_instance_event(prefix='api')
4019         def detach_volume(self, context, instance, bdms):
4020             self._local_cleanup_bdm_volumes(bdms, instance, context)
4021 
4022         try:
4023             self.volume_api.begin_detaching(context, volume['id'])
4024         except exception.InvalidInput as exc:
4025             raise exception.InvalidVolume(reason=exc.format_message())
4026         bdms = [objects.BlockDeviceMapping.get_by_volume_id(
4027                 context, volume['id'], instance.uuid)]
4028         self._record_action_start(
4029             context, instance,
4030             instance_actions.DETACH_VOLUME)
4031         detach_volume(self, context, instance, bdms)
4032 
4033     @check_instance_lock
4034     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
4035                                     vm_states.STOPPED, vm_states.RESIZED,
4036                                     vm_states.SOFT_DELETED, vm_states.SHELVED,
4037                                     vm_states.SHELVED_OFFLOADED])
4038     def detach_volume(self, context, instance, volume):
4039         """Detach a volume from an instance."""
4040         if instance.vm_state == vm_states.SHELVED_OFFLOADED:
4041             self._detach_volume_shelved_offloaded(context, instance, volume)
4042         else:
4043             self._detach_volume(context, instance, volume)
4044 
4045     @check_instance_lock
4046     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
4047                                     vm_states.RESIZED])
4048     def swap_volume(self, context, instance, old_volume, new_volume):
4049         """Swap volume attached to an instance."""
4050         # The caller likely got the instance from volume['attachments']
4051         # in the first place, but let's sanity check.
4052         if not old_volume.get('attachments', {}).get(instance.uuid):
4053             msg = _("Old volume is attached to a different instance.")
4054             raise exception.InvalidVolume(reason=msg)
4055         if new_volume['attach_status'] == 'attached':
4056             msg = _("New volume must be detached in order to swap.")
4057             raise exception.InvalidVolume(reason=msg)
4058         if int(new_volume['size']) < int(old_volume['size']):
4059             msg = _("New volume must be the same size or larger.")
4060             raise exception.InvalidVolume(reason=msg)
4061         self.volume_api.check_availability_zone(context, new_volume,
4062                                                 instance=instance)
4063         try:
4064             self.volume_api.begin_detaching(context, old_volume['id'])
4065         except exception.InvalidInput as exc:
4066             raise exception.InvalidVolume(reason=exc.format_message())
4067 
4068         # Get the BDM for the attached (old) volume so we can tell if it was
4069         # attached with the new-style Cinder 3.44 API.
4070         bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
4071             context, old_volume['id'], instance.uuid)
4072         new_attachment_id = None
4073         if bdm.attachment_id is None:
4074             # This is an old-style attachment so reserve the new volume before
4075             # we cast to the compute host.
4076             self.volume_api.reserve_volume(context, new_volume['id'])
4077         else:
4078             try:
4079                 self._check_volume_already_attached_to_instance(
4080                     context, instance, new_volume['id'])
4081             except exception.InvalidVolume:
4082                 with excutils.save_and_reraise_exception():
4083                     self.volume_api.roll_detaching(context, old_volume['id'])
4084 
4085             # This is a new-style attachment so for the volume that we are
4086             # going to swap to, create a new volume attachment.
4087             new_attachment_id = self.volume_api.attachment_create(
4088                 context, new_volume['id'], instance.uuid)['id']
4089 
4090         self._record_action_start(
4091             context, instance, instance_actions.SWAP_VOLUME)
4092 
4093         try:
4094             self.compute_rpcapi.swap_volume(
4095                     context, instance=instance,
4096                     old_volume_id=old_volume['id'],
4097                     new_volume_id=new_volume['id'],
4098                     new_attachment_id=new_attachment_id)
4099         except Exception:
4100             with excutils.save_and_reraise_exception():
4101                 self.volume_api.roll_detaching(context, old_volume['id'])
4102                 if new_attachment_id is None:
4103                     self.volume_api.unreserve_volume(context, new_volume['id'])
4104                 else:
4105                     self.volume_api.attachment_delete(
4106                         context, new_attachment_id)
4107 
4108     @check_instance_lock
4109     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
4110                                     vm_states.STOPPED],
4111                           task_state=[None])
4112     def attach_interface(self, context, instance, network_id, port_id,
4113                          requested_ip, tag=None):
4114         """Use hotplug to add an network adapter to an instance."""
4115         self._record_action_start(
4116             context, instance, instance_actions.ATTACH_INTERFACE)
4117         return self.compute_rpcapi.attach_interface(context,
4118             instance=instance, network_id=network_id, port_id=port_id,
4119             requested_ip=requested_ip, tag=tag)
4120 
4121     @check_instance_lock
4122     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
4123                                     vm_states.STOPPED],
4124                           task_state=[None])
4125     def detach_interface(self, context, instance, port_id):
4126         """Detach an network adapter from an instance."""
4127         self._record_action_start(
4128             context, instance, instance_actions.DETACH_INTERFACE)
4129         self.compute_rpcapi.detach_interface(context, instance=instance,
4130             port_id=port_id)
4131 
4132     def get_instance_metadata(self, context, instance):
4133         """Get all metadata associated with an instance."""
4134         return self.db.instance_metadata_get(context, instance.uuid)
4135 
4136     @check_instance_lock
4137     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
4138                                     vm_states.SUSPENDED, vm_states.STOPPED],
4139                           task_state=None)
4140     def delete_instance_metadata(self, context, instance, key):
4141         """Delete the given metadata item from an instance."""
4142         instance.delete_metadata_key(key)
4143         self.compute_rpcapi.change_instance_metadata(context,
4144                                                      instance=instance,
4145                                                      diff={key: ['-']})
4146 
4147     @check_instance_lock
4148     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
4149                                     vm_states.SUSPENDED, vm_states.STOPPED],
4150                           task_state=None)
4151     def update_instance_metadata(self, context, instance,
4152                                  metadata, delete=False):
4153         """Updates or creates instance metadata.
4154 
4155         If delete is True, metadata items that are not specified in the
4156         `metadata` argument will be deleted.
4157 
4158         """
4159         orig = dict(instance.metadata)
4160         if delete:
4161             _metadata = metadata
4162         else:
4163             _metadata = dict(instance.metadata)
4164             _metadata.update(metadata)
4165 
4166         self._check_metadata_properties_quota(context, _metadata)
4167         instance.metadata = _metadata
4168         instance.save()
4169         diff = _diff_dict(orig, instance.metadata)
4170         self.compute_rpcapi.change_instance_metadata(context,
4171                                                      instance=instance,
4172                                                      diff=diff)
4173         return _metadata
4174 
4175     @check_instance_lock
4176     @check_instance_cell
4177     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED])
4178     def live_migrate(self, context, instance, block_migration,
4179                      disk_over_commit, host_name, force=None, async=False):
4180         """Migrate a server lively to a new host."""
4181         LOG.debug("Going to try to live migrate instance to %s",
4182                   host_name or "another host", instance=instance)
4183 
4184         instance.task_state = task_states.MIGRATING
4185         instance.save(expected_task_state=[None])
4186 
4187         self._record_action_start(context, instance,
4188                                   instance_actions.LIVE_MIGRATION)
4189 
4190         self.consoleauth_rpcapi.delete_tokens_for_instance(
4191             context, instance.uuid)
4192 
4193         try:
4194             request_spec = objects.RequestSpec.get_by_instance_uuid(
4195                 context, instance.uuid)
4196         except exception.RequestSpecNotFound:
4197             # Some old instances can still have no RequestSpec object attached
4198             # to them, we need to support the old way
4199             request_spec = None
4200 
4201         # NOTE(sbauza): Force is a boolean by the new related API version
4202         if force is False and host_name:
4203             nodes = objects.ComputeNodeList.get_all_by_host(context, host_name)
4204             # Unset the host to make sure we call the scheduler
4205             # from the conductor LiveMigrationTask. Yes this is tightly-coupled
4206             # to behavior in conductor and not great.
4207             host_name = None
4208             # FIXME(sbauza): Since only Ironic driver uses more than one
4209             # compute per service but doesn't support live migrations,
4210             # let's provide the first one.
4211             target = nodes[0]
4212             if request_spec:
4213                 # TODO(sbauza): Hydrate a fake spec for old instances not yet
4214                 # having a request spec attached to them (particularly true for
4215                 # cells v1). For the moment, let's keep the same behaviour for
4216                 # all the instances but provide the destination only if a spec
4217                 # is found.
4218                 destination = objects.Destination(
4219                     host=target.host,
4220                     node=target.hypervisor_hostname
4221                 )
4222                 # This is essentially a hint to the scheduler to only consider
4223                 # the specified host but still run it through the filters.
4224                 request_spec.requested_destination = destination
4225 
4226         try:
4227             self.compute_task_api.live_migrate_instance(context, instance,
4228                 host_name, block_migration=block_migration,
4229                 disk_over_commit=disk_over_commit,
4230                 request_spec=request_spec, async=async)
4231         except oslo_exceptions.MessagingTimeout as messaging_timeout:
4232             with excutils.save_and_reraise_exception():
4233                 # NOTE(pkoniszewski): It is possible that MessagingTimeout
4234                 # occurs, but LM will still be in progress, so write
4235                 # instance fault to database
4236                 compute_utils.add_instance_fault_from_exc(context,
4237                                                           instance,
4238                                                           messaging_timeout)
4239 
4240     @check_instance_lock
4241     @check_instance_cell
4242     @check_instance_state(vm_state=[vm_states.ACTIVE],
4243                           task_state=[task_states.MIGRATING])
4244     def live_migrate_force_complete(self, context, instance, migration_id):
4245         """Force live migration to complete.
4246 
4247         :param context: Security context
4248         :param instance: The instance that is being migrated
4249         :param migration_id: ID of ongoing migration
4250 
4251         """
4252         LOG.debug("Going to try to force live migration to complete",
4253                   instance=instance)
4254 
4255         # NOTE(pkoniszewski): Get migration object to check if there is ongoing
4256         # live migration for particular instance. Also pass migration id to
4257         # compute to double check and avoid possible race condition.
4258         migration = objects.Migration.get_by_id_and_instance(
4259             context, migration_id, instance.uuid)
4260         if migration.status != 'running':
4261             raise exception.InvalidMigrationState(migration_id=migration_id,
4262                                                   instance_uuid=instance.uuid,
4263                                                   state=migration.status,
4264                                                   method='force complete')
4265 
4266         self._record_action_start(
4267             context, instance, instance_actions.LIVE_MIGRATION_FORCE_COMPLETE)
4268 
4269         self.compute_rpcapi.live_migration_force_complete(
4270             context, instance, migration)
4271 
4272     @check_instance_lock
4273     @check_instance_cell
4274     @check_instance_state(task_state=[task_states.MIGRATING])
4275     def live_migrate_abort(self, context, instance, migration_id):
4276         """Abort an in-progress live migration.
4277 
4278         :param context: Security context
4279         :param instance: The instance that is being migrated
4280         :param migration_id: ID of in-progress live migration
4281 
4282         """
4283         migration = objects.Migration.get_by_id_and_instance(context,
4284                     migration_id, instance.uuid)
4285         LOG.debug("Going to cancel live migration %s",
4286                   migration.id, instance=instance)
4287 
4288         if migration.status != 'running':
4289             raise exception.InvalidMigrationState(migration_id=migration_id,
4290                     instance_uuid=instance.uuid,
4291                     state=migration.status,
4292                     method='abort live migration')
4293         self._record_action_start(context, instance,
4294                                   instance_actions.LIVE_MIGRATION_CANCEL)
4295 
4296         self.compute_rpcapi.live_migration_abort(context,
4297                 instance, migration.id)
4298 
4299     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
4300                                     vm_states.ERROR])
4301     def evacuate(self, context, instance, host, on_shared_storage,
4302                  admin_password=None, force=None):
4303         """Running evacuate to target host.
4304 
4305         Checking vm compute host state, if the host not in expected_state,
4306         raising an exception.
4307 
4308         :param instance: The instance to evacuate
4309         :param host: Target host. if not set, the scheduler will pick up one
4310         :param on_shared_storage: True if instance files on shared storage
4311         :param admin_password: password to set on rebuilt instance
4312         :param force: Force the evacuation to the specific host target
4313 
4314         """
4315         LOG.debug('vm evacuation scheduled', instance=instance)
4316         inst_host = instance.host
4317         service = objects.Service.get_by_compute_host(context, inst_host)
4318         if self.servicegroup_api.service_is_up(service):
4319             LOG.error('Instance compute service state on %s '
4320                       'expected to be down, but it was up.', inst_host)
4321             raise exception.ComputeServiceInUse(host=inst_host)
4322 
4323         instance.task_state = task_states.REBUILDING
4324         instance.save(expected_task_state=[None])
4325         self._record_action_start(context, instance, instance_actions.EVACUATE)
4326 
4327         # NOTE(danms): Create this as a tombstone for the source compute
4328         # to find and cleanup. No need to pass it anywhere else.
4329         migration = objects.Migration(context,
4330                                       source_compute=instance.host,
4331                                       source_node=instance.node,
4332                                       instance_uuid=instance.uuid,
4333                                       status='accepted',
4334                                       migration_type='evacuation')
4335         if host:
4336             migration.dest_compute = host
4337         migration.create()
4338 
4339         compute_utils.notify_about_instance_usage(
4340             self.notifier, context, instance, "evacuate")
4341         compute_utils.notify_about_instance_action(
4342             context, instance, CONF.host,
4343             action=fields_obj.NotificationAction.EVACUATE,
4344             source=fields_obj.NotificationSource.API)
4345 
4346         try:
4347             request_spec = objects.RequestSpec.get_by_instance_uuid(
4348                 context, instance.uuid)
4349         except exception.RequestSpecNotFound:
4350             # Some old instances can still have no RequestSpec object attached
4351             # to them, we need to support the old way
4352             request_spec = None
4353 
4354         # NOTE(sbauza): Force is a boolean by the new related API version
4355         if force is False and host:
4356             nodes = objects.ComputeNodeList.get_all_by_host(context, host)
4357             # NOTE(sbauza): Unset the host to make sure we call the scheduler
4358             host = None
4359             # FIXME(sbauza): Since only Ironic driver uses more than one
4360             # compute per service but doesn't support evacuations,
4361             # let's provide the first one.
4362             target = nodes[0]
4363             if request_spec:
4364                 # TODO(sbauza): Hydrate a fake spec for old instances not yet
4365                 # having a request spec attached to them (particularly true for
4366                 # cells v1). For the moment, let's keep the same behaviour for
4367                 # all the instances but provide the destination only if a spec
4368                 # is found.
4369                 destination = objects.Destination(
4370                     host=target.host,
4371                     node=target.hypervisor_hostname
4372                 )
4373                 request_spec.requested_destination = destination
4374 
4375         return self.compute_task_api.rebuild_instance(context,
4376                        instance=instance,
4377                        new_pass=admin_password,
4378                        injected_files=None,
4379                        image_ref=None,
4380                        orig_image_ref=None,
4381                        orig_sys_metadata=None,
4382                        bdms=None,
4383                        recreate=True,
4384                        on_shared_storage=on_shared_storage,
4385                        host=host,
4386                        request_spec=request_spec,
4387                        )
4388 
4389     def get_migrations(self, context, filters):
4390         """Get all migrations for the given filters."""
4391         load_cells()
4392 
4393         migrations = []
4394         for cell in CELLS:
4395             if cell.uuid == objects.CellMapping.CELL0_UUID:
4396                 continue
4397             with nova_context.target_cell(context, cell) as cctxt:
4398                 migrations.extend(objects.MigrationList.get_by_filters(
4399                     cctxt, filters).objects)
4400         return objects.MigrationList(objects=migrations)
4401 
4402     def get_migrations_sorted(self, context, filters, sort_dirs=None,
4403                               sort_keys=None, limit=None, marker=None):
4404         """Get all migrations for the given parameters."""
4405         mig_objs = migration_list.get_migration_objects_sorted(
4406             context, filters, limit, marker, sort_keys, sort_dirs)
4407         return mig_objs
4408 
4409     def get_migrations_in_progress_by_instance(self, context, instance_uuid,
4410                                                migration_type=None):
4411         """Get all migrations of an instance in progress."""
4412         return objects.MigrationList.get_in_progress_by_instance(
4413                 context, instance_uuid, migration_type)
4414 
4415     def get_migration_by_id_and_instance(self, context,
4416                                          migration_id, instance_uuid):
4417         """Get the migration of an instance by id."""
4418         return objects.Migration.get_by_id_and_instance(
4419                 context, migration_id, instance_uuid)
4420 
4421     def _get_bdm_by_volume_id(self, context, volume_id, expected_attrs=None):
4422         """Retrieve a BDM without knowing its cell.
4423 
4424         .. note:: The context will be targeted to the cell in which the
4425             BDM is found, if any.
4426 
4427         :param context: The API request context.
4428         :param volume_id: The ID of the volume.
4429         :param expected_attrs: list of any additional attributes that should
4430             be joined when the BDM is loaded from the database.
4431         :raises: nova.exception.VolumeBDMNotFound if not found in any cell
4432         """
4433         load_cells()
4434         for cell in CELLS:
4435             nova_context.set_target_cell(context, cell)
4436             try:
4437                 return objects.BlockDeviceMapping.get_by_volume(
4438                     context, volume_id, expected_attrs=expected_attrs)
4439             except exception.NotFound:
4440                 continue
4441         raise exception.VolumeBDMNotFound(volume_id=volume_id)
4442 
4443     def volume_snapshot_create(self, context, volume_id, create_info):
4444         bdm = self._get_bdm_by_volume_id(
4445             context, volume_id, expected_attrs=['instance'])
4446 
4447         # We allow creating the snapshot in any vm_state as long as there is
4448         # no task being performed on the instance and it has a host.
4449         @check_instance_host
4450         @check_instance_state(vm_state=None)
4451         def do_volume_snapshot_create(self, context, instance):
4452             self.compute_rpcapi.volume_snapshot_create(context, instance,
4453                     volume_id, create_info)
4454             snapshot = {
4455                 'snapshot': {
4456                     'id': create_info.get('id'),
4457                     'volumeId': volume_id
4458                 }
4459             }
4460             return snapshot
4461 
4462         return do_volume_snapshot_create(self, context, bdm.instance)
4463 
4464     def volume_snapshot_delete(self, context, volume_id, snapshot_id,
4465                                delete_info):
4466         bdm = self._get_bdm_by_volume_id(
4467             context, volume_id, expected_attrs=['instance'])
4468 
4469         # We allow deleting the snapshot in any vm_state as long as there is
4470         # no task being performed on the instance and it has a host.
4471         @check_instance_host
4472         @check_instance_state(vm_state=None)
4473         def do_volume_snapshot_delete(self, context, instance):
4474             self.compute_rpcapi.volume_snapshot_delete(context, instance,
4475                     volume_id, snapshot_id, delete_info)
4476 
4477         do_volume_snapshot_delete(self, context, bdm.instance)
4478 
4479     def external_instance_event(self, api_context, instances, events):
4480         # NOTE(danms): The external API consumer just provides events,
4481         # but doesn't know where they go. We need to collate lists
4482         # by the host the affected instance is on and dispatch them
4483         # according to host
4484         instances_by_host = collections.defaultdict(list)
4485         events_by_host = collections.defaultdict(list)
4486         hosts_by_instance = collections.defaultdict(list)
4487         cell_contexts_by_host = {}
4488         for instance in instances:
4489             # instance._context is used here since it's already targeted to
4490             # the cell that the instance lives in, and we need to use that
4491             # cell context to lookup any migrations associated to the instance.
4492             for host in self._get_relevant_hosts(instance._context, instance):
4493                 # NOTE(danms): All instances on a host must have the same
4494                 # mapping, so just use that
4495                 # NOTE(mdbooth): We don't currently support migrations between
4496                 # cells, and given that the Migration record is hosted in the
4497                 # cell _get_relevant_hosts will likely have to change before we
4498                 # do. Consequently we can currently assume that the context for
4499                 # both the source and destination hosts of a migration is the
4500                 # same.
4501                 if host not in cell_contexts_by_host:
4502                     cell_contexts_by_host[host] = instance._context
4503 
4504                 instances_by_host[host].append(instance)
4505                 hosts_by_instance[instance.uuid].append(host)
4506 
4507         for event in events:
4508             if event.name == 'volume-extended':
4509                 # Volume extend is a user-initiated operation starting in the
4510                 # Block Storage service API. We record an instance action so
4511                 # the user can monitor the operation to completion.
4512                 host = hosts_by_instance[event.instance_uuid][0]
4513                 cell_context = cell_contexts_by_host[host]
4514                 objects.InstanceAction.action_start(
4515                     cell_context, event.instance_uuid,
4516                     instance_actions.EXTEND_VOLUME, want_result=False)
4517             for host in hosts_by_instance[event.instance_uuid]:
4518                 events_by_host[host].append(event)
4519 
4520         for host in instances_by_host:
4521             cell_context = cell_contexts_by_host[host]
4522 
4523             # TODO(salv-orlando): Handle exceptions raised by the rpc api layer
4524             # in order to ensure that a failure in processing events on a host
4525             # will not prevent processing events on other hosts
4526             self.compute_rpcapi.external_instance_event(
4527                 cell_context, instances_by_host[host], events_by_host[host],
4528                 host=host)
4529 
4530     def _get_relevant_hosts(self, context, instance):
4531         hosts = set()
4532         hosts.add(instance.host)
4533         if instance.migration_context is not None:
4534             migration_id = instance.migration_context.migration_id
4535             migration = objects.Migration.get_by_id(context, migration_id)
4536             hosts.add(migration.dest_compute)
4537             hosts.add(migration.source_compute)
4538             LOG.debug('Instance %(instance)s is migrating, '
4539                       'copying events to all relevant hosts: '
4540                       '%(hosts)s', {'instance': instance.uuid,
4541                                     'hosts': hosts})
4542         return hosts
4543 
4544     def get_instance_host_status(self, instance):
4545         if instance.host:
4546             try:
4547                 service = [service for service in instance.services if
4548                            service.binary == 'nova-compute'][0]
4549                 if service.forced_down:
4550                     host_status = fields_obj.HostStatus.DOWN
4551                 elif service.disabled:
4552                     host_status = fields_obj.HostStatus.MAINTENANCE
4553                 else:
4554                     alive = self.servicegroup_api.service_is_up(service)
4555                     host_status = ((alive and fields_obj.HostStatus.UP) or
4556                                    fields_obj.HostStatus.UNKNOWN)
4557             except IndexError:
4558                 host_status = fields_obj.HostStatus.NONE
4559         else:
4560             host_status = fields_obj.HostStatus.NONE
4561         return host_status
4562 
4563     def get_instances_host_statuses(self, instance_list):
4564         host_status_dict = dict()
4565         host_statuses = dict()
4566         for instance in instance_list:
4567             if instance.host:
4568                 if instance.host not in host_status_dict:
4569                     host_status = self.get_instance_host_status(instance)
4570                     host_status_dict[instance.host] = host_status
4571                 else:
4572                     host_status = host_status_dict[instance.host]
4573             else:
4574                 host_status = fields_obj.HostStatus.NONE
4575             host_statuses[instance.uuid] = host_status
4576         return host_statuses
4577 
4578 
4579 def target_host_cell(fn):
4580     """Target a host-based function to a cell.
4581 
4582     Expects to wrap a function of signature:
4583 
4584        func(self, context, host, ...)
4585     """
4586 
4587     @functools.wraps(fn)
4588     def targeted(self, context, host, *args, **kwargs):
4589         mapping = objects.HostMapping.get_by_host(context, host)
4590         nova_context.set_target_cell(context, mapping.cell_mapping)
4591         return fn(self, context, host, *args, **kwargs)
4592     return targeted
4593 
4594 
4595 def _find_service_in_cell(context, service_id=None, service_host=None):
4596     """Find a service by id or hostname by searching all cells.
4597 
4598     If one matching service is found, return it. If none or multiple
4599     are found, raise an exception.
4600 
4601     :param context: A context.RequestContext
4602     :param service_id: If not none, the DB ID of the service to find
4603     :param service_host: If not None, the hostname of the service to find
4604     :returns: An objects.Service
4605     :raises: ServiceNotUnique if multiple matching IDs are found
4606     :raises: NotFound if no matches are found
4607     :raises: NovaException if called with neither search option
4608     """
4609 
4610     load_cells()
4611     service = None
4612     found_in_cell = None
4613 
4614     is_uuid = False
4615     if service_id is not None:
4616         is_uuid = uuidutils.is_uuid_like(service_id)
4617         if is_uuid:
4618             lookup_fn = lambda c: objects.Service.get_by_uuid(c, service_id)
4619         else:
4620             lookup_fn = lambda c: objects.Service.get_by_id(c, service_id)
4621     elif service_host is not None:
4622         lookup_fn = lambda c: (
4623             objects.Service.get_by_compute_host(c, service_host))
4624     else:
4625         LOG.exception('_find_service_in_cell called with no search parameters')
4626         # This is intentionally cryptic so we don't leak implementation details
4627         # out of the API.
4628         raise exception.NovaException()
4629 
4630     for cell in CELLS:
4631         # NOTE(danms): Services can be in cell0, so don't skip it here
4632         try:
4633             with nova_context.target_cell(context, cell) as cctxt:
4634                 cell_service = lookup_fn(cctxt)
4635         except exception.NotFound:
4636             # NOTE(danms): Keep looking in other cells
4637             continue
4638         if service and cell_service:
4639             raise exception.ServiceNotUnique()
4640         service = cell_service
4641         found_in_cell = cell
4642         if service and is_uuid:
4643             break
4644 
4645     if service:
4646         # NOTE(danms): Set the cell on the context so it remains
4647         # when we return to our caller
4648         nova_context.set_target_cell(context, found_in_cell)
4649         return service
4650     else:
4651         raise exception.NotFound()
4652 
4653 
4654 class HostAPI(base.Base):
4655     """Sub-set of the Compute Manager API for managing host operations."""
4656 
4657     def __init__(self, rpcapi=None):
4658         self.rpcapi = rpcapi or compute_rpcapi.ComputeAPI()
4659         self.servicegroup_api = servicegroup.API()
4660         super(HostAPI, self).__init__()
4661 
4662     def _assert_host_exists(self, context, host_name, must_be_up=False):
4663         """Raise HostNotFound if compute host doesn't exist."""
4664         service = objects.Service.get_by_compute_host(context, host_name)
4665         if not service:
4666             raise exception.HostNotFound(host=host_name)
4667         if must_be_up and not self.servicegroup_api.service_is_up(service):
4668             raise exception.ComputeServiceUnavailable(host=host_name)
4669         return service['host']
4670 
4671     @wrap_exception()
4672     @target_host_cell
4673     def set_host_enabled(self, context, host_name, enabled):
4674         """Sets the specified host's ability to accept new instances."""
4675         host_name = self._assert_host_exists(context, host_name)
4676         payload = {'host_name': host_name, 'enabled': enabled}
4677         compute_utils.notify_about_host_update(context,
4678                                                'set_enabled.start',
4679                                                payload)
4680         result = self.rpcapi.set_host_enabled(context, enabled=enabled,
4681                 host=host_name)
4682         compute_utils.notify_about_host_update(context,
4683                                                'set_enabled.end',
4684                                                payload)
4685         return result
4686 
4687     @target_host_cell
4688     def get_host_uptime(self, context, host_name):
4689         """Returns the result of calling "uptime" on the target host."""
4690         host_name = self._assert_host_exists(context, host_name,
4691                          must_be_up=True)
4692         return self.rpcapi.get_host_uptime(context, host=host_name)
4693 
4694     @wrap_exception()
4695     @target_host_cell
4696     def host_power_action(self, context, host_name, action):
4697         """Reboots, shuts down or powers up the host."""
4698         host_name = self._assert_host_exists(context, host_name)
4699         payload = {'host_name': host_name, 'action': action}
4700         compute_utils.notify_about_host_update(context,
4701                                                'power_action.start',
4702                                                payload)
4703         result = self.rpcapi.host_power_action(context, action=action,
4704                 host=host_name)
4705         compute_utils.notify_about_host_update(context,
4706                                                'power_action.end',
4707                                                payload)
4708         return result
4709 
4710     @wrap_exception()
4711     @target_host_cell
4712     def set_host_maintenance(self, context, host_name, mode):
4713         """Start/Stop host maintenance window. On start, it triggers
4714         guest VMs evacuation.
4715         """
4716         host_name = self._assert_host_exists(context, host_name)
4717         payload = {'host_name': host_name, 'mode': mode}
4718         compute_utils.notify_about_host_update(context,
4719                                                'set_maintenance.start',
4720                                                payload)
4721         result = self.rpcapi.host_maintenance_mode(context,
4722                 host_param=host_name, mode=mode, host=host_name)
4723         compute_utils.notify_about_host_update(context,
4724                                                'set_maintenance.end',
4725                                                payload)
4726         return result
4727 
4728     def service_get_all(self, context, filters=None, set_zones=False,
4729                         all_cells=False):
4730         """Returns a list of services, optionally filtering the results.
4731 
4732         If specified, 'filters' should be a dictionary containing services
4733         attributes and matching values.  Ie, to get a list of services for
4734         the 'compute' topic, use filters={'topic': 'compute'}.
4735 
4736         If all_cells=True, then scan all cells and merge the results.
4737         """
4738         if filters is None:
4739             filters = {}
4740         disabled = filters.pop('disabled', None)
4741         if 'availability_zone' in filters:
4742             set_zones = True
4743 
4744         # NOTE(danms): Eventually this all_cells nonsense should go away
4745         # and we should always iterate over the cells. However, certain
4746         # callers need the legacy behavior for now.
4747         if all_cells:
4748             load_cells()
4749             services = []
4750             for cell in CELLS:
4751                 with nova_context.target_cell(context, cell) as cctxt:
4752                     cell_services = objects.ServiceList.get_all(
4753                         cctxt, disabled, set_zones=set_zones)
4754                 services.extend(cell_services)
4755         else:
4756             services = objects.ServiceList.get_all(context, disabled,
4757                                                    set_zones=set_zones)
4758         ret_services = []
4759         for service in services:
4760             for key, val in filters.items():
4761                 if service[key] != val:
4762                     break
4763             else:
4764                 # All filters matched.
4765                 ret_services.append(service)
4766         return ret_services
4767 
4768     def service_get_by_id(self, context, service_id):
4769         """Get service entry for the given service id or uuid."""
4770         try:
4771             return _find_service_in_cell(context, service_id=service_id)
4772         except exception.NotFound:
4773             raise exception.ServiceNotFound(service_id=service_id)
4774 
4775     @target_host_cell
4776     def service_get_by_compute_host(self, context, host_name):
4777         """Get service entry for the given compute hostname."""
4778         return objects.Service.get_by_compute_host(context, host_name)
4779 
4780     def _service_update(self, context, host_name, binary, params_to_update):
4781         """Performs the actual service update operation."""
4782         service = objects.Service.get_by_args(context, host_name, binary)
4783         service.update(params_to_update)
4784         service.save()
4785         return service
4786 
4787     @target_host_cell
4788     def service_update(self, context, host_name, binary, params_to_update):
4789         """Enable / Disable a service.
4790 
4791         For compute services, this stops new builds and migrations going to
4792         the host.
4793         """
4794         return self._service_update(context, host_name, binary,
4795                                     params_to_update)
4796 
4797     def _service_delete(self, context, service_id):
4798         """Performs the actual Service deletion operation."""
4799         try:
4800             service = _find_service_in_cell(context, service_id=service_id)
4801         except exception.NotFound:
4802             raise exception.ServiceNotFound(service_id=service_id)
4803         service.destroy()
4804 
4805     def service_delete(self, context, service_id):
4806         """Deletes the specified service found via id or uuid."""
4807         self._service_delete(context, service_id)
4808 
4809     @target_host_cell
4810     def instance_get_all_by_host(self, context, host_name):
4811         """Return all instances on the given host."""
4812         return objects.InstanceList.get_by_host(context, host_name)
4813 
4814     def task_log_get_all(self, context, task_name, period_beginning,
4815                          period_ending, host=None, state=None):
4816         """Return the task logs within a given range, optionally
4817         filtering by host and/or state.
4818         """
4819         return self.db.task_log_get_all(context, task_name,
4820                                         period_beginning,
4821                                         period_ending,
4822                                         host=host,
4823                                         state=state)
4824 
4825     def compute_node_get(self, context, compute_id):
4826         """Return compute node entry for particular integer ID or UUID."""
4827         load_cells()
4828 
4829         # NOTE(danms): Unfortunately this API exposes database identifiers
4830         # which means we really can't do something efficient here
4831         is_uuid = uuidutils.is_uuid_like(compute_id)
4832         for cell in CELLS:
4833             if cell.uuid == objects.CellMapping.CELL0_UUID:
4834                 continue
4835             with nova_context.target_cell(context, cell) as cctxt:
4836                 try:
4837                     if is_uuid:
4838                         return objects.ComputeNode.get_by_uuid(cctxt,
4839                                                                compute_id)
4840                     return objects.ComputeNode.get_by_id(cctxt,
4841                                                          int(compute_id))
4842                 except exception.ComputeHostNotFound:
4843                     # NOTE(danms): Keep looking in other cells
4844                     continue
4845 
4846         raise exception.ComputeHostNotFound(host=compute_id)
4847 
4848     def compute_node_get_all(self, context, limit=None, marker=None):
4849         load_cells()
4850 
4851         computes = []
4852         uuid_marker = marker and uuidutils.is_uuid_like(marker)
4853         for cell in CELLS:
4854             if cell.uuid == objects.CellMapping.CELL0_UUID:
4855                 continue
4856             with nova_context.target_cell(context, cell) as cctxt:
4857 
4858                 # If we have a marker and it's a uuid, see if the compute node
4859                 # is in this cell.
4860                 if marker and uuid_marker:
4861                     try:
4862                         compute_marker = objects.ComputeNode.get_by_uuid(
4863                             cctxt, marker)
4864                         # we found the marker compute node, so use it's id
4865                         # for the actual marker for paging in this cell's db
4866                         marker = compute_marker.id
4867                     except exception.ComputeHostNotFound:
4868                         # The marker node isn't in this cell so keep looking.
4869                         continue
4870 
4871                 try:
4872                     cell_computes = objects.ComputeNodeList.get_by_pagination(
4873                         cctxt, limit=limit, marker=marker)
4874                 except exception.MarkerNotFound:
4875                     # NOTE(danms): Keep looking through cells
4876                     continue
4877                 computes.extend(cell_computes)
4878                 # NOTE(danms): We must have found the marker, so continue on
4879                 # without one
4880                 marker = None
4881                 if limit:
4882                     limit -= len(cell_computes)
4883                     if limit <= 0:
4884                         break
4885 
4886         if marker is not None and len(computes) == 0:
4887             # NOTE(danms): If we did not find the marker in any cell,
4888             # mimic the db_api behavior here.
4889             raise exception.MarkerNotFound(marker=marker)
4890 
4891         return objects.ComputeNodeList(objects=computes)
4892 
4893     def compute_node_search_by_hypervisor(self, context, hypervisor_match):
4894         load_cells()
4895 
4896         computes = []
4897         for cell in CELLS:
4898             if cell.uuid == objects.CellMapping.CELL0_UUID:
4899                 continue
4900             with nova_context.target_cell(context, cell) as cctxt:
4901                 cell_computes = objects.ComputeNodeList.get_by_hypervisor(
4902                     cctxt, hypervisor_match)
4903             computes.extend(cell_computes)
4904         return objects.ComputeNodeList(objects=computes)
4905 
4906     def compute_node_statistics(self, context):
4907         load_cells()
4908 
4909         cell_stats = []
4910         for cell in CELLS:
4911             if cell.uuid == objects.CellMapping.CELL0_UUID:
4912                 continue
4913             with nova_context.target_cell(context, cell) as cctxt:
4914                 cell_stats.append(self.db.compute_node_statistics(cctxt))
4915 
4916         if cell_stats:
4917             keys = cell_stats[0].keys()
4918             return {k: sum(stats[k] for stats in cell_stats)
4919                     for k in keys}
4920         else:
4921             return {}
4922 
4923 
4924 class InstanceActionAPI(base.Base):
4925     """Sub-set of the Compute Manager API for managing instance actions."""
4926 
4927     def actions_get(self, context, instance, limit=None, marker=None,
4928                     filters=None):
4929         return objects.InstanceActionList.get_by_instance_uuid(
4930             context, instance.uuid, limit, marker, filters)
4931 
4932     def action_get_by_request_id(self, context, instance, request_id):
4933         return objects.InstanceAction.get_by_request_id(
4934             context, instance.uuid, request_id)
4935 
4936     def action_events_get(self, context, instance, action_id):
4937         return objects.InstanceActionEventList.get_by_action(
4938             context, action_id)
4939 
4940 
4941 class AggregateAPI(base.Base):
4942     """Sub-set of the Compute Manager API for managing host aggregates."""
4943     def __init__(self, **kwargs):
4944         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
4945         self.scheduler_client = scheduler_client.SchedulerClient()
4946         super(AggregateAPI, self).__init__(**kwargs)
4947 
4948     @wrap_exception()
4949     def create_aggregate(self, context, aggregate_name, availability_zone):
4950         """Creates the model for the aggregate."""
4951 
4952         aggregate = objects.Aggregate(context=context)
4953         aggregate.name = aggregate_name
4954         if availability_zone:
4955             aggregate.metadata = {'availability_zone': availability_zone}
4956         aggregate.create()
4957         self.scheduler_client.update_aggregates(context, [aggregate])
4958         return aggregate
4959 
4960     def get_aggregate(self, context, aggregate_id):
4961         """Get an aggregate by id."""
4962         return objects.Aggregate.get_by_id(context, aggregate_id)
4963 
4964     def get_aggregate_list(self, context):
4965         """Get all the aggregates."""
4966         return objects.AggregateList.get_all(context)
4967 
4968     def get_aggregates_by_host(self, context, compute_host):
4969         """Get all the aggregates where the given host is presented."""
4970         return objects.AggregateList.get_by_host(context, compute_host)
4971 
4972     @wrap_exception()
4973     def update_aggregate(self, context, aggregate_id, values):
4974         """Update the properties of an aggregate."""
4975         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
4976         if 'name' in values:
4977             aggregate.name = values.pop('name')
4978             aggregate.save()
4979         self.is_safe_to_update_az(context, values, aggregate=aggregate,
4980                                   action_name=AGGREGATE_ACTION_UPDATE)
4981         if values:
4982             aggregate.update_metadata(values)
4983             aggregate.updated_at = timeutils.utcnow()
4984         self.scheduler_client.update_aggregates(context, [aggregate])
4985         # If updated values include availability_zones, then the cache
4986         # which stored availability_zones and host need to be reset
4987         if values.get('availability_zone'):
4988             availability_zones.reset_cache()
4989         return aggregate
4990 
4991     @wrap_exception()
4992     def update_aggregate_metadata(self, context, aggregate_id, metadata):
4993         """Updates the aggregate metadata."""
4994         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
4995         self.is_safe_to_update_az(context, metadata, aggregate=aggregate,
4996                                   action_name=AGGREGATE_ACTION_UPDATE_META)
4997         aggregate.update_metadata(metadata)
4998         self.scheduler_client.update_aggregates(context, [aggregate])
4999         # If updated metadata include availability_zones, then the cache
5000         # which stored availability_zones and host need to be reset
5001         if metadata and metadata.get('availability_zone'):
5002             availability_zones.reset_cache()
5003         aggregate.updated_at = timeutils.utcnow()
5004         return aggregate
5005 
5006     @wrap_exception()
5007     def delete_aggregate(self, context, aggregate_id):
5008         """Deletes the aggregate."""
5009         aggregate_payload = {'aggregate_id': aggregate_id}
5010         compute_utils.notify_about_aggregate_update(context,
5011                                                     "delete.start",
5012                                                     aggregate_payload)
5013         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
5014 
5015         compute_utils.notify_about_aggregate_action(
5016             context=context,
5017             aggregate=aggregate,
5018             action=fields_obj.NotificationAction.DELETE,
5019             phase=fields_obj.NotificationPhase.START)
5020 
5021         if len(aggregate.hosts) > 0:
5022             msg = _("Host aggregate is not empty")
5023             raise exception.InvalidAggregateActionDelete(
5024                 aggregate_id=aggregate_id, reason=msg)
5025         aggregate.destroy()
5026         self.scheduler_client.delete_aggregate(context, aggregate)
5027         compute_utils.notify_about_aggregate_update(context,
5028                                                     "delete.end",
5029                                                     aggregate_payload)
5030         compute_utils.notify_about_aggregate_action(
5031             context=context,
5032             aggregate=aggregate,
5033             action=fields_obj.NotificationAction.DELETE,
5034             phase=fields_obj.NotificationPhase.END)
5035 
5036     def is_safe_to_update_az(self, context, metadata, aggregate,
5037                              hosts=None,
5038                              action_name=AGGREGATE_ACTION_ADD):
5039         """Determine if updates alter an aggregate's availability zone.
5040 
5041             :param context: local context
5042             :param metadata: Target metadata for updating aggregate
5043             :param aggregate: Aggregate to update
5044             :param hosts: Hosts to check. If None, aggregate.hosts is used
5045             :type hosts: list
5046             :action_name: Calling method for logging purposes
5047 
5048         """
5049         if 'availability_zone' in metadata:
5050             if not metadata['availability_zone']:
5051                 msg = _("Aggregate %s does not support empty named "
5052                         "availability zone") % aggregate.name
5053                 self._raise_invalid_aggregate_exc(action_name, aggregate.id,
5054                                                   msg)
5055             _hosts = hosts or aggregate.hosts
5056             host_aggregates = objects.AggregateList.get_by_metadata_key(
5057                 context, 'availability_zone', hosts=_hosts)
5058             conflicting_azs = [
5059                 agg.availability_zone for agg in host_aggregates
5060                 if agg.availability_zone != metadata['availability_zone']
5061                 and agg.id != aggregate.id]
5062             if conflicting_azs:
5063                 msg = _("One or more hosts already in availability zone(s) "
5064                         "%s") % conflicting_azs
5065                 self._raise_invalid_aggregate_exc(action_name, aggregate.id,
5066                                                   msg)
5067 
5068     def _raise_invalid_aggregate_exc(self, action_name, aggregate_id, reason):
5069         if action_name == AGGREGATE_ACTION_ADD:
5070             raise exception.InvalidAggregateActionAdd(
5071                 aggregate_id=aggregate_id, reason=reason)
5072         elif action_name == AGGREGATE_ACTION_UPDATE:
5073             raise exception.InvalidAggregateActionUpdate(
5074                 aggregate_id=aggregate_id, reason=reason)
5075         elif action_name == AGGREGATE_ACTION_UPDATE_META:
5076             raise exception.InvalidAggregateActionUpdateMeta(
5077                 aggregate_id=aggregate_id, reason=reason)
5078         elif action_name == AGGREGATE_ACTION_DELETE:
5079             raise exception.InvalidAggregateActionDelete(
5080                 aggregate_id=aggregate_id, reason=reason)
5081 
5082         raise exception.NovaException(
5083             _("Unexpected aggregate action %s") % action_name)
5084 
5085     def _update_az_cache_for_host(self, context, host_name, aggregate_meta):
5086         # Update the availability_zone cache to avoid getting wrong
5087         # availability_zone in cache retention time when add/remove
5088         # host to/from aggregate.
5089         if aggregate_meta and aggregate_meta.get('availability_zone'):
5090             availability_zones.update_host_availability_zone_cache(context,
5091                                                                    host_name)
5092 
5093     @wrap_exception()
5094     def add_host_to_aggregate(self, context, aggregate_id, host_name):
5095         """Adds the host to an aggregate."""
5096         aggregate_payload = {'aggregate_id': aggregate_id,
5097                              'host_name': host_name}
5098         compute_utils.notify_about_aggregate_update(context,
5099                                                     "addhost.start",
5100                                                     aggregate_payload)
5101         # validates the host; HostMappingNotFound or ComputeHostNotFound
5102         # is raised if invalid
5103         try:
5104             mapping = objects.HostMapping.get_by_host(context, host_name)
5105             nova_context.set_target_cell(context, mapping.cell_mapping)
5106             objects.Service.get_by_compute_host(context, host_name)
5107         except exception.HostMappingNotFound:
5108             try:
5109                 # NOTE(danms): This targets our cell
5110                 _find_service_in_cell(context, service_host=host_name)
5111             except exception.NotFound:
5112                 raise exception.ComputeHostNotFound(host=host_name)
5113 
5114         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
5115 
5116         compute_utils.notify_about_aggregate_action(
5117             context=context,
5118             aggregate=aggregate,
5119             action=fields_obj.NotificationAction.ADD_HOST,
5120             phase=fields_obj.NotificationPhase.START)
5121 
5122         self.is_safe_to_update_az(context, aggregate.metadata,
5123                                   hosts=[host_name], aggregate=aggregate)
5124 
5125         aggregate.add_host(host_name)
5126         self.scheduler_client.update_aggregates(context, [aggregate])
5127         self._update_az_cache_for_host(context, host_name, aggregate.metadata)
5128         # NOTE(jogo): Send message to host to support resource pools
5129         self.compute_rpcapi.add_aggregate_host(context,
5130                 aggregate=aggregate, host_param=host_name, host=host_name)
5131         aggregate_payload.update({'name': aggregate.name})
5132         compute_utils.notify_about_aggregate_update(context,
5133                                                     "addhost.end",
5134                                                     aggregate_payload)
5135         compute_utils.notify_about_aggregate_action(
5136             context=context,
5137             aggregate=aggregate,
5138             action=fields_obj.NotificationAction.ADD_HOST,
5139             phase=fields_obj.NotificationPhase.END)
5140 
5141         return aggregate
5142 
5143     @wrap_exception()
5144     def remove_host_from_aggregate(self, context, aggregate_id, host_name):
5145         """Removes host from the aggregate."""
5146         aggregate_payload = {'aggregate_id': aggregate_id,
5147                              'host_name': host_name}
5148         compute_utils.notify_about_aggregate_update(context,
5149                                                     "removehost.start",
5150                                                     aggregate_payload)
5151         # validates the host; HostMappingNotFound or ComputeHostNotFound
5152         # is raised if invalid
5153         mapping = objects.HostMapping.get_by_host(context, host_name)
5154         nova_context.set_target_cell(context, mapping.cell_mapping)
5155         objects.Service.get_by_compute_host(context, host_name)
5156         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
5157 
5158         compute_utils.notify_about_aggregate_action(
5159             context=context,
5160             aggregate=aggregate,
5161             action=fields_obj.NotificationAction.REMOVE_HOST,
5162             phase=fields_obj.NotificationPhase.START)
5163 
5164         aggregate.delete_host(host_name)
5165         self.scheduler_client.update_aggregates(context, [aggregate])
5166         self._update_az_cache_for_host(context, host_name, aggregate.metadata)
5167         self.compute_rpcapi.remove_aggregate_host(context,
5168                 aggregate=aggregate, host_param=host_name, host=host_name)
5169         compute_utils.notify_about_aggregate_update(context,
5170                                                     "removehost.end",
5171                                                     aggregate_payload)
5172         compute_utils.notify_about_aggregate_action(
5173             context=context,
5174             aggregate=aggregate,
5175             action=fields_obj.NotificationAction.REMOVE_HOST,
5176             phase=fields_obj.NotificationPhase.END)
5177         return aggregate
5178 
5179 
5180 class KeypairAPI(base.Base):
5181     """Subset of the Compute Manager API for managing key pairs."""
5182 
5183     get_notifier = functools.partial(rpc.get_notifier, service='api')
5184     wrap_exception = functools.partial(exception_wrapper.wrap_exception,
5185                                        get_notifier=get_notifier,
5186                                        binary='nova-api')
5187 
5188     def _notify(self, context, event_suffix, keypair_name):
5189         payload = {
5190             'tenant_id': context.project_id,
5191             'user_id': context.user_id,
5192             'key_name': keypair_name,
5193         }
5194         notify = self.get_notifier()
5195         notify.info(context, 'keypair.%s' % event_suffix, payload)
5196 
5197     def _validate_new_key_pair(self, context, user_id, key_name, key_type):
5198         safe_chars = "_- " + string.digits + string.ascii_letters
5199         clean_value = "".join(x for x in key_name if x in safe_chars)
5200         if clean_value != key_name:
5201             raise exception.InvalidKeypair(
5202                 reason=_("Keypair name contains unsafe characters"))
5203 
5204         try:
5205             utils.check_string_length(key_name, min_length=1, max_length=255)
5206         except exception.InvalidInput:
5207             raise exception.InvalidKeypair(
5208                 reason=_('Keypair name must be string and between '
5209                          '1 and 255 characters long'))
5210         try:
5211             objects.Quotas.check_deltas(context, {'key_pairs': 1}, user_id)
5212         except exception.OverQuota:
5213             raise exception.KeypairLimitExceeded()
5214 
5215     @wrap_exception()
5216     def import_key_pair(self, context, user_id, key_name, public_key,
5217                         key_type=keypair_obj.KEYPAIR_TYPE_SSH):
5218         """Import a key pair using an existing public key."""
5219         self._validate_new_key_pair(context, user_id, key_name, key_type)
5220 
5221         self._notify(context, 'import.start', key_name)
5222 
5223         keypair = objects.KeyPair(context)
5224         keypair.user_id = user_id
5225         keypair.name = key_name
5226         keypair.type = key_type
5227         keypair.fingerprint = None
5228         keypair.public_key = public_key
5229 
5230         compute_utils.notify_about_keypair_action(
5231             context=context,
5232             keypair=keypair,
5233             action=fields_obj.NotificationAction.IMPORT,
5234             phase=fields_obj.NotificationPhase.START)
5235 
5236         fingerprint = self._generate_fingerprint(public_key, key_type)
5237 
5238         keypair.fingerprint = fingerprint
5239         keypair.create()
5240 
5241         compute_utils.notify_about_keypair_action(
5242             context=context,
5243             keypair=keypair,
5244             action=fields_obj.NotificationAction.IMPORT,
5245             phase=fields_obj.NotificationPhase.END)
5246         self._notify(context, 'import.end', key_name)
5247 
5248         return keypair
5249 
5250     @wrap_exception()
5251     def create_key_pair(self, context, user_id, key_name,
5252                         key_type=keypair_obj.KEYPAIR_TYPE_SSH):
5253         """Create a new key pair."""
5254         self._validate_new_key_pair(context, user_id, key_name, key_type)
5255 
5256         keypair = objects.KeyPair(context)
5257         keypair.user_id = user_id
5258         keypair.name = key_name
5259         keypair.type = key_type
5260         keypair.fingerprint = None
5261         keypair.public_key = None
5262 
5263         self._notify(context, 'create.start', key_name)
5264         compute_utils.notify_about_keypair_action(
5265             context=context,
5266             keypair=keypair,
5267             action=fields_obj.NotificationAction.CREATE,
5268             phase=fields_obj.NotificationPhase.START)
5269 
5270         private_key, public_key, fingerprint = self._generate_key_pair(
5271             user_id, key_type)
5272 
5273         keypair.fingerprint = fingerprint
5274         keypair.public_key = public_key
5275         keypair.create()
5276 
5277         # NOTE(melwitt): We recheck the quota after creating the object to
5278         # prevent users from allocating more resources than their allowed quota
5279         # in the event of a race. This is configurable because it can be
5280         # expensive if strict quota limits are not required in a deployment.
5281         if CONF.quota.recheck_quota:
5282             try:
5283                 objects.Quotas.check_deltas(context, {'key_pairs': 0}, user_id)
5284             except exception.OverQuota:
5285                 keypair.destroy()
5286                 raise exception.KeypairLimitExceeded()
5287 
5288         compute_utils.notify_about_keypair_action(
5289             context=context,
5290             keypair=keypair,
5291             action=fields_obj.NotificationAction.CREATE,
5292             phase=fields_obj.NotificationPhase.END)
5293 
5294         self._notify(context, 'create.end', key_name)
5295 
5296         return keypair, private_key
5297 
5298     def _generate_fingerprint(self, public_key, key_type):
5299         if key_type == keypair_obj.KEYPAIR_TYPE_SSH:
5300             return crypto.generate_fingerprint(public_key)
5301         elif key_type == keypair_obj.KEYPAIR_TYPE_X509:
5302             return crypto.generate_x509_fingerprint(public_key)
5303 
5304     def _generate_key_pair(self, user_id, key_type):
5305         if key_type == keypair_obj.KEYPAIR_TYPE_SSH:
5306             return crypto.generate_key_pair()
5307         elif key_type == keypair_obj.KEYPAIR_TYPE_X509:
5308             return crypto.generate_winrm_x509_cert(user_id)
5309 
5310     @wrap_exception()
5311     def delete_key_pair(self, context, user_id, key_name):
5312         """Delete a keypair by name."""
5313         self._notify(context, 'delete.start', key_name)
5314         keypair = self.get_key_pair(context, user_id, key_name)
5315         compute_utils.notify_about_keypair_action(
5316             context=context,
5317             keypair=keypair,
5318             action=fields_obj.NotificationAction.DELETE,
5319             phase=fields_obj.NotificationPhase.START)
5320         objects.KeyPair.destroy_by_name(context, user_id, key_name)
5321         compute_utils.notify_about_keypair_action(
5322             context=context,
5323             keypair=keypair,
5324             action=fields_obj.NotificationAction.DELETE,
5325             phase=fields_obj.NotificationPhase.END)
5326         self._notify(context, 'delete.end', key_name)
5327 
5328     def get_key_pairs(self, context, user_id, limit=None, marker=None):
5329         """List key pairs."""
5330         return objects.KeyPairList.get_by_user(
5331             context, user_id, limit=limit, marker=marker)
5332 
5333     def get_key_pair(self, context, user_id, key_name):
5334         """Get a keypair by name."""
5335         return objects.KeyPair.get_by_name(context, user_id, key_name)
5336 
5337 
5338 class SecurityGroupAPI(base.Base, security_group_base.SecurityGroupBase):
5339     """Sub-set of the Compute API related to managing security groups
5340     and security group rules
5341     """
5342 
5343     # The nova security group api does not use a uuid for the id.
5344     id_is_uuid = False
5345 
5346     def __init__(self, **kwargs):
5347         super(SecurityGroupAPI, self).__init__(**kwargs)
5348         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
5349 
5350     def validate_property(self, value, property, allowed):
5351         """Validate given security group property.
5352 
5353         :param value:          the value to validate, as a string or unicode
5354         :param property:       the property, either 'name' or 'description'
5355         :param allowed:        the range of characters allowed
5356         """
5357 
5358         try:
5359             val = value.strip()
5360         except AttributeError:
5361             msg = _("Security group %s is not a string or unicode") % property
5362             self.raise_invalid_property(msg)
5363         utils.check_string_length(val, name=property, min_length=1,
5364                                   max_length=255)
5365 
5366         if allowed and not re.match(allowed, val):
5367             # Some validation to ensure that values match API spec.
5368             # - Alphanumeric characters, spaces, dashes, and underscores.
5369             # TODO(Daviey): LP: #813685 extend beyond group_name checking, and
5370             #  probably create a param validator that can be used elsewhere.
5371             msg = (_("Value (%(value)s) for parameter Group%(property)s is "
5372                      "invalid. Content limited to '%(allowed)s'.") %
5373                    {'value': value, 'allowed': allowed,
5374                     'property': property.capitalize()})
5375             self.raise_invalid_property(msg)
5376 
5377     def ensure_default(self, context):
5378         """Ensure that a context has a security group.
5379 
5380         Creates a security group for the security context if it does not
5381         already exist.
5382 
5383         :param context: the security context
5384         """
5385         self.db.security_group_ensure_default(context)
5386 
5387     def create_security_group(self, context, name, description):
5388         try:
5389             objects.Quotas.check_deltas(context, {'security_groups': 1},
5390                                         context.project_id,
5391                                         user_id=context.user_id)
5392         except exception.OverQuota:
5393             msg = _("Quota exceeded, too many security groups.")
5394             self.raise_over_quota(msg)
5395 
5396         LOG.info("Create Security Group %s", name)
5397 
5398         self.ensure_default(context)
5399 
5400         group = {'user_id': context.user_id,
5401                  'project_id': context.project_id,
5402                  'name': name,
5403                  'description': description}
5404         try:
5405             group_ref = self.db.security_group_create(context, group)
5406         except exception.SecurityGroupExists:
5407             msg = _('Security group %s already exists') % name
5408             self.raise_group_already_exists(msg)
5409 
5410         # NOTE(melwitt): We recheck the quota after creating the object to
5411         # prevent users from allocating more resources than their allowed quota
5412         # in the event of a race. This is configurable because it can be
5413         # expensive if strict quota limits are not required in a deployment.
5414         if CONF.quota.recheck_quota:
5415             try:
5416                 objects.Quotas.check_deltas(context, {'security_groups': 0},
5417                                             context.project_id,
5418                                             user_id=context.user_id)
5419             except exception.OverQuota:
5420                 self.db.security_group_destroy(context, group_ref['id'])
5421                 msg = _("Quota exceeded, too many security groups.")
5422                 self.raise_over_quota(msg)
5423 
5424         return group_ref
5425 
5426     def update_security_group(self, context, security_group,
5427                                 name, description):
5428         if security_group['name'] in RO_SECURITY_GROUPS:
5429             msg = (_("Unable to update system group '%s'") %
5430                     security_group['name'])
5431             self.raise_invalid_group(msg)
5432 
5433         group = {'name': name,
5434                  'description': description}
5435 
5436         columns_to_join = ['rules.grantee_group']
5437         group_ref = self.db.security_group_update(context,
5438                 security_group['id'],
5439                 group,
5440                 columns_to_join=columns_to_join)
5441         return group_ref
5442 
5443     def get(self, context, name=None, id=None, map_exception=False):
5444         self.ensure_default(context)
5445         cols = ['rules']
5446         try:
5447             if name:
5448                 return self.db.security_group_get_by_name(context,
5449                                                           context.project_id,
5450                                                           name,
5451                                                           columns_to_join=cols)
5452             elif id:
5453                 return self.db.security_group_get(context, id,
5454                                                   columns_to_join=cols)
5455         except exception.NotFound as exp:
5456             if map_exception:
5457                 msg = exp.format_message()
5458                 self.raise_not_found(msg)
5459             else:
5460                 raise
5461 
5462     def list(self, context, names=None, ids=None, project=None,
5463              search_opts=None):
5464         self.ensure_default(context)
5465 
5466         groups = []
5467         if names or ids:
5468             if names:
5469                 for name in names:
5470                     groups.append(self.db.security_group_get_by_name(context,
5471                                                                      project,
5472                                                                      name))
5473             if ids:
5474                 for id in ids:
5475                     groups.append(self.db.security_group_get(context, id))
5476 
5477         elif context.is_admin:
5478             # TODO(eglynn): support a wider set of search options than just
5479             # all_tenants, at least include the standard filters defined for
5480             # the EC2 DescribeSecurityGroups API for the non-admin case also
5481             if (search_opts and 'all_tenants' in search_opts):
5482                 groups = self.db.security_group_get_all(context)
5483             else:
5484                 groups = self.db.security_group_get_by_project(context,
5485                                                                project)
5486 
5487         elif project:
5488             groups = self.db.security_group_get_by_project(context, project)
5489 
5490         return groups
5491 
5492     def destroy(self, context, security_group):
5493         if security_group['name'] in RO_SECURITY_GROUPS:
5494             msg = _("Unable to delete system group '%s'") % \
5495                     security_group['name']
5496             self.raise_invalid_group(msg)
5497 
5498         if self.db.security_group_in_use(context, security_group['id']):
5499             msg = _("Security group is still in use")
5500             self.raise_invalid_group(msg)
5501 
5502         LOG.info("Delete security group %s", security_group['name'])
5503         self.db.security_group_destroy(context, security_group['id'])
5504 
5505     def is_associated_with_server(self, security_group, instance_uuid):
5506         """Check if the security group is already associated
5507            with the instance. If Yes, return True.
5508         """
5509 
5510         if not security_group:
5511             return False
5512 
5513         instances = security_group.get('instances')
5514         if not instances:
5515             return False
5516 
5517         for inst in instances:
5518             if (instance_uuid == inst['uuid']):
5519                 return True
5520 
5521         return False
5522 
5523     def add_to_instance(self, context, instance, security_group_name):
5524         """Add security group to the instance."""
5525         security_group = self.db.security_group_get_by_name(context,
5526                 context.project_id,
5527                 security_group_name)
5528 
5529         instance_uuid = instance.uuid
5530 
5531         # check if the security group is associated with the server
5532         if self.is_associated_with_server(security_group, instance_uuid):
5533             raise exception.SecurityGroupExistsForInstance(
5534                                         security_group_id=security_group['id'],
5535                                         instance_id=instance_uuid)
5536 
5537         self.db.instance_add_security_group(context.elevated(),
5538                                             instance_uuid,
5539                                             security_group['id'])
5540         if instance.host:
5541             self.compute_rpcapi.refresh_instance_security_rules(
5542                     context, instance, instance.host)
5543 
5544     def remove_from_instance(self, context, instance, security_group_name):
5545         """Remove the security group associated with the instance."""
5546         security_group = self.db.security_group_get_by_name(context,
5547                 context.project_id,
5548                 security_group_name)
5549 
5550         instance_uuid = instance.uuid
5551 
5552         # check if the security group is associated with the server
5553         if not self.is_associated_with_server(security_group, instance_uuid):
5554             raise exception.SecurityGroupNotExistsForInstance(
5555                                     security_group_id=security_group['id'],
5556                                     instance_id=instance_uuid)
5557 
5558         self.db.instance_remove_security_group(context.elevated(),
5559                                                instance_uuid,
5560                                                security_group['id'])
5561         if instance.host:
5562             self.compute_rpcapi.refresh_instance_security_rules(
5563                     context, instance, instance.host)
5564 
5565     def get_rule(self, context, id):
5566         self.ensure_default(context)
5567         try:
5568             return self.db.security_group_rule_get(context, id)
5569         except exception.NotFound:
5570             msg = _("Rule (%s) not found") % id
5571             self.raise_not_found(msg)
5572 
5573     def add_rules(self, context, id, name, vals):
5574         """Add security group rule(s) to security group.
5575 
5576         Note: the Nova security group API doesn't support adding multiple
5577         security group rules at once but the EC2 one does. Therefore,
5578         this function is written to support both.
5579         """
5580 
5581         try:
5582             objects.Quotas.check_deltas(context,
5583                                         {'security_group_rules': len(vals)},
5584                                         id)
5585         except exception.OverQuota:
5586             msg = _("Quota exceeded, too many security group rules.")
5587             self.raise_over_quota(msg)
5588 
5589         msg = ("Security group %(name)s added %(protocol)s ingress "
5590                "(%(from_port)s:%(to_port)s)")
5591         rules = []
5592         for v in vals:
5593             rule = self.db.security_group_rule_create(context, v)
5594 
5595             # NOTE(melwitt): We recheck the quota after creating the object to
5596             # prevent users from allocating more resources than their allowed
5597             # quota in the event of a race. This is configurable because it can
5598             # be expensive if strict quota limits are not required in a
5599             # deployment.
5600             if CONF.quota.recheck_quota:
5601                 try:
5602                     objects.Quotas.check_deltas(context,
5603                                                 {'security_group_rules': 0},
5604                                                 id)
5605                 except exception.OverQuota:
5606                     self.db.security_group_rule_destroy(context, rule['id'])
5607                     msg = _("Quota exceeded, too many security group rules.")
5608                     self.raise_over_quota(msg)
5609 
5610             rules.append(rule)
5611             LOG.info(msg, {'name': name,
5612                            'protocol': rule.protocol,
5613                            'from_port': rule.from_port,
5614                            'to_port': rule.to_port})
5615 
5616         self.trigger_rules_refresh(context, id=id)
5617         return rules
5618 
5619     def remove_rules(self, context, security_group, rule_ids):
5620         msg = ("Security group %(name)s removed %(protocol)s ingress "
5621                "(%(from_port)s:%(to_port)s)")
5622         for rule_id in rule_ids:
5623             rule = self.get_rule(context, rule_id)
5624             LOG.info(msg, {'name': security_group['name'],
5625                            'protocol': rule.protocol,
5626                            'from_port': rule.from_port,
5627                            'to_port': rule.to_port})
5628 
5629             self.db.security_group_rule_destroy(context, rule_id)
5630 
5631         # NOTE(vish): we removed some rules, so refresh
5632         self.trigger_rules_refresh(context, id=security_group['id'])
5633 
5634     def remove_default_rules(self, context, rule_ids):
5635         for rule_id in rule_ids:
5636             self.db.security_group_default_rule_destroy(context, rule_id)
5637 
5638     def add_default_rules(self, context, vals):
5639         rules = [self.db.security_group_default_rule_create(context, v)
5640                  for v in vals]
5641         return rules
5642 
5643     def default_rule_exists(self, context, values):
5644         """Indicates whether the specified rule values are already
5645            defined in the default security group rules.
5646         """
5647         for rule in self.db.security_group_default_rule_list(context):
5648             keys = ('cidr', 'from_port', 'to_port', 'protocol')
5649             for key in keys:
5650                 if rule.get(key) != values.get(key):
5651                     break
5652             else:
5653                 return rule.get('id') or True
5654         return False
5655 
5656     def get_all_default_rules(self, context):
5657         try:
5658             rules = self.db.security_group_default_rule_list(context)
5659         except Exception:
5660             msg = 'cannot get default security group rules'
5661             raise exception.SecurityGroupDefaultRuleNotFound(msg)
5662 
5663         return rules
5664 
5665     def get_default_rule(self, context, id):
5666         return self.db.security_group_default_rule_get(context, id)
5667 
5668     def validate_id(self, id):
5669         try:
5670             return int(id)
5671         except ValueError:
5672             msg = _("Security group id should be integer")
5673             self.raise_invalid_property(msg)
5674 
5675     def _refresh_instance_security_rules(self, context, instances):
5676         for instance in instances:
5677             if instance.host is not None:
5678                 self.compute_rpcapi.refresh_instance_security_rules(
5679                         context, instance, instance.host)
5680 
5681     def trigger_rules_refresh(self, context, id):
5682         """Called when a rule is added to or removed from a security_group."""
5683         instances = objects.InstanceList.get_by_security_group_id(context, id)
5684         self._refresh_instance_security_rules(context, instances)
5685 
5686     def trigger_members_refresh(self, context, group_ids):
5687         """Called when a security group gains a new or loses a member.
5688 
5689         Sends an update request to each compute node for each instance for
5690         which this is relevant.
5691         """
5692         instances = objects.InstanceList.get_by_grantee_security_group_ids(
5693             context, group_ids)
5694         self._refresh_instance_security_rules(context, instances)
5695 
5696     def get_instance_security_groups(self, context, instance, detailed=False):
5697         if detailed:
5698             return self.db.security_group_get_by_instance(context,
5699                                                           instance.uuid)
5700         return [{'name': group.name} for group in instance.security_groups]
