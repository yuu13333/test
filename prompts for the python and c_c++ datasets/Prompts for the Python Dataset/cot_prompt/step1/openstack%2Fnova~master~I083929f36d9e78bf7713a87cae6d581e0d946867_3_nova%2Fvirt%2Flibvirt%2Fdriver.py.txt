Based on the given code from a commit, please generate supplementary code files according to the commit message.
####commit message
Reattach mdevs to guest on resume

When suspending a VM in OpenStack, Nova detaches all the mediated devices from the guest machine, but does not reattach them on the resume operation. This patch makes Nova reattach the mdevs that were detached when the guest was suspended.

This behavior is due to libvirt not supporting the hot-unplug of mediated devices at the time the feature was being developed. The limitation has been lifted since then, and now we have to amend the resume function so it will reattach the mediated devices that were detached on suspension.
####code

1 # Copyright 2010 United States Government as represented by the
2 # Administrator of the National Aeronautics and Space Administration.
3 # All Rights Reserved.
4 # Copyright (c) 2010 Citrix Systems, Inc.
5 # Copyright (c) 2011 Piston Cloud Computing, Inc
6 # Copyright (c) 2012 University Of Minho
7 # (c) Copyright 2013 Hewlett-Packard Development Company, L.P.
8 #
9 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
10 #    not use this file except in compliance with the License. You may obtain
11 #    a copy of the License at
12 #
13 #         http://www.apache.org/licenses/LICENSE-2.0
14 #
15 #    Unless required by applicable law or agreed to in writing, software
16 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
17 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
18 #    License for the specific language governing permissions and limitations
19 #    under the License.
20 
21 """
22 A connection to a hypervisor through libvirt.
23 
24 Supports KVM, LXC, QEMU, and Parallels.
25 """
26 
27 import binascii
28 import collections
29 from collections import deque
30 import contextlib
31 import copy
32 import errno
33 import functools
34 import glob
35 import grp
36 import itertools
37 import operator
38 import os
39 import pwd
40 import random
41 import shutil
42 import sys
43 import tempfile
44 import threading
45 import time
46 import typing as ty
47 import uuid
48 
49 from castellan import key_manager
50 from copy import deepcopy
51 import eventlet
52 from eventlet import greenthread
53 from eventlet import tpool
54 from lxml import etree
55 from os_brick import encryptors
56 from os_brick.encryptors import luks as luks_encryptor
57 from os_brick import exception as brick_exception
58 from os_brick.initiator import connector
59 import os_resource_classes as orc
60 import os_traits as ot
61 from oslo_concurrency import processutils
62 from oslo_log import log as logging
63 from oslo_serialization import base64
64 from oslo_serialization import jsonutils
65 from oslo_service import loopingcall
66 from oslo_utils import encodeutils
67 from oslo_utils import excutils
68 from oslo_utils import fileutils
69 from oslo_utils import importutils
70 from oslo_utils import netutils as oslo_netutils
71 from oslo_utils import strutils
72 from oslo_utils import timeutils
73 from oslo_utils import units
74 from oslo_utils import uuidutils
75 
76 from nova.api.metadata import base as instance_metadata
77 from nova.api.metadata import password
78 from nova import block_device
79 from nova.compute import power_state
80 from nova.compute import provider_tree
81 from nova.compute import task_states
82 from nova.compute import utils as compute_utils
83 from nova.compute import vm_states
84 import nova.conf
85 from nova.console import serial as serial_console
86 from nova.console import type as ctype
87 from nova import context as nova_context
88 from nova import crypto
89 from nova.db import constants as db_const
90 from nova import exception
91 from nova.i18n import _
92 from nova.image import glance
93 from nova.network import model as network_model
94 from nova.network import neutron
95 from nova import objects
96 from nova.objects import diagnostics as diagnostics_obj
97 from nova.objects import fields
98 from nova.objects import migrate_data as migrate_data_obj
99 from nova.pci import manager as pci_manager
100 from nova.pci import utils as pci_utils
101 import nova.privsep.libvirt
102 import nova.privsep.path
103 import nova.privsep.utils
104 from nova.storage import rbd_utils
105 from nova import utils
106 from nova import version
107 from nova.virt import block_device as driver_block_device
108 from nova.virt import configdrive
109 from nova.virt.disk import api as disk_api
110 from nova.virt.disk.vfs import guestfs
111 from nova.virt import driver
112 from nova.virt import event as virtevent
113 from nova.virt import hardware
114 from nova.virt.image import model as imgmodel
115 from nova.virt import images
116 from nova.virt.libvirt import blockinfo
117 from nova.virt.libvirt import config as vconfig
118 from nova.virt.libvirt import designer
119 from nova.virt.libvirt import event as libvirtevent
120 from nova.virt.libvirt import guest as libvirt_guest
121 from nova.virt.libvirt import host
122 from nova.virt.libvirt import imagebackend
123 from nova.virt.libvirt import imagecache
124 from nova.virt.libvirt import instancejobtracker
125 from nova.virt.libvirt import migration as libvirt_migrate
126 from nova.virt.libvirt.storage import dmcrypt
127 from nova.virt.libvirt.storage import lvm
128 from nova.virt.libvirt import utils as libvirt_utils
129 from nova.virt.libvirt import vif as libvirt_vif
130 from nova.virt.libvirt.volume import fs
131 from nova.virt.libvirt.volume import mount
132 from nova.virt.libvirt.volume import remotefs
133 from nova.virt.libvirt.volume import volume
134 from nova.virt import netutils
135 from nova.volume import cinder
136 
137 libvirt: ty.Any = None
138 
139 uefi_logged = False
140 
141 LOG = logging.getLogger(__name__)
142 
143 CONF = nova.conf.CONF
144 
145 MAX_CONSOLE_BYTES = 100 * units.Ki
146 VALID_DISK_CACHEMODES = [
147     "default", "none", "writethrough", "writeback", "directsync", "unsafe",
148 ]
149 
150 # The libvirt driver will prefix any disable reason codes with this string.
151 DISABLE_PREFIX = 'AUTO: '
152 # Disable reason for the service which was enabled or disabled without reason
153 DISABLE_REASON_UNDEFINED = None
154 
155 # Guest config console string
156 CONSOLE = "console=tty0 console=ttyS0 console=hvc0"
157 
158 GuestNumaConfig = collections.namedtuple(
159     'GuestNumaConfig', ['cpuset', 'cputune', 'numaconfig', 'numatune'])
160 
161 
162 class InjectionInfo(collections.namedtuple(
163         'InjectionInfo', ['network_info', 'files', 'admin_pass'])):
164     __slots__ = ()
165 
166     def __repr__(self):
167         return ('InjectionInfo(network_info=%r, files=%r, '
168                 'admin_pass=<SANITIZED>)') % (self.network_info, self.files)
169 
170 
171 # NOTE(lyarwood): Dict of volume drivers supported by the libvirt driver, keyed
172 # by the connection_info['driver_volume_type'] returned by Cinder for each
173 # volume type it supports
174 # TODO(lyarwood): Add host configurables to allow this list to be changed.
175 # Allowing native iSCSI to be reintroduced etc.
176 VOLUME_DRIVERS = {
177     'iscsi': 'nova.virt.libvirt.volume.iscsi.LibvirtISCSIVolumeDriver',
178     'iser': 'nova.virt.libvirt.volume.iser.LibvirtISERVolumeDriver',
179     'local': 'nova.virt.libvirt.volume.volume.LibvirtVolumeDriver',
180     'fake': 'nova.virt.libvirt.volume.volume.LibvirtFakeVolumeDriver',
181     'rbd': 'nova.virt.libvirt.volume.net.LibvirtNetVolumeDriver',
182     'nfs': 'nova.virt.libvirt.volume.nfs.LibvirtNFSVolumeDriver',
183     'smbfs': 'nova.virt.libvirt.volume.smbfs.LibvirtSMBFSVolumeDriver',
184     'fibre_channel': 'nova.virt.libvirt.volume.fibrechannel.LibvirtFibreChannelVolumeDriver',  # noqa:E501
185     'gpfs': 'nova.virt.libvirt.volume.gpfs.LibvirtGPFSVolumeDriver',
186     'quobyte': 'nova.virt.libvirt.volume.quobyte.LibvirtQuobyteVolumeDriver',
187     'scaleio': 'nova.virt.libvirt.volume.scaleio.LibvirtScaleIOVolumeDriver',
188     'vzstorage': 'nova.virt.libvirt.volume.vzstorage.LibvirtVZStorageVolumeDriver',  # noqa:E501
189     'storpool': 'nova.virt.libvirt.volume.storpool.LibvirtStorPoolVolumeDriver',  # noqa:E501
190     'nvmeof': 'nova.virt.libvirt.volume.nvme.LibvirtNVMEVolumeDriver',
191 }
192 
193 
194 def patch_tpool_proxy():
195     """eventlet.tpool.Proxy doesn't work with old-style class in __str__()
196     or __repr__() calls. See bug #962840 for details.
197     We perform a monkey patch to replace those two instance methods.
198     """
199     def str_method(self):
200         return str(self._obj)
201 
202     def repr_method(self):
203         return repr(self._obj)
204 
205     tpool.Proxy.__str__ = str_method
206     tpool.Proxy.__repr__ = repr_method
207 
208 
209 patch_tpool_proxy()
210 
211 # For information about when MIN_{LIBVIRT,QEMU}_VERSION and
212 # NEXT_MIN_{LIBVIRT,QEMU}_VERSION can be changed, consult the following:
213 #
214 # doc/source/reference/libvirt-distro-support-matrix.rst
215 #
216 # DO NOT FORGET to update this document when touching any versions below!
217 MIN_LIBVIRT_VERSION = (6, 0, 0)
218 MIN_QEMU_VERSION = (4, 2, 0)
219 NEXT_MIN_LIBVIRT_VERSION = (7, 0, 0)
220 NEXT_MIN_QEMU_VERSION = (5, 2, 0)
221 
222 MIN_LIBVIRT_AARCH64_CPU_COMPARE = (6, 9, 0)
223 
224 # Virtuozzo driver support
225 MIN_VIRTUOZZO_VERSION = (7, 0, 0)
226 
227 
228 # Names of the types that do not get compressed during migration
229 NO_COMPRESSION_TYPES = ('qcow2',)
230 
231 
232 # number of serial console limit
233 QEMU_MAX_SERIAL_PORTS = 4
234 # Qemu supports 4 serial consoles, we remove 1 because of the PTY one defined
235 ALLOWED_QEMU_SERIAL_PORTS = QEMU_MAX_SERIAL_PORTS - 1
236 
237 VGPU_RESOURCE_SEMAPHORE = 'vgpu_resources'
238 
239 LIBVIRT_PERF_EVENT_PREFIX = 'VIR_PERF_PARAM_'
240 
241 # VDPA interface support
242 MIN_LIBVIRT_VDPA = (6, 9, 0)
243 MIN_QEMU_VDPA = (5, 1, 0)
244 
245 
246 class AsyncDeviceEventsHandler:
247     """A synchornization point between libvirt events an clients waiting for
248     such events.
249 
250     It provides an interface for the clients to wait for one or more libvirt
251     event types. It implements event delivery by expecting the libvirt driver
252     to forward libvirt specific events to notify_waiters()
253 
254     It handles multiple clients for the same instance, device and event
255     type and delivers the event to each clients.
256     """
257 
258     class Waiter:
259         def __init__(
260             self,
261             instance_uuid: str,
262             device_name: str,
263             event_types: ty.Set[ty.Type[libvirtevent.DeviceEvent]]
264         ):
265             self.instance_uuid = instance_uuid
266             self.device_name = device_name
267             self.event_types = event_types
268             self.threading_event = threading.Event()
269             self.result: ty.Optional[libvirtevent.DeviceEvent] = None
270 
271         def matches(self, event: libvirtevent.DeviceEvent) -> bool:
272             """Returns true if the event is one of the expected event types
273             for the given instance and device.
274             """
275             return (
276                 self.instance_uuid == event.uuid and
277                 self.device_name == event.dev and
278                 isinstance(event, tuple(self.event_types)))
279 
280         def __repr__(self) -> str:
281             return (
282                 "AsyncDeviceEventsHandler.Waiter("
283                 f"instance_uuid={self.instance_uuid}, "
284                 f"device_name={self.device_name}, "
285                 f"event_types={self.event_types})")
286 
287     def __init__(self):
288         self._lock = threading.Lock()
289         # Ongoing device operations in libvirt where we wait for the events
290         # about success or failure.
291         self._waiters: ty.Set[AsyncDeviceEventsHandler.Waiter] = set()
292 
293     def create_waiter(
294         self,
295         instance_uuid: str,
296         device_name: str,
297         event_types: ty.Set[ty.Type[libvirtevent.DeviceEvent]]
298     ) -> 'AsyncDeviceEventsHandler.Waiter':
299         """Returns an opaque token the caller can use in wait() to
300         wait for the libvirt event
301 
302         :param instance_uuid: The UUID of the instance.
303         :param device_name: The device name alias used by libvirt for this
304             device.
305         :param event_type: A set of classes derived from DeviceEvent
306             specifying which event types the caller waits for. Specifying more
307             than one event type means waiting for either of the events to be
308             received.
309         :returns: an opaque token to be used with wait_for_event().
310         """
311         waiter = AsyncDeviceEventsHandler.Waiter(
312             instance_uuid, device_name, event_types)
313         with self._lock:
314             self._waiters.add(waiter)
315 
316         return waiter
317 
318     def delete_waiter(self, token: 'AsyncDeviceEventsHandler.Waiter'):
319         """Deletes the waiter
320 
321         :param token: the opaque token returned by create_waiter() to be
322             deleted
323         """
324         with self._lock:
325             self._waiters.remove(token)
326 
327     def wait(
328         self, token: 'AsyncDeviceEventsHandler.Waiter', timeout: float,
329     ) -> ty.Optional[libvirtevent.DeviceEvent]:
330         """Blocks waiting for the libvirt event represented by the opaque token
331 
332         :param token: A token created by calling create_waiter()
333         :param timeout: Maximum number of seconds this call blocks waiting for
334             the event to be received
335         :returns: The received libvirt event, or None in case of timeout
336         """
337         token.threading_event.wait(timeout)
338 
339         with self._lock:
340             self._waiters.remove(token)
341 
342         return token.result
343 
344     def notify_waiters(self, event: libvirtevent.DeviceEvent) -> bool:
345         """Unblocks the client waiting for this event.
346 
347         :param event: the libvirt event that is received
348         :returns: True if there was a client waiting and False otherwise.
349         """
350         dispatched = False
351         with self._lock:
352             for waiter in self._waiters:
353                 if waiter.matches(event):
354                     waiter.result = event
355                     waiter.threading_event.set()
356                     dispatched = True
357 
358         return dispatched
359 
360     def cleanup_waiters(self, instance_uuid: str) -> None:
361         """Deletes all waiters and unblock all clients related to the specific
362         instance.
363 
364         param instance_uuid: The instance UUID for which the cleanup is
365             requested
366         """
367         with self._lock:
368             instance_waiters = set()
369             for waiter in self._waiters:
370                 if waiter.instance_uuid == instance_uuid:
371                     # unblock any waiting thread
372                     waiter.threading_event.set()
373                     instance_waiters.add(waiter)
374 
375             self._waiters -= instance_waiters
376 
377         if instance_waiters:
378             LOG.debug(
379                 'Cleaned up device related libvirt event waiters: %s',
380                 instance_waiters)
381 
382 
383 class LibvirtDriver(driver.ComputeDriver):
384     def __init__(self, virtapi, read_only=False):
385         # NOTE(aspiers) Some of these are dynamic, so putting
386         # capabilities on the instance rather than on the class.
387         # This prevents the risk of one test setting a capability
388         # which bleeds over into other tests.
389 
390         # LVM and RBD require raw images. If we are not configured to
391         # force convert images into raw format, then we _require_ raw
392         # images only.
393         raw_only = ('rbd', 'lvm')
394         requires_raw_image = (CONF.libvirt.images_type in raw_only and
395                               not CONF.force_raw_images)
396         requires_ploop_image = CONF.libvirt.virt_type == 'parallels'
397 
398         self.capabilities = {
399             "has_imagecache": True,
400             "supports_evacuate": True,
401             "supports_migrate_to_same_host": False,
402             "supports_attach_interface": True,
403             "supports_device_tagging": True,
404             "supports_tagged_attach_interface": True,
405             "supports_tagged_attach_volume": True,
406             "supports_extend_volume": True,
407             "supports_multiattach": True,
408             "supports_trusted_certs": True,
409             # Supported image types
410             "supports_image_type_aki": True,
411             "supports_image_type_ari": True,
412             "supports_image_type_ami": True,
413             "supports_image_type_raw": True,
414             "supports_image_type_iso": True,
415             # NOTE(danms): Certain backends do not work with complex image
416             # formats. If we are configured for those backends, then we
417             # should not expose the corresponding support traits.
418             "supports_image_type_qcow2": not requires_raw_image,
419             "supports_image_type_ploop": requires_ploop_image,
420             "supports_pcpus": True,
421             "supports_accelerators": True,
422             "supports_bfv_rescue": True,
423             "supports_vtpm": CONF.libvirt.swtpm_enabled,
424             "supports_socket_pci_numa_affinity": True,
425         }
426         super(LibvirtDriver, self).__init__(virtapi)
427 
428         if not sys.platform.startswith('linux'):
429             raise exception.InternalError(
430                 _('The libvirt driver only works on Linux'))
431 
432         global libvirt
433         if libvirt is None:
434             libvirt = importutils.import_module('libvirt')
435             libvirt_migrate.libvirt = libvirt
436 
437         self._host = host.Host(self._uri(), read_only,
438                                lifecycle_event_handler=self.emit_event,
439                                conn_event_handler=self._handle_conn_event)
440         self._supported_perf_events = []
441 
442         self.vif_driver = libvirt_vif.LibvirtGenericVIFDriver(self._host)
443 
444         # NOTE(lyarwood): Volume drivers are loaded on-demand
445         self.volume_drivers: ty.Dict[str, volume.LibvirtBaseVolumeDriver] = {}
446 
447         self._disk_cachemode = None
448         self.image_cache_manager = imagecache.ImageCacheManager()
449         self.image_backend = imagebackend.Backend(CONF.use_cow_images)
450 
451         self.disk_cachemodes = {}
452 
453         for mode_str in CONF.libvirt.disk_cachemodes:
454             disk_type, sep, cache_mode = mode_str.partition('=')
455             if cache_mode not in VALID_DISK_CACHEMODES:
456                 LOG.warning('Invalid cachemode %(cache_mode)s specified '
457                             'for disk type %(disk_type)s.',
458                             {'cache_mode': cache_mode, 'disk_type': disk_type})
459                 continue
460             self.disk_cachemodes[disk_type] = cache_mode
461 
462         self._volume_api = cinder.API()
463         self._image_api = glance.API()
464         self._network_api = neutron.API()
465 
466         # The default choice for the sysinfo_serial config option is "unique"
467         # which does not have a special function since the value is just the
468         # instance.uuid.
469         sysinfo_serial_funcs = {
470             'none': lambda: None,
471             'hardware': self._get_host_sysinfo_serial_hardware,
472             'os': self._get_host_sysinfo_serial_os,
473             'auto': self._get_host_sysinfo_serial_auto,
474         }
475 
476         self._sysinfo_serial_func = sysinfo_serial_funcs.get(
477             CONF.libvirt.sysinfo_serial, lambda: None)
478 
479         self.job_tracker = instancejobtracker.InstanceJobTracker()
480         self._remotefs = remotefs.RemoteFilesystem()
481 
482         self._live_migration_flags = self._block_migration_flags = 0
483         self.active_migrations = {}
484 
485         # Compute reserved hugepages from conf file at the very
486         # beginning to ensure any syntax error will be reported and
487         # avoid any re-calculation when computing resources.
488         self._reserved_hugepages = hardware.numa_get_reserved_huge_pages()
489 
490         # Copy of the compute service ProviderTree object that is updated
491         # every time update_provider_tree() is called.
492         # NOTE(sbauza): We only want a read-only cache, this attribute is not
493         # intended to be updatable directly
494         self.provider_tree: provider_tree.ProviderTree = None
495 
496         # driver traits will not change during the runtime of the agent
497         # so calcuate them once and save them
498         self._static_traits = None
499 
500         # The CPU models in the configuration are case-insensitive, but the CPU
501         # model in the libvirt is case-sensitive, therefore create a mapping to
502         # map the lower case CPU model name to normal CPU model name.
503         self.cpu_models_mapping = {}
504         self.cpu_model_flag_mapping = {}
505 
506         self._vpmems_by_name, self._vpmems_by_rc = self._discover_vpmems(
507                 vpmem_conf=CONF.libvirt.pmem_namespaces)
508 
509         # We default to not support vGPUs unless the configuration is set.
510         self.pgpu_type_mapping = collections.defaultdict(str)
511         # This dict is for knowing which mdev class is supported by a specific
512         # PCI device like we do (the key being the PCI address and the value
513         # the mdev class)
514         self.mdev_class_mapping: ty.Dict[str, str] = (
515             collections.defaultdict(lambda: orc.VGPU)
516         )
517         # This set is for knowing all the mdev classes the operator provides
518         self.mdev_classes = set([])
519         self.supported_vgpu_types = self._get_supported_vgpu_types()
520 
521         # Handles ongoing device manipultion in libvirt where we wait for the
522         # events about success or failure.
523         self._device_event_handler = AsyncDeviceEventsHandler()
524 
525     def _discover_vpmems(self, vpmem_conf=None):
526         """Discover vpmems on host and configuration.
527 
528         :param vpmem_conf: pmem namespaces configuration from CONF
529         :returns: a dict of vpmem keyed by name, and
530                   a dict of vpmem list keyed by resource class
531         :raises: exception.InvalidConfiguration if Libvirt or QEMU version
532                  does not meet requirement.
533         """
534         if not vpmem_conf:
535             return {}, {}
536 
537         # vpmem keyed by name {name: objects.LibvirtVPMEMDevice,...}
538         vpmems_by_name: ty.Dict[str, 'objects.LibvirtVPMEMDevice'] = {}
539         # vpmem list keyed by resource class
540         # {'RC_0': [objects.LibvirtVPMEMDevice, ...], 'RC_1': [...]}
541         vpmems_by_rc: ty.Dict[str, ty.List['objects.LibvirtVPMEMDevice']] = (
542             collections.defaultdict(list)
543         )
544 
545         vpmems_host = self._get_vpmems_on_host()
546         for ns_conf in vpmem_conf:
547             try:
548                 ns_label, ns_names = ns_conf.split(":", 1)
549             except ValueError:
550                 reason = _("The configuration doesn't follow the format")
551                 raise exception.PMEMNamespaceConfigInvalid(
552                         reason=reason)
553             ns_names = ns_names.split("|")
554             for ns_name in ns_names:
555                 if ns_name not in vpmems_host:
556                     reason = _("The PMEM namespace %s isn't on host") % ns_name
557                     raise exception.PMEMNamespaceConfigInvalid(
558                             reason=reason)
559                 if ns_name in vpmems_by_name:
560                     reason = (_("Duplicated PMEM namespace %s configured") %
561                                 ns_name)
562                     raise exception.PMEMNamespaceConfigInvalid(
563                             reason=reason)
564                 pmem_ns_updated = vpmems_host[ns_name]
565                 pmem_ns_updated.label = ns_label
566                 vpmems_by_name[ns_name] = pmem_ns_updated
567                 rc = orc.normalize_name(
568                         "PMEM_NAMESPACE_%s" % ns_label)
569                 vpmems_by_rc[rc].append(pmem_ns_updated)
570 
571         return vpmems_by_name, vpmems_by_rc
572 
573     def _get_vpmems_on_host(self):
574         """Get PMEM namespaces on host using ndctl utility."""
575         try:
576             output = nova.privsep.libvirt.get_pmem_namespaces()
577         except Exception as e:
578             reason = _("Get PMEM namespaces by ndctl utility, "
579                     "please ensure ndctl is installed: %s") % e
580             raise exception.GetPMEMNamespacesFailed(reason=reason)
581 
582         if not output:
583             return {}
584         namespaces = jsonutils.loads(output)
585         vpmems_host = {}  # keyed by namespace name
586         for ns in namespaces:
587             # store namespace info parsed from ndctl utility return
588             if not ns.get('name'):
589                 # The name is used to identify namespaces, it's optional
590                 # config when creating namespace. If an namespace don't have
591                 # name, it can not be used by Nova, we will skip it.
592                 continue
593             vpmems_host[ns['name']] = objects.LibvirtVPMEMDevice(
594                 name=ns['name'],
595                 devpath= '/dev/' + ns['daxregion']['devices'][0]['chardev'],
596                 size=ns['size'],
597                 align=ns['daxregion']['align'])
598         return vpmems_host
599 
600     @property
601     def disk_cachemode(self):
602         # It can be confusing to understand the QEMU cache mode
603         # behaviour, because each cache=$MODE is a convenient shorthand
604         # to toggle _three_ cache.* booleans.  Consult the below table
605         # (quoting from the QEMU man page):
606         #
607         #              | cache.writeback | cache.direct | cache.no-flush
608         # --------------------------------------------------------------
609         # writeback    | on              | off          | off
610         # none         | on              | on           | off
611         # writethrough | off             | off          | off
612         # directsync   | off             | on           | off
613         # unsafe       | on              | off          | on
614         #
615         # Where:
616         #
617         #  - 'cache.writeback=off' means: QEMU adds an automatic fsync()
618         #    after each write request.
619         #
620         #  - 'cache.direct=on' means: Use Linux's O_DIRECT, i.e. bypass
621         #    the kernel page cache.  Caches in any other layer (disk
622         #    cache, QEMU metadata caches, etc.) can still be present.
623         #
624         #  - 'cache.no-flush=on' means: Ignore flush requests, i.e.
625         #    never call fsync(), even if the guest explicitly requested
626         #    it.
627         #
628         # Use cache mode "none" (cache.writeback=on, cache.direct=on,
629         # cache.no-flush=off) for consistent performance and
630         # migration correctness.  Some filesystems don't support
631         # O_DIRECT, though.  For those we fallback to the next
632         # reasonable option that is "writeback" (cache.writeback=on,
633         # cache.direct=off, cache.no-flush=off).
634 
635         if self._disk_cachemode is None:
636             self._disk_cachemode = "none"
637             if not nova.privsep.utils.supports_direct_io(CONF.instances_path):
638                 self._disk_cachemode = "writeback"
639         return self._disk_cachemode
640 
641     def _set_cache_mode(self, conf):
642         """Set cache mode on LibvirtConfigGuestDisk object."""
643         try:
644             source_type = conf.source_type
645             driver_cache = conf.driver_cache
646         except AttributeError:
647             return
648 
649         # Shareable disks like for a multi-attach volume need to have the
650         # driver cache disabled.
651         if getattr(conf, 'shareable', False):
652             conf.driver_cache = 'none'
653         else:
654             cache_mode = self.disk_cachemodes.get(source_type,
655                                                   driver_cache)
656             conf.driver_cache = cache_mode
657 
658         # NOTE(acewit): If the [libvirt]disk_cachemodes is set as
659         # `block=writeback` or `block=writethrough` or `block=unsafe`,
660         # whose correponding Linux's IO semantic is not O_DIRECT in
661         # file nova.conf, then it will result in an attachment failure
662         # because of the libvirt bug
663         # (https://bugzilla.redhat.com/show_bug.cgi?id=1086704)
664         if ((getattr(conf, 'driver_io', None) == "native") and
665                 conf.driver_cache not in [None, 'none', 'directsync']):
666             conf.driver_io = "threads"
667             LOG.warning("The guest disk driver io mode has fallen back "
668                         "from 'native' to 'threads' because the "
669                         "disk cache mode is set as %(cachemode)s, which does "
670                         "not use O_DIRECT. See the following bug report "
671                         "for more details: https://launchpad.net/bugs/1841363",
672                         {'cachemode': conf.driver_cache})
673 
674     def _do_quality_warnings(self):
675         """Warn about potential configuration issues.
676 
677         This will log a warning message for things such as untested driver or
678         host arch configurations in order to indicate potential issues to
679         administrators.
680         """
681         if CONF.libvirt.virt_type not in ('qemu', 'kvm'):
682             LOG.warning(
683                 "Support for the '%(type)s' libvirt backend has been "
684                 "deprecated and will be removed in a future release.",
685                 {'type': CONF.libvirt.virt_type},
686             )
687 
688         caps = self._host.get_capabilities()
689         hostarch = caps.host.cpu.arch
690         if hostarch not in (
691             fields.Architecture.I686, fields.Architecture.X86_64,
692         ):
693             LOG.warning(
694                 'The libvirt driver is not tested on %(arch)s by the '
695                 'OpenStack project and thus its quality can not be ensured. '
696                 'For more information, see: https://docs.openstack.org/'
697                 'nova/latest/user/support-matrix.html',
698                 {'arch': hostarch},
699             )
700 
701     def _handle_conn_event(self, enabled, reason):
702         LOG.info("Connection event '%(enabled)d' reason '%(reason)s'",
703                  {'enabled': enabled, 'reason': reason})
704         self._set_host_enabled(enabled, reason)
705 
706     def init_host(self, host):
707         self._host.initialize()
708 
709         self._update_host_specific_capabilities()
710 
711         self._check_cpu_set_configuration()
712 
713         self._do_quality_warnings()
714 
715         self._parse_migration_flags()
716 
717         self._supported_perf_events = self._get_supported_perf_events()
718 
719         self._check_file_backed_memory_support()
720 
721         self._check_my_ip()
722 
723         if (CONF.libvirt.virt_type == 'lxc' and
724                 not (CONF.libvirt.uid_maps and CONF.libvirt.gid_maps)):
725             LOG.warning("Running libvirt-lxc without user namespaces is "
726                         "dangerous. Containers spawned by Nova will be run "
727                         "as the host's root user. It is highly suggested "
728                         "that user namespaces be used in a public or "
729                         "multi-tenant environment.")
730 
731         # Stop libguestfs using KVM unless we're also configured
732         # to use this. This solves problem where people need to
733         # stop Nova use of KVM because nested-virt is broken
734         if CONF.libvirt.virt_type != "kvm":
735             guestfs.force_tcg()
736 
737         if not self._host.has_min_version(MIN_LIBVIRT_VERSION):
738             raise exception.InternalError(
739                 _('Nova requires libvirt version %s or greater.') %
740                 libvirt_utils.version_to_string(MIN_LIBVIRT_VERSION))
741 
742         if CONF.libvirt.virt_type in ("qemu", "kvm"):
743             if not self._host.has_min_version(hv_ver=MIN_QEMU_VERSION):
744                 raise exception.InternalError(
745                     _('Nova requires QEMU version %s or greater.') %
746                     libvirt_utils.version_to_string(MIN_QEMU_VERSION))
747 
748         if CONF.libvirt.virt_type == 'parallels':
749             if not self._host.has_min_version(hv_ver=MIN_VIRTUOZZO_VERSION):
750                 raise exception.InternalError(
751                     _('Nova requires Virtuozzo version %s or greater.') %
752                     libvirt_utils.version_to_string(MIN_VIRTUOZZO_VERSION))
753 
754         # Give the cloud admin a heads up if we are intending to
755         # change the MIN_LIBVIRT_VERSION in the next release.
756         if not self._host.has_min_version(NEXT_MIN_LIBVIRT_VERSION):
757             LOG.warning('Running Nova with a libvirt version less than '
758                         '%(version)s is deprecated. The required minimum '
759                         'version of libvirt will be raised to %(version)s '
760                         'in the next release.',
761                         {'version': libvirt_utils.version_to_string(
762                             NEXT_MIN_LIBVIRT_VERSION)})
763         if (CONF.libvirt.virt_type in ("qemu", "kvm") and
764             not self._host.has_min_version(hv_ver=NEXT_MIN_QEMU_VERSION)):
765             LOG.warning('Running Nova with a QEMU version less than '
766                         '%(version)s is deprecated. The required minimum '
767                         'version of QEMU will be raised to %(version)s '
768                         'in the next release.',
769                         {'version': libvirt_utils.version_to_string(
770                             NEXT_MIN_QEMU_VERSION)})
771 
772         # Allowing both "tunnelling via libvirtd" (which will be
773         # deprecated once the MIN_{LIBVIRT,QEMU}_VERSION is sufficiently
774         # new enough) and "native TLS" options at the same time is
775         # nonsensical.
776         if (CONF.libvirt.live_migration_tunnelled and
777                 CONF.libvirt.live_migration_with_native_tls):
778             msg = _("Setting both 'live_migration_tunnelled' and "
779                     "'live_migration_with_native_tls' at the same "
780                     "time is invalid. If you have the relevant "
781                     "libvirt and QEMU versions, and TLS configured "
782                     "in your environment, pick "
783                     "'live_migration_with_native_tls'.")
784             raise exception.Invalid(msg)
785 
786         # Some imagebackends are only able to import raw disk images,
787         # and will fail if given any other format. See the bug
788         # https://bugs.launchpad.net/nova/+bug/1816686 for more details.
789         if CONF.libvirt.images_type in ('rbd',):
790             if not CONF.force_raw_images:
791                 msg = _("'[DEFAULT]/force_raw_images = False' is not "
792                         "allowed with '[libvirt]/images_type = rbd'. "
793                         "Please check the two configs and if you really "
794                         "do want to use rbd as images_type, set "
795                         "force_raw_images to True.")
796                 raise exception.InvalidConfiguration(msg)
797 
798         # TODO(sbauza): Remove this code once mediated devices are persisted
799         # across reboots.
800         self._recreate_assigned_mediated_devices()
801 
802         self._check_cpu_compatibility()
803 
804         self._check_vtpm_support()
805 
806         self._register_instance_machine_type()
807 
808     def _update_host_specific_capabilities(self) -> None:
809         """Update driver capabilities based on capabilities of the host."""
810         # TODO(stephenfin): We should also be reporting e.g. SEV functionality
811         # or UEFI bootloader support in this manner
812         self.capabilities.update({
813             'supports_secure_boot': self._host.supports_secure_boot,
814         })
815 
816     def _register_instance_machine_type(self):
817         """Register the machine type of instances on this host
818 
819         For each instance found on this host by InstanceList.get_by_host ensure
820         a machine type is registered within the system metadata of the instance
821         """
822         context = nova_context.get_admin_context()
823         hostname = self._host.get_hostname()
824 
825         for instance in objects.InstanceList.get_by_host(context, hostname):
826             # NOTE(lyarwood): Skip if hw_machine_type is set already in the
827             # image_meta of the instance. Note that this value comes from the
828             # system metadata of the instance where it is stored under the
829             # image_hw_machine_type key.
830             if instance.image_meta.properties.get('hw_machine_type'):
831                 continue
832 
833             # Fetch and record the machine type from the config
834             hw_machine_type = libvirt_utils.get_machine_type(
835                 instance.image_meta)
836             # NOTE(lyarwood): As above this updates
837             # image_meta.properties.hw_machine_type within the instance and
838             # will be returned the next time libvirt_utils.get_machine_type is
839             # called for the instance image meta.
840             instance.system_metadata['image_hw_machine_type'] = hw_machine_type
841             instance.save()
842             LOG.debug("Instance machine_type updated to %s", hw_machine_type,
843                       instance=instance)
844 
845     def _prepare_cpu_flag(self, flag):
846         # NOTE(kchamart) This helper method will be used while computing
847         # guest CPU compatibility.  It will take into account a
848         # comma-separated list of CPU flags from
849         # `[libvirt]cpu_model_extra_flags`.  If the CPU flag starts
850         # with '+', it is enabled for the guest; if it starts with '-',
851         # it is disabled.  If neither '+' nor '-' is specified, the CPU
852         # flag is enabled.
853         if flag.startswith('-'):
854             flag = flag.lstrip('-')
855             policy_value = 'disable'
856         else:
857             flag = flag.lstrip('+')
858             policy_value = 'require'
859 
860         cpu_feature = vconfig.LibvirtConfigGuestCPUFeature(
861                         flag, policy=policy_value)
862         return cpu_feature
863 
864     def _check_cpu_compatibility(self):
865         mode = CONF.libvirt.cpu_mode
866         models = CONF.libvirt.cpu_models
867 
868         if (CONF.libvirt.virt_type not in ("kvm", "qemu") and
869                 mode not in (None, 'none')):
870             msg = _("Config requested an explicit CPU model, but "
871                     "the current libvirt hypervisor '%s' does not "
872                     "support selecting CPU models") % CONF.libvirt.virt_type
873             raise exception.Invalid(msg)
874 
875         if mode != "custom":
876             if not models:
877                 return
878             msg = _("The cpu_models option is not required when "
879                     "cpu_mode!=custom")
880             raise exception.Invalid(msg)
881 
882         if not models:
883             msg = _("The cpu_models option is required when cpu_mode=custom")
884             raise exception.Invalid(msg)
885 
886         cpu = vconfig.LibvirtConfigGuestCPU()
887         for model in models:
888             cpu.model = self._get_cpu_model_mapping(model)
889             try:
890                 self._compare_cpu(cpu, self._get_cpu_info(), None)
891             except exception.InvalidCPUInfo as e:
892                 msg = (_("Configured CPU model: %(model)s is not "
893                          "compatible with host CPU. Please correct your "
894                          "config and try again. %(e)s") % {
895                             'model': model, 'e': e})
896                 raise exception.InvalidCPUInfo(msg)
897 
898         # Use guest CPU model to check the compatibility between guest CPU and
899         # configured extra_flags
900         cpu = vconfig.LibvirtConfigGuestCPU()
901         cpu.model = self._host.get_capabilities().host.cpu.model
902         for flag in set(x.lower() for x in CONF.libvirt.cpu_model_extra_flags):
903             cpu_feature = self._prepare_cpu_flag(flag)
904             cpu.add_feature(cpu_feature)
905             try:
906                 self._compare_cpu(cpu, self._get_cpu_info(), None)
907             except exception.InvalidCPUInfo as e:
908                 msg = (_("Configured extra flag: %(flag)s it not correct, or "
909                          "the host CPU does not support this flag. Please "
910                          "correct the config and try again. %(e)s") % {
911                             'flag': flag, 'e': e})
912                 raise exception.InvalidCPUInfo(msg)
913 
914     def _check_vtpm_support(self) -> None:
915         # TODO(efried): A key manager must be configured to create/retrieve
916         # secrets. Is there a way to check that one is set up correctly?
917         # CONF.key_manager.backend is optional :(
918         if not CONF.libvirt.swtpm_enabled:
919             return
920 
921         if CONF.libvirt.virt_type not in ('qemu', 'kvm'):
922             msg = _(
923                 "vTPM support requires '[libvirt] virt_type' of 'qemu' or "
924                 "'kvm'; found '%s'.")
925             raise exception.InvalidConfiguration(msg % CONF.libvirt.virt_type)
926 
927         # These executables need to be installed for libvirt to make use of
928         # emulated TPM.
929         # NOTE(stephenfin): This checks using the PATH of the user running
930         # nova-compute rather than the libvirtd service, meaning it's an
931         # imperfect check but the best we can do
932         if not any(shutil.which(cmd) for cmd in ('swtpm_setup', 'swtpm')):
933             msg = _(
934                 "vTPM support is configured but the 'swtpm' and "
935                 "'swtpm_setup' binaries could not be found on PATH.")
936             raise exception.InvalidConfiguration(msg)
937 
938         # The user and group must be valid on this host for cold migration and
939         # resize to function.
940         try:
941             pwd.getpwnam(CONF.libvirt.swtpm_user)
942         except KeyError:
943             msg = _(
944                 "The user configured in '[libvirt] swtpm_user' does not exist "
945                 "on this host; expected '%s'.")
946             raise exception.InvalidConfiguration(msg % CONF.libvirt.swtpm_user)
947 
948         try:
949             grp.getgrnam(CONF.libvirt.swtpm_group)
950         except KeyError:
951             msg = _(
952                 "The group configured in '[libvirt] swtpm_group' does not "
953                 "exist on this host; expected '%s'.")
954             raise exception.InvalidConfiguration(
955                 msg % CONF.libvirt.swtpm_group)
956 
957         LOG.debug('Enabling emulated TPM support')
958 
959     @staticmethod
960     def _is_existing_mdev(uuid):
961         # FIXME(sbauza): Some kernel can have a uevent race meaning that the
962         # libvirt daemon won't know when a mediated device is created unless
963         # you restart that daemon. Until all kernels we support are not having
964         # that possible race, check the sysfs directly instead of asking the
965         # libvirt API.
966         # See https://bugzilla.redhat.com/show_bug.cgi?id=1376907 for ref.
967         return os.path.exists('/sys/bus/mdev/devices/{0}'.format(uuid))
968 
969     def _recreate_assigned_mediated_devices(self):
970         """Recreate assigned mdevs that could have disappeared if we reboot
971         the host.
972         """
973         # NOTE(sbauza): This method just calls sysfs to recreate mediated
974         # devices by looking up existing guest XMLs and doesn't use
975         # the Placement API so it works with or without a vGPU reshape.
976         mdevs = self._get_all_assigned_mediated_devices()
977         for (mdev_uuid, instance_uuid) in mdevs.items():
978             if not self._is_existing_mdev(mdev_uuid):
979                 dev_name = libvirt_utils.mdev_uuid2name(mdev_uuid)
980                 dev_info = self._get_mediated_device_information(dev_name)
981                 parent = dev_info['parent']
982                 parent_type = self._get_vgpu_type_per_pgpu(parent)
983                 if dev_info['type'] != parent_type:
984                     # NOTE(sbauza): The mdev was created by using a different
985                     # vGPU type. We can't recreate the mdev until the operator
986                     # modifies the configuration.
987                     parent = "{}:{}:{}.{}".format(*parent[4:].split('_'))
988                     msg = ("The instance UUID %(inst)s uses a mediated device "
989                            "type %(type)s that is no longer supported by the "
990                            "parent PCI device, %(parent)s. Please correct "
991                            "the configuration accordingly." %
992                            {'inst': instance_uuid,
993                             'parent': parent,
994                             'type': dev_info['type']})
995                     raise exception.InvalidLibvirtMdevConfig(reason=msg)
996                 self._create_new_mediated_device(parent, uuid=mdev_uuid)
997 
998     def _check_file_backed_memory_support(self):
999         if not CONF.libvirt.file_backed_memory:
1000             return
1001 
1002         # file_backed_memory is only compatible with qemu/kvm virts
1003         if CONF.libvirt.virt_type not in ("qemu", "kvm"):
1004             raise exception.InternalError(
1005                 _('Running Nova with file_backed_memory and virt_type '
1006                   '%(type)s is not supported. file_backed_memory is only '
1007                   'supported with qemu and kvm types.') %
1008                 {'type': CONF.libvirt.virt_type})
1009 
1010         # file-backed memory doesn't work with memory overcommit.
1011         # Block service startup if file-backed memory is enabled and
1012         # ram_allocation_ratio is not 1.0
1013         if CONF.ram_allocation_ratio != 1.0:
1014             raise exception.InternalError(
1015                 'Running Nova with file_backed_memory requires '
1016                 'ram_allocation_ratio configured to 1.0')
1017 
1018         if CONF.reserved_host_memory_mb:
1019             # this is a hard failure as placement won't allow total < reserved
1020             if CONF.reserved_host_memory_mb >= CONF.libvirt.file_backed_memory:
1021                 msg = _(
1022                     "'[libvirt] file_backed_memory', which represents total "
1023                     "memory reported to placement, must be greater than "
1024                     "reserved memory configured via '[DEFAULT] "
1025                     "reserved_host_memory_mb'"
1026                 )
1027                 raise exception.InternalError(msg)
1028 
1029             # TODO(stephenfin): Change this to an exception in W or later
1030             LOG.warning(
1031                 "Reserving memory via '[DEFAULT] reserved_host_memory_mb' "
1032                 "is not compatible with file-backed memory. Consider "
1033                 "setting '[DEFAULT] reserved_host_memory_mb' to 0. This will "
1034                 "be an error in a future release."
1035             )
1036 
1037     def _check_my_ip(self):
1038         ips = compute_utils.get_machine_ips()
1039         if CONF.my_ip not in ips:
1040             LOG.warning('my_ip address (%(my_ip)s) was not found on '
1041                         'any of the interfaces: %(ifaces)s',
1042                         {'my_ip': CONF.my_ip, 'ifaces': ", ".join(ips)})
1043 
1044     def _check_cpu_set_configuration(self):
1045         # evaluate these now to force a quick fail if they're invalid
1046         vcpu_pin_set = hardware.get_vcpu_pin_set() or set()
1047         cpu_shared_set = hardware.get_cpu_shared_set() or set()
1048         cpu_dedicated_set = hardware.get_cpu_dedicated_set() or set()
1049 
1050         # TODO(stephenfin): Remove this in U once we remove the 'vcpu_pin_set'
1051         # option
1052         if not vcpu_pin_set:
1053             if not (cpu_shared_set or cpu_dedicated_set):
1054                 return
1055 
1056             if not cpu_dedicated_set.isdisjoint(cpu_shared_set):
1057                 msg = _(
1058                     "The '[compute] cpu_dedicated_set' and '[compute] "
1059                     "cpu_shared_set' configuration options must be "
1060                     "disjoint.")
1061                 raise exception.InvalidConfiguration(msg)
1062 
1063             if CONF.reserved_host_cpus:
1064                 msg = _(
1065                     "The 'reserved_host_cpus' config option cannot be defined "
1066                     "alongside the '[compute] cpu_shared_set' or '[compute] "
1067                     "cpu_dedicated_set' options. Unset 'reserved_host_cpus'.")
1068                 raise exception.InvalidConfiguration(msg)
1069 
1070             return
1071 
1072         if cpu_dedicated_set:
1073             # NOTE(stephenfin): This is a new option in Train so it can be
1074             # an error
1075             msg = _(
1076                 "The 'vcpu_pin_set' config option has been deprecated and "
1077                 "cannot be defined alongside '[compute] cpu_dedicated_set'. "
1078                 "Unset 'vcpu_pin_set'.")
1079             raise exception.InvalidConfiguration(msg)
1080 
1081         if cpu_shared_set:
1082             LOG.warning(
1083                 "The '[compute] cpu_shared_set' and 'vcpu_pin_set' config "
1084                 "options have both been defined. While 'vcpu_pin_set' is "
1085                 "defined, it will continue to be used to configure the "
1086                 "specific host CPUs used for 'VCPU' inventory, while "
1087                 "'[compute] cpu_shared_set' will only be used for guest "
1088                 "emulator threads when 'hw:emulator_threads_policy=shared' "
1089                 "is defined in the flavor. This is legacy behavior and will "
1090                 "not be supported in a future release. "
1091                 "If you wish to define specific host CPUs to be used for "
1092                 "'VCPU' or 'PCPU' inventory, you must migrate the "
1093                 "'vcpu_pin_set' config option value to '[compute] "
1094                 "cpu_shared_set' and '[compute] cpu_dedicated_set', "
1095                 "respectively, and undefine 'vcpu_pin_set'.")
1096         else:
1097             LOG.warning(
1098                 "The 'vcpu_pin_set' config option has been deprecated and "
1099                 "will be removed in a future release. When defined, "
1100                 "'vcpu_pin_set' will be used to calculate 'VCPU' inventory "
1101                 "and schedule instances that have 'VCPU' allocations. "
1102                 "If you wish to define specific host CPUs to be used for "
1103                 "'VCPU' or 'PCPU' inventory, you must migrate the "
1104                 "'vcpu_pin_set' config option value to '[compute] "
1105                 "cpu_shared_set' and '[compute] cpu_dedicated_set', "
1106                 "respectively, and undefine 'vcpu_pin_set'.")
1107 
1108     def _prepare_migration_flags(self):
1109         migration_flags = 0
1110 
1111         migration_flags |= libvirt.VIR_MIGRATE_LIVE
1112 
1113         # Enable support for p2p migrations
1114         migration_flags |= libvirt.VIR_MIGRATE_PEER2PEER
1115 
1116         # Adding VIR_MIGRATE_UNDEFINE_SOURCE because, without it, migrated
1117         # instance will remain defined on the source host
1118         migration_flags |= libvirt.VIR_MIGRATE_UNDEFINE_SOURCE
1119 
1120         # Adding VIR_MIGRATE_PERSIST_DEST to persist the VM on the
1121         # destination host
1122         migration_flags |= libvirt.VIR_MIGRATE_PERSIST_DEST
1123 
1124         live_migration_flags = block_migration_flags = migration_flags
1125 
1126         # Adding VIR_MIGRATE_NON_SHARED_INC, otherwise all block-migrations
1127         # will be live-migrations instead
1128         block_migration_flags |= libvirt.VIR_MIGRATE_NON_SHARED_INC
1129 
1130         return (live_migration_flags, block_migration_flags)
1131 
1132     # TODO(kchamart) Once the MIN_LIBVIRT_VERSION and MIN_QEMU_VERSION
1133     # reach 4.4.0 and 2.11.0, which provide "native TLS" support by
1134     # default, deprecate and remove the support for "tunnelled live
1135     # migration" (and related config attribute), because:
1136     #
1137     #  (a) it cannot handle live migration of disks in a non-shared
1138     #      storage setup (a.k.a. "block migration");
1139     #
1140     #  (b) has a huge performance overhead and latency, because it burns
1141     #      more CPU and memory bandwidth due to increased number of data
1142     #      copies on both source and destination hosts.
1143     #
1144     # Both the above limitations are addressed by the QEMU-native TLS
1145     # support (`live_migration_with_native_tls`).
1146     def _handle_live_migration_tunnelled(self, migration_flags):
1147         if CONF.libvirt.live_migration_tunnelled:
1148             migration_flags |= libvirt.VIR_MIGRATE_TUNNELLED
1149         return migration_flags
1150 
1151     def _handle_native_tls(self, migration_flags):
1152         if (CONF.libvirt.live_migration_with_native_tls):
1153             migration_flags |= libvirt.VIR_MIGRATE_TLS
1154         return migration_flags
1155 
1156     def _handle_live_migration_post_copy(self, migration_flags):
1157         if CONF.libvirt.live_migration_permit_post_copy:
1158             migration_flags |= libvirt.VIR_MIGRATE_POSTCOPY
1159         return migration_flags
1160 
1161     def _handle_live_migration_auto_converge(self, migration_flags):
1162         if self._is_post_copy_enabled(migration_flags):
1163             LOG.info('The live_migration_permit_post_copy is set to '
1164                      'True and post copy live migration is available '
1165                      'so auto-converge will not be in use.')
1166         elif CONF.libvirt.live_migration_permit_auto_converge:
1167             migration_flags |= libvirt.VIR_MIGRATE_AUTO_CONVERGE
1168         return migration_flags
1169 
1170     def _parse_migration_flags(self):
1171         (live_migration_flags,
1172             block_migration_flags) = self._prepare_migration_flags()
1173 
1174         live_migration_flags = self._handle_live_migration_tunnelled(
1175             live_migration_flags)
1176         block_migration_flags = self._handle_live_migration_tunnelled(
1177             block_migration_flags)
1178 
1179         live_migration_flags = self._handle_native_tls(
1180             live_migration_flags)
1181         block_migration_flags = self._handle_native_tls(
1182             block_migration_flags)
1183 
1184         live_migration_flags = self._handle_live_migration_post_copy(
1185             live_migration_flags)
1186         block_migration_flags = self._handle_live_migration_post_copy(
1187             block_migration_flags)
1188 
1189         live_migration_flags = self._handle_live_migration_auto_converge(
1190             live_migration_flags)
1191         block_migration_flags = self._handle_live_migration_auto_converge(
1192             block_migration_flags)
1193 
1194         self._live_migration_flags = live_migration_flags
1195         self._block_migration_flags = block_migration_flags
1196 
1197     # TODO(sahid): This method is targeted for removal when the tests
1198     # have been updated to avoid its use
1199     #
1200     # All libvirt API calls on the libvirt.Connect object should be
1201     # encapsulated by methods on the nova.virt.libvirt.host.Host
1202     # object, rather than directly invoking the libvirt APIs. The goal
1203     # is to avoid a direct dependency on the libvirt API from the
1204     # driver.py file.
1205     def _get_connection(self):
1206         return self._host.get_connection()
1207 
1208     _conn = property(_get_connection)
1209 
1210     @staticmethod
1211     def _uri():
1212         if CONF.libvirt.virt_type == 'lxc':
1213             uri = CONF.libvirt.connection_uri or 'lxc:///'
1214         elif CONF.libvirt.virt_type == 'parallels':
1215             uri = CONF.libvirt.connection_uri or 'parallels:///system'
1216         else:
1217             uri = CONF.libvirt.connection_uri or 'qemu:///system'
1218         return uri
1219 
1220     @staticmethod
1221     def _live_migration_uri(dest):
1222         uris = {
1223             'kvm': 'qemu+%(scheme)s://%(dest)s/system',
1224             'qemu': 'qemu+%(scheme)s://%(dest)s/system',
1225             'parallels': 'parallels+tcp://%(dest)s/system',
1226         }
1227         dest = oslo_netutils.escape_ipv6(dest)
1228 
1229         virt_type = CONF.libvirt.virt_type
1230         # TODO(pkoniszewski): Remove fetching live_migration_uri in Pike
1231         uri = CONF.libvirt.live_migration_uri
1232         if uri:
1233             return uri % dest
1234 
1235         uri = uris.get(virt_type)
1236         if uri is None:
1237             raise exception.LiveMigrationURINotAvailable(virt_type=virt_type)
1238 
1239         str_format = {
1240             'dest': dest,
1241             'scheme': CONF.libvirt.live_migration_scheme or 'tcp',
1242         }
1243         return uri % str_format
1244 
1245     @staticmethod
1246     def _migrate_uri(dest):
1247         uri = None
1248         dest = oslo_netutils.escape_ipv6(dest)
1249 
1250         # Only QEMU live migrations supports migrate-uri parameter
1251         virt_type = CONF.libvirt.virt_type
1252         if virt_type in ('qemu', 'kvm'):
1253             # QEMU accept two schemes: tcp and rdma.  By default
1254             # libvirt build the URI using the remote hostname and the
1255             # tcp schema.
1256             uri = 'tcp://%s' % dest
1257         # Because dest might be of type unicode, here we might return value of
1258         # type unicode as well which is not acceptable by libvirt python
1259         # binding when Python 2.7 is in use, so let's convert it explicitly
1260         # back to string. When Python 3.x is in use, libvirt python binding
1261         # accepts unicode type so it is completely fine to do a no-op str(uri)
1262         # conversion which will return value of type unicode.
1263         return uri and str(uri)
1264 
1265     def instance_exists(self, instance):
1266         """Efficient override of base instance_exists method."""
1267         try:
1268             self._host.get_guest(instance)
1269             return True
1270         except (exception.InternalError, exception.InstanceNotFound):
1271             return False
1272 
1273     def list_instances(self):
1274         names = []
1275         for guest in self._host.list_guests(only_running=False):
1276             names.append(guest.name)
1277 
1278         return names
1279 
1280     def list_instance_uuids(self):
1281         uuids = []
1282         for guest in self._host.list_guests(only_running=False):
1283             uuids.append(guest.uuid)
1284 
1285         return uuids
1286 
1287     def plug_vifs(self, instance, network_info):
1288         """Plug VIFs into networks."""
1289         for vif in network_info:
1290             self.vif_driver.plug(instance, vif)
1291 
1292     def _unplug_vifs(self, instance, network_info, ignore_errors):
1293         """Unplug VIFs from networks."""
1294         for vif in network_info:
1295             try:
1296                 self.vif_driver.unplug(instance, vif)
1297             except exception.NovaException:
1298                 if not ignore_errors:
1299                     raise
1300 
1301     def unplug_vifs(self, instance, network_info):
1302         self._unplug_vifs(instance, network_info, False)
1303 
1304     def _teardown_container(self, instance):
1305         inst_path = libvirt_utils.get_instance_path(instance)
1306         container_dir = os.path.join(inst_path, 'rootfs')
1307         rootfs_dev = instance.system_metadata.get('rootfs_device_name')
1308         LOG.debug('Attempting to teardown container at path %(dir)s with '
1309                   'root device: %(rootfs_dev)s',
1310                   {'dir': container_dir, 'rootfs_dev': rootfs_dev},
1311                   instance=instance)
1312         disk_api.teardown_container(container_dir, rootfs_dev)
1313 
1314     def _destroy(self, instance):
1315         try:
1316             guest = self._host.get_guest(instance)
1317             if CONF.serial_console.enabled:
1318                 # This method is called for several events: destroy,
1319                 # rebuild, hard-reboot, power-off - For all of these
1320                 # events we want to release the serial ports acquired
1321                 # for the guest before destroying it.
1322                 serials = self._get_serial_ports_from_guest(guest)
1323                 for hostname, port in serials:
1324                     serial_console.release_port(host=hostname, port=port)
1325         except exception.InstanceNotFound:
1326             guest = None
1327 
1328         # If the instance is already terminated, we're still happy
1329         # Otherwise, destroy it
1330         old_domid = -1
1331         if guest is not None:
1332             try:
1333                 old_domid = guest.id
1334                 guest.poweroff()
1335 
1336             except libvirt.libvirtError as e:
1337                 is_okay = False
1338                 errcode = e.get_error_code()
1339                 if errcode == libvirt.VIR_ERR_NO_DOMAIN:
1340                     # Domain already gone. This can safely be ignored.
1341                     is_okay = True
1342                 elif errcode == libvirt.VIR_ERR_OPERATION_INVALID:
1343                     # If the instance is already shut off, we get this:
1344                     # Code=55 Error=Requested operation is not valid:
1345                     # domain is not running
1346 
1347                     state = guest.get_power_state(self._host)
1348                     if state == power_state.SHUTDOWN:
1349                         is_okay = True
1350                 elif errcode == libvirt.VIR_ERR_INTERNAL_ERROR:
1351                     errmsg = e.get_error_message()
1352                     if (CONF.libvirt.virt_type == 'lxc' and
1353                         errmsg == 'internal error: '
1354                                   'Some processes refused to die'):
1355                         # Some processes in the container didn't die
1356                         # fast enough for libvirt. The container will
1357                         # eventually die. For now, move on and let
1358                         # the wait_for_destroy logic take over.
1359                         is_okay = True
1360                 elif errcode == libvirt.VIR_ERR_OPERATION_TIMEOUT:
1361                     LOG.warning("Cannot destroy instance, operation time out",
1362                                 instance=instance)
1363                     reason = _("operation time out")
1364                     raise exception.InstancePowerOffFailure(reason=reason)
1365                 elif errcode == libvirt.VIR_ERR_SYSTEM_ERROR:
1366                     with excutils.save_and_reraise_exception():
1367                         LOG.warning("Cannot destroy instance, general system "
1368                                     "call failure", instance=instance)
1369                 if not is_okay:
1370                     with excutils.save_and_reraise_exception():
1371                         LOG.error('Error from libvirt during destroy. '
1372                                   'Code=%(errcode)s Error=%(e)s',
1373                                   {'errcode': errcode, 'e': e},
1374                                   instance=instance)
1375 
1376         def _wait_for_destroy(expected_domid):
1377             """Called at an interval until the VM is gone."""
1378             # NOTE(vish): If the instance disappears during the destroy
1379             #             we ignore it so the cleanup can still be
1380             #             attempted because we would prefer destroy to
1381             #             never fail.
1382             try:
1383                 dom_info = self.get_info(instance)
1384                 state = dom_info.state
1385                 new_domid = dom_info.internal_id
1386             except exception.InstanceNotFound:
1387                 LOG.debug("During wait destroy, instance disappeared.",
1388                           instance=instance)
1389                 state = power_state.SHUTDOWN
1390 
1391             if state == power_state.SHUTDOWN:
1392                 LOG.info("Instance destroyed successfully.", instance=instance)
1393                 raise loopingcall.LoopingCallDone()
1394 
1395             # NOTE(wangpan): If the instance was booted again after destroy,
1396             #                this may be an endless loop, so check the id of
1397             #                domain here, if it changed and the instance is
1398             #                still running, we should destroy it again.
1399             # see https://bugs.launchpad.net/nova/+bug/1111213 for more details
1400             if new_domid != expected_domid:
1401                 LOG.info("Instance may be started again.", instance=instance)
1402                 kwargs['is_running'] = True
1403                 raise loopingcall.LoopingCallDone()
1404 
1405         kwargs = {'is_running': False}
1406         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_destroy,
1407                                                      old_domid)
1408         timer.start(interval=0.5).wait()
1409         if kwargs['is_running']:
1410             LOG.info("Going to destroy instance again.", instance=instance)
1411             self._destroy(instance)
1412         else:
1413             # NOTE(GuanQiang): teardown container to avoid resource leak
1414             if CONF.libvirt.virt_type == 'lxc':
1415                 self._teardown_container(instance)
1416 
1417     def destroy(self, context, instance, network_info, block_device_info=None,
1418                 destroy_disks=True, destroy_secrets=True):
1419         self._destroy(instance)
1420         # NOTE(gibi): if there was device detach in progress then we need to
1421         # unblock the waiting threads and clean up.
1422         self._device_event_handler.cleanup_waiters(instance.uuid)
1423         self.cleanup(context, instance, network_info, block_device_info,
1424                      destroy_disks, destroy_secrets=destroy_secrets)
1425 
1426     def _undefine_domain(self, instance):
1427         try:
1428             guest = self._host.get_guest(instance)
1429             try:
1430                 hw_firmware_type = instance.image_meta.properties.get(
1431                     'hw_firmware_type')
1432                 support_uefi = self._check_uefi_support(hw_firmware_type)
1433                 guest.delete_configuration(support_uefi)
1434             except libvirt.libvirtError as e:
1435                 with excutils.save_and_reraise_exception() as ctxt:
1436                     errcode = e.get_error_code()
1437                     if errcode == libvirt.VIR_ERR_NO_DOMAIN:
1438                         LOG.debug("Called undefine, but domain already gone.",
1439                                   instance=instance)
1440                         ctxt.reraise = False
1441                     else:
1442                         LOG.error('Error from libvirt during undefine. '
1443                                   'Code=%(errcode)s Error=%(e)s',
1444                                   {'errcode': errcode,
1445                                    'e': encodeutils.exception_to_unicode(e)},
1446                                   instance=instance)
1447         except exception.InstanceNotFound:
1448             pass
1449 
1450     def cleanup(self, context, instance, network_info, block_device_info=None,
1451                 destroy_disks=True, migrate_data=None, destroy_vifs=True,
1452                 destroy_secrets=True):
1453         """Cleanup the instance from the host.
1454 
1455         Identify if the instance disks and instance path should be removed
1456         from the host before calling down into the _cleanup method for the
1457         actual removal of resources from the host.
1458 
1459         :param context: security context
1460         :param instance: instance object for the instance being cleaned up
1461         :param network_info: instance network information
1462         :param block_device_info: optional instance block device information
1463         :param destroy_disks: if local ephemeral disks should be destroyed
1464         :param migrate_data: optional migrate_data object
1465         :param destroy_vifs: if plugged vifs should be unplugged
1466         :param destroy_secrets: Indicates if secrets should be destroyed
1467         """
1468         cleanup_instance_dir = False
1469         cleanup_instance_disks = False
1470         # We assume destroy_disks means destroy instance directory and disks
1471         if destroy_disks:
1472             cleanup_instance_dir = True
1473             cleanup_instance_disks = True
1474         else:
1475             # NOTE(mdbooth): I think the theory here was that if this is a
1476             # migration with shared block storage then we need to delete the
1477             # instance directory because that's not shared. I'm pretty sure
1478             # this is wrong.
1479             if migrate_data and 'is_shared_block_storage' in migrate_data:
1480                 cleanup_instance_dir = migrate_data.is_shared_block_storage
1481 
1482             # NOTE(lyarwood): The following workaround allows operators to
1483             # ensure that non-shared instance directories are removed after an
1484             # evacuation or revert resize when using the shared RBD
1485             # imagebackend. This workaround is not required when cleaning up
1486             # migrations that provide migrate_data to this method as the
1487             # existing is_shared_block_storage conditional will cause the
1488             # instance directory to be removed.
1489             if not cleanup_instance_dir:
1490                 if CONF.workarounds.ensure_libvirt_rbd_instance_dir_cleanup:
1491                     cleanup_instance_dir = CONF.libvirt.images_type == 'rbd'
1492 
1493         return self._cleanup(
1494                 context, instance, network_info,
1495                 block_device_info=block_device_info,
1496                 destroy_vifs=destroy_vifs,
1497                 cleanup_instance_dir=cleanup_instance_dir,
1498                 cleanup_instance_disks=cleanup_instance_disks,
1499                 destroy_secrets=destroy_secrets)
1500 
1501     def _cleanup(self, context, instance, network_info, block_device_info=None,
1502                  destroy_vifs=True, cleanup_instance_dir=False,
1503                  cleanup_instance_disks=False, destroy_secrets=True):
1504         """Cleanup the domain and any attached resources from the host.
1505 
1506         This method cleans up any pmem devices, unplugs VIFs, disconnects
1507         attached volumes and undefines the instance domain within libvirt.
1508         It also optionally removes the ephemeral disks and the instance
1509         directory from the host depending on the cleanup_instance_dir|disks
1510         kwargs provided.
1511 
1512         :param context: security context
1513         :param instance: instance object for the instance being cleaned up
1514         :param network_info: instance network information
1515         :param block_device_info: optional instance block device information
1516         :param destroy_vifs: if plugged vifs should be unplugged
1517         :param cleanup_instance_dir: If the instance dir should be removed
1518         :param cleanup_instance_disks: If the instance disks should be removed
1519         """
1520         # zero the data on backend pmem device
1521         vpmems = self._get_vpmems(instance)
1522         if vpmems:
1523             self._cleanup_vpmems(vpmems)
1524 
1525         if destroy_vifs:
1526             self._unplug_vifs(instance, network_info, True)
1527 
1528         # FIXME(wangpan): if the instance is booted again here, such as the
1529         #                 soft reboot operation boot it here, it will become
1530         #                 "running deleted", should we check and destroy it
1531         #                 at the end of this method?
1532 
1533         # NOTE(vish): we disconnect from volumes regardless
1534         block_device_mapping = driver.block_device_info_get_mapping(
1535             block_device_info)
1536         for vol in block_device_mapping:
1537             connection_info = vol['connection_info']
1538             if not connection_info:
1539                 # if booting from a volume, creation could have failed meaning
1540                 # this would be unset
1541                 continue
1542 
1543             try:
1544                 self._disconnect_volume(
1545                     context, connection_info, instance,
1546                     destroy_secrets=destroy_secrets)
1547             except Exception as exc:
1548                 with excutils.save_and_reraise_exception() as ctxt:
1549                     if cleanup_instance_disks:
1550                         # Don't block on Volume errors if we're trying to
1551                         # delete the instance as we may be partially created
1552                         # or deleted
1553                         ctxt.reraise = False
1554                         LOG.warning(
1555                             "Ignoring Volume Error on vol %(vol_id)s "
1556                             "during delete %(exc)s",
1557                             {'vol_id': vol.get('volume_id'),
1558                              'exc': encodeutils.exception_to_unicode(exc)},
1559                             instance=instance)
1560 
1561         if cleanup_instance_disks:
1562             # NOTE(haomai): destroy volumes if needed
1563             if CONF.libvirt.images_type == 'lvm':
1564                 self._cleanup_lvm(instance, block_device_info)
1565             if CONF.libvirt.images_type == 'rbd':
1566                 self._cleanup_rbd(instance)
1567 
1568         if cleanup_instance_dir:
1569             attempts = int(instance.system_metadata.get('clean_attempts',
1570                                                         '0'))
1571             success = self.delete_instance_files(instance)
1572             # NOTE(mriedem): This is used in the _run_pending_deletes periodic
1573             # task in the compute manager. The tight coupling is not great...
1574             instance.system_metadata['clean_attempts'] = str(attempts + 1)
1575             if success:
1576                 instance.cleaned = True
1577             try:
1578                 instance.save()
1579             except exception.InstanceNotFound:
1580                 pass
1581 
1582         if cleanup_instance_disks:
1583             crypto.delete_vtpm_secret(context, instance)
1584 
1585         self._undefine_domain(instance)
1586 
1587     def cleanup_lingering_instance_resources(self, instance):
1588         # zero the data on backend pmem device, if fails
1589         # it will raise an exception
1590         vpmems = self._get_vpmems(instance)
1591         if vpmems:
1592             self._cleanup_vpmems(vpmems)
1593 
1594     def _cleanup_vpmems(self, vpmems):
1595         for vpmem in vpmems:
1596             try:
1597                 nova.privsep.libvirt.cleanup_vpmem(vpmem.devpath)
1598             except Exception as e:
1599                 raise exception.VPMEMCleanupFailed(dev=vpmem.devpath,
1600                                                    error=e)
1601 
1602     def _get_serial_ports_from_guest(self, guest, mode=None):
1603         """Returns an iterator over serial port(s) configured on guest.
1604 
1605         :param mode: Should be a value in (None, bind, connect)
1606         """
1607         xml = guest.get_xml_desc()
1608         tree = etree.fromstring(xml)
1609 
1610         # The 'serial' device is the base for x86 platforms. Other platforms
1611         # (e.g. kvm on system z = S390X) can only use 'console' devices.
1612         xpath_mode = "[@mode='%s']" % mode if mode else ""
1613         serial_tcp = "./devices/serial[@type='tcp']/source" + xpath_mode
1614         console_tcp = "./devices/console[@type='tcp']/source" + xpath_mode
1615 
1616         tcp_devices = tree.findall(serial_tcp)
1617         if len(tcp_devices) == 0:
1618             tcp_devices = tree.findall(console_tcp)
1619         for source in tcp_devices:
1620             yield (source.get("host"), int(source.get("service")))
1621 
1622     def _get_scsi_controller_next_unit(self, guest):
1623         """Returns the max disk unit used by scsi controller"""
1624         xml = guest.get_xml_desc()
1625         tree = etree.fromstring(xml)
1626         addrs = "./devices/disk[target/@bus='scsi']/address[@type='drive']"
1627 
1628         ret = []
1629         for obj in tree.xpath(addrs):
1630             ret.append(int(obj.get('unit', 0)))
1631         return max(ret) + 1 if ret else 0
1632 
1633     def _cleanup_rbd(self, instance):
1634         # NOTE(nic): On revert_resize, the cleanup steps for the root
1635         # volume are handled with an "rbd snap rollback" command,
1636         # and none of this is needed (and is, in fact, harmful) so
1637         # filter out non-ephemerals from the list
1638         if instance.task_state == task_states.RESIZE_REVERTING:
1639             filter_fn = lambda disk: (disk.startswith(instance.uuid) and
1640                                       disk.endswith('disk.local'))
1641         else:
1642             filter_fn = lambda disk: disk.startswith(instance.uuid)
1643         rbd_utils.RBDDriver().cleanup_volumes(filter_fn)
1644 
1645     def _cleanup_lvm(self, instance, block_device_info):
1646         """Delete all LVM disks for given instance object."""
1647         if instance.get('ephemeral_key_uuid') is not None:
1648             # detach encrypted volumes
1649             disks = self._get_instance_disk_info(instance, block_device_info)
1650             for disk in disks:
1651                 if dmcrypt.is_encrypted(disk['path']):
1652                     dmcrypt.delete_volume(disk['path'])
1653 
1654         disks = self._lvm_disks(instance)
1655         if disks:
1656             lvm.remove_volumes(disks)
1657 
1658     def _lvm_disks(self, instance):
1659         """Returns all LVM disks for given instance object."""
1660         if CONF.libvirt.images_volume_group:
1661             vg = os.path.join('/dev', CONF.libvirt.images_volume_group)
1662             if not os.path.exists(vg):
1663                 return []
1664             pattern = '%s_' % instance.uuid
1665 
1666             def belongs_to_instance(disk):
1667                 return disk.startswith(pattern)
1668 
1669             def fullpath(name):
1670                 return os.path.join(vg, name)
1671 
1672             logical_volumes = lvm.list_volumes(vg)
1673 
1674             disks = [fullpath(disk) for disk in logical_volumes
1675                      if belongs_to_instance(disk)]
1676             return disks
1677         return []
1678 
1679     def get_volume_connector(self, instance):
1680         root_helper = utils.get_root_helper()
1681         return connector.get_connector_properties(
1682             root_helper, CONF.my_block_storage_ip,
1683             CONF.libvirt.volume_use_multipath,
1684             enforce_multipath=True,
1685             host=CONF.host)
1686 
1687     def _cleanup_resize_vtpm(
1688         self,
1689         context: nova_context.RequestContext,
1690         instance: 'objects.Instance',
1691     ) -> None:
1692         """Handle vTPM when confirming a migration or resize.
1693 
1694         If the old flavor have vTPM and the new one doesn't, there are keys to
1695         be deleted.
1696         """
1697         old_vtpm_config = hardware.get_vtpm_constraint(
1698             instance.old_flavor, instance.image_meta)
1699         new_vtpm_config = hardware.get_vtpm_constraint(
1700             instance.new_flavor, instance.image_meta)
1701 
1702         if old_vtpm_config and not new_vtpm_config:
1703             # the instance no longer cares for its vTPM so delete the related
1704             # secret; the deletion of the instance directory and undefining of
1705             # the domain will take care of the TPM files themselves
1706             LOG.info('New flavor no longer requests vTPM; deleting secret.')
1707             crypto.delete_vtpm_secret(context, instance)
1708 
1709     # TODO(stephenfin): Fold this back into its only caller, cleanup_resize
1710     def _cleanup_resize(self, context, instance, network_info):
1711         inst_base = libvirt_utils.get_instance_path(instance)
1712         target = inst_base + '_resize'
1713 
1714         # zero the data on backend old pmem device
1715         vpmems = self._get_vpmems(instance, prefix='old')
1716         if vpmems:
1717             self._cleanup_vpmems(vpmems)
1718 
1719         # Remove any old vTPM data, if necessary
1720         self._cleanup_resize_vtpm(context, instance)
1721 
1722         # Deletion can fail over NFS, so retry the deletion as required.
1723         # Set maximum attempt as 5, most test can remove the directory
1724         # for the second time.
1725         attempts = 0
1726         while(os.path.exists(target) and attempts < 5):
1727             shutil.rmtree(target, ignore_errors=True)
1728             if os.path.exists(target):
1729                 time.sleep(random.randint(20, 200) / 100.0)
1730             attempts += 1
1731 
1732         # NOTE(mriedem): Some image backends will recreate the instance path
1733         # and disk.info during init, and all we need the root disk for
1734         # here is removing cloned snapshots which is backend-specific, so
1735         # check that first before initializing the image backend object. If
1736         # there is ever an image type that supports clone *and* re-creates
1737         # the instance directory and disk.info on init, this condition will
1738         # need to be re-visited to make sure that backend doesn't re-create
1739         # the disk. Refer to bugs: 1666831 1728603 1769131
1740         if self.image_backend.backend(CONF.libvirt.images_type).SUPPORTS_CLONE:
1741             root_disk = self.image_backend.by_name(instance, 'disk')
1742             if root_disk.exists():
1743                 root_disk.remove_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME)
1744 
1745         if instance.host != CONF.host:
1746             self._undefine_domain(instance)
1747             # TODO(sean-k-mooney): remove this call to unplug_vifs after
1748             # Wallaby is released. VIFs are now unplugged in resize_instance.
1749             try:
1750                 self.unplug_vifs(instance, network_info)
1751             except exception.InternalError as e:
1752                 LOG.debug(e, instance=instance)
1753 
1754     def _get_volume_driver(
1755         self, connection_info: ty.Dict[str, ty.Any]
1756     ) -> 'volume.LibvirtBaseVolumeDriver':
1757         """Fetch the nova.virt.libvirt.volume driver
1758 
1759         Based on the provided connection_info return a nova.virt.libvirt.volume
1760         driver. This will call out to os-brick to construct an connector and
1761         check if the connector is valid on the underlying host.
1762 
1763         :param connection_info: The connection_info associated with the volume
1764         :raises: VolumeDriverNotFound if no driver is found or if the host
1765             doesn't support the requested driver. This retains legacy behaviour
1766             when only supported drivers were loaded on startup leading to a
1767             VolumeDriverNotFound being raised later if an invalid driver was
1768             requested.
1769         """
1770         driver_type = connection_info.get('driver_volume_type')
1771 
1772         # If the driver_type isn't listed in the supported type list fail
1773         if driver_type not in VOLUME_DRIVERS:
1774             raise exception.VolumeDriverNotFound(driver_type=driver_type)
1775 
1776         # Return the cached driver
1777         if driver_type in self.volume_drivers:
1778             return self.volume_drivers.get(driver_type)
1779 
1780         @utils.synchronized('cache_volume_driver')
1781         def _cache_volume_driver(driver_type):
1782             # Check if another request cached the driver while we waited
1783             if driver_type in self.volume_drivers:
1784                 return self.volume_drivers.get(driver_type)
1785 
1786             try:
1787                 driver_class = importutils.import_class(
1788                     VOLUME_DRIVERS.get(driver_type))
1789                 self.volume_drivers[driver_type] = driver_class(self._host)
1790                 return self.volume_drivers.get(driver_type)
1791             except brick_exception.InvalidConnectorProtocol:
1792                 LOG.debug('Unable to load volume driver %s. It is not '
1793                           'supported on this host.', driver_type)
1794                 # NOTE(lyarwood): This exception is a subclass of
1795                 # VolumeDriverNotFound to ensure no callers have to change
1796                 # their error handling code after the move to on-demand loading
1797                 # of the volume drivers and associated os-brick connectors.
1798                 raise exception.VolumeDriverNotSupported(
1799                     volume_driver=VOLUME_DRIVERS.get(driver_type))
1800 
1801         # Cache the volume driver if it hasn't already been
1802         return _cache_volume_driver(driver_type)
1803 
1804     def _connect_volume(self, context, connection_info, instance,
1805                         encryption=None):
1806         vol_driver = self._get_volume_driver(connection_info)
1807         vol_driver.connect_volume(connection_info, instance)
1808         try:
1809             self._attach_encryptor(context, connection_info, encryption)
1810         except Exception:
1811             # Encryption failed so rollback the volume connection.
1812             with excutils.save_and_reraise_exception(logger=LOG):
1813                 LOG.exception("Failure attaching encryptor; rolling back "
1814                               "volume connection", instance=instance)
1815                 vol_driver.disconnect_volume(connection_info, instance)
1816 
1817     def _should_disconnect_target(self, context, instance, multiattach,
1818                                   vol_driver, volume_id):
1819         # NOTE(jdg): Multiattach is a special case (not to be confused
1820         # with shared_targets). With multiattach we may have a single volume
1821         # attached multiple times to *this* compute node (ie Server-1 and
1822         # Server-2).  So, if we receive a call to delete the attachment for
1823         # Server-1 we need to take special care to make sure that the Volume
1824         # isn't also attached to another Server on this Node.  Otherwise we
1825         # will indiscriminantly delete the connection for all Server and that's
1826         # no good.  So check if it's attached multiple times on this node
1827         # if it is we skip the call to brick to delete the connection.
1828         if not multiattach:
1829             return True
1830 
1831         # NOTE(deiter): Volume drivers using _HostMountStateManager are another
1832         # special case. _HostMountStateManager ensures that the compute node
1833         # only attempts to mount a single mountpoint in use by multiple
1834         # attachments once, and that it is not unmounted until it is no longer
1835         # in use by any attachments. So we can skip the multiattach check for
1836         # volume drivers that based on LibvirtMountedFileSystemVolumeDriver.
1837         if isinstance(vol_driver, fs.LibvirtMountedFileSystemVolumeDriver):
1838             return True
1839 
1840         connection_count = 0
1841         volume = self._volume_api.get(context, volume_id)
1842         attachments = volume.get('attachments', {})
1843         if len(attachments) > 1:
1844             # First we get a list of all Server UUID's associated with
1845             # this Host (Compute Node).  We're going to use this to
1846             # determine if the Volume being detached is also in-use by
1847             # another Server on this Host, ie just check to see if more
1848             # than one attachment.server_id for this volume is in our
1849             # list of Server UUID's for this Host
1850             servers_this_host = objects.InstanceList.get_uuids_by_host(
1851                 context, instance.host)
1852 
1853             # NOTE(jdg): nova.volume.cinder translates the
1854             # volume['attachments'] response into a dict which includes
1855             # the Server UUID as the key, so we're using that
1856             # here to check against our server_this_host list
1857             for server_id, data in attachments.items():
1858                 if server_id in servers_this_host:
1859                     connection_count += 1
1860         return (False if connection_count > 1 else True)
1861 
1862     def _disconnect_volume(self, context, connection_info, instance,
1863                            encryption=None, destroy_secrets=True):
1864         self._detach_encryptor(
1865             context,
1866             connection_info,
1867             encryption=encryption,
1868             destroy_secrets=destroy_secrets
1869         )
1870         vol_driver = self._get_volume_driver(connection_info)
1871         volume_id = driver_block_device.get_volume_id(connection_info)
1872         multiattach = connection_info.get('multiattach', False)
1873         if self._should_disconnect_target(
1874                 context, instance, multiattach, vol_driver, volume_id):
1875             vol_driver.disconnect_volume(connection_info, instance)
1876         else:
1877             LOG.info('Detected multiple connections on this host for '
1878                      'volume: %(volume)s, skipping target disconnect.',
1879                      {'volume': volume_id})
1880 
1881     def _extend_volume(self, connection_info, instance, requested_size):
1882         vol_driver = self._get_volume_driver(connection_info)
1883         return vol_driver.extend_volume(connection_info, instance,
1884                                         requested_size)
1885 
1886     def _allow_native_luksv1(self, encryption=None):
1887         """Check if QEMU's native LUKSv1 decryption should be used.
1888         """
1889         # NOTE(lyarwood): Ensure the LUKSv1 provider is used.
1890         provider = None
1891         if encryption:
1892             provider = encryption.get('provider', None)
1893         if provider in encryptors.LEGACY_PROVIDER_CLASS_TO_FORMAT_MAP:
1894             provider = encryptors.LEGACY_PROVIDER_CLASS_TO_FORMAT_MAP[provider]
1895         return provider == encryptors.LUKS
1896 
1897     def _get_volume_config(self, instance, connection_info, disk_info):
1898         vol_driver = self._get_volume_driver(connection_info)
1899         conf = vol_driver.get_config(connection_info, disk_info)
1900 
1901         if self._sev_enabled(instance.flavor, instance.image_meta):
1902             designer.set_driver_iommu_for_device(conf)
1903 
1904         self._set_cache_mode(conf)
1905         return conf
1906 
1907     def _get_volume_encryptor(self, connection_info, encryption):
1908         root_helper = utils.get_root_helper()
1909         return encryptors.get_volume_encryptor(root_helper=root_helper,
1910                                                keymgr=key_manager.API(CONF),
1911                                                connection_info=connection_info,
1912                                                **encryption)
1913 
1914     def _get_volume_encryption(self, context, connection_info):
1915         """Get the encryption metadata dict if it is not provided
1916         """
1917         encryption = {}
1918         volume_id = driver_block_device.get_volume_id(connection_info)
1919         if volume_id:
1920             encryption = encryptors.get_encryption_metadata(context,
1921                             self._volume_api, volume_id, connection_info)
1922         return encryption
1923 
1924     def _attach_encryptor(self, context, connection_info, encryption):
1925         """Attach the frontend encryptor if one is required by the volume.
1926 
1927         The request context is only used when an encryption metadata dict is
1928         not provided. The encryption metadata dict being populated is then used
1929         to determine if an attempt to attach the encryptor should be made.
1930 
1931         """
1932         # NOTE(lyarwood): Skip any attempt to fetch encryption metadata or the
1933         # actual passphrase from the key manager if a libvirt secert already
1934         # exists locally for the volume. This suggests that the instance was
1935         # only powered off or the underlying host rebooted.
1936         volume_id = driver_block_device.get_volume_id(connection_info)
1937         if self._host.find_secret('volume', volume_id):
1938             LOG.debug("A libvirt secret for volume %s has been found on the "
1939                       "host, skipping any attempt to create another or attach "
1940                       "an os-brick encryptor.", volume_id)
1941             return
1942 
1943         if encryption is None:
1944             encryption = self._get_volume_encryption(context, connection_info)
1945 
1946         if encryption and self._allow_native_luksv1(encryption=encryption):
1947             # NOTE(lyarwood): Fetch the associated key for the volume and
1948             # decode the passphrase from the key.
1949             # FIXME(lyarwood): c-vol currently creates symmetric keys for use
1950             # with volumes, leading to the binary to hex to string conversion
1951             # below.
1952             keymgr = key_manager.API(CONF)
1953             key = keymgr.get(context, encryption['encryption_key_id'])
1954             key_encoded = key.get_encoded()
1955             passphrase = binascii.hexlify(key_encoded).decode('utf-8')
1956 
1957             # NOTE(lyarwood): Retain the behaviour of the original os-brick
1958             # encryptors and format any volume that does not identify as
1959             # encrypted with LUKS.
1960             # FIXME(lyarwood): Remove this once c-vol correctly formats
1961             # encrypted volumes during their initial creation:
1962             # https://bugs.launchpad.net/cinder/+bug/1739442
1963             device_path = connection_info.get('data').get('device_path')
1964             if device_path:
1965                 root_helper = utils.get_root_helper()
1966                 if not luks_encryptor.is_luks(root_helper, device_path):
1967                     encryptor = self._get_volume_encryptor(connection_info,
1968                                                            encryption)
1969                     encryptor._format_volume(passphrase, **encryption)
1970 
1971             # NOTE(lyarwood): Store the passphrase as a libvirt secret locally
1972             # on the compute node. This secret is used later when generating
1973             # the volume config.
1974             self._host.create_secret('volume', volume_id, password=passphrase)
1975         elif encryption:
1976             encryptor = self._get_volume_encryptor(connection_info,
1977                                                    encryption)
1978             encryptor.attach_volume(context, **encryption)
1979 
1980     def _detach_encryptor(self, context, connection_info, encryption,
1981                           destroy_secrets=True):
1982         """Detach the frontend encryptor if one is required by the volume.
1983 
1984         The request context is only used when an encryption metadata dict is
1985         not provided. The encryption metadata dict being populated is then used
1986         to determine if an attempt to detach the encryptor should be made.
1987 
1988         If native LUKS decryption is enabled then delete previously created
1989         Libvirt volume secret from the host.
1990         """
1991         volume_id = driver_block_device.get_volume_id(connection_info)
1992         if volume_id and self._host.find_secret('volume', volume_id):
1993             if not destroy_secrets:
1994                 LOG.debug("Skipping volume secret destruction")
1995                 return
1996             return self._host.delete_secret('volume', volume_id)
1997 
1998         if encryption is None:
1999             encryption = self._get_volume_encryption(context, connection_info)
2000 
2001         # NOTE(lyarwood): Handle bugs #1821696 and #1917619 by avoiding the use
2002         # of the os-brick encryptors if we don't have a device_path. The lack
2003         # of a device_path here suggests the volume was natively attached to
2004         # QEMU anyway as volumes without a device_path are not supported by
2005         # os-brick encryptors. For volumes with a device_path the calls to
2006         # the os-brick encryptors are safe as they are actually idempotent,
2007         # ignoring any failures caused by the volumes actually being natively
2008         # attached previously.
2009         if (encryption and connection_info['data'].get('device_path') is None):
2010             return
2011 
2012         if encryption:
2013             encryptor = self._get_volume_encryptor(connection_info,
2014                                                    encryption)
2015             encryptor.detach_volume(**encryption)
2016 
2017     def _check_discard_for_attach_volume(self, conf, instance):
2018         """Perform some checks for volumes configured for discard support.
2019 
2020         If discard is configured for the volume, and the guest is using a
2021         configuration known to not work, we will log a message explaining
2022         the reason why.
2023         """
2024         if conf.driver_discard == 'unmap' and conf.target_bus == 'virtio':
2025             LOG.debug('Attempting to attach volume %(id)s with discard '
2026                       'support enabled to an instance using an '
2027                       'unsupported configuration. target_bus = '
2028                       '%(bus)s. Trim commands will not be issued to '
2029                       'the storage device.',
2030                       {'bus': conf.target_bus,
2031                        'id': conf.serial},
2032                       instance=instance)
2033 
2034     def attach_volume(self, context, connection_info, instance, mountpoint,
2035                       disk_bus=None, device_type=None, encryption=None):
2036         guest = self._host.get_guest(instance)
2037 
2038         disk_dev = mountpoint.rpartition("/")[2]
2039         bdm = {
2040             'device_name': disk_dev,
2041             'disk_bus': disk_bus,
2042             'device_type': device_type}
2043 
2044         # Note(cfb): If the volume has a custom block size, check that that we
2045         # are using QEMU/KVM. The presence of a block size is considered
2046         # mandatory by cinder so we fail if we can't honor the request.
2047         data = {}
2048         if ('data' in connection_info):
2049             data = connection_info['data']
2050         if ('logical_block_size' in data or 'physical_block_size' in data):
2051             if CONF.libvirt.virt_type not in ["kvm", "qemu"]:
2052                 msg = _("Volume sets block size, but the current "
2053                         "libvirt hypervisor '%s' does not support custom "
2054                         "block size") % CONF.libvirt.virt_type
2055                 raise exception.InvalidHypervisorType(msg)
2056 
2057         self._connect_volume(context, connection_info, instance,
2058                              encryption=encryption)
2059         disk_info = blockinfo.get_info_from_bdm(
2060             instance, CONF.libvirt.virt_type, instance.image_meta, bdm)
2061         if disk_info['bus'] == 'scsi':
2062             disk_info['unit'] = self._get_scsi_controller_next_unit(guest)
2063 
2064         conf = self._get_volume_config(instance, connection_info, disk_info)
2065 
2066         self._check_discard_for_attach_volume(conf, instance)
2067 
2068         try:
2069             state = guest.get_power_state(self._host)
2070             live = state in (power_state.RUNNING, power_state.PAUSED)
2071 
2072             guest.attach_device(conf, persistent=True, live=live)
2073             # NOTE(artom) If we're attaching with a device role tag, we need to
2074             # rebuild device_metadata. If we're attaching without a role
2075             # tag, we're rebuilding it here needlessly anyways. This isn't a
2076             # massive deal, and it helps reduce code complexity by not having
2077             # to indicate to the virt driver that the attach is tagged. The
2078             # really important optimization of not calling the database unless
2079             # device_metadata has actually changed is done for us by
2080             # instance.save().
2081             instance.device_metadata = self._build_device_metadata(
2082                 context, instance)
2083             instance.save()
2084         except Exception:
2085             LOG.exception('Failed to attach volume at mountpoint: %s',
2086                           mountpoint, instance=instance)
2087             with excutils.save_and_reraise_exception():
2088                 self._disconnect_volume(context, connection_info, instance,
2089                                         encryption=encryption)
2090 
2091     def _swap_volume(self, guest, disk_dev, conf, resize_to, hw_firmware_type):
2092         """Swap existing disk with a new block device.
2093 
2094         Call virDomainBlockRebase or virDomainBlockCopy with Libvirt >= 6.0.0
2095         to copy and then pivot to a new volume.
2096 
2097         :param: guest: Guest object representing the guest domain
2098         :param: disk_dev: Device within the domain that is being swapped
2099         :param: conf: LibvirtConfigGuestDisk object representing the new volume
2100         :param: resize_to: Size of the dst volume, 0 if the same as the src
2101         :param: hw_firmware_type: fields.FirmwareType if set in the imagemeta
2102         """
2103         dev = guest.get_block_device(disk_dev)
2104 
2105         # Save a copy of the domain's persistent XML file. We'll use this
2106         # to redefine the domain if anything fails during the volume swap.
2107         xml = guest.get_xml_desc(dump_inactive=True, dump_sensitive=True)
2108 
2109         # Abort is an idempotent operation, so make sure any block
2110         # jobs which may have failed are ended.
2111         try:
2112             dev.abort_job()
2113         except Exception:
2114             pass
2115 
2116         try:
2117             # NOTE (rmk): virDomainBlockRebase and virDomainBlockCopy cannot be
2118             # executed on persistent domains, so we need to temporarily
2119             # undefine it. If any part of this block fails, the domain is
2120             # re-defined regardless.
2121             if guest.has_persistent_configuration():
2122                 support_uefi = self._check_uefi_support(hw_firmware_type)
2123                 guest.delete_configuration(support_uefi)
2124 
2125             try:
2126                 dev.copy(conf.to_xml(), reuse_ext=True)
2127 
2128                 while not dev.is_job_complete():
2129                     time.sleep(0.5)
2130 
2131                 dev.abort_job(pivot=True)
2132 
2133             except Exception as exc:
2134                 # NOTE(lyarwood): conf.source_path is not set for RBD disks so
2135                 # fallback to conf.target_dev when None.
2136                 new_path = conf.source_path or conf.target_dev
2137                 old_path = disk_dev
2138                 LOG.exception("Failure rebasing volume %(new_path)s on "
2139                     "%(old_path)s.", {'new_path': new_path,
2140                                       'old_path': old_path})
2141                 raise exception.VolumeRebaseFailed(reason=str(exc))
2142 
2143             if resize_to:
2144                 dev.resize(resize_to * units.Gi)
2145 
2146             # Make sure we will redefine the domain using the updated
2147             # configuration after the volume was swapped. The dump_inactive
2148             # keyword arg controls whether we pull the inactive (persistent)
2149             # or active (live) config from the domain. We want to pull the
2150             # live config after the volume was updated to use when we redefine
2151             # the domain.
2152             xml = guest.get_xml_desc(dump_inactive=False, dump_sensitive=True)
2153         finally:
2154             self._host.write_instance_config(xml)
2155 
2156     def swap_volume(self, context, old_connection_info,
2157                     new_connection_info, instance, mountpoint, resize_to):
2158 
2159         # NOTE(lyarwood): https://bugzilla.redhat.com/show_bug.cgi?id=760547
2160         old_encrypt = self._get_volume_encryption(context, old_connection_info)
2161         new_encrypt = self._get_volume_encryption(context, new_connection_info)
2162         if ((old_encrypt and self._allow_native_luksv1(old_encrypt)) or
2163             (new_encrypt and self._allow_native_luksv1(new_encrypt))):
2164             raise NotImplementedError(_("Swap volume is not supported for "
2165                 "encrypted volumes when native LUKS decryption is enabled."))
2166 
2167         guest = self._host.get_guest(instance)
2168 
2169         disk_dev = mountpoint.rpartition("/")[2]
2170         if not guest.get_disk(disk_dev):
2171             raise exception.DiskNotFound(location=disk_dev)
2172         disk_info = {
2173             'dev': disk_dev,
2174             'bus': blockinfo.get_disk_bus_for_disk_dev(
2175                 CONF.libvirt.virt_type, disk_dev),
2176             'type': 'disk',
2177             }
2178         # NOTE (lyarwood): new_connection_info will be modified by the
2179         # following _connect_volume call down into the volume drivers. The
2180         # majority of the volume drivers will add a device_path that is in turn
2181         # used by _get_volume_config to set the source_path of the
2182         # LibvirtConfigGuestDisk object it returns. We do not explicitly save
2183         # this to the BDM here as the upper compute swap_volume method will
2184         # eventually do this for us.
2185         self._connect_volume(context, new_connection_info, instance)
2186         conf = self._get_volume_config(
2187             instance, new_connection_info, disk_info)
2188         hw_firmware_type = instance.image_meta.properties.get(
2189             'hw_firmware_type')
2190 
2191         try:
2192             self._swap_volume(guest, disk_dev, conf,
2193                               resize_to, hw_firmware_type)
2194         except exception.VolumeRebaseFailed:
2195             with excutils.save_and_reraise_exception():
2196                 self._disconnect_volume(context, new_connection_info, instance)
2197 
2198         self._disconnect_volume(context, old_connection_info, instance)
2199 
2200     def _get_existing_domain_xml(self, instance, network_info,
2201                                  block_device_info=None):
2202         try:
2203             guest = self._host.get_guest(instance)
2204             xml = guest.get_xml_desc()
2205         except exception.InstanceNotFound:
2206             disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
2207                                                 instance,
2208                                                 instance.image_meta,
2209                                                 block_device_info)
2210             xml = self._get_guest_xml(nova_context.get_admin_context(),
2211                                       instance, network_info, disk_info,
2212                                       instance.image_meta,
2213                                       block_device_info=block_device_info)
2214         return xml
2215 
2216     def emit_event(self, event: virtevent.InstanceEvent) -> None:
2217         """Handles libvirt specific events locally and dispatches the rest to
2218         the compute manager.
2219         """
2220         if isinstance(event, libvirtevent.LibvirtEvent):
2221             # These are libvirt specific events handled here on the driver
2222             # level instead of propagating them to the compute manager level
2223             if isinstance(event, libvirtevent.DeviceEvent):
2224                 had_clients = self._device_event_handler.notify_waiters(event)
2225 
2226                 if had_clients:
2227                     LOG.debug(
2228                         "Received event %s from libvirt while the driver is "
2229                         "waiting for it; dispatched.",
2230                         event,
2231                     )
2232                 else:
2233                     LOG.warning(
2234                         "Received event %s from libvirt but the driver is not "
2235                         "waiting for it; ignored.",
2236                         event,
2237                     )
2238             else:
2239                 LOG.debug(
2240                     "Received event %s from libvirt but no handler is "
2241                     "implemented for it in the libvirt driver so it is "
2242                     "ignored", event)
2243         else:
2244             # Let the generic driver code dispatch the event to the compute
2245             # manager
2246             super().emit_event(event)
2247 
2248     def _detach_with_retry(
2249         self,
2250         guest: libvirt_guest.Guest,
2251         instance_uuid: str,
2252         # to properly typehint this param we would need typing.Protocol but
2253         # that is only available since python 3.8
2254         get_device_conf_func: ty.Callable,
2255         device_name: str,
2256     ) -> None:
2257         """Detaches a device from the guest
2258 
2259         If the guest is a running state then the detach is performed on both
2260         the persistent and live domains.
2261 
2262         In case of live detach this call will wait for the libvirt event
2263         signalling the end of the detach process.
2264 
2265         If the live detach times out then it will retry the detach. Detach from
2266         the persistent config is not retried as it is:
2267 
2268         * synchronous and no event is sent from libvirt
2269         * it is always expected to succeed if the device is in the domain
2270           config
2271 
2272         :param guest: the guest we are detach the device from
2273         :param instance_uuid: the UUID of the instance we are detaching the
2274             device from
2275         :param get_device_conf_func: function which returns the configuration
2276             for device from the domain, having one optional boolean parameter
2277             `from_persistent_config` to select which domain config to query
2278         :param device_name: This is the name of the device used solely for
2279             error messages. Note that it is not the same as the device alias
2280             used by libvirt to identify the device.
2281         :raises exception.DeviceNotFound: if the device does not exist in the
2282             domain even before we try to detach or if libvirt reported that the
2283             device is missing from the domain synchronously.
2284         :raises exception.DeviceDetachFailed: if libvirt reported error during
2285             detaching from the live domain or we timed out waiting for libvirt
2286             events and run out of retries
2287         :raises libvirt.libvirtError: for any other errors reported by libvirt
2288             synchronously.
2289         """
2290         state = guest.get_power_state(self._host)
2291         live = state in (power_state.RUNNING, power_state.PAUSED)
2292 
2293         persistent = guest.has_persistent_configuration()
2294 
2295         if not persistent and not live:
2296             # nothing to do
2297             return
2298 
2299         persistent_dev = None
2300         if persistent:
2301             persistent_dev = get_device_conf_func(from_persistent_config=True)
2302 
2303         live_dev = None
2304         if live:
2305             live_dev = get_device_conf_func()
2306 
2307         # didn't find the device in either domain
2308         if persistent_dev is None and live_dev is None:
2309             raise exception.DeviceNotFound(device=device_name)
2310 
2311         if persistent_dev:
2312             try:
2313                 self._detach_from_persistent(
2314                     guest, instance_uuid, persistent_dev, get_device_conf_func,
2315                     device_name)
2316             except exception.DeviceNotFound:
2317                 if live_dev:
2318                     # ignore the error so that we can do the live detach
2319                     LOG.warning(
2320                         'Libvirt reported sync error while detaching '
2321                         'device %s from instance %s from the persistent '
2322                         'domain config. Ignoring the error to proceed with '
2323                         'live detach as the device exists in the live domain.',
2324                         device_name, instance_uuid)
2325                 else:
2326                     # if only persistent detach was requested then give up
2327                     raise
2328 
2329         if live_dev:
2330             self._detach_from_live_with_retry(
2331                 guest, instance_uuid, live_dev, get_device_conf_func,
2332                 device_name)
2333 
2334     def _detach_from_persistent(
2335         self,
2336         guest: libvirt_guest.Guest,
2337         instance_uuid: str,
2338         persistent_dev: ty.Union[
2339             vconfig.LibvirtConfigGuestDisk,
2340             vconfig.LibvirtConfigGuestInterface],
2341         get_device_conf_func,
2342         device_name: str,
2343     ):
2344         LOG.debug(
2345             'Attempting to detach device %s from instance %s from '
2346             'the persistent domain config.', device_name, instance_uuid)
2347 
2348         self._detach_sync(
2349             persistent_dev, guest, instance_uuid, device_name,
2350             persistent=True, live=False)
2351 
2352         # make sure the dev is really gone
2353         persistent_dev = get_device_conf_func(
2354             from_persistent_config=True)
2355         if not persistent_dev:
2356             LOG.info(
2357                 'Successfully detached device %s from instance %s '
2358                 'from the persistent domain config.',
2359                 device_name, instance_uuid)
2360         else:
2361             # Based on the libvirt devs this should never happen
2362             LOG.warning(
2363                 'Failed to detach device %s from instance %s '
2364                 'from the persistent domain config. Libvirt did not '
2365                 'report any error but the device is still in the '
2366                 'config.', device_name, instance_uuid)
2367 
2368     def _detach_from_live_with_retry(
2369         self,
2370         guest: libvirt_guest.Guest,
2371         instance_uuid: str,
2372         live_dev: ty.Union[
2373             vconfig.LibvirtConfigGuestDisk,
2374             vconfig.LibvirtConfigGuestInterface],
2375         get_device_conf_func,
2376         device_name: str,
2377     ):
2378         max_attempts = CONF.libvirt.device_detach_attempts
2379         for attempt in range(max_attempts):
2380             LOG.debug(
2381                 '(%s/%s): Attempting to detach device %s with device '
2382                 'alias %s from instance %s from the live domain config.',
2383                 attempt + 1, max_attempts, device_name, live_dev.alias,
2384                 instance_uuid)
2385 
2386             self._detach_from_live_and_wait_for_event(
2387                 live_dev, guest, instance_uuid, device_name)
2388 
2389             # make sure the dev is really gone
2390             live_dev = get_device_conf_func()
2391             if not live_dev:
2392                 LOG.info(
2393                     'Successfully detached device %s from instance %s '
2394                     'from the live domain config.', device_name, instance_uuid)
2395                 # we are done
2396                 return
2397 
2398             LOG.debug(
2399                 'Failed to detach device %s with device alias %s from '
2400                 'instance %s from the live domain config. Libvirt did not '
2401                 'report any error but the device is still in the config.',
2402                 device_name, live_dev.alias, instance_uuid)
2403 
2404         msg = (
2405             'Run out of retry while detaching device %s with device '
2406             'alias %s from instance %s from the live domain config. '
2407             'Device is still attached to the guest.')
2408         LOG.error(msg, device_name, live_dev.alias, instance_uuid)
2409         raise exception.DeviceDetachFailed(
2410             device=device_name,
2411             reason=msg % (device_name, live_dev.alias, instance_uuid))
2412 
2413     def _detach_from_live_and_wait_for_event(
2414         self,
2415         dev: ty.Union[
2416             vconfig.LibvirtConfigGuestDisk,
2417             vconfig.LibvirtConfigGuestInterface],
2418         guest: libvirt_guest.Guest,
2419         instance_uuid: str,
2420         device_name: str,
2421     ) -> None:
2422         """Detaches a device from the live config of the guest and waits for
2423         the libvirt event singling the finish of the detach.
2424 
2425         :param dev: the device configuration to be detached
2426         :param guest: the guest we are detach the device from
2427         :param instance_uuid: the UUID of the instance we are detaching the
2428             device from
2429         :param device_name: This is the name of the device used solely for
2430             error messages.
2431         :raises exception.DeviceNotFound: if libvirt reported that the device
2432             is missing from the domain synchronously.
2433         :raises libvirt.libvirtError: for any other errors reported by libvirt
2434             synchronously.
2435         :raises DeviceDetachFailed: if libvirt sent DeviceRemovalFailedEvent
2436         """
2437         # So we will issue an detach to libvirt and we will wait for an
2438         # event from libvirt about the result. We need to set up the event
2439         # handling before the detach to avoid missing the event if libvirt
2440         # is really fast
2441         # NOTE(gibi): we need to use the alias name of the device as that
2442         # is what libvirt will send back to us in the event
2443         waiter = self._device_event_handler.create_waiter(
2444             instance_uuid, dev.alias,
2445             {libvirtevent.DeviceRemovedEvent,
2446              libvirtevent.DeviceRemovalFailedEvent})
2447         try:
2448             self._detach_sync(
2449                 dev, guest, instance_uuid, device_name, persistent=False,
2450                 live=True)
2451         except Exception:
2452             # clean up the libvirt event handler as we failed synchronously
2453             self._device_event_handler.delete_waiter(waiter)
2454             raise
2455 
2456         LOG.debug(
2457             'Start waiting for the detach event from libvirt for '
2458             'device %s with device alias %s for instance %s',
2459             device_name, dev.alias, instance_uuid)
2460         # We issued the detach without any exception so we can wait for
2461         # a libvirt event to arrive to notify us about the result
2462         # NOTE(gibi): we expect that this call will be unblocked by an
2463         # incoming libvirt DeviceRemovedEvent or DeviceRemovalFailedEvent
2464         event = self._device_event_handler.wait(
2465             waiter, timeout=CONF.libvirt.device_detach_timeout)
2466 
2467         if not event:
2468             # This should not happen based on information from the libvirt
2469             # developers. But it does at least during the cleanup of the
2470             # tempest test case
2471             # ServerRescueNegativeTestJSON.test_rescued_vm_detach_volume
2472             # Log a warning and let the upper layer detect that the device is
2473             # still attached and retry
2474             LOG.error(
2475                 'Waiting for libvirt event about the detach of '
2476                 'device %s with device alias %s from instance %s is timed '
2477                 'out.', device_name, dev.alias, instance_uuid)
2478 
2479         if isinstance(event, libvirtevent.DeviceRemovalFailedEvent):
2480             # Based on the libvirt developers this signals a permanent failure
2481             LOG.error(
2482                 'Received DeviceRemovalFailedEvent from libvirt for the '
2483                 'detach of device %s with device alias %s from instance %s ',
2484                 device_name, dev.alias, instance_uuid)
2485             raise exception.DeviceDetachFailed(
2486                 device=device_name,
2487                 reason="DeviceRemovalFailedEvent received from libvirt")
2488 
2489     @staticmethod
2490     def _detach_sync(
2491         dev: ty.Union[
2492             vconfig.LibvirtConfigGuestDisk,
2493             vconfig.LibvirtConfigGuestInterface],
2494         guest: libvirt_guest.Guest,
2495         instance_uuid: str,
2496         device_name: str,
2497         persistent: bool,
2498         live: bool,
2499     ):
2500         """Detaches a device from the guest without waiting for libvirt events
2501 
2502         It only handles synchronous errors (i.e. exceptions) but does not wait
2503         for any event from libvirt.
2504 
2505         :param dev: the device configuration to be detached
2506         :param guest: the guest we are detach the device from
2507         :param instance_uuid: the UUID of the instance we are detaching the
2508             device from
2509         :param device_name: This is the name of the device used solely for
2510             error messages.
2511         :param live: detach the device from the live domain config only
2512         :param persistent: detach the device from the persistent domain config
2513             only
2514         :raises exception.DeviceNotFound: if libvirt reported that the device
2515             is missing from the domain synchronously.
2516         :raises libvirt.libvirtError: for any other errors reported by libvirt
2517             synchronously.
2518         """
2519         try:
2520             guest.detach_device(dev, persistent=persistent, live=live)
2521         except libvirt.libvirtError as ex:
2522             code = ex.get_error_code()
2523             msg = ex.get_error_message()
2524             LOG.debug(
2525                 "Libvirt returned error while detaching device %s from "
2526                 "instance %s. Libvirt error code: %d, error message: %s.",
2527                 device_name, instance_uuid, code, msg
2528             )
2529             if (code == libvirt.VIR_ERR_DEVICE_MISSING or
2530                 # Libvirt 4.1 improved error code usage but OPERATION_FAILED
2531                 # still used in one case during detach:
2532                 # https://github.com/libvirt/libvirt/blob/55ea45acc99c549c7757efe954aacc33ad30a8ef/src/qemu/qemu_hotplug.c#L5324-L5328
2533                 # TODO(gibi): remove this when a future version of libvirt
2534                 # transform this error to VIR_ERR_DEVICE_MISSING too.
2535                 (code == libvirt.VIR_ERR_OPERATION_FAILED and
2536                  'not found' in msg)
2537             ):
2538                 LOG.debug(
2539                     'Libvirt failed to detach device %s from instance %s '
2540                     'synchronously (persistent=%s, live=%s) with error: %s.',
2541                     device_name, instance_uuid, persistent, live, str(ex))
2542                 raise exception.DeviceNotFound(device=device_name) from ex
2543 
2544             # NOTE(lyarwood): https://bugzilla.redhat.com/1878659
2545             # Ignore this known QEMU bug for the time being allowing
2546             # our retry logic to handle it.
2547             # NOTE(gibi): This can only happen in case of detaching from the
2548             # live domain as we never retry a detach from the persistent
2549             # domain so we cannot hit an already running detach there.
2550             # In case of detaching from the live domain this error can happen
2551             # if the caller timed out during the first detach attempt then saw
2552             # that the device is still attached and therefore looped over and
2553             # and retried the detach. In this case the previous attempt stopped
2554             # waiting for the libvirt event. Also libvirt reports that there is
2555             # a detach ongoing, so the current attempt expects that a
2556             # libvirt event will be still emitted. Therefore we simply return
2557             # from here. Then the caller will wait for such event.
2558             if (code == libvirt.VIR_ERR_INTERNAL_ERROR and msg and
2559                     'already in the process of unplug' in msg
2560             ):
2561                 LOG.debug(
2562                     'Ignoring QEMU rejecting our request to detach device %s '
2563                     'from instance %s as it is caused by a previous request '
2564                     'still being in progress.', device_name, instance_uuid)
2565                 return
2566 
2567             if code == libvirt.VIR_ERR_NO_DOMAIN:
2568                 LOG.warning(
2569                     "During device detach, instance disappeared.",
2570                     instance_uuid=instance_uuid)
2571                 # if the domain has disappeared then we have nothing to detach
2572                 return
2573 
2574             LOG.warning(
2575                 'Unexpected libvirt error while detaching device %s from '
2576                 'instance %s: %s', device_name, instance_uuid, str(ex))
2577             raise
2578 
2579     def detach_volume(self, context, connection_info, instance, mountpoint,
2580                       encryption=None):
2581         disk_dev = mountpoint.rpartition("/")[2]
2582         try:
2583             guest = self._host.get_guest(instance)
2584 
2585             # NOTE(lyarwood): The volume must be detached from the VM before
2586             # detaching any attached encryptors or disconnecting the underlying
2587             # volume in _disconnect_volume. Otherwise, the encryptor or volume
2588             # driver may report that the volume is still in use.
2589             get_dev = functools.partial(guest.get_disk, disk_dev)
2590             self._detach_with_retry(
2591                 guest,
2592                 instance.uuid,
2593                 get_dev,
2594                 device_name=disk_dev,
2595             )
2596         except exception.InstanceNotFound:
2597             # NOTE(zhaoqin): If the instance does not exist, _lookup_by_name()
2598             #                will throw InstanceNotFound exception. Need to
2599             #                disconnect volume under this circumstance.
2600             LOG.warning("During detach_volume, instance disappeared.",
2601                         instance=instance)
2602         except exception.DeviceNotFound:
2603             # We should still try to disconnect logical device from
2604             # host, an error might have happened during a previous
2605             # call.
2606             LOG.info("Device %s not found in instance.",
2607                      disk_dev, instance=instance)
2608 
2609         self._disconnect_volume(context, connection_info, instance,
2610                                 encryption=encryption)
2611 
2612     def _resize_attached_volume(self, new_size, block_device, instance):
2613         LOG.debug('Resizing target device %(dev)s to %(size)u',
2614                   {'dev': block_device._disk, 'size': new_size},
2615                   instance=instance)
2616         block_device.resize(new_size)
2617 
2618     def _resize_attached_encrypted_volume(self, original_new_size,
2619                                           block_device, instance,
2620                                           connection_info, encryption):
2621         # TODO(lyarwood): Also handle the dm-crpyt encryption providers of
2622         # plain and LUKSv2, for now just use the original_new_size.
2623         decrypted_device_new_size = original_new_size
2624 
2625         # NOTE(lyarwood): original_new_size currently refers to the total size
2626         # of the extended volume in bytes. With natively decrypted LUKSv1
2627         # volumes we need to ensure this now takes the LUKSv1 header and key
2628         # material into account. Otherwise QEMU will attempt and fail to grow
2629         # host block devices and remote RBD volumes.
2630         if self._allow_native_luksv1(encryption):
2631             try:
2632                 # NOTE(lyarwood): Find the path to provide to qemu-img
2633                 if 'device_path' in connection_info['data']:
2634                     path = connection_info['data']['device_path']
2635                 elif connection_info['driver_volume_type'] == 'rbd':
2636                     volume_name = connection_info['data']['name']
2637                     path = f"rbd:{volume_name}"
2638                     if connection_info['data'].get('auth_enabled'):
2639                         username = connection_info['data']['auth_username']
2640                         path = f"rbd:{volume_name}:id={username}"
2641                 else:
2642                     path = 'unknown'
2643                     raise exception.DiskNotFound(location='unknown')
2644 
2645                 info = images.privileged_qemu_img_info(path)
2646                 format_specific_data = info.format_specific['data']
2647                 payload_offset = format_specific_data['payload-offset']
2648 
2649                 # NOTE(lyarwood): Ensure the underlying device is not resized
2650                 # by subtracting the LUKSv1 payload_offset (where the users
2651                 # encrypted data starts) from the original_new_size (the total
2652                 # size of the underlying volume). Both are reported in bytes.
2653                 decrypted_device_new_size = original_new_size - payload_offset
2654 
2655             except exception.DiskNotFound:
2656                 with excutils.save_and_reraise_exception():
2657                     LOG.exception('Unable to access the encrypted disk %s.',
2658                                   path, instance=instance)
2659             except Exception:
2660                 with excutils.save_and_reraise_exception():
2661                     LOG.exception('Unknown error when attempting to find the '
2662                                   'payload_offset for LUKSv1 encrypted disk '
2663                                   '%s.', path, instance=instance)
2664         # NOTE(lyarwood): Resize the decrypted device within the instance to
2665         # the calculated size as with normal volumes.
2666         self._resize_attached_volume(
2667             decrypted_device_new_size, block_device, instance)
2668 
2669     def extend_volume(self, context, connection_info, instance,
2670                       requested_size):
2671         volume_id = driver_block_device.get_volume_id(connection_info)
2672         try:
2673             new_size = self._extend_volume(
2674                 connection_info, instance, requested_size)
2675 
2676             # NOTE(lyarwood): Handle cases where os-brick has ignored failures
2677             # and returned an invalid new_size of None through the vol drivers
2678             if new_size is None:
2679                 raise exception.VolumeExtendFailed(
2680                     volume_id=volume_id,
2681                     reason="Failure to resize underlying volume on compute."
2682                 )
2683 
2684         except NotImplementedError:
2685             raise exception.ExtendVolumeNotSupported()
2686 
2687         # Resize the device in QEMU so its size is updated and
2688         # detected by the instance without rebooting.
2689         try:
2690             guest = self._host.get_guest(instance)
2691             state = guest.get_power_state(self._host)
2692             active_state = state in (power_state.RUNNING, power_state.PAUSED)
2693             if active_state:
2694                 if 'device_path' in connection_info['data']:
2695                     disk_path = connection_info['data']['device_path']
2696                 else:
2697                     # Some drivers (eg. net) don't put the device_path
2698                     # into the connection_info. Match disks by their serial
2699                     # number instead
2700                     disk = next(iter([
2701                         d for d in guest.get_all_disks()
2702                         if d.serial == volume_id
2703                     ]), None)
2704                     if not disk:
2705                         raise exception.VolumeNotFound(volume_id=volume_id)
2706                     disk_path = disk.target_dev
2707                 dev = guest.get_block_device(disk_path)
2708                 encryption = encryptors.get_encryption_metadata(
2709                     context, self._volume_api, volume_id, connection_info)
2710                 if encryption:
2711                     self._resize_attached_encrypted_volume(
2712                         new_size, dev, instance,
2713                         connection_info, encryption)
2714                 else:
2715                     self._resize_attached_volume(
2716                         new_size, dev, instance)
2717             else:
2718                 LOG.debug('Skipping block device resize, guest is not running',
2719                           instance=instance)
2720         except exception.InstanceNotFound:
2721             with excutils.save_and_reraise_exception():
2722                 LOG.warning('During extend_volume, instance disappeared.',
2723                             instance=instance)
2724         except libvirt.libvirtError:
2725             with excutils.save_and_reraise_exception():
2726                 LOG.exception('resizing block device failed.',
2727                               instance=instance)
2728 
2729     def attach_interface(self, context, instance, image_meta, vif):
2730         guest = self._host.get_guest(instance)
2731 
2732         self.vif_driver.plug(instance, vif)
2733         cfg = self.vif_driver.get_config(instance, vif, image_meta,
2734                                          instance.flavor,
2735                                          CONF.libvirt.virt_type)
2736 
2737         if self._sev_enabled(instance.flavor, image_meta):
2738             designer.set_driver_iommu_for_device(cfg)
2739 
2740         try:
2741             state = guest.get_power_state(self._host)
2742             live = state in (power_state.RUNNING, power_state.PAUSED)
2743             guest.attach_device(cfg, persistent=True, live=live)
2744         except libvirt.libvirtError:
2745             LOG.error('attaching network adapter failed.',
2746                       instance=instance, exc_info=True)
2747             self.vif_driver.unplug(instance, vif)
2748             raise exception.InterfaceAttachFailed(
2749                     instance_uuid=instance.uuid)
2750         try:
2751             # NOTE(artom) If we're attaching with a device role tag, we need to
2752             # rebuild device_metadata. If we're attaching without a role
2753             # tag, we're rebuilding it here needlessly anyways. This isn't a
2754             # massive deal, and it helps reduce code complexity by not having
2755             # to indicate to the virt driver that the attach is tagged. The
2756             # really important optimization of not calling the database unless
2757             # device_metadata has actually changed is done for us by
2758             # instance.save().
2759             instance.device_metadata = self._build_device_metadata(
2760                 context, instance)
2761             instance.save()
2762         except Exception:
2763             # NOTE(artom) If we fail here it means the interface attached
2764             # successfully but building and/or saving the device metadata
2765             # failed. Just unplugging the vif is therefore not enough cleanup,
2766             # we need to detach the interface.
2767             with excutils.save_and_reraise_exception(reraise=False):
2768                 LOG.error('Interface attached successfully but building '
2769                           'and/or saving device metadata failed.',
2770                           instance=instance, exc_info=True)
2771                 self.detach_interface(context, instance, vif)
2772                 raise exception.InterfaceAttachFailed(
2773                     instance_uuid=instance.uuid)
2774         try:
2775             guest.set_metadata(
2776                 self._get_guest_config_meta(
2777                     instance, instance.get_network_info()))
2778         except libvirt.libvirtError:
2779             LOG.warning('updating libvirt metadata failed.', instance=instance)
2780 
2781     def detach_interface(self, context, instance, vif):
2782         guest = self._host.get_guest(instance)
2783         cfg = self.vif_driver.get_config(instance, vif,
2784                                          instance.image_meta,
2785                                          instance.flavor,
2786                                          CONF.libvirt.virt_type)
2787         try:
2788             get_dev = functools.partial(guest.get_interface_by_cfg, cfg)
2789             self._detach_with_retry(
2790                 guest,
2791                 instance.uuid,
2792                 get_dev,
2793                 device_name=self.vif_driver.get_vif_devname(vif),
2794             )
2795         except exception.DeviceNotFound:
2796             # The interface is gone so just log it as a warning.
2797             LOG.warning('Detaching interface %(mac)s failed because '
2798                         'the device is no longer found on the guest.',
2799                         {'mac': vif.get('address')}, instance=instance)
2800         finally:
2801             # NOTE(gibi): we need to unplug the vif _after_ the detach is done
2802             # on the libvirt side as otherwise libvirt will still manage the
2803             # device that our unplug code trying to reset. This can cause a
2804             # race and leave the detached device configured. Also even if we
2805             # are failed to detach due to race conditions the unplug is
2806             # necessary for the same reason
2807             self.vif_driver.unplug(instance, vif)
2808         try:
2809             # NOTE(nmiki): In order for the interface to be removed from
2810             # network_info, the nova-compute process need to wait for
2811             # processing on the neutron side.
2812             # Here, I simply exclude the target VIF from metadata.
2813             network_info = list(filter(lambda info: info['id'] != vif['id'],
2814                                        instance.get_network_info()))
2815             guest.set_metadata(
2816                 self._get_guest_config_meta(instance, network_info))
2817         except libvirt.libvirtError:
2818             LOG.warning('updating libvirt metadata failed.', instance=instance)
2819 
2820     def _create_snapshot_metadata(self, image_meta, instance,
2821                                   img_fmt, snp_name):
2822         metadata = {'status': 'active',
2823                     'name': snp_name,
2824                     'properties': {
2825                                    'kernel_id': instance.kernel_id,
2826                                    'image_location': 'snapshot',
2827                                    'image_state': 'available',
2828                                    'owner_id': instance.project_id,
2829                                    'ramdisk_id': instance.ramdisk_id,
2830                                    }
2831                     }
2832         if instance.os_type:
2833             metadata['properties']['os_type'] = instance.os_type
2834 
2835         # NOTE(vish): glance forces ami disk format to be ami
2836         if image_meta.disk_format == 'ami':
2837             metadata['disk_format'] = 'ami'
2838         else:
2839             metadata['disk_format'] = img_fmt
2840 
2841         if image_meta.obj_attr_is_set("container_format"):
2842             metadata['container_format'] = image_meta.container_format
2843         else:
2844             metadata['container_format'] = "bare"
2845 
2846         return metadata
2847 
2848     def snapshot(self, context, instance, image_id, update_task_state):
2849         """Create snapshot from a running VM instance.
2850 
2851         This command only works with qemu 0.14+
2852         """
2853         try:
2854             guest = self._host.get_guest(instance)
2855         except exception.InstanceNotFound:
2856             raise exception.InstanceNotRunning(instance_id=instance.uuid)
2857 
2858         snapshot = self._image_api.get(context, image_id)
2859 
2860         # source_format is an on-disk format
2861         # source_type is a backend type
2862         disk_path, source_format = libvirt_utils.find_disk(guest)
2863         source_type = libvirt_utils.get_disk_type_from_path(disk_path)
2864 
2865         # We won't have source_type for raw or qcow2 disks, because we can't
2866         # determine that from the path. We should have it from the libvirt
2867         # xml, though.
2868         if source_type is None:
2869             source_type = source_format
2870         # For lxc instances we won't have it either from libvirt xml
2871         # (because we just gave libvirt the mounted filesystem), or the path,
2872         # so source_type is still going to be None. In this case,
2873         # root_disk is going to default to CONF.libvirt.images_type
2874         # below, which is still safe.
2875 
2876         image_format = CONF.libvirt.snapshot_image_format or source_type
2877 
2878         # NOTE(bfilippov): save lvm and rbd as raw
2879         if image_format == 'lvm' or image_format == 'rbd':
2880             image_format = 'raw'
2881 
2882         metadata = self._create_snapshot_metadata(instance.image_meta,
2883                                                   instance,
2884                                                   image_format,
2885                                                   snapshot['name'])
2886 
2887         snapshot_name = uuidutils.generate_uuid(dashed=False)
2888 
2889         # store current state so we know what to resume back to if we suspend
2890         original_power_state = guest.get_power_state(self._host)
2891 
2892         # NOTE(dgenin): Instances with LVM encrypted ephemeral storage require
2893         #               cold snapshots. Currently, checking for encryption is
2894         #               redundant because LVM supports only cold snapshots.
2895         #               It is necessary in case this situation changes in the
2896         #               future.
2897         if (
2898             self._host.has_min_version(hv_type=host.HV_DRIVER_QEMU) and
2899             source_type != 'lvm' and
2900             not CONF.ephemeral_storage_encryption.enabled and
2901             not CONF.workarounds.disable_libvirt_livesnapshot and
2902             # NOTE(stephenfin): Live snapshotting doesn't make sense for
2903             # shutdown instances
2904             original_power_state != power_state.SHUTDOWN
2905         ):
2906             live_snapshot = True
2907         else:
2908             live_snapshot = False
2909 
2910         self._suspend_guest_for_snapshot(
2911             context, live_snapshot, original_power_state, instance)
2912 
2913         root_disk = self.image_backend.by_libvirt_path(
2914             instance, disk_path, image_type=source_type)
2915 
2916         if live_snapshot:
2917             LOG.info("Beginning live snapshot process", instance=instance)
2918         else:
2919             LOG.info("Beginning cold snapshot process", instance=instance)
2920 
2921         update_task_state(task_state=task_states.IMAGE_PENDING_UPLOAD)
2922 
2923         update_task_state(task_state=task_states.IMAGE_UPLOADING,
2924                           expected_state=task_states.IMAGE_PENDING_UPLOAD)
2925 
2926         try:
2927             metadata['location'] = root_disk.direct_snapshot(
2928                 context, snapshot_name, image_format, image_id,
2929                 instance.image_ref)
2930             self._resume_guest_after_snapshot(
2931                 context, live_snapshot, original_power_state, instance, guest)
2932             self._image_api.update(context, image_id, metadata,
2933                                    purge_props=False)
2934         except (NotImplementedError, exception.ImageUnacceptable,
2935                 exception.Forbidden) as e:
2936             if type(e) != NotImplementedError:
2937                 LOG.warning('Performing standard snapshot because direct '
2938                             'snapshot failed: %(error)s',
2939                             {'error': encodeutils.exception_to_unicode(e)})
2940             failed_snap = metadata.pop('location', None)
2941             if failed_snap:
2942                 failed_snap = {'url': str(failed_snap)}
2943             root_disk.cleanup_direct_snapshot(failed_snap,
2944                                                   also_destroy_volume=True,
2945                                                   ignore_errors=True)
2946             update_task_state(task_state=task_states.IMAGE_PENDING_UPLOAD,
2947                               expected_state=task_states.IMAGE_UPLOADING)
2948 
2949             # TODO(nic): possibly abstract this out to the root_disk
2950             if source_type == 'rbd' and live_snapshot:
2951                 # Standard snapshot uses qemu-img convert from RBD which is
2952                 # not safe to run with live_snapshot.
2953                 live_snapshot = False
2954                 # Suspend the guest, so this is no longer a live snapshot
2955                 self._suspend_guest_for_snapshot(
2956                     context, live_snapshot, original_power_state, instance)
2957 
2958             snapshot_directory = CONF.libvirt.snapshots_directory
2959             fileutils.ensure_tree(snapshot_directory)
2960             with utils.tempdir(dir=snapshot_directory) as tmpdir:
2961                 try:
2962                     out_path = os.path.join(tmpdir, snapshot_name)
2963                     if live_snapshot:
2964                         # NOTE(xqueralt): libvirt needs o+x in the tempdir
2965                         os.chmod(tmpdir, 0o701)
2966                         self._live_snapshot(context, instance, guest,
2967                                             disk_path, out_path, source_format,
2968                                             image_format, instance.image_meta)
2969                     else:
2970                         root_disk.snapshot_extract(out_path, image_format)
2971                     LOG.info("Snapshot extracted, beginning image upload",
2972                              instance=instance)
2973                 except libvirt.libvirtError as ex:
2974                     error_code = ex.get_error_code()
2975                     if error_code == libvirt.VIR_ERR_NO_DOMAIN:
2976                         LOG.info('Instance %(instance_name)s disappeared '
2977                                  'while taking snapshot of it: [Error Code '
2978                                  '%(error_code)s] %(ex)s',
2979                                  {'instance_name': instance.name,
2980                                   'error_code': error_code,
2981                                   'ex': ex},
2982                                  instance=instance)
2983                         raise exception.InstanceNotFound(
2984                             instance_id=instance.uuid)
2985                     else:
2986                         raise
2987                 finally:
2988                     self._resume_guest_after_snapshot(
2989                         context, live_snapshot, original_power_state, instance,
2990                         guest)
2991 
2992                 # Upload that image to the image service
2993                 update_task_state(task_state=task_states.IMAGE_UPLOADING,
2994                         expected_state=task_states.IMAGE_PENDING_UPLOAD)
2995                 with libvirt_utils.file_open(out_path, 'rb') as image_file:
2996                     # execute operation with disk concurrency semaphore
2997                     with compute_utils.disk_ops_semaphore:
2998                         self._image_api.update(context,
2999                                                image_id,
3000                                                metadata,
3001                                                image_file)
3002         except Exception:
3003             with excutils.save_and_reraise_exception():
3004                 LOG.exception("Failed to snapshot image")
3005                 failed_snap = metadata.pop('location', None)
3006                 if failed_snap:
3007                     failed_snap = {'url': str(failed_snap)}
3008                 root_disk.cleanup_direct_snapshot(
3009                         failed_snap, also_destroy_volume=True,
3010                         ignore_errors=True)
3011 
3012         LOG.info("Snapshot image upload complete", instance=instance)
3013 
3014     def _needs_suspend_resume_for_snapshot(
3015         self,
3016         live_snapshot: bool,
3017         current_power_state: int,
3018     ):
3019         # NOTE(dkang): managedSave does not work for LXC
3020         if CONF.libvirt.virt_type == 'lxc':
3021             return False
3022 
3023         # Live snapshots do not necessitate suspending the domain
3024         if live_snapshot:
3025             return False
3026 
3027         # ...and neither does a non-running domain
3028         return current_power_state in (power_state.RUNNING, power_state.PAUSED)
3029 
3030     def _suspend_guest_for_snapshot(
3031         self,
3032         context: nova_context.RequestContext,
3033         live_snapshot: bool,
3034         current_power_state: int,
3035         instance: 'objects.Instance',
3036     ):
3037         if self._needs_suspend_resume_for_snapshot(
3038             live_snapshot, current_power_state,
3039         ):
3040             self.suspend(context, instance)
3041 
3042     def _resume_guest_after_snapshot(
3043         self,
3044         context: nova_context.RequestContext,
3045         live_snapshot: bool,
3046         original_power_state: int,
3047         instance: 'objects.Instance',
3048         guest: libvirt_guest.Guest,
3049     ):
3050         if not self._needs_suspend_resume_for_snapshot(
3051             live_snapshot, original_power_state,
3052         ):
3053             return
3054 
3055         current_power_state = guest.get_power_state(self._host)
3056 
3057         # TODO(stephenfin): Any reason we couldn't use 'self.resume' here?
3058         guest.launch(pause=current_power_state == power_state.PAUSED)
3059 
3060         self._attach_pci_devices(
3061             guest, pci_manager.get_instance_pci_devs(instance))
3062         self._attach_direct_passthrough_ports(context, instance, guest)
3063 
3064     def _can_set_admin_password(self, image_meta):
3065 
3066         if CONF.libvirt.virt_type in ('kvm', 'qemu'):
3067             if not image_meta.properties.get('hw_qemu_guest_agent', False):
3068                 raise exception.QemuGuestAgentNotEnabled()
3069         elif not CONF.libvirt.virt_type == 'parallels':
3070             raise exception.SetAdminPasswdNotSupported()
3071 
3072     def _save_instance_password_if_sshkey_present(self, instance, new_pass):
3073         sshkey = instance.key_data if 'key_data' in instance else None
3074         if sshkey and sshkey.startswith("ssh-rsa"):
3075             enc = crypto.ssh_encrypt_text(sshkey, new_pass)
3076             # NOTE(melwitt): The convert_password method doesn't actually do
3077             # anything with the context argument, so we can pass None.
3078             instance.system_metadata.update(
3079                 password.convert_password(None, base64.encode_as_text(enc)))
3080             instance.save()
3081 
3082     def set_admin_password(self, instance, new_pass):
3083         self._can_set_admin_password(instance.image_meta)
3084 
3085         guest = self._host.get_guest(instance)
3086         user = instance.image_meta.properties.get("os_admin_user")
3087         if not user:
3088             if instance.os_type == "windows":
3089                 user = "Administrator"
3090             else:
3091                 user = "root"
3092         try:
3093             guest.set_user_password(user, new_pass)
3094         except libvirt.libvirtError as ex:
3095             error_code = ex.get_error_code()
3096             if error_code == libvirt.VIR_ERR_AGENT_UNRESPONSIVE:
3097                 LOG.debug('Failed to set password: QEMU agent unresponsive',
3098                           instance_uuid=instance.uuid)
3099                 raise NotImplementedError()
3100 
3101             err_msg = encodeutils.exception_to_unicode(ex)
3102             msg = (_('Error from libvirt while set password for username '
3103                      '"%(user)s": [Error Code %(error_code)s] %(ex)s')
3104                    % {'user': user, 'error_code': error_code, 'ex': err_msg})
3105             raise exception.InternalError(msg)
3106         else:
3107             # Save the password in sysmeta so it may be retrieved from the
3108             # metadata service.
3109             self._save_instance_password_if_sshkey_present(instance, new_pass)
3110 
3111     def _can_quiesce(self, instance, image_meta):
3112         if CONF.libvirt.virt_type not in ('kvm', 'qemu'):
3113             raise exception.InstanceQuiesceNotSupported(
3114                 instance_id=instance.uuid)
3115 
3116         if not image_meta.properties.get('hw_qemu_guest_agent', False):
3117             raise exception.QemuGuestAgentNotEnabled()
3118 
3119     def _requires_quiesce(self, image_meta):
3120         return image_meta.properties.get('os_require_quiesce', False)
3121 
3122     def _set_quiesced(self, context, instance, image_meta, quiesced):
3123         self._can_quiesce(instance, image_meta)
3124         try:
3125             guest = self._host.get_guest(instance)
3126             if quiesced:
3127                 guest.freeze_filesystems()
3128             else:
3129                 guest.thaw_filesystems()
3130         except libvirt.libvirtError as ex:
3131             error_code = ex.get_error_code()
3132             err_msg = encodeutils.exception_to_unicode(ex)
3133             msg = (_('Error from libvirt while quiescing %(instance_name)s: '
3134                      '[Error Code %(error_code)s] %(ex)s')
3135                    % {'instance_name': instance.name,
3136                       'error_code': error_code, 'ex': err_msg})
3137             raise exception.InternalError(msg)
3138 
3139     def quiesce(self, context, instance, image_meta):
3140         """Freeze the guest filesystems to prepare for snapshot.
3141 
3142         The qemu-guest-agent must be setup to execute fsfreeze.
3143         """
3144         self._set_quiesced(context, instance, image_meta, True)
3145 
3146     def unquiesce(self, context, instance, image_meta):
3147         """Thaw the guest filesystems after snapshot."""
3148         self._set_quiesced(context, instance, image_meta, False)
3149 
3150     def _live_snapshot(self, context, instance, guest, disk_path, out_path,
3151                        source_format, image_format, image_meta):
3152         """Snapshot an instance without downtime."""
3153         dev = guest.get_block_device(disk_path)
3154 
3155         # Save a copy of the domain's persistent XML file
3156         xml = guest.get_xml_desc(dump_inactive=True, dump_sensitive=True)
3157 
3158         # Abort is an idempotent operation, so make sure any block
3159         # jobs which may have failed are ended.
3160         try:
3161             dev.abort_job()
3162         except Exception:
3163             pass
3164 
3165         # NOTE (rmk): We are using shallow rebases as a workaround to a bug
3166         #             in QEMU 1.3. In order to do this, we need to create
3167         #             a destination image with the original backing file
3168         #             and matching size of the instance root disk.
3169         src_disk_size = libvirt_utils.get_disk_size(disk_path,
3170                                                     format=source_format)
3171         src_back_path = libvirt_utils.get_disk_backing_file(disk_path,
3172                                                         format=source_format,
3173                                                         basename=False)
3174         disk_delta = out_path + '.delta'
3175         libvirt_utils.create_cow_image(src_back_path, disk_delta,
3176                                        src_disk_size)
3177 
3178         quiesced = False
3179         try:
3180             self._set_quiesced(context, instance, image_meta, True)
3181             quiesced = True
3182         except exception.NovaException as err:
3183             if self._requires_quiesce(image_meta):
3184                 raise
3185             LOG.info('Skipping quiescing instance: %(reason)s.',
3186                      {'reason': err}, instance=instance)
3187 
3188         try:
3189             # NOTE (rmk): blockRebase cannot be executed on persistent
3190             #             domains, so we need to temporarily undefine it.
3191             #             If any part of this block fails, the domain is
3192             #             re-defined regardless.
3193             if guest.has_persistent_configuration():
3194                 hw_firmware_type = image_meta.properties.get(
3195                     'hw_firmware_type')
3196                 support_uefi = self._check_uefi_support(hw_firmware_type)
3197                 guest.delete_configuration(support_uefi)
3198 
3199             # NOTE (rmk): Establish a temporary mirror of our root disk and
3200             #             issue an abort once we have a complete copy.
3201             dev.rebase(disk_delta, copy=True, reuse_ext=True, shallow=True)
3202 
3203             while not dev.is_job_complete():
3204                 time.sleep(0.5)
3205 
3206             dev.abort_job()
3207             nova.privsep.path.chown(disk_delta, uid=os.getuid())
3208         finally:
3209             self._host.write_instance_config(xml)
3210             if quiesced:
3211                 self._set_quiesced(context, instance, image_meta, False)
3212 
3213         # Convert the delta (CoW) image with a backing file to a flat
3214         # image with no backing file.
3215         libvirt_utils.extract_snapshot(disk_delta, 'qcow2',
3216                                        out_path, image_format)
3217 
3218         # Remove the disk_delta file once the snapshot extracted, so that
3219         # it doesn't hang around till the snapshot gets uploaded
3220         fileutils.delete_if_exists(disk_delta)
3221 
3222     def _volume_snapshot_update_status(self, context, snapshot_id, status):
3223         """Send a snapshot status update to Cinder.
3224 
3225         This method captures and logs exceptions that occur
3226         since callers cannot do anything useful with these exceptions.
3227 
3228         Operations on the Cinder side waiting for this will time out if
3229         a failure occurs sending the update.
3230 
3231         :param context: security context
3232         :param snapshot_id: id of snapshot being updated
3233         :param status: new status value
3234 
3235         """
3236 
3237         try:
3238             self._volume_api.update_snapshot_status(context,
3239                                                     snapshot_id,
3240                                                     status)
3241         except Exception:
3242             LOG.exception('Failed to send updated snapshot status '
3243                           'to volume service.')
3244 
3245     def _volume_snapshot_create(self, context, instance, guest,
3246                                 volume_id, new_file):
3247         """Perform volume snapshot.
3248 
3249            :param guest: VM that volume is attached to
3250            :param volume_id: volume UUID to snapshot
3251            :param new_file: relative path to new qcow2 file present on share
3252 
3253         """
3254         xml = guest.get_xml_desc()
3255         xml_doc = etree.fromstring(xml)
3256 
3257         device_info = vconfig.LibvirtConfigGuest()
3258         device_info.parse_dom(xml_doc)
3259 
3260         disks_to_snap = []          # to be snapshotted by libvirt
3261         network_disks_to_snap = []  # network disks (netfs, etc.)
3262         disks_to_skip = []          # local disks not snapshotted
3263 
3264         for guest_disk in device_info.devices:
3265             if (guest_disk.root_name != 'disk'):
3266                 continue
3267 
3268             if (guest_disk.target_dev is None):
3269                 continue
3270 
3271             if (guest_disk.serial is None or guest_disk.serial != volume_id):
3272                 disks_to_skip.append(guest_disk.target_dev)
3273                 continue
3274 
3275             # disk is a Cinder volume with the correct volume_id
3276 
3277             disk_info = {
3278                 'dev': guest_disk.target_dev,
3279                 'serial': guest_disk.serial,
3280                 'current_file': guest_disk.source_path,
3281                 'source_protocol': guest_disk.source_protocol,
3282                 'source_name': guest_disk.source_name,
3283                 'source_hosts': guest_disk.source_hosts,
3284                 'source_ports': guest_disk.source_ports
3285             }
3286 
3287             # Determine path for new_file based on current path
3288             if disk_info['current_file'] is not None:
3289                 current_file = disk_info['current_file']
3290                 new_file_path = os.path.join(os.path.dirname(current_file),
3291                                              new_file)
3292                 disks_to_snap.append((current_file, new_file_path))
3293             # NOTE(mriedem): This used to include a check for gluster in
3294             # addition to netfs since they were added together. Support for
3295             # gluster was removed in the 16.0.0 Pike release. It is unclear,
3296             # however, if other volume drivers rely on the netfs disk source
3297             # protocol.
3298             elif disk_info['source_protocol'] == 'netfs':
3299                 network_disks_to_snap.append((disk_info, new_file))
3300 
3301         if not disks_to_snap and not network_disks_to_snap:
3302             msg = _('Found no disk to snapshot.')
3303             raise exception.InternalError(msg)
3304 
3305         snapshot = vconfig.LibvirtConfigGuestSnapshot()
3306 
3307         for current_name, new_filename in disks_to_snap:
3308             snap_disk = vconfig.LibvirtConfigGuestSnapshotDisk()
3309             snap_disk.name = current_name
3310             snap_disk.source_path = new_filename
3311             snap_disk.source_type = 'file'
3312             snap_disk.snapshot = 'external'
3313             snap_disk.driver_name = 'qcow2'
3314 
3315             snapshot.add_disk(snap_disk)
3316 
3317         for disk_info, new_filename in network_disks_to_snap:
3318             snap_disk = vconfig.LibvirtConfigGuestSnapshotDisk()
3319             snap_disk.name = disk_info['dev']
3320             snap_disk.source_type = 'network'
3321             snap_disk.source_protocol = disk_info['source_protocol']
3322             snap_disk.snapshot = 'external'
3323             snap_disk.source_path = new_filename
3324             old_dir = disk_info['source_name'].split('/')[0]
3325             snap_disk.source_name = '%s/%s' % (old_dir, new_filename)
3326             snap_disk.source_hosts = disk_info['source_hosts']
3327             snap_disk.source_ports = disk_info['source_ports']
3328 
3329             snapshot.add_disk(snap_disk)
3330 
3331         for dev in disks_to_skip:
3332             snap_disk = vconfig.LibvirtConfigGuestSnapshotDisk()
3333             snap_disk.name = dev
3334             snap_disk.snapshot = 'no'
3335 
3336             snapshot.add_disk(snap_disk)
3337 
3338         snapshot_xml = snapshot.to_xml()
3339         LOG.debug("snap xml: %s", snapshot_xml, instance=instance)
3340 
3341         image_meta = instance.image_meta
3342         try:
3343             # Check to see if we can quiesce the guest before taking the
3344             # snapshot.
3345             self._can_quiesce(instance, image_meta)
3346             try:
3347                 guest.snapshot(snapshot, no_metadata=True, disk_only=True,
3348                                reuse_ext=True, quiesce=True)
3349                 return
3350             except libvirt.libvirtError:
3351                 # If the image says that quiesce is required then we fail.
3352                 if self._requires_quiesce(image_meta):
3353                     raise
3354                 LOG.exception('Unable to create quiesced VM snapshot, '
3355                               'attempting again with quiescing disabled.',
3356                               instance=instance)
3357         except (exception.InstanceQuiesceNotSupported,
3358                 exception.QemuGuestAgentNotEnabled) as err:
3359             # If the image says that quiesce is required then we need to fail.
3360             if self._requires_quiesce(image_meta):
3361                 raise
3362             LOG.info('Skipping quiescing instance: %(reason)s.',
3363                      {'reason': err}, instance=instance)
3364 
3365         try:
3366             guest.snapshot(snapshot, no_metadata=True, disk_only=True,
3367                            reuse_ext=True, quiesce=False)
3368         except libvirt.libvirtError:
3369             LOG.exception('Unable to create VM snapshot, '
3370                           'failing volume_snapshot operation.',
3371                           instance=instance)
3372 
3373             raise
3374 
3375     def _volume_refresh_connection_info(self, context, instance, volume_id):
3376         bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
3377                   context, volume_id, instance.uuid)
3378 
3379         driver_bdm = driver_block_device.convert_volume(bdm)
3380         if driver_bdm:
3381             driver_bdm.refresh_connection_info(context, instance,
3382                                                self._volume_api, self)
3383 
3384     def volume_snapshot_create(self, context, instance, volume_id,
3385                                create_info):
3386         """Create snapshots of a Cinder volume via libvirt.
3387 
3388         :param instance: VM instance object reference
3389         :param volume_id: id of volume being snapshotted
3390         :param create_info: dict of information used to create snapshots
3391                      - snapshot_id : ID of snapshot
3392                      - type : qcow2 / <other>
3393                      - new_file : qcow2 file created by Cinder which
3394                      becomes the VM's active image after
3395                      the snapshot is complete
3396         """
3397 
3398         LOG.debug("volume_snapshot_create: create_info: %(c_info)s",
3399                   {'c_info': create_info}, instance=instance)
3400 
3401         try:
3402             guest = self._host.get_guest(instance)
3403         except exception.InstanceNotFound:
3404             raise exception.InstanceNotRunning(instance_id=instance.uuid)
3405 
3406         if create_info['type'] != 'qcow2':
3407             msg = _('Unknown type: %s') % create_info['type']
3408             raise exception.InternalError(msg)
3409 
3410         snapshot_id = create_info.get('snapshot_id', None)
3411         if snapshot_id is None:
3412             msg = _('snapshot_id required in create_info')
3413             raise exception.InternalError(msg)
3414 
3415         try:
3416             self._volume_snapshot_create(context, instance, guest,
3417                                          volume_id, create_info['new_file'])
3418         except Exception:
3419             with excutils.save_and_reraise_exception():
3420                 LOG.exception('Error occurred during volume_snapshot_create, '
3421                               'sending error status to Cinder.',
3422                               instance=instance)
3423                 self._volume_snapshot_update_status(
3424                     context, snapshot_id, 'error')
3425 
3426         self._volume_snapshot_update_status(
3427             context, snapshot_id, 'creating')
3428 
3429         def _wait_for_snapshot():
3430             snapshot = self._volume_api.get_snapshot(context, snapshot_id)
3431 
3432             if snapshot.get('status') != 'creating':
3433                 self._volume_refresh_connection_info(context, instance,
3434                                                      volume_id)
3435                 raise loopingcall.LoopingCallDone()
3436 
3437         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_snapshot)
3438         timer.start(interval=0.5).wait()
3439 
3440     @staticmethod
3441     def _rebase_with_qemu_img(source_path, rebase_base):
3442         """Rebase a disk using qemu-img.
3443 
3444         :param source_path: the disk source path to rebase
3445         :type source_path: string
3446         :param rebase_base: the new parent in the backing chain
3447         :type rebase_base: None or string
3448         """
3449 
3450         if rebase_base is None:
3451             # If backing_file is specified as "" (the empty string), then
3452             # the image is rebased onto no backing file (i.e. it will exist
3453             # independently of any backing file).
3454             backing_file = ""
3455             qemu_img_extra_arg = []
3456         else:
3457             # If the rebased image is going to have a backing file then
3458             # explicitly set the backing file format to avoid any security
3459             # concerns related to file format auto detection.
3460             if os.path.isabs(rebase_base):
3461                 backing_file = rebase_base
3462             else:
3463                 # this is a probably a volume snapshot case where the
3464                 # rebase_base is relative. See bug
3465                 # https://bugs.launchpad.net/nova/+bug/1885528
3466                 backing_file_name = os.path.basename(rebase_base)
3467                 volume_path = os.path.dirname(source_path)
3468                 backing_file = os.path.join(volume_path, backing_file_name)
3469 
3470             b_file_fmt = images.qemu_img_info(backing_file).file_format
3471             qemu_img_extra_arg = ['-F', b_file_fmt]
3472 
3473         qemu_img_extra_arg.append(source_path)
3474         # execute operation with disk concurrency semaphore
3475         with compute_utils.disk_ops_semaphore:
3476             processutils.execute("qemu-img", "rebase", "-b", backing_file,
3477                                  *qemu_img_extra_arg)
3478 
3479     def _volume_snapshot_delete(self, context, instance, volume_id,
3480                                 snapshot_id, delete_info=None):
3481         """Note:
3482             if file being merged into == active image:
3483                 do a blockRebase (pull) operation
3484             else:
3485                 do a blockCommit operation
3486             Files must be adjacent in snap chain.
3487 
3488         :param instance: instance object reference
3489         :param volume_id: volume UUID
3490         :param snapshot_id: snapshot UUID (unused currently)
3491         :param delete_info: {
3492             'type':              'qcow2',
3493             'file_to_merge':     'a.img',
3494             'merge_target_file': 'b.img' or None (if merging file_to_merge into
3495                                                   active image)
3496           }
3497         """
3498 
3499         LOG.debug('volume_snapshot_delete: delete_info: %s', delete_info,
3500                   instance=instance)
3501 
3502         if delete_info['type'] != 'qcow2':
3503             msg = _('Unknown delete_info type %s') % delete_info['type']
3504             raise exception.InternalError(msg)
3505 
3506         try:
3507             guest = self._host.get_guest(instance)
3508         except exception.InstanceNotFound:
3509             raise exception.InstanceNotRunning(instance_id=instance.uuid)
3510 
3511         # Find dev name
3512         xml = guest.get_xml_desc()
3513         xml_doc = etree.fromstring(xml)
3514 
3515         device_info = vconfig.LibvirtConfigGuest()
3516         device_info.parse_dom(xml_doc)
3517 
3518         for guest_disk in device_info.devices:
3519             if (guest_disk.root_name != 'disk'):
3520                 continue
3521 
3522             if (guest_disk.target_dev is None or guest_disk.serial is None):
3523                 continue
3524 
3525             if (
3526                 guest_disk.source_path is None and
3527                 guest_disk.source_protocol is None
3528             ):
3529                 continue
3530 
3531             if guest_disk.serial == volume_id:
3532                 my_dev = guest_disk.target_dev
3533 
3534                 active_protocol = guest_disk.source_protocol
3535                 active_disk_object = guest_disk
3536                 break
3537         else:
3538             LOG.debug('Domain XML: %s', xml, instance=instance)
3539             msg = (_("Disk with id '%s' not found attached to instance.")
3540                    % volume_id)
3541             raise exception.InternalError(msg)
3542 
3543         LOG.debug("found device at %s", my_dev, instance=instance)
3544 
3545         def _get_snap_dev(filename, backing_store):
3546             if filename is None:
3547                 msg = _('filename cannot be None')
3548                 raise exception.InternalError(msg)
3549 
3550             # libgfapi delete
3551             LOG.debug("XML: %s", xml)
3552 
3553             LOG.debug("active disk object: %s", active_disk_object)
3554 
3555             # determine reference within backing store for desired image
3556             filename_to_merge = filename
3557             matched_name = None
3558             b = backing_store
3559             index = None
3560 
3561             current_filename = active_disk_object.source_name.split('/')[1]
3562             if current_filename == filename_to_merge:
3563                 return my_dev + '[0]'
3564 
3565             while b is not None:
3566                 source_filename = b.source_name.split('/')[1]
3567                 if source_filename == filename_to_merge:
3568                     LOG.debug('found match: %s', b.source_name)
3569                     matched_name = b.source_name
3570                     index = b.index
3571                     break
3572 
3573                 b = b.backing_store
3574 
3575             if matched_name is None:
3576                 msg = _('no match found for %s') % (filename_to_merge)
3577                 raise exception.InternalError(msg)
3578 
3579             LOG.debug('index of match (%s) is %s', b.source_name, index)
3580 
3581             my_snap_dev = '%s[%s]' % (my_dev, index)
3582             return my_snap_dev
3583 
3584         if delete_info['merge_target_file'] is None:
3585             # pull via blockRebase()
3586 
3587             # Merge the most recent snapshot into the active image
3588 
3589             rebase_disk = my_dev
3590             rebase_base = delete_info['file_to_merge']  # often None
3591             if (active_protocol is not None) and (rebase_base is not None):
3592                 rebase_base = _get_snap_dev(rebase_base,
3593                                             active_disk_object.backing_store)
3594 
3595             relative = rebase_base is not None
3596             LOG.debug(
3597                 'disk: %(disk)s, base: %(base)s, '
3598                 'bw: %(bw)s, relative: %(relative)s',
3599                 {'disk': rebase_disk,
3600                  'base': rebase_base,
3601                  'bw': libvirt_guest.BlockDevice.REBASE_DEFAULT_BANDWIDTH,
3602                  'relative': str(relative)}, instance=instance)
3603 
3604             dev = guest.get_block_device(rebase_disk)
3605             if guest.is_active():
3606                 result = dev.rebase(rebase_base, relative=relative)
3607                 if result == 0:
3608                     LOG.debug('blockRebase started successfully',
3609                               instance=instance)
3610 
3611                 while not dev.is_job_complete():
3612                     LOG.debug('waiting for blockRebase job completion',
3613                               instance=instance)
3614                     time.sleep(0.5)
3615 
3616             # If the guest is not running libvirt won't do a blockRebase.
3617             # In that case, let's ask qemu-img to rebase the disk.
3618             else:
3619                 LOG.debug('Guest is not running so doing a block rebase '
3620                           'using "qemu-img rebase"', instance=instance)
3621 
3622                 # It's unsure how well qemu-img handles network disks for
3623                 # every protocol. So let's be safe.
3624                 active_protocol = active_disk_object.source_protocol
3625                 if active_protocol is not None:
3626                     msg = _("Something went wrong when deleting a volume "
3627                             "snapshot: rebasing a %(protocol)s network disk "
3628                             "using qemu-img has not been fully tested"
3629                            ) % {'protocol': active_protocol}
3630                     LOG.error(msg)
3631                     raise exception.InternalError(msg)
3632                 self._rebase_with_qemu_img(active_disk_object.source_path,
3633                                            rebase_base)
3634 
3635         else:
3636             # commit with blockCommit()
3637             my_snap_base = None
3638             my_snap_top = None
3639             commit_disk = my_dev
3640 
3641             if active_protocol is not None:
3642                 my_snap_base = _get_snap_dev(delete_info['merge_target_file'],
3643                                              active_disk_object.backing_store)
3644                 my_snap_top = _get_snap_dev(delete_info['file_to_merge'],
3645                                             active_disk_object.backing_store)
3646 
3647             commit_base = my_snap_base or delete_info['merge_target_file']
3648             commit_top = my_snap_top or delete_info['file_to_merge']
3649 
3650             LOG.debug('will call blockCommit with commit_disk=%(commit_disk)s '
3651                       'commit_base=%(commit_base)s '
3652                       'commit_top=%(commit_top)s ',
3653                       {'commit_disk': commit_disk,
3654                        'commit_base': commit_base,
3655                        'commit_top': commit_top}, instance=instance)
3656 
3657             dev = guest.get_block_device(commit_disk)
3658             result = dev.commit(commit_base, commit_top, relative=True)
3659 
3660             if result == 0:
3661                 LOG.debug('blockCommit started successfully',
3662                           instance=instance)
3663 
3664             while not dev.is_job_complete():
3665                 LOG.debug('waiting for blockCommit job completion',
3666                           instance=instance)
3667                 time.sleep(0.5)
3668 
3669     def volume_snapshot_delete(self, context, instance, volume_id, snapshot_id,
3670                                delete_info):
3671         try:
3672             self._volume_snapshot_delete(context, instance, volume_id,
3673                                          snapshot_id, delete_info=delete_info)
3674         except Exception:
3675             with excutils.save_and_reraise_exception():
3676                 LOG.exception('Error occurred during volume_snapshot_delete, '
3677                               'sending error status to Cinder.',
3678                               instance=instance)
3679                 self._volume_snapshot_update_status(
3680                     context, snapshot_id, 'error_deleting')
3681 
3682         self._volume_snapshot_update_status(context, snapshot_id, 'deleting')
3683         self._volume_refresh_connection_info(context, instance, volume_id)
3684 
3685     def reboot(self, context, instance, network_info, reboot_type,
3686                block_device_info=None, bad_volumes_callback=None,
3687                accel_info=None):
3688         """Reboot a virtual machine, given an instance reference."""
3689         if reboot_type == 'SOFT':
3690             # NOTE(vish): This will attempt to do a graceful shutdown/restart.
3691             try:
3692                 soft_reboot_success = self._soft_reboot(instance)
3693             except libvirt.libvirtError as e:
3694                 LOG.debug("Instance soft reboot failed: %s",
3695                           encodeutils.exception_to_unicode(e),
3696                           instance=instance)
3697                 soft_reboot_success = False
3698 
3699             if soft_reboot_success:
3700                 LOG.info("Instance soft rebooted successfully.",
3701                          instance=instance)
3702                 return
3703             else:
3704                 LOG.warning("Failed to soft reboot instance. "
3705                             "Trying hard reboot.",
3706                             instance=instance)
3707         return self._hard_reboot(context, instance, network_info,
3708                                  block_device_info, accel_info)
3709 
3710     def _soft_reboot(self, instance):
3711         """Attempt to shutdown and restart the instance gracefully.
3712 
3713         We use shutdown and create here so we can return if the guest
3714         responded and actually rebooted. Note that this method only
3715         succeeds if the guest responds to acpi. Therefore we return
3716         success or failure so we can fall back to a hard reboot if
3717         necessary.
3718 
3719         :returns: True if the reboot succeeded
3720         """
3721         guest = self._host.get_guest(instance)
3722 
3723         state = guest.get_power_state(self._host)
3724         old_domid = guest.id
3725         # NOTE(vish): This check allows us to reboot an instance that
3726         #             is already shutdown.
3727         if state == power_state.RUNNING:
3728             guest.shutdown()
3729         # NOTE(vish): This actually could take slightly longer than the
3730         #             FLAG defines depending on how long the get_info
3731         #             call takes to return.
3732         for x in range(CONF.libvirt.wait_soft_reboot_seconds):
3733             guest = self._host.get_guest(instance)
3734 
3735             state = guest.get_power_state(self._host)
3736             new_domid = guest.id
3737 
3738             # NOTE(ivoks): By checking domain IDs, we make sure we are
3739             #              not recreating domain that's already running.
3740             if old_domid != new_domid:
3741                 if state in (power_state.SHUTDOWN, power_state.CRASHED):
3742                     LOG.info("Instance shutdown successfully.",
3743                              instance=instance)
3744                     guest.launch()
3745                     timer = loopingcall.FixedIntervalLoopingCall(
3746                         self._wait_for_running, instance)
3747                     timer.start(interval=0.5).wait()
3748                     return True
3749                 else:
3750                     LOG.info("Instance may have been rebooted during soft "
3751                              "reboot, so return now.", instance=instance)
3752                     return True
3753             greenthread.sleep(1)
3754         return False
3755 
3756     def _hard_reboot(self, context, instance, network_info,
3757                      block_device_info=None, accel_info=None):
3758         """Reboot a virtual machine, given an instance reference.
3759 
3760         Performs a Libvirt reset (if supported) on the domain.
3761 
3762         If Libvirt reset is unavailable this method actually destroys and
3763         re-creates the domain to ensure the reboot happens, as the guest
3764         OS cannot ignore this action.
3765         """
3766         # NOTE(sbauza): Since we undefine the guest XML when destroying, we
3767         # need to remember the existing mdevs for reusing them.
3768         mdevs = self._get_all_assigned_mediated_devices(instance)
3769         mdevs = list(mdevs.keys())
3770         # NOTE(mdbooth): In addition to performing a hard reboot of the domain,
3771         # the hard reboot operation is relied upon by operators to be an
3772         # automated attempt to fix as many things as possible about a
3773         # non-functioning instance before resorting to manual intervention.
3774         # With this goal in mind, we tear down all the aspects of an instance
3775         # we can here without losing data. This allows us to re-initialise from
3776         # scratch, and hopefully fix, most aspects of a non-functioning guest.
3777         self.destroy(context, instance, network_info, destroy_disks=False,
3778                      block_device_info=block_device_info,
3779                      destroy_secrets=False)
3780 
3781         # Convert the system metadata to image metadata
3782         # NOTE(mdbooth): This is a workaround for stateless Nova compute
3783         #                https://bugs.launchpad.net/nova/+bug/1349978
3784         instance_dir = libvirt_utils.get_instance_path(instance)
3785         fileutils.ensure_tree(instance_dir)
3786 
3787         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
3788                                             instance,
3789                                             instance.image_meta,
3790                                             block_device_info)
3791         # NOTE(vish): This could generate the wrong device_format if we are
3792         #             using the raw backend and the images don't exist yet.
3793         #             The create_images_and_backing below doesn't properly
3794         #             regenerate raw backend images, however, so when it
3795         #             does we need to (re)generate the xml after the images
3796         #             are in place.
3797         xml = self._get_guest_xml(context, instance, network_info, disk_info,
3798                                   instance.image_meta,
3799                                   block_device_info=block_device_info,
3800                                   mdevs=mdevs, accel_info=accel_info)
3801 
3802         # NOTE(mdbooth): context.auth_token will not be set when we call
3803         #                _hard_reboot from resume_state_on_host_boot()
3804         if context.auth_token is not None:
3805             # NOTE (rmk): Re-populate any missing backing files.
3806             config = vconfig.LibvirtConfigGuest()
3807             config.parse_str(xml)
3808             backing_disk_info = self._get_instance_disk_info_from_config(
3809                 config, block_device_info)
3810             self._create_images_and_backing(context, instance, instance_dir,
3811                                             backing_disk_info)
3812 
3813         # Initialize all the necessary networking, block devices and
3814         # start the instance.
3815         # NOTE(melwitt): Pass vifs_already_plugged=True here even though we've
3816         # unplugged vifs earlier. The behavior of neutron plug events depends
3817         # on which vif type we're using and we are working with a stale network
3818         # info cache here, so won't rely on waiting for neutron plug events.
3819         # vifs_already_plugged=True means "do not wait for neutron plug events"
3820         # NOTE(efried): The instance should already have a vtpm_secret_uuid
3821         # registered if appropriate.
3822         self._create_guest_with_network(
3823             context, xml, instance, network_info, block_device_info,
3824             vifs_already_plugged=True)
3825 
3826         def _wait_for_reboot():
3827             """Called at an interval until the VM is running again."""
3828             state = self.get_info(instance).state
3829 
3830             if state == power_state.RUNNING:
3831                 LOG.info("Instance rebooted successfully.",
3832                          instance=instance)
3833                 raise loopingcall.LoopingCallDone()
3834 
3835         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_reboot)
3836         timer.start(interval=0.5).wait()
3837 
3838     def pause(self, instance):
3839         """Pause VM instance."""
3840         self._host.get_guest(instance).pause()
3841 
3842     def unpause(self, instance):
3843         """Unpause paused VM instance."""
3844         guest = self._host.get_guest(instance)
3845         guest.resume()
3846         guest.sync_guest_time()
3847 
3848     def _clean_shutdown(self, instance, timeout, retry_interval):
3849         """Attempt to shutdown the instance gracefully.
3850 
3851         :param instance: The instance to be shutdown
3852         :param timeout: How long to wait in seconds for the instance to
3853                         shutdown
3854         :param retry_interval: How often in seconds to signal the instance
3855                                to shutdown while waiting
3856 
3857         :returns: True if the shutdown succeeded
3858         """
3859 
3860         # List of states that represent a shutdown instance
3861         SHUTDOWN_STATES = [power_state.SHUTDOWN,
3862                            power_state.CRASHED]
3863 
3864         try:
3865             guest = self._host.get_guest(instance)
3866         except exception.InstanceNotFound:
3867             # If the instance has gone then we don't need to
3868             # wait for it to shutdown
3869             return True
3870 
3871         state = guest.get_power_state(self._host)
3872         if state in SHUTDOWN_STATES:
3873             LOG.info("Instance already shutdown.", instance=instance)
3874             return True
3875 
3876         LOG.debug("Shutting down instance from state %s", state,
3877                   instance=instance)
3878         guest.shutdown()
3879         retry_countdown = retry_interval
3880 
3881         for sec in range(timeout):
3882 
3883             guest = self._host.get_guest(instance)
3884             state = guest.get_power_state(self._host)
3885 
3886             if state in SHUTDOWN_STATES:
3887                 LOG.info("Instance shutdown successfully after %d seconds.",
3888                          sec, instance=instance)
3889                 return True
3890 
3891             # Note(PhilD): We can't assume that the Guest was able to process
3892             #              any previous shutdown signal (for example it may
3893             #              have still been startingup, so within the overall
3894             #              timeout we re-trigger the shutdown every
3895             #              retry_interval
3896             if retry_countdown == 0:
3897                 retry_countdown = retry_interval
3898                 # Instance could shutdown at any time, in which case we
3899                 # will get an exception when we call shutdown
3900                 try:
3901                     LOG.debug("Instance in state %s after %d seconds - "
3902                               "resending shutdown", state, sec,
3903                               instance=instance)
3904                     guest.shutdown()
3905                 except libvirt.libvirtError:
3906                     # Assume this is because its now shutdown, so loop
3907                     # one more time to clean up.
3908                     LOG.debug("Ignoring libvirt exception from shutdown "
3909                               "request.", instance=instance)
3910                     continue
3911             else:
3912                 retry_countdown -= 1
3913 
3914             time.sleep(1)
3915 
3916         LOG.info("Instance failed to shutdown in %d seconds.",
3917                  timeout, instance=instance)
3918         return False
3919 
3920     def power_off(self, instance, timeout=0, retry_interval=0):
3921         """Power off the specified instance."""
3922         if timeout:
3923             self._clean_shutdown(instance, timeout, retry_interval)
3924         self._destroy(instance)
3925 
3926     def power_on(self, context, instance, network_info,
3927                  block_device_info=None, accel_info=None):
3928         """Power on the specified instance."""
3929         # We use _hard_reboot here to ensure that all backing files,
3930         # network, and block device connections, etc. are established
3931         # and available before we attempt to start the instance.
3932         self._hard_reboot(context, instance, network_info, block_device_info,
3933                           accel_info)
3934 
3935     def trigger_crash_dump(self, instance):
3936 
3937         """Trigger crash dump by injecting an NMI to the specified instance."""
3938         try:
3939             self._host.get_guest(instance).inject_nmi()
3940         except libvirt.libvirtError as ex:
3941             error_code = ex.get_error_code()
3942 
3943             if error_code == libvirt.VIR_ERR_NO_SUPPORT:
3944                 raise exception.TriggerCrashDumpNotSupported()
3945             elif error_code == libvirt.VIR_ERR_OPERATION_INVALID:
3946                 raise exception.InstanceNotRunning(instance_id=instance.uuid)
3947 
3948             LOG.exception(
3949                 'Error from libvirt while injecting an NMI to '
3950                 '%(instance_uuid)s: [Error Code %(error_code)s] %(ex)s',
3951                 {'instance_uuid': instance.uuid,
3952                  'error_code': error_code, 'ex': ex})
3953             raise
3954 
3955     def suspend(self, context, instance):
3956         """Suspend the specified instance."""
3957         guest = self._host.get_guest(instance)
3958 
3959         self._detach_pci_devices(guest,
3960             pci_manager.get_instance_pci_devs(instance))
3961         self._detach_direct_passthrough_ports(context, instance, guest)
3962         self._detach_mediated_devices(guest)
3963         guest.save_memory_state()
3964 
3965     def resume(self, context, instance, network_info, block_device_info=None):
3966         """resume the specified instance."""
3967         xml = self._get_existing_domain_xml(instance, network_info,
3968                                             block_device_info)
3969         # NOTE(gsantos): The mediated devices that were removed on suspension
3970         # are still present in the xml. Let's take their references from it
3971         # and re-attach them.
3972         mdevs = self._recover_attached_mdevs_from_guest_config(xml)
3973         # NOTE(efried): The instance should already have a vtpm_secret_uuid
3974         # registered if appropriate.
3975         guest = self._create_guest_with_network(
3976             context, xml, instance, network_info, block_device_info,
3977             vifs_already_plugged=True)
3978         self._attach_pci_devices(guest,
3979             pci_manager.get_instance_pci_devs(instance))
3980         self._attach_direct_passthrough_ports(
3981             context, instance, guest, network_info)
3982         self._attach_mediated_devices(guest, mdevs)
3983         timer = loopingcall.FixedIntervalLoopingCall(self._wait_for_running,
3984                                                      instance)
3985         timer.start(interval=0.5).wait()
3986         guest.sync_guest_time()
3987 
3988     def resume_state_on_host_boot(self, context, instance, network_info,
3989                                   block_device_info=None):
3990         """resume guest state when a host is booted."""
3991         # Check if the instance is running already and avoid doing
3992         # anything if it is.
3993         try:
3994             guest = self._host.get_guest(instance)
3995             state = guest.get_power_state(self._host)
3996 
3997             ignored_states = (power_state.RUNNING,
3998                               power_state.SUSPENDED,
3999                               power_state.NOSTATE,
4000                               power_state.PAUSED)
4001 
4002             if state in ignored_states:
4003                 return
4004         except (exception.InternalError, exception.InstanceNotFound):
4005             pass
4006 
4007         # Instance is not up and could be in an unknown state.
4008         # Be as absolute as possible about getting it back into
4009         # a known and running state.
4010         self._hard_reboot(context, instance, network_info, block_device_info)
4011 
4012     def rescue(self, context, instance, network_info, image_meta,
4013                rescue_password, block_device_info):
4014         """Loads a VM using rescue images.
4015 
4016         A rescue is normally performed when something goes wrong with the
4017         primary images and data needs to be corrected/recovered. Rescuing
4018         should not edit or over-ride the original image, only allow for
4019         data recovery.
4020 
4021         Two modes are provided when rescuing an instance with this driver.
4022 
4023         The original and default rescue mode, where the rescue boot disk,
4024         original root disk and optional regenerated config drive are attached
4025         to the instance.
4026 
4027         A second stable device rescue mode is also provided where all of the
4028         original devices are attached to the instance during the rescue attempt
4029         with the addition of the rescue boot disk. This second mode is
4030         controlled by the hw_rescue_device and hw_rescue_bus image properties
4031         on the rescue image provided to this method via image_meta.
4032 
4033         :param nova.context.RequestContext context:
4034             The context for the rescue.
4035         :param nova.objects.instance.Instance instance:
4036             The instance being rescued.
4037         :param nova.network.model.NetworkInfo network_info:
4038             Necessary network information for the resume.
4039         :param nova.objects.ImageMeta image_meta:
4040             The metadata of the image of the instance.
4041         :param rescue_password: new root password to set for rescue.
4042         :param dict block_device_info:
4043             The block device mapping of the instance.
4044         """
4045         instance_dir = libvirt_utils.get_instance_path(instance)
4046         unrescue_xml = self._get_existing_domain_xml(instance, network_info)
4047         unrescue_xml_path = os.path.join(instance_dir, 'unrescue.xml')
4048         with open(unrescue_xml_path, 'w') as f:
4049             f.write(unrescue_xml)
4050 
4051         rescue_image_id = None
4052         rescue_image_meta = None
4053         if image_meta.obj_attr_is_set("id"):
4054             rescue_image_id = image_meta.id
4055 
4056         rescue_images = {
4057             'image_id': (rescue_image_id or
4058                         CONF.libvirt.rescue_image_id or instance.image_ref),
4059             'kernel_id': (CONF.libvirt.rescue_kernel_id or
4060                           instance.kernel_id),
4061             'ramdisk_id': (CONF.libvirt.rescue_ramdisk_id or
4062                            instance.ramdisk_id),
4063         }
4064 
4065         virt_type = CONF.libvirt.virt_type
4066         if hardware.check_hw_rescue_props(image_meta):
4067             LOG.info("Attempting a stable device rescue", instance=instance)
4068             # NOTE(lyarwood): Stable device rescue is not supported when using
4069             # the LXC virt_type as it does not support the required
4070             # <boot order=''> definitions allowing an instance to boot from the
4071             # rescue device added as a final device to the domain.
4072             if virt_type == 'lxc':
4073                 reason = _(
4074                     "Stable device rescue is not supported by virt_type '%s'"
4075                 )
4076                 raise exception.InstanceNotRescuable(
4077                     instance_id=instance.uuid, reason=reason % virt_type)
4078             # NOTE(lyarwood): Stable device rescue provides the original disk
4079             # mapping of the instance with the rescue device appened to the
4080             # end. As a result we need to provide the original image_meta, the
4081             # new rescue_image_meta and block_device_info when calling
4082             # get_disk_info.
4083             rescue_image_meta = image_meta
4084             if instance.image_ref:
4085                 image_meta = objects.ImageMeta.from_image_ref(
4086                     context, self._image_api, instance.image_ref)
4087             else:
4088                 # NOTE(lyarwood): If instance.image_ref isn't set attempt to
4089                 # lookup the original image_meta from the bdms. This will
4090                 # return an empty dict if no valid image_meta is found.
4091                 image_meta_dict = block_device.get_bdm_image_metadata(
4092                     context, self._image_api, self._volume_api,
4093                     block_device_info['block_device_mapping'],
4094                     legacy_bdm=False)
4095                 image_meta = objects.ImageMeta.from_dict(image_meta_dict)
4096 
4097         else:
4098             LOG.info("Attempting rescue", instance=instance)
4099             # NOTE(lyarwood): A legacy rescue only provides the rescue device
4100             # and the original root device so we don't need to provide
4101             # block_device_info to the get_disk_info call.
4102             block_device_info = None
4103 
4104         disk_info = blockinfo.get_disk_info(virt_type, instance, image_meta,
4105             rescue=True, block_device_info=block_device_info,
4106             rescue_image_meta=rescue_image_meta)
4107         LOG.debug("rescue generated disk_info: %s", disk_info)
4108 
4109         injection_info = InjectionInfo(network_info=network_info,
4110                                        admin_pass=rescue_password,
4111                                        files=None)
4112         gen_confdrive = functools.partial(self._create_configdrive,
4113                                           context, instance, injection_info,
4114                                           rescue=True)
4115         # NOTE(sbauza): Since rescue recreates the guest XML, we need to
4116         # remember the existing mdevs for reusing them.
4117         mdevs = self._get_all_assigned_mediated_devices(instance)
4118         mdevs = list(mdevs.keys())
4119         self._create_image(context, instance, disk_info['mapping'],
4120                            injection_info=injection_info, suffix='.rescue',
4121                            disk_images=rescue_images)
4122         # NOTE(efried): The instance should already have a vtpm_secret_uuid
4123         # registered if appropriate.
4124         xml = self._get_guest_xml(context, instance, network_info, disk_info,
4125                                   image_meta, rescue=rescue_images,
4126                                   mdevs=mdevs,
4127                                   block_device_info=block_device_info)
4128         self._destroy(instance)
4129         self._create_guest(
4130             context, xml, instance, post_xml_callback=gen_confdrive,
4131         )
4132 
4133     def unrescue(
4134         self,
4135         context: nova_context.RequestContext,
4136         instance: 'objects.Instance',
4137     ):
4138         """Reboot the VM which is being rescued back into primary images."""
4139         instance_dir = libvirt_utils.get_instance_path(instance)
4140         unrescue_xml_path = os.path.join(instance_dir, 'unrescue.xml')
4141         # The xml should already contain the secret_uuid if relevant.
4142         xml = libvirt_utils.load_file(unrescue_xml_path)
4143 
4144         self._destroy(instance)
4145         self._create_guest(context, xml, instance)
4146         os.unlink(unrescue_xml_path)
4147         rescue_files = os.path.join(instance_dir, "*.rescue")
4148         for rescue_file in glob.iglob(rescue_files):
4149             if os.path.isdir(rescue_file):
4150                 shutil.rmtree(rescue_file)
4151             else:
4152                 os.unlink(rescue_file)
4153         # cleanup rescue volume
4154         lvm.remove_volumes([lvmdisk for lvmdisk in self._lvm_disks(instance)
4155                                 if lvmdisk.endswith('.rescue')])
4156         if CONF.libvirt.images_type == 'rbd':
4157             filter_fn = lambda disk: (disk.startswith(instance.uuid) and
4158                                       disk.endswith('.rescue'))
4159             rbd_utils.RBDDriver().cleanup_volumes(filter_fn)
4160 
4161     def poll_rebooting_instances(self, timeout, instances):
4162         pass
4163 
4164     def spawn(self, context, instance, image_meta, injected_files,
4165               admin_password, allocations, network_info=None,
4166               block_device_info=None, power_on=True, accel_info=None):
4167         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
4168                                             instance,
4169                                             image_meta,
4170                                             block_device_info)
4171         injection_info = InjectionInfo(network_info=network_info,
4172                                        files=injected_files,
4173                                        admin_pass=admin_password)
4174         gen_confdrive = functools.partial(self._create_configdrive,
4175                                           context, instance,
4176                                           injection_info)
4177         created_instance_dir, created_disks = self._create_image(
4178                 context, instance, disk_info['mapping'],
4179                 injection_info=injection_info,
4180                 block_device_info=block_device_info)
4181 
4182         # Required by Quobyte CI
4183         self._ensure_console_log_for_instance(instance)
4184 
4185         # Does the guest need to be assigned some vGPU mediated devices ?
4186         mdevs = self._allocate_mdevs(allocations)
4187 
4188         # If the guest needs a vTPM, _get_guest_xml needs its secret to exist
4189         # and its uuid to be registered in the instance prior to _get_guest_xml
4190         if CONF.libvirt.swtpm_enabled and hardware.get_vtpm_constraint(
4191             instance.flavor, image_meta
4192         ):
4193             if not instance.system_metadata.get('vtpm_secret_uuid'):
4194                 # Create the secret via the key manager service so that we have
4195                 # it to hand when generating the XML. This is slightly wasteful
4196                 # as we'll perform a redundant key manager API call later when
4197                 # we create the domain but the alternative is an ugly mess
4198                 crypto.ensure_vtpm_secret(context, instance)
4199 
4200         xml = self._get_guest_xml(context, instance, network_info,
4201                                   disk_info, image_meta,
4202                                   block_device_info=block_device_info,
4203                                   mdevs=mdevs, accel_info=accel_info)
4204         self._create_guest_with_network(
4205             context, xml, instance, network_info, block_device_info,
4206             post_xml_callback=gen_confdrive,
4207             power_on=power_on,
4208             cleanup_instance_dir=created_instance_dir,
4209             cleanup_instance_disks=created_disks)
4210         LOG.debug("Guest created on hypervisor", instance=instance)
4211 
4212         def _wait_for_boot():
4213             """Called at an interval until the VM is running."""
4214             state = self.get_info(instance).state
4215 
4216             if state == power_state.RUNNING:
4217                 LOG.info("Instance spawned successfully.", instance=instance)
4218                 raise loopingcall.LoopingCallDone()
4219 
4220         if power_on:
4221             timer = loopingcall.FixedIntervalLoopingCall(_wait_for_boot)
4222             timer.start(interval=0.5).wait()
4223         else:
4224             LOG.info("Instance spawned successfully.", instance=instance)
4225 
4226     def _get_console_output_file(self, instance, console_log):
4227         bytes_to_read = MAX_CONSOLE_BYTES
4228         log_data = b""  # The last N read bytes
4229         i = 0  # in case there is a log rotation (like "virtlogd")
4230         path = console_log
4231 
4232         while bytes_to_read > 0 and os.path.exists(path):
4233             read_log_data, remaining = nova.privsep.path.last_bytes(
4234                                         path, bytes_to_read)
4235             # We need the log file content in chronological order,
4236             # that's why we *prepend* the log data.
4237             log_data = read_log_data + log_data
4238 
4239             # Prep to read the next file in the chain
4240             bytes_to_read -= len(read_log_data)
4241             path = console_log + "." + str(i)
4242             i += 1
4243 
4244             if remaining > 0:
4245                 LOG.info('Truncated console log returned, '
4246                          '%d bytes ignored', remaining, instance=instance)
4247         return log_data
4248 
4249     def get_console_output(self, context, instance):
4250         guest = self._host.get_guest(instance)
4251 
4252         xml = guest.get_xml_desc()
4253         tree = etree.fromstring(xml)
4254 
4255         # check for different types of consoles
4256         path_sources = [
4257             ('file', "./devices/console[@type='file']/source[@path]", 'path'),
4258             ('tcp', "./devices/console[@type='tcp']/log[@file]", 'file'),
4259             ('pty', "./devices/console[@type='pty']/source[@path]", 'path')]
4260         console_type = ""
4261         console_path = ""
4262         for c_type, epath, attrib in path_sources:
4263             node = tree.find(epath)
4264             if (node is not None) and node.get(attrib):
4265                 console_type = c_type
4266                 console_path = node.get(attrib)
4267                 break
4268 
4269         # instance has no console at all
4270         if not console_path:
4271             raise exception.ConsoleNotAvailable()
4272 
4273         # instance has a console, but file doesn't exist (yet?)
4274         if not os.path.exists(console_path):
4275             LOG.info('console logfile for instance does not exist',
4276                       instance=instance)
4277             return ""
4278 
4279         # pty consoles need special handling
4280         if console_type == 'pty':
4281             console_log = self._get_console_log_path(instance)
4282             data = nova.privsep.libvirt.readpty(console_path)
4283 
4284             # NOTE(markus_z): The virt_types kvm and qemu are the only ones
4285             # which create a dedicated file device for the console Logs.
4286             # Other virt_types like lxc and parallels depend on the flush of
4287             # that PTY device into the "console.log" file to ensure that a
4288             # series of "get_console_output" calls return the complete content
4289             # even after rebooting a guest.
4290             nova.privsep.path.writefile(console_log, 'a+', data)
4291 
4292             # set console path to logfile, not to pty device
4293             console_path = console_log
4294 
4295         # return logfile content
4296         return self._get_console_output_file(instance, console_path)
4297 
4298     def get_host_ip_addr(self):
4299         return CONF.my_ip
4300 
4301     def get_vnc_console(self, context, instance):
4302         def get_vnc_port_for_instance(instance_name):
4303             guest = self._host.get_guest(instance)
4304 
4305             xml = guest.get_xml_desc()
4306             xml_dom = etree.fromstring(xml)
4307 
4308             graphic = xml_dom.find("./devices/graphics[@type='vnc']")
4309             if graphic is not None:
4310                 return graphic.get('port')
4311             # NOTE(rmk): We had VNC consoles enabled but the instance in
4312             # question is not actually listening for connections.
4313             raise exception.ConsoleTypeUnavailable(console_type='vnc')
4314 
4315         port = get_vnc_port_for_instance(instance.name)
4316         host = CONF.vnc.server_proxyclient_address
4317 
4318         return ctype.ConsoleVNC(host=host, port=port)
4319 
4320     def get_spice_console(self, context, instance):
4321         def get_spice_ports_for_instance(instance_name):
4322             guest = self._host.get_guest(instance)
4323 
4324             xml = guest.get_xml_desc()
4325             xml_dom = etree.fromstring(xml)
4326 
4327             graphic = xml_dom.find("./devices/graphics[@type='spice']")
4328             if graphic is not None:
4329                 return (graphic.get('port'), graphic.get('tlsPort'))
4330             # NOTE(rmk): We had Spice consoles enabled but the instance in
4331             # question is not actually listening for connections.
4332             raise exception.ConsoleTypeUnavailable(console_type='spice')
4333 
4334         ports = get_spice_ports_for_instance(instance.name)
4335         host = CONF.spice.server_proxyclient_address
4336 
4337         return ctype.ConsoleSpice(host=host, port=ports[0], tlsPort=ports[1])
4338 
4339     def get_serial_console(self, context, instance):
4340         guest = self._host.get_guest(instance)
4341         for hostname, port in self._get_serial_ports_from_guest(
4342                 guest, mode='bind'):
4343             return ctype.ConsoleSerial(host=hostname, port=port)
4344         raise exception.ConsoleTypeUnavailable(console_type='serial')
4345 
4346     @staticmethod
4347     def _create_ephemeral(target, ephemeral_size,
4348                           fs_label, os_type, is_block_dev=False,
4349                           context=None, specified_fs=None,
4350                           vm_mode=None):
4351         if not is_block_dev:
4352             if (CONF.libvirt.virt_type == "parallels" and
4353                     vm_mode == fields.VMMode.EXE):
4354 
4355                 libvirt_utils.create_ploop_image('expanded', target,
4356                                                  '%dG' % ephemeral_size,
4357                                                  specified_fs)
4358                 return
4359             libvirt_utils.create_image('raw', target, '%dG' % ephemeral_size)
4360 
4361         # Run as root only for block devices.
4362         disk_api.mkfs(os_type, fs_label, target, run_as_root=is_block_dev,
4363                       specified_fs=specified_fs)
4364 
4365     @staticmethod
4366     def _create_swap(target, swap_mb, context=None):
4367         """Create a swap file of specified size."""
4368         libvirt_utils.create_image('raw', target, '%dM' % swap_mb)
4369         nova.privsep.fs.unprivileged_mkfs('swap', target)
4370 
4371     @staticmethod
4372     def _get_console_log_path(instance):
4373         return os.path.join(libvirt_utils.get_instance_path(instance),
4374                             'console.log')
4375 
4376     def _ensure_console_log_for_instance(self, instance):
4377         # NOTE(mdbooth): Although libvirt will create this file for us
4378         # automatically when it starts, it will initially create it with
4379         # root ownership and then chown it depending on the configuration of
4380         # the domain it is launching. Quobyte CI explicitly disables the
4381         # chown by setting dynamic_ownership=0 in libvirt's config.
4382         # Consequently when the domain starts it is unable to write to its
4383         # console.log. See bug https://bugs.launchpad.net/nova/+bug/1597644
4384         #
4385         # To work around this, we create the file manually before starting
4386         # the domain so it has the same ownership as Nova. This works
4387         # for Quobyte CI because it is also configured to run qemu as the same
4388         # user as the Nova service. Installations which don't set
4389         # dynamic_ownership=0 are not affected because libvirt will always
4390         # correctly configure permissions regardless of initial ownership.
4391         #
4392         # Setting dynamic_ownership=0 is dubious and potentially broken in
4393         # more ways than console.log (see comment #22 on the above bug), so
4394         # Future Maintainer who finds this code problematic should check to see
4395         # if we still support it.
4396         console_file = self._get_console_log_path(instance)
4397         LOG.debug('Ensure instance console log exists: %s', console_file,
4398                   instance=instance)
4399         try:
4400             libvirt_utils.file_open(console_file, 'a').close()
4401         # NOTE(sfinucan): We can safely ignore permission issues here and
4402         # assume that it is libvirt that has taken ownership of this file.
4403         except IOError as ex:
4404             if ex.errno != errno.EACCES:
4405                 raise
4406             LOG.debug('Console file already exists: %s.', console_file)
4407 
4408     @staticmethod
4409     def _get_disk_config_image_type():
4410         # TODO(mikal): there is a bug here if images_type has
4411         # changed since creation of the instance, but I am pretty
4412         # sure that this bug already exists.
4413         return 'rbd' if CONF.libvirt.images_type == 'rbd' else 'raw'
4414 
4415     @staticmethod
4416     def _is_booted_from_volume(block_device_info):
4417         """Determines whether the VM is booting from volume
4418 
4419         Determines whether the block device info indicates that the VM
4420         is booting from a volume.
4421         """
4422         block_device_mapping = driver.block_device_info_get_mapping(
4423             block_device_info)
4424         return bool(block_device.get_root_bdm(block_device_mapping))
4425 
4426     def _inject_data(self, disk, instance, injection_info):
4427         """Injects data in a disk image
4428 
4429         Helper used for injecting data in a disk image file system.
4430 
4431         :param disk: The disk we're injecting into (an Image object)
4432         :param instance: The instance we're injecting into
4433         :param injection_info: Injection info
4434         """
4435         # Handles the partition need to be used.
4436         LOG.debug('Checking root disk injection %s',
4437                   str(injection_info), instance=instance)
4438         target_partition = None
4439         if not instance.kernel_id:
4440             target_partition = CONF.libvirt.inject_partition
4441             if target_partition == 0:
4442                 target_partition = None
4443         if CONF.libvirt.virt_type == 'lxc':
4444             target_partition = None
4445 
4446         # Handles the key injection.
4447         key = None
4448         if CONF.libvirt.inject_key and instance.get('key_data'):
4449             key = str(instance.key_data)
4450 
4451         # Handles the admin password injection.
4452         admin_pass = None
4453         if CONF.libvirt.inject_password:
4454             admin_pass = injection_info.admin_pass
4455 
4456         # Handles the network injection.
4457         net = netutils.get_injected_network_template(
4458             injection_info.network_info,
4459             libvirt_virt_type=CONF.libvirt.virt_type)
4460 
4461         # Handles the metadata injection
4462         metadata = instance.get('metadata')
4463 
4464         if any((key, net, metadata, admin_pass, injection_info.files)):
4465             LOG.debug('Injecting %s', str(injection_info),
4466                       instance=instance)
4467             img_id = instance.image_ref
4468             try:
4469                 disk_api.inject_data(disk.get_model(self._conn),
4470                                      key, net, metadata, admin_pass,
4471                                      injection_info.files,
4472                                      partition=target_partition,
4473                                      mandatory=('files',))
4474             except Exception as e:
4475                 with excutils.save_and_reraise_exception():
4476                     LOG.error('Error injecting data into image '
4477                               '%(img_id)s (%(e)s)',
4478                               {'img_id': img_id, 'e': e},
4479                               instance=instance)
4480 
4481     # NOTE(sileht): many callers of this method assume that this
4482     # method doesn't fail if an image already exists but instead
4483     # think that it will be reused (ie: (live)-migration/resize)
4484     def _create_image(self, context, instance,
4485                       disk_mapping, injection_info=None, suffix='',
4486                       disk_images=None, block_device_info=None,
4487                       fallback_from_host=None,
4488                       ignore_bdi_for_swap=False):
4489         booted_from_volume = self._is_booted_from_volume(block_device_info)
4490 
4491         def image(fname, image_type=CONF.libvirt.images_type):
4492             return self.image_backend.by_name(instance,
4493                                               fname + suffix, image_type)
4494 
4495         def raw(fname):
4496             return image(fname, image_type='raw')
4497 
4498         created_instance_dir = True
4499 
4500         # ensure directories exist and are writable
4501         instance_dir = libvirt_utils.get_instance_path(instance)
4502         if os.path.exists(instance_dir):
4503             LOG.debug("Instance directory exists: not creating",
4504                       instance=instance)
4505             created_instance_dir = False
4506         else:
4507             LOG.debug("Creating instance directory", instance=instance)
4508             fileutils.ensure_tree(libvirt_utils.get_instance_path(instance))
4509 
4510         LOG.info('Creating image', instance=instance)
4511 
4512         flavor = instance.get_flavor()
4513         swap_mb = 0
4514         if 'disk.swap' in disk_mapping:
4515             mapping = disk_mapping['disk.swap']
4516 
4517             if ignore_bdi_for_swap:
4518                 # This is a workaround to support legacy swap resizing,
4519                 # which does not touch swap size specified in bdm,
4520                 # but works with flavor specified size only.
4521                 # In this case we follow the legacy logic and ignore block
4522                 # device info completely.
4523                 # NOTE(ft): This workaround must be removed when a correct
4524                 # implementation of resize operation changing sizes in bdms is
4525                 # developed. Also at that stage we probably may get rid of
4526                 # the direct usage of flavor swap size here,
4527                 # leaving the work with bdm only.
4528                 swap_mb = flavor['swap']
4529             else:
4530                 swap = driver.block_device_info_get_swap(block_device_info)
4531                 if driver.swap_is_usable(swap):
4532                     swap_mb = swap['swap_size']
4533                 elif (flavor['swap'] > 0 and
4534                       not block_device.volume_in_mapping(
4535                         mapping['dev'], block_device_info)):
4536                     swap_mb = flavor['swap']
4537 
4538             if swap_mb > 0:
4539                 if (CONF.libvirt.virt_type == "parallels" and
4540                         instance.vm_mode == fields.VMMode.EXE):
4541                     msg = _("Swap disk is not supported "
4542                             "for Virtuozzo container")
4543                     raise exception.Invalid(msg)
4544 
4545         if not disk_images:
4546             disk_images = {'image_id': instance.image_ref,
4547                            'kernel_id': instance.kernel_id,
4548                            'ramdisk_id': instance.ramdisk_id}
4549 
4550         # NOTE(mdbooth): kernel and ramdisk, if they are defined, are hardcoded
4551         # to use raw, which means they will always be cleaned up with the
4552         # instance directory. We must not consider them for created_disks,
4553         # which may not be using the instance directory.
4554         if disk_images['kernel_id']:
4555             fname = imagecache.get_cache_fname(disk_images['kernel_id'])
4556             raw('kernel').cache(fetch_func=libvirt_utils.fetch_raw_image,
4557                                 context=context,
4558                                 filename=fname,
4559                                 image_id=disk_images['kernel_id'])
4560             if disk_images['ramdisk_id']:
4561                 fname = imagecache.get_cache_fname(disk_images['ramdisk_id'])
4562                 raw('ramdisk').cache(fetch_func=libvirt_utils.fetch_raw_image,
4563                                      context=context,
4564                                      filename=fname,
4565                                      image_id=disk_images['ramdisk_id'])
4566 
4567         created_disks = self._create_and_inject_local_root(
4568             context, instance, booted_from_volume, suffix, disk_images,
4569             injection_info, fallback_from_host)
4570 
4571         # Lookup the filesystem type if required
4572         os_type_with_default = nova.privsep.fs.get_fs_type_for_os_type(
4573             instance.os_type)
4574         # Generate a file extension based on the file system
4575         # type and the mkfs commands configured if any
4576         file_extension = nova.privsep.fs.get_file_extension_for_os_type(
4577             os_type_with_default, CONF.default_ephemeral_format)
4578 
4579         vm_mode = fields.VMMode.get_from_instance(instance)
4580         ephemeral_gb = instance.flavor.ephemeral_gb
4581         if 'disk.local' in disk_mapping:
4582             disk_image = image('disk.local')
4583             # Short circuit the exists() tests if we already created a disk
4584             created_disks = created_disks or not disk_image.exists()
4585 
4586             fn = functools.partial(self._create_ephemeral,
4587                                    fs_label='ephemeral0',
4588                                    os_type=instance.os_type,
4589                                    is_block_dev=disk_image.is_block_dev,
4590                                    vm_mode=vm_mode)
4591             fname = "ephemeral_%s_%s" % (ephemeral_gb, file_extension)
4592             size = ephemeral_gb * units.Gi
4593             disk_image.cache(fetch_func=fn,
4594                              context=context,
4595                              filename=fname,
4596                              size=size,
4597                              ephemeral_size=ephemeral_gb)
4598 
4599         for idx, eph in enumerate(driver.block_device_info_get_ephemerals(
4600                 block_device_info)):
4601             disk_image = image(blockinfo.get_eph_disk(idx))
4602             # Short circuit the exists() tests if we already created a disk
4603             created_disks = created_disks or not disk_image.exists()
4604 
4605             specified_fs = eph.get('guest_format')
4606             if specified_fs and not self.is_supported_fs_format(specified_fs):
4607                 msg = _("%s format is not supported") % specified_fs
4608                 raise exception.InvalidBDMFormat(details=msg)
4609 
4610             fn = functools.partial(self._create_ephemeral,
4611                                    fs_label='ephemeral%d' % idx,
4612                                    os_type=instance.os_type,
4613                                    is_block_dev=disk_image.is_block_dev,
4614                                    vm_mode=vm_mode)
4615             size = eph['size'] * units.Gi
4616             fname = "ephemeral_%s_%s" % (eph['size'], file_extension)
4617             disk_image.cache(fetch_func=fn,
4618                              context=context,
4619                              filename=fname,
4620                              size=size,
4621                              ephemeral_size=eph['size'],
4622                              specified_fs=specified_fs)
4623 
4624         if swap_mb > 0:
4625             size = swap_mb * units.Mi
4626             swap = image('disk.swap')
4627             # Short circuit the exists() tests if we already created a disk
4628             created_disks = created_disks or not swap.exists()
4629             swap.cache(fetch_func=self._create_swap, context=context,
4630                        filename="swap_%s" % swap_mb,
4631                        size=size, swap_mb=swap_mb)
4632 
4633         if created_disks:
4634             LOG.debug('Created local disks', instance=instance)
4635         else:
4636             LOG.debug('Did not create local disks', instance=instance)
4637 
4638         return (created_instance_dir, created_disks)
4639 
4640     def _create_and_inject_local_root(self, context, instance,
4641                                       booted_from_volume, suffix, disk_images,
4642                                       injection_info, fallback_from_host):
4643         created_disks = False
4644 
4645         # File injection only if needed
4646         need_inject = (not configdrive.required_by(instance) and
4647                        injection_info is not None and
4648                        CONF.libvirt.inject_partition != -2)
4649 
4650         # NOTE(ndipanov): Even if disk_mapping was passed in, which
4651         # currently happens only on rescue - we still don't want to
4652         # create a base image.
4653         if not booted_from_volume:
4654             root_fname = imagecache.get_cache_fname(disk_images['image_id'])
4655             size = instance.flavor.root_gb * units.Gi
4656 
4657             if size == 0 or suffix == '.rescue':
4658                 size = None
4659 
4660             backend = self.image_backend.by_name(instance, 'disk' + suffix,
4661                                                  CONF.libvirt.images_type)
4662             created_disks = not backend.exists()
4663 
4664             if instance.task_state == task_states.RESIZE_FINISH:
4665                 backend.create_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME)
4666             if backend.SUPPORTS_CLONE:
4667                 def clone_fallback_to_fetch(
4668                     context, target, image_id, trusted_certs=None,
4669                 ):
4670                     refuse_fetch = (
4671                         CONF.libvirt.images_type == 'rbd' and
4672                         CONF.workarounds.never_download_image_if_on_rbd)
4673                     try:
4674                         backend.clone(context, disk_images['image_id'])
4675                     except exception.ImageUnacceptable:
4676                         if refuse_fetch:
4677                             # Re-raise the exception from the failed
4678                             # ceph clone.  The compute manager expects
4679                             # ImageUnacceptable as a possible result
4680                             # of spawn(), from which this is called.
4681                             with excutils.save_and_reraise_exception():
4682                                 LOG.warning(
4683                                     'Image %s is not on my ceph and '
4684                                     '[workarounds]/'
4685                                     'never_download_image_if_on_rbd=True;'
4686                                     ' refusing to fetch and upload.',
4687                                     disk_images['image_id'])
4688                         libvirt_utils.fetch_image(
4689                             context, target, image_id, trusted_certs,
4690                         )
4691                 fetch_func = clone_fallback_to_fetch
4692             else:
4693                 fetch_func = libvirt_utils.fetch_image
4694 
4695             self._try_fetch_image_cache(backend, fetch_func, context,
4696                                         root_fname, disk_images['image_id'],
4697                                         instance, size, fallback_from_host)
4698 
4699             # During unshelve or cross cell resize on Qcow2 backend, we spawn()
4700             # using a snapshot image. Extra work is needed in order to rebase
4701             # disk image to its original image_ref. Disk backing file will
4702             # then represent back image_ref instead of snapshot image.
4703             self._rebase_original_qcow2_image(context, instance, backend)
4704 
4705             if need_inject:
4706                 self._inject_data(backend, instance, injection_info)
4707 
4708         elif need_inject:
4709             LOG.warning('File injection into a boot from volume '
4710                         'instance is not supported', instance=instance)
4711 
4712         return created_disks
4713 
4714     def _needs_rebase_original_qcow2_image(self, instance, backend):
4715         if not isinstance(backend, imagebackend.Qcow2):
4716             return False
4717         if instance.vm_state == vm_states.SHELVED_OFFLOADED:
4718             return True
4719         if instance.task_state == task_states.RESIZE_FINISH:
4720             # We need to distinguish between local versus cross cell resize.
4721             # Rebase is only needed in cross cell case because instance
4722             # is spawn from a snapshot.
4723             base_image_ref = instance.system_metadata.get(
4724                     'image_base_image_ref')
4725             if base_image_ref != instance.image_ref:
4726                 return True
4727         return False
4728 
4729     def _rebase_original_qcow2_image(self, context, instance, backend):
4730         # NOTE(aarents): During qcow2 instance unshelve/cross_cell_resize,
4731         # backing file represents a snapshot image, not original
4732         # instance.image_ref. We rebase here instance disk to original image.
4733         # This second fetch call does nothing except downloading original
4734         # backing file if missing, as image disk have already been
4735         # created/resized by first fetch call.
4736 
4737         if not self._needs_rebase_original_qcow2_image(instance, backend):
4738             return
4739 
4740         base_dir = self.image_cache_manager.cache_dir
4741         base_image_ref = instance.system_metadata.get('image_base_image_ref')
4742         root_fname = imagecache.get_cache_fname(base_image_ref)
4743         base_backing_fname = os.path.join(base_dir, root_fname)
4744 
4745         try:
4746             self._try_fetch_image_cache(backend, libvirt_utils.fetch_image,
4747                                         context, root_fname, base_image_ref,
4748                                         instance, None)
4749         except exception.ImageNotFound:
4750             # We must flatten here in order to remove dependency with an orphan
4751             # backing file (as snapshot image will be dropped once
4752             # unshelve/cross_cell_resize is successfull).
4753             LOG.warning('Current disk image is created on top of a snapshot '
4754                         'image and cannot be rebased to original image '
4755                         'because it is no longer available in the image '
4756                         'service, disk will be consequently flattened.',
4757                         instance=instance)
4758             base_backing_fname = None
4759 
4760         LOG.info('Rebasing disk image.', instance=instance)
4761         self._rebase_with_qemu_img(backend.path, base_backing_fname)
4762 
4763     def _create_configdrive(self, context, instance, injection_info,
4764                             rescue=False):
4765         # As this method being called right after the definition of a
4766         # domain, but before its actual launch, device metadata will be built
4767         # and saved in the instance for it to be used by the config drive and
4768         # the metadata service.
4769         instance.device_metadata = self._build_device_metadata(context,
4770                                                                instance)
4771         if configdrive.required_by(instance):
4772             LOG.info('Using config drive', instance=instance)
4773 
4774             name = 'disk.config'
4775             if rescue:
4776                 name += '.rescue'
4777 
4778             config_disk = self.image_backend.by_name(
4779                 instance, name, self._get_disk_config_image_type())
4780 
4781             # Don't overwrite an existing config drive
4782             if not config_disk.exists():
4783                 extra_md = {}
4784                 if injection_info.admin_pass:
4785                     extra_md['admin_pass'] = injection_info.admin_pass
4786 
4787                 inst_md = instance_metadata.InstanceMetadata(
4788                     instance, content=injection_info.files, extra_md=extra_md,
4789                     network_info=injection_info.network_info)
4790 
4791                 cdb = configdrive.ConfigDriveBuilder(instance_md=inst_md)
4792                 with cdb:
4793                     # NOTE(mdbooth): We're hardcoding here the path of the
4794                     # config disk when using the flat backend. This isn't
4795                     # good, but it's required because we need a local path we
4796                     # know we can write to in case we're subsequently
4797                     # importing into rbd. This will be cleaned up when we
4798                     # replace this with a call to create_from_func, but that
4799                     # can't happen until we've updated the backends and we
4800                     # teach them not to cache config disks. This isn't
4801                     # possible while we're still using cache() under the hood.
4802                     config_disk_local_path = os.path.join(
4803                         libvirt_utils.get_instance_path(instance), name)
4804                     LOG.info('Creating config drive at %(path)s',
4805                              {'path': config_disk_local_path},
4806                              instance=instance)
4807 
4808                     try:
4809                         cdb.make_drive(config_disk_local_path)
4810                     except processutils.ProcessExecutionError as e:
4811                         with excutils.save_and_reraise_exception():
4812                             LOG.error('Creating config drive failed with '
4813                                       'error: %s', e, instance=instance)
4814 
4815                 try:
4816                     config_disk.import_file(
4817                         instance, config_disk_local_path, name)
4818                 finally:
4819                     # NOTE(mikal): if the config drive was imported into RBD,
4820                     # then we no longer need the local copy
4821                     if CONF.libvirt.images_type == 'rbd':
4822                         LOG.info('Deleting local config drive %(path)s '
4823                                  'because it was imported into RBD.',
4824                                  {'path': config_disk_local_path},
4825                                  instance=instance)
4826                         os.unlink(config_disk_local_path)
4827 
4828     def _detach_pci_devices(self, guest, pci_devs):
4829         try:
4830             for dev in pci_devs:
4831                 guest.detach_device(self._get_guest_pci_device(dev), live=True)
4832                 # after detachDeviceFlags returned, we should check the dom to
4833                 # ensure the detaching is finished
4834                 xml = guest.get_xml_desc()
4835                 xml_doc = etree.fromstring(xml)
4836                 guest_config = vconfig.LibvirtConfigGuest()
4837                 guest_config.parse_dom(xml_doc)
4838 
4839                 for hdev in [d for d in guest_config.devices
4840                     if isinstance(d, vconfig.LibvirtConfigGuestHostdevPCI)]:
4841                     hdbsf = [hdev.domain, hdev.bus, hdev.slot, hdev.function]
4842                     dbsf = pci_utils.parse_address(dev.address)
4843                     if [int(x, 16) for x in hdbsf] ==\
4844                             [int(x, 16) for x in dbsf]:
4845                         raise exception.PciDeviceDetachFailed(reason=
4846                                                               "timeout",
4847                                                               dev=dev)
4848 
4849         except libvirt.libvirtError as ex:
4850             error_code = ex.get_error_code()
4851             if error_code == libvirt.VIR_ERR_NO_DOMAIN:
4852                 LOG.warning("Instance disappeared while detaching "
4853                             "a PCI device from it.")
4854             else:
4855                 raise
4856 
4857     def _attach_pci_devices(self, guest, pci_devs):
4858         try:
4859             for dev in pci_devs:
4860                 guest.attach_device(self._get_guest_pci_device(dev))
4861 
4862         except libvirt.libvirtError:
4863             LOG.error('Attaching PCI devices %(dev)s to %(dom)s failed.',
4864                       {'dev': pci_devs, 'dom': guest.id})
4865             raise
4866 
4867     @staticmethod
4868     def _has_direct_passthrough_port(network_info):
4869         for vif in network_info:
4870             if (vif['vnic_type'] in
4871                 network_model.VNIC_TYPES_DIRECT_PASSTHROUGH):
4872                 return True
4873         return False
4874 
4875     def _attach_direct_passthrough_ports(
4876         self, context, instance, guest, network_info=None):
4877         if network_info is None:
4878             network_info = instance.info_cache.network_info
4879         if network_info is None:
4880             return
4881 
4882         if self._has_direct_passthrough_port(network_info):
4883             for vif in network_info:
4884                 if (vif['vnic_type'] in
4885                     network_model.VNIC_TYPES_DIRECT_PASSTHROUGH):
4886                     cfg = self.vif_driver.get_config(instance,
4887                                                      vif,
4888                                                      instance.image_meta,
4889                                                      instance.flavor,
4890                                                      CONF.libvirt.virt_type)
4891                     LOG.debug('Attaching direct passthrough port %(port)s '
4892                               'to %(dom)s', {'port': vif, 'dom': guest.id},
4893                               instance=instance)
4894                     guest.attach_device(cfg)
4895 
4896     def _detach_direct_passthrough_ports(self, context, instance, guest):
4897         network_info = instance.info_cache.network_info
4898         if network_info is None:
4899             return
4900 
4901         if self._has_direct_passthrough_port(network_info):
4902             # In case of VNIC_TYPES_DIRECT_PASSTHROUGH ports we create
4903             # pci request per direct passthrough port. Therefore we can trust
4904             # that pci_slot value in the vif is correct.
4905             direct_passthrough_pci_addresses = [
4906                 vif['profile']['pci_slot']
4907                 for vif in network_info
4908                 if (vif['vnic_type'] in
4909                     network_model.VNIC_TYPES_DIRECT_PASSTHROUGH and
4910                     vif['profile'].get('pci_slot') is not None)
4911             ]
4912 
4913             # use detach_pci_devices to avoid failure in case of
4914             # multiple guest direct passthrough ports with the same MAC
4915             # (protection use-case, ports are on different physical
4916             # interfaces)
4917             pci_devs = pci_manager.get_instance_pci_devs(instance, 'all')
4918             direct_passthrough_pci_addresses = (
4919                 [pci_dev for pci_dev in pci_devs
4920                  if pci_dev.address in direct_passthrough_pci_addresses])
4921             self._detach_pci_devices(guest, direct_passthrough_pci_addresses)
4922 
4923     def _update_compute_provider_status(self, context, service):
4924         """Calls the ComputeVirtAPI.update_compute_provider_status method
4925 
4926         :param context: nova auth RequestContext
4927         :param service: nova.objects.Service record for this host which is
4928             expected to only manage a single ComputeNode
4929         """
4930         rp_uuid = None
4931         try:
4932             rp_uuid = service.compute_node.uuid
4933             self.virtapi.update_compute_provider_status(
4934                 context, rp_uuid, enabled=not service.disabled)
4935         except Exception:
4936             # This is best effort so just log the exception but don't fail.
4937             # The update_available_resource periodic task will sync the trait.
4938             LOG.warning(
4939                 'An error occurred while updating compute node '
4940                 'resource provider status to "%s" for provider: %s',
4941                 'disabled' if service.disabled else 'enabled',
4942                 rp_uuid or service.host, exc_info=True)
4943 
4944     def _set_host_enabled(self, enabled,
4945                           disable_reason=DISABLE_REASON_UNDEFINED):
4946         """Enables / Disables the compute service on this host.
4947 
4948            This doesn't override non-automatic disablement with an automatic
4949            setting; thereby permitting operators to keep otherwise
4950            healthy hosts out of rotation.
4951         """
4952 
4953         status_name = {True: 'disabled',
4954                        False: 'enabled'}
4955 
4956         disable_service = not enabled
4957 
4958         ctx = nova_context.get_admin_context()
4959         try:
4960             service = objects.Service.get_by_compute_host(ctx, CONF.host)
4961 
4962             if service.disabled != disable_service:
4963                 # Note(jang): this is a quick fix to stop operator-
4964                 # disabled compute hosts from re-enabling themselves
4965                 # automatically. We prefix any automatic reason code
4966                 # with a fixed string. We only re-enable a host
4967                 # automatically if we find that string in place.
4968                 # This should probably be replaced with a separate flag.
4969                 if not service.disabled or (
4970                         service.disabled_reason and
4971                         service.disabled_reason.startswith(DISABLE_PREFIX)):
4972                     service.disabled = disable_service
4973                     service.disabled_reason = (
4974                        DISABLE_PREFIX + disable_reason
4975                        if disable_service and disable_reason else
4976                            DISABLE_REASON_UNDEFINED)
4977                     service.save()
4978                     LOG.debug('Updating compute service status to %s',
4979                               status_name[disable_service])
4980                     # Update the disabled trait status on the corresponding
4981                     # compute node resource provider in placement.
4982                     self._update_compute_provider_status(ctx, service)
4983                 else:
4984                     LOG.debug('Not overriding manual compute service '
4985                               'status with: %s',
4986                               status_name[disable_service])
4987         except exception.ComputeHostNotFound:
4988             LOG.warning('Cannot update service status on host "%s" '
4989                         'since it is not registered.', CONF.host)
4990         except Exception:
4991             LOG.warning('Cannot update service status on host "%s" '
4992                         'due to an unexpected exception.', CONF.host,
4993                         exc_info=True)
4994 
4995         if enabled:
4996             mount.get_manager().host_up(self._host)
4997         else:
4998             mount.get_manager().host_down()
4999 
5000     def _get_cpu_model_mapping(self, model):
5001         """Get the CPU model mapping
5002 
5003         The CPU models which admin configured are case-insensitive, libvirt is
5004         case-sensitive, therefore build a mapping to get the correct CPU model
5005         name.
5006 
5007         :param model: Case-insensitive CPU model name.
5008         :return: It will validate and return the case-sensitive CPU model name
5009                  if on a supported platform, otherwise it will just return
5010                  what was provided
5011         :raises: exception.InvalidCPUInfo if the CPU model is not supported.
5012         """
5013         cpu_info = self._get_cpu_info()
5014         if cpu_info['arch'] not in (fields.Architecture.I686,
5015                                     fields.Architecture.X86_64,
5016                                     fields.Architecture.PPC64,
5017                                     fields.Architecture.PPC64LE,
5018                                     fields.Architecture.PPC):
5019             return model
5020 
5021         if not self.cpu_models_mapping:
5022             cpu_models = self._host.get_cpu_model_names()
5023             for cpu_model in cpu_models:
5024                 self.cpu_models_mapping[cpu_model.lower()] = cpu_model
5025 
5026         if model.lower() not in self.cpu_models_mapping:
5027             msg = (_("Configured CPU model: %(model)s is not correct, "
5028                      "or your host CPU arch does not support this "
5029                      "model. Please correct your config and try "
5030                      "again.") % {'model': model})
5031             raise exception.InvalidCPUInfo(msg)
5032 
5033         return self.cpu_models_mapping.get(model.lower())
5034 
5035     # TODO(stephenfin): Libvirt exposes information about possible CPU models
5036     # via 'getDomainCapabilities' and we should use it
5037     def _get_guest_cpu_model_config(self, flavor=None, arch=None):
5038         mode = CONF.libvirt.cpu_mode
5039         models = [self._get_cpu_model_mapping(model)
5040                   for model in CONF.libvirt.cpu_models]
5041         extra_flags = set([flag.lower() for flag in
5042             CONF.libvirt.cpu_model_extra_flags])
5043 
5044         if not arch:
5045             caps = self._host.get_capabilities()
5046             arch = caps.host.cpu.arch
5047 
5048         if (
5049             CONF.libvirt.virt_type == "kvm" or
5050             CONF.libvirt.virt_type == "qemu"
5051         ):
5052             if mode is None:
5053                 # AArch64 lacks 'host-model' support because neither libvirt
5054                 # nor QEMU are able to tell what the host CPU model exactly is.
5055                 # And there is no CPU description code for ARM(64) at this
5056                 # point.
5057 
5058                 # Also worth noting: 'host-passthrough' mode will completely
5059                 # break live migration, *unless* all the Compute nodes (running
5060                 # libvirtd) have *identical* CPUs.
5061                 if arch == fields.Architecture.AARCH64:
5062                     mode = "host-passthrough"
5063                     LOG.info('CPU mode "host-passthrough" was chosen. Live '
5064                              'migration can break unless all compute nodes '
5065                              'have identical cpus. AArch64 does not support '
5066                              'other modes.')
5067                 else:
5068                     mode = "host-model"
5069             if mode == "none":
5070                 return vconfig.LibvirtConfigGuestCPU()
5071             # On AArch64 platform the return of _get_cpu_model_mapping will not
5072             # return the default CPU model.
5073             if mode == "custom":
5074                 if arch == fields.Architecture.AARCH64:
5075                     if not models:
5076                         models = ['max']
5077 
5078         else:
5079             if mode is None or mode == "none":
5080                 return None
5081 
5082         cpu = vconfig.LibvirtConfigGuestCPU()
5083         cpu.mode = mode
5084         cpu.model = models[0] if models else None
5085 
5086         # compare flavor trait and cpu models, select the first mathched model
5087         if flavor and mode == "custom":
5088             flags = libvirt_utils.get_flags_by_flavor_specs(flavor)
5089             if flags:
5090                 cpu.model = self._match_cpu_model_by_flags(models, flags)
5091 
5092         LOG.debug("CPU mode '%(mode)s' models '%(models)s' was chosen, "
5093                   "with extra flags: '%(extra_flags)s'",
5094                   {'mode': mode,
5095                    'models': (cpu.model or ""),
5096                    'extra_flags': (extra_flags or "")})
5097 
5098         # NOTE (kchamart): Currently there's no existing way to ask if a
5099         # given CPU model + CPU flags combination is supported by KVM &
5100         # a specific QEMU binary.  However, libvirt runs the 'CPUID'
5101         # command upfront -- before even a Nova instance (a QEMU
5102         # process) is launched -- to construct CPU models and check
5103         # their validity; so we are good there.  In the long-term,
5104         # upstream libvirt intends to add an additional new API that can
5105         # do fine-grained validation of a certain CPU model + CPU flags
5106         # against a specific QEMU binary (the libvirt RFE bug for that:
5107         # https://bugzilla.redhat.com/show_bug.cgi?id=1559832).
5108         #
5109         # NOTE(kchamart) Similar to what was done in
5110         # _check_cpu_compatibility(), the below parses a comma-separated
5111         # list of CPU flags from `[libvirt]cpu_model_extra_flags` and
5112         # will selectively enable or disable a given CPU flag for the
5113         # guest, before it is launched by Nova.
5114         for flag in extra_flags:
5115             cpu_feature = self._prepare_cpu_flag(flag)
5116             cpu.add_feature(cpu_feature)
5117         return cpu
5118 
5119     def _match_cpu_model_by_flags(self, models, flags):
5120         for model in models:
5121             if flags.issubset(self.cpu_model_flag_mapping.get(model, set([]))):
5122                 return model
5123             cpu = vconfig.LibvirtConfigCPU()
5124             cpu.arch = self._host.get_capabilities().host.cpu.arch
5125             cpu.model = model
5126             features_xml = self._get_guest_baseline_cpu_features(cpu.to_xml())
5127             if features_xml:
5128                 cpu.parse_str(features_xml)
5129                 feature_names = [f.name for f in cpu.features]
5130                 self.cpu_model_flag_mapping[model] = feature_names
5131                 if flags.issubset(feature_names):
5132                     return model
5133 
5134         msg = ('No CPU model match traits, models: {models}, required '
5135                'flags: {flags}'.format(models=models, flags=flags))
5136         raise exception.InvalidCPUInfo(msg)
5137 
5138     def _get_guest_cpu_config(self, flavor, image_meta,
5139                               guest_cpu_numa_config, instance_numa_topology):
5140         arch = libvirt_utils.get_arch(image_meta)
5141         cpu = self._get_guest_cpu_model_config(flavor, arch)
5142 
5143         if cpu is None:
5144             return None
5145 
5146         topology = hardware.get_best_cpu_topology(flavor, image_meta)
5147 
5148         cpu.sockets = topology.sockets
5149         cpu.cores = topology.cores
5150         cpu.threads = topology.threads
5151         cpu.numa = guest_cpu_numa_config
5152 
5153         return cpu
5154 
5155     def _get_guest_disk_config(
5156         self, instance, name, disk_mapping, flavor, image_type=None,
5157         boot_order=None,
5158     ):
5159         disk_unit = None
5160         disk = self.image_backend.by_name(instance, name, image_type)
5161         if (name == 'disk.config' and image_type == 'rbd' and
5162                 not disk.exists()):
5163             # This is likely an older config drive that has not been migrated
5164             # to rbd yet. Try to fall back on 'flat' image type.
5165             # TODO(melwitt): Add online migration of some sort so we can
5166             # remove this fall back once we know all config drives are in rbd.
5167             # NOTE(vladikr): make sure that the flat image exist, otherwise
5168             # the image will be created after the domain definition.
5169             flat_disk = self.image_backend.by_name(instance, name, 'flat')
5170             if flat_disk.exists():
5171                 disk = flat_disk
5172                 LOG.debug('Config drive not found in RBD, falling back to the '
5173                           'instance directory', instance=instance)
5174         disk_info = disk_mapping[name]
5175         if 'unit' in disk_mapping and disk_info['bus'] == 'scsi':
5176             disk_unit = disk_mapping['unit']
5177             disk_mapping['unit'] += 1  # Increments for the next disk added
5178         conf = disk.libvirt_info(
5179             disk_info, self.disk_cachemode, flavor['extra_specs'],
5180             disk_unit=disk_unit, boot_order=boot_order)
5181         return conf
5182 
5183     def _get_guest_fs_config(self, instance, name, image_type=None):
5184         disk = self.image_backend.by_name(instance, name, image_type)
5185         return disk.libvirt_fs_info("/", "ploop")
5186 
5187     def _get_guest_storage_config(
5188         self, context, instance, image_meta, disk_info, rescue,
5189         block_device_info, flavor, os_type,
5190     ):
5191         devices = []
5192         disk_mapping = disk_info['mapping']
5193 
5194         block_device_mapping = driver.block_device_info_get_mapping(
5195             block_device_info)
5196         mount_rootfs = CONF.libvirt.virt_type == "lxc"
5197         scsi_controller = self._get_scsi_controller(image_meta)
5198 
5199         if scsi_controller and scsi_controller.model == 'virtio-scsi':
5200             # The virtio-scsi can handle up to 256 devices but the
5201             # optional element "address" must be defined to describe
5202             # where the device is placed on the controller (see:
5203             # LibvirtConfigGuestDeviceAddressDrive).
5204             #
5205             # Note about why it's added in disk_mapping: It's not
5206             # possible to pass an 'int' by reference in Python, so we
5207             # use disk_mapping as container to keep reference of the
5208             # unit added and be able to increment it for each disk
5209             # added.
5210             #
5211             # NOTE(jaypipes,melwitt): If this is a boot-from-volume instance,
5212             # we need to start the disk mapping unit at 1 since we set the
5213             # bootable volume's unit to 0 for the bootable volume.
5214             disk_mapping['unit'] = 0
5215             if self._is_booted_from_volume(block_device_info):
5216                 disk_mapping['unit'] = 1
5217 
5218         def _get_ephemeral_devices():
5219             eph_devices = []
5220             for idx, eph in enumerate(
5221                 driver.block_device_info_get_ephemerals(
5222                     block_device_info)):
5223                 diskeph = self._get_guest_disk_config(
5224                     instance,
5225                     blockinfo.get_eph_disk(idx),
5226                     disk_mapping, flavor)
5227                 eph_devices.append(diskeph)
5228             return eph_devices
5229 
5230         if mount_rootfs:
5231             fs = vconfig.LibvirtConfigGuestFilesys()
5232             fs.source_type = "mount"
5233             fs.source_dir = os.path.join(
5234                 libvirt_utils.get_instance_path(instance), 'rootfs')
5235             devices.append(fs)
5236         elif (os_type == fields.VMMode.EXE and
5237               CONF.libvirt.virt_type == "parallels"):
5238             if rescue:
5239                 fsrescue = self._get_guest_fs_config(instance, "disk.rescue")
5240                 devices.append(fsrescue)
5241 
5242                 fsos = self._get_guest_fs_config(instance, "disk")
5243                 fsos.target_dir = "/mnt/rescue"
5244                 devices.append(fsos)
5245             else:
5246                 if 'disk' in disk_mapping:
5247                     fs = self._get_guest_fs_config(instance, "disk")
5248                     devices.append(fs)
5249                 devices = devices + _get_ephemeral_devices()
5250         else:
5251 
5252             if rescue and disk_mapping['disk.rescue'] == disk_mapping['root']:
5253                 diskrescue = self._get_guest_disk_config(
5254                     instance, 'disk.rescue', disk_mapping, flavor)
5255                 devices.append(diskrescue)
5256 
5257                 diskos = self._get_guest_disk_config(
5258                     instance, 'disk', disk_mapping, flavor)
5259                 devices.append(diskos)
5260             else:
5261                 if 'disk' in disk_mapping:
5262                     diskos = self._get_guest_disk_config(
5263                         instance, 'disk', disk_mapping, flavor)
5264                     devices.append(diskos)
5265 
5266                 if 'disk.local' in disk_mapping:
5267                     disklocal = self._get_guest_disk_config(
5268                         instance, 'disk.local', disk_mapping, flavor)
5269                     devices.append(disklocal)
5270                     instance.default_ephemeral_device = (
5271                         block_device.prepend_dev(disklocal.target_dev))
5272 
5273                 devices = devices + _get_ephemeral_devices()
5274 
5275                 if 'disk.swap' in disk_mapping:
5276                     diskswap = self._get_guest_disk_config(
5277                         instance, 'disk.swap', disk_mapping, flavor)
5278                     devices.append(diskswap)
5279                     instance.default_swap_device = (
5280                         block_device.prepend_dev(diskswap.target_dev))
5281 
5282             config_name = 'disk.config'
5283             if rescue and disk_mapping['disk.rescue'] == disk_mapping['root']:
5284                 config_name = 'disk.config.rescue'
5285 
5286             if config_name in disk_mapping:
5287                 diskconfig = self._get_guest_disk_config(
5288                     instance, config_name, disk_mapping, flavor,
5289                     self._get_disk_config_image_type())
5290                 devices.append(diskconfig)
5291 
5292         for vol in block_device.get_bdms_to_connect(block_device_mapping,
5293                                                    mount_rootfs):
5294             connection_info = vol['connection_info']
5295             vol_dev = block_device.prepend_dev(vol['mount_device'])
5296             info = disk_mapping[vol_dev]
5297             self._connect_volume(context, connection_info, instance)
5298             if scsi_controller and scsi_controller.model == 'virtio-scsi':
5299                 # Check if this is the bootable volume when in a
5300                 # boot-from-volume instance, and if so, ensure the unit
5301                 # attribute is 0.
5302                 if vol.get('boot_index') == 0:
5303                     info['unit'] = 0
5304                 else:
5305                     info['unit'] = disk_mapping['unit']
5306                     disk_mapping['unit'] += 1
5307             cfg = self._get_volume_config(instance, connection_info, info)
5308             devices.append(cfg)
5309             vol['connection_info'] = connection_info
5310             vol.save()
5311 
5312         for d in devices:
5313             self._set_cache_mode(d)
5314 
5315         if scsi_controller:
5316             devices.append(scsi_controller)
5317 
5318         if rescue and disk_mapping['disk.rescue'] != disk_mapping['root']:
5319             diskrescue = self._get_guest_disk_config(
5320                 instance, 'disk.rescue', disk_mapping, flavor, boot_order='1')
5321             devices.append(diskrescue)
5322 
5323         return devices
5324 
5325     @staticmethod
5326     def _get_scsi_controller(image_meta):
5327         """Return scsi controller or None based on image meta"""
5328         if image_meta.properties.get('hw_scsi_model'):
5329             hw_scsi_model = image_meta.properties.hw_scsi_model
5330             scsi_controller = vconfig.LibvirtConfigGuestController()
5331             scsi_controller.type = 'scsi'
5332             scsi_controller.model = hw_scsi_model
5333             scsi_controller.index = 0
5334             return scsi_controller
5335 
5336     def _get_host_sysinfo_serial_hardware(self):
5337         """Get a UUID from the host hardware
5338 
5339         Get a UUID for the host hardware reported by libvirt.
5340         This is typically from the SMBIOS data, unless it has
5341         been overridden in /etc/libvirt/libvirtd.conf
5342         """
5343         caps = self._host.get_capabilities()
5344         return caps.host.uuid
5345 
5346     def _get_host_sysinfo_serial_os(self):
5347         """Get a UUID from the host operating system
5348 
5349         Get a UUID for the host operating system. Modern Linux
5350         distros based on systemd provide a /etc/machine-id
5351         file containing a UUID. This is also provided inside
5352         systemd based containers and can be provided by other
5353         init systems too, since it is just a plain text file.
5354         """
5355         if not os.path.exists("/etc/machine-id"):
5356             msg = _("Unable to get host UUID: /etc/machine-id does not exist")
5357             raise exception.InternalError(msg)
5358 
5359         with open("/etc/machine-id") as f:
5360             # We want to have '-' in the right place
5361             # so we parse & reformat the value
5362             lines = f.read().split()
5363             if not lines:
5364                 msg = _("Unable to get host UUID: /etc/machine-id is empty")
5365                 raise exception.InternalError(msg)
5366 
5367             return str(uuid.UUID(lines[0]))
5368 
5369     def _get_host_sysinfo_serial_auto(self):
5370         if os.path.exists("/etc/machine-id"):
5371             return self._get_host_sysinfo_serial_os()
5372         else:
5373             return self._get_host_sysinfo_serial_hardware()
5374 
5375     def _get_guest_config_sysinfo(self, instance):
5376         sysinfo = vconfig.LibvirtConfigGuestSysinfo()
5377 
5378         sysinfo.system_manufacturer = version.vendor_string()
5379         sysinfo.system_product = version.product_string()
5380         sysinfo.system_version = version.version_string_with_package()
5381 
5382         if CONF.libvirt.sysinfo_serial == 'unique':
5383             sysinfo.system_serial = instance.uuid
5384         else:
5385             sysinfo.system_serial = self._sysinfo_serial_func()
5386         sysinfo.system_uuid = instance.uuid
5387 
5388         sysinfo.system_family = "Virtual Machine"
5389 
5390         return sysinfo
5391 
5392     def _set_managed_mode(self, pcidev):
5393         # only kvm support managed mode
5394         if CONF.libvirt.virt_type in ('parallels',):
5395             pcidev.managed = 'no'
5396         if CONF.libvirt.virt_type in ('kvm', 'qemu'):
5397             pcidev.managed = 'yes'
5398 
5399     def _get_guest_pci_device(self, pci_device):
5400 
5401         dbsf = pci_utils.parse_address(pci_device.address)
5402         dev = vconfig.LibvirtConfigGuestHostdevPCI()
5403         dev.domain, dev.bus, dev.slot, dev.function = dbsf
5404         self._set_managed_mode(dev)
5405 
5406         return dev
5407 
5408     def _get_guest_config_meta(self, instance, network_info):
5409         """Get metadata config for guest."""
5410 
5411         meta = vconfig.LibvirtConfigGuestMetaNovaInstance()
5412         meta.package = version.version_string_with_package()
5413         meta.name = instance.display_name
5414         meta.creationTime = time.time()
5415 
5416         if instance.image_ref not in ("", None):
5417             meta.roottype = "image"
5418             meta.rootid = instance.image_ref
5419 
5420         system_meta = instance.system_metadata
5421         ometa = vconfig.LibvirtConfigGuestMetaNovaOwner()
5422         ometa.userid = instance.user_id
5423         ometa.username = system_meta.get('owner_user_name', 'N/A')
5424         ometa.projectid = instance.project_id
5425         ometa.projectname = system_meta.get('owner_project_name', 'N/A')
5426         meta.owner = ometa
5427 
5428         fmeta = vconfig.LibvirtConfigGuestMetaNovaFlavor()
5429         flavor = instance.flavor
5430         fmeta.name = flavor.name
5431         fmeta.memory = flavor.memory_mb
5432         fmeta.vcpus = flavor.vcpus
5433         fmeta.ephemeral = flavor.ephemeral_gb
5434         fmeta.disk = flavor.root_gb
5435         fmeta.swap = flavor.swap
5436 
5437         meta.flavor = fmeta
5438 
5439         ports = []
5440         for vif in network_info:
5441             ips = []
5442             for subnet in vif.get('network', {}).get('subnets', []):
5443                 for ip in subnet.get('ips', []):
5444                     ips.append(vconfig.LibvirtConfigGuestMetaNovaIp(
5445                         ip.get('type'), ip.get('address'), ip.get('version')))
5446             ports.append(vconfig.LibvirtConfigGuestMetaNovaPort(
5447                 vif.get('id'), ips=ips))
5448 
5449         meta.ports = vconfig.LibvirtConfigGuestMetaNovaPorts(ports)
5450 
5451         return meta
5452 
5453     @staticmethod
5454     def _create_idmaps(klass, map_strings):
5455         idmaps = []
5456         if len(map_strings) > 5:
5457             map_strings = map_strings[0:5]
5458             LOG.warning("Too many id maps, only included first five.")
5459         for map_string in map_strings:
5460             try:
5461                 idmap = klass()
5462                 values = [int(i) for i in map_string.split(":")]
5463                 idmap.start = values[0]
5464                 idmap.target = values[1]
5465                 idmap.count = values[2]
5466                 idmaps.append(idmap)
5467             except (ValueError, IndexError):
5468                 LOG.warning("Invalid value for id mapping %s", map_string)
5469         return idmaps
5470 
5471     def _get_guest_idmaps(self):
5472         id_maps: ty.List[vconfig.LibvirtConfigGuestIDMap] = []
5473         if CONF.libvirt.virt_type == 'lxc' and CONF.libvirt.uid_maps:
5474             uid_maps = self._create_idmaps(vconfig.LibvirtConfigGuestUIDMap,
5475                                            CONF.libvirt.uid_maps)
5476             id_maps.extend(uid_maps)
5477         if CONF.libvirt.virt_type == 'lxc' and CONF.libvirt.gid_maps:
5478             gid_maps = self._create_idmaps(vconfig.LibvirtConfigGuestGIDMap,
5479                                            CONF.libvirt.gid_maps)
5480             id_maps.extend(gid_maps)
5481         return id_maps
5482 
5483     def _update_guest_cputune(self, guest, flavor):
5484         is_able = self._host.is_cpu_control_policy_capable()
5485 
5486         cputuning = ['shares', 'period', 'quota']
5487         wants_cputune = any([k for k in cputuning
5488             if "quota:cpu_" + k in flavor.extra_specs.keys()])
5489 
5490         if wants_cputune and not is_able:
5491             raise exception.UnsupportedHostCPUControlPolicy()
5492 
5493         if not is_able or CONF.libvirt.virt_type not in ('lxc', 'kvm', 'qemu'):
5494             return
5495 
5496         if guest.cputune is None:
5497             guest.cputune = vconfig.LibvirtConfigGuestCPUTune()
5498             # Setting the default cpu.shares value to be a value
5499             # dependent on the number of vcpus
5500         guest.cputune.shares = 1024 * guest.vcpus
5501 
5502         for name in cputuning:
5503             key = "quota:cpu_" + name
5504             if key in flavor.extra_specs:
5505                 setattr(guest.cputune, name,
5506                         int(flavor.extra_specs[key]))
5507 
5508     def _get_cpu_numa_config_from_instance(self, instance_numa_topology,
5509                                            wants_hugepages):
5510         if instance_numa_topology:
5511             guest_cpu_numa = vconfig.LibvirtConfigGuestCPUNUMA()
5512             for instance_cell in instance_numa_topology.cells:
5513                 guest_cell = vconfig.LibvirtConfigGuestCPUNUMACell()
5514                 guest_cell.id = instance_cell.id
5515                 guest_cell.cpus = instance_cell.total_cpus
5516                 guest_cell.memory = instance_cell.memory * units.Ki
5517 
5518                 # The vhost-user network backend requires file backed
5519                 # guest memory (ie huge pages) to be marked as shared
5520                 # access, not private, so an external process can read
5521                 # and write the pages.
5522                 #
5523                 # You can't change the shared vs private flag for an
5524                 # already running guest, and since we can't predict what
5525                 # types of NIC may be hotplugged, we have no choice but
5526                 # to unconditionally turn on the shared flag. This has
5527                 # no real negative functional effect on the guest, so
5528                 # is a reasonable approach to take
5529                 if wants_hugepages:
5530                     guest_cell.memAccess = "shared"
5531                 guest_cpu_numa.cells.append(guest_cell)
5532             return guest_cpu_numa
5533 
5534     def _wants_hugepages(self, host_topology, instance_topology):
5535         """Determine if the guest / host topology implies the
5536            use of huge pages for guest RAM backing
5537         """
5538 
5539         if host_topology is None or instance_topology is None:
5540             return False
5541 
5542         avail_pagesize = [page.size_kb
5543                           for page in host_topology.cells[0].mempages]
5544         avail_pagesize.sort()
5545         # Remove smallest page size as that's not classed as a largepage
5546         avail_pagesize = avail_pagesize[1:]
5547 
5548         # See if we have page size set
5549         for cell in instance_topology.cells:
5550             if (cell.pagesize is not None and
5551                 cell.pagesize in avail_pagesize):
5552                 return True
5553 
5554         return False
5555 
5556     def _get_cell_pairs(self, guest_cpu_numa_config, host_topology):
5557         """Returns the lists of pairs(tuple) of an instance cell and
5558         corresponding host cell:
5559             [(LibvirtConfigGuestCPUNUMACell, NUMACell), ...]
5560         """
5561         cell_pairs = []
5562         for guest_config_cell in guest_cpu_numa_config.cells:
5563             for host_cell in host_topology.cells:
5564                 if guest_config_cell.id == host_cell.id:
5565                     cell_pairs.append((guest_config_cell, host_cell))
5566         return cell_pairs
5567 
5568     def _get_pin_cpuset(self, vcpu, inst_cell, host_cell):
5569         """Returns the config object of LibvirtConfigGuestCPUTuneVCPUPin.
5570 
5571         Prepares vcpupin config for the guest with the following caveats:
5572 
5573             a) If the specified instance vCPU is intended to be pinned, we pin
5574                it to the previously selected host CPU.
5575             b) Otherwise we float over the whole host NUMA node
5576         """
5577         pin_cpuset = vconfig.LibvirtConfigGuestCPUTuneVCPUPin()
5578         pin_cpuset.id = vcpu
5579 
5580         # 'InstanceNUMACell.cpu_pinning' tracks the CPU pinning pair for guest
5581         # CPU and host CPU. If the guest CPU is in the keys of 'cpu_pinning',
5582         # fetch the host CPU from it and pin on it, otherwise, let the guest
5583         # CPU be floating on the sharing CPU set belonging to this NUMA cell.
5584         if inst_cell.cpu_pinning and vcpu in inst_cell.cpu_pinning:
5585             pin_cpuset.cpuset = set([inst_cell.cpu_pinning[vcpu]])
5586         else:
5587             pin_cpuset.cpuset = host_cell.cpuset
5588 
5589         return pin_cpuset
5590 
5591     def _get_emulatorpin_cpuset(self, vcpu, object_numa_cell, vcpus_rt,
5592                                 emulator_threads_policy,
5593                                 pin_cpuset):
5594         """Returns a set of cpu_ids to add to the cpuset for emulator threads
5595            with the following caveats:
5596 
5597             a) If emulator threads policy is isolated, we pin emulator threads
5598                to one cpu we have reserved for it.
5599             b) If emulator threads policy is shared and CONF.cpu_shared_set is
5600                defined, we pin emulator threads on the set of pCPUs defined by
5601                CONF.cpu_shared_set
5602             c) Otherwise;
5603                 c1) If realtime IS NOT enabled, the emulator threads are
5604                     allowed to float cross all the pCPUs associated with
5605                     the guest vCPUs.
5606                 c2) If realtime IS enabled, at least 1 vCPU is required
5607                     to be set aside for non-realtime usage. The emulator
5608                     threads are allowed to float across the pCPUs that
5609                     are associated with the non-realtime VCPUs.
5610         """
5611         emulatorpin_cpuset = set([])
5612         shared_ids = hardware.get_cpu_shared_set()
5613 
5614         if emulator_threads_policy == fields.CPUEmulatorThreadsPolicy.ISOLATE:
5615             if object_numa_cell.cpuset_reserved:
5616                 emulatorpin_cpuset = object_numa_cell.cpuset_reserved
5617         elif ((emulator_threads_policy ==
5618               fields.CPUEmulatorThreadsPolicy.SHARE) and
5619               shared_ids):
5620             online_pcpus = self._host.get_online_cpus()
5621             cpuset = shared_ids & online_pcpus
5622             if not cpuset:
5623                 msg = (_("Invalid cpu_shared_set config, one or more of the "
5624                          "specified cpuset is not online. Online cpuset(s): "
5625                          "%(online)s, requested cpuset(s): %(req)s"),
5626                        {'online': sorted(online_pcpus),
5627                         'req': sorted(shared_ids)})
5628                 raise exception.Invalid(msg)
5629             emulatorpin_cpuset = cpuset
5630         elif not vcpus_rt or vcpu not in vcpus_rt:
5631             emulatorpin_cpuset = pin_cpuset.cpuset
5632 
5633         return emulatorpin_cpuset
5634 
5635     def _get_guest_numa_config(self, instance_numa_topology, flavor,
5636                                image_meta):
5637         """Returns the config objects for the guest NUMA specs.
5638 
5639         Determines the CPUs that the guest can be pinned to if the guest
5640         specifies a cell topology and the host supports it. Constructs the
5641         libvirt XML config object representing the NUMA topology selected
5642         for the guest. Returns a tuple of:
5643 
5644             (cpu_set, guest_cpu_tune, guest_cpu_numa, guest_numa_tune)
5645 
5646         With the following caveats:
5647 
5648             a) If there is no specified guest NUMA topology, then
5649                all tuple elements except cpu_set shall be None. cpu_set
5650                will be populated with the chosen CPUs that the guest
5651                allowed CPUs fit within.
5652 
5653             b) If there is a specified guest NUMA topology, then
5654                cpu_set will be None and guest_cpu_numa will be the
5655                LibvirtConfigGuestCPUNUMA object representing the guest's
5656                NUMA topology. If the host supports NUMA, then guest_cpu_tune
5657                will contain a LibvirtConfigGuestCPUTune object representing
5658                the optimized chosen cells that match the host capabilities
5659                with the instance's requested topology. If the host does
5660                not support NUMA, then guest_cpu_tune and guest_numa_tune
5661                will be None.
5662         """
5663 
5664         if (not self._has_numa_support() and
5665                 instance_numa_topology is not None):
5666             # We should not get here, since we should have avoided
5667             # reporting NUMA topology from _get_host_numa_topology
5668             # in the first place. Just in case of a scheduler
5669             # mess up though, raise an exception
5670             raise exception.NUMATopologyUnsupported()
5671 
5672         # We only pin an instance to some host cores if the user has provided
5673         # configuration to suggest we should.
5674         shared_cpus = None
5675         if CONF.vcpu_pin_set or CONF.compute.cpu_shared_set:
5676             shared_cpus = self._get_vcpu_available()
5677 
5678         topology = self._get_host_numa_topology()
5679 
5680         # We have instance NUMA so translate it to the config class
5681         guest_cpu_numa_config = self._get_cpu_numa_config_from_instance(
5682                 instance_numa_topology,
5683                 self._wants_hugepages(topology, instance_numa_topology))
5684 
5685         if not guest_cpu_numa_config:
5686             # No NUMA topology defined for instance - let the host kernel deal
5687             # with the NUMA effects.
5688             # TODO(ndipanov): Attempt to spread the instance
5689             # across NUMA nodes and expose the topology to the
5690             # instance as an optimisation
5691             return GuestNumaConfig(shared_cpus, None, None, None)
5692 
5693         if not topology:
5694             # No NUMA topology defined for host - This will only happen with
5695             # some libvirt versions and certain platforms.
5696             return GuestNumaConfig(shared_cpus, None,
5697                                    guest_cpu_numa_config, None)
5698 
5699         # Now get configuration from the numa_topology
5700         # Init CPUTune configuration
5701         guest_cpu_tune = vconfig.LibvirtConfigGuestCPUTune()
5702         guest_cpu_tune.emulatorpin = (
5703             vconfig.LibvirtConfigGuestCPUTuneEmulatorPin())
5704         guest_cpu_tune.emulatorpin.cpuset = set([])
5705 
5706         # Init NUMATune configuration
5707         guest_numa_tune = vconfig.LibvirtConfigGuestNUMATune()
5708         guest_numa_tune.memory = vconfig.LibvirtConfigGuestNUMATuneMemory()
5709         guest_numa_tune.memnodes = []
5710 
5711         emulator_threads_policy = None
5712         if 'emulator_threads_policy' in instance_numa_topology:
5713             emulator_threads_policy = (
5714                 instance_numa_topology.emulator_threads_policy)
5715 
5716         # Set realtime scheduler for CPUTune
5717         vcpus_rt = hardware.get_realtime_cpu_constraint(flavor, image_meta)
5718         if vcpus_rt:
5719             vcpusched = vconfig.LibvirtConfigGuestCPUTuneVCPUSched()
5720             designer.set_vcpu_realtime_scheduler(
5721                 vcpusched, vcpus_rt, CONF.libvirt.realtime_scheduler_priority)
5722             guest_cpu_tune.vcpusched.append(vcpusched)
5723 
5724         cell_pairs = self._get_cell_pairs(guest_cpu_numa_config, topology)
5725         for guest_node_id, (guest_config_cell, host_cell) in enumerate(
5726                 cell_pairs):
5727             # set NUMATune for the cell
5728             tnode = vconfig.LibvirtConfigGuestNUMATuneMemNode()
5729             designer.set_numa_memnode(tnode, guest_node_id, host_cell.id)
5730             guest_numa_tune.memnodes.append(tnode)
5731             guest_numa_tune.memory.nodeset.append(host_cell.id)
5732 
5733             # set CPUTune for the cell
5734             object_numa_cell = instance_numa_topology.cells[guest_node_id]
5735             for cpu in guest_config_cell.cpus:
5736                 pin_cpuset = self._get_pin_cpuset(cpu, object_numa_cell,
5737                                                   host_cell)
5738                 guest_cpu_tune.vcpupin.append(pin_cpuset)
5739 
5740                 emu_pin_cpuset = self._get_emulatorpin_cpuset(
5741                     cpu, object_numa_cell, vcpus_rt,
5742                     emulator_threads_policy, pin_cpuset)
5743                 guest_cpu_tune.emulatorpin.cpuset.update(emu_pin_cpuset)
5744 
5745         # TODO(berrange) When the guest has >1 NUMA node, it will
5746         # span multiple host NUMA nodes. By pinning emulator threads
5747         # to the union of all nodes, we guarantee there will be
5748         # cross-node memory access by the emulator threads when
5749         # responding to guest I/O operations. The only way to avoid
5750         # this would be to pin emulator threads to a single node and
5751         # tell the guest OS to only do I/O from one of its virtual
5752         # NUMA nodes. This is not even remotely practical.
5753         #
5754         # The long term solution is to make use of a new QEMU feature
5755         # called "I/O Threads" which will let us configure an explicit
5756         # I/O thread for each guest vCPU or guest NUMA node. It is
5757         # still TBD how to make use of this feature though, especially
5758         # how to associate IO threads with guest devices to eliminate
5759         # cross NUMA node traffic. This is an area of investigation
5760         # for QEMU community devs.
5761 
5762         # Sort the vcpupin list per vCPU id for human-friendlier XML
5763         guest_cpu_tune.vcpupin.sort(key=operator.attrgetter("id"))
5764 
5765         # normalize cell.id
5766         for i, (cell, memnode) in enumerate(zip(guest_cpu_numa_config.cells,
5767                                                 guest_numa_tune.memnodes)):
5768             cell.id = i
5769             memnode.cellid = i
5770 
5771         return GuestNumaConfig(None, guest_cpu_tune, guest_cpu_numa_config,
5772                                guest_numa_tune)
5773 
5774     def _get_guest_os_type(self):
5775         """Returns the guest OS type based on virt type."""
5776         if CONF.libvirt.virt_type == "lxc":
5777             ret = fields.VMMode.EXE
5778         else:
5779             ret = fields.VMMode.HVM
5780         return ret
5781 
5782     def _set_guest_for_rescue(
5783         self, rescue, guest, inst_path, root_device_name,
5784     ):
5785         if rescue.get('kernel_id'):
5786             guest.os_kernel = os.path.join(inst_path, "kernel.rescue")
5787             guest.os_cmdline = ("root=%s %s" % (root_device_name, CONSOLE))
5788             if CONF.libvirt.virt_type == "qemu":
5789                 guest.os_cmdline += " no_timer_check"
5790         if rescue.get('ramdisk_id'):
5791             guest.os_initrd = os.path.join(inst_path, "ramdisk.rescue")
5792 
5793     def _set_guest_for_inst_kernel(
5794         self, instance, guest, inst_path, root_device_name, image_meta,
5795     ):
5796         guest.os_kernel = os.path.join(inst_path, "kernel")
5797         guest.os_cmdline = ("root=%s %s" % (root_device_name, CONSOLE))
5798         if CONF.libvirt.virt_type == "qemu":
5799             guest.os_cmdline += " no_timer_check"
5800         if instance.ramdisk_id:
5801             guest.os_initrd = os.path.join(inst_path, "ramdisk")
5802         # we only support os_command_line with images with an explicit
5803         # kernel set and don't want to break nova if there's an
5804         # os_command_line property without a specified kernel_id param
5805         if image_meta.properties.get("os_command_line"):
5806             guest.os_cmdline = image_meta.properties.os_command_line
5807 
5808     def _set_clock(self, guest, os_type, image_meta):
5809         # NOTE(mikal): Microsoft Windows expects the clock to be in
5810         # "localtime". If the clock is set to UTC, then you can use a
5811         # registry key to let windows know, but Microsoft says this is
5812         # buggy in http://support.microsoft.com/kb/2687252
5813         clk = vconfig.LibvirtConfigGuestClock()
5814         if os_type == 'windows':
5815             LOG.info('Configuring timezone for windows instance to localtime')
5816             clk.offset = 'localtime'
5817         else:
5818             clk.offset = 'utc'
5819         guest.set_clock(clk)
5820 
5821         if CONF.libvirt.virt_type == "kvm":
5822             self._set_kvm_timers(clk, os_type, image_meta)
5823 
5824     def _set_kvm_timers(self, clk, os_type, image_meta):
5825         # TODO(berrange) One day this should be per-guest
5826         # OS type configurable
5827         tmpit = vconfig.LibvirtConfigGuestTimer()
5828         tmpit.name = "pit"
5829         tmpit.tickpolicy = "delay"
5830 
5831         tmrtc = vconfig.LibvirtConfigGuestTimer()
5832         tmrtc.name = "rtc"
5833         tmrtc.tickpolicy = "catchup"
5834 
5835         clk.add_timer(tmpit)
5836         clk.add_timer(tmrtc)
5837 
5838         hpet = image_meta.properties.get('hw_time_hpet', False)
5839         guestarch = libvirt_utils.get_arch(image_meta)
5840         if guestarch in (fields.Architecture.I686,
5841                          fields.Architecture.X86_64):
5842             # NOTE(rfolco): HPET is a hardware timer for x86 arch.
5843             # qemu -no-hpet is not supported on non-x86 targets.
5844             tmhpet = vconfig.LibvirtConfigGuestTimer()
5845             tmhpet.name = "hpet"
5846             tmhpet.present = hpet
5847             clk.add_timer(tmhpet)
5848         else:
5849             if hpet:
5850                 LOG.warning('HPET is not turned on for non-x86 guests in image'
5851                             ' %s.', image_meta.id)
5852 
5853         # Provide Windows guests with the paravirtualized hyperv timer source.
5854         # This is the windows equiv of kvm-clock, allowing Windows
5855         # guests to accurately keep time.
5856         if os_type == 'windows':
5857             tmhyperv = vconfig.LibvirtConfigGuestTimer()
5858             tmhyperv.name = "hypervclock"
5859             tmhyperv.present = True
5860             clk.add_timer(tmhyperv)
5861 
5862     def _set_features(self, guest, os_type, image_meta, flavor):
5863         hide_hypervisor_id = (strutils.bool_from_string(
5864                 flavor.extra_specs.get('hide_hypervisor_id')) or
5865                 strutils.bool_from_string(
5866                     flavor.extra_specs.get('hw:hide_hypervisor_id')) or
5867                 image_meta.properties.get('img_hide_hypervisor_id'))
5868 
5869         if CONF.libvirt.virt_type in ('qemu', 'kvm'):
5870             guest.features.append(vconfig.LibvirtConfigGuestFeatureACPI())
5871             if not CONF.workarounds.libvirt_disable_apic:
5872                 guest.features.append(vconfig.LibvirtConfigGuestFeatureAPIC())
5873 
5874         if CONF.libvirt.virt_type in ('qemu', 'kvm') and os_type == 'windows':
5875             hv = vconfig.LibvirtConfigGuestFeatureHyperV()
5876             hv.relaxed = True
5877 
5878             hv.spinlocks = True
5879             # Increase spinlock retries - value recommended by
5880             # KVM maintainers who certify Windows guests
5881             # with Microsoft
5882             hv.spinlock_retries = 8191
5883             hv.vapic = True
5884 
5885             # NOTE(kosamara): Spoofing the vendor_id aims to allow the nvidia
5886             # driver to work on windows VMs. At the moment, the nvidia driver
5887             # checks for the hyperv vendorid, and if it doesn't find that, it
5888             # works. In the future, its behaviour could become more strict,
5889             # checking for the presence of other hyperv feature flags to
5890             # determine that it's loaded in a VM. If that happens, this
5891             # workaround will not be enough, and we'll need to drop the whole
5892             # hyperv element.
5893             # That would disable some optimizations, reducing the guest's
5894             # performance.
5895             if hide_hypervisor_id:
5896                 hv.vendorid_spoof = True
5897 
5898             guest.features.append(hv)
5899 
5900         if CONF.libvirt.virt_type in ("qemu", "kvm"):
5901             # vmcoreinfo support is x86, ARM-only for now
5902             guestarch = libvirt_utils.get_arch(image_meta)
5903             if guestarch in (
5904                 fields.Architecture.I686, fields.Architecture.X86_64,
5905                 fields.Architecture.AARCH64,
5906             ):
5907                 guest.features.append(
5908                     vconfig.LibvirtConfigGuestFeatureVMCoreInfo())
5909 
5910             if hide_hypervisor_id:
5911                 guest.features.append(
5912                     vconfig.LibvirtConfigGuestFeatureKvmHidden())
5913 
5914             # NOTE(sean-k-mooney): we validate that the image and flavor
5915             # cannot have conflicting values in the compute API
5916             # so we just use the values directly. If it is not set in
5917             # either the flavor or image pmu will be none and we should
5918             # not generate the element to allow qemu to decide if a vPMU
5919             # should be provided for backwards compatibility.
5920             pmu = (flavor.extra_specs.get('hw:pmu') or
5921                    image_meta.properties.get('hw_pmu'))
5922             if pmu is not None:
5923                 guest.features.append(
5924                     vconfig.LibvirtConfigGuestFeaturePMU(pmu))
5925 
5926     def _check_number_of_serial_console(self, num_ports):
5927         if (
5928             CONF.libvirt.virt_type in ("kvm", "qemu") and
5929             num_ports > ALLOWED_QEMU_SERIAL_PORTS
5930         ):
5931             raise exception.SerialPortNumberLimitExceeded(
5932                 allowed=ALLOWED_QEMU_SERIAL_PORTS,
5933                 virt_type=CONF.libvirt.virt_type)
5934 
5935     def _video_model_supported(self, model):
5936         return model in fields.VideoModel.ALL
5937 
5938     def _add_video_driver(self, guest, image_meta, flavor):
5939         video = vconfig.LibvirtConfigGuestVideo()
5940         # NOTE(ldbragst): The following logic sets the video.type
5941         # depending on supported defaults given the architecture,
5942         # virtualization type, and features. The video.type attribute can
5943         # be overridden by the user with image_meta.properties, which
5944         # is carried out in the next if statement below this one.
5945         guestarch = libvirt_utils.get_arch(image_meta)
5946         if CONF.libvirt.virt_type == 'parallels':
5947             video.type = 'vga'
5948         elif guestarch in (fields.Architecture.PPC,
5949                            fields.Architecture.PPC64,
5950                            fields.Architecture.PPC64LE):
5951             # NOTE(ldbragst): PowerKVM doesn't support 'cirrus' be default
5952             # so use 'vga' instead when running on Power hardware.
5953             video.type = 'vga'
5954         elif guestarch == fields.Architecture.AARCH64:
5955             # NOTE(kevinz): Only virtio device type is supported by AARCH64
5956             # so use 'virtio' instead when running on AArch64 hardware.
5957             video.type = 'virtio'
5958         elif CONF.spice.enabled:
5959             video.type = 'qxl'
5960         if image_meta.properties.get('hw_video_model'):
5961             video.type = image_meta.properties.hw_video_model
5962             if not self._video_model_supported(video.type):
5963                 raise exception.InvalidVideoMode(model=video.type)
5964 
5965         # Set video memory, only if the flavor's limit is set
5966         video_ram = image_meta.properties.get('hw_video_ram', 0)
5967         max_vram = int(flavor.extra_specs.get('hw_video:ram_max_mb', 0))
5968         if video_ram > max_vram:
5969             raise exception.RequestedVRamTooHigh(req_vram=video_ram,
5970                                                  max_vram=max_vram)
5971         if max_vram and video_ram:
5972             video.vram = video_ram * units.Mi // units.Ki
5973         guest.add_device(video)
5974 
5975         # NOTE(sean-k-mooney): return the video device we added
5976         # for simpler testing.
5977         return video
5978 
5979     def _add_qga_device(self, guest, instance):
5980         qga = vconfig.LibvirtConfigGuestChannel()
5981         qga.type = "unix"
5982         qga.target_name = "org.qemu.guest_agent.0"
5983         qga.source_path = ("/var/lib/libvirt/qemu/%s.%s.sock" %
5984                           ("org.qemu.guest_agent.0", instance.name))
5985         guest.add_device(qga)
5986 
5987     def _add_rng_device(self, guest, flavor, image_meta):
5988         rng_allowed_str = flavor.extra_specs.get('hw_rng:allowed', 'True')
5989         rng_allowed = strutils.bool_from_string(rng_allowed_str)
5990 
5991         if not rng_allowed:
5992             return
5993 
5994         rng_device = vconfig.LibvirtConfigGuestRng()
5995         rate_bytes = flavor.extra_specs.get('hw_rng:rate_bytes', 0)
5996         period = flavor.extra_specs.get('hw_rng:rate_period', 0)
5997         if rate_bytes:
5998             rng_device.rate_bytes = int(rate_bytes)
5999             rng_device.rate_period = int(period)
6000         rng_path = CONF.libvirt.rng_dev_path
6001         if (rng_path and not os.path.exists(rng_path)):
6002             raise exception.RngDeviceNotExist(path=rng_path)
6003         rng_device.backend = rng_path
6004         guest.add_device(rng_device)
6005 
6006     def _add_virtio_serial_controller(self, guest, instance):
6007         virtio_controller = vconfig.LibvirtConfigGuestController()
6008         virtio_controller.type = 'virtio-serial'
6009         guest.add_device(virtio_controller)
6010 
6011     def _add_vtpm_device(
6012         self,
6013         guest: vconfig.LibvirtConfigGuest,
6014         flavor: 'objects.Flavor',
6015         instance: 'objects.Instance',
6016         image_meta: 'objects.ImageMeta',
6017     ) -> None:
6018         """Add a vTPM device to the guest, if requested."""
6019         # Enable virtual tpm support if required in the flavor or image.
6020         vtpm_config = hardware.get_vtpm_constraint(flavor, image_meta)
6021         if not vtpm_config:
6022             return None
6023 
6024         vtpm_secret_uuid = instance.system_metadata.get('vtpm_secret_uuid')
6025         if not vtpm_secret_uuid:
6026             raise exception.Invalid(
6027                 'Refusing to create an emulated TPM with no secret!')
6028 
6029         vtpm = vconfig.LibvirtConfigGuestVTPM(vtpm_config, vtpm_secret_uuid)
6030         guest.add_device(vtpm)
6031 
6032     def _set_qemu_guest_agent(self, guest, flavor, instance, image_meta):
6033         # Enable qga only if the 'hw_qemu_guest_agent' is equal to yes
6034         if image_meta.properties.get('hw_qemu_guest_agent', False):
6035             # a virtio-serial controller is required for qga. If it is not
6036             # created explicitly, libvirt will do it by itself. But in case
6037             # of AMD SEV, any virtio device should use iommu driver, and
6038             # libvirt does not know about it. That is why the controller
6039             # should be created manually.
6040             if self._sev_enabled(flavor, image_meta):
6041                 self._add_virtio_serial_controller(guest, instance)
6042 
6043             LOG.debug("Qemu guest agent is enabled through image "
6044                       "metadata", instance=instance)
6045             self._add_qga_device(guest, instance)
6046 
6047     def _get_guest_memory_backing_config(
6048             self, inst_topology, numatune, flavor, image_meta):
6049         wantsrealtime = hardware.is_realtime_enabled(flavor)
6050         if (
6051             wantsrealtime and
6052             hardware.get_emulator_thread_policy_constraint(flavor) ==
6053                 fields.CPUEmulatorThreadsPolicy.SHARE and
6054             not CONF.compute.cpu_shared_set
6055         ):
6056             # NOTE(stephenfin) Yes, it's horrible that we're doing this here,
6057             # but the shared policy unfortunately has different behavior
6058             # depending on whether the '[compute] cpu_shared_set' is configured
6059             # or not and we need it to be configured. Also note that we have
6060             # already handled other conditions, such as no emulator thread
6061             # policy being configured whatsoever, at the API level.
6062             LOG.warning(
6063                 'Instance is requesting real-time CPUs with pooled '
6064                 'emulator threads, but a shared CPU pool has not been '
6065                 'configured on this host.'
6066             )
6067             raise exception.RealtimeMaskNotFoundOrInvalid()
6068 
6069         wantsmempages = False
6070         if inst_topology:
6071             for cell in inst_topology.cells:
6072                 if cell.pagesize:
6073                     wantsmempages = True
6074                     break
6075 
6076         wantsfilebacked = CONF.libvirt.file_backed_memory > 0
6077 
6078         if wantsmempages and wantsfilebacked:
6079             # Can't use file-backed memory with hugepages
6080             LOG.warning("Instance requested huge pages, but file-backed "
6081                     "memory is enabled, and incompatible with huge pages")
6082             raise exception.MemoryPagesUnsupported()
6083 
6084         membacking = None
6085         if wantsmempages:
6086             pages = self._get_memory_backing_hugepages_support(
6087                 inst_topology, numatune)
6088             if pages:
6089                 membacking = vconfig.LibvirtConfigGuestMemoryBacking()
6090                 membacking.hugepages = pages
6091         if wantsrealtime:
6092             if not membacking:
6093                 membacking = vconfig.LibvirtConfigGuestMemoryBacking()
6094             membacking.locked = True
6095             membacking.sharedpages = False
6096         if wantsfilebacked:
6097             if not membacking:
6098                 membacking = vconfig.LibvirtConfigGuestMemoryBacking()
6099             membacking.filesource = True
6100             membacking.sharedaccess = True
6101             membacking.allocateimmediate = True
6102             membacking.discard = True
6103         if self._sev_enabled(flavor, image_meta):
6104             if not membacking:
6105                 membacking = vconfig.LibvirtConfigGuestMemoryBacking()
6106             membacking.locked = True
6107 
6108         return membacking
6109 
6110     def _get_memory_backing_hugepages_support(self, inst_topology, numatune):
6111         if not self._has_numa_support():
6112             # We should not get here, since we should have avoided
6113             # reporting NUMA topology from _get_host_numa_topology
6114             # in the first place. Just in case of a scheduler
6115             # mess up though, raise an exception
6116             raise exception.MemoryPagesUnsupported()
6117 
6118         host_topology = self._get_host_numa_topology()
6119 
6120         if host_topology is None:
6121             # As above, we should not get here but just in case...
6122             raise exception.MemoryPagesUnsupported()
6123 
6124         # Currently libvirt does not support the smallest
6125         # pagesize set as a backend memory.
6126         # https://bugzilla.redhat.com/show_bug.cgi?id=1173507
6127         avail_pagesize = [page.size_kb
6128                           for page in host_topology.cells[0].mempages]
6129         avail_pagesize.sort()
6130         smallest = avail_pagesize[0]
6131 
6132         pages = []
6133         for guest_cellid, inst_cell in enumerate(inst_topology.cells):
6134             if inst_cell.pagesize and inst_cell.pagesize > smallest:
6135                 for memnode in numatune.memnodes:
6136                     if guest_cellid == memnode.cellid:
6137                         page = (
6138                             vconfig.LibvirtConfigGuestMemoryBackingPage())
6139                         page.nodeset = [guest_cellid]
6140                         page.size_kb = inst_cell.pagesize
6141                         pages.append(page)
6142                         break  # Quit early...
6143         return pages
6144 
6145     def _get_flavor(self, ctxt, instance, flavor):
6146         if flavor is not None:
6147             return flavor
6148         return instance.flavor
6149 
6150     def _check_uefi_support(self, hw_firmware_type):
6151         caps = self._host.get_capabilities()
6152         return self._host.supports_uefi and (
6153             hw_firmware_type == fields.FirmwareType.UEFI or
6154             caps.host.cpu.arch == fields.Architecture.AARCH64
6155         )
6156 
6157     def _check_secure_boot_support(
6158         self,
6159         arch: str,
6160         machine_type: str,
6161         firmware_type: str,
6162     ) -> bool:
6163         if not self._host.supports_secure_boot:
6164             # secure boot requires host configuration
6165             return False
6166 
6167         if firmware_type != fields.FirmwareType.UEFI:
6168             # secure boot is only supported with UEFI
6169             return False
6170 
6171         if (
6172             arch == fields.Architecture.X86_64 and
6173             'q35' not in machine_type
6174         ):
6175             # secure boot on x86_64 requires the Q35 machine type
6176             return False
6177 
6178         return True
6179 
6180     def _get_supported_perf_events(self):
6181         if not len(CONF.libvirt.enabled_perf_events):
6182             return []
6183 
6184         supported_events = []
6185         for event in CONF.libvirt.enabled_perf_events:
6186             libvirt_perf_event_name = LIBVIRT_PERF_EVENT_PREFIX + event.upper()
6187 
6188             if not hasattr(libvirt, libvirt_perf_event_name):
6189                 LOG.warning("Libvirt does not support event type '%s'.", event)
6190                 continue
6191 
6192             if event in ('cmt', 'mbml', 'mbmt'):
6193                 LOG.warning(
6194                     "Monitoring of Intel CMT `perf` event(s) '%s' is not "
6195                     "supported by recent Linux kernels; ignoring.",
6196                     event,
6197                 )
6198                 continue
6199 
6200             supported_events.append(event)
6201 
6202         return supported_events
6203 
6204     def _configure_guest_by_virt_type(
6205         self,
6206         guest: vconfig.LibvirtConfigGuest,
6207         instance: 'objects.Instance',
6208         image_meta: 'objects.ImageMeta',
6209         flavor: 'objects.Flavor',
6210     ) -> None:
6211         if CONF.libvirt.virt_type in ("kvm", "qemu"):
6212             arch = libvirt_utils.get_arch(image_meta)
6213             if arch in (fields.Architecture.I686, fields.Architecture.X86_64):
6214                 guest.sysinfo = self._get_guest_config_sysinfo(instance)
6215                 guest.os_smbios = vconfig.LibvirtConfigGuestSMBIOS()
6216 
6217             mach_type = libvirt_utils.get_machine_type(image_meta)
6218             guest.os_mach_type = mach_type
6219 
6220             hw_firmware_type = image_meta.properties.get('hw_firmware_type')
6221 
6222             if arch == fields.Architecture.AARCH64:
6223                 if not hw_firmware_type:
6224                     hw_firmware_type = fields.FirmwareType.UEFI
6225 
6226             if hw_firmware_type == fields.FirmwareType.UEFI:
6227                 global uefi_logged
6228                 if not uefi_logged:
6229                     LOG.warning("uefi support is without some kind of "
6230                                 "functional testing and therefore "
6231                                 "considered experimental.")
6232                     uefi_logged = True
6233 
6234                 if not self._host.supports_uefi:
6235                     raise exception.UEFINotSupported()
6236 
6237                 # TODO(stephenfin): Drop this when we drop support for legacy
6238                 # architectures
6239                 if not mach_type:
6240                     # loaders are specific to arch and machine type - if we
6241                     # don't have a machine type here, we're on a legacy
6242                     # architecture that we have no default machine type for
6243                     raise exception.UEFINotSupported()
6244 
6245                 os_secure_boot = hardware.get_secure_boot_constraint(
6246                     flavor, image_meta)
6247                 if os_secure_boot == 'required':
6248                     # hard fail if we don't support secure boot and it's
6249                     # required
6250                     if not self._check_secure_boot_support(
6251                         arch, mach_type, hw_firmware_type,
6252                     ):
6253                         raise exception.SecureBootNotSupported()
6254 
6255                     guest.os_loader_secure = True
6256                 elif os_secure_boot == 'optional':
6257                     # only enable it if the host is configured appropriately
6258                     guest.os_loader_secure = self._check_secure_boot_support(
6259                         arch, mach_type, hw_firmware_type,
6260                     )
6261                 else:
6262                     guest.os_loader_secure = False
6263 
6264                 try:
6265                     loader, nvram_template = self._host.get_loader(
6266                         arch, mach_type,
6267                         has_secure_boot=guest.os_loader_secure)
6268                 except exception.UEFINotSupported as exc:
6269                     if guest.os_loader_secure:
6270                         # we raise a specific exception if we requested secure
6271                         # boot and couldn't get that
6272                         raise exception.SecureBootNotSupported() from exc
6273                     raise
6274 
6275                 guest.os_loader = loader
6276                 guest.os_loader_type = 'pflash'
6277                 guest.os_nvram_template = nvram_template
6278 
6279             # NOTE(lyarwood): If the machine type isn't recorded in the stashed
6280             # image metadata then record it through the system metadata table.
6281             # This will allow the host configuration to change in the future
6282             # without impacting existing instances.
6283             # NOTE(lyarwood): The value of ``hw_machine_type`` within the
6284             # stashed image metadata of the instance actually comes from the
6285             # system metadata table under the ``image_hw_machine_type`` key via
6286             # nova.objects.ImageMeta.from_instance and the
6287             # nova.utils.get_image_from_system_metadata function.
6288             if image_meta.properties.get('hw_machine_type') is None:
6289                 instance.system_metadata['image_hw_machine_type'] = mach_type
6290 
6291             if image_meta.properties.get('hw_boot_menu') is None:
6292                 guest.os_bootmenu = strutils.bool_from_string(
6293                     flavor.extra_specs.get('hw:boot_menu', 'no'))
6294             else:
6295                 guest.os_bootmenu = image_meta.properties.hw_boot_menu
6296         elif CONF.libvirt.virt_type == "lxc":
6297             guest.os_init_path = "/sbin/init"
6298             guest.os_cmdline = CONSOLE
6299             guest.os_init_env["product_name"] = "OpenStack Nova"
6300         elif CONF.libvirt.virt_type == "parallels":
6301             if guest.os_type == fields.VMMode.EXE:
6302                 guest.os_init_path = "/sbin/init"
6303 
6304         return None
6305 
6306     def _conf_non_lxc(
6307         self,
6308         guest: vconfig.LibvirtConfigGuest,
6309         root_device_name: str,
6310         rescue: bool,
6311         instance: 'objects.Instance',
6312         inst_path: str,
6313         image_meta: 'objects.ImageMeta',
6314         disk_info: ty.Dict[str, ty.Any],
6315     ):
6316         if rescue:
6317             self._set_guest_for_rescue(
6318                 rescue, guest, inst_path, root_device_name)
6319         elif instance.kernel_id:
6320             self._set_guest_for_inst_kernel(
6321                 instance, guest, inst_path, root_device_name, image_meta)
6322         else:
6323             guest.os_boot_dev = blockinfo.get_boot_order(disk_info)
6324 
6325     def _create_consoles(self, guest_cfg, instance, flavor, image_meta):
6326         # NOTE(markus_z): Beware! Below are so many conditionals that it is
6327         # easy to lose track. Use this chart to figure out your case:
6328         #
6329         # case | is serial | is qemu | resulting
6330         #      | enabled?  | or kvm? | devices
6331         # -------------------------------------------
6332         #    1 |        no |     no  | pty*
6333         #    2 |        no |     yes | pty with logd
6334         #    3 |       yes |      no | see case 1
6335         #    4 |       yes |     yes | tcp with logd
6336         #
6337         #    * exception: `virt_type=parallels` doesn't create a device
6338         if CONF.libvirt.virt_type == 'parallels':
6339             pass
6340         elif CONF.libvirt.virt_type == 'lxc':
6341             log_path = self._get_console_log_path(instance)
6342             self._create_pty_device(
6343                 guest_cfg, vconfig.LibvirtConfigGuestConsole,
6344                 log_path=log_path)
6345         else:  # qemu, kvm
6346             if self._is_s390x_guest(image_meta):
6347                 self._create_consoles_s390x(
6348                     guest_cfg, instance, flavor, image_meta)
6349             else:
6350                 self._create_consoles_qemu_kvm(
6351                     guest_cfg, instance, flavor, image_meta)
6352 
6353     def _is_s390x_guest(self, image_meta):
6354         s390x_archs = (fields.Architecture.S390, fields.Architecture.S390X)
6355         return libvirt_utils.get_arch(image_meta) in s390x_archs
6356 
6357     def _is_ppc64_guest(self, image_meta):
6358         archs = (fields.Architecture.PPC64, fields.Architecture.PPC64LE)
6359         return libvirt_utils.get_arch(image_meta) in archs
6360 
6361     def _create_consoles_qemu_kvm(self, guest_cfg, instance, flavor,
6362                                   image_meta):
6363         char_dev_cls = vconfig.LibvirtConfigGuestSerial
6364         log_path = self._get_console_log_path(instance)
6365         if CONF.serial_console.enabled:
6366             if not self._serial_ports_already_defined(instance):
6367                 num_ports = hardware.get_number_of_serial_ports(flavor,
6368                                                                 image_meta)
6369                 self._check_number_of_serial_console(num_ports)
6370                 self._create_serial_consoles(guest_cfg, num_ports,
6371                                              char_dev_cls, log_path)
6372         else:
6373             self._create_pty_device(guest_cfg, char_dev_cls,
6374                                     log_path=log_path)
6375 
6376     def _create_consoles_s390x(self, guest_cfg, instance, flavor, image_meta):
6377         char_dev_cls = vconfig.LibvirtConfigGuestConsole
6378         log_path = self._get_console_log_path(instance)
6379         if CONF.serial_console.enabled:
6380             if not self._serial_ports_already_defined(instance):
6381                 num_ports = hardware.get_number_of_serial_ports(flavor,
6382                                                                 image_meta)
6383                 self._create_serial_consoles(guest_cfg, num_ports,
6384                                              char_dev_cls, log_path)
6385         else:
6386             self._create_pty_device(guest_cfg, char_dev_cls,
6387                                     "sclp", log_path)
6388 
6389     def _create_pty_device(self, guest_cfg, char_dev_cls, target_type=None,
6390                            log_path=None):
6391 
6392         consolepty = char_dev_cls()
6393         consolepty.target_type = target_type
6394         consolepty.type = "pty"
6395 
6396         log = vconfig.LibvirtConfigGuestCharDeviceLog()
6397         log.file = log_path
6398         consolepty.log = log
6399 
6400         guest_cfg.add_device(consolepty)
6401 
6402     def _serial_ports_already_defined(self, instance):
6403         try:
6404             guest = self._host.get_guest(instance)
6405             if list(self._get_serial_ports_from_guest(guest)):
6406                 # Serial port are already configured for instance that
6407                 # means we are in a context of migration.
6408                 return True
6409         except exception.InstanceNotFound:
6410             LOG.debug(
6411                 "Instance does not exist yet on libvirt, we can "
6412                 "safely pass on looking for already defined serial "
6413                 "ports in its domain XML", instance=instance)
6414         return False
6415 
6416     def _create_serial_consoles(self, guest_cfg, num_ports, char_dev_cls,
6417                                 log_path):
6418         for port in range(num_ports):
6419             console = char_dev_cls()
6420             console.port = port
6421             console.type = "tcp"
6422             console.listen_host = CONF.serial_console.proxyclient_address
6423             listen_port = serial_console.acquire_port(console.listen_host)
6424             console.listen_port = listen_port
6425             # NOTE: only the first serial console gets the boot messages,
6426             # that's why we attach the logd subdevice only to that.
6427             if port == 0:
6428                 log = vconfig.LibvirtConfigGuestCharDeviceLog()
6429                 log.file = log_path
6430                 console.log = log
6431             guest_cfg.add_device(console)
6432 
6433     def _cpu_config_to_vcpu_model(self, cpu_config, vcpu_model):
6434         """Update VirtCPUModel object according to libvirt CPU config.
6435 
6436         :param:cpu_config: vconfig.LibvirtConfigGuestCPU presenting the
6437                            instance's virtual cpu configuration.
6438         :param:vcpu_model: VirtCPUModel object. A new object will be created
6439                            if None.
6440 
6441         :return: Updated VirtCPUModel object, or None if cpu_config is None
6442 
6443         """
6444 
6445         if not cpu_config:
6446             return
6447         if not vcpu_model:
6448             vcpu_model = objects.VirtCPUModel()
6449 
6450         vcpu_model.arch = cpu_config.arch
6451         vcpu_model.vendor = cpu_config.vendor
6452         vcpu_model.model = cpu_config.model
6453         vcpu_model.mode = cpu_config.mode
6454         vcpu_model.match = cpu_config.match
6455 
6456         if cpu_config.sockets:
6457             vcpu_model.topology = objects.VirtCPUTopology(
6458                 sockets=cpu_config.sockets,
6459                 cores=cpu_config.cores,
6460                 threads=cpu_config.threads)
6461         else:
6462             vcpu_model.topology = None
6463 
6464         features = [objects.VirtCPUFeature(
6465             name=f.name,
6466             policy=f.policy) for f in cpu_config.features]
6467         vcpu_model.features = features
6468 
6469         return vcpu_model
6470 
6471     def _vcpu_model_to_cpu_config(self, vcpu_model):
6472         """Create libvirt CPU config according to VirtCPUModel object.
6473 
6474         :param:vcpu_model: VirtCPUModel object.
6475 
6476         :return: vconfig.LibvirtConfigGuestCPU.
6477 
6478         """
6479 
6480         cpu_config = vconfig.LibvirtConfigGuestCPU()
6481         cpu_config.arch = vcpu_model.arch
6482         cpu_config.model = vcpu_model.model
6483         cpu_config.mode = vcpu_model.mode
6484         cpu_config.match = vcpu_model.match
6485         cpu_config.vendor = vcpu_model.vendor
6486         if vcpu_model.topology:
6487             cpu_config.sockets = vcpu_model.topology.sockets
6488             cpu_config.cores = vcpu_model.topology.cores
6489             cpu_config.threads = vcpu_model.topology.threads
6490         if vcpu_model.features:
6491             for f in vcpu_model.features:
6492                 xf = vconfig.LibvirtConfigGuestCPUFeature()
6493                 xf.name = f.name
6494                 xf.policy = f.policy
6495                 cpu_config.features.add(xf)
6496         return cpu_config
6497 
6498     def _guest_needs_usb(self, guest, image_meta):
6499         """Evaluate devices currently attached to the guest."""
6500         if self._is_ppc64_guest(image_meta):
6501             # PPC64 guests get a USB keyboard and mouse automatically
6502             return True
6503 
6504         for dev in guest.devices:
6505             if isinstance(dev, vconfig.LibvirtConfigGuestDisk):
6506                 if dev.target_bus == 'usb':
6507                     return True
6508 
6509             if isinstance(dev, vconfig.LibvirtConfigGuestInput):
6510                 if dev.bus == 'usb':
6511                     return True
6512 
6513         return False
6514 
6515     def _guest_add_usb_root_controller(self, guest, image_meta):
6516         """Add USB root controller, if necessary.
6517 
6518         Note that these are added by default on x86-64. We add the controller
6519         here explicitly so that we can _disable_ it (by setting the model to
6520         'none') if it's not necessary.
6521         """
6522         usbhost = vconfig.LibvirtConfigGuestUSBHostController()
6523         usbhost.index = 0
6524         # an unset model means autodetect, while 'none' means don't add a
6525         # controller (x86 gets one by default)
6526         usbhost.model = None
6527         if not self._guest_needs_usb(guest, image_meta):
6528             usbhost.model = 'none'
6529         guest.add_device(usbhost)
6530 
6531     def _guest_add_pcie_root_ports(self, guest):
6532         """Add PCI Express root ports.
6533 
6534         PCI Express machine can have as many PCIe devices as it has
6535         pcie-root-port controllers (slots in virtual motherboard).
6536 
6537         If we want to have more PCIe slots for hotplug then we need to create
6538         whole PCIe structure (libvirt limitation).
6539         """
6540 
6541         pcieroot = vconfig.LibvirtConfigGuestPCIeRootController()
6542         guest.add_device(pcieroot)
6543 
6544         for x in range(0, CONF.libvirt.num_pcie_ports):
6545             pcierootport = vconfig.LibvirtConfigGuestPCIeRootPortController()
6546             guest.add_device(pcierootport)
6547 
6548     def _guest_needs_pcie(self, guest):
6549         """Check for prerequisites for adding PCIe root port
6550         controllers
6551         """
6552         caps = self._host.get_capabilities()
6553 
6554         # TODO(kchamart) In the third 'if' conditional below, for 'x86'
6555         # arch, we're assuming: when 'os_mach_type' is 'None', you'll
6556         # have "pc" machine type.  That assumption, although it is
6557         # correct for the "forseeable future", it will be invalid when
6558         # libvirt / QEMU changes the default machine types.
6559         #
6560         # From libvirt 4.7.0 onwards (September 2018), it will ensure
6561         # that *if* 'pc' is available, it will be used as the default --
6562         # to not break existing applications.  (Refer:
6563         # https://libvirt.org/git/?p=libvirt.git;a=commit;h=26cfb1a3
6564         # --"qemu: ensure default machine types don't change if QEMU
6565         # changes").
6566         #
6567         # But even if libvirt (>=v4.7.0) handled the default case,
6568         # relying on such assumptions is not robust.  Instead we should
6569         # get the default machine type for a given architecture reliably
6570         # -- by Nova setting it explicitly (we already do it for Arm /
6571         # AArch64 & s390x).  A part of this bug is being tracked here:
6572         # https://bugs.launchpad.net/nova/+bug/1780138).
6573 
6574         # Add PCIe root port controllers for PCI Express machines
6575         # but only if their amount is configured
6576 
6577         if not CONF.libvirt.num_pcie_ports:
6578             return False
6579         if (caps.host.cpu.arch == fields.Architecture.AARCH64 and
6580                 guest.os_mach_type.startswith('virt')):
6581             return True
6582         if (caps.host.cpu.arch == fields.Architecture.X86_64 and
6583                 guest.os_mach_type is not None and
6584                 'q35' in guest.os_mach_type):
6585             return True
6586         return False
6587 
6588     def _get_guest_config(self, instance, network_info, image_meta,
6589                           disk_info, rescue=None, block_device_info=None,
6590                           context=None, mdevs=None, accel_info=None):
6591         """Get config data for parameters.
6592 
6593         :param rescue: optional dictionary that should contain the key
6594             'ramdisk_id' if a ramdisk is needed for the rescue image and
6595             'kernel_id' if a kernel is needed for the rescue image.
6596 
6597         :param mdevs: optional list of mediated devices to assign to the guest.
6598         :param accel_info: optional list of accelerator requests (ARQs)
6599         """
6600         flavor = instance.flavor
6601         inst_path = libvirt_utils.get_instance_path(instance)
6602         disk_mapping = disk_info['mapping']
6603         vpmems = self._get_ordered_vpmems(instance, flavor)
6604 
6605         guest = vconfig.LibvirtConfigGuest()
6606         guest.virt_type = CONF.libvirt.virt_type
6607         guest.name = instance.name
6608         guest.uuid = instance.uuid
6609         # We are using default unit for memory: KiB
6610         guest.memory = flavor.memory_mb * units.Ki
6611         guest.vcpus = flavor.vcpus
6612 
6613         guest_numa_config = self._get_guest_numa_config(
6614             instance.numa_topology, flavor, image_meta)
6615 
6616         guest.cpuset = guest_numa_config.cpuset
6617         guest.cputune = guest_numa_config.cputune
6618         guest.numatune = guest_numa_config.numatune
6619 
6620         guest.membacking = self._get_guest_memory_backing_config(
6621             instance.numa_topology,
6622             guest_numa_config.numatune,
6623             flavor, image_meta)
6624 
6625         guest.metadata.append(self._get_guest_config_meta(
6626             instance, network_info))
6627         guest.idmaps = self._get_guest_idmaps()
6628 
6629         for event in self._supported_perf_events:
6630             guest.add_perf_event(event)
6631 
6632         self._update_guest_cputune(guest, flavor)
6633 
6634         guest.cpu = self._get_guest_cpu_config(
6635             flavor, image_meta, guest_numa_config.numaconfig,
6636             instance.numa_topology)
6637 
6638         # Notes(yjiang5): we always sync the instance's vcpu model with
6639         # the corresponding config file.
6640         instance.vcpu_model = self._cpu_config_to_vcpu_model(
6641             guest.cpu, instance.vcpu_model)
6642 
6643         if 'root' in disk_mapping:
6644             root_device_name = block_device.prepend_dev(
6645                 disk_mapping['root']['dev'])
6646         else:
6647             root_device_name = None
6648 
6649         guest.os_type = (
6650             fields.VMMode.get_from_instance(instance) or
6651             self._get_guest_os_type()
6652         )
6653 
6654         sev_enabled = self._sev_enabled(flavor, image_meta)
6655 
6656         self._configure_guest_by_virt_type(guest, instance, image_meta, flavor)
6657         if CONF.libvirt.virt_type != 'lxc':
6658             self._conf_non_lxc(
6659                 guest, root_device_name, rescue, instance, inst_path,
6660                 image_meta, disk_info)
6661 
6662         self._set_features(guest, instance.os_type, image_meta, flavor)
6663         self._set_clock(guest, instance.os_type, image_meta)
6664 
6665         storage_configs = self._get_guest_storage_config(context,
6666                 instance, image_meta, disk_info, rescue, block_device_info,
6667                 flavor, guest.os_type)
6668         for config in storage_configs:
6669             guest.add_device(config)
6670 
6671         for vif in network_info:
6672             config = self.vif_driver.get_config(
6673                 instance, vif, image_meta, flavor, CONF.libvirt.virt_type,
6674             )
6675             guest.add_device(config)
6676 
6677         self._create_consoles(guest, instance, flavor, image_meta)
6678 
6679         self._guest_add_spice_channel(guest)
6680 
6681         if self._guest_add_video_device(guest):
6682             self._add_video_driver(guest, image_meta, flavor)
6683 
6684             self._guest_add_pointer_device(guest, image_meta)
6685             self._guest_add_keyboard_device(guest, image_meta)
6686 
6687         # Some features are only supported 'qemu' and 'kvm' hypervisor
6688         if CONF.libvirt.virt_type in ('qemu', 'kvm'):
6689             self._set_qemu_guest_agent(guest, flavor, instance, image_meta)
6690             self._add_rng_device(guest, flavor, image_meta)
6691             self._add_vtpm_device(guest, flavor, instance, image_meta)
6692 
6693         if self._guest_needs_pcie(guest):
6694             self._guest_add_pcie_root_ports(guest)
6695 
6696         self._guest_add_usb_root_controller(guest, image_meta)
6697 
6698         self._guest_add_pci_devices(guest, instance)
6699 
6700         pci_arq_list = []
6701         if accel_info:
6702             # NOTE(Sundar): We handle only the case where all attach handles
6703             # are of type 'PCI'. The Cyborg fake driver used for testing
6704             # returns attach handles of type 'TEST_PCI' and so its ARQs will
6705             # not get composed into the VM's domain XML. For now, we do not
6706             # expect a mixture of different attach handles for the same
6707             # instance; but that case also gets ignored by this logic.
6708             ah_types_set = {arq['attach_handle_type'] for arq in accel_info}
6709             supported_types_set = {'PCI'}
6710             if ah_types_set == supported_types_set:
6711                 pci_arq_list = accel_info
6712             else:
6713                 LOG.info('Ignoring accelerator requests for instance %s. '
6714                          'Supported Attach handle types: %s. '
6715                          'But got these unsupported types: %s.',
6716                          instance.uuid, supported_types_set,
6717                          ah_types_set.difference(supported_types_set))
6718 
6719         self._guest_add_accel_pci_devices(guest, pci_arq_list)
6720 
6721         self._guest_add_watchdog_action(guest, flavor, image_meta)
6722 
6723         self._guest_add_memory_balloon(guest)
6724 
6725         if mdevs:
6726             self._guest_add_mdevs(guest, mdevs)
6727 
6728         if sev_enabled:
6729             caps = self._host.get_capabilities()
6730             self._guest_configure_sev(guest, caps.host.cpu.arch,
6731                                       guest.os_mach_type)
6732 
6733         if vpmems:
6734             self._guest_add_vpmems(guest, vpmems)
6735 
6736         return guest
6737 
6738     def _get_ordered_vpmems(self, instance, flavor):
6739         resources = self._get_resources(instance)
6740         ordered_vpmem_resources = self._get_ordered_vpmem_resources(
6741             resources, flavor)
6742         ordered_vpmems = [self._vpmems_by_name[resource.identifier]
6743             for resource in ordered_vpmem_resources]
6744         return ordered_vpmems
6745 
6746     def _get_vpmems(self, instance, prefix=None):
6747         resources = self._get_resources(instance, prefix=prefix)
6748         vpmem_resources = self._get_vpmem_resources(resources)
6749         vpmems = [self._vpmems_by_name[resource.identifier]
6750             for resource in vpmem_resources]
6751         return vpmems
6752 
6753     def _guest_add_vpmems(self, guest, vpmems):
6754         guest.max_memory_size = guest.memory
6755         guest.max_memory_slots = 0
6756         for vpmem in vpmems:
6757             size_kb = vpmem.size // units.Ki
6758             align_kb = vpmem.align // units.Ki
6759 
6760             vpmem_config = vconfig.LibvirtConfigGuestVPMEM(
6761                 devpath=vpmem.devpath, size_kb=size_kb, align_kb=align_kb)
6762 
6763             # max memory size needs contain vpmem size
6764             guest.max_memory_size += size_kb
6765             # one vpmem will occupy one memory slot
6766             guest.max_memory_slots += 1
6767             guest.add_device(vpmem_config)
6768 
6769     def _sev_enabled(self, flavor, image_meta):
6770         """To enable AMD SEV, the following should be true:
6771 
6772         a) the supports_amd_sev instance variable in the host is
6773            true,
6774         b) the instance extra specs and/or image properties request
6775            memory encryption to be enabled, and
6776         c) there are no conflicts between extra specs, image properties
6777            and machine type selection.
6778 
6779         Most potential conflicts in c) should already be caught in the
6780         API layer.  However there is still one remaining case which
6781         needs to be handled here: when the image does not contain an
6782         hw_machine_type property, the machine type will be chosen from
6783         CONF.libvirt.hw_machine_type if configured, otherwise falling
6784         back to the hardcoded value which is currently 'pc'.  If it
6785         ends up being 'pc' or another value not in the q35 family, we
6786         need to raise an exception.  So calculate the machine type and
6787         pass it to be checked alongside the other sanity checks which
6788         are run while determining whether SEV is selected.
6789         """
6790         if not self._host.supports_amd_sev:
6791             return False
6792 
6793         mach_type = libvirt_utils.get_machine_type(image_meta)
6794         return hardware.get_mem_encryption_constraint(flavor, image_meta,
6795                                                       mach_type)
6796 
6797     def _guest_configure_sev(self, guest, arch, mach_type):
6798         sev = self._find_sev_feature(arch, mach_type)
6799         if sev is None:
6800             # In theory this should never happen because it should
6801             # only get called if SEV was requested, in which case the
6802             # guest should only get scheduled on this host if it
6803             # supports SEV, and SEV support is dependent on the
6804             # presence of this <sev> feature.  That said, it's
6805             # conceivable that something could get messed up along the
6806             # way, e.g. a mismatch in the choice of machine type.  So
6807             # make sure that if it ever does happen, we at least get a
6808             # helpful error rather than something cryptic like
6809             # "AttributeError: 'NoneType' object has no attribute 'cbitpos'
6810             raise exception.MissingDomainCapabilityFeatureException(
6811                 feature='sev')
6812 
6813         designer.set_driver_iommu_for_all_devices(guest)
6814         self._guest_add_launch_security(guest, sev)
6815 
6816     def _guest_add_launch_security(self, guest, sev):
6817         launch_security = vconfig.LibvirtConfigGuestSEVLaunchSecurity()
6818         launch_security.cbitpos = sev.cbitpos
6819         launch_security.reduced_phys_bits = sev.reduced_phys_bits
6820         guest.launch_security = launch_security
6821 
6822     def _find_sev_feature(self, arch, mach_type):
6823         """Search domain capabilities for the given arch and machine type
6824         for the <sev> element under <features>, and return it if found.
6825         """
6826         domain_caps = self._host.get_domain_capabilities()
6827         if arch not in domain_caps:
6828             LOG.warning(
6829                 "Wanted to add SEV to config for guest with arch %(arch)s "
6830                 "but only had domain capabilities for: %(archs)s",
6831                 {'arch': arch, 'archs': ' '.join(domain_caps)})
6832             return None
6833 
6834         if mach_type not in domain_caps[arch]:
6835             LOG.warning(
6836                 "Wanted to add SEV to config for guest with machine type "
6837                 "%(mtype)s but for arch %(arch)s only had domain capabilities "
6838                 "for machine types: %(mtypes)s",
6839                 {'mtype': mach_type, 'arch': arch,
6840                  'mtypes': ' '.join(domain_caps[arch])})
6841             return None
6842 
6843         for feature in domain_caps[arch][mach_type].features:
6844             if feature.root_name == 'sev':
6845                 return feature
6846 
6847         return None
6848 
6849     def _guest_add_mdevs(self, guest, chosen_mdevs):
6850         for chosen_mdev in chosen_mdevs:
6851             mdev = vconfig.LibvirtConfigGuestHostdevMDEV()
6852             mdev.uuid = chosen_mdev
6853             guest.add_device(mdev)
6854 
6855     @staticmethod
6856     def _guest_add_spice_channel(guest):
6857         if (
6858             CONF.spice.enabled and CONF.spice.agent_enabled and
6859             CONF.libvirt.virt_type != 'lxc'
6860         ):
6861             channel = vconfig.LibvirtConfigGuestChannel()
6862             channel.type = 'spicevmc'
6863             channel.target_name = "com.redhat.spice.0"
6864             guest.add_device(channel)
6865 
6866     @staticmethod
6867     def _guest_add_memory_balloon(guest):
6868         # Memory balloon device only support 'qemu/kvm' hypervisor
6869         if (
6870             CONF.libvirt.virt_type in ('qemu', 'kvm') and
6871             CONF.libvirt.mem_stats_period_seconds > 0
6872         ):
6873             balloon = vconfig.LibvirtConfigMemoryBalloon()
6874             balloon.model = 'virtio'
6875             balloon.period = CONF.libvirt.mem_stats_period_seconds
6876             guest.add_device(balloon)
6877 
6878     @staticmethod
6879     def _guest_add_watchdog_action(guest, flavor, image_meta):
6880         # image meta takes precedence over flavor extra specs; disable the
6881         # watchdog action by default
6882         watchdog_action = (flavor.extra_specs.get('hw:watchdog_action') or
6883                            'disabled')
6884         watchdog_action = image_meta.properties.get('hw_watchdog_action',
6885                                                     watchdog_action)
6886         # NB(sross): currently only actually supported by KVM/QEmu
6887         if watchdog_action != 'disabled':
6888             if watchdog_action in fields.WatchdogAction.ALL:
6889                 bark = vconfig.LibvirtConfigGuestWatchdog()
6890                 bark.action = watchdog_action
6891                 guest.add_device(bark)
6892             else:
6893                 raise exception.InvalidWatchdogAction(action=watchdog_action)
6894 
6895     def _guest_add_pci_devices(self, guest, instance):
6896         if CONF.libvirt.virt_type in ('qemu', 'kvm'):
6897             # Get all generic PCI devices (non-SR-IOV).
6898             for pci_dev in pci_manager.get_instance_pci_devs(instance):
6899                 guest.add_device(self._get_guest_pci_device(pci_dev))
6900         else:
6901             # PCI devices is only supported for QEMU/KVM hypervisor
6902             if pci_manager.get_instance_pci_devs(instance, 'all'):
6903                 raise exception.PciDeviceUnsupportedHypervisor(
6904                     type=CONF.libvirt.virt_type
6905                 )
6906 
6907     def _guest_add_accel_pci_devices(self, guest, accel_info):
6908         """Add all accelerator PCI functions from ARQ list."""
6909         for arq in accel_info:
6910             dev = vconfig.LibvirtConfigGuestHostdevPCI()
6911             pci_addr = arq['attach_handle_info']
6912             dev.domain, dev.bus, dev.slot, dev.function = (
6913                 pci_addr['domain'], pci_addr['bus'],
6914                 pci_addr['device'], pci_addr['function'])
6915             self._set_managed_mode(dev)
6916 
6917             guest.add_device(dev)
6918 
6919     @staticmethod
6920     def _guest_add_video_device(guest):
6921         if CONF.libvirt.virt_type == 'lxc':
6922             return False
6923 
6924         # NB some versions of libvirt support both SPICE and VNC
6925         # at the same time. We're not trying to second guess which
6926         # those versions are. We'll just let libvirt report the
6927         # errors appropriately if the user enables both.
6928         add_video_driver = False
6929 
6930         if CONF.vnc.enabled:
6931             graphics = vconfig.LibvirtConfigGuestGraphics()
6932             graphics.type = "vnc"
6933             graphics.listen = CONF.vnc.server_listen
6934             guest.add_device(graphics)
6935             add_video_driver = True
6936 
6937         if CONF.spice.enabled:
6938             graphics = vconfig.LibvirtConfigGuestGraphics()
6939             graphics.type = "spice"
6940             graphics.listen = CONF.spice.server_listen
6941             guest.add_device(graphics)
6942             add_video_driver = True
6943 
6944         return add_video_driver
6945 
6946     def _guest_add_pointer_device(self, guest, image_meta):
6947         """Build the pointer device to add to the instance.
6948 
6949         The configuration is determined by examining the 'hw_input_bus' image
6950         metadata property, the 'hw_pointer_model' image metadata property, and
6951         the '[DEFAULT] pointer_model' config option in that order.
6952         """
6953         pointer_bus = image_meta.properties.get('hw_input_bus')
6954         pointer_model = image_meta.properties.get('hw_pointer_model')
6955 
6956         if pointer_bus:
6957             pointer_model = 'tablet'
6958             pointer_bus = pointer_bus
6959         elif pointer_model or CONF.pointer_model == 'usbtablet':
6960             # Handle the legacy 'hw_pointer_model' image metadata property
6961             pointer_model = 'tablet'
6962             pointer_bus = 'usb'
6963         else:
6964             # If the user hasn't requested anything and the host config says to
6965             # use something other than a USB tablet, there's nothing to do
6966             return
6967 
6968         # For backward compatibility, we don't want to error out if the host
6969         # configuration requests a USB tablet but the virtual machine mode is
6970         # not configured as HVM.
6971         if guest.os_type != fields.VMMode.HVM:
6972             LOG.warning(
6973                 'USB tablet requested for guests on non-HVM host; '
6974                 'in order to accept this request the machine mode should '
6975                 'be configured as HVM.')
6976             return
6977 
6978         # Ditto for using a USB tablet when the SPICE agent is enabled, since
6979         # that has a paravirt mouse builtin which drastically reduces overhead;
6980         # this only applies if VNC is not also enabled though, since that still
6981         # needs the device
6982         if (
6983             CONF.spice.enabled and CONF.spice.agent_enabled and
6984             not CONF.vnc.enabled
6985         ):
6986             LOG.warning(
6987                 'USB tablet requested for guests but the SPICE agent is '
6988                 'enabled; ignoring request in favour of default '
6989                 'configuration.')
6990             return
6991 
6992         pointer = vconfig.LibvirtConfigGuestInput()
6993         pointer.type = pointer_model
6994         pointer.bus = pointer_bus
6995         guest.add_device(pointer)
6996 
6997         # returned for unit testing purposes
6998         return pointer
6999 
7000     def _guest_add_keyboard_device(self, guest, image_meta):
7001         """Add keyboard for graphical console use."""
7002         bus = image_meta.properties.get('hw_input_bus')
7003 
7004         if not bus:
7005             # AArch64 doesn't provide a default keyboard so we explicitly add
7006             # one; for everything else we rely on default (e.g. for x86,
7007             # libvirt will automatically add a PS2 keyboard)
7008             # TODO(stephenfin): We might want to do this for other non-x86
7009             # architectures
7010             arch = libvirt_utils.get_arch(image_meta)
7011             if arch != fields.Architecture.AARCH64:
7012                 return None
7013 
7014             bus = 'usb'
7015 
7016         keyboard = vconfig.LibvirtConfigGuestInput()
7017         keyboard.type = 'keyboard'
7018         keyboard.bus = bus
7019         guest.add_device(keyboard)
7020 
7021         # returned for unit testing purposes
7022         return keyboard
7023 
7024     def _get_guest_xml(self, context, instance, network_info, disk_info,
7025                        image_meta, rescue=None,
7026                        block_device_info=None,
7027                        mdevs=None, accel_info=None):
7028         # NOTE(danms): Stringifying a NetworkInfo will take a lock. Do
7029         # this ahead of time so that we don't acquire it while also
7030         # holding the Logs lock.
7031         network_info_str = str(network_info)
7032         msg = ('Start _get_guest_xml '
7033                'network_info=%(network_info)s '
7034                'disk_info=%(disk_info)s '
7035                'image_meta=%(image_meta)s rescue=%(rescue)s '
7036                'block_device_info=%(block_device_info)s' %
7037                {'network_info': network_info_str, 'disk_info': disk_info,
7038                 'image_meta': image_meta, 'rescue': rescue,
7039                 'block_device_info': block_device_info})
7040         # NOTE(mriedem): block_device_info can contain auth_password so we
7041         # need to sanitize the password in the message.
7042         LOG.debug(strutils.mask_password(msg), instance=instance)
7043         conf = self._get_guest_config(instance, network_info, image_meta,
7044                                       disk_info, rescue, block_device_info,
7045                                       context, mdevs, accel_info)
7046         xml = conf.to_xml()
7047 
7048         LOG.debug('End _get_guest_xml xml=%(xml)s',
7049                   {'xml': xml}, instance=instance)
7050         return xml
7051 
7052     def get_info(self, instance, use_cache=True):
7053         """Retrieve information from libvirt for a specific instance.
7054 
7055         If a libvirt error is encountered during lookup, we might raise a
7056         NotFound exception or Error exception depending on how severe the
7057         libvirt error is.
7058 
7059         :param instance: nova.objects.instance.Instance object
7060         :param use_cache: unused in this driver
7061         :returns: An InstanceInfo object
7062         """
7063         guest = self._host.get_guest(instance)
7064         # Kind of ugly but we need to pass host to get_info as for a
7065         # workaround, see libvirt/compat.py
7066         return guest.get_info(self._host)
7067 
7068     def _create_domain_setup_lxc(self, context, instance, image_meta,
7069                                  block_device_info):
7070         inst_path = libvirt_utils.get_instance_path(instance)
7071         block_device_mapping = driver.block_device_info_get_mapping(
7072             block_device_info)
7073         root_disk = block_device.get_root_bdm(block_device_mapping)
7074         if root_disk:
7075             self._connect_volume(context, root_disk['connection_info'],
7076                                  instance)
7077             disk_path = root_disk['connection_info']['data']['device_path']
7078 
7079             # NOTE(apmelton) - Even though the instance is being booted from a
7080             # cinder volume, it is still presented as a local block device.
7081             # LocalBlockImage is used here to indicate that the instance's
7082             # disk is backed by a local block device.
7083             image_model = imgmodel.LocalBlockImage(disk_path)
7084         else:
7085             root_disk = self.image_backend.by_name(instance, 'disk')
7086             image_model = root_disk.get_model(self._conn)
7087 
7088         container_dir = os.path.join(inst_path, 'rootfs')
7089         fileutils.ensure_tree(container_dir)
7090         rootfs_dev = disk_api.setup_container(image_model,
7091                                               container_dir=container_dir)
7092 
7093         try:
7094             # Save rootfs device to disconnect it when deleting the instance
7095             if rootfs_dev:
7096                 instance.system_metadata['rootfs_device_name'] = rootfs_dev
7097             if CONF.libvirt.uid_maps or CONF.libvirt.gid_maps:
7098                 id_maps = self._get_guest_idmaps()
7099                 libvirt_utils.chown_for_id_maps(container_dir, id_maps)
7100         except Exception:
7101             with excutils.save_and_reraise_exception():
7102                 self._create_domain_cleanup_lxc(instance)
7103 
7104     def _create_domain_cleanup_lxc(self, instance):
7105         inst_path = libvirt_utils.get_instance_path(instance)
7106         container_dir = os.path.join(inst_path, 'rootfs')
7107 
7108         try:
7109             state = self.get_info(instance).state
7110         except exception.InstanceNotFound:
7111             # The domain may not be present if the instance failed to start
7112             state = None
7113 
7114         if state == power_state.RUNNING:
7115             # NOTE(uni): Now the container is running with its own private
7116             # mount namespace and so there is no need to keep the container
7117             # rootfs mounted in the host namespace
7118             LOG.debug('Attempting to unmount container filesystem: %s',
7119                       container_dir, instance=instance)
7120             disk_api.clean_lxc_namespace(container_dir=container_dir)
7121         else:
7122             disk_api.teardown_container(container_dir=container_dir)
7123 
7124     @contextlib.contextmanager
7125     def _lxc_disk_handler(self, context, instance, image_meta,
7126                           block_device_info):
7127         """Context manager to handle the pre and post instance boot,
7128            LXC specific disk operations.
7129 
7130            An image or a volume path will be prepared and setup to be
7131            used by the container, prior to starting it.
7132            The disk will be disconnected and unmounted if a container has
7133            failed to start.
7134         """
7135 
7136         if CONF.libvirt.virt_type != 'lxc':
7137             yield
7138             return
7139 
7140         self._create_domain_setup_lxc(context, instance, image_meta,
7141                                       block_device_info)
7142 
7143         try:
7144             yield
7145         finally:
7146             self._create_domain_cleanup_lxc(instance)
7147 
7148     def _create_guest(
7149         self,
7150         context: nova_context.RequestContext,
7151         xml: str,
7152         instance: 'objects.Instance',
7153         power_on: bool = True,
7154         pause: bool = False,
7155         post_xml_callback: ty.Callable = None,
7156     ) -> libvirt_guest.Guest:
7157         """Create a Guest from XML.
7158 
7159         Create a Guest, which in turn creates a libvirt domain, from XML,
7160         optionally starting it after creation.
7161 
7162         :returns guest.Guest: Created guest.
7163         """
7164         libvirt_secret = None
7165         # determine whether vTPM is in use and, if so, create the secret
7166         if CONF.libvirt.swtpm_enabled and hardware.get_vtpm_constraint(
7167             instance.flavor, instance.image_meta,
7168         ):
7169             secret_uuid, passphrase = crypto.ensure_vtpm_secret(
7170                 context, instance)
7171             libvirt_secret = self._host.create_secret(
7172                 'vtpm', instance.uuid, password=passphrase,
7173                 uuid=secret_uuid)
7174 
7175         try:
7176             guest = libvirt_guest.Guest.create(xml, self._host)
7177             if post_xml_callback is not None:
7178                 post_xml_callback()
7179 
7180             if power_on or pause:
7181                 guest.launch(pause=pause)
7182 
7183             return guest
7184         finally:
7185             if libvirt_secret is not None:
7186                 libvirt_secret.undefine()
7187 
7188     def _neutron_failed_callback(self, event_name, instance):
7189         LOG.error('Neutron Reported failure on event '
7190                   '%(event)s for instance %(uuid)s',
7191                   {'event': event_name, 'uuid': instance.uuid},
7192                   instance=instance)
7193         if CONF.vif_plugging_is_fatal:
7194             raise exception.VirtualInterfaceCreateException()
7195 
7196     def _get_neutron_events(self, network_info):
7197         # NOTE(danms): We need to collect any VIFs that are currently
7198         # down that we expect a down->up event for. Anything that is
7199         # already up will not undergo that transition, and for
7200         # anything that might be stale (cache-wise) assume it's
7201         # already up so we don't block on it.
7202         return [('network-vif-plugged', vif['id'])
7203                 for vif in network_info if vif.get('active', True) is False]
7204 
7205     def _create_guest_with_network(
7206         self,
7207         context: nova_context.RequestContext,
7208         xml: str,
7209         instance: 'objects.Instance',
7210         network_info: network_model.NetworkInfo,
7211         block_device_info: ty.Optional[ty.Dict[str, ty.Any]],
7212         power_on: bool = True,
7213         vifs_already_plugged: bool = False,
7214         post_xml_callback: ty.Callable = None,
7215         external_events: ty.Optional[ty.List[str]] = None,
7216         cleanup_instance_dir: bool = False,
7217         cleanup_instance_disks: bool = False,
7218     ) -> libvirt_guest.Guest:
7219         """Do required network setup and create domain."""
7220 
7221         timeout = CONF.vif_plugging_timeout
7222         if (
7223             CONF.libvirt.virt_type in ('kvm', 'qemu') and
7224             not vifs_already_plugged and power_on and timeout
7225         ):
7226             events = (external_events if external_events
7227                       else self._get_neutron_events(network_info))
7228         else:
7229             events = []
7230 
7231         pause = bool(events)
7232         try:
7233             with self.virtapi.wait_for_instance_event(
7234                 instance, events, deadline=timeout,
7235                 error_callback=self._neutron_failed_callback,
7236             ):
7237                 self.plug_vifs(instance, network_info)
7238                 with self._lxc_disk_handler(
7239                     context, instance, instance.image_meta, block_device_info,
7240                 ):
7241                     guest = self._create_guest(
7242                         context, xml, instance,
7243                         pause=pause, power_on=power_on,
7244                         post_xml_callback=post_xml_callback)
7245         except eventlet.timeout.Timeout:
7246             # We never heard from Neutron
7247             LOG.warning(
7248                 'Timeout waiting for %(events)s for instance with '
7249                 'vm_state %(vm_state)s and task_state %(task_state)s',
7250                 {
7251                     'events': events,
7252                     'vm_state': instance.vm_state,
7253                     'task_state': instance.task_state,
7254                 },
7255                 instance=instance)
7256 
7257             if CONF.vif_plugging_is_fatal:
7258                 # NOTE(stephenfin): don't worry, guest will be in scope since
7259                 # we can only hit this branch if the VIF plug timed out
7260                 if guest.is_active():
7261                     guest.poweroff()
7262                 self._cleanup(
7263                     context, instance, network_info, block_device_info,
7264                     destroy_vifs=True,
7265                     cleanup_instance_dir=cleanup_instance_dir,
7266                     cleanup_instance_disks=cleanup_instance_disks)
7267                 raise exception.VirtualInterfaceCreateException()
7268         except Exception:
7269             # Any other error, be sure to clean up
7270             LOG.error('Failed to start libvirt guest', instance=instance)
7271             with excutils.save_and_reraise_exception():
7272                 self._cleanup(
7273                     context, instance, network_info, block_device_info,
7274                     destroy_vifs=True,
7275                     cleanup_instance_dir=cleanup_instance_dir,
7276                     cleanup_instance_disks=cleanup_instance_disks)
7277 
7278         # Resume only if domain has been paused
7279         if pause:
7280             guest.resume()
7281 
7282         return guest
7283 
7284     def _get_pcpu_available(self):
7285         """Get number of host cores to be used for PCPUs.
7286 
7287         :returns: The number of host cores to be used for PCPUs.
7288         """
7289         if not CONF.compute.cpu_dedicated_set:
7290             return set()
7291 
7292         online_cpus = self._host.get_online_cpus()
7293         dedicated_cpus = hardware.get_cpu_dedicated_set()
7294 
7295         if not dedicated_cpus.issubset(online_cpus):
7296             msg = _("Invalid '[compute] cpu_dedicated_set' config: one or "
7297                     "more of the configured CPUs is not online. Online "
7298                     "cpuset(s): %(online)s, configured cpuset(s): %(req)s")
7299             raise exception.Invalid(msg % {
7300                 'online': sorted(online_cpus),
7301                 'req': sorted(dedicated_cpus)})
7302 
7303         return dedicated_cpus
7304 
7305     def _get_vcpu_available(self):
7306         """Get host cores to be used for VCPUs.
7307 
7308         :returns: A list of host CPU cores that can be used for VCPUs.
7309         """
7310         online_cpus = self._host.get_online_cpus()
7311 
7312         # NOTE(stephenfin): The use of the legacy 'vcpu_pin_set' option happens
7313         # if it's defined, regardless of whether '[compute] cpu_shared_set' is
7314         # also configured. This is legacy behavior required for upgrades that
7315         # should be removed in the future, when we can rely exclusively on
7316         # '[compute] cpu_shared_set'.
7317         if CONF.vcpu_pin_set:
7318             # TODO(stephenfin): Remove this in U
7319             shared_cpus = hardware.get_vcpu_pin_set()
7320         elif CONF.compute.cpu_shared_set:
7321             shared_cpus = hardware.get_cpu_shared_set()
7322         elif CONF.compute.cpu_dedicated_set:
7323             return set()
7324         else:
7325             return online_cpus
7326 
7327         if not shared_cpus.issubset(online_cpus):
7328             msg = _("Invalid '%(config_opt)s' config: one or "
7329                     "more of the configured CPUs is not online. Online "
7330                     "cpuset(s): %(online)s, configured cpuset(s): %(req)s")
7331 
7332             if CONF.vcpu_pin_set:
7333                 config_opt = 'vcpu_pin_set'
7334             else:  # CONF.compute.cpu_shared_set
7335                 config_opt = '[compute] cpu_shared_set'
7336 
7337             raise exception.Invalid(msg % {
7338                 'config_opt': config_opt,
7339                 'online': sorted(online_cpus),
7340                 'req': sorted(shared_cpus)})
7341 
7342         return shared_cpus
7343 
7344     @staticmethod
7345     def _get_local_gb_info():
7346         """Get local storage info of the compute node in GB.
7347 
7348         :returns: A dict containing:
7349              :total: How big the overall usable filesystem is (in gigabytes)
7350              :free: How much space is free (in gigabytes)
7351              :used: How much space is used (in gigabytes)
7352         """
7353 
7354         if CONF.libvirt.images_type == 'lvm':
7355             info = lvm.get_volume_group_info(
7356                                CONF.libvirt.images_volume_group)
7357         elif CONF.libvirt.images_type == 'rbd':
7358             info = rbd_utils.RBDDriver().get_pool_info()
7359         else:
7360             info = libvirt_utils.get_fs_info(CONF.instances_path)
7361 
7362         for (k, v) in info.items():
7363             info[k] = v / units.Gi
7364 
7365         return info
7366 
7367     def _get_vcpu_used(self):
7368         """Get vcpu usage number of physical computer.
7369 
7370         :returns: The total number of vcpu(s) that are currently being used.
7371 
7372         """
7373 
7374         total = 0
7375 
7376         # Not all libvirt drivers will support the get_vcpus_info()
7377         #
7378         # For example, LXC does not have a concept of vCPUs, while
7379         # QEMU (TCG) traditionally handles all vCPUs in a single
7380         # thread. So both will report an exception when the vcpus()
7381         # API call is made. In such a case we should report the
7382         # guest as having 1 vCPU, since that lets us still do
7383         # CPU over commit calculations that apply as the total
7384         # guest count scales.
7385         #
7386         # It is also possible that we might see an exception if
7387         # the guest is just in middle of shutting down. Technically
7388         # we should report 0 for vCPU usage in this case, but we
7389         # we can't reliably distinguish the vcpu not supported
7390         # case from the just shutting down case. Thus we don't know
7391         # whether to report 1 or 0 for vCPU count.
7392         #
7393         # Under-reporting vCPUs is bad because it could conceivably
7394         # let the scheduler place too many guests on the host. Over-
7395         # reporting vCPUs is not a problem as it'll auto-correct on
7396         # the next refresh of usage data.
7397         #
7398         # Thus when getting an exception we always report 1 as the
7399         # vCPU count, as the least worst value.
7400         for guest in self._host.list_guests():
7401             try:
7402                 vcpus = guest.get_vcpus_info()
7403                 total += len(list(vcpus))
7404             except libvirt.libvirtError:
7405                 total += 1
7406             # NOTE(gtt116): give other tasks a chance.
7407             greenthread.sleep(0)
7408         return total
7409 
7410     def _get_supported_vgpu_types(self):
7411         if not CONF.devices.enabled_mdev_types:
7412             return []
7413 
7414         # Make sure we register all the types as the compute service could
7415         # be calling this method before init_host()
7416         if len(CONF.devices.enabled_mdev_types) > 1:
7417             nova.conf.devices.register_dynamic_opts(CONF)
7418 
7419         for vgpu_type in CONF.devices.enabled_mdev_types:
7420             group = getattr(CONF, 'mdev_%s' % vgpu_type, None)
7421             if group is None or not group.device_addresses:
7422                 first_type = CONF.devices.enabled_mdev_types[0]
7423                 if len(CONF.devices.enabled_mdev_types) > 1:
7424                     # Only provide the warning if the operator provided more
7425                     # than one type as it's not needed to provide groups
7426                     # if you only use one vGPU type.
7427                     msg = ("The mdev type '%(type)s' was listed in '[devices] "
7428                            "enabled_mdev_types' but no corresponding "
7429                            "'[mdev_%(type)s]' group or "
7430                            "'[mdev_%(type)s] device_addresses' "
7431                            "option was defined. Only the first type "
7432                            "'%(ftype)s' will be used." % {'type': vgpu_type,
7433                                                          'ftype': first_type})
7434                     LOG.warning(msg)
7435                 # We need to reset the mapping tables that we started to
7436                 # provide keys and values from previously processed vGPUs but
7437                 # since there is a problem for this vGPU type, we only want to
7438                 # support only the first type.
7439                 self.pgpu_type_mapping.clear()
7440                 self.mdev_class_mapping.clear()
7441                 # Given we only have one type, we default to only support the
7442                 # VGPU resource class.
7443                 self.mdev_classes = {orc.VGPU}
7444                 return [first_type]
7445             mdev_class = group.mdev_class
7446             for device_address in group.device_addresses:
7447                 if device_address in self.pgpu_type_mapping:
7448                     raise exception.InvalidLibvirtMdevConfig(
7449                         reason="duplicate types for PCI ID %s" % device_address
7450                     )
7451                 # Just checking whether the operator fat-fingered the address.
7452                 # If it's wrong, it will return an exception
7453                 try:
7454                     pci_utils.parse_address(device_address)
7455                 except exception.PciDeviceWrongAddressFormat:
7456                     raise exception.InvalidLibvirtMdevConfig(
7457                         reason="incorrect PCI address: %s" % device_address
7458                     )
7459                 self.pgpu_type_mapping[device_address] = vgpu_type
7460                 self.mdev_class_mapping[device_address] = mdev_class
7461                 self.mdev_classes.add(mdev_class)
7462         return CONF.devices.enabled_mdev_types
7463 
7464     @staticmethod
7465     def _get_pci_id_from_libvirt_name(
7466             libvirt_address: str
7467         ) -> ty.Optional[str]:
7468         """Returns a PCI ID from a libvirt pci address name.
7469 
7470         :param libvirt_address: the libvirt PCI device name,
7471                                 eg.'pci_0000_84_00_0'
7472         """
7473         try:
7474             device_address = "{}:{}:{}.{}".format(
7475                 *libvirt_address[4:].split('_'))
7476             # Validates whether it's a PCI ID...
7477             pci_utils.parse_address(device_address)
7478         # .format() can return IndexError
7479         except (exception.PciDeviceWrongAddressFormat, IndexError):
7480             # this is not a valid PCI address
7481             LOG.warning("The PCI address %s was invalid for getting the "
7482                         "related mdev type", libvirt_address)
7483             return None
7484         return device_address
7485 
7486     def _get_vgpu_type_per_pgpu(self, device_address):
7487         """Provides the vGPU type the pGPU supports.
7488 
7489         :param device_address: the libvirt PCI device name,
7490                                eg.'pci_0000_84_00_0'
7491         """
7492         # Bail out quickly if we don't support vGPUs
7493         if not self.supported_vgpu_types:
7494             return
7495 
7496         if len(self.supported_vgpu_types) == 1:
7497             # The operator wanted to only support one single type so we can
7498             # blindly return it for every single pGPU
7499             return self.supported_vgpu_types[0]
7500         device_address = self._get_pci_id_from_libvirt_name(device_address)
7501         if not device_address:
7502             return
7503         try:
7504             return self.pgpu_type_mapping.get(device_address)
7505         except KeyError:
7506             LOG.warning("No mdev type was configured for PCI address: %s",
7507                         device_address)
7508             # We accept to return None instead of raising an exception
7509             # because we prefer the callers to return the existing exceptions
7510             # in case we can't find a specific pGPU
7511             return
7512 
7513     def _get_resource_class_for_device(self, device_address):
7514         """Returns the resource class for the inventory of this device.
7515 
7516         :param device_address: the libvirt PCI device name,
7517                                eg.'pci_0000_84_00_0'
7518         """
7519 
7520         device_address = self._get_pci_id_from_libvirt_name(device_address)
7521         if not device_address:
7522             # By default, we should always support VGPU as the standard RC
7523             return orc.VGPU
7524         # Remember, this is a defaultdict with orc.VGPU as the default RC
7525         mdev_class = self.mdev_class_mapping[device_address]
7526         return mdev_class
7527 
7528     def _get_supported_mdev_resource_classes(self):
7529         return self.mdev_classes
7530 
7531     def _count_mediated_devices(self, enabled_mdev_types):
7532         """Counts the sysfs objects (handles) that represent a mediated device
7533         and filtered by $enabled_mdev_types.
7534 
7535         Those handles can be in use by a libvirt guest or not.
7536 
7537         :param enabled_mdev_types: list of enabled VGPU types on this host
7538         :returns: dict, keyed by parent GPU libvirt PCI device ID, of number of
7539         mdev device handles for that GPU
7540         """
7541 
7542         counts_per_parent: ty.Dict[str, int] = collections.defaultdict(int)
7543         mediated_devices = self._get_mediated_devices(types=enabled_mdev_types)
7544         for mdev in mediated_devices:
7545             parent_vgpu_type = self._get_vgpu_type_per_pgpu(mdev['parent'])
7546             if mdev['type'] != parent_vgpu_type:
7547                 # Even if some mdev was created for another vGPU type, just
7548                 # verify all the mdevs related to the type that their pGPU
7549                 # has
7550                 continue
7551             counts_per_parent[mdev['parent']] += 1
7552         return counts_per_parent
7553 
7554     def _count_mdev_capable_devices(self, enabled_mdev_types):
7555         """Counts the mdev-capable devices on this host filtered by
7556         $enabled_mdev_types.
7557 
7558         :param enabled_mdev_types: list of enabled VGPU types on this host
7559         :returns: dict, keyed by device name, to an integer count of available
7560             instances of each type per device
7561         """
7562         mdev_capable_devices = self._get_mdev_capable_devices(
7563             types=enabled_mdev_types)
7564         counts_per_dev: ty.Dict[str, int] = collections.defaultdict(int)
7565         for dev in mdev_capable_devices:
7566             # dev_id is the libvirt name for the PCI device,
7567             # eg. pci_0000_84_00_0 which matches a PCI address of 0000:84:00.0
7568             dev_name = dev['dev_id']
7569             dev_supported_type = self._get_vgpu_type_per_pgpu(dev_name)
7570             for _type in dev['types']:
7571                 if _type != dev_supported_type:
7572                     # This is not the type the operator wanted to support for
7573                     # this physical GPU
7574                     continue
7575                 available = dev['types'][_type]['availableInstances']
7576                 # NOTE(sbauza): Even if we support multiple types, Nova will
7577                 # only use one per physical GPU.
7578                 counts_per_dev[dev_name] += available
7579         return counts_per_dev
7580 
7581     def _get_gpu_inventories(self):
7582         """Returns the inventories for each physical GPU for a specific type
7583         supported by the enabled_mdev_types CONF option.
7584 
7585         :returns: dict, keyed by libvirt PCI name, of dicts like:
7586                 {'pci_0000_84_00_0':
7587                     {'total': $TOTAL,
7588                      'min_unit': 1,
7589                      'max_unit': $TOTAL,
7590                      'step_size': 1,
7591                      'reserved': 0,
7592                      'allocation_ratio': 1.0,
7593                     }
7594                 }
7595         """
7596 
7597         # Bail out early if operator doesn't care about providing vGPUs
7598         enabled_mdev_types = self.supported_vgpu_types
7599         if not enabled_mdev_types:
7600             return {}
7601         inventories = {}
7602         count_per_parent = self._count_mediated_devices(enabled_mdev_types)
7603         for dev_name, count in count_per_parent.items():
7604             inventories[dev_name] = {'total': count}
7605         # Filter how many available mdevs we can create for all the supported
7606         # types.
7607         count_per_dev = self._count_mdev_capable_devices(enabled_mdev_types)
7608         # Combine the counts into the dict that we return to the caller.
7609         for dev_name, count in count_per_dev.items():
7610             inv_per_parent = inventories.setdefault(
7611                 dev_name, {'total': 0})
7612             inv_per_parent['total'] += count
7613             inv_per_parent.update({
7614                 'min_unit': 1,
7615                 'step_size': 1,
7616                 'reserved': 0,
7617                 # NOTE(sbauza): There is no sense to have a ratio but 1.0
7618                 # since we can't overallocate vGPU resources
7619                 'allocation_ratio': 1.0,
7620                 # FIXME(sbauza): Some vendors could support only one
7621                 'max_unit': inv_per_parent['total'],
7622             })
7623 
7624         return inventories
7625 
7626     def _get_instance_capabilities(self):
7627         """Get hypervisor instance capabilities
7628 
7629         Returns a list of tuples that describe instances the
7630         hypervisor is capable of hosting.  Each tuple consists
7631         of the triplet (arch, hypervisor_type, vm_mode).
7632 
7633         :returns: List of tuples describing instance capabilities
7634         """
7635         caps = self._host.get_capabilities()
7636         instance_caps = list()
7637         for g in caps.guests:
7638             for domain_type in g.domains:
7639                 try:
7640                     instance_cap = (
7641                         fields.Architecture.canonicalize(g.arch),
7642                         fields.HVType.canonicalize(domain_type),
7643                         fields.VMMode.canonicalize(g.ostype))
7644                     instance_caps.append(instance_cap)
7645                 except exception.InvalidArchitectureName:
7646                     # NOTE(danms): Libvirt is exposing a guest arch that nova
7647                     # does not even know about. Avoid aborting here and
7648                     # continue to process the rest.
7649                     pass
7650 
7651         return instance_caps
7652 
7653     def _get_cpu_info(self):
7654         """Get cpuinfo information.
7655 
7656         Obtains cpu feature from virConnect.getCapabilities.
7657 
7658         :return: see above description
7659 
7660         """
7661 
7662         caps = self._host.get_capabilities()
7663         cpu_info = dict()
7664 
7665         cpu_info['arch'] = caps.host.cpu.arch
7666         cpu_info['model'] = caps.host.cpu.model
7667         cpu_info['vendor'] = caps.host.cpu.vendor
7668 
7669         topology = dict()
7670         topology['cells'] = len(getattr(caps.host.topology, 'cells', [1]))
7671         topology['sockets'] = caps.host.cpu.sockets
7672         topology['cores'] = caps.host.cpu.cores
7673         topology['threads'] = caps.host.cpu.threads
7674         cpu_info['topology'] = topology
7675 
7676         features = set()
7677         for f in caps.host.cpu.features:
7678             features.add(f.name)
7679         cpu_info['features'] = features
7680         return cpu_info
7681 
7682     # TODO(stephenfin): Move this to 'host.py'
7683     def _get_pci_passthrough_devices(self):
7684         """Get host PCI devices information.
7685 
7686         Obtains pci devices information from libvirt, and returns
7687         as a JSON string.
7688 
7689         Each device information is a dictionary, with mandatory keys
7690         of 'address', 'vendor_id', 'product_id', 'dev_type', 'dev_id',
7691         'label' and other optional device specific information.
7692 
7693         Refer to the objects/pci_device.py for more idea of these keys.
7694 
7695         :returns: a JSON string containing a list of the assignable PCI
7696                   devices information
7697         """
7698         dev_flags = (
7699             libvirt.VIR_CONNECT_LIST_NODE_DEVICES_CAP_NET |
7700             libvirt.VIR_CONNECT_LIST_NODE_DEVICES_CAP_PCI_DEV
7701         )
7702         if self._host.has_min_version(
7703             lv_ver=MIN_LIBVIRT_VDPA, hv_ver=MIN_QEMU_VDPA,
7704         ):
7705             dev_flags |= libvirt.VIR_CONNECT_LIST_NODE_DEVICES_CAP_VDPA
7706 
7707         devices = {
7708             dev.name(): dev for dev in
7709             self._host.list_all_devices(flags=dev_flags)
7710         }
7711         net_devs = [dev for dev in devices.values() if "net" in dev.listCaps()]
7712         vdpa_devs = [
7713             dev for dev in devices.values() if "vdpa" in dev.listCaps()
7714         ]
7715         pci_info = [
7716             self._host._get_pcidev_info(name, dev, net_devs, vdpa_devs)
7717             for name, dev in devices.items() if "pci" in dev.listCaps()
7718         ]
7719         return jsonutils.dumps(pci_info)
7720 
7721     def _get_mdev_capabilities_for_dev(self, devname, types=None):
7722         """Returns a dict of MDEV capable device with the ID as first key
7723         and then a list of supported types, each of them being a dict.
7724 
7725         :param types: Only return those specific types.
7726         """
7727         virtdev = self._host.device_lookup_by_name(devname)
7728         xmlstr = virtdev.XMLDesc(0)
7729         cfgdev = vconfig.LibvirtConfigNodeDevice()
7730         cfgdev.parse_str(xmlstr)
7731 
7732         device = {
7733             "dev_id": cfgdev.name,
7734             "types": {},
7735             "vendor_id": cfgdev.pci_capability.vendor_id,
7736         }
7737         for mdev_cap in cfgdev.pci_capability.mdev_capability:
7738             for cap in mdev_cap.mdev_types:
7739                 if not types or cap['type'] in types:
7740                     device["types"].update({cap['type']: {
7741                         'availableInstances': cap['availableInstances'],
7742                         # This attribute is optional
7743                         'name': cap.get('name'),
7744                         'deviceAPI': cap['deviceAPI']}})
7745         return device
7746 
7747     def _get_mdev_capable_devices(self, types=None):
7748         """Get host devices supporting mdev types.
7749 
7750         Obtain devices information from libvirt and returns a list of
7751         dictionaries.
7752 
7753         :param types: Filter only devices supporting those types.
7754         """
7755         dev_names = self._host.list_mdev_capable_devices() or []
7756         mdev_capable_devices = []
7757         for name in dev_names:
7758             device = self._get_mdev_capabilities_for_dev(name, types)
7759             if not device["types"]:
7760                 continue
7761             mdev_capable_devices.append(device)
7762         return mdev_capable_devices
7763 
7764     def _get_mediated_device_information(self, devname):
7765         """Returns a dict of a mediated device."""
7766         virtdev = self._host.device_lookup_by_name(devname)
7767         xmlstr = virtdev.XMLDesc(0)
7768         cfgdev = vconfig.LibvirtConfigNodeDevice()
7769         cfgdev.parse_str(xmlstr)
7770 
7771         device = {
7772             "dev_id": cfgdev.name,
7773             # name is like mdev_00ead764_fdc0_46b6_8db9_2963f5c815b4
7774             "uuid": libvirt_utils.mdev_name2uuid(cfgdev.name),
7775             # the physical GPU PCI device
7776             "parent": cfgdev.parent,
7777             "type": cfgdev.mdev_information.type,
7778             "iommu_group": cfgdev.mdev_information.iommu_group,
7779         }
7780         return device
7781 
7782     def _get_mediated_devices(self, types=None):
7783         """Get host mediated devices.
7784 
7785         Obtain devices information from libvirt and returns a list of
7786         dictionaries.
7787 
7788         :param types: Filter only devices supporting those types.
7789         """
7790         dev_names = self._host.list_mediated_devices() or []
7791         mediated_devices = []
7792         for name in dev_names:
7793             device = self._get_mediated_device_information(name)
7794             if not types or device["type"] in types:
7795                 mediated_devices.append(device)
7796         return mediated_devices
7797 
7798     def _get_all_assigned_mediated_devices(self, instance=None):
7799         """Lookup all instances from the host and return all the mediated
7800         devices that are assigned to a guest.
7801 
7802         :param instance: Only return mediated devices for that instance.
7803 
7804         :returns: A dictionary of keys being mediated device UUIDs and their
7805                   respective values the instance UUID of the guest using it.
7806                   Returns an empty dict if an instance is provided but not
7807                   found in the hypervisor.
7808         """
7809         allocated_mdevs = {}
7810         if instance:
7811             # NOTE(sbauza): In some cases (like a migration issue), the
7812             # instance can exist in the Nova database but libvirt doesn't know
7813             # about it. For such cases, the way to fix that is to hard reboot
7814             # the instance, which will recreate the libvirt guest.
7815             # For that reason, we need to support that case by making sure
7816             # we don't raise an exception if the libvirt guest doesn't exist.
7817             try:
7818                 guest = self._host.get_guest(instance)
7819             except exception.InstanceNotFound:
7820                 # Bail out early if libvirt doesn't know about it since we
7821                 # can't know the existing mediated devices
7822                 return {}
7823             guests = [guest]
7824         else:
7825             guests = self._host.list_guests(only_running=False)
7826         for guest in guests:
7827             cfg = guest.get_config()
7828             for device in cfg.devices:
7829                 if isinstance(device, vconfig.LibvirtConfigGuestHostdevMDEV):
7830                     allocated_mdevs[device.uuid] = guest.uuid
7831         return allocated_mdevs
7832 
7833     # TODO(sbauza): Rename this method into _mdev_allocations
7834     def _vgpu_allocations(self, allocations):
7835         """Filtering only the mdev allocations from a list of allocations.
7836 
7837         :param allocations: Information about resources allocated to the
7838                             instance via placement, of the form returned by
7839                             SchedulerReportClient.get_allocations_for_consumer.
7840         """
7841         if not allocations:
7842             # If no allocations, there is no vGPU request.
7843             return {}
7844         mdev_rcs = self._get_supported_mdev_resource_classes()
7845         vgpu_allocations = {}
7846         for rp in allocations:
7847             res = allocations[rp]['resources']
7848             mdev_resources = {mdev_RC: res[mdev_RC] for mdev_RC in mdev_rcs
7849                               if mdev_RC in res and res[mdev_RC] > 0}
7850             if mdev_resources:
7851                 vgpu_allocations[rp] = {'resources': mdev_resources}
7852         return vgpu_allocations
7853 
7854     def _get_existing_mdevs_not_assigned(self, parent, requested_types=None):
7855         """Returns the already created mediated devices that are not assigned
7856         to a guest yet.
7857 
7858         :param parent: Filter out result for only mdevs from the parent device.
7859         :param requested_types: Filter out the result for only mediated devices
7860                                 having those types.
7861         """
7862         allocated_mdevs = self._get_all_assigned_mediated_devices()
7863         mdevs = self._get_mediated_devices(requested_types)
7864         available_mdevs = set()
7865         for mdev in mdevs:
7866             parent_vgpu_type = self._get_vgpu_type_per_pgpu(mdev['parent'])
7867             if mdev['type'] != parent_vgpu_type:
7868                 # This mdev is using a vGPU type that is not supported by the
7869                 # configuration option for its pGPU parent, so we can't use it.
7870                 continue
7871             # FIXME(sbauza): No longer accept the parent value to be nullable
7872             # once we fix the reshape functional test
7873             if parent is None or mdev['parent'] == parent:
7874                 available_mdevs.add(mdev["uuid"])
7875 
7876         available_mdevs -= set(allocated_mdevs)
7877         return available_mdevs
7878 
7879     def _create_new_mediated_device(self, parent, uuid=None):
7880         """Find a physical device that can support a new mediated device and
7881         create it.
7882 
7883         :param parent: The libvirt name of the parent GPU, eg. pci_0000_06_00_0
7884         :param uuid: The possible mdev UUID we want to create again
7885 
7886         :returns: the newly created mdev UUID or None if not possible
7887         """
7888         supported_types = self.supported_vgpu_types
7889         # Try to see if we can still create a new mediated device
7890         devices = self._get_mdev_capable_devices(supported_types)
7891         for device in devices:
7892             dev_name = device['dev_id']
7893             # FIXME(sbauza): No longer accept the parent value to be nullable
7894             # once we fix the reshape functional test
7895             if parent is not None and dev_name != parent:
7896                 # The device is not the one that was called, not creating
7897                 # the mdev
7898                 continue
7899             dev_supported_type = self._get_vgpu_type_per_pgpu(dev_name)
7900             if dev_supported_type and device['types'][
7901                     dev_supported_type]['availableInstances'] > 0:
7902                 # That physical GPU has enough room for a new mdev
7903                 # We need the PCI address, not the libvirt name
7904                 # The libvirt name is like 'pci_0000_84_00_0'
7905                 pci_addr = "{}:{}:{}.{}".format(*dev_name[4:].split('_'))
7906                 chosen_mdev = nova.privsep.libvirt.create_mdev(
7907                     pci_addr, dev_supported_type, uuid=uuid)
7908                 return chosen_mdev
7909 
7910     @utils.synchronized(VGPU_RESOURCE_SEMAPHORE)
7911     def _allocate_mdevs(self, allocations):
7912         """Returns a list of mediated device UUIDs corresponding to available
7913         resources we can assign to the guest(s) corresponding to the allocation
7914         requests passed as argument.
7915 
7916         That method can either find an existing but unassigned mediated device
7917         it can allocate, or create a new mediated device from a capable
7918         physical device if the latter has enough left capacity.
7919 
7920         :param allocations: Information about resources allocated to the
7921                             instance via placement, of the form returned by
7922                             SchedulerReportClient.get_allocations_for_consumer.
7923                             That code is supporting Placement API version 1.12
7924         """
7925         vgpu_allocations = self._vgpu_allocations(allocations)
7926         if not vgpu_allocations:
7927             return
7928         # TODO(sbauza): For the moment, we only support allocations for only
7929         # one pGPU.
7930         if len(vgpu_allocations) > 1:
7931             LOG.warning('More than one allocation was passed over to libvirt '
7932                         'while at the moment libvirt only supports one. Only '
7933                         'the first allocation will be looked up.')
7934         rp_uuid, alloc = next(iter(vgpu_allocations.items()))
7935         # We only have one allocation with a supported resource class
7936         vgpus_asked = list(alloc['resources'].values())[0]
7937 
7938         # Find if we allocated against a specific pGPU (and then the allocation
7939         # is made against a child RP) or any pGPU (in case the VGPU inventory
7940         # is still on the root RP)
7941         try:
7942             allocated_rp = self.provider_tree.data(rp_uuid)
7943         except ValueError:
7944             # The provider doesn't exist, return a better understandable
7945             # exception
7946             raise exception.ComputeResourcesUnavailable(
7947                 reason='mdev-capable resource is not available')
7948         # FIXME(sbauza): The functional reshape test assumes that we could
7949         # run _allocate_mdevs() against non-nested RPs but this is impossible
7950         # as all inventories have been reshaped *before now* since it's done
7951         # on init_host() (when the compute restarts or whatever else calls it).
7952         # That said, since fixing the functional test isn't easy yet, let's
7953         # assume we still support a non-nested RP for now.
7954         if allocated_rp.parent_uuid is None:
7955             # We are on a root RP
7956             parent_device = None
7957         else:
7958             rp_name = allocated_rp.name
7959             # There can be multiple roots, we need to find the root name
7960             # to guess the physical device name
7961             roots = list(self.provider_tree.roots)
7962             for root in roots:
7963                 if rp_name.startswith(root.name + '_'):
7964                     # The RP name convention is :
7965                     #    root_name + '_' + parent_device
7966                     parent_device = rp_name[len(root.name) + 1:]
7967                     break
7968             else:
7969                 LOG.warning(
7970                     "mdev-capable device name %(name)s can't be guessed from "
7971                     "the ProviderTree roots %(roots)s",
7972                     {'name': rp_name,
7973                      'roots': ', '.join([root.name for root in roots])})
7974                 # We f... have no idea what was the parent device
7975                 # If we can't find devices having available VGPUs, just raise
7976                 raise exception.ComputeResourcesUnavailable(
7977                     reason='mdev-capable resource is not available')
7978 
7979         supported_types = self.supported_vgpu_types
7980         # Which mediated devices are created but not assigned to a guest ?
7981         mdevs_available = self._get_existing_mdevs_not_assigned(
7982             parent_device, supported_types)
7983 
7984         chosen_mdevs = []
7985         for c in range(vgpus_asked):
7986             chosen_mdev = None
7987             if mdevs_available:
7988                 # Take the first available mdev
7989                 chosen_mdev = mdevs_available.pop()
7990             else:
7991                 chosen_mdev = self._create_new_mediated_device(parent_device)
7992             if not chosen_mdev:
7993                 # If we can't find devices having available VGPUs, just raise
7994                 raise exception.ComputeResourcesUnavailable(
7995                     reason='mdev-capable resource is not available')
7996             else:
7997                 chosen_mdevs.append(chosen_mdev)
7998         return chosen_mdevs
7999 
8000     def _detach_mediated_devices(self, guest):
8001         mdevs = guest.get_all_devices(
8002             devtype=vconfig.LibvirtConfigGuestHostdevMDEV)
8003         for mdev_cfg in mdevs:
8004             try:
8005                 guest.detach_device(mdev_cfg, live=True)
8006             except libvirt.libvirtError as ex:
8007                 error_code = ex.get_error_code()
8008                 # NOTE(sbauza): There is a pending issue with libvirt that
8009                 # doesn't allow to hot-unplug mediated devices. Let's
8010                 # short-circuit the suspend action and set the instance back
8011                 # to ACTIVE.
8012                 # TODO(sbauza): Once libvirt supports this, amend the resume()
8013                 # operation to support reallocating mediated devices.
8014                 if error_code == libvirt.VIR_ERR_CONFIG_UNSUPPORTED:
8015                     reason = _("Suspend is not supported for instances having "
8016                                "attached mediated devices.")
8017                     raise exception.InstanceFaultRollback(
8018                         exception.InstanceSuspendFailure(reason=reason))
8019                 else:
8020                     raise
8021 
8022     def _attach_mediated_devices(self, guest, devs):
8023         for mdev_cfg in devs:
8024             try:
8025                 guest.attach_device(mdev_cfg, live=True)
8026             except libvirt.libvirtError as ex:
8027                 error_code = ex.get_error_code()
8028                 if error_code == libvirt.VIR_ERR_DEVICE_MISSING:
8029                     LOG.warning("The mediated device %s was not found and "
8030                                 "won't be reattached to %s.", mdev_cfg, guest)
8031                 else:
8032                     raise
8033 
8034     def _recover_attached_mdevs_from_guest_config(self, xml):
8035         config = vconfig.LibvirtConfigGuest()
8036         config.parse_str(xml)
8037 
8038         devs = []
8039         for dev in config.devices:
8040             if isinstance(dev, vconfig.LibvirtConfigGuestHostdevMDEV):
8041                 devs.append(dev)
8042         return devs
8043 
8044     def _has_numa_support(self):
8045         # This means that the host can support LibvirtConfigGuestNUMATune
8046         # and the nodeset field in LibvirtConfigGuestMemoryBackingPage
8047         caps = self._host.get_capabilities()
8048 
8049         if (caps.host.cpu.arch in (fields.Architecture.I686,
8050                                    fields.Architecture.X86_64,
8051                                    fields.Architecture.AARCH64) and
8052                 self._host.has_min_version(hv_type=host.HV_DRIVER_QEMU)):
8053             return True
8054         elif (caps.host.cpu.arch in (fields.Architecture.PPC64,
8055                                      fields.Architecture.PPC64LE)):
8056             return True
8057 
8058         return False
8059 
8060     def _get_host_numa_topology(self):
8061         if not self._has_numa_support():
8062             return
8063 
8064         caps = self._host.get_capabilities()
8065         topology = caps.host.topology
8066 
8067         if topology is None or not topology.cells:
8068             return
8069 
8070         cells = []
8071 
8072         available_shared_cpus = self._get_vcpu_available()
8073         available_dedicated_cpus = self._get_pcpu_available()
8074 
8075         # NOTE(stephenfin): In an ideal world, if the operator had not
8076         # configured this host to report PCPUs using the '[compute]
8077         # cpu_dedicated_set' option, then we should not be able to used pinned
8078         # instances on this host. However, that would force operators to update
8079         # their configuration as part of the Stein -> Train upgrade or be
8080         # unable to schedule instances on the host. As a result, we need to
8081         # revert to legacy behavior and use 'vcpu_pin_set' for both VCPUs and
8082         # PCPUs.
8083         # TODO(stephenfin): Remove this in U
8084         if not available_dedicated_cpus and not (
8085                 CONF.compute.cpu_shared_set and not CONF.vcpu_pin_set):
8086             available_dedicated_cpus = available_shared_cpus
8087 
8088         def _get_reserved_memory_for_cell(self, cell_id, page_size):
8089             cell = self._reserved_hugepages.get(cell_id, {})
8090             return cell.get(page_size, 0)
8091 
8092         def _get_physnet_numa_affinity():
8093             affinities: ty.Dict[int, ty.Set[str]] = {
8094                 cell.id: set() for cell in topology.cells
8095             }
8096             for physnet in CONF.neutron.physnets:
8097                 # This will error out if the group is not registered, which is
8098                 # exactly what we want as that would be a bug
8099                 group = getattr(CONF, 'neutron_physnet_%s' % physnet)
8100 
8101                 if not group.numa_nodes:
8102                     msg = ("the physnet '%s' was listed in '[neutron] "
8103                            "physnets' but no corresponding "
8104                            "'[neutron_physnet_%s] numa_nodes' option was "
8105                            "defined." % (physnet, physnet))
8106                     raise exception.InvalidNetworkNUMAAffinity(reason=msg)
8107 
8108                 for node in group.numa_nodes:
8109                     if node not in affinities:
8110                         msg = ("node %d for physnet %s is not present in host "
8111                                "affinity set %r" % (node, physnet, affinities))
8112                         # The config option referenced an invalid node
8113                         raise exception.InvalidNetworkNUMAAffinity(reason=msg)
8114                     affinities[node].add(physnet)
8115 
8116             return affinities
8117 
8118         def _get_tunnel_numa_affinity():
8119             affinities = {cell.id: False for cell in topology.cells}
8120 
8121             for node in CONF.neutron_tunnel.numa_nodes:
8122                 if node not in affinities:
8123                     msg = ("node %d for tunneled networks is not present "
8124                            "in host affinity set %r" % (node, affinities))
8125                     # The config option referenced an invalid node
8126                     raise exception.InvalidNetworkNUMAAffinity(reason=msg)
8127                 affinities[node] = True
8128 
8129             return affinities
8130 
8131         physnet_affinities = _get_physnet_numa_affinity()
8132         tunnel_affinities = _get_tunnel_numa_affinity()
8133 
8134         for cell in topology.cells:
8135             cpus = set(cpu.id for cpu in cell.cpus)
8136 
8137             # NOTE(artom) We assume we'll never see hardware with multipe
8138             # sockets in a single NUMA node - IOW, the socket_id for all CPUs
8139             # in a single cell will be the same. To make that assumption
8140             # explicit, we leave the cell's socket_id as None if that's the
8141             # case.
8142             socket_id = None
8143             sockets = set([cpu.socket_id for cpu in cell.cpus])
8144             if len(sockets) == 1:
8145                 socket_id = sockets.pop()
8146             else:
8147                 LOG.warning('This host appears to have multiple sockets per '
8148                             'NUMA node. The `socket` PCI NUMA affinity '
8149                             'will not be supported.')
8150 
8151             cpuset = cpus & available_shared_cpus
8152             pcpuset = cpus & available_dedicated_cpus
8153 
8154             # de-duplicate and sort the list of CPU sibling sets
8155             siblings = sorted(
8156                 set(x) for x in set(
8157                     tuple(cpu.siblings) or () for cpu in cell.cpus
8158                 )
8159             )
8160 
8161             cpus &= available_shared_cpus | available_dedicated_cpus
8162             siblings = [sib & cpus for sib in siblings]
8163             # Filter out empty sibling sets that may be left
8164             siblings = [sib for sib in siblings if len(sib) > 0]
8165 
8166             mempages = [
8167                 objects.NUMAPagesTopology(
8168                     size_kb=pages.size,
8169                     total=pages.total,
8170                     used=0,
8171                     reserved=_get_reserved_memory_for_cell(
8172                         self, cell.id, pages.size))
8173                 for pages in cell.mempages]
8174 
8175             network_metadata = objects.NetworkMetadata(
8176                 physnets=physnet_affinities[cell.id],
8177                 tunneled=tunnel_affinities[cell.id])
8178 
8179             # NOTE(stephenfin): Note that we don't actually return any usage
8180             # information here. This is because this is handled by the resource
8181             # tracker via the 'update_available_resource' periodic task, which
8182             # loops through all instances and calculated usage accordingly
8183             cell = objects.NUMACell(
8184                 id=cell.id,
8185                 socket=socket_id,
8186                 cpuset=cpuset,
8187                 pcpuset=pcpuset,
8188                 memory=cell.memory / units.Ki,
8189                 cpu_usage=0,
8190                 pinned_cpus=set(),
8191                 memory_usage=0,
8192                 siblings=siblings,
8193                 mempages=mempages,
8194                 network_metadata=network_metadata)
8195             cells.append(cell)
8196 
8197         return objects.NUMATopology(cells=cells)
8198 
8199     def get_all_volume_usage(self, context, compute_host_bdms):
8200         """Return usage info for volumes attached to vms on
8201            a given host.
8202         """
8203         vol_usage = []
8204 
8205         for instance_bdms in compute_host_bdms:
8206             instance = instance_bdms['instance']
8207 
8208             for bdm in instance_bdms['instance_bdms']:
8209                 mountpoint = bdm['device_name']
8210                 if mountpoint.startswith('/dev/'):
8211                     mountpoint = mountpoint[5:]
8212                 volume_id = bdm['volume_id']
8213 
8214                 LOG.debug("Trying to get stats for the volume %s",
8215                           volume_id, instance=instance)
8216                 vol_stats = self.block_stats(instance, mountpoint)
8217 
8218                 if vol_stats:
8219                     stats = dict(volume=volume_id,
8220                                  instance=instance,
8221                                  rd_req=vol_stats[0],
8222                                  rd_bytes=vol_stats[1],
8223                                  wr_req=vol_stats[2],
8224                                  wr_bytes=vol_stats[3])
8225                     LOG.debug(
8226                         "Got volume usage stats for the volume=%(volume)s,"
8227                         " rd_req=%(rd_req)d, rd_bytes=%(rd_bytes)d, "
8228                         "wr_req=%(wr_req)d, wr_bytes=%(wr_bytes)d",
8229                         stats, instance=instance)
8230                     vol_usage.append(stats)
8231 
8232         return vol_usage
8233 
8234     def block_stats(self, instance, disk_id):
8235         """Note that this function takes an instance name."""
8236         try:
8237             guest = self._host.get_guest(instance)
8238             dev = guest.get_block_device(disk_id)
8239             return dev.blockStats()
8240         except libvirt.libvirtError as e:
8241             errcode = e.get_error_code()
8242             LOG.info('Getting block stats failed, device might have '
8243                      'been detached. Instance=%(instance_name)s '
8244                      'Disk=%(disk)s Code=%(errcode)s Error=%(e)s',
8245                      {'instance_name': instance.name, 'disk': disk_id,
8246                       'errcode': errcode, 'e': e},
8247                      instance=instance)
8248         except exception.InstanceNotFound:
8249             LOG.info('Could not find domain in libvirt for instance %s. '
8250                      'Cannot get block stats for device', instance.name,
8251                      instance=instance)
8252 
8253     def update_provider_tree(self, provider_tree, nodename, allocations=None):
8254         """Update a ProviderTree object with current resource provider,
8255         inventory information and CPU traits.
8256 
8257         :param nova.compute.provider_tree.ProviderTree provider_tree:
8258             A nova.compute.provider_tree.ProviderTree object representing all
8259             the providers in the tree associated with the compute node, and any
8260             sharing providers (those with the ``MISC_SHARES_VIA_AGGREGATE``
8261             trait) associated via aggregate with any of those providers (but
8262             not *their* tree- or aggregate-associated providers), as currently
8263             known by placement.
8264         :param nodename:
8265             String name of the compute node (i.e.
8266             ComputeNode.hypervisor_hostname) for which the caller is requesting
8267             updated provider information.
8268         :param allocations:
8269             Dict of allocation data of the form:
8270               { $CONSUMER_UUID: {
8271                     # The shape of each "allocations" dict below is identical
8272                     # to the return from GET /allocations/{consumer_uuid}
8273                     "allocations": {
8274                         $RP_UUID: {
8275                             "generation": $RP_GEN,
8276                             "resources": {
8277                                 $RESOURCE_CLASS: $AMOUNT,
8278                                 ...
8279                             },
8280                         },
8281                         ...
8282                     },
8283                     "project_id": $PROJ_ID,
8284                     "user_id": $USER_ID,
8285                     "consumer_generation": $CONSUMER_GEN,
8286                 },
8287                 ...
8288               }
8289             If None, and the method determines that any inventory needs to be
8290             moved (from one provider to another and/or to a different resource
8291             class), the ReshapeNeeded exception must be raised. Otherwise, this
8292             dict must be edited in place to indicate the desired final state of
8293             allocations.
8294         :raises ReshapeNeeded: If allocations is None and any inventory needs
8295             to be moved from one provider to another and/or to a different
8296             resource class.
8297         :raises: ReshapeFailed if the requested tree reshape fails for
8298             whatever reason.
8299         """
8300         disk_gb = int(self._get_local_gb_info()['total'])
8301         memory_mb = int(self._host.get_memory_mb_total())
8302         vcpus = len(self._get_vcpu_available())
8303         pcpus = len(self._get_pcpu_available())
8304         memory_enc_slots = self._get_memory_encrypted_slots()
8305 
8306         # NOTE(yikun): If the inv record does not exists, the allocation_ratio
8307         # will use the CONF.xxx_allocation_ratio value if xxx_allocation_ratio
8308         # is set, and fallback to use the initial_xxx_allocation_ratio
8309         # otherwise.
8310         inv = provider_tree.data(nodename).inventory
8311         ratios = self._get_allocation_ratios(inv)
8312         resources: ty.Dict[str, ty.Set['objects.Resource']] = (
8313             collections.defaultdict(set)
8314         )
8315 
8316         result = {}
8317         if memory_mb:
8318             result[orc.MEMORY_MB] = {
8319                 'total': memory_mb,
8320                 'min_unit': 1,
8321                 'max_unit': memory_mb,
8322                 'step_size': 1,
8323                 'allocation_ratio': ratios[orc.MEMORY_MB],
8324                 'reserved': CONF.reserved_host_memory_mb,
8325             }
8326 
8327         # NOTE(stephenfin): We have to optionally report these since placement
8328         # forbids reporting inventory with total=0
8329         if vcpus:
8330             result[orc.VCPU] = {
8331                 'total': vcpus,
8332                 'min_unit': 1,
8333                 'max_unit': vcpus,
8334                 'step_size': 1,
8335                 'allocation_ratio': ratios[orc.VCPU],
8336                 'reserved': CONF.reserved_host_cpus,
8337             }
8338 
8339         if pcpus:
8340             result[orc.PCPU] = {
8341                 'total': pcpus,
8342                 'min_unit': 1,
8343                 'max_unit': pcpus,
8344                 'step_size': 1,
8345                 'allocation_ratio': 1,
8346                 'reserved': 0,
8347             }
8348 
8349         if memory_enc_slots:
8350             result[orc.MEM_ENCRYPTION_CONTEXT] = {
8351                 'total': memory_enc_slots,
8352                 'min_unit': 1,
8353                 'max_unit': 1,
8354                 'step_size': 1,
8355                 'allocation_ratio': 1.0,
8356                 'reserved': 0,
8357             }
8358 
8359         # If a sharing DISK_GB provider exists in the provider tree, then our
8360         # storage is shared, and we should not report the DISK_GB inventory in
8361         # the compute node provider.
8362         # TODO(efried): Reinstate non-reporting of shared resource by the
8363         # compute RP once the issues from bug #1784020 have been resolved.
8364         if provider_tree.has_sharing_provider(orc.DISK_GB):
8365             LOG.debug('Ignoring sharing provider - see bug #1784020')
8366 
8367         if disk_gb:
8368             result[orc.DISK_GB] = {
8369                 'total': disk_gb,
8370                 'min_unit': 1,
8371                 'max_unit': disk_gb,
8372                 'step_size': 1,
8373                 'allocation_ratio': ratios[orc.DISK_GB],
8374                 'reserved': (self._get_reserved_host_disk_gb_from_config() +
8375                              self._get_disk_size_reserved_for_image_cache()),
8376             }
8377 
8378         # TODO(sbauza): Use traits to providing vGPU types. For the moment,
8379         # it will be only documentation support by explaining to use
8380         # osc-placement to create custom traits for each of the pGPU RPs.
8381         self._update_provider_tree_for_vgpu(
8382            provider_tree, nodename, allocations=allocations)
8383 
8384         self._update_provider_tree_for_pcpu(
8385             provider_tree, nodename, allocations=allocations)
8386 
8387         self._update_provider_tree_for_vpmems(
8388             provider_tree, nodename, result, resources)
8389 
8390         provider_tree.update_inventory(nodename, result)
8391         provider_tree.update_resources(nodename, resources)
8392 
8393         # Add supported traits i.e. those equal to True to provider tree while
8394         # removing the unsupported ones
8395         traits_to_add = [
8396             t for t in self.static_traits if self.static_traits[t]
8397         ]
8398         traits_to_remove = set(self.static_traits) - set(traits_to_add)
8399         provider_tree.add_traits(nodename, *traits_to_add)
8400         provider_tree.remove_traits(nodename, *traits_to_remove)
8401 
8402         # Now that we updated the ProviderTree, we want to store it locally
8403         # so that spawn() or other methods can access it thru a getter
8404         self.provider_tree = copy.deepcopy(provider_tree)
8405 
8406     def _update_provider_tree_for_vpmems(self, provider_tree, nodename,
8407                                          inventory, resources):
8408         """Update resources and inventory for vpmems in provider tree."""
8409         prov_data = provider_tree.data(nodename)
8410         for rc, vpmems in self._vpmems_by_rc.items():
8411             # Skip (and omit) inventories with total=0 because placement does
8412             # not allow setting total=0 for inventory.
8413             if not len(vpmems):
8414                 continue
8415             inventory[rc] = {
8416                 'total': len(vpmems),
8417                 'max_unit': len(vpmems),
8418                 'min_unit': 1,
8419                 'step_size': 1,
8420                 'allocation_ratio': 1.0,
8421                 'reserved': 0
8422             }
8423             for vpmem in vpmems:
8424                 resource_obj = objects.Resource(
8425                     provider_uuid=prov_data.uuid,
8426                     resource_class=rc,
8427                     identifier=vpmem.name,
8428                     metadata=vpmem)
8429                 resources[rc].add(resource_obj)
8430 
8431     def _get_memory_encrypted_slots(self):
8432         slots = CONF.libvirt.num_memory_encrypted_guests
8433         if not self._host.supports_amd_sev:
8434             if slots and slots > 0:
8435                 LOG.warning("Host is configured with "
8436                             "libvirt.num_memory_encrypted_guests set to "
8437                             "%d, but is not SEV-capable.", slots)
8438             return 0
8439 
8440         # NOTE(aspiers): Auto-detection of the number of available
8441         # slots for AMD SEV is not yet possible, so honor the
8442         # configured value, or impose no limit if this is not
8443         # specified.  This does incur a risk that if operators don't
8444         # read the instructions and configure the maximum correctly,
8445         # the maximum could be exceeded resulting in SEV guests
8446         # failing at launch-time.  However at least SEV guests will
8447         # launch until the maximum, and when auto-detection code is
8448         # added later, an upgrade will magically fix the issue.
8449         #
8450         # Note also that the configured value can be 0 on an
8451         # SEV-capable host, since there might conceivably be good
8452         # reasons for the operator to want to disable SEV even when
8453         # it's available (e.g. due to performance impact, or
8454         # implementation bugs which may surface later).
8455         if slots is not None:
8456             return slots
8457         else:
8458             return db_const.MAX_INT
8459 
8460     @property
8461     def static_traits(self) -> ty.Dict[str, bool]:
8462         if self._static_traits is not None:
8463             return self._static_traits
8464 
8465         traits: ty.Dict[str, bool] = {}
8466         traits.update(self._get_cpu_traits())
8467         traits.update(self._get_storage_bus_traits())
8468         traits.update(self._get_video_model_traits())
8469         traits.update(self._get_vif_model_traits())
8470         traits.update(self._get_tpm_traits())
8471 
8472         _, invalid_traits = ot.check_traits(traits)
8473         for invalid_trait in invalid_traits:
8474             LOG.debug("Trait '%s' is not valid; ignoring.", invalid_trait)
8475             del traits[invalid_trait]
8476 
8477         self._static_traits = traits
8478 
8479         return self._static_traits
8480 
8481     @staticmethod
8482     def _is_reshape_needed_vgpu_on_root(provider_tree, nodename):
8483         """Determine if root RP has VGPU inventories.
8484 
8485         Check to see if the root compute node provider in the tree for
8486         this host already has VGPU inventory because if it does, we either
8487         need to signal for a reshape (if _update_provider_tree_for_vgpu()
8488         has no allocations) or move the allocations within the ProviderTree if
8489         passed.
8490 
8491         :param provider_tree: The ProviderTree object for this host.
8492         :param nodename: The ComputeNode.hypervisor_hostname, also known as
8493             the name of the root node provider in the tree for this host.
8494         :returns: boolean, whether we have VGPU root inventory.
8495         """
8496         root_node = provider_tree.data(nodename)
8497         return orc.VGPU in root_node.inventory
8498 
8499     def _ensure_pgpu_providers(self, inventories_dict, provider_tree,
8500                                nodename):
8501         """Ensures GPU inventory providers exist in the tree for $nodename.
8502 
8503         GPU providers are named $nodename_$gpu-device-id, e.g.
8504         ``somehost.foo.bar.com_pci_0000_84_00_0``.
8505 
8506         :param inventories_dict: Dictionary of inventories for VGPU class
8507             directly provided by _get_gpu_inventories() and which looks like:
8508                 {'pci_0000_84_00_0':
8509                     {'total': $TOTAL,
8510                      'min_unit': 1,
8511                      'max_unit': $MAX_UNIT, # defaults to $TOTAL
8512                      'step_size': 1,
8513                      'reserved': 0,
8514                      'allocation_ratio': 1.0,
8515                     }
8516                 }
8517         :param provider_tree: The ProviderTree to update.
8518         :param nodename: The ComputeNode.hypervisor_hostname, also known as
8519             the name of the root node provider in the tree for this host.
8520         :returns: dict, keyed by GPU device ID, to ProviderData object
8521             representing that resource provider in the tree
8522         """
8523         # Create the VGPU child providers if they do not already exist.
8524         # Dict of PGPU RPs keyed by their libvirt PCI name
8525         pgpu_rps = {}
8526         for pgpu_dev_id, inventory in inventories_dict.items():
8527             # Skip (and omit) inventories with total=0 because placement does
8528             # not allow setting total=0 for inventory.
8529             if not inventory['total']:
8530                 continue
8531             # For each physical GPU, we make sure to have a child provider
8532             pgpu_rp_name = '%s_%s' % (nodename, pgpu_dev_id)
8533             if not provider_tree.exists(pgpu_rp_name):
8534                 # This is the first time creating the child provider so add
8535                 # it to the tree under the root node provider.
8536                 provider_tree.new_child(pgpu_rp_name, nodename)
8537             # We want to idempotently return the resource providers with VGPUs
8538             pgpu_rp = provider_tree.data(pgpu_rp_name)
8539             pgpu_rps[pgpu_dev_id] = pgpu_rp
8540 
8541             # The VGPU inventory goes on a child provider of the given root
8542             # node, identified by $nodename.
8543             mdev_rc = self._get_resource_class_for_device(pgpu_dev_id)
8544             pgpu_inventory = {mdev_rc: inventory}
8545             provider_tree.update_inventory(pgpu_rp_name, pgpu_inventory)
8546         return pgpu_rps
8547 
8548     @staticmethod
8549     def _assert_is_root_provider(
8550             rp_uuid, root_node, consumer_uuid, alloc_data):
8551         """Asserts during a reshape that rp_uuid is for the root node provider.
8552 
8553         When reshaping, inventory and allocations should be on the root node
8554         provider and then moved to child providers.
8555 
8556         :param rp_uuid: UUID of the provider that holds inventory/allocations.
8557         :param root_node: ProviderData object representing the root node in a
8558             provider tree.
8559         :param consumer_uuid: UUID of the consumer (instance) holding resource
8560             allocations against the given rp_uuid provider.
8561         :param alloc_data: dict of allocation data for the consumer.
8562         :raises: ReshapeFailed if rp_uuid is not the root node indicating a
8563             reshape was needed but the inventory/allocation structure is not
8564             expected.
8565         """
8566         if rp_uuid != root_node.uuid:
8567             # Something is wrong - VGPU inventory should
8568             # only be on the root node provider if we are
8569             # reshaping the tree.
8570             msg = (_('Unexpected VGPU resource allocation '
8571                      'on provider %(rp_uuid)s for consumer '
8572                      '%(consumer_uuid)s: %(alloc_data)s. '
8573                      'Expected VGPU allocation to be on root '
8574                      'compute node provider %(root_uuid)s.')
8575                    % {'rp_uuid': rp_uuid,
8576                       'consumer_uuid': consumer_uuid,
8577                       'alloc_data': alloc_data,
8578                       'root_uuid': root_node.uuid})
8579             raise exception.ReshapeFailed(error=msg)
8580 
8581     def _get_assigned_mdevs_for_reshape(
8582             self, instance_uuid, rp_uuid, alloc_data):
8583         """Gets the mediated devices assigned to the instance during a reshape.
8584 
8585         :param instance_uuid: UUID of the instance consuming VGPU resources
8586             on this host.
8587         :param rp_uuid: UUID of the resource provider with VGPU inventory being
8588             consumed by the instance.
8589         :param alloc_data: dict of allocation data for the instance consumer.
8590         :return: list of mediated device UUIDs assigned to the instance
8591         :raises: ReshapeFailed if the instance is not found in the hypervisor
8592             or no mediated devices were found to be assigned to the instance
8593             indicating VGPU allocations are out of sync with the hypervisor
8594         """
8595         # FIXME(sbauza): We don't really need an Instance
8596         # object, but given some libvirt.host logs needs
8597         # to have an instance name, just provide a fake one
8598         Instance = collections.namedtuple('Instance', ['uuid', 'name'])
8599         instance = Instance(uuid=instance_uuid, name=instance_uuid)
8600         mdevs = self._get_all_assigned_mediated_devices(instance)
8601         # _get_all_assigned_mediated_devices returns {} if the instance is
8602         # not found in the hypervisor
8603         if not mdevs:
8604             # If we found a VGPU allocation against a consumer
8605             # which is not an instance, the only left case for
8606             # Nova would be a migration but we don't support
8607             # this at the moment.
8608             msg = (_('Unexpected VGPU resource allocation on provider '
8609                      '%(rp_uuid)s for consumer %(consumer_uuid)s: '
8610                      '%(alloc_data)s. The allocation is made against a '
8611                      'non-existing instance or there are no devices assigned.')
8612                    % {'rp_uuid': rp_uuid, 'consumer_uuid': instance_uuid,
8613                       'alloc_data': alloc_data})
8614             raise exception.ReshapeFailed(error=msg)
8615         return mdevs
8616 
8617     def _count_vgpus_per_pgpu(self, mdev_uuids):
8618         """Count the number of VGPUs per physical GPU mediated device.
8619 
8620         :param mdev_uuids: List of physical GPU mediated device UUIDs.
8621         :return: dict, keyed by PGPU device ID, to count of VGPUs on that
8622             device
8623         """
8624         vgpu_count_per_pgpu: ty.Dict[str, int] = collections.defaultdict(int)
8625         for mdev_uuid in mdev_uuids:
8626             # libvirt name is like mdev_00ead764_fdc0_46b6_8db9_2963f5c815b4
8627             dev_name = libvirt_utils.mdev_uuid2name(mdev_uuid)
8628             # Count how many vGPUs are in use for this instance
8629             dev_info = self._get_mediated_device_information(dev_name)
8630             pgpu_dev_id = dev_info['parent']
8631             vgpu_count_per_pgpu[pgpu_dev_id] += 1
8632         return vgpu_count_per_pgpu
8633 
8634     @staticmethod
8635     def _check_vgpu_allocations_match_real_use(
8636             vgpu_count_per_pgpu, expected_usage, rp_uuid, consumer_uuid,
8637             alloc_data):
8638         """Checks that the number of GPU devices assigned to the consumer
8639         matches what is expected from the allocations in the placement service
8640         and logs a warning if there is a mismatch.
8641 
8642         :param vgpu_count_per_pgpu: dict, keyed by PGPU device ID, to count of
8643             VGPUs on that device where each device is assigned to the consumer
8644             (guest instance on this hypervisor)
8645         :param expected_usage: The expected usage from placement for the
8646             given resource provider and consumer
8647         :param rp_uuid: UUID of the resource provider with VGPU inventory being
8648             consumed by the instance
8649         :param consumer_uuid: UUID of the consumer (instance) holding resource
8650             allocations against the given rp_uuid provider
8651         :param alloc_data: dict of allocation data for the instance consumer
8652         """
8653         actual_usage = sum(vgpu_count_per_pgpu.values())
8654         if actual_usage != expected_usage:
8655             # Don't make it blocking, just make sure you actually correctly
8656             # allocate the existing resources
8657             LOG.warning(
8658                 'Unexpected VGPU resource allocation on provider %(rp_uuid)s '
8659                 'for consumer %(consumer_uuid)s: %(alloc_data)s. Allocations '
8660                 '(%(expected_usage)s) differ from actual use '
8661                 '(%(actual_usage)s).',
8662                 {'rp_uuid': rp_uuid, 'consumer_uuid': consumer_uuid,
8663                  'alloc_data': alloc_data, 'expected_usage': expected_usage,
8664                  'actual_usage': actual_usage})
8665 
8666     def _reshape_vgpu_allocations(
8667             self, rp_uuid, root_node, consumer_uuid, alloc_data, resources,
8668             pgpu_rps):
8669         """Update existing VGPU allocations by moving them from the root node
8670         provider to the child provider for the given VGPU provider.
8671 
8672         :param rp_uuid: UUID of the VGPU resource provider with allocations
8673             from consumer_uuid (should be the root node provider before
8674             reshaping occurs)
8675         :param root_node: ProviderData object for the root compute node
8676             resource provider in the provider tree
8677         :param consumer_uuid: UUID of the consumer (instance) with VGPU
8678             allocations against the resource provider represented by rp_uuid
8679         :param alloc_data: dict of allocation information for consumer_uuid
8680         :param resources: dict, keyed by resource class, of resources allocated
8681             to consumer_uuid from rp_uuid
8682         :param pgpu_rps: dict, keyed by GPU device ID, to ProviderData object
8683             representing that resource provider in the tree
8684         :raises: ReshapeFailed if the reshape fails for whatever reason
8685         """
8686         # We've found VGPU allocations on a provider. It should be the root
8687         # node provider.
8688         self._assert_is_root_provider(
8689             rp_uuid, root_node, consumer_uuid, alloc_data)
8690 
8691         # Find which physical GPU corresponds to this allocation.
8692         mdev_uuids = self._get_assigned_mdevs_for_reshape(
8693             consumer_uuid, rp_uuid, alloc_data)
8694 
8695         vgpu_count_per_pgpu = self._count_vgpus_per_pgpu(mdev_uuids)
8696 
8697         # We need to make sure we found all the mediated devices that
8698         # correspond to an allocation.
8699         self._check_vgpu_allocations_match_real_use(
8700             vgpu_count_per_pgpu, resources[orc.VGPU],
8701             rp_uuid, consumer_uuid, alloc_data)
8702 
8703         # Add the VGPU allocation for each VGPU provider.
8704         allocs = alloc_data['allocations']
8705         for pgpu_dev_id, pgpu_rp in pgpu_rps.items():
8706             vgpu_count = vgpu_count_per_pgpu[pgpu_dev_id]
8707             if vgpu_count:
8708                 allocs[pgpu_rp.uuid] = {
8709                     'resources': {
8710                         orc.VGPU: vgpu_count
8711                     }
8712                 }
8713         # And remove the VGPU allocation from the root node provider.
8714         del resources[orc.VGPU]
8715 
8716     def _reshape_gpu_resources(
8717             self, allocations, root_node, pgpu_rps):
8718         """Reshapes the provider tree moving VGPU inventory from root to child
8719 
8720         :param allocations:
8721             Dict of allocation data of the form:
8722               { $CONSUMER_UUID: {
8723                     # The shape of each "allocations" dict below is identical
8724                     # to the return from GET /allocations/{consumer_uuid}
8725                     "allocations": {
8726                         $RP_UUID: {
8727                             "generation": $RP_GEN,
8728                             "resources": {
8729                                 $RESOURCE_CLASS: $AMOUNT,
8730                                 ...
8731                             },
8732                         },
8733                         ...
8734                     },
8735                     "project_id": $PROJ_ID,
8736                     "user_id": $USER_ID,
8737                     "consumer_generation": $CONSUMER_GEN,
8738                 },
8739                 ...
8740               }
8741         :params root_node: The root node in the provider tree
8742         :params pgpu_rps: dict, keyed by GPU device ID, to ProviderData object
8743             representing that resource provider in the tree
8744         """
8745         LOG.info('Reshaping tree; moving VGPU allocations from root '
8746                  'provider %s to child providers %s.', root_node.uuid,
8747                  pgpu_rps.values())
8748         # For each consumer in the allocations dict, look for VGPU
8749         # allocations and move them to the VGPU provider.
8750         for consumer_uuid, alloc_data in allocations.items():
8751             # Copy and iterate over the current set of providers to avoid
8752             # modifying keys while iterating.
8753             allocs = alloc_data['allocations']
8754             for rp_uuid in list(allocs):
8755                 resources = allocs[rp_uuid]['resources']
8756                 if orc.VGPU in resources:
8757                     self._reshape_vgpu_allocations(
8758                         rp_uuid, root_node, consumer_uuid, alloc_data,
8759                         resources, pgpu_rps)
8760 
8761     def _update_provider_tree_for_vgpu(self, provider_tree, nodename,
8762                                        allocations=None):
8763         """Updates the provider tree for VGPU inventory.
8764 
8765         Before Stein, VGPU inventory and allocations were on the root compute
8766         node provider in the tree. Starting in Stein, the VGPU inventory is
8767         on a child provider in the tree. As a result, this method will
8768         "reshape" the tree if necessary on first start of this compute service
8769         in Stein.
8770 
8771         :param provider_tree: The ProviderTree to update.
8772         :param nodename: The ComputeNode.hypervisor_hostname, also known as
8773             the name of the root node provider in the tree for this host.
8774         :param allocations: If not None, indicates a reshape was requested and
8775             should be performed.
8776         :raises: nova.exception.ReshapeNeeded if ``allocations`` is None and
8777             the method determines a reshape of the tree is needed, i.e. VGPU
8778             inventory and allocations must be migrated from the root node
8779             provider to a child provider of VGPU resources in the tree.
8780         :raises: nova.exception.ReshapeFailed if the requested tree reshape
8781             fails for whatever reason.
8782         """
8783         # First, check if this host actually has vGPU to reshape
8784         inventories_dict = self._get_gpu_inventories()
8785         if not inventories_dict:
8786             return
8787 
8788         # Check to see if the root compute node provider in the tree for
8789         # this host already has VGPU inventory because if it does, and
8790         # we're not currently reshaping (allocations is None), we need
8791         # to indicate that a reshape is needed to move the VGPU inventory
8792         # onto a child provider in the tree.
8793 
8794         # Ensure GPU providers are in the ProviderTree for the given inventory.
8795         pgpu_rps = self._ensure_pgpu_providers(
8796             inventories_dict, provider_tree, nodename)
8797 
8798         if self._is_reshape_needed_vgpu_on_root(provider_tree, nodename):
8799             if allocations is None:
8800                 # We have old VGPU inventory on root RP, but we haven't yet
8801                 # allocations. That means we need to ask for a reshape.
8802                 LOG.info('Requesting provider tree reshape in order to move '
8803                          'VGPU inventory from the root compute node provider '
8804                          '%s to a child provider.', nodename)
8805                 raise exception.ReshapeNeeded()
8806             # We have allocations, that means we already asked for a reshape
8807             # and the Placement API returned us them. We now need to move
8808             # those from the root RP to the needed children RPs.
8809             root_node = provider_tree.data(nodename)
8810             # Reshape VGPU provider inventory and allocations, moving them
8811             # from the root node provider to the child providers.
8812             self._reshape_gpu_resources(allocations, root_node, pgpu_rps)
8813             # Only delete the root inventory once the reshape is done
8814             if orc.VGPU in root_node.inventory:
8815                 del root_node.inventory[orc.VGPU]
8816                 provider_tree.update_inventory(nodename, root_node.inventory)
8817 
8818     def _update_provider_tree_for_pcpu(self, provider_tree, nodename,
8819                                        allocations=None):
8820         """Updates the provider tree for PCPU inventory.
8821 
8822         Before Train, pinned instances consumed VCPU inventory just like
8823         unpinned instances. Starting in Train, these instances now consume PCPU
8824         inventory. The function can reshape the inventory, changing allocations
8825         of VCPUs to PCPUs.
8826 
8827         :param provider_tree: The ProviderTree to update.
8828         :param nodename: The ComputeNode.hypervisor_hostname, also known as
8829             the name of the root node provider in the tree for this host.
8830         :param allocations: A dict, keyed by consumer UUID, of allocation
8831             records, or None::
8832 
8833                 {
8834                     $CONSUMER_UUID: {
8835                         "allocations": {
8836                             $RP_UUID: {
8837                                 "generation": $RP_GEN,
8838                                 "resources": {
8839                                     $RESOURCE_CLASS: $AMOUNT,
8840                                     ...
8841                                 },
8842                             },
8843                             ...
8844                         },
8845                         "project_id": $PROJ_ID,
8846                         "user_id": $USER_ID,
8847                         "consumer_generation": $CONSUMER_GEN,
8848                     },
8849                     ...
8850                 }
8851 
8852             If provided, this indicates a reshape was requested and should be
8853             performed.
8854         :raises: nova.exception.ReshapeNeeded if ``allocations`` is None and
8855             the method determines a reshape of the tree is needed, i.e. VCPU
8856             inventory and allocations must be migrated to PCPU resources.
8857         :raises: nova.exception.ReshapeFailed if the requested tree reshape
8858             fails for whatever reason.
8859         """
8860         # If we're not configuring PCPUs, then we've nothing to worry about
8861         # (yet)
8862         if not CONF.compute.cpu_dedicated_set:
8863             return
8864 
8865         root_node = provider_tree.data(nodename)
8866 
8867         # Similarly, if PCPU inventories are already reported then there is no
8868         # need to reshape
8869         if orc.PCPU in root_node.inventory:
8870             return
8871 
8872         ctx = nova_context.get_admin_context()
8873         compute_node = objects.ComputeNode.get_by_nodename(ctx, nodename)
8874 
8875         # Finally, if the compute node doesn't appear to support NUMA, move
8876         # swiftly on
8877         if not compute_node.numa_topology:
8878             return
8879 
8880         # The ComputeNode.numa_topology is a StringField, deserialize
8881         numa = objects.NUMATopology.obj_from_db_obj(compute_node.numa_topology)
8882 
8883         # If the host doesn't know of any pinned CPUs, we can continue
8884         if not any(cell.pinned_cpus for cell in numa.cells):
8885             return
8886 
8887         # At this point, we know there's something to be migrated here but not
8888         # how much. If the allocations are None, we're at the startup of the
8889         # compute node and a Reshape is needed. Indicate this by raising the
8890         # ReshapeNeeded exception
8891 
8892         if allocations is None:
8893             LOG.info(
8894                 'Requesting provider tree reshape in order to move '
8895                 'VCPU to PCPU allocations to the compute node '
8896                 'provider %s', nodename)
8897             raise exception.ReshapeNeeded()
8898 
8899         # Go figure out how many VCPUs to migrate to PCPUs. We've been telling
8900         # people for years *not* to mix pinned and unpinned instances, meaning
8901         # we should be able to move all VCPUs to PCPUs, but we never actually
8902         # enforced this in code and there's an all-too-high chance someone
8903         # didn't get the memo
8904 
8905         allocations_needing_reshape = []
8906 
8907         # we need to tackle the allocations against instances on this host...
8908 
8909         instances = objects.InstanceList.get_by_host(
8910             ctx, compute_node.host, expected_attrs=['numa_topology'])
8911         for instance in instances:
8912             if not instance.numa_topology:
8913                 continue
8914 
8915             if instance.numa_topology.cpu_policy != (
8916                 fields.CPUAllocationPolicy.DEDICATED
8917             ):
8918                 continue
8919 
8920             allocations_needing_reshape.append(instance.uuid)
8921 
8922         # ...and those for any migrations
8923 
8924         migrations = objects.MigrationList.get_in_progress_by_host_and_node(
8925             ctx, compute_node.host, compute_node.hypervisor_hostname)
8926         for migration in migrations:
8927             # we don't care about migrations that have landed here, since we
8928             # already have those instances above
8929             if not migration.dest_compute or (
8930                     migration.dest_compute == compute_node.host):
8931                 continue
8932 
8933             instance = objects.Instance.get_by_uuid(
8934                 ctx, migration.instance_uuid, expected_attrs=['numa_topology'])
8935 
8936             if not instance.numa_topology:
8937                 continue
8938 
8939             if instance.numa_topology.cpu_policy != (
8940                 fields.CPUAllocationPolicy.DEDICATED
8941             ):
8942                 continue
8943 
8944             allocations_needing_reshape.append(migration.uuid)
8945 
8946         for allocation_uuid in allocations_needing_reshape:
8947             consumer_allocations = allocations.get(allocation_uuid, {}).get(
8948                 'allocations', {})
8949             # TODO(stephenfin): We can probably just check the allocations for
8950             # ComputeNode.uuid since compute nodes are the only (?) provider of
8951             # VCPU and PCPU resources
8952             for rp_uuid in consumer_allocations:
8953                 resources = consumer_allocations[rp_uuid]['resources']
8954 
8955                 if orc.PCPU in resources or orc.VCPU not in resources:
8956                     # Either this has been migrated or it's not a compute node
8957                     continue
8958 
8959                 # Switch stuff around. We can do a straight swap since an
8960                 # instance is either pinned or unpinned. By doing this, we're
8961                 # modifying the provided 'allocations' dict, which will
8962                 # eventually be used by the resource tracker to update
8963                 # placement
8964                 resources['PCPU'] = resources['VCPU']
8965                 del resources[orc.VCPU]
8966 
8967     def get_available_resource(self, nodename):
8968         """Retrieve resource information.
8969 
8970         This method is called when nova-compute launches, and
8971         as part of a periodic task that records the results in the DB.
8972 
8973         :param nodename: unused in this driver
8974         :returns: dictionary containing resource info
8975         """
8976 
8977         disk_info_dict = self._get_local_gb_info()
8978         data = {}
8979 
8980         # NOTE(dprince): calling capabilities before getVersion works around
8981         # an initialization issue with some versions of Libvirt (1.0.5.5).
8982         # See: https://bugzilla.redhat.com/show_bug.cgi?id=1000116
8983         # See: https://bugs.launchpad.net/nova/+bug/1215593
8984         data["supported_instances"] = self._get_instance_capabilities()
8985 
8986         data["vcpus"] = len(self._get_vcpu_available())
8987         data["memory_mb"] = self._host.get_memory_mb_total()
8988         data["local_gb"] = disk_info_dict['total']
8989         data["vcpus_used"] = self._get_vcpu_used()
8990         data["memory_mb_used"] = self._host.get_memory_mb_used()
8991         data["local_gb_used"] = disk_info_dict['used']
8992         data["hypervisor_type"] = self._host.get_driver_type()
8993         data["hypervisor_version"] = self._host.get_version()
8994         data["hypervisor_hostname"] = self._host.get_hostname()
8995         # TODO(berrange): why do we bother converting the
8996         # libvirt capabilities XML into a special JSON format ?
8997         # The data format is different across all the drivers
8998         # so we could just return the raw capabilities XML
8999         # which 'compare_cpu' could use directly
9000         #
9001         # That said, arch_filter.py now seems to rely on
9002         # the libvirt drivers format which suggests this
9003         # data format needs to be standardized across drivers
9004         data["cpu_info"] = jsonutils.dumps(self._get_cpu_info())
9005 
9006         disk_free_gb = disk_info_dict['free']
9007         disk_over_committed = self._get_disk_over_committed_size_total()
9008         available_least = disk_free_gb * units.Gi - disk_over_committed
9009         data['disk_available_least'] = available_least / units.Gi
9010 
9011         data['pci_passthrough_devices'] = self._get_pci_passthrough_devices()
9012 
9013         numa_topology = self._get_host_numa_topology()
9014         if numa_topology:
9015             data['numa_topology'] = numa_topology._to_json()
9016         else:
9017             data['numa_topology'] = None
9018 
9019         return data
9020 
9021     def check_instance_shared_storage_local(self, context, instance):
9022         """Check if instance files located on shared storage.
9023 
9024         This runs check on the destination host, and then calls
9025         back to the source host to check the results.
9026 
9027         :param context: security context
9028         :param instance: nova.objects.instance.Instance object
9029         :returns:
9030          - tempfile: A dict containing the tempfile info on the destination
9031                      host
9032          - None:
9033 
9034             1. If the instance path is not existing.
9035             2. If the image backend is shared block storage type.
9036         """
9037         if self.image_backend.backend().is_shared_block_storage():
9038             return None
9039 
9040         dirpath = libvirt_utils.get_instance_path(instance)
9041 
9042         if not os.path.exists(dirpath):
9043             return None
9044 
9045         fd, tmp_file = tempfile.mkstemp(dir=dirpath)
9046         LOG.debug("Creating tmpfile %s to verify with other "
9047                   "compute node that the instance is on "
9048                   "the same shared storage.",
9049                   tmp_file, instance=instance)
9050         os.close(fd)
9051         return {"filename": tmp_file}
9052 
9053     def check_instance_shared_storage_remote(self, context, data):
9054         return os.path.exists(data['filename'])
9055 
9056     def check_instance_shared_storage_cleanup(self, context, data):
9057         fileutils.delete_if_exists(data["filename"])
9058 
9059     def check_can_live_migrate_destination(self, context, instance,
9060                                            src_compute_info, dst_compute_info,
9061                                            block_migration=False,
9062                                            disk_over_commit=False):
9063         """Check if it is possible to execute live migration.
9064 
9065         This runs checks on the destination host, and then calls
9066         back to the source host to check the results.
9067 
9068         :param context: security context
9069         :param instance: nova.db.main.models.Instance
9070         :param block_migration: if true, prepare for block migration
9071         :param disk_over_commit: if true, allow disk over commit
9072         :returns: a LibvirtLiveMigrateData object
9073         """
9074         if disk_over_commit:
9075             disk_available_gb = dst_compute_info['free_disk_gb']
9076         else:
9077             disk_available_gb = dst_compute_info['disk_available_least']
9078         disk_available_mb = (
9079             (disk_available_gb * units.Ki) - CONF.reserved_host_disk_mb)
9080 
9081         # Compare CPU
9082         try:
9083             if not instance.vcpu_model or not instance.vcpu_model.model:
9084                 source_cpu_info = src_compute_info['cpu_info']
9085                 self._compare_cpu(None, source_cpu_info, instance)
9086             else:
9087                 self._compare_cpu(instance.vcpu_model, None, instance)
9088         except exception.InvalidCPUInfo as e:
9089             raise exception.MigrationPreCheckError(reason=e)
9090 
9091         # Create file on storage, to be checked on source host
9092         filename = self._create_shared_storage_test_file(instance)
9093 
9094         data = objects.LibvirtLiveMigrateData()
9095         data.filename = filename
9096         data.image_type = CONF.libvirt.images_type
9097         data.graphics_listen_addr_vnc = CONF.vnc.server_listen
9098         data.graphics_listen_addr_spice = CONF.spice.server_listen
9099         if CONF.serial_console.enabled:
9100             data.serial_listen_addr = CONF.serial_console.proxyclient_address
9101         else:
9102             data.serial_listen_addr = None
9103         # Notes(eliqiao): block_migration and disk_over_commit are not
9104         # nullable, so just don't set them if they are None
9105         if block_migration is not None:
9106             data.block_migration = block_migration
9107         if disk_over_commit is not None:
9108             data.disk_over_commit = disk_over_commit
9109         data.disk_available_mb = disk_available_mb
9110         data.dst_wants_file_backed_memory = CONF.libvirt.file_backed_memory > 0
9111 
9112         # TODO(artom) Set to indicate that the destination (us) can perform a
9113         # NUMA-aware live migration. NUMA-aware live migration will become
9114         # unconditionally supported in RPC 6.0, so this sentinel can be removed
9115         # then.
9116         if instance.numa_topology:
9117             data.dst_supports_numa_live_migration = True
9118 
9119         # NOTE(sean-k-mooney): The migrate_data vifs field is used to signal
9120         # that we are using the multiple port binding workflow so we can only
9121         # populate it if we are using multiple port bindings.
9122         # TODO(stephenfin): Remove once we can do this unconditionally in X or
9123         # later
9124         if self._network_api.supports_port_binding_extension(context):
9125             data.vifs = (
9126                 migrate_data_obj.VIFMigrateData.create_skeleton_migrate_vifs(
9127                     instance.get_network_info()))
9128             for vif in data.vifs:
9129                 vif.supports_os_vif_delegation = True
9130 
9131         return data
9132 
9133     def post_claim_migrate_data(self, context, instance, migrate_data, claim):
9134         migrate_data.dst_numa_info = self._get_live_migrate_numa_info(
9135             claim.claimed_numa_topology, claim.flavor, claim.image_meta)
9136         return migrate_data
9137 
9138     def _get_resources(self, instance, prefix=None):
9139         resources: 'objects.ResourceList' = []
9140         if prefix:
9141             migr_context = instance.migration_context
9142             attr_name = prefix + 'resources'
9143             if migr_context and attr_name in migr_context:
9144                 resources = getattr(migr_context, attr_name) or []
9145         else:
9146             resources = instance.resources or []
9147         return resources
9148 
9149     def _get_vpmem_resources(self, resources):
9150         vpmem_resources = []
9151         for resource in resources:
9152             if 'metadata' in resource and \
9153                 isinstance(resource.metadata, objects.LibvirtVPMEMDevice):
9154                 vpmem_resources.append(resource)
9155         return vpmem_resources
9156 
9157     def _get_ordered_vpmem_resources(self, resources, flavor):
9158         vpmem_resources = self._get_vpmem_resources(resources)
9159         ordered_vpmem_resources = []
9160         labels = hardware.get_vpmems(flavor)
9161         for label in labels:
9162             for vpmem_resource in vpmem_resources:
9163                 if vpmem_resource.metadata.label == label:
9164                     ordered_vpmem_resources.append(vpmem_resource)
9165                     vpmem_resources.remove(vpmem_resource)
9166                     break
9167         return ordered_vpmem_resources
9168 
9169     def _sorted_migrating_resources(self, instance, flavor):
9170         """This method is used to sort instance.migration_context.new_resources
9171         claimed on dest host, then the ordered new resources will be used to
9172         update resources info (e.g. vpmems) in the new xml which is used for
9173         live migration.
9174         """
9175         resources = self._get_resources(instance, prefix='new_')
9176         if not resources:
9177             return
9178         ordered_resources = []
9179         ordered_vpmem_resources = self._get_ordered_vpmem_resources(
9180                 resources, flavor)
9181         ordered_resources.extend(ordered_vpmem_resources)
9182         ordered_resources_obj = objects.ResourceList(objects=ordered_resources)
9183         return ordered_resources_obj
9184 
9185     def _get_live_migrate_numa_info(self, instance_numa_topology, flavor,
9186                                     image_meta):
9187         """Builds a LibvirtLiveMigrateNUMAInfo object to send to the source of
9188         a live migration, containing information about how the instance is to
9189         be pinned on the destination host.
9190 
9191         :param instance_numa_topology: The InstanceNUMATopology as fitted to
9192                                        the destination by the live migration
9193                                        Claim.
9194         :param flavor: The Flavor object for the instance.
9195         :param image_meta: The ImageMeta object for the instance.
9196         :returns: A LibvirtLiveMigrateNUMAInfo object indicating how to update
9197                   the XML for the destination host.
9198         """
9199         info = objects.LibvirtLiveMigrateNUMAInfo()
9200         cpu_set, guest_cpu_tune, guest_cpu_numa, guest_numa_tune = \
9201             self._get_guest_numa_config(instance_numa_topology, flavor,
9202                                         image_meta)
9203         # NOTE(artom) These two should always be either None together, or
9204         # truth-y together.
9205         if guest_cpu_tune and guest_numa_tune:
9206             info.cpu_pins = {}
9207             for pin in guest_cpu_tune.vcpupin:
9208                 info.cpu_pins[str(pin.id)] = pin.cpuset
9209 
9210             info.emulator_pins = guest_cpu_tune.emulatorpin.cpuset
9211 
9212             if guest_cpu_tune.vcpusched:
9213                 # NOTE(artom) vcpusched is a list, but there's only ever one
9214                 # element in it (see _get_guest_numa_config under
9215                 # wants_realtime)
9216                 info.sched_vcpus = guest_cpu_tune.vcpusched[0].vcpus
9217                 info.sched_priority = guest_cpu_tune.vcpusched[0].priority
9218 
9219             info.cell_pins = {}
9220             for node in guest_numa_tune.memnodes:
9221                 info.cell_pins[str(node.cellid)] = set(node.nodeset)
9222 
9223         LOG.debug('Built NUMA live migration info: %s', info)
9224         return info
9225 
9226     def cleanup_live_migration_destination_check(self, context,
9227                                                  dest_check_data):
9228         """Do required cleanup on dest host after check_can_live_migrate calls
9229 
9230         :param context: security context
9231         """
9232         filename = dest_check_data.filename
9233         self._cleanup_shared_storage_test_file(filename)
9234 
9235     def check_can_live_migrate_source(self, context, instance,
9236                                       dest_check_data,
9237                                       block_device_info=None):
9238         """Check if it is possible to execute live migration.
9239 
9240         This checks if the live migration can succeed, based on the
9241         results from check_can_live_migrate_destination.
9242 
9243         :param context: security context
9244         :param instance: nova.db.main.models.Instance
9245         :param dest_check_data: result of check_can_live_migrate_destination
9246         :param block_device_info: result of _get_instance_block_device_info
9247         :returns: a LibvirtLiveMigrateData object
9248         """
9249         # Checking shared storage connectivity
9250         # if block migration, instances_path should not be on shared storage.
9251         source = CONF.host
9252 
9253         dest_check_data.is_shared_instance_path = (
9254             self._check_shared_storage_test_file(
9255                 dest_check_data.filename, instance))
9256 
9257         dest_check_data.is_shared_block_storage = (
9258             self._is_shared_block_storage(instance, dest_check_data,
9259                                           block_device_info))
9260 
9261         if 'block_migration' not in dest_check_data:
9262             dest_check_data.block_migration = (
9263                 not dest_check_data.is_on_shared_storage())
9264 
9265         if dest_check_data.block_migration:
9266             # TODO(eliqiao): Once block_migration flag is removed from the API
9267             # we can safely remove the if condition
9268             if dest_check_data.is_on_shared_storage():
9269                 reason = _("Block migration can not be used "
9270                            "with shared storage.")
9271                 raise exception.InvalidLocalStorage(reason=reason, path=source)
9272             if 'disk_over_commit' in dest_check_data:
9273                 self._assert_dest_node_has_enough_disk(context, instance,
9274                                         dest_check_data.disk_available_mb,
9275                                         dest_check_data.disk_over_commit,
9276                                         block_device_info)
9277             if block_device_info:
9278                 bdm = block_device_info.get('block_device_mapping')
9279                 # NOTE(eliqiao): Selective disk migrations are not supported
9280                 # with tunnelled block migrations so we can block them early.
9281                 if (bdm and
9282                     (self._block_migration_flags &
9283                      libvirt.VIR_MIGRATE_TUNNELLED != 0)):
9284                     msg = (_('Cannot block migrate instance %(uuid)s with'
9285                              ' mapped volumes. Selective block device'
9286                              ' migration is not supported with tunnelled'
9287                              ' block migrations.') % {'uuid': instance.uuid})
9288                     LOG.error(msg, instance=instance)
9289                     raise exception.MigrationPreCheckError(reason=msg)
9290         elif not (dest_check_data.is_shared_block_storage or
9291                   dest_check_data.is_shared_instance_path):
9292             reason = _("Shared storage live-migration requires either shared "
9293                        "storage or boot-from-volume with no local disks.")
9294             raise exception.InvalidSharedStorage(reason=reason, path=source)
9295 
9296         # NOTE(mikal): include the instance directory name here because it
9297         # doesn't yet exist on the destination but we want to force that
9298         # same name to be used
9299         instance_path = libvirt_utils.get_instance_path(instance,
9300                                                         relative=True)
9301         dest_check_data.instance_relative_path = instance_path
9302 
9303         # TODO(artom) Set to indicate that the source (us) can perform a
9304         # NUMA-aware live migration. NUMA-aware live migration will become
9305         # unconditionally supported in RPC 6.0, so this sentinel can be removed
9306         # then.
9307         if instance.numa_topology:
9308             dest_check_data.src_supports_numa_live_migration = True
9309 
9310         return dest_check_data
9311 
9312     def _is_shared_block_storage(self, instance, dest_check_data,
9313                                  block_device_info=None):
9314         """Check if all block storage of an instance can be shared
9315         between source and destination of a live migration.
9316 
9317         Returns true if the instance is volume backed and has no local disks,
9318         or if the image backend is the same on source and destination and the
9319         backend shares block storage between compute nodes.
9320 
9321         :param instance: nova.objects.instance.Instance object
9322         :param dest_check_data: dict with boolean fields image_type,
9323                                 is_shared_instance_path, and is_volume_backed
9324         """
9325         if (dest_check_data.obj_attr_is_set('image_type') and
9326                 CONF.libvirt.images_type == dest_check_data.image_type and
9327                 self.image_backend.backend().is_shared_block_storage()):
9328             # NOTE(dgenin): currently true only for RBD image backend
9329             return True
9330 
9331         if (dest_check_data.is_shared_instance_path and
9332                 self.image_backend.backend().is_file_in_instance_path()):
9333             # NOTE(angdraug): file based image backends (Flat, Qcow2)
9334             # place block device files under the instance path
9335             return True
9336 
9337         if (dest_check_data.is_volume_backed and
9338                 not bool(self._get_instance_disk_info(instance,
9339                                                       block_device_info))):
9340             return True
9341 
9342         return False
9343 
9344     def _assert_dest_node_has_enough_disk(self, context, instance,
9345                                              available_mb, disk_over_commit,
9346                                              block_device_info):
9347         """Checks if destination has enough disk for block migration."""
9348         # Libvirt supports qcow2 disk format,which is usually compressed
9349         # on compute nodes.
9350         # Real disk image (compressed) may enlarged to "virtual disk size",
9351         # that is specified as the maximum disk size.
9352         # (See qemu-img -f path-to-disk)
9353         # Scheduler recognizes destination host still has enough disk space
9354         # if real disk size < available disk size
9355         # if disk_over_commit is True,
9356         #  otherwise virtual disk size < available disk size.
9357 
9358         available = 0
9359         if available_mb:
9360             available = available_mb * units.Mi
9361 
9362         disk_infos = self._get_instance_disk_info(instance, block_device_info)
9363 
9364         necessary = 0
9365         if disk_over_commit:
9366             for info in disk_infos:
9367                 necessary += int(info['disk_size'])
9368         else:
9369             for info in disk_infos:
9370                 necessary += int(info['virt_disk_size'])
9371 
9372         # Check that available disk > necessary disk
9373         if (available - necessary) < 0:
9374             reason = (_('Unable to migrate %(instance_uuid)s: '
9375                         'Disk of instance is too large(available'
9376                         ' on destination host:%(available)s '
9377                         '< need:%(necessary)s)') %
9378                       {'instance_uuid': instance.uuid,
9379                        'available': available,
9380                        'necessary': necessary})
9381             raise exception.MigrationPreCheckError(reason=reason)
9382 
9383     def _compare_cpu(self, guest_cpu, host_cpu_str, instance):
9384         """Check the host is compatible with the requested CPU
9385 
9386         :param guest_cpu: nova.objects.VirtCPUModel
9387             or nova.virt.libvirt.vconfig.LibvirtConfigGuestCPU or None.
9388         :param host_cpu_str: JSON from _get_cpu_info() method
9389 
9390         If the 'guest_cpu' parameter is not None, this will be
9391         validated for migration compatibility with the host.
9392         Otherwise the 'host_cpu_str' JSON string will be used for
9393         validation.
9394 
9395         :returns:
9396             None. if given cpu info is not compatible to this server,
9397             raise exception.
9398         """
9399 
9400         # NOTE(kchamart): Comparing host to guest CPU model for emulated
9401         # guests (<domain type='qemu'>) should not matter -- in this
9402         # mode (QEMU "TCG") the CPU is fully emulated in software and no
9403         # hardware acceleration, like KVM, is involved. So, skip the CPU
9404         # compatibility check for the QEMU domain type, and retain it for
9405         # KVM guests.
9406         if CONF.libvirt.virt_type not in ['kvm']:
9407             return
9408 
9409         if guest_cpu is None:
9410             info = jsonutils.loads(host_cpu_str)
9411             LOG.info('Instance launched has CPU info: %s', host_cpu_str)
9412             cpu = vconfig.LibvirtConfigCPU()
9413             cpu.arch = info['arch']
9414             cpu.model = info['model']
9415             cpu.vendor = info['vendor']
9416             cpu.sockets = info['topology']['sockets']
9417             cpu.cores = info['topology']['cores']
9418             cpu.threads = info['topology']['threads']
9419             for f in info['features']:
9420                 cpu.add_feature(vconfig.LibvirtConfigCPUFeature(f))
9421         elif isinstance(guest_cpu, vconfig.LibvirtConfigGuestCPU):
9422             cpu = guest_cpu
9423         else:
9424             cpu = self._vcpu_model_to_cpu_config(guest_cpu)
9425 
9426         host_cpu = self._host.get_capabilities().host.cpu
9427         if host_cpu.arch == fields.Architecture.AARCH64:
9428             if self._host.has_min_version(MIN_LIBVIRT_AARCH64_CPU_COMPARE):
9429                 LOG.debug("On AArch64 hosts, source and destination host "
9430                           "CPUs are compared to check if they're compatible"
9431                           "(the only use-case supported by libvirt for "
9432                           "Arm64/AArch64)")
9433                 cpu = host_cpu
9434             else:
9435                 LOG.debug("You need %s libvirt version to be able to compare "
9436                           "source host CPU with destination host CPU; skip "
9437                           "CPU comparison", MIN_LIBVIRT_AARCH64_CPU_COMPARE)
9438                 return
9439 
9440         u = ("http://libvirt.org/html/libvirt-libvirt-host.html#"
9441              "virCPUCompareResult")
9442         m = _("CPU doesn't have compatibility.\n\n%(ret)s\n\nRefer to %(u)s")
9443         # unknown character exists in xml, then libvirt complains
9444         try:
9445             cpu_xml = cpu.to_xml()
9446             LOG.debug("cpu compare xml: %s", cpu_xml, instance=instance)
9447             ret = self._host.compare_cpu(cpu_xml)
9448         except libvirt.libvirtError as e:
9449             error_code = e.get_error_code()
9450             if error_code == libvirt.VIR_ERR_NO_SUPPORT:
9451                 LOG.debug("URI %(uri)s does not support cpu comparison. "
9452                           "It will be proceeded though. Error: %(error)s",
9453                           {'uri': self._uri(), 'error': e})
9454                 return
9455             else:
9456                 LOG.error(m, {'ret': e, 'u': u})
9457                 raise exception.InvalidCPUInfo(
9458                     reason=m % {'ret': e, 'u': u})
9459 
9460         if ret <= 0:
9461             LOG.error(m, {'ret': ret, 'u': u})
9462             raise exception.InvalidCPUInfo(reason=m % {'ret': ret, 'u': u})
9463 
9464     def _create_shared_storage_test_file(self, instance):
9465         """Makes tmpfile under CONF.instances_path."""
9466         dirpath = CONF.instances_path
9467         fd, tmp_file = tempfile.mkstemp(dir=dirpath)
9468         LOG.debug("Creating tmpfile %s to notify to other "
9469                   "compute nodes that they should mount "
9470                   "the same storage.", tmp_file, instance=instance)
9471         os.close(fd)
9472         return os.path.basename(tmp_file)
9473 
9474     def _check_shared_storage_test_file(self, filename, instance):
9475         """Confirms existence of the tmpfile under CONF.instances_path.
9476 
9477         Cannot confirm tmpfile return False.
9478         """
9479         # NOTE(tpatzig): if instances_path is a shared volume that is
9480         # under heavy IO (many instances on many compute nodes),
9481         # then checking the existence of the testfile fails,
9482         # just because it takes longer until the client refreshes and new
9483         # content gets visible.
9484         # os.utime (like touch) on the directory forces the client to refresh.
9485         os.utime(CONF.instances_path, None)
9486 
9487         tmp_file = os.path.join(CONF.instances_path, filename)
9488         if not os.path.exists(tmp_file):
9489             exists = False
9490         else:
9491             exists = True
9492         LOG.debug('Check if temp file %s exists to indicate shared storage '
9493                   'is being used for migration. Exists? %s', tmp_file, exists,
9494                   instance=instance)
9495         return exists
9496 
9497     def _cleanup_shared_storage_test_file(self, filename):
9498         """Removes existence of the tmpfile under CONF.instances_path."""
9499         tmp_file = os.path.join(CONF.instances_path, filename)
9500         os.remove(tmp_file)
9501 
9502     def live_migration(self, context, instance, dest,
9503                        post_method, recover_method, block_migration=False,
9504                        migrate_data=None):
9505         """Spawning live_migration operation for distributing high-load.
9506 
9507         :param context: security context
9508         :param instance:
9509             nova.db.main.models.Instance object
9510             instance object that is migrated.
9511         :param dest: destination host
9512         :param post_method:
9513             post operation method.
9514             expected nova.compute.manager._post_live_migration.
9515         :param recover_method:
9516             recovery method when any exception occurs.
9517             expected nova.compute.manager._rollback_live_migration.
9518         :param block_migration: if true, do block migration.
9519         :param migrate_data: a LibvirtLiveMigrateData object
9520 
9521         """
9522 
9523         # 'dest' will be substituted into 'migration_uri' so ensure
9524         # it does't contain any characters that could be used to
9525         # exploit the URI accepted by libvirt
9526         if not libvirt_utils.is_valid_hostname(dest):
9527             raise exception.InvalidHostname(hostname=dest)
9528 
9529         self._live_migration(context, instance, dest,
9530                              post_method, recover_method, block_migration,
9531                              migrate_data)
9532 
9533     def live_migration_abort(self, instance):
9534         """Aborting a running live-migration.
9535 
9536         :param instance: instance object that is in migration
9537 
9538         """
9539 
9540         guest = self._host.get_guest(instance)
9541         dom = guest._domain
9542 
9543         try:
9544             dom.abortJob()
9545         except libvirt.libvirtError as e:
9546             LOG.error("Failed to cancel migration %s",
9547                     encodeutils.exception_to_unicode(e), instance=instance)
9548             raise
9549 
9550     def _verify_serial_console_is_disabled(self):
9551         if CONF.serial_console.enabled:
9552 
9553             msg = _('Your destination node does not support'
9554                     ' retrieving listen addresses. In order'
9555                     ' for live migration to work properly you'
9556                     ' must disable serial console.')
9557             raise exception.MigrationError(reason=msg)
9558 
9559     def _detach_direct_passthrough_vifs(self, context,
9560                                         migrate_data, instance):
9561         """detaches passthrough vif to enable live migration
9562 
9563         :param context: security context
9564         :param migrate_data: a LibvirtLiveMigrateData object
9565         :param instance: instance object that is migrated.
9566         """
9567         # NOTE(sean-k-mooney): if we have vif data available we
9568         # loop over each vif and detach all direct passthrough
9569         # vifs to allow sriov live migration.
9570         direct_vnics = network_model.VNIC_TYPES_DIRECT_PASSTHROUGH
9571         vifs = [vif.source_vif for vif in migrate_data.vifs
9572                 if "source_vif" in vif and vif.source_vif]
9573         for vif in vifs:
9574             if vif['vnic_type'] in direct_vnics:
9575                 LOG.info("Detaching vif %s from instance "
9576                          "%s for live migration", vif['id'], instance.id)
9577                 self.detach_interface(context, instance, vif)
9578 
9579     def _live_migration_operation(self, context, instance, dest,
9580                                   block_migration, migrate_data, guest,
9581                                   device_names):
9582         """Invoke the live migration operation
9583 
9584         :param context: security context
9585         :param instance:
9586             nova.db.main.models.Instance object
9587             instance object that is migrated.
9588         :param dest: destination host
9589         :param block_migration: if true, do block migration.
9590         :param migrate_data: a LibvirtLiveMigrateData object
9591         :param guest: the guest domain object
9592         :param device_names: list of device names that are being migrated with
9593             instance
9594 
9595         This method is intended to be run in a background thread and will
9596         block that thread until the migration is finished or failed.
9597         """
9598         try:
9599             if migrate_data.block_migration:
9600                 migration_flags = self._block_migration_flags
9601             else:
9602                 migration_flags = self._live_migration_flags
9603 
9604             if not migrate_data.serial_listen_addr:
9605                 # In this context we want to ensure that serial console is
9606                 # disabled on source node. This is because nova couldn't
9607                 # retrieve serial listen address from destination node, so we
9608                 # consider that destination node might have serial console
9609                 # disabled as well.
9610                 self._verify_serial_console_is_disabled()
9611 
9612             # NOTE(aplanas) migrate_uri will have a value only in the
9613             # case that `live_migration_inbound_addr` parameter is
9614             # set, and we propose a non tunneled migration.
9615             migrate_uri = None
9616             if ('target_connect_addr' in migrate_data and
9617                     migrate_data.target_connect_addr is not None):
9618                 dest = migrate_data.target_connect_addr
9619                 if (migration_flags &
9620                     libvirt.VIR_MIGRATE_TUNNELLED == 0):
9621                     migrate_uri = self._migrate_uri(dest)
9622 
9623             new_xml_str = None
9624             if CONF.libvirt.virt_type != "parallels":
9625                 # If the migrate_data has port binding information for the
9626                 # destination host, we need to prepare the guest vif config
9627                 # for the destination before we start migrating the guest.
9628                 get_vif_config = None
9629                 if 'vifs' in migrate_data and migrate_data.vifs:
9630                     # NOTE(mriedem): The vif kwarg must be built on the fly
9631                     # within get_updated_guest_xml based on migrate_data.vifs.
9632                     # We could stash the virt_type from the destination host
9633                     # into LibvirtLiveMigrateData but the host kwarg is a
9634                     # nova.virt.libvirt.host.Host object and is used to check
9635                     # information like libvirt version on the destination.
9636                     # If this becomes a problem, what we could do is get the
9637                     # VIF configs while on the destination host during
9638                     # pre_live_migration() and store those in the
9639                     # LibvirtLiveMigrateData object. For now we just use the
9640                     # source host information for virt_type and
9641                     # host (version) since the conductor live_migrate method
9642                     # _check_compatible_with_source_hypervisor() ensures that
9643                     # the hypervisor types and versions are compatible.
9644                     get_vif_config = functools.partial(
9645                         self.vif_driver.get_config,
9646                         instance=instance,
9647                         image_meta=instance.image_meta,
9648                         flavor=instance.flavor,
9649                         virt_type=CONF.libvirt.virt_type,
9650                     )
9651                     self._detach_direct_passthrough_vifs(context,
9652                         migrate_data, instance)
9653                 new_resources = None
9654                 if isinstance(instance, objects.Instance):
9655                     new_resources = self._sorted_migrating_resources(
9656                         instance, instance.flavor)
9657                 new_xml_str = libvirt_migrate.get_updated_guest_xml(
9658                     # TODO(sahid): It's not a really good idea to pass
9659                     # the method _get_volume_config and we should to find
9660                     # a way to avoid this in future.
9661                     instance, guest, migrate_data, self._get_volume_config,
9662                     get_vif_config=get_vif_config, new_resources=new_resources)
9663 
9664             # NOTE(pkoniszewski): Because of precheck which blocks
9665             # tunnelled block live migration with mapped volumes we
9666             # can safely remove migrate_disks when tunnelling is on.
9667             # Otherwise we will block all tunnelled block migrations,
9668             # even when an instance does not have volumes mapped.
9669             # This is because selective disk migration is not
9670             # supported in tunnelled block live migration. Also we
9671             # cannot fallback to migrateToURI2 in this case because of
9672             # bug #1398999
9673             #
9674             # TODO(kchamart) Move the following bit to guest.migrate()
9675             if (migration_flags & libvirt.VIR_MIGRATE_TUNNELLED != 0):
9676                 device_names = []
9677 
9678             # TODO(sahid): This should be in
9679             # post_live_migration_at_source but no way to retrieve
9680             # ports acquired on the host for the guest at this
9681             # step. Since the domain is going to be removed from
9682             # libvird on source host after migration, we backup the
9683             # serial ports to release them if all went well.
9684             serial_ports = []
9685             if CONF.serial_console.enabled:
9686                 serial_ports = list(self._get_serial_ports_from_guest(guest))
9687 
9688             LOG.debug("About to invoke the migrate API", instance=instance)
9689             guest.migrate(self._live_migration_uri(dest),
9690                           migrate_uri=migrate_uri,
9691                           flags=migration_flags,
9692                           migrate_disks=device_names,
9693                           destination_xml=new_xml_str,
9694                           bandwidth=CONF.libvirt.live_migration_bandwidth)
9695             LOG.debug("Migrate API has completed", instance=instance)
9696 
9697             for hostname, port in serial_ports:
9698                 serial_console.release_port(host=hostname, port=port)
9699         except Exception as e:
9700             with excutils.save_and_reraise_exception():
9701                 LOG.error("Live Migration failure: %s", e, instance=instance)
9702 
9703         # If 'migrateToURI' fails we don't know what state the
9704         # VM instances on each host are in. Possibilities include
9705         #
9706         #  1. src==running, dst==none
9707         #
9708         #     Migration failed & rolled back, or never started
9709         #
9710         #  2. src==running, dst==paused
9711         #
9712         #     Migration started but is still ongoing
9713         #
9714         #  3. src==paused,  dst==paused
9715         #
9716         #     Migration data transfer completed, but switchover
9717         #     is still ongoing, or failed
9718         #
9719         #  4. src==paused,  dst==running
9720         #
9721         #     Migration data transfer completed, switchover
9722         #     happened but cleanup on source failed
9723         #
9724         #  5. src==none,    dst==running
9725         #
9726         #     Migration fully succeeded.
9727         #
9728         # Libvirt will aim to complete any migration operation
9729         # or roll it back. So even if the migrateToURI call has
9730         # returned an error, if the migration was not finished
9731         # libvirt should clean up.
9732         #
9733         # So we take the error raise here with a pinch of salt
9734         # and rely on the domain job info status to figure out
9735         # what really happened to the VM, which is a much more
9736         # reliable indicator.
9737         #
9738         # In particular we need to try very hard to ensure that
9739         # Nova does not "forget" about the guest. ie leaving it
9740         # running on a different host to the one recorded in
9741         # the database, as that would be a serious resource leak
9742 
9743         LOG.debug("Migration operation thread has finished",
9744                   instance=instance)
9745 
9746     def _live_migration_copy_disk_paths(self, context, instance, guest):
9747         '''Get list of disks to copy during migration
9748 
9749         :param context: security context
9750         :param instance: the instance being migrated
9751         :param guest: the Guest instance being migrated
9752 
9753         Get the list of disks to copy during migration.
9754 
9755         :returns: a list of local source paths and a list of device names to
9756             copy
9757         '''
9758 
9759         disk_paths = []
9760         device_names = []
9761         block_devices = []
9762 
9763         if (self._block_migration_flags &
9764                 libvirt.VIR_MIGRATE_TUNNELLED == 0):
9765             bdm_list = objects.BlockDeviceMappingList.get_by_instance_uuid(
9766                 context, instance.uuid)
9767             block_device_info = driver.get_block_device_info(instance,
9768                                                              bdm_list)
9769 
9770             block_device_mappings = driver.block_device_info_get_mapping(
9771                 block_device_info)
9772             for bdm in block_device_mappings:
9773                 device_name = str(bdm['mount_device'].rsplit('/', 1)[1])
9774                 block_devices.append(device_name)
9775 
9776         for dev in guest.get_all_disks():
9777             if dev.readonly or dev.shareable:
9778                 continue
9779             if dev.source_type not in ["file", "block"]:
9780                 continue
9781             if dev.target_dev in block_devices:
9782                 continue
9783             disk_paths.append(dev.source_path)
9784             device_names.append(dev.target_dev)
9785         return (disk_paths, device_names)
9786 
9787     def _live_migration_data_gb(self, instance, disk_paths):
9788         '''Calculate total amount of data to be transferred
9789 
9790         :param instance: the nova.objects.Instance being migrated
9791         :param disk_paths: list of disk paths that are being migrated
9792         with instance
9793 
9794         Calculates the total amount of data that needs to be
9795         transferred during the live migration. The actual
9796         amount copied will be larger than this, due to the
9797         guest OS continuing to dirty RAM while the migration
9798         is taking place. So this value represents the minimal
9799         data size possible.
9800 
9801         :returns: data size to be copied in GB
9802         '''
9803 
9804         ram_gb = instance.flavor.memory_mb * units.Mi / units.Gi
9805         if ram_gb < 2:
9806             ram_gb = 2
9807 
9808         disk_gb = 0
9809         for path in disk_paths:
9810             try:
9811                 size = os.stat(path).st_size
9812                 size_gb = (size / units.Gi)
9813                 if size_gb < 2:
9814                     size_gb = 2
9815                 disk_gb += size_gb
9816             except OSError as e:
9817                 LOG.warning("Unable to stat %(disk)s: %(ex)s",
9818                             {'disk': path, 'ex': e})
9819                 # Ignore error since we don't want to break
9820                 # the migration monitoring thread operation
9821 
9822         return ram_gb + disk_gb
9823 
9824     def _get_migration_flags(self, is_block_migration):
9825         if is_block_migration:
9826             return self._block_migration_flags
9827         return self._live_migration_flags
9828 
9829     def _live_migration_monitor(self, context, instance, guest,
9830                                 dest, post_method,
9831                                 recover_method, block_migration,
9832                                 migrate_data, finish_event,
9833                                 disk_paths):
9834 
9835         on_migration_failure: ty.Deque[str] = deque()
9836         data_gb = self._live_migration_data_gb(instance, disk_paths)
9837         downtime_steps = list(libvirt_migrate.downtime_steps(data_gb))
9838         migration = migrate_data.migration
9839         curdowntime = None
9840 
9841         migration_flags = self._get_migration_flags(
9842                                   migrate_data.block_migration)
9843 
9844         n = 0
9845         start = time.time()
9846         is_post_copy_enabled = self._is_post_copy_enabled(migration_flags)
9847         # vpmem does not support post copy
9848         is_post_copy_enabled &= not bool(self._get_vpmems(instance))
9849         while True:
9850             info = guest.get_job_info()
9851 
9852             if info.type == libvirt.VIR_DOMAIN_JOB_NONE:
9853                 # Either still running, or failed or completed,
9854                 # lets untangle the mess
9855                 if not finish_event.ready():
9856                     LOG.debug("Operation thread is still running",
9857                               instance=instance)
9858                 else:
9859                     info.type = libvirt_migrate.find_job_type(guest, instance)
9860                     LOG.debug("Fixed incorrect job type to be %d",
9861                               info.type, instance=instance)
9862 
9863             if info.type == libvirt.VIR_DOMAIN_JOB_NONE:
9864                 # Migration is not yet started
9865                 LOG.debug("Migration not running yet",
9866                           instance=instance)
9867             elif info.type == libvirt.VIR_DOMAIN_JOB_UNBOUNDED:
9868                 # Migration is still running
9869                 #
9870                 # This is where we wire up calls to change live
9871                 # migration status. eg change max downtime, cancel
9872                 # the operation, change max bandwidth
9873                 libvirt_migrate.run_tasks(guest, instance,
9874                                           self.active_migrations,
9875                                           on_migration_failure,
9876                                           migration,
9877                                           is_post_copy_enabled)
9878 
9879                 now = time.time()
9880                 elapsed = now - start
9881 
9882                 completion_timeout = int(
9883                     CONF.libvirt.live_migration_completion_timeout * data_gb)
9884                 # NOTE(yikun): Check the completion timeout to determine
9885                 # should trigger the timeout action, and there are two choices
9886                 # ``abort`` (default) or ``force_complete``. If the action is
9887                 # set to ``force_complete``, the post-copy will be triggered
9888                 # if available else the VM will be suspended, otherwise the
9889                 # live migrate operation will be aborted.
9890                 if libvirt_migrate.should_trigger_timeout_action(
9891                         instance, elapsed, completion_timeout,
9892                         migration.status):
9893                     timeout_act = CONF.libvirt.live_migration_timeout_action
9894                     if timeout_act == 'force_complete':
9895                         self.live_migration_force_complete(instance)
9896                     else:
9897                         # timeout action is 'abort'
9898                         try:
9899                             guest.abort_job()
9900                         except libvirt.libvirtError as e:
9901                             LOG.warning("Failed to abort migration %s",
9902                                     encodeutils.exception_to_unicode(e),
9903                                     instance=instance)
9904                             self._clear_empty_migration(instance)
9905                             raise
9906 
9907                 curdowntime = libvirt_migrate.update_downtime(
9908                     guest, instance, curdowntime,
9909                     downtime_steps, elapsed)
9910 
9911                 # We loop every 500ms, so don't log on every
9912                 # iteration to avoid spamming logs for long
9913                 # running migrations. Just once every 5 secs
9914                 # is sufficient for developers to debug problems.
9915                 # We log once every 30 seconds at info to help
9916                 # admins see slow running migration operations
9917                 # when debug logs are off.
9918                 if (n % 10) == 0:
9919                     # Ignoring memory_processed, as due to repeated
9920                     # dirtying of data, this can be way larger than
9921                     # memory_total. Best to just look at what's
9922                     # remaining to copy and ignore what's done already
9923                     #
9924                     # TODO(berrange) perhaps we could include disk
9925                     # transfer stats in the progress too, but it
9926                     # might make memory info more obscure as large
9927                     # disk sizes might dwarf memory size
9928                     remaining = 100
9929                     if info.memory_total != 0:
9930                         remaining = round(info.memory_remaining *
9931                                           100 / info.memory_total)
9932 
9933                     libvirt_migrate.save_stats(instance, migration,
9934                                                info, remaining)
9935 
9936                     # NOTE(fanzhang): do not include disk transfer stats in
9937                     # the progress percentage calculation but log them.
9938                     disk_remaining = 100
9939                     if info.disk_total != 0:
9940                         disk_remaining = round(info.disk_remaining *
9941                                                100 / info.disk_total)
9942 
9943                     lg = LOG.debug
9944                     if (n % 60) == 0:
9945                         lg = LOG.info
9946 
9947                     lg("Migration running for %(secs)d secs, "
9948                        "memory %(remaining)d%% remaining "
9949                        "(bytes processed=%(processed_memory)d, "
9950                        "remaining=%(remaining_memory)d, "
9951                        "total=%(total_memory)d); "
9952                        "disk %(disk_remaining)d%% remaining "
9953                        "(bytes processed=%(processed_disk)d, "
9954                        "remaining=%(remaining_disk)d, "
9955                        "total=%(total_disk)d).",
9956                        {"secs": elapsed, "remaining": remaining,
9957                         "processed_memory": info.memory_processed,
9958                         "remaining_memory": info.memory_remaining,
9959                         "total_memory": info.memory_total,
9960                         "disk_remaining": disk_remaining,
9961                         "processed_disk": info.disk_processed,
9962                         "remaining_disk": info.disk_remaining,
9963                         "total_disk": info.disk_total}, instance=instance)
9964 
9965                 n = n + 1
9966             elif info.type == libvirt.VIR_DOMAIN_JOB_COMPLETED:
9967                 # Migration is all done
9968                 LOG.info("Migration operation has completed",
9969                          instance=instance)
9970                 post_method(context, instance, dest, block_migration,
9971                             migrate_data)
9972                 break
9973             elif info.type == libvirt.VIR_DOMAIN_JOB_FAILED:
9974                 # Migration did not succeed
9975                 LOG.error("Migration operation has aborted", instance=instance)
9976                 libvirt_migrate.run_recover_tasks(self._host, guest, instance,
9977                                                   on_migration_failure)
9978                 recover_method(context, instance, dest, migrate_data)
9979                 break
9980             elif info.type == libvirt.VIR_DOMAIN_JOB_CANCELLED:
9981                 # Migration was stopped by admin
9982                 LOG.warning("Migration operation was cancelled",
9983                             instance=instance)
9984                 libvirt_migrate.run_recover_tasks(self._host, guest, instance,
9985                                                   on_migration_failure)
9986                 recover_method(context, instance, dest, migrate_data,
9987                                migration_status='cancelled')
9988                 break
9989             else:
9990                 LOG.warning("Unexpected migration job type: %d",
9991                             info.type, instance=instance)
9992 
9993             time.sleep(0.5)
9994         self._clear_empty_migration(instance)
9995 
9996     def _clear_empty_migration(self, instance):
9997         try:
9998             del self.active_migrations[instance.uuid]
9999         except KeyError:
10000             LOG.warning("There are no records in active migrations "
10001                         "for instance", instance=instance)
10002 
10003     def _live_migration(self, context, instance, dest, post_method,
10004                         recover_method, block_migration,
10005                         migrate_data):
10006         """Do live migration.
10007 
10008         :param context: security context
10009         :param instance:
10010             nova.db.main.models.Instance object
10011             instance object that is migrated.
10012         :param dest: destination host
10013         :param post_method:
10014             post operation method.
10015             expected nova.compute.manager._post_live_migration.
10016         :param recover_method:
10017             recovery method when any exception occurs.
10018             expected nova.compute.manager._rollback_live_migration.
10019         :param block_migration: if true, do block migration.
10020         :param migrate_data: a LibvirtLiveMigrateData object
10021 
10022         This fires off a new thread to run the blocking migration
10023         operation, and then this thread monitors the progress of
10024         migration and controls its operation
10025         """
10026 
10027         guest = self._host.get_guest(instance)
10028 
10029         disk_paths = []
10030         device_names = []
10031         if (migrate_data.block_migration and
10032                 CONF.libvirt.virt_type != "parallels"):
10033             disk_paths, device_names = self._live_migration_copy_disk_paths(
10034                 context, instance, guest)
10035 
10036         opthread = utils.spawn(self._live_migration_operation,
10037                                      context, instance, dest,
10038                                      block_migration,
10039                                      migrate_data, guest,
10040                                      device_names)
10041 
10042         finish_event = eventlet.event.Event()
10043         self.active_migrations[instance.uuid] = deque()
10044 
10045         def thread_finished(thread, event):
10046             LOG.debug("Migration operation thread notification",
10047                       instance=instance)
10048             event.send()
10049         opthread.link(thread_finished, finish_event)
10050 
10051         # Let eventlet schedule the new thread right away
10052         time.sleep(0)
10053 
10054         try:
10055             LOG.debug("Starting monitoring of live migration",
10056                       instance=instance)
10057             self._live_migration_monitor(context, instance, guest, dest,
10058                                          post_method, recover_method,
10059                                          block_migration, migrate_data,
10060                                          finish_event, disk_paths)
10061         except Exception as ex:
10062             LOG.warning("Error monitoring migration: %(ex)s",
10063                         {"ex": ex}, instance=instance, exc_info=True)
10064             # NOTE(aarents): Ensure job is aborted if still running before
10065             # raising the exception so this would avoid the migration to be
10066             # done and the libvirt guest to be resumed on the target while
10067             # the instance record would still related to the source host.
10068             try:
10069                 # If migration is running in post-copy mode and guest
10070                 # already running on dest host, libvirt will refuse to
10071                 # cancel migration job.
10072                 self.live_migration_abort(instance)
10073             except libvirt.libvirtError:
10074                 LOG.warning("Error occured when trying to abort live ",
10075                             "migration job, ignoring it.", instance=instance)
10076             raise
10077         finally:
10078             LOG.debug("Live migration monitoring is all done",
10079                       instance=instance)
10080 
10081     def _is_post_copy_enabled(self, migration_flags):
10082         return (migration_flags & libvirt.VIR_MIGRATE_POSTCOPY) != 0
10083 
10084     def live_migration_force_complete(self, instance):
10085         try:
10086             self.active_migrations[instance.uuid].append('force-complete')
10087         except KeyError:
10088             raise exception.NoActiveMigrationForInstance(
10089                 instance_id=instance.uuid)
10090 
10091     def _try_fetch_image(self, context, path, image_id, instance,
10092                          fallback_from_host=None):
10093         try:
10094             libvirt_utils.fetch_image(context, path, image_id,
10095                                       instance.trusted_certs)
10096         except exception.ImageNotFound:
10097             if not fallback_from_host:
10098                 raise
10099             LOG.debug("Image %(image_id)s doesn't exist anymore on "
10100                       "image service, attempting to copy image "
10101                       "from %(host)s",
10102                       {'image_id': image_id, 'host': fallback_from_host})
10103             libvirt_utils.copy_image(src=path, dest=path,
10104                                      host=fallback_from_host,
10105                                      receive=True)
10106 
10107     def _fetch_instance_kernel_ramdisk(self, context, instance,
10108                                        fallback_from_host=None):
10109         """Download kernel and ramdisk for instance in instance directory."""
10110         instance_dir = libvirt_utils.get_instance_path(instance)
10111         if instance.kernel_id:
10112             kernel_path = os.path.join(instance_dir, 'kernel')
10113             # NOTE(dsanders): only fetch image if it's not available at
10114             # kernel_path. This also avoids ImageNotFound exception if
10115             # the image has been deleted from glance
10116             if not os.path.exists(kernel_path):
10117                 self._try_fetch_image(context,
10118                                       kernel_path,
10119                                       instance.kernel_id,
10120                                       instance, fallback_from_host)
10121             if instance.ramdisk_id:
10122                 ramdisk_path = os.path.join(instance_dir, 'ramdisk')
10123                 # NOTE(dsanders): only fetch image if it's not available at
10124                 # ramdisk_path. This also avoids ImageNotFound exception if
10125                 # the image has been deleted from glance
10126                 if not os.path.exists(ramdisk_path):
10127                     self._try_fetch_image(context,
10128                                           ramdisk_path,
10129                                           instance.ramdisk_id,
10130                                           instance, fallback_from_host)
10131 
10132     def _reattach_instance_vifs(self, context, instance, network_info):
10133         guest = self._host.get_guest(instance)
10134         # validate that the guest has the expected number of interfaces
10135         # attached.
10136         guest_interfaces = guest.get_interfaces()
10137         # NOTE(sean-k-mooney): In general len(guest_interfaces) will
10138         # be equal to len(network_info) as interfaces will not be hot unplugged
10139         # unless they are SR-IOV direct mode interfaces. As such we do not
10140         # need an else block here as it would be a noop.
10141         if len(guest_interfaces) < len(network_info):
10142             # NOTE(sean-k-mooney): we are doing a post live migration
10143             # for a guest with sriov vif that were detached as part of
10144             # the migration. loop over the vifs and attach the missing
10145             # vif as part of the post live migration phase.
10146             direct_vnics = network_model.VNIC_TYPES_DIRECT_PASSTHROUGH
10147             for vif in network_info:
10148                 if vif['vnic_type'] in direct_vnics:
10149                     LOG.info("Attaching vif %s to instance %s",
10150                              vif['id'], instance.id)
10151                     self.attach_interface(context, instance,
10152                                           instance.image_meta, vif)
10153 
10154     def rollback_live_migration_at_source(self, context, instance,
10155                                           migrate_data):
10156         """reconnect sriov interfaces after failed live migration
10157         :param context: security context
10158         :param instance:  the instance being migrated
10159         :param migrate_date: a LibvirtLiveMigrateData object
10160         """
10161         network_info = network_model.NetworkInfo(
10162             [vif.source_vif for vif in migrate_data.vifs
10163                             if "source_vif" in vif and vif.source_vif])
10164         self._reattach_instance_vifs(context, instance, network_info)
10165 
10166     def rollback_live_migration_at_destination(self, context, instance,
10167                                                network_info,
10168                                                block_device_info,
10169                                                destroy_disks=True,
10170                                                migrate_data=None):
10171         """Clean up destination node after a failed live migration."""
10172         try:
10173             self.destroy(context, instance, network_info, block_device_info,
10174                          destroy_disks)
10175         finally:
10176             # NOTE(gcb): Failed block live migration may leave instance
10177             # directory at destination node, ensure it is always deleted.
10178             is_shared_instance_path = True
10179             if migrate_data:
10180                 is_shared_instance_path = migrate_data.is_shared_instance_path
10181                 if (migrate_data.obj_attr_is_set("serial_listen_ports") and
10182                         migrate_data.serial_listen_ports):
10183                     # Releases serial ports reserved.
10184                     for port in migrate_data.serial_listen_ports:
10185                         serial_console.release_port(
10186                             host=migrate_data.serial_listen_addr, port=port)
10187 
10188             if not is_shared_instance_path:
10189                 instance_dir = libvirt_utils.get_instance_path_at_destination(
10190                     instance, migrate_data)
10191                 if os.path.exists(instance_dir):
10192                     shutil.rmtree(instance_dir)
10193 
10194     def _pre_live_migration_plug_vifs(self, instance, network_info,
10195                                       migrate_data):
10196         if 'vifs' in migrate_data and migrate_data.vifs:
10197             LOG.debug('Plugging VIFs using destination host port bindings '
10198                       'before live migration.', instance=instance)
10199             vif_plug_nw_info = network_model.NetworkInfo([])
10200             for migrate_vif in migrate_data.vifs:
10201                 vif_plug_nw_info.append(migrate_vif.get_dest_vif())
10202         else:
10203             LOG.debug('Plugging VIFs before live migration.',
10204                       instance=instance)
10205             vif_plug_nw_info = network_info
10206         # Retry operation is necessary because continuous live migration
10207         # requests to the same host cause concurrent requests to iptables,
10208         # then it complains.
10209         max_retry = CONF.live_migration_retry_count
10210         for cnt in range(max_retry):
10211             try:
10212                 self.plug_vifs(instance, vif_plug_nw_info)
10213                 break
10214             except processutils.ProcessExecutionError:
10215                 if cnt == max_retry - 1:
10216                     raise
10217                 else:
10218                     LOG.warning('plug_vifs() failed %(cnt)d. Retry up to '
10219                                 '%(max_retry)d.',
10220                                 {'cnt': cnt, 'max_retry': max_retry},
10221                                 instance=instance)
10222                     greenthread.sleep(1)
10223 
10224     def pre_live_migration(self, context, instance, block_device_info,
10225                            network_info, disk_info, migrate_data):
10226         """Preparation live migration."""
10227         if disk_info is not None:
10228             disk_info = jsonutils.loads(disk_info)
10229 
10230         LOG.debug('migrate_data in pre_live_migration: %s', migrate_data,
10231                   instance=instance)
10232         is_shared_block_storage = migrate_data.is_shared_block_storage
10233         is_shared_instance_path = migrate_data.is_shared_instance_path
10234         is_block_migration = migrate_data.block_migration
10235 
10236         if not is_shared_instance_path:
10237             instance_dir = libvirt_utils.get_instance_path_at_destination(
10238                             instance, migrate_data)
10239 
10240             if os.path.exists(instance_dir):
10241                 raise exception.DestinationDiskExists(path=instance_dir)
10242 
10243             LOG.debug('Creating instance directory: %s', instance_dir,
10244                       instance=instance)
10245             os.mkdir(instance_dir)
10246 
10247             # Recreate the disk.info file and in doing so stop the
10248             # imagebackend from recreating it incorrectly by inspecting the
10249             # contents of each file when using the Raw backend.
10250             if disk_info:
10251                 image_disk_info = {}
10252                 for info in disk_info:
10253                     image_file = os.path.basename(info['path'])
10254                     image_path = os.path.join(instance_dir, image_file)
10255                     image_disk_info[image_path] = info['type']
10256 
10257                 LOG.debug('Creating disk.info with the contents: %s',
10258                           image_disk_info, instance=instance)
10259 
10260                 image_disk_info_path = os.path.join(instance_dir,
10261                                                     'disk.info')
10262                 with open(image_disk_info_path, 'w') as f:
10263                     f.write(jsonutils.dumps(image_disk_info))
10264 
10265             if not is_shared_block_storage:
10266                 # Ensure images and backing files are present.
10267                 LOG.debug('Checking to make sure images and backing files are '
10268                           'present before live migration.', instance=instance)
10269                 self._create_images_and_backing(
10270                     context, instance, instance_dir, disk_info,
10271                     fallback_from_host=instance.host)
10272                 if (configdrive.required_by(instance) and
10273                         CONF.config_drive_format == 'iso9660'):
10274                     # NOTE(pkoniszewski): Due to a bug in libvirt iso config
10275                     # drive needs to be copied to destination prior to
10276                     # migration when instance path is not shared and block
10277                     # storage is not shared. Files that are already present
10278                     # on destination are excluded from a list of files that
10279                     # need to be copied to destination. If we don't do that
10280                     # live migration will fail on copying iso config drive to
10281                     # destination and writing to read-only device.
10282                     # Please see bug/1246201 for more details.
10283                     src = "%s:%s/disk.config" % (instance.host, instance_dir)
10284                     self._remotefs.copy_file(src, instance_dir)
10285 
10286             if not is_block_migration:
10287                 # NOTE(angdraug): when block storage is shared between source
10288                 # and destination and instance path isn't (e.g. volume backed
10289                 # or rbd backed instance), instance path on destination has to
10290                 # be prepared
10291 
10292                 # Required by Quobyte CI
10293                 self._ensure_console_log_for_instance(instance)
10294 
10295                 # if image has kernel and ramdisk, just download
10296                 # following normal way.
10297                 self._fetch_instance_kernel_ramdisk(context, instance)
10298 
10299         # Establishing connection to volume server.
10300         block_device_mapping = driver.block_device_info_get_mapping(
10301             block_device_info)
10302 
10303         if len(block_device_mapping):
10304             LOG.debug('Connecting volumes before live migration.',
10305                       instance=instance)
10306 
10307         for bdm in block_device_mapping:
10308             connection_info = bdm['connection_info']
10309             self._connect_volume(context, connection_info, instance)
10310 
10311         self._pre_live_migration_plug_vifs(
10312             instance, network_info, migrate_data)
10313 
10314         # Store server_listen and latest disk device info
10315         if not migrate_data:
10316             migrate_data = objects.LibvirtLiveMigrateData(bdms=[])
10317         else:
10318             migrate_data.bdms = []
10319         # Store live_migration_inbound_addr
10320         migrate_data.target_connect_addr = \
10321             CONF.libvirt.live_migration_inbound_addr
10322         migrate_data.supported_perf_events = self._supported_perf_events
10323 
10324         migrate_data.serial_listen_ports = []
10325         if CONF.serial_console.enabled:
10326             num_ports = hardware.get_number_of_serial_ports(
10327                 instance.flavor, instance.image_meta)
10328             for port in range(num_ports):
10329                 migrate_data.serial_listen_ports.append(
10330                     serial_console.acquire_port(
10331                         migrate_data.serial_listen_addr))
10332 
10333         for vol in block_device_mapping:
10334             connection_info = vol['connection_info']
10335             if connection_info.get('serial'):
10336                 disk_info = blockinfo.get_info_from_bdm(
10337                     instance, CONF.libvirt.virt_type,
10338                     instance.image_meta, vol)
10339 
10340                 bdmi = objects.LibvirtLiveMigrateBDMInfo()
10341                 bdmi.serial = connection_info['serial']
10342                 bdmi.connection_info = connection_info
10343                 bdmi.bus = disk_info['bus']
10344                 bdmi.dev = disk_info['dev']
10345                 bdmi.type = disk_info['type']
10346                 bdmi.format = disk_info.get('format')
10347                 bdmi.boot_index = disk_info.get('boot_index')
10348                 volume_secret = self._host.find_secret('volume', vol.volume_id)
10349                 if volume_secret:
10350                     bdmi.encryption_secret_uuid = volume_secret.UUIDString()
10351 
10352                 migrate_data.bdms.append(bdmi)
10353 
10354         return migrate_data
10355 
10356     def _try_fetch_image_cache(self, image, fetch_func, context, filename,
10357                                image_id, instance, size,
10358                                fallback_from_host=None):
10359         try:
10360             image.cache(fetch_func=fetch_func,
10361                         context=context,
10362                         filename=filename,
10363                         image_id=image_id,
10364                         size=size,
10365                         trusted_certs=instance.trusted_certs)
10366         except exception.ImageNotFound:
10367             if not fallback_from_host:
10368                 raise
10369             LOG.debug("Image %(image_id)s doesn't exist anymore "
10370                       "on image service, attempting to copy "
10371                       "image from %(host)s",
10372                       {'image_id': image_id, 'host': fallback_from_host},
10373                       instance=instance)
10374 
10375             def copy_from_host(target):
10376                 libvirt_utils.copy_image(src=target,
10377                                          dest=target,
10378                                          host=fallback_from_host,
10379                                          receive=True)
10380             image.cache(fetch_func=copy_from_host, size=size,
10381                         filename=filename)
10382 
10383         # NOTE(lyarwood): If the instance vm_state is shelved offloaded then we
10384         # must be unshelving for _try_fetch_image_cache to be called.
10385         # NOTE(mriedem): Alternatively if we are doing a cross-cell move of a
10386         # non-volume-backed server and finishing (spawning) on the dest host,
10387         # we have to flatten the rbd image so we can delete the temporary
10388         # snapshot in the compute manager.
10389         mig_context = instance.migration_context
10390         cross_cell_move = (
10391                 mig_context and mig_context.is_cross_cell_move() or False)
10392         if instance.vm_state == vm_states.SHELVED_OFFLOADED or cross_cell_move:
10393             # NOTE(lyarwood): When using the rbd imagebackend the call to cache
10394             # above will attempt to clone from the shelved snapshot in Glance
10395             # if available from this compute. We then need to flatten the
10396             # resulting image to avoid it still referencing and ultimately
10397             # blocking the removal of the shelved snapshot at the end of the
10398             # unshelve. This is a no-op for all but the rbd imagebackend.
10399             action = (
10400                 'migrating instance across cells' if cross_cell_move
10401                 else 'unshelving instance')
10402             try:
10403                 image.flatten()
10404                 LOG.debug('Image %s flattened successfully while %s.',
10405                           image.path, action, instance=instance)
10406             except NotImplementedError:
10407                 # NOTE(lyarwood): There's an argument to be made for Logs
10408                 # our inability to call flatten here, however given this isn't
10409                 # implemented for most of the backends it may do more harm than
10410                 # good, concerning operators etc so for now just pass.
10411                 pass
10412 
10413     def _create_images_and_backing(self, context, instance, instance_dir,
10414                                    disk_info, fallback_from_host=None):
10415         """:param context: security context
10416            :param instance:
10417                nova.db.main.models.Instance object
10418                instance object that is migrated.
10419            :param instance_dir:
10420                instance path to use, calculated externally to handle block
10421                migrating an instance with an old style instance path
10422            :param disk_info:
10423                disk info specified in _get_instance_disk_info_from_config
10424                (list of dicts)
10425            :param fallback_from_host:
10426                host where we can retrieve images if the glance images are
10427                not available.
10428 
10429         """
10430 
10431         # Virtuozzo containers don't use backing file
10432         if (CONF.libvirt.virt_type == "parallels" and
10433                 instance.vm_mode == fields.VMMode.EXE):
10434             return
10435 
10436         if not disk_info:
10437             disk_info = []
10438 
10439         for info in disk_info:
10440             base = os.path.basename(info['path'])
10441             # Get image type and create empty disk image, and
10442             # create backing file in case of qcow2.
10443             instance_disk = os.path.join(instance_dir, base)
10444             if not info['backing_file'] and not os.path.exists(instance_disk):
10445                 libvirt_utils.create_image(info['type'], instance_disk,
10446                                            info['virt_disk_size'])
10447             elif info['backing_file']:
10448                 # Creating backing file follows same way as spawning instances.
10449                 cache_name = os.path.basename(info['backing_file'])
10450 
10451                 disk = self.image_backend.by_name(instance, instance_disk,
10452                                                   CONF.libvirt.images_type)
10453                 if cache_name.startswith('ephemeral'):
10454                     # The argument 'size' is used by image.cache to
10455                     # validate disk size retrieved from cache against
10456                     # the instance disk size (should always return OK)
10457                     # and ephemeral_size is used by _create_ephemeral
10458                     # to build the image if the disk is not already
10459                     # cached.
10460                     disk.cache(
10461                         fetch_func=self._create_ephemeral,
10462                         fs_label=cache_name,
10463                         os_type=instance.os_type,
10464                         filename=cache_name,
10465                         size=info['virt_disk_size'],
10466                         ephemeral_size=info['virt_disk_size'] / units.Gi)
10467                 elif cache_name.startswith('swap'):
10468                     flavor = instance.get_flavor()
10469                     swap_mb = flavor.swap
10470                     disk.cache(fetch_func=self._create_swap,
10471                                 filename="swap_%s" % swap_mb,
10472                                 size=swap_mb * units.Mi,
10473                                 swap_mb=swap_mb)
10474                 else:
10475                     self._try_fetch_image_cache(disk,
10476                                                 libvirt_utils.fetch_image,
10477                                                 context, cache_name,
10478                                                 instance.image_ref,
10479                                                 instance,
10480                                                 info['virt_disk_size'],
10481                                                 fallback_from_host)
10482 
10483         # if disk has kernel and ramdisk, just download
10484         # following normal way.
10485         self._fetch_instance_kernel_ramdisk(
10486             context, instance, fallback_from_host=fallback_from_host)
10487 
10488     def post_live_migration(self, context, instance, block_device_info,
10489                             migrate_data=None):
10490         # NOTE(mdbooth): The block_device_info we were passed was initialized
10491         # with BDMs from the source host before they were updated to point to
10492         # the destination. We can safely use this to disconnect the source
10493         # without re-fetching.
10494         block_device_mapping = driver.block_device_info_get_mapping(
10495                 block_device_info)
10496 
10497         for vol in block_device_mapping:
10498             connection_info = vol['connection_info']
10499             # NOTE(lyarwood): Ignore exceptions here to avoid the instance
10500             # being left in an ERROR state and still marked on the source.
10501             try:
10502                 self._disconnect_volume(context, connection_info, instance)
10503             except Exception:
10504                 volume_id = driver_block_device.get_volume_id(connection_info)
10505                 LOG.exception("Ignoring exception while attempting to "
10506                               "disconnect volume %s from the source host "
10507                               "during post_live_migration", volume_id,
10508                               instance=instance)
10509 
10510     def post_live_migration_at_source(self, context, instance, network_info):
10511         """Unplug VIFs from networks at source.
10512 
10513         :param context: security context
10514         :param instance: instance object reference
10515         :param network_info: instance network information
10516         """
10517         self.unplug_vifs(instance, network_info)
10518 
10519     def post_live_migration_at_destination(self, context,
10520                                            instance,
10521                                            network_info,
10522                                            block_migration=False,
10523                                            block_device_info=None):
10524         """Post operation of live migration at destination host.
10525 
10526         :param context: security context
10527         :param instance:
10528             nova.db.main.models.Instance object
10529             instance object that is migrated.
10530         :param network_info: instance network information
10531         :param block_migration: if true, post operation of block_migration.
10532         """
10533         self._reattach_instance_vifs(context, instance, network_info)
10534 
10535     def _get_instance_disk_info_from_config(self, guest_config,
10536                                             block_device_info):
10537         """Get the non-volume disk information from the domain xml
10538 
10539         :param LibvirtConfigGuest guest_config: the libvirt domain config
10540                                                 for the instance
10541         :param dict block_device_info: block device info for BDMs
10542         :returns disk_info: list of dicts with keys:
10543 
10544           * 'type': the disk type (str)
10545           * 'path': the disk path (str)
10546           * 'virt_disk_size': the virtual disk size (int)
10547           * 'backing_file': backing file of a disk image (str)
10548           * 'disk_size': physical disk size (int)
10549           * 'over_committed_disk_size': virt_disk_size - disk_size or 0
10550         """
10551         block_device_mapping = driver.block_device_info_get_mapping(
10552             block_device_info)
10553 
10554         volume_devices = set()
10555         for vol in block_device_mapping:
10556             disk_dev = vol['mount_device'].rpartition("/")[2]
10557             volume_devices.add(disk_dev)
10558 
10559         disk_info = []
10560 
10561         if (
10562             CONF.libvirt.virt_type == 'parallels' and
10563             guest_config.os_type == fields.VMMode.EXE
10564         ):
10565             node_type = 'filesystem'
10566         else:
10567             node_type = 'disk'
10568 
10569         for device in guest_config.devices:
10570             if device.root_name != node_type:
10571                 continue
10572             disk_type = device.source_type
10573             if device.root_name == 'filesystem':
10574                 target = device.target_dir
10575                 if device.source_type == 'file':
10576                     path = device.source_file
10577                 elif device.source_type == 'block':
10578                     path = device.source_dev
10579                 else:
10580                     path = None
10581             else:
10582                 target = device.target_dev
10583                 path = device.source_path
10584 
10585             if not path:
10586                 LOG.debug('skipping disk for %s as it does not have a path',
10587                           guest_config.name)
10588                 continue
10589 
10590             if disk_type not in ['file', 'block']:
10591                 LOG.debug('skipping disk because it looks like a volume', path)
10592                 continue
10593 
10594             if target in volume_devices:
10595                 LOG.debug('skipping disk %(path)s (%(target)s) as it is a '
10596                           'volume', {'path': path, 'target': target})
10597                 continue
10598 
10599             if device.root_name == 'filesystem':
10600                 driver_type = device.driver_type
10601             else:
10602                 driver_type = device.driver_format
10603             # get the real disk size or
10604             # raise a localized error if image is unavailable
10605             if disk_type == 'file' and driver_type == 'ploop':
10606                 dk_size = 0
10607                 for dirpath, dirnames, filenames in os.walk(path):
10608                     for f in filenames:
10609                         fp = os.path.join(dirpath, f)
10610                         dk_size += os.path.getsize(fp)
10611                 qemu_img_info = disk_api.get_disk_info(path)
10612                 virt_size = qemu_img_info.virtual_size
10613                 backing_file = libvirt_utils.get_disk_backing_file(path)
10614                 over_commit_size = int(virt_size) - dk_size
10615 
10616             elif disk_type == 'file' and driver_type == 'qcow2':
10617                 qemu_img_info = disk_api.get_disk_info(path)
10618                 dk_size = qemu_img_info.disk_size
10619                 virt_size = qemu_img_info.virtual_size
10620                 backing_file = libvirt_utils.get_disk_backing_file(path)
10621                 over_commit_size = max(0, int(virt_size) - dk_size)
10622 
10623             elif disk_type == 'file':
10624                 dk_size = os.stat(path).st_blocks * 512
10625                 virt_size = os.path.getsize(path)
10626                 backing_file = ""
10627                 over_commit_size = int(virt_size) - dk_size
10628 
10629             elif disk_type == 'block' and block_device_info:
10630                 dk_size = lvm.get_volume_size(path)
10631                 virt_size = dk_size
10632                 backing_file = ""
10633                 over_commit_size = 0
10634 
10635             else:
10636                 LOG.debug('skipping disk %(path)s (%(target)s) - unable to '
10637                           'determine if volume',
10638                           {'path': path, 'target': target})
10639                 continue
10640 
10641             disk_info.append({'type': driver_type,
10642                               'path': path,
10643                               'virt_disk_size': virt_size,
10644                               'backing_file': backing_file,
10645                               'disk_size': dk_size,
10646                               'over_committed_disk_size': over_commit_size})
10647         return disk_info
10648 
10649     def _get_instance_disk_info(self, instance, block_device_info):
10650         try:
10651             guest = self._host.get_guest(instance)
10652             config = guest.get_config()
10653         except libvirt.libvirtError as ex:
10654             error_code = ex.get_error_code()
10655             LOG.warning('Error from libvirt while getting description of '
10656                         '%(instance_name)s: [Error Code %(error_code)s] '
10657                         '%(ex)s',
10658                         {'instance_name': instance.name,
10659                          'error_code': error_code,
10660                          'ex': encodeutils.exception_to_unicode(ex)},
10661                         instance=instance)
10662             raise exception.InstanceNotFound(instance_id=instance.uuid)
10663 
10664         return self._get_instance_disk_info_from_config(config,
10665                                                         block_device_info)
10666 
10667     def get_instance_disk_info(self, instance,
10668                                block_device_info=None):
10669         return jsonutils.dumps(
10670             self._get_instance_disk_info(instance, block_device_info))
10671 
10672     def _get_disk_over_committed_size_total(self):
10673         """Return total over committed disk size for all instances."""
10674         # Disk size that all instance uses : virtual_size - disk_size
10675         disk_over_committed_size = 0
10676         instance_domains = self._host.list_instance_domains(only_running=False)
10677         if not instance_domains:
10678             return disk_over_committed_size
10679 
10680         # Get all instance uuids
10681         instance_uuids = [dom.UUIDString() for dom in instance_domains]
10682         ctx = nova_context.get_admin_context()
10683         # Get instance object list by uuid filter
10684         filters = {'uuid': instance_uuids}
10685         # NOTE(ankit): objects.InstanceList.get_by_filters method is
10686         # getting called twice one is here and another in the
10687         # _update_available_resource method of resource_tracker. Since
10688         # _update_available_resource method is synchronized, there is a
10689         # possibility the instances list retrieved here to calculate
10690         # disk_over_committed_size would differ to the list you would get
10691         # in _update_available_resource method for calculating usages based
10692         # on instance utilization.
10693         local_instance_list = objects.InstanceList.get_by_filters(
10694             ctx, filters, use_slave=True)
10695         # Convert instance list to dictionary with instance uuid as key.
10696         local_instances = {inst.uuid: inst for inst in local_instance_list}
10697 
10698         # Get bdms by instance uuids
10699         bdms = objects.BlockDeviceMappingList.bdms_by_instance_uuid(
10700             ctx, instance_uuids)
10701 
10702         for dom in instance_domains:
10703             try:
10704                 guest = libvirt_guest.Guest(dom)
10705                 config = guest.get_config()
10706 
10707                 block_device_info = None
10708                 if guest.uuid in local_instances \
10709                         and (bdms and guest.uuid in bdms):
10710                     # Get block device info for instance
10711                     block_device_info = driver.get_block_device_info(
10712                         local_instances[guest.uuid], bdms[guest.uuid])
10713 
10714                 disk_infos = self._get_instance_disk_info_from_config(
10715                     config, block_device_info)
10716                 if not disk_infos:
10717                     continue
10718 
10719                 for info in disk_infos:
10720                     disk_over_committed_size += int(
10721                         info['over_committed_disk_size'])
10722             except libvirt.libvirtError as ex:
10723                 error_code = ex.get_error_code()
10724                 LOG.warning(
10725                     'Error from libvirt while getting description of '
10726                     '%(instance_name)s: [Error Code %(error_code)s] %(ex)s',
10727                     {'instance_name': guest.name,
10728                      'error_code': error_code,
10729                      'ex': encodeutils.exception_to_unicode(ex)})
10730             except OSError as e:
10731                 if e.errno in (errno.ENOENT, errno.ESTALE):
10732                     LOG.warning('Periodic task is updating the host stat, '
10733                                 'it is trying to get disk %(i_name)s, '
10734                                 'but disk file was removed by concurrent '
10735                                 'operations such as resize.',
10736                                 {'i_name': guest.name})
10737                 elif e.errno == errno.EACCES:
10738                     LOG.warning('Periodic task is updating the host stat, '
10739                                 'it is trying to get disk %(i_name)s, '
10740                                 'but access is denied. It is most likely '
10741                                 'due to a VM that exists on the compute '
10742                                 'node but is not managed by Nova.',
10743                                 {'i_name': guest.name})
10744                 else:
10745                     raise
10746             except (exception.VolumeBDMPathNotFound,
10747                     exception.DiskNotFound) as e:
10748                 if isinstance(e, exception.VolumeBDMPathNotFound):
10749                     thing = 'backing volume block device'
10750                 elif isinstance(e, exception.DiskNotFound):
10751                     thing = 'backing disk storage'
10752 
10753                 LOG.warning('Periodic task is updating the host stats, '
10754                             'it is trying to get disk info for %(i_name)s, '
10755                             'but the %(thing)s was removed by a concurrent '
10756                             'operation such as resize. Error: %(error)s',
10757                             {'i_name': guest.name, 'thing': thing, 'error': e})
10758 
10759             # NOTE(gtt116): give other tasks a chance.
10760             greenthread.sleep(0)
10761         return disk_over_committed_size
10762 
10763     def get_available_nodes(self, refresh=False):
10764         return [self._host.get_hostname()]
10765 
10766     def get_host_cpu_stats(self):
10767         """Return the current CPU state of the host."""
10768         return self._host.get_cpu_stats()
10769 
10770     def get_host_uptime(self):
10771         """Returns the result of calling "uptime"."""
10772         out, err = processutils.execute('env', 'LANG=C', 'uptime')
10773         return out
10774 
10775     def manage_image_cache(self, context, all_instances):
10776         """Manage the local cache of images."""
10777         self.image_cache_manager.update(context, all_instances)
10778 
10779     def _cleanup_remote_migration(self, dest, inst_base, inst_base_resize,
10780                                   shared_storage=False):
10781         """Used only for cleanup in case migrate_disk_and_power_off fails."""
10782         try:
10783             if os.path.exists(inst_base_resize):
10784                 shutil.rmtree(inst_base, ignore_errors=True)
10785                 os.rename(inst_base_resize, inst_base)
10786                 if not shared_storage:
10787                     self._remotefs.remove_dir(dest, inst_base)
10788         except Exception:
10789             pass
10790 
10791     def cache_image(self, context, image_id):
10792         cache_dir = os.path.join(CONF.instances_path,
10793                                  CONF.image_cache.subdirectory_name)
10794         path = os.path.join(cache_dir,
10795                             imagecache.get_cache_fname(image_id))
10796         if os.path.exists(path):
10797             LOG.info('Image %(image_id)s already cached; updating timestamp',
10798                      {'image_id': image_id})
10799             # NOTE(danms): The regular image cache routines use a wrapper
10800             # (_update_utime_ignore_eacces()) around this to avoid failing
10801             # on permissions (which may or may not be legit due to an NFS
10802             # race). However, since this is best-effort, errors are swallowed
10803             # by compute manager per-image, and we are compelled to report
10804             # errors up our stack, we use the raw method here to avoid the
10805             # silent ignore of the EACCESS.
10806             nova.privsep.path.utime(path)
10807             return False
10808         else:
10809             # NOTE(danms): In case we are running before the first boot, make
10810             # sure the cache directory is created
10811             if not os.path.isdir(cache_dir):
10812                 fileutils.ensure_tree(cache_dir)
10813             LOG.info('Caching image %(image_id)s by request',
10814                      {'image_id': image_id})
10815             # NOTE(danms): The imagebackend code, as called via spawn() where
10816             # images are normally cached, uses a lock on the root disk it is
10817             # creating at the time, but relies on the
10818             # compute_utils.disk_ops_semaphore for cache fetch mutual
10819             # exclusion, which is grabbed in images.fetch() (which is called
10820             # by images.fetch_to_raw() below). So, by calling fetch_to_raw(),
10821             # we are sharing the same locking for the cache fetch as the
10822             # rest of the code currently called only from spawn().
10823             images.fetch_to_raw(context, image_id, path)
10824             return True
10825 
10826     def _get_disk_size_reserved_for_image_cache(self):
10827         """Return the amount of DISK_GB resource need to be reserved for the
10828         image cache.
10829 
10830         :returns: The disk space in GB
10831         """
10832         if not CONF.workarounds.reserve_disk_resource_for_image_cache:
10833             return 0
10834 
10835         return compute_utils.convert_mb_to_ceil_gb(
10836             self.image_cache_manager.get_disk_usage() / 1024.0 / 1024.0)
10837 
10838     def _is_path_shared_with(self, dest, path):
10839         # NOTE (rmk): There are two methods of determining whether we are
10840         #             on the same filesystem: the source and dest IP are the
10841         #             same, or we create a file on the dest system via SSH
10842         #             and check whether the source system can also see it.
10843         shared_path = (dest == self.get_host_ip_addr())
10844         if not shared_path:
10845             tmp_file = uuidutils.generate_uuid(dashed=False) + '.tmp'
10846             tmp_path = os.path.join(path, tmp_file)
10847 
10848             try:
10849                 self._remotefs.create_file(dest, tmp_path)
10850                 if os.path.exists(tmp_path):
10851                     shared_path = True
10852                     os.unlink(tmp_path)
10853                 else:
10854                     self._remotefs.remove_file(dest, tmp_path)
10855             except Exception:
10856                 pass
10857         return shared_path
10858 
10859     def migrate_disk_and_power_off(self, context, instance, dest,
10860                                    flavor, network_info,
10861                                    block_device_info=None,
10862                                    timeout=0, retry_interval=0):
10863         LOG.debug("Starting migrate_disk_and_power_off",
10864                    instance=instance)
10865 
10866         ephemerals = driver.block_device_info_get_ephemerals(block_device_info)
10867 
10868         # get_bdm_ephemeral_disk_size() will return 0 if the new
10869         # instance's requested block device mapping contain no
10870         # ephemeral devices. However, we still want to check if
10871         # the original instance's ephemeral_gb property was set and
10872         # ensure that the new requested flavor ephemeral size is greater
10873         eph_size = (block_device.get_bdm_ephemeral_disk_size(ephemerals) or
10874                     instance.flavor.ephemeral_gb)
10875 
10876         # Checks if the migration needs a disk resize down.
10877         root_down = flavor.root_gb < instance.flavor.root_gb
10878         ephemeral_down = flavor.ephemeral_gb < eph_size
10879         booted_from_volume = self._is_booted_from_volume(block_device_info)
10880 
10881         if (root_down and not booted_from_volume) or ephemeral_down:
10882             reason = _("Unable to resize disk down.")
10883             raise exception.InstanceFaultRollback(
10884                 exception.ResizeError(reason=reason))
10885 
10886         # NOTE(dgenin): Migration is not implemented for LVM backed instances.
10887         if CONF.libvirt.images_type == 'lvm' and not booted_from_volume:
10888             reason = _("Migration is not supported for LVM backed instances")
10889             raise exception.InstanceFaultRollback(
10890                 exception.MigrationPreCheckError(reason=reason))
10891 
10892         # copy disks to destination
10893         # rename instance dir to +_resize at first for using
10894         # shared storage for instance dir (eg. NFS).
10895         inst_base = libvirt_utils.get_instance_path(instance)
10896         inst_base_resize = inst_base + "_resize"
10897         shared_instance_path = self._is_path_shared_with(dest, inst_base)
10898 
10899         # try to create the directory on the remote compute node
10900         # if this fails we pass the exception up the stack so we can catch
10901         # failures here earlier
10902         if not shared_instance_path:
10903             try:
10904                 self._remotefs.create_dir(dest, inst_base)
10905             except processutils.ProcessExecutionError as e:
10906                 reason = _("not able to execute ssh command: %s") % e
10907                 raise exception.InstanceFaultRollback(
10908                     exception.ResizeError(reason=reason))
10909 
10910         self.power_off(instance, timeout, retry_interval)
10911         self.unplug_vifs(instance, network_info)
10912         block_device_mapping = driver.block_device_info_get_mapping(
10913             block_device_info)
10914         for vol in block_device_mapping:
10915             connection_info = vol['connection_info']
10916             self._disconnect_volume(context, connection_info, instance)
10917 
10918         disk_info = self._get_instance_disk_info(instance, block_device_info)
10919 
10920         try:
10921             os.rename(inst_base, inst_base_resize)
10922             # if we are migrating the instance with shared instance path then
10923             # create the directory.  If it is a remote node the directory
10924             # has already been created
10925             if shared_instance_path:
10926                 dest = None
10927                 fileutils.ensure_tree(inst_base)
10928 
10929             on_execute = lambda process: \
10930                 self.job_tracker.add_job(instance, process.pid)
10931             on_completion = lambda process: \
10932                 self.job_tracker.remove_job(instance, process.pid)
10933 
10934             for info in disk_info:
10935                 # assume inst_base == dirname(info['path'])
10936                 img_path = info['path']
10937                 fname = os.path.basename(img_path)
10938                 from_path = os.path.join(inst_base_resize, fname)
10939 
10940                 # We will not copy over the swap disk here, and rely on
10941                 # finish_migration to re-create it for us. This is ok because
10942                 # the OS is shut down, and as recreating a swap disk is very
10943                 # cheap it is more efficient than copying either locally or
10944                 # over the network. This also means we don't have to resize it.
10945                 if fname == 'disk.swap':
10946                     continue
10947 
10948                 compression = info['type'] not in NO_COMPRESSION_TYPES
10949                 libvirt_utils.copy_image(from_path, img_path, host=dest,
10950                                          on_execute=on_execute,
10951                                          on_completion=on_completion,
10952                                          compression=compression)
10953 
10954             # Ensure disk.info is written to the new path to avoid disks being
10955             # reinspected and potentially changing format.
10956             src_disk_info_path = os.path.join(inst_base_resize, 'disk.info')
10957             if os.path.exists(src_disk_info_path):
10958                 dst_disk_info_path = os.path.join(inst_base, 'disk.info')
10959                 libvirt_utils.copy_image(src_disk_info_path,
10960                                          dst_disk_info_path,
10961                                          host=dest, on_execute=on_execute,
10962                                          on_completion=on_completion)
10963 
10964             # Handle migration of vTPM data if needed
10965             libvirt_utils.save_and_migrate_vtpm_dir(
10966                 instance.uuid, inst_base_resize, inst_base, dest,
10967                 on_execute, on_completion)
10968 
10969         except Exception:
10970             with excutils.save_and_reraise_exception():
10971                 self._cleanup_remote_migration(dest, inst_base,
10972                                                inst_base_resize,
10973                                                shared_instance_path)
10974 
10975         return jsonutils.dumps(disk_info)
10976 
10977     def _wait_for_running(self, instance):
10978         state = self.get_info(instance).state
10979 
10980         if state == power_state.RUNNING:
10981             LOG.info("Instance running successfully.", instance=instance)
10982             raise loopingcall.LoopingCallDone()
10983 
10984     @staticmethod
10985     def _disk_raw_to_qcow2(path):
10986         """Converts a raw disk to qcow2."""
10987         path_qcow = path + '_qcow'
10988         images.convert_image(path, path_qcow, 'raw', 'qcow2')
10989         os.rename(path_qcow, path)
10990 
10991     def _finish_migration_vtpm(
10992         self,
10993         context: nova_context.RequestContext,
10994         instance: 'objects.Instance',
10995     ) -> None:
10996         """Handle vTPM when migrating or resizing an instance.
10997 
10998         Handle the case where we're resizing between different versions of TPM,
10999         or enabling/disabling TPM.
11000         """
11001         old_vtpm_config = hardware.get_vtpm_constraint(
11002             instance.old_flavor, instance.image_meta)
11003         new_vtpm_config = hardware.get_vtpm_constraint(
11004             instance.new_flavor, instance.image_meta)
11005 
11006         if old_vtpm_config:
11007             # we had a vTPM in the old flavor; figure out if we need to do
11008             # anything with it
11009             inst_base = libvirt_utils.get_instance_path(instance)
11010             swtpm_dir = os.path.join(inst_base, 'swtpm', instance.uuid)
11011             copy_swtpm_dir = True
11012 
11013             if old_vtpm_config != new_vtpm_config:
11014                 # we had vTPM in the old flavor but the new flavor either
11015                 # doesn't or has different config; delete old TPM data and let
11016                 # libvirt create new data
11017                 if os.path.exists(swtpm_dir):
11018                     LOG.info(
11019                         'Old flavor and new flavor have different vTPM '
11020                         'configuration; removing existing vTPM data.')
11021                     copy_swtpm_dir = False
11022                     shutil.rmtree(swtpm_dir)
11023 
11024             # apparently shutil.rmtree() isn't reliable on NFS so don't rely
11025             # only on path existance here.
11026             if copy_swtpm_dir and os.path.exists(swtpm_dir):
11027                 libvirt_utils.restore_vtpm_dir(swtpm_dir)
11028         elif new_vtpm_config:
11029             # we've requested vTPM in the new flavor and didn't have one
11030             # previously so we need to create a new secret
11031             crypto.ensure_vtpm_secret(context, instance)
11032 
11033     def finish_migration(
11034         self,
11035         context: nova_context.RequestContext,
11036         migration: 'objects.Migration',
11037         instance: 'objects.Instance',
11038         disk_info: str,
11039         network_info: network_model.NetworkInfo,
11040         image_meta: 'objects.ImageMeta',
11041         resize_instance: bool,
11042         allocations: ty.Dict[str, ty.Any],
11043         block_device_info: ty.Optional[ty.Dict[str, ty.Any]] = None,
11044         power_on: bool = True,
11045     ) -> None:
11046         """Complete the migration process on the destination host."""
11047         LOG.debug("Starting finish_migration", instance=instance)
11048 
11049         block_disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
11050                                                   instance,
11051                                                   image_meta,
11052                                                   block_device_info)
11053         # assume _create_image does nothing if a target file exists.
11054         # NOTE: This has the intended side-effect of fetching a missing
11055         # backing file.
11056         self._create_image(context, instance, block_disk_info['mapping'],
11057                            block_device_info=block_device_info,
11058                            ignore_bdi_for_swap=True,
11059                            fallback_from_host=migration.source_compute)
11060 
11061         # Required by Quobyte CI
11062         self._ensure_console_log_for_instance(instance)
11063 
11064         gen_confdrive = functools.partial(
11065             self._create_configdrive, context, instance,
11066             InjectionInfo(admin_pass=None, network_info=network_info,
11067                           files=None))
11068 
11069         # Convert raw disks to qcow2 if migrating to host which uses
11070         # qcow2 from host which uses raw.
11071         for info in jsonutils.loads(disk_info):
11072             path = info['path']
11073             disk_name = os.path.basename(path)
11074 
11075             # NOTE(mdbooth): The code below looks wrong, but is actually
11076             # required to prevent a security hole when migrating from a host
11077             # with use_cow_images=False to one with use_cow_images=True.
11078             # Imagebackend uses use_cow_images to select between the
11079             # atrociously-named-Raw and Qcow2 backends. The Qcow2 backend
11080             # writes to disk.info, but does not read it as it assumes qcow2.
11081             # Therefore if we don't convert raw to qcow2 here, a raw disk will
11082             # be incorrectly assumed to be qcow2, which is a severe security
11083             # flaw. The reverse is not true, because the atrociously-named-Raw
11084             # backend supports both qcow2 and raw disks, and will choose
11085             # appropriately between them as long as disk.info exists and is
11086             # correctly populated, which it is because Qcow2 writes to
11087             # disk.info.
11088             #
11089             # In general, we do not yet support format conversion during
11090             # migration. For example:
11091             #   * Converting from use_cow_images=True to use_cow_images=False
11092             #     isn't handled. This isn't a security bug, but is almost
11093             #     certainly buggy in other cases, as the 'Raw' backend doesn't
11094             #     expect a backing file.
11095             #   * Converting to/from lvm and rbd backends is not supported.
11096             #
11097             # This behaviour is inconsistent, and therefore undesirable for
11098             # users. It is tightly-coupled to implementation quirks of 2
11099             # out of 5 backends in imagebackend and defends against a severe
11100             # security flaw which is not at all obvious without deep analysis,
11101             # and is therefore undesirable to developers. We should aim to
11102             # remove it. This will not be possible, though, until we can
11103             # represent the storage layout of a specific instance
11104             # independent of the default configuration of the local compute
11105             # host.
11106 
11107             # Config disks are hard-coded to be raw even when
11108             # use_cow_images=True (see _get_disk_config_image_type),so don't
11109             # need to be converted.
11110             if (disk_name != 'disk.config' and
11111                         info['type'] == 'raw' and CONF.use_cow_images):
11112                 self._disk_raw_to_qcow2(info['path'])
11113 
11114         # Does the guest need to be assigned some vGPU mediated devices ?
11115         mdevs = self._allocate_mdevs(allocations)
11116 
11117         # Handle the case where the guest has emulated TPM
11118         self._finish_migration_vtpm(context, instance)
11119 
11120         xml = self._get_guest_xml(context, instance, network_info,
11121                                   block_disk_info, image_meta,
11122                                   block_device_info=block_device_info,
11123                                   mdevs=mdevs)
11124         # NOTE(mriedem): vifs_already_plugged=True here, regardless of whether
11125         # or not we've migrated to another host, because we unplug VIFs locally
11126         # and the status change in the port might go undetected by the neutron
11127         # L2 agent (or neutron server) so neutron may not know that the VIF was
11128         # unplugged in the first place and never send an event.
11129         guest = self._create_guest_with_network(
11130             context, xml, instance, network_info, block_device_info,
11131             power_on=power_on, vifs_already_plugged=True,
11132             post_xml_callback=gen_confdrive)
11133         if power_on:
11134             timer = loopingcall.FixedIntervalLoopingCall(
11135                                                     self._wait_for_running,
11136                                                     instance)
11137             timer.start(interval=0.5).wait()
11138 
11139             # Sync guest time after migration.
11140             guest.sync_guest_time()
11141 
11142         LOG.debug("finish_migration finished successfully.", instance=instance)
11143 
11144     def _cleanup_failed_migration(self, inst_base):
11145         """Make sure that a failed migrate doesn't prevent us from rolling
11146         back in a revert.
11147         """
11148         try:
11149             shutil.rmtree(inst_base)
11150         except OSError as e:
11151             if e.errno != errno.ENOENT:
11152                 raise
11153 
11154     def _finish_revert_migration_vtpm(
11155         self,
11156         context: nova_context.RequestContext,
11157         instance: 'objects.Instance',
11158     ) -> None:
11159         """Handle vTPM differences when reverting a migration or resize.
11160 
11161         We should either restore any emulated vTPM persistent storage files or
11162         create new ones.
11163         """
11164         old_vtpm_config = hardware.get_vtpm_constraint(
11165             instance.old_flavor, instance.image_meta)
11166         new_vtpm_config = hardware.get_vtpm_constraint(
11167             instance.new_flavor, instance.image_meta)
11168 
11169         if old_vtpm_config:
11170             # the instance had a vTPM before resize and should have one again;
11171             # move the previously-saved vTPM data back to its proper location
11172             inst_base = libvirt_utils.get_instance_path(instance)
11173             swtpm_dir = os.path.join(inst_base, 'swtpm', instance.uuid)
11174             if os.path.exists(swtpm_dir):
11175                 libvirt_utils.restore_vtpm_dir(swtpm_dir)
11176         elif new_vtpm_config:
11177             # the instance gained a vTPM and must now lose it; delete the vTPM
11178             # secret, knowing that libvirt will take care of everything else on
11179             # the destination side
11180             crypto.delete_vtpm_secret(context, instance)
11181 
11182     def finish_revert_migration(
11183         self,
11184         context: nova.context.RequestContext,
11185         instance: 'objects.Instance',
11186         network_info: network_model.NetworkInfo,
11187         migration: 'objects.Migration',
11188         block_device_info: ty.Optional[ty.Dict[str, ty.Any]] = None,
11189         power_on: bool = True,
11190     ) -> None:
11191         """Finish the second half of reverting a resize on the source host."""
11192         LOG.debug('Starting finish_revert_migration', instance=instance)
11193 
11194         inst_base = libvirt_utils.get_instance_path(instance)
11195         inst_base_resize = inst_base + "_resize"
11196 
11197         # NOTE(danms): if we're recovering from a failed migration,
11198         # make sure we don't have a left-over same-host base directory
11199         # that would conflict. Also, don't fail on the rename if the
11200         # failure happened early.
11201         if os.path.exists(inst_base_resize):
11202             self._cleanup_failed_migration(inst_base)
11203             os.rename(inst_base_resize, inst_base)
11204 
11205         root_disk = self.image_backend.by_name(instance, 'disk')
11206         # Once we rollback, the snapshot is no longer needed, so remove it
11207         if root_disk.exists():
11208             root_disk.rollback_to_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME)
11209             root_disk.remove_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME)
11210 
11211         self._finish_revert_migration_vtpm(context, instance)
11212 
11213         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
11214                                             instance,
11215                                             instance.image_meta,
11216                                             block_device_info)
11217 
11218         # The guest could already have mediated devices, using them for
11219         # the new XML
11220         mdevs = list(self._get_all_assigned_mediated_devices(instance))
11221 
11222         xml = self._get_guest_xml(context, instance, network_info, disk_info,
11223                                   instance.image_meta,
11224                                   block_device_info=block_device_info,
11225                                   mdevs=mdevs)
11226         # NOTE(artom) In some Neutron or port configurations we've already
11227         # waited for vif-plugged events in the compute manager's
11228         # _finish_revert_resize_network_migrate_finish(), right after updating
11229         # the port binding. For any ports not covered by those "bind-time"
11230         # events, we wait for "plug-time" events here.
11231         events = network_info.get_plug_time_events(migration)
11232         if events:
11233             LOG.debug('Instance is using plug-time events: %s', events,
11234                       instance=instance)
11235         self._create_guest_with_network(
11236             context, xml, instance, network_info, block_device_info,
11237             power_on=power_on, external_events=events)
11238 
11239         if power_on:
11240             timer = loopingcall.FixedIntervalLoopingCall(
11241                                                     self._wait_for_running,
11242                                                     instance)
11243             timer.start(interval=0.5).wait()
11244 
11245         LOG.debug("finish_revert_migration finished successfully.",
11246                   instance=instance)
11247 
11248     def confirm_migration(self, context, migration, instance, network_info):
11249         """Confirms a resize, destroying the source VM."""
11250         self._cleanup_resize(context, instance, network_info)
11251 
11252     @staticmethod
11253     def _get_io_devices(xml_doc):
11254         """get the list of io devices from the xml document."""
11255         result: ty.Dict[str, ty.List[str]] = {"volumes": [], "ifaces": []}
11256         try:
11257             doc = etree.fromstring(xml_doc)
11258         except Exception:
11259             return result
11260         blocks = [('./devices/disk', 'volumes'),
11261             ('./devices/interface', 'ifaces')]
11262         for block, key in blocks:
11263             section = doc.findall(block)
11264             for node in section:
11265                 for child in node:
11266                     if child.tag == 'target' and child.get('dev'):
11267                         result[key].append(child.get('dev'))
11268         return result
11269 
11270     def get_diagnostics(self, instance):
11271         guest = self._host.get_guest(instance)
11272 
11273         # TODO(sahid): We are converting all calls from a
11274         # virDomain object to use nova.virt.libvirt.Guest.
11275         # We should be able to remove domain at the end.
11276         domain = guest._domain
11277         output = {}
11278         # get cpu time, might launch an exception if the method
11279         # is not supported by the underlying hypervisor being
11280         # used by libvirt
11281         try:
11282             for vcpu in guest.get_vcpus_info():
11283                 output["cpu" + str(vcpu.id) + "_time"] = vcpu.time
11284         except libvirt.libvirtError:
11285             pass
11286         # get io status
11287         xml = guest.get_xml_desc()
11288         dom_io = LibvirtDriver._get_io_devices(xml)
11289         for guest_disk in dom_io["volumes"]:
11290             try:
11291                 # blockStats might launch an exception if the method
11292                 # is not supported by the underlying hypervisor being
11293                 # used by libvirt
11294                 stats = domain.blockStats(guest_disk)
11295                 output[guest_disk + "_read_req"] = stats[0]
11296                 output[guest_disk + "_read"] = stats[1]
11297                 output[guest_disk + "_write_req"] = stats[2]
11298                 output[guest_disk + "_write"] = stats[3]
11299                 output[guest_disk + "_errors"] = stats[4]
11300             except libvirt.libvirtError:
11301                 pass
11302         for interface in dom_io["ifaces"]:
11303             try:
11304                 # interfaceStats might launch an exception if the method
11305                 # is not supported by the underlying hypervisor being
11306                 # used by libvirt
11307                 stats = domain.interfaceStats(interface)
11308                 output[interface + "_rx"] = stats[0]
11309                 output[interface + "_rx_packets"] = stats[1]
11310                 output[interface + "_rx_errors"] = stats[2]
11311                 output[interface + "_rx_drop"] = stats[3]
11312                 output[interface + "_tx"] = stats[4]
11313                 output[interface + "_tx_packets"] = stats[5]
11314                 output[interface + "_tx_errors"] = stats[6]
11315                 output[interface + "_tx_drop"] = stats[7]
11316             except libvirt.libvirtError:
11317                 pass
11318         output["memory"] = domain.maxMemory()
11319         # memoryStats might launch an exception if the method
11320         # is not supported by the underlying hypervisor being
11321         # used by libvirt
11322         try:
11323             mem = domain.memoryStats()
11324             for key in mem.keys():
11325                 output["memory-" + key] = mem[key]
11326         except (libvirt.libvirtError, AttributeError):
11327             pass
11328         return output
11329 
11330     def get_instance_diagnostics(self, instance):
11331         guest = self._host.get_guest(instance)
11332 
11333         # TODO(sahid): We are converting all calls from a
11334         # virDomain object to use nova.virt.libvirt.Guest.
11335         # We should be able to remove domain at the end.
11336         domain = guest._domain
11337 
11338         xml = guest.get_xml_desc()
11339         xml_doc = etree.fromstring(xml)
11340 
11341         # TODO(sahid): Needs to use get_info but more changes have to
11342         # be done since a mapping STATE_MAP LIBVIRT_POWER_STATE is
11343         # needed.
11344         state, max_mem, mem, num_cpu, cpu_time = guest._get_domain_info()
11345         config_drive = configdrive.required_by(instance)
11346         launched_at = timeutils.normalize_time(instance.launched_at)
11347         uptime = timeutils.delta_seconds(launched_at,
11348                                          timeutils.utcnow())
11349         diags = diagnostics_obj.Diagnostics(state=power_state.STATE_MAP[state],
11350                                         driver='libvirt',
11351                                         config_drive=config_drive,
11352                                         hypervisor=CONF.libvirt.virt_type,
11353                                         hypervisor_os='linux',
11354                                         uptime=uptime)
11355         diags.memory_details = diagnostics_obj.MemoryDiagnostics(
11356             maximum=max_mem / units.Mi,
11357             used=mem / units.Mi)
11358 
11359         # get cpu time, might launch an exception if the method
11360         # is not supported by the underlying hypervisor being
11361         # used by libvirt
11362         try:
11363             for vcpu in guest.get_vcpus_info():
11364                 diags.add_cpu(id=vcpu.id, time=vcpu.time)
11365         except libvirt.libvirtError:
11366             pass
11367         # get io status
11368         dom_io = LibvirtDriver._get_io_devices(xml)
11369         for guest_disk in dom_io["volumes"]:
11370             try:
11371                 # blockStats might launch an exception if the method
11372                 # is not supported by the underlying hypervisor being
11373                 # used by libvirt
11374                 stats = domain.blockStats(guest_disk)
11375                 diags.add_disk(read_bytes=stats[1],
11376                                read_requests=stats[0],
11377                                write_bytes=stats[3],
11378                                write_requests=stats[2],
11379                                errors_count=stats[4])
11380             except libvirt.libvirtError:
11381                 pass
11382 
11383         for interface in xml_doc.findall('./devices/interface'):
11384             mac_address = interface.find('mac').get('address')
11385             target = interface.find('./target')
11386 
11387             # add nic that has no target (therefore no stats)
11388             if target is None:
11389                 diags.add_nic(mac_address=mac_address)
11390                 continue
11391 
11392             # add nic with stats
11393             dev = target.get('dev')
11394             try:
11395                 if dev:
11396                     # interfaceStats might launch an exception if the
11397                     # method is not supported by the underlying hypervisor
11398                     # being used by libvirt
11399                     stats = domain.interfaceStats(dev)
11400                     diags.add_nic(mac_address=mac_address,
11401                                   rx_octets=stats[0],
11402                                   rx_errors=stats[2],
11403                                   rx_drop=stats[3],
11404                                   rx_packets=stats[1],
11405                                   tx_octets=stats[4],
11406                                   tx_errors=stats[6],
11407                                   tx_drop=stats[7],
11408                                   tx_packets=stats[5])
11409 
11410             except libvirt.libvirtError:
11411                 pass
11412 
11413         return diags
11414 
11415     @staticmethod
11416     def _prepare_device_bus(dev):
11417         """Determines the device bus and its hypervisor assigned address
11418         """
11419         bus = None
11420         address = (dev.device_addr.format_address() if
11421                    dev.device_addr else None)
11422         if isinstance(dev.device_addr,
11423                       vconfig.LibvirtConfigGuestDeviceAddressPCI):
11424             bus = objects.PCIDeviceBus()
11425         elif isinstance(dev, vconfig.LibvirtConfigGuestDisk):
11426             if dev.target_bus == 'scsi':
11427                 bus = objects.SCSIDeviceBus()
11428             elif dev.target_bus == 'ide':
11429                 bus = objects.IDEDeviceBus()
11430             elif dev.target_bus == 'usb':
11431                 bus = objects.USBDeviceBus()
11432         if address is not None and bus is not None:
11433             bus.address = address
11434         return bus
11435 
11436     def _build_interface_metadata(self, dev, vifs_to_expose, vlans_by_mac,
11437                                   trusted_by_mac):
11438         """Builds a metadata object for a network interface
11439 
11440         :param dev: The LibvirtConfigGuestInterface to build metadata for.
11441         :param vifs_to_expose: The list of tagged and/or vlan'ed
11442                                VirtualInterface objects.
11443         :param vlans_by_mac: A dictionary of mac address -> vlan associations.
11444         :param trusted_by_mac: A dictionary of mac address -> vf_trusted
11445                                associations.
11446         :return: A NetworkInterfaceMetadata object, or None.
11447         """
11448         vif = vifs_to_expose.get(dev.mac_addr)
11449         if not vif:
11450             LOG.debug('No VIF found with MAC %s, not building metadata',
11451                       dev.mac_addr)
11452             return None
11453         bus = self._prepare_device_bus(dev)
11454         device = objects.NetworkInterfaceMetadata(mac=vif.address)
11455         if 'tag' in vif and vif.tag:
11456             device.tags = [vif.tag]
11457         if bus:
11458             device.bus = bus
11459         vlan = vlans_by_mac.get(vif.address)
11460         if vlan:
11461             device.vlan = int(vlan)
11462         device.vf_trusted = trusted_by_mac.get(vif.address, False)
11463         return device
11464 
11465     def _build_disk_metadata(self, dev, tagged_bdms):
11466         """Builds a metadata object for a disk
11467 
11468         :param dev: The vconfig.LibvirtConfigGuestDisk to build metadata for.
11469         :param tagged_bdms: The list of tagged BlockDeviceMapping objects.
11470         :return: A DiskMetadata object, or None.
11471         """
11472         bdm = tagged_bdms.get(dev.target_dev)
11473         if not bdm:
11474             LOG.debug('No BDM found with device name %s, not building '
11475                       'metadata.', dev.target_dev)
11476             return None
11477         bus = self._prepare_device_bus(dev)
11478         device = objects.DiskMetadata(tags=[bdm.tag])
11479         # NOTE(artom) Setting the serial (which corresponds to
11480         # volume_id in BlockDeviceMapping) in DiskMetadata allows us to
11481         # find the disks's BlockDeviceMapping object when we detach the
11482         # volume and want to clean up its metadata.
11483         device.serial = bdm.volume_id
11484         if bus:
11485             device.bus = bus
11486         return device
11487 
11488     def _build_hostdev_metadata(self, dev, vifs_to_expose, vlans_by_mac):
11489         """Builds a metadata object for a hostdev. This can only be a PF, so we
11490         don't need trusted_by_mac like in _build_interface_metadata because
11491         only VFs can be trusted.
11492 
11493         :param dev: The LibvirtConfigGuestHostdevPCI to build metadata for.
11494         :param vifs_to_expose: The list of tagged and/or vlan'ed
11495                                VirtualInterface objects.
11496         :param vlans_by_mac: A dictionary of mac address -> vlan associations.
11497         :return: A NetworkInterfaceMetadata object, or None.
11498         """
11499         # Strip out the leading '0x'
11500         pci_address = pci_utils.get_pci_address(
11501             *[x[2:] for x in (dev.domain, dev.bus, dev.slot, dev.function)])
11502         try:
11503             mac = pci_utils.get_mac_by_pci_address(pci_address,
11504                                                    pf_interface=True)
11505         except exception.PciDeviceNotFoundById:
11506             LOG.debug('Not exposing metadata for not found PCI device %s',
11507                       pci_address)
11508             return None
11509 
11510         vif = vifs_to_expose.get(mac)
11511         if not vif:
11512             LOG.debug('No VIF found with MAC %s, not building metadata', mac)
11513             return None
11514 
11515         device = objects.NetworkInterfaceMetadata(mac=mac)
11516         device.bus = objects.PCIDeviceBus(address=pci_address)
11517         if 'tag' in vif and vif.tag:
11518             device.tags = [vif.tag]
11519         vlan = vlans_by_mac.get(mac)
11520         if vlan:
11521             device.vlan = int(vlan)
11522         return device
11523 
11524     def _build_device_metadata(self, context, instance):
11525         """Builds a metadata object for instance devices, that maps the user
11526            provided tag to the hypervisor assigned device address.
11527         """
11528         def _get_device_name(bdm):
11529             return block_device.strip_dev(bdm.device_name)
11530 
11531         network_info = instance.info_cache.network_info
11532         vlans_by_mac = netutils.get_cached_vifs_with_vlan(network_info)
11533         trusted_by_mac = netutils.get_cached_vifs_with_trusted(network_info)
11534         vifs = objects.VirtualInterfaceList.get_by_instance_uuid(context,
11535                                                                  instance.uuid)
11536         vifs_to_expose = {vif.address: vif for vif in vifs
11537                           if ('tag' in vif and vif.tag) or
11538                              vlans_by_mac.get(vif.address)}
11539         # TODO(mriedem): We should be able to avoid the DB query here by using
11540         # block_device_info['block_device_mapping'] which is passed into most
11541         # methods that call this function.
11542         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
11543             context, instance.uuid)
11544         tagged_bdms = {_get_device_name(bdm): bdm for bdm in bdms if bdm.tag}
11545 
11546         devices = []
11547         guest = self._host.get_guest(instance)
11548         xml = guest.get_xml_desc()
11549         xml_dom = etree.fromstring(xml)
11550         guest_config = vconfig.LibvirtConfigGuest()
11551         guest_config.parse_dom(xml_dom)
11552 
11553         for dev in guest_config.devices:
11554             device = None
11555             if isinstance(dev, vconfig.LibvirtConfigGuestInterface):
11556                 device = self._build_interface_metadata(dev, vifs_to_expose,
11557                                                         vlans_by_mac,
11558                                                         trusted_by_mac)
11559             if isinstance(dev, vconfig.LibvirtConfigGuestDisk):
11560                 device = self._build_disk_metadata(dev, tagged_bdms)
11561             if isinstance(dev, vconfig.LibvirtConfigGuestHostdevPCI):
11562                 device = self._build_hostdev_metadata(dev, vifs_to_expose,
11563                                                       vlans_by_mac)
11564             if device:
11565                 devices.append(device)
11566         if devices:
11567             dev_meta = objects.InstanceDeviceMetadata(devices=devices)
11568             return dev_meta
11569 
11570     def instance_on_disk(self, instance):
11571         # ensure directories exist and are writable
11572         instance_path = libvirt_utils.get_instance_path(instance)
11573         LOG.debug('Checking instance files accessibility %s', instance_path,
11574                   instance=instance)
11575         shared_instance_path = os.access(instance_path, os.W_OK)
11576         # NOTE(flwang): For shared block storage scenario, the file system is
11577         # not really shared by the two hosts, but the volume of evacuated
11578         # instance is reachable.
11579         shared_block_storage = (self.image_backend.backend().
11580                                 is_shared_block_storage())
11581         return shared_instance_path or shared_block_storage
11582 
11583     def inject_network_info(self, instance, nw_info):
11584         pass
11585 
11586     def delete_instance_files(self, instance):
11587         target = libvirt_utils.get_instance_path(instance)
11588         # A resize may be in progress
11589         target_resize = target + '_resize'
11590         # Other threads may attempt to rename the path, so renaming the path
11591         # to target + '_del' (because it is atomic) and iterating through
11592         # twice in the unlikely event that a concurrent rename occurs between
11593         # the two rename attempts in this method. In general this method
11594         # should be fairly thread-safe without these additional checks, since
11595         # other operations involving renames are not permitted when the task
11596         # state is not None and the task state should be set to something
11597         # other than None by the time this method is invoked.
11598         target_del = target + '_del'
11599         for i in range(2):
11600             try:
11601                 os.rename(target, target_del)
11602                 break
11603             except Exception:
11604                 pass
11605             try:
11606                 os.rename(target_resize, target_del)
11607                 break
11608             except Exception:
11609                 pass
11610         # Either the target or target_resize path may still exist if all
11611         # rename attempts failed.
11612         remaining_path = None
11613         for p in (target, target_resize):
11614             if os.path.exists(p):
11615                 remaining_path = p
11616                 break
11617 
11618         # A previous delete attempt may have been interrupted, so target_del
11619         # may exist even if all rename attempts during the present method
11620         # invocation failed due to the absence of both target and
11621         # target_resize.
11622         if not remaining_path and os.path.exists(target_del):
11623             self.job_tracker.terminate_jobs(instance)
11624 
11625             LOG.info('Deleting instance files %s', target_del,
11626                      instance=instance)
11627             remaining_path = target_del
11628             try:
11629                 shutil.rmtree(target_del)
11630             except OSError as e:
11631                 LOG.error('Failed to cleanup directory %(target)s: %(e)s',
11632                           {'target': target_del, 'e': e}, instance=instance)
11633 
11634         # It is possible that the delete failed, if so don't mark the instance
11635         # as cleaned.
11636         if remaining_path and os.path.exists(remaining_path):
11637             LOG.info('Deletion of %s failed', remaining_path,
11638                      instance=instance)
11639             return False
11640 
11641         LOG.info('Deletion of %s complete', target_del, instance=instance)
11642         return True
11643 
11644     def default_root_device_name(self, instance, image_meta, root_bdm):
11645         disk_bus = blockinfo.get_disk_bus_for_device_type(
11646             instance, CONF.libvirt.virt_type, image_meta, "disk")
11647         cdrom_bus = blockinfo.get_disk_bus_for_device_type(
11648             instance, CONF.libvirt.virt_type, image_meta, "cdrom")
11649         root_info = blockinfo.get_root_info(
11650             instance, CONF.libvirt.virt_type, image_meta,
11651             root_bdm, disk_bus, cdrom_bus)
11652         return block_device.prepend_dev(root_info['dev'])
11653 
11654     def default_device_names_for_instance(self, instance, root_device_name,
11655                                           *block_device_lists):
11656         block_device_mapping = list(itertools.chain(*block_device_lists))
11657         # NOTE(ndipanov): Null out the device names so that blockinfo code
11658         #                 will assign them
11659         for bdm in block_device_mapping:
11660             if bdm.device_name is not None:
11661                 LOG.info(
11662                     "Ignoring supplied device name: %(device_name)s. "
11663                     "Libvirt can't honour user-supplied dev names",
11664                     {'device_name': bdm.device_name}, instance=instance)
11665                 bdm.device_name = None
11666         block_device_info = driver.get_block_device_info(instance,
11667                                                          block_device_mapping)
11668 
11669         blockinfo.default_device_names(CONF.libvirt.virt_type,
11670                                        nova_context.get_admin_context(),
11671                                        instance,
11672                                        block_device_info,
11673                                        instance.image_meta)
11674 
11675     def get_device_name_for_instance(self, instance, bdms, block_device_obj):
11676         block_device_info = driver.get_block_device_info(instance, bdms)
11677         instance_info = blockinfo.get_disk_info(
11678                 CONF.libvirt.virt_type, instance,
11679                 instance.image_meta, block_device_info=block_device_info)
11680 
11681         suggested_dev_name = block_device_obj.device_name
11682         if suggested_dev_name is not None:
11683             LOG.info(
11684                 'Ignoring supplied device name: %(suggested_dev)s',
11685                 {'suggested_dev': suggested_dev_name}, instance=instance)
11686 
11687         # NOTE(ndipanov): get_info_from_bdm will generate the new device name
11688         #                 only when it's actually not set on the bd object
11689         block_device_obj.device_name = None
11690         disk_info = blockinfo.get_info_from_bdm(
11691             instance, CONF.libvirt.virt_type, instance.image_meta,
11692             block_device_obj, mapping=instance_info['mapping'])
11693         return block_device.prepend_dev(disk_info['dev'])
11694 
11695     def is_supported_fs_format(self, fs_type):
11696         return fs_type in [nova.privsep.fs.FS_FORMAT_EXT2,
11697                            nova.privsep.fs.FS_FORMAT_EXT3,
11698                            nova.privsep.fs.FS_FORMAT_EXT4,
11699                            nova.privsep.fs.FS_FORMAT_XFS]
11700 
11701     def _get_tpm_traits(self) -> ty.Dict[str, bool]:
11702         # Assert or deassert TPM support traits
11703         return {
11704             ot.COMPUTE_SECURITY_TPM_2_0: CONF.libvirt.swtpm_enabled,
11705             ot.COMPUTE_SECURITY_TPM_1_2: CONF.libvirt.swtpm_enabled,
11706         }
11707 
11708     def _get_vif_model_traits(self) -> ty.Dict[str, bool]:
11709         """Get vif model traits based on the currently enabled virt_type.
11710 
11711         Not all traits generated by this function may be valid and the result
11712         should be validated.
11713 
11714         :return: A dict of trait names mapped to boolean values.
11715         """
11716         all_models = set(itertools.chain(
11717             *libvirt_vif.SUPPORTED_VIF_MODELS.values()
11718         ))
11719         supported_models = libvirt_vif.SUPPORTED_VIF_MODELS.get(
11720             CONF.libvirt.virt_type, []
11721         )
11722         # construct the corresponding standard trait from the VIF model name
11723         return {
11724             f'COMPUTE_NET_VIF_MODEL_{model.replace("-", "_").upper()}': model
11725             in supported_models for model in all_models
11726         }
11727 
11728     def _get_storage_bus_traits(self) -> ty.Dict[str, bool]:
11729         """Get storage bus traits based on the currently enabled virt_type.
11730 
11731         For QEMU and KVM this function uses the information returned by the
11732         libvirt domain capabilities API. For other virt types we generate the
11733         traits based on the static information in the blockinfo module.
11734 
11735         Not all traits generated by this function may be valid and the result
11736         should be validated.
11737 
11738         :return: A dict of trait names mapped to boolean values.
11739         """
11740         all_buses = set(itertools.chain(
11741             *blockinfo.SUPPORTED_DEVICE_BUSES.values()
11742         ))
11743 
11744         if CONF.libvirt.virt_type in ('qemu', 'kvm'):
11745             dom_caps = self._host.get_domain_capabilities()
11746             supported_buses: ty.Set[str] = set()
11747             for arch_type in dom_caps:
11748                 for machine_type in dom_caps[arch_type]:
11749                     supported_buses.update(
11750                         dom_caps[arch_type][machine_type].devices.disk.buses
11751                     )
11752         else:
11753             supported_buses = blockinfo.SUPPORTED_DEVICE_BUSES.get(
11754                 CONF.libvirt.virt_type, []
11755             )
11756 
11757         # construct the corresponding standard trait from the storage bus name
11758         return {
11759             f'COMPUTE_STORAGE_BUS_{bus.replace("-", "_").upper()}': bus in
11760             supported_buses for bus in all_buses
11761         }
11762 
11763     def _get_video_model_traits(self) -> ty.Dict[str, bool]:
11764         """Get video model traits from libvirt.
11765 
11766         Not all traits generated by this function may be valid and the result
11767         should be validated.
11768 
11769         :return: A dict of trait names mapped to boolean values.
11770         """
11771         all_models = fields.VideoModel.ALL
11772 
11773         dom_caps = self._host.get_domain_capabilities()
11774         supported_models: ty.Set[str] = set()
11775         for arch_type in dom_caps:
11776             for machine_type in dom_caps[arch_type]:
11777                 supported_models.update(
11778                     dom_caps[arch_type][machine_type].devices.video.models
11779                 )
11780 
11781         # construct the corresponding standard trait from the video model name
11782         return {
11783             f'COMPUTE_GRAPHICS_MODEL_{model.replace("-", "_").upper()}': model
11784             in supported_models for model in all_models
11785         }
11786 
11787     def _get_cpu_traits(self) -> ty.Dict[str, bool]:
11788         """Get CPU-related traits to be set and unset on the host's resource
11789         provider.
11790 
11791         :return: A dict of trait names mapped to boolean values.
11792         """
11793         traits = self._get_cpu_feature_traits()
11794         traits[ot.HW_CPU_X86_AMD_SEV] = self._host.supports_amd_sev
11795         traits[ot.HW_CPU_HYPERTHREADING] = self._host.has_hyperthreading
11796 
11797         return traits
11798 
11799     def _get_cpu_feature_traits(self) -> ty.Dict[str, bool]:
11800         """Get CPU traits of VMs based on guest CPU model config.
11801 
11802         1. If mode is 'host-model' or 'host-passthrough', use host's
11803            CPU features.
11804         2. If mode is None, choose a default CPU model based on CPU
11805            architecture.
11806         3. If mode is 'custom', use cpu_models to generate CPU features.
11807 
11808         The code also accounts for cpu_model_extra_flags configuration when
11809         cpu_mode is 'host-model', 'host-passthrough' or 'custom', this
11810         ensures user specified CPU feature flags to be included.
11811 
11812         :return: A dict of trait names mapped to boolean values.
11813         """
11814         cpu = self._get_guest_cpu_model_config()
11815         if not cpu:
11816             LOG.info('The current libvirt hypervisor %(virt_type)s '
11817                      'does not support reporting CPU traits.',
11818                      {'virt_type': CONF.libvirt.virt_type})
11819             return {}
11820 
11821         caps = deepcopy(self._host.get_capabilities())
11822         if cpu.mode in ('host-model', 'host-passthrough'):
11823             # Account for features in cpu_model_extra_flags conf
11824             host_features: ty.Set[str] = {
11825                 f.name for f in caps.host.cpu.features | cpu.features
11826             }
11827             return libvirt_utils.cpu_features_to_traits(host_features)
11828 
11829         def _resolve_features(cpu):
11830             xml_str = cpu.to_xml()
11831             features_xml = self._get_guest_baseline_cpu_features(xml_str)
11832             feature_names = []
11833             if features_xml:
11834                 cpu = vconfig.LibvirtConfigCPU()
11835                 cpu.parse_str(features_xml)
11836                 feature_names = [f.name for f in cpu.features]
11837             return feature_names
11838 
11839         features: ty.Set[str] = set()
11840         # Choose a default CPU model when cpu_mode is not specified
11841         if cpu.mode is None:
11842             caps.host.cpu.model = libvirt_utils.get_cpu_model_from_arch(
11843                 caps.host.cpu.arch)
11844             caps.host.cpu.features = set()
11845             features = features.union(_resolve_features(caps.host.cpu))
11846         else:
11847             models = [self._get_cpu_model_mapping(model)
11848                       for model in CONF.libvirt.cpu_models]
11849 
11850             # Aarch64 platform doesn't return the default CPU models
11851             if caps.host.cpu.arch == fields.Architecture.AARCH64:
11852                 if not models:
11853                     models = ['max']
11854             # For custom mode, iterate through cpu models
11855             for model in models:
11856                 caps.host.cpu.model = model
11857                 caps.host.cpu.features = set()
11858                 features = features.union(_resolve_features(caps.host.cpu))
11859             # Account for features in cpu_model_extra_flags conf
11860             features = features.union([f.name for f in cpu.features])
11861 
11862         return libvirt_utils.cpu_features_to_traits(features)
11863 
11864     def _get_guest_baseline_cpu_features(self, xml_str):
11865         """Calls libvirt's baselineCPU API to compute the biggest set of
11866         CPU features which is compatible with the given host CPU.
11867 
11868         :param xml_str: XML description of host CPU
11869         :return: An XML string of the computed CPU, or None on error
11870         """
11871         LOG.debug("Libvirt baseline CPU %s", xml_str)
11872         # TODO(lei-zh): baselineCPU is not supported on all platforms.
11873         # There is some work going on in the libvirt community to replace the
11874         # baseline call. Consider using the new apis when they are ready. See
11875         # https://www.redhat.com/archives/libvir-list/2018-May/msg01204.html.
11876         try:
11877             if hasattr(libvirt, 'VIR_CONNECT_BASELINE_CPU_EXPAND_FEATURES'):
11878                 return self._host.get_connection().baselineCPU(
11879                     [xml_str],
11880                     libvirt.VIR_CONNECT_BASELINE_CPU_EXPAND_FEATURES)
11881             else:
11882                 return self._host.get_connection().baselineCPU([xml_str])
11883         except libvirt.libvirtError as ex:
11884             with excutils.save_and_reraise_exception() as ctxt:
11885                 error_code = ex.get_error_code()
11886                 if error_code == libvirt.VIR_ERR_NO_SUPPORT:
11887                     ctxt.reraise = False
11888                     LOG.debug('URI %(uri)s does not support full set'
11889                               ' of host capabilities: %(error)s',
11890                               {'uri': self._host._uri, 'error': ex})
11891                     return None
