Based on the given code from a commit, please generate supplementary code files according to the commit message.
####commit message
Log when port resource is leaked during port delete

When a bound port deleted in the neutron nova got notified and the port
is detached from the server. However if the port had resource request
then the resulting allocation is leaked.

This patch makes sure nova at least logs and ERROR. Also this patch
asserts that the leaked allocation is reclaimed when the server is
deleted.

Change-Id: I5d905aeb5b25f84d406dbf238d0d3a46f0f81161
Related-Bug: #1820588

####code 
1 # Copyright 2011 Justin Santa Barbara
2 # All Rights Reserved.
3 #
4 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
5 #    not use this file except in compliance with the License. You may obtain
6 #    a copy of the License at
7 #
8 #         http://www.apache.org/licenses/LICENSE-2.0
9 #
10 #    Unless required by applicable law or agreed to in writing, software
11 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
12 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
13 #    License for the specific language governing permissions and limitations
14 #    under the License.
15 from __future__ import absolute_import
16 
17 import collections
18 import copy
19 import datetime
20 import time
21 import zlib
22 
23 from keystoneauth1 import adapter
24 import mock
25 import os_resource_classes as orc
26 import os_traits
27 from oslo_config import cfg
28 from oslo_log import log as logging
29 from oslo_serialization import base64
30 from oslo_serialization import jsonutils
31 from oslo_utils.fixture import uuidsentinel as uuids
32 from oslo_utils import timeutils
33 import six
34 
35 from nova.compute import api as compute_api
36 from nova.compute import instance_actions
37 from nova.compute import manager as compute_manager
38 from nova.compute import rpcapi
39 from nova.conductor import manager
40 from nova import context
41 from nova import exception
42 from nova import objects
43 from nova.objects import block_device as block_device_obj
44 from nova.scheduler import utils
45 from nova.scheduler import weights
46 from nova import test
47 from nova.tests import fixtures as nova_fixtures
48 from nova.tests.functional.api import client
49 from nova.tests.functional import integrated_helpers
50 from nova.tests.unit.api.openstack import fakes
51 from nova.tests.unit import fake_block_device
52 from nova.tests.unit import fake_notifier
53 from nova.tests.unit import fake_requests
54 import nova.tests.unit.image.fake
55 from nova.tests.unit.objects import test_instance_info_cache
56 from nova.virt import fake
57 from nova import volume
58 
59 CONF = cfg.CONF
60 
61 LOG = logging.getLogger(__name__)
62 
63 
64 class AltHostWeigher(weights.BaseHostWeigher):
65     """Used in the alternate host tests to return a pre-determined list of
66     hosts.
67     """
68     def _weigh_object(self, host_state, weight_properties):
69         """Return a defined order of hosts."""
70         weights = {"selection": 999, "alt_host1": 888, "alt_host2": 777,
71                    "alt_host3": 666, "host1": 0, "host2": 0}
72         return weights.get(host_state.host, 0)
73 
74 
75 class ServersTestBase(integrated_helpers._IntegratedTestBase):
76     api_major_version = 'v2'
77     _force_delete_parameter = 'forceDelete'
78     _image_ref_parameter = 'imageRef'
79     _flavor_ref_parameter = 'flavorRef'
80     _access_ipv4_parameter = 'accessIPv4'
81     _access_ipv6_parameter = 'accessIPv6'
82     _return_resv_id_parameter = 'return_reservation_id'
83     _min_count_parameter = 'min_count'
84 
85     USE_NEUTRON = True
86 
87     def setUp(self):
88         self.computes = {}
89         super(ServersTestBase, self).setUp()
90         self.conductor = self.start_service(
91             'conductor', manager='nova.conductor.manager.ConductorManager')
92 
93     def _wait_for_state_change(self, server, from_status):
94         for i in range(0, 50):
95             server = self.api.get_server(server['id'])
96             if server['status'] != from_status:
97                 break
98             time.sleep(.1)
99 
100         return server
101 
102     def _wait_for_deletion(self, server_id):
103         # Wait (briefly) for deletion
104         for _retries in range(50):
105             try:
106                 found_server = self.api.get_server(server_id)
107             except client.OpenStackApiNotFoundException:
108                 found_server = None
109                 LOG.debug("Got 404, proceeding")
110                 break
111 
112             LOG.debug("Found_server=%s", found_server)
113 
114             # TODO(justinsb): Mock doesn't yet do accurate state changes
115             # if found_server['status'] != 'deleting':
116             #    break
117             time.sleep(.1)
118 
119         # Should be gone
120         self.assertFalse(found_server)
121 
122     def _delete_server(self, server_id):
123         # Delete the server
124         self.api.delete_server(server_id)
125         self._wait_for_deletion(server_id)
126 
127     def _get_access_ips_params(self):
128         return {self._access_ipv4_parameter: "172.19.0.2",
129                 self._access_ipv6_parameter: "fe80::2"}
130 
131     def _verify_access_ips(self, server):
132         self.assertEqual('172.19.0.2',
133                          server[self._access_ipv4_parameter])
134         self.assertEqual('fe80::2', server[self._access_ipv6_parameter])
135 
136 
137 class ServersTest(ServersTestBase):
138 
139     def test_get_servers(self):
140         # Simple check that listing servers works.
141         servers = self.api.get_servers()
142         for server in servers:
143             LOG.debug("server: %s", server)
144 
145     def _get_node_build_failures(self):
146         ctxt = context.get_admin_context()
147         computes = objects.ComputeNodeList.get_all(ctxt)
148         return {
149             node.hypervisor_hostname: int(node.stats.get('failed_builds', 0))
150             for node in computes}
151 
152     def _run_periodics(self):
153         """Run the update_available_resource task on every compute manager
154 
155         This runs periodics on the computes in an undefined order; some child
156         class redefined this function to force a specific order.
157         """
158 
159         if self.compute.host not in self.computes:
160             self.computes[self.compute.host] = self.compute
161 
162         ctx = context.get_admin_context()
163         for compute in self.computes.values():
164             LOG.info('Running periodic for compute (%s)',
165                 compute.manager.host)
166             compute.manager.update_available_resource(ctx)
167         LOG.info('Finished with periodics')
168 
169     def test_create_server_with_error(self):
170         # Create a server which will enter error state.
171 
172         def throw_error(*args, **kwargs):
173             raise exception.BuildAbortException(reason='',
174                     instance_uuid='fake')
175 
176         self.stub_out('nova.virt.fake.FakeDriver.spawn', throw_error)
177 
178         server = self._build_minimal_create_server_request()
179         created_server = self.api.post_server({"server": server})
180         created_server_id = created_server['id']
181 
182         found_server = self.api.get_server(created_server_id)
183         self.assertEqual(created_server_id, found_server['id'])
184 
185         found_server = self._wait_for_state_change(found_server, 'BUILD')
186 
187         self.assertEqual('ERROR', found_server['status'])
188         self._delete_server(created_server_id)
189 
190         # We should have no (persisted) build failures until we update
191         # resources, after which we should have one
192         self.assertEqual([0], list(self._get_node_build_failures().values()))
193         self._run_periodics()
194         self.assertEqual([1], list(self._get_node_build_failures().values()))
195 
196     def _test_create_server_with_error_with_retries(self):
197         # Create a server which will enter error state.
198 
199         fake.set_nodes(['host2'])
200         self.addCleanup(fake.restore_nodes)
201         self.flags(host='host2')
202         self.compute2 = self.start_service('compute', host='host2')
203         self.computes['compute2'] = self.compute2
204 
205         fails = []
206 
207         def throw_error(*args, **kwargs):
208             fails.append('one')
209             raise test.TestingException('Please retry me')
210 
211         self.stub_out('nova.virt.fake.FakeDriver.spawn', throw_error)
212 
213         server = self._build_minimal_create_server_request()
214         created_server = self.api.post_server({"server": server})
215         created_server_id = created_server['id']
216 
217         found_server = self.api.get_server(created_server_id)
218         self.assertEqual(created_server_id, found_server['id'])
219 
220         found_server = self._wait_for_state_change(found_server, 'BUILD')
221 
222         self.assertEqual('ERROR', found_server['status'])
223         self._delete_server(created_server_id)
224 
225         return len(fails)
226 
227     def test_create_server_with_error_with_retries(self):
228         self.flags(max_attempts=2, group='scheduler')
229         fails = self._test_create_server_with_error_with_retries()
230         self.assertEqual(2, fails)
231         self._run_periodics()
232         self.assertEqual(
233             [1, 1], list(self._get_node_build_failures().values()))
234 
235     def test_create_server_with_error_with_no_retries(self):
236         self.flags(max_attempts=1, group='scheduler')
237         fails = self._test_create_server_with_error_with_retries()
238         self.assertEqual(1, fails)
239         self._run_periodics()
240         self.assertEqual(
241             [0, 1], list(sorted(self._get_node_build_failures().values())))
242 
243     def test_create_and_delete_server(self):
244         # Creates and deletes a server.
245 
246         # Create server
247         # Build the server data gradually, checking errors along the way
248         server = {}
249         good_server = self._build_minimal_create_server_request()
250 
251         post = {'server': server}
252 
253         # Without an imageRef, this throws 500.
254         # TODO(justinsb): Check whatever the spec says should be thrown here
255         self.assertRaises(client.OpenStackApiException,
256                           self.api.post_server, post)
257 
258         # With an invalid imageRef, this throws 500.
259         server[self._image_ref_parameter] = self.get_invalid_image()
260         # TODO(justinsb): Check whatever the spec says should be thrown here
261         self.assertRaises(client.OpenStackApiException,
262                           self.api.post_server, post)
263 
264         # Add a valid imageRef
265         server[self._image_ref_parameter] = good_server.get(
266             self._image_ref_parameter)
267 
268         # Without flavorRef, this throws 500
269         # TODO(justinsb): Check whatever the spec says should be thrown here
270         self.assertRaises(client.OpenStackApiException,
271                           self.api.post_server, post)
272 
273         server[self._flavor_ref_parameter] = good_server.get(
274             self._flavor_ref_parameter)
275 
276         # Without a name, this throws 500
277         # TODO(justinsb): Check whatever the spec says should be thrown here
278         self.assertRaises(client.OpenStackApiException,
279                           self.api.post_server, post)
280 
281         # Set a valid server name
282         server['name'] = good_server['name']
283 
284         created_server = self.api.post_server(post)
285         LOG.debug("created_server: %s", created_server)
286         self.assertTrue(created_server['id'])
287         created_server_id = created_server['id']
288 
289         # Check it's there
290         found_server = self.api.get_server(created_server_id)
291         self.assertEqual(created_server_id, found_server['id'])
292 
293         # It should also be in the all-servers list
294         servers = self.api.get_servers()
295         server_ids = [s['id'] for s in servers]
296         self.assertIn(created_server_id, server_ids)
297 
298         found_server = self._wait_for_state_change(found_server, 'BUILD')
299         # It should be available...
300         # TODO(justinsb): Mock doesn't yet do this...
301         self.assertEqual('ACTIVE', found_server['status'])
302         servers = self.api.get_servers(detail=True)
303         for server in servers:
304             self.assertIn("image", server)
305             self.assertIn("flavor", server)
306 
307         self._delete_server(created_server_id)
308 
309     def _force_reclaim(self):
310         # Make sure that compute manager thinks the instance is
311         # old enough to be expired
312         the_past = timeutils.utcnow() + datetime.timedelta(hours=1)
313         timeutils.set_time_override(override_time=the_past)
314         self.addCleanup(timeutils.clear_time_override)
315         ctxt = context.get_admin_context()
316         self.compute._reclaim_queued_deletes(ctxt)
317 
318     def test_deferred_delete(self):
319         # Creates, deletes and waits for server to be reclaimed.
320         self.flags(reclaim_instance_interval=1)
321 
322         # Create server
323         server = self._build_minimal_create_server_request()
324 
325         created_server = self.api.post_server({'server': server})
326         LOG.debug("created_server: %s", created_server)
327         self.assertTrue(created_server['id'])
328         created_server_id = created_server['id']
329 
330         # Wait for it to finish being created
331         found_server = self._wait_for_state_change(created_server, 'BUILD')
332 
333         # It should be available...
334         self.assertEqual('ACTIVE', found_server['status'])
335 
336         # Cannot restore unless instance is deleted
337         self.assertRaises(client.OpenStackApiException,
338                           self.api.post_server_action, created_server_id,
339                           {'restore': {}})
340 
341         # Delete the server
342         self.api.delete_server(created_server_id)
343 
344         # Wait for queued deletion
345         found_server = self._wait_for_state_change(found_server, 'ACTIVE')
346         self.assertEqual('SOFT_DELETED', found_server['status'])
347 
348         self._force_reclaim()
349 
350         # Wait for real deletion
351         self._wait_for_deletion(created_server_id)
352 
353     def test_deferred_delete_restore(self):
354         # Creates, deletes and restores a server.
355         self.flags(reclaim_instance_interval=3600)
356 
357         # Create server
358         server = self._build_minimal_create_server_request()
359 
360         created_server = self.api.post_server({'server': server})
361         LOG.debug("created_server: %s", created_server)
362         self.assertTrue(created_server['id'])
363         created_server_id = created_server['id']
364 
365         # Wait for it to finish being created
366         found_server = self._wait_for_state_change(created_server, 'BUILD')
367 
368         # It should be available...
369         self.assertEqual('ACTIVE', found_server['status'])
370 
371         # Delete the server
372         self.api.delete_server(created_server_id)
373 
374         # Wait for queued deletion
375         found_server = self._wait_for_state_change(found_server, 'ACTIVE')
376         self.assertEqual('SOFT_DELETED', found_server['status'])
377 
378         # Restore server
379         self.api.post_server_action(created_server_id, {'restore': {}})
380 
381         # Wait for server to become active again
382         found_server = self._wait_for_state_change(found_server, 'DELETED')
383         self.assertEqual('ACTIVE', found_server['status'])
384 
385     def test_deferred_delete_restore_overquota(self):
386         # Test that a restore that would put the user over quota fails
387         self.flags(instances=1, group='quota')
388         # Creates, deletes and restores a server.
389         self.flags(reclaim_instance_interval=3600)
390 
391         # Create server
392         server = self._build_minimal_create_server_request()
393 
394         created_server1 = self.api.post_server({'server': server})
395         LOG.debug("created_server: %s", created_server1)
396         self.assertTrue(created_server1['id'])
397         created_server_id1 = created_server1['id']
398 
399         # Wait for it to finish being created
400         found_server1 = self._wait_for_state_change(created_server1, 'BUILD')
401 
402         # It should be available...
403         self.assertEqual('ACTIVE', found_server1['status'])
404 
405         # Delete the server
406         self.api.delete_server(created_server_id1)
407 
408         # Wait for queued deletion
409         found_server1 = self._wait_for_state_change(found_server1, 'ACTIVE')
410         self.assertEqual('SOFT_DELETED', found_server1['status'])
411 
412         # Create a second server
413         server = self._build_minimal_create_server_request()
414 
415         created_server2 = self.api.post_server({'server': server})
416         LOG.debug("created_server: %s", created_server2)
417         self.assertTrue(created_server2['id'])
418 
419         # Wait for it to finish being created
420         found_server2 = self._wait_for_state_change(created_server2, 'BUILD')
421 
422         # It should be available...
423         self.assertEqual('ACTIVE', found_server2['status'])
424 
425         # Try to restore the first server, it should fail
426         ex = self.assertRaises(client.OpenStackApiException,
427                                self.api.post_server_action,
428                                created_server_id1, {'restore': {}})
429         self.assertEqual(403, ex.response.status_code)
430         self.assertEqual('SOFT_DELETED', found_server1['status'])
431 
432     def test_deferred_delete_force(self):
433         # Creates, deletes and force deletes a server.
434         self.flags(reclaim_instance_interval=3600)
435 
436         # Create server
437         server = self._build_minimal_create_server_request()
438 
439         created_server = self.api.post_server({'server': server})
440         LOG.debug("created_server: %s", created_server)
441         self.assertTrue(created_server['id'])
442         created_server_id = created_server['id']
443 
444         # Wait for it to finish being created
445         found_server = self._wait_for_state_change(created_server, 'BUILD')
446 
447         # It should be available...
448         self.assertEqual('ACTIVE', found_server['status'])
449 
450         # Delete the server
451         self.api.delete_server(created_server_id)
452 
453         # Wait for queued deletion
454         found_server = self._wait_for_state_change(found_server, 'ACTIVE')
455         self.assertEqual('SOFT_DELETED', found_server['status'])
456 
457         # Force delete server
458         self.api.post_server_action(created_server_id,
459                                     {self._force_delete_parameter: {}})
460 
461         # Wait for real deletion
462         self._wait_for_deletion(created_server_id)
463 
464     def test_create_server_with_metadata(self):
465         # Creates a server with metadata.
466 
467         # Build the server data gradually, checking errors along the way
468         server = self._build_minimal_create_server_request()
469 
470         metadata = {}
471         for i in range(30):
472             metadata['key_%s' % i] = 'value_%s' % i
473 
474         server['metadata'] = metadata
475 
476         post = {'server': server}
477         created_server = self.api.post_server(post)
478         LOG.debug("created_server: %s", created_server)
479         self.assertTrue(created_server['id'])
480         created_server_id = created_server['id']
481 
482         found_server = self.api.get_server(created_server_id)
483         self.assertEqual(created_server_id, found_server['id'])
484         self.assertEqual(metadata, found_server.get('metadata'))
485 
486         # The server should also be in the all-servers details list
487         servers = self.api.get_servers(detail=True)
488         server_map = {server['id']: server for server in servers}
489         found_server = server_map.get(created_server_id)
490         self.assertTrue(found_server)
491         # Details do include metadata
492         self.assertEqual(metadata, found_server.get('metadata'))
493 
494         # The server should also be in the all-servers summary list
495         servers = self.api.get_servers(detail=False)
496         server_map = {server['id']: server for server in servers}
497         found_server = server_map.get(created_server_id)
498         self.assertTrue(found_server)
499         # Summary should not include metadata
500         self.assertFalse(found_server.get('metadata'))
501 
502         # Cleanup
503         self._delete_server(created_server_id)
504 
505     def test_server_metadata_actions_negative_invalid_state(self):
506         # Create server with metadata
507         server = self._build_minimal_create_server_request()
508 
509         metadata = {'key_1': 'value_1'}
510 
511         server['metadata'] = metadata
512 
513         post = {'server': server}
514         created_server = self.api.post_server(post)
515 
516         found_server = self._wait_for_state_change(created_server, 'BUILD')
517         self.assertEqual('ACTIVE', found_server['status'])
518         self.assertEqual(metadata, found_server.get('metadata'))
519         server_id = found_server['id']
520 
521         # Change status from ACTIVE to SHELVED for negative test
522         self.flags(shelved_offload_time = -1)
523         self.api.post_server_action(server_id, {'shelve': {}})
524         found_server = self._wait_for_state_change(found_server, 'ACTIVE')
525         self.assertEqual('SHELVED', found_server['status'])
526 
527         metadata = {'key_2': 'value_2'}
528 
529         # Update Metadata item in SHELVED (not ACTIVE, etc.)
530         ex = self.assertRaises(client.OpenStackApiException,
531                                self.api.post_server_metadata,
532                                server_id, metadata)
533         self.assertEqual(409, ex.response.status_code)
534         self.assertEqual('SHELVED', found_server['status'])
535 
536         # Delete Metadata item in SHELVED (not ACTIVE, etc.)
537         ex = self.assertRaises(client.OpenStackApiException,
538                                self.api.delete_server_metadata,
539                                server_id, 'key_1')
540         self.assertEqual(409, ex.response.status_code)
541         self.assertEqual('SHELVED', found_server['status'])
542 
543         # Cleanup
544         self._delete_server(server_id)
545 
546     def test_create_and_rebuild_server(self):
547         # Rebuild a server with metadata.
548 
549         # create a server with initially has no metadata
550         server = self._build_minimal_create_server_request()
551         server_post = {'server': server}
552 
553         metadata = {}
554         for i in range(30):
555             metadata['key_%s' % i] = 'value_%s' % i
556 
557         server_post['server']['metadata'] = metadata
558 
559         created_server = self.api.post_server(server_post)
560         LOG.debug("created_server: %s", created_server)
561         self.assertTrue(created_server['id'])
562         created_server_id = created_server['id']
563 
564         created_server = self._wait_for_state_change(created_server, 'BUILD')
565 
566         # rebuild the server with metadata and other server attributes
567         post = {}
568         post['rebuild'] = {
569             self._image_ref_parameter: "76fa36fc-c930-4bf3-8c8a-ea2a2420deb6",
570             "name": "blah",
571             self._access_ipv4_parameter: "172.19.0.2",
572             self._access_ipv6_parameter: "fe80::2",
573             "metadata": {'some': 'thing'},
574         }
575         post['rebuild'].update(self._get_access_ips_params())
576 
577         self.api.post_server_action(created_server_id, post)
578         LOG.debug("rebuilt server: %s", created_server)
579         self.assertTrue(created_server['id'])
580 
581         found_server = self.api.get_server(created_server_id)
582         self.assertEqual(created_server_id, found_server['id'])
583         self.assertEqual({'some': 'thing'}, found_server.get('metadata'))
584         self.assertEqual('blah', found_server.get('name'))
585         self.assertEqual(post['rebuild'][self._image_ref_parameter],
586                          found_server.get('image')['id'])
587         self._verify_access_ips(found_server)
588 
589         # rebuild the server with empty metadata and nothing else
590         post = {}
591         post['rebuild'] = {
592             self._image_ref_parameter: "76fa36fc-c930-4bf3-8c8a-ea2a2420deb6",
593             "metadata": {},
594         }
595 
596         self.api.post_server_action(created_server_id, post)
597         LOG.debug("rebuilt server: %s", created_server)
598         self.assertTrue(created_server['id'])
599 
600         found_server = self.api.get_server(created_server_id)
601         self.assertEqual(created_server_id, found_server['id'])
602         self.assertEqual({}, found_server.get('metadata'))
603         self.assertEqual('blah', found_server.get('name'))
604         self.assertEqual(post['rebuild'][self._image_ref_parameter],
605                          found_server.get('image')['id'])
606         self._verify_access_ips(found_server)
607 
608         # Cleanup
609         self._delete_server(created_server_id)
610 
611     def test_rename_server(self):
612         # Test building and renaming a server.
613 
614         # Create a server
615         server = self._build_minimal_create_server_request()
616         created_server = self.api.post_server({'server': server})
617         LOG.debug("created_server: %s", created_server)
618         server_id = created_server['id']
619         self.assertTrue(server_id)
620 
621         # Rename the server to 'new-name'
622         self.api.put_server(server_id, {'server': {'name': 'new-name'}})
623 
624         # Check the name of the server
625         created_server = self.api.get_server(server_id)
626         self.assertEqual(created_server['name'], 'new-name')
627 
628         # Cleanup
629         self._delete_server(server_id)
630 
631     def test_create_multiple_servers(self):
632         # Creates multiple servers and checks for reservation_id.
633 
634         # Create 2 servers, setting 'return_reservation_id, which should
635         # return a reservation_id
636         server = self._build_minimal_create_server_request()
637         server[self._min_count_parameter] = 2
638         server[self._return_resv_id_parameter] = True
639         post = {'server': server}
640         response = self.api.post_server(post)
641         self.assertIn('reservation_id', response)
642         reservation_id = response['reservation_id']
643         self.assertNotIn(reservation_id, ['', None])
644         # Assert that the reservation_id itself has the expected format
645         self.assertRegex(reservation_id, 'r-[0-9a-zA-Z]{8}')
646 
647         # Create 1 more server, which should not return a reservation_id
648         server = self._build_minimal_create_server_request()
649         post = {'server': server}
650         created_server = self.api.post_server(post)
651         self.assertTrue(created_server['id'])
652         created_server_id = created_server['id']
653 
654         # lookup servers created by the first request.
655         servers = self.api.get_servers(detail=True,
656                 search_opts={'reservation_id': reservation_id})
657         server_map = {server['id']: server for server in servers}
658         found_server = server_map.get(created_server_id)
659         # The server from the 2nd request should not be there.
660         self.assertIsNone(found_server)
661         # Should have found 2 servers.
662         self.assertEqual(len(server_map), 2)
663 
664         # Cleanup
665         self._delete_server(created_server_id)
666         for server_id in server_map:
667             self._delete_server(server_id)
668 
669     def test_create_server_with_injected_files(self):
670         # Creates a server with injected_files.
671         personality = []
672 
673         # Inject a text file
674         data = 'Hello, World!'
675         personality.append({
676             'path': '/helloworld.txt',
677             'contents': base64.encode_as_bytes(data),
678         })
679 
680         # Inject a binary file
681         data = zlib.compress(b'Hello, World!')
682         personality.append({
683             'path': '/helloworld.zip',
684             'contents': base64.encode_as_bytes(data),
685         })
686 
687         # Create server
688         server = self._build_minimal_create_server_request()
689         server['personality'] = personality
690 
691         post = {'server': server}
692 
693         created_server = self.api.post_server(post)
694         LOG.debug("created_server: %s", created_server)
695         self.assertTrue(created_server['id'])
696         created_server_id = created_server['id']
697 
698         # Check it's there
699         found_server = self.api.get_server(created_server_id)
700         self.assertEqual(created_server_id, found_server['id'])
701 
702         found_server = self._wait_for_state_change(found_server, 'BUILD')
703         self.assertEqual('ACTIVE', found_server['status'])
704 
705         # Cleanup
706         self._delete_server(created_server_id)
707 
708     def test_stop_start_servers_negative_invalid_state(self):
709         # Create server
710         server = self._build_minimal_create_server_request()
711         created_server = self.api.post_server({"server": server})
712         created_server_id = created_server['id']
713 
714         found_server = self._wait_for_state_change(created_server, 'BUILD')
715         self.assertEqual('ACTIVE', found_server['status'])
716 
717         # Start server in ACTIVE
718         # NOTE(mkoshiya): When os-start API runs, the server status
719         # must be SHUTOFF.
720         # By returning 409, I want to confirm that the ACTIVE server does not
721         # cause unexpected behavior.
722         post = {'os-start': {}}
723         ex = self.assertRaises(client.OpenStackApiException,
724                                self.api.post_server_action,
725                                created_server_id, post)
726         self.assertEqual(409, ex.response.status_code)
727         self.assertEqual('ACTIVE', found_server['status'])
728 
729         # Stop server
730         post = {'os-stop': {}}
731         self.api.post_server_action(created_server_id, post)
732         found_server = self._wait_for_state_change(found_server, 'ACTIVE')
733         self.assertEqual('SHUTOFF', found_server['status'])
734 
735         # Stop server in SHUTOFF
736         # NOTE(mkoshiya): When os-stop API runs, the server status
737         # must be ACTIVE or ERROR.
738         # By returning 409, I want to confirm that the SHUTOFF server does not
739         # cause unexpected behavior.
740         post = {'os-stop': {}}
741         ex = self.assertRaises(client.OpenStackApiException,
742                                self.api.post_server_action,
743                                created_server_id, post)
744         self.assertEqual(409, ex.response.status_code)
745         self.assertEqual('SHUTOFF', found_server['status'])
746 
747         # Cleanup
748         self._delete_server(created_server_id)
749 
750     def test_revert_resized_server_negative_invalid_state(self):
751         # Create server
752         server = self._build_minimal_create_server_request()
753         created_server = self.api.post_server({"server": server})
754         created_server_id = created_server['id']
755         found_server = self._wait_for_state_change(created_server, 'BUILD')
756         self.assertEqual('ACTIVE', found_server['status'])
757 
758         # Revert resized server in ACTIVE
759         # NOTE(yatsumi): When revert resized server API runs,
760         # the server status must be VERIFY_RESIZE.
761         # By returning 409, I want to confirm that the ACTIVE server does not
762         # cause unexpected behavior.
763         post = {'revertResize': {}}
764         ex = self.assertRaises(client.OpenStackApiException,
765                                self.api.post_server_action,
766                                created_server_id, post)
767         self.assertEqual(409, ex.response.status_code)
768         self.assertEqual('ACTIVE', found_server['status'])
769 
770         # Cleanup
771         self._delete_server(created_server_id)
772 
773     def test_resize_server_negative_invalid_state(self):
774         # Avoid migration
775         self.flags(allow_resize_to_same_host=True)
776 
777         # Create server
778         server = self._build_minimal_create_server_request()
779         created_server = self.api.post_server({"server": server})
780         created_server_id = created_server['id']
781         found_server = self._wait_for_state_change(created_server, 'BUILD')
782         self.assertEqual('ACTIVE', found_server['status'])
783 
784         # Resize server(flavorRef: 1 -> 2)
785         post = {'resize': {"flavorRef": "2", "OS-DCF:diskConfig": "AUTO"}}
786         self.api.post_server_action(created_server_id, post)
787         found_server = self._wait_for_state_change(found_server, 'RESIZE')
788         self.assertEqual('VERIFY_RESIZE', found_server['status'])
789 
790         # Resize server in VERIFY_RESIZE(flavorRef: 2 -> 1)
791         # NOTE(yatsumi): When resize API runs, the server status
792         # must be ACTIVE or SHUTOFF.
793         # By returning 409, I want to confirm that the VERIFY_RESIZE server
794         # does not cause unexpected behavior.
795         post = {'resize': {"flavorRef": "1", "OS-DCF:diskConfig": "AUTO"}}
796         ex = self.assertRaises(client.OpenStackApiException,
797                                self.api.post_server_action,
798                                created_server_id, post)
799         self.assertEqual(409, ex.response.status_code)
800         self.assertEqual('VERIFY_RESIZE', found_server['status'])
801 
802         # Cleanup
803         self._delete_server(created_server_id)
804 
805     def test_confirm_resized_server_negative_invalid_state(self):
806         # Create server
807         server = self._build_minimal_create_server_request()
808         created_server = self.api.post_server({"server": server})
809         created_server_id = created_server['id']
810         found_server = self._wait_for_state_change(created_server, 'BUILD')
811         self.assertEqual('ACTIVE', found_server['status'])
812 
813         # Confirm resized server in ACTIVE
814         # NOTE(yatsumi): When confirm resized server API runs,
815         # the server status must be VERIFY_RESIZE.
816         # By returning 409, I want to confirm that the ACTIVE server does not
817         # cause unexpected behavior.
818         post = {'confirmResize': {}}
819         ex = self.assertRaises(client.OpenStackApiException,
820                                self.api.post_server_action,
821                                created_server_id, post)
822         self.assertEqual(409, ex.response.status_code)
823         self.assertEqual('ACTIVE', found_server['status'])
824 
825         # Cleanup
826         self._delete_server(created_server_id)
827 
828     def test_resize_server_overquota(self):
829         self.flags(cores=1, group='quota')
830         self.flags(ram=512, group='quota')
831         # Create server with default flavor, 1 core, 512 ram
832         server = self._build_minimal_create_server_request()
833         created_server = self.api.post_server({"server": server})
834         created_server_id = created_server['id']
835 
836         found_server = self._wait_for_state_change(created_server, 'BUILD')
837         self.assertEqual('ACTIVE', found_server['status'])
838 
839         # Try to resize to flavorid 2, 1 core, 2048 ram
840         post = {'resize': {'flavorRef': '2'}}
841         ex = self.assertRaises(client.OpenStackApiException,
842                                self.api.post_server_action,
843                                created_server_id, post)
844         self.assertEqual(403, ex.response.status_code)
845 
846     def test_attach_vol_maximum_disk_devices_exceeded(self):
847         self.useFixture(nova_fixtures.CinderFixtureNewAttachFlow(self))
848 
849         server = self._build_minimal_create_server_request()
850         created_server = self.api.post_server({"server": server})
851         server_id = created_server['id']
852         self._wait_for_state_change(created_server, 'BUILD')
853 
854         volume_id = '9a695496-44aa-4404-b2cc-ccab2501f87e'
855         LOG.info('Attaching volume %s to server %s', volume_id, server_id)
856 
857         # The fake driver doesn't implement get_device_name_for_instance, so
858         # we'll just raise the exception directly here, instead of simuluating
859         # an instance with 26 disk devices already attached.
860         with mock.patch.object(self.compute.driver,
861                                'get_device_name_for_instance') as mock_get:
862             mock_get.side_effect = exception.TooManyDiskDevices(maximum=26)
863             ex = self.assertRaises(
864                 client.OpenStackApiException, self.api.post_server_volume,
865                 server_id, dict(volumeAttachment=dict(volumeId=volume_id)))
866             expected = ('The maximum allowed number of disk devices (26) to '
867                         'attach to a single instance has been exceeded.')
868             self.assertEqual(403, ex.response.status_code)
869             self.assertIn(expected, six.text_type(ex))
870 
871 
872 class ServersTestV21(ServersTest):
873     api_major_version = 'v2.1'
874 
875 
876 class ServersTestV219(ServersTestBase):
877     api_major_version = 'v2.1'
878 
879     def _create_server(self, set_desc = True, desc = None):
880         server = self._build_minimal_create_server_request()
881         if set_desc:
882             server['description'] = desc
883         post = {'server': server}
884         response = self.api.api_post('/servers', post).body
885         return (server, response['server'])
886 
887     def _update_server(self, server_id, set_desc = True, desc = None):
888         new_name = integrated_helpers.generate_random_alphanumeric(8)
889         server = {'server': {'name': new_name}}
890         if set_desc:
891             server['server']['description'] = desc
892         self.api.api_put('/servers/%s' % server_id, server)
893 
894     def _rebuild_server(self, server_id, set_desc = True, desc = None):
895         new_name = integrated_helpers.generate_random_alphanumeric(8)
896         post = {}
897         post['rebuild'] = {
898             "name": new_name,
899             self._image_ref_parameter: "76fa36fc-c930-4bf3-8c8a-ea2a2420deb6",
900             self._access_ipv4_parameter: "172.19.0.2",
901             self._access_ipv6_parameter: "fe80::2",
902             "metadata": {'some': 'thing'},
903         }
904         post['rebuild'].update(self._get_access_ips_params())
905         if set_desc:
906             post['rebuild']['description'] = desc
907         self.api.api_post('/servers/%s/action' % server_id, post)
908 
909     def _create_server_and_verify(self, set_desc = True, expected_desc = None):
910         # Creates a server with a description and verifies it is
911         # in the GET responses.
912         created_server_id = self._create_server(set_desc,
913                                                 expected_desc)[1]['id']
914         self._verify_server_description(created_server_id, expected_desc)
915         self._delete_server(created_server_id)
916 
917     def _update_server_and_verify(self, server_id, set_desc = True,
918                                   expected_desc = None):
919         # Updates a server with a description and verifies it is
920         # in the GET responses.
921         self._update_server(server_id, set_desc, expected_desc)
922         self._verify_server_description(server_id, expected_desc)
923 
924     def _rebuild_server_and_verify(self, server_id, set_desc = True,
925                                   expected_desc = None):
926         # Rebuilds a server with a description and verifies it is
927         # in the GET responses.
928         self._rebuild_server(server_id, set_desc, expected_desc)
929         self._verify_server_description(server_id, expected_desc)
930 
931     def _verify_server_description(self, server_id, expected_desc = None,
932                                    desc_in_resp = True):
933         # Calls GET on the servers and verifies that the description
934         # is set as expected in the response, or not set at all.
935         response = self.api.api_get('/servers/%s' % server_id)
936         found_server = response.body['server']
937         self.assertEqual(server_id, found_server['id'])
938         if desc_in_resp:
939             # Verify the description is set as expected (can be None)
940             self.assertEqual(expected_desc, found_server.get('description'))
941         else:
942             # Verify the description is not included in the response.
943             self.assertNotIn('description', found_server)
944 
945         servers = self.api.api_get('/servers/detail').body['servers']
946         server_map = {server['id']: server for server in servers}
947         found_server = server_map.get(server_id)
948         self.assertTrue(found_server)
949         if desc_in_resp:
950             # Verify the description is set as expected (can be None)
951             self.assertEqual(expected_desc, found_server.get('description'))
952         else:
953             # Verify the description is not included in the response.
954             self.assertNotIn('description', found_server)
955 
956     def _create_assertRaisesRegex(self, desc):
957         # Verifies that a 400 error is thrown on create server
958         with self.assertRaisesRegex(client.OpenStackApiException,
959                                     ".*Unexpected status code.*") as cm:
960             self._create_server(True, desc)
961             self.assertEqual(400, cm.exception.response.status_code)
962 
963     def _update_assertRaisesRegex(self, server_id, desc):
964         # Verifies that a 400 error is thrown on update server
965         with self.assertRaisesRegex(client.OpenStackApiException,
966                                     ".*Unexpected status code.*") as cm:
967             self._update_server(server_id, True, desc)
968             self.assertEqual(400, cm.exception.response.status_code)
969 
970     def _rebuild_assertRaisesRegex(self, server_id, desc):
971         # Verifies that a 400 error is thrown on rebuild server
972         with self.assertRaisesRegex(client.OpenStackApiException,
973                                     ".*Unexpected status code.*") as cm:
974             self._rebuild_server(server_id, True, desc)
975             self.assertEqual(400, cm.exception.response.status_code)
976 
977     def test_create_server_with_description(self):
978         self.api.microversion = '2.19'
979         # Create and get a server with a description
980         self._create_server_and_verify(True, 'test description')
981         # Create and get a server with an empty description
982         self._create_server_and_verify(True, '')
983         # Create and get a server with description set to None
984         self._create_server_and_verify()
985         # Create and get a server without setting the description
986         self._create_server_and_verify(False)
987 
988     def test_update_server_with_description(self):
989         self.api.microversion = '2.19'
990         # Create a server with an initial description
991         server_id = self._create_server(True, 'test desc 1')[1]['id']
992 
993         # Update and get the server with a description
994         self._update_server_and_verify(server_id, True, 'updated desc')
995         # Update and get the server name without changing the description
996         self._update_server_and_verify(server_id, False, 'updated desc')
997         # Update and get the server with an empty description
998         self._update_server_and_verify(server_id, True, '')
999         # Update and get the server by removing the description (set to None)
1000         self._update_server_and_verify(server_id)
1001         # Update and get the server with a 2nd new description
1002         self._update_server_and_verify(server_id, True, 'updated desc2')
1003 
1004         # Cleanup
1005         self._delete_server(server_id)
1006 
1007     def test_rebuild_server_with_description(self):
1008         self.api.microversion = '2.19'
1009 
1010         # Create a server with an initial description
1011         server = self._create_server(True, 'test desc 1')[1]
1012         server_id = server['id']
1013         self._wait_for_state_change(server, 'BUILD')
1014 
1015         # Rebuild and get the server with a description
1016         self._rebuild_server_and_verify(server_id, True, 'updated desc')
1017         # Rebuild and get the server name without changing the description
1018         self._rebuild_server_and_verify(server_id, False, 'updated desc')
1019         # Rebuild and get the server with an empty description
1020         self._rebuild_server_and_verify(server_id, True, '')
1021         # Rebuild and get the server by removing the description (set to None)
1022         self._rebuild_server_and_verify(server_id)
1023         # Rebuild and get the server with a 2nd new description
1024         self._rebuild_server_and_verify(server_id, True, 'updated desc2')
1025 
1026         # Cleanup
1027         self._delete_server(server_id)
1028 
1029     def test_version_compatibility(self):
1030         # Create a server with microversion v2.19 and a description.
1031         self.api.microversion = '2.19'
1032         server_id = self._create_server(True, 'test desc 1')[1]['id']
1033         # Verify that the description is not included on V2.18 GETs
1034         self.api.microversion = '2.18'
1035         self._verify_server_description(server_id, desc_in_resp = False)
1036         # Verify that updating the server with description on V2.18
1037         # results in a 400 error
1038         self._update_assertRaisesRegex(server_id, 'test update 2.18')
1039         # Verify that rebuilding the server with description on V2.18
1040         # results in a 400 error
1041         self._rebuild_assertRaisesRegex(server_id, 'test rebuild 2.18')
1042 
1043         # Cleanup
1044         self._delete_server(server_id)
1045 
1046         # Create a server on V2.18 and verify that the description
1047         # defaults to the name on a V2.19 GET
1048         server_req, response = self._create_server(False)
1049         server_id = response['id']
1050         self.api.microversion = '2.19'
1051         self._verify_server_description(server_id, server_req['name'])
1052 
1053         # Cleanup
1054         self._delete_server(server_id)
1055 
1056         # Verify that creating a server with description on V2.18
1057         # results in a 400 error
1058         self.api.microversion = '2.18'
1059         self._create_assertRaisesRegex('test create 2.18')
1060 
1061     def test_description_errors(self):
1062         self.api.microversion = '2.19'
1063         # Create servers with invalid descriptions.  These throw 400.
1064         # Invalid unicode with non-printable control char
1065         self._create_assertRaisesRegex(u'invalid\0dstring')
1066         # Description is longer than 255 chars
1067         self._create_assertRaisesRegex('x' * 256)
1068 
1069         # Update and rebuild servers with invalid descriptions.
1070         # These throw 400.
1071         server_id = self._create_server(True, "desc")[1]['id']
1072         # Invalid unicode with non-printable control char
1073         self._update_assertRaisesRegex(server_id, u'invalid\u0604string')
1074         self._rebuild_assertRaisesRegex(server_id, u'invalid\u0604string')
1075         # Description is longer than 255 chars
1076         self._update_assertRaisesRegex(server_id, 'x' * 256)
1077         self._rebuild_assertRaisesRegex(server_id, 'x' * 256)
1078 
1079 
1080 class ServerTestV220(ServersTestBase):
1081     api_major_version = 'v2.1'
1082 
1083     def setUp(self):
1084         super(ServerTestV220, self).setUp()
1085         self.api.microversion = '2.20'
1086         self.ctxt = context.get_admin_context()
1087 
1088     def _create_server(self):
1089         server = self._build_minimal_create_server_request()
1090         post = {'server': server}
1091         response = self.api.api_post('/servers', post).body
1092         return (server, response['server'])
1093 
1094     def _shelve_server(self):
1095         server = self._create_server()[1]
1096         server_id = server['id']
1097         self._wait_for_state_change(server, 'BUILD')
1098         self.api.post_server_action(server_id, {'shelve': None})
1099         return self._wait_for_state_change(server, 'ACTIVE')
1100 
1101     def _get_fake_bdms(self, ctxt):
1102         return block_device_obj.block_device_make_list(self.ctxt,
1103                     [fake_block_device.FakeDbBlockDeviceDict(
1104                     {'device_name': '/dev/vda',
1105                      'source_type': 'volume',
1106                      'destination_type': 'volume',
1107                      'volume_id': '5d721593-f033-4f6d-ab6f-b5b067e61bc4'})])
1108 
1109     def test_attach_detach_vol_to_shelved_server(self):
1110         self.flags(shelved_offload_time=-1)
1111         found_server = self._shelve_server()
1112         self.assertEqual('SHELVED', found_server['status'])
1113         server_id = found_server['id']
1114 
1115         # Test attach volume
1116         self.stub_out('nova.volume.cinder.API.get', fakes.stub_volume_get)
1117         with test.nested(mock.patch.object(volume.cinder,
1118                                        'is_microversion_supported'),
1119                          mock.patch.object(compute_api.API,
1120                                        '_check_attach_and_reserve_volume'),
1121                          mock.patch.object(rpcapi.ComputeAPI,
1122                                        'attach_volume')) as (mock_cinder_mv,
1123                                                              mock_reserve,
1124                                                              mock_attach):
1125             mock_cinder_mv.side_effect = \
1126                 exception.CinderAPIVersionNotAvailable(version='3.44')
1127             volume_attachment = {"volumeAttachment": {"volumeId":
1128                                        "5d721593-f033-4f6d-ab6f-b5b067e61bc4"}}
1129             self.api.api_post(
1130                             '/servers/%s/os-volume_attachments' % (server_id),
1131                             volume_attachment)
1132             self.assertTrue(mock_reserve.called)
1133             self.assertTrue(mock_attach.called)
1134 
1135         # Test detach volume
1136         with test.nested(mock.patch.object(volume.cinder.API,
1137                                            'begin_detaching'),
1138                          mock.patch.object(objects.BlockDeviceMappingList,
1139                                            'get_by_instance_uuid'),
1140                          mock.patch.object(rpcapi.ComputeAPI,
1141                                            'detach_volume')
1142                          ) as (mock_check, mock_get_bdms, mock_rpc):
1143 
1144             mock_get_bdms.return_value = self._get_fake_bdms(self.ctxt)
1145             attachment_id = mock_get_bdms.return_value[0]['volume_id']
1146 
1147             self.api.api_delete('/servers/%s/os-volume_attachments/%s' %
1148                             (server_id, attachment_id))
1149             self.assertTrue(mock_check.called)
1150             self.assertTrue(mock_rpc.called)
1151 
1152         self._delete_server(server_id)
1153 
1154     def test_attach_detach_vol_to_shelved_offloaded_server(self):
1155         self.flags(shelved_offload_time=0)
1156         found_server = self._shelve_server()
1157         self.assertEqual('SHELVED_OFFLOADED', found_server['status'])
1158         server_id = found_server['id']
1159 
1160         # Test attach volume
1161         self.stub_out('nova.volume.cinder.API.get', fakes.stub_volume_get)
1162         with test.nested(mock.patch.object(volume.cinder,
1163                                        'is_microversion_supported'),
1164                          mock.patch.object(compute_api.API,
1165                                        '_check_attach_and_reserve_volume'),
1166                          mock.patch.object(volume.cinder.API,
1167                                        'attach')) as (mock_cinder_mv,
1168                                                       mock_reserve, mock_vol):
1169             mock_cinder_mv.side_effect = \
1170                 exception.CinderAPIVersionNotAvailable(version='3.44')
1171             volume_attachment = {"volumeAttachment": {"volumeId":
1172                                        "5d721593-f033-4f6d-ab6f-b5b067e61bc4"}}
1173             attach_response = self.api.api_post(
1174                              '/servers/%s/os-volume_attachments' % (server_id),
1175                              volume_attachment).body['volumeAttachment']
1176             self.assertTrue(mock_reserve.called)
1177             self.assertTrue(mock_vol.called)
1178             self.assertIsNone(attach_response['device'])
1179 
1180         # Test detach volume
1181         with test.nested(mock.patch.object(volume.cinder.API,
1182                                            'begin_detaching'),
1183                          mock.patch.object(objects.BlockDeviceMappingList,
1184                                            'get_by_instance_uuid'),
1185                          mock.patch.object(compute_api.API,
1186                                            '_local_cleanup_bdm_volumes')
1187                          ) as (mock_check, mock_get_bdms, mock_clean_vols):
1188 
1189             mock_get_bdms.return_value = self._get_fake_bdms(self.ctxt)
1190             attachment_id = mock_get_bdms.return_value[0]['volume_id']
1191             self.api.api_delete('/servers/%s/os-volume_attachments/%s' %
1192                             (server_id, attachment_id))
1193             self.assertTrue(mock_check.called)
1194             self.assertTrue(mock_clean_vols.called)
1195 
1196         self._delete_server(server_id)
1197 
1198     def test_attach_detach_vol_to_shelved_offloaded_server_new_flow(self):
1199         self.flags(shelved_offload_time=0)
1200         found_server = self._shelve_server()
1201         self.assertEqual('SHELVED_OFFLOADED', found_server['status'])
1202         server_id = found_server['id']
1203         fake_bdms = self._get_fake_bdms(self.ctxt)
1204 
1205         # Test attach volume
1206         self.stub_out('nova.volume.cinder.API.get', fakes.stub_volume_get)
1207         with test.nested(mock.patch.object(volume.cinder,
1208                                        'is_microversion_supported'),
1209                          mock.patch.object(compute_api.API,
1210                             '_check_volume_already_attached_to_instance'),
1211                          mock.patch.object(volume.cinder.API,
1212                                         'check_availability_zone'),
1213                          mock.patch.object(volume.cinder.API,
1214                                         'attachment_create'),
1215                          mock.patch.object(volume.cinder.API,
1216                                         'attachment_complete')
1217                          ) as (mock_cinder_mv, mock_check_vol_attached,
1218                                mock_check_av_zone, mock_attach_create,
1219                                mock_attachment_complete):
1220             mock_attach_create.return_value = {'id': uuids.volume}
1221             volume_attachment = {"volumeAttachment": {"volumeId":
1222                                        "5d721593-f033-4f6d-ab6f-b5b067e61bc4"}}
1223             attach_response = self.api.api_post(
1224                              '/servers/%s/os-volume_attachments' % (server_id),
1225                              volume_attachment).body['volumeAttachment']
1226             self.assertTrue(mock_attach_create.called)
1227             mock_attachment_complete.assert_called_once_with(
1228                 mock.ANY, uuids.volume)
1229             self.assertIsNone(attach_response['device'])
1230 
1231         # Test detach volume
1232         with test.nested(mock.patch.object(objects.BlockDeviceMappingList,
1233                                            'get_by_instance_uuid'),
1234                          mock.patch.object(compute_api.API,
1235                                            '_local_cleanup_bdm_volumes')
1236                          ) as (mock_get_bdms, mock_clean_vols):
1237 
1238             mock_get_bdms.return_value = fake_bdms
1239             attachment_id = mock_get_bdms.return_value[0]['volume_id']
1240             self.api.api_delete('/servers/%s/os-volume_attachments/%s' %
1241                             (server_id, attachment_id))
1242             self.assertTrue(mock_clean_vols.called)
1243 
1244         self._delete_server(server_id)
1245 
1246 
1247 class ServerTestV269(ServersTestBase):
1248     api_major_version = 'v2.1'
1249     NUMBER_OF_CELLS = 3
1250 
1251     def setUp(self):
1252         super(ServerTestV269, self).setUp()
1253         self.api.microversion = '2.69'
1254 
1255         self.ctxt = context.get_admin_context()
1256         self.project_id = self.api.project_id
1257         self.cells = objects.CellMappingList.get_all(self.ctxt)
1258         self.down_cell_insts = []
1259         self.up_cell_insts = []
1260         self.down_cell_mappings = objects.CellMappingList()
1261         flavor = objects.Flavor(id=1, name='flavor1',
1262                                 memory_mb=256, vcpus=1,
1263                                 root_gb=1, ephemeral_gb=1,
1264                                 flavorid='1',
1265                                 swap=0, rxtx_factor=1.0,
1266                                 vcpu_weight=1,
1267                                 disabled=False,
1268                                 is_public=True,
1269                                 extra_specs={},
1270                                 projects=[])
1271         _info_cache = objects.InstanceInfoCache(context)
1272         objects.InstanceInfoCache._from_db_object(context, _info_cache,
1273             test_instance_info_cache.fake_info_cache)
1274         # cell1 and cell2 will be the down cells while
1275         # cell0 and cell3 will be the up cells.
1276         down_cell_names = ['cell1', 'cell2']
1277         for cell in self.cells:
1278             # create 2 instances and their mappings in all the 4 cells
1279             for i in range(2):
1280                 with context.target_cell(self.ctxt, cell) as cctxt:
1281                     inst = objects.Instance(
1282                         context=cctxt,
1283                         project_id=self.project_id,
1284                         user_id=self.project_id,
1285                         instance_type_id=flavor.id,
1286                         hostname='%s-inst%i' % (cell.name, i),
1287                         flavor=flavor,
1288                         info_cache=_info_cache,
1289                         display_name='server-test')
1290                     inst.create()
1291                 im = objects.InstanceMapping(context=self.ctxt,
1292                                              instance_uuid=inst.uuid,
1293                                              cell_mapping=cell,
1294                                              project_id=self.project_id,
1295                                              queued_for_delete=False)
1296                 im.create()
1297                 if cell.name in down_cell_names:
1298                     self.down_cell_insts.append(inst.uuid)
1299                 else:
1300                     self.up_cell_insts.append(inst.uuid)
1301             # In cell1 and cell3 add a third instance in a different project
1302             # to show the --all-tenants case.
1303             if cell.name == 'cell1' or cell.name == 'cell3':
1304                 with context.target_cell(self.ctxt, cell) as cctxt:
1305                     inst = objects.Instance(
1306                         context=cctxt,
1307                         project_id='faker',
1308                         user_id='faker',
1309                         instance_type_id=flavor.id,
1310                         hostname='%s-inst%i' % (cell.name, 3),
1311                         flavor=flavor,
1312                         info_cache=_info_cache,
1313                         display_name='server-test')
1314                     inst.create()
1315                 im = objects.InstanceMapping(context=self.ctxt,
1316                                              instance_uuid=inst.uuid,
1317                                              cell_mapping=cell,
1318                                              project_id='faker',
1319                                              queued_for_delete=False)
1320                 im.create()
1321             if cell.name in down_cell_names:
1322                 self.down_cell_mappings.objects.append(cell)
1323         self.useFixture(nova_fixtures.DownCellFixture(self.down_cell_mappings))
1324 
1325     def test_get_servers_with_down_cells(self):
1326         servers = self.api.get_servers(detail=False)
1327         # 4 servers from the up cells and 4 servers from the down cells
1328         self.assertEqual(8, len(servers))
1329         for server in servers:
1330             if 'name' not in server:
1331                 # server is in the down cell.
1332                 self.assertEqual('UNKNOWN', server['status'])
1333                 self.assertIn(server['id'], self.down_cell_insts)
1334                 self.assertIn('links', server)
1335                 # the partial construct will have only the above 3 keys
1336                 self.assertEqual(3, len(server))
1337             else:
1338                 # server in up cell
1339                 self.assertIn(server['id'], self.up_cell_insts)
1340                 # has all the keys
1341                 self.assertEqual(server['name'], 'server-test')
1342                 self.assertIn('links', server)
1343 
1344     def test_get_servers_detail_with_down_cells(self):
1345         servers = self.api.get_servers()
1346         # 4 servers from the up cells and 4 servers from the down cells
1347         self.assertEqual(8, len(servers))
1348         for server in servers:
1349             if 'user_id' not in server:
1350                 # server is in the down cell.
1351                 self.assertEqual('UNKNOWN', server['status'])
1352                 self.assertIn(server['id'], self.down_cell_insts)
1353                 # the partial construct will have only 5 keys:
1354                 # created, tenant_id, status, id and links.
1355                 self.assertEqual(5, len(server))
1356             else:
1357                 # server in up cell
1358                 self.assertIn(server['id'], self.up_cell_insts)
1359                 # has all the keys
1360                 self.assertEqual(server['user_id'], self.project_id)
1361                 self.assertIn('image', server)
1362 
1363     def test_get_servers_detail_limits_with_down_cells(self):
1364         servers = self.api.get_servers(search_opts={'limit': 5})
1365         # 4 servers from the up cells since we skip down cell
1366         # results by default for paging.
1367         self.assertEqual(4, len(servers), servers)
1368         for server in servers:
1369             # server in up cell
1370             self.assertIn(server['id'], self.up_cell_insts)
1371             # has all the keys
1372             self.assertEqual(server['user_id'], self.project_id)
1373             self.assertIn('image', server)
1374 
1375     def test_get_servers_detail_limits_with_down_cells_the_500_gift(self):
1376         self.flags(list_records_by_skipping_down_cells=False, group='api')
1377         # We get an API error with a 500 response code since the
1378         # list_records_by_skipping_down_cells config option is False.
1379         exp = self.assertRaises(client.OpenStackApiException,
1380                                 self.api.get_servers,
1381                                 search_opts={'limit': 5})
1382         self.assertEqual(500, exp.response.status_code)
1383         self.assertIn('NovaException', six.text_type(exp))
1384 
1385     def test_get_servers_detail_marker_in_down_cells(self):
1386         marker = self.down_cell_insts[2]
1387         # It will fail with a 500 if the marker is in the down cell.
1388         exp = self.assertRaises(client.OpenStackApiException,
1389                                 self.api.get_servers,
1390                                 search_opts={'marker': marker})
1391         self.assertEqual(500, exp.response.status_code)
1392         self.assertIn('oslo_db.exception.DBError', six.text_type(exp))
1393 
1394     def test_get_servers_detail_marker_sorting(self):
1395         marker = self.up_cell_insts[1]
1396         # It will give the results from the up cell if
1397         # list_records_by_skipping_down_cells config option is True.
1398         servers = self.api.get_servers(search_opts={'marker': marker,
1399                                                     'sort_key': "created_at",
1400                                                     'sort_dir': "asc"})
1401         # since there are 4 servers from the up cells, when giving the
1402         # second instance as marker, sorted by creation time in ascending
1403         # third and fourth instances will be returned.
1404         self.assertEqual(2, len(servers))
1405         for server in servers:
1406             self.assertIn(
1407                 server['id'], [self.up_cell_insts[2], self.up_cell_insts[3]])
1408 
1409     def test_get_servers_detail_non_admin_with_deleted_flag(self):
1410         # if list_records_by_skipping_down_cells config option is True
1411         # this deleted option should be ignored and the rest of the instances
1412         # from the up cells and the partial results from the down cells should
1413         # be returned.
1414         # Set the policy so we don't have permission to allow
1415         # all filters but are able to get server details.
1416         servers_rule = 'os_compute_api:servers:detail'
1417         extraspec_rule = 'os_compute_api:servers:allow_all_filters'
1418         self.policy.set_rules({
1419             extraspec_rule: 'rule:admin_api',
1420             servers_rule: '@'})
1421         servers = self.api.get_servers(search_opts={'deleted': True})
1422         # gets 4 results from up cells and 4 from down cells.
1423         self.assertEqual(8, len(servers))
1424         for server in servers:
1425             if "image" not in server:
1426                 self.assertIn(server['id'], self.down_cell_insts)
1427             else:
1428                 self.assertIn(server['id'], self.up_cell_insts)
1429 
1430     def test_get_servers_detail_filters(self):
1431         # We get the results only from the up cells, this ignoring the down
1432         # cells if list_records_by_skipping_down_cells config option is True.
1433         api_fixture = self.useFixture(nova_fixtures.OSAPIFixture(
1434             api_version='v2.1'))
1435         self.admin_api = api_fixture.admin_api
1436         self.admin_api.microversion = '2.69'
1437         servers = self.admin_api.get_servers(
1438             search_opts={'hostname': "cell3-inst0"})
1439         self.assertEqual(1, len(servers))
1440         self.assertEqual(self.up_cell_insts[2], servers[0]['id'])
1441 
1442     def test_get_servers_detail_all_tenants_with_down_cells(self):
1443         api_fixture = self.useFixture(nova_fixtures.OSAPIFixture(
1444             api_version='v2.1'))
1445         self.admin_api = api_fixture.admin_api
1446         self.admin_api.microversion = '2.69'
1447         servers = self.admin_api.get_servers(search_opts={'all_tenants': True})
1448         # 4 servers from the up cells and 4 servers from the down cells
1449         # plus the 2 instances from cell1 and cell3 which are in a different
1450         # project.
1451         self.assertEqual(10, len(servers))
1452         for server in servers:
1453             if 'user_id' not in server:
1454                 # server is in the down cell.
1455                 self.assertEqual('UNKNOWN', server['status'])
1456                 if server['tenant_id'] != 'faker':
1457                     self.assertIn(server['id'], self.down_cell_insts)
1458                 # the partial construct will have only 5 keys:
1459                 # created, tenant_id, status, id and links
1460                 self.assertEqual(5, len(server))
1461             else:
1462                 # server in up cell
1463                 if server['tenant_id'] != 'faker':
1464                     self.assertIn(server['id'], self.up_cell_insts)
1465                     self.assertEqual(server['user_id'], self.project_id)
1466                 self.assertIn('image', server)
1467 
1468 
1469 class ServerRebuildTestCase(integrated_helpers._IntegratedTestBase,
1470                             integrated_helpers.InstanceHelperMixin):
1471     api_major_version = 'v2.1'
1472     # We have to cap the microversion at 2.38 because that's the max we
1473     # can use to update image metadata via our compute images proxy API.
1474     microversion = '2.38'
1475 
1476     def _disable_compute_for(self, server):
1477         # Refresh to get its host
1478         server = self.api.get_server(server['id'])
1479         host = server['OS-EXT-SRV-ATTR:host']
1480 
1481         # Disable the service it is on
1482         self.api_fixture.admin_api.put_service('disable',
1483                                                {'host': host,
1484                                                 'binary': 'nova-compute'})
1485 
1486     def test_rebuild_with_image_novalidhost(self):
1487         """Creates a server with an image that is valid for the single compute
1488         that we have. Then rebuilds the server, passing in an image with
1489         metadata that does not fit the single compute which should result in
1490         a NoValidHost error. The ImagePropertiesFilter filter is enabled by
1491         default so that should filter out the host based on the image meta.
1492         """
1493 
1494         fake.set_nodes(['host2'])
1495         self.addCleanup(fake.restore_nodes)
1496         self.flags(host='host2')
1497         self.compute2 = self.start_service('compute', host='host2')
1498 
1499         # We hard-code from a fake image since we can't get images
1500         # via the compute /images proxy API with microversion > 2.35.
1501         original_image_ref = '155d900f-4e14-4e4c-a73d-069cbf4541e6'
1502         server_req_body = {
1503             'server': {
1504                 'imageRef': original_image_ref,
1505                 'flavorRef': '1',   # m1.tiny from DefaultFlavorsFixture,
1506                 'name': 'test_rebuild_with_image_novalidhost',
1507                 # We don't care about networking for this test. This requires
1508                 # microversion >= 2.37.
1509                 'networks': 'none'
1510             }
1511         }
1512         server = self.api.post_server(server_req_body)
1513         self._wait_for_state_change(self.api, server, 'ACTIVE')
1514 
1515         # Disable the host we're on so ComputeFilter would have ruled it out
1516         # normally
1517         self._disable_compute_for(server)
1518 
1519         # Now update the image metadata to be something that won't work with
1520         # the fake compute driver we're using since the fake driver has an
1521         # "x86_64" architecture.
1522         rebuild_image_ref = (
1523             nova.tests.unit.image.fake.AUTO_DISK_CONFIG_ENABLED_IMAGE_UUID)
1524         self.api.put_image_meta_key(
1525             rebuild_image_ref, 'hw_architecture', 'unicore32')
1526         # Now rebuild the server with that updated image and it should result
1527         # in a NoValidHost failure from the scheduler.
1528         rebuild_req_body = {
1529             'rebuild': {
1530                 'imageRef': rebuild_image_ref
1531             }
1532         }
1533         # Since we're using the CastAsCall fixture, the NoValidHost error
1534         # should actually come back to the API and result in a 500 error.
1535         # Normally the user would get a 202 response because nova-api RPC casts
1536         # to nova-conductor which RPC calls the scheduler which raises the
1537         # NoValidHost. We can mimic the end user way to figure out the failure
1538         # by looking for the failed 'rebuild' instance action event.
1539         self.api.api_post('/servers/%s/action' % server['id'],
1540                           rebuild_req_body, check_response_status=[500])
1541         # Look for the failed rebuild action.
1542         self._wait_for_action_fail_completion(
1543             server, instance_actions.REBUILD, 'rebuild_server',
1544             # Before microversion 2.51 events are only returned for instance
1545             # actions if you're an admin.
1546             self.api_fixture.admin_api)
1547         # Assert the server image_ref was rolled back on failure.
1548         server = self.api.get_server(server['id'])
1549         self.assertEqual(original_image_ref, server['image']['id'])
1550 
1551         # The server should be in ERROR state
1552         self.assertEqual('ERROR', server['status'])
1553         self.assertIn('No valid host', server['fault']['message'])
1554 
1555         # Rebuild it again with the same bad image to make sure it's rejected
1556         # again. Since we're using CastAsCall here, there is no 202 from the
1557         # API, and the exception from conductor gets passed back through the
1558         # API.
1559         ex = self.assertRaises(
1560             client.OpenStackApiException, self.api.api_post,
1561             '/servers/%s/action' % server['id'], rebuild_req_body)
1562         self.assertIn('NoValidHost', six.text_type(ex))
1563 
1564     # A rebuild to the same host should never attempt a rebuild claim.
1565     @mock.patch('nova.compute.resource_tracker.ResourceTracker.rebuild_claim',
1566                 new_callable=mock.NonCallableMock)
1567     def test_rebuild_with_new_image(self, mock_rebuild_claim):
1568         """Rebuilds a server with a different image which will run it through
1569         the scheduler to validate the image is still OK with the compute host
1570         that the instance is running on.
1571 
1572         Validates that additional resources are not allocated against the
1573         instance.host in Placement due to the rebuild on same host.
1574         """
1575         admin_api = self.api_fixture.admin_api
1576         admin_api.microversion = '2.53'
1577 
1578         def _get_provider_uuid_by_host(host):
1579             resp = admin_api.api_get(
1580                 'os-hypervisors?hypervisor_hostname_pattern=%s' % host).body
1581             return resp['hypervisors'][0]['id']
1582 
1583         def _get_provider_usages(provider_uuid):
1584             return self.placement_api.get(
1585                 '/resource_providers/%s/usages' % provider_uuid).body['usages']
1586 
1587         def _get_allocations_by_server_uuid(server_uuid):
1588             return self.placement_api.get(
1589                 '/allocations/%s' % server_uuid).body['allocations']
1590 
1591         def _set_provider_inventory(rp_uuid, resource_class, inventory):
1592             # Get the resource provider generation for the inventory update.
1593             rp = self.placement_api.get(
1594                 '/resource_providers/%s' % rp_uuid).body
1595             inventory['resource_provider_generation'] = rp['generation']
1596             return self.placement_api.put(
1597                 '/resource_providers/%s/inventories/%s' %
1598                 (rp_uuid, resource_class), inventory).body
1599 
1600         def assertFlavorMatchesAllocation(flavor, allocation):
1601             self.assertEqual(flavor['vcpus'], allocation['VCPU'])
1602             self.assertEqual(flavor['ram'], allocation['MEMORY_MB'])
1603             self.assertEqual(flavor['disk'], allocation['DISK_GB'])
1604 
1605         nodename = self.compute.manager._get_nodename(None)
1606         rp_uuid = _get_provider_uuid_by_host(nodename)
1607         # make sure we start with no usage on the compute node
1608         rp_usages = _get_provider_usages(rp_uuid)
1609         self.assertEqual({'VCPU': 0, 'MEMORY_MB': 0, 'DISK_GB': 0}, rp_usages)
1610 
1611         server_req_body = {
1612             'server': {
1613                 # We hard-code from a fake image since we can't get images
1614                 # via the compute /images proxy API with microversion > 2.35.
1615                 'imageRef': '155d900f-4e14-4e4c-a73d-069cbf4541e6',
1616                 'flavorRef': '1',   # m1.tiny from DefaultFlavorsFixture,
1617                 'name': 'test_rebuild_with_new_image',
1618                 # We don't care about networking for this test. This requires
1619                 # microversion >= 2.37.
1620                 'networks': 'none'
1621             }
1622         }
1623         server = self.api.post_server(server_req_body)
1624         self._wait_for_state_change(self.api, server, 'ACTIVE')
1625 
1626         flavor = self.api.api_get('/flavors/1').body['flavor']
1627 
1628         # make the compute node full and ensure rebuild still succeed
1629         _set_provider_inventory(rp_uuid, "VCPU", {"total": 1})
1630 
1631         # There should be usage for the server on the compute node now.
1632         rp_usages = _get_provider_usages(rp_uuid)
1633         assertFlavorMatchesAllocation(flavor, rp_usages)
1634         allocs = _get_allocations_by_server_uuid(server['id'])
1635         self.assertIn(rp_uuid, allocs)
1636         allocs = allocs[rp_uuid]['resources']
1637         assertFlavorMatchesAllocation(flavor, allocs)
1638 
1639         rebuild_image_ref = (
1640             nova.tests.unit.image.fake.AUTO_DISK_CONFIG_ENABLED_IMAGE_UUID)
1641         # Now rebuild the server with a different image.
1642         rebuild_req_body = {
1643             'rebuild': {
1644                 'imageRef': rebuild_image_ref
1645             }
1646         }
1647         self.api.api_post('/servers/%s/action' % server['id'],
1648                           rebuild_req_body)
1649         self._wait_for_server_parameter(
1650             self.api, server, {'OS-EXT-STS:task_state': None})
1651 
1652         # The usage and allocations should not have changed.
1653         rp_usages = _get_provider_usages(rp_uuid)
1654         assertFlavorMatchesAllocation(flavor, rp_usages)
1655 
1656         allocs = _get_allocations_by_server_uuid(server['id'])
1657         self.assertIn(rp_uuid, allocs)
1658         allocs = allocs[rp_uuid]['resources']
1659         assertFlavorMatchesAllocation(flavor, allocs)
1660 
1661     def test_volume_backed_rebuild_different_image(self):
1662         """Tests that trying to rebuild a volume-backed instance with a
1663         different image than what is in the root disk of the root volume
1664         will result in a 400 BadRequest error.
1665         """
1666         self.useFixture(nova_fixtures.CinderFixtureNewAttachFlow(self))
1667         # First create our server as normal.
1668         server_req_body = {
1669             # There is no imageRef because this is boot from volume.
1670             'server': {
1671                 'flavorRef': '1',  # m1.tiny from DefaultFlavorsFixture,
1672                 'name': 'test_volume_backed_rebuild_different_image',
1673                 # We don't care about networking for this test. This requires
1674                 # microversion >= 2.37.
1675                 'networks': 'none',
1676                 'block_device_mapping_v2': [{
1677                     'boot_index': 0,
1678                     'uuid':
1679                     nova_fixtures.CinderFixtureNewAttachFlow.IMAGE_BACKED_VOL,
1680                     'source_type': 'volume',
1681                     'destination_type': 'volume'
1682                 }]
1683             }
1684         }
1685         server = self.api.post_server(server_req_body)
1686         server = self._wait_for_state_change(self.api, server, 'ACTIVE')
1687         # For a volume-backed server, the image ref will be an empty string
1688         # in the server response.
1689         self.assertEqual('', server['image'])
1690 
1691         # Now rebuild the server with a different image than was used to create
1692         # our fake volume.
1693         rebuild_image_ref = (
1694             nova.tests.unit.image.fake.AUTO_DISK_CONFIG_ENABLED_IMAGE_UUID)
1695         rebuild_req_body = {
1696             'rebuild': {
1697                 'imageRef': rebuild_image_ref
1698             }
1699         }
1700         resp = self.api.api_post('/servers/%s/action' % server['id'],
1701                                  rebuild_req_body, check_response_status=[400])
1702         # Assert that we failed because of the image change and not something
1703         # else.
1704         self.assertIn('Unable to rebuild with a different image for a '
1705                       'volume-backed server', six.text_type(resp))
1706 
1707 
1708 class ProviderTreeTests(integrated_helpers.ProviderUsageBaseTestCase):
1709     compute_driver = 'fake.MediumFakeDriver'
1710 
1711     def setUp(self):
1712         super(ProviderTreeTests, self).setUp()
1713         # Before starting compute, placement has no providers registered
1714         self.assertEqual([], self._get_all_providers())
1715 
1716         # Start compute without mocking update_provider_tree. The fake driver
1717         # doesn't implement the method, so this will cause us to start with the
1718         # legacy get_available_resource()-based inventory discovery and
1719         # boostrapping of placement data.
1720         self.compute = self._start_compute(host='host1')
1721 
1722         # Mock out update_provider_tree *after* starting compute with the
1723         # (unmocked, default, unimplemented) version from the fake driver.
1724         _p = mock.patch.object(fake.MediumFakeDriver, 'update_provider_tree')
1725         self.addCleanup(_p.stop)
1726         self.mock_upt = _p.start()
1727 
1728         # The compute host should have been created in placement with
1729         # appropriate inventory and no traits
1730         rps = self._get_all_providers()
1731         self.assertEqual(1, len(rps))
1732         self.assertEqual(self.compute.host, rps[0]['name'])
1733         self.host_uuid = self._get_provider_uuid_by_host(self.compute.host)
1734         self.assertEqual({
1735             'DISK_GB': {
1736                 'total': 1028,
1737                 'allocation_ratio': 1.0,
1738                 'max_unit': 1028,
1739                 'min_unit': 1,
1740                 'reserved': 0,
1741                 'step_size': 1,
1742             },
1743             'MEMORY_MB': {
1744                 'total': 8192,
1745                 'allocation_ratio': 1.5,
1746                 'max_unit': 8192,
1747                 'min_unit': 1,
1748                 'reserved': 512,
1749                 'step_size': 1,
1750             },
1751             'VCPU': {
1752                 'total': 10,
1753                 'allocation_ratio': 16.0,
1754                 'max_unit': 10,
1755                 'min_unit': 1,
1756                 'reserved': 0,
1757                 'step_size': 1,
1758             },
1759         }, self._get_provider_inventory(self.host_uuid))
1760         self.assertItemsEqual(self.expected_fake_driver_capability_traits,
1761                               self._get_provider_traits(self.host_uuid))
1762 
1763     def _run_update_available_resource(self, startup):
1764         self.compute.rt.update_available_resource(
1765             context.get_admin_context(), self.compute.host, startup=startup)
1766 
1767     def _run_update_available_resource_and_assert_raises(
1768             self, exc=exception.ResourceProviderSyncFailed, startup=False):
1769         """Invoke ResourceTracker.update_available_resource and assert that it
1770         results in ResourceProviderSyncFailed.
1771 
1772         _run_periodicals is a little too high up in the call stack to be useful
1773         for this, because ResourceTracker.update_available_resource_for_node
1774         swallows all exceptions.
1775         """
1776         self.assertRaises(exc, self._run_update_available_resource, startup)
1777 
1778     def test_update_provider_tree_associated_info(self):
1779         """Inventory in some standard and custom resource classes.  Standard
1780         and custom traits.  Aggregates.  Custom resource class and trait get
1781         created; inventory, traits, and aggregates get set properly.
1782         """
1783         inv = {
1784             'VCPU': {
1785                 'total': 10,
1786                 'reserved': 0,
1787                 'min_unit': 1,
1788                 'max_unit': 2,
1789                 'step_size': 1,
1790                 'allocation_ratio': 10.0,
1791             },
1792             'MEMORY_MB': {
1793                 'total': 1048576,
1794                 'reserved': 2048,
1795                 'min_unit': 1024,
1796                 'max_unit': 131072,
1797                 'step_size': 1024,
1798                 'allocation_ratio': 1.0,
1799             },
1800             'CUSTOM_BANDWIDTH': {
1801                 'total': 1250000,
1802                 'reserved': 10000,
1803                 'min_unit': 5000,
1804                 'max_unit': 250000,
1805                 'step_size': 5000,
1806                 'allocation_ratio': 8.0,
1807             },
1808         }
1809         traits = set(['HW_CPU_X86_AVX', 'HW_CPU_X86_AVX2', 'CUSTOM_GOLD'])
1810         aggs = set([uuids.agg1, uuids.agg2])
1811 
1812         def update_provider_tree(prov_tree, nodename):
1813             prov_tree.update_inventory(self.compute.host, inv)
1814             prov_tree.update_traits(self.compute.host, traits)
1815             prov_tree.update_aggregates(self.compute.host, aggs)
1816         self.mock_upt.side_effect = update_provider_tree
1817 
1818         self.assertNotIn('CUSTOM_BANDWIDTH', self._get_all_resource_classes())
1819         self.assertNotIn('CUSTOM_GOLD', self._get_all_traits())
1820 
1821         self._run_periodics()
1822 
1823         self.assertIn('CUSTOM_BANDWIDTH', self._get_all_resource_classes())
1824         self.assertIn('CUSTOM_GOLD', self._get_all_traits())
1825         self.assertEqual(inv, self._get_provider_inventory(self.host_uuid))
1826         self.assertItemsEqual(
1827             traits.union(self.expected_fake_driver_capability_traits),
1828             self._get_provider_traits(self.host_uuid)
1829         )
1830         self.assertEqual(aggs,
1831                          set(self._get_provider_aggregates(self.host_uuid)))
1832 
1833     def _update_provider_tree_multiple_providers(self, startup=False,
1834                                                  do_reshape=False):
1835         """Make update_provider_tree create multiple providers, including an
1836         additional root as a sharing provider; and some descendants in the
1837         compute node's tree.
1838 
1839                    +---------------------------+   +--------------------------+
1840                    |uuid: self.host_uuid       |   |uuid: uuids.ssp           |
1841                    |name: self.compute.host    |   |name: 'ssp'               |
1842                    |inv: (per MediumFakeDriver)|   |inv: DISK_GB=500          |
1843                    |     VCPU=10               |...|traits: [MISC_SHARES_..., |
1844                    |     MEMORY_MB=8192        |   |         STORAGE_DISK_SSD]|
1845                    |     DISK_GB=1028          |   |aggs: [uuids.agg]         |
1846                    |aggs: [uuids.agg]          |   +--------------------------+
1847                    +---------------------------+
1848                          /                   \
1849              +-----------------+          +-----------------+
1850              |uuid: uuids.numa1|          |uuid: uuids.numa2|
1851              |name: 'numa1'    |          |name: 'numa2'    |
1852              |inv: VCPU=10     |          |inv: VCPU=20     |
1853              |     MEMORY_MB=1G|          |     MEMORY_MB=2G|
1854              +-----------------+          +-----------------+
1855                  /          \                    /         \
1856         +------------+  +------------+   +------------+  +------------+
1857         |uuid:       |  |uuid:       |   |uuid:       |  |uuid:       |
1858         | uuids.pf1_1|  | uuids.pf1_2|   | uuids.pf2_1|  | uuids.pf2_2|
1859         |name:       |  |name:       |   |name:       |  |name:       |
1860         | 'pf1_1'    |  | 'pf1_2'    |   | 'pf2_1'    |  | 'pf2_2'    |
1861         |inv:        |  |inv:        |   |inv:        |  |inv:        |
1862         | ..NET_VF: 2|  | ..NET_VF: 3|   | ..NET_VF: 3|  | ..NET_VF: 4|
1863         |traits:     |  |traits:     |   |traits:     |  |traits:     |
1864         | ..PHYSNET_0|  | ..PHYSNET_1|   | ..PHYSNET_0|  | ..PHYSNET_1|
1865         +------------+  +------------+   +------------+  +------------+
1866         """
1867         def update_provider_tree(prov_tree, nodename, allocations=None):
1868             if do_reshape and allocations is None:
1869                 raise exception.ReshapeNeeded()
1870 
1871             # Create a shared storage provider as a root
1872             prov_tree.new_root('ssp', uuids.ssp)
1873             prov_tree.update_traits(
1874                 'ssp', ['MISC_SHARES_VIA_AGGREGATE', 'STORAGE_DISK_SSD'])
1875             prov_tree.update_aggregates('ssp', [uuids.agg])
1876             prov_tree.update_inventory('ssp', {'DISK_GB': {'total': 500}})
1877             # Compute node is in the same aggregate
1878             prov_tree.update_aggregates(self.compute.host, [uuids.agg])
1879             # Create two NUMA nodes as children
1880             prov_tree.new_child('numa1', self.host_uuid, uuid=uuids.numa1)
1881             prov_tree.new_child('numa2', self.host_uuid, uuid=uuids.numa2)
1882             # Give the NUMA nodes the proc/mem inventory.  NUMA 2 has twice as
1883             # much as NUMA 1 (so we can validate later that everything is where
1884             # it should be).
1885             for n in (1, 2):
1886                 inv = {
1887                     'VCPU': {
1888                         'total': 10 * n,
1889                         'reserved': 0,
1890                         'min_unit': 1,
1891                         'max_unit': 2,
1892                         'step_size': 1,
1893                         'allocation_ratio': 10.0,
1894                     },
1895                     'MEMORY_MB': {
1896                          'total': 1048576 * n,
1897                          'reserved': 2048,
1898                          'min_unit': 512,
1899                          'max_unit': 131072,
1900                          'step_size': 512,
1901                          'allocation_ratio': 1.0,
1902                      },
1903                 }
1904                 prov_tree.update_inventory('numa%d' % n, inv)
1905             # Each NUMA node has two PFs providing VF inventory on one of two
1906             # networks
1907             for n in (1, 2):
1908                 for p in (1, 2):
1909                     name = 'pf%d_%d' % (n, p)
1910                     prov_tree.new_child(
1911                         name, getattr(uuids, 'numa%d' % n),
1912                         uuid=getattr(uuids, name))
1913                     trait = 'CUSTOM_PHYSNET_%d' % ((n + p) % 2)
1914                     prov_tree.update_traits(name, [trait])
1915                     inv = {
1916                         'SRIOV_NET_VF': {
1917                             'total': n + p,
1918                             'reserved': 0,
1919                             'min_unit': 1,
1920                             'max_unit': 1,
1921                             'step_size': 1,
1922                             'allocation_ratio': 1.0,
1923                         },
1924                     }
1925                     prov_tree.update_inventory(name, inv)
1926             if do_reshape:
1927                 # Clear out the compute node's inventory. Its VCPU and
1928                 # MEMORY_MB "moved" to the NUMA RPs and its DISK_GB "moved" to
1929                 # the shared storage provider.
1930                 prov_tree.update_inventory(self.host_uuid, {})
1931                 # Move all the allocations
1932                 for consumer_uuid, alloc_info in allocations.items():
1933                     allocs = alloc_info['allocations']
1934                     # All allocations should belong to the compute node.
1935                     self.assertEqual([self.host_uuid], list(allocs))
1936                     new_allocs = {}
1937                     for rc, amt in allocs[self.host_uuid]['resources'].items():
1938                         # Move VCPU to NUMA1 and MEMORY_MB to NUMA2. Bogus, but
1939                         # lets us prove stuff ends up where we tell it to go.
1940                         if rc == 'VCPU':
1941                             rp_uuid = uuids.numa1
1942                         elif rc == 'MEMORY_MB':
1943                             rp_uuid = uuids.numa2
1944                         elif rc == 'DISK_GB':
1945                             rp_uuid = uuids.ssp
1946                         else:
1947                             self.fail("Unexpected resource on compute node: "
1948                                       "%s=%d" % (rc, amt))
1949                         new_allocs[rp_uuid] = {
1950                             'resources': {rc: amt},
1951                         }
1952                     # Add a VF for the heck of it. Again bogus, but see above.
1953                     new_allocs[uuids.pf1_1] = {
1954                         'resources': {'SRIOV_NET_VF': 1}
1955                     }
1956                     # Now replace just the allocations, leaving the other stuff
1957                     # (proj/user ID and consumer generation) alone
1958                     alloc_info['allocations'] = new_allocs
1959 
1960         self.mock_upt.side_effect = update_provider_tree
1961 
1962         if startup:
1963             self.restart_compute_service(self.compute)
1964         else:
1965             self._run_update_available_resource(False)
1966 
1967         # Create a dict, keyed by provider UUID, of all the providers
1968         rps_by_uuid = {}
1969         for rp_dict in self._get_all_providers():
1970             rps_by_uuid[rp_dict['uuid']] = rp_dict
1971 
1972         # All and only the expected providers got created.
1973         all_uuids = set([self.host_uuid, uuids.ssp, uuids.numa1, uuids.numa2,
1974                          uuids.pf1_1, uuids.pf1_2, uuids.pf2_1, uuids.pf2_2])
1975         self.assertEqual(all_uuids, set(rps_by_uuid))
1976 
1977         # Validate tree roots
1978         tree_uuids = [self.host_uuid, uuids.numa1, uuids.numa2,
1979                       uuids.pf1_1, uuids.pf1_2, uuids.pf2_1, uuids.pf2_2]
1980         for tree_uuid in tree_uuids:
1981             self.assertEqual(self.host_uuid,
1982                              rps_by_uuid[tree_uuid]['root_provider_uuid'])
1983         self.assertEqual(uuids.ssp,
1984                          rps_by_uuid[uuids.ssp]['root_provider_uuid'])
1985 
1986         # SSP has the right traits
1987         self.assertEqual(
1988             set(['MISC_SHARES_VIA_AGGREGATE', 'STORAGE_DISK_SSD']),
1989             set(self._get_provider_traits(uuids.ssp)))
1990 
1991         # SSP has the right inventory
1992         self.assertEqual(
1993             500, self._get_provider_inventory(uuids.ssp)['DISK_GB']['total'])
1994 
1995         # SSP and compute are in the same aggregate
1996         agg_uuids = set([self.host_uuid, uuids.ssp])
1997         for uuid in agg_uuids:
1998             self.assertEqual(set([uuids.agg]),
1999                              set(self._get_provider_aggregates(uuid)))
2000 
2001         # The rest aren't in aggregates
2002         for uuid in (all_uuids - agg_uuids):
2003             self.assertEqual(set(), set(self._get_provider_aggregates(uuid)))
2004 
2005         # NUMAs have the right inventory and parentage
2006         for n in (1, 2):
2007             numa_uuid = getattr(uuids, 'numa%d' % n)
2008             self.assertEqual(self.host_uuid,
2009                              rps_by_uuid[numa_uuid]['parent_provider_uuid'])
2010             inv = self._get_provider_inventory(numa_uuid)
2011             self.assertEqual(10 * n, inv['VCPU']['total'])
2012             self.assertEqual(1048576 * n, inv['MEMORY_MB']['total'])
2013 
2014         # PFs have the right inventory, physnet, and parentage
2015         self.assertEqual(uuids.numa1,
2016                          rps_by_uuid[uuids.pf1_1]['parent_provider_uuid'])
2017         self.assertEqual(['CUSTOM_PHYSNET_0'],
2018                          self._get_provider_traits(uuids.pf1_1))
2019         self.assertEqual(
2020             2,
2021             self._get_provider_inventory(uuids.pf1_1)['SRIOV_NET_VF']['total'])
2022 
2023         self.assertEqual(uuids.numa1,
2024                          rps_by_uuid[uuids.pf1_2]['parent_provider_uuid'])
2025         self.assertEqual(['CUSTOM_PHYSNET_1'],
2026                          self._get_provider_traits(uuids.pf1_2))
2027         self.assertEqual(
2028             3,
2029             self._get_provider_inventory(uuids.pf1_2)['SRIOV_NET_VF']['total'])
2030 
2031         self.assertEqual(uuids.numa2,
2032                          rps_by_uuid[uuids.pf2_1]['parent_provider_uuid'])
2033         self.assertEqual(['CUSTOM_PHYSNET_1'],
2034                          self._get_provider_traits(uuids.pf2_1))
2035         self.assertEqual(
2036             3,
2037             self._get_provider_inventory(uuids.pf2_1)['SRIOV_NET_VF']['total'])
2038 
2039         self.assertEqual(uuids.numa2,
2040                          rps_by_uuid[uuids.pf2_2]['parent_provider_uuid'])
2041         self.assertEqual(['CUSTOM_PHYSNET_0'],
2042                          self._get_provider_traits(uuids.pf2_2))
2043         self.assertEqual(
2044             4,
2045             self._get_provider_inventory(uuids.pf2_2)['SRIOV_NET_VF']['total'])
2046 
2047         # Compute don't have any extra traits
2048         self.assertItemsEqual(self.expected_fake_driver_capability_traits,
2049                               self._get_provider_traits(self.host_uuid))
2050 
2051         # NUMAs don't have any traits
2052         for uuid in (uuids.numa1, uuids.numa2):
2053             self.assertEqual([], self._get_provider_traits(uuid))
2054 
2055     def test_update_provider_tree_multiple_providers(self):
2056         self._update_provider_tree_multiple_providers()
2057 
2058     def test_update_provider_tree_multiple_providers_startup(self):
2059         """The above works the same for startup when no reshape requested."""
2060         self._update_provider_tree_multiple_providers(startup=True)
2061 
2062     def test_update_provider_tree_bogus_resource_class(self):
2063         def update_provider_tree(prov_tree, nodename):
2064             prov_tree.update_inventory(self.compute.host, {'FOO': {}})
2065         self.mock_upt.side_effect = update_provider_tree
2066 
2067         rcs = self._get_all_resource_classes()
2068         self.assertIn('VCPU', rcs)
2069         self.assertNotIn('FOO', rcs)
2070 
2071         self._run_update_available_resource_and_assert_raises()
2072 
2073         rcs = self._get_all_resource_classes()
2074         self.assertIn('VCPU', rcs)
2075         self.assertNotIn('FOO', rcs)
2076 
2077     def test_update_provider_tree_bogus_trait(self):
2078         def update_provider_tree(prov_tree, nodename):
2079             prov_tree.update_traits(self.compute.host, ['FOO'])
2080         self.mock_upt.side_effect = update_provider_tree
2081 
2082         traits = self._get_all_traits()
2083         self.assertIn('HW_CPU_X86_AVX', traits)
2084         self.assertNotIn('FOO', traits)
2085 
2086         self._run_update_available_resource_and_assert_raises()
2087 
2088         traits = self._get_all_traits()
2089         self.assertIn('HW_CPU_X86_AVX', traits)
2090         self.assertNotIn('FOO', traits)
2091 
2092     def _create_instance(self, flavor):
2093         server_req = self._build_minimal_create_server_request(
2094             self.api, 'some-server', flavor_id=flavor['id'],
2095             image_uuid='155d900f-4e14-4e4c-a73d-069cbf4541e6',
2096             networks='none', az='nova:host1')
2097         inst = self.api.post_server({'server': server_req})
2098         return self._wait_for_state_change(self.admin_api, inst, 'ACTIVE')
2099 
2100     def test_reshape(self):
2101         """On startup, virt driver signals it needs to reshape, then does so.
2102 
2103         This test creates a couple of instances so there are allocations to be
2104         moved by the reshape operation. Then we do the reshape and make sure
2105         the inventories and allocations end up where they should.
2106         """
2107         # First let's create some instances so we have allocations to move.
2108         flavors = self.api.get_flavors()
2109         inst1 = self._create_instance(flavors[0])
2110         inst2 = self._create_instance(flavors[1])
2111 
2112         # Instance create calls RT._update, which calls
2113         # driver.update_provider_tree, which is currently mocked to a no-op.
2114         self.assertEqual(2, self.mock_upt.call_count)
2115         self.mock_upt.reset_mock()
2116 
2117         # Hit the reshape.
2118         self._update_provider_tree_multiple_providers(startup=True,
2119                                                       do_reshape=True)
2120 
2121         # Check the final allocations
2122         # The compute node provider should have *no* allocations.
2123         self.assertEqual(
2124             {}, self._get_allocations_by_provider_uuid(self.host_uuid))
2125         # And no inventory
2126         self.assertEqual({}, self._get_provider_inventory(self.host_uuid))
2127         # NUMA1 got all the VCPU
2128         self.assertEqual(
2129             {inst1['id']: {'resources': {'VCPU': 1}},
2130              inst2['id']: {'resources': {'VCPU': 1}}},
2131             self._get_allocations_by_provider_uuid(uuids.numa1))
2132         # NUMA2 got all the memory
2133         self.assertEqual(
2134             {inst1['id']: {'resources': {'MEMORY_MB': 512}},
2135              inst2['id']: {'resources': {'MEMORY_MB': 2048}}},
2136             self._get_allocations_by_provider_uuid(uuids.numa2))
2137         # Disk resource ended up on the shared storage provider
2138         self.assertEqual(
2139             {inst1['id']: {'resources': {'DISK_GB': 1}},
2140              inst2['id']: {'resources': {'DISK_GB': 20}}},
2141             self._get_allocations_by_provider_uuid(uuids.ssp))
2142         # We put VFs on the first PF in NUMA1
2143         self.assertEqual(
2144             {inst1['id']: {'resources': {'SRIOV_NET_VF': 1}},
2145              inst2['id']: {'resources': {'SRIOV_NET_VF': 1}}},
2146             self._get_allocations_by_provider_uuid(uuids.pf1_1))
2147         self.assertEqual(
2148             {}, self._get_allocations_by_provider_uuid(uuids.pf1_2))
2149         self.assertEqual(
2150             {}, self._get_allocations_by_provider_uuid(uuids.pf2_1))
2151         self.assertEqual(
2152             {}, self._get_allocations_by_provider_uuid(uuids.pf2_2))
2153         # This is *almost* redundant - but it makes sure the instances don't
2154         # have extra allocations from some other provider.
2155         self.assertEqual(
2156             {
2157                 uuids.numa1: {
2158                     'resources': {'VCPU': 1},
2159                     # Don't care about the generations - rely on placement db
2160                     # tests to validate that those behave properly.
2161                     'generation': mock.ANY,
2162                 },
2163                 uuids.numa2: {
2164                     'resources': {'MEMORY_MB': 512},
2165                     'generation': mock.ANY,
2166                 },
2167                 uuids.ssp: {
2168                     'resources': {'DISK_GB': 1},
2169                     'generation': mock.ANY,
2170                 },
2171                 uuids.pf1_1: {
2172                     'resources': {'SRIOV_NET_VF': 1},
2173                     'generation': mock.ANY,
2174                 },
2175             }, self._get_allocations_by_server_uuid(inst1['id']))
2176         self.assertEqual(
2177             {
2178                 uuids.numa1: {
2179                     'resources': {'VCPU': 1},
2180                     'generation': mock.ANY,
2181                 },
2182                 uuids.numa2: {
2183                     'resources': {'MEMORY_MB': 2048},
2184                     'generation': mock.ANY,
2185                 },
2186                 uuids.ssp: {
2187                     'resources': {'DISK_GB': 20},
2188                     'generation': mock.ANY,
2189                 },
2190                 uuids.pf1_1: {
2191                     'resources': {'SRIOV_NET_VF': 1},
2192                     'generation': mock.ANY,
2193                 },
2194             }, self._get_allocations_by_server_uuid(inst2['id']))
2195 
2196         # The first call raises ReshapeNeeded, resulting in the second.
2197         self.assertEqual(2, self.mock_upt.call_count)
2198         # The expected value of the allocations kwarg to update_provider_tree
2199         # for that second call:
2200         exp_allocs = {
2201             inst1['id']: {
2202                 'allocations': {
2203                     uuids.numa1: {'resources': {'VCPU': 1}},
2204                     uuids.numa2: {'resources': {'MEMORY_MB': 512}},
2205                     uuids.ssp: {'resources': {'DISK_GB': 1}},
2206                     uuids.pf1_1: {'resources': {'SRIOV_NET_VF': 1}},
2207                 },
2208                 'consumer_generation': mock.ANY,
2209                 'project_id': mock.ANY,
2210                 'user_id': mock.ANY,
2211             },
2212             inst2['id']: {
2213                 'allocations': {
2214                     uuids.numa1: {'resources': {'VCPU': 1}},
2215                     uuids.numa2: {'resources': {'MEMORY_MB': 2048}},
2216                     uuids.ssp: {'resources': {'DISK_GB': 20}},
2217                     uuids.pf1_1: {'resources': {'SRIOV_NET_VF': 1}},
2218                 },
2219                 'consumer_generation': mock.ANY,
2220                 'project_id': mock.ANY,
2221                 'user_id': mock.ANY,
2222             },
2223         }
2224         self.mock_upt.assert_has_calls([
2225             mock.call(mock.ANY, 'host1'),
2226             mock.call(mock.ANY, 'host1', allocations=exp_allocs),
2227         ])
2228 
2229 
2230 class TraitsTrackingTests(integrated_helpers.ProviderUsageBaseTestCase):
2231     compute_driver = 'fake.SmallFakeDriver'
2232 
2233     fake_caps = {
2234         'supports_attach_interface': True,
2235         'supports_device_tagging': False,
2236     }
2237 
2238     def _mock_upt(self, traits_to_add, traits_to_remove):
2239         """Set up the compute driver with a fake update_provider_tree()
2240         which injects the given traits into the provider tree
2241         """
2242         original_upt = fake.SmallFakeDriver.update_provider_tree
2243 
2244         def fake_upt(self2, ptree, nodename, allocations=None):
2245             original_upt(self2, ptree, nodename, allocations)
2246             LOG.debug("injecting traits via fake update_provider_tree(): %s",
2247                       traits_to_add)
2248             ptree.add_traits(nodename, *traits_to_add)
2249             LOG.debug("removing traits via fake update_provider_tree(): %s",
2250                       traits_to_remove)
2251             ptree.remove_traits(nodename, *traits_to_remove)
2252 
2253         self.stub_out('nova.virt.fake.FakeDriver.update_provider_tree',
2254                       fake_upt)
2255 
2256     @mock.patch.dict(fake.SmallFakeDriver.capabilities, clear=True,
2257                      values=fake_caps)
2258     def test_resource_provider_traits(self):
2259         """Test that the compute service reports traits via driver
2260         capabilities and registers them on the compute host resource
2261         provider in the placement API.
2262         """
2263         custom_trait = 'CUSTOM_FOO'
2264         ptree_traits = [custom_trait, 'HW_CPU_X86_VMX']
2265 
2266         global_traits = self._get_all_traits()
2267         self.assertNotIn(custom_trait, global_traits)
2268         self.assertIn(os_traits.COMPUTE_NET_ATTACH_INTERFACE, global_traits)
2269         self.assertIn(os_traits.COMPUTE_DEVICE_TAGGING, global_traits)
2270         self.assertEqual([], self._get_all_providers())
2271 
2272         self._mock_upt(ptree_traits, [])
2273 
2274         self.compute = self._start_compute(host='host1')
2275 
2276         rp_uuid = self._get_provider_uuid_by_host('host1')
2277         expected_traits = set(
2278             ptree_traits + [os_traits.COMPUTE_NET_ATTACH_INTERFACE]
2279         )
2280         self.assertItemsEqual(expected_traits,
2281                               self._get_provider_traits(rp_uuid))
2282         global_traits = self._get_all_traits()
2283         # CUSTOM_FOO is now a registered trait because the virt driver
2284         # reported it.
2285         self.assertIn(custom_trait, global_traits)
2286 
2287         # Now simulate user deletion of driver-provided traits from
2288         # the compute node provider.
2289         expected_traits.remove(custom_trait)
2290         expected_traits.remove(os_traits.COMPUTE_NET_ATTACH_INTERFACE)
2291         self._set_provider_traits(rp_uuid, list(expected_traits))
2292         self.assertItemsEqual(expected_traits,
2293                               self._get_provider_traits(rp_uuid))
2294 
2295         # The above trait deletions are simulations of an out-of-band
2296         # placement operation, as if the operator used the CLI.  So
2297         # now we have to "SIGHUP the compute process" to clear the
2298         # report client cache so the subsequent update picks up the
2299         # changes.
2300         self.compute.manager.reset()
2301 
2302         # Add the traits back so that the mock update_provider_tree()
2303         # can reinject them.
2304         expected_traits.update(
2305             [custom_trait, os_traits.COMPUTE_NET_ATTACH_INTERFACE])
2306 
2307         # Now when we run the periodic update task, the trait should
2308         # reappear in the provider tree and get synced back to
2309         # placement.
2310         self._run_periodics()
2311 
2312         self.assertItemsEqual(expected_traits,
2313                               self._get_provider_traits(rp_uuid))
2314         global_traits = self._get_all_traits()
2315         self.assertIn(custom_trait, global_traits)
2316         self.assertIn(os_traits.COMPUTE_NET_ATTACH_INTERFACE, global_traits)
2317 
2318     @mock.patch.dict(fake.SmallFakeDriver.capabilities, clear=True,
2319                      values=fake_caps)
2320     def test_admin_traits_preserved(self):
2321         """Test that if admin externally sets traits on the resource provider
2322         then the compute periodic doesn't remove them from placement.
2323         """
2324         admin_trait = 'CUSTOM_TRAIT_FROM_ADMIN'
2325         self._create_trait(admin_trait)
2326         global_traits = self._get_all_traits()
2327         self.assertIn(admin_trait, global_traits)
2328 
2329         self.compute = self._start_compute(host='host1')
2330         rp_uuid = self._get_provider_uuid_by_host('host1')
2331         traits = self._get_provider_traits(rp_uuid)
2332         traits.append(admin_trait)
2333         self._set_provider_traits(rp_uuid, traits)
2334         self.assertIn(admin_trait, self._get_provider_traits(rp_uuid))
2335 
2336         # SIGHUP the compute process to clear the report client
2337         # cache, so the subsequent periodic update recalculates everything.
2338         self.compute.manager.reset()
2339 
2340         self._run_periodics()
2341         self.assertIn(admin_trait, self._get_provider_traits(rp_uuid))
2342 
2343     @mock.patch.dict(fake.SmallFakeDriver.capabilities, clear=True,
2344                      values=fake_caps)
2345     def test_driver_removing_support_for_trait_via_capability(self):
2346         """Test that if a driver initially reports a trait via a supported
2347         capability, then at the next periodic update doesn't report
2348         support for it again, it gets removed from the provider in the
2349         placement service.
2350         """
2351         self.compute = self._start_compute(host='host1')
2352         rp_uuid = self._get_provider_uuid_by_host('host1')
2353         trait = os_traits.COMPUTE_NET_ATTACH_INTERFACE
2354         self.assertIn(trait, self._get_provider_traits(rp_uuid))
2355 
2356         new_caps = dict(fake.SmallFakeDriver.capabilities,
2357                         **{'supports_attach_interface': False})
2358         with mock.patch.dict(fake.SmallFakeDriver.capabilities, new_caps):
2359             self._run_periodics()
2360 
2361         self.assertNotIn(trait, self._get_provider_traits(rp_uuid))
2362 
2363     def test_driver_removing_trait_via_upt(self):
2364         """Test that if a driver reports a trait via update_provider_tree()
2365         initially, but at the next periodic update doesn't report it
2366         again, that it gets removed from placement.
2367         """
2368         custom_trait = "CUSTOM_TRAIT_FROM_DRIVER"
2369         standard_trait = os_traits.HW_CPU_X86_SGX
2370         self._mock_upt([custom_trait, standard_trait], [])
2371 
2372         self.compute = self._start_compute(host='host1')
2373         rp_uuid = self._get_provider_uuid_by_host('host1')
2374         self.assertIn(custom_trait, self._get_provider_traits(rp_uuid))
2375         self.assertIn(standard_trait, self._get_provider_traits(rp_uuid))
2376 
2377         # Now change the fake update_provider_tree() from injecting the
2378         # traits to removing them, and run the periodic update.
2379         self._mock_upt([], [custom_trait, standard_trait])
2380         self._run_periodics()
2381 
2382         self.assertNotIn(custom_trait, self._get_provider_traits(rp_uuid))
2383         self.assertNotIn(standard_trait, self._get_provider_traits(rp_uuid))
2384 
2385     @mock.patch.dict(fake.SmallFakeDriver.capabilities, clear=True,
2386                      values=fake_caps)
2387     def test_driver_removes_unsupported_trait_from_admin(self):
2388         """Test that if an admin adds a trait corresponding to a
2389         capability which is unsupported, then if the provider cache is
2390         reset, the driver will remove it during the next update.
2391         """
2392         self.compute = self._start_compute(host='host1')
2393         rp_uuid = self._get_provider_uuid_by_host('host1')
2394 
2395         traits = self._get_provider_traits(rp_uuid)
2396         trait = os_traits.COMPUTE_DEVICE_TAGGING
2397         self.assertNotIn(trait, traits)
2398 
2399         # Simulate an admin associating the trait with the host via
2400         # the placement API.
2401         traits.append(trait)
2402         self._set_provider_traits(rp_uuid, traits)
2403 
2404         # Check that worked.
2405         traits = self._get_provider_traits(rp_uuid)
2406         self.assertIn(trait, traits)
2407 
2408         # SIGHUP the compute process to clear the report client
2409         # cache, so the subsequent periodic update recalculates everything.
2410         self.compute.manager.reset()
2411 
2412         self._run_periodics()
2413         self.assertNotIn(trait, self._get_provider_traits(rp_uuid))
2414 
2415 
2416 class ServerMovingTests(integrated_helpers.ProviderUsageBaseTestCase):
2417     """Tests moving servers while checking the resource allocations and usages
2418 
2419     These tests use two compute hosts. Boot a server on one of them then try to
2420     move the server to the other. At every step resource allocation of the
2421     server and the resource usages of the computes are queried from placement
2422     API and asserted.
2423     """
2424 
2425     REQUIRES_LOCKING = True
2426     # NOTE(danms): The test defaults to using SmallFakeDriver,
2427     # which only has one vcpu, which can't take the doubled allocation
2428     # we're now giving it. So, use the bigger MediumFakeDriver here.
2429     compute_driver = 'fake.MediumFakeDriver'
2430 
2431     def setUp(self):
2432         super(ServerMovingTests, self).setUp()
2433         fake_notifier.stub_notifier(self)
2434         self.addCleanup(fake_notifier.reset)
2435 
2436         self.compute1 = self._start_compute(host='host1')
2437         self.compute2 = self._start_compute(host='host2')
2438 
2439         flavors = self.api.get_flavors()
2440         self.flavor1 = flavors[0]
2441         self.flavor2 = flavors[1]
2442         # create flavor3 which has less MEMORY_MB but more DISK_GB than flavor2
2443         flavor_body = {'flavor':
2444                            {'name': 'test_flavor3',
2445                             'ram': int(self.flavor2['ram'] / 2),
2446                             'vcpus': 1,
2447                             'disk': self.flavor2['disk'] * 2,
2448                             'id': 'a22d5517-147c-4147-a0d1-e698df5cd4e3'
2449                             }}
2450 
2451         self.flavor3 = self.api.post_flavor(flavor_body)
2452 
2453     def _other_hostname(self, host):
2454         other_host = {'host1': 'host2',
2455                       'host2': 'host1'}
2456         return other_host[host]
2457 
2458     def _run_periodics(self):
2459         # NOTE(jaypipes): We always run periodics in the same order: first on
2460         # compute1, then on compute2. However, we want to test scenarios when
2461         # the periodics run at different times during mover operations. This is
2462         # why we have the "reverse" tests which simply switch the source and
2463         # dest host while keeping the order in which we run the
2464         # periodics. This effectively allows us to test the matrix of timing
2465         # scenarios during move operations.
2466         ctx = context.get_admin_context()
2467         LOG.info('Running periodic for compute1 (%s)',
2468             self.compute1.manager.host)
2469         self.compute1.manager.update_available_resource(ctx)
2470         LOG.info('Running periodic for compute2 (%s)',
2471             self.compute2.manager.host)
2472         self.compute2.manager.update_available_resource(ctx)
2473         LOG.info('Finished with periodics')
2474 
2475     def test_resize_revert(self):
2476         self._test_resize_revert(dest_hostname='host1')
2477 
2478     def test_resize_revert_reverse(self):
2479         self._test_resize_revert(dest_hostname='host2')
2480 
2481     def test_resize_confirm(self):
2482         self._test_resize_confirm(dest_hostname='host1')
2483 
2484     def test_resize_confirm_reverse(self):
2485         self._test_resize_confirm(dest_hostname='host2')
2486 
2487     def _resize_and_check_allocations(self, server, old_flavor, new_flavor,
2488             source_rp_uuid, dest_rp_uuid):
2489         self.flags(allow_resize_to_same_host=False)
2490         resize_req = {
2491             'resize': {
2492                 'flavorRef': new_flavor['id']
2493             }
2494         }
2495         self._move_and_check_allocations(
2496             server, request=resize_req, old_flavor=old_flavor,
2497             new_flavor=new_flavor, source_rp_uuid=source_rp_uuid,
2498             dest_rp_uuid=dest_rp_uuid)
2499 
2500     def _test_resize_revert(self, dest_hostname):
2501         source_hostname = self._other_hostname(dest_hostname)
2502         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2503         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
2504 
2505         server = self._boot_and_check_allocations(self.flavor1,
2506             source_hostname)
2507 
2508         self._resize_and_check_allocations(server, self.flavor1, self.flavor2,
2509             source_rp_uuid, dest_rp_uuid)
2510 
2511         # Revert the resize and check the usages
2512         post = {'revertResize': None}
2513         self.api.post_server_action(server['id'], post)
2514         self._wait_for_state_change(self.api, server, 'ACTIVE')
2515 
2516         # Make sure the RequestSpec.flavor matches the original flavor.
2517         ctxt = context.get_admin_context()
2518         reqspec = objects.RequestSpec.get_by_instance_uuid(ctxt, server['id'])
2519         self.assertEqual(self.flavor1['id'], reqspec.flavor.flavorid)
2520 
2521         self._run_periodics()
2522 
2523         # the original host expected to have the old resource allocation
2524         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
2525 
2526         self.assertRequestMatchesUsage({'VCPU': 0,
2527                                         'MEMORY_MB': 0,
2528                                         'DISK_GB': 0}, dest_rp_uuid)
2529 
2530         # Check that the server only allocates resource from the original host
2531         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
2532                                            source_rp_uuid)
2533 
2534         self._delete_and_check_allocations(server)
2535 
2536     def _test_resize_confirm(self, dest_hostname):
2537         source_hostname = self._other_hostname(dest_hostname)
2538         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2539         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
2540 
2541         server = self._boot_and_check_allocations(self.flavor1,
2542             source_hostname)
2543 
2544         self._resize_and_check_allocations(server, self.flavor1, self.flavor2,
2545             source_rp_uuid, dest_rp_uuid)
2546 
2547         # Confirm the resize and check the usages
2548         post = {'confirmResize': None}
2549         self.api.post_server_action(
2550             server['id'], post, check_response_status=[204])
2551         self._wait_for_state_change(self.api, server, 'ACTIVE')
2552 
2553         # After confirming, we should have an allocation only on the
2554         # destination host
2555 
2556         # The target host usage should be according to the new flavor
2557         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor2)
2558         self.assertRequestMatchesUsage({'VCPU': 0,
2559                                         'MEMORY_MB': 0,
2560                                         'DISK_GB': 0}, source_rp_uuid)
2561 
2562         # and the target host allocation should be according to the new flavor
2563         self.assertFlavorMatchesAllocation(self.flavor2, server['id'],
2564                                            dest_rp_uuid)
2565 
2566         self._run_periodics()
2567 
2568         # Check we're still accurate after running the periodics
2569 
2570         # and the target host usage should be according to the new flavor
2571         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor2)
2572         self.assertRequestMatchesUsage({'VCPU': 0,
2573                                         'MEMORY_MB': 0,
2574                                         'DISK_GB': 0}, source_rp_uuid)
2575 
2576         # and the server allocates only from the target host
2577         self.assertFlavorMatchesAllocation(self.flavor2, server['id'],
2578                                            dest_rp_uuid)
2579 
2580         self._delete_and_check_allocations(server)
2581 
2582     def test_resize_revert_same_host(self):
2583         # make sure that the test only uses a single host
2584         compute2_service_id = self.admin_api.get_services(
2585             host=self.compute2.host, binary='nova-compute')[0]['id']
2586         self.admin_api.put_service(compute2_service_id, {'status': 'disabled'})
2587 
2588         hostname = self.compute1.manager.host
2589         rp_uuid = self._get_provider_uuid_by_host(hostname)
2590 
2591         server = self._boot_and_check_allocations(self.flavor2, hostname)
2592 
2593         self._resize_to_same_host_and_check_allocations(
2594             server, self.flavor2, self.flavor3, rp_uuid)
2595 
2596         # Revert the resize and check the usages
2597         post = {'revertResize': None}
2598         self.api.post_server_action(server['id'], post)
2599         self._wait_for_state_change(self.api, server, 'ACTIVE')
2600 
2601         self._run_periodics()
2602 
2603         # after revert only allocations due to the old flavor should remain
2604         self.assertFlavorMatchesUsage(rp_uuid, self.flavor2)
2605 
2606         self.assertFlavorMatchesAllocation(self.flavor2, server['id'],
2607                                            rp_uuid)
2608 
2609         self._delete_and_check_allocations(server)
2610 
2611     def test_resize_confirm_same_host(self):
2612         # make sure that the test only uses a single host
2613         compute2_service_id = self.admin_api.get_services(
2614             host=self.compute2.host, binary='nova-compute')[0]['id']
2615         self.admin_api.put_service(compute2_service_id, {'status': 'disabled'})
2616 
2617         hostname = self.compute1.manager.host
2618         rp_uuid = self._get_provider_uuid_by_host(hostname)
2619 
2620         server = self._boot_and_check_allocations(self.flavor2, hostname)
2621 
2622         self._resize_to_same_host_and_check_allocations(
2623             server, self.flavor2, self.flavor3, rp_uuid)
2624 
2625         # Confirm the resize and check the usages
2626         post = {'confirmResize': None}
2627         self.api.post_server_action(
2628             server['id'], post, check_response_status=[204])
2629         self._wait_for_state_change(self.api, server, 'ACTIVE')
2630 
2631         self._run_periodics()
2632 
2633         # after confirm only allocations due to the new flavor should remain
2634         self.assertFlavorMatchesUsage(rp_uuid, self.flavor3)
2635 
2636         self.assertFlavorMatchesAllocation(self.flavor3, server['id'],
2637                                            rp_uuid)
2638 
2639         self._delete_and_check_allocations(server)
2640 
2641     def test_resize_not_enough_resource(self):
2642         # Try to resize to a flavor that requests more VCPU than what the
2643         # compute hosts has available and expect the resize to fail
2644 
2645         flavor_body = {'flavor':
2646                            {'name': 'test_too_big_flavor',
2647                             'ram': 1024,
2648                             'vcpus': fake.MediumFakeDriver.vcpus + 1,
2649                             'disk': 20,
2650                             }}
2651 
2652         big_flavor = self.api.post_flavor(flavor_body)
2653 
2654         dest_hostname = self.compute2.host
2655         source_hostname = self._other_hostname(dest_hostname)
2656         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2657         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
2658 
2659         server = self._boot_and_check_allocations(
2660             self.flavor1, source_hostname)
2661 
2662         self.flags(allow_resize_to_same_host=False)
2663         resize_req = {
2664             'resize': {
2665                 'flavorRef': big_flavor['id']
2666             }
2667         }
2668 
2669         resp = self.api.post_server_action(
2670             server['id'], resize_req, check_response_status=[400])
2671         self.assertEqual(
2672             resp['badRequest']['message'],
2673             "No valid host was found. No valid host found for resize")
2674         server = self.admin_api.get_server(server['id'])
2675         self.assertEqual(source_hostname, server['OS-EXT-SRV-ATTR:host'])
2676 
2677         # only the source host shall have usages after the failed resize
2678         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
2679 
2680         # Check that the other provider has no usage
2681         self.assertRequestMatchesUsage(
2682             {'VCPU': 0,
2683              'MEMORY_MB': 0,
2684              'DISK_GB': 0}, dest_rp_uuid)
2685 
2686         # Check that the server only allocates resource from the host it is
2687         # booted on
2688         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
2689                                            source_rp_uuid)
2690 
2691         self._delete_and_check_allocations(server)
2692 
2693     def test_resize_delete_while_verify(self):
2694         """Test scenario where the server is deleted while in the
2695         VERIFY_RESIZE state and ensures the allocations are properly
2696         cleaned up from the source and target compute node resource providers.
2697         The _confirm_resize_on_deleting() method in the API is actually
2698         responsible for making sure the migration-based allocations get
2699         cleaned up by confirming the resize on the source host before deleting
2700         the server from the target host.
2701         """
2702         dest_hostname = 'host2'
2703         source_hostname = self._other_hostname(dest_hostname)
2704         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2705         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
2706 
2707         server = self._boot_and_check_allocations(self.flavor1,
2708                                                   source_hostname)
2709 
2710         self._resize_and_check_allocations(server, self.flavor1, self.flavor2,
2711                                            source_rp_uuid, dest_rp_uuid)
2712 
2713         self._delete_and_check_allocations(server)
2714 
2715     def test_resize_confirm_assert_hypervisor_usage_no_periodics(self):
2716         """Resize confirm test for bug 1818914 to make sure the tracked
2717         resource usage in the os-hypervisors API (not placement) is as
2718         expected during a confirmed resize. This intentionally does not
2719         use _test_resize_confirm in order to avoid running periodics.
2720         """
2721         # There should be no usage from a server on either hypervisor.
2722         source_rp_uuid = self._get_provider_uuid_by_host('host1')
2723         dest_rp_uuid = self._get_provider_uuid_by_host('host2')
2724         no_usage = {'vcpus': 0, 'disk': 0, 'ram': 0}
2725         for rp_uuid in (source_rp_uuid, dest_rp_uuid):
2726             self.assert_hypervisor_usage(
2727                 rp_uuid, no_usage, volume_backed=False)
2728 
2729         # Create the server and wait for it to be ACTIVE.
2730         server = self._boot_and_check_allocations(self.flavor1, 'host1')
2731 
2732         # There should be resource usage for flavor1 on the source host.
2733         self.assert_hypervisor_usage(
2734             source_rp_uuid, self.flavor1, volume_backed=False)
2735         # And still no usage on the dest host.
2736         self.assert_hypervisor_usage(
2737             dest_rp_uuid, no_usage, volume_backed=False)
2738 
2739         # Resize the server to flavor2 and wait for VERIFY_RESIZE.
2740         self.flags(allow_resize_to_same_host=False)
2741         resize_req = {
2742             'resize': {
2743                 'flavorRef': self.flavor2['id']
2744             }
2745         }
2746         self.api.post_server_action(server['id'], resize_req)
2747         self._wait_for_state_change(self.api, server, 'VERIFY_RESIZE')
2748 
2749         # There should be resource usage for flavor1 on the source host.
2750         self.assert_hypervisor_usage(
2751             source_rp_uuid, self.flavor1, volume_backed=False)
2752         # And resource usage for flavor2 on the target host.
2753         self.assert_hypervisor_usage(
2754             dest_rp_uuid, self.flavor2, volume_backed=False)
2755 
2756         # Now confirm the resize and check hypervisor usage again.
2757         self.api.post_server_action(server['id'], {'confirmResize': None})
2758         self._wait_for_state_change(self.api, server, 'ACTIVE')
2759 
2760         # There should no resource usage for flavor1 on the source host.
2761         # FIXME(mriedem): This is bug 1818914 where the source host continues
2762         # to report old_flavor usage until the update_available_resource
2763         # periodic task runs. Uncomment this once fixed.
2764         # self.assert_hypervisor_usage(
2765         #     source_rp_uuid, no_usage, volume_backed=False)
2766         self.assert_hypervisor_usage(
2767             source_rp_uuid, self.flavor1, volume_backed=False)
2768         # And resource usage for flavor2 should still be on the target host.
2769         self.assert_hypervisor_usage(
2770             dest_rp_uuid, self.flavor2, volume_backed=False)
2771 
2772         # Run periodics and make sure usage is still as expected.
2773         self._run_periodics()
2774         self.assert_hypervisor_usage(
2775             source_rp_uuid, no_usage, volume_backed=False)
2776         self.assert_hypervisor_usage(
2777             dest_rp_uuid, self.flavor2, volume_backed=False)
2778 
2779     def _wait_for_notification_event_type(self, event_type, max_retries=50):
2780         retry_counter = 0
2781         while True:
2782             if len(fake_notifier.NOTIFICATIONS) > 0:
2783                 for notification in fake_notifier.NOTIFICATIONS:
2784                     if notification.event_type == event_type:
2785                         return
2786             if retry_counter == max_retries:
2787                 self.fail('Wait for notification event type (%s) failed'
2788                           % event_type)
2789             retry_counter += 1
2790             time.sleep(0.1)
2791 
2792     def test_evacuate_with_no_compute(self):
2793         source_hostname = self.compute1.host
2794         dest_hostname = self.compute2.host
2795         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2796         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
2797 
2798         # Disable compute service on destination host
2799         compute2_service_id = self.admin_api.get_services(
2800             host=dest_hostname, binary='nova-compute')[0]['id']
2801         self.admin_api.put_service(compute2_service_id, {'status': 'disabled'})
2802 
2803         server = self._boot_and_check_allocations(
2804             self.flavor1, source_hostname)
2805 
2806         # Force source compute down
2807         source_compute_id = self.admin_api.get_services(
2808             host=source_hostname, binary='nova-compute')[0]['id']
2809         self.compute1.stop()
2810         self.admin_api.put_service(
2811             source_compute_id, {'forced_down': 'true'})
2812 
2813         # Initialize fake_notifier
2814         fake_notifier.stub_notifier(self)
2815         fake_notifier.reset()
2816 
2817         # Initiate evacuation
2818         post = {'evacuate': {}}
2819         self.api.post_server_action(server['id'], post)
2820 
2821         # NOTE(elod.illes): Should be changed to non-polling solution when
2822         # patch https://review.opendev.org/#/c/482629/ gets merged:
2823         # fake_notifier.wait_for_versioned_notifications(
2824         #     'compute_task.rebuild_server')
2825         self._wait_for_notification_event_type('compute_task.rebuild_server')
2826 
2827         self._run_periodics()
2828 
2829         # There is no other host to evacuate to so the rebuild should put the
2830         # VM to ERROR state, but it should remain on source compute
2831         expected_params = {'OS-EXT-SRV-ATTR:host': source_hostname,
2832                            'status': 'ERROR'}
2833         server = self._wait_for_server_parameter(self.api, server,
2834                                                  expected_params)
2835 
2836         # Check migrations
2837         migrations = self.api.get_migrations()
2838         self.assertEqual(1, len(migrations))
2839         self.assertEqual('evacuation', migrations[0]['migration_type'])
2840         self.assertEqual(server['id'], migrations[0]['instance_uuid'])
2841         self.assertEqual(source_hostname, migrations[0]['source_compute'])
2842         self.assertEqual('error', migrations[0]['status'])
2843 
2844         # Restart source host
2845         self.admin_api.put_service(
2846             source_compute_id, {'forced_down': 'false'})
2847         self.compute1.start()
2848 
2849         self._run_periodics()
2850 
2851         # Check allocation and usages: should only use resources on source host
2852         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
2853 
2854         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
2855                                            source_rp_uuid)
2856         zero_usage = {'VCPU': 0, 'DISK_GB': 0, 'MEMORY_MB': 0}
2857         self.assertRequestMatchesUsage(zero_usage, dest_rp_uuid)
2858 
2859         self._delete_and_check_allocations(server)
2860 
2861     def test_migrate_no_valid_host(self):
2862         source_hostname = self.compute1.host
2863         dest_hostname = self.compute2.host
2864         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2865         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
2866 
2867         server = self._boot_and_check_allocations(
2868             self.flavor1, source_hostname)
2869 
2870         dest_compute_id = self.admin_api.get_services(
2871             host=dest_hostname, binary='nova-compute')[0]['id']
2872         self.compute2.stop()
2873         # force it down to avoid waiting for the service group to time out
2874         self.admin_api.put_service(
2875             dest_compute_id, {'forced_down': 'true'})
2876 
2877         # migrate the server
2878         post = {'migrate': None}
2879         ex = self.assertRaises(client.OpenStackApiException,
2880                                self.api.post_server_action,
2881                                server['id'], post)
2882         self.assertIn('No valid host', six.text_type(ex))
2883         expected_params = {'OS-EXT-SRV-ATTR:host': source_hostname,
2884                            'status': 'ACTIVE'}
2885         self._wait_for_server_parameter(self.api, server, expected_params)
2886 
2887         self._run_periodics()
2888 
2889         # Expect to have allocation only on source_host
2890         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
2891         zero_usage = {'VCPU': 0, 'DISK_GB': 0, 'MEMORY_MB': 0}
2892         self.assertRequestMatchesUsage(zero_usage, dest_rp_uuid)
2893 
2894         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
2895                                            source_rp_uuid)
2896 
2897         self._delete_and_check_allocations(server)
2898 
2899     def test_evacuate(self):
2900         source_hostname = self.compute1.host
2901         dest_hostname = self.compute2.host
2902         server = self._boot_and_check_allocations(
2903             self.flavor1, source_hostname)
2904 
2905         source_compute_id = self.admin_api.get_services(
2906             host=source_hostname, binary='nova-compute')[0]['id']
2907 
2908         self.compute1.stop()
2909         # force it down to avoid waiting for the service group to time out
2910         self.admin_api.put_service(
2911             source_compute_id, {'forced_down': 'true'})
2912 
2913         # evacuate the server
2914         post = {'evacuate': {}}
2915         self.api.post_server_action(
2916             server['id'], post)
2917         expected_params = {'OS-EXT-SRV-ATTR:host': dest_hostname,
2918                            'status': 'ACTIVE'}
2919         server = self._wait_for_server_parameter(self.api, server,
2920                                                  expected_params)
2921 
2922         # Expect to have allocation and usages on both computes as the
2923         # source compute is still down
2924         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2925         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
2926 
2927         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
2928 
2929         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
2930 
2931         self._check_allocation_during_evacuate(
2932             self.flavor1, server['id'], source_rp_uuid, dest_rp_uuid)
2933 
2934         # restart the source compute
2935         self.restart_compute_service(self.compute1)
2936 
2937         self.admin_api.put_service(
2938             source_compute_id, {'forced_down': 'false'})
2939 
2940         source_usages = self._get_provider_usages(source_rp_uuid)
2941         self.assertEqual({'VCPU': 0,
2942                           'MEMORY_MB': 0,
2943                           'DISK_GB': 0},
2944                          source_usages)
2945 
2946         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
2947 
2948         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
2949                                            dest_rp_uuid)
2950 
2951         self._delete_and_check_allocations(server)
2952 
2953     def test_evacuate_forced_host(self):
2954         """Evacuating a server with a forced host bypasses the scheduler
2955         which means conductor has to create the allocations against the
2956         destination node. This test recreates the scenarios and asserts
2957         the allocations on the source and destination nodes are as expected.
2958         """
2959         source_hostname = self.compute1.host
2960         dest_hostname = self.compute2.host
2961 
2962         # the ability to force evacuate a server is removed entirely in 2.68
2963         self.api.microversion = '2.67'
2964 
2965         server = self._boot_and_check_allocations(
2966             self.flavor1, source_hostname)
2967 
2968         source_compute_id = self.admin_api.get_services(
2969             host=source_hostname, binary='nova-compute')[0]['id']
2970 
2971         self.compute1.stop()
2972         # force it down to avoid waiting for the service group to time out
2973         self.admin_api.put_service(
2974             source_compute_id, {'forced_down': 'true'})
2975 
2976         # evacuate the server and force the destination host which bypasses
2977         # the scheduler
2978         post = {
2979             'evacuate': {
2980                 'host': dest_hostname,
2981                 'force': True
2982             }
2983         }
2984         self.api.post_server_action(server['id'], post)
2985         expected_params = {'OS-EXT-SRV-ATTR:host': dest_hostname,
2986                            'status': 'ACTIVE'}
2987         server = self._wait_for_server_parameter(self.api, server,
2988                                                  expected_params)
2989 
2990         # Run the periodics to show those don't modify allocations.
2991         self._run_periodics()
2992 
2993         # Expect to have allocation and usages on both computes as the
2994         # source compute is still down
2995         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2996         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
2997 
2998         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
2999 
3000         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
3001 
3002         self._check_allocation_during_evacuate(
3003             self.flavor1, server['id'], source_rp_uuid, dest_rp_uuid)
3004 
3005         # restart the source compute
3006         self.restart_compute_service(self.compute1)
3007         self.admin_api.put_service(
3008             source_compute_id, {'forced_down': 'false'})
3009 
3010         # Run the periodics again to show they don't change anything.
3011         self._run_periodics()
3012 
3013         # When the source node starts up, the instance has moved so the
3014         # ResourceTracker should cleanup allocations for the source node.
3015         source_usages = self._get_provider_usages(source_rp_uuid)
3016         self.assertEqual(
3017             {'VCPU': 0, 'MEMORY_MB': 0, 'DISK_GB': 0}, source_usages)
3018 
3019         # The usages/allocations should still exist on the destination node
3020         # after the source node starts back up.
3021         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
3022 
3023         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
3024                                            dest_rp_uuid)
3025 
3026         self._delete_and_check_allocations(server)
3027 
3028     def test_evacuate_forced_host_v268(self):
3029         """Evacuating a server with a forced host was removed in API
3030         microversion 2.68. This test ensures that the request is rejected.
3031         """
3032         source_hostname = self.compute1.host
3033         dest_hostname = self.compute2.host
3034 
3035         server = self._boot_and_check_allocations(
3036             self.flavor1, source_hostname)
3037 
3038         # evacuate the server and force the destination host which bypasses
3039         # the scheduler
3040         post = {
3041             'evacuate': {
3042                 'host': dest_hostname,
3043                 'force': True
3044             }
3045         }
3046         ex = self.assertRaises(client.OpenStackApiException,
3047                                self.api.post_server_action,
3048                                server['id'], post)
3049         self.assertIn("'force' was unexpected", six.text_type(ex))
3050 
3051     # NOTE(gibi): there is a similar test in SchedulerOnlyChecksTargetTest but
3052     # we want this test here as well because ServerMovingTest is a parent class
3053     # of multiple test classes that run this test case with different compute
3054     # node setups.
3055     def test_evacuate_host_specified_but_not_forced(self):
3056         """Evacuating a server with a host but using the scheduler to create
3057         the allocations against the destination node. This test recreates the
3058         scenarios and asserts the allocations on the source and destination
3059         nodes are as expected.
3060         """
3061         source_hostname = self.compute1.host
3062         dest_hostname = self.compute2.host
3063 
3064         server = self._boot_and_check_allocations(
3065             self.flavor1, source_hostname)
3066 
3067         source_compute_id = self.admin_api.get_services(
3068             host=source_hostname, binary='nova-compute')[0]['id']
3069 
3070         self.compute1.stop()
3071         # force it down to avoid waiting for the service group to time out
3072         self.admin_api.put_service(
3073             source_compute_id, {'forced_down': 'true'})
3074 
3075         # evacuate the server specify the target but do not force the
3076         # destination host to use the scheduler to validate the target host
3077         post = {
3078             'evacuate': {
3079                 'host': dest_hostname,
3080             }
3081         }
3082         self.api.post_server_action(server['id'], post)
3083         expected_params = {'OS-EXT-SRV-ATTR:host': dest_hostname,
3084                            'status': 'ACTIVE'}
3085         server = self._wait_for_server_parameter(self.api, server,
3086                                                  expected_params)
3087 
3088         # Run the periodics to show those don't modify allocations.
3089         self._run_periodics()
3090 
3091         # Expect to have allocation and usages on both computes as the
3092         # source compute is still down
3093         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
3094         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
3095 
3096         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
3097 
3098         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
3099 
3100         self._check_allocation_during_evacuate(
3101             self.flavor1, server['id'], source_rp_uuid, dest_rp_uuid)
3102 
3103         # restart the source compute
3104         self.restart_compute_service(self.compute1)
3105         self.admin_api.put_service(
3106             source_compute_id, {'forced_down': 'false'})
3107 
3108         # Run the periodics again to show they don't change anything.
3109         self._run_periodics()
3110 
3111         # When the source node starts up, the instance has moved so the
3112         # ResourceTracker should cleanup allocations for the source node.
3113         source_usages = self._get_provider_usages(source_rp_uuid)
3114         self.assertEqual(
3115             {'VCPU': 0, 'MEMORY_MB': 0, 'DISK_GB': 0}, source_usages)
3116 
3117         # The usages/allocations should still exist on the destination node
3118         # after the source node starts back up.
3119         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
3120 
3121         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
3122                                            dest_rp_uuid)
3123 
3124         self._delete_and_check_allocations(server)
3125 
3126     def test_evacuate_claim_on_dest_fails(self):
3127         """Tests that the allocations on the destination node are cleaned up
3128         when the rebuild move claim fails due to insufficient resources.
3129         """
3130         source_hostname = self.compute1.host
3131         dest_hostname = self.compute2.host
3132         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
3133 
3134         server = self._boot_and_check_allocations(
3135             self.flavor1, source_hostname)
3136 
3137         source_compute_id = self.admin_api.get_services(
3138             host=source_hostname, binary='nova-compute')[0]['id']
3139 
3140         self.compute1.stop()
3141         # force it down to avoid waiting for the service group to time out
3142         self.admin_api.put_service(
3143             source_compute_id, {'forced_down': 'true'})
3144 
3145         # NOTE(mriedem): This isn't great, and I'd like to fake out the driver
3146         # to make the claim fail, by doing something like returning a too high
3147         # memory_mb overhead, but the limits dict passed to the claim is empty
3148         # so the claim test is considering it as unlimited and never actually
3149         # performs a claim test. Configuring the scheduler to use the RamFilter
3150         # to get the memory_mb limit at least seems like it should work but
3151         # it doesn't appear to for some reason...
3152         def fake_move_claim(*args, **kwargs):
3153             # Assert the destination node allocation exists.
3154             self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
3155             raise exception.ComputeResourcesUnavailable(
3156                     reason='test_evacuate_claim_on_dest_fails')
3157 
3158         with mock.patch('nova.compute.claims.MoveClaim', fake_move_claim):
3159             # evacuate the server
3160             self.api.post_server_action(server['id'], {'evacuate': {}})
3161             # the migration will fail on the dest node and the instance will
3162             # go into error state
3163             server = self._wait_for_state_change(self.api, server, 'ERROR')
3164 
3165         # Run the periodics to show those don't modify allocations.
3166         self._run_periodics()
3167 
3168         # The allocation should still exist on the source node since it's
3169         # still down, and the allocation on the destination node should be
3170         # cleaned up.
3171         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
3172 
3173         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
3174 
3175         self.assertRequestMatchesUsage(
3176             {'VCPU': 0,
3177              'MEMORY_MB': 0,
3178              'DISK_GB': 0}, dest_rp_uuid)
3179 
3180         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
3181                                            source_rp_uuid)
3182 
3183         # restart the source compute
3184         self.restart_compute_service(self.compute1)
3185         self.admin_api.put_service(
3186             source_compute_id, {'forced_down': 'false'})
3187 
3188         # Run the periodics again to show they don't change anything.
3189         self._run_periodics()
3190 
3191         # The source compute shouldn't have cleaned up the allocation for
3192         # itself since the instance didn't move.
3193         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
3194 
3195         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
3196                                            source_rp_uuid)
3197 
3198     def test_evacuate_rebuild_on_dest_fails(self):
3199         """Tests that the allocations on the destination node are cleaned up
3200         automatically when the claim is made but the actual rebuild
3201         via the driver fails.
3202 
3203         """
3204         source_hostname = self.compute1.host
3205         dest_hostname = self.compute2.host
3206         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
3207 
3208         server = self._boot_and_check_allocations(
3209             self.flavor1, source_hostname)
3210 
3211         source_compute_id = self.admin_api.get_services(
3212             host=source_hostname, binary='nova-compute')[0]['id']
3213 
3214         self.compute1.stop()
3215         # force it down to avoid waiting for the service group to time out
3216         self.admin_api.put_service(
3217             source_compute_id, {'forced_down': 'true'})
3218 
3219         def fake_rebuild(*args, **kwargs):
3220             # Assert the destination node allocation exists.
3221             self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
3222             raise test.TestingException('test_evacuate_rebuild_on_dest_fails')
3223 
3224         with mock.patch.object(
3225                 self.compute2.driver, 'rebuild', fake_rebuild):
3226             # evacuate the server
3227             self.api.post_server_action(server['id'], {'evacuate': {}})
3228             # the migration will fail on the dest node and the instance will
3229             # go into error state
3230             server = self._wait_for_state_change(self.api, server, 'ERROR')
3231 
3232         # Run the periodics to show those don't modify allocations.
3233         self._run_periodics()
3234 
3235         # The allocation should still exist on the source node since it's
3236         # still down, and the allocation on the destination node should be
3237         # cleaned up.
3238         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
3239 
3240         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
3241 
3242         self.assertRequestMatchesUsage(
3243             {'VCPU': 0,
3244              'MEMORY_MB': 0,
3245              'DISK_GB': 0}, dest_rp_uuid)
3246 
3247         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
3248                                            source_rp_uuid)
3249 
3250         # restart the source compute
3251         self.restart_compute_service(self.compute1)
3252         self.admin_api.put_service(
3253             source_compute_id, {'forced_down': 'false'})
3254 
3255         # Run the periodics again to show they don't change anything.
3256         self._run_periodics()
3257 
3258         # The source compute shouldn't have cleaned up the allocation for
3259         # itself since the instance didn't move.
3260         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
3261 
3262         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
3263                                            source_rp_uuid)
3264 
3265     def _boot_then_shelve_and_check_allocations(self, hostname, rp_uuid):
3266         # avoid automatic shelve offloading
3267         self.flags(shelved_offload_time=-1)
3268         server = self._boot_and_check_allocations(
3269             self.flavor1, hostname)
3270         req = {
3271             'shelve': {}
3272         }
3273         self.api.post_server_action(server['id'], req)
3274         self._wait_for_state_change(self.api, server, 'SHELVED')
3275         # the host should maintain the existing allocation for this instance
3276         # while the instance is shelved
3277         self.assertFlavorMatchesUsage(rp_uuid, self.flavor1)
3278         # Check that the server only allocates resource from the host it is
3279         # booted on
3280         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
3281                                            rp_uuid)
3282         return server
3283 
3284     def test_shelve_unshelve(self):
3285         source_hostname = self.compute1.host
3286         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
3287         server = self._boot_then_shelve_and_check_allocations(
3288             source_hostname, source_rp_uuid)
3289 
3290         req = {
3291             'unshelve': {}
3292         }
3293         self.api.post_server_action(server['id'], req)
3294         self._wait_for_state_change(self.api, server, 'ACTIVE')
3295 
3296         # the host should have resource usage as the instance is ACTIVE
3297         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
3298 
3299         # Check that the server only allocates resource from the host it is
3300         # booted on
3301         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
3302                                            source_rp_uuid)
3303 
3304         self._delete_and_check_allocations(server)
3305 
3306     def _shelve_offload_and_check_allocations(self, server, source_rp_uuid):
3307         req = {
3308             'shelveOffload': {}
3309         }
3310         self.api.post_server_action(server['id'], req)
3311         self._wait_for_server_parameter(
3312             self.api, server, {'status': 'SHELVED_OFFLOADED',
3313                                'OS-EXT-SRV-ATTR:host': None,
3314                                'OS-EXT-AZ:availability_zone': ''})
3315         source_usages = self._get_provider_usages(source_rp_uuid)
3316         self.assertEqual({'VCPU': 0,
3317                           'MEMORY_MB': 0,
3318                           'DISK_GB': 0},
3319                          source_usages)
3320 
3321         allocations = self._get_allocations_by_server_uuid(server['id'])
3322         self.assertEqual(0, len(allocations))
3323 
3324     def test_shelve_offload_unshelve_diff_host(self):
3325         source_hostname = self.compute1.host
3326         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
3327         server = self._boot_then_shelve_and_check_allocations(
3328             source_hostname, source_rp_uuid)
3329 
3330         self._shelve_offload_and_check_allocations(server, source_rp_uuid)
3331 
3332         # unshelve after shelve offload will do scheduling. this test case
3333         # wants to test the scenario when the scheduler select a different host
3334         # to ushelve the instance. So we disable the original host.
3335         source_service_id = self.admin_api.get_services(
3336             host=source_hostname, binary='nova-compute')[0]['id']
3337         self.admin_api.put_service(source_service_id, {'status': 'disabled'})
3338 
3339         req = {
3340             'unshelve': {}
3341         }
3342         self.api.post_server_action(server['id'], req)
3343         server = self._wait_for_state_change(self.api, server, 'ACTIVE')
3344         # unshelving an offloaded instance will call the scheduler so the
3345         # instance might end up on a different host
3346         current_hostname = server['OS-EXT-SRV-ATTR:host']
3347         self.assertEqual(current_hostname, self._other_hostname(
3348             source_hostname))
3349 
3350         # the host running the instance should have resource usage
3351         current_rp_uuid = self._get_provider_uuid_by_host(current_hostname)
3352         self.assertFlavorMatchesUsage(current_rp_uuid, self.flavor1)
3353 
3354         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
3355                                            current_rp_uuid)
3356 
3357         self._delete_and_check_allocations(server)
3358 
3359     def test_shelve_offload_unshelve_same_host(self):
3360         source_hostname = self.compute1.host
3361         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
3362         server = self._boot_then_shelve_and_check_allocations(
3363             source_hostname, source_rp_uuid)
3364 
3365         self._shelve_offload_and_check_allocations(server, source_rp_uuid)
3366 
3367         # unshelve after shelve offload will do scheduling. this test case
3368         # wants to test the scenario when the scheduler select the same host
3369         # to ushelve the instance. So we disable the other host.
3370         source_service_id = self.admin_api.get_services(
3371             host=self._other_hostname(source_hostname),
3372             binary='nova-compute')[0]['id']
3373         self.admin_api.put_service(source_service_id, {'status': 'disabled'})
3374 
3375         req = {
3376             'unshelve': {}
3377         }
3378         self.api.post_server_action(server['id'], req)
3379         server = self._wait_for_state_change(self.api, server, 'ACTIVE')
3380         # unshelving an offloaded instance will call the scheduler so the
3381         # instance might end up on a different host
3382         current_hostname = server['OS-EXT-SRV-ATTR:host']
3383         self.assertEqual(current_hostname, source_hostname)
3384 
3385         # the host running the instance should have resource usage
3386         current_rp_uuid = self._get_provider_uuid_by_host(current_hostname)
3387         self.assertFlavorMatchesUsage(current_rp_uuid, self.flavor1)
3388 
3389         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
3390                                            current_rp_uuid)
3391 
3392         self._delete_and_check_allocations(server)
3393 
3394     def test_live_migrate_force(self):
3395         source_hostname = self.compute1.host
3396         dest_hostname = self.compute2.host
3397         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
3398         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
3399 
3400         # the ability to force live migrate a server is removed entirely in
3401         # 2.68
3402         self.api.microversion = '2.67'
3403 
3404         server = self._boot_and_check_allocations(
3405             self.flavor1, source_hostname)
3406 
3407         # live migrate the server and force the destination host which bypasses
3408         # the scheduler
3409         post = {
3410             'os-migrateLive': {
3411                 'host': dest_hostname,
3412                 'block_migration': True,
3413                 'force': True,
3414             }
3415         }
3416 
3417         self.api.post_server_action(server['id'], post)
3418         self._wait_for_server_parameter(self.api, server,
3419             {'OS-EXT-SRV-ATTR:host': dest_hostname,
3420              'status': 'ACTIVE'})
3421 
3422         self._run_periodics()
3423 
3424         # NOTE(danms): There should be no usage for the source
3425         self.assertRequestMatchesUsage(
3426             {'VCPU': 0,
3427              'MEMORY_MB': 0,
3428              'DISK_GB': 0}, source_rp_uuid)
3429 
3430         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
3431 
3432         # the server has an allocation on only the dest node
3433         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
3434                                            dest_rp_uuid)
3435 
3436         self._delete_and_check_allocations(server)
3437 
3438     def test_live_migrate_forced_v268(self):
3439         """Live migrating a server with a forced host was removed in API
3440         microversion 2.68. This test ensures that the request is rejected.
3441         """
3442         source_hostname = self.compute1.host
3443         dest_hostname = self.compute2.host
3444 
3445         server = self._boot_and_check_allocations(
3446             self.flavor1, source_hostname)
3447 
3448         # live migrate the server and force the destination host which bypasses
3449         # the scheduler
3450         post = {
3451             'os-migrateLive': {
3452                 'host': dest_hostname,
3453                 'block_migration': True,
3454                 'force': True,
3455             }
3456         }
3457 
3458         ex = self.assertRaises(client.OpenStackApiException,
3459                                self.api.post_server_action,
3460                                server['id'], post)
3461         self.assertIn("'force' was unexpected", six.text_type(ex))
3462 
3463     def test_live_migrate(self):
3464         source_hostname = self.compute1.host
3465         dest_hostname = self.compute2.host
3466         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
3467         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
3468 
3469         server = self._boot_and_check_allocations(
3470             self.flavor1, source_hostname)
3471         post = {
3472             'os-migrateLive': {
3473                 'host': dest_hostname,
3474                 'block_migration': True,
3475             }
3476         }
3477 
3478         self.api.post_server_action(server['id'], post)
3479         self._wait_for_server_parameter(self.api, server,
3480                                         {'OS-EXT-SRV-ATTR:host': dest_hostname,
3481                                          'status': 'ACTIVE'})
3482 
3483         self._run_periodics()
3484 
3485         # NOTE(danms): There should be no usage for the source
3486         self.assertRequestMatchesUsage(
3487             {'VCPU': 0,
3488              'MEMORY_MB': 0,
3489              'DISK_GB': 0}, source_rp_uuid)
3490 
3491         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
3492 
3493         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
3494                                            dest_rp_uuid)
3495 
3496         self._delete_and_check_allocations(server)
3497 
3498     def test_live_migrate_pre_check_fails(self):
3499         """Tests the case that the LiveMigrationTask in conductor has
3500         called the scheduler which picked a host and created allocations
3501         against it in Placement, but then when the conductor task calls
3502         check_can_live_migrate_destination on the destination compute it
3503         fails. The allocations on the destination compute node should be
3504         cleaned up before the conductor task asks the scheduler for another
3505         host to try the live migration.
3506         """
3507         self.failed_hostname = None
3508         source_hostname = self.compute1.host
3509         dest_hostname = self.compute2.host
3510         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
3511         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
3512 
3513         server = self._boot_and_check_allocations(
3514             self.flavor1, source_hostname)
3515 
3516         def fake_check_can_live_migrate_destination(
3517                 context, instance, src_compute_info, dst_compute_info,
3518                 block_migration=False, disk_over_commit=False):
3519             self.failed_hostname = dst_compute_info['host']
3520             raise exception.MigrationPreCheckError(
3521                 reason='test_live_migrate_pre_check_fails')
3522 
3523         with mock.patch('nova.virt.fake.FakeDriver.'
3524                         'check_can_live_migrate_destination',
3525                         side_effect=fake_check_can_live_migrate_destination):
3526             post = {
3527                 'os-migrateLive': {
3528                     'host': dest_hostname,
3529                     'block_migration': True,
3530                 }
3531             }
3532             self.api.post_server_action(server['id'], post)
3533             # As there are only two computes and we failed to live migrate to
3534             # the only other destination host, the LiveMigrationTask raises
3535             # MaxRetriesExceeded back to the conductor manager which handles it
3536             # generically and sets the instance back to ACTIVE status and
3537             # clears the task_state. The migration record status is set to
3538             # 'error', so that's what we need to look for to know when this
3539             # is done.
3540             migration = self._wait_for_migration_status(server, ['error'])
3541 
3542         # The source_compute should be set on the migration record, but the
3543         # destination shouldn't be as we never made it to one.
3544         self.assertEqual(source_hostname, migration['source_compute'])
3545         self.assertIsNone(migration['dest_compute'])
3546         # Make sure the destination host (the only other host) is the failed
3547         # host.
3548         self.assertEqual(dest_hostname, self.failed_hostname)
3549 
3550         # Since the instance didn't move, assert the allocations are still
3551         # on the source node.
3552         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
3553 
3554         # Assert the allocations, created by the scheduler, are cleaned up
3555         # after the migration pre-check error happens.
3556         self.assertRequestMatchesUsage(
3557             {'VCPU': 0,
3558              'MEMORY_MB': 0,
3559              'DISK_GB': 0}, dest_rp_uuid)
3560 
3561         # There should only be 1 allocation for the instance on the source node
3562         self.assertFlavorMatchesAllocation(
3563             self.flavor1, server['id'], source_rp_uuid)
3564 
3565         self._delete_and_check_allocations(server)
3566 
3567     @mock.patch('nova.virt.fake.FakeDriver.pre_live_migration')
3568     def test_live_migrate_rollback_cleans_dest_node_allocations(
3569             self, mock_pre_live_migration, force=False):
3570         """Tests the case that when live migration fails, either during the
3571         call to pre_live_migration on the destination, or during the actual
3572         live migration in the virt driver, the allocations on the destination
3573         node are rolled back since the instance is still on the source node.
3574         """
3575         source_hostname = self.compute1.host
3576         dest_hostname = self.compute2.host
3577         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
3578         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
3579 
3580         # the ability to force live migrate a server is removed entirely in
3581         # 2.68
3582         self.api.microversion = '2.67'
3583 
3584         server = self._boot_and_check_allocations(
3585             self.flavor1, source_hostname)
3586 
3587         def stub_pre_live_migration(context, instance, block_device_info,
3588                                     network_info, disk_info, migrate_data):
3589             # Make sure the source node allocations are against the migration
3590             # record and the dest node allocations are against the instance.
3591             self.assertFlavorMatchesAllocation(
3592                 self.flavor1, migrate_data.migration.uuid, source_rp_uuid)
3593 
3594             self.assertFlavorMatchesAllocation(
3595                 self.flavor1, server['id'], dest_rp_uuid)
3596             # The actual type of exception here doesn't matter. The point
3597             # is that the virt driver raised an exception from the
3598             # pre_live_migration method on the destination host.
3599             raise test.TestingException(
3600                 'test_live_migrate_rollback_cleans_dest_node_allocations')
3601 
3602         mock_pre_live_migration.side_effect = stub_pre_live_migration
3603 
3604         post = {
3605             'os-migrateLive': {
3606                 'host': dest_hostname,
3607                 'block_migration': True,
3608                 'force': force
3609             }
3610         }
3611         self.api.post_server_action(server['id'], post)
3612         # The compute manager will put the migration record into error status
3613         # when pre_live_migration fails, so wait for that to happen.
3614         migration = self._wait_for_migration_status(server, ['error'])
3615         # The _rollback_live_migration method in the compute manager will reset
3616         # the task_state on the instance, so wait for that to happen.
3617         server = self._wait_for_server_parameter(
3618             self.api, server, {'OS-EXT-STS:task_state': None})
3619 
3620         self.assertEqual(source_hostname, migration['source_compute'])
3621         self.assertEqual(dest_hostname, migration['dest_compute'])
3622 
3623         # Since the instance didn't move, assert the allocations are still
3624         # on the source node.
3625         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
3626 
3627         # Assert the allocations, created by the scheduler, are cleaned up
3628         # after the rollback happens.
3629         self.assertRequestMatchesUsage(
3630             {'VCPU': 0,
3631              'MEMORY_MB': 0,
3632              'DISK_GB': 0}, dest_rp_uuid)
3633 
3634         # There should only be 1 allocation for the instance on the source node
3635         self.assertFlavorMatchesAllocation(
3636             self.flavor1, server['id'], source_rp_uuid)
3637 
3638         self._delete_and_check_allocations(server)
3639 
3640     def test_live_migrate_rollback_cleans_dest_node_allocations_forced(self):
3641         """Tests the case that when a forced host live migration fails, either
3642         during the call to pre_live_migration on the destination, or during
3643         the actual live migration in the virt driver, the allocations on the
3644         destination node are rolled back since the instance is still on the
3645         source node.
3646         """
3647         self.test_live_migrate_rollback_cleans_dest_node_allocations(
3648             force=True)
3649 
3650     def test_rescheduling_when_migrating_instance(self):
3651         """Tests that allocations are removed from the destination node by
3652         the compute service when a cold migrate / resize fails and a reschedule
3653         request is sent back to conductor.
3654         """
3655         source_hostname = self.compute1.manager.host
3656         server = self._boot_and_check_allocations(
3657             self.flavor1, source_hostname)
3658 
3659         def fake_prep_resize(*args, **kwargs):
3660             dest_hostname = self._other_hostname(source_hostname)
3661             dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
3662             self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
3663             allocations = self._get_allocations_by_server_uuid(server['id'])
3664             self.assertIn(dest_rp_uuid, allocations)
3665 
3666             source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
3667             self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
3668             migration_uuid = self.get_migration_uuid_for_instance(server['id'])
3669             allocations = self._get_allocations_by_server_uuid(migration_uuid)
3670             self.assertIn(source_rp_uuid, allocations)
3671 
3672             raise test.TestingException('Simulated _prep_resize failure.')
3673 
3674         # Yes this isn't great in a functional test, but it's simple.
3675         self.stub_out('nova.compute.manager.ComputeManager._prep_resize',
3676                       fake_prep_resize)
3677 
3678         # Now migrate the server which is going to fail on the destination.
3679         self.api.post_server_action(server['id'], {'migrate': None})
3680 
3681         self._wait_for_action_fail_completion(
3682             server, instance_actions.MIGRATE, 'compute_prep_resize')
3683 
3684         dest_hostname = self._other_hostname(source_hostname)
3685         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
3686 
3687         # Expects no allocation records on the failed host.
3688         self.assertRequestMatchesUsage(
3689             {'VCPU': 0,
3690              'MEMORY_MB': 0,
3691              'DISK_GB': 0}, dest_rp_uuid)
3692 
3693         # Ensure the allocation records still exist on the source host.
3694         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
3695         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
3696         allocations = self._get_allocations_by_server_uuid(server['id'])
3697         self.assertIn(source_rp_uuid, allocations)
3698 
3699     def _test_resize_to_same_host_instance_fails(self, failing_method,
3700                                                  event_name):
3701         """Tests that when we resize to the same host and resize fails in
3702         the given method, we cleanup the allocations before rescheduling.
3703         """
3704         # make sure that the test only uses a single host
3705         compute2_service_id = self.admin_api.get_services(
3706             host=self.compute2.host, binary='nova-compute')[0]['id']
3707         self.admin_api.put_service(compute2_service_id, {'status': 'disabled'})
3708 
3709         hostname = self.compute1.manager.host
3710         rp_uuid = self._get_provider_uuid_by_host(hostname)
3711 
3712         server = self._boot_and_check_allocations(self.flavor1, hostname)
3713 
3714         def fake_resize_method(*args, **kwargs):
3715             # Ensure the allocations are doubled now before we fail.
3716             self.assertFlavorMatchesUsage(rp_uuid, self.flavor1, self.flavor2)
3717             raise test.TestingException('Simulated resize failure.')
3718 
3719         # Yes this isn't great in a functional test, but it's simple.
3720         self.stub_out(
3721             'nova.compute.manager.ComputeManager.%s' % failing_method,
3722             fake_resize_method)
3723 
3724         self.flags(allow_resize_to_same_host=True)
3725         resize_req = {
3726             'resize': {
3727                 'flavorRef': self.flavor2['id']
3728             }
3729         }
3730         self.api.post_server_action(server['id'], resize_req)
3731 
3732         self._wait_for_action_fail_completion(
3733             server, instance_actions.RESIZE, event_name)
3734 
3735         # Ensure the allocation records still exist on the host.
3736         source_rp_uuid = self._get_provider_uuid_by_host(hostname)
3737         # The new_flavor should have been subtracted from the doubled
3738         # allocation which just leaves us with the original flavor.
3739         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
3740 
3741     def test_resize_to_same_host_prep_resize_fails(self):
3742         self._test_resize_to_same_host_instance_fails(
3743             '_prep_resize', 'compute_prep_resize')
3744 
3745     def test_resize_instance_fails_allocation_cleanup(self):
3746         self._test_resize_to_same_host_instance_fails(
3747             '_resize_instance', 'compute_resize_instance')
3748 
3749     def test_finish_resize_fails_allocation_cleanup(self):
3750         self._test_resize_to_same_host_instance_fails(
3751             '_finish_resize', 'compute_finish_resize')
3752 
3753     def _test_resize_reschedule_uses_host_lists(self, fails, num_alts=None):
3754         """Test that when a resize attempt fails, the retry comes from the
3755         supplied host_list, and does not call the scheduler.
3756         """
3757         server_req = self._build_minimal_create_server_request(
3758                 self.api, "some-server", flavor_id=self.flavor1["id"],
3759                 image_uuid="155d900f-4e14-4e4c-a73d-069cbf4541e6",
3760                 networks='none')
3761 
3762         created_server = self.api.post_server({"server": server_req})
3763         server = self._wait_for_state_change(self.api, created_server,
3764                 "ACTIVE")
3765         inst_host = server["OS-EXT-SRV-ATTR:host"]
3766         uuid_orig = self._get_provider_uuid_by_host(inst_host)
3767 
3768         # We will need four new compute nodes to test the resize, representing
3769         # the host selected by select_destinations(), along with 3 alternates.
3770         self._start_compute(host="selection")
3771         self._start_compute(host="alt_host1")
3772         self._start_compute(host="alt_host2")
3773         self._start_compute(host="alt_host3")
3774         uuid_sel = self._get_provider_uuid_by_host("selection")
3775         uuid_alt1 = self._get_provider_uuid_by_host("alt_host1")
3776         uuid_alt2 = self._get_provider_uuid_by_host("alt_host2")
3777         uuid_alt3 = self._get_provider_uuid_by_host("alt_host3")
3778         hosts = [{"name": "selection", "uuid": uuid_sel},
3779                  {"name": "alt_host1", "uuid": uuid_alt1},
3780                  {"name": "alt_host2", "uuid": uuid_alt2},
3781                  {"name": "alt_host3", "uuid": uuid_alt3},
3782                 ]
3783 
3784         self.flags(weight_classes=[__name__ + '.AltHostWeigher'],
3785                    group='filter_scheduler')
3786         self.scheduler_service.stop()
3787         self.scheduler_service = self.start_service('scheduler')
3788 
3789         def fake_prep_resize(*args, **kwargs):
3790             if self.num_fails < fails:
3791                 self.num_fails += 1
3792                 raise Exception("fake_prep_resize")
3793             actual_prep_resize(*args, **kwargs)
3794 
3795         # Yes this isn't great in a functional test, but it's simple.
3796         actual_prep_resize = compute_manager.ComputeManager._prep_resize
3797         self.stub_out("nova.compute.manager.ComputeManager._prep_resize",
3798                       fake_prep_resize)
3799         self.num_fails = 0
3800         num_alts = 4 if num_alts is None else num_alts
3801         # Make sure we have enough retries available for the number of
3802         # requested fails.
3803         attempts = min(fails + 2, num_alts)
3804         self.flags(max_attempts=attempts, group='scheduler')
3805         server_uuid = server["id"]
3806         data = {"resize": {"flavorRef": self.flavor2["id"]}}
3807         self.api.post_server_action(server_uuid, data)
3808 
3809         if num_alts < fails:
3810             # We will run out of alternates before populate_retry will
3811             # raise a MaxRetriesExceeded exception, so the migration will
3812             # fail and the server should be in status "ERROR"
3813             server = self._wait_for_state_change(self.api, created_server,
3814                     "ERROR")
3815             # The usage should be unchanged from the original flavor
3816             self.assertFlavorMatchesUsage(uuid_orig, self.flavor1)
3817             # There should be no usages on any of the hosts
3818             target_uuids = (uuid_sel, uuid_alt1, uuid_alt2, uuid_alt3)
3819             empty_usage = {"VCPU": 0, "MEMORY_MB": 0, "DISK_GB": 0}
3820             for target_uuid in target_uuids:
3821                 usage = self._get_provider_usages(target_uuid)
3822                 self.assertEqual(empty_usage, usage)
3823         else:
3824             server = self._wait_for_state_change(self.api, created_server,
3825                     "VERIFY_RESIZE")
3826             # Verify that the selected host failed, and was rescheduled to
3827             # an alternate host.
3828             new_server_host = server.get("OS-EXT-SRV-ATTR:host")
3829             expected_host = hosts[fails]["name"]
3830             self.assertEqual(expected_host, new_server_host)
3831             uuid_dest = hosts[fails]["uuid"]
3832             # The usage should match the resized flavor
3833             self.assertFlavorMatchesUsage(uuid_dest, self.flavor2)
3834             # Verify that the other host have no allocations
3835             target_uuids = (uuid_sel, uuid_alt1, uuid_alt2, uuid_alt3)
3836             empty_usage = {"VCPU": 0, "MEMORY_MB": 0, "DISK_GB": 0}
3837             for target_uuid in target_uuids:
3838                 if target_uuid == uuid_dest:
3839                     continue
3840                 usage = self._get_provider_usages(target_uuid)
3841                 self.assertEqual(empty_usage, usage)
3842 
3843             # Verify that there is only one migration record for the instance.
3844             ctxt = context.get_admin_context()
3845             filters = {"instance_uuid": server["id"]}
3846             migrations = objects.MigrationList.get_by_filters(ctxt, filters)
3847             self.assertEqual(1, len(migrations.objects))
3848 
3849     def test_resize_reschedule_uses_host_lists_1_fail(self):
3850         self._test_resize_reschedule_uses_host_lists(fails=1)
3851 
3852     def test_resize_reschedule_uses_host_lists_3_fails(self):
3853         self._test_resize_reschedule_uses_host_lists(fails=3)
3854 
3855     def test_resize_reschedule_uses_host_lists_not_enough_alts(self):
3856         self._test_resize_reschedule_uses_host_lists(fails=3, num_alts=1)
3857 
3858     def test_migrate_confirm(self):
3859         source_hostname = self.compute1.host
3860         dest_hostname = self.compute2.host
3861         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
3862         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
3863 
3864         server = self._boot_and_check_allocations(
3865             self.flavor1, source_hostname)
3866 
3867         self._migrate_and_check_allocations(
3868             server, self.flavor1, source_rp_uuid, dest_rp_uuid)
3869 
3870         # Confirm the move and check the usages
3871         post = {'confirmResize': None}
3872         self.api.post_server_action(
3873             server['id'], post, check_response_status=[204])
3874         self._wait_for_state_change(self.api, server, 'ACTIVE')
3875 
3876         def _check_allocation():
3877             # the target host usage should be according to the flavor
3878             self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
3879             # the source host has no usage
3880             self.assertRequestMatchesUsage({'VCPU': 0,
3881                                             'MEMORY_MB': 0,
3882                                             'DISK_GB': 0}, source_rp_uuid)
3883 
3884             # and the target host allocation should be according to the flavor
3885             self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
3886                                                dest_rp_uuid)
3887 
3888         # After confirming, we should have an allocation only on the
3889         # destination host
3890         _check_allocation()
3891         self._run_periodics()
3892 
3893         # Check we're still accurate after running the periodics
3894         _check_allocation()
3895 
3896         self._delete_and_check_allocations(server)
3897 
3898     def test_migrate_revert(self):
3899         source_hostname = self.compute1.host
3900         dest_hostname = self.compute2.host
3901         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
3902         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
3903 
3904         server = self._boot_and_check_allocations(
3905             self.flavor1, source_hostname)
3906 
3907         self._migrate_and_check_allocations(
3908             server, self.flavor1, source_rp_uuid, dest_rp_uuid)
3909 
3910         # Revert the move and check the usages
3911         post = {'revertResize': None}
3912         self.api.post_server_action(server['id'], post)
3913         self._wait_for_state_change(self.api, server, 'ACTIVE')
3914 
3915         def _check_allocation():
3916             self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
3917             self.assertRequestMatchesUsage({'VCPU': 0,
3918                                             'MEMORY_MB': 0,
3919                                             'DISK_GB': 0}, dest_rp_uuid)
3920 
3921             # Check that the server only allocates resource from the original
3922             # host
3923             self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
3924                                                source_rp_uuid)
3925 
3926         # the original host expected to have the old resource allocation
3927         _check_allocation()
3928         self._run_periodics()
3929         _check_allocation()
3930 
3931         self._delete_and_check_allocations(server)
3932 
3933 
3934 class ServerLiveMigrateForceAndAbort(
3935         integrated_helpers.ProviderUsageBaseTestCase):
3936     """Test Server live migrations, which delete the migration or
3937     force_complete it, and check the allocations after the operations.
3938 
3939     The test are using fakedriver to handle the force_completion and deletion
3940     of live migration.
3941     """
3942 
3943     compute_driver = 'fake.FakeLiveMigrateDriver'
3944 
3945     def setUp(self):
3946         super(ServerLiveMigrateForceAndAbort, self).setUp()
3947 
3948         self.compute1 = self._start_compute(host='host1')
3949         self.compute2 = self._start_compute(host='host2')
3950 
3951         flavors = self.api.get_flavors()
3952         self.flavor1 = flavors[0]
3953 
3954     def test_live_migrate_force_complete(self):
3955         source_hostname = self.compute1.host
3956         dest_hostname = self.compute2.host
3957         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
3958         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
3959 
3960         server = self._boot_and_check_allocations(
3961             self.flavor1, source_hostname)
3962 
3963         post = {
3964             'os-migrateLive': {
3965                 'host': dest_hostname,
3966                 'block_migration': True,
3967             }
3968         }
3969         self.api.post_server_action(server['id'], post)
3970 
3971         migration = self._wait_for_migration_status(server, ['running'])
3972         self.api.force_complete_migration(server['id'],
3973                                           migration['id'])
3974 
3975         self._wait_for_server_parameter(self.api, server,
3976                                         {'OS-EXT-SRV-ATTR:host': dest_hostname,
3977                                          'status': 'ACTIVE'})
3978 
3979         self._run_periodics()
3980 
3981         self.assertRequestMatchesUsage(
3982             {'VCPU': 0,
3983              'MEMORY_MB': 0,
3984              'DISK_GB': 0}, source_rp_uuid)
3985 
3986         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
3987         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
3988                                            dest_rp_uuid)
3989 
3990         self._delete_and_check_allocations(server)
3991 
3992     def test_live_migrate_delete(self):
3993         source_hostname = self.compute1.host
3994         dest_hostname = self.compute2.host
3995         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
3996         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
3997 
3998         server = self._boot_and_check_allocations(
3999             self.flavor1, source_hostname)
4000 
4001         post = {
4002             'os-migrateLive': {
4003                 'host': dest_hostname,
4004                 'block_migration': True,
4005             }
4006         }
4007         self.api.post_server_action(server['id'], post)
4008 
4009         migration = self._wait_for_migration_status(server, ['running'])
4010 
4011         self.api.delete_migration(server['id'], migration['id'])
4012         self._wait_for_server_parameter(self.api, server,
4013             {'OS-EXT-SRV-ATTR:host': source_hostname,
4014              'status': 'ACTIVE'})
4015 
4016         self._run_periodics()
4017 
4018         allocations = self._get_allocations_by_server_uuid(server['id'])
4019         self.assertNotIn(dest_rp_uuid, allocations)
4020 
4021         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
4022 
4023         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
4024                                            source_rp_uuid)
4025 
4026         self.assertRequestMatchesUsage({'VCPU': 0,
4027                                         'MEMORY_MB': 0,
4028                                         'DISK_GB': 0}, dest_rp_uuid)
4029 
4030         self._delete_and_check_allocations(server)
4031 
4032 
4033 class ServerLiveMigrateForceAndAbortWithNestedResourcesRequest(
4034         ServerLiveMigrateForceAndAbort):
4035     compute_driver = 'fake.FakeLiveMigrateDriverWithNestedCustomResources'
4036 
4037     def setUp(self):
4038         super(ServerLiveMigrateForceAndAbortWithNestedResourcesRequest,
4039               self).setUp()
4040         # modify the flavor used in the test base class to require one piece of
4041         # CUSTOM_MAGIC resource as well.
4042 
4043         self.api.post_extra_spec(
4044             self.flavor1['id'], {'extra_specs': {'resources:CUSTOM_MAGIC': 1}})
4045         # save the extra_specs in the flavor stored in the test case as
4046         # well
4047         self.flavor1['extra_specs'] = {'resources:CUSTOM_MAGIC': 1}
4048 
4049 
4050 class ServerRescheduleTests(integrated_helpers.ProviderUsageBaseTestCase):
4051     """Tests server create scenarios which trigger a reschedule during
4052     a server build and validates that allocations in Placement
4053     are properly cleaned up.
4054 
4055     Uses a fake virt driver that fails the build on the first attempt.
4056     """
4057 
4058     compute_driver = 'fake.FakeRescheduleDriver'
4059 
4060     def setUp(self):
4061         super(ServerRescheduleTests, self).setUp()
4062         self.compute1 = self._start_compute(host='host1')
4063         self.compute2 = self._start_compute(host='host2')
4064 
4065         flavors = self.api.get_flavors()
4066         self.flavor1 = flavors[0]
4067 
4068     def _other_hostname(self, host):
4069         other_host = {'host1': 'host2',
4070                       'host2': 'host1'}
4071         return other_host[host]
4072 
4073     def test_rescheduling_when_booting_instance(self):
4074         """Tests that allocations, created by the scheduler, are cleaned
4075         from the source node when the build fails on that node and is
4076         rescheduled to another node.
4077         """
4078         server_req = self._build_minimal_create_server_request(
4079                 self.api, 'some-server', flavor_id=self.flavor1['id'],
4080                 image_uuid='155d900f-4e14-4e4c-a73d-069cbf4541e6',
4081                 networks='none')
4082 
4083         created_server = self.api.post_server({'server': server_req})
4084         server = self._wait_for_state_change(
4085                 self.api, created_server, 'ACTIVE')
4086         dest_hostname = server['OS-EXT-SRV-ATTR:host']
4087         failed_hostname = self._other_hostname(dest_hostname)
4088 
4089         LOG.info('failed on %s', failed_hostname)
4090         LOG.info('booting on %s', dest_hostname)
4091 
4092         failed_rp_uuid = self._get_provider_uuid_by_host(failed_hostname)
4093         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
4094 
4095         # Expects no allocation records on the failed host.
4096         self.assertRequestMatchesUsage(
4097             {'VCPU': 0,
4098              'MEMORY_MB': 0,
4099              'DISK_GB': 0}, failed_rp_uuid)
4100 
4101         # Ensure the allocation records on the destination host.
4102         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
4103 
4104     def test_allocation_fails_during_reschedule(self):
4105         """Verify that if nova fails to allocate resources during re-schedule
4106         then the server is put into ERROR state properly.
4107         """
4108 
4109         server_req = self._build_minimal_create_server_request(
4110             self.api, 'some-server', flavor_id=self.flavor1['id'],
4111             image_uuid='155d900f-4e14-4e4c-a73d-069cbf4541e6',
4112             networks='none')
4113 
4114         orig_claim = utils.claim_resources
4115         # First call is during boot, we want that to succeed normally. Then the
4116         # fake virt driver triggers a re-schedule. During that re-schedule we
4117         # simulate that the placement call fails.
4118         with mock.patch('nova.scheduler.utils.claim_resources',
4119                         side_effect=[
4120                             orig_claim,
4121                             exception.AllocationUpdateFailed(
4122                                 consumer_uuid=uuids.inst1, error='testing')]):
4123 
4124             server = self.api.post_server({'server': server_req})
4125             server = self._wait_for_state_change(
4126                 self.admin_api, server, 'ERROR')
4127 
4128         self._delete_and_check_allocations(server)
4129 
4130 
4131 class ServerRescheduleTestsWithNestedResourcesRequest(ServerRescheduleTests):
4132     compute_driver = 'fake.FakeRescheduleDriverWithNestedCustomResources'
4133 
4134     def setUp(self):
4135         super(ServerRescheduleTestsWithNestedResourcesRequest, self).setUp()
4136         # modify the flavor used in the test base class to require one piece of
4137         # CUSTOM_MAGIC resource as well.
4138 
4139         self.api.post_extra_spec(
4140             self.flavor1['id'], {'extra_specs': {'resources:CUSTOM_MAGIC': 1}})
4141         # save the extra_specs in the flavor stored in the test case as
4142         # well
4143         self.flavor1['extra_specs'] = {'resources:CUSTOM_MAGIC': 1}
4144 
4145 
4146 class ServerBuildAbortTests(integrated_helpers.ProviderUsageBaseTestCase):
4147     """Tests server create scenarios which trigger a build abort during
4148     a server build and validates that allocations in Placement
4149     are properly cleaned up.
4150 
4151     Uses a fake virt driver that aborts the build on the first attempt.
4152     """
4153 
4154     compute_driver = 'fake.FakeBuildAbortDriver'
4155 
4156     def setUp(self):
4157         super(ServerBuildAbortTests, self).setUp()
4158         # We only need one compute service/host/node for these tests.
4159         self.compute1 = self._start_compute(host='host1')
4160 
4161         flavors = self.api.get_flavors()
4162         self.flavor1 = flavors[0]
4163 
4164     def test_abort_when_booting_instance(self):
4165         """Tests that allocations, created by the scheduler, are cleaned
4166         from the source node when the build is aborted on that node.
4167         """
4168         server_req = self._build_minimal_create_server_request(
4169                 self.api, 'some-server', flavor_id=self.flavor1['id'],
4170                 image_uuid='155d900f-4e14-4e4c-a73d-069cbf4541e6',
4171                 networks='none')
4172 
4173         created_server = self.api.post_server({'server': server_req})
4174         self._wait_for_state_change(self.api, created_server, 'ERROR')
4175 
4176         failed_hostname = self.compute1.manager.host
4177 
4178         failed_rp_uuid = self._get_provider_uuid_by_host(failed_hostname)
4179         # Expects no allocation records on the failed host.
4180         self.assertRequestMatchesUsage({'VCPU': 0,
4181                                         'MEMORY_MB': 0,
4182                                         'DISK_GB': 0}, failed_rp_uuid)
4183 
4184 
4185 class ServerBuildAbortTestsWithNestedResourceRequest(ServerBuildAbortTests):
4186     compute_driver = 'fake.FakeBuildAbortDriverWithNestedCustomResources'
4187 
4188     def setUp(self):
4189         super(ServerBuildAbortTestsWithNestedResourceRequest, self).setUp()
4190         # modify the flavor used in the test base class to require one piece of
4191         # CUSTOM_MAGIC resource as well.
4192 
4193         self.api.post_extra_spec(
4194             self.flavor1['id'], {'extra_specs': {'resources:CUSTOM_MAGIC': 1}})
4195         # save the extra_specs in the flavor stored in the test case as
4196         # well
4197         self.flavor1['extra_specs'] = {'resources:CUSTOM_MAGIC': 1}
4198 
4199 
4200 class ServerUnshelveSpawnFailTests(
4201         integrated_helpers.ProviderUsageBaseTestCase):
4202     """Tests server unshelve scenarios which trigger a
4203     VirtualInterfaceCreateException during driver.spawn() and validates that
4204     allocations in Placement are properly cleaned up.
4205     """
4206 
4207     compute_driver = 'fake.FakeUnshelveSpawnFailDriver'
4208 
4209     def setUp(self):
4210         super(ServerUnshelveSpawnFailTests, self).setUp()
4211         # We only need one compute service/host/node for these tests.
4212         self.compute1 = self._start_compute('host1')
4213 
4214         flavors = self.api.get_flavors()
4215         self.flavor1 = flavors[0]
4216 
4217     def test_driver_spawn_fail_when_unshelving_instance(self):
4218         """Tests that allocations, created by the scheduler, are cleaned
4219         from the target node when the unshelve driver.spawn fails on that node.
4220         """
4221         hostname = self.compute1.manager.host
4222         rp_uuid = self._get_provider_uuid_by_host(hostname)
4223         # We start with no usages on the host.
4224         self.assertRequestMatchesUsage(
4225             {'VCPU': 0,
4226              'MEMORY_MB': 0,
4227              'DISK_GB': 0}, rp_uuid)
4228 
4229         server_req = self._build_minimal_create_server_request(
4230             self.api, 'unshelve-spawn-fail', flavor_id=self.flavor1['id'],
4231             image_uuid='155d900f-4e14-4e4c-a73d-069cbf4541e6',
4232             networks='none')
4233 
4234         server = self.api.post_server({'server': server_req})
4235         self._wait_for_state_change(self.api, server, 'ACTIVE')
4236 
4237         # assert allocations exist for the host
4238         self.assertFlavorMatchesUsage(rp_uuid, self.flavor1)
4239 
4240         # shelve offload the server
4241         self.flags(shelved_offload_time=0)
4242         self.api.post_server_action(server['id'], {'shelve': None})
4243         self._wait_for_server_parameter(
4244             self.api, server, {'status': 'SHELVED_OFFLOADED',
4245                                'OS-EXT-SRV-ATTR:host': None})
4246 
4247         # assert allocations were removed from the host
4248         self.assertRequestMatchesUsage(
4249             {'VCPU': 0,
4250              'MEMORY_MB': 0,
4251              'DISK_GB': 0}, rp_uuid)
4252 
4253         # unshelve the server, which should fail
4254         self.api.post_server_action(server['id'], {'unshelve': None})
4255         self._wait_for_action_fail_completion(
4256             server, instance_actions.UNSHELVE, 'compute_unshelve_instance')
4257 
4258         # assert allocations were removed from the host
4259         self.assertRequestMatchesUsage(
4260             {'VCPU': 0,
4261              'MEMORY_MB': 0,
4262              'DISK_GB': 0}, rp_uuid)
4263 
4264 
4265 class ServerUnshelveSpawnFailTestsWithNestedResourceRequest(
4266     ServerUnshelveSpawnFailTests):
4267     compute_driver = ('fake.'
4268                       'FakeUnshelveSpawnFailDriverWithNestedCustomResources')
4269 
4270     def setUp(self):
4271         super(ServerUnshelveSpawnFailTestsWithNestedResourceRequest,
4272               self).setUp()
4273         # modify the flavor used in the test base class to require one piece of
4274         # CUSTOM_MAGIC resource as well.
4275 
4276         self.api.post_extra_spec(
4277             self.flavor1['id'], {'extra_specs': {'resources:CUSTOM_MAGIC': 1}})
4278         # save the extra_specs in the flavor stored in the test case as
4279         # well
4280         self.flavor1['extra_specs'] = {'resources:CUSTOM_MAGIC': 1}
4281 
4282 
4283 class ServerSoftDeleteTests(integrated_helpers.ProviderUsageBaseTestCase):
4284 
4285     compute_driver = 'fake.SmallFakeDriver'
4286 
4287     def setUp(self):
4288         super(ServerSoftDeleteTests, self).setUp()
4289         # We only need one compute service/host/node for these tests.
4290         self.compute1 = self._start_compute('host1')
4291 
4292         flavors = self.api.get_flavors()
4293         self.flavor1 = flavors[0]
4294 
4295     def _soft_delete_and_check_allocation(self, server, hostname):
4296         self.api.delete_server(server['id'])
4297         server = self._wait_for_state_change(self.api, server, 'SOFT_DELETED')
4298 
4299         self._run_periodics()
4300 
4301         # in soft delete state nova should keep the resource allocation as
4302         # the instance can be restored
4303         rp_uuid = self._get_provider_uuid_by_host(hostname)
4304 
4305         self.assertFlavorMatchesUsage(rp_uuid, self.flavor1)
4306 
4307         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
4308                                            rp_uuid)
4309 
4310         # run the periodic reclaim but as time isn't advanced it should not
4311         # reclaim the instance
4312         ctxt = context.get_admin_context()
4313         self.compute1._reclaim_queued_deletes(ctxt)
4314 
4315         self._run_periodics()
4316 
4317         self.assertFlavorMatchesUsage(rp_uuid, self.flavor1)
4318 
4319         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
4320                                            rp_uuid)
4321 
4322     def test_soft_delete_then_reclaim(self):
4323         """Asserts that the automatic reclaim of soft deleted instance cleans
4324         up the allocations in placement.
4325         """
4326 
4327         # make sure that instance will go to SOFT_DELETED state instead of
4328         # deleted immediately
4329         self.flags(reclaim_instance_interval=30)
4330 
4331         hostname = self.compute1.host
4332         rp_uuid = self._get_provider_uuid_by_host(hostname)
4333 
4334         server = self._boot_and_check_allocations(self.flavor1, hostname)
4335 
4336         self._soft_delete_and_check_allocation(server, hostname)
4337 
4338         # advance the time and run periodic reclaim, instance should be deleted
4339         # and resources should be freed
4340         the_past = timeutils.utcnow() + datetime.timedelta(hours=1)
4341         timeutils.set_time_override(override_time=the_past)
4342         self.addCleanup(timeutils.clear_time_override)
4343         ctxt = context.get_admin_context()
4344         self.compute1._reclaim_queued_deletes(ctxt)
4345 
4346         # Wait for real deletion
4347         self._wait_until_deleted(server)
4348 
4349         usages = self._get_provider_usages(rp_uuid)
4350         self.assertEqual({'VCPU': 0,
4351                           'MEMORY_MB': 0,
4352                           'DISK_GB': 0}, usages)
4353         allocations = self._get_allocations_by_server_uuid(server['id'])
4354         self.assertEqual(0, len(allocations))
4355 
4356     def test_soft_delete_then_restore(self):
4357         """Asserts that restoring a soft deleted instance keeps the proper
4358         allocation in placement.
4359         """
4360 
4361         # make sure that instance will go to SOFT_DELETED state instead of
4362         # deleted immediately
4363         self.flags(reclaim_instance_interval=30)
4364 
4365         hostname = self.compute1.host
4366         rp_uuid = self._get_provider_uuid_by_host(hostname)
4367 
4368         server = self._boot_and_check_allocations(
4369             self.flavor1, hostname)
4370 
4371         self._soft_delete_and_check_allocation(server, hostname)
4372 
4373         post = {'restore': {}}
4374         self.api.post_server_action(server['id'], post)
4375         server = self._wait_for_state_change(self.api, server, 'ACTIVE')
4376 
4377         # after restore the allocations should be kept
4378         self.assertFlavorMatchesUsage(rp_uuid, self.flavor1)
4379 
4380         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
4381                                            rp_uuid)
4382 
4383         # Now we want a real delete
4384         self.flags(reclaim_instance_interval=0)
4385         self._delete_and_check_allocations(server)
4386 
4387 
4388 class ServerSoftDeleteTestsWithNestedResourceRequest(ServerSoftDeleteTests):
4389     compute_driver = 'fake.MediumFakeDriverWithNestedCustomResources'
4390 
4391     def setUp(self):
4392         super(ServerSoftDeleteTestsWithNestedResourceRequest, self).setUp()
4393         # modify the flavor used in the test base class to require one piece of
4394         # CUSTOM_MAGIC resource as well.
4395 
4396         self.api.post_extra_spec(
4397             self.flavor1['id'], {'extra_specs': {'resources:CUSTOM_MAGIC': 1}})
4398         # save the extra_specs in the flavor stored in the test case as
4399         # well
4400         self.flavor1['extra_specs'] = {'resources:CUSTOM_MAGIC': 1}
4401 
4402 
4403 class VolumeBackedServerTest(integrated_helpers.ProviderUsageBaseTestCase):
4404     """Tests for volume-backed servers."""
4405 
4406     compute_driver = 'fake.SmallFakeDriver'
4407 
4408     def setUp(self):
4409         super(VolumeBackedServerTest, self).setUp()
4410         self.compute1 = self._start_compute('host1')
4411         self.flavor_id = self._create_flavor()
4412 
4413     def _create_flavor(self):
4414         body = {
4415             'flavor': {
4416                 'id': 'vbst',
4417                 'name': 'special',
4418                 'ram': 512,
4419                 'vcpus': 1,
4420                 'disk': 10,
4421                 'OS-FLV-EXT-DATA:ephemeral': 20,
4422                 'swap': 5 * 1024,
4423                 'rxtx_factor': 1.0,
4424                 'os-flavor-access:is_public': True,
4425             },
4426         }
4427         self.admin_api.post_flavor(body)
4428         return body['flavor']['id']
4429 
4430     def _create_server(self):
4431         with nova.utils.temporary_mutation(self.api, microversion='2.35'):
4432             image_id = self.api.get_images()[0]['id']
4433         server_req = self._build_minimal_create_server_request(
4434             self.api, 'trait-based-server',
4435             image_uuid=image_id,
4436             flavor_id=self.flavor_id, networks='none')
4437         server = self.api.post_server({'server': server_req})
4438         server = self._wait_for_state_change(self.api, server, 'ACTIVE')
4439         return server
4440 
4441     def _create_volume_backed_server(self):
4442         self.useFixture(nova_fixtures.CinderFixtureNewAttachFlow(self))
4443         volume_id = nova_fixtures.CinderFixtureNewAttachFlow.IMAGE_BACKED_VOL
4444         server_req_body = {
4445             # There is no imageRef because this is boot from volume.
4446             'server': {
4447                 'flavorRef': self.flavor_id,
4448                 'name': 'test_volume_backed',
4449                 # We don't care about networking for this test. This
4450                 # requires microversion >= 2.37.
4451                 'networks': 'none',
4452                 'block_device_mapping_v2': [{
4453                     'boot_index': 0,
4454                     'uuid': volume_id,
4455                     'source_type': 'volume',
4456                     'destination_type': 'volume'
4457                 }]
4458             }
4459         }
4460         server = self.api.post_server(server_req_body)
4461         server = self._wait_for_state_change(self.api, server, 'ACTIVE')
4462         return server
4463 
4464     def test_ephemeral_has_disk_allocation(self):
4465         server = self._create_server()
4466         allocs = self._get_allocations_by_server_uuid(server['id'])
4467         resources = list(allocs.values())[0]['resources']
4468         self.assertIn('MEMORY_MB', resources)
4469         # 10gb root, 20gb ephemeral, 5gb swap
4470         expected_usage = 35
4471         self.assertEqual(expected_usage, resources['DISK_GB'])
4472         # Ensure the compute node is reporting the correct disk usage
4473         self.assertEqual(
4474             expected_usage,
4475             self.admin_api.get_hypervisor_stats()['local_gb_used'])
4476 
4477     def test_volume_backed_no_disk_allocation(self):
4478         server = self._create_volume_backed_server()
4479         allocs = self._get_allocations_by_server_uuid(server['id'])
4480         resources = list(allocs.values())[0]['resources']
4481         self.assertIn('MEMORY_MB', resources)
4482         # 0gb root, 20gb ephemeral, 5gb swap
4483         expected_usage = 25
4484         self.assertEqual(expected_usage, resources['DISK_GB'])
4485         # Ensure the compute node is reporting the correct disk usage
4486         self.assertEqual(
4487             expected_usage,
4488             self.admin_api.get_hypervisor_stats()['local_gb_used'])
4489 
4490         # Now let's hack the RequestSpec.is_bfv field to mimic migrating an
4491         # old instance created before RequestSpec.is_bfv was set in the API,
4492         # move the instance and verify that the RequestSpec.is_bfv is set
4493         # and the instance still reports the same DISK_GB allocations as during
4494         # the initial create.
4495         ctxt = context.get_admin_context()
4496         reqspec = objects.RequestSpec.get_by_instance_uuid(ctxt, server['id'])
4497         # Make sure it's set.
4498         self.assertTrue(reqspec.is_bfv)
4499         del reqspec.is_bfv
4500         reqspec.save()
4501         reqspec = objects.RequestSpec.get_by_instance_uuid(ctxt, server['id'])
4502         # Make sure it's not set.
4503         self.assertNotIn('is_bfv', reqspec)
4504         # Now migrate the instance to another host and check the request spec
4505         # and allocations after the migration.
4506         self._start_compute('host2')
4507         self.admin_api.post_server_action(server['id'], {'migrate': None})
4508         # Wait for the server to complete the cold migration.
4509         server = self._wait_for_state_change(
4510             self.admin_api, server, 'VERIFY_RESIZE')
4511         self.assertEqual('host2', server['OS-EXT-SRV-ATTR:host'])
4512         # Confirm the cold migration and check usage and the request spec.
4513         self.api.post_server_action(server['id'], {'confirmResize': None})
4514         self._wait_for_state_change(self.api, server, 'ACTIVE')
4515         reqspec = objects.RequestSpec.get_by_instance_uuid(ctxt, server['id'])
4516         # Make sure it's set.
4517         self.assertTrue(reqspec.is_bfv)
4518         allocs = self._get_allocations_by_server_uuid(server['id'])
4519         resources = list(allocs.values())[0]['resources']
4520         self.assertEqual(expected_usage, resources['DISK_GB'])
4521 
4522         # Now shelve and unshelve the server to make sure root_gb DISK_GB
4523         # isn't reported for allocations after we unshelve the server.
4524         fake_notifier.stub_notifier(self)
4525         self.addCleanup(fake_notifier.reset)
4526         self.api.post_server_action(server['id'], {'shelve': None})
4527         self._wait_for_state_change(self.api, server, 'SHELVED_OFFLOADED')
4528         fake_notifier.wait_for_versioned_notifications(
4529                 'instance.shelve_offload.end')
4530         # The server should not have any allocations since it's not currently
4531         # hosted on any compute service.
4532         allocs = self._get_allocations_by_server_uuid(server['id'])
4533         self.assertDictEqual({}, allocs)
4534         # Now unshelve the server and make sure there are still no DISK_GB
4535         # allocations for the root disk.
4536         self.api.post_server_action(server['id'], {'unshelve': None})
4537         self._wait_for_state_change(self.api, server, 'ACTIVE')
4538         allocs = self._get_allocations_by_server_uuid(server['id'])
4539         resources = list(allocs.values())[0]['resources']
4540         self.assertEqual(expected_usage, resources['DISK_GB'])
4541 
4542 
4543 class TraitsBasedSchedulingTest(integrated_helpers.ProviderUsageBaseTestCase):
4544     """Tests for requesting a server with required traits in Placement"""
4545 
4546     compute_driver = 'fake.SmallFakeDriver'
4547 
4548     def setUp(self):
4549         super(TraitsBasedSchedulingTest, self).setUp()
4550         self.compute1 = self._start_compute('host1')
4551         self.compute2 = self._start_compute('host2')
4552         # Using a standard trait from the os-traits library, set a required
4553         # trait extra spec on the flavor.
4554         flavors = self.api.get_flavors()
4555         self.flavor_with_trait = flavors[0]
4556         self.admin_api.post_extra_spec(
4557             self.flavor_with_trait['id'],
4558             {'extra_specs': {'trait:HW_CPU_X86_VMX': 'required'}})
4559         self.flavor_without_trait = flavors[1]
4560         self.flavor_with_forbidden_trait = flavors[2]
4561         self.admin_api.post_extra_spec(
4562             self.flavor_with_forbidden_trait['id'],
4563             {'extra_specs': {'trait:HW_CPU_X86_SGX': 'forbidden'}})
4564 
4565         # Note that we're using v2.35 explicitly as the api returns 404
4566         # starting with 2.36
4567         with nova.utils.temporary_mutation(self.api, microversion='2.35'):
4568             images = self.api.get_images()
4569             self.image_id_with_trait = images[0]['id']
4570             self.api.api_put('/images/%s/metadata' % self.image_id_with_trait,
4571                              {'metadata': {
4572                                  'trait:HW_CPU_X86_SGX': 'required'}})
4573             self.image_id_without_trait = images[1]['id']
4574 
4575     def _create_server_with_traits(self, flavor_id, image_id):
4576         """Create a server with given flavor and image id's
4577         :param flavor_id: the flavor id
4578         :param image_id: the image id
4579         :return: create server response
4580         """
4581 
4582         server_req = self._build_minimal_create_server_request(
4583             self.api, 'trait-based-server',
4584             image_uuid=image_id,
4585             flavor_id=flavor_id, networks='none')
4586         return self.api.post_server({'server': server_req})
4587 
4588     def _create_volume_backed_server_with_traits(self, flavor_id, volume_id):
4589         """Create a server with block device mapping(volume) with the given
4590         flavor and volume id's. Either the flavor or the image backing the
4591         volume is expected to have the traits
4592         :param flavor_id: the flavor id
4593         :param volume_id: the volume id
4594         :return: create server response
4595         """
4596 
4597         server_req_body = {
4598             # There is no imageRef because this is boot from volume.
4599             'server': {
4600                 'flavorRef': flavor_id,
4601                 'name': 'test_image_trait_on_volume_backed',
4602                 # We don't care about networking for this test. This
4603                 # requires microversion >= 2.37.
4604                 'networks': 'none',
4605                 'block_device_mapping_v2': [{
4606                     'boot_index': 0,
4607                     'uuid': volume_id,
4608                     'source_type': 'volume',
4609                     'destination_type': 'volume'
4610                 }]
4611             }
4612         }
4613         server = self.api.post_server(server_req_body)
4614         return server
4615 
4616     def test_flavor_traits_based_scheduling(self):
4617         """Tests that a server create request using a required trait in the
4618         flavor ends up on the single compute node resource provider that also
4619         has that trait in Placement. That test will however pass half of the
4620         times even if the trait is not taken into consideration, so we are
4621         also disabling the compute node that has the required trait and try
4622         again, which should result in a no valid host error.
4623         """
4624 
4625         # Decorate compute1 resource provider with the required trait.
4626         rp_uuid = self._get_provider_uuid_by_host(self.compute1.host)
4627         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_VMX'])
4628 
4629         # Create server using flavor with required trait
4630         server = self._create_server_with_traits(self.flavor_with_trait['id'],
4631                                                  self.image_id_without_trait)
4632         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
4633         # Assert the server ended up on the expected compute host that has
4634         # the required trait.
4635         self.assertEqual(self.compute1.host, server['OS-EXT-SRV-ATTR:host'])
4636 
4637         # Disable the compute node that has the required trait
4638         compute1_service_id = self.admin_api.get_services(
4639             host=self.compute1.host, binary='nova-compute')[0]['id']
4640         self.admin_api.put_service(compute1_service_id, {'status': 'disabled'})
4641 
4642         # Create server using flavor with required trait
4643         server = self._create_server_with_traits(self.flavor_with_trait['id'],
4644                                                  self.image_id_without_trait)
4645 
4646         # The server should go to ERROR state because there is no valid host.
4647         server = self._wait_for_state_change(self.admin_api, server, 'ERROR')
4648         self.assertIsNone(server['OS-EXT-SRV-ATTR:host'])
4649         # Make sure the failure was due to NoValidHost by checking the fault.
4650         self.assertIn('fault', server)
4651         self.assertIn('No valid host', server['fault']['message'])
4652 
4653     def test_flavor_forbidden_traits_based_scheduling(self):
4654         """Tests that a server create request using a forbidden trait in the
4655         flavor ends up on the single compute host that doesn't have that
4656         trait in Placement. That test will however pass half of the times even
4657         if the trait is not taken into consideration, so we are also disabling
4658         the compute node that doesn't have the forbidden trait and try again,
4659         which should result in a no valid host error.
4660         """
4661 
4662         # Decorate compute1 resource provider with forbidden trait
4663         rp_uuid = self._get_provider_uuid_by_host(self.compute1.host)
4664         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_SGX'])
4665 
4666         # Create server using flavor with forbidden trait
4667         server = self._create_server_with_traits(
4668             self.flavor_with_forbidden_trait['id'],
4669             self.image_id_without_trait
4670         )
4671 
4672         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
4673 
4674         # Assert the server ended up on the expected compute host that doesn't
4675         # have the forbidden trait.
4676         self.assertEqual(self.compute2.host, server['OS-EXT-SRV-ATTR:host'])
4677 
4678         # Disable the compute node that doesn't have the forbidden trait
4679         compute2_service_id = self.admin_api.get_services(
4680             host=self.compute2.host, binary='nova-compute')[0]['id']
4681         self.admin_api.put_service(compute2_service_id, {'status': 'disabled'})
4682 
4683         # Create server using flavor with forbidden trait
4684         server = self._create_server_with_traits(
4685             self.flavor_with_forbidden_trait['id'],
4686             self.image_id_without_trait
4687         )
4688 
4689         # The server should go to ERROR state because there is no valid host.
4690         server = self._wait_for_state_change(self.admin_api, server, 'ERROR')
4691         self.assertIsNone(server['OS-EXT-SRV-ATTR:host'])
4692         # Make sure the failure was due to NoValidHost by checking the fault.
4693         self.assertIn('fault', server)
4694         self.assertIn('No valid host', server['fault']['message'])
4695 
4696     def test_image_traits_based_scheduling(self):
4697         """Tests that a server create request using a required trait on image
4698         ends up on the single compute node resource provider that also has that
4699         trait in Placement.
4700         """
4701 
4702         # Decorate compute2 resource provider with image trait.
4703         rp_uuid = self._get_provider_uuid_by_host(self.compute2.host)
4704         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_SGX'])
4705 
4706         # Create server using only image trait
4707         server = self._create_server_with_traits(
4708             self.flavor_without_trait['id'], self.image_id_with_trait)
4709         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
4710         # Assert the server ended up on the expected compute host that has
4711         # the required trait.
4712         self.assertEqual(self.compute2.host, server['OS-EXT-SRV-ATTR:host'])
4713 
4714     def test_flavor_image_traits_based_scheduling(self):
4715         """Tests that a server create request using a required trait on flavor
4716         AND a required trait on the image ends up on the single compute node
4717         resource provider that also has that trait in Placement.
4718         """
4719 
4720         # Decorate compute2 resource provider with both flavor and image trait.
4721         rp_uuid = self._get_provider_uuid_by_host(self.compute2.host)
4722         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_VMX',
4723                                             'HW_CPU_X86_SGX'])
4724 
4725         # Create server using flavor and image trait
4726         server = self._create_server_with_traits(
4727             self.flavor_with_trait['id'], self.image_id_with_trait)
4728         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
4729         # Assert the server ended up on the expected compute host that has
4730         # the required trait.
4731         self.assertEqual(self.compute2.host, server['OS-EXT-SRV-ATTR:host'])
4732 
4733     def test_image_trait_on_volume_backed_instance(self):
4734         """Tests that when trying to launch a volume-backed instance with a
4735         required trait on the image metadata contained within the volume,
4736         the instance ends up on the single compute node resource provider
4737         that also has that trait in Placement.
4738         """
4739         # Decorate compute2 resource provider with volume image metadata trait.
4740         rp_uuid = self._get_provider_uuid_by_host(self.compute2.host)
4741         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_SGX'])
4742 
4743         self.useFixture(nova_fixtures.CinderFixtureNewAttachFlow(self))
4744         # Create our server with a volume containing the image meta data with a
4745         # required trait
4746         server = self._create_volume_backed_server_with_traits(
4747             self.flavor_without_trait['id'],
4748             nova_fixtures.CinderFixtureNewAttachFlow.
4749             IMAGE_WITH_TRAITS_BACKED_VOL)
4750 
4751         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
4752         # Assert the server ended up on the expected compute host that has
4753         # the required trait.
4754         self.assertEqual(self.compute2.host, server['OS-EXT-SRV-ATTR:host'])
4755 
4756     def test_flavor_image_trait_on_volume_backed_instance(self):
4757         """Tests that when trying to launch a volume-backed instance with a
4758         required trait on flavor AND a required trait on the image metadata
4759         contained within the volume, the instance ends up on the single
4760         compute node resource provider that also has those traits in Placement.
4761         """
4762         # Decorate compute2 resource provider with volume image metadata trait.
4763         rp_uuid = self._get_provider_uuid_by_host(self.compute2.host)
4764         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_VMX',
4765                                             'HW_CPU_X86_SGX'])
4766 
4767         self.useFixture(nova_fixtures.CinderFixtureNewAttachFlow(self))
4768         # Create our server with a flavor trait and a volume containing the
4769         # image meta data with a required trait
4770         server = self._create_volume_backed_server_with_traits(
4771             self.flavor_with_trait['id'],
4772             nova_fixtures.CinderFixtureNewAttachFlow.
4773             IMAGE_WITH_TRAITS_BACKED_VOL)
4774 
4775         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
4776         # Assert the server ended up on the expected compute host that has
4777         # the required trait.
4778         self.assertEqual(self.compute2.host, server['OS-EXT-SRV-ATTR:host'])
4779 
4780     def test_flavor_traits_based_scheduling_no_valid_host(self):
4781         """Tests that a server create request using a required trait expressed
4782          in flavor fails to find a valid host since no compute node resource
4783          providers have the trait.
4784         """
4785 
4786         # Decorate compute1 resource provider with the image trait.
4787         rp_uuid = self._get_provider_uuid_by_host(self.compute1.host)
4788         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_SGX'])
4789 
4790         server = self._create_server_with_traits(self.flavor_with_trait['id'],
4791                                                  self.image_id_without_trait)
4792         # The server should go to ERROR state because there is no valid host.
4793         server = self._wait_for_state_change(self.admin_api, server, 'ERROR')
4794         self.assertIsNone(server['OS-EXT-SRV-ATTR:host'])
4795         # Make sure the failure was due to NoValidHost by checking the fault.
4796         self.assertIn('fault', server)
4797         self.assertIn('No valid host', server['fault']['message'])
4798 
4799     def test_image_traits_based_scheduling_no_valid_host(self):
4800         """Tests that a server create request using a required trait expressed
4801          in image fails to find a valid host since no compute node resource
4802          providers have the trait.
4803         """
4804 
4805         # Decorate compute1 resource provider with that flavor trait.
4806         rp_uuid = self._get_provider_uuid_by_host(self.compute1.host)
4807         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_VMX'])
4808 
4809         server = self._create_server_with_traits(
4810             self.flavor_without_trait['id'], self.image_id_with_trait)
4811         # The server should go to ERROR state because there is no valid host.
4812         server = self._wait_for_state_change(self.admin_api, server, 'ERROR')
4813         self.assertIsNone(server['OS-EXT-SRV-ATTR:host'])
4814         # Make sure the failure was due to NoValidHost by checking the fault.
4815         self.assertIn('fault', server)
4816         self.assertIn('No valid host', server['fault']['message'])
4817 
4818     def test_flavor_image_traits_based_scheduling_no_valid_host(self):
4819         """Tests that a server create request using a required trait expressed
4820          in flavor AND a required trait expressed in the image fails to find a
4821          valid host since no compute node resource providers have the trait.
4822         """
4823 
4824         server = self._create_server_with_traits(
4825             self.flavor_with_trait['id'], self.image_id_with_trait)
4826         # The server should go to ERROR state because there is no valid host.
4827         server = self._wait_for_state_change(self.admin_api, server, 'ERROR')
4828         self.assertIsNone(server['OS-EXT-SRV-ATTR:host'])
4829         # Make sure the failure was due to NoValidHost by checking the fault.
4830         self.assertIn('fault', server)
4831         self.assertIn('No valid host', server['fault']['message'])
4832 
4833     def test_image_trait_on_volume_backed_instance_no_valid_host(self):
4834         """Tests that when trying to launch a volume-backed instance with a
4835         required trait on the image metadata contained within the volume
4836         fails to find a valid host since no compute node resource providers
4837         have the trait.
4838         """
4839         self.useFixture(nova_fixtures.CinderFixtureNewAttachFlow(self))
4840         # Create our server with a volume
4841         server = self._create_volume_backed_server_with_traits(
4842             self.flavor_without_trait['id'],
4843             nova_fixtures.CinderFixtureNewAttachFlow.
4844             IMAGE_WITH_TRAITS_BACKED_VOL)
4845 
4846         # The server should go to ERROR state because there is no valid host.
4847         server = self._wait_for_state_change(self.admin_api, server, 'ERROR')
4848         self.assertIsNone(server['OS-EXT-SRV-ATTR:host'])
4849         # Make sure the failure was due to NoValidHost by checking the fault.
4850         self.assertIn('fault', server)
4851         self.assertIn('No valid host', server['fault']['message'])
4852 
4853     def test_rebuild_instance_with_image_traits(self):
4854         """Rebuilds a server with a different image which has traits
4855         associated with it and which will run it through the scheduler to
4856         validate the image is still OK with the compute host that the
4857         instance is running on.
4858          """
4859         # Decorate compute2 resource provider with both flavor and image trait.
4860         rp_uuid = self._get_provider_uuid_by_host(self.compute2.host)
4861         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_VMX',
4862                                             'HW_CPU_X86_SGX'])
4863         # make sure we start with no usage on the compute node
4864         rp_usages = self._get_provider_usages(rp_uuid)
4865         self.assertEqual({'VCPU': 0, 'MEMORY_MB': 0, 'DISK_GB': 0}, rp_usages)
4866 
4867         # create a server without traits on image and with traits on flavour
4868         server = self._create_server_with_traits(
4869             self.flavor_with_trait['id'], self.image_id_without_trait)
4870         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
4871 
4872         # make the compute node full and ensure rebuild still succeed
4873         inv = {"resource_class": "VCPU",
4874                "total": 1}
4875         self._set_inventory(rp_uuid, inv)
4876 
4877         # Now rebuild the server with a different image with traits
4878         rebuild_req_body = {
4879             'rebuild': {
4880                 'imageRef': self.image_id_with_trait
4881             }
4882         }
4883         self.api.api_post('/servers/%s/action' % server['id'],
4884                           rebuild_req_body)
4885         self._wait_for_server_parameter(
4886             self.api, server, {'OS-EXT-STS:task_state': None})
4887 
4888         allocs = self._get_allocations_by_server_uuid(server['id'])
4889         self.assertIn(rp_uuid, allocs)
4890 
4891         # Assert the server ended up on the expected compute host that has
4892         # the required trait.
4893         self.assertEqual(self.compute2.host, server['OS-EXT-SRV-ATTR:host'])
4894 
4895     def test_rebuild_instance_with_image_traits_no_host(self):
4896         """Rebuilding a server with a different image which has required
4897         traits on the image fails to valid the host that this server is
4898         currently running, cause the compute host resource provider is not
4899         associated with similar trait.
4900         """
4901         # Decorate compute2 resource provider with traits on flavor
4902         rp_uuid = self._get_provider_uuid_by_host(self.compute2.host)
4903         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_VMX'])
4904 
4905         # make sure we start with no usage on the compute node
4906         rp_usages = self._get_provider_usages(rp_uuid)
4907         self.assertEqual({'VCPU': 0, 'MEMORY_MB': 0, 'DISK_GB': 0}, rp_usages)
4908 
4909         # create a server without traits on image and with traits on flavour
4910         server = self._create_server_with_traits(
4911             self.flavor_with_trait['id'], self.image_id_without_trait)
4912         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
4913 
4914         # Now rebuild the server with a different image with traits
4915         rebuild_req_body = {
4916             'rebuild': {
4917                 'imageRef': self.image_id_with_trait
4918             }
4919         }
4920 
4921         self.api.api_post('/servers/%s/action' % server['id'],
4922                           rebuild_req_body)
4923         # Look for the failed rebuild action.
4924         self._wait_for_action_fail_completion(
4925             server, instance_actions.REBUILD, 'rebuild_server', self.admin_api)
4926         # Assert the server image_ref was rolled back on failure.
4927         server = self.api.get_server(server['id'])
4928         self.assertEqual(self.image_id_without_trait, server['image']['id'])
4929 
4930         # The server should be in ERROR state
4931         self.assertEqual('ERROR', server['status'])
4932         self.assertEqual("No valid host was found. Image traits cannot be "
4933                          "satisfied by the current resource providers. "
4934                          "Either specify a different image during rebuild "
4935                          "or create a new server with the specified image.",
4936                          server['fault']['message'])
4937 
4938     def test_rebuild_instance_with_image_traits_no_image_change(self):
4939         """Rebuilds a server with a same image which has traits
4940         associated with it and which will run it through the scheduler to
4941         validate the image is still OK with the compute host that the
4942         instance is running on.
4943          """
4944         # Decorate compute2 resource provider with both flavor and image trait.
4945         rp_uuid = self._get_provider_uuid_by_host(self.compute2.host)
4946         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_VMX',
4947                                             'HW_CPU_X86_SGX'])
4948         # make sure we start with no usage on the compute node
4949         rp_usages = self._get_provider_usages(rp_uuid)
4950         self.assertEqual({'VCPU': 0, 'MEMORY_MB': 0, 'DISK_GB': 0},
4951                          rp_usages)
4952 
4953         # create a server with traits in both image and flavour
4954         server = self._create_server_with_traits(
4955             self.flavor_with_trait['id'], self.image_id_with_trait)
4956         server = self._wait_for_state_change(self.admin_api, server,
4957                                              'ACTIVE')
4958 
4959         # Now rebuild the server with a different image with traits
4960         rebuild_req_body = {
4961             'rebuild': {
4962                 'imageRef': self.image_id_with_trait
4963             }
4964         }
4965         self.api.api_post('/servers/%s/action' % server['id'],
4966                           rebuild_req_body)
4967         self._wait_for_server_parameter(
4968             self.api, server, {'OS-EXT-STS:task_state': None})
4969 
4970         allocs = self._get_allocations_by_server_uuid(server['id'])
4971         self.assertIn(rp_uuid, allocs)
4972 
4973         # Assert the server ended up on the expected compute host that has
4974         # the required trait.
4975         self.assertEqual(self.compute2.host,
4976                          server['OS-EXT-SRV-ATTR:host'])
4977 
4978     def test_rebuild_instance_with_image_traits_and_forbidden_flavor_traits(
4979                                                                         self):
4980         """Rebuilding a server with a different image which has required
4981         traits on the image fails to validate image traits because flavor
4982         associated with the current instance has the similar trait that is
4983         forbidden
4984         """
4985         # Decorate compute2 resource provider with traits on flavor
4986         rp_uuid = self._get_provider_uuid_by_host(self.compute2.host)
4987         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_VMX'])
4988 
4989         # make sure we start with no usage on the compute node
4990         rp_usages = self._get_provider_usages(rp_uuid)
4991         self.assertEqual({'VCPU': 0, 'MEMORY_MB': 0, 'DISK_GB': 0}, rp_usages)
4992 
4993         # create a server with forbidden traits on flavor and no triats on
4994         # image
4995         server = self._create_server_with_traits(
4996             self.flavor_with_forbidden_trait['id'],
4997             self.image_id_without_trait)
4998         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
4999 
5000         # Now rebuild the server with a different image with traits
5001         rebuild_req_body = {
5002             'rebuild': {
5003                 'imageRef': self.image_id_with_trait
5004             }
5005         }
5006 
5007         self.api.api_post('/servers/%s/action' % server['id'],
5008                           rebuild_req_body)
5009         # Look for the failed rebuild action.
5010         self._wait_for_action_fail_completion(
5011             server, instance_actions.REBUILD, 'rebuild_server', self.admin_api)
5012         # Assert the server image_ref was rolled back on failure.
5013         server = self.api.get_server(server['id'])
5014         self.assertEqual(self.image_id_without_trait, server['image']['id'])
5015 
5016         # The server should be in ERROR state
5017         self.assertEqual('ERROR', server['status'])
5018         self.assertEqual("No valid host was found. Image traits are part of "
5019                          "forbidden traits in flavor associated with the "
5020                          "server. Either specify a different image during "
5021                          "rebuild or create a new server with the specified "
5022                          "image and a compatible flavor.",
5023                          server['fault']['message'])
5024 
5025 
5026 class ServerTestV256Common(ServersTestBase):
5027     api_major_version = 'v2.1'
5028     microversion = '2.56'
5029     ADMIN_API = True
5030 
5031     def _setup_compute_service(self):
5032         # Set up 3 compute services in the same cell
5033         self.addCleanup(fake.restore_nodes)
5034         for host in ('host1', 'host2', 'host3'):
5035             fake.set_nodes([host])
5036             self.start_service('compute', host=host)
5037 
5038     def _create_server(self, target_host=None):
5039         server = self._build_minimal_create_server_request(
5040             image_uuid='a2459075-d96c-40d5-893e-577ff92e721c')
5041         server.update({'networks': 'auto'})
5042         if target_host is not None:
5043             server['availability_zone'] = 'nova:%s' % target_host
5044         post = {'server': server}
5045         response = self.api.api_post('/servers', post).body
5046         return response['server']
5047 
5048     @staticmethod
5049     def _get_target_and_other_hosts(host):
5050         target_other_hosts = {'host1': ['host2', 'host3'],
5051                               'host2': ['host3', 'host1'],
5052                               'host3': ['host1', 'host2']}
5053         return target_other_hosts[host]
5054 
5055 
5056 class ServerTestV256MultiCellTestCase(ServerTestV256Common):
5057     """Negative test to ensure we fail with ComputeHostNotFound if we try to
5058     target a host in another cell from where the instance lives.
5059     """
5060     NUMBER_OF_CELLS = 2
5061 
5062     def _setup_compute_service(self):
5063         # Set up 2 compute services in different cells
5064         host_to_cell_mappings = {
5065             'host1': 'cell1',
5066             'host2': 'cell2'}
5067         self.addCleanup(fake.restore_nodes)
5068         for host in sorted(host_to_cell_mappings):
5069             fake.set_nodes([host])
5070             self.start_service('compute', host=host,
5071                                cell=host_to_cell_mappings[host])
5072 
5073     def test_migrate_server_to_host_in_different_cell(self):
5074         # We target host1 specifically so that we have a predictable target for
5075         # the cold migration in cell2.
5076         server = self._create_server(target_host='host1')
5077         server = self._wait_for_state_change(server, 'BUILD')
5078 
5079         self.assertEqual('host1', server['OS-EXT-SRV-ATTR:host'])
5080         ex = self.assertRaises(client.OpenStackApiException,
5081                                self.api.post_server_action,
5082                                server['id'],
5083                                {'migrate': {'host': 'host2'}})
5084         # When the API pulls the instance out of cell1, the context is targeted
5085         # to cell1, so when the compute API resize() method attempts to lookup
5086         # the target host in cell1, it will result in a ComputeHostNotFound
5087         # error.
5088         self.assertEqual(400, ex.response.status_code)
5089         self.assertIn('Compute host host2 could not be found',
5090                       six.text_type(ex))
5091 
5092 
5093 class ServerTestV256SingleCellMultiHostTestCase(ServerTestV256Common):
5094     """Happy path test where we create a server on one host, migrate it to
5095     another host of our choosing and ensure it lands there.
5096     """
5097     def test_migrate_server_to_host_in_same_cell(self):
5098         server = self._create_server()
5099         server = self._wait_for_state_change(server, 'BUILD')
5100         source_host = server['OS-EXT-SRV-ATTR:host']
5101         target_host = self._get_target_and_other_hosts(source_host)[0]
5102         self.api.post_server_action(server['id'],
5103                                     {'migrate': {'host': target_host}})
5104         # Assert the server is now on the target host.
5105         server = self.api.get_server(server['id'])
5106         self.assertEqual(target_host, server['OS-EXT-SRV-ATTR:host'])
5107 
5108 
5109 class ServerTestV256RescheduleTestCase(ServerTestV256Common):
5110 
5111     @mock.patch.object(compute_manager.ComputeManager, '_prep_resize',
5112                        side_effect=exception.MigrationError(
5113                            reason='Test Exception'))
5114     def test_migrate_server_not_reschedule(self, mock_prep_resize):
5115         server = self._create_server()
5116         found_server = self._wait_for_state_change(server, 'BUILD')
5117 
5118         target_host, other_host = self._get_target_and_other_hosts(
5119             found_server['OS-EXT-SRV-ATTR:host'])
5120 
5121         self.assertRaises(client.OpenStackApiException,
5122                           self.api.post_server_action,
5123                           server['id'],
5124                           {'migrate': {'host': target_host}})
5125         self.assertEqual(1, mock_prep_resize.call_count)
5126         found_server = self.api.get_server(server['id'])
5127         # Check that rescheduling is not occurred.
5128         self.assertNotEqual(other_host, found_server['OS-EXT-SRV-ATTR:host'])
5129 
5130 
5131 class ConsumerGenerationConflictTest(
5132         integrated_helpers.ProviderUsageBaseTestCase):
5133 
5134     # we need the medium driver to be able to allocate resource not just for
5135     # a single instance
5136     compute_driver = 'fake.MediumFakeDriver'
5137 
5138     def setUp(self):
5139         super(ConsumerGenerationConflictTest, self).setUp()
5140         flavors = self.api.get_flavors()
5141         self.flavor = flavors[0]
5142         self.other_flavor = flavors[1]
5143         self.compute1 = self._start_compute('compute1')
5144         self.compute2 = self._start_compute('compute2')
5145 
5146     def test_create_server_fails_as_placement_reports_consumer_conflict(self):
5147         server_req = self._build_minimal_create_server_request(
5148             self.api, 'some-server', flavor_id=self.flavor['id'],
5149             image_uuid='155d900f-4e14-4e4c-a73d-069cbf4541e6',
5150             networks='none')
5151 
5152         # We cannot pre-create a consumer with the uuid of the instance created
5153         # below as that uuid is generated. Instead we have to simulate that
5154         # Placement returns 409, consumer generation conflict for the PUT
5155         # /allocation request the scheduler does for the instance.
5156         with mock.patch('keystoneauth1.adapter.Adapter.put') as mock_put:
5157             rsp = fake_requests.FakeResponse(
5158                 409,
5159                 jsonutils.dumps(
5160                     {'errors': [
5161                         {'code': 'placement.concurrent_update',
5162                          'detail': 'consumer generation conflict'}]}))
5163             mock_put.return_value = rsp
5164 
5165             created_server = self.api.post_server({'server': server_req})
5166             server = self._wait_for_state_change(
5167                 self.admin_api, created_server, 'ERROR')
5168 
5169         # This is not a conflict that the API user can ever resolve. It is a
5170         # serious inconsistency in our database or a bug in the scheduler code
5171         # doing the claim.
5172         self.assertEqual(500, server['fault']['code'])
5173         self.assertIn('Failed to update allocations for consumer',
5174                       server['fault']['message'])
5175 
5176         allocations = self._get_allocations_by_server_uuid(server['id'])
5177         self.assertEqual(0, len(allocations))
5178 
5179         self._delete_and_check_allocations(server)
5180 
5181     def test_migrate_claim_on_dest_fails(self):
5182         source_hostname = self.compute1.host
5183         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
5184 
5185         server = self._boot_and_check_allocations(self.flavor, source_hostname)
5186 
5187         # We have to simulate that Placement returns 409, consumer generation
5188         # conflict for the PUT /allocation request the scheduler does on the
5189         # destination host for the instance.
5190         with mock.patch('keystoneauth1.adapter.Adapter.put') as mock_put:
5191             rsp = fake_requests.FakeResponse(
5192                 409,
5193                 jsonutils.dumps(
5194                     {'errors': [
5195                         {'code': 'placement.concurrent_update',
5196                          'detail': 'consumer generation conflict'}]}))
5197             mock_put.return_value = rsp
5198 
5199             request = {'migrate': None}
5200             exception = self.assertRaises(client.OpenStackApiException,
5201                                           self.api.post_server_action,
5202                                           server['id'], request)
5203 
5204         # I know that HTTP 500 is harsh code but I think this conflict case
5205         # signals either a serious db inconsistency or a bug in nova's
5206         # claim code.
5207         self.assertEqual(500, exception.response.status_code)
5208 
5209         # The migration is aborted so the instance is ACTIVE on the source
5210         # host instead of being in VERIFY_RESIZE state.
5211         server = self.api.get_server(server['id'])
5212         self.assertEqual('ACTIVE', server['status'])
5213         self.assertEqual(source_hostname, server['OS-EXT-SRV-ATTR:host'])
5214 
5215         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor)
5216 
5217         self.assertFlavorMatchesAllocation(self.flavor, server['id'],
5218                                            source_rp_uuid)
5219 
5220         self._delete_and_check_allocations(server)
5221 
5222     def test_migrate_move_allocation_fails_due_to_conflict(self):
5223         source_hostname = self.compute1.host
5224         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
5225 
5226         server = self._boot_and_check_allocations(self.flavor, source_hostname)
5227 
5228         rsp = fake_requests.FakeResponse(
5229             409,
5230             jsonutils.dumps(
5231                 {'errors': [
5232                     {'code': 'placement.concurrent_update',
5233                      'detail': 'consumer generation conflict'}]}))
5234 
5235         with mock.patch('keystoneauth1.adapter.Adapter.post',
5236                         autospec=True) as mock_post:
5237             mock_post.return_value = rsp
5238 
5239             request = {'migrate': None}
5240             exception = self.assertRaises(client.OpenStackApiException,
5241                                           self.api.post_server_action,
5242                                           server['id'], request)
5243 
5244         self.assertEqual(1, mock_post.call_count)
5245 
5246         self.assertEqual(409, exception.response.status_code)
5247         self.assertIn('Failed to move allocations', exception.response.text)
5248 
5249         migrations = self.api.get_migrations()
5250         self.assertEqual(1, len(migrations))
5251         self.assertEqual('migration', migrations[0]['migration_type'])
5252         self.assertEqual(server['id'], migrations[0]['instance_uuid'])
5253         self.assertEqual(source_hostname, migrations[0]['source_compute'])
5254         self.assertEqual('error', migrations[0]['status'])
5255 
5256         # The migration is aborted so the instance is ACTIVE on the source
5257         # host instead of being in VERIFY_RESIZE state.
5258         server = self.api.get_server(server['id'])
5259         self.assertEqual('ACTIVE', server['status'])
5260         self.assertEqual(source_hostname, server['OS-EXT-SRV-ATTR:host'])
5261 
5262         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor)
5263 
5264         self.assertFlavorMatchesAllocation(self.flavor, server['id'],
5265                                            source_rp_uuid)
5266 
5267         self._delete_and_check_allocations(server)
5268 
5269     def test_confirm_migrate_delete_alloc_on_source_fails(self):
5270         source_hostname = self.compute1.host
5271         dest_hostname = self.compute2.host
5272         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
5273         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
5274 
5275         server = self._boot_and_check_allocations(self.flavor, source_hostname)
5276         self._migrate_and_check_allocations(
5277             server, self.flavor, source_rp_uuid, dest_rp_uuid)
5278 
5279         rsp = fake_requests.FakeResponse(
5280             409,
5281             jsonutils.dumps(
5282                 {'errors': [
5283                     {'code': 'placement.concurrent_update',
5284                      'detail': 'consumer generation conflict'}]}))
5285 
5286         with mock.patch('keystoneauth1.adapter.Adapter.put',
5287                         autospec=True) as mock_put:
5288             mock_put.return_value = rsp
5289 
5290             post = {'confirmResize': None}
5291             self.api.post_server_action(
5292                 server['id'], post, check_response_status=[204])
5293             server = self._wait_for_state_change(self.api, server, 'ERROR')
5294             self.assertIn('Failed to delete allocations',
5295                           server['fault']['message'])
5296 
5297         self.assertEqual(1, mock_put.call_count)
5298 
5299         migrations = self.api.get_migrations()
5300         self.assertEqual(1, len(migrations))
5301         self.assertEqual('migration', migrations[0]['migration_type'])
5302         self.assertEqual(server['id'], migrations[0]['instance_uuid'])
5303         self.assertEqual(source_hostname, migrations[0]['source_compute'])
5304         self.assertEqual('error', migrations[0]['status'])
5305 
5306         # NOTE(gibi): Nova leaks the allocation held by the migration_uuid even
5307         # after the instance is deleted. At least nova logs a fat ERROR.
5308         self.assertIn('Deleting allocation in placement for migration %s '
5309                       'failed. The instance %s will be put to ERROR state but '
5310                       'the allocation held by the migration is leaked.' %
5311                       (migrations[0]['uuid'], server['id']),
5312                       self.stdlog.logger.output)
5313         self.api.delete_server(server['id'])
5314         self._wait_until_deleted(server)
5315         fake_notifier.wait_for_versioned_notifications('instance.delete.end')
5316 
5317         allocations = self._get_allocations_by_server_uuid(
5318             migrations[0]['uuid'])
5319         self.assertEqual(1, len(allocations))
5320 
5321     def test_revert_migrate_delete_dest_allocation_fails_due_to_conflict(self):
5322         source_hostname = self.compute1.host
5323         dest_hostname = self.compute2.host
5324         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
5325         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
5326 
5327         server = self._boot_and_check_allocations(self.flavor, source_hostname)
5328         self._migrate_and_check_allocations(
5329             server, self.flavor, source_rp_uuid, dest_rp_uuid)
5330 
5331         rsp = fake_requests.FakeResponse(
5332             409,
5333             jsonutils.dumps(
5334                 {'errors': [
5335                     {'code': 'placement.concurrent_update',
5336                      'detail': 'consumer generation conflict'}]}))
5337 
5338         with mock.patch('keystoneauth1.adapter.Adapter.post',
5339                         autospec=True) as mock_post:
5340             mock_post.return_value = rsp
5341 
5342             post = {'revertResize': None}
5343             self.api.post_server_action(server['id'], post)
5344             server = self._wait_for_state_change(self.api, server, 'ERROR')
5345 
5346         self.assertEqual(1, mock_post.call_count)
5347 
5348         migrations = self.api.get_migrations()
5349         self.assertEqual(1, len(migrations))
5350         self.assertEqual('migration', migrations[0]['migration_type'])
5351         self.assertEqual(server['id'], migrations[0]['instance_uuid'])
5352         self.assertEqual(source_hostname, migrations[0]['source_compute'])
5353         self.assertEqual('error', migrations[0]['status'])
5354 
5355         # NOTE(gibi): Nova leaks the allocation held by the migration_uuid even
5356         # after the instance is deleted. At least nova logs a fat ERROR.
5357         self.assertIn('Reverting allocation in placement for migration %s '
5358                       'failed. The instance %s will be put into ERROR state '
5359                       'but the allocation held by the migration is leaked.' %
5360                       (migrations[0]['uuid'], server['id']),
5361                       self.stdlog.logger.output)
5362         self.api.delete_server(server['id'])
5363         self._wait_until_deleted(server)
5364         fake_notifier.wait_for_versioned_notifications('instance.delete.end')
5365 
5366         allocations = self._get_allocations_by_server_uuid(
5367             migrations[0]['uuid'])
5368         self.assertEqual(1, len(allocations))
5369 
5370     def test_revert_resize_same_host_delete_dest_fails_due_to_conflict(self):
5371         # make sure that the test only uses a single host
5372         compute2_service_id = self.admin_api.get_services(
5373             host=self.compute2.host, binary='nova-compute')[0]['id']
5374         self.admin_api.put_service(compute2_service_id, {'status': 'disabled'})
5375 
5376         hostname = self.compute1.manager.host
5377         rp_uuid = self._get_provider_uuid_by_host(hostname)
5378 
5379         server = self._boot_and_check_allocations(self.flavor, hostname)
5380 
5381         self._resize_to_same_host_and_check_allocations(
5382             server, self.flavor, self.other_flavor, rp_uuid)
5383 
5384         rsp = fake_requests.FakeResponse(
5385             409,
5386             jsonutils.dumps(
5387                 {'errors': [
5388                     {'code': 'placement.concurrent_update',
5389                      'detail': 'consumer generation conflict'}]}))
5390         with mock.patch('keystoneauth1.adapter.Adapter.post',
5391                         autospec=True) as mock_post:
5392             mock_post.return_value = rsp
5393 
5394             post = {'revertResize': None}
5395             self.api.post_server_action(server['id'], post)
5396             server = self._wait_for_state_change(self.api, server, 'ERROR',)
5397 
5398         self.assertEqual(1, mock_post.call_count)
5399 
5400         migrations = self.api.get_migrations()
5401         self.assertEqual(1, len(migrations))
5402         self.assertEqual('resize', migrations[0]['migration_type'])
5403         self.assertEqual(server['id'], migrations[0]['instance_uuid'])
5404         self.assertEqual(hostname, migrations[0]['source_compute'])
5405         self.assertEqual('error', migrations[0]['status'])
5406 
5407         # NOTE(gibi): Nova leaks the allocation held by the migration_uuid even
5408         # after the instance is deleted. At least nova logs a fat ERROR.
5409         self.assertIn('Reverting allocation in placement for migration %s '
5410                       'failed. The instance %s will be put into ERROR state '
5411                       'but the allocation held by the migration is leaked.' %
5412                       (migrations[0]['uuid'], server['id']),
5413                       self.stdlog.logger.output)
5414         self.api.delete_server(server['id'])
5415         self._wait_until_deleted(server)
5416         fake_notifier.wait_for_versioned_notifications('instance.delete.end')
5417 
5418         allocations = self._get_allocations_by_server_uuid(
5419             migrations[0]['uuid'])
5420         self.assertEqual(1, len(allocations))
5421 
5422     def test_force_live_migrate_claim_on_dest_fails(self):
5423         # Normal live migrate moves source allocation from instance to
5424         # migration like a normal migrate tested above.
5425         # Normal live migrate claims on dest like a normal boot tested above.
5426         source_hostname = self.compute1.host
5427         dest_hostname = self.compute2.host
5428 
5429         # the ability to force live migrate a server is removed entirely in
5430         # 2.68
5431         self.api.microversion = '2.67'
5432 
5433         server = self._boot_and_check_allocations(
5434             self.flavor, source_hostname)
5435 
5436         rsp = fake_requests.FakeResponse(
5437             409,
5438             jsonutils.dumps(
5439                 {'errors': [
5440                     {'code': 'placement.concurrent_update',
5441                      'detail': 'consumer generation conflict'}]}))
5442         with mock.patch('keystoneauth1.adapter.Adapter.put',
5443                         autospec=True) as mock_put:
5444             mock_put.return_value = rsp
5445 
5446             post = {
5447                 'os-migrateLive': {
5448                     'host': dest_hostname,
5449                     'block_migration': True,
5450                     'force': True,
5451                 }
5452             }
5453 
5454             self.api.post_server_action(server['id'], post)
5455             server = self._wait_for_state_change(self.api, server, 'ERROR')
5456 
5457         self.assertEqual(1, mock_put.call_count)
5458 
5459         # This is not a conflict that the API user can ever resolve. It is a
5460         # serious inconsistency in our database or a bug in the scheduler code
5461         # doing the claim.
5462         self.assertEqual(500, server['fault']['code'])
5463         # The instance is in ERROR state so the allocations are in limbo but
5464         # at least we expect that when the instance is deleted the allocations
5465         # are cleaned up properly.
5466         self._delete_and_check_allocations(server)
5467 
5468     def test_live_migrate_drop_allocation_on_source_fails(self):
5469         source_hostname = self.compute1.host
5470         dest_hostname = self.compute2.host
5471         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
5472         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
5473 
5474         # the ability to force live migrate a server is removed entirely in
5475         # 2.68
5476         self.api.microversion = '2.67'
5477 
5478         server = self._boot_and_check_allocations(
5479             self.flavor, source_hostname)
5480 
5481         fake_notifier.stub_notifier(self)
5482         self.addCleanup(fake_notifier.reset)
5483 
5484         orig_put = adapter.Adapter.put
5485 
5486         rsp = fake_requests.FakeResponse(
5487             409,
5488             jsonutils.dumps(
5489                 {'errors': [
5490                     {'code': 'placement.concurrent_update',
5491                      'detail': 'consumer generation conflict'}]}))
5492 
5493         def fake_put(_self, url, *args, **kwargs):
5494             migration_uuid = self.get_migration_uuid_for_instance(server['id'])
5495             if url == '/allocations/%s' % migration_uuid:
5496                 return rsp
5497             else:
5498                 return orig_put(_self, url, *args, **kwargs)
5499 
5500         with mock.patch('keystoneauth1.adapter.Adapter.put',
5501                         autospec=True) as mock_put:
5502             mock_put.side_effect = fake_put
5503 
5504             post = {
5505                 'os-migrateLive': {
5506                     'host': dest_hostname,
5507                     'block_migration': True,
5508                     'force': True,
5509                 }
5510             }
5511 
5512             self.api.post_server_action(server['id'], post)
5513 
5514             # nova does the source host cleanup _after_ setting the migration
5515             # to completed and sending end notifications so we have to wait
5516             # here a bit.
5517             time.sleep(1)
5518 
5519             # Nova failed to clean up on the source host. This right now puts
5520             # the instance to ERROR state and fails the migration.
5521             server = self._wait_for_server_parameter(self.api, server,
5522                 {'OS-EXT-SRV-ATTR:host': dest_hostname,
5523                  'status': 'ERROR'})
5524             self._wait_for_migration_status(server, ['error'])
5525             fake_notifier.wait_for_versioned_notifications(
5526                 'instance.live_migration_post.end')
5527 
5528         # 1 claim on destination, 1 normal delete on dest that fails,
5529         self.assertEqual(2, mock_put.call_count)
5530 
5531         # As the cleanup on the source host failed Nova leaks the allocation
5532         # held by the migration.
5533         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor)
5534         migration_uuid = self.get_migration_uuid_for_instance(server['id'])
5535         self.assertFlavorMatchesAllocation(self.flavor, migration_uuid,
5536                                            source_rp_uuid)
5537 
5538         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor)
5539 
5540         self.assertFlavorMatchesAllocation(self.flavor, server['id'],
5541                                            dest_rp_uuid)
5542 
5543         # NOTE(gibi): Nova leaks the allocation held by the migration_uuid even
5544         # after the instance is deleted. At least nova logs a fat ERROR.
5545         self.assertIn('Deleting allocation in placement for migration %s '
5546                       'failed. The instance %s will be put to ERROR state but '
5547                       'the allocation held by the migration is leaked.' %
5548                       (migration_uuid, server['id']),
5549                       self.stdlog.logger.output)
5550 
5551         self.api.delete_server(server['id'])
5552         self._wait_until_deleted(server)
5553         fake_notifier.wait_for_versioned_notifications('instance.delete.end')
5554 
5555         self.assertFlavorMatchesAllocation(self.flavor, migration_uuid,
5556                                            source_rp_uuid)
5557 
5558     def _test_evacuate_fails_allocating_on_dest_host(self, force):
5559         source_hostname = self.compute1.host
5560         dest_hostname = self.compute2.host
5561 
5562         # the ability to force evacuate a server is removed entirely in 2.68
5563         self.api.microversion = '2.67'
5564 
5565         server = self._boot_and_check_allocations(
5566             self.flavor, source_hostname)
5567 
5568         source_compute_id = self.admin_api.get_services(
5569             host=source_hostname, binary='nova-compute')[0]['id']
5570 
5571         self.compute1.stop()
5572         # force it down to avoid waiting for the service group to time out
5573         self.admin_api.put_service(
5574             source_compute_id, {'forced_down': 'true'})
5575 
5576         rsp = fake_requests.FakeResponse(
5577             409,
5578             jsonutils.dumps(
5579                 {'errors': [
5580                     {'code': 'placement.concurrent_update',
5581                      'detail': 'consumer generation conflict'}]}))
5582 
5583         with mock.patch('keystoneauth1.adapter.Adapter.put',
5584                         autospec=True) as mock_put:
5585             mock_put.return_value = rsp
5586             post = {
5587                 'evacuate': {
5588                     'force': force
5589                 }
5590             }
5591             if force:
5592                 post['evacuate']['host'] = dest_hostname
5593 
5594             self.api.post_server_action(server['id'], post)
5595             server = self._wait_for_state_change(self.api, server, 'ERROR')
5596 
5597         self.assertEqual(1, mock_put.call_count)
5598 
5599         # As nova failed to allocate on the dest host we only expect allocation
5600         # on the source
5601         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
5602         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
5603 
5604         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor)
5605 
5606         self.assertRequestMatchesUsage({'VCPU': 0,
5607                                         'MEMORY_MB': 0,
5608                                         'DISK_GB': 0}, dest_rp_uuid)
5609 
5610         self.assertFlavorMatchesAllocation(self.flavor, server['id'],
5611                                            source_rp_uuid)
5612 
5613         self._delete_and_check_allocations(server)
5614 
5615     def test_force_evacuate_fails_allocating_on_dest_host(self):
5616         self._test_evacuate_fails_allocating_on_dest_host(force=True)
5617 
5618     def test_evacuate_fails_allocating_on_dest_host(self):
5619         self._test_evacuate_fails_allocating_on_dest_host(force=False)
5620 
5621     def test_server_delete_fails_due_to_conflict(self):
5622         source_hostname = self.compute1.host
5623 
5624         server = self._boot_and_check_allocations(self.flavor, source_hostname)
5625 
5626         rsp = fake_requests.FakeResponse(
5627             409, jsonutils.dumps({'text': 'consumer generation conflict'}))
5628 
5629         with mock.patch('keystoneauth1.adapter.Adapter.put',
5630                         autospec=True) as mock_put:
5631             mock_put.return_value = rsp
5632 
5633             self.api.delete_server(server['id'])
5634             server = self._wait_for_state_change(self.admin_api, server,
5635                                                  'ERROR')
5636             self.assertEqual(1, mock_put.call_count)
5637 
5638         # We still have the allocations as deletion failed
5639         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
5640         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor)
5641 
5642         self.assertFlavorMatchesAllocation(self.flavor, server['id'],
5643                                            source_rp_uuid)
5644 
5645         # retry the delete to make sure that allocations are removed this time
5646         self._delete_and_check_allocations(server)
5647 
5648     def test_server_local_delete_fails_due_to_conflict(self):
5649         source_hostname = self.compute1.host
5650 
5651         server = self._boot_and_check_allocations(self.flavor, source_hostname)
5652         source_compute_id = self.admin_api.get_services(
5653             host=self.compute1.host, binary='nova-compute')[0]['id']
5654         self.compute1.stop()
5655         self.admin_api.put_service(
5656             source_compute_id, {'forced_down': 'true'})
5657 
5658         rsp = fake_requests.FakeResponse(
5659             409, jsonutils.dumps({'text': 'consumer generation conflict'}))
5660 
5661         with mock.patch('keystoneauth1.adapter.Adapter.put',
5662                         autospec=True) as mock_put:
5663             mock_put.return_value = rsp
5664 
5665             ex = self.assertRaises(client.OpenStackApiException,
5666                                    self.api.delete_server, server['id'])
5667             self.assertEqual(409, ex.response.status_code)
5668             self.assertIn('Failed to delete allocations for consumer',
5669                           jsonutils.loads(ex.response.content)[
5670                               'conflictingRequest']['message'])
5671             self.assertEqual(1, mock_put.call_count)
5672 
5673         # We still have the allocations as deletion failed
5674         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
5675         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor)
5676 
5677         self.assertFlavorMatchesAllocation(self.flavor, server['id'],
5678                                            source_rp_uuid)
5679 
5680         # retry the delete to make sure that allocations are removed this time
5681         self._delete_and_check_allocations(server)
5682 
5683 
5684 class ServerMovingTestsWithNestedComputes(ServerMovingTests):
5685     """Runs all the server moving tests while the computes have nested trees.
5686     The servers still do not request resources from any child provider though.
5687     """
5688     compute_driver = 'fake.MediumFakeDriverWithNestedCustomResources'
5689 
5690 
5691 class ServerMovingTestsWithNestedResourceRequests(
5692     ServerMovingTestsWithNestedComputes):
5693     """Runs all the server moving tests while the computes have nested trees.
5694     The servers also request resources from child providers.
5695     """
5696 
5697     def setUp(self):
5698         super(ServerMovingTestsWithNestedResourceRequests, self).setUp()
5699         # modify the flavors used in the ServerMoving test base class to
5700         # require one piece of CUSTOM_MAGIC resource as well.
5701 
5702         for flavor in [self.flavor1, self.flavor2, self.flavor3]:
5703             self.api.post_extra_spec(
5704                 flavor['id'], {'extra_specs': {'resources:CUSTOM_MAGIC': 1}})
5705             # save the extra_specs in the flavor stored in the test case as
5706             # well
5707             flavor['extra_specs'] = {'resources:CUSTOM_MAGIC': 1}
5708 
5709     def _check_allocation_during_evacuate(
5710             self, flavor, server_uuid, source_root_rp_uuid, dest_root_rp_uuid):
5711         # NOTE(gibi): evacuate is the only case when the same consumer has
5712         # allocation from two different RP trees so we need a special check
5713         # here.
5714         allocations = self._get_allocations_by_server_uuid(server_uuid)
5715         source_rps = self._get_all_rp_uuids_in_a_tree(source_root_rp_uuid)
5716         dest_rps = self._get_all_rp_uuids_in_a_tree(dest_root_rp_uuid)
5717 
5718         self.assertEqual(set(source_rps + dest_rps), set(allocations))
5719 
5720         total_source_allocation = collections.defaultdict(int)
5721         total_dest_allocation = collections.defaultdict(int)
5722         for rp, alloc in allocations.items():
5723             for rc, value in alloc['resources'].items():
5724                 if rp in source_rps:
5725                     total_source_allocation[rc] += value
5726                 else:
5727                     total_dest_allocation[rc] += value
5728 
5729         self.assertEqual(
5730             self._resources_from_flavor(flavor), total_source_allocation)
5731         self.assertEqual(
5732             self._resources_from_flavor(flavor), total_dest_allocation)
5733 
5734     def test_live_migrate_force(self):
5735         # Nova intentionally does not support force live-migrating server
5736         # with nested allocations.
5737 
5738         source_hostname = self.compute1.host
5739         dest_hostname = self.compute2.host
5740         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
5741         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
5742 
5743         # the ability to force live migrate a server is removed entirely in
5744         # 2.68
5745         self.api.microversion = '2.67'
5746 
5747         server = self._boot_and_check_allocations(
5748             self.flavor1, source_hostname)
5749         post = {
5750             'os-migrateLive': {
5751                 'host': dest_hostname,
5752                 'block_migration': True,
5753                 'force': True,
5754             }
5755         }
5756 
5757         self.api.post_server_action(server['id'], post)
5758         self._wait_for_migration_status(server, ['error'])
5759         self._wait_for_server_parameter(self.api, server,
5760             {'OS-EXT-SRV-ATTR:host': source_hostname,
5761              'status': 'ACTIVE'})
5762         self.assertIn('Unable to move instance %s to host host2. The instance '
5763                       'has complex allocations on the source host so move '
5764                       'cannot be forced.' %
5765                       server['id'],
5766                       self.stdlog.logger.output)
5767 
5768         self._run_periodics()
5769 
5770         # NOTE(danms): There should be no usage for the dest
5771         self.assertRequestMatchesUsage(
5772             {'VCPU': 0,
5773              'MEMORY_MB': 0,
5774              'DISK_GB': 0}, dest_rp_uuid)
5775 
5776         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
5777 
5778         # the server has an allocation on only the source node
5779         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
5780                                            source_rp_uuid)
5781 
5782         self._delete_and_check_allocations(server)
5783 
5784     def test_evacuate_forced_host(self):
5785         # Nova intentionally does not support force evacuating server
5786         # with nested allocations.
5787 
5788         source_hostname = self.compute1.host
5789         dest_hostname = self.compute2.host
5790 
5791         # the ability to force evacuate a server is removed entirely in 2.68
5792         self.api.microversion = '2.67'
5793 
5794         server = self._boot_and_check_allocations(
5795             self.flavor1, source_hostname)
5796 
5797         source_compute_id = self.admin_api.get_services(
5798             host=source_hostname, binary='nova-compute')[0]['id']
5799 
5800         self.compute1.stop()
5801         # force it down to avoid waiting for the service group to time out
5802         self.admin_api.put_service(
5803             source_compute_id, {'forced_down': 'true'})
5804 
5805         # evacuate the server and force the destination host which bypasses
5806         # the scheduler
5807         post = {
5808             'evacuate': {
5809                 'host': dest_hostname,
5810                 'force': True
5811             }
5812         }
5813         self.api.post_server_action(server['id'], post)
5814         self._wait_for_migration_status(server, ['error'])
5815         expected_params = {'OS-EXT-SRV-ATTR:host': source_hostname,
5816                            'status': 'ACTIVE'}
5817         server = self._wait_for_server_parameter(self.api, server,
5818                                                  expected_params)
5819         self.assertIn('Unable to move instance %s to host host2. The instance '
5820                       'has complex allocations on the source host so move '
5821                       'cannot be forced.' %
5822                       server['id'],
5823                       self.stdlog.logger.output)
5824 
5825         # Run the periodics to show those don't modify allocations.
5826         self._run_periodics()
5827 
5828         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
5829         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
5830 
5831         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
5832 
5833         self.assertRequestMatchesUsage(
5834             {'VCPU': 0,
5835              'MEMORY_MB': 0,
5836              'DISK_GB': 0}, dest_rp_uuid)
5837 
5838         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
5839                                            source_rp_uuid)
5840 
5841         # restart the source compute
5842         self.restart_compute_service(self.compute1)
5843         self.admin_api.put_service(
5844             source_compute_id, {'forced_down': 'false'})
5845 
5846         # Run the periodics again to show they don't change anything.
5847         self._run_periodics()
5848 
5849         # When the source node starts up nothing should change as the
5850         # evacuation failed
5851         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
5852 
5853         self.assertRequestMatchesUsage(
5854             {'VCPU': 0,
5855              'MEMORY_MB': 0,
5856              'DISK_GB': 0}, dest_rp_uuid)
5857 
5858         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
5859                                            source_rp_uuid)
5860 
5861         self._delete_and_check_allocations(server)
5862 
5863 
5864 # NOTE(gibi): There is another case NestedToFlat but that leads to the same
5865 # code path that NestedToNested as in both cases the instance will have
5866 # complex allocation on the source host which is already covered in
5867 # ServerMovingTestsWithNestedResourceRequests
5868 class ServerMovingTestsFromFlatToNested(
5869         integrated_helpers.ProviderUsageBaseTestCase):
5870     """Tests trying to move servers from a compute with a flat RP tree to a
5871     compute with a nested RP tree and assert that the blind allocation copy
5872     fails cleanly.
5873     """
5874 
5875     REQUIRES_LOCKING = True
5876     compute_driver = 'fake.MediumFakeDriver'
5877 
5878     def setUp(self):
5879         super(ServerMovingTestsFromFlatToNested, self).setUp()
5880         flavors = self.api.get_flavors()
5881         self.flavor1 = flavors[0]
5882         self.api.post_extra_spec(
5883             self.flavor1['id'], {'extra_specs': {'resources:CUSTOM_MAGIC': 1}})
5884         self.flavor1['extra_specs'] = {'resources:CUSTOM_MAGIC': 1}
5885 
5886     def test_force_live_migrate_from_flat_to_nested(self):
5887         # first compute will start with the flat RP tree but we add
5888         # CUSTOM_MAGIC inventory to the root compute RP
5889         orig_update_provider_tree = fake.MediumFakeDriver.update_provider_tree
5890 
5891         # the ability to force live migrate a server is removed entirely in
5892         # 2.68
5893         self.api.microversion = '2.67'
5894 
5895         def stub_update_provider_tree(self, provider_tree, nodename,
5896                                       allocations=None):
5897             # do the regular inventory update
5898             orig_update_provider_tree(
5899                 self, provider_tree, nodename, allocations)
5900             if nodename == 'host1':
5901                 # add the extra resource
5902                 inv = provider_tree.data(nodename).inventory
5903                 inv['CUSTOM_MAGIC'] = {
5904                     'total': 10,
5905                     'reserved': 0,
5906                     'min_unit': 1,
5907                     'max_unit': 10,
5908                     'step_size': 1,
5909                     'allocation_ratio': 1,
5910                 }
5911                 provider_tree.update_inventory(nodename, inv)
5912 
5913         self.stub_out('nova.virt.fake.FakeDriver.update_provider_tree',
5914                       stub_update_provider_tree)
5915         self.compute1 = self._start_compute(host='host1')
5916         source_rp_uuid = self._get_provider_uuid_by_host('host1')
5917 
5918         server = self._boot_and_check_allocations(self.flavor1, 'host1')
5919         # start the second compute with nested RP tree
5920         self.flags(
5921             compute_driver='fake.MediumFakeDriverWithNestedCustomResources')
5922         self.compute2 = self._start_compute(host='host2')
5923 
5924         # try to force live migrate from flat to nested.
5925         post = {
5926             'os-migrateLive': {
5927                 'host': 'host2',
5928                 'block_migration': True,
5929                 'force': True,
5930             }
5931         }
5932 
5933         self.api.post_server_action(server['id'], post)
5934         # We expect that the migration will fail as force migrate tries to
5935         # blindly copy the source allocation to the destination but on the
5936         # destination there is no inventory of CUSTOM_MAGIC on the compute node
5937         # provider as that resource is reported on a child provider.
5938         self._wait_for_server_parameter(self.api, server,
5939             {'OS-EXT-SRV-ATTR:host': 'host1',
5940              'status': 'ACTIVE'})
5941 
5942         migration = self._wait_for_migration_status(server, ['error'])
5943         self.assertEqual('host1', migration['source_compute'])
5944         self.assertEqual('host2', migration['dest_compute'])
5945 
5946         # Nova fails the migration because it ties to allocation CUSTOM_MAGIC
5947         # from the dest node root RP and placement rejects the that allocation.
5948         self.assertIn("Unable to allocate inventory: Inventory for "
5949                       "'CUSTOM_MAGIC'", self.stdlog.logger.output)
5950         self.assertIn('No valid host was found. Unable to move instance %s to '
5951                       'host host2. There is not enough capacity on the host '
5952                       'for the instance.' % server['id'],
5953                       self.stdlog.logger.output)
5954 
5955         dest_rp_uuid = self._get_provider_uuid_by_host('host2')
5956 
5957         # There should be no usage for the dest
5958         self.assertRequestMatchesUsage(
5959             {'VCPU': 0,
5960              'MEMORY_MB': 0,
5961              'DISK_GB': 0}, dest_rp_uuid)
5962 
5963         # and everything stays at the source
5964         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
5965         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
5966                                            source_rp_uuid)
5967 
5968         self._delete_and_check_allocations(server)
5969 
5970     def test_force_evacuate_from_flat_to_nested(self):
5971         # first compute will start with the flat RP tree but we add
5972         # CUSTOM_MAGIC inventory to the root compute RP
5973         orig_update_provider_tree = fake.MediumFakeDriver.update_provider_tree
5974 
5975         # the ability to force evacuate a server is removed entirely in 2.68
5976         self.api.microversion = '2.67'
5977 
5978         def stub_update_provider_tree(self, provider_tree, nodename,
5979                                       allocations=None):
5980             # do the regular inventory update
5981             orig_update_provider_tree(
5982                 self, provider_tree, nodename, allocations)
5983             if nodename == 'host1':
5984                 # add the extra resource
5985                 inv = provider_tree.data(nodename).inventory
5986                 inv['CUSTOM_MAGIC'] = {
5987                     'total': 10,
5988                     'reserved': 0,
5989                     'min_unit': 1,
5990                     'max_unit': 10,
5991                     'step_size': 1,
5992                     'allocation_ratio': 1,
5993                 }
5994                 provider_tree.update_inventory(nodename, inv)
5995 
5996         self.stub_out('nova.virt.fake.FakeDriver.update_provider_tree',
5997                       stub_update_provider_tree)
5998         self.compute1 = self._start_compute(host='host1')
5999         source_rp_uuid = self._get_provider_uuid_by_host('host1')
6000 
6001         server = self._boot_and_check_allocations(self.flavor1, 'host1')
6002         # start the second compute with nested RP tree
6003         self.flags(
6004             compute_driver='fake.MediumFakeDriverWithNestedCustomResources')
6005         self.compute2 = self._start_compute(host='host2')
6006 
6007         source_compute_id = self.admin_api.get_services(
6008             host='host1', binary='nova-compute')[0]['id']
6009         self.compute1.stop()
6010         # force it down to avoid waiting for the service group to time out
6011         self.admin_api.put_service(
6012             source_compute_id, {'forced_down': 'true'})
6013 
6014         # try to force evacuate from flat to nested.
6015         post = {
6016             'evacuate': {
6017                 'host': 'host2',
6018                 'force': True,
6019             }
6020         }
6021 
6022         self.api.post_server_action(server['id'], post)
6023         # We expect that the evacuation will fail as force evacuate tries to
6024         # blindly copy the source allocation to the destination but on the
6025         # destination there is no inventory of CUSTOM_MAGIC on the compute node
6026         # provider as that resource is reported on a child provider.
6027         self._wait_for_server_parameter(self.api, server,
6028             {'OS-EXT-SRV-ATTR:host': 'host1',
6029              'status': 'ACTIVE'})
6030 
6031         migration = self._wait_for_migration_status(server, ['error'])
6032         self.assertEqual('host1', migration['source_compute'])
6033         self.assertEqual('host2', migration['dest_compute'])
6034 
6035         # Nova fails the migration because it ties to allocation CUSTOM_MAGIC
6036         # from the dest node root RP and placement rejects the that allocation.
6037         self.assertIn("Unable to allocate inventory: Inventory for "
6038                       "'CUSTOM_MAGIC'", self.stdlog.logger.output)
6039         self.assertIn('No valid host was found. Unable to move instance %s to '
6040                       'host host2. There is not enough capacity on the host '
6041                       'for the instance.' % server['id'],
6042                       self.stdlog.logger.output)
6043 
6044         dest_rp_uuid = self._get_provider_uuid_by_host('host2')
6045 
6046         # There should be no usage for the dest
6047         self.assertRequestMatchesUsage(
6048             {'VCPU': 0,
6049              'MEMORY_MB': 0,
6050              'DISK_GB': 0}, dest_rp_uuid)
6051 
6052         # and everything stays at the source
6053         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
6054         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
6055                                            source_rp_uuid)
6056 
6057         self._delete_and_check_allocations(server)
6058 
6059 
6060 class PortResourceRequestBasedSchedulingTestBase(
6061         integrated_helpers.ProviderUsageBaseTestCase):
6062 
6063     compute_driver = 'fake.FakeDriverWithPciResources'
6064 
6065     CUSTOM_VNIC_TYPE_NORMAL = 'CUSTOM_VNIC_TYPE_NORMAL'
6066     CUSTOM_VNIC_TYPE_DIRECT = 'CUSTOM_VNIC_TYPE_DIRECT'
6067     CUSTOM_PHYSNET1 = 'CUSTOM_PHYSNET1'
6068     CUSTOM_PHYSNET2 = 'CUSTOM_PHYSNET2'
6069     CUSTOM_PHYSNET3 = 'CUSTOM_PHYSNET3'
6070 
6071     def setUp(self):
6072         # enable PciPassthroughFilter to support SRIOV before the base class
6073         # starts the scheduler
6074         if 'PciPassthroughFilter' not in CONF.filter_scheduler.enabled_filters:
6075             self.flags(
6076                 enabled_filters=CONF.filter_scheduler.enabled_filters
6077                                 + ['PciPassthroughFilter'],
6078                 group='filter_scheduler')
6079 
6080         self.useFixture(
6081             fake.FakeDriverWithPciResources.
6082                 FakeDriverWithPciResourcesConfigFixture())
6083 
6084         super(PortResourceRequestBasedSchedulingTestBase, self).setUp()
6085         self.compute1 = self._start_compute('host1')
6086         self.compute1_rp_uuid = self._get_provider_uuid_by_host('host1')
6087         self.ovs_bridge_rp_per_host = {}
6088         self.flavor = self.api.get_flavors()[0]
6089         self.flavor_with_group_policy = self.api.get_flavors()[1]
6090 
6091         # Setting group policy for placement. This is mandatory when more than
6092         # one request group is included in the allocation candidate request and
6093         # we have tests with two ports both having resource request modelled as
6094         # two separate request groups.
6095         self.admin_api.post_extra_spec(
6096             self.flavor_with_group_policy['id'],
6097             {'extra_specs': {'group_policy': 'isolate'}})
6098 
6099         self._create_networking_rp_tree(self.compute1_rp_uuid)
6100 
6101         # add extra ports and the related network to the neutron fixture
6102         # specifically for these tests. It cannot be added globally in the
6103         # fixture init as it adds a second network that makes auto allocation
6104         # based test to fail due to ambiguous networks.
6105         self.neutron._ports[
6106             self.neutron.port_with_sriov_resource_request['id']] = \
6107             copy.deepcopy(self.neutron.port_with_sriov_resource_request)
6108         self.neutron._ports[self.neutron.sriov_port['id']] = \
6109             copy.deepcopy(self.neutron.sriov_port)
6110         self.neutron._networks[
6111             self.neutron.network_2['id']] = self.neutron.network_2
6112         self.neutron._subnets[
6113             self.neutron.subnet_2['id']] = self.neutron.subnet_2
6114 
6115     def _create_server(self, flavor, networks):
6116         server_req = self._build_minimal_create_server_request(
6117             self.api, 'bandwidth-aware-server',
6118             image_uuid='76fa36fc-c930-4bf3-8c8a-ea2a2420deb6',
6119             flavor_id=flavor['id'], networks=networks)
6120         return self.api.post_server({'server': server_req})
6121 
6122     def _set_provider_inventories(self, rp_uuid, inventories):
6123         rp = self.placement_api.get(
6124             '/resource_providers/%s' % rp_uuid).body
6125         inventories['resource_provider_generation'] = rp['generation']
6126         return self._update_inventory(rp_uuid, inventories)
6127 
6128     def _create_ovs_networking_rp_tree(self, compute_rp_uuid):
6129         # we need uuid sentinel for the test to make pep8 happy but we need a
6130         # unique one per compute so here is some ugliness
6131         ovs_agent_rp_uuid = getattr(uuids, compute_rp_uuid + 'ovs agent')
6132         agent_rp_req = {
6133             "name": ovs_agent_rp_uuid,
6134             "uuid": ovs_agent_rp_uuid,
6135             "parent_provider_uuid": compute_rp_uuid
6136         }
6137         self.placement_api.post('/resource_providers',
6138                                 body=agent_rp_req,
6139                                 version='1.20')
6140         ovs_bridge_rp_uuid = getattr(uuids, ovs_agent_rp_uuid + 'ovs br')
6141         ovs_bridge_req = {
6142             "name": ovs_bridge_rp_uuid,
6143             "uuid": ovs_bridge_rp_uuid,
6144             "parent_provider_uuid": ovs_agent_rp_uuid
6145         }
6146         self.placement_api.post('/resource_providers',
6147                                 body=ovs_bridge_req,
6148                                 version='1.20')
6149         self.ovs_bridge_rp_per_host[compute_rp_uuid] = ovs_bridge_rp_uuid
6150 
6151         self._set_provider_inventories(
6152             ovs_bridge_rp_uuid,
6153             {"inventories": {
6154                 orc.NET_BW_IGR_KILOBIT_PER_SEC: {"total": 10000},
6155                 orc.NET_BW_EGR_KILOBIT_PER_SEC: {"total": 10000},
6156             }})
6157 
6158         self._create_trait(self.CUSTOM_VNIC_TYPE_NORMAL)
6159         self._create_trait(self.CUSTOM_PHYSNET2)
6160 
6161         self._set_provider_traits(
6162             ovs_bridge_rp_uuid,
6163             [self.CUSTOM_VNIC_TYPE_NORMAL, self.CUSTOM_PHYSNET2])
6164 
6165     def _create_pf_device_rp(
6166             self, device_rp_uuid, parent_rp_uuid, inventories, traits,
6167             device_rp_name=None):
6168         """Create a RP in placement for a physical function network device with
6169         traits and inventories.
6170         """
6171 
6172         if not device_rp_name:
6173             device_rp_name = device_rp_uuid
6174 
6175         sriov_pf_req = {
6176             "name": device_rp_name,
6177             "uuid": device_rp_uuid,
6178             "parent_provider_uuid": parent_rp_uuid
6179         }
6180         self.placement_api.post('/resource_providers',
6181                                 body=sriov_pf_req,
6182                                 version='1.20')
6183 
6184         self._set_provider_inventories(
6185             device_rp_uuid,
6186             {"inventories": inventories})
6187 
6188         for trait in traits:
6189             self._create_trait(trait)
6190 
6191         self._set_provider_traits(
6192             device_rp_uuid,
6193             traits)
6194 
6195     def _create_sriov_networking_rp_tree(self, compute_rp_uuid):
6196         # Create a matching RP tree in placement for the PCI devices added to
6197         # the passthrough_whitelist config during setUp() and PCI devices
6198         # present in the FakeDriverWithPciResources virt driver.
6199         #
6200         # * PF1 represents the PCI device 0000:01:00, it will be mapped to
6201         # physnet1 and it will have bandwidth inventory.
6202         # * PF2 represents the PCI device 0000:02:00, it will be mapped to
6203         # physnet2 it will have bandwidth inventory.
6204         # * PF3 represents the PCI device 0000:03:00 and, it will be mapped to
6205         # physnet2 but it will not have bandwidth inventory.
6206 
6207         compute_name = compute_rp_uuid
6208         sriov_agent_rp_uuid = getattr(uuids, compute_rp_uuid + 'sriov agent')
6209         agent_rp_req = {
6210             "name": "%s:NIC Switch agent" % compute_name,
6211             "uuid": sriov_agent_rp_uuid,
6212             "parent_provider_uuid": compute_rp_uuid
6213         }
6214         self.placement_api.post('/resource_providers',
6215                                 body=agent_rp_req,
6216                                 version='1.20')
6217 
6218         self.sriov_pf1_rp_uuid = getattr(uuids, sriov_agent_rp_uuid + 'PF1')
6219         inventories = {
6220             orc.NET_BW_IGR_KILOBIT_PER_SEC: {"total": 100000},
6221             orc.NET_BW_EGR_KILOBIT_PER_SEC: {"total": 100000},
6222         }
6223         traits = [self.CUSTOM_VNIC_TYPE_DIRECT, self.CUSTOM_PHYSNET1]
6224         self._create_pf_device_rp(
6225             self.sriov_pf1_rp_uuid, sriov_agent_rp_uuid, inventories, traits,
6226             device_rp_name="%s:NIC Switch agent:ens1" % compute_name)
6227 
6228         self.sriov_pf2_rp_uuid = getattr(uuids, sriov_agent_rp_uuid + 'PF2')
6229         inventories = {
6230             orc.NET_BW_IGR_KILOBIT_PER_SEC: {"total": 100000},
6231             orc.NET_BW_EGR_KILOBIT_PER_SEC: {"total": 100000},
6232         }
6233         traits = [self.CUSTOM_VNIC_TYPE_DIRECT, self.CUSTOM_PHYSNET2]
6234         self._create_pf_device_rp(
6235             self.sriov_pf2_rp_uuid, sriov_agent_rp_uuid, inventories, traits,
6236             device_rp_name="%s:NIC Switch agent:ens2" % compute_name)
6237 
6238         self.sriov_pf3_rp_uuid = getattr(uuids, sriov_agent_rp_uuid + 'PF3')
6239         inventories = {}
6240         traits = [self.CUSTOM_VNIC_TYPE_DIRECT, self.CUSTOM_PHYSNET2]
6241         self._create_pf_device_rp(
6242             self.sriov_pf3_rp_uuid, sriov_agent_rp_uuid, inventories, traits,
6243             device_rp_name="%s:NIC Switch agent:ens3" % compute_name)
6244 
6245     def _create_networking_rp_tree(self, compute_rp_uuid):
6246         # let's simulate what the neutron would do
6247         self._create_ovs_networking_rp_tree(compute_rp_uuid)
6248         self._create_sriov_networking_rp_tree(compute_rp_uuid)
6249 
6250     def assertPortMatchesAllocation(self, port, allocations):
6251         port_request = port['resource_request']['resources']
6252         for rc, amount in allocations.items():
6253             self.assertEqual(port_request[rc], amount,
6254                              'port %s requested %d %s '
6255                              'resources but got allocation %d' %
6256                              (port['id'], port_request[rc], rc,
6257                               amount))
6258 
6259 
6260 class UnsupportedPortResourceRequestBasedSchedulingTest(
6261         PortResourceRequestBasedSchedulingTestBase):
6262     """Tests for handling servers with ports having resource requests """
6263 
6264     def _add_resource_request_to_a_bound_port(self, port_id):
6265         # NOTE(gibi): self.neutron._ports contains a copy of each neutron port
6266         # defined on class level in the fixture. So modifying what is in the
6267         # _ports list is safe as it is re-created for each Neutron fixture
6268         # instance therefore for each individual test using that fixture.
6269         bound_port = self.neutron._ports[port_id]
6270         bound_port['resource_request'] = (
6271             self.neutron.port_with_resource_request['resource_request'])
6272 
6273     def test_interface_attach_with_port_resource_request(self):
6274         # create a server
6275         server = self._create_server(
6276             flavor=self.flavor,
6277             networks=[{'port': self.neutron.port_1['id']}])
6278         self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6279 
6280         # try to add a port with resource request
6281         post = {
6282             'interfaceAttachment': {
6283                 'port_id': self.neutron.port_with_resource_request['id']
6284         }}
6285         ex = self.assertRaises(client.OpenStackApiException,
6286                                self.api.attach_interface,
6287                                server['id'], post)
6288         self.assertEqual(400, ex.response.status_code)
6289         self.assertIn('Attaching interfaces with QoS policy is '
6290                       'not supported for instance',
6291                       six.text_type(ex))
6292 
6293     @mock.patch('nova.tests.fixtures.NeutronFixture.create_port')
6294     def test_interface_attach_with_network_create_port_has_resource_request(
6295             self, mock_neutron_create_port):
6296         # create a server
6297         server = self._create_server(
6298             flavor=self.flavor,
6299             networks=[{'port': self.neutron.port_1['id']}])
6300         self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6301 
6302         # the interfaceAttach operation below will result in a new port being
6303         # created in the network that is attached. Make sure that neutron
6304         # returns a port that has resource request.
6305         mock_neutron_create_port.return_value = (
6306             {'port': copy.deepcopy(self.neutron.port_with_resource_request)})
6307 
6308         # try to attach a network
6309         post = {
6310             'interfaceAttachment': {
6311                 'net_id': self.neutron.network_1['id']
6312         }}
6313         ex = self.assertRaises(client.OpenStackApiException,
6314                                self.api.attach_interface,
6315                                server['id'], post)
6316         self.assertEqual(400, ex.response.status_code)
6317         self.assertIn('Using networks with QoS policy is not supported for '
6318                       'instance',
6319                       six.text_type(ex))
6320 
6321     @mock.patch('nova.tests.fixtures.NeutronFixture.create_port')
6322     def test_create_server_with_network_create_port_has_resource_request(
6323             self, mock_neutron_create_port):
6324         # the server create operation below will result in a new port being
6325         # created in the network. Make sure that neutron returns a port that
6326         # has resource request.
6327         mock_neutron_create_port.return_value = (
6328             {'port': copy.deepcopy(self.neutron.port_with_resource_request)})
6329 
6330         server = self._create_server(
6331             flavor=self.flavor,
6332             networks=[{'uuid': self.neutron.network_1['id']}])
6333         server = self._wait_for_state_change(self.admin_api, server, 'ERROR')
6334 
6335         self.assertEqual(500, server['fault']['code'])
6336         self.assertIn('Failed to allocate the network',
6337                       server['fault']['message'])
6338 
6339     def test_create_server_with_port_resource_request_old_microversion(self):
6340 
6341         # NOTE(gibi): 2.71 is the last microversion where nova does not support
6342         # this kind of create server
6343         self.api.microversion = '2.71'
6344         ex = self.assertRaises(
6345             client.OpenStackApiException, self._create_server,
6346             flavor=self.flavor,
6347             networks=[{'port': self.neutron.port_with_resource_request['id']}])
6348 
6349         self.assertEqual(400, ex.response.status_code)
6350         self.assertIn(
6351             "Creating servers with ports having resource requests, like a "
6352             "port with a QoS minimum bandwidth policy, is not supported "
6353             "until microversion 2.72.",
6354             six.text_type(ex))
6355 
6356     def test_resize_server_with_port_resource_request_old_microversion(self):
6357         server = self._create_server(
6358             flavor=self.flavor,
6359             networks=[{'port': self.neutron.port_1['id']}])
6360         self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6361 
6362         # We need to simulate that the above server has a port that has
6363         # resource request; we cannot boot with such a port but legacy servers
6364         # can exist with such a port.
6365         self._add_resource_request_to_a_bound_port(self.neutron.port_1['id'])
6366 
6367         resize_req = {
6368             'resize': {
6369                 'flavorRef': self.flavor['id']
6370             }
6371         }
6372         ex = self.assertRaises(
6373             client.OpenStackApiException,
6374             self.api.post_server_action, server['id'], resize_req)
6375 
6376         self.assertEqual(400, ex.response.status_code)
6377         self.assertIn(
6378             'The resize action on a server with ports having resource '
6379             'requests', six.text_type(ex))
6380 
6381     def test_migrate_server_with_port_resource_request_old_microversion(self):
6382         server = self._create_server(
6383             flavor=self.flavor,
6384             networks=[{'port': self.neutron.port_1['id']}])
6385         self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6386 
6387         # We need to simulate that the above server has a port that has
6388         # resource request; we cannot boot with such a port but legacy servers
6389         # can exist with such a port.
6390         self._add_resource_request_to_a_bound_port(self.neutron.port_1['id'])
6391 
6392         ex = self.assertRaises(
6393             client.OpenStackApiException,
6394             self.api.post_server_action, server['id'], {'migrate': None})
6395 
6396         self.assertEqual(400, ex.response.status_code)
6397         self.assertIn(
6398             'The migrate action on a server with ports having resource '
6399             'requests', six.text_type(ex))
6400 
6401     def test_live_migrate_server_with_port_resource_request_old_microversion(
6402             self):
6403         server = self._create_server(
6404             flavor=self.flavor,
6405             networks=[{'port': self.neutron.port_1['id']}])
6406         self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6407 
6408         # We need to simulate that the above server has a port that has
6409         # resource request; we cannot boot with such a port but legacy servers
6410         # can exist with such a port.
6411         self._add_resource_request_to_a_bound_port(self.neutron.port_1['id'])
6412 
6413         post = {
6414             'os-migrateLive': {
6415                 'host': None,
6416                 'block_migration': False,
6417             }
6418         }
6419         ex = self.assertRaises(
6420             client.OpenStackApiException,
6421             self.api.post_server_action, server['id'], post)
6422 
6423         self.assertEqual(400, ex.response.status_code)
6424         self.assertIn(
6425             'The os-migrateLive action on a server with ports having resource '
6426             'requests', six.text_type(ex))
6427 
6428     def test_evacuate_server_with_port_resource_request_old_microversion(
6429             self):
6430         server = self._create_server(
6431             flavor=self.flavor,
6432             networks=[{'port': self.neutron.port_1['id']}])
6433         self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6434 
6435         # We need to simulate that the above server has a port that has
6436         # resource request; we cannot boot with such a port but legacy servers
6437         # can exist with such a port.
6438         self._add_resource_request_to_a_bound_port(self.neutron.port_1['id'])
6439 
6440         ex = self.assertRaises(
6441             client.OpenStackApiException,
6442             self.api.post_server_action, server['id'], {'evacuate': {}})
6443 
6444         self.assertEqual(400, ex.response.status_code)
6445         self.assertIn(
6446             'The evacuate action on a server with ports having resource '
6447             'requests', six.text_type(ex))
6448 
6449     def test_unshelve_offloaded_server_with_port_resource_request_old_version(
6450             self):
6451         server = self._create_server(
6452             flavor=self.flavor,
6453             networks=[{'port': self.neutron.port_1['id']}])
6454         self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6455 
6456         # with default config shelve means immediate offload as well
6457         req = {
6458             'shelve': {}
6459         }
6460         self.api.post_server_action(server['id'], req)
6461         self._wait_for_server_parameter(
6462             self.api, server, {'status': 'SHELVED_OFFLOADED'})
6463 
6464         # We need to simulate that the above server has a port that has
6465         # resource request; we cannot boot with such a port but legacy servers
6466         # can exist with such a port.
6467         self._add_resource_request_to_a_bound_port(self.neutron.port_1['id'])
6468 
6469         ex = self.assertRaises(
6470             client.OpenStackApiException,
6471             self.api.post_server_action, server['id'], {'unshelve': {}})
6472 
6473         self.assertEqual(400, ex.response.status_code)
6474         self.assertIn(
6475             'The unshelve action on a server with ports having resource '
6476             'requests', six.text_type(ex))
6477 
6478     def test_unshelve_not_offloaded_server_with_port_resource_request(
6479             self):
6480         """If the server is not offloaded then unshelving does not cause a new
6481         resource allocation therefore having port resource request is
6482         irrelevant. This test asserts that such unshelve request is not
6483         rejected.
6484         """
6485         server = self._create_server(
6486             flavor=self.flavor,
6487             networks=[{'port': self.neutron.port_1['id']}])
6488         self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6489 
6490         # avoid automatic shelve offloading
6491         self.flags(shelved_offload_time=-1)
6492         req = {
6493             'shelve': {}
6494         }
6495         self.api.post_server_action(server['id'], req)
6496         self._wait_for_server_parameter(
6497             self.api, server, {'status': 'SHELVED'})
6498 
6499         # We need to simulate that the above server has a port that has
6500         # resource request; we cannot boot with such a port but legacy servers
6501         # can exist with such a port.
6502         self._add_resource_request_to_a_bound_port(self.neutron.port_1['id'])
6503 
6504         self.api.post_server_action(server['id'], {'unshelve': {}})
6505         self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6506 
6507 
6508 class PortResourceRequestBasedSchedulingTest(
6509         PortResourceRequestBasedSchedulingTestBase):
6510     """Tests creating a server with a pre-existing port that has a resource
6511     request for a QoS minimum bandwidth policy.
6512     """
6513 
6514     def test_boot_server_with_two_ports_one_having_resource_request(self):
6515         non_qos_port = self.neutron.port_1
6516         qos_port = self.neutron.port_with_resource_request
6517 
6518         server = self._create_server(
6519             flavor=self.flavor,
6520             networks=[{'port': non_qos_port['id']},
6521                       {'port': qos_port['id']}])
6522         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6523         updated_non_qos_port = self.neutron.show_port(
6524             non_qos_port['id'])['port']
6525         updated_qos_port = self.neutron.show_port(qos_port['id'])['port']
6526 
6527         allocations = self.placement_api.get(
6528             '/allocations/%s' % server['id']).body['allocations']
6529 
6530         # We expect one set of allocations for the compute resources on the
6531         # compute rp and one set for the networking resources on the ovs bridge
6532         # rp due to the qos_port resource request
6533         self.assertEqual(2, len(allocations))
6534         compute_allocations = allocations[self.compute1_rp_uuid]['resources']
6535         network_allocations = allocations[
6536             self.ovs_bridge_rp_per_host[self.compute1_rp_uuid]]['resources']
6537 
6538         self.assertEqual(self._resources_from_flavor(self.flavor),
6539                          compute_allocations)
6540         self.assertPortMatchesAllocation(qos_port, network_allocations)
6541 
6542         # We expect that only the RP uuid of the networking RP having the port
6543         # allocation is sent in the port binding for the port having resource
6544         # request
6545         qos_binding_profile = updated_qos_port['binding:profile']
6546         self.assertEqual(self.ovs_bridge_rp_per_host[self.compute1_rp_uuid],
6547                          qos_binding_profile['allocation'])
6548 
6549         # And we expect not to have any allocation set in the port binding for
6550         # the port that doesn't have resource request
6551         self.assertNotIn('binding:profile', updated_non_qos_port)
6552 
6553         self._delete_and_check_allocations(server)
6554 
6555         # assert that unbind removes the allocation from the binding of the
6556         # port that got allocation during the bind
6557         updated_qos_port = self.neutron.show_port(qos_port['id'])['port']
6558         binding_profile = updated_qos_port['binding:profile']
6559         self.assertNotIn('allocation', binding_profile)
6560 
6561     def test_one_ovs_one_sriov_port(self):
6562         ovs_port = self.neutron.port_with_resource_request
6563         sriov_port = self.neutron.port_with_sriov_resource_request
6564 
6565         server = self._create_server(flavor=self.flavor_with_group_policy,
6566                                      networks=[{'port': ovs_port['id']},
6567                                                {'port': sriov_port['id']}])
6568 
6569         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6570 
6571         ovs_port = self.neutron.show_port(ovs_port['id'])['port']
6572         sriov_port = self.neutron.show_port(sriov_port['id'])['port']
6573 
6574         allocations = self.placement_api.get(
6575             '/allocations/%s' % server['id']).body['allocations']
6576 
6577         # We expect one set of allocations for the compute resources on the
6578         # compute rp and one set for the networking resources on the ovs bridge
6579         # rp and on the sriov PF rp.
6580         self.assertEqual(3, len(allocations))
6581         compute_allocations = allocations[self.compute1_rp_uuid]['resources']
6582         ovs_allocations = allocations[
6583             self.ovs_bridge_rp_per_host[self.compute1_rp_uuid]]['resources']
6584         sriov_allocations = allocations[self.sriov_pf2_rp_uuid]['resources']
6585 
6586         self.assertEqual(
6587             self._resources_from_flavor(self.flavor_with_group_policy),
6588             compute_allocations)
6589 
6590         self.assertPortMatchesAllocation(ovs_port, ovs_allocations)
6591         self.assertPortMatchesAllocation(sriov_port, sriov_allocations)
6592 
6593         # We expect that only the RP uuid of the networking RP having the port
6594         # allocation is sent in the port binding for the port having resource
6595         # request
6596         ovs_binding = ovs_port['binding:profile']
6597         self.assertEqual(self.ovs_bridge_rp_per_host[self.compute1_rp_uuid],
6598                          ovs_binding['allocation'])
6599         sriov_binding = sriov_port['binding:profile']
6600         self.assertEqual(self.sriov_pf2_rp_uuid,
6601                          sriov_binding['allocation'])
6602 
6603     def test_interface_detach_with_port_with_bandwidth_request(self):
6604         port = self.neutron.port_with_resource_request
6605 
6606         # create a server
6607         server = self._create_server(
6608             flavor=self.flavor,
6609             networks=[{'port': port['id']}])
6610         self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6611 
6612         allocations = self.placement_api.get(
6613             '/allocations/%s' % server['id']).body['allocations']
6614         # We expect one set of allocations for the compute resources on the
6615         # compute rp and one set for the networking resources on the ovs bridge
6616         # rp due to the port resource request
6617         self.assertEqual(2, len(allocations))
6618         compute_allocations = allocations[self.compute1_rp_uuid]['resources']
6619         network_allocations = allocations[
6620             self.ovs_bridge_rp_per_host[self.compute1_rp_uuid]]['resources']
6621 
6622         self.assertEqual(self._resources_from_flavor(self.flavor),
6623                          compute_allocations)
6624         self.assertPortMatchesAllocation(port, network_allocations)
6625 
6626         # We expect that only the RP uuid of the networking RP having the port
6627         # allocation is sent in the port binding for the port having resource
6628         # request
6629         updated_port = self.neutron.show_port(port['id'])['port']
6630         binding_profile = updated_port['binding:profile']
6631         self.assertEqual(self.ovs_bridge_rp_per_host[self.compute1_rp_uuid],
6632                          binding_profile['allocation'])
6633 
6634         self.api.detach_interface(
6635             server['id'], self.neutron.port_with_resource_request['id'])
6636 
6637         fake_notifier.wait_for_versioned_notifications(
6638             'instance.interface_detach.end')
6639 
6640         updated_port = self.neutron.show_port(
6641             self.neutron.port_with_resource_request['id'])['port']
6642 
6643         allocations = self.placement_api.get(
6644             '/allocations/%s' % server['id']).body['allocations']
6645 
6646         # We expect that the port related resource allocations are removed
6647         self.assertEqual(1, len(allocations))
6648         compute_allocations = allocations[self.compute1_rp_uuid]['resources']
6649         self.assertEqual(self._resources_from_flavor(self.flavor),
6650                          compute_allocations)
6651 
6652         # We expect that the allocation is removed from the port too
6653         binding_profile = updated_port['binding:profile']
6654         self.assertNotIn('allocation', binding_profile)
6655 
6656     def test_delete_bound_port_in_neutron_with_resource_request(self):
6657         """Neutron sends a network-vif-deleted os-server-external notification
6658         to nova when a bound port is deleted. Nova detach the vif from the
6659         server. If the port had resource allocation then that allocation is
6660         leaked. This test makes sure that 1) an ERROR is logged when the leak
6661         happens. 2) the leaked resource is reclaimed when the server is
6662         deleted.
6663         """
6664         port = self.neutron.port_with_resource_request
6665 
6666         # create a server
6667         server = self._create_server(
6668             flavor=self.flavor,
6669             networks=[{'port': port['id']}])
6670         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6671 
6672         allocations = self.placement_api.get(
6673             '/allocations/%s' % server['id']).body['allocations']
6674         # We expect one set of allocations for the compute resources on the
6675         # compute rp and one set for the networking resources on the ovs bridge
6676         # rp due to the port resource request
6677         self.assertEqual(2, len(allocations))
6678         compute_allocations = allocations[self.compute1_rp_uuid]['resources']
6679         network_allocations = allocations[
6680             self.ovs_bridge_rp_per_host[self.compute1_rp_uuid]]['resources']
6681 
6682         self.assertEqual(self._resources_from_flavor(self.flavor),
6683                          compute_allocations)
6684         self.assertPortMatchesAllocation(port, network_allocations)
6685 
6686         # We expect that only the RP uuid of the networking RP having the port
6687         # allocation is sent in the port binding for the port having resource
6688         # request
6689         updated_port = self.neutron.show_port(port['id'])['port']
6690         binding_profile = updated_port['binding:profile']
6691         self.assertEqual(self.ovs_bridge_rp_per_host[self.compute1_rp_uuid],
6692                          binding_profile['allocation'])
6693 
6694         # neutron is faked in the functional test so this test just send in
6695         # a os-server-external notification to trigger the detach + ERROR log.
6696         events = {
6697             "events": [
6698                 {
6699                     "name": "network-vif-deleted",
6700                     "server_uuid": server['id'],
6701                     "tag": port['id'],
6702                 }
6703             ]
6704         }
6705         response = self.api.api_post('/os-server-external-events', events).body
6706         self.assertEqual(200, response['events'][0]['code'])
6707 
6708         # NOTE(gibi): the os-server-external-events is asynchron and there is
6709         # no notification emitted when this event is handled so we can only
6710         # do sleep a bit to ensure the thet detach happens
6711         time.sleep(2)
6712 
6713         allocations = self.placement_api.get(
6714             '/allocations/%s' % server['id']).body['allocations']
6715 
6716         # Nova leaks the port allocation so the server still has the same
6717         # allocation before the port delete.
6718         self.assertEqual(2, len(allocations))
6719         compute_allocations = allocations[self.compute1_rp_uuid]['resources']
6720         port_rp_uuid = self.ovs_bridge_rp_per_host[self.compute1_rp_uuid]
6721         network_allocations = allocations[port_rp_uuid]['resources']
6722 
6723         self.assertEqual(self._resources_from_flavor(self.flavor),
6724                          compute_allocations)
6725         self.assertPortMatchesAllocation(port, network_allocations)
6726 
6727         # 1) At least nova logs an ERROR about the leak
6728         self.assertIn('ERROR [nova.compute.manager] The bound port '
6729                       '%(port_id)s is deleted in Neutron but the resource '
6730                       'allocation on the resource provider %(rp_uuid)s is '
6731                       'leaked until the server %(server_uuid)s is deleted.'
6732                       % {'port_id': port['id'],
6733                          'rp_uuid': port_rp_uuid,
6734                          'server_uuid': server['id']},
6735                       self.stdlog.logger.output)
6736 
6737         # 2) Also nova will reclaim the leaked resource during the server
6738         # delete
6739         self._delete_and_check_allocations(server)
6740 
6741     def test_two_sriov_ports_one_with_request_two_available_pfs(self):
6742         """Verify that the port's bandwidth allocated from the same PF as
6743         the allocated VF.
6744 
6745         One compute host:
6746         * PF1 (0000:01:00) is configured for physnet1
6747         * PF2 (0000:02:00) is configured for physnet2, with 1 VF and bandwidth
6748           inventory
6749         * PF3 (0000:03:00) is configured for physnet2, with 1 VF but without
6750           bandwidth inventory
6751 
6752         One instance will be booted with two neutron ports, both ports
6753         requested to be connected to physnet2. One port has resource request
6754         the other does not have resource request. The port having the resource
6755         request cannot be allocated to PF3 and PF1 while the other port that
6756         does not have resource request can be allocated to PF2 or PF3.
6757 
6758         For the detailed compute host config see the FakeDriverWithPciResources
6759         class. For the necessary passthrough_whitelist config see the setUp of
6760         the PortResourceRequestBasedSchedulingTestBase class.
6761         """
6762 
6763         sriov_port = self.neutron.sriov_port
6764         sriov_port_with_res_req = self.neutron.port_with_sriov_resource_request
6765         server = self._create_server(
6766             flavor=self.flavor_with_group_policy,
6767             networks=[
6768                 {'port': sriov_port_with_res_req['id']},
6769                 {'port': sriov_port['id']}])
6770 
6771         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6772 
6773         sriov_port = self.neutron.show_port(sriov_port['id'])['port']
6774         sriov_port_with_res_req = self.neutron.show_port(
6775             sriov_port_with_res_req['id'])['port']
6776 
6777         allocations = self.placement_api.get(
6778             '/allocations/%s' % server['id']).body['allocations']
6779 
6780         # We expect one set of allocations for the compute resources on the
6781         # compute rp and one set for the networking resources on the sriov PF2
6782         # rp.
6783         self.assertEqual(2, len(allocations))
6784         compute_allocations = allocations[self.compute1_rp_uuid]['resources']
6785         self.assertEqual(
6786             self._resources_from_flavor(self.flavor_with_group_policy),
6787             compute_allocations)
6788 
6789         sriov_allocations = allocations[self.sriov_pf2_rp_uuid]['resources']
6790         self.assertPortMatchesAllocation(
6791             sriov_port_with_res_req, sriov_allocations)
6792 
6793         # We expect that only the RP uuid of the networking RP having the port
6794         # allocation is sent in the port binding for the port having resource
6795         # request
6796         sriov_with_req_binding = sriov_port_with_res_req['binding:profile']
6797         self.assertEqual(
6798             self.sriov_pf2_rp_uuid, sriov_with_req_binding['allocation'])
6799         # and the port without resource request does not have allocation
6800         sriov_binding = sriov_port['binding:profile']
6801         self.assertNotIn('allocation', sriov_binding)
6802 
6803         # We expect that the selected PCI device matches with the RP from
6804         # where the bandwidth is allocated from. The bandwidth is allocated
6805         # from 0000:02:00 (PF2) so the PCI device should be a VF of that PF
6806         self.assertEqual('0000:02:00.1', sriov_with_req_binding['pci_slot'])
6807         # But also the port that has no resource request still gets a pci slot
6808         # allocated. The 0000:02:00 has no more VF available but 0000:03:00 has
6809         # one VF available and that PF is also on physnet2
6810         self.assertEqual('0000:03:00.1', sriov_binding['pci_slot'])
6811 
6812     def test_one_sriov_port_no_vf_and_bandwidth_available_on_the_same_pf(self):
6813         """Verify that if there is no PF that both provides bandwidth and VFs
6814         then the boot will fail.
6815         """
6816 
6817         # boot a server with a single sriov port that has no resource request
6818         sriov_port = self.neutron.sriov_port
6819         server = self._create_server(
6820             flavor=self.flavor_with_group_policy,
6821             networks=[{'port': sriov_port['id']}])
6822 
6823         self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6824         sriov_port = self.neutron.show_port(sriov_port['id'])['port']
6825         sriov_binding = sriov_port['binding:profile']
6826 
6827         # We expect that this consume the last available VF from the PF2
6828         self.assertEqual('0000:02:00.1', sriov_binding['pci_slot'])
6829 
6830         # Now boot a second server with a port that has resource request
6831         # At this point PF2 has available bandwidth but no available VF
6832         # and PF3 has available VF but no available bandwidth so we expect
6833         # the boot to fail.
6834 
6835         sriov_port_with_res_req = self.neutron.port_with_sriov_resource_request
6836         server = self._create_server(
6837             flavor=self.flavor_with_group_policy,
6838             networks=[{'port': sriov_port_with_res_req['id']}])
6839 
6840         # NOTE(gibi): It should be NoValidHost in an ideal world but that would
6841         # require the scheduler to detect the situation instead of the pci
6842         # claim. However that is pretty hard as the scheduler does not know
6843         # anything about allocation candidates (e.g. that the only candidate
6844         # for the port in this case is PF2) it see the whole host as a
6845         # candidate and in our host there is available VF for the request even
6846         # if that is on the wrong PF.
6847         server = self._wait_for_state_change(self.admin_api, server, 'ERROR')
6848         self.assertIn(
6849             'Exceeded maximum number of retries. Exhausted all hosts '
6850             'available for retrying build failures for instance',
6851             server['fault']['message'])
6852 
6853 
6854 class PortResourceRequestReSchedulingTest(
6855         PortResourceRequestBasedSchedulingTestBase):
6856     """Similar to PortResourceRequestBasedSchedulingTest
6857     except this test uses FakeRescheduleDriver which will test reschedules
6858     during server create work as expected, i.e. that the resource request
6859     allocations are moved from the initially selected compute to the
6860     alternative compute.
6861     """
6862 
6863     compute_driver = 'fake.FakeRescheduleDriver'
6864 
6865     def setUp(self):
6866         super(PortResourceRequestReSchedulingTest, self).setUp()
6867         self.compute2 = self._start_compute('host2')
6868         self.compute2_rp_uuid = self._get_provider_uuid_by_host('host2')
6869         self._create_networking_rp_tree(self.compute2_rp_uuid)
6870 
6871     def _create_networking_rp_tree(self, compute_rp_uuid):
6872         # let's simulate what the neutron would do
6873         self._create_ovs_networking_rp_tree(compute_rp_uuid)
6874 
6875     def test_boot_reschedule_success(self):
6876         port = self.neutron.port_with_resource_request
6877 
6878         server = self._create_server(
6879             flavor=self.flavor,
6880             networks=[{'port': port['id']}])
6881         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6882         updated_port = self.neutron.show_port(port['id'])['port']
6883 
6884         dest_hostname = server['OS-EXT-SRV-ATTR:host']
6885         dest_compute_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
6886 
6887         failed_compute_rp = (self.compute1_rp_uuid
6888                              if dest_compute_rp_uuid == self.compute2_rp_uuid
6889                              else self.compute2_rp_uuid)
6890 
6891         allocations = self.placement_api.get(
6892             '/allocations/%s' % server['id']).body['allocations']
6893 
6894         # We expect one set of allocations for the compute resources on the
6895         # compute rp and one set for the networking resources on the ovs bridge
6896         # rp
6897         self.assertEqual(2, len(allocations))
6898         compute_allocations = allocations[dest_compute_rp_uuid]['resources']
6899         network_allocations = allocations[
6900             self.ovs_bridge_rp_per_host[dest_compute_rp_uuid]]['resources']
6901 
6902         self.assertEqual(self._resources_from_flavor(self.flavor),
6903                          compute_allocations)
6904         self.assertPortMatchesAllocation(port, network_allocations)
6905 
6906         # assert that the allocations against the host where the spawn
6907         # failed are cleaned up properly
6908         self.assertEqual(
6909             {'VCPU': 0, 'MEMORY_MB': 0, 'DISK_GB': 0},
6910             self._get_provider_usages(failed_compute_rp))
6911         self.assertEqual(
6912             {'NET_BW_EGR_KILOBIT_PER_SEC': 0, 'NET_BW_IGR_KILOBIT_PER_SEC': 0},
6913             self._get_provider_usages(
6914                 self.ovs_bridge_rp_per_host[failed_compute_rp]))
6915 
6916         # We expect that only the RP uuid of the networking RP having the port
6917         # allocation is sent in the port binding
6918         binding_profile = updated_port['binding:profile']
6919         self.assertEqual(self.ovs_bridge_rp_per_host[dest_compute_rp_uuid],
6920                          binding_profile['allocation'])
6921 
6922         self._delete_and_check_allocations(server)
6923 
6924         # assert that unbind removes the allocation from the binding
6925         updated_port = self.neutron.show_port(port['id'])['port']
6926         binding_profile = updated_port['binding:profile']
6927         self.assertNotIn('allocation', binding_profile)
6928 
6929     def test_boot_reschedule_fill_provider_mapping_raises(self):
6930         """Verify that if the  _fill_provider_mapping raises during re-schedule
6931         then the instance is properly put into ERROR state.
6932         """
6933 
6934         port = self.neutron.port_with_resource_request
6935 
6936         # First call is during boot, we want that to succeed normally. Then the
6937         # fake virt driver triggers a re-schedule. During that re-schedule the
6938         # fill is called again, and we simulate that call raises.
6939         fill = manager.ComputeTaskManager._fill_provider_mapping
6940 
6941         with mock.patch(
6942                 'nova.conductor.manager.ComputeTaskManager.'
6943                 '_fill_provider_mapping',
6944                 side_effect=[
6945                     fill,
6946                     exception.ResourceProviderTraitRetrievalFailed(
6947                         uuid=uuids.rp1)],
6948                 autospec=True):
6949             server = self._create_server(
6950                 flavor=self.flavor,
6951                 networks=[{'port': port['id']}])
6952             server = self._wait_for_state_change(
6953                 self.admin_api, server, 'ERROR')
6954 
6955         self.assertIn(
6956             'Failed to get traits for resource provider',
6957             server['fault']['message'])
6958 
6959         self._delete_and_check_allocations(server)
6960 
6961         # assert that unbind removes the allocation from the binding
6962         updated_port = self.neutron.show_port(port['id'])['port']
6963         binding_profile = updated_port['binding:profile']
6964         self.assertNotIn('allocation', binding_profile)
