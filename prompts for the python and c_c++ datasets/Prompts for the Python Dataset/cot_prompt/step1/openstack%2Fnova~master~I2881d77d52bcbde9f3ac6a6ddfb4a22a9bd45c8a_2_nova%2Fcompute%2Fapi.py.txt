Based on the given code from a commit, please generate supplementary code files according to the commit message.
####commit message
api: Reject volume attach requests when an active bdm exists

When attaching volumes to instances Nova has previously relied on checks
carried out by c-api to ensure that a single non-multiattach volume is
not attached to multiple instances at once. While this works well in
most cases it does not handle PEBKAC issues when admins reset the state
of a volume to available, allowing users to request that Nova attach the
volume to another instance.

This change aims to address this by including a simple check in the
attach flow for non-multiattach volumes ensuring that there are no
existing active block device mapping records for the volume already
present within Nova.

Closes-Bug: #1908075
Change-Id: I2881d77d52bcbde9f3ac6a6ddfb4a22a9bd45c8a

####code 
1 # Copyright 2010 United States Government as represented by the
2 # Administrator of the National Aeronautics and Space Administration.
3 # Copyright 2011 Piston Cloud Computing, Inc.
4 # Copyright 2012-2013 Red Hat, Inc.
5 # All Rights Reserved.
6 #
7 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
8 #    not use this file except in compliance with the License. You may obtain
9 #    a copy of the License at
10 #
11 #         http://www.apache.org/licenses/LICENSE-2.0
12 #
13 #    Unless required by applicable law or agreed to in writing, software
14 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
15 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
16 #    License for the specific language governing permissions and limitations
17 #    under the License.
18 
19 """Handles all requests relating to compute resources (e.g. guest VMs,
20 networking and storage of VMs, and compute hosts on which they run)."""
21 
22 import collections
23 import functools
24 import re
25 import string
26 
27 from castellan import key_manager
28 import os_traits
29 from oslo_log import log as logging
30 from oslo_messaging import exceptions as oslo_exceptions
31 from oslo_serialization import base64 as base64utils
32 from oslo_utils import excutils
33 from oslo_utils import strutils
34 from oslo_utils import timeutils
35 from oslo_utils import units
36 from oslo_utils import uuidutils
37 
38 from nova.accelerator import cyborg
39 from nova import availability_zones
40 from nova import block_device
41 from nova.compute import flavors
42 from nova.compute import instance_actions
43 from nova.compute import instance_list
44 from nova.compute import migration_list
45 from nova.compute import power_state
46 from nova.compute import rpcapi as compute_rpcapi
47 from nova.compute import task_states
48 from nova.compute import utils as compute_utils
49 from nova.compute.utils import wrap_instance_event
50 from nova.compute import vm_states
51 from nova import conductor
52 import nova.conf
53 from nova import context as nova_context
54 from nova import crypto
55 from nova.db import base
56 from nova.db.sqlalchemy import api as db_api
57 from nova import exception
58 from nova import exception_wrapper
59 from nova.i18n import _
60 from nova.image import glance
61 from nova.network import constants
62 from nova.network import model as network_model
63 from nova.network import neutron
64 from nova.network import security_group_api
65 from nova import objects
66 from nova.objects import block_device as block_device_obj
67 from nova.objects import external_event as external_event_obj
68 from nova.objects import fields as fields_obj
69 from nova.objects import image_meta as image_meta_obj
70 from nova.objects import keypair as keypair_obj
71 from nova.objects import quotas as quotas_obj
72 from nova.pci import request as pci_request
73 from nova.policies import servers as servers_policies
74 import nova.policy
75 from nova import profiler
76 from nova import rpc
77 from nova.scheduler.client import query
78 from nova.scheduler.client import report
79 from nova.scheduler import utils as scheduler_utils
80 from nova import servicegroup
81 from nova import utils
82 from nova.virt import hardware
83 from nova.volume import cinder
84 
85 LOG = logging.getLogger(__name__)
86 
87 get_notifier = functools.partial(rpc.get_notifier, service='compute')
88 # NOTE(gibi): legacy notification used compute as a service but these
89 # calls still run on the client side of the compute service which is
90 # nova-api. By setting the binary to nova-api below, we can make sure
91 # that the new versioned notifications has the right publisher_id but the
92 # legacy notifications does not change.
93 wrap_exception = functools.partial(exception_wrapper.wrap_exception,
94                                    get_notifier=get_notifier,
95                                    binary='nova-api')
96 CONF = nova.conf.CONF
97 
98 AGGREGATE_ACTION_UPDATE = 'Update'
99 AGGREGATE_ACTION_UPDATE_META = 'UpdateMeta'
100 AGGREGATE_ACTION_DELETE = 'Delete'
101 AGGREGATE_ACTION_ADD = 'Add'
102 
103 MIN_COMPUTE_SYNC_COMPUTE_STATUS_DISABLED = 38
104 MIN_COMPUTE_CROSS_CELL_RESIZE = 47
105 MIN_COMPUTE_SAME_HOST_COLD_MIGRATE = 48
106 
107 # TODO(huaqiang): Remove in Wallaby
108 MIN_VER_NOVA_COMPUTE_MIXED_POLICY = 52
109 
110 SUPPORT_ACCELERATOR_SERVICE_FOR_REBUILD = 53
111 
112 # FIXME(danms): Keep a global cache of the cells we find the
113 # first time we look. This needs to be refreshed on a timer or
114 # trigger.
115 CELLS = []
116 
117 
118 def check_instance_state(vm_state=None, task_state=(None,),
119                          must_have_launched=True):
120     """Decorator to check VM and/or task state before entry to API functions.
121 
122     If the instance is in the wrong state, or has not been successfully
123     started at least once the wrapper will raise an exception.
124     """
125 
126     if vm_state is not None and not isinstance(vm_state, set):
127         vm_state = set(vm_state)
128     if task_state is not None and not isinstance(task_state, set):
129         task_state = set(task_state)
130 
131     def outer(f):
132         @functools.wraps(f)
133         def inner(self, context, instance, *args, **kw):
134             if vm_state is not None and instance.vm_state not in vm_state:
135                 raise exception.InstanceInvalidState(
136                     attr='vm_state',
137                     instance_uuid=instance.uuid,
138                     state=instance.vm_state,
139                     method=f.__name__)
140             if (task_state is not None and
141                     instance.task_state not in task_state):
142                 raise exception.InstanceInvalidState(
143                     attr='task_state',
144                     instance_uuid=instance.uuid,
145                     state=instance.task_state,
146                     method=f.__name__)
147             if must_have_launched and not instance.launched_at:
148                 raise exception.InstanceInvalidState(
149                     attr='launched_at',
150                     instance_uuid=instance.uuid,
151                     state=instance.launched_at,
152                     method=f.__name__)
153 
154             return f(self, context, instance, *args, **kw)
155         return inner
156     return outer
157 
158 
159 def _set_or_none(q):
160     return q if q is None or isinstance(q, set) else set(q)
161 
162 
163 def reject_instance_state(vm_state=None, task_state=None):
164     """Decorator.  Raise InstanceInvalidState if instance is in any of the
165     given states.
166     """
167 
168     vm_state = _set_or_none(vm_state)
169     task_state = _set_or_none(task_state)
170 
171     def outer(f):
172         @functools.wraps(f)
173         def inner(self, context, instance, *args, **kw):
174             _InstanceInvalidState = functools.partial(
175                 exception.InstanceInvalidState,
176                 instance_uuid=instance.uuid,
177                 method=f.__name__)
178 
179             if vm_state is not None and instance.vm_state in vm_state:
180                 raise _InstanceInvalidState(
181                     attr='vm_state', state=instance.vm_state)
182 
183             if task_state is not None and instance.task_state in task_state:
184                 raise _InstanceInvalidState(
185                     attr='task_state', state=instance.task_state)
186 
187             return f(self, context, instance, *args, **kw)
188         return inner
189     return outer
190 
191 
192 def check_instance_host(check_is_up=False):
193     """Validate the instance.host before performing the operation.
194 
195     At a minimum this method will check that the instance.host is set.
196 
197     :param check_is_up: If True, check that the instance.host status is UP
198         or MAINTENANCE (disabled but not down).
199     :raises: InstanceNotReady if the instance.host is not set
200     :raises: ServiceUnavailable if check_is_up=True and the instance.host
201         compute service status is not UP or MAINTENANCE
202     """
203     def outer(function):
204         @functools.wraps(function)
205         def wrapped(self, context, instance, *args, **kwargs):
206             if not instance.host:
207                 raise exception.InstanceNotReady(instance_id=instance.uuid)
208             if check_is_up:
209                 # Make sure the source compute service is not down otherwise we
210                 # cannot proceed.
211                 host_status = self.get_instance_host_status(instance)
212                 if host_status not in (fields_obj.HostStatus.UP,
213                                        fields_obj.HostStatus.MAINTENANCE):
214                     # ComputeServiceUnavailable would make more sense here but
215                     # we do not want to leak hostnames to end users.
216                     raise exception.ServiceUnavailable()
217             return function(self, context, instance, *args, **kwargs)
218         return wrapped
219     return outer
220 
221 
222 def check_instance_lock(function):
223     @functools.wraps(function)
224     def inner(self, context, instance, *args, **kwargs):
225         if instance.locked and not context.is_admin:
226             raise exception.InstanceIsLocked(instance_uuid=instance.uuid)
227         return function(self, context, instance, *args, **kwargs)
228     return inner
229 
230 
231 def reject_sev_instances(operation):
232     """Reject requests to decorated function if instance has SEV enabled.
233 
234     Raise OperationNotSupportedForSEV if instance has SEV enabled.
235     """
236 
237     def outer(f):
238         @functools.wraps(f)
239         def inner(self, context, instance, *args, **kw):
240             if hardware.get_mem_encryption_constraint(instance.flavor,
241                                                       instance.image_meta):
242                 raise exception.OperationNotSupportedForSEV(
243                     instance_uuid=instance.uuid,
244                     operation=operation)
245             return f(self, context, instance, *args, **kw)
246         return inner
247     return outer
248 
249 
250 def reject_vtpm_instances(operation):
251     """Reject requests to decorated function if instance has vTPM enabled.
252 
253     Raise OperationNotSupportedForVTPM if instance has vTPM enabled.
254     """
255 
256     def outer(f):
257         @functools.wraps(f)
258         def inner(self, context, instance, *args, **kw):
259             if hardware.get_vtpm_constraint(
260                 instance.flavor, instance.image_meta,
261             ):
262                 raise exception.OperationNotSupportedForVTPM(
263                     instance_uuid=instance.uuid, operation=operation)
264             return f(self, context, instance, *args, **kw)
265         return inner
266     return outer
267 
268 
269 def load_cells():
270     global CELLS
271     if not CELLS:
272         CELLS = objects.CellMappingList.get_all(
273             nova_context.get_admin_context())
274         LOG.debug('Found %(count)i cells: %(cells)s',
275                   dict(count=len(CELLS),
276                        cells=','.join([c.identity for c in CELLS])))
277 
278     if not CELLS:
279         LOG.error('No cells are configured, unable to continue')
280 
281 
282 def _get_image_meta_obj(image_meta_dict):
283     try:
284         image_meta = objects.ImageMeta.from_dict(image_meta_dict)
285     except ValueError as e:
286         # there must be invalid values in the image meta properties so
287         # consider this an invalid request
288         msg = _('Invalid image metadata. Error: %s') % str(e)
289         raise exception.InvalidRequest(msg)
290     return image_meta
291 
292 
293 def block_accelerators(until_service=None):
294     def inner(func):
295         @functools.wraps(func)
296         def wrapper(self, context, instance, *args, **kwargs):
297             # NOTE(brinzhang): Catch a request operating a mixed instance,
298             # make sure all nova-compute services have been upgraded and
299             # support the accelerators.
300             dp_name = instance.flavor.extra_specs.get('accel:device_profile')
301             service_support = False
302             if not dp_name:
303                 service_support = True
304             elif until_service:
305                 min_version = objects.service.get_minimum_version_all_cells(
306                     nova_context.get_admin_context(), ['nova-compute'])
307                 if min_version >= until_service:
308                     service_support = True
309             if not service_support:
310                 raise exception.ForbiddenWithAccelerators()
311             return func(self, context, instance, *args, **kwargs)
312         return wrapper
313     return inner
314 
315 
316 @profiler.trace_cls("compute_api")
317 class API(base.Base):
318     """API for interacting with the compute manager."""
319 
320     def __init__(self, image_api=None, network_api=None, volume_api=None,
321                  **kwargs):
322         self.image_api = image_api or glance.API()
323         self.network_api = network_api or neutron.API()
324         self.volume_api = volume_api or cinder.API()
325         self._placementclient = None  # Lazy-load on first access.
326         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
327         self.compute_task_api = conductor.ComputeTaskAPI()
328         self.servicegroup_api = servicegroup.API()
329         self.host_api = HostAPI(self.compute_rpcapi, self.servicegroup_api)
330         self.notifier = rpc.get_notifier('compute', CONF.host)
331         if CONF.ephemeral_storage_encryption.enabled:
332             self.key_manager = key_manager.API()
333         # Help us to record host in EventReporter
334         self.host = CONF.host
335         super(API, self).__init__(**kwargs)
336 
337     def _record_action_start(self, context, instance, action):
338         objects.InstanceAction.action_start(context, instance.uuid,
339                                             action, want_result=False)
340 
341     def _check_injected_file_quota(self, context, injected_files):
342         """Enforce quota limits on injected files.
343 
344         Raises a QuotaError if any limit is exceeded.
345         """
346         if not injected_files:
347             return
348 
349         # Check number of files first
350         try:
351             objects.Quotas.limit_check(context,
352                                        injected_files=len(injected_files))
353         except exception.OverQuota:
354             raise exception.OnsetFileLimitExceeded()
355 
356         # OK, now count path and content lengths; we're looking for
357         # the max...
358         max_path = 0
359         max_content = 0
360         for path, content in injected_files:
361             max_path = max(max_path, len(path))
362             max_content = max(max_content, len(content))
363 
364         try:
365             objects.Quotas.limit_check(context,
366                                        injected_file_path_bytes=max_path,
367                                        injected_file_content_bytes=max_content)
368         except exception.OverQuota as exc:
369             # Favor path limit over content limit for reporting
370             # purposes
371             if 'injected_file_path_bytes' in exc.kwargs['overs']:
372                 raise exception.OnsetFilePathLimitExceeded(
373                       allowed=exc.kwargs['quotas']['injected_file_path_bytes'])
374             else:
375                 raise exception.OnsetFileContentLimitExceeded(
376                    allowed=exc.kwargs['quotas']['injected_file_content_bytes'])
377 
378     def _check_metadata_properties_quota(self, context, metadata=None):
379         """Enforce quota limits on metadata properties."""
380         if not metadata:
381             return
382         if not isinstance(metadata, dict):
383             msg = (_("Metadata type should be dict."))
384             raise exception.InvalidMetadata(reason=msg)
385         num_metadata = len(metadata)
386         try:
387             objects.Quotas.limit_check(context, metadata_items=num_metadata)
388         except exception.OverQuota as exc:
389             quota_metadata = exc.kwargs['quotas']['metadata_items']
390             raise exception.MetadataLimitExceeded(allowed=quota_metadata)
391 
392         # Because metadata is stored in the DB, we hard-code the size limits
393         # In future, we may support more variable length strings, so we act
394         #  as if this is quota-controlled for forwards compatibility.
395         # Those are only used in V2 API, from V2.1 API, those checks are
396         # validated at API layer schema validation.
397         for k, v in metadata.items():
398             try:
399                 utils.check_string_length(v)
400                 utils.check_string_length(k, min_length=1)
401             except exception.InvalidInput as e:
402                 raise exception.InvalidMetadata(reason=e.format_message())
403 
404             if len(k) > 255:
405                 msg = _("Metadata property key greater than 255 characters")
406                 raise exception.InvalidMetadataSize(reason=msg)
407             if len(v) > 255:
408                 msg = _("Metadata property value greater than 255 characters")
409                 raise exception.InvalidMetadataSize(reason=msg)
410 
411     def _check_requested_secgroups(self, context, secgroups):
412         """Check if the security group requested exists and belongs to
413         the project.
414 
415         :param context: The nova request context.
416         :type context: nova.context.RequestContext
417         :param secgroups: list of requested security group names
418         :type secgroups: list
419         :returns: list of requested security group UUIDs; note that 'default'
420             is a special case and will be unmodified if it's requested.
421         """
422         security_groups = []
423         for secgroup in secgroups:
424             # NOTE(sdague): default is handled special
425             if secgroup == "default":
426                 security_groups.append(secgroup)
427                 continue
428             secgroup_uuid = security_group_api.validate_name(context, secgroup)
429             security_groups.append(secgroup_uuid)
430 
431         return security_groups
432 
433     def _check_requested_networks(self, context, requested_networks,
434                                   max_count):
435         """Check if the networks requested belongs to the project
436         and the fixed IP address for each network provided is within
437         same the network block
438         """
439         if requested_networks is not None:
440             if requested_networks.no_allocate:
441                 # If the network request was specifically 'none' meaning don't
442                 # allocate any networks, we just return the number of requested
443                 # instances since quotas don't change at all.
444                 return max_count
445 
446             # NOTE(danms): Temporary transition
447             requested_networks = requested_networks.as_tuples()
448 
449         return self.network_api.validate_networks(context, requested_networks,
450                                                   max_count)
451 
452     def _handle_kernel_and_ramdisk(self, context, kernel_id, ramdisk_id,
453                                    image):
454         """Choose kernel and ramdisk appropriate for the instance.
455 
456         The kernel and ramdisk can be chosen in one of two ways:
457 
458             1. Passed in with create-instance request.
459 
460             2. Inherited from image metadata.
461 
462         If inherited from image metadata, and if that image metadata value is
463         set to 'nokernel', both kernel and ramdisk will default to None.
464         """
465         # Inherit from image if not specified
466         image_properties = image.get('properties', {})
467 
468         if kernel_id is None:
469             kernel_id = image_properties.get('kernel_id')
470 
471         if ramdisk_id is None:
472             ramdisk_id = image_properties.get('ramdisk_id')
473 
474         # Force to None if kernel_id indicates that a kernel is not to be used
475         if kernel_id == 'nokernel':
476             kernel_id = None
477             ramdisk_id = None
478 
479         # Verify kernel and ramdisk exist (fail-fast)
480         if kernel_id is not None:
481             kernel_image = self.image_api.get(context, kernel_id)
482             # kernel_id could have been a URI, not a UUID, so to keep behaviour
483             # from before, which leaked that implementation detail out to the
484             # caller, we return the image UUID of the kernel image and ramdisk
485             # image (below) and not any image URIs that might have been
486             # supplied.
487             # TODO(jaypipes): Get rid of this silliness once we move to a real
488             # Image object and hide all of that stuff within nova.image.glance
489             kernel_id = kernel_image['id']
490 
491         if ramdisk_id is not None:
492             ramdisk_image = self.image_api.get(context, ramdisk_id)
493             ramdisk_id = ramdisk_image['id']
494 
495         return kernel_id, ramdisk_id
496 
497     @staticmethod
498     def parse_availability_zone(context, availability_zone):
499         # NOTE(vish): We have a legacy hack to allow admins to specify hosts
500         #             via az using az:host:node. It might be nice to expose an
501         #             api to specify specific hosts to force onto, but for
502         #             now it just supports this legacy hack.
503         # NOTE(deva): It is also possible to specify az::node, in which case
504         #             the host manager will determine the correct host.
505         forced_host = None
506         forced_node = None
507         if availability_zone and ':' in availability_zone:
508             c = availability_zone.count(':')
509             if c == 1:
510                 availability_zone, forced_host = availability_zone.split(':')
511             elif c == 2:
512                 if '::' in availability_zone:
513                     availability_zone, forced_node = \
514                             availability_zone.split('::')
515                 else:
516                     availability_zone, forced_host, forced_node = \
517                             availability_zone.split(':')
518             else:
519                 raise exception.InvalidInput(
520                         reason="Unable to parse availability_zone")
521 
522         if not availability_zone:
523             availability_zone = CONF.default_schedule_zone
524 
525         return availability_zone, forced_host, forced_node
526 
527     def _ensure_auto_disk_config_is_valid(self, auto_disk_config_img,
528                                           auto_disk_config, image):
529         auto_disk_config_disabled = \
530                 utils.is_auto_disk_config_disabled(auto_disk_config_img)
531         if auto_disk_config_disabled and auto_disk_config:
532             raise exception.AutoDiskConfigDisabledByImage(image=image)
533 
534     def _inherit_properties_from_image(self, image, auto_disk_config):
535         image_properties = image.get('properties', {})
536         auto_disk_config_img = \
537                 utils.get_auto_disk_config_from_image_props(image_properties)
538         self._ensure_auto_disk_config_is_valid(auto_disk_config_img,
539                                                auto_disk_config,
540                                                image.get("id"))
541         if auto_disk_config is None:
542             auto_disk_config = strutils.bool_from_string(auto_disk_config_img)
543 
544         return {
545             'os_type': image_properties.get('os_type'),
546             'architecture': image_properties.get('architecture'),
547             'vm_mode': image_properties.get('vm_mode'),
548             'auto_disk_config': auto_disk_config
549         }
550 
551     def _check_config_drive(self, config_drive):
552         if config_drive:
553             try:
554                 bool_val = strutils.bool_from_string(config_drive,
555                                                      strict=True)
556             except ValueError:
557                 raise exception.ConfigDriveInvalidValue(option=config_drive)
558         else:
559             bool_val = False
560         # FIXME(comstud):  Bug ID 1193438 filed for this. This looks silly,
561         # but this is because the config drive column is a String.  False
562         # is represented by using an empty string.  And for whatever
563         # reason, we rely on the DB to cast True to a String.
564         return True if bool_val else ''
565 
566     def _validate_flavor_image(self, context, image_id, image,
567                                instance_type, root_bdm, validate_numa=True):
568         """Validate the flavor and image.
569 
570         This is called from the API service to ensure that the flavor
571         extra-specs and image properties are self-consistent and compatible
572         with each other.
573 
574         :param context: A context.RequestContext
575         :param image_id: UUID of the image
576         :param image: a dict representation of the image including properties,
577                       enforces the image status is active.
578         :param instance_type: Flavor object
579         :param root_bdm: BlockDeviceMapping for root disk.  Will be None for
580                the resize case.
581         :param validate_numa: Flag to indicate whether or not to validate
582                the NUMA-related metadata.
583         :raises: Many different possible exceptions.  See
584                  api.openstack.compute.servers.INVALID_FLAVOR_IMAGE_EXCEPTIONS
585                  for the full list.
586         """
587         if image and image['status'] != 'active':
588             raise exception.ImageNotActive(image_id=image_id)
589         self._validate_flavor_image_nostatus(context, image, instance_type,
590                                              root_bdm, validate_numa)
591 
592     @staticmethod
593     def _detect_nonbootable_image_from_properties(image_id, image):
594         """Check image for a property indicating it's nonbootable.
595 
596         This is called from the API service to ensure that there are
597         no known image properties indicating that this image is of a
598         type that we do not support booting from.
599 
600         Currently the only such property is 'cinder_encryption_key_id'.
601 
602         :param image_id: UUID of the image
603         :param image: a dict representation of the image including properties
604         :raises: ImageUnacceptable if the image properties indicate
605                  that booting this image is not supported
606         """
607         if not image:
608             return
609 
610         image_properties = image.get('properties', {})
611         # NOTE(lyarwood) Skip this check when image_id is None indicating that
612         # the instance is booting from a volume that was itself initially
613         # created from an image. As such we don't care if
614         # cinder_encryption_key_id was against the original image as we are now
615         # booting from an encrypted volume.
616         if image_properties.get('cinder_encryption_key_id') and image_id:
617             reason = _('Direct booting of an image uploaded from an '
618                        'encrypted volume is unsupported.')
619             raise exception.ImageUnacceptable(image_id=image_id,
620                                               reason=reason)
621 
622     @staticmethod
623     def _validate_flavor_image_nostatus(context, image, instance_type,
624                                         root_bdm, validate_numa=True,
625                                         validate_pci=False):
626         """Validate the flavor and image.
627 
628         This is called from the API service to ensure that the flavor
629         extra-specs and image properties are self-consistent and compatible
630         with each other.
631 
632         :param context: A context.RequestContext
633         :param image: a dict representation of the image including properties
634         :param instance_type: Flavor object
635         :param root_bdm: BlockDeviceMapping for root disk.  Will be None for
636                the resize case.
637         :param validate_numa: Flag to indicate whether or not to validate
638                the NUMA-related metadata.
639         :param validate_pci: Flag to indicate whether or not to validate
640                the PCI-related metadata.
641         :raises: Many different possible exceptions.  See
642                  api.openstack.compute.servers.INVALID_FLAVOR_IMAGE_EXCEPTIONS
643                  for the full list.
644         """
645         if not image:
646             return
647 
648         image_properties = image.get('properties', {})
649         config_drive_option = image_properties.get(
650             'img_config_drive', 'optional')
651         if config_drive_option not in ['optional', 'mandatory']:
652             raise exception.InvalidImageConfigDrive(
653                 config_drive=config_drive_option)
654 
655         if instance_type['memory_mb'] < int(image.get('min_ram') or 0):
656             raise exception.FlavorMemoryTooSmall()
657 
658         # Image min_disk is in gb, size is in bytes. For sanity, have them both
659         # in bytes.
660         image_min_disk = int(image.get('min_disk') or 0) * units.Gi
661         image_size = int(image.get('size') or 0)
662 
663         # Target disk is a volume. Don't check flavor disk size because it
664         # doesn't make sense, and check min_disk against the volume size.
665         if root_bdm is not None and root_bdm.is_volume:
666             # There are 2 possibilities here:
667             #
668             # 1. The target volume already exists but bdm.volume_size is not
669             #    yet set because this method is called before
670             #    _bdm_validate_set_size_and_instance during server create.
671             # 2. The target volume doesn't exist, in which case the bdm will
672             #    contain the intended volume size
673             #
674             # Note that rebuild also calls this method with potentially a new
675             # image but you can't rebuild a volume-backed server with a new
676             # image (yet).
677             #
678             # Cinder does its own check against min_disk, so if the target
679             # volume already exists this has already been done and we don't
680             # need to check it again here. In this case, volume_size may not be
681             # set on the bdm.
682             #
683             # If we're going to create the volume, the bdm will contain
684             # volume_size. Therefore we should check it if it exists. This will
685             # still be checked again by cinder when the volume is created, but
686             # that will not happen until the request reaches a host. By
687             # checking it here, the user gets an immediate and useful failure
688             # indication.
689             #
690             # The third possibility is that we have failed to consider
691             # something, and there are actually more than 2 possibilities. In
692             # this case cinder will still do the check at volume creation time.
693             # The behaviour will still be correct, but the user will not get an
694             # immediate failure from the api, and will instead have to
695             # determine why the instance is in an error state with a task of
696             # block_device_mapping.
697             #
698             # We could reasonably refactor this check into _validate_bdm at
699             # some future date, as the various size logic is already split out
700             # in there.
701             dest_size = root_bdm.volume_size
702             if dest_size is not None:
703                 dest_size *= units.Gi
704 
705                 if image_min_disk > dest_size:
706                     raise exception.VolumeSmallerThanMinDisk(
707                         volume_size=dest_size, image_min_disk=image_min_disk)
708 
709         # Target disk is a local disk whose size is taken from the flavor
710         else:
711             dest_size = instance_type['root_gb'] * units.Gi
712 
713             # NOTE(johannes): root_gb is allowed to be 0 for legacy reasons
714             # since libvirt interpreted the value differently than other
715             # drivers. A value of 0 means don't check size.
716             if dest_size != 0:
717                 if image_size > dest_size:
718                     raise exception.FlavorDiskSmallerThanImage(
719                         flavor_size=dest_size, image_size=image_size)
720 
721                 if image_min_disk > dest_size:
722                     raise exception.FlavorDiskSmallerThanMinDisk(
723                         flavor_size=dest_size, image_min_disk=image_min_disk)
724             else:
725                 # The user is attempting to create a server with a 0-disk
726                 # image-backed flavor, which can lead to issues with a large
727                 # image consuming an unexpectedly large amount of local disk
728                 # on the compute host. Check to see if the deployment will
729                 # allow that.
730                 if not context.can(
731                         servers_policies.ZERO_DISK_FLAVOR, fatal=False):
732                     raise exception.BootFromVolumeRequiredForZeroDiskFlavor()
733 
734         API._validate_flavor_image_numa_pci(
735             image, instance_type, validate_numa=validate_numa,
736             validate_pci=validate_pci)
737 
738     # TODO(huaqiang): Remove in Wallaby when there is no nova-compute node
739     # having a version prior to Victoria.
740     @staticmethod
741     def _check_compute_service_for_mixed_instance(numa_topology):
742         """Check if the nova-compute service is ready to support mixed instance
743         when the CPU allocation policy is 'mixed'.
744         """
745         # No need to check the instance with no NUMA topology associated with.
746         if numa_topology is None:
747             return
748 
749         # No need to check if instance CPU policy is not 'mixed'
750         if numa_topology.cpu_policy != fields_obj.CPUAllocationPolicy.MIXED:
751             return
752 
753         # Catch a request creating a mixed instance, make sure all nova-compute
754         # service have been upgraded and support the mixed policy.
755         minimal_version = objects.service.get_minimum_version_all_cells(
756             nova_context.get_admin_context(), ['nova-compute'])
757         if minimal_version < MIN_VER_NOVA_COMPUTE_MIXED_POLICY:
758             raise exception.MixedInstanceNotSupportByComputeService()
759 
760     @staticmethod
761     def _validate_flavor_image_numa_pci(image, instance_type,
762                                         validate_numa=True,
763                                         validate_pci=False):
764         """Validate the flavor and image NUMA/PCI values.
765 
766         This is called from the API service to ensure that the flavor
767         extra-specs and image properties are self-consistent and compatible
768         with each other.
769 
770         :param image: a dict representation of the image including properties
771         :param instance_type: Flavor object
772         :param validate_numa: Flag to indicate whether or not to validate
773                the NUMA-related metadata.
774         :param validate_pci: Flag to indicate whether or not to validate
775                the PCI-related metadata.
776         :raises: Many different possible exceptions.  See
777                  api.openstack.compute.servers.INVALID_FLAVOR_IMAGE_EXCEPTIONS
778                  for the full list.
779         """
780         image_meta = _get_image_meta_obj(image)
781 
782         API._validate_flavor_image_mem_encryption(instance_type, image_meta)
783 
784         # validate PMU extra spec and image metadata
785         flavor_pmu = instance_type.extra_specs.get('hw:pmu')
786         image_pmu = image_meta.properties.get('hw_pmu')
787         if (flavor_pmu is not None and image_pmu is not None and
788                 image_pmu != strutils.bool_from_string(flavor_pmu)):
789             raise exception.ImagePMUConflict()
790 
791         # Only validate values of flavor/image so the return results of
792         # following 'get' functions are not used.
793         hardware.get_number_of_serial_ports(instance_type, image_meta)
794         hardware.get_realtime_cpu_constraint(instance_type, image_meta)
795         hardware.get_cpu_topology_constraints(instance_type, image_meta)
796         if validate_numa:
797             hardware.numa_get_constraints(instance_type, image_meta)
798         if validate_pci:
799             pci_request.get_pci_requests_from_flavor(instance_type)
800 
801     @staticmethod
802     def _validate_flavor_image_mem_encryption(instance_type, image):
803         """Validate that the flavor and image don't make contradictory
804         requests regarding memory encryption.
805 
806         :param instance_type: Flavor object
807         :param image: an ImageMeta object
808         :raises: nova.exception.FlavorImageConflict
809         """
810         # This library function will raise the exception for us if
811         # necessary; if not, we can ignore the result returned.
812         hardware.get_mem_encryption_constraint(instance_type, image)
813 
814     def _get_image_defined_bdms(self, instance_type, image_meta,
815                                 root_device_name):
816         image_properties = image_meta.get('properties', {})
817 
818         # Get the block device mappings defined by the image.
819         image_defined_bdms = image_properties.get('block_device_mapping', [])
820         legacy_image_defined = not image_properties.get('bdm_v2', False)
821 
822         image_mapping = image_properties.get('mappings', [])
823 
824         if legacy_image_defined:
825             image_defined_bdms = block_device.from_legacy_mapping(
826                 image_defined_bdms, None, root_device_name)
827         else:
828             image_defined_bdms = list(map(block_device.BlockDeviceDict,
829                                           image_defined_bdms))
830 
831         if image_mapping:
832             image_mapping = self._prepare_image_mapping(instance_type,
833                                                         image_mapping)
834             image_defined_bdms = self._merge_bdms_lists(
835                 image_mapping, image_defined_bdms)
836 
837         return image_defined_bdms
838 
839     def _get_flavor_defined_bdms(self, instance_type, block_device_mapping):
840         flavor_defined_bdms = []
841 
842         have_ephemeral_bdms = any(filter(
843             block_device.new_format_is_ephemeral, block_device_mapping))
844         have_swap_bdms = any(filter(
845             block_device.new_format_is_swap, block_device_mapping))
846 
847         if instance_type.get('ephemeral_gb') and not have_ephemeral_bdms:
848             flavor_defined_bdms.append(
849                 block_device.create_blank_bdm(instance_type['ephemeral_gb']))
850         if instance_type.get('swap') and not have_swap_bdms:
851             flavor_defined_bdms.append(
852                 block_device.create_blank_bdm(instance_type['swap'], 'swap'))
853 
854         return flavor_defined_bdms
855 
856     def _merge_bdms_lists(self, overridable_mappings, overrider_mappings):
857         """Override any block devices from the first list by device name
858 
859         :param overridable_mappings: list which items are overridden
860         :param overrider_mappings: list which items override
861 
862         :returns: A merged list of bdms
863         """
864         device_names = set(bdm['device_name'] for bdm in overrider_mappings
865                            if bdm['device_name'])
866         return (overrider_mappings +
867                 [bdm for bdm in overridable_mappings
868                  if bdm['device_name'] not in device_names])
869 
870     def _check_and_transform_bdm(self, context, base_options, instance_type,
871                                  image_meta, min_count, max_count,
872                                  block_device_mapping, legacy_bdm):
873         # NOTE (ndipanov): Assume root dev name is 'vda' if not supplied.
874         #                  It's needed for legacy conversion to work.
875         root_device_name = (base_options.get('root_device_name') or 'vda')
876         image_ref = base_options.get('image_ref', '')
877         # If the instance is booted by image and has a volume attached,
878         # the volume cannot have the same device name as root_device_name
879         if image_ref:
880             for bdm in block_device_mapping:
881                 if (bdm.get('destination_type') == 'volume' and
882                     block_device.strip_dev(bdm.get(
883                     'device_name')) == root_device_name):
884                     msg = _('The volume cannot be assigned the same device'
885                             ' name as the root device %s') % root_device_name
886                     raise exception.InvalidRequest(msg)
887 
888         image_defined_bdms = self._get_image_defined_bdms(
889             instance_type, image_meta, root_device_name)
890         root_in_image_bdms = (
891             block_device.get_root_bdm(image_defined_bdms) is not None)
892 
893         if legacy_bdm:
894             block_device_mapping = block_device.from_legacy_mapping(
895                 block_device_mapping, image_ref, root_device_name,
896                 no_root=root_in_image_bdms)
897         elif root_in_image_bdms:
898             # NOTE (ndipanov): client will insert an image mapping into the v2
899             # block_device_mapping, but if there is a bootable device in image
900             # mappings - we need to get rid of the inserted image
901             # NOTE (gibi): another case is when a server is booted with an
902             # image to bdm mapping where the image only contains a bdm to a
903             # snapshot. In this case the other image to bdm mapping
904             # contains an unnecessary device with boot_index == 0.
905             # Also in this case the image_ref is None as we are booting from
906             # an image to volume bdm.
907             def not_image_and_root_bdm(bdm):
908                 return not (bdm.get('boot_index') == 0 and
909                             bdm.get('source_type') == 'image')
910 
911             block_device_mapping = list(
912                 filter(not_image_and_root_bdm, block_device_mapping))
913 
914         block_device_mapping = self._merge_bdms_lists(
915             image_defined_bdms, block_device_mapping)
916 
917         if min_count > 1 or max_count > 1:
918             if any(map(lambda bdm: bdm['source_type'] == 'volume',
919                        block_device_mapping)):
920                 msg = _('Cannot attach one or more volumes to multiple'
921                         ' instances')
922                 raise exception.InvalidRequest(msg)
923 
924         block_device_mapping += self._get_flavor_defined_bdms(
925             instance_type, block_device_mapping)
926 
927         return block_device_obj.block_device_make_list_from_dicts(
928                 context, block_device_mapping)
929 
930     def _get_image(self, context, image_href):
931         if not image_href:
932             return None, {}
933 
934         image = self.image_api.get(context, image_href)
935         return image['id'], image
936 
937     def _checks_for_create_and_rebuild(self, context, image_id, image,
938                                        instance_type, metadata,
939                                        files_to_inject, root_bdm,
940                                        validate_numa=True):
941         self._check_metadata_properties_quota(context, metadata)
942         self._check_injected_file_quota(context, files_to_inject)
943         self._detect_nonbootable_image_from_properties(image_id, image)
944         self._validate_flavor_image(context, image_id, image,
945                                     instance_type, root_bdm,
946                                     validate_numa=validate_numa)
947 
948     def _validate_and_build_base_options(self, context, instance_type,
949                                          boot_meta, image_href, image_id,
950                                          kernel_id, ramdisk_id, display_name,
951                                          display_description, key_name,
952                                          key_data, security_groups,
953                                          availability_zone, user_data,
954                                          metadata, access_ip_v4, access_ip_v6,
955                                          requested_networks, config_drive,
956                                          auto_disk_config, reservation_id,
957                                          max_count,
958                                          supports_port_resource_request):
959         """Verify all the input parameters regardless of the provisioning
960         strategy being performed.
961         """
962         if instance_type['disabled']:
963             raise exception.FlavorNotFound(flavor_id=instance_type['id'])
964 
965         if user_data:
966             try:
967                 base64utils.decode_as_bytes(user_data)
968             except TypeError:
969                 raise exception.InstanceUserDataMalformed()
970 
971         # When using Neutron, _check_requested_secgroups will translate and
972         # return any requested security group names to uuids.
973         security_groups = self._check_requested_secgroups(
974             context, security_groups)
975 
976         # Note:  max_count is the number of instances requested by the user,
977         # max_network_count is the maximum number of instances taking into
978         # account any network quotas
979         max_network_count = self._check_requested_networks(
980             context, requested_networks, max_count)
981 
982         kernel_id, ramdisk_id = self._handle_kernel_and_ramdisk(
983             context, kernel_id, ramdisk_id, boot_meta)
984 
985         config_drive = self._check_config_drive(config_drive)
986 
987         if key_data is None and key_name is not None:
988             key_pair = objects.KeyPair.get_by_name(context,
989                                                    context.user_id,
990                                                    key_name)
991             key_data = key_pair.public_key
992         else:
993             key_pair = None
994 
995         root_device_name = block_device.prepend_dev(
996                 block_device.properties_root_device_name(
997                     boot_meta.get('properties', {})))
998 
999         image_meta = _get_image_meta_obj(boot_meta)
1000         numa_topology = hardware.numa_get_constraints(
1001                 instance_type, image_meta)
1002 
1003         system_metadata = {}
1004 
1005         pci_numa_affinity_policy = hardware.get_pci_numa_policy_constraint(
1006             instance_type, image_meta)
1007 
1008         # PCI requests come from two sources: instance flavor and
1009         # requested_networks. The first call in below returns an
1010         # InstancePCIRequests object which is a list of InstancePCIRequest
1011         # objects. The second call in below creates an InstancePCIRequest
1012         # object for each SR-IOV port, and append it to the list in the
1013         # InstancePCIRequests object
1014         pci_request_info = pci_request.get_pci_requests_from_flavor(
1015             instance_type, affinity_policy=pci_numa_affinity_policy)
1016         result = self.network_api.create_resource_requests(
1017             context, requested_networks, pci_request_info,
1018             affinity_policy=pci_numa_affinity_policy)
1019         network_metadata, port_resource_requests = result
1020 
1021         # Creating servers with ports that have resource requests, like QoS
1022         # minimum bandwidth rules, is only supported in a requested minimum
1023         # microversion.
1024         if port_resource_requests and not supports_port_resource_request:
1025             raise exception.CreateWithPortResourceRequestOldVersion()
1026 
1027         base_options = {
1028             'reservation_id': reservation_id,
1029             'image_ref': image_href,
1030             'kernel_id': kernel_id or '',
1031             'ramdisk_id': ramdisk_id or '',
1032             'power_state': power_state.NOSTATE,
1033             'vm_state': vm_states.BUILDING,
1034             'config_drive': config_drive,
1035             'user_id': context.user_id,
1036             'project_id': context.project_id,
1037             'instance_type_id': instance_type['id'],
1038             'memory_mb': instance_type['memory_mb'],
1039             'vcpus': instance_type['vcpus'],
1040             'root_gb': instance_type['root_gb'],
1041             'ephemeral_gb': instance_type['ephemeral_gb'],
1042             'display_name': display_name,
1043             'display_description': display_description,
1044             'user_data': user_data,
1045             'key_name': key_name,
1046             'key_data': key_data,
1047             'locked': False,
1048             'metadata': metadata or {},
1049             'access_ip_v4': access_ip_v4,
1050             'access_ip_v6': access_ip_v6,
1051             'availability_zone': availability_zone,
1052             'root_device_name': root_device_name,
1053             'progress': 0,
1054             'pci_requests': pci_request_info,
1055             'numa_topology': numa_topology,
1056             'system_metadata': system_metadata,
1057             'port_resource_requests': port_resource_requests}
1058 
1059         options_from_image = self._inherit_properties_from_image(
1060                 boot_meta, auto_disk_config)
1061 
1062         base_options.update(options_from_image)
1063 
1064         # return the validated options and maximum number of instances allowed
1065         # by the network quotas
1066         return (base_options, max_network_count, key_pair, security_groups,
1067                 network_metadata)
1068 
1069     @staticmethod
1070     @db_api.api_context_manager.writer
1071     def _create_reqspec_buildreq_instmapping(context, rs, br, im):
1072         """Create the request spec, build request, and instance mapping in a
1073         single database transaction.
1074 
1075         The RequestContext must be passed in to this method so that the
1076         database transaction context manager decorator will nest properly and
1077         include each create() into the same transaction context.
1078         """
1079         rs.create()
1080         br.create()
1081         im.create()
1082 
1083     def _validate_host_or_node(self, context, host, hypervisor_hostname):
1084         """Check whether compute nodes exist by validating the host
1085         and/or the hypervisor_hostname. There are three cases:
1086         1. If only host is supplied, we can lookup the HostMapping in
1087         the API DB.
1088         2. If only node is supplied, we can query a resource provider
1089         with that name in placement.
1090         3. If both host and node are supplied, we can get the cell from
1091         HostMapping and from that lookup the ComputeNode with the
1092         given cell.
1093 
1094         :param context: The API request context.
1095         :param host: Target host.
1096         :param hypervisor_hostname: Target node.
1097         :raises: ComputeHostNotFound if we find no compute nodes with host
1098                  and/or hypervisor_hostname.
1099         """
1100 
1101         if host:
1102             # When host is specified.
1103             try:
1104                 host_mapping = objects.HostMapping.get_by_host(context, host)
1105             except exception.HostMappingNotFound:
1106                 LOG.warning('No host-to-cell mapping found for host '
1107                             '%(host)s.', {'host': host})
1108                 raise exception.ComputeHostNotFound(host=host)
1109             # When both host and node are specified.
1110             if hypervisor_hostname:
1111                 cell = host_mapping.cell_mapping
1112                 with nova_context.target_cell(context, cell) as cctxt:
1113                     # Here we only do an existence check, so we don't
1114                     # need to store the return value into a variable.
1115                     objects.ComputeNode.get_by_host_and_nodename(
1116                         cctxt, host, hypervisor_hostname)
1117         elif hypervisor_hostname:
1118             # When only node is specified.
1119             try:
1120                 self.placementclient.get_provider_by_name(
1121                     context, hypervisor_hostname)
1122             except exception.ResourceProviderNotFound:
1123                 raise exception.ComputeHostNotFound(host=hypervisor_hostname)
1124 
1125     def _get_volumes_for_bdms(self, context, bdms):
1126         """Get the pre-existing volumes from cinder for the list of BDMs.
1127 
1128         :param context: nova auth RequestContext
1129         :param bdms: BlockDeviceMappingList which has zero or more BDMs with
1130             a pre-existing volume_id specified.
1131         :return: dict, keyed by volume id, of volume dicts
1132         :raises: VolumeNotFound - if a given volume does not exist
1133         :raises: CinderConnectionFailed - if there are problems communicating
1134             with the cinder API
1135         :raises: Forbidden - if the user token does not have authority to see
1136             a volume
1137         """
1138         volumes = {}
1139         for bdm in bdms:
1140             if bdm.volume_id:
1141                 volumes[bdm.volume_id] = self.volume_api.get(
1142                     context, bdm.volume_id)
1143         return volumes
1144 
1145     @staticmethod
1146     def _validate_vol_az_for_create(instance_az, volumes):
1147         """Performs cross_az_attach validation for the instance and volumes.
1148 
1149         If [cinder]/cross_az_attach=True (default) this method is a no-op.
1150 
1151         If [cinder]/cross_az_attach=False, this method will validate that:
1152 
1153         1. All volumes are in the same availability zone.
1154         2. The volume AZ matches the instance AZ. If the instance is being
1155            created without a specific AZ (either via the user request or the
1156            [DEFAULT]/default_schedule_zone option), and the volume AZ matches
1157            [DEFAULT]/default_availability_zone for compute services, then the
1158            method returns the volume AZ so it can be set in the RequestSpec as
1159            if the user requested the zone explicitly.
1160 
1161         :param instance_az: Availability zone for the instance. In this case
1162             the host is not yet selected so the instance AZ value should come
1163             from one of the following cases:
1164 
1165             * The user requested availability zone.
1166             * [DEFAULT]/default_schedule_zone (defaults to None) if the request
1167               does not specify an AZ (see parse_availability_zone).
1168         :param volumes: iterable of dicts of cinder volumes to be attached to
1169             the server being created
1170         :returns: None or volume AZ to set in the RequestSpec for the instance
1171         :raises: MismatchVolumeAZException if the instance and volume AZ do
1172             not match
1173         """
1174         if CONF.cinder.cross_az_attach:
1175             return
1176 
1177         if not volumes:
1178             return
1179 
1180         # First make sure that all of the volumes are in the same zone.
1181         vol_zones = [vol['availability_zone'] for vol in volumes]
1182         if len(set(vol_zones)) > 1:
1183             msg = (_("Volumes are in different availability zones: %s")
1184                    % ','.join(vol_zones))
1185             raise exception.MismatchVolumeAZException(reason=msg)
1186 
1187         volume_az = vol_zones[0]
1188         # In this case the instance.host should not be set so the instance AZ
1189         # value should come from instance.availability_zone which will be one
1190         # of the following cases:
1191         # * The user requested availability zone.
1192         # * [DEFAULT]/default_schedule_zone (defaults to None) if the request
1193         #   does not specify an AZ (see parse_availability_zone).
1194 
1195         # If the instance is not being created with a specific AZ (the AZ is
1196         # input via the API create request *or* [DEFAULT]/default_schedule_zone
1197         # is not None), then check to see if we should use the default AZ
1198         # (which by default matches the default AZ in Cinder, i.e. 'nova').
1199         if instance_az is None:
1200             # Check if the volume AZ is the same as our default AZ for compute
1201             # hosts (nova) and if so, assume we are OK because the user did not
1202             # request an AZ and will get the same default. If the volume AZ is
1203             # not the same as our default, return the volume AZ so the caller
1204             # can put it into the request spec so the instance is scheduled
1205             # to the same zone as the volume. Note that we are paranoid about
1206             # the default here since both nova and cinder's default backend AZ
1207             # is "nova" and we do not want to pin the server to that AZ since
1208             # it's special, i.e. just like we tell users in the docs to not
1209             # specify availability_zone='nova' when creating a server since we
1210             # might not be able to migrate it later.
1211             if volume_az != CONF.default_availability_zone:
1212                 return volume_az  # indication to set in request spec
1213             # The volume AZ is the same as the default nova AZ so we will be OK
1214             return
1215 
1216         if instance_az != volume_az:
1217             msg = _("Server and volumes are not in the same availability "
1218                     "zone. Server is in: %(instance_az)s. Volumes are in: "
1219                     "%(volume_az)s") % {
1220                 'instance_az': instance_az, 'volume_az': volume_az}
1221             raise exception.MismatchVolumeAZException(reason=msg)
1222 
1223     def _provision_instances(self, context, instance_type, min_count,
1224             max_count, base_options, boot_meta, security_groups,
1225             block_device_mapping, shutdown_terminate,
1226             instance_group, check_server_group_quota, filter_properties,
1227             key_pair, tags, trusted_certs, supports_multiattach,
1228             network_metadata=None, requested_host=None,
1229             requested_hypervisor_hostname=None):
1230         # NOTE(boxiang): Check whether compute nodes exist by validating
1231         # the host and/or the hypervisor_hostname. Pass the destination
1232         # to the scheduler with host and/or hypervisor_hostname(node).
1233         destination = None
1234         if requested_host or requested_hypervisor_hostname:
1235             self._validate_host_or_node(context, requested_host,
1236                                         requested_hypervisor_hostname)
1237             destination = objects.Destination()
1238             if requested_host:
1239                 destination.host = requested_host
1240             destination.node = requested_hypervisor_hostname
1241         # Check quotas
1242         num_instances = compute_utils.check_num_instances_quota(
1243                 context, instance_type, min_count, max_count)
1244         security_groups = security_group_api.populate_security_groups(
1245                 security_groups)
1246         port_resource_requests = base_options.pop('port_resource_requests')
1247         instances_to_build = []
1248         # We could be iterating over several instances with several BDMs per
1249         # instance and those BDMs could be using a lot of the same images so
1250         # we want to cache the image API GET results for performance.
1251         image_cache = {}  # dict of image dicts keyed by image id
1252         # Before processing the list of instances get all of the requested
1253         # pre-existing volumes so we can do some validation here rather than
1254         # down in the bowels of _validate_bdm.
1255         volumes = self._get_volumes_for_bdms(context, block_device_mapping)
1256         volume_az = self._validate_vol_az_for_create(
1257             base_options['availability_zone'], volumes.values())
1258         if volume_az:
1259             # This means the instance is not being created in a specific zone
1260             # but needs to match the zone that the volumes are in so update
1261             # base_options to match the volume zone.
1262             base_options['availability_zone'] = volume_az
1263         LOG.debug("Going to run %s instances...", num_instances)
1264         extra_specs = instance_type.extra_specs
1265         dp_name = extra_specs.get('accel:device_profile')
1266         dp_request_groups = []
1267         if dp_name:
1268             dp_request_groups = cyborg.get_device_profile_request_groups(
1269                 context, dp_name)
1270         try:
1271             for i in range(num_instances):
1272                 # Create a uuid for the instance so we can store the
1273                 # RequestSpec before the instance is created.
1274                 instance_uuid = uuidutils.generate_uuid()
1275                 # Store the RequestSpec that will be used for scheduling.
1276                 req_spec = objects.RequestSpec.from_components(context,
1277                         instance_uuid, boot_meta, instance_type,
1278                         base_options['numa_topology'],
1279                         base_options['pci_requests'], filter_properties,
1280                         instance_group, base_options['availability_zone'],
1281                         security_groups=security_groups,
1282                         port_resource_requests=port_resource_requests)
1283 
1284                 if block_device_mapping:
1285                     # Record whether or not we are a BFV instance
1286                     root = block_device_mapping.root_bdm()
1287                     req_spec.is_bfv = bool(root and root.is_volume)
1288                 else:
1289                     # If we have no BDMs, we're clearly not BFV
1290                     req_spec.is_bfv = False
1291 
1292                 # NOTE(danms): We need to record num_instances on the request
1293                 # spec as this is how the conductor knows how many were in this
1294                 # batch.
1295                 req_spec.num_instances = num_instances
1296 
1297                 # NOTE(stephenfin): The network_metadata field is not persisted
1298                 # inside RequestSpec object.
1299                 if network_metadata:
1300                     req_spec.network_metadata = network_metadata
1301 
1302                 if destination:
1303                     req_spec.requested_destination = destination
1304 
1305                 if dp_request_groups:
1306                     req_spec.requested_resources.extend(dp_request_groups)
1307 
1308                 # Create an instance object, but do not store in db yet.
1309                 instance = objects.Instance(context=context)
1310                 instance.uuid = instance_uuid
1311                 instance.update(base_options)
1312                 instance.keypairs = objects.KeyPairList(objects=[])
1313                 if key_pair:
1314                     instance.keypairs.objects.append(key_pair)
1315 
1316                 instance.trusted_certs = self._retrieve_trusted_certs_object(
1317                     context, trusted_certs)
1318 
1319                 instance = self.create_db_entry_for_new_instance(context,
1320                         instance_type, boot_meta, instance, security_groups,
1321                         block_device_mapping, num_instances, i,
1322                         shutdown_terminate, create_instance=False)
1323                 block_device_mapping = (
1324                     self._bdm_validate_set_size_and_instance(context,
1325                         instance, instance_type, block_device_mapping,
1326                         image_cache, volumes, supports_multiattach))
1327                 instance_tags = self._transform_tags(tags, instance.uuid)
1328 
1329                 build_request = objects.BuildRequest(context,
1330                         instance=instance, instance_uuid=instance.uuid,
1331                         project_id=instance.project_id,
1332                         block_device_mappings=block_device_mapping,
1333                         tags=instance_tags)
1334 
1335                 # Create an instance_mapping.  The null cell_mapping indicates
1336                 # that the instance doesn't yet exist in a cell, and lookups
1337                 # for it need to instead look for the RequestSpec.
1338                 # cell_mapping will be populated after scheduling, with a
1339                 # scheduling failure using the cell_mapping for the special
1340                 # cell0.
1341                 inst_mapping = objects.InstanceMapping(context=context)
1342                 inst_mapping.instance_uuid = instance_uuid
1343                 inst_mapping.project_id = context.project_id
1344                 inst_mapping.user_id = context.user_id
1345                 inst_mapping.cell_mapping = None
1346 
1347                 # Create the request spec, build request, and instance mapping
1348                 # records in a single transaction so that if a DBError is
1349                 # raised from any of them, all INSERTs will be rolled back and
1350                 # no orphaned records will be left behind.
1351                 self._create_reqspec_buildreq_instmapping(context, req_spec,
1352                                                           build_request,
1353                                                           inst_mapping)
1354 
1355                 instances_to_build.append(
1356                     (req_spec, build_request, inst_mapping))
1357 
1358                 if instance_group:
1359                     if check_server_group_quota:
1360                         try:
1361                             objects.Quotas.check_deltas(
1362                                 context, {'server_group_members': 1},
1363                                 instance_group, context.user_id)
1364                         except exception.OverQuota:
1365                             msg = _("Quota exceeded, too many servers in "
1366                                     "group")
1367                             raise exception.QuotaError(msg)
1368 
1369                     members = objects.InstanceGroup.add_members(
1370                         context, instance_group.uuid, [instance.uuid])
1371 
1372                     # NOTE(melwitt): We recheck the quota after creating the
1373                     # object to prevent users from allocating more resources
1374                     # than their allowed quota in the event of a race. This is
1375                     # configurable because it can be expensive if strict quota
1376                     # limits are not required in a deployment.
1377                     if CONF.quota.recheck_quota and check_server_group_quota:
1378                         try:
1379                             objects.Quotas.check_deltas(
1380                                 context, {'server_group_members': 0},
1381                                 instance_group, context.user_id)
1382                         except exception.OverQuota:
1383                             objects.InstanceGroup._remove_members_in_db(
1384                                 context, instance_group.id, [instance.uuid])
1385                             msg = _("Quota exceeded, too many servers in "
1386                                     "group")
1387                             raise exception.QuotaError(msg)
1388                     # list of members added to servers group in this iteration
1389                     # is needed to check quota of server group during add next
1390                     # instance
1391                     instance_group.members.extend(members)
1392 
1393         # In the case of any exceptions, attempt DB cleanup
1394         except Exception:
1395             with excutils.save_and_reraise_exception():
1396                 self._cleanup_build_artifacts(None, instances_to_build)
1397 
1398         return instances_to_build
1399 
1400     @staticmethod
1401     def _retrieve_trusted_certs_object(context, trusted_certs, rebuild=False):
1402         """Convert user-requested trusted cert IDs to TrustedCerts object
1403 
1404         Also validates that the deployment is new enough to support trusted
1405         image certification validation.
1406 
1407         :param context: The user request auth context
1408         :param trusted_certs: list of user-specified trusted cert string IDs,
1409             may be None
1410         :param rebuild: True if rebuilding the server, False if creating a
1411             new server
1412         :returns: nova.objects.TrustedCerts object or None if no user-specified
1413             trusted cert IDs were given and nova is not configured with
1414             default trusted cert IDs
1415         """
1416         # Retrieve trusted_certs parameter, or use CONF value if certificate
1417         # validation is enabled
1418         if trusted_certs:
1419             certs_to_return = objects.TrustedCerts(ids=trusted_certs)
1420         elif (CONF.glance.verify_glance_signatures and
1421               CONF.glance.enable_certificate_validation and
1422               CONF.glance.default_trusted_certificate_ids):
1423             certs_to_return = objects.TrustedCerts(
1424                 ids=CONF.glance.default_trusted_certificate_ids)
1425         else:
1426             return None
1427 
1428         return certs_to_return
1429 
1430     @staticmethod
1431     def _get_requested_instance_group(context, filter_properties):
1432         if (not filter_properties or
1433                 not filter_properties.get('scheduler_hints')):
1434             return
1435 
1436         group_hint = filter_properties.get('scheduler_hints').get('group')
1437         if not group_hint:
1438             return
1439 
1440         return objects.InstanceGroup.get_by_uuid(context, group_hint)
1441 
1442     def _create_instance(self, context, instance_type,
1443                image_href, kernel_id, ramdisk_id,
1444                min_count, max_count,
1445                display_name, display_description,
1446                key_name, key_data, security_groups,
1447                availability_zone, user_data, metadata, injected_files,
1448                admin_password, access_ip_v4, access_ip_v6,
1449                requested_networks, config_drive,
1450                block_device_mapping, auto_disk_config, filter_properties,
1451                reservation_id=None, legacy_bdm=True, shutdown_terminate=False,
1452                check_server_group_quota=False, tags=None,
1453                supports_multiattach=False, trusted_certs=None,
1454                supports_port_resource_request=False,
1455                requested_host=None, requested_hypervisor_hostname=None):
1456         """Verify all the input parameters regardless of the provisioning
1457         strategy being performed and schedule the instance(s) for
1458         creation.
1459         """
1460 
1461         # Normalize and setup some parameters
1462         if reservation_id is None:
1463             reservation_id = utils.generate_uid('r')
1464         security_groups = security_groups or ['default']
1465         min_count = min_count or 1
1466         max_count = max_count or min_count
1467         block_device_mapping = block_device_mapping or []
1468         tags = tags or []
1469 
1470         if image_href:
1471             image_id, boot_meta = self._get_image(context, image_href)
1472         else:
1473             # This is similar to the logic in _retrieve_trusted_certs_object.
1474             if (trusted_certs or
1475                 (CONF.glance.verify_glance_signatures and
1476                  CONF.glance.enable_certificate_validation and
1477                  CONF.glance.default_trusted_certificate_ids)):
1478                 msg = _("Image certificate validation is not supported "
1479                         "when booting from volume")
1480                 raise exception.CertificateValidationFailed(message=msg)
1481             image_id = None
1482             boot_meta = block_device.get_bdm_image_metadata(
1483                 context, self.image_api, self.volume_api, block_device_mapping,
1484                 legacy_bdm)
1485 
1486         self._check_auto_disk_config(image=boot_meta,
1487                                      auto_disk_config=auto_disk_config)
1488 
1489         base_options, max_net_count, key_pair, security_groups, \
1490             network_metadata = self._validate_and_build_base_options(
1491                     context, instance_type, boot_meta, image_href, image_id,
1492                     kernel_id, ramdisk_id, display_name, display_description,
1493                     key_name, key_data, security_groups, availability_zone,
1494                     user_data, metadata, access_ip_v4, access_ip_v6,
1495                     requested_networks, config_drive, auto_disk_config,
1496                     reservation_id, max_count, supports_port_resource_request)
1497 
1498         # TODO(huaqiang): Remove in Wallaby
1499         # check nova-compute nodes have been updated to Victoria to support the
1500         # mixed CPU policy for creating a new instance.
1501         numa_topology = base_options.get('numa_topology')
1502         self._check_compute_service_for_mixed_instance(numa_topology)
1503 
1504         # max_net_count is the maximum number of instances requested by the
1505         # user adjusted for any network quota constraints, including
1506         # consideration of connections to each requested network
1507         if max_net_count < min_count:
1508             raise exception.PortLimitExceeded()
1509         elif max_net_count < max_count:
1510             LOG.info("max count reduced from %(max_count)d to "
1511                      "%(max_net_count)d due to network port quota",
1512                      {'max_count': max_count,
1513                       'max_net_count': max_net_count})
1514             max_count = max_net_count
1515 
1516         block_device_mapping = self._check_and_transform_bdm(context,
1517             base_options, instance_type, boot_meta, min_count, max_count,
1518             block_device_mapping, legacy_bdm)
1519 
1520         # We can't do this check earlier because we need bdms from all sources
1521         # to have been merged in order to get the root bdm.
1522         # Set validate_numa=False since numa validation is already done by
1523         # _validate_and_build_base_options().
1524         self._checks_for_create_and_rebuild(context, image_id, boot_meta,
1525                 instance_type, metadata, injected_files,
1526                 block_device_mapping.root_bdm(), validate_numa=False)
1527 
1528         instance_group = self._get_requested_instance_group(context,
1529                                    filter_properties)
1530 
1531         tags = self._create_tag_list_obj(context, tags)
1532 
1533         instances_to_build = self._provision_instances(
1534             context, instance_type, min_count, max_count, base_options,
1535             boot_meta, security_groups, block_device_mapping,
1536             shutdown_terminate, instance_group, check_server_group_quota,
1537             filter_properties, key_pair, tags, trusted_certs,
1538             supports_multiattach, network_metadata,
1539             requested_host, requested_hypervisor_hostname)
1540 
1541         instances = []
1542         request_specs = []
1543         build_requests = []
1544         for rs, build_request, im in instances_to_build:
1545             build_requests.append(build_request)
1546             instance = build_request.get_new_instance(context)
1547             instances.append(instance)
1548             request_specs.append(rs)
1549 
1550         self.compute_task_api.schedule_and_build_instances(
1551             context,
1552             build_requests=build_requests,
1553             request_spec=request_specs,
1554             image=boot_meta,
1555             admin_password=admin_password,
1556             injected_files=injected_files,
1557             requested_networks=requested_networks,
1558             block_device_mapping=block_device_mapping,
1559             tags=tags)
1560 
1561         return instances, reservation_id
1562 
1563     @staticmethod
1564     def _cleanup_build_artifacts(instances, instances_to_build):
1565         # instances_to_build is a list of tuples:
1566         # (RequestSpec, BuildRequest, InstanceMapping)
1567 
1568         # Be paranoid about artifacts being deleted underneath us.
1569         for instance in instances or []:
1570             try:
1571                 instance.destroy()
1572             except exception.InstanceNotFound:
1573                 pass
1574         for rs, build_request, im in instances_to_build or []:
1575             try:
1576                 rs.destroy()
1577             except exception.RequestSpecNotFound:
1578                 pass
1579             try:
1580                 build_request.destroy()
1581             except exception.BuildRequestNotFound:
1582                 pass
1583             try:
1584                 im.destroy()
1585             except exception.InstanceMappingNotFound:
1586                 pass
1587 
1588     @staticmethod
1589     def _volume_size(instance_type, bdm):
1590         size = bdm.get('volume_size')
1591         # NOTE (ndipanov): inherit flavor size only for swap and ephemeral
1592         if (size is None and bdm.get('source_type') == 'blank' and
1593                 bdm.get('destination_type') == 'local'):
1594             if bdm.get('guest_format') == 'swap':
1595                 size = instance_type.get('swap', 0)
1596             else:
1597                 size = instance_type.get('ephemeral_gb', 0)
1598         return size
1599 
1600     def _prepare_image_mapping(self, instance_type, mappings):
1601         """Extract and format blank devices from image mappings."""
1602 
1603         prepared_mappings = []
1604 
1605         for bdm in block_device.mappings_prepend_dev(mappings):
1606             LOG.debug("Image bdm %s", bdm)
1607 
1608             virtual_name = bdm['virtual']
1609             if virtual_name == 'ami' or virtual_name == 'root':
1610                 continue
1611 
1612             if not block_device.is_swap_or_ephemeral(virtual_name):
1613                 continue
1614 
1615             guest_format = bdm.get('guest_format')
1616             if virtual_name == 'swap':
1617                 guest_format = 'swap'
1618             if not guest_format:
1619                 guest_format = CONF.default_ephemeral_format
1620 
1621             values = block_device.BlockDeviceDict({
1622                 'device_name': bdm['device'],
1623                 'source_type': 'blank',
1624                 'destination_type': 'local',
1625                 'device_type': 'disk',
1626                 'guest_format': guest_format,
1627                 'delete_on_termination': True,
1628                 'boot_index': -1})
1629 
1630             values['volume_size'] = self._volume_size(
1631                 instance_type, values)
1632             if values['volume_size'] == 0:
1633                 continue
1634 
1635             prepared_mappings.append(values)
1636 
1637         return prepared_mappings
1638 
1639     def _bdm_validate_set_size_and_instance(self, context, instance,
1640                                             instance_type,
1641                                             block_device_mapping,
1642                                             image_cache, volumes,
1643                                             supports_multiattach=False):
1644         """Ensure the bdms are valid, then set size and associate with instance
1645 
1646         Because this method can be called multiple times when more than one
1647         instance is booted in a single request it makes a copy of the bdm list.
1648 
1649         :param context: nova auth RequestContext
1650         :param instance: Instance object
1651         :param instance_type: Flavor object - used for swap and ephemeral BDMs
1652         :param block_device_mapping: BlockDeviceMappingList object
1653         :param image_cache: dict of image dicts keyed by id which is used as a
1654             cache in case there are multiple BDMs in the same request using
1655             the same image to avoid redundant GET calls to the image service
1656         :param volumes: dict, keyed by volume id, of volume dicts from cinder
1657         :param supports_multiattach: True if the request supports multiattach
1658             volumes, False otherwise
1659         """
1660         LOG.debug("block_device_mapping %s", list(block_device_mapping),
1661                   instance_uuid=instance.uuid)
1662         self._validate_bdm(
1663             context, instance, instance_type, block_device_mapping,
1664             image_cache, volumes, supports_multiattach)
1665         instance_block_device_mapping = block_device_mapping.obj_clone()
1666         for bdm in instance_block_device_mapping:
1667             bdm.volume_size = self._volume_size(instance_type, bdm)
1668             bdm.instance_uuid = instance.uuid
1669         return instance_block_device_mapping
1670 
1671     @staticmethod
1672     def _check_requested_volume_type(bdm, volume_type_id_or_name,
1673                                      volume_types):
1674         """If we are specifying a volume type, we need to get the
1675         volume type details from Cinder and make sure the ``volume_type``
1676         is available.
1677         """
1678 
1679         # NOTE(brinzhang): Verify that the specified volume type exists.
1680         # And save the volume type name internally for consistency in the
1681         # BlockDeviceMapping object.
1682         for vol_type in volume_types:
1683             if (volume_type_id_or_name == vol_type['id'] or
1684                         volume_type_id_or_name == vol_type['name']):
1685                 bdm.volume_type = vol_type['name']
1686                 break
1687         else:
1688             raise exception.VolumeTypeNotFound(
1689                 id_or_name=volume_type_id_or_name)
1690 
1691     def _validate_bdm(self, context, instance, instance_type,
1692                       block_device_mappings, image_cache, volumes,
1693                       supports_multiattach=False):
1694         """Validate requested block device mappings.
1695 
1696         :param context: nova auth RequestContext
1697         :param instance: Instance object
1698         :param instance_type: Flavor object - used for swap and ephemeral BDMs
1699         :param block_device_mappings: BlockDeviceMappingList object
1700         :param image_cache: dict of image dicts keyed by id which is used as a
1701             cache in case there are multiple BDMs in the same request using
1702             the same image to avoid redundant GET calls to the image service
1703         :param volumes: dict, keyed by volume id, of volume dicts from cinder
1704         :param supports_multiattach: True if the request supports multiattach
1705             volumes, False otherwise
1706         """
1707         # Make sure that the boot indexes make sense.
1708         # Setting a negative value or None indicates that the device should not
1709         # be used for booting.
1710         boot_indexes = sorted([bdm.boot_index
1711                                for bdm in block_device_mappings
1712                                if bdm.boot_index is not None and
1713                                bdm.boot_index >= 0])
1714 
1715         # Each device which is capable of being used as boot device should
1716         # be given a unique boot index, starting from 0 in ascending order, and
1717         # there needs to be at least one boot device.
1718         if not boot_indexes or any(i != v for i, v in enumerate(boot_indexes)):
1719             # Convert the BlockDeviceMappingList to a list for repr details.
1720             LOG.debug('Invalid block device mapping boot sequence for '
1721                       'instance: %s', list(block_device_mappings),
1722                       instance=instance)
1723             raise exception.InvalidBDMBootSequence()
1724 
1725         volume_types = None
1726         for bdm in block_device_mappings:
1727             volume_type = bdm.volume_type
1728             if volume_type:
1729                 if not volume_types:
1730                     # In order to reduce the number of hit cinder APIs,
1731                     # initialize our cache of volume types.
1732                     volume_types = self.volume_api.get_all_volume_types(
1733                         context)
1734                 # NOTE(brinzhang): Ensure the validity of volume_type.
1735                 self._check_requested_volume_type(bdm, volume_type,
1736                                                   volume_types)
1737 
1738             # NOTE(vish): For now, just make sure the volumes are accessible.
1739             # Additionally, check that the volume can be attached to this
1740             # instance.
1741             snapshot_id = bdm.snapshot_id
1742             volume_id = bdm.volume_id
1743             image_id = bdm.image_id
1744             if image_id is not None:
1745                 if (image_id != instance.get('image_ref') and
1746                         image_id not in image_cache):
1747                     try:
1748                         # Cache the results of the image GET so we do not make
1749                         # the same request for the same image if processing
1750                         # multiple BDMs or multiple servers with the same image
1751                         image_cache[image_id] = self._get_image(
1752                             context, image_id)
1753                     except Exception:
1754                         raise exception.InvalidBDMImage(id=image_id)
1755                 if (bdm.source_type == 'image' and
1756                         bdm.destination_type == 'volume' and
1757                         not bdm.volume_size):
1758                     raise exception.InvalidBDM(message=_("Images with "
1759                         "destination_type 'volume' need to have a non-zero "
1760                         "size specified"))
1761             elif volume_id is not None:
1762                 try:
1763                     volume = volumes[volume_id]
1764                     # We do not validate the instance and volume AZ here
1765                     # because that is done earlier by _provision_instances.
1766                     self._check_attach_and_reserve_volume(
1767                         context, volume, instance, bdm, supports_multiattach,
1768                         validate_az=False)
1769                     bdm.volume_size = volume.get('size')
1770                 except (exception.CinderConnectionFailed,
1771                         exception.InvalidVolume,
1772                         exception.MultiattachNotSupportedOldMicroversion):
1773                     raise
1774                 except exception.InvalidInput as exc:
1775                     raise exception.InvalidVolume(reason=exc.format_message())
1776                 except Exception as e:
1777                     LOG.info('Failed validating volume %s. Error: %s',
1778                              volume_id, e)
1779                     raise exception.InvalidBDMVolume(id=volume_id)
1780             elif snapshot_id is not None:
1781                 try:
1782                     snap = self.volume_api.get_snapshot(context, snapshot_id)
1783                     bdm.volume_size = bdm.volume_size or snap.get('size')
1784                 except exception.CinderConnectionFailed:
1785                     raise
1786                 except Exception:
1787                     raise exception.InvalidBDMSnapshot(id=snapshot_id)
1788             elif (bdm.source_type == 'blank' and
1789                     bdm.destination_type == 'volume' and
1790                     not bdm.volume_size):
1791                 raise exception.InvalidBDM(message=_("Blank volumes "
1792                     "(source: 'blank', dest: 'volume') need to have non-zero "
1793                     "size"))
1794 
1795             # NOTE(lyarwood): Ensure the disk_bus is at least known to Nova.
1796             # The virt driver may reject this later but for now just ensure
1797             # it's listed as an acceptable value of the DiskBus field class.
1798             disk_bus = bdm.disk_bus if 'disk_bus' in bdm else None
1799             if disk_bus and disk_bus not in fields_obj.DiskBus.ALL:
1800                 raise exception.InvalidBDMDiskBus(disk_bus=disk_bus)
1801 
1802         ephemeral_size = sum(bdm.volume_size or instance_type['ephemeral_gb']
1803                 for bdm in block_device_mappings
1804                 if block_device.new_format_is_ephemeral(bdm))
1805         if ephemeral_size > instance_type['ephemeral_gb']:
1806             raise exception.InvalidBDMEphemeralSize()
1807 
1808         # There should be only one swap
1809         swap_list = block_device.get_bdm_swap_list(block_device_mappings)
1810         if len(swap_list) > 1:
1811             msg = _("More than one swap drive requested.")
1812             raise exception.InvalidBDMFormat(details=msg)
1813 
1814         if swap_list:
1815             swap_size = swap_list[0].volume_size or 0
1816             if swap_size > instance_type['swap']:
1817                 raise exception.InvalidBDMSwapSize()
1818 
1819         max_local = CONF.max_local_block_devices
1820         if max_local >= 0:
1821             num_local = len([bdm for bdm in block_device_mappings
1822                              if bdm.destination_type == 'local'])
1823             if num_local > max_local:
1824                 raise exception.InvalidBDMLocalsLimit()
1825 
1826     def _populate_instance_names(self, instance, num_instances, index):
1827         """Populate instance display_name and hostname.
1828 
1829         :param instance: The instance to set the display_name, hostname for
1830         :type instance: nova.objects.Instance
1831         :param num_instances: Total number of instances being created in this
1832             request
1833         :param index: The 0-based index of this particular instance
1834         """
1835         # NOTE(mriedem): This is only here for test simplicity since a server
1836         # name is required in the REST API.
1837         if 'display_name' not in instance or instance.display_name is None:
1838             instance.display_name = 'Server %s' % instance.uuid
1839 
1840         # if we're booting multiple instances, we need to add an indexing
1841         # suffix to both instance.hostname and instance.display_name. This is
1842         # not necessary for a single instance.
1843         if num_instances == 1:
1844             default_hostname = 'Server-%s' % instance.uuid
1845             instance.hostname = utils.sanitize_hostname(
1846                 instance.display_name, default_hostname)
1847         elif num_instances > 1:
1848             old_display_name = instance.display_name
1849             new_display_name = '%s-%d' % (old_display_name, index + 1)
1850 
1851             if utils.sanitize_hostname(old_display_name) == "":
1852                 instance.hostname = 'Server-%s' % instance.uuid
1853             else:
1854                 instance.hostname = utils.sanitize_hostname(
1855                     new_display_name)
1856 
1857             instance.display_name = new_display_name
1858 
1859     def _populate_instance_for_create(self, context, instance, image,
1860                                       index, security_groups, instance_type,
1861                                       num_instances, shutdown_terminate):
1862         """Build the beginning of a new instance."""
1863 
1864         instance.launch_index = index
1865         instance.vm_state = vm_states.BUILDING
1866         instance.task_state = task_states.SCHEDULING
1867         info_cache = objects.InstanceInfoCache()
1868         info_cache.instance_uuid = instance.uuid
1869         info_cache.network_info = network_model.NetworkInfo()
1870         instance.info_cache = info_cache
1871         instance.flavor = instance_type
1872         instance.old_flavor = None
1873         instance.new_flavor = None
1874         if CONF.ephemeral_storage_encryption.enabled:
1875             # NOTE(kfarr): dm-crypt expects the cipher in a
1876             # hyphenated format: cipher-chainmode-ivmode
1877             # (ex: aes-xts-plain64). The algorithm needs
1878             # to be parsed out to pass to the key manager (ex: aes).
1879             cipher = CONF.ephemeral_storage_encryption.cipher
1880             algorithm = cipher.split('-')[0] if cipher else None
1881             instance.ephemeral_key_uuid = self.key_manager.create_key(
1882                 context,
1883                 algorithm=algorithm,
1884                 length=CONF.ephemeral_storage_encryption.key_size)
1885         else:
1886             instance.ephemeral_key_uuid = None
1887 
1888         # Store image properties so we can use them later
1889         # (for notifications, etc).  Only store what we can.
1890         if not instance.obj_attr_is_set('system_metadata'):
1891             instance.system_metadata = {}
1892         # Make sure we have the dict form that we need for instance_update.
1893         instance.system_metadata = utils.instance_sys_meta(instance)
1894 
1895         system_meta = utils.get_system_metadata_from_image(
1896             image, instance_type)
1897 
1898         # In case we couldn't find any suitable base_image
1899         system_meta.setdefault('image_base_image_ref', instance.image_ref)
1900 
1901         system_meta['owner_user_name'] = context.user_name
1902         system_meta['owner_project_name'] = context.project_name
1903 
1904         instance.system_metadata.update(system_meta)
1905 
1906         # Since the removal of nova-network, we don't actually store anything
1907         # in the database. Instead, we proxy the security groups on the
1908         # instance from the ports attached to the instance.
1909         instance.security_groups = objects.SecurityGroupList()
1910 
1911         self._populate_instance_names(instance, num_instances, index)
1912         instance.shutdown_terminate = shutdown_terminate
1913 
1914         return instance
1915 
1916     def _create_tag_list_obj(self, context, tags):
1917         """Create TagList objects from simple string tags.
1918 
1919         :param context: security context.
1920         :param tags: simple string tags from API request.
1921         :returns: TagList object.
1922         """
1923         tag_list = [objects.Tag(context=context, tag=t) for t in tags]
1924         tag_list_obj = objects.TagList(objects=tag_list)
1925         return tag_list_obj
1926 
1927     def _transform_tags(self, tags, resource_id):
1928         """Change the resource_id of the tags according to the input param.
1929 
1930         Because this method can be called multiple times when more than one
1931         instance is booted in a single request it makes a copy of the tags
1932         list.
1933 
1934         :param tags: TagList object.
1935         :param resource_id: string.
1936         :returns: TagList object.
1937         """
1938         instance_tags = tags.obj_clone()
1939         for tag in instance_tags:
1940             tag.resource_id = resource_id
1941         return instance_tags
1942 
1943     # This method remains because cellsv1 uses it in the scheduler
1944     def create_db_entry_for_new_instance(self, context, instance_type, image,
1945             instance, security_group, block_device_mapping, num_instances,
1946             index, shutdown_terminate=False, create_instance=True):
1947         """Create an entry in the DB for this new instance,
1948         including any related table updates (such as security group,
1949         etc).
1950 
1951         This is called by the scheduler after a location for the
1952         instance has been determined.
1953 
1954         :param create_instance: Determines if the instance is created here or
1955             just populated for later creation. This is done so that this code
1956             can be shared with cellsv1 which needs the instance creation to
1957             happen here. It should be removed and this method cleaned up when
1958             cellsv1 is a distant memory.
1959         """
1960         self._populate_instance_for_create(context, instance, image, index,
1961                                            security_group, instance_type,
1962                                            num_instances, shutdown_terminate)
1963 
1964         if create_instance:
1965             instance.create()
1966 
1967         return instance
1968 
1969     def _check_multiple_instances_with_neutron_ports(self,
1970                                                      requested_networks):
1971         """Check whether multiple instances are created from port id(s)."""
1972         for requested_net in requested_networks:
1973             if requested_net.port_id:
1974                 msg = _("Unable to launch multiple instances with"
1975                         " a single configured port ID. Please launch your"
1976                         " instance one by one with different ports.")
1977                 raise exception.MultiplePortsNotApplicable(reason=msg)
1978 
1979     def _check_multiple_instances_with_specified_ip(self, requested_networks):
1980         """Check whether multiple instances are created with specified ip."""
1981 
1982         for requested_net in requested_networks:
1983             if requested_net.network_id and requested_net.address:
1984                 msg = _("max_count cannot be greater than 1 if an fixed_ip "
1985                         "is specified.")
1986                 raise exception.InvalidFixedIpAndMaxCountRequest(reason=msg)
1987 
1988     def create(self, context, instance_type,
1989                image_href, kernel_id=None, ramdisk_id=None,
1990                min_count=None, max_count=None,
1991                display_name=None, display_description=None,
1992                key_name=None, key_data=None, security_groups=None,
1993                availability_zone=None, forced_host=None, forced_node=None,
1994                user_data=None, metadata=None, injected_files=None,
1995                admin_password=None, block_device_mapping=None,
1996                access_ip_v4=None, access_ip_v6=None, requested_networks=None,
1997                config_drive=None, auto_disk_config=None, scheduler_hints=None,
1998                legacy_bdm=True, shutdown_terminate=False,
1999                check_server_group_quota=False, tags=None,
2000                supports_multiattach=False, trusted_certs=None,
2001                supports_port_resource_request=False,
2002                requested_host=None, requested_hypervisor_hostname=None):
2003         """Provision instances, sending instance information to the
2004         scheduler.  The scheduler will determine where the instance(s)
2005         go and will handle creating the DB entries.
2006 
2007         Returns a tuple of (instances, reservation_id)
2008         """
2009         if requested_networks and max_count is not None and max_count > 1:
2010             self._check_multiple_instances_with_specified_ip(
2011                 requested_networks)
2012             self._check_multiple_instances_with_neutron_ports(
2013                 requested_networks)
2014 
2015         if availability_zone:
2016             available_zones = availability_zones.\
2017                 get_availability_zones(context.elevated(), self.host_api,
2018                                        get_only_available=True)
2019             if forced_host is None and availability_zone not in \
2020                     available_zones:
2021                 msg = _('The requested availability zone is not available')
2022                 raise exception.InvalidRequest(msg)
2023 
2024         filter_properties = scheduler_utils.build_filter_properties(
2025                 scheduler_hints, forced_host, forced_node, instance_type)
2026 
2027         return self._create_instance(
2028             context, instance_type,
2029             image_href, kernel_id, ramdisk_id,
2030             min_count, max_count,
2031             display_name, display_description,
2032             key_name, key_data, security_groups,
2033             availability_zone, user_data, metadata,
2034             injected_files, admin_password,
2035             access_ip_v4, access_ip_v6,
2036             requested_networks, config_drive,
2037             block_device_mapping, auto_disk_config,
2038             filter_properties=filter_properties,
2039             legacy_bdm=legacy_bdm,
2040             shutdown_terminate=shutdown_terminate,
2041             check_server_group_quota=check_server_group_quota,
2042             tags=tags, supports_multiattach=supports_multiattach,
2043             trusted_certs=trusted_certs,
2044             supports_port_resource_request=supports_port_resource_request,
2045             requested_host=requested_host,
2046             requested_hypervisor_hostname=requested_hypervisor_hostname)
2047 
2048     def _check_auto_disk_config(self, instance=None, image=None,
2049                                 auto_disk_config=None):
2050         if auto_disk_config is None:
2051             return
2052         if not image and not instance:
2053             return
2054 
2055         if image:
2056             image_props = image.get("properties", {})
2057             auto_disk_config_img = \
2058                 utils.get_auto_disk_config_from_image_props(image_props)
2059             image_ref = image.get("id")
2060         else:
2061             sys_meta = utils.instance_sys_meta(instance)
2062             image_ref = sys_meta.get('image_base_image_ref')
2063             auto_disk_config_img = \
2064                 utils.get_auto_disk_config_from_instance(sys_meta=sys_meta)
2065 
2066         self._ensure_auto_disk_config_is_valid(auto_disk_config_img,
2067                                                auto_disk_config,
2068                                                image_ref)
2069 
2070     def _lookup_instance(self, context, uuid):
2071         '''Helper method for pulling an instance object from a database.
2072 
2073         During the transition to cellsv2 there is some complexity around
2074         retrieving an instance from the database which this method hides. If
2075         there is an instance mapping then query the cell for the instance, if
2076         no mapping exists then query the configured nova database.
2077 
2078         Once we are past the point that all deployments can be assumed to be
2079         migrated to cellsv2 this method can go away.
2080         '''
2081         inst_map = None
2082         try:
2083             inst_map = objects.InstanceMapping.get_by_instance_uuid(
2084                 context, uuid)
2085         except exception.InstanceMappingNotFound:
2086             # TODO(alaski): This exception block can be removed once we're
2087             # guaranteed everyone is using cellsv2.
2088             pass
2089 
2090         if inst_map is None or inst_map.cell_mapping is None:
2091             # If inst_map is None then the deployment has not migrated to
2092             # cellsv2 yet.
2093             # If inst_map.cell_mapping is None then the instance is not in a
2094             # cell yet. Until instance creation moves to the conductor the
2095             # instance can be found in the configured database, so attempt
2096             # to look it up.
2097             cell = None
2098             try:
2099                 instance = objects.Instance.get_by_uuid(context, uuid)
2100             except exception.InstanceNotFound:
2101                 # If we get here then the conductor is in charge of writing the
2102                 # instance to the database and hasn't done that yet. It's up to
2103                 # the caller of this method to determine what to do with that
2104                 # information.
2105                 return None, None
2106         else:
2107             cell = inst_map.cell_mapping
2108             with nova_context.target_cell(context, cell) as cctxt:
2109                 try:
2110                     instance = objects.Instance.get_by_uuid(cctxt, uuid)
2111                 except exception.InstanceNotFound:
2112                     # Since the cell_mapping exists we know the instance is in
2113                     # the cell, however InstanceNotFound means it's already
2114                     # deleted.
2115                     return None, None
2116         return cell, instance
2117 
2118     def _delete_while_booting(self, context, instance):
2119         """Handle deletion if the instance has not reached a cell yet
2120 
2121         Deletion before an instance reaches a cell needs to be handled
2122         differently. What we're attempting to do is delete the BuildRequest
2123         before the api level conductor does.  If we succeed here then the boot
2124         request stops before reaching a cell.  If not then the instance will
2125         need to be looked up in a cell db and the normal delete path taken.
2126         """
2127         deleted = self._attempt_delete_of_buildrequest(context, instance)
2128         if deleted:
2129             # If we've reached this block the successful deletion of the
2130             # buildrequest indicates that the build process should be halted by
2131             # the conductor.
2132 
2133             # NOTE(alaski): Though the conductor halts the build process it
2134             # does not currently delete the instance record. This is
2135             # because in the near future the instance record will not be
2136             # created if the buildrequest has been deleted here. For now we
2137             # ensure the instance has been set to deleted at this point.
2138             # Yes this directly contradicts the comment earlier in this
2139             # method, but this is a temporary measure.
2140             # Look up the instance because the current instance object was
2141             # stashed on the buildrequest and therefore not complete enough
2142             # to run .destroy().
2143             try:
2144                 instance_uuid = instance.uuid
2145                 cell, instance = self._lookup_instance(context, instance_uuid)
2146                 if instance is not None:
2147                     # If instance is None it has already been deleted.
2148                     if cell:
2149                         with nova_context.target_cell(context, cell) as cctxt:
2150                             # FIXME: When the instance context is targeted,
2151                             # we can remove this
2152                             with compute_utils.notify_about_instance_delete(
2153                                     self.notifier, cctxt, instance):
2154                                 instance.destroy()
2155                     else:
2156                         instance.destroy()
2157             except exception.InstanceNotFound:
2158                 pass
2159 
2160             return True
2161         return False
2162 
2163     def _local_delete_cleanup(self, context, instance):
2164         # NOTE(aarents) Ensure instance allocation is cleared and instance
2165         # mapping queued as deleted before _delete() return
2166         try:
2167             self.placementclient.delete_allocation_for_instance(
2168                 context, instance.uuid)
2169         except exception.AllocationDeleteFailed:
2170             LOG.info("Allocation delete failed during local delete cleanup.",
2171                      instance=instance)
2172 
2173         try:
2174             self._update_queued_for_deletion(context, instance, True)
2175         except exception.InstanceMappingNotFound:
2176             LOG.info("Instance Mapping does not exist while attempting "
2177                      "local delete cleanup.",
2178                      instance=instance)
2179 
2180     def _attempt_delete_of_buildrequest(self, context, instance):
2181         # If there is a BuildRequest then the instance may not have been
2182         # written to a cell db yet. Delete the BuildRequest here, which
2183         # will indicate that the Instance build should not proceed.
2184         try:
2185             build_req = objects.BuildRequest.get_by_instance_uuid(
2186                 context, instance.uuid)
2187             build_req.destroy()
2188         except exception.BuildRequestNotFound:
2189             # This means that conductor has deleted the BuildRequest so the
2190             # instance is now in a cell and the delete needs to proceed
2191             # normally.
2192             return False
2193 
2194         # We need to detach from any volumes so they aren't orphaned.
2195         self._local_cleanup_bdm_volumes(
2196             build_req.block_device_mappings, instance, context)
2197 
2198         return True
2199 
2200     def _delete(self, context, instance, delete_type, cb, **instance_attrs):
2201         if instance.disable_terminate:
2202             LOG.info('instance termination disabled', instance=instance)
2203             return
2204 
2205         cell = None
2206         # If there is an instance.host (or the instance is shelved-offloaded or
2207         # in error state), the instance has been scheduled and sent to a
2208         # cell/compute which means it was pulled from the cell db.
2209         # Normal delete should be attempted.
2210         may_have_ports_or_volumes = compute_utils.may_have_ports_or_volumes(
2211             instance)
2212         if not instance.host and not may_have_ports_or_volumes:
2213             try:
2214                 if self._delete_while_booting(context, instance):
2215                     self._local_delete_cleanup(context, instance)
2216                     return
2217                 # If instance.host was not set it's possible that the Instance
2218                 # object here was pulled from a BuildRequest object and is not
2219                 # fully populated. Notably it will be missing an 'id' field
2220                 # which will prevent instance.destroy from functioning
2221                 # properly. A lookup is attempted which will either return a
2222                 # full Instance or None if not found. If not found then it's
2223                 # acceptable to skip the rest of the delete processing.
2224                 cell, instance = self._lookup_instance(context, instance.uuid)
2225                 if cell and instance:
2226                     try:
2227                         # Now destroy the instance from the cell it lives in.
2228                         with compute_utils.notify_about_instance_delete(
2229                                 self.notifier, context, instance):
2230                             instance.destroy()
2231                     except exception.InstanceNotFound:
2232                         pass
2233                     # The instance was deleted or is already gone.
2234                     self._local_delete_cleanup(context, instance)
2235                     return
2236                 if not instance:
2237                     # Instance is already deleted.
2238                     self._local_delete_cleanup(context, instance)
2239                     return
2240             except exception.ObjectActionError:
2241                 # NOTE(melwitt): This means the instance.host changed
2242                 # under us indicating the instance became scheduled
2243                 # during the destroy(). Refresh the instance from the DB and
2244                 # continue on with the delete logic for a scheduled instance.
2245                 # NOTE(danms): If instance.host is set, we should be able to
2246                 # do the following lookup. If not, there's not much we can
2247                 # do to recover.
2248                 cell, instance = self._lookup_instance(context, instance.uuid)
2249                 if not instance:
2250                     # Instance is already deleted
2251                     self._local_delete_cleanup(context, instance)
2252                     return
2253 
2254         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
2255                 context, instance.uuid)
2256 
2257         # At these states an instance has a snapshot associate.
2258         if instance.vm_state in (vm_states.SHELVED,
2259                                  vm_states.SHELVED_OFFLOADED):
2260             snapshot_id = instance.system_metadata.get('shelved_image_id')
2261             LOG.info("Working on deleting snapshot %s "
2262                      "from shelved instance...",
2263                      snapshot_id, instance=instance)
2264             try:
2265                 self.image_api.delete(context, snapshot_id)
2266             except (exception.ImageNotFound,
2267                     exception.ImageNotAuthorized) as exc:
2268                 LOG.warning("Failed to delete snapshot "
2269                             "from shelved instance (%s).",
2270                             exc.format_message(), instance=instance)
2271             except Exception:
2272                 LOG.exception("Something wrong happened when trying to "
2273                               "delete snapshot from shelved instance.",
2274                               instance=instance)
2275 
2276         original_task_state = instance.task_state
2277         try:
2278             # NOTE(maoy): no expected_task_state needs to be set
2279             instance.update(instance_attrs)
2280             instance.progress = 0
2281             instance.save()
2282 
2283             if not instance.host and not may_have_ports_or_volumes:
2284                 try:
2285                     with compute_utils.notify_about_instance_delete(
2286                             self.notifier, context, instance,
2287                             delete_type
2288                             if delete_type != 'soft_delete'
2289                             else 'delete'):
2290                         instance.destroy()
2291                     LOG.info('Instance deleted and does not have host '
2292                              'field, its vm_state is %(state)s.',
2293                              {'state': instance.vm_state},
2294                               instance=instance)
2295                     self._local_delete_cleanup(context, instance)
2296                     return
2297                 except exception.ObjectActionError as ex:
2298                     # The instance's host likely changed under us as
2299                     # this instance could be building and has since been
2300                     # scheduled. Continue with attempts to delete it.
2301                     LOG.debug('Refreshing instance because: %s', ex,
2302                               instance=instance)
2303                     instance.refresh()
2304 
2305             if instance.vm_state == vm_states.RESIZED:
2306                 self._confirm_resize_on_deleting(context, instance)
2307                 # NOTE(neha_alhat): After confirm resize vm_state will become
2308                 # 'active' and task_state will be set to 'None'. But for soft
2309                 # deleting a vm, the _do_soft_delete callback requires
2310                 # task_state in 'SOFT_DELETING' status. So, we need to set
2311                 # task_state as 'SOFT_DELETING' again for soft_delete case.
2312                 # After confirm resize and before saving the task_state to
2313                 # "SOFT_DELETING", during the short window, user can submit
2314                 # soft delete vm request again and system will accept and
2315                 # process it without any errors.
2316                 if delete_type == 'soft_delete':
2317                     instance.task_state = instance_attrs['task_state']
2318                     instance.save()
2319 
2320             is_local_delete = True
2321             try:
2322                 # instance.host must be set in order to look up the service.
2323                 if instance.host is not None:
2324                     service = objects.Service.get_by_compute_host(
2325                         context.elevated(), instance.host)
2326                     is_local_delete = not self.servicegroup_api.service_is_up(
2327                         service)
2328                 if not is_local_delete:
2329                     if original_task_state in (task_states.DELETING,
2330                                                   task_states.SOFT_DELETING):
2331                         LOG.info('Instance is already in deleting state, '
2332                                  'ignoring this request',
2333                                  instance=instance)
2334                         return
2335                     self._record_action_start(context, instance,
2336                                               instance_actions.DELETE)
2337 
2338                     cb(context, instance, bdms)
2339             except exception.ComputeHostNotFound:
2340                 LOG.debug('Compute host %s not found during service up check, '
2341                           'going to local delete instance', instance.host,
2342                           instance=instance)
2343 
2344             if is_local_delete:
2345                 # If instance is in shelved_offloaded state or compute node
2346                 # isn't up, delete instance from db and clean bdms info and
2347                 # network info
2348                 if cell is None:
2349                     # NOTE(danms): If we didn't get our cell from one of the
2350                     # paths above, look it up now.
2351                     try:
2352                         im = objects.InstanceMapping.get_by_instance_uuid(
2353                             context, instance.uuid)
2354                         cell = im.cell_mapping
2355                     except exception.InstanceMappingNotFound:
2356                         LOG.warning('During local delete, failed to find '
2357                                     'instance mapping', instance=instance)
2358                         return
2359 
2360                 LOG.debug('Doing local delete in cell %s', cell.identity,
2361                           instance=instance)
2362                 with nova_context.target_cell(context, cell) as cctxt:
2363                     self._local_delete(cctxt, instance, bdms, delete_type, cb)
2364 
2365         except exception.InstanceNotFound:
2366             # NOTE(comstud): Race condition. Instance already gone.
2367             pass
2368 
2369     def _confirm_resize_on_deleting(self, context, instance):
2370         # If in the middle of a resize, use confirm_resize to
2371         # ensure the original instance is cleaned up too along
2372         # with its allocations (and migration-based allocations)
2373         # in placement.
2374         migration = None
2375         for status in ('finished', 'confirming'):
2376             try:
2377                 migration = objects.Migration.get_by_instance_and_status(
2378                         context.elevated(), instance.uuid, status)
2379                 LOG.info('Found an unconfirmed migration during delete, '
2380                          'id: %(id)s, status: %(status)s',
2381                          {'id': migration.id,
2382                           'status': migration.status},
2383                          instance=instance)
2384                 break
2385             except exception.MigrationNotFoundByStatus:
2386                 pass
2387 
2388         if not migration:
2389             LOG.info('Instance may have been confirmed during delete',
2390                      instance=instance)
2391             return
2392 
2393         self._record_action_start(context, instance,
2394                                   instance_actions.CONFIRM_RESIZE)
2395 
2396         # If migration.cross_cell_move, we need to also cleanup the instance
2397         # data from the source cell database.
2398         if migration.cross_cell_move:
2399             self.compute_task_api.confirm_snapshot_based_resize(
2400                 context, instance, migration, do_cast=False)
2401         else:
2402             self.compute_rpcapi.confirm_resize(context,
2403                     instance, migration, migration.source_compute, cast=False)
2404 
2405     def _local_cleanup_bdm_volumes(self, bdms, instance, context):
2406         """The method deletes the bdm records and, if a bdm is a volume, call
2407         the terminate connection and the detach volume via the Volume API.
2408         """
2409         elevated = context.elevated()
2410         for bdm in bdms:
2411             if bdm.is_volume:
2412                 try:
2413                     if bdm.attachment_id:
2414                         self.volume_api.attachment_delete(context,
2415                                                           bdm.attachment_id)
2416                     else:
2417                         connector = compute_utils.get_stashed_volume_connector(
2418                             bdm, instance)
2419                         if connector:
2420                             self.volume_api.terminate_connection(context,
2421                                                                  bdm.volume_id,
2422                                                                  connector)
2423                         else:
2424                             LOG.debug('Unable to find connector for volume %s,'
2425                                       ' not attempting terminate_connection.',
2426                                       bdm.volume_id, instance=instance)
2427                         # Attempt to detach the volume. If there was no
2428                         # connection made in the first place this is just
2429                         # cleaning up the volume state in the Cinder DB.
2430                         self.volume_api.detach(elevated, bdm.volume_id,
2431                                                instance.uuid)
2432 
2433                     if bdm.delete_on_termination:
2434                         self.volume_api.delete(context, bdm.volume_id)
2435                 except Exception as exc:
2436                     LOG.warning("Ignoring volume cleanup failure due to %s",
2437                                 exc, instance=instance)
2438             # If we're cleaning up volumes from an instance that wasn't yet
2439             # created in a cell, i.e. the user deleted the server while
2440             # the BuildRequest still existed, then the BDM doesn't actually
2441             # exist in the DB to destroy it.
2442             if 'id' in bdm:
2443                 bdm.destroy()
2444 
2445     @property
2446     def placementclient(self):
2447         if self._placementclient is None:
2448             self._placementclient = report.SchedulerReportClient()
2449         return self._placementclient
2450 
2451     def _local_delete(self, context, instance, bdms, delete_type, cb):
2452         if instance.vm_state == vm_states.SHELVED_OFFLOADED:
2453             LOG.info("instance is in SHELVED_OFFLOADED state, cleanup"
2454                      " the instance's info from database.",
2455                      instance=instance)
2456         else:
2457             LOG.warning("instance's host %s is down, deleting from "
2458                         "database", instance.host, instance=instance)
2459         with compute_utils.notify_about_instance_delete(
2460                 self.notifier, context, instance,
2461                 delete_type if delete_type != 'soft_delete' else 'delete'):
2462 
2463             elevated = context.elevated()
2464             self.network_api.deallocate_for_instance(elevated, instance)
2465 
2466             # cleanup volumes
2467             self._local_cleanup_bdm_volumes(bdms, instance, context)
2468 
2469             # cleanup accelerator requests (ARQs)
2470             compute_utils.delete_arqs_if_needed(context, instance)
2471 
2472             # Cleanup allocations in Placement since we can't do it from the
2473             # compute service.
2474             self.placementclient.delete_allocation_for_instance(
2475                 context, instance.uuid)
2476             cb(context, instance, bdms, local=True)
2477             instance.destroy()
2478 
2479     @staticmethod
2480     def _update_queued_for_deletion(context, instance, qfd):
2481         # NOTE(tssurya): We query the instance_mapping record of this instance
2482         # and update the queued_for_delete flag to True (or False according to
2483         # the state of the instance). This just means that the instance is
2484         # queued for deletion (or is no longer queued for deletion). It does
2485         # not guarantee its successful deletion (or restoration). Hence the
2486         # value could be stale which is fine, considering its use is only
2487         # during down cell (desperate) situation.
2488         im = objects.InstanceMapping.get_by_instance_uuid(context,
2489                                                           instance.uuid)
2490         im.queued_for_delete = qfd
2491         im.save()
2492 
2493     def _do_delete(self, context, instance, bdms, local=False):
2494         if local:
2495             instance.vm_state = vm_states.DELETED
2496             instance.task_state = None
2497             instance.terminated_at = timeutils.utcnow()
2498             instance.save()
2499         else:
2500             self.compute_rpcapi.terminate_instance(context, instance, bdms)
2501         self._update_queued_for_deletion(context, instance, True)
2502 
2503     def _do_soft_delete(self, context, instance, bdms, local=False):
2504         if local:
2505             instance.vm_state = vm_states.SOFT_DELETED
2506             instance.task_state = None
2507             instance.terminated_at = timeutils.utcnow()
2508             instance.save()
2509         else:
2510             self.compute_rpcapi.soft_delete_instance(context, instance)
2511         self._update_queued_for_deletion(context, instance, True)
2512 
2513     # NOTE(maoy): we allow delete to be called no matter what vm_state says.
2514     @check_instance_lock
2515     @check_instance_state(vm_state=None, task_state=None,
2516                           must_have_launched=True)
2517     def soft_delete(self, context, instance):
2518         """Terminate an instance."""
2519         LOG.debug('Going to try to soft delete instance',
2520                   instance=instance)
2521 
2522         self._delete(context, instance, 'soft_delete', self._do_soft_delete,
2523                      task_state=task_states.SOFT_DELETING,
2524                      deleted_at=timeutils.utcnow())
2525 
2526     def _delete_instance(self, context, instance):
2527         self._delete(context, instance, 'delete', self._do_delete,
2528                      task_state=task_states.DELETING)
2529 
2530     @check_instance_lock
2531     @check_instance_state(vm_state=None, task_state=None,
2532                           must_have_launched=False)
2533     def delete(self, context, instance):
2534         """Terminate an instance."""
2535         LOG.debug("Going to try to terminate instance", instance=instance)
2536         self._delete_instance(context, instance)
2537 
2538     @check_instance_lock
2539     @check_instance_state(vm_state=[vm_states.SOFT_DELETED])
2540     def restore(self, context, instance):
2541         """Restore a previously deleted (but not reclaimed) instance."""
2542         # Check quotas
2543         flavor = instance.get_flavor()
2544         project_id, user_id = quotas_obj.ids_from_instance(context, instance)
2545         compute_utils.check_num_instances_quota(context, flavor, 1, 1,
2546                 project_id=project_id, user_id=user_id)
2547 
2548         self._record_action_start(context, instance, instance_actions.RESTORE)
2549 
2550         if instance.host:
2551             instance.task_state = task_states.RESTORING
2552             instance.deleted_at = None
2553             instance.save(expected_task_state=[None])
2554             # TODO(melwitt): We're not rechecking for strict quota here to
2555             # guard against going over quota during a race at this time because
2556             # the resource consumption for this operation is written to the
2557             # database by compute.
2558             self.compute_rpcapi.restore_instance(context, instance)
2559         else:
2560             instance.vm_state = vm_states.ACTIVE
2561             instance.task_state = None
2562             instance.deleted_at = None
2563             instance.save(expected_task_state=[None])
2564         self._update_queued_for_deletion(context, instance, False)
2565 
2566     @check_instance_lock
2567     @check_instance_state(task_state=None,
2568                           must_have_launched=False)
2569     def force_delete(self, context, instance):
2570         """Force delete an instance in any vm_state/task_state."""
2571         self._delete(context, instance, 'force_delete', self._do_delete,
2572                      task_state=task_states.DELETING)
2573 
2574     def force_stop(self, context, instance, do_cast=True, clean_shutdown=True):
2575         LOG.debug("Going to try to stop instance", instance=instance)
2576 
2577         instance.task_state = task_states.POWERING_OFF
2578         instance.progress = 0
2579         instance.save(expected_task_state=[None])
2580 
2581         self._record_action_start(context, instance, instance_actions.STOP)
2582 
2583         self.compute_rpcapi.stop_instance(context, instance, do_cast=do_cast,
2584                                           clean_shutdown=clean_shutdown)
2585 
2586     @check_instance_lock
2587     @check_instance_host()
2588     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.ERROR])
2589     def stop(self, context, instance, do_cast=True, clean_shutdown=True):
2590         """Stop an instance."""
2591         self.force_stop(context, instance, do_cast, clean_shutdown)
2592 
2593     @check_instance_lock
2594     @check_instance_host()
2595     @check_instance_state(vm_state=[vm_states.STOPPED])
2596     def start(self, context, instance):
2597         """Start an instance."""
2598         LOG.debug("Going to try to start instance", instance=instance)
2599 
2600         instance.task_state = task_states.POWERING_ON
2601         instance.save(expected_task_state=[None])
2602 
2603         self._record_action_start(context, instance, instance_actions.START)
2604         self.compute_rpcapi.start_instance(context, instance)
2605 
2606     @check_instance_lock
2607     @check_instance_host()
2608     @check_instance_state(vm_state=vm_states.ALLOW_TRIGGER_CRASH_DUMP)
2609     def trigger_crash_dump(self, context, instance):
2610         """Trigger crash dump in an instance."""
2611         LOG.debug("Try to trigger crash dump", instance=instance)
2612 
2613         self._record_action_start(context, instance,
2614                                   instance_actions.TRIGGER_CRASH_DUMP)
2615 
2616         self.compute_rpcapi.trigger_crash_dump(context, instance)
2617 
2618     def _generate_minimal_construct_for_down_cells(self, context,
2619                                                    down_cell_uuids,
2620                                                    project, limit):
2621         """Generate a list of minimal instance constructs for a given list of
2622         cells that did not respond to a list operation. This will list
2623         every instance mapping in the affected cells and return a minimal
2624         objects.Instance for each (non-queued-for-delete) mapping.
2625 
2626         :param context: RequestContext
2627         :param down_cell_uuids: A list of cell UUIDs that did not respond
2628         :param project: A project ID to filter mappings, or None
2629         :param limit: A numeric limit on the number of results, or None
2630         :returns: An InstanceList() of partial Instance() objects
2631         """
2632         unavailable_servers = objects.InstanceList()
2633         for cell_uuid in down_cell_uuids:
2634             LOG.warning("Cell %s is not responding and hence only "
2635                         "partial results are available from this "
2636                         "cell if any.", cell_uuid)
2637             instance_mappings = (objects.InstanceMappingList.
2638                 get_not_deleted_by_cell_and_project(context, cell_uuid,
2639                                                     project, limit=limit))
2640             for im in instance_mappings:
2641                 unavailable_servers.objects.append(
2642                     objects.Instance(
2643                         context=context,
2644                         uuid=im.instance_uuid,
2645                         project_id=im.project_id,
2646                         created_at=im.created_at
2647                     )
2648                 )
2649             if limit is not None:
2650                 limit -= len(instance_mappings)
2651                 if limit <= 0:
2652                     break
2653         return unavailable_servers
2654 
2655     def _get_instance_map_or_none(self, context, instance_uuid):
2656         try:
2657             inst_map = objects.InstanceMapping.get_by_instance_uuid(
2658                     context, instance_uuid)
2659         except exception.InstanceMappingNotFound:
2660             # InstanceMapping should always be found generally. This exception
2661             # may be raised if a deployment has partially migrated the nova-api
2662             # services.
2663             inst_map = None
2664         return inst_map
2665 
2666     @staticmethod
2667     def _save_user_id_in_instance_mapping(mapping, instance):
2668         # TODO(melwitt): We take the opportunity to migrate user_id on the
2669         # instance mapping if it's not yet been migrated. This can be removed
2670         # in a future release, when all migrations are complete.
2671         # If the instance came from a RequestSpec because of a down cell, its
2672         # user_id could be None and the InstanceMapping.user_id field is
2673         # non-nullable. Avoid trying to set/save the user_id in that case.
2674         if 'user_id' not in mapping and instance.user_id is not None:
2675             mapping.user_id = instance.user_id
2676             mapping.save()
2677 
2678     def _get_instance_from_cell(self, context, im, expected_attrs,
2679                                 cell_down_support):
2680         # NOTE(danms): Even though we're going to scatter/gather to the
2681         # right cell, other code depends on this being force targeted when
2682         # the get call returns.
2683         nova_context.set_target_cell(context, im.cell_mapping)
2684 
2685         uuid = im.instance_uuid
2686         result = nova_context.scatter_gather_single_cell(context,
2687             im.cell_mapping, objects.Instance.get_by_uuid, uuid,
2688             expected_attrs=expected_attrs)
2689         cell_uuid = im.cell_mapping.uuid
2690         if not nova_context.is_cell_failure_sentinel(result[cell_uuid]):
2691             inst = result[cell_uuid]
2692             self._save_user_id_in_instance_mapping(im, inst)
2693             return inst
2694         elif isinstance(result[cell_uuid], exception.InstanceNotFound):
2695             raise exception.InstanceNotFound(instance_id=uuid)
2696         elif cell_down_support:
2697             if im.queued_for_delete:
2698                 # should be treated like deleted instance.
2699                 raise exception.InstanceNotFound(instance_id=uuid)
2700 
2701             # instance in down cell, return a minimal construct
2702             LOG.warning("Cell %s is not responding and hence only "
2703                         "partial results are available from this "
2704                         "cell.", cell_uuid)
2705             try:
2706                 rs = objects.RequestSpec.get_by_instance_uuid(context,
2707                                                               uuid)
2708                 # For BFV case, we could have rs.image but rs.image.id might
2709                 # still not be set. So we check the existence of both image
2710                 # and its id.
2711                 image_ref = (rs.image.id if rs.image and
2712                              'id' in rs.image else None)
2713                 inst = objects.Instance(context=context, power_state=0,
2714                                         uuid=uuid,
2715                                         project_id=im.project_id,
2716                                         created_at=im.created_at,
2717                                         user_id=rs.user_id,
2718                                         flavor=rs.flavor,
2719                                         image_ref=image_ref,
2720                                         availability_zone=rs.availability_zone)
2721                 self._save_user_id_in_instance_mapping(im, inst)
2722                 return inst
2723             except exception.RequestSpecNotFound:
2724                 # could be that a deleted instance whose request
2725                 # spec has been archived is being queried.
2726                 raise exception.InstanceNotFound(instance_id=uuid)
2727         else:
2728             raise exception.NovaException(
2729                 _("Cell %s is not responding and hence instance "
2730                   "info is not available.") % cell_uuid)
2731 
2732     def _get_instance(self, context, instance_uuid, expected_attrs,
2733                       cell_down_support=False):
2734         inst_map = self._get_instance_map_or_none(context, instance_uuid)
2735         if inst_map and (inst_map.cell_mapping is not None):
2736             instance = self._get_instance_from_cell(context, inst_map,
2737                 expected_attrs, cell_down_support)
2738         elif inst_map and (inst_map.cell_mapping is None):
2739             # This means the instance has not been scheduled and put in
2740             # a cell yet. For now it also may mean that the deployer
2741             # has not created their cell(s) yet.
2742             try:
2743                 build_req = objects.BuildRequest.get_by_instance_uuid(
2744                         context, instance_uuid)
2745                 instance = build_req.instance
2746             except exception.BuildRequestNotFound:
2747                 # Instance was mapped and the BuildRequest was deleted
2748                 # while fetching. Try again.
2749                 inst_map = self._get_instance_map_or_none(context,
2750                                                           instance_uuid)
2751                 if inst_map and (inst_map.cell_mapping is not None):
2752                     instance = self._get_instance_from_cell(context, inst_map,
2753                         expected_attrs, cell_down_support)
2754                 else:
2755                     raise exception.InstanceNotFound(instance_id=instance_uuid)
2756         else:
2757             # If we got here, we don't have an instance mapping, but we aren't
2758             # sure why. The instance mapping might be missing because the
2759             # upgrade is incomplete (map_instances wasn't run). Or because the
2760             # instance was deleted and the DB was archived at which point the
2761             # mapping is deleted. The former case is bad, but because of the
2762             # latter case we can't really log any kind of warning/error here
2763             # since it might be normal.
2764             raise exception.InstanceNotFound(instance_id=instance_uuid)
2765 
2766         return instance
2767 
2768     def get(self, context, instance_id, expected_attrs=None,
2769             cell_down_support=False):
2770         """Get a single instance with the given instance_id.
2771 
2772         :param cell_down_support: True if the API (and caller) support
2773                                   returning a minimal instance
2774                                   construct if the relevant cell is
2775                                   down. If False, an error is raised
2776                                   since the instance cannot be retrieved
2777                                   due to the cell being down.
2778         """
2779         if not expected_attrs:
2780             expected_attrs = []
2781         expected_attrs.extend(['metadata', 'system_metadata',
2782                                'security_groups', 'info_cache'])
2783         # NOTE(ameade): we still need to support integer ids for ec2
2784         try:
2785             if uuidutils.is_uuid_like(instance_id):
2786                 LOG.debug("Fetching instance by UUID",
2787                            instance_uuid=instance_id)
2788 
2789                 instance = self._get_instance(context, instance_id,
2790                     expected_attrs, cell_down_support=cell_down_support)
2791             else:
2792                 LOG.debug("Failed to fetch instance by id %s", instance_id)
2793                 raise exception.InstanceNotFound(instance_id=instance_id)
2794         except exception.InvalidID:
2795             LOG.debug("Invalid instance id %s", instance_id)
2796             raise exception.InstanceNotFound(instance_id=instance_id)
2797 
2798         return instance
2799 
2800     def get_all(self, context, search_opts=None, limit=None, marker=None,
2801                 expected_attrs=None, sort_keys=None, sort_dirs=None,
2802                 cell_down_support=False, all_tenants=False):
2803         """Get all instances filtered by one of the given parameters.
2804 
2805         If there is no filter and the context is an admin, it will retrieve
2806         all instances in the system.
2807 
2808         Deleted instances will be returned by default, unless there is a
2809         search option that says otherwise.
2810 
2811         The results will be sorted based on the list of sort keys in the
2812         'sort_keys' parameter (first value is primary sort key, second value is
2813         secondary sort ket, etc.). For each sort key, the associated sort
2814         direction is based on the list of sort directions in the 'sort_dirs'
2815         parameter.
2816 
2817         :param cell_down_support: True if the API (and caller) support
2818                                   returning a minimal instance
2819                                   construct if the relevant cell is
2820                                   down. If False, instances from
2821                                   unreachable cells will be omitted.
2822         :param all_tenants: True if the "all_tenants" filter was passed.
2823 
2824         """
2825         if search_opts is None:
2826             search_opts = {}
2827 
2828         LOG.debug("Searching by: %s", str(search_opts))
2829 
2830         # Fixups for the DB call
2831         filters = {}
2832 
2833         def _remap_flavor_filter(flavor_id):
2834             flavor = objects.Flavor.get_by_flavor_id(context, flavor_id)
2835             filters['instance_type_id'] = flavor.id
2836 
2837         def _remap_fixed_ip_filter(fixed_ip):
2838             # Turn fixed_ip into a regexp match. Since '.' matches
2839             # any character, we need to use regexp escaping for it.
2840             filters['ip'] = '^%s$' % fixed_ip.replace('.', '\\.')
2841 
2842         # search_option to filter_name mapping.
2843         filter_mapping = {
2844                 'image': 'image_ref',
2845                 'name': 'display_name',
2846                 'tenant_id': 'project_id',
2847                 'flavor': _remap_flavor_filter,
2848                 'fixed_ip': _remap_fixed_ip_filter}
2849 
2850         # copy from search_opts, doing various remappings as necessary
2851         for opt, value in search_opts.items():
2852             # Do remappings.
2853             # Values not in the filter_mapping table are copied as-is.
2854             # If remapping is None, option is not copied
2855             # If the remapping is a string, it is the filter_name to use
2856             try:
2857                 remap_object = filter_mapping[opt]
2858             except KeyError:
2859                 filters[opt] = value
2860             else:
2861                 # Remaps are strings to translate to, or functions to call
2862                 # to do the translating as defined by the table above.
2863                 if isinstance(remap_object, str):
2864                     filters[remap_object] = value
2865                 else:
2866                     try:
2867                         remap_object(value)
2868 
2869                     # We already know we can't match the filter, so
2870                     # return an empty list
2871                     except ValueError:
2872                         return objects.InstanceList()
2873 
2874         # IP address filtering cannot be applied at the DB layer, remove any DB
2875         # limit so that it can be applied after the IP filter.
2876         filter_ip = 'ip6' in filters or 'ip' in filters
2877         skip_build_request = False
2878         orig_limit = limit
2879         if filter_ip:
2880             # We cannot skip build requests if there is a marker since the
2881             # the marker could be a build request.
2882             skip_build_request = marker is None
2883             if self.network_api.has_substr_port_filtering_extension(context):
2884                 # We're going to filter by IP using Neutron so set filter_ip
2885                 # to False so we don't attempt post-DB query filtering in
2886                 # memory below.
2887                 filter_ip = False
2888                 instance_uuids = self._ip_filter_using_neutron(context,
2889                                                                filters)
2890                 if instance_uuids:
2891                     # Note that 'uuid' is not in the 2.1 GET /servers query
2892                     # parameter schema, however, we allow additionalProperties
2893                     # so someone could filter instances by uuid, which doesn't
2894                     # make a lot of sense but we have to account for it.
2895                     if 'uuid' in filters and filters['uuid']:
2896                         filter_uuids = filters['uuid']
2897                         if isinstance(filter_uuids, list):
2898                             instance_uuids.extend(filter_uuids)
2899                         else:
2900                             # Assume a string. If it's a dict or tuple or
2901                             # something, well...that's too bad. This is why
2902                             # we have query parameter schema definitions.
2903                             if filter_uuids not in instance_uuids:
2904                                 instance_uuids.append(filter_uuids)
2905                     filters['uuid'] = instance_uuids
2906                 else:
2907                     # No matches on the ip filter(s), return an empty list.
2908                     return objects.InstanceList()
2909             elif limit:
2910                 LOG.debug('Removing limit for DB query due to IP filter')
2911                 limit = None
2912 
2913         # Skip get BuildRequest if filtering by IP address, as building
2914         # instances will not have IP addresses.
2915         if skip_build_request:
2916             build_requests = objects.BuildRequestList()
2917         else:
2918             # The ordering of instances will be
2919             # [sorted instances with no host] + [sorted instances with host].
2920             # This means BuildRequest and cell0 instances first, then cell
2921             # instances
2922             try:
2923                 build_requests = objects.BuildRequestList.get_by_filters(
2924                     context, filters, limit=limit, marker=marker,
2925                     sort_keys=sort_keys, sort_dirs=sort_dirs)
2926                 # If we found the marker in we need to set it to None
2927                 # so we don't expect to find it in the cells below.
2928                 marker = None
2929             except exception.MarkerNotFound:
2930                 # If we didn't find the marker in the build requests then keep
2931                 # looking for it in the cells.
2932                 build_requests = objects.BuildRequestList()
2933 
2934         build_req_instances = objects.InstanceList(
2935             objects=[build_req.instance for build_req in build_requests])
2936         # Only subtract from limit if it is not None
2937         limit = (limit - len(build_req_instances)) if limit else limit
2938 
2939         # We could arguably avoid joining on security_groups if we're using
2940         # neutron (which is the default) but if you're using neutron then the
2941         # security_group_instance_association table should be empty anyway
2942         # and the DB should optimize out that join, making it insignificant.
2943         fields = ['metadata', 'info_cache', 'security_groups']
2944         if expected_attrs:
2945             fields.extend(expected_attrs)
2946 
2947         insts, down_cell_uuids = instance_list.get_instance_objects_sorted(
2948             context, filters, limit, marker, fields, sort_keys, sort_dirs,
2949             cell_down_support=cell_down_support)
2950 
2951         def _get_unique_filter_method():
2952             seen_uuids = set()
2953 
2954             def _filter(instance):
2955                 # During a cross-cell move operation we could have the instance
2956                 # in more than one cell database so we not only have to filter
2957                 # duplicates but we want to make sure we only return the
2958                 # "current" one which should also be the one that the instance
2959                 # mapping points to, but we don't want to do that expensive
2960                 # lookup here. The DB API will filter out hidden instances by
2961                 # default but there is a small window where two copies of an
2962                 # instance could be hidden=False in separate cell DBs.
2963                 # NOTE(mriedem): We could make this better in the case that we
2964                 # have duplicate instances that are both hidden=False by
2965                 # showing the one with the newer updated_at value, but that
2966                 # could be tricky if the user is filtering on
2967                 # changes-since/before or updated_at, or sorting on updated_at,
2968                 # but technically that was already potentially broken with this
2969                 # _filter method if we return an older BuildRequest.instance,
2970                 # and given the window should be very small where we have
2971                 # duplicates, it's probably not worth the complexity.
2972                 if instance.uuid in seen_uuids:
2973                     return False
2974                 seen_uuids.add(instance.uuid)
2975                 return True
2976 
2977             return _filter
2978 
2979         filter_method = _get_unique_filter_method()
2980         # Only subtract from limit if it is not None
2981         limit = (limit - len(insts)) if limit else limit
2982         # TODO(alaski): Clean up the objects concatenation when List objects
2983         # support it natively.
2984         instances = objects.InstanceList(
2985             objects=list(filter(filter_method,
2986                            build_req_instances.objects +
2987                            insts.objects)))
2988 
2989         if filter_ip:
2990             instances = self._ip_filter(instances, filters, orig_limit)
2991 
2992         if cell_down_support:
2993             # API and client want minimal construct instances for any cells
2994             # that didn't return, so generate and prefix those to the actual
2995             # results.
2996             project = search_opts.get('project_id', context.project_id)
2997             if all_tenants:
2998                 # NOTE(tssurya): The only scenario where project has to be None
2999                 # is when using "all_tenants" in which case we do not want
3000                 # the query to be restricted based on the project_id.
3001                 project = None
3002             limit = (orig_limit - len(instances)) if limit else limit
3003             return (self._generate_minimal_construct_for_down_cells(context,
3004                 down_cell_uuids, project, limit) + instances)
3005 
3006         return instances
3007 
3008     @staticmethod
3009     def _ip_filter(inst_models, filters, limit):
3010         ipv4_f = re.compile(str(filters.get('ip')))
3011         ipv6_f = re.compile(str(filters.get('ip6')))
3012 
3013         def _match_instance(instance):
3014             nw_info = instance.get_network_info()
3015             for vif in nw_info:
3016                 for fixed_ip in vif.fixed_ips():
3017                     address = fixed_ip.get('address')
3018                     if not address:
3019                         continue
3020                     version = fixed_ip.get('version')
3021                     if ((version == 4 and ipv4_f.match(address)) or
3022                         (version == 6 and ipv6_f.match(address))):
3023                         return True
3024             return False
3025 
3026         result_objs = []
3027         for instance in inst_models:
3028             if _match_instance(instance):
3029                 result_objs.append(instance)
3030                 if limit and len(result_objs) == limit:
3031                     break
3032         return objects.InstanceList(objects=result_objs)
3033 
3034     def _ip_filter_using_neutron(self, context, filters):
3035         ip4_address = filters.get('ip')
3036         ip6_address = filters.get('ip6')
3037         addresses = [ip4_address, ip6_address]
3038         uuids = []
3039         for address in addresses:
3040             if address:
3041                 try:
3042                     ports = self.network_api.list_ports(
3043                         context, fixed_ips='ip_address_substr=' + address,
3044                         fields=['device_id'])['ports']
3045                     for port in ports:
3046                         uuids.append(port['device_id'])
3047                 except Exception as e:
3048                     LOG.error('An error occurred while listing ports '
3049                               'with an ip_address filter value of "%s". '
3050                               'Error: %s',
3051                               address, str(e))
3052         return uuids
3053 
3054     def update_instance(self, context, instance, updates):
3055         """Updates a single Instance object with some updates dict.
3056 
3057         Returns the updated instance.
3058         """
3059 
3060         # NOTE(sbauza): Given we only persist the Instance object after we
3061         # create the BuildRequest, we are sure that if the Instance object
3062         # has an ID field set, then it was persisted in the right Cell DB.
3063         if instance.obj_attr_is_set('id'):
3064             instance.update(updates)
3065             instance.save()
3066         else:
3067             # Instance is not yet mapped to a cell, so we need to update
3068             # BuildRequest instead
3069             # TODO(sbauza): Fix the possible race conditions where BuildRequest
3070             # could be deleted because of either a concurrent instance delete
3071             # or because the scheduler just returned a destination right
3072             # after we called the instance in the API.
3073             try:
3074                 build_req = objects.BuildRequest.get_by_instance_uuid(
3075                     context, instance.uuid)
3076                 instance = build_req.instance
3077                 instance.update(updates)
3078                 # FIXME(sbauza): Here we are updating the current
3079                 # thread-related BuildRequest object. Given that another worker
3080                 # could have looking up at that BuildRequest in the API, it
3081                 # means that it could pass it down to the conductor without
3082                 # making sure that it's not updated, we could have some race
3083                 # condition where it would missing the updated fields, but
3084                 # that's something we could discuss once the instance record
3085                 # is persisted by the conductor.
3086                 build_req.save()
3087             except exception.BuildRequestNotFound:
3088                 # Instance was mapped and the BuildRequest was deleted
3089                 # while fetching (and possibly the instance could have been
3090                 # deleted as well). We need to lookup again the Instance object
3091                 # in order to correctly update it.
3092                 # TODO(sbauza): Figure out a good way to know the expected
3093                 # attributes by checking which fields are set or not.
3094                 expected_attrs = ['flavor', 'pci_devices', 'numa_topology',
3095                                   'tags', 'metadata', 'system_metadata',
3096                                   'security_groups', 'info_cache']
3097                 inst_map = self._get_instance_map_or_none(context,
3098                                                           instance.uuid)
3099                 if inst_map and (inst_map.cell_mapping is not None):
3100                     with nova_context.target_cell(
3101                             context,
3102                             inst_map.cell_mapping) as cctxt:
3103                         instance = objects.Instance.get_by_uuid(
3104                             cctxt, instance.uuid,
3105                             expected_attrs=expected_attrs)
3106                         instance.update(updates)
3107                         instance.save()
3108                 else:
3109                     # Conductor doesn't delete the BuildRequest until after the
3110                     # InstanceMapping record is created, so if we didn't get
3111                     # that and the BuildRequest doesn't exist, then the
3112                     # instance is already gone and we need to just error out.
3113                     raise exception.InstanceNotFound(instance_id=instance.uuid)
3114         return instance
3115 
3116     # NOTE(melwitt): We don't check instance lock for backup because lock is
3117     #                intended to prevent accidental change/delete of instances
3118     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
3119                                     vm_states.PAUSED, vm_states.SUSPENDED])
3120     def backup(self, context, instance, name, backup_type, rotation,
3121                extra_properties=None):
3122         """Backup the given instance
3123 
3124         :param instance: nova.objects.instance.Instance object
3125         :param name: name of the backup
3126         :param backup_type: 'daily' or 'weekly'
3127         :param rotation: int representing how many backups to keep around;
3128             None if rotation shouldn't be used (as in the case of snapshots)
3129         :param extra_properties: dict of extra image properties to include
3130                                  when creating the image.
3131         :returns: A dict containing image metadata
3132         """
3133         props_copy = dict(extra_properties, backup_type=backup_type)
3134 
3135         if compute_utils.is_volume_backed_instance(context, instance):
3136             LOG.info("It's not supported to backup volume backed "
3137                      "instance.", instance=instance)
3138             raise exception.InvalidRequest(
3139                 _('Backup is not supported for volume-backed instances.'))
3140         else:
3141             image_meta = compute_utils.create_image(
3142                 context, instance, name, 'backup', self.image_api,
3143                 extra_properties=props_copy)
3144 
3145         instance.task_state = task_states.IMAGE_BACKUP
3146         instance.save(expected_task_state=[None])
3147 
3148         self._record_action_start(context, instance,
3149                                   instance_actions.BACKUP)
3150 
3151         self.compute_rpcapi.backup_instance(context, instance,
3152                                             image_meta['id'],
3153                                             backup_type,
3154                                             rotation)
3155         return image_meta
3156 
3157     # NOTE(melwitt): We don't check instance lock for snapshot because lock is
3158     #                intended to prevent accidental change/delete of instances
3159     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
3160                                     vm_states.PAUSED, vm_states.SUSPENDED])
3161     def snapshot(self, context, instance, name, extra_properties=None):
3162         """Snapshot the given instance.
3163 
3164         :param instance: nova.objects.instance.Instance object
3165         :param name: name of the snapshot
3166         :param extra_properties: dict of extra image properties to include
3167                                  when creating the image.
3168         :returns: A dict containing image metadata
3169         """
3170         image_meta = compute_utils.create_image(
3171             context, instance, name, 'snapshot', self.image_api,
3172             extra_properties=extra_properties)
3173 
3174         instance.task_state = task_states.IMAGE_SNAPSHOT_PENDING
3175         try:
3176             instance.save(expected_task_state=[None])
3177         except (exception.InstanceNotFound,
3178                 exception.UnexpectedDeletingTaskStateError) as ex:
3179             # Changing the instance task state to use in raising the
3180             # InstanceInvalidException below
3181             LOG.debug('Instance disappeared during snapshot.',
3182                       instance=instance)
3183             try:
3184                 image_id = image_meta['id']
3185                 self.image_api.delete(context, image_id)
3186                 LOG.info('Image %s deleted because instance '
3187                          'deleted before snapshot started.',
3188                          image_id, instance=instance)
3189             except exception.ImageNotFound:
3190                 pass
3191             except Exception as exc:
3192                 LOG.warning("Error while trying to clean up image %(img_id)s: "
3193                             "%(error_msg)s",
3194                             {"img_id": image_meta['id'],
3195                              "error_msg": str(exc)})
3196             attr = 'task_state'
3197             state = task_states.DELETING
3198             if type(ex) == exception.InstanceNotFound:
3199                 attr = 'vm_state'
3200                 state = vm_states.DELETED
3201             raise exception.InstanceInvalidState(attr=attr,
3202                                            instance_uuid=instance.uuid,
3203                                            state=state,
3204                                            method='snapshot')
3205 
3206         self._record_action_start(context, instance,
3207                                   instance_actions.CREATE_IMAGE)
3208 
3209         self.compute_rpcapi.snapshot_instance(context, instance,
3210                                               image_meta['id'])
3211 
3212         return image_meta
3213 
3214     # NOTE(melwitt): We don't check instance lock for snapshot because lock is
3215     #                intended to prevent accidental change/delete of instances
3216     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
3217                                     vm_states.PAUSED, vm_states.SUSPENDED])
3218     def snapshot_volume_backed(self, context, instance, name,
3219                                extra_properties=None):
3220         """Snapshot the given volume-backed instance.
3221 
3222         :param instance: nova.objects.instance.Instance object
3223         :param name: name of the backup or snapshot
3224         :param extra_properties: dict of extra image properties to include
3225 
3226         :returns: the new image metadata
3227         """
3228         image_meta = compute_utils.initialize_instance_snapshot_metadata(
3229             context, instance, name, extra_properties)
3230         # the new image is simply a bucket of properties (particularly the
3231         # block device mapping, kernel and ramdisk IDs) with no image data,
3232         # hence the zero size
3233         image_meta['size'] = 0
3234         for attr in ('container_format', 'disk_format'):
3235             image_meta.pop(attr, None)
3236         properties = image_meta['properties']
3237         # clean properties before filling
3238         for key in ('block_device_mapping', 'bdm_v2', 'root_device_name'):
3239             properties.pop(key, None)
3240         if instance.root_device_name:
3241             properties['root_device_name'] = instance.root_device_name
3242 
3243         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
3244                 context, instance.uuid)
3245 
3246         mapping = []  # list of BDM dicts that can go into the image properties
3247         # Do some up-front filtering of the list of BDMs from
3248         # which we are going to create snapshots.
3249         volume_bdms = []
3250         for bdm in bdms:
3251             if bdm.no_device:
3252                 continue
3253             if bdm.is_volume:
3254                 # These will be handled below.
3255                 volume_bdms.append(bdm)
3256             else:
3257                 mapping.append(bdm.get_image_mapping())
3258 
3259         # Check limits in Cinder before creating snapshots to avoid going over
3260         # quota in the middle of a list of volumes. This is a best-effort check
3261         # but concurrently running snapshot requests from the same project
3262         # could still fail to create volume snapshots if they go over limit.
3263         if volume_bdms:
3264             limits = self.volume_api.get_absolute_limits(context)
3265             total_snapshots_used = limits['totalSnapshotsUsed']
3266             max_snapshots = limits['maxTotalSnapshots']
3267             # -1 means there is unlimited quota for snapshots
3268             if (max_snapshots > -1 and
3269                     len(volume_bdms) + total_snapshots_used > max_snapshots):
3270                 LOG.debug('Unable to create volume snapshots for instance. '
3271                           'Currently has %s snapshots, requesting %s new '
3272                           'snapshots, with a limit of %s.',
3273                           total_snapshots_used, len(volume_bdms),
3274                           max_snapshots, instance=instance)
3275                 raise exception.OverQuota(overs='snapshots')
3276 
3277         quiesced = False
3278         if instance.vm_state == vm_states.ACTIVE:
3279             try:
3280                 LOG.info("Attempting to quiesce instance before volume "
3281                          "snapshot.", instance=instance)
3282                 self.compute_rpcapi.quiesce_instance(context, instance)
3283                 quiesced = True
3284             except (exception.InstanceQuiesceNotSupported,
3285                     exception.QemuGuestAgentNotEnabled,
3286                     exception.NovaException, NotImplementedError) as err:
3287                 if strutils.bool_from_string(instance.system_metadata.get(
3288                         'image_os_require_quiesce')):
3289                     raise
3290 
3291                 if isinstance(err, exception.NovaException):
3292                     LOG.info('Skipping quiescing instance: %(reason)s.',
3293                              {'reason': err.format_message()},
3294                              instance=instance)
3295                 else:
3296                     LOG.info('Skipping quiescing instance because the '
3297                              'operation is not supported by the underlying '
3298                              'compute driver.', instance=instance)
3299             # NOTE(tasker): discovered that an uncaught exception could occur
3300             #               after the instance has been frozen. catch and thaw.
3301             except Exception as ex:
3302                 with excutils.save_and_reraise_exception():
3303                     LOG.error("An error occurred during quiesce of instance. "
3304                               "Unquiescing to ensure instance is thawed. "
3305                               "Error: %s", str(ex),
3306                               instance=instance)
3307                     self.compute_rpcapi.unquiesce_instance(context, instance,
3308                                                            mapping=None)
3309 
3310         @wrap_instance_event(prefix='api')
3311         def snapshot_instance(self, context, instance, bdms):
3312             try:
3313                 for bdm in volume_bdms:
3314                     # create snapshot based on volume_id
3315                     volume = self.volume_api.get(context, bdm.volume_id)
3316                     # NOTE(yamahata): Should we wait for snapshot creation?
3317                     #                 Linux LVM snapshot creation completes in
3318                     #                 short time, it doesn't matter for now.
3319                     name = _('snapshot for %s') % image_meta['name']
3320                     LOG.debug('Creating snapshot from volume %s.',
3321                               volume['id'], instance=instance)
3322                     snapshot = self.volume_api.create_snapshot_force(
3323                         context, volume['id'],
3324                         name, volume['display_description'])
3325                     mapping_dict = block_device.snapshot_from_bdm(
3326                         snapshot['id'], bdm)
3327                     mapping_dict = mapping_dict.get_image_mapping()
3328                     mapping.append(mapping_dict)
3329                 return mapping
3330             # NOTE(tasker): No error handling is done in the above for loop.
3331             # This means that if the snapshot fails and throws an exception
3332             # the traceback will skip right over the unquiesce needed below.
3333             # Here, catch any exception, unquiesce the instance, and raise the
3334             # error so that the calling function can do what it needs to in
3335             # order to properly treat a failed snap.
3336             except Exception:
3337                 with excutils.save_and_reraise_exception():
3338                     if quiesced:
3339                         LOG.info("Unquiescing instance after volume snapshot "
3340                                  "failure.", instance=instance)
3341                         self.compute_rpcapi.unquiesce_instance(
3342                             context, instance, mapping)
3343 
3344         self._record_action_start(context, instance,
3345                                   instance_actions.CREATE_IMAGE)
3346         mapping = snapshot_instance(self, context, instance, bdms)
3347 
3348         if quiesced:
3349             self.compute_rpcapi.unquiesce_instance(context, instance, mapping)
3350 
3351         if mapping:
3352             properties['block_device_mapping'] = mapping
3353             properties['bdm_v2'] = True
3354 
3355         return self.image_api.create(context, image_meta)
3356 
3357     @check_instance_lock
3358     def reboot(self, context, instance, reboot_type):
3359         """Reboot the given instance."""
3360         if reboot_type == 'SOFT':
3361             self._soft_reboot(context, instance)
3362         else:
3363             self._hard_reboot(context, instance)
3364 
3365     @check_instance_state(vm_state=set(vm_states.ALLOW_SOFT_REBOOT),
3366                           task_state=[None])
3367     def _soft_reboot(self, context, instance):
3368         expected_task_state = [None]
3369         instance.task_state = task_states.REBOOTING
3370         instance.save(expected_task_state=expected_task_state)
3371 
3372         self._record_action_start(context, instance, instance_actions.REBOOT)
3373 
3374         self.compute_rpcapi.reboot_instance(context, instance=instance,
3375                                             block_device_info=None,
3376                                             reboot_type='SOFT')
3377 
3378     @check_instance_state(vm_state=set(vm_states.ALLOW_HARD_REBOOT),
3379                           task_state=task_states.ALLOW_REBOOT)
3380     def _hard_reboot(self, context, instance):
3381         instance.task_state = task_states.REBOOTING_HARD
3382         instance.save(expected_task_state=task_states.ALLOW_REBOOT)
3383 
3384         self._record_action_start(context, instance, instance_actions.REBOOT)
3385 
3386         self.compute_rpcapi.reboot_instance(context, instance=instance,
3387                                             block_device_info=None,
3388                                             reboot_type='HARD')
3389 
3390     def _check_image_arch(self, image=None):
3391         if image:
3392             img_arch = image.get("properties", {}).get('hw_architecture')
3393             if img_arch:
3394                 fields_obj.Architecture.canonicalize(img_arch)
3395 
3396     @reject_vtpm_instances(instance_actions.REBUILD)
3397     @block_accelerators(until_service=SUPPORT_ACCELERATOR_SERVICE_FOR_REBUILD)
3398     # TODO(stephenfin): We should expand kwargs out to named args
3399     @check_instance_lock
3400     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
3401                                     vm_states.ERROR])
3402     def rebuild(self, context, instance, image_href, admin_password,
3403                 files_to_inject=None, **kwargs):
3404         """Rebuild the given instance with the provided attributes."""
3405         files_to_inject = files_to_inject or []
3406         metadata = kwargs.get('metadata', {})
3407         preserve_ephemeral = kwargs.get('preserve_ephemeral', False)
3408         auto_disk_config = kwargs.get('auto_disk_config')
3409 
3410         if 'key_name' in kwargs:
3411             key_name = kwargs.pop('key_name')
3412             if key_name:
3413                 # NOTE(liuyulong): we are intentionally using the user_id from
3414                 # the request context rather than the instance.user_id because
3415                 # users own keys but instances are owned by projects, and
3416                 # another user in the same project can rebuild an instance
3417                 # even if they didn't create it.
3418                 key_pair = objects.KeyPair.get_by_name(context,
3419                                                        context.user_id,
3420                                                        key_name)
3421                 instance.key_name = key_pair.name
3422                 instance.key_data = key_pair.public_key
3423                 instance.keypairs = objects.KeyPairList(objects=[key_pair])
3424             else:
3425                 instance.key_name = None
3426                 instance.key_data = None
3427                 instance.keypairs = objects.KeyPairList(objects=[])
3428 
3429         # Use trusted_certs value from kwargs to create TrustedCerts object
3430         trusted_certs = None
3431         if 'trusted_certs' in kwargs:
3432             # Note that the user can set, change, or unset / reset trusted
3433             # certs. If they are explicitly specifying
3434             # trusted_image_certificates=None, that means we'll either unset
3435             # them on the instance *or* reset to use the defaults (if defaults
3436             # are configured).
3437             trusted_certs = kwargs.pop('trusted_certs')
3438             instance.trusted_certs = self._retrieve_trusted_certs_object(
3439                 context, trusted_certs, rebuild=True)
3440 
3441         image_id, image = self._get_image(context, image_href)
3442         self._check_auto_disk_config(image=image,
3443                                      auto_disk_config=auto_disk_config)
3444         self._check_image_arch(image=image)
3445 
3446         flavor = instance.get_flavor()
3447         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
3448             context, instance.uuid)
3449         root_bdm = compute_utils.get_root_bdm(context, instance, bdms)
3450 
3451         # Check to see if the image is changing and we have a volume-backed
3452         # server. The compute doesn't support changing the image in the
3453         # root disk of a volume-backed server, so we need to just fail fast.
3454         is_volume_backed = compute_utils.is_volume_backed_instance(
3455             context, instance, bdms)
3456         if is_volume_backed:
3457             if trusted_certs:
3458                 # The only way we can get here is if the user tried to set
3459                 # trusted certs or specified trusted_image_certificates=None
3460                 # and default_trusted_certificate_ids is configured.
3461                 msg = _("Image certificate validation is not supported "
3462                         "for volume-backed servers.")
3463                 raise exception.CertificateValidationFailed(message=msg)
3464 
3465             # For boot from volume, instance.image_ref is empty, so we need to
3466             # query the image from the volume.
3467             if root_bdm is None:
3468                 # This shouldn't happen and is an error, we need to fail. This
3469                 # is not the users fault, it's an internal error. Without a
3470                 # root BDM we have no way of knowing the backing volume (or
3471                 # image in that volume) for this instance.
3472                 raise exception.NovaException(
3473                     _('Unable to find root block device mapping for '
3474                       'volume-backed instance.'))
3475 
3476             volume = self.volume_api.get(context, root_bdm.volume_id)
3477             volume_image_metadata = volume.get('volume_image_metadata', {})
3478             orig_image_ref = volume_image_metadata.get('image_id')
3479 
3480             if orig_image_ref != image_href:
3481                 # Leave a breadcrumb.
3482                 LOG.debug('Requested to rebuild instance with a new image %s '
3483                           'for a volume-backed server with image %s in its '
3484                           'root volume which is not supported.', image_href,
3485                           orig_image_ref, instance=instance)
3486                 msg = _('Unable to rebuild with a different image for a '
3487                         'volume-backed server.')
3488                 raise exception.ImageUnacceptable(
3489                     image_id=image_href, reason=msg)
3490         else:
3491             orig_image_ref = instance.image_ref
3492 
3493         request_spec = objects.RequestSpec.get_by_instance_uuid(
3494             context, instance.uuid)
3495 
3496         self._checks_for_create_and_rebuild(context, image_id, image,
3497                 flavor, metadata, files_to_inject, root_bdm)
3498 
3499         # Check the state of the volume. If it is not in-use, an exception
3500         # will occur when creating attachment during reconstruction,
3501         # resulting in the failure of reconstruction and the instance
3502         # turning into an error state.
3503         self._check_volume_status(context, bdms)
3504 
3505         # NOTE(sean-k-mooney): When we rebuild with a new image we need to
3506         # validate that the NUMA topology does not change as we do a NOOP claim
3507         # in resource tracker. As such we cannot allow the resource usage or
3508         # assignment to change as a result of a new image altering the
3509         # numa constraints.
3510         if orig_image_ref != image_href:
3511             self._validate_numa_rebuild(instance, image, flavor)
3512 
3513         kernel_id, ramdisk_id = self._handle_kernel_and_ramdisk(
3514                 context, None, None, image)
3515 
3516         def _reset_image_metadata():
3517             """Remove old image properties that we're storing as instance
3518             system metadata.  These properties start with 'image_'.
3519             Then add the properties for the new image.
3520             """
3521             # FIXME(comstud): There's a race condition here in that if
3522             # the system_metadata for this instance is updated after
3523             # we do the previous save() and before we update.. those
3524             # other updates will be lost. Since this problem exists in
3525             # a lot of other places, I think it should be addressed in
3526             # a DB layer overhaul.
3527 
3528             orig_sys_metadata = dict(instance.system_metadata)
3529             # Remove the old keys
3530             for key in list(instance.system_metadata.keys()):
3531                 if key.startswith(utils.SM_IMAGE_PROP_PREFIX):
3532                     del instance.system_metadata[key]
3533 
3534             # Add the new ones
3535             new_sys_metadata = utils.get_system_metadata_from_image(
3536                 image, flavor)
3537 
3538             new_sys_metadata.update({'image_base_image_ref': image_id})
3539 
3540             instance.system_metadata.update(new_sys_metadata)
3541             instance.save()
3542             return orig_sys_metadata
3543 
3544         # Since image might have changed, we may have new values for
3545         # os_type, vm_mode, etc
3546         options_from_image = self._inherit_properties_from_image(
3547                 image, auto_disk_config)
3548         instance.update(options_from_image)
3549 
3550         instance.task_state = task_states.REBUILDING
3551         # An empty instance.image_ref is currently used as an indication
3552         # of BFV.  Preserve that over a rebuild to not break users.
3553         if not is_volume_backed:
3554             instance.image_ref = image_href
3555         instance.kernel_id = kernel_id or ""
3556         instance.ramdisk_id = ramdisk_id or ""
3557         instance.progress = 0
3558         instance.update(kwargs)
3559         instance.save(expected_task_state=[None])
3560 
3561         # On a rebuild, since we're potentially changing images, we need to
3562         # wipe out the old image properties that we're storing as instance
3563         # system metadata... and copy in the properties for the new image.
3564         orig_sys_metadata = _reset_image_metadata()
3565 
3566         self._record_action_start(context, instance, instance_actions.REBUILD)
3567 
3568         # NOTE(sbauza): The migration script we provided in Newton should make
3569         # sure that all our instances are currently migrated to have an
3570         # attached RequestSpec object but let's consider that the operator only
3571         # half migrated all their instances in the meantime.
3572         host = instance.host
3573         # If a new image is provided on rebuild, we will need to run
3574         # through the scheduler again, but we want the instance to be
3575         # rebuilt on the same host it's already on.
3576         if orig_image_ref != image_href:
3577             # We have to modify the request spec that goes to the scheduler
3578             # to contain the new image. We persist this since we've already
3579             # changed the instance.image_ref above so we're being
3580             # consistent.
3581             request_spec.image = objects.ImageMeta.from_dict(image)
3582             request_spec.save()
3583             if 'scheduler_hints' not in request_spec:
3584                 request_spec.scheduler_hints = {}
3585             # Nuke the id on this so we can't accidentally save
3586             # this hint hack later
3587             del request_spec.id
3588 
3589             # NOTE(danms): Passing host=None tells conductor to
3590             # call the scheduler. The _nova_check_type hint
3591             # requires that the scheduler returns only the same
3592             # host that we are currently on and only checks
3593             # rebuild-related filters.
3594             request_spec.scheduler_hints['_nova_check_type'] = ['rebuild']
3595             request_spec.force_hosts = [instance.host]
3596             request_spec.force_nodes = [instance.node]
3597             host = None
3598 
3599         self.compute_task_api.rebuild_instance(context, instance=instance,
3600                 new_pass=admin_password, injected_files=files_to_inject,
3601                 image_ref=image_href, orig_image_ref=orig_image_ref,
3602                 orig_sys_metadata=orig_sys_metadata, bdms=bdms,
3603                 preserve_ephemeral=preserve_ephemeral, host=host,
3604                 request_spec=request_spec)
3605 
3606     def _check_volume_status(self, context, bdms):
3607         """Check whether the status of the volume is "in-use".
3608 
3609         :param context: A context.RequestContext
3610         :param bdms: BlockDeviceMappingList of BDMs for the instance
3611         """
3612         for bdm in bdms:
3613             if bdm.volume_id:
3614                 vol = self.volume_api.get(context, bdm.volume_id)
3615                 self.volume_api.check_attached(context, vol)
3616 
3617     @staticmethod
3618     def _validate_numa_rebuild(instance, image, flavor):
3619         """validates that the NUMA constraints do not change on rebuild.
3620 
3621         :param instance: nova.objects.instance.Instance object
3622         :param image: the new image the instance will be rebuilt with.
3623         :param flavor: the flavor of the current instance.
3624         :raises: nova.exception.ImageNUMATopologyRebuildConflict
3625         """
3626 
3627         # NOTE(sean-k-mooney): currently it is not possible to express
3628         # a PCI NUMA affinity policy via flavor or image but that will
3629         # change in the future. we pull out the image metadata into
3630         # separate variable to make future testing of this easier.
3631         old_image_meta = instance.image_meta
3632         new_image_meta = objects.ImageMeta.from_dict(image)
3633         old_constraints = hardware.numa_get_constraints(flavor, old_image_meta)
3634         new_constraints = hardware.numa_get_constraints(flavor, new_image_meta)
3635 
3636         # early out for non NUMA instances
3637         if old_constraints is None and new_constraints is None:
3638             return
3639 
3640         # if only one of the constraints are non-None (or 'set') then the
3641         # constraints changed so raise an exception.
3642         if old_constraints is None or new_constraints is None:
3643             action = "removing" if old_constraints else "introducing"
3644             LOG.debug("NUMA rebuild validation failed. The requested image "
3645                       "would alter the NUMA constraints by %s a NUMA "
3646                       "topology.", action, instance=instance)
3647             raise exception.ImageNUMATopologyRebuildConflict()
3648 
3649         # otherwise since both the old a new constraints are non none compare
3650         # them as dictionaries.
3651         old = old_constraints.obj_to_primitive()
3652         new = new_constraints.obj_to_primitive()
3653         if old != new:
3654             LOG.debug("NUMA rebuild validation failed. The requested image "
3655                       "conflicts with the existing NUMA constraints.",
3656                       instance=instance)
3657             raise exception.ImageNUMATopologyRebuildConflict()
3658         # TODO(sean-k-mooney): add PCI NUMA affinity policy check.
3659 
3660     @staticmethod
3661     def _check_quota_for_upsize(context, instance, current_flavor, new_flavor):
3662         project_id, user_id = quotas_obj.ids_from_instance(context,
3663                                                            instance)
3664         # Deltas will be empty if the resize is not an upsize.
3665         deltas = compute_utils.upsize_quota_delta(new_flavor,
3666                                                   current_flavor)
3667         if deltas:
3668             try:
3669                 res_deltas = {'cores': deltas.get('cores', 0),
3670                               'ram': deltas.get('ram', 0)}
3671                 objects.Quotas.check_deltas(context, res_deltas,
3672                                             project_id, user_id=user_id,
3673                                             check_project_id=project_id,
3674                                             check_user_id=user_id)
3675             except exception.OverQuota as exc:
3676                 quotas = exc.kwargs['quotas']
3677                 overs = exc.kwargs['overs']
3678                 usages = exc.kwargs['usages']
3679                 headroom = compute_utils.get_headroom(quotas, usages,
3680                                                       deltas)
3681                 (overs, reqs, total_alloweds,
3682                  useds) = compute_utils.get_over_quota_detail(headroom,
3683                                                               overs,
3684                                                               quotas,
3685                                                               deltas)
3686                 LOG.info("%(overs)s quota exceeded for %(pid)s,"
3687                          " tried to resize instance.",
3688                          {'overs': overs, 'pid': context.project_id})
3689                 raise exception.TooManyInstances(overs=overs,
3690                                                  req=reqs,
3691                                                  used=useds,
3692                                                  allowed=total_alloweds)
3693 
3694     @check_instance_lock
3695     @check_instance_state(vm_state=[vm_states.RESIZED])
3696     def revert_resize(self, context, instance):
3697         """Reverts a resize or cold migration, deleting the 'new' instance in
3698         the process.
3699         """
3700         elevated = context.elevated()
3701         migration = objects.Migration.get_by_instance_and_status(
3702             elevated, instance.uuid, 'finished')
3703 
3704         # If this is a resize down, a revert might go over quota.
3705         self._check_quota_for_upsize(context, instance, instance.flavor,
3706                                      instance.old_flavor)
3707 
3708         # The AZ for the server may have changed when it was migrated so while
3709         # we are in the API and have access to the API DB, update the
3710         # instance.availability_zone before casting off to the compute service.
3711         # Note that we do this in the API to avoid an "up-call" from the
3712         # compute service to the API DB. This is not great in case something
3713         # fails during revert before the instance.host is updated to the
3714         # original source host, but it is good enough for now. Long-term we
3715         # could consider passing the AZ down to compute so it can set it when
3716         # the instance.host value is set in finish_revert_resize.
3717         instance.availability_zone = (
3718             availability_zones.get_host_availability_zone(
3719                 context, migration.source_compute))
3720 
3721         # If this was a resize, the conductor may have updated the
3722         # RequestSpec.flavor field (to point at the new flavor) and the
3723         # RequestSpec.numa_topology field (to reflect the new flavor's extra
3724         # specs) during the initial resize operation, so we need to update the
3725         # RequestSpec to point back at the original flavor and reflect the NUMA
3726         # settings of this flavor, otherwise subsequent move operations through
3727         # the scheduler will be using the wrong values. There's no need to do
3728         # this if the flavor hasn't changed though and we're migrating rather
3729         # than resizing.
3730         reqspec = objects.RequestSpec.get_by_instance_uuid(
3731             context, instance.uuid)
3732         if reqspec.flavor['id'] != instance.old_flavor['id']:
3733             reqspec.flavor = instance.old_flavor
3734             reqspec.numa_topology = hardware.numa_get_constraints(
3735                 instance.old_flavor, instance.image_meta)
3736             reqspec.save()
3737 
3738         # NOTE(gibi): This is a performance optimization. If the network info
3739         # cache does not have ports with allocations in the binding profile
3740         # then we can skip reading port resource request from neutron below.
3741         # If a port has resource request then that would have already caused
3742         # that the finish_resize call put allocation in the binding profile
3743         # during the resize.
3744         if instance.get_network_info().has_port_with_allocation():
3745             # TODO(gibi): do not directly overwrite the
3746             # RequestSpec.requested_resources as others like cyborg might added
3747             # to things there already
3748             # NOTE(gibi): We need to collect the requested resource again as it
3749             # is intentionally not persisted in nova. Note that this needs to
3750             # be done here as the nova API code directly calls revert on the
3751             # dest compute service skipping the conductor.
3752             port_res_req = (
3753                 self.network_api.get_requested_resource_for_instance(
3754                     context, instance.uuid))
3755             reqspec.requested_resources = port_res_req
3756 
3757         instance.task_state = task_states.RESIZE_REVERTING
3758         instance.save(expected_task_state=[None])
3759 
3760         migration.status = 'reverting'
3761         migration.save()
3762 
3763         self._record_action_start(context, instance,
3764                                   instance_actions.REVERT_RESIZE)
3765 
3766         if migration.cross_cell_move:
3767             # RPC cast to conductor to orchestrate the revert of the cross-cell
3768             # resize.
3769             self.compute_task_api.revert_snapshot_based_resize(
3770                 context, instance, migration)
3771         else:
3772             # TODO(melwitt): We're not rechecking for strict quota here to
3773             # guard against going over quota during a race at this time because
3774             # the resource consumption for this operation is written to the
3775             # database by compute.
3776             self.compute_rpcapi.revert_resize(context, instance,
3777                                               migration,
3778                                               migration.dest_compute,
3779                                               reqspec)
3780 
3781     @staticmethod
3782     def _get_source_compute_service(context, migration):
3783         """Find the source compute Service object given the Migration.
3784 
3785         :param context: nova auth RequestContext target at the destination
3786             compute cell
3787         :param migration: Migration object for the move operation
3788         :return: Service object representing the source host nova-compute
3789         """
3790         if migration.cross_cell_move:
3791             # The source compute could be in another cell so look up the
3792             # HostMapping to determine the source cell.
3793             hm = objects.HostMapping.get_by_host(
3794                 context, migration.source_compute)
3795             with nova_context.target_cell(context, hm.cell_mapping) as cctxt:
3796                 return objects.Service.get_by_compute_host(
3797                     cctxt, migration.source_compute)
3798         # Same-cell migration so just use the context we have.
3799         return objects.Service.get_by_compute_host(
3800             context, migration.source_compute)
3801 
3802     @check_instance_lock
3803     @check_instance_state(vm_state=[vm_states.RESIZED])
3804     def confirm_resize(self, context, instance, migration=None):
3805         """Confirms a migration/resize and deletes the 'old' instance.
3806 
3807         :param context: nova auth RequestContext
3808         :param instance: Instance object to confirm the resize
3809         :param migration: Migration object; provided if called from the
3810             _poll_unconfirmed_resizes periodic task on the dest compute.
3811         :raises: MigrationNotFound if migration is not provided and a migration
3812             cannot be found for the instance with status "finished".
3813         :raises: ServiceUnavailable if the source compute service is down.
3814         """
3815         elevated = context.elevated()
3816         # NOTE(melwitt): We're not checking quota here because there isn't a
3817         # change in resource usage when confirming a resize. Resource
3818         # consumption for resizes are written to the database by compute, so
3819         # a confirm resize is just a clean up of the migration objects and a
3820         # state change in compute.
3821         if migration is None:
3822             migration = objects.Migration.get_by_instance_and_status(
3823                 elevated, instance.uuid, 'finished')
3824 
3825         # Check if the source compute service is up before modifying the
3826         # migration record because once we do we cannot come back through this
3827         # method since it will be looking for a "finished" status migration.
3828         source_svc = self._get_source_compute_service(context, migration)
3829         if not self.servicegroup_api.service_is_up(source_svc):
3830             raise exception.ServiceUnavailable()
3831 
3832         migration.status = 'confirming'
3833         migration.save()
3834 
3835         self._record_action_start(context, instance,
3836                                   instance_actions.CONFIRM_RESIZE)
3837 
3838         # Check to see if this was a cross-cell resize, in which case the
3839         # resized instance is in the target cell (the migration and instance
3840         # came from the target cell DB in this case), and we need to cleanup
3841         # the source host and source cell database records.
3842         if migration.cross_cell_move:
3843             self.compute_task_api.confirm_snapshot_based_resize(
3844                 context, instance, migration)
3845         else:
3846             # It's a traditional resize within a single cell, so RPC cast to
3847             # the source compute host to cleanup the host since the instance
3848             # is already on the target host.
3849             self.compute_rpcapi.confirm_resize(context,
3850                                                instance,
3851                                                migration,
3852                                                migration.source_compute)
3853 
3854     def _allow_cross_cell_resize(self, context, instance):
3855         """Determine if the request can perform a cross-cell resize on this
3856         instance.
3857 
3858         :param context: nova auth request context for the resize operation
3859         :param instance: Instance object being resized
3860         :returns: True if cross-cell resize is allowed, False otherwise
3861         """
3862         # First check to see if the requesting project/user is allowed by
3863         # policy to perform cross-cell resize.
3864         allowed = context.can(
3865             servers_policies.CROSS_CELL_RESIZE,
3866             target={'user_id': instance.user_id,
3867                     'project_id': instance.project_id},
3868             fatal=False)
3869         # If the user is allowed by policy, check to make sure the deployment
3870         # is upgraded to the point of supporting cross-cell resize on all
3871         # compute services.
3872         if allowed:
3873             # TODO(mriedem): We can remove this minimum compute version check
3874             # in the 22.0.0 "V" release.
3875             min_compute_version = (
3876                 objects.service.get_minimum_version_all_cells(
3877                     context, ['nova-compute']))
3878             if min_compute_version < MIN_COMPUTE_CROSS_CELL_RESIZE:
3879                 LOG.debug('Request is allowed by policy to perform cross-cell '
3880                           'resize but the minimum nova-compute service '
3881                           'version in the deployment %s is less than %s so '
3882                           'cross-cell resize is not allowed at this time.',
3883                           min_compute_version, MIN_COMPUTE_CROSS_CELL_RESIZE)
3884                 return False
3885 
3886             if self.network_api.get_requested_resource_for_instance(
3887                     context, instance.uuid):
3888                 LOG.info(
3889                     'Request is allowed by policy to perform cross-cell '
3890                     'resize but the instance has ports with resource request '
3891                     'and cross-cell resize is not supported with such ports.',
3892                     instance=instance)
3893                 return False
3894 
3895         return allowed
3896 
3897     @staticmethod
3898     def _validate_host_for_cold_migrate(
3899             context, instance, host_name, allow_cross_cell_resize):
3900         """Validates a host specified for cold migration.
3901 
3902         :param context: nova auth request context for the cold migration
3903         :param instance: Instance object being cold migrated
3904         :param host_name: User-specified compute service hostname for the
3905             desired destination of the instance during the cold migration
3906         :param allow_cross_cell_resize: If True, cross-cell resize is allowed
3907             for this operation and the host could be in a different cell from
3908             the one that the instance is currently in. If False, the speciifed
3909             host must be in the same cell as the instance.
3910         :returns: ComputeNode object of the requested host
3911         :raises: CannotMigrateToSameHost if the host is the same as the
3912             current instance.host
3913         :raises: ComputeHostNotFound if the specified host cannot be found
3914         """
3915         # Cannot migrate to the host where the instance exists
3916         # because it is useless.
3917         if host_name == instance.host:
3918             raise exception.CannotMigrateToSameHost()
3919 
3920         # Check whether host exists or not. If a cross-cell resize is
3921         # allowed, the host could be in another cell from the one the
3922         # instance is currently in, so we need to lookup the HostMapping
3923         # to get the cell and lookup the ComputeNode in that cell.
3924         if allow_cross_cell_resize:
3925             try:
3926                 hm = objects.HostMapping.get_by_host(context, host_name)
3927             except exception.HostMappingNotFound:
3928                 LOG.info('HostMapping not found for host: %s', host_name)
3929                 raise exception.ComputeHostNotFound(host=host_name)
3930 
3931             with nova_context.target_cell(context, hm.cell_mapping) as cctxt:
3932                 node = objects.ComputeNode.\
3933                     get_first_node_by_host_for_old_compat(
3934                         cctxt, host_name, use_slave=True)
3935         else:
3936             node = objects.ComputeNode.get_first_node_by_host_for_old_compat(
3937                 context, host_name, use_slave=True)
3938 
3939         return node
3940 
3941     # TODO(stephenfin): This logic would be so much easier to grok if we
3942     # finally split resize and cold migration into separate code paths
3943     @block_accelerators()
3944     @check_instance_lock
3945     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED])
3946     @check_instance_host(check_is_up=True)
3947     def resize(self, context, instance, flavor_id=None, clean_shutdown=True,
3948                host_name=None, auto_disk_config=None):
3949         """Resize (ie, migrate) a running instance.
3950 
3951         If flavor_id is None, the process is considered a migration, keeping
3952         the original flavor_id. If flavor_id is not None, the instance should
3953         be migrated to a new host and resized to the new flavor_id.
3954         host_name is always None in the resize case.
3955         host_name can be set in the cold migration case only.
3956         """
3957         allow_cross_cell_resize = self._allow_cross_cell_resize(
3958             context, instance)
3959 
3960         if host_name is not None:
3961             node = self._validate_host_for_cold_migrate(
3962                 context, instance, host_name, allow_cross_cell_resize)
3963 
3964         self._check_auto_disk_config(
3965             instance, auto_disk_config=auto_disk_config)
3966 
3967         current_instance_type = instance.get_flavor()
3968 
3969         # NOTE(aarents): Ensure image_base_image_ref is present as it will be
3970         # needed during finish_resize/cross_cell_resize. Instances upgraded
3971         # from an older nova release may not have this property because of
3972         # a rebuild bug Bug/1893618.
3973         instance.system_metadata.update(
3974                 {'image_base_image_ref': instance.image_ref}
3975         )
3976 
3977         # If flavor_id is not provided, only migrate the instance.
3978         volume_backed = None
3979         if not flavor_id:
3980             LOG.debug("flavor_id is None. Assuming migration.",
3981                       instance=instance)
3982             new_instance_type = current_instance_type
3983         else:
3984             new_instance_type = flavors.get_flavor_by_flavor_id(
3985                     flavor_id, read_deleted="no")
3986             # NOTE(wenping): We use this instead of the 'block_accelerator'
3987             # decorator since the operation can differ depending on args,
3988             # and for resize we have two flavors to worry about, we should
3989             # reject resize with new flavor with accelerator.
3990             if new_instance_type.extra_specs.get('accel:device_profile'):
3991                 raise exception.ForbiddenWithAccelerators()
3992             # Check to see if we're resizing to a zero-disk flavor which is
3993             # only supported with volume-backed servers.
3994             if (new_instance_type.get('root_gb') == 0 and
3995                     current_instance_type.get('root_gb') != 0):
3996                 volume_backed = compute_utils.is_volume_backed_instance(
3997                         context, instance)
3998                 if not volume_backed:
3999                     reason = _('Resize to zero disk flavor is not allowed.')
4000                     raise exception.CannotResizeDisk(reason=reason)
4001 
4002         current_instance_type_name = current_instance_type['name']
4003         new_instance_type_name = new_instance_type['name']
4004         LOG.debug("Old instance type %(current_instance_type_name)s, "
4005                   "new instance type %(new_instance_type_name)s",
4006                   {'current_instance_type_name': current_instance_type_name,
4007                    'new_instance_type_name': new_instance_type_name},
4008                   instance=instance)
4009 
4010         same_instance_type = (current_instance_type['id'] ==
4011                               new_instance_type['id'])
4012 
4013         # NOTE(sirp): We don't want to force a customer to change their flavor
4014         # when Ops is migrating off of a failed host.
4015         if not same_instance_type and new_instance_type.get('disabled'):
4016             raise exception.FlavorNotFound(flavor_id=flavor_id)
4017 
4018         if same_instance_type and flavor_id:
4019             raise exception.CannotResizeToSameFlavor()
4020 
4021         # ensure there is sufficient headroom for upsizes
4022         if flavor_id:
4023             self._check_quota_for_upsize(context, instance,
4024                                          current_instance_type,
4025                                          new_instance_type)
4026 
4027         if not same_instance_type:
4028             image = utils.get_image_from_system_metadata(
4029                 instance.system_metadata)
4030             # Figure out if the instance is volume-backed but only if we didn't
4031             # already figure that out above (avoid the extra db hit).
4032             if volume_backed is None:
4033                 volume_backed = compute_utils.is_volume_backed_instance(
4034                     context, instance)
4035             # If the server is volume-backed, we still want to validate numa
4036             # and pci information in the new flavor, but we don't call
4037             # _validate_flavor_image_nostatus because how it handles checking
4038             # disk size validation was not intended for a volume-backed
4039             # resize case.
4040             if volume_backed:
4041                 self._validate_flavor_image_numa_pci(
4042                     image, new_instance_type, validate_pci=True)
4043             else:
4044                 self._validate_flavor_image_nostatus(
4045                     context, image, new_instance_type, root_bdm=None,
4046                     validate_pci=True)
4047 
4048         filter_properties = {'ignore_hosts': []}
4049         if not self._allow_resize_to_same_host(same_instance_type, instance):
4050             filter_properties['ignore_hosts'].append(instance.host)
4051 
4052         request_spec = objects.RequestSpec.get_by_instance_uuid(
4053             context, instance.uuid)
4054         request_spec.ignore_hosts = filter_properties['ignore_hosts']
4055 
4056         # don't recalculate the NUMA topology unless the flavor has changed
4057         if not same_instance_type:
4058             request_spec.numa_topology = hardware.numa_get_constraints(
4059                 new_instance_type, instance.image_meta)
4060             # TODO(huaqiang): Remove in Wallaby
4061             # check nova-compute nodes have been updated to Victoria to resize
4062             # instance to a new mixed instance from a dedicated or shared
4063             # instance.
4064             self._check_compute_service_for_mixed_instance(
4065                 request_spec.numa_topology)
4066 
4067         instance.task_state = task_states.RESIZE_PREP
4068         instance.progress = 0
4069         instance.auto_disk_config = auto_disk_config or False
4070         instance.save(expected_task_state=[None])
4071 
4072         if not flavor_id:
4073             self._record_action_start(context, instance,
4074                                       instance_actions.MIGRATE)
4075         else:
4076             self._record_action_start(context, instance,
4077                                       instance_actions.RESIZE)
4078 
4079         # TODO(melwitt): We're not rechecking for strict quota here to guard
4080         # against going over quota during a race at this time because the
4081         # resource consumption for this operation is written to the database
4082         # by compute.
4083         scheduler_hint = {'filter_properties': filter_properties}
4084 
4085         if host_name is None:
4086             # If 'host_name' is not specified,
4087             # clear the 'requested_destination' field of the RequestSpec
4088             # except set the allow_cross_cell_move flag since conductor uses
4089             # it prior to scheduling.
4090             request_spec.requested_destination = objects.Destination(
4091                 allow_cross_cell_move=allow_cross_cell_resize)
4092         else:
4093             # Set the host and the node so that the scheduler will
4094             # validate them.
4095             request_spec.requested_destination = objects.Destination(
4096                 host=node.host, node=node.hypervisor_hostname,
4097                 allow_cross_cell_move=allow_cross_cell_resize)
4098 
4099         # Asynchronously RPC cast to conductor so the response is not blocked
4100         # during scheduling. If something fails the user can find out via
4101         # instance actions.
4102         self.compute_task_api.resize_instance(context, instance,
4103             scheduler_hint=scheduler_hint,
4104             flavor=new_instance_type,
4105             clean_shutdown=clean_shutdown,
4106             request_spec=request_spec,
4107             do_cast=True)
4108 
4109     def _allow_resize_to_same_host(self, cold_migrate, instance):
4110         """Contains logic for excluding the instance.host on resize/migrate.
4111 
4112         If performing a cold migration and the compute node resource provider
4113         reports the COMPUTE_SAME_HOST_COLD_MIGRATE trait then same-host cold
4114         migration is allowed otherwise it is not and the current instance.host
4115         should be excluded as a scheduling candidate.
4116 
4117         :param cold_migrate: true if performing a cold migration, false
4118             for resize
4119         :param instance: Instance object being resized or cold migrated
4120         :returns: True if same-host resize/cold migrate is allowed, False
4121             otherwise
4122         """
4123         if cold_migrate:
4124             # Check to see if the compute node resource provider on which the
4125             # instance is running has the COMPUTE_SAME_HOST_COLD_MIGRATE
4126             # trait.
4127             # Note that we check this here in the API since we cannot
4128             # pre-filter allocation candidates in the scheduler using this
4129             # trait as it would not work. For example, libvirt nodes will not
4130             # report the trait but using it as a forbidden trait filter when
4131             # getting allocation candidates would still return libvirt nodes
4132             # which means we could attempt to cold migrate to the same libvirt
4133             # node, which would fail.
4134             ctxt = instance._context
4135             cn = objects.ComputeNode.get_by_host_and_nodename(
4136                 ctxt, instance.host, instance.node)
4137             traits = self.placementclient.get_provider_traits(
4138                 ctxt, cn.uuid).traits
4139             # If the provider has the trait it is (1) new enough to report that
4140             # trait and (2) supports cold migration on the same host.
4141             if os_traits.COMPUTE_SAME_HOST_COLD_MIGRATE in traits:
4142                 allow_same_host = True
4143             else:
4144                 # TODO(mriedem): Remove this compatibility code after one
4145                 # release. If the compute is old we will not know if it
4146                 # supports same-host cold migration so we fallback to config.
4147                 service = objects.Service.get_by_compute_host(ctxt, cn.host)
4148                 if service.version >= MIN_COMPUTE_SAME_HOST_COLD_MIGRATE:
4149                     # The compute is new enough to report the trait but does
4150                     # not so same-host cold migration is not allowed.
4151                     allow_same_host = False
4152                 else:
4153                     # The compute is not new enough to report the trait so we
4154                     # fallback to config.
4155                     allow_same_host = CONF.allow_resize_to_same_host
4156         else:
4157             allow_same_host = CONF.allow_resize_to_same_host
4158         return allow_same_host
4159 
4160     @reject_vtpm_instances(instance_actions.SHELVE)
4161     @block_accelerators()
4162     @check_instance_lock
4163     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
4164                                     vm_states.PAUSED, vm_states.SUSPENDED])
4165     def shelve(self, context, instance, clean_shutdown=True):
4166         """Shelve an instance.
4167 
4168         Shuts down an instance and frees it up to be removed from the
4169         hypervisor.
4170         """
4171         instance.task_state = task_states.SHELVING
4172 
4173         # NOTE(aarents): Ensure image_base_image_ref is present as it will be
4174         # needed during unshelve and instance rebuild done before Bug/1893618
4175         # Fix dropped it.
4176         instance.system_metadata.update(
4177                 {'image_base_image_ref': instance.image_ref}
4178         )
4179 
4180         instance.save(expected_task_state=[None])
4181 
4182         self._record_action_start(context, instance, instance_actions.SHELVE)
4183 
4184         if not compute_utils.is_volume_backed_instance(context, instance):
4185             name = '%s-shelved' % instance.display_name
4186             image_meta = compute_utils.create_image(
4187                 context, instance, name, 'snapshot', self.image_api)
4188             image_id = image_meta['id']
4189             self.compute_rpcapi.shelve_instance(context, instance=instance,
4190                     image_id=image_id, clean_shutdown=clean_shutdown)
4191         else:
4192             self.compute_rpcapi.shelve_offload_instance(context,
4193                     instance=instance, clean_shutdown=clean_shutdown)
4194 
4195     @check_instance_lock
4196     @check_instance_state(vm_state=[vm_states.SHELVED])
4197     def shelve_offload(self, context, instance, clean_shutdown=True):
4198         """Remove a shelved instance from the hypervisor."""
4199         instance.task_state = task_states.SHELVING_OFFLOADING
4200         instance.save(expected_task_state=[None])
4201 
4202         self._record_action_start(context, instance,
4203                                   instance_actions.SHELVE_OFFLOAD)
4204 
4205         self.compute_rpcapi.shelve_offload_instance(context, instance=instance,
4206             clean_shutdown=clean_shutdown)
4207 
4208     def _validate_unshelve_az(self, context, instance, availability_zone):
4209         """Verify the specified availability_zone during unshelve.
4210 
4211         Verifies that the server is shelved offloaded, the AZ exists and
4212         if [cinder]/cross_az_attach=False, that any attached volumes are in
4213         the same AZ.
4214 
4215         :param context: nova auth RequestContext for the unshelve action
4216         :param instance: Instance object for the server being unshelved
4217         :param availability_zone: The user-requested availability zone in
4218             which to unshelve the server.
4219         :raises: UnshelveInstanceInvalidState if the server is not shelved
4220             offloaded
4221         :raises: InvalidRequest if the requested AZ does not exist
4222         :raises: MismatchVolumeAZException if [cinder]/cross_az_attach=False
4223             and any attached volumes are not in the requested AZ
4224         """
4225         if instance.vm_state != vm_states.SHELVED_OFFLOADED:
4226             # NOTE(brinzhang): If the server status is 'SHELVED', it still
4227             # belongs to a host, the availability_zone has not changed.
4228             # Unshelving a shelved offloaded server will go through the
4229             # scheduler to find a new host.
4230             raise exception.UnshelveInstanceInvalidState(
4231                 state=instance.vm_state, instance_uuid=instance.uuid)
4232 
4233         available_zones = availability_zones.get_availability_zones(
4234             context, self.host_api, get_only_available=True)
4235         if availability_zone not in available_zones:
4236             msg = _('The requested availability zone is not available')
4237             raise exception.InvalidRequest(msg)
4238 
4239         # NOTE(brinzhang): When specifying a availability zone to unshelve
4240         # a shelved offloaded server, and conf cross_az_attach=False, need
4241         # to determine if attached volume AZ matches the user-specified AZ.
4242         if not CONF.cinder.cross_az_attach:
4243             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
4244                 context, instance.uuid)
4245             for bdm in bdms:
4246                 if bdm.is_volume and bdm.volume_id:
4247                     volume = self.volume_api.get(context, bdm.volume_id)
4248                     if availability_zone != volume['availability_zone']:
4249                         msg = _("The specified availability zone does not "
4250                                 "match the volume %(vol_id)s attached to the "
4251                                 "server. Specified availability zone is "
4252                                 "%(az)s. Volume is in %(vol_zone)s.") % {
4253                             "vol_id": volume['id'],
4254                             "az": availability_zone,
4255                             "vol_zone": volume['availability_zone']}
4256                         raise exception.MismatchVolumeAZException(reason=msg)
4257 
4258     @check_instance_lock
4259     @check_instance_state(vm_state=[vm_states.SHELVED,
4260         vm_states.SHELVED_OFFLOADED])
4261     def unshelve(self, context, instance, new_az=None):
4262         """Restore a shelved instance."""
4263         request_spec = objects.RequestSpec.get_by_instance_uuid(
4264             context, instance.uuid)
4265 
4266         if new_az:
4267             self._validate_unshelve_az(context, instance, new_az)
4268             LOG.debug("Replace the old AZ %(old_az)s in RequestSpec "
4269                       "with a new AZ %(new_az)s of the instance.",
4270                       {"old_az": request_spec.availability_zone,
4271                        "new_az": new_az}, instance=instance)
4272             # Unshelving a shelved offloaded server will go through the
4273             # scheduler to pick a new host, so we update the
4274             # RequestSpec.availability_zone here. Note that if scheduling
4275             # fails the RequestSpec will remain updated, which is not great,
4276             # but if we want to change that we need to defer updating the
4277             # RequestSpec until conductor which probably means RPC changes to
4278             # pass the new_az variable to conductor. This is likely low
4279             # priority since the RequestSpec.availability_zone on a shelved
4280             # offloaded server does not mean much anyway and clearly the user
4281             # is trying to put the server in the target AZ.
4282             request_spec.availability_zone = new_az
4283             request_spec.save()
4284 
4285         instance.task_state = task_states.UNSHELVING
4286         instance.save(expected_task_state=[None])
4287 
4288         self._record_action_start(context, instance, instance_actions.UNSHELVE)
4289 
4290         self.compute_task_api.unshelve_instance(context, instance,
4291                                                 request_spec)
4292 
4293     @check_instance_lock
4294     def add_fixed_ip(self, context, instance, network_id):
4295         """Add fixed_ip from specified network to given instance."""
4296         self.compute_rpcapi.add_fixed_ip_to_instance(context,
4297                 instance=instance, network_id=network_id)
4298 
4299     @check_instance_lock
4300     def remove_fixed_ip(self, context, instance, address):
4301         """Remove fixed_ip from specified network to given instance."""
4302         self.compute_rpcapi.remove_fixed_ip_from_instance(context,
4303                 instance=instance, address=address)
4304 
4305     @check_instance_lock
4306     @check_instance_state(vm_state=[vm_states.ACTIVE])
4307     def pause(self, context, instance):
4308         """Pause the given instance."""
4309         instance.task_state = task_states.PAUSING
4310         instance.save(expected_task_state=[None])
4311         self._record_action_start(context, instance, instance_actions.PAUSE)
4312         self.compute_rpcapi.pause_instance(context, instance)
4313 
4314     @check_instance_lock
4315     @check_instance_state(vm_state=[vm_states.PAUSED])
4316     def unpause(self, context, instance):
4317         """Unpause the given instance."""
4318         instance.task_state = task_states.UNPAUSING
4319         instance.save(expected_task_state=[None])
4320         self._record_action_start(context, instance, instance_actions.UNPAUSE)
4321         self.compute_rpcapi.unpause_instance(context, instance)
4322 
4323     @check_instance_host()
4324     def get_diagnostics(self, context, instance):
4325         """Retrieve diagnostics for the given instance."""
4326         return self.compute_rpcapi.get_diagnostics(context, instance=instance)
4327 
4328     @check_instance_host()
4329     def get_instance_diagnostics(self, context, instance):
4330         """Retrieve diagnostics for the given instance."""
4331         return self.compute_rpcapi.get_instance_diagnostics(context,
4332                                                             instance=instance)
4333 
4334     @block_accelerators()
4335     @reject_sev_instances(instance_actions.SUSPEND)
4336     @check_instance_lock
4337     @check_instance_state(vm_state=[vm_states.ACTIVE])
4338     def suspend(self, context, instance):
4339         """Suspend the given instance."""
4340         instance.task_state = task_states.SUSPENDING
4341         instance.save(expected_task_state=[None])
4342         self._record_action_start(context, instance, instance_actions.SUSPEND)
4343         self.compute_rpcapi.suspend_instance(context, instance)
4344 
4345     @check_instance_lock
4346     @check_instance_state(vm_state=[vm_states.SUSPENDED])
4347     def resume(self, context, instance):
4348         """Resume the given instance."""
4349         instance.task_state = task_states.RESUMING
4350         instance.save(expected_task_state=[None])
4351         self._record_action_start(context, instance, instance_actions.RESUME)
4352         self.compute_rpcapi.resume_instance(context, instance)
4353 
4354     @reject_vtpm_instances(instance_actions.RESCUE)
4355     @check_instance_lock
4356     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
4357                                     vm_states.ERROR])
4358     def rescue(self, context, instance, rescue_password=None,
4359                rescue_image_ref=None, clean_shutdown=True,
4360                allow_bfv_rescue=False):
4361         """Rescue the given instance."""
4362 
4363         if rescue_image_ref:
4364             try:
4365                 image_meta = image_meta_obj.ImageMeta.from_image_ref(
4366                     context, self.image_api, rescue_image_ref)
4367             except (exception.ImageNotFound, exception.ImageBadRequest):
4368                 LOG.warning("Failed to fetch rescue image metadata using "
4369                             "image_ref %(image_ref)s",
4370                             {'image_ref': rescue_image_ref})
4371                 raise exception.UnsupportedRescueImage(
4372                     image=rescue_image_ref)
4373 
4374             # FIXME(lyarwood): There is currently no support for rescuing
4375             # instances using a volume snapshot so fail here before we cast to
4376             # the compute.
4377             if image_meta.properties.get('img_block_device_mapping'):
4378                 LOG.warning("Unable to rescue an instance using a volume "
4379                             "snapshot image with img_block_device_mapping "
4380                             "image properties set")
4381                 raise exception.UnsupportedRescueImage(
4382                     image=rescue_image_ref)
4383 
4384         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
4385                     context, instance.uuid)
4386         self._check_volume_status(context, bdms)
4387 
4388         volume_backed = compute_utils.is_volume_backed_instance(
4389             context, instance, bdms)
4390 
4391         if volume_backed and allow_bfv_rescue:
4392             cn = objects.ComputeNode.get_by_host_and_nodename(
4393                 context, instance.host, instance.node)
4394             traits = self.placementclient.get_provider_traits(
4395                 context, cn.uuid).traits
4396             if os_traits.COMPUTE_RESCUE_BFV not in traits:
4397                 reason = _("Host unable to rescue a volume-backed instance")
4398                 raise exception.InstanceNotRescuable(instance_id=instance.uuid,
4399                                                      reason=reason)
4400         elif volume_backed:
4401             reason = _("Cannot rescue a volume-backed instance")
4402             raise exception.InstanceNotRescuable(instance_id=instance.uuid,
4403                                                  reason=reason)
4404 
4405         instance.task_state = task_states.RESCUING
4406         instance.save(expected_task_state=[None])
4407 
4408         self._record_action_start(context, instance, instance_actions.RESCUE)
4409 
4410         self.compute_rpcapi.rescue_instance(context, instance=instance,
4411             rescue_password=rescue_password, rescue_image_ref=rescue_image_ref,
4412             clean_shutdown=clean_shutdown)
4413 
4414     @check_instance_lock
4415     @check_instance_state(vm_state=[vm_states.RESCUED])
4416     def unrescue(self, context, instance):
4417         """Unrescue the given instance."""
4418         instance.task_state = task_states.UNRESCUING
4419         instance.save(expected_task_state=[None])
4420 
4421         self._record_action_start(context, instance, instance_actions.UNRESCUE)
4422 
4423         self.compute_rpcapi.unrescue_instance(context, instance=instance)
4424 
4425     @check_instance_lock
4426     @check_instance_state(vm_state=[vm_states.ACTIVE])
4427     def set_admin_password(self, context, instance, password):
4428         """Set the root/admin password for the given instance.
4429 
4430         @param context: Nova auth context.
4431         @param instance: Nova instance object.
4432         @param password: The admin password for the instance.
4433         """
4434         instance.task_state = task_states.UPDATING_PASSWORD
4435         instance.save(expected_task_state=[None])
4436 
4437         self._record_action_start(context, instance,
4438                                   instance_actions.CHANGE_PASSWORD)
4439 
4440         self.compute_rpcapi.set_admin_password(context,
4441                                                instance=instance,
4442                                                new_pass=password)
4443 
4444     @check_instance_host()
4445     @reject_instance_state(
4446         task_state=[task_states.DELETING, task_states.MIGRATING])
4447     def get_vnc_console(self, context, instance, console_type):
4448         """Get a url to an instance Console."""
4449         connect_info = self.compute_rpcapi.get_vnc_console(context,
4450                 instance=instance, console_type=console_type)
4451         return {'url': connect_info['access_url']}
4452 
4453     @check_instance_host()
4454     @reject_instance_state(
4455         task_state=[task_states.DELETING, task_states.MIGRATING])
4456     def get_spice_console(self, context, instance, console_type):
4457         """Get a url to an instance Console."""
4458         connect_info = self.compute_rpcapi.get_spice_console(context,
4459                 instance=instance, console_type=console_type)
4460         return {'url': connect_info['access_url']}
4461 
4462     @check_instance_host()
4463     @reject_instance_state(
4464         task_state=[task_states.DELETING, task_states.MIGRATING])
4465     def get_rdp_console(self, context, instance, console_type):
4466         """Get a url to an instance Console."""
4467         connect_info = self.compute_rpcapi.get_rdp_console(context,
4468                 instance=instance, console_type=console_type)
4469         return {'url': connect_info['access_url']}
4470 
4471     @check_instance_host()
4472     @reject_instance_state(
4473         task_state=[task_states.DELETING, task_states.MIGRATING])
4474     def get_serial_console(self, context, instance, console_type):
4475         """Get a url to a serial console."""
4476         connect_info = self.compute_rpcapi.get_serial_console(context,
4477                 instance=instance, console_type=console_type)
4478         return {'url': connect_info['access_url']}
4479 
4480     @check_instance_host()
4481     @reject_instance_state(
4482         task_state=[task_states.DELETING, task_states.MIGRATING])
4483     def get_mks_console(self, context, instance, console_type):
4484         """Get a url to a MKS console."""
4485         connect_info = self.compute_rpcapi.get_mks_console(context,
4486                 instance=instance, console_type=console_type)
4487         return {'url': connect_info['access_url']}
4488 
4489     @check_instance_host()
4490     def get_console_output(self, context, instance, tail_length=None):
4491         """Get console output for an instance."""
4492         return self.compute_rpcapi.get_console_output(context,
4493                 instance=instance, tail_length=tail_length)
4494 
4495     def lock(self, context, instance, reason=None):
4496         """Lock the given instance."""
4497         # Only update the lock if we are an admin (non-owner)
4498         is_owner = instance.project_id == context.project_id
4499         if instance.locked and is_owner:
4500             return
4501 
4502         context = context.elevated()
4503         self._record_action_start(context, instance,
4504                                   instance_actions.LOCK)
4505 
4506         @wrap_instance_event(prefix='api')
4507         def lock(self, context, instance, reason=None):
4508             LOG.debug('Locking', instance=instance)
4509             instance.locked = True
4510             instance.locked_by = 'owner' if is_owner else 'admin'
4511             if reason:
4512                 instance.system_metadata['locked_reason'] = reason
4513             instance.save()
4514 
4515         lock(self, context, instance, reason=reason)
4516         compute_utils.notify_about_instance_action(
4517             context, instance, CONF.host,
4518             action=fields_obj.NotificationAction.LOCK,
4519             source=fields_obj.NotificationSource.API)
4520 
4521     def is_expected_locked_by(self, context, instance):
4522         is_owner = instance.project_id == context.project_id
4523         expect_locked_by = 'owner' if is_owner else 'admin'
4524         locked_by = instance.locked_by
4525         if locked_by and locked_by != expect_locked_by:
4526             return False
4527         return True
4528 
4529     def unlock(self, context, instance):
4530         """Unlock the given instance."""
4531         context = context.elevated()
4532         self._record_action_start(context, instance,
4533                                   instance_actions.UNLOCK)
4534 
4535         @wrap_instance_event(prefix='api')
4536         def unlock(self, context, instance):
4537             LOG.debug('Unlocking', instance=instance)
4538             instance.locked = False
4539             instance.locked_by = None
4540             instance.system_metadata.pop('locked_reason', None)
4541             instance.save()
4542 
4543         unlock(self, context, instance)
4544         compute_utils.notify_about_instance_action(
4545             context, instance, CONF.host,
4546             action=fields_obj.NotificationAction.UNLOCK,
4547             source=fields_obj.NotificationSource.API)
4548 
4549     @check_instance_lock
4550     def inject_network_info(self, context, instance):
4551         """Inject network info for the instance."""
4552         self.compute_rpcapi.inject_network_info(context, instance=instance)
4553 
4554     def _create_volume_bdm(self, context, instance, device, volume,
4555                            disk_bus, device_type, is_local_creation=False,
4556                            tag=None, delete_on_termination=False):
4557         volume_id = volume['id']
4558         if is_local_creation:
4559             # when the creation is done locally we can't specify the device
4560             # name as we do not have a way to check that the name specified is
4561             # a valid one.
4562             # We leave the setting of that value when the actual attach
4563             # happens on the compute manager
4564             # NOTE(artom) Local attach (to a shelved-offload instance) cannot
4565             # support device tagging because we have no way to call the compute
4566             # manager to check that it supports device tagging. In fact, we
4567             # don't even know which computer manager the instance will
4568             # eventually end up on when it's unshelved.
4569             volume_bdm = objects.BlockDeviceMapping(
4570                 context=context,
4571                 source_type='volume', destination_type='volume',
4572                 instance_uuid=instance.uuid, boot_index=None,
4573                 volume_id=volume_id,
4574                 device_name=None, guest_format=None,
4575                 disk_bus=disk_bus, device_type=device_type,
4576                 delete_on_termination=delete_on_termination)
4577             volume_bdm.create()
4578         else:
4579             # NOTE(vish): This is done on the compute host because we want
4580             #             to avoid a race where two devices are requested at
4581             #             the same time. When db access is removed from
4582             #             compute, the bdm will be created here and we will
4583             #             have to make sure that they are assigned atomically.
4584             volume_bdm = self.compute_rpcapi.reserve_block_device_name(
4585                 context, instance, device, volume_id, disk_bus=disk_bus,
4586                 device_type=device_type, tag=tag,
4587                 multiattach=volume['multiattach'])
4588             volume_bdm.delete_on_termination = delete_on_termination
4589             volume_bdm.save()
4590         return volume_bdm
4591 
4592     def _check_volume_already_attached_to_instance(self, context, instance,
4593                                                    volume_id):
4594         """Avoid attaching the same volume to the same instance twice.
4595 
4596            As the new Cinder flow (microversion 3.44) is handling the checks
4597            differently and allows to attach the same volume to the same
4598            instance twice to enable live_migrate we are checking whether the
4599            BDM already exists for this combination for the new flow and fail
4600            if it does.
4601         """
4602 
4603         try:
4604             objects.BlockDeviceMapping.get_by_volume_and_instance(
4605                 context, volume_id, instance.uuid)
4606 
4607             msg = _("volume %s already attached") % volume_id
4608             raise exception.InvalidVolume(reason=msg)
4609         except exception.VolumeBDMNotFound:
4610             pass
4611 
4612     def _check_volume_already_attached(self, context, volume_id):
4613         """Avoid allowing a non-multiattach volumes being attached twice
4614 
4615         Unlike the above _check_volume_already_attached_to_instance check we
4616         also need to ensure that non-multiattached volumes are not attached to
4617         multiple instances. This check is also carried out later by c-api
4618         itself but it can however be circumvented by admins resetting the state
4619         of an attached volume to available. As a result we also need to perform
4620         a check within Nova before creating a new BDM for the attachment.
4621         """
4622         try:
4623             bdm = objects.BlockDeviceMapping.get_by_volume(
4624                 context, volume_id)
4625             msg = _("volume %(volume_id)s is already attached to instance "
4626                     "%(instance_uuid)s") % {'volume_id': volume_id,
4627                     'instance_uuid': bdm.instance_uuid}
4628             raise exception.InvalidVolume(reason=msg)
4629         except exception.VolumeBDMNotFound:
4630             pass
4631 
4632     def _check_attach_and_reserve_volume(self, context, volume, instance,
4633                                          bdm, supports_multiattach=False,
4634                                          validate_az=True):
4635         """Perform checks against the instance and volume before attaching.
4636 
4637         If validation succeeds, the bdm is updated with an attachment_id which
4638         effectively reserves it during the attach process in cinder.
4639 
4640         :param context: nova auth RequestContext
4641         :param volume: volume dict from cinder
4642         :param instance: Instance object
4643         :param bdm: BlockDeviceMapping object
4644         :param supports_multiattach: True if the request supports multiattach
4645             volumes, i.e. microversion >= 2.60, False otherwise
4646         :param validate_az: True if the instance and volume availability zones
4647             should be validated for cross_az_attach, False to not validate AZ
4648         """
4649         volume_id = volume['id']
4650         if validate_az:
4651             self.volume_api.check_availability_zone(context, volume,
4652                                                     instance=instance)
4653         # If volume.multiattach=True and the microversion to
4654         # support multiattach is not used, fail the request.
4655         if volume['multiattach'] and not supports_multiattach:
4656             raise exception.MultiattachNotSupportedOldMicroversion()
4657 
4658         attachment_id = self.volume_api.attachment_create(
4659             context, volume_id, instance.uuid)['id']
4660         bdm.attachment_id = attachment_id
4661         # NOTE(ildikov): In case of boot from volume the BDM at this
4662         # point is not yet created in a cell database, so we can't
4663         # call save().  When attaching a volume to an existing
4664         # instance, the instance is already in a cell and the BDM has
4665         # been created in that same cell so updating here in that case
4666         # is "ok".
4667         if bdm.obj_attr_is_set('id'):
4668             bdm.save()
4669 
4670     # TODO(stephenfin): Fold this back in now that cells v1 no longer needs to
4671     # override it.
4672     def _attach_volume(self, context, instance, volume, device,
4673                        disk_bus, device_type, tag=None,
4674                        supports_multiattach=False,
4675                        delete_on_termination=False):
4676         """Attach an existing volume to an existing instance.
4677 
4678         This method is separated to make it possible for cells version
4679         to override it.
4680         """
4681         volume_bdm = self._create_volume_bdm(
4682             context, instance, device, volume, disk_bus=disk_bus,
4683             device_type=device_type, tag=tag,
4684             delete_on_termination=delete_on_termination)
4685         try:
4686             self._check_attach_and_reserve_volume(context, volume, instance,
4687                                                   volume_bdm,
4688                                                   supports_multiattach)
4689             self._record_action_start(
4690                 context, instance, instance_actions.ATTACH_VOLUME)
4691             self.compute_rpcapi.attach_volume(context, instance, volume_bdm)
4692         except Exception:
4693             with excutils.save_and_reraise_exception():
4694                 volume_bdm.destroy()
4695 
4696         return volume_bdm.device_name
4697 
4698     def _attach_volume_shelved_offloaded(self, context, instance, volume,
4699                                          device, disk_bus, device_type,
4700                                          delete_on_termination):
4701         """Attach an existing volume to an instance in shelved offloaded state.
4702 
4703         Attaching a volume for an instance in shelved offloaded state requires
4704         to perform the regular check to see if we can attach and reserve the
4705         volume then we need to call the attach method on the volume API
4706         to mark the volume as 'in-use'.
4707         The instance at this stage is not managed by a compute manager
4708         therefore the actual attachment will be performed once the
4709         instance will be unshelved.
4710         """
4711         volume_id = volume['id']
4712 
4713         @wrap_instance_event(prefix='api')
4714         def attach_volume(self, context, v_id, instance, dev, attachment_id):
4715             if attachment_id:
4716                 # Normally we wouldn't complete an attachment without a host
4717                 # connector, but we do this to make the volume status change
4718                 # to "in-use" to maintain the API semantics with the old flow.
4719                 # When unshelving the instance, the compute service will deal
4720                 # with this disconnected attachment.
4721                 self.volume_api.attachment_complete(context, attachment_id)
4722             else:
4723                 self.volume_api.attach(context,
4724                                        v_id,
4725                                        instance.uuid,
4726                                        dev)
4727 
4728         volume_bdm = self._create_volume_bdm(
4729             context, instance, device, volume, disk_bus=disk_bus,
4730             device_type=device_type, is_local_creation=True,
4731             delete_on_termination=delete_on_termination)
4732         try:
4733             self._check_attach_and_reserve_volume(context, volume, instance,
4734                                                   volume_bdm)
4735             self._record_action_start(
4736                 context, instance,
4737                 instance_actions.ATTACH_VOLUME)
4738             attach_volume(self, context, volume_id, instance, device,
4739                           volume_bdm.attachment_id)
4740         except Exception:
4741             with excutils.save_and_reraise_exception():
4742                 volume_bdm.destroy()
4743 
4744         return volume_bdm.device_name
4745 
4746     @check_instance_lock
4747     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
4748                                     vm_states.STOPPED, vm_states.RESIZED,
4749                                     vm_states.SOFT_DELETED, vm_states.SHELVED,
4750                                     vm_states.SHELVED_OFFLOADED])
4751     def attach_volume(self, context, instance, volume_id, device=None,
4752                       disk_bus=None, device_type=None, tag=None,
4753                       supports_multiattach=False,
4754                       delete_on_termination=False):
4755         """Attach an existing volume to an existing instance."""
4756         # NOTE(vish): Fail fast if the device is not going to pass. This
4757         #             will need to be removed along with the test if we
4758         #             change the logic in the manager for what constitutes
4759         #             a valid device.
4760         if device and not block_device.match_device(device):
4761             raise exception.InvalidDevicePath(path=device)
4762 
4763         # Make sure the volume isn't already attached to this instance
4764         # because we'll use the v3.44 attachment flow in
4765         # _check_attach_and_reserve_volume and Cinder will allow multiple
4766         # attachments between the same volume and instance but the old flow
4767         # API semantics don't allow that so we enforce it here.
4768         self._check_volume_already_attached_to_instance(context,
4769                                                         instance,
4770                                                         volume_id)
4771         volume = self.volume_api.get(context, volume_id)
4772 
4773         # NOTE(lyarwood): Ensure that non multiattach volumes don't already
4774         # have active block device mappings present in Nova.
4775         if not volume.get('multiattach', False):
4776             self._check_volume_already_attached(context, volume_id)
4777 
4778         is_shelved_offloaded = instance.vm_state == vm_states.SHELVED_OFFLOADED
4779         if is_shelved_offloaded:
4780             if tag:
4781                 # NOTE(artom) Local attach (to a shelved-offload instance)
4782                 # cannot support device tagging because we have no way to call
4783                 # the compute manager to check that it supports device tagging.
4784                 # In fact, we don't even know which computer manager the
4785                 # instance will eventually end up on when it's unshelved.
4786                 raise exception.VolumeTaggedAttachToShelvedNotSupported()
4787             if volume['multiattach']:
4788                 # NOTE(mriedem): Similar to tagged attach, we don't support
4789                 # attaching a multiattach volume to shelved offloaded instances
4790                 # because we can't tell if the compute host (since there isn't
4791                 # one) supports it. This could possibly be supported in the
4792                 # future if the scheduler was made aware of which computes
4793                 # support multiattach volumes.
4794                 raise exception.MultiattachToShelvedNotSupported()
4795             return self._attach_volume_shelved_offloaded(context,
4796                                                          instance,
4797                                                          volume,
4798                                                          device,
4799                                                          disk_bus,
4800                                                          device_type,
4801                                                          delete_on_termination)
4802 
4803         return self._attach_volume(context, instance, volume, device,
4804                                    disk_bus, device_type, tag=tag,
4805                                    supports_multiattach=supports_multiattach,
4806                                    delete_on_termination=delete_on_termination)
4807 
4808     def _detach_volume_shelved_offloaded(self, context, instance, volume):
4809         """Detach a volume from an instance in shelved offloaded state.
4810 
4811         If the instance is shelved offloaded we just need to cleanup volume
4812         calling the volume api detach, the volume api terminate_connection
4813         and delete the bdm record.
4814         If the volume has delete_on_termination option set then we call the
4815         volume api delete as well.
4816         """
4817         @wrap_instance_event(prefix='api')
4818         def detach_volume(self, context, instance, bdms):
4819             self._local_cleanup_bdm_volumes(bdms, instance, context)
4820 
4821         bdms = [objects.BlockDeviceMapping.get_by_volume_id(
4822                 context, volume['id'], instance.uuid)]
4823         # The begin_detaching() call only works with in-use volumes,
4824         # which will not be the case for volumes attached to a shelved
4825         # offloaded server via the attachments API since those volumes
4826         # will have `reserved` status.
4827         if not bdms[0].attachment_id:
4828             try:
4829                 self.volume_api.begin_detaching(context, volume['id'])
4830             except exception.InvalidInput as exc:
4831                 raise exception.InvalidVolume(reason=exc.format_message())
4832         self._record_action_start(
4833             context, instance,
4834             instance_actions.DETACH_VOLUME)
4835         detach_volume(self, context, instance, bdms)
4836 
4837     @check_instance_lock
4838     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
4839                                     vm_states.STOPPED, vm_states.RESIZED,
4840                                     vm_states.SOFT_DELETED, vm_states.SHELVED,
4841                                     vm_states.SHELVED_OFFLOADED])
4842     def detach_volume(self, context, instance, volume):
4843         """Detach a volume from an instance."""
4844         if instance.vm_state == vm_states.SHELVED_OFFLOADED:
4845             self._detach_volume_shelved_offloaded(context, instance, volume)
4846         else:
4847             try:
4848                 self.volume_api.begin_detaching(context, volume['id'])
4849             except exception.InvalidInput as exc:
4850                 raise exception.InvalidVolume(reason=exc.format_message())
4851             attachments = volume.get('attachments', {})
4852             attachment_id = None
4853             if attachments and instance.uuid in attachments:
4854                 attachment_id = attachments[instance.uuid]['attachment_id']
4855             self._record_action_start(
4856                 context, instance, instance_actions.DETACH_VOLUME)
4857             self.compute_rpcapi.detach_volume(context, instance=instance,
4858                     volume_id=volume['id'], attachment_id=attachment_id)
4859 
4860     def _count_attachments_for_swap(self, ctxt, volume):
4861         """Counts the number of attachments for a swap-related volume.
4862 
4863         Attempts to only count read/write attachments if the volume attachment
4864         records exist, otherwise simply just counts the number of attachments
4865         regardless of attach mode.
4866 
4867         :param ctxt: nova.context.RequestContext - user request context
4868         :param volume: nova-translated volume dict from nova.volume.cinder.
4869         :returns: count of attachments for the volume
4870         """
4871         # This is a dict, keyed by server ID, to a dict of attachment_id and
4872         # mountpoint.
4873         attachments = volume.get('attachments', {})
4874         # Multiattach volumes can have more than one attachment, so if there
4875         # is more than one attachment, attempt to count the read/write
4876         # attachments.
4877         if len(attachments) > 1:
4878             count = 0
4879             for attachment in attachments.values():
4880                 attachment_id = attachment['attachment_id']
4881                 # Get the attachment record for this attachment so we can
4882                 # get the attach_mode.
4883                 # TODO(mriedem): This could be optimized if we had
4884                 # GET /attachments/detail?volume_id=volume['id'] in Cinder.
4885                 try:
4886                     attachment_record = self.volume_api.attachment_get(
4887                         ctxt, attachment_id)
4888                     # Note that the attachment record from Cinder has
4889                     # attach_mode in the top-level of the resource but the
4890                     # nova.volume.cinder code translates it and puts the
4891                     # attach_mode in the connection_info for some legacy
4892                     # reason...
4893                     if attachment_record['attach_mode'] == 'rw':
4894                         count += 1
4895                 except exception.VolumeAttachmentNotFound:
4896                     # attachments are read/write by default so count it
4897                     count += 1
4898         else:
4899             count = len(attachments)
4900 
4901         return count
4902 
4903     @check_instance_lock
4904     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
4905                                     vm_states.RESIZED])
4906     def swap_volume(self, context, instance, old_volume, new_volume):
4907         """Swap volume attached to an instance."""
4908         # The caller likely got the instance from volume['attachments']
4909         # in the first place, but let's sanity check.
4910         if not old_volume.get('attachments', {}).get(instance.uuid):
4911             msg = _("Old volume is attached to a different instance.")
4912             raise exception.InvalidVolume(reason=msg)
4913         if new_volume['attach_status'] == 'attached':
4914             msg = _("New volume must be detached in order to swap.")
4915             raise exception.InvalidVolume(reason=msg)
4916         if int(new_volume['size']) < int(old_volume['size']):
4917             msg = _("New volume must be the same size or larger.")
4918             raise exception.InvalidVolume(reason=msg)
4919         self.volume_api.check_availability_zone(context, new_volume,
4920                                                 instance=instance)
4921         try:
4922             self.volume_api.begin_detaching(context, old_volume['id'])
4923         except exception.InvalidInput as exc:
4924             raise exception.InvalidVolume(reason=exc.format_message())
4925 
4926         # Disallow swapping from multiattach volumes that have more than one
4927         # read/write attachment. We know the old_volume has at least one
4928         # attachment since it's attached to this server. The new_volume
4929         # can't have any attachments because of the attach_status check above.
4930         # We do this count after calling "begin_detaching" to lock against
4931         # concurrent attachments being made while we're counting.
4932         try:
4933             if self._count_attachments_for_swap(context, old_volume) > 1:
4934                 raise exception.MultiattachSwapVolumeNotSupported()
4935         except Exception:  # This is generic to handle failures while counting
4936             # We need to reset the detaching status before raising.
4937             with excutils.save_and_reraise_exception():
4938                 self.volume_api.roll_detaching(context, old_volume['id'])
4939 
4940         # Get the BDM for the attached (old) volume so we can tell if it was
4941         # attached with the new-style Cinder 3.44 API.
4942         bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
4943             context, old_volume['id'], instance.uuid)
4944         new_attachment_id = None
4945         if bdm.attachment_id is None:
4946             # This is an old-style attachment so reserve the new volume before
4947             # we cast to the compute host.
4948             self.volume_api.reserve_volume(context, new_volume['id'])
4949         else:
4950             try:
4951                 self._check_volume_already_attached_to_instance(
4952                     context, instance, new_volume['id'])
4953             except exception.InvalidVolume:
4954                 with excutils.save_and_reraise_exception():
4955                     self.volume_api.roll_detaching(context, old_volume['id'])
4956 
4957             # This is a new-style attachment so for the volume that we are
4958             # going to swap to, create a new volume attachment.
4959             new_attachment_id = self.volume_api.attachment_create(
4960                 context, new_volume['id'], instance.uuid)['id']
4961 
4962         self._record_action_start(
4963             context, instance, instance_actions.SWAP_VOLUME)
4964 
4965         try:
4966             self.compute_rpcapi.swap_volume(
4967                     context, instance=instance,
4968                     old_volume_id=old_volume['id'],
4969                     new_volume_id=new_volume['id'],
4970                     new_attachment_id=new_attachment_id)
4971         except Exception:
4972             with excutils.save_and_reraise_exception():
4973                 self.volume_api.roll_detaching(context, old_volume['id'])
4974                 if new_attachment_id is None:
4975                     self.volume_api.unreserve_volume(context, new_volume['id'])
4976                 else:
4977                     self.volume_api.attachment_delete(
4978                         context, new_attachment_id)
4979 
4980     @check_instance_lock
4981     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
4982                                     vm_states.STOPPED],
4983                           task_state=[None])
4984     def attach_interface(self, context, instance, network_id, port_id,
4985                          requested_ip, tag=None):
4986         """Use hotplug to add an network adapter to an instance."""
4987         self._record_action_start(
4988             context, instance, instance_actions.ATTACH_INTERFACE)
4989 
4990         # NOTE(gibi): Checking if the requested port has resource request as
4991         # such ports are currently not supported as they would at least
4992         # need resource allocation manipulation in placement but might also
4993         # need a new scheduling if resource on this host is not available.
4994         if port_id:
4995             port = self.network_api.show_port(context, port_id)
4996             if port['port'].get(constants.RESOURCE_REQUEST):
4997                 raise exception.AttachInterfaceWithQoSPolicyNotSupported(
4998                     instance_uuid=instance.uuid)
4999 
5000         return self.compute_rpcapi.attach_interface(context,
5001             instance=instance, network_id=network_id, port_id=port_id,
5002             requested_ip=requested_ip, tag=tag)
5003 
5004     @check_instance_lock
5005     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
5006                                     vm_states.STOPPED],
5007                           task_state=[None])
5008     def detach_interface(self, context, instance, port_id):
5009         """Detach an network adapter from an instance."""
5010         self._record_action_start(
5011             context, instance, instance_actions.DETACH_INTERFACE)
5012         self.compute_rpcapi.detach_interface(context, instance=instance,
5013             port_id=port_id)
5014 
5015     def get_instance_metadata(self, context, instance):
5016         """Get all metadata associated with an instance."""
5017         return self.db.instance_metadata_get(context, instance.uuid)
5018 
5019     @check_instance_lock
5020     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
5021                                     vm_states.SUSPENDED, vm_states.STOPPED],
5022                           task_state=None)
5023     def delete_instance_metadata(self, context, instance, key):
5024         """Delete the given metadata item from an instance."""
5025         instance.delete_metadata_key(key)
5026 
5027     @check_instance_lock
5028     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
5029                                     vm_states.SUSPENDED, vm_states.STOPPED],
5030                           task_state=None)
5031     def update_instance_metadata(self, context, instance,
5032                                  metadata, delete=False):
5033         """Updates or creates instance metadata.
5034 
5035         If delete is True, metadata items that are not specified in the
5036         `metadata` argument will be deleted.
5037 
5038         """
5039         if delete:
5040             _metadata = metadata
5041         else:
5042             _metadata = dict(instance.metadata)
5043             _metadata.update(metadata)
5044 
5045         self._check_metadata_properties_quota(context, _metadata)
5046         instance.metadata = _metadata
5047         instance.save()
5048 
5049         return _metadata
5050 
5051     @block_accelerators()
5052     @reject_vtpm_instances(instance_actions.LIVE_MIGRATION)
5053     @reject_sev_instances(instance_actions.LIVE_MIGRATION)
5054     @check_instance_lock
5055     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED])
5056     def live_migrate(self, context, instance, block_migration,
5057                      disk_over_commit, host_name, force=None, async_=False):
5058         """Migrate a server lively to a new host."""
5059         LOG.debug("Going to try to live migrate instance to %s",
5060                   host_name or "another host", instance=instance)
5061 
5062         if host_name:
5063             # Validate the specified host before changing the instance task
5064             # state.
5065             nodes = objects.ComputeNodeList.get_all_by_host(context, host_name)
5066 
5067         request_spec = objects.RequestSpec.get_by_instance_uuid(
5068             context, instance.uuid)
5069 
5070         instance.task_state = task_states.MIGRATING
5071         instance.save(expected_task_state=[None])
5072 
5073         self._record_action_start(context, instance,
5074                                   instance_actions.LIVE_MIGRATION)
5075 
5076         # NOTE(sbauza): Force is a boolean by the new related API version
5077         if force is False and host_name:
5078             # Unset the host to make sure we call the scheduler
5079             # from the conductor LiveMigrationTask. Yes this is tightly-coupled
5080             # to behavior in conductor and not great.
5081             host_name = None
5082             # FIXME(sbauza): Since only Ironic driver uses more than one
5083             # compute per service but doesn't support live migrations,
5084             # let's provide the first one.
5085             target = nodes[0]
5086             destination = objects.Destination(
5087                 host=target.host,
5088                 node=target.hypervisor_hostname
5089             )
5090             # This is essentially a hint to the scheduler to only consider
5091             # the specified host but still run it through the filters.
5092             request_spec.requested_destination = destination
5093 
5094         try:
5095             self.compute_task_api.live_migrate_instance(context, instance,
5096                 host_name, block_migration=block_migration,
5097                 disk_over_commit=disk_over_commit,
5098                 request_spec=request_spec, async_=async_)
5099         except oslo_exceptions.MessagingTimeout as messaging_timeout:
5100             with excutils.save_and_reraise_exception():
5101                 # NOTE(pkoniszewski): It is possible that MessagingTimeout
5102                 # occurs, but LM will still be in progress, so write
5103                 # instance fault to database
5104                 compute_utils.add_instance_fault_from_exc(context,
5105                                                           instance,
5106                                                           messaging_timeout)
5107 
5108     @check_instance_lock
5109     @check_instance_state(vm_state=[vm_states.ACTIVE],
5110                           task_state=[task_states.MIGRATING])
5111     def live_migrate_force_complete(self, context, instance, migration_id):
5112         """Force live migration to complete.
5113 
5114         :param context: Security context
5115         :param instance: The instance that is being migrated
5116         :param migration_id: ID of ongoing migration
5117 
5118         """
5119         LOG.debug("Going to try to force live migration to complete",
5120                   instance=instance)
5121 
5122         # NOTE(pkoniszewski): Get migration object to check if there is ongoing
5123         # live migration for particular instance. Also pass migration id to
5124         # compute to double check and avoid possible race condition.
5125         migration = objects.Migration.get_by_id_and_instance(
5126             context, migration_id, instance.uuid)
5127         if migration.status != 'running':
5128             raise exception.InvalidMigrationState(migration_id=migration_id,
5129                                                   instance_uuid=instance.uuid,
5130                                                   state=migration.status,
5131                                                   method='force complete')
5132 
5133         self._record_action_start(
5134             context, instance, instance_actions.LIVE_MIGRATION_FORCE_COMPLETE)
5135 
5136         self.compute_rpcapi.live_migration_force_complete(
5137             context, instance, migration)
5138 
5139     @check_instance_lock
5140     @check_instance_state(task_state=[task_states.MIGRATING])
5141     def live_migrate_abort(self, context, instance, migration_id,
5142                            support_abort_in_queue=False):
5143         """Abort an in-progress live migration.
5144 
5145         :param context: Security context
5146         :param instance: The instance that is being migrated
5147         :param migration_id: ID of in-progress live migration
5148         :param support_abort_in_queue: Flag indicating whether we can support
5149             abort migrations in "queued" or "preparing" status.
5150 
5151         """
5152         migration = objects.Migration.get_by_id_and_instance(context,
5153                     migration_id, instance.uuid)
5154         LOG.debug("Going to cancel live migration %s",
5155                   migration.id, instance=instance)
5156 
5157         # If the microversion does not support abort migration in queue,
5158         # we are only be able to abort migrations with `running` status;
5159         # if it is supported, we are able to also abort migrations in
5160         # `queued` and `preparing` status.
5161         allowed_states = ['running']
5162         queued_states = ['queued', 'preparing']
5163         if support_abort_in_queue:
5164             # The user requested a microversion that supports aborting a queued
5165             # or preparing live migration. But we need to check that the
5166             # compute service hosting the instance is new enough to support
5167             # aborting a queued/preparing live migration, so we check the
5168             # service version here.
5169             allowed_states.extend(queued_states)
5170 
5171         if migration.status not in allowed_states:
5172             raise exception.InvalidMigrationState(migration_id=migration_id,
5173                     instance_uuid=instance.uuid,
5174                     state=migration.status,
5175                     method='abort live migration')
5176         self._record_action_start(context, instance,
5177                                   instance_actions.LIVE_MIGRATION_CANCEL)
5178 
5179         self.compute_rpcapi.live_migration_abort(context,
5180                 instance, migration.id)
5181 
5182     @reject_vtpm_instances(instance_actions.EVACUATE)
5183     @block_accelerators(until_service=SUPPORT_ACCELERATOR_SERVICE_FOR_REBUILD)
5184     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
5185                                     vm_states.ERROR])
5186     def evacuate(self, context, instance, host, on_shared_storage,
5187                  admin_password=None, force=None):
5188         """Running evacuate to target host.
5189 
5190         Checking vm compute host state, if the host not in expected_state,
5191         raising an exception.
5192 
5193         :param instance: The instance to evacuate
5194         :param host: Target host. if not set, the scheduler will pick up one
5195         :param on_shared_storage: True if instance files on shared storage
5196         :param admin_password: password to set on rebuilt instance
5197         :param force: Force the evacuation to the specific host target
5198 
5199         """
5200         LOG.debug('vm evacuation scheduled', instance=instance)
5201         inst_host = instance.host
5202         service = objects.Service.get_by_compute_host(context, inst_host)
5203         if self.servicegroup_api.service_is_up(service):
5204             LOG.error('Instance compute service state on %s '
5205                       'expected to be down, but it was up.', inst_host)
5206             raise exception.ComputeServiceInUse(host=inst_host)
5207 
5208         request_spec = objects.RequestSpec.get_by_instance_uuid(
5209             context, instance.uuid)
5210 
5211         instance.task_state = task_states.REBUILDING
5212         instance.save(expected_task_state=[None])
5213         self._record_action_start(context, instance, instance_actions.EVACUATE)
5214 
5215         # NOTE(danms): Create this as a tombstone for the source compute
5216         # to find and cleanup. No need to pass it anywhere else.
5217         migration = objects.Migration(
5218             context, source_compute=instance.host, source_node=instance.node,
5219             instance_uuid=instance.uuid, status='accepted',
5220             migration_type=fields_obj.MigrationType.EVACUATION)
5221         if host:
5222             migration.dest_compute = host
5223         migration.create()
5224 
5225         compute_utils.notify_about_instance_usage(
5226             self.notifier, context, instance, "evacuate")
5227         compute_utils.notify_about_instance_action(
5228             context, instance, CONF.host,
5229             action=fields_obj.NotificationAction.EVACUATE,
5230             source=fields_obj.NotificationSource.API)
5231 
5232         # NOTE(sbauza): Force is a boolean by the new related API version
5233         # TODO(stephenfin): Any reason we can't use 'not force' here to handle
5234         # the pre-v2.29 API microversion, which wouldn't set force
5235         if force is False and host:
5236             nodes = objects.ComputeNodeList.get_all_by_host(context, host)
5237             # NOTE(sbauza): Unset the host to make sure we call the scheduler
5238             host = None
5239             # FIXME(sbauza): Since only Ironic driver uses more than one
5240             # compute per service but doesn't support evacuations,
5241             # let's provide the first one.
5242             target = nodes[0]
5243             destination = objects.Destination(
5244                 host=target.host,
5245                 node=target.hypervisor_hostname
5246             )
5247             request_spec.requested_destination = destination
5248 
5249         return self.compute_task_api.rebuild_instance(context,
5250                        instance=instance,
5251                        new_pass=admin_password,
5252                        injected_files=None,
5253                        image_ref=None,
5254                        orig_image_ref=None,
5255                        orig_sys_metadata=None,
5256                        bdms=None,
5257                        recreate=True,
5258                        on_shared_storage=on_shared_storage,
5259                        host=host,
5260                        request_spec=request_spec,
5261                        )
5262 
5263     def get_migrations(self, context, filters):
5264         """Get all migrations for the given filters."""
5265         load_cells()
5266 
5267         migrations = []
5268         for cell in CELLS:
5269             if cell.uuid == objects.CellMapping.CELL0_UUID:
5270                 continue
5271             with nova_context.target_cell(context, cell) as cctxt:
5272                 migrations.extend(objects.MigrationList.get_by_filters(
5273                     cctxt, filters).objects)
5274         return objects.MigrationList(objects=migrations)
5275 
5276     def get_migrations_sorted(self, context, filters, sort_dirs=None,
5277                               sort_keys=None, limit=None, marker=None):
5278         """Get all migrations for the given parameters."""
5279         mig_objs = migration_list.get_migration_objects_sorted(
5280             context, filters, limit, marker, sort_keys, sort_dirs)
5281         # Due to cross-cell resize, we could have duplicate migration records
5282         # while the instance is in VERIFY_RESIZE state in the destination cell
5283         # but the original migration record still exists in the source cell.
5284         # Filter out duplicate migration records here based on which record
5285         # is newer (last updated).
5286 
5287         def _get_newer_obj(obj1, obj2):
5288             # created_at will always be set.
5289             created_at1 = obj1.created_at
5290             created_at2 = obj2.created_at
5291             # updated_at might be None
5292             updated_at1 = obj1.updated_at
5293             updated_at2 = obj2.updated_at
5294             # If both have updated_at, compare using that field.
5295             if updated_at1 and updated_at2:
5296                 if updated_at1 > updated_at2:
5297                     return obj1
5298                 return obj2
5299             # Compare created_at versus updated_at.
5300             if updated_at1:
5301                 if updated_at1 > created_at2:
5302                     return obj1
5303                 return obj2
5304             if updated_at2:
5305                 if updated_at2 > created_at1:
5306                     return obj2
5307                 return obj1
5308             # Compare created_at only.
5309             if created_at1 > created_at2:
5310                 return obj1
5311             return obj2
5312 
5313         # TODO(mriedem): This could be easier if we leveraged the "hidden"
5314         # field on the Migration record and then just did like
5315         # _get_unique_filter_method in the get_all() method for instances.
5316         migrations_by_uuid = collections.OrderedDict()  # maintain sort order
5317         for migration in mig_objs:
5318             if migration.uuid not in migrations_by_uuid:
5319                 migrations_by_uuid[migration.uuid] = migration
5320             else:
5321                 # We have a collision, keep the newer record.
5322                 # Note that using updated_at could be wrong if changes-since or
5323                 # changes-before filters are being used but we have the same
5324                 # issue in _get_unique_filter_method for instances.
5325                 doppelganger = migrations_by_uuid[migration.uuid]
5326                 newer = _get_newer_obj(doppelganger, migration)
5327                 migrations_by_uuid[migration.uuid] = newer
5328         return objects.MigrationList(objects=list(migrations_by_uuid.values()))
5329 
5330     def get_migrations_in_progress_by_instance(self, context, instance_uuid,
5331                                                migration_type=None):
5332         """Get all migrations of an instance in progress."""
5333         return objects.MigrationList.get_in_progress_by_instance(
5334                 context, instance_uuid, migration_type)
5335 
5336     def get_migration_by_id_and_instance(self, context,
5337                                          migration_id, instance_uuid):
5338         """Get the migration of an instance by id."""
5339         return objects.Migration.get_by_id_and_instance(
5340                 context, migration_id, instance_uuid)
5341 
5342     def _get_bdm_by_volume_id(self, context, volume_id, expected_attrs=None):
5343         """Retrieve a BDM without knowing its cell.
5344 
5345         .. note:: The context will be targeted to the cell in which the
5346             BDM is found, if any.
5347 
5348         :param context: The API request context.
5349         :param volume_id: The ID of the volume.
5350         :param expected_attrs: list of any additional attributes that should
5351             be joined when the BDM is loaded from the database.
5352         :raises: nova.exception.VolumeBDMNotFound if not found in any cell
5353         """
5354         load_cells()
5355         for cell in CELLS:
5356             nova_context.set_target_cell(context, cell)
5357             try:
5358                 return objects.BlockDeviceMapping.get_by_volume(
5359                     context, volume_id, expected_attrs=expected_attrs)
5360             except exception.NotFound:
5361                 continue
5362         raise exception.VolumeBDMNotFound(volume_id=volume_id)
5363 
5364     def volume_snapshot_create(self, context, volume_id, create_info):
5365         bdm = self._get_bdm_by_volume_id(
5366             context, volume_id, expected_attrs=['instance'])
5367 
5368         # We allow creating the snapshot in any vm_state as long as there is
5369         # no task being performed on the instance and it has a host.
5370         @check_instance_host()
5371         @check_instance_state(vm_state=None)
5372         def do_volume_snapshot_create(self, context, instance):
5373             self.compute_rpcapi.volume_snapshot_create(context, instance,
5374                     volume_id, create_info)
5375             snapshot = {
5376                 'snapshot': {
5377                     'id': create_info.get('id'),
5378                     'volumeId': volume_id
5379                 }
5380             }
5381             return snapshot
5382 
5383         return do_volume_snapshot_create(self, context, bdm.instance)
5384 
5385     def volume_snapshot_delete(self, context, volume_id, snapshot_id,
5386                                delete_info):
5387         bdm = self._get_bdm_by_volume_id(
5388             context, volume_id, expected_attrs=['instance'])
5389 
5390         # We allow deleting the snapshot in any vm_state as long as there is
5391         # no task being performed on the instance and it has a host.
5392         @check_instance_host()
5393         @check_instance_state(vm_state=None)
5394         def do_volume_snapshot_delete(self, context, instance):
5395             self.compute_rpcapi.volume_snapshot_delete(context, instance,
5396                     volume_id, snapshot_id, delete_info)
5397 
5398         do_volume_snapshot_delete(self, context, bdm.instance)
5399 
5400     def external_instance_event(self, api_context, instances, events):
5401         # NOTE(danms): The external API consumer just provides events,
5402         # but doesn't know where they go. We need to collate lists
5403         # by the host the affected instance is on and dispatch them
5404         # according to host
5405         instances_by_host = collections.defaultdict(list)
5406         events_by_host = collections.defaultdict(list)
5407         hosts_by_instance = collections.defaultdict(list)
5408         cell_contexts_by_host = {}
5409         for instance in instances:
5410             # instance._context is used here since it's already targeted to
5411             # the cell that the instance lives in, and we need to use that
5412             # cell context to lookup any migrations associated to the instance.
5413             hosts, cross_cell_move = self._get_relevant_hosts(
5414                 instance._context, instance)
5415             for host in hosts:
5416                 # NOTE(danms): All instances on a host must have the same
5417                 # mapping, so just use that
5418                 if host not in cell_contexts_by_host:
5419                     # NOTE(mriedem): If the instance is being migrated across
5420                     # cells then we have to get the host mapping to determine
5421                     # which cell a given host is in.
5422                     if cross_cell_move:
5423                         hm = objects.HostMapping.get_by_host(api_context, host)
5424                         ctxt = nova_context.get_admin_context()
5425                         nova_context.set_target_cell(ctxt, hm.cell_mapping)
5426                         cell_contexts_by_host[host] = ctxt
5427                     else:
5428                         # The instance is not migrating across cells so just
5429                         # use the cell-targeted context already in the
5430                         # instance since the host has to be in that same cell.
5431                         cell_contexts_by_host[host] = instance._context
5432 
5433                 instances_by_host[host].append(instance)
5434                 hosts_by_instance[instance.uuid].append(host)
5435 
5436         for event in events:
5437             if event.name == 'volume-extended':
5438                 # Volume extend is a user-initiated operation starting in the
5439                 # Block Storage service API. We record an instance action so
5440                 # the user can monitor the operation to completion.
5441                 host = hosts_by_instance[event.instance_uuid][0]
5442                 cell_context = cell_contexts_by_host[host]
5443                 objects.InstanceAction.action_start(
5444                     cell_context, event.instance_uuid,
5445                     instance_actions.EXTEND_VOLUME, want_result=False)
5446             elif event.name == 'power-update':
5447                 host = hosts_by_instance[event.instance_uuid][0]
5448                 cell_context = cell_contexts_by_host[host]
5449                 if event.tag == external_event_obj.POWER_ON:
5450                     inst_action = instance_actions.START
5451                 elif event.tag == external_event_obj.POWER_OFF:
5452                     inst_action = instance_actions.STOP
5453                 else:
5454                     LOG.warning("Invalid power state %s. Cannot process "
5455                                 "the event %s. Skipping it.", event.tag,
5456                                 event)
5457                     continue
5458                 objects.InstanceAction.action_start(
5459                     cell_context, event.instance_uuid, inst_action,
5460                     want_result=False)
5461 
5462             for host in hosts_by_instance[event.instance_uuid]:
5463                 events_by_host[host].append(event)
5464 
5465         for host in instances_by_host:
5466             cell_context = cell_contexts_by_host[host]
5467 
5468             # TODO(salv-orlando): Handle exceptions raised by the rpc api layer
5469             # in order to ensure that a failure in processing events on a host
5470             # will not prevent processing events on other hosts
5471             self.compute_rpcapi.external_instance_event(
5472                 cell_context, instances_by_host[host], events_by_host[host],
5473                 host=host)
5474 
5475     def _get_relevant_hosts(self, context, instance):
5476         """Get the relevant hosts for an external server event on an instance.
5477 
5478         :param context: nova auth request context targeted at the same cell
5479             that the instance lives in
5480         :param instance: Instance object which is the target of an external
5481             server event
5482         :returns: 2-item tuple of:
5483             - set of at least one host (the host where the instance lives); if
5484               the instance is being migrated the source and dest compute
5485               hostnames are in the returned set
5486             - boolean indicating if the instance is being migrated across cells
5487         """
5488         hosts = set()
5489         hosts.add(instance.host)
5490         cross_cell_move = False
5491         if instance.migration_context is not None:
5492             migration_id = instance.migration_context.migration_id
5493             migration = objects.Migration.get_by_id(context, migration_id)
5494             cross_cell_move = migration.cross_cell_move
5495             hosts.add(migration.dest_compute)
5496             hosts.add(migration.source_compute)
5497             cells_msg = (
5498                 'across cells' if cross_cell_move else 'within the same cell')
5499             LOG.debug('Instance %(instance)s is migrating %(cells_msg)s, '
5500                       'copying events to all relevant hosts: '
5501                       '%(hosts)s', {'cells_msg': cells_msg,
5502                                     'instance': instance.uuid,
5503                                     'hosts': hosts})
5504         return hosts, cross_cell_move
5505 
5506     def get_instance_host_status(self, instance):
5507         if instance.host:
5508             try:
5509                 service = [service for service in instance.services if
5510                            service.binary == 'nova-compute'][0]
5511                 if service.forced_down:
5512                     host_status = fields_obj.HostStatus.DOWN
5513                 elif service.disabled:
5514                     host_status = fields_obj.HostStatus.MAINTENANCE
5515                 else:
5516                     alive = self.servicegroup_api.service_is_up(service)
5517                     host_status = ((alive and fields_obj.HostStatus.UP) or
5518                                    fields_obj.HostStatus.UNKNOWN)
5519             except IndexError:
5520                 host_status = fields_obj.HostStatus.NONE
5521         else:
5522             host_status = fields_obj.HostStatus.NONE
5523         return host_status
5524 
5525     def get_instances_host_statuses(self, instance_list):
5526         host_status_dict = dict()
5527         host_statuses = dict()
5528         for instance in instance_list:
5529             if instance.host:
5530                 if instance.host not in host_status_dict:
5531                     host_status = self.get_instance_host_status(instance)
5532                     host_status_dict[instance.host] = host_status
5533                 else:
5534                     host_status = host_status_dict[instance.host]
5535             else:
5536                 host_status = fields_obj.HostStatus.NONE
5537             host_statuses[instance.uuid] = host_status
5538         return host_statuses
5539 
5540 
5541 def target_host_cell(fn):
5542     """Target a host-based function to a cell.
5543 
5544     Expects to wrap a function of signature:
5545 
5546        func(self, context, host, ...)
5547     """
5548 
5549     @functools.wraps(fn)
5550     def targeted(self, context, host, *args, **kwargs):
5551         mapping = objects.HostMapping.get_by_host(context, host)
5552         nova_context.set_target_cell(context, mapping.cell_mapping)
5553         return fn(self, context, host, *args, **kwargs)
5554     return targeted
5555 
5556 
5557 def _get_service_in_cell_by_host(context, host_name):
5558     # validates the host; ComputeHostNotFound is raised if invalid
5559     try:
5560         mapping = objects.HostMapping.get_by_host(context, host_name)
5561         nova_context.set_target_cell(context, mapping.cell_mapping)
5562         service = objects.Service.get_by_compute_host(context, host_name)
5563     except exception.HostMappingNotFound:
5564         try:
5565             # NOTE(danms): This targets our cell
5566             service = _find_service_in_cell(context, service_host=host_name)
5567         except exception.NotFound:
5568             raise exception.ComputeHostNotFound(host=host_name)
5569     return service
5570 
5571 
5572 def _find_service_in_cell(context, service_id=None, service_host=None):
5573     """Find a service by id or hostname by searching all cells.
5574 
5575     If one matching service is found, return it. If none or multiple
5576     are found, raise an exception.
5577 
5578     :param context: A context.RequestContext
5579     :param service_id: If not none, the DB ID of the service to find
5580     :param service_host: If not None, the hostname of the service to find
5581     :returns: An objects.Service
5582     :raises: ServiceNotUnique if multiple matching IDs are found
5583     :raises: NotFound if no matches are found
5584     :raises: NovaException if called with neither search option
5585     """
5586 
5587     load_cells()
5588     service = None
5589     found_in_cell = None
5590 
5591     is_uuid = False
5592     if service_id is not None:
5593         is_uuid = uuidutils.is_uuid_like(service_id)
5594         if is_uuid:
5595             lookup_fn = lambda c: objects.Service.get_by_uuid(c, service_id)
5596         else:
5597             lookup_fn = lambda c: objects.Service.get_by_id(c, service_id)
5598     elif service_host is not None:
5599         lookup_fn = lambda c: (
5600             objects.Service.get_by_compute_host(c, service_host))
5601     else:
5602         LOG.exception('_find_service_in_cell called with no search parameters')
5603         # This is intentionally cryptic so we don't leak implementation details
5604         # out of the API.
5605         raise exception.NovaException()
5606 
5607     for cell in CELLS:
5608         # NOTE(danms): Services can be in cell0, so don't skip it here
5609         try:
5610             with nova_context.target_cell(context, cell) as cctxt:
5611                 cell_service = lookup_fn(cctxt)
5612         except exception.NotFound:
5613             # NOTE(danms): Keep looking in other cells
5614             continue
5615         if service and cell_service:
5616             raise exception.ServiceNotUnique()
5617         service = cell_service
5618         found_in_cell = cell
5619         if service and is_uuid:
5620             break
5621 
5622     if service:
5623         # NOTE(danms): Set the cell on the context so it remains
5624         # when we return to our caller
5625         nova_context.set_target_cell(context, found_in_cell)
5626         return service
5627     else:
5628         raise exception.NotFound()
5629 
5630 
5631 class HostAPI(base.Base):
5632     """Sub-set of the Compute Manager API for managing host operations."""
5633 
5634     def __init__(self, rpcapi=None, servicegroup_api=None):
5635         self.rpcapi = rpcapi or compute_rpcapi.ComputeAPI()
5636         self.servicegroup_api = servicegroup_api or servicegroup.API()
5637         super(HostAPI, self).__init__()
5638 
5639     def _assert_host_exists(self, context, host_name, must_be_up=False):
5640         """Raise HostNotFound if compute host doesn't exist."""
5641         service = objects.Service.get_by_compute_host(context, host_name)
5642         if not service:
5643             raise exception.HostNotFound(host=host_name)
5644         if must_be_up and not self.servicegroup_api.service_is_up(service):
5645             raise exception.ComputeServiceUnavailable(host=host_name)
5646         return service['host']
5647 
5648     @wrap_exception()
5649     @target_host_cell
5650     def set_host_enabled(self, context, host_name, enabled):
5651         """Sets the specified host's ability to accept new instances."""
5652         host_name = self._assert_host_exists(context, host_name)
5653         payload = {'host_name': host_name, 'enabled': enabled}
5654         compute_utils.notify_about_host_update(context,
5655                                                'set_enabled.start',
5656                                                payload)
5657         result = self.rpcapi.set_host_enabled(context, enabled=enabled,
5658                 host=host_name)
5659         compute_utils.notify_about_host_update(context,
5660                                                'set_enabled.end',
5661                                                payload)
5662         return result
5663 
5664     @target_host_cell
5665     def get_host_uptime(self, context, host_name):
5666         """Returns the result of calling "uptime" on the target host."""
5667         host_name = self._assert_host_exists(context, host_name,
5668                          must_be_up=True)
5669         return self.rpcapi.get_host_uptime(context, host=host_name)
5670 
5671     @wrap_exception()
5672     @target_host_cell
5673     def host_power_action(self, context, host_name, action):
5674         """Reboots, shuts down or powers up the host."""
5675         host_name = self._assert_host_exists(context, host_name)
5676         payload = {'host_name': host_name, 'action': action}
5677         compute_utils.notify_about_host_update(context,
5678                                                'power_action.start',
5679                                                payload)
5680         result = self.rpcapi.host_power_action(context, action=action,
5681                 host=host_name)
5682         compute_utils.notify_about_host_update(context,
5683                                                'power_action.end',
5684                                                payload)
5685         return result
5686 
5687     @wrap_exception()
5688     @target_host_cell
5689     def set_host_maintenance(self, context, host_name, mode):
5690         """Start/Stop host maintenance window. On start, it triggers
5691         guest VMs evacuation.
5692         """
5693         host_name = self._assert_host_exists(context, host_name)
5694         payload = {'host_name': host_name, 'mode': mode}
5695         compute_utils.notify_about_host_update(context,
5696                                                'set_maintenance.start',
5697                                                payload)
5698         result = self.rpcapi.host_maintenance_mode(context,
5699                 host_param=host_name, mode=mode, host=host_name)
5700         compute_utils.notify_about_host_update(context,
5701                                                'set_maintenance.end',
5702                                                payload)
5703         return result
5704 
5705     def service_get_all(self, context, filters=None, set_zones=False,
5706                         all_cells=False, cell_down_support=False):
5707         """Returns a list of services, optionally filtering the results.
5708 
5709         If specified, 'filters' should be a dictionary containing services
5710         attributes and matching values.  Ie, to get a list of services for
5711         the 'compute' topic, use filters={'topic': 'compute'}.
5712 
5713         If all_cells=True, then scan all cells and merge the results.
5714 
5715         If cell_down_support=True then return minimal service records
5716         for cells that do not respond based on what we have in the
5717         host mappings. These will have only 'binary' and 'host' set.
5718         """
5719         if filters is None:
5720             filters = {}
5721         disabled = filters.pop('disabled', None)
5722         if 'availability_zone' in filters:
5723             set_zones = True
5724 
5725         # NOTE(danms): Eventually this all_cells nonsense should go away
5726         # and we should always iterate over the cells. However, certain
5727         # callers need the legacy behavior for now.
5728         if all_cells:
5729             services = []
5730             service_dict = nova_context.scatter_gather_all_cells(context,
5731                 objects.ServiceList.get_all, disabled, set_zones=set_zones)
5732             for cell_uuid, service in service_dict.items():
5733                 if not nova_context.is_cell_failure_sentinel(service):
5734                     services.extend(service)
5735                 elif cell_down_support:
5736                     unavailable_services = objects.ServiceList()
5737                     cid = [cm.id for cm in nova_context.CELLS
5738                            if cm.uuid == cell_uuid]
5739                     # We know cid[0] is in the list because we are using the
5740                     # same list that scatter_gather_all_cells used
5741                     hms = objects.HostMappingList.get_by_cell_id(context,
5742                                                                  cid[0])
5743                     for hm in hms:
5744                         unavailable_services.objects.append(objects.Service(
5745                             binary='nova-compute', host=hm.host))
5746                     LOG.warning("Cell %s is not responding and hence only "
5747                                 "partial results are available from this "
5748                                 "cell.", cell_uuid)
5749                     services.extend(unavailable_services)
5750                 else:
5751                     LOG.warning("Cell %s is not responding and hence skipped "
5752                                 "from the results.", cell_uuid)
5753         else:
5754             services = objects.ServiceList.get_all(context, disabled,
5755                                                    set_zones=set_zones)
5756         ret_services = []
5757         for service in services:
5758             for key, val in filters.items():
5759                 if service[key] != val:
5760                     break
5761             else:
5762                 # All filters matched.
5763                 ret_services.append(service)
5764         return ret_services
5765 
5766     def service_get_by_id(self, context, service_id):
5767         """Get service entry for the given service id or uuid."""
5768         try:
5769             return _find_service_in_cell(context, service_id=service_id)
5770         except exception.NotFound:
5771             raise exception.ServiceNotFound(service_id=service_id)
5772 
5773     @target_host_cell
5774     def service_get_by_compute_host(self, context, host_name):
5775         """Get service entry for the given compute hostname."""
5776         return objects.Service.get_by_compute_host(context, host_name)
5777 
5778     def _update_compute_provider_status(self, context, service):
5779         """Calls the compute service to sync the COMPUTE_STATUS_DISABLED trait.
5780 
5781         There are two cases where the API will not call the compute service:
5782 
5783         * The compute service is down. In this case the trait is synchronized
5784           when the compute service is restarted.
5785         * The compute service is old. In this case the trait is synchronized
5786           when the compute service is upgraded and restarted.
5787 
5788         :param context: nova auth RequestContext
5789         :param service: nova.objects.Service object which has been enabled
5790             or disabled (see ``service_update``).
5791         """
5792         # Make sure the service is up so we can make the RPC call.
5793         if not self.servicegroup_api.service_is_up(service):
5794             LOG.info('Compute service on host %s is down. The '
5795                      'COMPUTE_STATUS_DISABLED trait will be synchronized '
5796                      'when the service is restarted.', service.host)
5797             return
5798 
5799         # Make sure the compute service is new enough for the trait sync
5800         # behavior.
5801         # TODO(mriedem): Remove this compat check in the U release.
5802         if service.version < MIN_COMPUTE_SYNC_COMPUTE_STATUS_DISABLED:
5803             LOG.info('Compute service on host %s is too old to sync the '
5804                      'COMPUTE_STATUS_DISABLED trait in Placement. The '
5805                      'trait will be synchronized when the service is '
5806                      'upgraded and restarted.', service.host)
5807             return
5808 
5809         enabled = not service.disabled
5810         # Avoid leaking errors out of the API.
5811         try:
5812             LOG.debug('Calling the compute service on host %s to sync the '
5813                       'COMPUTE_STATUS_DISABLED trait.', service.host)
5814             self.rpcapi.set_host_enabled(context, service.host, enabled)
5815         except Exception:
5816             LOG.exception('An error occurred while updating the '
5817                           'COMPUTE_STATUS_DISABLED trait on compute node '
5818                           'resource providers managed by host %s. The trait '
5819                           'will be synchronized automatically by the compute '
5820                           'service when the update_available_resource '
5821                           'periodic task runs.', service.host)
5822 
5823     def service_update(self, context, service):
5824         """Performs the actual service update operation.
5825 
5826         If the "disabled" field is changed, potentially calls the compute
5827         service to sync the COMPUTE_STATUS_DISABLED trait on the compute node
5828         resource providers managed by this compute service.
5829 
5830         :param context: nova auth RequestContext
5831         :param service: nova.objects.Service object with changes already
5832             set on the object
5833         """
5834         # Before persisting changes and resetting the changed fields on the
5835         # Service object, determine if the disabled field changed.
5836         update_placement = 'disabled' in service.obj_what_changed()
5837         # Persist the Service object changes to the database.
5838         service.save()
5839         # If the disabled field changed, potentially call the compute service
5840         # to sync the COMPUTE_STATUS_DISABLED trait.
5841         if update_placement:
5842             self._update_compute_provider_status(context, service)
5843         return service
5844 
5845     @target_host_cell
5846     def service_update_by_host_and_binary(self, context, host_name, binary,
5847                                           params_to_update):
5848         """Enable / Disable a service.
5849 
5850         Determines the cell that the service is in using the HostMapping.
5851 
5852         For compute services, this stops new builds and migrations going to
5853         the host.
5854 
5855         See also ``service_update``.
5856 
5857         :param context: nova auth RequestContext
5858         :param host_name: hostname of the service
5859         :param binary: service binary (really only supports "nova-compute")
5860         :param params_to_update: dict of changes to make to the Service object
5861         :raises: HostMappingNotFound if the host is not mapped to a cell
5862         :raises: HostBinaryNotFound if a services table record is not found
5863             with the given host_name and binary
5864         """
5865         # TODO(mriedem): Service.get_by_args is deprecated; we should use
5866         # get_by_compute_host here (remember to update the "raises" docstring).
5867         service = objects.Service.get_by_args(context, host_name, binary)
5868         service.update(params_to_update)
5869         return self.service_update(context, service)
5870 
5871     @target_host_cell
5872     def instance_get_all_by_host(self, context, host_name):
5873         """Return all instances on the given host."""
5874         return objects.InstanceList.get_by_host(context, host_name)
5875 
5876     def task_log_get_all(self, context, task_name, period_beginning,
5877                          period_ending, host=None, state=None):
5878         """Return the task logs within a given range, optionally
5879         filtering by host and/or state.
5880         """
5881         return self.db.task_log_get_all(context, task_name,
5882                                         period_beginning,
5883                                         period_ending,
5884                                         host=host,
5885                                         state=state)
5886 
5887     def compute_node_get(self, context, compute_id):
5888         """Return compute node entry for particular integer ID or UUID."""
5889         load_cells()
5890 
5891         # NOTE(danms): Unfortunately this API exposes database identifiers
5892         # which means we really can't do something efficient here
5893         is_uuid = uuidutils.is_uuid_like(compute_id)
5894         for cell in CELLS:
5895             if cell.uuid == objects.CellMapping.CELL0_UUID:
5896                 continue
5897             with nova_context.target_cell(context, cell) as cctxt:
5898                 try:
5899                     if is_uuid:
5900                         return objects.ComputeNode.get_by_uuid(cctxt,
5901                                                                compute_id)
5902                     return objects.ComputeNode.get_by_id(cctxt,
5903                                                          int(compute_id))
5904                 except exception.ComputeHostNotFound:
5905                     # NOTE(danms): Keep looking in other cells
5906                     continue
5907 
5908         raise exception.ComputeHostNotFound(host=compute_id)
5909 
5910     def compute_node_get_all(self, context, limit=None, marker=None):
5911         load_cells()
5912 
5913         computes = []
5914         uuid_marker = marker and uuidutils.is_uuid_like(marker)
5915         for cell in CELLS:
5916             if cell.uuid == objects.CellMapping.CELL0_UUID:
5917                 continue
5918             with nova_context.target_cell(context, cell) as cctxt:
5919 
5920                 # If we have a marker and it's a uuid, see if the compute node
5921                 # is in this cell.
5922                 if marker and uuid_marker:
5923                     try:
5924                         compute_marker = objects.ComputeNode.get_by_uuid(
5925                             cctxt, marker)
5926                         # we found the marker compute node, so use it's id
5927                         # for the actual marker for paging in this cell's db
5928                         marker = compute_marker.id
5929                     except exception.ComputeHostNotFound:
5930                         # The marker node isn't in this cell so keep looking.
5931                         continue
5932 
5933                 try:
5934                     cell_computes = objects.ComputeNodeList.get_by_pagination(
5935                         cctxt, limit=limit, marker=marker)
5936                 except exception.MarkerNotFound:
5937                     # NOTE(danms): Keep looking through cells
5938                     continue
5939                 computes.extend(cell_computes)
5940                 # NOTE(danms): We must have found the marker, so continue on
5941                 # without one
5942                 marker = None
5943                 if limit:
5944                     limit -= len(cell_computes)
5945                     if limit <= 0:
5946                         break
5947 
5948         if marker is not None and len(computes) == 0:
5949             # NOTE(danms): If we did not find the marker in any cell,
5950             # mimic the db_api behavior here.
5951             raise exception.MarkerNotFound(marker=marker)
5952 
5953         return objects.ComputeNodeList(objects=computes)
5954 
5955     def compute_node_search_by_hypervisor(self, context, hypervisor_match):
5956         load_cells()
5957 
5958         computes = []
5959         for cell in CELLS:
5960             if cell.uuid == objects.CellMapping.CELL0_UUID:
5961                 continue
5962             with nova_context.target_cell(context, cell) as cctxt:
5963                 cell_computes = objects.ComputeNodeList.get_by_hypervisor(
5964                     cctxt, hypervisor_match)
5965             computes.extend(cell_computes)
5966         return objects.ComputeNodeList(objects=computes)
5967 
5968     def compute_node_statistics(self, context):
5969         load_cells()
5970 
5971         cell_stats = []
5972         for cell in CELLS:
5973             if cell.uuid == objects.CellMapping.CELL0_UUID:
5974                 continue
5975             with nova_context.target_cell(context, cell) as cctxt:
5976                 cell_stats.append(self.db.compute_node_statistics(cctxt))
5977 
5978         if cell_stats:
5979             keys = cell_stats[0].keys()
5980             return {k: sum(stats[k] for stats in cell_stats)
5981                     for k in keys}
5982         else:
5983             return {}
5984 
5985 
5986 class InstanceActionAPI(base.Base):
5987     """Sub-set of the Compute Manager API for managing instance actions."""
5988 
5989     def actions_get(self, context, instance, limit=None, marker=None,
5990                     filters=None):
5991         return objects.InstanceActionList.get_by_instance_uuid(
5992             context, instance.uuid, limit, marker, filters)
5993 
5994     def action_get_by_request_id(self, context, instance, request_id):
5995         return objects.InstanceAction.get_by_request_id(
5996             context, instance.uuid, request_id)
5997 
5998     def action_events_get(self, context, instance, action_id):
5999         return objects.InstanceActionEventList.get_by_action(
6000             context, action_id)
6001 
6002 
6003 class AggregateAPI(base.Base):
6004     """Sub-set of the Compute Manager API for managing host aggregates."""
6005     def __init__(self, **kwargs):
6006         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
6007         self.query_client = query.SchedulerQueryClient()
6008         self._placement_client = None  # Lazy-load on first access.
6009         super(AggregateAPI, self).__init__(**kwargs)
6010 
6011     @property
6012     def placement_client(self):
6013         if self._placement_client is None:
6014             self._placement_client = report.SchedulerReportClient()
6015         return self._placement_client
6016 
6017     @wrap_exception()
6018     def create_aggregate(self, context, aggregate_name, availability_zone):
6019         """Creates the model for the aggregate."""
6020 
6021         aggregate = objects.Aggregate(context=context)
6022         aggregate.name = aggregate_name
6023         if availability_zone:
6024             aggregate.metadata = {'availability_zone': availability_zone}
6025         aggregate.create()
6026         self.query_client.update_aggregates(context, [aggregate])
6027         return aggregate
6028 
6029     def get_aggregate(self, context, aggregate_id):
6030         """Get an aggregate by id."""
6031         return objects.Aggregate.get_by_id(context, aggregate_id)
6032 
6033     def get_aggregate_list(self, context):
6034         """Get all the aggregates."""
6035         return objects.AggregateList.get_all(context)
6036 
6037     def get_aggregates_by_host(self, context, compute_host):
6038         """Get all the aggregates where the given host is presented."""
6039         return objects.AggregateList.get_by_host(context, compute_host)
6040 
6041     @wrap_exception()
6042     def update_aggregate(self, context, aggregate_id, values):
6043         """Update the properties of an aggregate."""
6044         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
6045         if 'name' in values:
6046             aggregate.name = values.pop('name')
6047             aggregate.save()
6048         self.is_safe_to_update_az(context, values, aggregate=aggregate,
6049                                   action_name=AGGREGATE_ACTION_UPDATE,
6050                                   check_no_instances_in_az=True)
6051         if values:
6052             aggregate.update_metadata(values)
6053             aggregate.updated_at = timeutils.utcnow()
6054         self.query_client.update_aggregates(context, [aggregate])
6055         # If updated values include availability_zones, then the cache
6056         # which stored availability_zones and host need to be reset
6057         if values.get('availability_zone'):
6058             availability_zones.reset_cache()
6059         return aggregate
6060 
6061     @wrap_exception()
6062     def update_aggregate_metadata(self, context, aggregate_id, metadata):
6063         """Updates the aggregate metadata."""
6064         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
6065         self.is_safe_to_update_az(context, metadata, aggregate=aggregate,
6066                                   action_name=AGGREGATE_ACTION_UPDATE_META,
6067                                   check_no_instances_in_az=True)
6068         aggregate.update_metadata(metadata)
6069         self.query_client.update_aggregates(context, [aggregate])
6070         # If updated metadata include availability_zones, then the cache
6071         # which stored availability_zones and host need to be reset
6072         if metadata and metadata.get('availability_zone'):
6073             availability_zones.reset_cache()
6074         aggregate.updated_at = timeutils.utcnow()
6075         return aggregate
6076 
6077     @wrap_exception()
6078     def delete_aggregate(self, context, aggregate_id):
6079         """Deletes the aggregate."""
6080         aggregate_payload = {'aggregate_id': aggregate_id}
6081         compute_utils.notify_about_aggregate_update(context,
6082                                                     "delete.start",
6083                                                     aggregate_payload)
6084         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
6085 
6086         compute_utils.notify_about_aggregate_action(
6087             context=context,
6088             aggregate=aggregate,
6089             action=fields_obj.NotificationAction.DELETE,
6090             phase=fields_obj.NotificationPhase.START)
6091 
6092         if len(aggregate.hosts) > 0:
6093             msg = _("Host aggregate is not empty")
6094             raise exception.InvalidAggregateActionDelete(
6095                 aggregate_id=aggregate_id, reason=msg)
6096         aggregate.destroy()
6097         self.query_client.delete_aggregate(context, aggregate)
6098         compute_utils.notify_about_aggregate_update(context,
6099                                                     "delete.end",
6100                                                     aggregate_payload)
6101         compute_utils.notify_about_aggregate_action(
6102             context=context,
6103             aggregate=aggregate,
6104             action=fields_obj.NotificationAction.DELETE,
6105             phase=fields_obj.NotificationPhase.END)
6106 
6107     def is_safe_to_update_az(self, context, metadata, aggregate,
6108                              hosts=None,
6109                              action_name=AGGREGATE_ACTION_ADD,
6110                              check_no_instances_in_az=False):
6111         """Determine if updates alter an aggregate's availability zone.
6112 
6113             :param context: local context
6114             :param metadata: Target metadata for updating aggregate
6115             :param aggregate: Aggregate to update
6116             :param hosts: Hosts to check. If None, aggregate.hosts is used
6117             :type hosts: list
6118             :param action_name: Calling method for logging purposes
6119             :param check_no_instances_in_az: if True, it checks
6120                 there is no instances on any hosts of the aggregate
6121 
6122         """
6123         if 'availability_zone' in metadata:
6124             if not metadata['availability_zone']:
6125                 msg = _("Aggregate %s does not support empty named "
6126                         "availability zone") % aggregate.name
6127                 self._raise_invalid_aggregate_exc(action_name, aggregate.id,
6128                                                   msg)
6129             _hosts = hosts or aggregate.hosts
6130             host_aggregates = objects.AggregateList.get_by_metadata_key(
6131                 context, 'availability_zone', hosts=_hosts)
6132             conflicting_azs = [
6133                 agg.availability_zone for agg in host_aggregates
6134                 if agg.availability_zone != metadata['availability_zone'] and
6135                 agg.id != aggregate.id]
6136             if conflicting_azs:
6137                 msg = _("One or more hosts already in availability zone(s) "
6138                         "%s") % conflicting_azs
6139                 self._raise_invalid_aggregate_exc(action_name, aggregate.id,
6140                                                   msg)
6141             same_az_name = (aggregate.availability_zone ==
6142                             metadata['availability_zone'])
6143             if check_no_instances_in_az and not same_az_name:
6144                 instance_count_by_cell = (
6145                     nova_context.scatter_gather_skip_cell0(
6146                         context,
6147                         objects.InstanceList.get_count_by_hosts,
6148                         _hosts))
6149                 if any(cnt for cnt in instance_count_by_cell.values()):
6150                     msg = _("One or more hosts contain instances in this zone")
6151                     self._raise_invalid_aggregate_exc(
6152                         action_name, aggregate.id, msg)
6153 
6154     def _raise_invalid_aggregate_exc(self, action_name, aggregate_id, reason):
6155         if action_name == AGGREGATE_ACTION_ADD:
6156             raise exception.InvalidAggregateActionAdd(
6157                 aggregate_id=aggregate_id, reason=reason)
6158         elif action_name == AGGREGATE_ACTION_UPDATE:
6159             raise exception.InvalidAggregateActionUpdate(
6160                 aggregate_id=aggregate_id, reason=reason)
6161         elif action_name == AGGREGATE_ACTION_UPDATE_META:
6162             raise exception.InvalidAggregateActionUpdateMeta(
6163                 aggregate_id=aggregate_id, reason=reason)
6164         elif action_name == AGGREGATE_ACTION_DELETE:
6165             raise exception.InvalidAggregateActionDelete(
6166                 aggregate_id=aggregate_id, reason=reason)
6167 
6168         raise exception.NovaException(
6169             _("Unexpected aggregate action %s") % action_name)
6170 
6171     def _update_az_cache_for_host(self, context, host_name, aggregate_meta):
6172         # Update the availability_zone cache to avoid getting wrong
6173         # availability_zone in cache retention time when add/remove
6174         # host to/from aggregate.
6175         if aggregate_meta and aggregate_meta.get('availability_zone'):
6176             availability_zones.update_host_availability_zone_cache(context,
6177                                                                    host_name)
6178 
6179     @wrap_exception()
6180     def add_host_to_aggregate(self, context, aggregate_id, host_name):
6181         """Adds the host to an aggregate."""
6182         aggregate_payload = {'aggregate_id': aggregate_id,
6183                              'host_name': host_name}
6184         compute_utils.notify_about_aggregate_update(context,
6185                                                     "addhost.start",
6186                                                     aggregate_payload)
6187 
6188         service = _get_service_in_cell_by_host(context, host_name)
6189         if service.host != host_name:
6190             # NOTE(danms): If we found a service but it is not an
6191             # exact match, we may have a case-insensitive backend
6192             # database (like mysql) which will end up with us
6193             # adding the host-aggregate mapping with a
6194             # non-matching hostname.
6195             raise exception.ComputeHostNotFound(host=host_name)
6196 
6197         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
6198 
6199         compute_utils.notify_about_aggregate_action(
6200             context=context,
6201             aggregate=aggregate,
6202             action=fields_obj.NotificationAction.ADD_HOST,
6203             phase=fields_obj.NotificationPhase.START)
6204 
6205         self.is_safe_to_update_az(context, aggregate.metadata,
6206                                   hosts=[host_name], aggregate=aggregate)
6207 
6208         aggregate.add_host(host_name)
6209         self.query_client.update_aggregates(context, [aggregate])
6210         nodes = objects.ComputeNodeList.get_all_by_host(context, host_name)
6211         node_name = nodes[0].hypervisor_hostname
6212         try:
6213             self.placement_client.aggregate_add_host(
6214                 context, aggregate.uuid, host_name=node_name)
6215         except (exception.ResourceProviderNotFound,
6216                 exception.ResourceProviderAggregateRetrievalFailed,
6217                 exception.ResourceProviderUpdateFailed,
6218                 exception.ResourceProviderUpdateConflict) as err:
6219             # NOTE(jaypipes): We don't want a failure perform the mirroring
6220             # action in the placement service to be returned to the user (they
6221             # probably don't know anything about the placement service and
6222             # would just be confused). So, we just log a warning here, noting
6223             # that on the next run of nova-manage placement sync_aggregates
6224             # things will go back to normal
6225             LOG.warning("Failed to associate %s with a placement "
6226                         "aggregate: %s. This may be corrected after running "
6227                         "nova-manage placement sync_aggregates.",
6228                         node_name, err)
6229         self._update_az_cache_for_host(context, host_name, aggregate.metadata)
6230         aggregate_payload.update({'name': aggregate.name})
6231         compute_utils.notify_about_aggregate_update(context,
6232                                                     "addhost.end",
6233                                                     aggregate_payload)
6234         compute_utils.notify_about_aggregate_action(
6235             context=context,
6236             aggregate=aggregate,
6237             action=fields_obj.NotificationAction.ADD_HOST,
6238             phase=fields_obj.NotificationPhase.END)
6239 
6240         return aggregate
6241 
6242     @wrap_exception()
6243     def remove_host_from_aggregate(self, context, aggregate_id, host_name):
6244         """Removes host from the aggregate."""
6245         aggregate_payload = {'aggregate_id': aggregate_id,
6246                              'host_name': host_name}
6247         compute_utils.notify_about_aggregate_update(context,
6248                                                     "removehost.start",
6249                                                     aggregate_payload)
6250         _get_service_in_cell_by_host(context, host_name)
6251         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
6252 
6253         compute_utils.notify_about_aggregate_action(
6254             context=context,
6255             aggregate=aggregate,
6256             action=fields_obj.NotificationAction.REMOVE_HOST,
6257             phase=fields_obj.NotificationPhase.START)
6258 
6259         # Remove the resource provider from the provider aggregate first before
6260         # we change anything on the nova side because if we did the nova stuff
6261         # first we can't re-attempt this from the compute API if cleaning up
6262         # placement fails.
6263         nodes = objects.ComputeNodeList.get_all_by_host(context, host_name)
6264         node_name = nodes[0].hypervisor_hostname
6265         try:
6266             # Anything else this raises is handled in the route handler as
6267             # either a 409 (ResourceProviderUpdateConflict) or 500.
6268             self.placement_client.aggregate_remove_host(
6269                 context, aggregate.uuid, node_name)
6270         except exception.ResourceProviderNotFound as err:
6271             # If the resource provider is not found then it's likely not part
6272             # of the aggregate anymore anyway since provider aggregates are
6273             # not resources themselves with metadata like nova aggregates, they
6274             # are just a grouping concept around resource providers. Log and
6275             # continue.
6276             LOG.warning("Failed to remove association of %s with a placement "
6277                         "aggregate: %s.", node_name, err)
6278 
6279         aggregate.delete_host(host_name)
6280         self.query_client.update_aggregates(context, [aggregate])
6281         self._update_az_cache_for_host(context, host_name, aggregate.metadata)
6282         compute_utils.notify_about_aggregate_update(context,
6283                                                     "removehost.end",
6284                                                     aggregate_payload)
6285         compute_utils.notify_about_aggregate_action(
6286             context=context,
6287             aggregate=aggregate,
6288             action=fields_obj.NotificationAction.REMOVE_HOST,
6289             phase=fields_obj.NotificationPhase.END)
6290         return aggregate
6291 
6292 
6293 class KeypairAPI(base.Base):
6294     """Subset of the Compute Manager API for managing key pairs."""
6295 
6296     get_notifier = functools.partial(rpc.get_notifier, service='api')
6297     wrap_exception = functools.partial(exception_wrapper.wrap_exception,
6298                                        get_notifier=get_notifier,
6299                                        binary='nova-api')
6300 
6301     def _notify(self, context, event_suffix, keypair_name):
6302         payload = {
6303             'tenant_id': context.project_id,
6304             'user_id': context.user_id,
6305             'key_name': keypair_name,
6306         }
6307         notify = self.get_notifier()
6308         notify.info(context, 'keypair.%s' % event_suffix, payload)
6309 
6310     def _validate_new_key_pair(self, context, user_id, key_name, key_type):
6311         safe_chars = "_- " + string.digits + string.ascii_letters
6312         clean_value = "".join(x for x in key_name if x in safe_chars)
6313         if clean_value != key_name:
6314             raise exception.InvalidKeypair(
6315                 reason=_("Keypair name contains unsafe characters"))
6316 
6317         try:
6318             utils.check_string_length(key_name, min_length=1, max_length=255)
6319         except exception.InvalidInput:
6320             raise exception.InvalidKeypair(
6321                 reason=_('Keypair name must be string and between '
6322                          '1 and 255 characters long'))
6323         try:
6324             objects.Quotas.check_deltas(context, {'key_pairs': 1}, user_id)
6325         except exception.OverQuota:
6326             raise exception.KeypairLimitExceeded()
6327 
6328     @wrap_exception()
6329     def import_key_pair(self, context, user_id, key_name, public_key,
6330                         key_type=keypair_obj.KEYPAIR_TYPE_SSH):
6331         """Import a key pair using an existing public key."""
6332         self._validate_new_key_pair(context, user_id, key_name, key_type)
6333 
6334         self._notify(context, 'import.start', key_name)
6335 
6336         keypair = objects.KeyPair(context)
6337         keypair.user_id = user_id
6338         keypair.name = key_name
6339         keypair.type = key_type
6340         keypair.fingerprint = None
6341         keypair.public_key = public_key
6342 
6343         compute_utils.notify_about_keypair_action(
6344             context=context,
6345             keypair=keypair,
6346             action=fields_obj.NotificationAction.IMPORT,
6347             phase=fields_obj.NotificationPhase.START)
6348 
6349         fingerprint = self._generate_fingerprint(public_key, key_type)
6350 
6351         keypair.fingerprint = fingerprint
6352         keypair.create()
6353 
6354         compute_utils.notify_about_keypair_action(
6355             context=context,
6356             keypair=keypair,
6357             action=fields_obj.NotificationAction.IMPORT,
6358             phase=fields_obj.NotificationPhase.END)
6359         self._notify(context, 'import.end', key_name)
6360 
6361         return keypair
6362 
6363     @wrap_exception()
6364     def create_key_pair(self, context, user_id, key_name,
6365                         key_type=keypair_obj.KEYPAIR_TYPE_SSH):
6366         """Create a new key pair."""
6367         self._validate_new_key_pair(context, user_id, key_name, key_type)
6368 
6369         keypair = objects.KeyPair(context)
6370         keypair.user_id = user_id
6371         keypair.name = key_name
6372         keypair.type = key_type
6373         keypair.fingerprint = None
6374         keypair.public_key = None
6375 
6376         self._notify(context, 'create.start', key_name)
6377         compute_utils.notify_about_keypair_action(
6378             context=context,
6379             keypair=keypair,
6380             action=fields_obj.NotificationAction.CREATE,
6381             phase=fields_obj.NotificationPhase.START)
6382 
6383         private_key, public_key, fingerprint = self._generate_key_pair(
6384             user_id, key_type)
6385 
6386         keypair.fingerprint = fingerprint
6387         keypair.public_key = public_key
6388         keypair.create()
6389 
6390         # NOTE(melwitt): We recheck the quota after creating the object to
6391         # prevent users from allocating more resources than their allowed quota
6392         # in the event of a race. This is configurable because it can be
6393         # expensive if strict quota limits are not required in a deployment.
6394         if CONF.quota.recheck_quota:
6395             try:
6396                 objects.Quotas.check_deltas(context, {'key_pairs': 0}, user_id)
6397             except exception.OverQuota:
6398                 keypair.destroy()
6399                 raise exception.KeypairLimitExceeded()
6400 
6401         compute_utils.notify_about_keypair_action(
6402             context=context,
6403             keypair=keypair,
6404             action=fields_obj.NotificationAction.CREATE,
6405             phase=fields_obj.NotificationPhase.END)
6406 
6407         self._notify(context, 'create.end', key_name)
6408 
6409         return keypair, private_key
6410 
6411     def _generate_fingerprint(self, public_key, key_type):
6412         if key_type == keypair_obj.KEYPAIR_TYPE_SSH:
6413             return crypto.generate_fingerprint(public_key)
6414         elif key_type == keypair_obj.KEYPAIR_TYPE_X509:
6415             return crypto.generate_x509_fingerprint(public_key)
6416 
6417     def _generate_key_pair(self, user_id, key_type):
6418         if key_type == keypair_obj.KEYPAIR_TYPE_SSH:
6419             return crypto.generate_key_pair()
6420         elif key_type == keypair_obj.KEYPAIR_TYPE_X509:
6421             return crypto.generate_winrm_x509_cert(user_id)
6422 
6423     @wrap_exception()
6424     def delete_key_pair(self, context, user_id, key_name):
6425         """Delete a keypair by name."""
6426         self._notify(context, 'delete.start', key_name)
6427         keypair = self.get_key_pair(context, user_id, key_name)
6428         compute_utils.notify_about_keypair_action(
6429             context=context,
6430             keypair=keypair,
6431             action=fields_obj.NotificationAction.DELETE,
6432             phase=fields_obj.NotificationPhase.START)
6433         objects.KeyPair.destroy_by_name(context, user_id, key_name)
6434         compute_utils.notify_about_keypair_action(
6435             context=context,
6436             keypair=keypair,
6437             action=fields_obj.NotificationAction.DELETE,
6438             phase=fields_obj.NotificationPhase.END)
6439         self._notify(context, 'delete.end', key_name)
6440 
6441     def get_key_pairs(self, context, user_id, limit=None, marker=None):
6442         """List key pairs."""
6443         return objects.KeyPairList.get_by_user(
6444             context, user_id, limit=limit, marker=marker)
6445 
6446     def get_key_pair(self, context, user_id, key_name):
6447         """Get a keypair by name."""
6448         return objects.KeyPair.get_by_name(context, user_id, key_name)
