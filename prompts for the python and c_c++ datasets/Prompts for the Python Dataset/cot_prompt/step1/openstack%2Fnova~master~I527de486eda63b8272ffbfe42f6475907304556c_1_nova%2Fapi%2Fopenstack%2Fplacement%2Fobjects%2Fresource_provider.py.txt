Based on the given code from a commit, please generate supplementary code files according to the commit message.
####commit message
perform reshaper operations in single transaction

Adds a nova.api.openstack.placement.objects.resource_provider.reshape()
function that accepts a dict, keyed by provider UUID, of inventory
information for a set of providers and an AllocationList object that
contains all of the rejiggered allocation records for all consumers on
the providers involved in the reshape operation.

The reshape() function is decorated with the placement API's DB
transaction context manager which will catch all exceptions and issue a
ROLLBACK of the single writer transaction that is involved in the myriad
sub-operations that happen inside reshape(). Likewise, a single COMMIT
will be executed in the writer transaction when reshape() completes
without an exception.

Change-Id: I527de486eda63b8272ffbfe42f6475907304556c
blueprint: reshape-provider-tree

####code 
1 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
2 #    not use this file except in compliance with the License. You may obtain
3 #    a copy of the License at
4 #
5 #         http://www.apache.org/licenses/LICENSE-2.0
6 #
7 #    Unless required by applicable law or agreed to in writing, software
8 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
9 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
10 #    License for the specific language governing permissions and limitations
11 #    under the License.
12 
13 import collections
14 import copy
15 import itertools
16 import random
17 
18 # NOTE(cdent): The resource provider objects are designed to never be
19 # used over RPC. Remote manipulation is done with the placement HTTP
20 # API. The 'remotable' decorators should not be used, the objects should
21 # not be registered and there is no need to express VERSIONs nor handle
22 # obj_make_compatible.
23 
24 import os_traits
25 from oslo_concurrency import lockutils
26 from oslo_config import cfg
27 from oslo_db import api as oslo_db_api
28 from oslo_db import exception as db_exc
29 from oslo_log import log as logging
30 from oslo_utils import encodeutils
31 from oslo_versionedobjects import base
32 from oslo_versionedobjects import fields
33 import six
34 import sqlalchemy as sa
35 from sqlalchemy import exc as sqla_exc
36 from sqlalchemy import func
37 from sqlalchemy import sql
38 from sqlalchemy.sql import null
39 
40 from nova.api.openstack.placement import db_api
41 from nova.api.openstack.placement import exception
42 from nova.api.openstack.placement.objects import consumer as consumer_obj
43 from nova.api.openstack.placement.objects import project as project_obj
44 from nova.api.openstack.placement.objects import user as user_obj
45 from nova.db.sqlalchemy import api_models as models
46 from nova.db.sqlalchemy import resource_class_cache as rc_cache
47 from nova.i18n import _
48 from nova import rc_fields
49 
50 _TRAIT_TBL = models.Trait.__table__
51 _ALLOC_TBL = models.Allocation.__table__
52 _INV_TBL = models.Inventory.__table__
53 _RP_TBL = models.ResourceProvider.__table__
54 # Not used in this file but used in tests.
55 _RC_TBL = models.ResourceClass.__table__
56 _AGG_TBL = models.PlacementAggregate.__table__
57 _RP_AGG_TBL = models.ResourceProviderAggregate.__table__
58 _RP_TRAIT_TBL = models.ResourceProviderTrait.__table__
59 _PROJECT_TBL = models.Project.__table__
60 _USER_TBL = models.User.__table__
61 _CONSUMER_TBL = models.Consumer.__table__
62 _RC_CACHE = None
63 _TRAIT_LOCK = 'trait_sync'
64 _TRAITS_SYNCED = False
65 
66 CONF = cfg.CONF
67 LOG = logging.getLogger(__name__)
68 
69 
70 @db_api.placement_context_manager.reader
71 def _ensure_rc_cache(ctx):
72     """Ensures that a singleton resource class cache has been created in the
73     module's scope.
74 
75     :param ctx: `nova.context.RequestContext` that may be used to grab a DB
76                 connection.
77     """
78     global _RC_CACHE
79     if _RC_CACHE is not None:
80         return
81     _RC_CACHE = rc_cache.ResourceClassCache(ctx)
82 
83 
84 @oslo_db_api.wrap_db_retry(max_retries=5, retry_on_deadlock=True)
85 # Bug #1760322: If the caller raises an exception, we don't want the trait
86 # sync rolled back; so use an .independent transaction
87 @db_api.placement_context_manager.writer.independent
88 def _trait_sync(ctx):
89     """Sync the os_traits symbols to the database.
90 
91     Reads all symbols from the os_traits library, checks if any of them do
92     not exist in the database and bulk-inserts those that are not. This is
93     done once per process using this code if either Trait.get_by_name or
94     TraitList.get_all is called.
95 
96     :param ctx: `nova.context.RequestContext` that may be used to grab a DB
97                 connection.
98     """
99     # Create a set of all traits in the os_traits library.
100     std_traits = set(os_traits.get_traits())
101     sel = sa.select([_TRAIT_TBL.c.name])
102     res = ctx.session.execute(sel).fetchall()
103     # Create a set of all traits in the db that are not custom
104     # traits.
105     db_traits = set(
106         r[0] for r in res
107         if not os_traits.is_custom(r[0])
108     )
109     # Determine those traits which are in os_traits but not
110     # currently in the database, and insert them.
111     need_sync = std_traits - db_traits
112     ins = _TRAIT_TBL.insert()
113     batch_args = [
114         {'name': six.text_type(trait)}
115         for trait in need_sync
116     ]
117     if batch_args:
118         try:
119             ctx.session.execute(ins, batch_args)
120             LOG.info("Synced traits from os_traits into API DB: %s",
121                      need_sync)
122         except db_exc.DBDuplicateEntry:
123             pass  # some other process sync'd, just ignore
124 
125 
126 def ensure_trait_sync(ctx):
127     """Ensures that the os_traits library is synchronized to the traits db.
128 
129     If _TRAITS_SYNCED is False then this process has not tried to update the
130     traits db. Do so by calling _trait_sync. Since the placement API server
131     could be multi-threaded, lock around testing _TRAITS_SYNCED to avoid
132     duplicating work.
133 
134     Different placement API server processes that talk to the same database
135     will avoid issues through the power of transactions.
136 
137     :param ctx: `nova.context.RequestContext` that may be used to grab a DB
138                 connection.
139     """
140     global _TRAITS_SYNCED
141     # If another thread is doing this work, wait for it to complete.
142     # When that thread is done _TRAITS_SYNCED will be true in this
143     # thread and we'll simply return.
144     with lockutils.lock(_TRAIT_LOCK):
145         if not _TRAITS_SYNCED:
146             _trait_sync(ctx)
147             _TRAITS_SYNCED = True
148 
149 
150 def _get_current_inventory_resources(ctx, rp):
151     """Returns a set() containing the resource class IDs for all resources
152     currently having an inventory record for the supplied resource provider.
153 
154     :param ctx: `nova.context.RequestContext` that may be used to grab a DB
155                 connection.
156     :param rp: Resource provider to query inventory for.
157     """
158     cur_res_sel = sa.select([_INV_TBL.c.resource_class_id]).where(
159             _INV_TBL.c.resource_provider_id == rp.id)
160     existing_resources = ctx.session.execute(cur_res_sel).fetchall()
161     return set([r[0] for r in existing_resources])
162 
163 
164 def _delete_inventory_from_provider(ctx, rp, to_delete):
165     """Deletes any inventory records from the supplied provider and set() of
166     resource class identifiers.
167 
168     If there are allocations for any of the inventories to be deleted raise
169     InventoryInUse exception.
170 
171     :param ctx: `nova.context.RequestContext` that contains an oslo_db Session
172     :param rp: Resource provider from which to delete inventory.
173     :param to_delete: set() containing resource class IDs for records to
174                       delete.
175     """
176     allocation_query = sa.select(
177         [_ALLOC_TBL.c.resource_class_id.label('resource_class')]).where(
178              sa.and_(_ALLOC_TBL.c.resource_provider_id == rp.id,
179                      _ALLOC_TBL.c.resource_class_id.in_(to_delete))
180          ).group_by(_ALLOC_TBL.c.resource_class_id)
181     allocations = ctx.session.execute(allocation_query).fetchall()
182     if allocations:
183         resource_classes = ', '.join([_RC_CACHE.string_from_id(alloc[0])
184                                       for alloc in allocations])
185         raise exception.InventoryInUse(resource_classes=resource_classes,
186                                        resource_provider=rp.uuid)
187 
188     del_stmt = _INV_TBL.delete().where(sa.and_(
189             _INV_TBL.c.resource_provider_id == rp.id,
190             _INV_TBL.c.resource_class_id.in_(to_delete)))
191     res = ctx.session.execute(del_stmt)
192     return res.rowcount
193 
194 
195 def _add_inventory_to_provider(ctx, rp, inv_list, to_add):
196     """Inserts new inventory records for the supplied resource provider.
197 
198     :param ctx: `nova.context.RequestContext` that contains an oslo_db Session
199     :param rp: Resource provider to add inventory to.
200     :param inv_list: InventoryList object
201     :param to_add: set() containing resource class IDs to search inv_list for
202                    adding to resource provider.
203     """
204     for rc_id in to_add:
205         rc_str = _RC_CACHE.string_from_id(rc_id)
206         inv_record = inv_list.find(rc_str)
207         ins_stmt = _INV_TBL.insert().values(
208                 resource_provider_id=rp.id,
209                 resource_class_id=rc_id,
210                 total=inv_record.total,
211                 reserved=inv_record.reserved,
212                 min_unit=inv_record.min_unit,
213                 max_unit=inv_record.max_unit,
214                 step_size=inv_record.step_size,
215                 allocation_ratio=inv_record.allocation_ratio)
216         ctx.session.execute(ins_stmt)
217 
218 
219 def _update_inventory_for_provider(ctx, rp, inv_list, to_update):
220     """Updates existing inventory records for the supplied resource provider.
221 
222     :param ctx: `nova.context.RequestContext` that contains an oslo_db Session
223     :param rp: Resource provider on which to update inventory.
224     :param inv_list: InventoryList object
225     :param to_update: set() containing resource class IDs to search inv_list
226                       for updating in resource provider.
227     :returns: A list of (uuid, class) tuples that have exceeded their
228               capacity after this inventory update.
229     """
230     exceeded = []
231     for rc_id in to_update:
232         rc_str = _RC_CACHE.string_from_id(rc_id)
233         inv_record = inv_list.find(rc_str)
234         allocation_query = sa.select(
235             [func.sum(_ALLOC_TBL.c.used).label('usage')]).\
236             where(sa.and_(
237                 _ALLOC_TBL.c.resource_provider_id == rp.id,
238                 _ALLOC_TBL.c.resource_class_id == rc_id))
239         allocations = ctx.session.execute(allocation_query).first()
240         if (allocations
241             and allocations['usage'] is not None
242             and allocations['usage'] > inv_record.capacity):
243             exceeded.append((rp.uuid, rc_str))
244         upd_stmt = _INV_TBL.update().where(sa.and_(
245                 _INV_TBL.c.resource_provider_id == rp.id,
246                 _INV_TBL.c.resource_class_id == rc_id)).values(
247                         total=inv_record.total,
248                         reserved=inv_record.reserved,
249                         min_unit=inv_record.min_unit,
250                         max_unit=inv_record.max_unit,
251                         step_size=inv_record.step_size,
252                         allocation_ratio=inv_record.allocation_ratio)
253         res = ctx.session.execute(upd_stmt)
254         if not res.rowcount:
255             raise exception.InventoryWithResourceClassNotFound(
256                     resource_class=rc_str)
257     return exceeded
258 
259 
260 def _increment_provider_generation(ctx, rp):
261     """Increments the supplied provider's generation value, supplying the
262     currently-known generation. Returns whether the increment succeeded.
263 
264     :param ctx: `nova.context.RequestContext` that contains an oslo_db Session
265     :param rp: `ResourceProvider` whose generation should be updated.
266     :returns: The new resource provider generation value if successful.
267     :raises nova.exception.ConcurrentUpdateDetected: if another thread updated
268             the same resource provider's view of its inventory or allocations
269             in between the time when this object was originally read
270             and the call to set the inventory.
271     """
272     rp_gen = rp.generation
273     new_generation = rp_gen + 1
274     upd_stmt = _RP_TBL.update().where(sa.and_(
275             _RP_TBL.c.id == rp.id,
276             _RP_TBL.c.generation == rp_gen)).values(
277                     generation=(new_generation))
278 
279     res = ctx.session.execute(upd_stmt)
280     if res.rowcount != 1:
281         raise exception.ConcurrentUpdateDetected
282     return new_generation
283 
284 
285 @db_api.placement_context_manager.writer
286 def _add_inventory(context, rp, inventory):
287     """Add one Inventory that wasn't already on the provider.
288 
289     :raises `exception.ResourceClassNotFound` if inventory.resource_class
290             cannot be found in either the standard classes or the DB.
291     """
292     _ensure_rc_cache(context)
293     rc_id = _RC_CACHE.id_from_string(inventory.resource_class)
294     inv_list = InventoryList(objects=[inventory])
295     _add_inventory_to_provider(
296         context, rp, inv_list, set([rc_id]))
297     rp.generation = _increment_provider_generation(context, rp)
298 
299 
300 @db_api.placement_context_manager.writer
301 def _update_inventory(context, rp, inventory):
302     """Update an inventory already on the provider.
303 
304     :raises `exception.ResourceClassNotFound` if inventory.resource_class
305             cannot be found in either the standard classes or the DB.
306     """
307     _ensure_rc_cache(context)
308     rc_id = _RC_CACHE.id_from_string(inventory.resource_class)
309     inv_list = InventoryList(objects=[inventory])
310     exceeded = _update_inventory_for_provider(
311         context, rp, inv_list, set([rc_id]))
312     rp.generation = _increment_provider_generation(context, rp)
313     return exceeded
314 
315 
316 @db_api.placement_context_manager.writer
317 def _delete_inventory(context, rp, resource_class):
318     """Delete up to one Inventory of the given resource_class string.
319 
320     :raises `exception.ResourceClassNotFound` if resource_class
321             cannot be found in either the standard classes or the DB.
322     """
323     _ensure_rc_cache(context)
324     rc_id = _RC_CACHE.id_from_string(resource_class)
325     if not _delete_inventory_from_provider(context, rp, [rc_id]):
326         raise exception.NotFound(
327             'No inventory of class %s found for delete'
328             % resource_class)
329     rp.generation = _increment_provider_generation(context, rp)
330 
331 
332 @db_api.placement_context_manager.writer
333 def _set_inventory(context, rp, inv_list):
334     """Given an InventoryList object, replaces the inventory of the
335     resource provider in a safe, atomic fashion using the resource
336     provider's generation as a consistent view marker.
337 
338     :param context: Nova RequestContext.
339     :param rp: `ResourceProvider` object upon which to set inventory.
340     :param inv_list: `InventoryList` object to save to backend storage.
341     :returns: A list of (uuid, class) tuples that have exceeded their
342               capacity after this inventory update.
343     :raises nova.exception.ConcurrentUpdateDetected: if another thread updated
344             the same resource provider's view of its inventory or allocations
345             in between the time when this object was originally read
346             and the call to set the inventory.
347     :raises `exception.ResourceClassNotFound` if any resource class in any
348             inventory in inv_list cannot be found in either the standard
349             classes or the DB.
350     :raises `exception.InventoryInUse` if we attempt to delete inventory
351             from a provider that has allocations for that resource class.
352     """
353     _ensure_rc_cache(context)
354 
355     existing_resources = _get_current_inventory_resources(context, rp)
356     these_resources = set([_RC_CACHE.id_from_string(r.resource_class)
357                            for r in inv_list.objects])
358 
359     # Determine which resources we should be adding, deleting and/or
360     # updating in the resource provider's inventory by comparing sets
361     # of resource class identifiers.
362     to_add = these_resources - existing_resources
363     to_delete = existing_resources - these_resources
364     to_update = these_resources & existing_resources
365     exceeded = []
366 
367     if to_delete:
368         _delete_inventory_from_provider(context, rp, to_delete)
369     if to_add:
370         _add_inventory_to_provider(context, rp, inv_list, to_add)
371     if to_update:
372         exceeded = _update_inventory_for_provider(context, rp, inv_list,
373                                                   to_update)
374 
375     # Here is where we update the resource provider's generation value.  If
376     # this update updates zero rows, that means that another thread has updated
377     # the inventory for this resource provider between the time the caller
378     # originally read the resource provider record and inventory information
379     # and this point. We raise an exception here which will rollback the above
380     # transaction and return an error to the caller to indicate that they can
381     # attempt to retry the inventory save after reverifying any capacity
382     # conditions and re-reading the existing inventory information.
383     rp.generation = _increment_provider_generation(context, rp)
384 
385     return exceeded
386 
387 
388 @db_api.placement_context_manager.reader
389 def _get_provider_by_uuid(context, uuid):
390     """Given a UUID, return a dict of information about the resource provider
391     from the database.
392 
393     :raises: NotFound if no such provider was found
394     :param uuid: The UUID to look up
395     """
396     rpt = sa.alias(_RP_TBL, name="rp")
397     parent = sa.alias(_RP_TBL, name="parent")
398     root = sa.alias(_RP_TBL, name="root")
399     # TODO(jaypipes): Change this to an inner join when we are sure all
400     # root_provider_id values are NOT NULL
401     rp_to_root = sa.outerjoin(rpt, root, rpt.c.root_provider_id == root.c.id)
402     rp_to_parent = sa.outerjoin(rp_to_root, parent,
403         rpt.c.parent_provider_id == parent.c.id)
404     cols = [
405         rpt.c.id,
406         rpt.c.uuid,
407         rpt.c.name,
408         rpt.c.generation,
409         root.c.uuid.label("root_provider_uuid"),
410         parent.c.uuid.label("parent_provider_uuid"),
411         rpt.c.updated_at,
412         rpt.c.created_at,
413     ]
414     sel = sa.select(cols).select_from(rp_to_parent).where(rpt.c.uuid == uuid)
415     res = context.session.execute(sel).fetchone()
416     if not res:
417         raise exception.NotFound(
418             'No resource provider with uuid %s found' % uuid)
419     return dict(res)
420 
421 
422 @db_api.placement_context_manager.reader
423 def _get_aggregates_by_provider_id(context, rp_id):
424     join_statement = sa.join(
425         _AGG_TBL, _RP_AGG_TBL, sa.and_(
426             _AGG_TBL.c.id == _RP_AGG_TBL.c.aggregate_id,
427             _RP_AGG_TBL.c.resource_provider_id == rp_id))
428     sel = sa.select([_AGG_TBL.c.uuid]).select_from(join_statement)
429     return [r[0] for r in context.session.execute(sel).fetchall()]
430 
431 
432 @db_api.placement_context_manager.reader
433 def _anchors_for_sharing_providers(context, rp_ids, get_id=False):
434     """Given a list of internal IDs of sharing providers, returns a set of
435     tuples of (sharing provider UUID, anchor provider UUID), where each of
436     anchor is the unique root provider of a tree associated with the same
437     aggregate as the sharing provider. (These are the providers that can
438     "anchor" a single AllocationRequest.)
439 
440     The sharing provider may or may not itself be part of a tree; in either
441     case, an entry for this root provider is included in the result.
442 
443     If the sharing provider is not part of any aggregate, the empty list is
444     returned.
445 
446     If get_id is True, it returns a set of tuples of (sharing provider ID,
447     anchor provider ID) instead.
448     """
449     # SELECT sps.uuid, COALESCE(rps.uuid, shr_with_sps.uuid)
450     # FROM resource_providers AS sps
451     # INNER JOIN resource_provider_aggregates AS shr_aggs
452     #   ON sps.id = shr_aggs.resource_provider_id
453     # INNER JOIN resource_provider_aggregates AS shr_with_sps_aggs
454     #   ON shr_aggs.aggregate_id = shr_with_sps_aggs.aggregate_id
455     # INNER JOIN resource_providers AS shr_with_sps
456     #   ON shr_with_sps_aggs.resource_provider_id = shr_with_sps.id
457     # LEFT JOIN resource_providers AS rps
458     #   ON shr_with_sps.root_provider_id = rps.id
459     # WHERE sps.id IN $(RP_IDs)
460     # GROUP by shr_with_sps.root_provider_id
461     rps = sa.alias(_RP_TBL, name='rps')
462     sps = sa.alias(_RP_TBL, name='sps')
463     shr_aggs = sa.alias(_RP_AGG_TBL, name='shr_aggs')
464     shr_with_sps_aggs = sa.alias(_RP_AGG_TBL, name='shr_with_sps_aggs')
465     shr_with_sps = sa.alias(_RP_TBL, name='shr_with_sps')
466     join_chain = sa.join(
467         sps, shr_aggs, sps.c.id == shr_aggs.c.resource_provider_id)
468     join_chain = sa.join(
469         join_chain, shr_with_sps_aggs,
470         shr_aggs.c.aggregate_id == shr_with_sps_aggs.c.aggregate_id)
471     join_chain = sa.join(
472         join_chain, shr_with_sps,
473         shr_with_sps_aggs.c.resource_provider_id == shr_with_sps.c.id)
474     # TODO(efried): Change this to an inner join when we are sure all
475     # root_provider_id values are NOT NULL
476     join_chain = sa.outerjoin(
477         join_chain, rps, shr_with_sps.c.root_provider_id == rps.c.id)
478     if get_id:
479         sel = sa.select([sps.c.id, func.coalesce(rps.c.id, shr_with_sps.c.id)])
480     else:
481         sel = sa.select([sps.c.uuid, func.coalesce(rps.c.uuid,
482                                                    shr_with_sps.c.uuid)])
483     sel = sel.select_from(join_chain)
484     sel = sel.where(sps.c.id.in_(rp_ids))
485     return set([(r[0], r[1]) for r in context.session.execute(sel).fetchall()])
486 
487 
488 @db_api.placement_context_manager.writer
489 def _set_aggregates(context, resource_provider, provided_aggregates,
490                     increment_generation=False):
491     rp_id = resource_provider.id
492     # When aggregate uuids are persisted no validation is done
493     # to ensure that they refer to something that has meaning
494     # elsewhere. It is assumed that code which makes use of the
495     # aggregates, later, will validate their fitness.
496     # TODO(cdent): At the moment we do not delete
497     # a PlacementAggregate that no longer has any associations
498     # with at least one resource provider. We may wish to do that
499     # to avoid bloat if it turns out we're creating a lot of noise.
500     # Not doing now to move things along.
501     provided_aggregates = set(provided_aggregates)
502     existing_aggregates = set(_get_aggregates_by_provider_id(context, rp_id))
503     to_add = provided_aggregates - existing_aggregates
504     target_aggregates = list(provided_aggregates)
505 
506     # Create any aggregates that do not yet exist in
507     # PlacementAggregates. This is different from
508     # the set in existing_aggregates; those are aggregates for
509     # which there are associations for the resource provider
510     # at rp_id. The following loop checks for the existence of any
511     # aggregate with the provided uuid. In this way we only
512     # create a new row in the PlacementAggregate table if the
513     # aggregate uuid has never been seen before. Code further
514     # below will update the associations.
515     for agg_uuid in to_add:
516         found_agg = context.session.query(models.PlacementAggregate.uuid).\
517             filter_by(uuid=agg_uuid).first()
518         if not found_agg:
519             new_aggregate = models.PlacementAggregate(uuid=agg_uuid)
520             try:
521                 context.session.add(new_aggregate)
522                 # Flush each aggregate to explicitly call the INSERT
523                 # statement that could result in an integrity error
524                 # if some other thread has added this agg_uuid. This
525                 # also makes sure that the new aggregates have
526                 # ids when the SELECT below happens.
527                 context.session.flush()
528             except db_exc.DBDuplicateEntry:
529                 # Something else has already added this agg_uuid
530                 pass
531 
532     # Remove all aggregate associations so we can refresh them
533     # below. This means that all associations are added, but the
534     # aggregates themselves stay around.
535     context.session.query(models.ResourceProviderAggregate).filter_by(
536         resource_provider_id=rp_id).delete()
537 
538     # Set resource_provider_id, aggregate_id pairs to
539     # ResourceProviderAggregate table.
540     if target_aggregates:
541         select_agg_id = sa.select([rp_id, models.PlacementAggregate.id]).\
542             where(models.PlacementAggregate.uuid.in_(target_aggregates))
543         insert_aggregates = models.ResourceProviderAggregate.__table__.\
544             insert().from_select(['resource_provider_id', 'aggregate_id'],
545                                  select_agg_id)
546         context.session.execute(insert_aggregates)
547 
548     if increment_generation:
549         resource_provider.generation = _increment_provider_generation(
550             context, resource_provider)
551 
552 
553 @db_api.placement_context_manager.reader
554 def _get_traits_by_provider_id(context, rp_id):
555     t = sa.alias(_TRAIT_TBL, name='t')
556     rpt = sa.alias(_RP_TRAIT_TBL, name='rpt')
557 
558     join_cond = sa.and_(t.c.id == rpt.c.trait_id,
559                         rpt.c.resource_provider_id == rp_id)
560     join = sa.join(t, rpt, join_cond)
561     sel = sa.select([t.c.id, t.c.name,
562                      t.c.created_at, t.c.updated_at]).select_from(join)
563     return [dict(r) for r in context.session.execute(sel).fetchall()]
564 
565 
566 def _add_traits_to_provider(ctx, rp_id, to_add):
567     """Adds trait associations to the provider with the supplied ID.
568 
569     :param ctx: `nova.context.RequestContext` that has an oslo_db Session
570     :param rp_id: Internal ID of the resource provider on which to add
571                   trait associations
572     :param to_add: set() containing internal trait IDs for traits to add
573     """
574     for trait_id in to_add:
575         try:
576             ins_stmt = _RP_TRAIT_TBL.insert().values(
577                 resource_provider_id=rp_id,
578                 trait_id=trait_id)
579             ctx.session.execute(ins_stmt)
580         except db_exc.DBDuplicateEntry:
581             # Another thread already set this trait for this provider. Ignore
582             # this for now (but ConcurrentUpdateDetected will end up being
583             # raised almost assuredly when we go to increment the resource
584             # provider's generation later, but that's also fine)
585             pass
586 
587 
588 def _delete_traits_from_provider(ctx, rp_id, to_delete):
589     """Deletes trait associations from the provider with the supplied ID and
590     set() of internal trait IDs.
591 
592     :param ctx: `nova.context.RequestContext` that has an oslo_db Session
593     :param rp_id: Internal ID of the resource provider from which to delete
594                   trait associations
595     :param to_delete: set() containing internal trait IDs for traits to
596                       delete
597     """
598     del_stmt = _RP_TRAIT_TBL.delete().where(
599         sa.and_(
600             _RP_TRAIT_TBL.c.resource_provider_id == rp_id,
601             _RP_TRAIT_TBL.c.trait_id.in_(to_delete)))
602     ctx.session.execute(del_stmt)
603 
604 
605 @db_api.placement_context_manager.writer
606 def _set_traits(context, rp, traits):
607     """Given a ResourceProvider object and a TraitList object, replaces the set
608     of traits associated with the resource provider.
609 
610     :raises: ConcurrentUpdateDetected if the resource provider's traits or
611              inventory was changed in between the time when we first started to
612              set traits and the end of this routine.
613 
614     :param rp: The ResourceProvider object to set traits against
615     :param traits: A TraitList object or list of Trait objects
616     """
617     # Get the internal IDs of our existing traits
618     existing_traits = _get_traits_by_provider_id(context, rp.id)
619     existing_traits = set(rec['id'] for rec in existing_traits)
620     want_traits = set(trait.id for trait in traits)
621 
622     to_add = want_traits - existing_traits
623     to_delete = existing_traits - want_traits
624 
625     if not to_add and not to_delete:
626         return
627 
628     if to_delete:
629         _delete_traits_from_provider(context, rp.id, to_delete)
630     if to_add:
631         _add_traits_to_provider(context, rp.id, to_add)
632     rp.generation = _increment_provider_generation(context, rp)
633 
634 
635 @db_api.placement_context_manager.reader
636 def _has_child_providers(context, rp_id):
637     """Returns True if the supplied resource provider has any child providers,
638     False otherwise
639     """
640     child_sel = sa.select([_RP_TBL.c.id])
641     child_sel = child_sel.where(_RP_TBL.c.parent_provider_id == rp_id)
642     child_res = context.session.execute(child_sel.limit(1)).fetchone()
643     if child_res:
644         return True
645     return False
646 
647 
648 @db_api.placement_context_manager.writer
649 def _set_root_provider_id(context, rp_id, root_id):
650     """Simply sets the root_provider_id value for a provider identified by
651     rp_id. Used in online data migration.
652 
653     :param rp_id: Internal ID of the provider to update
654     :param root_id: Value to set root provider to
655     """
656     upd = _RP_TBL.update().where(_RP_TBL.c.id == rp_id)
657     upd = upd.values(root_provider_id=root_id)
658     context.session.execute(upd)
659 
660 
661 ProviderIds = collections.namedtuple(
662     'ProviderIds', 'id uuid parent_id parent_uuid root_id root_uuid')
663 
664 
665 def _provider_ids_from_uuid(context, uuid):
666     """Given the UUID of a resource provider, returns a namedtuple
667     (ProviderIds) with the internal ID, the UUID, the parent provider's
668     internal ID, parent provider's UUID, the root provider's internal ID and
669     the root provider UUID.
670 
671     :returns: ProviderIds object containing the internal IDs and UUIDs of the
672               provider identified by the supplied UUID
673     :param uuid: The UUID of the provider to look up
674     """
675     # SELECT
676     #   rp.id, rp.uuid,
677     #   parent.id AS parent_id, parent.uuid AS parent_uuid,
678     #   root.id AS root_id, root.uuid AS root_uuid
679     # FROM resource_providers AS rp
680     # LEFT JOIN resource_providers AS parent
681     #   ON rp.parent_provider_id = parent.id
682     # LEFT JOIN resource_providers AS root
683     #   ON rp.root_provider_id = root.id
684     me = sa.alias(_RP_TBL, name="me")
685     parent = sa.alias(_RP_TBL, name="parent")
686     root = sa.alias(_RP_TBL, name="root")
687     cols = [
688         me.c.id,
689         me.c.uuid,
690         parent.c.id.label('parent_id'),
691         parent.c.uuid.label('parent_uuid'),
692         root.c.id.label('root_id'),
693         root.c.uuid.label('root_uuid'),
694     ]
695     # TODO(jaypipes): Change this to an inner join when we are sure all
696     # root_provider_id values are NOT NULL
697     me_to_root = sa.outerjoin(me, root, me.c.root_provider_id == root.c.id)
698     me_to_parent = sa.outerjoin(me_to_root, parent,
699         me.c.parent_provider_id == parent.c.id)
700     sel = sa.select(cols).select_from(me_to_parent)
701     sel = sel.where(me.c.uuid == uuid)
702     res = context.session.execute(sel).fetchone()
703     if not res:
704         return None
705     return ProviderIds(**dict(res))
706 
707 
708 def _provider_ids_matching_aggregates(context, member_of, rp_ids=None):
709     """Given a list of lists of aggregate UUIDs, return the internal IDs of all
710     resource providers associated with the aggregates.
711 
712     :param member_of: A list containing lists of aggregate UUIDs. Each item in
713         the outer list is to be AND'd together. If that item contains multiple
714         values, they are OR'd together.
715 
716         For example, if member_of is::
717 
718             [
719                 ['agg1'],
720                 ['agg2', 'agg3'],
721             ]
722 
723         we will return all the resource providers that are
724         associated with agg1 as well as either (agg2 or agg3)
725     :param rp_ids: When present, returned resource providers are limited
726         to only those in this value
727 
728     :returns: A list of internal resource provider IDs having all required
729         aggregate associations
730     """
731     # Given a request for the following:
732     #
733     # member_of = [
734     #   [agg1],
735     #   [agg2],
736     #   [agg3, agg4]
737     # ]
738     #
739     # we need to produce the following SQL expression:
740     #
741     # SELECT
742     #   rp.id
743     # FROM resource_providers AS rp
744     # JOIN resource_provider_aggregates AS rpa1
745     #   ON rp.id = rpa1.resource_provider_id
746     #   AND rpa1.aggregate_id IN ($AGG1_ID)
747     # JOIN resource_provider_aggregates AS rpa2
748     #   ON rp.id = rpa2.resource_provider_id
749     #   AND rpa2.aggregate_id IN ($AGG2_ID)
750     # JOIN resource_provider_aggregates AS rpa3
751     #   ON rp.id = rpa3.resource_provider_id
752     #   AND rpa3.aggregate_id IN ($AGG3_ID, $AGG4_ID)
753     # # Only if we have rp_ids...
754     # WHERE rp.id IN ($RP_IDs)
755 
756     # First things first, get a map of all the aggregate UUID to internal
757     # aggregate IDs
758     agg_uuids = set()
759     for members in member_of:
760         for member in members:
761             agg_uuids.add(member)
762     agg_tbl = sa.alias(_AGG_TBL, name='aggs')
763     agg_sel = sa.select([agg_tbl.c.uuid, agg_tbl.c.id])
764     agg_sel = agg_sel.where(agg_tbl.c.uuid.in_(agg_uuids))
765     agg_uuid_map = {
766         r[0]: r[1] for r in context.session.execute(agg_sel).fetchall()
767     }
768 
769     rp_tbl = sa.alias(_RP_TBL, name='rp')
770     join_chain = rp_tbl
771 
772     for x, members in enumerate(member_of):
773         rpa_tbl = sa.alias(_RP_AGG_TBL, name='rpa%d' % x)
774 
775         agg_ids = [agg_uuid_map[member] for member in members
776                    if member in agg_uuid_map]
777         if not agg_ids:
778             # This member_of list contains only non-existent aggregate UUIDs
779             # and therefore we will always return 0 results, so short-circuit
780             return []
781 
782         join_cond = sa.and_(
783             rp_tbl.c.id == rpa_tbl.c.resource_provider_id,
784             rpa_tbl.c.aggregate_id.in_(agg_ids))
785         join_chain = sa.join(join_chain, rpa_tbl, join_cond)
786     sel = sa.select([rp_tbl.c.id]).select_from(join_chain)
787     if rp_ids:
788         sel = sel.where(rp_tbl.c.id.in_(rp_ids))
789     return [r[0] for r in context.session.execute(sel).fetchall()]
790 
791 
792 @db_api.placement_context_manager.writer
793 def _delete_rp_record(context, _id):
794     return context.session.query(models.ResourceProvider).\
795         filter(models.ResourceProvider.id == _id).\
796         delete(synchronize_session=False)
797 
798 
799 @base.VersionedObjectRegistry.register_if(False)
800 class ResourceProvider(base.VersionedObject, base.TimestampedObject):
801     SETTABLE_FIELDS = ('name', 'parent_provider_uuid')
802 
803     fields = {
804         'id': fields.IntegerField(read_only=True),
805         'uuid': fields.UUIDField(nullable=False),
806         'name': fields.StringField(nullable=False),
807         'generation': fields.IntegerField(nullable=False),
808         # UUID of the root provider in a hierarchy of providers. Will be equal
809         # to the uuid field if this provider is the root provider of a
810         # hierarchy. This field is never manually set by the user. Instead, it
811         # is automatically set to either the root provider UUID of the parent
812         # or the UUID of the provider itself if there is no parent. This field
813         # is an optimization field that allows us to very quickly query for all
814         # providers within a particular tree without doing any recursive
815         # querying.
816         'root_provider_uuid': fields.UUIDField(nullable=False),
817         # UUID of the direct parent provider, or None if this provider is a
818         # "root" provider.
819         'parent_provider_uuid': fields.UUIDField(nullable=True, default=None),
820     }
821 
822     def create(self):
823         if 'id' in self:
824             raise exception.ObjectActionError(action='create',
825                                               reason='already created')
826         if 'uuid' not in self:
827             raise exception.ObjectActionError(action='create',
828                                               reason='uuid is required')
829         if 'name' not in self:
830             raise exception.ObjectActionError(action='create',
831                                               reason='name is required')
832         if 'root_provider_uuid' in self:
833             raise exception.ObjectActionError(
834                 action='create',
835                 reason=_('root provider UUID cannot be manually set.'))
836 
837         self.obj_set_defaults()
838         updates = self.obj_get_changes()
839         self._create_in_db(self._context, updates)
840         self.obj_reset_changes()
841 
842     def destroy(self):
843         self._delete(self._context, self.id)
844 
845     def save(self):
846         updates = self.obj_get_changes()
847         if updates and any(k not in self.SETTABLE_FIELDS
848                            for k in updates.keys()):
849             raise exception.ObjectActionError(
850                 action='save',
851                 reason='Immutable fields changed')
852         self._update_in_db(self._context, self.id, updates)
853         self.obj_reset_changes()
854 
855     @classmethod
856     def get_by_uuid(cls, context, uuid):
857         """Returns a new ResourceProvider object with the supplied UUID.
858 
859         :raises NotFound if no such provider could be found
860         :param uuid: UUID of the provider to search for
861         """
862         rp_rec = _get_provider_by_uuid(context, uuid)
863         return cls._from_db_object(context, cls(), rp_rec)
864 
865     def add_inventory(self, inventory):
866         """Add one new Inventory to the resource provider.
867 
868         Fails if Inventory of the provided resource class is
869         already present.
870         """
871         _add_inventory(self._context, self, inventory)
872         self.obj_reset_changes()
873 
874     def delete_inventory(self, resource_class):
875         """Delete Inventory of provided resource_class."""
876         _delete_inventory(self._context, self, resource_class)
877         self.obj_reset_changes()
878 
879     def set_inventory(self, inv_list):
880         """Set all resource provider Inventory to be the provided list."""
881         exceeded = _set_inventory(self._context, self, inv_list)
882         for uuid, rclass in exceeded:
883             LOG.warning('Resource provider %(uuid)s is now over-'
884                         'capacity for %(resource)s',
885                         {'uuid': uuid, 'resource': rclass})
886         self.obj_reset_changes()
887 
888     def update_inventory(self, inventory):
889         """Update one existing Inventory of the same resource class.
890 
891         Fails if no Inventory of the same class is present.
892         """
893         exceeded = _update_inventory(self._context, self, inventory)
894         for uuid, rclass in exceeded:
895             LOG.warning('Resource provider %(uuid)s is now over-'
896                         'capacity for %(resource)s',
897                         {'uuid': uuid, 'resource': rclass})
898         self.obj_reset_changes()
899 
900     def get_aggregates(self):
901         """Get the aggregate uuids associated with this resource provider."""
902         return _get_aggregates_by_provider_id(self._context, self.id)
903 
904     def set_aggregates(self, aggregate_uuids, increment_generation=False):
905         """Set the aggregate uuids associated with this resource provider.
906 
907         If an aggregate does not exist, one will be created using the
908         provided uuid.
909 
910         The resource provider generation is incremented if and only if the
911         increment_generation parameter is True.
912         """
913         _set_aggregates(self._context, self, aggregate_uuids,
914                         increment_generation=increment_generation)
915 
916     def set_traits(self, traits):
917         """Replaces the set of traits associated with the resource provider
918         with the given list of Trait objects.
919 
920         :param traits: A list of Trait objects representing the traits to
921                        associate with the provider.
922         """
923         _set_traits(self._context, self, traits)
924         self.obj_reset_changes()
925 
926     @db_api.placement_context_manager.writer
927     def _create_in_db(self, context, updates):
928         parent_id = None
929         root_id = None
930         # User supplied a parent, let's make sure it exists
931         parent_uuid = updates.pop('parent_provider_uuid')
932         if parent_uuid is not None:
933             # Setting parent to ourselves doesn't make any sense
934             if parent_uuid == self.uuid:
935                 raise exception.ObjectActionError(
936                         action='create',
937                         reason=_('parent provider UUID cannot be same as '
938                                  'UUID. Please set parent provider UUID to '
939                                  'None if there is no parent.'))
940 
941             parent_ids = _provider_ids_from_uuid(context, parent_uuid)
942             if parent_ids is None:
943                 raise exception.ObjectActionError(
944                         action='create',
945                         reason=_('parent provider UUID does not exist.'))
946 
947             parent_id = parent_ids.id
948             root_id = parent_ids.root_id
949             updates['root_provider_id'] = root_id
950             updates['parent_provider_id'] = parent_id
951             self.root_provider_uuid = parent_ids.root_uuid
952 
953         db_rp = models.ResourceProvider()
954         db_rp.update(updates)
955         context.session.add(db_rp)
956         context.session.flush()
957 
958         self.id = db_rp.id
959         self.generation = db_rp.generation
960 
961         if root_id is None:
962             # User did not specify a parent when creating this provider, so the
963             # root_provider_id needs to be set to this provider's newly-created
964             # internal ID
965             db_rp.root_provider_id = db_rp.id
966             context.session.add(db_rp)
967             context.session.flush()
968             self.root_provider_uuid = self.uuid
969 
970     @staticmethod
971     @db_api.placement_context_manager.writer
972     def _delete(context, _id):
973         # Do a quick check to see if the provider is a parent. If it is, don't
974         # allow deleting the provider. Note that the foreign key constraint on
975         # resource_providers.parent_provider_id will prevent deletion of the
976         # parent within the transaction below. This is just a quick
977         # short-circuit outside of the transaction boundary.
978         if _has_child_providers(context, _id):
979             raise exception.CannotDeleteParentResourceProvider()
980 
981         # Don't delete the resource provider if it has allocations.
982         rp_allocations = context.session.query(models.Allocation).\
983              filter(models.Allocation.resource_provider_id == _id).\
984              count()
985         if rp_allocations:
986             raise exception.ResourceProviderInUse()
987         # Delete any inventory associated with the resource provider
988         context.session.query(models.Inventory).\
989             filter(models.Inventory.resource_provider_id == _id).\
990             delete(synchronize_session=False)
991         # Delete any aggregate associations for the resource provider
992         # The name substitution on the next line is needed to satisfy pep8
993         RPA_model = models.ResourceProviderAggregate
994         context.session.query(RPA_model).\
995                 filter(RPA_model.resource_provider_id == _id).delete()
996         # delete any trait associations for the resource provider
997         RPT_model = models.ResourceProviderTrait
998         context.session.query(RPT_model).\
999                 filter(RPT_model.resource_provider_id == _id).delete()
1000         # set root_provider_id to null to make deletion possible
1001         context.session.query(models.ResourceProvider).\
1002             filter(models.ResourceProvider.id == _id,
1003                    models.ResourceProvider.root_provider_id == _id).\
1004             update({'root_provider_id': None})
1005         # Now delete the RP record
1006         try:
1007             result = _delete_rp_record(context, _id)
1008         except sqla_exc.IntegrityError:
1009             # NOTE(jaypipes): Another thread snuck in and parented this
1010             # resource provider in between the above check for
1011             # _has_child_providers() and our attempt to delete the record
1012             raise exception.CannotDeleteParentResourceProvider()
1013         if not result:
1014             raise exception.NotFound()
1015 
1016     @db_api.placement_context_manager.writer
1017     def _update_in_db(self, context, id, updates):
1018         # A list of resource providers in the same tree with the
1019         # resource provider to update
1020         same_tree = []
1021         if 'parent_provider_uuid' in updates:
1022             # TODO(jaypipes): For now, "re-parenting" and "un-parenting" are
1023             # not possible. If the provider already had a parent, we don't
1024             # allow changing that parent due to various issues, including:
1025             #
1026             # * if the new parent is a descendant of this resource provider, we
1027             #   introduce the possibility of a loop in the graph, which would
1028             #   be very bad
1029             # * potentially orphaning heretofore-descendants
1030             #
1031             # So, for now, let's just prevent re-parenting...
1032             my_ids = _provider_ids_from_uuid(context, self.uuid)
1033             parent_uuid = updates.pop('parent_provider_uuid')
1034             if parent_uuid is not None:
1035                 parent_ids = _provider_ids_from_uuid(context, parent_uuid)
1036                 # User supplied a parent, let's make sure it exists
1037                 if parent_ids is None:
1038                     raise exception.ObjectActionError(
1039                             action='create',
1040                             reason=_('parent provider UUID does not exist.'))
1041                 if (my_ids.parent_id is not None and
1042                         my_ids.parent_id != parent_ids.id):
1043                     raise exception.ObjectActionError(
1044                             action='update',
1045                             reason=_('re-parenting a provider is not '
1046                                      'currently allowed.'))
1047                 if my_ids.parent_uuid is None:
1048                     # So the user specifies a parent for an RP that doesn't
1049                     # have one. We have to check that by this new parent we
1050                     # don't create a loop in the tree. Basically the new parent
1051                     # cannot be the RP itself or one of its descendants.
1052                     # However as the RP's current parent is None the above
1053                     # condition is the same as "the new parent cannot be any RP
1054                     # from the current RP tree".
1055                     same_tree = ResourceProviderList.get_all_by_filters(
1056                         context,
1057                         filters={'in_tree': self.uuid})
1058                     rp_uuids_in_the_same_tree = [rp.uuid for rp in same_tree]
1059                     if parent_uuid in rp_uuids_in_the_same_tree:
1060                         raise exception.ObjectActionError(
1061                             action='update',
1062                             reason=_('creating loop in the provider tree is '
1063                                      'not allowed.'))
1064 
1065                 updates['root_provider_id'] = parent_ids.root_id
1066                 updates['parent_provider_id'] = parent_ids.id
1067                 self.root_provider_uuid = parent_ids.root_uuid
1068             else:
1069                 if my_ids.parent_id is not None:
1070                     raise exception.ObjectActionError(
1071                             action='update',
1072                             reason=_('un-parenting a provider is not '
1073                                      'currently allowed.'))
1074 
1075         db_rp = context.session.query(models.ResourceProvider).filter_by(
1076             id=id).first()
1077         db_rp.update(updates)
1078         context.session.add(db_rp)
1079 
1080         # We should also update the root providers of resource providers
1081         # originally in the same tree. If re-parenting is supported,
1082         # this logic should be changed to update only descendents of the
1083         # re-parented resource providers, not all the providers in the tree.
1084         for rp in same_tree:
1085             # If the parent is not updated, this clause is skipped since the
1086             # `same_tree` has no element.
1087             rp.root_provider_uuid = parent_ids.root_uuid
1088             db_rp = context.session.query(
1089                 models.ResourceProvider).filter_by(id=rp.id).first()
1090             data = {'root_provider_id': parent_ids.root_id}
1091             db_rp.update(data)
1092             context.session.add(db_rp)
1093 
1094         try:
1095             context.session.flush()
1096         except sqla_exc.IntegrityError:
1097             # NOTE(jaypipes): Another thread snuck in and deleted the parent
1098             # for this resource provider in between the above check for a valid
1099             # parent provider and here...
1100             raise exception.ObjectActionError(
1101                     action='update',
1102                     reason=_('parent provider UUID does not exist.'))
1103 
1104     @staticmethod
1105     @db_api.placement_context_manager.writer  # For online data migration
1106     def _from_db_object(context, resource_provider, db_resource_provider):
1107         # Online data migration to populate root_provider_id
1108         # TODO(jaypipes): Remove when all root_provider_id values are NOT NULL
1109         if db_resource_provider['root_provider_uuid'] is None:
1110             rp_id = db_resource_provider['id']
1111             uuid = db_resource_provider['uuid']
1112             db_resource_provider['root_provider_uuid'] = uuid
1113             _set_root_provider_id(context, rp_id, rp_id)
1114         for field in resource_provider.fields:
1115             setattr(resource_provider, field, db_resource_provider[field])
1116         resource_provider._context = context
1117         resource_provider.obj_reset_changes()
1118         return resource_provider
1119 
1120 
1121 @db_api.placement_context_manager.reader
1122 def _get_providers_with_shared_capacity(ctx, rc_id, amount, member_of=None):
1123     """Returns a list of resource provider IDs (internal IDs, not UUIDs)
1124     that have capacity for a requested amount of a resource and indicate that
1125     they share resource via an aggregate association.
1126 
1127     Shared resource providers are marked with a standard trait called
1128     MISC_SHARES_VIA_AGGREGATE. This indicates that the provider allows its
1129     inventory to be consumed by other resource providers associated via an
1130     aggregate link.
1131 
1132     For example, assume we have two compute nodes, CN_1 and CN_2, each with
1133     inventory of VCPU and MEMORY_MB but not DISK_GB (in other words, these are
1134     compute nodes with no local disk). There is a resource provider called
1135     "NFS_SHARE" that has an inventory of DISK_GB and has the
1136     MISC_SHARES_VIA_AGGREGATE trait. Both the "CN_1" and "CN_2" compute node
1137     resource providers and the "NFS_SHARE" resource provider are associated
1138     with an aggregate called "AGG_1".
1139 
1140     The scheduler needs to determine the resource providers that can fulfill a
1141     request for 2 VCPU, 1024 MEMORY_MB and 100 DISK_GB.
1142 
1143     Clearly, no single provider can satisfy the request for all three
1144     resources, since neither compute node has DISK_GB inventory and the
1145     NFS_SHARE provider has no VCPU or MEMORY_MB inventories.
1146 
1147     However, if we consider the NFS_SHARE resource provider as providing
1148     inventory of DISK_GB for both CN_1 and CN_2, we can include CN_1 and CN_2
1149     as potential fits for the requested set of resources.
1150 
1151     To facilitate that matching query, this function returns all providers that
1152     indicate they share their inventory with providers in some aggregate and
1153     have enough capacity for the requested amount of a resource.
1154 
1155     To follow the example above, if we were to call
1156     _get_providers_with_shared_capacity(ctx, "DISK_GB", 100), we would want to
1157     get back the ID for the NFS_SHARE resource provider.
1158 
1159     :param rc_id: Internal ID of the requested resource class.
1160     :param amount: Amount of the requested resource.
1161     :param member_of: When present, contains a list of lists of aggregate
1162                       uuids that are used to filter the returned list of
1163                       resource providers that *directly* belong to the
1164                       aggregates referenced.
1165     """
1166     # The SQL we need to generate here looks like this:
1167     #
1168     # SELECT rp.id
1169     # FROM resource_providers AS rp
1170     #   INNER JOIN resource_provider_traits AS rpt
1171     #     ON rp.id = rpt.resource_provider_id
1172     #   INNER JOIN traits AS t
1173     #     AND rpt.trait_id = t.id
1174     #     AND t.name = "MISC_SHARES_VIA_AGGREGATE"
1175     #   INNER JOIN inventories AS inv
1176     #     ON rp.id = inv.resource_provider_id
1177     #     AND inv.resource_class_id = $rc_id
1178     #   LEFT JOIN (
1179     #     SELECT resource_provider_id, SUM(used) as used
1180     #     FROM allocations
1181     #     WHERE resource_class_id = $rc_id
1182     #     GROUP BY resource_provider_id
1183     #   ) AS usage
1184     #     ON rp.id = usage.resource_provider_id
1185     # WHERE COALESCE(usage.used, 0) + $amount <= (
1186     #   inv.total - inv.reserved) * inv.allocation_ratio
1187     # ) AND
1188     #   inv.min_unit <= $amount AND
1189     #   inv.max_unit >= $amount AND
1190     #   $amount % inv.step_size = 0
1191     # GROUP BY rp.id
1192 
1193     rp_tbl = sa.alias(_RP_TBL, name='rp')
1194     inv_tbl = sa.alias(_INV_TBL, name='inv')
1195     t_tbl = sa.alias(_TRAIT_TBL, name='t')
1196     rpt_tbl = sa.alias(_RP_TRAIT_TBL, name='rpt')
1197 
1198     rp_to_rpt_join = sa.join(
1199         rp_tbl, rpt_tbl,
1200         rp_tbl.c.id == rpt_tbl.c.resource_provider_id,
1201     )
1202 
1203     rpt_to_t_join = sa.join(
1204         rp_to_rpt_join, t_tbl,
1205         sa.and_(
1206             rpt_tbl.c.trait_id == t_tbl.c.id,
1207             # The traits table wants unicode trait names, but os_traits
1208             # presents native str, so we need to cast.
1209             t_tbl.c.name == six.text_type(os_traits.MISC_SHARES_VIA_AGGREGATE),
1210         ),
1211     )
1212 
1213     rp_to_inv_join = sa.join(
1214         rpt_to_t_join, inv_tbl,
1215         sa.and_(
1216             rpt_tbl.c.resource_provider_id == inv_tbl.c.resource_provider_id,
1217             inv_tbl.c.resource_class_id == rc_id,
1218         ),
1219     )
1220 
1221     usage = sa.select([_ALLOC_TBL.c.resource_provider_id,
1222                        sql.func.sum(_ALLOC_TBL.c.used).label('used')])
1223     usage = usage.where(_ALLOC_TBL.c.resource_class_id == rc_id)
1224     usage = usage.group_by(_ALLOC_TBL.c.resource_provider_id)
1225     usage = sa.alias(usage, name='usage')
1226 
1227     inv_to_usage_join = sa.outerjoin(
1228         rp_to_inv_join, usage,
1229         inv_tbl.c.resource_provider_id == usage.c.resource_provider_id,
1230     )
1231 
1232     where_conds = sa.and_(
1233         func.coalesce(usage.c.used, 0) + amount <= (
1234             inv_tbl.c.total - inv_tbl.c.reserved) * inv_tbl.c.allocation_ratio,
1235         inv_tbl.c.min_unit <= amount,
1236         inv_tbl.c.max_unit >= amount,
1237         amount % inv_tbl.c.step_size == 0)
1238 
1239     # If 'member_of' has values, do a separate lookup to identify the
1240     # resource providers that meet the member_of constraints.
1241     if member_of:
1242         rps_in_aggs = _provider_ids_matching_aggregates(ctx, member_of)
1243         if not rps_in_aggs:
1244             # Short-circuit. The user either asked for a non-existing
1245             # aggregate or there were no resource providers that matched
1246             # the requirements...
1247             return []
1248         where_conds.append(rp_tbl.c.id.in_(rps_in_aggs))
1249 
1250     sel = sa.select([rp_tbl.c.id]).select_from(inv_to_usage_join)
1251     sel = sel.where(where_conds)
1252     sel = sel.group_by(rp_tbl.c.id)
1253 
1254     return [r[0] for r in ctx.session.execute(sel)]
1255 
1256 
1257 @base.VersionedObjectRegistry.register_if(False)
1258 class ResourceProviderList(base.ObjectListBase, base.VersionedObject):
1259 
1260     fields = {
1261         'objects': fields.ListOfObjectsField('ResourceProvider'),
1262     }
1263 
1264     @staticmethod
1265     @db_api.placement_context_manager.reader
1266     def _get_all_by_filters_from_db(context, filters):
1267         # Eg. filters can be:
1268         #  filters = {
1269         #      'name': <name>,
1270         #      'uuid': <uuid>,
1271         #      'member_of': [[<aggregate_uuid>, <aggregate_uuid>],
1272         #                    [<aggregate_uuid>]]
1273         #      'resources': {
1274         #          'VCPU': 1,
1275         #          'MEMORY_MB': 1024
1276         #      },
1277         #      'in_tree': <uuid>,
1278         #      'required': [<trait_name>, ...]
1279         #  }
1280         if not filters:
1281             filters = {}
1282         else:
1283             # Since we modify the filters, copy them so that we don't modify
1284             # them in the calling program.
1285             filters = copy.deepcopy(filters)
1286         name = filters.pop('name', None)
1287         uuid = filters.pop('uuid', None)
1288         member_of = filters.pop('member_of', [])
1289         required = set(filters.pop('required', []))
1290         forbidden = set([trait for trait in required
1291                          if trait.startswith('!')])
1292         required = required - forbidden
1293         forbidden = set([trait.lstrip('!') for trait in forbidden])
1294 
1295         resources = filters.pop('resources', {})
1296         # NOTE(sbauza): We want to key the dict by the resource class IDs
1297         # and we want to make sure those class names aren't incorrect.
1298         resources = {_RC_CACHE.id_from_string(r_name): amount
1299                      for r_name, amount in resources.items()}
1300         rp = sa.alias(_RP_TBL, name="rp")
1301         root_rp = sa.alias(_RP_TBL, name="root_rp")
1302         parent_rp = sa.alias(_RP_TBL, name="parent_rp")
1303 
1304         cols = [
1305             rp.c.id,
1306             rp.c.uuid,
1307             rp.c.name,
1308             rp.c.generation,
1309             rp.c.updated_at,
1310             rp.c.created_at,
1311             root_rp.c.uuid.label("root_provider_uuid"),
1312             parent_rp.c.uuid.label("parent_provider_uuid"),
1313         ]
1314 
1315         # TODO(jaypipes): Convert this to an inner join once all
1316         # root_provider_id values are NOT NULL
1317         rp_to_root = sa.outerjoin(rp, root_rp,
1318             rp.c.root_provider_id == root_rp.c.id)
1319         rp_to_parent = sa.outerjoin(rp_to_root, parent_rp,
1320             rp.c.parent_provider_id == parent_rp.c.id)
1321 
1322         query = sa.select(cols).select_from(rp_to_parent)
1323 
1324         if name:
1325             query = query.where(rp.c.name == name)
1326         if uuid:
1327             query = query.where(rp.c.uuid == uuid)
1328         if 'in_tree' in filters:
1329             # The 'in_tree' parameter is the UUID of a resource provider that
1330             # the caller wants to limit the returned providers to only those
1331             # within its "provider tree". So, we look up the resource provider
1332             # having the UUID specified by the 'in_tree' parameter and grab the
1333             # root_provider_id value of that record. We can then ask for only
1334             # those resource providers having a root_provider_id of that value.
1335             tree_uuid = filters.pop('in_tree')
1336             tree_ids = _provider_ids_from_uuid(context, tree_uuid)
1337             if tree_ids is None:
1338                 # List operations should simply return an empty list when a
1339                 # non-existing resource provider UUID is given.
1340                 return []
1341             root_id = tree_ids.root_id
1342             # TODO(jaypipes): Remove this OR condition when root_provider_id
1343             # is not nullable in the database and all resource provider records
1344             # have populated the root provider ID.
1345             where_cond = sa.or_(rp.c.id == root_id,
1346                 rp.c.root_provider_id == root_id)
1347             query = query.where(where_cond)
1348 
1349         # If 'member_of' has values, do a separate lookup to identify the
1350         # resource providers that meet the member_of constraints.
1351         if member_of:
1352             rps_in_aggs = _provider_ids_matching_aggregates(context, member_of)
1353             if not rps_in_aggs:
1354                 # Short-circuit. The user either asked for a non-existing
1355                 # aggregate or there were no resource providers that matched
1356                 # the requirements...
1357                 return []
1358             query = query.where(rp.c.id.in_(rps_in_aggs))
1359 
1360         # If 'required' has values, add a filter to limit results to providers
1361         # possessing *all* of the listed traits.
1362         if required:
1363             trait_map = _trait_ids_from_names(context, required)
1364             if len(trait_map) != len(required):
1365                 missing = required - set(trait_map)
1366                 raise exception.TraitNotFound(names=', '.join(missing))
1367             rp_ids = _get_provider_ids_having_all_traits(context, trait_map)
1368             if not rp_ids:
1369                 # If no providers have the required traits, we're done
1370                 return []
1371             query = query.where(rp.c.id.in_(rp_ids))
1372 
1373         # If 'forbidden' has values, filter out those providers that have
1374         # that trait as one their traits.
1375         if forbidden:
1376             trait_map = _trait_ids_from_names(context, forbidden)
1377             if len(trait_map) != len(forbidden):
1378                 missing = forbidden - set(trait_map)
1379                 raise exception.TraitNotFound(names=', '.join(missing))
1380             rp_ids = _get_provider_ids_having_any_trait(context, trait_map)
1381             if rp_ids:
1382                 query = query.where(~rp.c.id.in_(rp_ids))
1383 
1384         if not resources:
1385             # Returns quickly the list in case we don't need to check the
1386             # resource usage
1387             res = context.session.execute(query).fetchall()
1388             return [dict(r) for r in res]
1389 
1390         # NOTE(sbauza): In case we want to look at the resource criteria, then
1391         # the SQL generated from this case looks something like:
1392         # SELECT
1393         #   rp.*
1394         # FROM resource_providers AS rp
1395         # JOIN inventories AS inv
1396         # ON rp.id = inv.resource_provider_id
1397         # LEFT JOIN (
1398         #    SELECT resource_provider_id, resource_class_id, SUM(used) AS used
1399         #    FROM allocations
1400         #    WHERE resource_class_id IN ($RESOURCE_CLASSES)
1401         #    GROUP BY resource_provider_id, resource_class_id
1402         # ) AS usage
1403         #     ON inv.resource_provider_id = usage.resource_provider_id
1404         #     AND inv.resource_class_id = usage.resource_class_id
1405         # AND (inv.resource_class_id = $X AND (used + $AMOUNT_X <= (
1406         #        total - reserved) * inv.allocation_ratio) AND
1407         #        inv.min_unit <= $AMOUNT_X AND inv.max_unit >= $AMOUNT_X AND
1408         #        $AMOUNT_X % inv.step_size == 0)
1409         #      OR (inv.resource_class_id = $Y AND (used + $AMOUNT_Y <= (
1410         #        total - reserved) * inv.allocation_ratio) AND
1411         #        inv.min_unit <= $AMOUNT_Y AND inv.max_unit >= $AMOUNT_Y AND
1412         #        $AMOUNT_Y % inv.step_size == 0)
1413         #      OR (inv.resource_class_id = $Z AND (used + $AMOUNT_Z <= (
1414         #        total - reserved) * inv.allocation_ratio) AND
1415         #        inv.min_unit <= $AMOUNT_Z AND inv.max_unit >= $AMOUNT_Z AND
1416         #        $AMOUNT_Z % inv.step_size == 0))
1417         # GROUP BY rp.id
1418         # HAVING
1419         #  COUNT(DISTINCT(inv.resource_class_id)) == len($RESOURCE_CLASSES)
1420         #
1421         # with a possible additional WHERE clause for the name and uuid that
1422         # comes from the above filters
1423 
1424         # First JOIN between inventories and RPs is here
1425         inv_join = sa.join(rp_to_parent, _INV_TBL,
1426             rp.c.id == _INV_TBL.c.resource_provider_id)
1427 
1428         # Now, below is the LEFT JOIN for getting the allocations usage
1429         usage = sa.select([_ALLOC_TBL.c.resource_provider_id,
1430                            _ALLOC_TBL.c.resource_class_id,
1431                            sql.func.sum(_ALLOC_TBL.c.used).label('used')])
1432         usage = usage.where(_ALLOC_TBL.c.resource_class_id.in_(resources))
1433         usage = usage.group_by(_ALLOC_TBL.c.resource_provider_id,
1434                                _ALLOC_TBL.c.resource_class_id)
1435         usage = sa.alias(usage, name='usage')
1436         usage_join = sa.outerjoin(inv_join, usage,
1437             sa.and_(
1438                 usage.c.resource_provider_id == (
1439                     _INV_TBL.c.resource_provider_id),
1440                 usage.c.resource_class_id == _INV_TBL.c.resource_class_id))
1441 
1442         # And finally, we verify for each resource class if the requested
1443         # amount isn't more than the left space (considering the allocation
1444         # ratio, the reserved space and the min and max amount possible sizes)
1445         where_clauses = [
1446             sa.and_(
1447                 _INV_TBL.c.resource_class_id == r_idx,
1448                 (func.coalesce(usage.c.used, 0) + amount <= (
1449                     _INV_TBL.c.total - _INV_TBL.c.reserved
1450                 ) * _INV_TBL.c.allocation_ratio),
1451                 _INV_TBL.c.min_unit <= amount,
1452                 _INV_TBL.c.max_unit >= amount,
1453                 amount % _INV_TBL.c.step_size == 0
1454             )
1455             for (r_idx, amount) in resources.items()]
1456         query = query.select_from(usage_join)
1457         query = query.where(sa.or_(*where_clauses))
1458         query = query.group_by(rp.c.id, root_rp.c.uuid, parent_rp.c.uuid)
1459         # NOTE(sbauza): Only RPs having all the asked resources can be provided
1460         query = query.having(sql.func.count(
1461             sa.distinct(_INV_TBL.c.resource_class_id)) == len(resources))
1462 
1463         res = context.session.execute(query).fetchall()
1464         return [dict(r) for r in res]
1465 
1466     @classmethod
1467     def get_all_by_filters(cls, context, filters=None):
1468         """Returns a list of `ResourceProvider` objects that have sufficient
1469         resources in their inventories to satisfy the amounts specified in the
1470         `filters` parameter.
1471 
1472         If no resource providers can be found, the function will return an
1473         empty list.
1474 
1475         :param context: `nova.context.RequestContext` that may be used to grab
1476                         a DB connection.
1477         :param filters: Can be `name`, `uuid`, `member_of`, `in_tree` or
1478                         `resources` where `member_of` is a list of list of
1479                         aggregate UUIDs, `in_tree` is a UUID of a resource
1480                         provider that we can use to find the root provider ID
1481                         of the tree of providers to filter results by and
1482                         `resources` is a dict of amounts keyed by resource
1483                         classes.
1484         :type filters: dict
1485         """
1486         _ensure_rc_cache(context)
1487         resource_providers = cls._get_all_by_filters_from_db(context, filters)
1488         return base.obj_make_list(context, cls(context),
1489                                   ResourceProvider, resource_providers)
1490 
1491 
1492 @base.VersionedObjectRegistry.register_if(False)
1493 class Inventory(base.VersionedObject, base.TimestampedObject):
1494 
1495     fields = {
1496         'id': fields.IntegerField(read_only=True),
1497         'resource_provider': fields.ObjectField('ResourceProvider'),
1498         'resource_class': rc_fields.ResourceClassField(read_only=True),
1499         'total': fields.NonNegativeIntegerField(),
1500         'reserved': fields.NonNegativeIntegerField(default=0),
1501         'min_unit': fields.NonNegativeIntegerField(default=1),
1502         'max_unit': fields.NonNegativeIntegerField(default=1),
1503         'step_size': fields.NonNegativeIntegerField(default=1),
1504         'allocation_ratio': fields.NonNegativeFloatField(default=1.0),
1505     }
1506 
1507     @property
1508     def capacity(self):
1509         """Inventory capacity, adjusted by allocation_ratio."""
1510         return int((self.total - self.reserved) * self.allocation_ratio)
1511 
1512 
1513 @db_api.placement_context_manager.reader
1514 def _get_inventory_by_provider_id(ctx, rp_id):
1515     inv = sa.alias(_INV_TBL, name="i")
1516     cols = [
1517         inv.c.resource_class_id,
1518         inv.c.total,
1519         inv.c.reserved,
1520         inv.c.min_unit,
1521         inv.c.max_unit,
1522         inv.c.step_size,
1523         inv.c.allocation_ratio,
1524         inv.c.updated_at,
1525         inv.c.created_at,
1526     ]
1527     sel = sa.select(cols)
1528     sel = sel.where(inv.c.resource_provider_id == rp_id)
1529 
1530     return [dict(r) for r in ctx.session.execute(sel)]
1531 
1532 
1533 @base.VersionedObjectRegistry.register_if(False)
1534 class InventoryList(base.ObjectListBase, base.VersionedObject):
1535 
1536     fields = {
1537         'objects': fields.ListOfObjectsField('Inventory'),
1538     }
1539 
1540     def find(self, res_class):
1541         """Return the inventory record from the list of Inventory records that
1542         matches the supplied resource class, or None.
1543 
1544         :param res_class: An integer or string representing a resource
1545                           class. If the value is a string, the method first
1546                           looks up the resource class identifier from the
1547                           string.
1548         """
1549         if not isinstance(res_class, six.string_types):
1550             raise ValueError
1551 
1552         for inv_rec in self.objects:
1553             if inv_rec.resource_class == res_class:
1554                 return inv_rec
1555 
1556     @classmethod
1557     def get_all_by_resource_provider(cls, context, rp):
1558         _ensure_rc_cache(context)
1559         db_inv = _get_inventory_by_provider_id(context, rp.id)
1560         # Build up a list of Inventory objects, setting the Inventory object
1561         # fields to the same-named database record field we got from
1562         # _get_inventory_by_provider_id(). We already have the ResourceProvider
1563         # object so we just pass that object to the Inventory object
1564         # constructor as-is
1565         objs = [
1566             Inventory(
1567                 context, resource_provider=rp,
1568                 resource_class=_RC_CACHE.string_from_id(
1569                     rec['resource_class_id']),
1570                 **rec)
1571             for rec in db_inv
1572         ]
1573         inv_list = cls(context, objects=objs)
1574         return inv_list
1575 
1576 
1577 @base.VersionedObjectRegistry.register_if(False)
1578 class Allocation(base.VersionedObject, base.TimestampedObject):
1579 
1580     fields = {
1581         'id': fields.IntegerField(),
1582         'resource_provider': fields.ObjectField('ResourceProvider'),
1583         'consumer': fields.ObjectField('Consumer', nullable=False),
1584         'resource_class': rc_fields.ResourceClassField(),
1585         'used': fields.IntegerField(),
1586     }
1587 
1588 
1589 @db_api.placement_context_manager.writer
1590 def _delete_allocations_for_consumer(ctx, consumer_id):
1591     """Deletes any existing allocations that correspond to the allocations to
1592     be written. This is wrapped in a transaction, so if the write subsequently
1593     fails, the deletion will also be rolled back.
1594     """
1595     del_sql = _ALLOC_TBL.delete().where(
1596         _ALLOC_TBL.c.consumer_id == consumer_id)
1597     ctx.session.execute(del_sql)
1598 
1599 
1600 @db_api.placement_context_manager.writer
1601 def _delete_allocations_by_ids(ctx, alloc_ids):
1602     """Deletes allocations having an internal id value in the set of supplied
1603     IDs
1604     """
1605     del_sql = _ALLOC_TBL.delete().where(_ALLOC_TBL.c.id.in_(alloc_ids))
1606     ctx.session.execute(del_sql)
1607 
1608 
1609 def _check_capacity_exceeded(ctx, allocs):
1610     """Checks to see if the supplied allocation records would result in any of
1611     the inventories involved having their capacity exceeded.
1612 
1613     Raises an InvalidAllocationCapacityExceeded exception if any inventory
1614     would be exhausted by the allocation. Raises an
1615     InvalidAllocationConstraintsViolated exception if any of the `step_size`,
1616     `min_unit` or `max_unit` constraints in an inventory will be violated
1617     by any one of the allocations.
1618 
1619     If no inventories would be exceeded or violated by the allocations, the
1620     function returns a list of `ResourceProvider` objects that contain the
1621     generation at the time of the check.
1622 
1623     :param ctx: `nova.context.RequestContext` that has an oslo_db Session
1624     :param allocs: List of `Allocation` objects to check
1625     """
1626     # The SQL generated below looks like this:
1627     # SELECT
1628     #   rp.id,
1629     #   rp.uuid,
1630     #   rp.generation,
1631     #   inv.resource_class_id,
1632     #   inv.total,
1633     #   inv.reserved,
1634     #   inv.allocation_ratio,
1635     #   allocs.used
1636     # FROM resource_providers AS rp
1637     # JOIN inventories AS i1
1638     # ON rp.id = i1.resource_provider_id
1639     # LEFT JOIN (
1640     #    SELECT resource_provider_id, resource_class_id, SUM(used) AS used
1641     #    FROM allocations
1642     #    WHERE resource_class_id IN ($RESOURCE_CLASSES)
1643     #    AND resource_provider_id IN ($RESOURCE_PROVIDERS)
1644     #    GROUP BY resource_provider_id, resource_class_id
1645     # ) AS allocs
1646     # ON inv.resource_provider_id = allocs.resource_provider_id
1647     # AND inv.resource_class_id = allocs.resource_class_id
1648     # WHERE rp.id IN ($RESOURCE_PROVIDERS)
1649     # AND inv.resource_class_id IN ($RESOURCE_CLASSES)
1650     #
1651     # We then take the results of the above and determine if any of the
1652     # inventory will have its capacity exceeded.
1653     rc_ids = set([_RC_CACHE.id_from_string(a.resource_class)
1654                        for a in allocs])
1655     provider_uuids = set([a.resource_provider.uuid for a in allocs])
1656     provider_ids = set([a.resource_provider.id for a in allocs])
1657     usage = sa.select([_ALLOC_TBL.c.resource_provider_id,
1658                        _ALLOC_TBL.c.resource_class_id,
1659                        sql.func.sum(_ALLOC_TBL.c.used).label('used')])
1660     usage = usage.where(
1661             sa.and_(_ALLOC_TBL.c.resource_class_id.in_(rc_ids),
1662                     _ALLOC_TBL.c.resource_provider_id.in_(provider_ids)))
1663     usage = usage.group_by(_ALLOC_TBL.c.resource_provider_id,
1664                            _ALLOC_TBL.c.resource_class_id)
1665     usage = sa.alias(usage, name='usage')
1666 
1667     inv_join = sql.join(_RP_TBL, _INV_TBL,
1668             sql.and_(_RP_TBL.c.id == _INV_TBL.c.resource_provider_id,
1669                      _INV_TBL.c.resource_class_id.in_(rc_ids)))
1670     primary_join = sql.outerjoin(inv_join, usage,
1671         sql.and_(
1672             _INV_TBL.c.resource_provider_id == usage.c.resource_provider_id,
1673             _INV_TBL.c.resource_class_id == usage.c.resource_class_id)
1674     )
1675     cols_in_output = [
1676         _RP_TBL.c.id.label('resource_provider_id'),
1677         _RP_TBL.c.uuid,
1678         _RP_TBL.c.generation,
1679         _INV_TBL.c.resource_class_id,
1680         _INV_TBL.c.total,
1681         _INV_TBL.c.reserved,
1682         _INV_TBL.c.allocation_ratio,
1683         _INV_TBL.c.min_unit,
1684         _INV_TBL.c.max_unit,
1685         _INV_TBL.c.step_size,
1686         usage.c.used,
1687     ]
1688 
1689     sel = sa.select(cols_in_output).select_from(primary_join)
1690     sel = sel.where(
1691             sa.and_(_RP_TBL.c.id.in_(provider_ids),
1692                     _INV_TBL.c.resource_class_id.in_(rc_ids)))
1693     records = ctx.session.execute(sel)
1694     # Create a map keyed by (rp_uuid, res_class) for the records in the DB
1695     usage_map = {}
1696     provs_with_inv = set()
1697     for record in records:
1698         map_key = (record['uuid'], record['resource_class_id'])
1699         if map_key in usage_map:
1700             raise KeyError("%s already in usage_map, bad query" % str(map_key))
1701         usage_map[map_key] = record
1702         provs_with_inv.add(record["uuid"])
1703     # Ensure that all providers have existing inventory
1704     missing_provs = provider_uuids - provs_with_inv
1705     if missing_provs:
1706         class_str = ', '.join([_RC_CACHE.string_from_id(rc_id)
1707                                for rc_id in rc_ids])
1708         provider_str = ', '.join(missing_provs)
1709         raise exception.InvalidInventory(resource_class=class_str,
1710                 resource_provider=provider_str)
1711 
1712     res_providers = {}
1713     rp_resource_class_sum = collections.defaultdict(
1714         lambda: collections.defaultdict(int))
1715     for alloc in allocs:
1716         rc_id = _RC_CACHE.id_from_string(alloc.resource_class)
1717         rp_uuid = alloc.resource_provider.uuid
1718         if rp_uuid not in res_providers:
1719             res_providers[rp_uuid] = alloc.resource_provider
1720         amount_needed = alloc.used
1721         rp_resource_class_sum[rp_uuid][rc_id] += amount_needed
1722         # No use checking usage if we're not asking for anything
1723         if amount_needed == 0:
1724             continue
1725         key = (rp_uuid, rc_id)
1726         try:
1727             usage = usage_map[key]
1728         except KeyError:
1729             # The resource class at rc_id is not in the usage map.
1730             raise exception.InvalidInventory(
1731                     resource_class=alloc.resource_class,
1732                     resource_provider=rp_uuid)
1733         allocation_ratio = usage['allocation_ratio']
1734         min_unit = usage['min_unit']
1735         max_unit = usage['max_unit']
1736         step_size = usage['step_size']
1737 
1738         # check min_unit, max_unit, step_size
1739         if (amount_needed < min_unit or amount_needed > max_unit or
1740                 amount_needed % step_size != 0):
1741             LOG.warning(
1742                 "Allocation for %(rc)s on resource provider %(rp)s "
1743                 "violates min_unit, max_unit, or step_size. "
1744                 "Requested: %(requested)s, min_unit: %(min_unit)s, "
1745                 "max_unit: %(max_unit)s, step_size: %(step_size)s",
1746                 {'rc': alloc.resource_class,
1747                  'rp': rp_uuid,
1748                  'requested': amount_needed,
1749                  'min_unit': min_unit,
1750                  'max_unit': max_unit,
1751                  'step_size': step_size})
1752             raise exception.InvalidAllocationConstraintsViolated(
1753                 resource_class=alloc.resource_class,
1754                 resource_provider=rp_uuid)
1755 
1756         # usage["used"] can be returned as None
1757         used = usage['used'] or 0
1758         capacity = (usage['total'] - usage['reserved']) * allocation_ratio
1759         if (capacity < (used + amount_needed) or
1760             capacity < (used + rp_resource_class_sum[rp_uuid][rc_id])):
1761             LOG.warning(
1762                 "Over capacity for %(rc)s on resource provider %(rp)s. "
1763                 "Needed: %(needed)s, Used: %(used)s, Capacity: %(cap)s",
1764                 {'rc': alloc.resource_class,
1765                  'rp': rp_uuid,
1766                  'needed': amount_needed,
1767                  'used': used,
1768                  'cap': capacity})
1769             raise exception.InvalidAllocationCapacityExceeded(
1770                 resource_class=alloc.resource_class,
1771                 resource_provider=rp_uuid)
1772     return res_providers
1773 
1774 
1775 @db_api.placement_context_manager.reader
1776 def _get_allocations_by_provider_id(ctx, rp_id):
1777     allocs = sa.alias(_ALLOC_TBL, name="a")
1778     consumers = sa.alias(_CONSUMER_TBL, name="c")
1779     projects = sa.alias(_PROJECT_TBL, name="p")
1780     users = sa.alias(_PROJECT_TBL, name="u")
1781     cols = [
1782         allocs.c.id,
1783         allocs.c.resource_class_id,
1784         allocs.c.used,
1785         allocs.c.updated_at,
1786         allocs.c.created_at,
1787         consumers.c.id.label("consumer_id"),
1788         consumers.c.generation.label("consumer_generation"),
1789         sql.func.coalesce(
1790             consumers.c.uuid, allocs.c.consumer_id).label("consumer_uuid"),
1791         projects.c.id.label("project_id"),
1792         projects.c.external_id.label("project_external_id"),
1793         users.c.id.label("user_id"),
1794         users.c.external_id.label("user_external_id"),
1795     ]
1796     # TODO(jaypipes): change this join to be on ID not UUID
1797     consumers_join = sa.join(
1798         allocs, consumers, allocs.c.consumer_id == consumers.c.uuid)
1799     projects_join = sa.join(
1800         consumers_join, projects, consumers.c.project_id == projects.c.id)
1801     users_join = sa.join(
1802         projects_join, users, consumers.c.user_id == users.c.id)
1803     sel = sa.select(cols).select_from(users_join)
1804     sel = sel.where(allocs.c.resource_provider_id == rp_id)
1805 
1806     return [dict(r) for r in ctx.session.execute(sel)]
1807 
1808 
1809 @db_api.placement_context_manager.reader
1810 def _get_allocations_by_consumer_uuid(ctx, consumer_uuid):
1811     allocs = sa.alias(_ALLOC_TBL, name="a")
1812     rp = sa.alias(_RP_TBL, name="rp")
1813     consumer = sa.alias(_CONSUMER_TBL, name="c")
1814     project = sa.alias(_PROJECT_TBL, name="p")
1815     user = sa.alias(_USER_TBL, name="u")
1816     cols = [
1817         allocs.c.id,
1818         allocs.c.resource_provider_id,
1819         rp.c.name.label("resource_provider_name"),
1820         rp.c.uuid.label("resource_provider_uuid"),
1821         rp.c.generation.label("resource_provider_generation"),
1822         allocs.c.resource_class_id,
1823         allocs.c.used,
1824         consumer.c.id.label("consumer_id"),
1825         consumer.c.generation.label("consumer_generation"),
1826         sql.func.coalesce(
1827             consumer.c.uuid, allocs.c.consumer_id).label("consumer_uuid"),
1828         project.c.id.label("project_id"),
1829         project.c.external_id.label("project_external_id"),
1830         user.c.id.label("user_id"),
1831         user.c.external_id.label("user_external_id"),
1832     ]
1833     # Build up the joins of the five tables we need to interact with.
1834     rp_join = sa.join(allocs, rp, allocs.c.resource_provider_id == rp.c.id)
1835     consumer_join = sa.join(rp_join, consumer,
1836                             allocs.c.consumer_id == consumer.c.uuid)
1837     project_join = sa.join(consumer_join, project,
1838                            consumer.c.project_id == project.c.id)
1839     user_join = sa.join(project_join, user,
1840                         consumer.c.user_id == user.c.id)
1841 
1842     sel = sa.select(cols).select_from(user_join)
1843     sel = sel.where(allocs.c.consumer_id == consumer_uuid)
1844 
1845     return [dict(r) for r in ctx.session.execute(sel)]
1846 
1847 
1848 @db_api.placement_context_manager.writer.independent
1849 def _create_incomplete_consumers_for_provider(ctx, rp_id):
1850     # TODO(jaypipes): Remove in Stein after a blocker migration is added.
1851     """Creates consumer record if consumer relationship between allocations ->
1852     consumers table is missing for any allocation on the supplied provider
1853     internal ID, using the "incomplete consumer" project and user CONF options.
1854     """
1855     alloc_to_consumer = sa.outerjoin(
1856         _ALLOC_TBL, consumer_obj.CONSUMER_TBL,
1857         _ALLOC_TBL.c.consumer_id == consumer_obj.CONSUMER_TBL.c.uuid)
1858     sel = sa.select([_ALLOC_TBL.c.consumer_id])
1859     sel = sel.select_from(alloc_to_consumer)
1860     sel = sel.where(
1861         sa.and_(
1862             _ALLOC_TBL.c.resource_provider_id == rp_id,
1863             consumer_obj.CONSUMER_TBL.c.id.is_(None)))
1864     missing = ctx.session.execute(sel).fetchall()
1865     if missing:
1866         # Do a single INSERT for all missing consumer relationships for the
1867         # provider
1868         incomplete_proj_id = project_obj.ensure_incomplete_project(ctx)
1869         incomplete_user_id = user_obj.ensure_incomplete_user(ctx)
1870 
1871         cols = [
1872             _ALLOC_TBL.c.consumer_id,
1873             incomplete_proj_id,
1874             incomplete_user_id,
1875         ]
1876         sel = sa.select(cols)
1877         sel = sel.select_from(alloc_to_consumer)
1878         sel = sel.where(
1879             sa.and_(
1880                 _ALLOC_TBL.c.resource_provider_id == rp_id,
1881                 consumer_obj.CONSUMER_TBL.c.id.is_(None)))
1882         target_cols = ['uuid', 'project_id', 'user_id']
1883         ins_stmt = consumer_obj.CONSUMER_TBL.insert().from_select(
1884             target_cols, sel)
1885         res = ctx.session.execute(ins_stmt)
1886         if res.rowcount > 0:
1887             LOG.info("Online data migration to fix incomplete consumers "
1888                      "for resource provider %s has been run. Migrated %d "
1889                      "incomplete consumer records on the fly.", rp_id,
1890                      res.rowcount)
1891 
1892 
1893 @db_api.placement_context_manager.writer.independent
1894 def _create_incomplete_consumer(ctx, consumer_id):
1895     # TODO(jaypipes): Remove in Stein after a blocker migration is added.
1896     """Creates consumer record if consumer relationship between allocations ->
1897     consumers table is missing for the supplied consumer UUID, using the
1898     "incomplete consumer" project and user CONF options.
1899     """
1900     alloc_to_consumer = sa.outerjoin(
1901         _ALLOC_TBL, consumer_obj.CONSUMER_TBL,
1902         _ALLOC_TBL.c.consumer_id == consumer_obj.CONSUMER_TBL.c.uuid)
1903     sel = sa.select([_ALLOC_TBL.c.consumer_id])
1904     sel = sel.select_from(alloc_to_consumer)
1905     sel = sel.where(
1906         sa.and_(
1907             _ALLOC_TBL.c.consumer_id == consumer_id,
1908             consumer_obj.CONSUMER_TBL.c.id.is_(None)))
1909     missing = ctx.session.execute(sel).fetchall()
1910     if missing:
1911         incomplete_proj_id = project_obj.ensure_incomplete_project(ctx)
1912         incomplete_user_id = user_obj.ensure_incomplete_user(ctx)
1913 
1914         ins_stmt = consumer_obj.CONSUMER_TBL.insert().values(
1915             uuid=consumer_id, project_id=incomplete_proj_id,
1916             user_id=incomplete_user_id)
1917         res = ctx.session.execute(ins_stmt)
1918         if res.rowcount > 0:
1919             LOG.info("Online data migration to fix incomplete consumers "
1920                      "for consumer %s has been run. Migrated %d incomplete "
1921                      "consumer records on the fly.", consumer_id, res.rowcount)
1922 
1923 
1924 @base.VersionedObjectRegistry.register_if(False)
1925 class AllocationList(base.ObjectListBase, base.VersionedObject):
1926 
1927     fields = {
1928         'objects': fields.ListOfObjectsField('Allocation'),
1929     }
1930 
1931     @oslo_db_api.wrap_db_retry(max_retries=5, retry_on_deadlock=True)
1932     @db_api.placement_context_manager.writer
1933     def _set_allocations(self, context, allocs):
1934         """Write a set of allocations.
1935 
1936         We must check that there is capacity for each allocation.
1937         If there is not we roll back the entire set.
1938 
1939         :raises `exception.ResourceClassNotFound` if any resource class in any
1940                 allocation in allocs cannot be found in either the standard
1941                 classes or the DB.
1942         :raises `exception.InvalidAllocationCapacityExceeded` if any inventory
1943                 would be exhausted by the allocation.
1944         :raises `InvalidAllocationConstraintsViolated` if any of the
1945                 `step_size`, `min_unit` or `max_unit` constraints in an
1946                 inventory will be violated by any one of the allocations.
1947         :raises `ConcurrentUpdateDetected` if a generation for a resource
1948                 provider or consumer failed its increment check.
1949         """
1950         _ensure_rc_cache(context)
1951 
1952         # First delete any existing allocations for any consumers. This
1953         # provides a clean slate for the consumers mentioned in the list of
1954         # allocations being manipulated.
1955         consumer_ids = set(alloc.consumer.uuid for alloc in allocs)
1956         for consumer_id in consumer_ids:
1957             _delete_allocations_for_consumer(context, consumer_id)
1958 
1959         # Before writing any allocation records, we check that the submitted
1960         # allocations do not cause any inventory capacity to be exceeded for
1961         # any resource provider and resource class involved in the allocation
1962         # transaction. _check_capacity_exceeded() raises an exception if any
1963         # inventory capacity is exceeded. If capacity is not exceeeded, the
1964         # function returns a list of ResourceProvider objects containing the
1965         # generation of the resource provider at the time of the check. These
1966         # objects are used at the end of the allocation transaction as a guard
1967         # against concurrent updates.
1968         #
1969         # Don't check capacity when alloc.used is zero. Zero is not a valid
1970         # amount when making an allocation (the minimum consumption of a
1971         # resource is one) but is used in this method to indicate a need for
1972         # removal. Providing 0 is controlled at the HTTP API layer where PUT
1973         # /allocations does not allow empty allocations. When POST /allocations
1974         # is implemented it will for the special case of atomically setting and
1975         # removing different allocations in the same request.
1976         # _check_capacity_exceeded will raise a ResourceClassNotFound # if any
1977         # allocation is using a resource class that does not exist.
1978         visited_consumers = {}
1979         visited_rps = _check_capacity_exceeded(context, allocs)
1980         for alloc in allocs:
1981             if alloc.consumer.id not in visited_consumers:
1982                 visited_consumers[alloc.consumer.id] = alloc.consumer
1983 
1984             # If alloc.used is set to zero that is a signal that we don't want
1985             # to (re-)create any allocations for this resource class.
1986             # _delete_current_allocs has already wiped out allocations so just
1987             # continue
1988             if alloc.used == 0:
1989                 continue
1990             consumer_id = alloc.consumer.uuid
1991             rp = alloc.resource_provider
1992             rc_id = _RC_CACHE.id_from_string(alloc.resource_class)
1993             ins_stmt = _ALLOC_TBL.insert().values(
1994                     resource_provider_id=rp.id,
1995                     resource_class_id=rc_id,
1996                     consumer_id=consumer_id,
1997                     used=alloc.used)
1998             res = context.session.execute(ins_stmt)
1999             alloc.id = res.lastrowid
2000             alloc.obj_reset_changes()
2001 
2002         # Generation checking happens here. If the inventory for this resource
2003         # provider changed out from under us, this will raise a
2004         # ConcurrentUpdateDetected which can be caught by the caller to choose
2005         # to try again. It will also rollback the transaction so that these
2006         # changes always happen atomically.
2007         for rp in visited_rps.values():
2008             rp.generation = _increment_provider_generation(context, rp)
2009         for consumer in visited_consumers.values():
2010             consumer.increment_generation()
2011         # If any consumers involved in this transaction ended up having no
2012         # allocations, delete the consumer records. Exclude consumers that had
2013         # *some resource* in the allocation list with a total > 0 since clearly
2014         # those consumers have allocations...
2015         cons_with_allocs = set(a.consumer.uuid for a in allocs if a.used > 0)
2016         all_cons = set(c.uuid for c in visited_consumers.values())
2017         consumers_to_check = all_cons - cons_with_allocs
2018         consumer_obj.delete_consumers_if_no_allocations(
2019             context, consumers_to_check)
2020 
2021     @classmethod
2022     def get_all_by_resource_provider(cls, context, rp):
2023         _ensure_rc_cache(context)
2024         _create_incomplete_consumers_for_provider(context, rp.id)
2025         db_allocs = _get_allocations_by_provider_id(context, rp.id)
2026         # Build up a list of Allocation objects, setting the Allocation object
2027         # fields to the same-named database record field we got from
2028         # _get_allocations_by_provider_id(). We already have the
2029         # ResourceProvider object so we just pass that object to the Allocation
2030         # object constructor as-is
2031         objs = []
2032         for rec in db_allocs:
2033             consumer = consumer_obj.Consumer(
2034                 context, id=rec['consumer_id'],
2035                 uuid=rec['consumer_uuid'],
2036                 generation=rec['consumer_generation'],
2037                 project=project_obj.Project(
2038                     context, id=rec['project_id'],
2039                     external_id=rec['project_external_id']),
2040                 user=user_obj.User(
2041                     context, id=rec['user_id'],
2042                     external_id=rec['user_external_id']))
2043             objs.append(
2044                 Allocation(
2045                     context, id=rec['id'], resource_provider=rp,
2046                     resource_class=_RC_CACHE.string_from_id(
2047                         rec['resource_class_id']),
2048                     consumer=consumer,
2049                     used=rec['used']))
2050         alloc_list = cls(context, objects=objs)
2051         return alloc_list
2052 
2053     @classmethod
2054     def get_all_by_consumer_id(cls, context, consumer_id):
2055         _ensure_rc_cache(context)
2056         _create_incomplete_consumer(context, consumer_id)
2057         db_allocs = _get_allocations_by_consumer_uuid(context, consumer_id)
2058 
2059         if db_allocs:
2060             # Build up the Consumer object (it's the same for all allocations
2061             # since we looked up by consumer ID)
2062             db_first = db_allocs[0]
2063             consumer = consumer_obj.Consumer(
2064                 context, id=db_first['consumer_id'],
2065                 uuid=db_first['consumer_uuid'],
2066                 generation=db_first['consumer_generation'],
2067                 project=project_obj.Project(
2068                     context, id=db_first['project_id'],
2069                     external_id=db_first['project_external_id']),
2070                 user=user_obj.User(
2071                     context, id=db_first['user_id'],
2072                     external_id=db_first['user_external_id']))
2073 
2074         # Build up a list of Allocation objects, setting the Allocation object
2075         # fields to the same-named database record field we got from
2076         # _get_allocations_by_consumer_id().
2077         #
2078         # NOTE(jaypipes):  Unlike with get_all_by_resource_provider(), we do
2079         # NOT already have the ResourceProvider object so we construct a new
2080         # ResourceProvider object below by looking at the resource provider
2081         # fields returned by _get_allocations_by_consumer_id().
2082         objs = [
2083             Allocation(
2084                 context, id=rec['id'],
2085                 resource_provider=ResourceProvider(
2086                     context,
2087                     id=rec['resource_provider_id'],
2088                     uuid=rec['resource_provider_uuid'],
2089                     name=rec['resource_provider_name'],
2090                     generation=rec['resource_provider_generation']),
2091                 resource_class=_RC_CACHE.string_from_id(
2092                     rec['resource_class_id']),
2093                 consumer=consumer,
2094                 used=rec['used'])
2095             for rec in db_allocs
2096         ]
2097         alloc_list = cls(context, objects=objs)
2098         return alloc_list
2099 
2100     def replace_all(self):
2101         """Replace the supplied allocations.
2102 
2103         :note: This method always deletes all allocations for all consumers
2104                referenced in the list of Allocation objects and then replaces
2105                the consumer's allocations with the Allocation objects. In doing
2106                so, it will end up setting the Allocation.id attribute of each
2107                Allocation object.
2108         """
2109         # TODO(jaypipes): Retry the allocation writes on
2110         # ConcurrentUpdateDetected
2111         self._set_allocations(self._context, self.objects)
2112 
2113     def delete_all(self):
2114         consumer_uuids = set(alloc.consumer.uuid for alloc in self.objects)
2115         alloc_ids = [alloc.id for alloc in self.objects]
2116         _delete_allocations_by_ids(self._context, alloc_ids)
2117         consumer_obj.delete_consumers_if_no_allocations(
2118             self._context, consumer_uuids)
2119 
2120     def __repr__(self):
2121         strings = [repr(x) for x in self.objects]
2122         return "AllocationList[" + ", ".join(strings) + "]"
2123 
2124 
2125 @base.VersionedObjectRegistry.register_if(False)
2126 class Usage(base.VersionedObject):
2127 
2128     fields = {
2129         'resource_class': rc_fields.ResourceClassField(read_only=True),
2130         'usage': fields.NonNegativeIntegerField(),
2131     }
2132 
2133     @staticmethod
2134     def _from_db_object(context, target, source):
2135         for field in target.fields:
2136             if field not in ('resource_class'):
2137                 setattr(target, field, source[field])
2138 
2139         if 'resource_class' not in target:
2140             rc_str = _RC_CACHE.string_from_id(source['resource_class_id'])
2141             target.resource_class = rc_str
2142 
2143         target._context = context
2144         target.obj_reset_changes()
2145         return target
2146 
2147 
2148 @base.VersionedObjectRegistry.register_if(False)
2149 class UsageList(base.ObjectListBase, base.VersionedObject):
2150 
2151     fields = {
2152         'objects': fields.ListOfObjectsField('Usage'),
2153     }
2154 
2155     @staticmethod
2156     @db_api.placement_context_manager.reader
2157     def _get_all_by_resource_provider_uuid(context, rp_uuid):
2158         query = (context.session.query(models.Inventory.resource_class_id,
2159                  func.coalesce(func.sum(models.Allocation.used), 0))
2160                  .join(models.ResourceProvider,
2161                        models.Inventory.resource_provider_id ==
2162                        models.ResourceProvider.id)
2163                  .outerjoin(models.Allocation,
2164                             sql.and_(models.Inventory.resource_provider_id ==
2165                                      models.Allocation.resource_provider_id,
2166                                      models.Inventory.resource_class_id ==
2167                                      models.Allocation.resource_class_id))
2168                  .filter(models.ResourceProvider.uuid == rp_uuid)
2169                  .group_by(models.Inventory.resource_class_id))
2170         result = [dict(resource_class_id=item[0], usage=item[1])
2171                   for item in query.all()]
2172         return result
2173 
2174     @staticmethod
2175     @db_api.placement_context_manager.reader
2176     def _get_all_by_project_user(context, project_id, user_id=None):
2177         query = (context.session.query(models.Allocation.resource_class_id,
2178                  func.coalesce(func.sum(models.Allocation.used), 0))
2179                  .join(models.Consumer,
2180                        models.Allocation.consumer_id == models.Consumer.uuid)
2181                  .join(models.Project,
2182                        models.Consumer.project_id == models.Project.id)
2183                  .filter(models.Project.external_id == project_id))
2184         if user_id:
2185             query = query.join(models.User,
2186                                models.Consumer.user_id == models.User.id)
2187             query = query.filter(models.User.external_id == user_id)
2188         query = query.group_by(models.Allocation.resource_class_id)
2189         result = [dict(resource_class_id=item[0], usage=item[1])
2190                   for item in query.all()]
2191         return result
2192 
2193     @classmethod
2194     def get_all_by_resource_provider_uuid(cls, context, rp_uuid):
2195         _ensure_rc_cache(context)
2196         usage_list = cls._get_all_by_resource_provider_uuid(context, rp_uuid)
2197         return base.obj_make_list(context, cls(context), Usage, usage_list)
2198 
2199     @classmethod
2200     def get_all_by_project_user(cls, context, project_id, user_id=None):
2201         _ensure_rc_cache(context)
2202         usage_list = cls._get_all_by_project_user(context, project_id,
2203                                                   user_id=user_id)
2204         return base.obj_make_list(context, cls(context), Usage, usage_list)
2205 
2206     def __repr__(self):
2207         strings = [repr(x) for x in self.objects]
2208         return "UsageList[" + ", ".join(strings) + "]"
2209 
2210 
2211 @base.VersionedObjectRegistry.register_if(False)
2212 class ResourceClass(base.VersionedObject, base.TimestampedObject):
2213 
2214     MIN_CUSTOM_RESOURCE_CLASS_ID = 10000
2215     """Any user-defined resource classes must have an identifier greater than
2216     or equal to this number.
2217     """
2218 
2219     # Retry count for handling possible race condition in creating resource
2220     # class. We don't ever want to hit this, as it is simply a race when
2221     # creating these classes, but this is just a stopgap to prevent a potential
2222     # infinite loop.
2223     RESOURCE_CREATE_RETRY_COUNT = 100
2224 
2225     fields = {
2226         'id': fields.IntegerField(read_only=True),
2227         'name': rc_fields.ResourceClassField(nullable=False),
2228     }
2229 
2230     @staticmethod
2231     def _from_db_object(context, target, source):
2232         for field in target.fields:
2233             setattr(target, field, source[field])
2234 
2235         target._context = context
2236         target.obj_reset_changes()
2237         return target
2238 
2239     @classmethod
2240     def get_by_name(cls, context, name):
2241         """Return a ResourceClass object with the given string name.
2242 
2243         :param name: String name of the resource class to find
2244 
2245         :raises: ResourceClassNotFound if no such resource class was found
2246         """
2247         _ensure_rc_cache(context)
2248         rc = _RC_CACHE.all_from_string(name)
2249         obj = cls(context, id=rc['id'], name=rc['name'],
2250                   updated_at=rc['updated_at'], created_at=rc['created_at'])
2251         obj.obj_reset_changes()
2252         return obj
2253 
2254     @staticmethod
2255     @db_api.placement_context_manager.reader
2256     def _get_next_id(context):
2257         """Utility method to grab the next resource class identifier to use for
2258          user-defined resource classes.
2259         """
2260         query = context.session.query(func.max(models.ResourceClass.id))
2261         max_id = query.one()[0]
2262         if not max_id:
2263             return ResourceClass.MIN_CUSTOM_RESOURCE_CLASS_ID
2264         else:
2265             return max_id + 1
2266 
2267     def create(self):
2268         if 'id' in self:
2269             raise exception.ObjectActionError(action='create',
2270                                               reason='already created')
2271         if 'name' not in self:
2272             raise exception.ObjectActionError(action='create',
2273                                               reason='name is required')
2274         if self.name in rc_fields.ResourceClass.STANDARD:
2275             raise exception.ResourceClassExists(resource_class=self.name)
2276 
2277         if not self.name.startswith(rc_fields.ResourceClass.CUSTOM_NAMESPACE):
2278             raise exception.ObjectActionError(
2279                 action='create',
2280                 reason='name must start with ' +
2281                         rc_fields.ResourceClass.CUSTOM_NAMESPACE)
2282 
2283         updates = self.obj_get_changes()
2284         # There is the possibility of a race when adding resource classes, as
2285         # the ID is generated locally. This loop catches that exception, and
2286         # retries until either it succeeds, or a different exception is
2287         # encountered.
2288         retries = self.RESOURCE_CREATE_RETRY_COUNT
2289         while retries:
2290             retries -= 1
2291             try:
2292                 rc = self._create_in_db(self._context, updates)
2293                 self._from_db_object(self._context, self, rc)
2294                 break
2295             except db_exc.DBDuplicateEntry as e:
2296                 if 'id' in e.columns:
2297                     # Race condition for ID creation; try again
2298                     continue
2299                 # The duplication is on the other unique column, 'name'. So do
2300                 # not retry; raise the exception immediately.
2301                 raise exception.ResourceClassExists(resource_class=self.name)
2302         else:
2303             # We have no idea how common it will be in practice for the retry
2304             # limit to be exceeded. We set it high in the hope that we never
2305             # hit this point, but added this log message so we know that this
2306             # specific situation occurred.
2307             LOG.warning("Exceeded retry limit on ID generation while "
2308                         "creating ResourceClass %(name)s",
2309                         {'name': self.name})
2310             msg = _("creating resource class %s") % self.name
2311             raise exception.MaxDBRetriesExceeded(action=msg)
2312 
2313     @staticmethod
2314     @db_api.placement_context_manager.writer
2315     def _create_in_db(context, updates):
2316         next_id = ResourceClass._get_next_id(context)
2317         rc = models.ResourceClass()
2318         rc.update(updates)
2319         rc.id = next_id
2320         context.session.add(rc)
2321         return rc
2322 
2323     def destroy(self):
2324         if 'id' not in self:
2325             raise exception.ObjectActionError(action='destroy',
2326                                               reason='ID attribute not found')
2327         # Never delete any standard resource class, since the standard resource
2328         # classes don't even exist in the database table anyway.
2329         _ensure_rc_cache(self._context)
2330         if self.id in (rc['id'] for rc in _RC_CACHE.STANDARDS):
2331             raise exception.ResourceClassCannotDeleteStandard(
2332                     resource_class=self.name)
2333 
2334         self._destroy(self._context, self.id, self.name)
2335         _RC_CACHE.clear()
2336 
2337     @staticmethod
2338     @db_api.placement_context_manager.writer
2339     def _destroy(context, _id, name):
2340         # Don't delete the resource class if it is referred to in the
2341         # inventories table.
2342         num_inv = context.session.query(models.Inventory).filter(
2343                 models.Inventory.resource_class_id == _id).count()
2344         if num_inv:
2345             raise exception.ResourceClassInUse(resource_class=name)
2346 
2347         res = context.session.query(models.ResourceClass).filter(
2348                 models.ResourceClass.id == _id).delete()
2349         if not res:
2350             raise exception.NotFound()
2351 
2352     def save(self):
2353         if 'id' not in self:
2354             raise exception.ObjectActionError(action='save',
2355                                               reason='ID attribute not found')
2356         updates = self.obj_get_changes()
2357         # Never update any standard resource class, since the standard resource
2358         # classes don't even exist in the database table anyway.
2359         _ensure_rc_cache(self._context)
2360         if self.id in (rc['id'] for rc in _RC_CACHE.STANDARDS):
2361             raise exception.ResourceClassCannotUpdateStandard(
2362                     resource_class=self.name)
2363         self._save(self._context, self.id, self.name, updates)
2364         _RC_CACHE.clear()
2365 
2366     @staticmethod
2367     @db_api.placement_context_manager.writer
2368     def _save(context, id, name, updates):
2369         db_rc = context.session.query(models.ResourceClass).filter_by(
2370             id=id).first()
2371         db_rc.update(updates)
2372         try:
2373             db_rc.save(context.session)
2374         except db_exc.DBDuplicateEntry:
2375             raise exception.ResourceClassExists(resource_class=name)
2376 
2377 
2378 @base.VersionedObjectRegistry.register_if(False)
2379 class ResourceClassList(base.ObjectListBase, base.VersionedObject):
2380 
2381     fields = {
2382         'objects': fields.ListOfObjectsField('ResourceClass'),
2383     }
2384 
2385     @staticmethod
2386     @db_api.placement_context_manager.reader
2387     def _get_all(context):
2388         _ensure_rc_cache(context)
2389         customs = list(context.session.query(models.ResourceClass).all())
2390         return _RC_CACHE.STANDARDS + customs
2391 
2392     @classmethod
2393     def get_all(cls, context):
2394         resource_classes = cls._get_all(context)
2395         return base.obj_make_list(context, cls(context),
2396                                   ResourceClass, resource_classes)
2397 
2398     def __repr__(self):
2399         strings = [repr(x) for x in self.objects]
2400         return "ResourceClassList[" + ", ".join(strings) + "]"
2401 
2402 
2403 @base.VersionedObjectRegistry.register_if(False)
2404 class Trait(base.VersionedObject, base.TimestampedObject):
2405 
2406     # All the user-defined traits must begin with this prefix.
2407     CUSTOM_NAMESPACE = 'CUSTOM_'
2408 
2409     fields = {
2410         'id': fields.IntegerField(read_only=True),
2411         'name': fields.StringField(nullable=False)
2412     }
2413 
2414     @staticmethod
2415     def _from_db_object(context, trait, db_trait):
2416         for key in trait.fields:
2417             setattr(trait, key, db_trait[key])
2418         trait.obj_reset_changes()
2419         trait._context = context
2420         return trait
2421 
2422     @staticmethod
2423     @db_api.placement_context_manager.writer
2424     def _create_in_db(context, updates):
2425         trait = models.Trait()
2426         trait.update(updates)
2427         context.session.add(trait)
2428         return trait
2429 
2430     def create(self):
2431         if 'id' in self:
2432             raise exception.ObjectActionError(action='create',
2433                                               reason='already created')
2434         if 'name' not in self:
2435             raise exception.ObjectActionError(action='create',
2436                                               reason='name is required')
2437 
2438         updates = self.obj_get_changes()
2439 
2440         try:
2441             db_trait = self._create_in_db(self._context, updates)
2442         except db_exc.DBDuplicateEntry:
2443             raise exception.TraitExists(name=self.name)
2444 
2445         self._from_db_object(self._context, self, db_trait)
2446 
2447     @staticmethod
2448     @db_api.placement_context_manager.writer  # trait sync can cause a write
2449     def _get_by_name_from_db(context, name):
2450         result = context.session.query(models.Trait).filter_by(
2451             name=name).first()
2452         if not result:
2453             raise exception.TraitNotFound(names=name)
2454         return result
2455 
2456     @classmethod
2457     def get_by_name(cls, context, name):
2458         db_trait = cls._get_by_name_from_db(context, six.text_type(name))
2459         return cls._from_db_object(context, cls(), db_trait)
2460 
2461     @staticmethod
2462     @db_api.placement_context_manager.writer
2463     def _destroy_in_db(context, _id, name):
2464         num = context.session.query(models.ResourceProviderTrait).filter(
2465             models.ResourceProviderTrait.trait_id == _id).count()
2466         if num:
2467             raise exception.TraitInUse(name=name)
2468 
2469         res = context.session.query(models.Trait).filter_by(
2470             name=name).delete()
2471         if not res:
2472             raise exception.TraitNotFound(names=name)
2473 
2474     def destroy(self):
2475         if 'name' not in self:
2476             raise exception.ObjectActionError(action='destroy',
2477                                               reason='name is required')
2478 
2479         if not self.name.startswith(self.CUSTOM_NAMESPACE):
2480             raise exception.TraitCannotDeleteStandard(name=self.name)
2481 
2482         if 'id' not in self:
2483             raise exception.ObjectActionError(action='destroy',
2484                                               reason='ID attribute not found')
2485 
2486         self._destroy_in_db(self._context, self.id, self.name)
2487 
2488 
2489 @base.VersionedObjectRegistry.register_if(False)
2490 class TraitList(base.ObjectListBase, base.VersionedObject):
2491 
2492     fields = {
2493         'objects': fields.ListOfObjectsField('Trait')
2494     }
2495 
2496     @staticmethod
2497     @db_api.placement_context_manager.writer  # trait sync can cause a write
2498     def _get_all_from_db(context, filters):
2499         if not filters:
2500             filters = {}
2501 
2502         query = context.session.query(models.Trait)
2503         if 'name_in' in filters:
2504             query = query.filter(models.Trait.name.in_(
2505                 [six.text_type(n) for n in filters['name_in']]
2506             ))
2507         if 'prefix' in filters:
2508             query = query.filter(
2509                 models.Trait.name.like(six.text_type(filters['prefix'] + '%')))
2510         if 'associated' in filters:
2511             if filters['associated']:
2512                 query = query.join(models.ResourceProviderTrait,
2513                     models.Trait.id == models.ResourceProviderTrait.trait_id
2514                 ).distinct()
2515             else:
2516                 query = query.outerjoin(models.ResourceProviderTrait,
2517                     models.Trait.id == models.ResourceProviderTrait.trait_id
2518                 ).filter(models.ResourceProviderTrait.trait_id == null())
2519 
2520         return query.all()
2521 
2522     @base.remotable_classmethod
2523     def get_all(cls, context, filters=None):
2524         db_traits = cls._get_all_from_db(context, filters)
2525         return base.obj_make_list(context, cls(context), Trait, db_traits)
2526 
2527     @classmethod
2528     def get_all_by_resource_provider(cls, context, rp):
2529         """Returns a TraitList containing Trait objects for any trait
2530         associated with the supplied resource provider.
2531         """
2532         db_traits = _get_traits_by_provider_id(context, rp.id)
2533         return base.obj_make_list(context, cls(context), Trait, db_traits)
2534 
2535 
2536 @base.VersionedObjectRegistry.register_if(False)
2537 class AllocationRequestResource(base.VersionedObject):
2538 
2539     fields = {
2540         'resource_provider': fields.ObjectField('ResourceProvider'),
2541         'resource_class': rc_fields.ResourceClassField(read_only=True),
2542         'amount': fields.NonNegativeIntegerField(),
2543     }
2544 
2545 
2546 @base.VersionedObjectRegistry.register_if(False)
2547 class AllocationRequest(base.VersionedObject):
2548 
2549     fields = {
2550         # UUID of (the root of the tree including) the non-sharing resource
2551         # provider associated with this AllocationRequest. Internal use only,
2552         # not included when the object is serialized for output.
2553         'anchor_root_provider_uuid': fields.UUIDField(),
2554         # Whether all AllocationRequestResources in this AllocationRequest are
2555         # required to be satisfied by the same provider (based on the
2556         # corresponding RequestGroup's use_same_provider attribute). Internal
2557         # use only, not included when the object is serialized for output.
2558         'use_same_provider': fields.BooleanField(),
2559         'resource_requests': fields.ListOfObjectsField(
2560             'AllocationRequestResource'
2561         ),
2562     }
2563 
2564     def __repr__(self):
2565         anchor = (self.anchor_root_provider_uuid[-8:]
2566                   if 'anchor_root_provider_uuid' in self else '<?>')
2567         usp = self.use_same_provider if 'use_same_provider' in self else '<?>'
2568         repr_str = ('%s(anchor=...%s, same_provider=%s, '
2569                     'resource_requests=[%s])' %
2570                     (self.obj_name(), anchor, usp,
2571                      ', '.join([str(arr) for arr in self.resource_requests])))
2572         if six.PY2:
2573             repr_str = encodeutils.safe_encode(repr_str, incoming='utf-8')
2574         return repr_str
2575 
2576 
2577 @base.VersionedObjectRegistry.register_if(False)
2578 class ProviderSummaryResource(base.VersionedObject):
2579 
2580     fields = {
2581         'resource_class': rc_fields.ResourceClassField(read_only=True),
2582         'capacity': fields.NonNegativeIntegerField(),
2583         'used': fields.NonNegativeIntegerField(),
2584         # Internal use only; not included when the object is serialized for
2585         # output.
2586         'max_unit': fields.NonNegativeIntegerField(),
2587     }
2588 
2589 
2590 @base.VersionedObjectRegistry.register_if(False)
2591 class ProviderSummary(base.VersionedObject):
2592 
2593     fields = {
2594         'resource_provider': fields.ObjectField('ResourceProvider'),
2595         'resources': fields.ListOfObjectsField('ProviderSummaryResource'),
2596         'traits': fields.ListOfObjectsField('Trait'),
2597     }
2598 
2599     @property
2600     def resource_class_names(self):
2601         """Helper property that returns a set() of resource class string names
2602         that are included in the provider summary.
2603         """
2604         return set(res.resource_class for res in self.resources)
2605 
2606 
2607 @db_api.placement_context_manager.reader
2608 def _get_usages_by_provider_tree(ctx, root_ids):
2609     """Returns a row iterator of usage records grouped by provider ID
2610     for all resource providers in all trees indicated in the ``root_ids``.
2611     """
2612     # We build up a SQL expression that looks like this:
2613     # SELECT
2614     #   rp.id as resource_provider_id
2615     # , rp.uuid as resource_provider_uuid
2616     # , inv.resource_class_id
2617     # , inv.total
2618     # , inv.reserved
2619     # , inv.allocation_ratio
2620     # , inv.max_unit
2621     # , usage.used
2622     # FROM resource_providers AS rp
2623     # LEFT JOIN inventories AS inv
2624     #  ON rp.id = inv.resource_provider_id
2625     # LEFT JOIN (
2626     #   SELECT resource_provider_id, resource_class_id, SUM(used) as used
2627     #   FROM allocations
2628     #   JOIN resource_providers
2629     #     ON allocations.resource_provider_id = resource_providers.id
2630     #     AND resource_providers.root_provider_id IN($root_ids)
2631     #   GROUP BY resource_provider_id, resource_class_id
2632     # )
2633     # AS usages
2634     #   ON inv.resource_provider_id = usage.resource_provider_id
2635     #   AND inv.resource_class_id = usage.resource_class_id
2636     # WHERE rp.root_provider_id IN ($root_ids)
2637     rpt = sa.alias(_RP_TBL, name="rp")
2638     inv = sa.alias(_INV_TBL, name="inv")
2639     # Build our derived table (subquery in the FROM clause) that sums used
2640     # amounts for resource provider and resource class
2641     derived_alloc_to_rp = sa.join(
2642         _ALLOC_TBL, _RP_TBL,
2643         sa.and_(_ALLOC_TBL.c.resource_provider_id == _RP_TBL.c.id,
2644                 _RP_TBL.c.root_provider_id.in_(root_ids)))
2645     usage = sa.alias(
2646         sa.select([
2647             _ALLOC_TBL.c.resource_provider_id,
2648             _ALLOC_TBL.c.resource_class_id,
2649             sql.func.sum(_ALLOC_TBL.c.used).label('used'),
2650         ]).select_from(derived_alloc_to_rp).group_by(
2651             _ALLOC_TBL.c.resource_provider_id,
2652             _ALLOC_TBL.c.resource_class_id
2653         ),
2654         name='usage')
2655     # Build a join between the resource providers and inventories table
2656     rpt_inv_join = sa.outerjoin(rpt, inv,
2657                                 rpt.c.id == inv.c.resource_provider_id)
2658     # And then join to the derived table of usages
2659     usage_join = sa.outerjoin(
2660         rpt_inv_join,
2661         usage,
2662         sa.and_(
2663             usage.c.resource_provider_id == inv.c.resource_provider_id,
2664             usage.c.resource_class_id == inv.c.resource_class_id,
2665         ),
2666     )
2667     query = sa.select([
2668         rpt.c.id.label("resource_provider_id"),
2669         rpt.c.uuid.label("resource_provider_uuid"),
2670         inv.c.resource_class_id,
2671         inv.c.total,
2672         inv.c.reserved,
2673         inv.c.allocation_ratio,
2674         inv.c.max_unit,
2675         usage.c.used,
2676     ]).select_from(usage_join).where(rpt.c.root_provider_id.in_(root_ids))
2677     return ctx.session.execute(query).fetchall()
2678 
2679 
2680 @db_api.placement_context_manager.reader
2681 def _get_provider_ids_having_any_trait(ctx, traits):
2682     """Returns a list of resource provider internal IDs that have ANY of the
2683     supplied traits.
2684 
2685     :param ctx: Session context to use
2686     :param traits: A map, keyed by trait string name, of trait internal IDs, at
2687                    least one of which each provider must have associated with
2688                    it.
2689     :raise ValueError: If traits is empty or None.
2690     """
2691     if not traits:
2692         raise ValueError(_('traits must not be empty'))
2693 
2694     rptt = sa.alias(_RP_TRAIT_TBL, name="rpt")
2695     sel = sa.select([rptt.c.resource_provider_id])
2696     sel = sel.where(rptt.c.trait_id.in_(traits.values()))
2697     sel = sel.group_by(rptt.c.resource_provider_id)
2698     return [r[0] for r in ctx.session.execute(sel)]
2699 
2700 
2701 @db_api.placement_context_manager.reader
2702 def _get_provider_ids_having_all_traits(ctx, required_traits):
2703     """Returns a list of resource provider internal IDs that have ALL of the
2704     required traits.
2705 
2706     NOTE: Don't call this method with no required_traits.
2707 
2708     :param ctx: Session context to use
2709     :param required_traits: A map, keyed by trait string name, of required
2710                             trait internal IDs that each provider must have
2711                             associated with it
2712     :raise ValueError: If required_traits is empty or None.
2713     """
2714     if not required_traits:
2715         raise ValueError(_('required_traits must not be empty'))
2716 
2717     rptt = sa.alias(_RP_TRAIT_TBL, name="rpt")
2718     sel = sa.select([rptt.c.resource_provider_id])
2719     sel = sel.where(rptt.c.trait_id.in_(required_traits.values()))
2720     sel = sel.group_by(rptt.c.resource_provider_id)
2721     # Only get the resource providers that have ALL the required traits, so we
2722     # need to GROUP BY the resource provider and ensure that the
2723     # COUNT(trait_id) is equal to the number of traits we are requiring
2724     num_traits = len(required_traits)
2725     cond = sa.func.count(rptt.c.trait_id) == num_traits
2726     sel = sel.having(cond)
2727     return [r[0] for r in ctx.session.execute(sel)]
2728 
2729 
2730 @db_api.placement_context_manager.reader
2731 def _has_provider_trees(ctx):
2732     """Simple method that returns whether provider trees (i.e. nested resource
2733     providers) are in use in the deployment at all. This information is used to
2734     switch code paths when attempting to retrieve allocation candidate
2735     information. The code paths are eminently easier to execute and follow for
2736     non-nested scenarios...
2737 
2738     NOTE(jaypipes): The result of this function can be cached extensively.
2739     """
2740     sel = sa.select([_RP_TBL.c.id])
2741     sel = sel.where(_RP_TBL.c.parent_provider_id.isnot(None))
2742     sel = sel.limit(1)
2743     res = ctx.session.execute(sel).fetchall()
2744     return len(res) > 0
2745 
2746 
2747 @db_api.placement_context_manager.reader
2748 def _get_provider_ids_matching(ctx, resources, required_traits,
2749         forbidden_traits, member_of=None):
2750     """Returns a list of tuples of (internal provider ID, root provider ID)
2751     that have available inventory to satisfy all the supplied requests for
2752     resources.
2753 
2754     :note: This function is used for scenarios that do NOT involve sharing
2755     providers.
2756 
2757     :param ctx: Session context to use
2758     :param resources: A dict, keyed by resource class ID, of the amount
2759                       requested of that resource class.
2760     :param required_traits: A map, keyed by trait string name, of required
2761                             trait internal IDs that each provider must have
2762                             associated with it
2763     :param forbidden_traits: A map, keyed by trait string name, of forbidden
2764                              trait internal IDs that each provider must not
2765                              have associated with it
2766     :param member_of: An optional list of list of aggregate UUIDs. If provided,
2767                       the allocation_candidates returned will only be for
2768                       resource providers that are members of one or more of the
2769                       supplied aggregates of each aggregate UUID list.
2770     """
2771     trait_rps = None
2772     forbidden_rp_ids = None
2773     if required_traits:
2774         trait_rps = _get_provider_ids_having_all_traits(ctx, required_traits)
2775         if not trait_rps:
2776             return []
2777     if forbidden_traits:
2778         forbidden_rp_ids = _get_provider_ids_having_any_trait(
2779             ctx, forbidden_traits)
2780 
2781     rpt = sa.alias(_RP_TBL, name="rp")
2782 
2783     rc_name_map = {
2784         rc_id: _RC_CACHE.string_from_id(rc_id).lower() for rc_id in resources
2785     }
2786 
2787     # Dict, keyed by resource class ID, of an aliased table object for the
2788     # inventories table winnowed to only that resource class.
2789     inv_tables = {
2790         rc_id: sa.alias(_INV_TBL, name='inv_%s' % rc_name_map[rc_id])
2791         for rc_id in resources
2792     }
2793 
2794     # Dict, keyed by resource class ID, of a derived table (subquery in the
2795     # FROM clause or JOIN) against the allocations table winnowed to only that
2796     # resource class, grouped by resource provider.
2797     usage_tables = {
2798         rc_id: sa.alias(
2799             sa.select([
2800                 _ALLOC_TBL.c.resource_provider_id,
2801                 sql.func.sum(_ALLOC_TBL.c.used).label('used'),
2802             ]).where(
2803                 _ALLOC_TBL.c.resource_class_id == rc_id
2804             ).group_by(
2805                 _ALLOC_TBL.c.resource_provider_id
2806             ),
2807             name='usage_%s' % rc_name_map[rc_id],
2808         )
2809         for rc_id in resources
2810     }
2811 
2812     sel = sa.select([rpt.c.id, rpt.c.root_provider_id])
2813 
2814     # List of the WHERE conditions we build up by iterating over the requested
2815     # resources
2816     where_conds = []
2817 
2818     # First filter by the resource providers that had all the required traits
2819     if trait_rps:
2820         where_conds.append(rpt.c.id.in_(trait_rps))
2821     # or have any forbidden trait
2822     if forbidden_rp_ids:
2823         where_conds.append(~rpt.c.id.in_(forbidden_rp_ids))
2824 
2825     # The chain of joins that we eventually pass to select_from()
2826     join_chain = rpt
2827 
2828     for rc_id, amount in resources.items():
2829         inv_by_rc = inv_tables[rc_id]
2830         usage_by_rc = usage_tables[rc_id]
2831 
2832         # We can do a more efficient INNER JOIN because we don't have shared
2833         # resource providers to deal with
2834         rp_inv_join = sa.join(
2835             join_chain, inv_by_rc,
2836             sa.and_(
2837                 inv_by_rc.c.resource_provider_id == rpt.c.id,
2838                 # Add a join condition winnowing this copy of inventories table
2839                 # to only the resource class being analyzed in this loop...
2840                 inv_by_rc.c.resource_class_id == rc_id,
2841             ),
2842         )
2843         rp_inv_usage_join = sa.outerjoin(
2844             rp_inv_join, usage_by_rc,
2845             inv_by_rc.c.resource_provider_id ==
2846                 usage_by_rc.c.resource_provider_id,
2847         )
2848         join_chain = rp_inv_usage_join
2849 
2850         usage_cond = sa.and_(
2851             (
2852             (sql.func.coalesce(usage_by_rc.c.used, 0) + amount) <=
2853             (inv_by_rc.c.total - inv_by_rc.c.reserved) *
2854                 inv_by_rc.c.allocation_ratio
2855             ),
2856             inv_by_rc.c.min_unit <= amount,
2857             inv_by_rc.c.max_unit >= amount,
2858             amount % inv_by_rc.c.step_size == 0,
2859         )
2860         where_conds.append(usage_cond)
2861 
2862     # If 'member_of' has values, do a separate lookup to identify the
2863     # resource providers that meet the member_of constraints.
2864     if member_of:
2865         rps_in_aggs = _provider_ids_matching_aggregates(ctx, member_of)
2866         if not rps_in_aggs:
2867             # Short-circuit. The user either asked for a non-existing
2868             # aggregate or there were no resource providers that matched
2869             # the requirements...
2870             return []
2871         where_conds.append(rpt.c.id.in_(rps_in_aggs))
2872 
2873     sel = sel.select_from(join_chain)
2874     sel = sel.where(sa.and_(*where_conds))
2875 
2876     return [(r[0], r[1]) for r in ctx.session.execute(sel)]
2877 
2878 
2879 @db_api.placement_context_manager.reader
2880 def _provider_aggregates(ctx, rp_ids):
2881     """Given a list of resource provider internal IDs, returns a dict,
2882     keyed by those provider IDs, of sets of aggregate ids associated
2883     with that provider.
2884 
2885     :raises: ValueError when rp_ids is empty.
2886 
2887     :param ctx: nova.context.RequestContext object
2888     :param rp_ids: list of resource provider IDs
2889     """
2890     if not rp_ids:
2891         raise ValueError(_("Expected rp_ids to be a list of resource provider "
2892                            "internal IDs, but got an empty list."))
2893 
2894     rpat = sa.alias(_RP_AGG_TBL, name='rpat')
2895     sel = sa.select([rpat.c.resource_provider_id,
2896                      rpat.c.aggregate_id])
2897     sel = sel.where(rpat.c.resource_provider_id.in_(rp_ids))
2898     res = collections.defaultdict(set)
2899     for r in ctx.session.execute(sel):
2900         res[r[0]].add(r[1])
2901     return res
2902 
2903 
2904 @db_api.placement_context_manager.reader
2905 def _get_providers_with_resource(ctx, rc_id, amount):
2906     """Returns a set of tuples of (provider ID, root provider ID) of providers
2907     that satisfy the request for a single resource class.
2908 
2909     :param ctx: Session context to use
2910     :param rc_id: Internal ID of resource class to check inventory for
2911     :param amount: Amount of resource being requested
2912     """
2913     # SELECT rp.id, rp.root_provider_id
2914     # FROM resource_providers AS rp
2915     # JOIN inventories AS inv
2916     #  ON rp.id = inv.resource_provider_id
2917     #  AND inv.resource_class_id = $RC_ID
2918     # LEFT JOIN (
2919     #  SELECT
2920     #    alloc.resource_provider_id,
2921     #    SUM(allocs.used) AS used
2922     #  FROM allocations AS alloc
2923     #  WHERE allocs.resource_class_id = $RC_ID
2924     #  GROUP BY allocs.resource_provider_id
2925     # ) AS usage
2926     #  ON inv.resource_provider_id = usage.resource_provider_id
2927     # WHERE
2928     #  used + $AMOUNT <= ((total - reserved) * inv.allocation_ratio)
2929     #  AND inv.min_unit <= $AMOUNT
2930     #  AND inv.max_unit >= $AMOUNT
2931     #  AND $AMOUNT % inv.step_size == 0
2932     rpt = sa.alias(_RP_TBL, name="rp")
2933     inv = sa.alias(_INV_TBL, name="inv")
2934     allocs = sa.alias(_ALLOC_TBL, name="alloc")
2935     usage = sa.select([
2936             allocs.c.resource_provider_id,
2937             sql.func.sum(allocs.c.used).label('used')])
2938     usage = usage.where(allocs.c.resource_class_id == rc_id)
2939     usage = usage.group_by(allocs.c.resource_provider_id)
2940     usage = sa.alias(usage, name="usage")
2941     where_conds = [
2942         sql.func.coalesce(usage.c.used, 0) + amount <= (
2943             (inv.c.total - inv.c.reserved) * inv.c.allocation_ratio),
2944         inv.c.min_unit <= amount,
2945         inv.c.max_unit >= amount,
2946         amount % inv.c.step_size == 0,
2947     ]
2948     rp_to_inv = sa.join(
2949         rpt, inv, sa.and_(
2950             rpt.c.id == inv.c.resource_provider_id,
2951             inv.c.resource_class_id == rc_id))
2952     inv_to_usage = sa.outerjoin(
2953         rp_to_inv, usage,
2954         inv.c.resource_provider_id == usage.c.resource_provider_id)
2955     sel = sa.select([rpt.c.id, rpt.c.root_provider_id])
2956     sel = sel.select_from(inv_to_usage)
2957     sel = sel.where(sa.and_(*where_conds))
2958     res = ctx.session.execute(sel).fetchall()
2959     res = set((r[0], r[1]) for r in res)
2960     return res
2961 
2962 
2963 @db_api.placement_context_manager.reader
2964 def _get_trees_with_traits(ctx, rp_ids, required_traits, forbidden_traits):
2965     """Given a list of provider IDs, filter them to return a set of tuples of
2966     (provider ID, root provider ID) of providers which belong to a tree that
2967     can satisfy trait requirements.
2968 
2969     :param ctx: Session context to use
2970     :param rp_ids: a set of resource provider IDs
2971     :param required_traits: A map, keyed by trait string name, of required
2972                             trait internal IDs that each provider TREE must
2973                             COLLECTIVELY have associated with it
2974     :param forbidden_traits: A map, keyed by trait string name, of trait
2975                              internal IDs that a resource provider must
2976                              not have.
2977     """
2978     # We now want to restrict the returned providers to only those provider
2979     # trees that have all our required traits.
2980     #
2981     # The SQL we want looks like this:
2982     #
2983     # SELECT outer_rp.id, outer_rp.root_provider_id
2984     # FROM resource_providers AS outer_rp
2985     # JOIN (
2986     #   SELECT rp.root_provider_id
2987     #   FROM resource_providers AS rp
2988     #   # Only if we have required traits...
2989     #   INNER JOIN resource_provider_traits AS rptt
2990     #   ON rp.id = rptt.resource_provider_id
2991     #   AND rptt.trait_id IN ($REQUIRED_TRAIT_IDS)
2992     #   # Only if we have forbidden_traits...
2993     #   LEFT JOIN resource_provider_traits AS rptt_forbid
2994     #   ON rp.id = rptt_forbid.resource_provider_id
2995     #   AND rptt_forbid.trait_id IN ($FORBIDDEN_TRAIT_IDS)
2996     #   WHERE rp.id IN ($RP_IDS)
2997     #   # Only if we have forbidden traits...
2998     #   AND rptt_forbid.resource_provider_id IS NULL
2999     #   GROUP BY rp.root_provider_id
3000     #   # Only if have required traits...
3001     #   HAVING COUNT(DISTINCT rptt.trait_id) == $NUM_REQUIRED_TRAITS
3002     # ) AS trees_with_traits
3003     #  ON outer_rp.root_provider_id = trees_with_traits.root_provider_id
3004     rpt = sa.alias(_RP_TBL, name="rp")
3005     cond = [rpt.c.id.in_(rp_ids)]
3006     subq = sa.select([rpt.c.root_provider_id])
3007     subq_join = None
3008     if required_traits:
3009         rptt = sa.alias(_RP_TRAIT_TBL, name="rptt")
3010         rpt_to_rptt = sa.join(
3011             rpt, rptt, sa.and_(
3012                 rpt.c.id == rptt.c.resource_provider_id,
3013                 rptt.c.trait_id.in_(required_traits.values())))
3014         subq_join = rpt_to_rptt
3015         # Only get the resource providers that have ALL the required traits,
3016         # so we need to GROUP BY the root provider and ensure that the
3017         # COUNT(trait_id) is equal to the number of traits we are requiring
3018         num_traits = len(required_traits)
3019         having_cond = sa.func.count(sa.distinct(rptt.c.trait_id)) == num_traits
3020         subq = subq.having(having_cond)
3021 
3022     # Tack on an additional LEFT JOIN clause inside the derived table if we've
3023     # got forbidden traits in the mix.
3024     if forbidden_traits:
3025         rptt_forbid = sa.alias(_RP_TRAIT_TBL, name="rptt_forbid")
3026         join_to = rpt
3027         if subq_join is not None:
3028             join_to = subq_join
3029         rpt_to_rptt_forbid = sa.outerjoin(
3030             join_to, rptt_forbid, sa.and_(
3031                 rpt.c.id == rptt_forbid.c.resource_provider_id,
3032                 rptt_forbid.c.trait_id.in_(forbidden_traits.values())))
3033         cond.append(rptt_forbid.c.resource_provider_id == sa.null())
3034         subq_join = rpt_to_rptt_forbid
3035 
3036     subq = subq.select_from(subq_join)
3037     subq = subq.where(sa.and_(*cond))
3038     subq = subq.group_by(rpt.c.root_provider_id)
3039     trees_with_traits = sa.alias(subq, name="trees_with_traits")
3040 
3041     outer_rps = sa.alias(_RP_TBL, name="outer_rps")
3042     outer_to_subq = sa.join(
3043         outer_rps, trees_with_traits,
3044         outer_rps.c.root_provider_id == trees_with_traits.c.root_provider_id)
3045     sel = sa.select([outer_rps.c.id, outer_rps.c.root_provider_id])
3046     sel = sel.select_from(outer_to_subq)
3047     res = ctx.session.execute(sel).fetchall()
3048 
3049     return [(rp_id, root_id) for rp_id, root_id in res]
3050 
3051 
3052 @db_api.placement_context_manager.reader
3053 def _get_trees_matching_all(ctx, resources, required_traits, forbidden_traits,
3054                             sharing, member_of):
3055     """Returns a list of two-tuples (provider internal ID, root provider
3056     internal ID) for providers that satisfy the request for resources.
3057 
3058     If traits are also required, this function only returns results where the
3059     set of providers within a tree that satisfy the resource request
3060     collectively have all the required traits associated with them. This means
3061     that given the following provider tree:
3062 
3063     cn1
3064      |
3065      --> pf1 (SRIOV_NET_VF:2)
3066      |
3067      --> pf2 (SRIOV_NET_VF:1, HW_NIC_OFFLOAD_GENEVE)
3068 
3069     If a user requests 1 SRIOV_NET_VF resource and no required traits will
3070     return both pf1 and pf2. However, a request for 2 SRIOV_NET_VF and required
3071     trait of HW_NIC_OFFLOAD_GENEVE will return no results (since pf1 is the
3072     only provider with enough inventory of SRIOV_NET_VF but it does not have
3073     the required HW_NIC_OFFLOAD_GENEVE trait).
3074 
3075     :note: This function is used for scenarios to get results for a
3076     RequestGroup with use_same_provider=False. In this scenario, we are able
3077     to use multiple providers within the same provider tree including sharing
3078     providers to satisfy different resources involved in a single RequestGroup.
3079 
3080     :param ctx: Session context to use
3081     :param resources: A dict, keyed by resource class ID, of the amount
3082                       requested of that resource class.
3083     :param required_traits: A map, keyed by trait string name, of required
3084                             trait internal IDs that each provider TREE must
3085                             COLLECTIVELY have associated with it
3086     :param forbidden_traits: A map, keyed by trait string name, of trait
3087                              internal IDs that a resource provider must
3088                              not have.
3089     :param sharing: dict, keyed by resource class ID, of lists of resource
3090                     provider IDs that share that resource class and can
3091                     contribute to the overall allocation request
3092     :param member_of: An optional list of lists of aggregate UUIDs. If
3093                       provided, the allocation_candidates returned will only be
3094                       for resource providers that are members of one or more of
3095                       the supplied aggregates in each aggregate UUID list.
3096     """
3097     # We first grab the provider trees that have nodes that meet the request
3098     # for each resource class.  Once we have this information, we'll then do a
3099     # followup query to winnow the set of resource providers to only those
3100     # provider *trees* that have all of the required traits.
3101     provs_with_inv = set()
3102     # provs_with_inv is a list of three-tuples with the second element being
3103     # the root provider ID and the third being resource class ID. Get the list
3104     # of root provider IDs and get all trees that collectively have all
3105     # required traits.
3106     trees_with_inv = set()
3107 
3108     for rc_id, amount in resources.items():
3109         rc_provs_with_inv = _get_providers_with_resource(ctx, rc_id, amount)
3110         if not rc_provs_with_inv:
3111             # If there's no providers that have one of the resource classes,
3112             # then we can short-circuit
3113             return []
3114         rc_trees = set(p[1] for p in rc_provs_with_inv)
3115         provs_with_inv |= set((p[0], p[1], rc_id) for p in rc_provs_with_inv)
3116 
3117         sharing_providers = sharing.get(rc_id)
3118         if sharing_providers:
3119             # There are sharing providers for this resource class, so we
3120             # should also get combinations of (sharing provider, anchor root)
3121             # in addition to (non-sharing provider, anchor root) we already
3122             # have.
3123             rc_provs_with_inv = _anchors_for_sharing_providers(
3124                                         ctx, sharing_providers, get_id=True)
3125             rc_provs_with_inv = set(
3126                 (p[0], p[1], rc_id) for p in rc_provs_with_inv)
3127             rc_trees |= set(p[1] for p in rc_provs_with_inv)
3128             provs_with_inv |= rc_provs_with_inv
3129 
3130         # Filter trees_with_inv to have only trees with enough inventories
3131         # for this resource class. Here "tree" includes sharing providers
3132         # in its terminology
3133         if trees_with_inv:
3134             trees_with_inv &= rc_trees
3135         else:
3136             trees_with_inv = rc_trees
3137 
3138         if not trees_with_inv:
3139             return []
3140 
3141     # Select only those tuples where there are providers for all requested
3142     # resource classes (trees_with_inv contains the root provider IDs of those
3143     # trees that contain all our requested resources)
3144     provs_with_inv = set(p for p in provs_with_inv if p[1] in trees_with_inv)
3145 
3146     if not provs_with_inv:
3147         return []
3148 
3149     # If 'member_of' has values, do a separate lookup to identify the
3150     # resource providers that meet the member_of constraints.
3151     if member_of:
3152         rps_in_aggs = _provider_ids_matching_aggregates(ctx, member_of,
3153                                                         rp_ids=trees_with_inv)
3154         if not rps_in_aggs:
3155             # Short-circuit. The user either asked for a non-existing
3156             # aggregate or there were no resource providers that matched
3157             # the requirements...
3158             return []
3159         provs_with_inv = set(p for p in provs_with_inv if p[1] in rps_in_aggs)
3160 
3161     if (not required_traits and not forbidden_traits) or (
3162             any(sharing.values())):
3163         # If there were no traits required, there's no difference in how we
3164         # calculate allocation requests between nested and non-nested
3165         # environments, so just short-circuit and return. Or if sharing
3166         # providers are in play, we check the trait constraints later
3167         # in _alloc_candidates_multiple_providers(), so skip.
3168         return list(provs_with_inv)
3169 
3170     # Return the providers where the providers have the available inventory
3171     # capacity and that set of providers (grouped by their tree) have all
3172     # of the required traits and none of the forbidden traits
3173     rp_ids_with_inv = set(p[0] for p in provs_with_inv)
3174     rp_tuples_with_trait = _get_trees_with_traits(
3175         ctx, rp_ids_with_inv, required_traits, forbidden_traits)
3176 
3177     ret = [rp_tuple for rp_tuple in provs_with_inv if (
3178         rp_tuple[0], rp_tuple[1]) in rp_tuples_with_trait]
3179 
3180     return ret
3181 
3182 
3183 def _build_provider_summaries(context, usages, prov_traits):
3184     """Given a list of dicts of usage information and a map of providers to
3185     their associated string traits, returns a dict, keyed by resource provider
3186     ID, of ProviderSummary objects.
3187 
3188     :param context: nova.context.RequestContext object
3189     :param usages: A list of dicts with the following format:
3190 
3191         {
3192             'resource_provider_id': <internal resource provider ID>,
3193             'resource_provider_uuid': <UUID>,
3194             'resource_class_id': <internal resource class ID>,
3195             'total': integer,
3196             'reserved': integer,
3197             'allocation_ratio': float,
3198         }
3199     :param prov_traits: A dict, keyed by internal resource provider ID, of
3200                         string trait names associated with that provider
3201     """
3202     # Build up a dict, keyed by internal resource provider ID, of
3203     # ProviderSummary objects containing one or more ProviderSummaryResource
3204     # objects representing the resources the provider has inventory for.
3205     summaries = {}
3206     for usage in usages:
3207         rp_id = usage['resource_provider_id']
3208         rp_uuid = usage['resource_provider_uuid']
3209         summary = summaries.get(rp_id)
3210         if not summary:
3211             summary = ProviderSummary(
3212                 context,
3213                 resource_provider=ResourceProvider.get_by_uuid(context,
3214                                                                uuid=rp_uuid),
3215                 resources=[],
3216             )
3217             summaries[rp_id] = summary
3218 
3219         traits = prov_traits[rp_id]
3220         summary.traits = [Trait(context, name=tname) for tname in traits]
3221 
3222         rc_id = usage['resource_class_id']
3223         if rc_id is None:
3224             # NOTE(tetsuro): This provider doesn't have any inventory itself.
3225             # But we include this provider in summaries since another
3226             # provider in the same tree will be in the "allocation_request".
3227             # Let's skip the following and leave "ProviderSummary.resources"
3228             # field empty.
3229             continue
3230         # NOTE(jaypipes): usage['used'] may be None due to the LEFT JOIN of
3231         # the usages subquery, so we coerce NULL values to 0 here.
3232         used = usage['used'] or 0
3233         allocation_ratio = usage['allocation_ratio']
3234         cap = int((usage['total'] - usage['reserved']) * allocation_ratio)
3235         rc_name = _RC_CACHE.string_from_id(rc_id)
3236         rpsr = ProviderSummaryResource(
3237             context,
3238             resource_class=rc_name,
3239             capacity=cap,
3240             used=used,
3241             max_unit=usage['max_unit'],
3242         )
3243         summary.resources.append(rpsr)
3244     return summaries
3245 
3246 
3247 def _aggregates_associated_with_providers(a, b, prov_aggs):
3248     """quickly check if the two rps are in the same aggregates
3249 
3250     :param a: resource provider ID for first provider
3251     :param b: resource provider ID for second provider
3252     :param prov_aggs: a dict keyed by resource provider IDs, of sets
3253                       of aggregate ids associated with that provider
3254     """
3255     a_aggs = prov_aggs[a]
3256     b_aggs = prov_aggs[b]
3257     return a_aggs & b_aggs
3258 
3259 
3260 def _shared_allocation_request_resources(ctx, ns_rp_id, requested_resources,
3261                                          sharing, summaries, prov_aggs):
3262     """Returns a dict, keyed by resource class ID, of lists of
3263     AllocationRequestResource objects that represent resources that are
3264     provided by a sharing provider.
3265 
3266     :param ctx: nova.context.RequestContext object
3267     :param ns_rp_id: an internal ID of a non-sharing resource provider
3268     :param requested_resources: dict, keyed by resource class ID, of amounts
3269                                 being requested for that resource class
3270     :param sharing: dict, keyed by resource class ID, of lists of resource
3271                     provider IDs that share that resource class and can
3272                     contribute to the overall allocation request
3273     :param summaries: dict, keyed by resource provider ID, of ProviderSummary
3274                       objects containing usage and trait information for
3275                       resource providers involved in the overall request
3276     :param prov_aggs: dict, keyed by resource provider ID, of sets of
3277                       aggregate ids associated with that provider.
3278     """
3279     res_requests = collections.defaultdict(list)
3280     for rc_id in sharing:
3281         for rp_id in sharing[rc_id]:
3282             aggs_in_both = _aggregates_associated_with_providers(
3283                 ns_rp_id, rp_id, prov_aggs)
3284             if not aggs_in_both:
3285                 continue
3286             summary = summaries[rp_id]
3287             rp_uuid = summary.resource_provider.uuid
3288             res_req = AllocationRequestResource(
3289                 ctx,
3290                 resource_provider=ResourceProvider(ctx, uuid=rp_uuid),
3291                 resource_class=_RC_CACHE.string_from_id(rc_id),
3292                 amount=requested_resources[rc_id],
3293             )
3294             res_requests[rc_id].append(res_req)
3295     return res_requests
3296 
3297 
3298 def _allocation_request_for_provider(ctx, requested_resources, provider):
3299     """Returns an AllocationRequest object containing AllocationRequestResource
3300     objects for each resource class in the supplied requested resources dict.
3301 
3302     :param ctx: nova.context.RequestContext object
3303     :param requested_resources: dict, keyed by resource class ID, of amounts
3304                                 being requested for that resource class
3305     :param provider: ResourceProvider object representing the provider of the
3306                      resources.
3307     """
3308     resource_requests = [
3309         AllocationRequestResource(
3310             ctx, resource_provider=provider,
3311             resource_class=_RC_CACHE.string_from_id(rc_id),
3312             amount=amount,
3313         ) for rc_id, amount in requested_resources.items()
3314     ]
3315     # NOTE(efried): This method only produces an AllocationRequest with its
3316     # anchor in its own tree.  If the provider is a sharing provider, the
3317     # caller needs to identify the other anchors with which it might be
3318     # associated.
3319     return AllocationRequest(
3320             ctx, resource_requests=resource_requests,
3321             anchor_root_provider_uuid=provider.root_provider_uuid)
3322 
3323 
3324 def _check_traits_for_alloc_request(res_requests, summaries, prov_traits,
3325                                     required_traits, forbidden_traits):
3326     """Given a list of AllocationRequestResource objects, check if that
3327     combination can provide trait constraints. If it can, returns all
3328     resource provider internal IDs in play, else return an empty list.
3329 
3330     TODO(tetsuro): For optimization, we should move this logic to SQL in
3331                    _get_trees_matching_all().
3332 
3333     :param res_requests: a list of AllocationRequestResource objects that have
3334                          resource providers to be checked if they collectively
3335                          satisfy trait constraints in the required_traits and
3336                          forbidden_traits parameters.
3337     :param summaries: dict, keyed by resource provider ID, of ProviderSummary
3338                       objects containing usage and trait information for
3339                       resource providers involved in the overall request
3340     :param prov_traits: A dict, keyed by internal resource provider ID, of
3341                         string trait names associated with that provider
3342     :param required_traits: A map, keyed by trait string name, of required
3343                             trait internal IDs that each *allocation request's
3344                             set of providers* must *collectively* have
3345                             associated with them
3346     :param forbidden_traits: A map, keyed by trait string name, of trait
3347                              internal IDs that a resource provider must
3348                              not have.
3349     """
3350     all_prov_ids = []
3351     all_traits = set()
3352     for res_req in res_requests:
3353         rp_uuid = res_req.resource_provider.uuid
3354         for rp_id, summary in summaries.items():
3355             if summary.resource_provider.uuid == rp_uuid:
3356                 break
3357         rp_traits = set(prov_traits.get(rp_id, []))
3358 
3359         # Check if there are forbidden_traits
3360         conflict_traits = set(forbidden_traits) & set(rp_traits)
3361         if conflict_traits:
3362             LOG.debug('Excluding resource provider %s, it has '
3363                       'forbidden traits: (%s).',
3364                       rp_id, ', '.join(conflict_traits))
3365             return []
3366 
3367         all_prov_ids.append(rp_id)
3368         all_traits |= rp_traits
3369 
3370     # Check if there are missing traits
3371     missing_traits = set(required_traits) - all_traits
3372     if missing_traits:
3373         LOG.debug('Excluding a set of allocation candidate %s : '
3374                   'missing traits %s are not satisfied.',
3375                   all_prov_ids, ','.join(missing_traits))
3376         return []
3377 
3378     return all_prov_ids
3379 
3380 
3381 def _alloc_candidates_single_provider(ctx, requested_resources, rp_tuples):
3382     """Returns a tuple of (allocation requests, provider summaries) for a
3383     supplied set of requested resource amounts and resource providers. The
3384     supplied resource providers have capacity to satisfy ALL of the resources
3385     in the requested resources as well as ALL required traits that were
3386     requested by the user.
3387 
3388     This is used in two circumstances:
3389     - To get results for a RequestGroup with use_same_provider=True.
3390     - As an optimization when no sharing providers satisfy any of the requested
3391       resources, and nested providers are not in play.
3392     In these scenarios, we can more efficiently build the list of
3393     AllocationRequest and ProviderSummary objects due to not having to
3394     determine requests across multiple providers.
3395 
3396     :param ctx: nova.context.RequestContext object
3397     :param requested_resources: dict, keyed by resource class ID, of amounts
3398                                 being requested for that resource class
3399     :param rp_tuples: List of two-tuples of (provider ID, root provider ID)s
3400                       for providers that matched the requested resources
3401     """
3402     if not rp_tuples:
3403         return [], []
3404 
3405     # Get all root resource provider IDs.
3406     root_ids = set(p[1] for p in rp_tuples)
3407 
3408     # Grab usage summaries for each provider
3409     usages = _get_usages_by_provider_tree(ctx, root_ids)
3410 
3411     # Get a dict, keyed by resource provider internal ID, of trait string names
3412     # that provider has associated with it
3413     prov_traits = _get_traits_by_provider_tree(ctx, root_ids)
3414 
3415     # Get a dict, keyed by resource provider internal ID, of ProviderSummary
3416     # objects for all providers
3417     summaries = _build_provider_summaries(ctx, usages, prov_traits)
3418 
3419     # Next, build up a list of allocation requests. These allocation requests
3420     # are AllocationRequest objects, containing resource provider UUIDs,
3421     # resource class names and amounts to consume from that resource provider
3422     alloc_requests = []
3423     for rp_id, root_id in rp_tuples:
3424         rp_summary = summaries[rp_id]
3425         req_obj = _allocation_request_for_provider(
3426                 ctx, requested_resources, rp_summary.resource_provider)
3427         alloc_requests.append(req_obj)
3428         # If this is a sharing provider, we have to include an extra
3429         # AllocationRequest for every possible anchor.
3430         traits = [trait.name for trait in rp_summary.traits]
3431         if os_traits.MISC_SHARES_VIA_AGGREGATE in traits:
3432             anchors = set([p[1] for p in _anchors_for_sharing_providers(
3433                 ctx, [rp_summary.resource_provider.id])])
3434             for anchor in anchors:
3435                 # We already added self
3436                 if anchor == rp_summary.resource_provider.root_provider_uuid:
3437                     continue
3438                 req_obj = copy.deepcopy(req_obj)
3439                 req_obj.anchor_root_provider_uuid = anchor
3440                 alloc_requests.append(req_obj)
3441     return alloc_requests, list(summaries.values())
3442 
3443 
3444 def _alloc_candidates_multiple_providers(ctx, requested_resources,
3445         required_traits, forbidden_traits, rp_tuples):
3446     """Returns a tuple of (allocation requests, provider summaries) for a
3447     supplied set of requested resource amounts and tuples of
3448     (rp_id, root_id, rc_id). The supplied resource provider trees have
3449     capacity to satisfy ALL of the resources in the requested resources as
3450     well as ALL required traits that were requested by the user.
3451 
3452     This is a code path to get results for a RequestGroup with
3453     use_same_provider=False. In this scenario, we are able to use multiple
3454     providers within the same provider tree including sharing providers to
3455     satisfy different resources involved in a single request group.
3456 
3457     :param ctx: nova.context.RequestContext object
3458     :param requested_resources: dict, keyed by resource class ID, of amounts
3459                                 being requested for that resource class
3460     :param required_traits: A map, keyed by trait string name, of required
3461                             trait internal IDs that each *allocation request's
3462                             set of providers* must *collectively* have
3463                             associated with them
3464     :param forbidden_traits: A map, keyed by trait string name, of trait
3465                              internal IDs that a resource provider must
3466                              not have.
3467     :param rp_tuples: List of tuples of (provider ID, anchor root provider ID,
3468                       resource class ID)s for providers that matched the
3469                       requested resources
3470     """
3471     if not rp_tuples:
3472         return [], []
3473 
3474     # Get all the root resource provider IDs. We should include the first
3475     # values of rp_tuples because while sharing providers are root providers,
3476     # they have their "anchor" providers for the second value.
3477     root_ids = set(p[0] for p in rp_tuples) | set(p[1] for p in rp_tuples)
3478 
3479     # Grab usage summaries for each provider in the trees
3480     usages = _get_usages_by_provider_tree(ctx, root_ids)
3481 
3482     # Get a dict, keyed by resource provider internal ID, of trait string names
3483     # that provider has associated with it
3484     prov_traits = _get_traits_by_provider_tree(ctx, root_ids)
3485 
3486     # Get a dict, keyed by resource provider internal ID, of ProviderSummary
3487     # objects for all providers
3488     summaries = _build_provider_summaries(ctx, usages, prov_traits)
3489 
3490     # Get a dict, keyed by root provider internal ID, of a dict, keyed by
3491     # resource class internal ID, of lists of AllocationRequestResource objects
3492     tree_dict = collections.defaultdict(lambda: collections.defaultdict(list))
3493 
3494     for rp_id, root_id, rc_id in rp_tuples:
3495         rp_summary = summaries[rp_id]
3496         rp_uuid = rp_summary.resource_provider.uuid
3497         tree_dict[root_id][rc_id].append(
3498             AllocationRequestResource(
3499                 ctx, resource_provider=ResourceProvider.get_by_uuid(ctx,
3500                                                                     rp_uuid),
3501                 resource_class=_RC_CACHE.string_from_id(rc_id),
3502                 amount=requested_resources[rc_id]))
3503 
3504     # Next, build up a list of allocation requests. These allocation requests
3505     # are AllocationRequest objects, containing resource provider UUIDs,
3506     # resource class names and amounts to consume from that resource provider
3507     alloc_requests = []
3508 
3509     # Build a list of lists of provider internal IDs that end up in
3510     # allocation request objects. This is used to ensure we don't end up
3511     # having allocation requests with duplicate sets of resource providers.
3512     alloc_prov_ids = []
3513 
3514     # Let's look into each tree
3515     for root_id, alloc_dict in tree_dict.items():
3516         # Get request_groups, which is a list of lists of
3517         # AllocationRequestResource per requested resource class.
3518         request_groups = alloc_dict.values()
3519 
3520         root_summary = summaries[root_id]
3521         root_uuid = root_summary.resource_provider.uuid
3522 
3523         # Using itertools.product, we get all the combinations of resource
3524         # providers in a tree.
3525         for res_requests in itertools.product(*request_groups):
3526             all_prov_ids = _check_traits_for_alloc_request(res_requests,
3527                 summaries, prov_traits, required_traits, forbidden_traits)
3528             if (not all_prov_ids) or (all_prov_ids in alloc_prov_ids):
3529                 # This combination doesn't satisfy trait constraints,
3530                 # ...or we already have this permutation, which happens
3531                 # when multiple sharing providers with different resource
3532                 # classes are in one request.
3533                 continue
3534             alloc_prov_ids.append(all_prov_ids)
3535             alloc_requests.append(
3536                 AllocationRequest(ctx, resource_requests=list(res_requests),
3537                                   anchor_root_provider_uuid=root_uuid)
3538             )
3539     return alloc_requests, list(summaries.values())
3540 
3541 
3542 @db_api.placement_context_manager.reader
3543 def _get_traits_by_provider_tree(ctx, root_ids):
3544     """Returns a dict, keyed by provider IDs for all resource providers
3545     in all trees indicated in the ``root_ids``, of string trait names
3546     associated with that provider.
3547 
3548     :raises: ValueError when root_ids is empty.
3549 
3550     :param ctx: nova.context.RequestContext object
3551     :param root_ids: list of root resource provider IDs
3552     """
3553     if not root_ids:
3554         raise ValueError(_("Expected root_ids to be a list of root resource"
3555                            "provider internal IDs, but got an empty list."))
3556 
3557     rpt = sa.alias(_RP_TBL, name='rpt')
3558     rptt = sa.alias(_RP_TRAIT_TBL, name='rptt')
3559     tt = sa.alias(_TRAIT_TBL, name='t')
3560     rpt_rptt = sa.join(rpt, rptt, rpt.c.id == rptt.c.resource_provider_id)
3561     j = sa.join(rpt_rptt, tt, rptt.c.trait_id == tt.c.id)
3562     sel = sa.select([rptt.c.resource_provider_id, tt.c.name]).select_from(j)
3563     sel = sel.where(rpt.c.root_provider_id.in_(root_ids))
3564     res = collections.defaultdict(list)
3565     for r in ctx.session.execute(sel):
3566         res[r[0]].append(r[1])
3567     return res
3568 
3569 
3570 @db_api.placement_context_manager.reader
3571 def _trait_ids_from_names(ctx, names):
3572     """Given a list of string trait names, returns a dict, keyed by those
3573     string names, of the corresponding internal integer trait ID.
3574 
3575     :raises: ValueError when names is empty.
3576 
3577     :param ctx: nova.context.RequestContext object
3578     :param names: list of string trait names
3579     """
3580     if not names:
3581         raise ValueError(_("Expected names to be a list of string trait "
3582                            "names, but got an empty list."))
3583 
3584     # Avoid SAWarnings about unicode types...
3585     unames = map(six.text_type, names)
3586     tt = sa.alias(_TRAIT_TBL, name='t')
3587     sel = sa.select([tt.c.name, tt.c.id]).where(tt.c.name.in_(unames))
3588     return {r[0]: r[1] for r in ctx.session.execute(sel)}
3589 
3590 
3591 def _rp_rc_key(rp, rc):
3592     """Creates hashable key unique to a provider + resource class."""
3593     return rp.uuid, rc
3594 
3595 
3596 def _consolidate_allocation_requests(areqs):
3597     """Consolidates a list of AllocationRequest into one.
3598 
3599     :param areqs: A list containing one AllocationRequest for each input
3600             RequestGroup.  This may mean that multiple resource_requests
3601             contain resource amounts of the same class from the same provider.
3602     :return: A single consolidated AllocationRequest, containing no
3603             resource_requests with duplicated (resource_provider,
3604             resource_class).
3605     """
3606     # Construct a dict, keyed by resource provider UUID + resource class, of
3607     # AllocationRequestResource, consolidating as we go.
3608     arrs_by_rp_rc = {}
3609     # areqs must have at least one element.  Save the anchor to populate the
3610     # returned AllocationRequest.
3611     anchor_rp_uuid = areqs[0].anchor_root_provider_uuid
3612     for areq in areqs:
3613         # Sanity check: the anchor should be the same for every areq
3614         if anchor_rp_uuid != areq.anchor_root_provider_uuid:
3615             # This should never happen.  If it does, it's a dev bug.
3616             raise ValueError(
3617                 _("Expected every AllocationRequest in "
3618                   "`_consolidate_allocation_requests` to have the same "
3619                   "anchor!"))
3620         for arr in areq.resource_requests:
3621             key = _rp_rc_key(arr.resource_provider, arr.resource_class)
3622             if key not in arrs_by_rp_rc:
3623                 arrs_by_rp_rc[key] = copy.deepcopy(arr)
3624             else:
3625                 arrs_by_rp_rc[key].amount += arr.amount
3626     return AllocationRequest(
3627         resource_requests=list(arrs_by_rp_rc.values()),
3628         anchor_root_provider_uuid=anchor_rp_uuid)
3629 
3630 
3631 def _satisfies_group_policy(areqs, group_policy, num_granular_groups):
3632     """Applies group_policy to a list of AllocationRequest.
3633 
3634     Returns True or False, indicating whether this list of
3635     AllocationRequest satisfies group_policy, as follows:
3636 
3637     * "isolate": Each AllocationRequest with use_same_provider=True
3638                  is satisfied by a single resource provider.  If the "isolate"
3639                  policy is in effect, each such AllocationRequest must be
3640                  satisfied by a *unique* resource provider.
3641     * "none" or None: Always returns True.
3642 
3643     :param areqs: A list containing one AllocationRequest for each input
3644             RequestGroup.
3645     :param group_policy: String indicating how RequestGroups should interact
3646             with each other.  If the value is "isolate", we will return False
3647             if AllocationRequests that came from RequestGroups keyed by
3648             nonempty suffixes are satisfied by the same provider.
3649     :param num_granular_groups: The number of granular (use_same_provider=True)
3650             RequestGroups in the request.
3651     :return: True if areqs satisfies group_policy; False otherwise.
3652     """
3653     if group_policy != 'isolate':
3654         # group_policy="none" means no filtering
3655         return True
3656 
3657     # The number of unique resource providers referenced in the request groups
3658     # having use_same_provider=True must be equal to the number of granular
3659     # groups.
3660     num_granular_groups_in_areqs = len(set(
3661         # We can reliably use the first resource_request's provider: all the
3662         # resource_requests are satisfied by the same provider by definition
3663         # because use_same_provider is True.
3664         areq.resource_requests[0].resource_provider.uuid
3665         for areq in areqs
3666         if areq.use_same_provider))
3667     if num_granular_groups == num_granular_groups_in_areqs:
3668         return True
3669     LOG.debug('Excluding the following set of AllocationRequest because '
3670               'group_policy=isolate and the number of granular groups in the '
3671               'set (%d) does not match the number of granular groups in the '
3672               'request (%d): %s',
3673               num_granular_groups_in_areqs, num_granular_groups, str(areqs))
3674     return False
3675 
3676 
3677 def _exceeds_capacity(areq, psum_res_by_rp_rc):
3678     """Checks a (consolidated) AllocationRequest against the provider summaries
3679     to ensure that it does not exceed capacity.
3680 
3681     Exceeding capacity can mean the total amount (already used plus this
3682     allocation) exceeds the total inventory amount; or this allocation exceeds
3683     the max_unit in the inventory record.
3684 
3685     :param areq: An AllocationRequest produced by the
3686             `_consolidate_allocation_requests` method.
3687     :param psum_res_by_rp_rc: A dict, keyed by provider + resource class via
3688             _rp_rc_key, of ProviderSummaryResource.
3689     :return: True if areq exceeds capacity; False otherwise.
3690     """
3691     for arr in areq.resource_requests:
3692         key = _rp_rc_key(arr.resource_provider, arr.resource_class)
3693         psum_res = psum_res_by_rp_rc[key]
3694         if psum_res.used + arr.amount > psum_res.capacity:
3695             LOG.debug('Excluding the following AllocationRequest because used '
3696                       '(%d) + amount (%d) > capacity (%d) for resource class '
3697                       '%s: %s',
3698                       psum_res.used, arr.amount, psum_res.capacity,
3699                       arr.resource_class, str(areq))
3700             return True
3701         if arr.amount > psum_res.max_unit:
3702             LOG.debug('Excluding the following AllocationRequest because '
3703                       'amount (%d) > max_unit (%d) for resource class %s: %s',
3704                       arr.amount, psum_res.max_unit, arr.resource_class,
3705                       str(areq))
3706             return True
3707     return False
3708 
3709 
3710 def _merge_candidates(candidates, group_policy=None):
3711     """Given a dict, keyed by RequestGroup suffix, of tuples of
3712     (allocation_requests, provider_summaries), produce a single tuple of
3713     (allocation_requests, provider_summaries) that appropriately incorporates
3714     the elements from each.
3715 
3716     Each (alloc_reqs, prov_sums) in `candidates` satisfies one RequestGroup.
3717     This method creates a list of alloc_reqs, *each* of which satisfies *all*
3718     of the RequestGroups.
3719 
3720     For that merged list of alloc_reqs, a corresponding provider_summaries is
3721     produced.
3722 
3723     :param candidates: A dict, keyed by integer suffix or '', of tuples of
3724             (allocation_requests, provider_summaries) to be merged.
3725     :param group_policy: String indicating how RequestGroups should interact
3726             with each other.  If the value is "isolate", we will filter out
3727             candidates where AllocationRequests that came from RequestGroups
3728             keyed by nonempty suffixes are satisfied by the same provider.
3729     :return: A tuple of (allocation_requests, provider_summaries).
3730     """
3731     # Build a dict, keyed by anchor root provider UUID, of dicts, keyed by
3732     # suffix, of nonempty lists of AllocationRequest.  Each inner dict must
3733     # possess all of the suffix keys to be viable (i.e. contains at least
3734     # one AllocationRequest per RequestGroup).
3735     #
3736     # areq_lists_by_anchor =
3737     #   { anchor_root_provider_uuid: {
3738     #         '': [AllocationRequest, ...],   \  This dict must contain
3739     #         '1': [AllocationRequest, ...],   \ exactly one nonempty list per
3740     #         ...                              / suffix to be viable. That
3741     #         '42': [AllocationRequest, ...], /  filtering is done later.
3742     #     },
3743     #     ...
3744     #   }
3745     areq_lists_by_anchor = collections.defaultdict(
3746             lambda: collections.defaultdict(list))
3747     # Save off all the provider summaries lists - we'll use 'em later.
3748     all_psums = []
3749     # Construct a dict, keyed by resource provider + resource class, of
3750     # ProviderSummaryResource.  This will be used to do a final capacity
3751     # check/filter on each merged AllocationRequest.
3752     psum_res_by_rp_rc = {}
3753     for suffix, (areqs, psums) in candidates.items():
3754         for areq in areqs:
3755             anchor = areq.anchor_root_provider_uuid
3756             areq_lists_by_anchor[anchor][suffix].append(areq)
3757         for psum in psums:
3758             all_psums.append(psum)
3759             for psum_res in psum.resources:
3760                 key = _rp_rc_key(
3761                         psum.resource_provider, psum_res.resource_class)
3762                 psum_res_by_rp_rc[key] = psum_res
3763 
3764     # Create all combinations picking one AllocationRequest from each list
3765     # for each anchor.
3766     areqs = []
3767     all_suffixes = set(candidates)
3768     num_granular_groups = len(all_suffixes - set(['']))
3769     for areq_lists_by_suffix in areq_lists_by_anchor.values():
3770         # Filter out any entries that don't have allocation requests for
3771         # *all* suffixes (i.e. all RequestGroups)
3772         if set(areq_lists_by_suffix) != all_suffixes:
3773             continue
3774         # We're using itertools.product to go from this:
3775         # areq_lists_by_suffix = {
3776         #     '':   [areq__A,   areq__B,   ...],
3777         #     '1':  [areq_1_A,  areq_1_B,  ...],
3778         #     ...
3779         #     '42': [areq_42_A, areq_42_B, ...],
3780         # }
3781         # to this:
3782         # [ [areq__A, areq_1_A, ..., areq_42_A],  Each of these lists is one
3783         #   [areq__A, areq_1_A, ..., areq_42_B],  areq_list in the loop below.
3784         #   [areq__A, areq_1_B, ..., areq_42_A],  each areq_list contains one
3785         #   [areq__A, areq_1_B, ..., areq_42_B],  AllocationRequest from each
3786         #   [areq__B, areq_1_A, ..., areq_42_A],  RequestGroup. So taken as a
3787         #   [areq__B, areq_1_A, ..., areq_42_B],  whole, each list is a viable
3788         #   [areq__B, areq_1_B, ..., areq_42_A],  (preliminary) candidate to
3789         #   [areq__B, areq_1_B, ..., areq_42_B],  return.
3790         #   ...,
3791         # ]
3792         for areq_list in itertools.product(
3793                 *list(areq_lists_by_suffix.values())):
3794             # At this point, each AllocationRequest in areq_list is still
3795             # marked as use_same_provider. This is necessary to filter by group
3796             # policy, which enforces how these interact with each other.
3797             if not _satisfies_group_policy(
3798                     areq_list, group_policy, num_granular_groups):
3799                 continue
3800             # Now we go from this (where 'arr' is AllocationRequestResource):
3801             # [ areq__B(arrX, arrY, arrZ),
3802             #   areq_1_A(arrM, arrN),
3803             #   ...,
3804             #   areq_42_B(arrQ)
3805             # ]
3806             # to this:
3807             # areq_combined(arrX, arrY, arrZ, arrM, arrN, arrQ)
3808             # Note that this discards the information telling us which
3809             # RequestGroup led to which piece of the final AllocationRequest.
3810             # We needed that to be present for the previous filter; we need it
3811             # to be *absent* for the next one (and for the final output).
3812             areq = _consolidate_allocation_requests(areq_list)
3813             # Since we sourced this AllocationRequest from multiple
3814             # *independent* queries, it's possible that the combined result
3815             # now exceeds capacity where amounts of the same RP+RC were
3816             # folded together.  So do a final capacity check/filter.
3817             if _exceeds_capacity(areq, psum_res_by_rp_rc):
3818                 continue
3819             areqs.append(areq)
3820 
3821     # It's possible we've filtered out everything.  If so, short out.
3822     if not areqs:
3823         return [], []
3824 
3825     # Now we have to produce provider summaries.  The provider summaries in
3826     # the candidates input contain all the information; we just need to
3827     # filter it down to only the providers in trees represented by our merged
3828     # list of allocation requests.
3829     tree_uuids = set()
3830     for areq in areqs:
3831         for arr in areq.resource_requests:
3832             tree_uuids.add(arr.resource_provider.root_provider_uuid)
3833     psums = [psum for psum in all_psums if
3834              psum.resource_provider.root_provider_uuid in tree_uuids]
3835 
3836     return areqs, psums
3837 
3838 
3839 @base.VersionedObjectRegistry.register_if(False)
3840 class AllocationCandidates(base.VersionedObject):
3841     """The AllocationCandidates object is a collection of possible allocations
3842     that match some request for resources, along with some summary information
3843     about the resource providers involved in these allocation candidates.
3844     """
3845 
3846     fields = {
3847         # A collection of allocation possibilities that can be attempted by the
3848         # caller that would, at the time of calling, meet the requested
3849         # resource constraints
3850         'allocation_requests': fields.ListOfObjectsField('AllocationRequest'),
3851         # Information about usage and inventory that relate to any provider
3852         # contained in any of the AllocationRequest objects in the
3853         # allocation_requests field
3854         'provider_summaries': fields.ListOfObjectsField('ProviderSummary'),
3855     }
3856 
3857     @classmethod
3858     def get_by_requests(cls, context, requests, limit=None, group_policy=None):
3859         """Returns an AllocationCandidates object containing all resource
3860         providers matching a set of supplied resource constraints, with a set
3861         of allocation requests constructed from that list of resource
3862         providers. If CONF.placement.randomize_allocation_candidates is True
3863         (default is False) then the order of the allocation requests will
3864         be randomized.
3865 
3866         :param context: Nova RequestContext.
3867         :param requests: Dict, keyed by suffix, of
3868                          nova.api.openstack.placement.util.RequestGroup
3869         :param limit: An integer, N, representing the maximum number of
3870                       allocation candidates to return. If
3871                       CONF.placement.randomize_allocation_candidates is True
3872                       this will be a random sampling of N of the available
3873                       results. If False then the first N results, in whatever
3874                       order the database picked them, will be returned. In
3875                       either case if there are fewer than N total results,
3876                       all the results will be returned.
3877         :param group_policy: String indicating how RequestGroups with
3878                              use_same_provider=True should interact with each
3879                              other.  If the value is "isolate", we will filter
3880                              out allocation requests where any such
3881                              RequestGroups are satisfied by the same RP.
3882         :return: An instance of AllocationCandidates with allocation_requests
3883                  and provider_summaries satisfying `requests`, limited
3884                  according to `limit`.
3885         """
3886         _ensure_rc_cache(context)
3887         alloc_reqs, provider_summaries = cls._get_by_requests(
3888             context, requests, limit=limit, group_policy=group_policy)
3889         return cls(
3890             context,
3891             allocation_requests=alloc_reqs,
3892             provider_summaries=provider_summaries,
3893         )
3894 
3895     @staticmethod
3896     def _get_by_one_request(context, request):
3897         """Get allocation candidates for one RequestGroup.
3898 
3899         Must be called from within an placement_context_manager.reader
3900         (or writer) context.
3901 
3902         :param context: Nova RequestContext.
3903         :param request: One nova.api.openstack.placement.util.RequestGroup
3904         :return: A tuple of (allocation_requests, provider_summaries)
3905                  satisfying `request`.
3906         """
3907         # Transform resource string names to internal integer IDs
3908         resources = {
3909             _RC_CACHE.id_from_string(key): value
3910             for key, value in request.resources.items()
3911         }
3912 
3913         # maps the trait name to the trait internal ID
3914         required_trait_map = {}
3915         forbidden_trait_map = {}
3916         for trait_map, traits in (
3917                 (required_trait_map, request.required_traits),
3918                 (forbidden_trait_map, request.forbidden_traits)):
3919             if traits:
3920                 trait_map.update(_trait_ids_from_names(context, traits))
3921                 # Double-check that we found a trait ID for each requested name
3922                 if len(trait_map) != len(traits):
3923                     missing = traits - set(trait_map)
3924                     raise exception.TraitNotFound(names=', '.join(missing))
3925 
3926         # Microversions prior to 1.21 will not have 'member_of' in the groups.
3927         # This allows earlier microversions to continue to work.
3928         member_of = getattr(request, "member_of", None)
3929 
3930         if not request.use_same_provider:
3931             # TODO(jaypipes): The check/callout to handle trees goes here.
3932             # Build a dict, keyed by resource class internal ID, of lists of
3933             # internal IDs of resource providers that share some inventory for
3934             # each resource class requested.
3935             # TODO(jaypipes): Consider caching this for some amount of time
3936             # since sharing providers generally don't change often and here we
3937             # aren't concerned with how *much* inventory/capacity the sharing
3938             # provider has, only that it is sharing *some* inventory of a
3939             # particular resource class.
3940             sharing_providers = {
3941                 rc_id: _get_providers_with_shared_capacity(context, rc_id,
3942                                                            amount, member_of)
3943                 for rc_id, amount in resources.items()
3944             }
3945 
3946             # If there aren't any providers that have any of the
3947             # required traits, just exit early...
3948             if required_trait_map:
3949                 # TODO(cdent): Now that there is also a forbidden_trait_map
3950                 # it should be possible to further optimize this attempt at
3951                 # a quick return, but we leave that to future patches for
3952                 # now.
3953                 trait_rps = _get_provider_ids_having_any_trait(
3954                     context, required_trait_map)
3955                 if not trait_rps:
3956                     return [], []
3957             rp_tuples = _get_trees_matching_all(context, resources,
3958                 required_trait_map, forbidden_trait_map,
3959                 sharing_providers, member_of)
3960             return _alloc_candidates_multiple_providers(context, resources,
3961                 required_trait_map, forbidden_trait_map, rp_tuples)
3962 
3963         # Either we are processing a single-RP request group, or there are no
3964         # sharing providers that (help) satisfy the request.  Get a list of
3965         # resource provider IDs that have ALL the requested resources and more
3966         # efficiently construct the allocation requests.
3967         # NOTE(jaypipes): When we start handling nested providers, we may
3968         # add new code paths or modify this code path to return root
3969         # provider IDs of provider trees instead of the resource provider
3970         # IDs.
3971         rp_ids = _get_provider_ids_matching(context, resources,
3972                                             required_trait_map,
3973                                             forbidden_trait_map, member_of)
3974         return _alloc_candidates_single_provider(context, resources, rp_ids)
3975 
3976     @classmethod
3977     # TODO(efried): This is only a writer context because it accesses the
3978     # resource_providers table via ResourceProvider.get_by_uuid, which does
3979     # data migration to populate the root_provider_uuid.  Change this back to a
3980     # reader when that migration is no longer happening.
3981     @db_api.placement_context_manager.writer
3982     def _get_by_requests(cls, context, requests, limit=None,
3983                          group_policy=None):
3984         candidates = {}
3985         for suffix, request in requests.items():
3986             alloc_reqs, summaries = cls._get_by_one_request(context, request)
3987             if not alloc_reqs:
3988                 # Shortcut: If any one request resulted in no candidates, the
3989                 # whole operation is shot.
3990                 return [], []
3991             # Mark each allocation request according to whether its
3992             # corresponding RequestGroup required it to be restricted to a
3993             # single provider.  We'll need this later to evaluate group_policy.
3994             for areq in alloc_reqs:
3995                 areq.use_same_provider = request.use_same_provider
3996             candidates[suffix] = alloc_reqs, summaries
3997 
3998         # At this point, each (alloc_requests, summary_obj) in `candidates` is
3999         # independent of the others. We need to fold them together such that
4000         # each allocation request satisfies *all* the incoming `requests`.  The
4001         # `candidates` dict is guaranteed to contain entries for all suffixes,
4002         # or we would have short-circuited above.
4003         alloc_request_objs, summary_objs = _merge_candidates(
4004                 candidates, group_policy=group_policy)
4005 
4006         # Limit the number of allocation request objects. We do this after
4007         # creating all of them so that we can do a random slice without
4008         # needing to mess with the complex sql above or add additional
4009         # columns to the DB.
4010         if limit and limit <= len(alloc_request_objs):
4011             if CONF.placement.randomize_allocation_candidates:
4012                 alloc_request_objs = random.sample(alloc_request_objs, limit)
4013             else:
4014                 alloc_request_objs = alloc_request_objs[:limit]
4015         elif CONF.placement.randomize_allocation_candidates:
4016             random.shuffle(alloc_request_objs)
4017 
4018         # Limit summaries to only those mentioned in the allocation requests.
4019         if limit and limit <= len(alloc_request_objs):
4020             kept_summary_objs = []
4021             alloc_req_rp_uuids = set()
4022             # Extract resource provider uuids from the resource requests.
4023             for aro in alloc_request_objs:
4024                 for arr in aro.resource_requests:
4025                     alloc_req_rp_uuids.add(arr.resource_provider.uuid)
4026             for summary in summary_objs:
4027                 rp_uuid = summary.resource_provider.uuid
4028                 # Skip a summary if we are limiting and haven't selected an
4029                 # allocation request that uses the resource provider.
4030                 if rp_uuid not in alloc_req_rp_uuids:
4031                     continue
4032                 kept_summary_objs.append(summary)
4033         else:
4034             kept_summary_objs = summary_objs
4035 
4036         return alloc_request_objs, kept_summary_objs
4037 
4038 
4039 @db_api.placement_context_manager.writer
4040 def reshape(ctx, inventories, allocations):
4041     """The 'replace the world' strategy that is executed when we want to
4042     completely replace a set of provider, inventory, allocation and consumer
4043     information in a single transaction.
4044 
4045     :note: The reason this has to be done in a single monolithic function is so
4046            we have a single top-level function on which to decorate with the
4047            @db_api.placement_context_manager.writer transaction context
4048            manager. Each time a top-level function that is decorated with this
4049            exits, the transaction is either COMMIT'd or ROLLBACK'd. We need to
4050            avoid calling two functions that are already decorated with a
4051            transaction context manager from a function that *isn't* decorated
4052            with the transaction context manager if we want all changes involved
4053            in the sub-functions to operate within a single DB transaction.
4054 
4055     :param ctx: `nova.api.openstack.placement.context.RequestContext` object
4056                 containing the DB transaction context.
4057     :param inventories: dict, keyed by resource provider UUID, of
4058                         `InventoryList` objects representing the replaced
4059                         inventory information for the provider.
4060     :param allocations: optional `AllocationList` object containing all
4061                         allocations for all consumers being modified by the
4062                         reshape operatiom
4063     :raises: `exception.ConcurrentUpdateDetected` when any resource provider or
4064              consumer generation increment fails due to concurrent changes to
4065              the same objects.
4066     """
4067     # The resource provider objects, keyed by provider UUID, that are involved
4068     # in this transaction. We keep a cache of these because as we perform the
4069     # various operations on the providers, their generations increment
4070     affected_providers = {}
4071     for rp_uuid, inv_list in inventories.items():
4072         LOG.debug("reshaping: attempting inventory replacement for "
4073                   "provider %s", rp_uuid)
4074         # Before attempting to reshape any inventory, we first need to clear
4075         # allocations on the resource providers in question. The reason for
4076         # this is that we will get InventoryInUse exceptions below when we try
4077         # to update the provider's inventory records.
4078         rp = inv_list[0].resource_provider
4079         try:
4080             rp_allocs = AllocationList.get_all_by_resource_provider(ctx, rp)
4081             rp_allocs.delete_all()
4082             affected_providers[rp_uuid] = rp
4083             LOG.info("reshaping: removed all allocations for "
4084                      "provider %s", rp_uuid)
4085         except Exception as err:
4086             LOG.warning("reshaping: failed to delete existing allocations for "
4087                         "provider %s. Got error: %s", rp_uuid, err)
4088             raise
4089 
4090         try:
4091             rp.set_inventory(inv_list)
4092             affected_providers[rp_uuid] = rp
4093             LOG.info("reshaping: completed inventory replacement for "
4094                      "provider %s", rp_uuid)
4095         except Exception as err:
4096             LOG.warning("reshaping: failed to replace inventory for "
4097                         "provider %s. Got error: %s", rp_uuid, err)
4098             raise
4099 
4100     if allocations:
4101         # NOTE(jaypipes): The above inventory replacements will have
4102         # incremented the resource provider generations, so we need to look in
4103         # the AllocationList and update any resource provider generation for
4104         # that provider.
4105         for alloc in allocations:
4106             rp_uuid = alloc.resource_provider.uuid
4107             if rp_uuid in affected_providers:
4108                 alloc.resource_provider = affected_providers[rp_uuid]
4109 
4110         # Note that because we're now automatically deleting consumers that
4111         # have no allocations, and we deleted all the consumer's allocations in
4112         # the call above to AllocationList.delete_all(), that means we now have
4113         # to recreate the consumers because when we call
4114         # AllocationList.replace_all() below, we'll get a
4115         # ConcurrentUpdateDetected due to the Consumer.increment_generation()
4116         # failing because there's no more consumer record.
4117         consumers = {}
4118         for alloc in allocations:
4119             if alloc.consumer.uuid not in consumers:
4120                 consumers[alloc.consumer.uuid] = alloc.consumer
4121 
4122         # Create all the original consumer records that were auto-deleted
4123         # earlier. Make sure they don't use the supplied consumer generation,
4124         # since we now have to start over with a new consumer record.
4125         for consumer in consumers.values():
4126             new_consumer = consumer_obj.Consumer(
4127                 ctx, uuid=consumer.uuid,
4128                 project=consumer.project, user=consumer.user)
4129             new_consumer.create()
4130             # And replace the AllocationList's consumers with the ones we just
4131             # recreated
4132             for alloc in allocations:
4133                 if alloc.consumer.uuid == consumer.uuid:
4134                     alloc.consumer = new_consumer
4135 
4136         allocations.replace_all()
