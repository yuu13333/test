Based on the given code from a commit, please generate supplementary code files according to the commit message.
####commit message
libvirt: add IP address to libvirt guest metadata

Libvirt XML contains useful configuration information such as instance names,
flavors and images as metadata. This change extends this metadata to include
the IP addresses of the instances.

Example:
<metadata>
  <nova:instance xmlns:nova="http://openstack.org/xmlns/libvirt/nova/1.1">
    ...
    <nova:ports>
      <nova:port uuid="567a4527-b0e4-4d0a-bcc2-71fda37897f7">
        <nova:ip type="fixed" address="192.168.1.1" ipVersion="4"/>
        <nova:ip type="fixed" address="fe80::f95c:b030:7094" ipVersion="6"/>
        <nova:ip type="floating" address="11.22.33.44" ipVersion="4"/>
      </nova:port>
    </nova:ports>
    ...
  </nova:instance>
</metadata>

Change-Id: I45f1df4935905170957c2ea2496c8a698a7464a2
blueprint: libvirt-driver-ip-metadata
Signed-off-by: Nobuhiro MIKI <nmiki@yahoo-corp.jp>

####code 
1 # Copyright 2010 United States Government as represented by the
2 # Administrator of the National Aeronautics and Space Administration.
3 # All Rights Reserved.
4 # Copyright (c) 2010 Citrix Systems, Inc.
5 # Copyright (c) 2011 Piston Cloud Computing, Inc
6 # Copyright (c) 2012 University Of Minho
7 # (c) Copyright 2013 Hewlett-Packard Development Company, L.P.
8 #
9 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
10 #    not use this file except in compliance with the License. You may obtain
11 #    a copy of the License at
12 #
13 #         http://www.apache.org/licenses/LICENSE-2.0
14 #
15 #    Unless required by applicable law or agreed to in writing, software
16 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
17 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
18 #    License for the specific language governing permissions and limitations
19 #    under the License.
20 
21 """
22 A connection to a hypervisor through libvirt.
23 
24 Supports KVM, LXC, QEMU, UML, XEN and Parallels.
25 
26 """
27 
28 import binascii
29 import collections
30 from collections import deque
31 import contextlib
32 import copy
33 import errno
34 import functools
35 import glob
36 import grp
37 import itertools
38 import operator
39 import os
40 import pwd
41 import random
42 import shutil
43 import sys
44 import tempfile
45 import time
46 import typing as ty
47 import uuid
48 
49 from castellan import key_manager
50 from copy import deepcopy
51 import eventlet
52 from eventlet import greenthread
53 from eventlet import tpool
54 from lxml import etree
55 from os_brick import encryptors
56 from os_brick.encryptors import luks as luks_encryptor
57 from os_brick import exception as brick_exception
58 from os_brick.initiator import connector
59 import os_resource_classes as orc
60 import os_traits as ot
61 from oslo_concurrency import processutils
62 from oslo_log import log as logging
63 from oslo_serialization import base64
64 from oslo_serialization import jsonutils
65 from oslo_service import loopingcall
66 from oslo_utils import encodeutils
67 from oslo_utils import excutils
68 from oslo_utils import fileutils
69 from oslo_utils import importutils
70 from oslo_utils import netutils as oslo_netutils
71 from oslo_utils import strutils
72 from oslo_utils import timeutils
73 from oslo_utils import units
74 from oslo_utils import uuidutils
75 
76 from nova.api.metadata import base as instance_metadata
77 from nova.api.metadata import password
78 from nova import block_device
79 from nova.compute import power_state
80 from nova.compute import provider_tree
81 from nova.compute import task_states
82 from nova.compute import utils as compute_utils
83 from nova.compute import vm_states
84 import nova.conf
85 from nova.console import serial as serial_console
86 from nova.console import type as ctype
87 from nova import context as nova_context
88 from nova import crypto
89 from nova.db import constants as db_const
90 from nova import exception
91 from nova.i18n import _
92 from nova.image import glance
93 from nova.network import model as network_model
94 from nova import objects
95 from nova.objects import diagnostics as diagnostics_obj
96 from nova.objects import fields
97 from nova.pci import manager as pci_manager
98 from nova.pci import utils as pci_utils
99 import nova.privsep.libvirt
100 import nova.privsep.path
101 import nova.privsep.utils
102 from nova.storage import rbd_utils
103 from nova import utils
104 from nova import version
105 from nova.virt import arch
106 from nova.virt import block_device as driver_block_device
107 from nova.virt import configdrive
108 from nova.virt.disk import api as disk_api
109 from nova.virt.disk.vfs import guestfs
110 from nova.virt import driver
111 from nova.virt import hardware
112 from nova.virt.image import model as imgmodel
113 from nova.virt import images
114 from nova.virt.libvirt import blockinfo
115 from nova.virt.libvirt import config as vconfig
116 from nova.virt.libvirt import designer
117 from nova.virt.libvirt import guest as libvirt_guest
118 from nova.virt.libvirt import host
119 from nova.virt.libvirt import imagebackend
120 from nova.virt.libvirt import imagecache
121 from nova.virt.libvirt import instancejobtracker
122 from nova.virt.libvirt import migration as libvirt_migrate
123 from nova.virt.libvirt.storage import dmcrypt
124 from nova.virt.libvirt.storage import lvm
125 from nova.virt.libvirt import utils as libvirt_utils
126 from nova.virt.libvirt import vif as libvirt_vif
127 from nova.virt.libvirt.volume import fs
128 from nova.virt.libvirt.volume import mount
129 from nova.virt.libvirt.volume import remotefs
130 from nova.virt.libvirt.volume import volume
131 from nova.virt import netutils
132 from nova.volume import cinder
133 
134 libvirt: ty.Any = None
135 
136 uefi_logged = False
137 
138 LOG = logging.getLogger(__name__)
139 
140 CONF = nova.conf.CONF
141 
142 DEFAULT_UEFI_LOADER_PATH = {
143     "x86_64": ['/usr/share/OVMF/OVMF_CODE.fd',
144                '/usr/share/OVMF/OVMF_CODE.secboot.fd',
145                '/usr/share/qemu/ovmf-x86_64-code.bin'],
146     "aarch64": ['/usr/share/AAVMF/AAVMF_CODE.fd',
147                 '/usr/share/qemu/aavmf-aarch64-code.bin']
148 }
149 
150 MAX_CONSOLE_BYTES = 100 * units.Ki
151 VALID_DISK_CACHEMODES = [
152     "default", "none", "writethrough", "writeback", "directsync", "unsafe",
153 ]
154 
155 # The libvirt driver will prefix any disable reason codes with this string.
156 DISABLE_PREFIX = 'AUTO: '
157 # Disable reason for the service which was enabled or disabled without reason
158 DISABLE_REASON_UNDEFINED = None
159 
160 # Guest config console string
161 CONSOLE = "console=tty0 console=ttyS0 console=hvc0"
162 
163 GuestNumaConfig = collections.namedtuple(
164     'GuestNumaConfig', ['cpuset', 'cputune', 'numaconfig', 'numatune'])
165 
166 
167 class InjectionInfo(collections.namedtuple(
168         'InjectionInfo', ['network_info', 'files', 'admin_pass'])):
169     __slots__ = ()
170 
171     def __repr__(self):
172         return ('InjectionInfo(network_info=%r, files=%r, '
173                 'admin_pass=<SANITIZED>)') % (self.network_info, self.files)
174 
175 
176 # NOTE(lyarwood): Dict of volume drivers supported by the libvirt driver, keyed
177 # by the connection_info['driver_volume_type'] returned by Cinder for each
178 # volume type it supports
179 # TODO(lyarwood): Add host configurables to allow this list to be changed.
180 # Allowing native iSCSI to be reintroduced etc.
181 VOLUME_DRIVERS = {
182     'iscsi': 'nova.virt.libvirt.volume.iscsi.LibvirtISCSIVolumeDriver',
183     'iser': 'nova.virt.libvirt.volume.iser.LibvirtISERVolumeDriver',
184     'local': 'nova.virt.libvirt.volume.volume.LibvirtVolumeDriver',
185     'fake': 'nova.virt.libvirt.volume.volume.LibvirtFakeVolumeDriver',
186     'rbd': 'nova.virt.libvirt.volume.net.LibvirtNetVolumeDriver',
187     'nfs': 'nova.virt.libvirt.volume.nfs.LibvirtNFSVolumeDriver',
188     'smbfs': 'nova.virt.libvirt.volume.smbfs.LibvirtSMBFSVolumeDriver',
189     'fibre_channel': 'nova.virt.libvirt.volume.fibrechannel.LibvirtFibreChannelVolumeDriver',  # noqa:E501
190     'gpfs': 'nova.virt.libvirt.volume.gpfs.LibvirtGPFSVolumeDriver',
191     'quobyte': 'nova.virt.libvirt.volume.quobyte.LibvirtQuobyteVolumeDriver',
192     'scaleio': 'nova.virt.libvirt.volume.scaleio.LibvirtScaleIOVolumeDriver',
193     'vzstorage': 'nova.virt.libvirt.volume.vzstorage.LibvirtVZStorageVolumeDriver',  # noqa:E501
194     'storpool': 'nova.virt.libvirt.volume.storpool.LibvirtStorPoolVolumeDriver',  # noqa:E501
195     'nvmeof': 'nova.virt.libvirt.volume.nvme.LibvirtNVMEVolumeDriver',
196 }
197 
198 
199 def patch_tpool_proxy():
200     """eventlet.tpool.Proxy doesn't work with old-style class in __str__()
201     or __repr__() calls. See bug #962840 for details.
202     We perform a monkey patch to replace those two instance methods.
203     """
204     def str_method(self):
205         return str(self._obj)
206 
207     def repr_method(self):
208         return repr(self._obj)
209 
210     tpool.Proxy.__str__ = str_method
211     tpool.Proxy.__repr__ = repr_method
212 
213 
214 patch_tpool_proxy()
215 
216 # For information about when MIN_LIBVIRT_VERSION and
217 # NEXT_MIN_LIBVIRT_VERSION can be changed, consult
218 #
219 #   https://wiki.openstack.org/wiki/LibvirtDistroSupportMatrix
220 #
221 # Currently this is effectively the min version for i686/x86_64
222 # + KVM/QEMU, as other architectures/hypervisors require newer
223 # versions. Over time, this will become a common min version
224 # for all architectures/hypervisors, as this value rises to
225 # meet them.
226 MIN_LIBVIRT_VERSION = (5, 0, 0)
227 MIN_QEMU_VERSION = (4, 0, 0)
228 # TODO(berrange): Re-evaluate this at start of each release cycle
229 # to decide if we want to plan a future min version bump.
230 # MIN_LIBVIRT_VERSION can be updated to match this after
231 # NEXT_MIN_LIBVIRT_VERSION  has been at a higher value for
232 # one cycle
233 NEXT_MIN_LIBVIRT_VERSION = (6, 0, 0)
234 NEXT_MIN_QEMU_VERSION = (4, 2, 0)
235 
236 # Virtuozzo driver support
237 MIN_VIRTUOZZO_VERSION = (7, 0, 0)
238 
239 
240 # Names of the types that do not get compressed during migration
241 NO_COMPRESSION_TYPES = ('qcow2',)
242 
243 
244 # number of serial console limit
245 QEMU_MAX_SERIAL_PORTS = 4
246 # Qemu supports 4 serial consoles, we remove 1 because of the PTY one defined
247 ALLOWED_QEMU_SERIAL_PORTS = QEMU_MAX_SERIAL_PORTS - 1
248 
249 VGPU_RESOURCE_SEMAPHORE = 'vgpu_resources'
250 
251 LIBVIRT_PERF_EVENT_PREFIX = 'VIR_PERF_PARAM_'
252 
253 # -blockdev support (replacing -drive)
254 MIN_LIBVIRT_BLOCKDEV = (6, 0, 0)
255 MIN_QEMU_BLOCKDEV = (4, 2, 0)
256 
257 # Virtual TPM (vTPM) support
258 MIN_LIBVIRT_VTPM = (5, 6, 0)
259 
260 MIN_LIBVIRT_S390X_CPU_COMPARE = (5, 9, 0)
261 
262 
263 class LibvirtDriver(driver.ComputeDriver):
264     def __init__(self, virtapi, read_only=False):
265         # NOTE(aspiers) Some of these are dynamic, so putting
266         # capabilities on the instance rather than on the class.
267         # This prevents the risk of one test setting a capability
268         # which bleeds over into other tests.
269 
270         # LVM and RBD require raw images. If we are not configured to
271         # force convert images into raw format, then we _require_ raw
272         # images only.
273         raw_only = ('rbd', 'lvm')
274         requires_raw_image = (CONF.libvirt.images_type in raw_only and
275                               not CONF.force_raw_images)
276         requires_ploop_image = CONF.libvirt.virt_type == 'parallels'
277 
278         self.capabilities = {
279             "has_imagecache": True,
280             "supports_evacuate": True,
281             "supports_migrate_to_same_host": False,
282             "supports_attach_interface": True,
283             "supports_device_tagging": True,
284             "supports_tagged_attach_interface": True,
285             "supports_tagged_attach_volume": True,
286             "supports_extend_volume": True,
287             "supports_multiattach": True,
288             "supports_trusted_certs": True,
289             # Supported image types
290             "supports_image_type_aki": True,
291             "supports_image_type_ari": True,
292             "supports_image_type_ami": True,
293             "supports_image_type_raw": True,
294             "supports_image_type_iso": True,
295             # NOTE(danms): Certain backends do not work with complex image
296             # formats. If we are configured for those backends, then we
297             # should not expose the corresponding support traits.
298             "supports_image_type_qcow2": not requires_raw_image,
299             "supports_image_type_ploop": requires_ploop_image,
300             "supports_pcpus": True,
301             "supports_accelerators": True,
302             "supports_bfv_rescue": True,
303             "supports_vtpm": CONF.libvirt.swtpm_enabled,
304         }
305         super(LibvirtDriver, self).__init__(virtapi)
306 
307         if not sys.platform.startswith('linux'):
308             raise exception.InternalError(
309                 _('The libvirt driver only works on Linux'))
310 
311         global libvirt
312         if libvirt is None:
313             libvirt = importutils.import_module('libvirt')
314             libvirt_migrate.libvirt = libvirt
315 
316         self._host = host.Host(self._uri(), read_only,
317                                lifecycle_event_handler=self.emit_event,
318                                conn_event_handler=self._handle_conn_event)
319         self._supported_perf_events = []
320 
321         self.vif_driver = libvirt_vif.LibvirtGenericVIFDriver()
322 
323         # NOTE(lyarwood): Volume drivers are loaded on-demand
324         self.volume_drivers: ty.Dict[str, volume.LibvirtBaseVolumeDriver] = {}
325 
326         self._disk_cachemode = None
327         self.image_cache_manager = imagecache.ImageCacheManager()
328         self.image_backend = imagebackend.Backend(CONF.use_cow_images)
329 
330         self.disk_cachemodes = {}
331 
332         for mode_str in CONF.libvirt.disk_cachemodes:
333             disk_type, sep, cache_mode = mode_str.partition('=')
334             if cache_mode not in VALID_DISK_CACHEMODES:
335                 LOG.warning('Invalid cachemode %(cache_mode)s specified '
336                             'for disk type %(disk_type)s.',
337                             {'cache_mode': cache_mode, 'disk_type': disk_type})
338                 continue
339             self.disk_cachemodes[disk_type] = cache_mode
340 
341         self._volume_api = cinder.API()
342         self._image_api = glance.API()
343 
344         # The default choice for the sysinfo_serial config option is "unique"
345         # which does not have a special function since the value is just the
346         # instance.uuid.
347         sysinfo_serial_funcs = {
348             'none': lambda: None,
349             'hardware': self._get_host_sysinfo_serial_hardware,
350             'os': self._get_host_sysinfo_serial_os,
351             'auto': self._get_host_sysinfo_serial_auto,
352         }
353 
354         self._sysinfo_serial_func = sysinfo_serial_funcs.get(
355             CONF.libvirt.sysinfo_serial, lambda: None)
356 
357         self.job_tracker = instancejobtracker.InstanceJobTracker()
358         self._remotefs = remotefs.RemoteFilesystem()
359 
360         self._live_migration_flags = self._block_migration_flags = 0
361         self.active_migrations = {}
362 
363         # Compute reserved hugepages from conf file at the very
364         # beginning to ensure any syntax error will be reported and
365         # avoid any re-calculation when computing resources.
366         self._reserved_hugepages = hardware.numa_get_reserved_huge_pages()
367 
368         # Copy of the compute service ProviderTree object that is updated
369         # every time update_provider_tree() is called.
370         # NOTE(sbauza): We only want a read-only cache, this attribute is not
371         # intended to be updatable directly
372         self.provider_tree: provider_tree.ProviderTree = None
373 
374         # driver traits will not change during the runtime of the agent
375         # so calcuate them once and save them
376         self._static_traits = None
377 
378         # The CPU models in the configuration are case-insensitive, but the CPU
379         # model in the libvirt is case-sensitive, therefore create a mapping to
380         # map the lower case CPU model name to normal CPU model name.
381         self.cpu_models_mapping = {}
382         self.cpu_model_flag_mapping = {}
383 
384         self._vpmems_by_name, self._vpmems_by_rc = self._discover_vpmems(
385                 vpmem_conf=CONF.libvirt.pmem_namespaces)
386 
387         # We default to not support vGPUs unless the configuration is set.
388         self.pgpu_type_mapping = collections.defaultdict(str)
389         self.supported_vgpu_types = self._get_supported_vgpu_types()
390 
391     def _discover_vpmems(self, vpmem_conf=None):
392         """Discover vpmems on host and configuration.
393 
394         :param vpmem_conf: pmem namespaces configuration from CONF
395         :returns: a dict of vpmem keyed by name, and
396                   a dict of vpmem list keyed by resource class
397         :raises: exception.InvalidConfiguration if Libvirt or QEMU version
398                  does not meet requirement.
399         """
400         if not vpmem_conf:
401             return {}, {}
402 
403         # vpmem keyed by name {name: objects.LibvirtVPMEMDevice,...}
404         vpmems_by_name: ty.Dict[str, 'objects.LibvirtVPMEMDevice'] = {}
405         # vpmem list keyed by resource class
406         # {'RC_0': [objects.LibvirtVPMEMDevice, ...], 'RC_1': [...]}
407         vpmems_by_rc: ty.Dict[str, ty.List['objects.LibvirtVPMEMDevice']] = (
408             collections.defaultdict(list)
409         )
410 
411         vpmems_host = self._get_vpmems_on_host()
412         for ns_conf in vpmem_conf:
413             try:
414                 ns_label, ns_names = ns_conf.split(":", 1)
415             except ValueError:
416                 reason = _("The configuration doesn't follow the format")
417                 raise exception.PMEMNamespaceConfigInvalid(
418                         reason=reason)
419             ns_names = ns_names.split("|")
420             for ns_name in ns_names:
421                 if ns_name not in vpmems_host:
422                     reason = _("The PMEM namespace %s isn't on host") % ns_name
423                     raise exception.PMEMNamespaceConfigInvalid(
424                             reason=reason)
425                 if ns_name in vpmems_by_name:
426                     reason = (_("Duplicated PMEM namespace %s configured") %
427                                 ns_name)
428                     raise exception.PMEMNamespaceConfigInvalid(
429                             reason=reason)
430                 pmem_ns_updated = vpmems_host[ns_name]
431                 pmem_ns_updated.label = ns_label
432                 vpmems_by_name[ns_name] = pmem_ns_updated
433                 rc = orc.normalize_name(
434                         "PMEM_NAMESPACE_%s" % ns_label)
435                 vpmems_by_rc[rc].append(pmem_ns_updated)
436 
437         return vpmems_by_name, vpmems_by_rc
438 
439     def _get_vpmems_on_host(self):
440         """Get PMEM namespaces on host using ndctl utility."""
441         try:
442             output = nova.privsep.libvirt.get_pmem_namespaces()
443         except Exception as e:
444             reason = _("Get PMEM namespaces by ndctl utility, "
445                     "please ensure ndctl is installed: %s") % e
446             raise exception.GetPMEMNamespacesFailed(reason=reason)
447 
448         if not output:
449             return {}
450         namespaces = jsonutils.loads(output)
451         vpmems_host = {}  # keyed by namespace name
452         for ns in namespaces:
453             # store namespace info parsed from ndctl utility return
454             if not ns.get('name'):
455                 # The name is used to identify namespaces, it's optional
456                 # config when creating namespace. If an namespace don't have
457                 # name, it can not be used by Nova, we will skip it.
458                 continue
459             vpmems_host[ns['name']] = objects.LibvirtVPMEMDevice(
460                 name=ns['name'],
461                 devpath= '/dev/' + ns['daxregion']['devices'][0]['chardev'],
462                 size=ns['size'],
463                 align=ns['daxregion']['align'])
464         return vpmems_host
465 
466     @property
467     def disk_cachemode(self):
468         # It can be confusing to understand the QEMU cache mode
469         # behaviour, because each cache=$MODE is a convenient shorthand
470         # to toggle _three_ cache.* booleans.  Consult the below table
471         # (quoting from the QEMU man page):
472         #
473         #              | cache.writeback | cache.direct | cache.no-flush
474         # --------------------------------------------------------------
475         # writeback    | on              | off          | off
476         # none         | on              | on           | off
477         # writethrough | off             | off          | off
478         # directsync   | off             | on           | off
479         # unsafe       | on              | off          | on
480         #
481         # Where:
482         #
483         #  - 'cache.writeback=off' means: QEMU adds an automatic fsync()
484         #    after each write request.
485         #
486         #  - 'cache.direct=on' means: Use Linux's O_DIRECT, i.e. bypass
487         #    the kernel page cache.  Caches in any other layer (disk
488         #    cache, QEMU metadata caches, etc.) can still be present.
489         #
490         #  - 'cache.no-flush=on' means: Ignore flush requests, i.e.
491         #    never call fsync(), even if the guest explicitly requested
492         #    it.
493         #
494         # Use cache mode "none" (cache.writeback=on, cache.direct=on,
495         # cache.no-flush=off) for consistent performance and
496         # migration correctness.  Some filesystems don't support
497         # O_DIRECT, though.  For those we fallback to the next
498         # reasonable option that is "writeback" (cache.writeback=on,
499         # cache.direct=off, cache.no-flush=off).
500 
501         if self._disk_cachemode is None:
502             self._disk_cachemode = "none"
503             if not nova.privsep.utils.supports_direct_io(CONF.instances_path):
504                 self._disk_cachemode = "writeback"
505         return self._disk_cachemode
506 
507     def _set_cache_mode(self, conf):
508         """Set cache mode on LibvirtConfigGuestDisk object."""
509         try:
510             source_type = conf.source_type
511             driver_cache = conf.driver_cache
512         except AttributeError:
513             return
514 
515         # Shareable disks like for a multi-attach volume need to have the
516         # driver cache disabled.
517         if getattr(conf, 'shareable', False):
518             conf.driver_cache = 'none'
519         else:
520             cache_mode = self.disk_cachemodes.get(source_type,
521                                                   driver_cache)
522             conf.driver_cache = cache_mode
523 
524         # NOTE(acewit): If the [libvirt]disk_cachemodes is set as
525         # `block=writeback` or `block=writethrough` or `block=unsafe`,
526         # whose correponding Linux's IO semantic is not O_DIRECT in
527         # file nova.conf, then it will result in an attachment failure
528         # because of the libvirt bug
529         # (https://bugzilla.redhat.com/show_bug.cgi?id=1086704)
530         if ((getattr(conf, 'driver_io', None) == "native") and
531                 conf.driver_cache not in [None, 'none', 'directsync']):
532             conf.driver_io = "threads"
533             LOG.warning("The guest disk driver io mode has fallen back "
534                         "from 'native' to 'threads' because the "
535                         "disk cache mode is set as %(cachemode)s, which does "
536                         "not use O_DIRECT. See the following bug report "
537                         "for more details: https://launchpad.net/bugs/1841363",
538                         {'cachemode': conf.driver_cache})
539 
540     def _do_quality_warnings(self):
541         """Warn about potential configuration issues.
542 
543         This will log a warning message for things such as untested driver or
544         host arch configurations in order to indicate potential issues to
545         administrators.
546         """
547         if CONF.libvirt.virt_type not in ('qemu', 'kvm'):
548             LOG.warning(
549                 "Support for the '%(type)s' libvirt backend has been "
550                 "deprecated and will be removed in a future release.",
551                 {'type': CONF.libvirt.virt_type},
552             )
553 
554         caps = self._host.get_capabilities()
555         hostarch = caps.host.cpu.arch
556         if hostarch not in (
557             fields.Architecture.I686, fields.Architecture.X86_64,
558         ):
559             LOG.warning(
560                 'The libvirt driver is not tested on %(arch)s by the '
561                 'OpenStack project and thus its quality can not be ensured. '
562                 'For more information, see: https://docs.openstack.org/'
563                 'nova/latest/user/support-matrix.html',
564                 {'arch': hostarch},
565             )
566 
567     def _handle_conn_event(self, enabled, reason):
568         LOG.info("Connection event '%(enabled)d' reason '%(reason)s'",
569                  {'enabled': enabled, 'reason': reason})
570         self._set_host_enabled(enabled, reason)
571 
572     def init_host(self, host):
573         self._host.initialize()
574 
575         self._check_cpu_set_configuration()
576 
577         self._do_quality_warnings()
578 
579         self._parse_migration_flags()
580 
581         self._supported_perf_events = self._get_supported_perf_events()
582 
583         self._check_file_backed_memory_support()
584 
585         self._check_my_ip()
586 
587         if (CONF.libvirt.virt_type == 'lxc' and
588                 not (CONF.libvirt.uid_maps and CONF.libvirt.gid_maps)):
589             LOG.warning("Running libvirt-lxc without user namespaces is "
590                         "dangerous. Containers spawned by Nova will be run "
591                         "as the host's root user. It is highly suggested "
592                         "that user namespaces be used in a public or "
593                         "multi-tenant environment.")
594 
595         # Stop libguestfs using KVM unless we're also configured
596         # to use this. This solves problem where people need to
597         # stop Nova use of KVM because nested-virt is broken
598         if CONF.libvirt.virt_type != "kvm":
599             guestfs.force_tcg()
600 
601         if not self._host.has_min_version(MIN_LIBVIRT_VERSION):
602             raise exception.InternalError(
603                 _('Nova requires libvirt version %s or greater.') %
604                 libvirt_utils.version_to_string(MIN_LIBVIRT_VERSION))
605 
606         if CONF.libvirt.virt_type in ("qemu", "kvm"):
607             if not self._host.has_min_version(hv_ver=MIN_QEMU_VERSION):
608                 raise exception.InternalError(
609                     _('Nova requires QEMU version %s or greater.') %
610                     libvirt_utils.version_to_string(MIN_QEMU_VERSION))
611 
612         if CONF.libvirt.virt_type == 'parallels':
613             if not self._host.has_min_version(hv_ver=MIN_VIRTUOZZO_VERSION):
614                 raise exception.InternalError(
615                     _('Nova requires Virtuozzo version %s or greater.') %
616                     libvirt_utils.version_to_string(MIN_VIRTUOZZO_VERSION))
617 
618         # Give the cloud admin a heads up if we are intending to
619         # change the MIN_LIBVIRT_VERSION in the next release.
620         if not self._host.has_min_version(NEXT_MIN_LIBVIRT_VERSION):
621             LOG.warning('Running Nova with a libvirt version less than '
622                         '%(version)s is deprecated. The required minimum '
623                         'version of libvirt will be raised to %(version)s '
624                         'in the next release.',
625                         {'version': libvirt_utils.version_to_string(
626                             NEXT_MIN_LIBVIRT_VERSION)})
627         if (CONF.libvirt.virt_type in ("qemu", "kvm") and
628             not self._host.has_min_version(hv_ver=NEXT_MIN_QEMU_VERSION)):
629             LOG.warning('Running Nova with a QEMU version less than '
630                         '%(version)s is deprecated. The required minimum '
631                         'version of QEMU will be raised to %(version)s '
632                         'in the next release.',
633                         {'version': libvirt_utils.version_to_string(
634                             NEXT_MIN_QEMU_VERSION)})
635 
636         # Allowing both "tunnelling via libvirtd" (which will be
637         # deprecated once the MIN_{LIBVIRT,QEMU}_VERSION is sufficiently
638         # new enough) and "native TLS" options at the same time is
639         # nonsensical.
640         if (CONF.libvirt.live_migration_tunnelled and
641                 CONF.libvirt.live_migration_with_native_tls):
642             msg = _("Setting both 'live_migration_tunnelled' and "
643                     "'live_migration_with_native_tls' at the same "
644                     "time is invalid. If you have the relevant "
645                     "libvirt and QEMU versions, and TLS configured "
646                     "in your environment, pick "
647                     "'live_migration_with_native_tls'.")
648             raise exception.Invalid(msg)
649 
650         # Some imagebackends are only able to import raw disk images,
651         # and will fail if given any other format. See the bug
652         # https://bugs.launchpad.net/nova/+bug/1816686 for more details.
653         if CONF.libvirt.images_type in ('rbd',):
654             if not CONF.force_raw_images:
655                 msg = _("'[DEFAULT]/force_raw_images = False' is not "
656                         "allowed with '[libvirt]/images_type = rbd'. "
657                         "Please check the two configs and if you really "
658                         "do want to use rbd as images_type, set "
659                         "force_raw_images to True.")
660                 raise exception.InvalidConfiguration(msg)
661 
662         # TODO(sbauza): Remove this code once mediated devices are persisted
663         # across reboots.
664         self._recreate_assigned_mediated_devices()
665 
666         self._check_cpu_compatibility()
667 
668         self._check_vtpm_support()
669 
670     def _check_cpu_compatibility(self):
671         mode = CONF.libvirt.cpu_mode
672         models = CONF.libvirt.cpu_models
673 
674         if (CONF.libvirt.virt_type not in ("kvm", "qemu") and
675                 mode not in (None, 'none')):
676             msg = _("Config requested an explicit CPU model, but "
677                     "the current libvirt hypervisor '%s' does not "
678                     "support selecting CPU models") % CONF.libvirt.virt_type
679             raise exception.Invalid(msg)
680 
681         if mode != "custom":
682             if not models:
683                 return
684             msg = _("The cpu_models option is not required when "
685                     "cpu_mode!=custom")
686             raise exception.Invalid(msg)
687 
688         if not models:
689             msg = _("The cpu_models option is required when cpu_mode=custom")
690             raise exception.Invalid(msg)
691 
692         cpu = vconfig.LibvirtConfigGuestCPU()
693         for model in models:
694             cpu.model = self._get_cpu_model_mapping(model)
695             try:
696                 self._compare_cpu(cpu, self._get_cpu_info(), None)
697             except exception.InvalidCPUInfo as e:
698                 msg = (_("Configured CPU model: %(model)s is not "
699                          "compatible with host CPU. Please correct your "
700                          "config and try again. %(e)s") % {
701                             'model': model, 'e': e})
702                 raise exception.InvalidCPUInfo(msg)
703 
704         # Use guest CPU model to check the compatibility between guest CPU and
705         # configured extra_flags
706         cpu = vconfig.LibvirtConfigGuestCPU()
707         cpu.model = self._host.get_capabilities().host.cpu.model
708         for flag in set(x.lower() for x in CONF.libvirt.cpu_model_extra_flags):
709             cpu.add_feature(vconfig.LibvirtConfigCPUFeature(flag))
710             try:
711                 self._compare_cpu(cpu, self._get_cpu_info(), None)
712             except exception.InvalidCPUInfo as e:
713                 msg = (_("Configured extra flag: %(flag)s it not correct, or "
714                          "the host CPU does not support this flag. Please "
715                          "correct the config and try again. %(e)s") % {
716                             'flag': flag, 'e': e})
717                 raise exception.InvalidCPUInfo(msg)
718 
719     def _check_vtpm_support(self) -> None:
720         # TODO(efried): A key manager must be configured to create/retrieve
721         # secrets. Is there a way to check that one is set up correctly?
722         # CONF.key_manager.backend is optional :(
723         if not CONF.libvirt.swtpm_enabled:
724             return
725 
726         if CONF.libvirt.virt_type not in ('qemu', 'kvm'):
727             msg = _(
728                 "vTPM support requires '[libvirt] virt_type' of 'qemu' or "
729                 "'kvm'; found '%s'.")
730             raise exception.InvalidConfiguration(msg % CONF.libvirt.virt_type)
731 
732         if not self._host.has_min_version(lv_ver=MIN_LIBVIRT_VTPM):
733             msg = _(
734                 'vTPM support requires Libvirt version %(libvirt)s or '
735                 'greater.')
736             raise exception.InvalidConfiguration(msg % {
737                 'libvirt': libvirt_utils.version_to_string(MIN_LIBVIRT_VTPM),
738             })
739 
740         # These executables need to be installed for libvirt to make use of
741         # emulated TPM.
742         # NOTE(stephenfin): This checks using the PATH of the user running
743         # nova-compute rather than the libvirtd service, meaning it's an
744         # imperfect check but the best we can do
745         if not any(shutil.which(cmd) for cmd in ('swtpm_setup', 'swtpm')):
746             msg = _(
747                 "vTPM support is configured but the 'swtpm' and "
748                 "'swtpm_setup' binaries could not be found on PATH.")
749             raise exception.InvalidConfiguration(msg)
750 
751         # The user and group must be valid on this host for cold migration and
752         # resize to function.
753         try:
754             pwd.getpwnam(CONF.libvirt.swtpm_user)
755         except KeyError:
756             msg = _(
757                 "The user configured in '[libvirt] swtpm_user' does not exist "
758                 "on this host; expected '%s'.")
759             raise exception.InvalidConfiguration(msg % CONF.libvirt.swtpm_user)
760 
761         try:
762             grp.getgrnam(CONF.libvirt.swtpm_group)
763         except KeyError:
764             msg = _(
765                 "The group configured in '[libvirt] swtpm_group' does not "
766                 "exist on this host; expected '%s'.")
767             raise exception.InvalidConfiguration(
768                 msg % CONF.libvirt.swtpm_group)
769 
770         LOG.debug('Enabling emulated TPM support')
771 
772     @staticmethod
773     def _is_existing_mdev(uuid):
774         # FIXME(sbauza): Some kernel can have a uevent race meaning that the
775         # libvirt daemon won't know when a mediated device is created unless
776         # you restart that daemon. Until all kernels we support are not having
777         # that possible race, check the sysfs directly instead of asking the
778         # libvirt API.
779         # See https://bugzilla.redhat.com/show_bug.cgi?id=1376907 for ref.
780         return os.path.exists('/sys/bus/mdev/devices/{0}'.format(uuid))
781 
782     def _recreate_assigned_mediated_devices(self):
783         """Recreate assigned mdevs that could have disappeared if we reboot
784         the host.
785         """
786         # NOTE(sbauza): This method just calls sysfs to recreate mediated
787         # devices by looking up existing guest XMLs and doesn't use
788         # the Placement API so it works with or without a vGPU reshape.
789         mdevs = self._get_all_assigned_mediated_devices()
790         for (mdev_uuid, instance_uuid) in mdevs.items():
791             if not self._is_existing_mdev(mdev_uuid):
792                 dev_name = libvirt_utils.mdev_uuid2name(mdev_uuid)
793                 dev_info = self._get_mediated_device_information(dev_name)
794                 parent = dev_info['parent']
795                 parent_type = self._get_vgpu_type_per_pgpu(parent)
796                 if dev_info['type'] != parent_type:
797                     # NOTE(sbauza): The mdev was created by using a different
798                     # vGPU type. We can't recreate the mdev until the operator
799                     # modifies the configuration.
800                     parent = "{}:{}:{}.{}".format(*parent[4:].split('_'))
801                     msg = ("The instance UUID %(inst)s uses a VGPU that "
802                            "its parent pGPU %(parent)s no longer "
803                            "supports as the instance vGPU type %(type)s "
804                            "is not accepted for the pGPU. Please correct "
805                            "the configuration accordingly." %
806                            {'inst': instance_uuid,
807                             'parent': parent,
808                             'type': dev_info['type']})
809                     raise exception.InvalidLibvirtGPUConfig(reason=msg)
810                 self._create_new_mediated_device(parent, uuid=mdev_uuid)
811 
812     def _check_file_backed_memory_support(self):
813         if not CONF.libvirt.file_backed_memory:
814             return
815 
816         # file_backed_memory is only compatible with qemu/kvm virts
817         if CONF.libvirt.virt_type not in ("qemu", "kvm"):
818             raise exception.InternalError(
819                 _('Running Nova with file_backed_memory and virt_type '
820                   '%(type)s is not supported. file_backed_memory is only '
821                   'supported with qemu and kvm types.') %
822                 {'type': CONF.libvirt.virt_type})
823 
824         # file-backed memory doesn't work with memory overcommit.
825         # Block service startup if file-backed memory is enabled and
826         # ram_allocation_ratio is not 1.0
827         if CONF.ram_allocation_ratio != 1.0:
828             raise exception.InternalError(
829                 'Running Nova with file_backed_memory requires '
830                 'ram_allocation_ratio configured to 1.0')
831 
832         if CONF.reserved_host_memory_mb:
833             # this is a hard failure as placement won't allow total < reserved
834             if CONF.reserved_host_memory_mb >= CONF.libvirt.file_backed_memory:
835                 msg = _(
836                     "'[libvirt] file_backed_memory', which represents total "
837                     "memory reported to placement, must be greater than "
838                     "reserved memory configured via '[DEFAULT] "
839                     "reserved_host_memory_mb'"
840                 )
841                 raise exception.InternalError(msg)
842 
843             # TODO(stephenfin): Change this to an exception in W or later
844             LOG.warning(
845                 "Reserving memory via '[DEFAULT] reserved_host_memory_mb' "
846                 "is not compatible with file-backed memory. Consider "
847                 "setting '[DEFAULT] reserved_host_memory_mb' to 0. This will "
848                 "be an error in a future release."
849             )
850 
851     def _check_my_ip(self):
852         ips = compute_utils.get_machine_ips()
853         if CONF.my_ip not in ips:
854             LOG.warning('my_ip address (%(my_ip)s) was not found on '
855                         'any of the interfaces: %(ifaces)s',
856                         {'my_ip': CONF.my_ip, 'ifaces': ", ".join(ips)})
857 
858     def _check_cpu_set_configuration(self):
859         # evaluate these now to force a quick fail if they're invalid
860         vcpu_pin_set = hardware.get_vcpu_pin_set() or set()
861         cpu_shared_set = hardware.get_cpu_shared_set() or set()
862         cpu_dedicated_set = hardware.get_cpu_dedicated_set() or set()
863 
864         # TODO(stephenfin): Remove this in U once we remove the 'vcpu_pin_set'
865         # option
866         if not vcpu_pin_set:
867             if not (cpu_shared_set or cpu_dedicated_set):
868                 return
869 
870             if not cpu_dedicated_set.isdisjoint(cpu_shared_set):
871                 msg = _(
872                     "The '[compute] cpu_dedicated_set' and '[compute] "
873                     "cpu_shared_set' configuration options must be "
874                     "disjoint.")
875                 raise exception.InvalidConfiguration(msg)
876 
877             if CONF.reserved_host_cpus:
878                 msg = _(
879                     "The 'reserved_host_cpus' config option cannot be defined "
880                     "alongside the '[compute] cpu_shared_set' or '[compute] "
881                     "cpu_dedicated_set' options. Unset 'reserved_host_cpus'.")
882                 raise exception.InvalidConfiguration(msg)
883 
884             return
885 
886         if cpu_dedicated_set:
887             # NOTE(stephenfin): This is a new option in Train so it can be
888             # an error
889             msg = _(
890                 "The 'vcpu_pin_set' config option has been deprecated and "
891                 "cannot be defined alongside '[compute] cpu_dedicated_set'. "
892                 "Unset 'vcpu_pin_set'.")
893             raise exception.InvalidConfiguration(msg)
894 
895         if cpu_shared_set:
896             LOG.warning(
897                 "The '[compute] cpu_shared_set' and 'vcpu_pin_set' config "
898                 "options have both been defined. While 'vcpu_pin_set' is "
899                 "defined, it will continue to be used to configure the "
900                 "specific host CPUs used for 'VCPU' inventory, while "
901                 "'[compute] cpu_shared_set' will only be used for guest "
902                 "emulator threads when 'hw:emulator_threads_policy=shared' "
903                 "is defined in the flavor. This is legacy behavior and will "
904                 "not be supported in a future release. "
905                 "If you wish to define specific host CPUs to be used for "
906                 "'VCPU' or 'PCPU' inventory, you must migrate the "
907                 "'vcpu_pin_set' config option value to '[compute] "
908                 "cpu_shared_set' and '[compute] cpu_dedicated_set', "
909                 "respectively, and undefine 'vcpu_pin_set'.")
910         else:
911             LOG.warning(
912                 "The 'vcpu_pin_set' config option has been deprecated and "
913                 "will be removed in a future release. When defined, "
914                 "'vcpu_pin_set' will be used to calculate 'VCPU' inventory "
915                 "and schedule instances that have 'VCPU' allocations. "
916                 "If you wish to define specific host CPUs to be used for "
917                 "'VCPU' or 'PCPU' inventory, you must migrate the "
918                 "'vcpu_pin_set' config option value to '[compute] "
919                 "cpu_shared_set' and '[compute] cpu_dedicated_set', "
920                 "respectively, and undefine 'vcpu_pin_set'.")
921 
922     def _prepare_migration_flags(self):
923         migration_flags = 0
924 
925         migration_flags |= libvirt.VIR_MIGRATE_LIVE
926 
927         # Adding p2p flag only if xen is not in use, because xen does not
928         # support p2p migrations
929         if CONF.libvirt.virt_type != 'xen':
930             migration_flags |= libvirt.VIR_MIGRATE_PEER2PEER
931 
932         # Adding VIR_MIGRATE_UNDEFINE_SOURCE because, without it, migrated
933         # instance will remain defined on the source host
934         migration_flags |= libvirt.VIR_MIGRATE_UNDEFINE_SOURCE
935 
936         # Adding VIR_MIGRATE_PERSIST_DEST to persist the VM on the
937         # destination host
938         migration_flags |= libvirt.VIR_MIGRATE_PERSIST_DEST
939 
940         live_migration_flags = block_migration_flags = migration_flags
941 
942         # Adding VIR_MIGRATE_NON_SHARED_INC, otherwise all block-migrations
943         # will be live-migrations instead
944         block_migration_flags |= libvirt.VIR_MIGRATE_NON_SHARED_INC
945 
946         return (live_migration_flags, block_migration_flags)
947 
948     # TODO(kchamart) Once the MIN_LIBVIRT_VERSION and MIN_QEMU_VERSION
949     # reach 4.4.0 and 2.11.0, which provide "native TLS" support by
950     # default, deprecate and remove the support for "tunnelled live
951     # migration" (and related config attribute), because:
952     #
953     #  (a) it cannot handle live migration of disks in a non-shared
954     #      storage setup (a.k.a. "block migration");
955     #
956     #  (b) has a huge performance overhead and latency, because it burns
957     #      more CPU and memory bandwidth due to increased number of data
958     #      copies on both source and destination hosts.
959     #
960     # Both the above limitations are addressed by the QEMU-native TLS
961     # support (`live_migration_with_native_tls`).
962     def _handle_live_migration_tunnelled(self, migration_flags):
963         if CONF.libvirt.live_migration_tunnelled:
964             migration_flags |= libvirt.VIR_MIGRATE_TUNNELLED
965         return migration_flags
966 
967     def _handle_native_tls(self, migration_flags):
968         if (CONF.libvirt.live_migration_with_native_tls):
969             migration_flags |= libvirt.VIR_MIGRATE_TLS
970         return migration_flags
971 
972     def _handle_live_migration_post_copy(self, migration_flags):
973         if CONF.libvirt.live_migration_permit_post_copy:
974             migration_flags |= libvirt.VIR_MIGRATE_POSTCOPY
975         return migration_flags
976 
977     def _handle_live_migration_auto_converge(self, migration_flags):
978         if self._is_post_copy_enabled(migration_flags):
979             LOG.info('The live_migration_permit_post_copy is set to '
980                      'True and post copy live migration is available '
981                      'so auto-converge will not be in use.')
982         elif CONF.libvirt.live_migration_permit_auto_converge:
983             migration_flags |= libvirt.VIR_MIGRATE_AUTO_CONVERGE
984         return migration_flags
985 
986     def _parse_migration_flags(self):
987         (live_migration_flags,
988             block_migration_flags) = self._prepare_migration_flags()
989 
990         live_migration_flags = self._handle_live_migration_tunnelled(
991             live_migration_flags)
992         block_migration_flags = self._handle_live_migration_tunnelled(
993             block_migration_flags)
994 
995         live_migration_flags = self._handle_native_tls(
996             live_migration_flags)
997         block_migration_flags = self._handle_native_tls(
998             block_migration_flags)
999 
1000         live_migration_flags = self._handle_live_migration_post_copy(
1001             live_migration_flags)
1002         block_migration_flags = self._handle_live_migration_post_copy(
1003             block_migration_flags)
1004 
1005         live_migration_flags = self._handle_live_migration_auto_converge(
1006             live_migration_flags)
1007         block_migration_flags = self._handle_live_migration_auto_converge(
1008             block_migration_flags)
1009 
1010         self._live_migration_flags = live_migration_flags
1011         self._block_migration_flags = block_migration_flags
1012 
1013     # TODO(sahid): This method is targeted for removal when the tests
1014     # have been updated to avoid its use
1015     #
1016     # All libvirt API calls on the libvirt.Connect object should be
1017     # encapsulated by methods on the nova.virt.libvirt.host.Host
1018     # object, rather than directly invoking the libvirt APIs. The goal
1019     # is to avoid a direct dependency on the libvirt API from the
1020     # driver.py file.
1021     def _get_connection(self):
1022         return self._host.get_connection()
1023 
1024     _conn = property(_get_connection)
1025 
1026     @staticmethod
1027     def _uri():
1028         if CONF.libvirt.virt_type == 'uml':
1029             uri = CONF.libvirt.connection_uri or 'uml:///system'
1030         elif CONF.libvirt.virt_type == 'xen':
1031             uri = CONF.libvirt.connection_uri or 'xen:///'
1032         elif CONF.libvirt.virt_type == 'lxc':
1033             uri = CONF.libvirt.connection_uri or 'lxc:///'
1034         elif CONF.libvirt.virt_type == 'parallels':
1035             uri = CONF.libvirt.connection_uri or 'parallels:///system'
1036         else:
1037             uri = CONF.libvirt.connection_uri or 'qemu:///system'
1038         return uri
1039 
1040     @staticmethod
1041     def _live_migration_uri(dest):
1042         uris = {
1043             'kvm': 'qemu+%(scheme)s://%(dest)s/system',
1044             'qemu': 'qemu+%(scheme)s://%(dest)s/system',
1045             'xen': 'xenmigr://%(dest)s/system',
1046             'parallels': 'parallels+tcp://%(dest)s/system',
1047         }
1048         dest = oslo_netutils.escape_ipv6(dest)
1049 
1050         virt_type = CONF.libvirt.virt_type
1051         # TODO(pkoniszewski): Remove fetching live_migration_uri in Pike
1052         uri = CONF.libvirt.live_migration_uri
1053         if uri:
1054             return uri % dest
1055 
1056         uri = uris.get(virt_type)
1057         if uri is None:
1058             raise exception.LiveMigrationURINotAvailable(virt_type=virt_type)
1059 
1060         str_format = {
1061             'dest': dest,
1062             'scheme': CONF.libvirt.live_migration_scheme or 'tcp',
1063         }
1064         return uri % str_format
1065 
1066     @staticmethod
1067     def _migrate_uri(dest):
1068         uri = None
1069         dest = oslo_netutils.escape_ipv6(dest)
1070 
1071         # Only QEMU live migrations supports migrate-uri parameter
1072         virt_type = CONF.libvirt.virt_type
1073         if virt_type in ('qemu', 'kvm'):
1074             # QEMU accept two schemes: tcp and rdma.  By default
1075             # libvirt build the URI using the remote hostname and the
1076             # tcp schema.
1077             uri = 'tcp://%s' % dest
1078         # Because dest might be of type unicode, here we might return value of
1079         # type unicode as well which is not acceptable by libvirt python
1080         # binding when Python 2.7 is in use, so let's convert it explicitly
1081         # back to string. When Python 3.x is in use, libvirt python binding
1082         # accepts unicode type so it is completely fine to do a no-op str(uri)
1083         # conversion which will return value of type unicode.
1084         return uri and str(uri)
1085 
1086     def instance_exists(self, instance):
1087         """Efficient override of base instance_exists method."""
1088         try:
1089             self._host.get_guest(instance)
1090             return True
1091         except (exception.InternalError, exception.InstanceNotFound):
1092             return False
1093 
1094     def list_instances(self):
1095         names = []
1096         for guest in self._host.list_guests(only_running=False):
1097             names.append(guest.name)
1098 
1099         return names
1100 
1101     def list_instance_uuids(self):
1102         uuids = []
1103         for guest in self._host.list_guests(only_running=False):
1104             uuids.append(guest.uuid)
1105 
1106         return uuids
1107 
1108     def plug_vifs(self, instance, network_info):
1109         """Plug VIFs into networks."""
1110         for vif in network_info:
1111             self.vif_driver.plug(instance, vif)
1112 
1113     def _unplug_vifs(self, instance, network_info, ignore_errors):
1114         """Unplug VIFs from networks."""
1115         for vif in network_info:
1116             try:
1117                 self.vif_driver.unplug(instance, vif)
1118             except exception.NovaException:
1119                 if not ignore_errors:
1120                     raise
1121 
1122     def unplug_vifs(self, instance, network_info):
1123         self._unplug_vifs(instance, network_info, False)
1124 
1125     def _teardown_container(self, instance):
1126         inst_path = libvirt_utils.get_instance_path(instance)
1127         container_dir = os.path.join(inst_path, 'rootfs')
1128         rootfs_dev = instance.system_metadata.get('rootfs_device_name')
1129         LOG.debug('Attempting to teardown container at path %(dir)s with '
1130                   'root device: %(rootfs_dev)s',
1131                   {'dir': container_dir, 'rootfs_dev': rootfs_dev},
1132                   instance=instance)
1133         disk_api.teardown_container(container_dir, rootfs_dev)
1134 
1135     def _destroy(self, instance):
1136         try:
1137             guest = self._host.get_guest(instance)
1138             if CONF.serial_console.enabled:
1139                 # This method is called for several events: destroy,
1140                 # rebuild, hard-reboot, power-off - For all of these
1141                 # events we want to release the serial ports acquired
1142                 # for the guest before destroying it.
1143                 serials = self._get_serial_ports_from_guest(guest)
1144                 for hostname, port in serials:
1145                     serial_console.release_port(host=hostname, port=port)
1146         except exception.InstanceNotFound:
1147             guest = None
1148 
1149         # If the instance is already terminated, we're still happy
1150         # Otherwise, destroy it
1151         old_domid = -1
1152         if guest is not None:
1153             try:
1154                 old_domid = guest.id
1155                 guest.poweroff()
1156 
1157             except libvirt.libvirtError as e:
1158                 is_okay = False
1159                 errcode = e.get_error_code()
1160                 if errcode == libvirt.VIR_ERR_NO_DOMAIN:
1161                     # Domain already gone. This can safely be ignored.
1162                     is_okay = True
1163                 elif errcode == libvirt.VIR_ERR_OPERATION_INVALID:
1164                     # If the instance is already shut off, we get this:
1165                     # Code=55 Error=Requested operation is not valid:
1166                     # domain is not running
1167 
1168                     state = guest.get_power_state(self._host)
1169                     if state == power_state.SHUTDOWN:
1170                         is_okay = True
1171                 elif errcode == libvirt.VIR_ERR_INTERNAL_ERROR:
1172                     errmsg = e.get_error_message()
1173                     if (CONF.libvirt.virt_type == 'lxc' and
1174                         errmsg == 'internal error: '
1175                                   'Some processes refused to die'):
1176                         # Some processes in the container didn't die
1177                         # fast enough for libvirt. The container will
1178                         # eventually die. For now, move on and let
1179                         # the wait_for_destroy logic take over.
1180                         is_okay = True
1181                 elif errcode == libvirt.VIR_ERR_OPERATION_TIMEOUT:
1182                     LOG.warning("Cannot destroy instance, operation time out",
1183                                 instance=instance)
1184                     reason = _("operation time out")
1185                     raise exception.InstancePowerOffFailure(reason=reason)
1186                 elif errcode == libvirt.VIR_ERR_SYSTEM_ERROR:
1187                     with excutils.save_and_reraise_exception():
1188                         LOG.warning("Cannot destroy instance, general system "
1189                                     "call failure", instance=instance)
1190                 if not is_okay:
1191                     with excutils.save_and_reraise_exception():
1192                         LOG.error('Error from libvirt during destroy. '
1193                                   'Code=%(errcode)s Error=%(e)s',
1194                                   {'errcode': errcode, 'e': e},
1195                                   instance=instance)
1196 
1197         def _wait_for_destroy(expected_domid):
1198             """Called at an interval until the VM is gone."""
1199             # NOTE(vish): If the instance disappears during the destroy
1200             #             we ignore it so the cleanup can still be
1201             #             attempted because we would prefer destroy to
1202             #             never fail.
1203             try:
1204                 dom_info = self.get_info(instance)
1205                 state = dom_info.state
1206                 new_domid = dom_info.internal_id
1207             except exception.InstanceNotFound:
1208                 LOG.debug("During wait destroy, instance disappeared.",
1209                           instance=instance)
1210                 state = power_state.SHUTDOWN
1211 
1212             if state == power_state.SHUTDOWN:
1213                 LOG.info("Instance destroyed successfully.", instance=instance)
1214                 raise loopingcall.LoopingCallDone()
1215 
1216             # NOTE(wangpan): If the instance was booted again after destroy,
1217             #                this may be an endless loop, so check the id of
1218             #                domain here, if it changed and the instance is
1219             #                still running, we should destroy it again.
1220             # see https://bugs.launchpad.net/nova/+bug/1111213 for more details
1221             if new_domid != expected_domid:
1222                 LOG.info("Instance may be started again.", instance=instance)
1223                 kwargs['is_running'] = True
1224                 raise loopingcall.LoopingCallDone()
1225 
1226         kwargs = {'is_running': False}
1227         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_destroy,
1228                                                      old_domid)
1229         timer.start(interval=0.5).wait()
1230         if kwargs['is_running']:
1231             LOG.info("Going to destroy instance again.", instance=instance)
1232             self._destroy(instance)
1233         else:
1234             # NOTE(GuanQiang): teardown container to avoid resource leak
1235             if CONF.libvirt.virt_type == 'lxc':
1236                 self._teardown_container(instance)
1237 
1238     def destroy(self, context, instance, network_info, block_device_info=None,
1239                 destroy_disks=True):
1240         self._destroy(instance)
1241         self.cleanup(context, instance, network_info, block_device_info,
1242                      destroy_disks)
1243 
1244     def _undefine_domain(self, instance):
1245         try:
1246             guest = self._host.get_guest(instance)
1247             try:
1248                 hw_firmware_type = instance.image_meta.properties.get(
1249                     'hw_firmware_type')
1250                 support_uefi = self._check_uefi_support(hw_firmware_type)
1251                 guest.delete_configuration(support_uefi)
1252             except libvirt.libvirtError as e:
1253                 with excutils.save_and_reraise_exception() as ctxt:
1254                     errcode = e.get_error_code()
1255                     if errcode == libvirt.VIR_ERR_NO_DOMAIN:
1256                         LOG.debug("Called undefine, but domain already gone.",
1257                                   instance=instance)
1258                         ctxt.reraise = False
1259                     else:
1260                         LOG.error('Error from libvirt during undefine. '
1261                                   'Code=%(errcode)s Error=%(e)s',
1262                                   {'errcode': errcode,
1263                                    'e': encodeutils.exception_to_unicode(e)},
1264                                   instance=instance)
1265         except exception.InstanceNotFound:
1266             pass
1267 
1268     def cleanup(self, context, instance, network_info, block_device_info=None,
1269                 destroy_disks=True, migrate_data=None, destroy_vifs=True):
1270         """Cleanup the instance from the host.
1271 
1272         Identify if the instance disks and instance path should be removed
1273         from the host before calling down into the _cleanup method for the
1274         actual removal of resources from the host.
1275 
1276         :param context: security context
1277         :param instance: instance object for the instance being cleaned up
1278         :param network_info: instance network information
1279         :param block_device_info: optional instance block device information
1280         :param destroy_disks: if local ephemeral disks should be destroyed
1281         :param migrate_data: optional migrate_data object
1282         :param destroy_vifs: if plugged vifs should be unplugged
1283         """
1284         cleanup_instance_dir = False
1285         cleanup_instance_disks = False
1286         # We assume destroy_disks means destroy instance directory and disks
1287         if destroy_disks:
1288             cleanup_instance_dir = True
1289             cleanup_instance_disks = True
1290         else:
1291             # NOTE(mdbooth): I think the theory here was that if this is a
1292             # migration with shared block storage then we need to delete the
1293             # instance directory because that's not shared. I'm pretty sure
1294             # this is wrong.
1295             if migrate_data and 'is_shared_block_storage' in migrate_data:
1296                 cleanup_instance_dir = migrate_data.is_shared_block_storage
1297 
1298             # NOTE(lyarwood): The following workaround allows operators to
1299             # ensure that non-shared instance directories are removed after an
1300             # evacuation or revert resize when using the shared RBD
1301             # imagebackend. This workaround is not required when cleaning up
1302             # migrations that provide migrate_data to this method as the
1303             # existing is_shared_block_storage conditional will cause the
1304             # instance directory to be removed.
1305             if not cleanup_instance_dir:
1306                 if CONF.workarounds.ensure_libvirt_rbd_instance_dir_cleanup:
1307                     cleanup_instance_dir = CONF.libvirt.images_type == 'rbd'
1308 
1309         return self._cleanup(
1310                 context, instance, network_info,
1311                 block_device_info=block_device_info,
1312                 destroy_vifs=destroy_vifs,
1313                 cleanup_instance_dir=cleanup_instance_dir,
1314                 cleanup_instance_disks=cleanup_instance_disks)
1315 
1316     def _cleanup(self, context, instance, network_info, block_device_info=None,
1317                  destroy_vifs=True, cleanup_instance_dir=False,
1318                  cleanup_instance_disks=False):
1319         """Cleanup the domain and any attached resources from the host.
1320 
1321         This method cleans up any pmem devices, unplugs VIFs, disconnects
1322         attached volumes and undefines the instance domain within libvirt.
1323         It also optionally removes the ephemeral disks and the instance
1324         directory from the host depending on the cleanup_instance_dir|disks
1325         kwargs provided.
1326 
1327         :param context: security context
1328         :param instance: instance object for the instance being cleaned up
1329         :param network_info: instance network information
1330         :param block_device_info: optional instance block device information
1331         :param destroy_vifs: if plugged vifs should be unplugged
1332         :param cleanup_instance_dir: If the instance dir should be removed
1333         :param cleanup_instance_disks: If the instance disks should be removed
1334         """
1335         # zero the data on backend pmem device
1336         vpmems = self._get_vpmems(instance)
1337         if vpmems:
1338             self._cleanup_vpmems(vpmems)
1339 
1340         if destroy_vifs:
1341             self._unplug_vifs(instance, network_info, True)
1342 
1343         # FIXME(wangpan): if the instance is booted again here, such as the
1344         #                 soft reboot operation boot it here, it will become
1345         #                 "running deleted", should we check and destroy it
1346         #                 at the end of this method?
1347 
1348         # NOTE(vish): we disconnect from volumes regardless
1349         block_device_mapping = driver.block_device_info_get_mapping(
1350             block_device_info)
1351         for vol in block_device_mapping:
1352             connection_info = vol['connection_info']
1353             if not connection_info:
1354                 # if booting from a volume, creation could have failed meaning
1355                 # this would be unset
1356                 continue
1357 
1358             try:
1359                 self._disconnect_volume(context, connection_info, instance)
1360             except Exception as exc:
1361                 with excutils.save_and_reraise_exception() as ctxt:
1362                     if cleanup_instance_disks:
1363                         # Don't block on Volume errors if we're trying to
1364                         # delete the instance as we may be partially created
1365                         # or deleted
1366                         ctxt.reraise = False
1367                         LOG.warning(
1368                             "Ignoring Volume Error on vol %(vol_id)s "
1369                             "during delete %(exc)s",
1370                             {'vol_id': vol.get('volume_id'),
1371                              'exc': encodeutils.exception_to_unicode(exc)},
1372                             instance=instance)
1373 
1374         if cleanup_instance_disks:
1375             # NOTE(haomai): destroy volumes if needed
1376             if CONF.libvirt.images_type == 'lvm':
1377                 self._cleanup_lvm(instance, block_device_info)
1378             if CONF.libvirt.images_type == 'rbd':
1379                 self._cleanup_rbd(instance)
1380 
1381         if cleanup_instance_dir:
1382             attempts = int(instance.system_metadata.get('clean_attempts',
1383                                                         '0'))
1384             success = self.delete_instance_files(instance)
1385             # NOTE(mriedem): This is used in the _run_pending_deletes periodic
1386             # task in the compute manager. The tight coupling is not great...
1387             instance.system_metadata['clean_attempts'] = str(attempts + 1)
1388             if success:
1389                 instance.cleaned = True
1390             try:
1391                 instance.save()
1392             except exception.InstanceNotFound:
1393                 pass
1394 
1395         if cleanup_instance_disks:
1396             crypto.delete_vtpm_secret(context, instance)
1397 
1398         self._undefine_domain(instance)
1399 
1400     def cleanup_lingering_instance_resources(self, instance):
1401         # zero the data on backend pmem device, if fails
1402         # it will raise an exception
1403         vpmems = self._get_vpmems(instance)
1404         if vpmems:
1405             self._cleanup_vpmems(vpmems)
1406 
1407     def _cleanup_vpmems(self, vpmems):
1408         for vpmem in vpmems:
1409             try:
1410                 nova.privsep.libvirt.cleanup_vpmem(vpmem.devpath)
1411             except Exception as e:
1412                 raise exception.VPMEMCleanupFailed(dev=vpmem.devpath,
1413                                                    error=e)
1414 
1415     def _get_serial_ports_from_guest(self, guest, mode=None):
1416         """Returns an iterator over serial port(s) configured on guest.
1417 
1418         :param mode: Should be a value in (None, bind, connect)
1419         """
1420         xml = guest.get_xml_desc()
1421         tree = etree.fromstring(xml)
1422 
1423         # The 'serial' device is the base for x86 platforms. Other platforms
1424         # (e.g. kvm on system z = S390X) can only use 'console' devices.
1425         xpath_mode = "[@mode='%s']" % mode if mode else ""
1426         serial_tcp = "./devices/serial[@type='tcp']/source" + xpath_mode
1427         console_tcp = "./devices/console[@type='tcp']/source" + xpath_mode
1428 
1429         tcp_devices = tree.findall(serial_tcp)
1430         if len(tcp_devices) == 0:
1431             tcp_devices = tree.findall(console_tcp)
1432         for source in tcp_devices:
1433             yield (source.get("host"), int(source.get("service")))
1434 
1435     def _get_scsi_controller_next_unit(self, guest):
1436         """Returns the max disk unit used by scsi controller"""
1437         xml = guest.get_xml_desc()
1438         tree = etree.fromstring(xml)
1439         addrs = "./devices/disk[target/@bus='scsi']/address[@type='drive']"
1440 
1441         ret = []
1442         for obj in tree.xpath(addrs):
1443             ret.append(int(obj.get('unit', 0)))
1444         return max(ret) + 1 if ret else 0
1445 
1446     def _cleanup_rbd(self, instance):
1447         # NOTE(nic): On revert_resize, the cleanup steps for the root
1448         # volume are handled with an "rbd snap rollback" command,
1449         # and none of this is needed (and is, in fact, harmful) so
1450         # filter out non-ephemerals from the list
1451         if instance.task_state == task_states.RESIZE_REVERTING:
1452             filter_fn = lambda disk: (disk.startswith(instance.uuid) and
1453                                       disk.endswith('disk.local'))
1454         else:
1455             filter_fn = lambda disk: disk.startswith(instance.uuid)
1456         rbd_utils.RBDDriver().cleanup_volumes(filter_fn)
1457 
1458     def _cleanup_lvm(self, instance, block_device_info):
1459         """Delete all LVM disks for given instance object."""
1460         if instance.get('ephemeral_key_uuid') is not None:
1461             # detach encrypted volumes
1462             disks = self._get_instance_disk_info(instance, block_device_info)
1463             for disk in disks:
1464                 if dmcrypt.is_encrypted(disk['path']):
1465                     dmcrypt.delete_volume(disk['path'])
1466 
1467         disks = self._lvm_disks(instance)
1468         if disks:
1469             lvm.remove_volumes(disks)
1470 
1471     def _lvm_disks(self, instance):
1472         """Returns all LVM disks for given instance object."""
1473         if CONF.libvirt.images_volume_group:
1474             vg = os.path.join('/dev', CONF.libvirt.images_volume_group)
1475             if not os.path.exists(vg):
1476                 return []
1477             pattern = '%s_' % instance.uuid
1478 
1479             def belongs_to_instance(disk):
1480                 return disk.startswith(pattern)
1481 
1482             def fullpath(name):
1483                 return os.path.join(vg, name)
1484 
1485             logical_volumes = lvm.list_volumes(vg)
1486 
1487             disks = [fullpath(disk) for disk in logical_volumes
1488                      if belongs_to_instance(disk)]
1489             return disks
1490         return []
1491 
1492     def get_volume_connector(self, instance):
1493         root_helper = utils.get_root_helper()
1494         return connector.get_connector_properties(
1495             root_helper, CONF.my_block_storage_ip,
1496             CONF.libvirt.volume_use_multipath,
1497             enforce_multipath=True,
1498             host=CONF.host)
1499 
1500     def _cleanup_resize_vtpm(
1501         self,
1502         context: nova_context.RequestContext,
1503         instance: 'objects.Instance',
1504     ) -> None:
1505         """Handle vTPM when confirming a migration or resize.
1506 
1507         If the old flavor have vTPM and the new one doesn't, there are keys to
1508         be deleted.
1509         """
1510         old_vtpm_config = hardware.get_vtpm_constraint(
1511             instance.old_flavor, instance.image_meta)
1512         new_vtpm_config = hardware.get_vtpm_constraint(
1513             instance.new_flavor, instance.image_meta)
1514 
1515         if old_vtpm_config and not new_vtpm_config:
1516             # the instance no longer cares for its vTPM so delete the related
1517             # secret; the deletion of the instance directory and undefining of
1518             # the domain will take care of the TPM files themselves
1519             LOG.info('New flavor no longer requests vTPM; deleting secret.')
1520             crypto.delete_vtpm_secret(context, instance)
1521 
1522     # TODO(stephenfin): Fold this back into its only caller, cleanup_resize
1523     def _cleanup_resize(self, context, instance, network_info):
1524         inst_base = libvirt_utils.get_instance_path(instance)
1525         target = inst_base + '_resize'
1526 
1527         # zero the data on backend old pmem device
1528         vpmems = self._get_vpmems(instance, prefix='old')
1529         if vpmems:
1530             self._cleanup_vpmems(vpmems)
1531 
1532         # Remove any old vTPM data, if necessary
1533         self._cleanup_resize_vtpm(context, instance)
1534 
1535         # Deletion can fail over NFS, so retry the deletion as required.
1536         # Set maximum attempt as 5, most test can remove the directory
1537         # for the second time.
1538         attempts = 0
1539         while(os.path.exists(target) and attempts < 5):
1540             shutil.rmtree(target, ignore_errors=True)
1541             if os.path.exists(target):
1542                 time.sleep(random.randint(20, 200) / 100.0)
1543             attempts += 1
1544 
1545         # NOTE(mriedem): Some image backends will recreate the instance path
1546         # and disk.info during init, and all we need the root disk for
1547         # here is removing cloned snapshots which is backend-specific, so
1548         # check that first before initializing the image backend object. If
1549         # there is ever an image type that supports clone *and* re-creates
1550         # the instance directory and disk.info on init, this condition will
1551         # need to be re-visited to make sure that backend doesn't re-create
1552         # the disk. Refer to bugs: 1666831 1728603 1769131
1553         if self.image_backend.backend(CONF.libvirt.images_type).SUPPORTS_CLONE:
1554             root_disk = self.image_backend.by_name(instance, 'disk')
1555             if root_disk.exists():
1556                 root_disk.remove_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME)
1557 
1558         if instance.host != CONF.host:
1559             self._undefine_domain(instance)
1560             # TODO(sean-k-mooney): remove this call to unplug_vifs after
1561             # Wallaby is released. VIFs are now unplugged in resize_instance.
1562             try:
1563                 self.unplug_vifs(instance, network_info)
1564             except exception.InternalError as e:
1565                 LOG.debug(e, instance=instance)
1566 
1567     def _get_volume_driver(
1568         self, connection_info: ty.Dict[str, ty.Any]
1569     ) -> 'volume.LibvirtBaseVolumeDriver':
1570         """Fetch the nova.virt.libvirt.volume driver
1571 
1572         Based on the provided connection_info return a nova.virt.libvirt.volume
1573         driver. This will call out to os-brick to construct an connector and
1574         check if the connector is valid on the underlying host.
1575 
1576         :param connection_info: The connection_info associated with the volume
1577         :raises: VolumeDriverNotFound if no driver is found or if the host
1578             doesn't support the requested driver. This retains legacy behaviour
1579             when only supported drivers were loaded on startup leading to a
1580             VolumeDriverNotFound being raised later if an invalid driver was
1581             requested.
1582         """
1583         driver_type = connection_info.get('driver_volume_type')
1584 
1585         # If the driver_type isn't listed in the supported type list fail
1586         if driver_type not in VOLUME_DRIVERS:
1587             raise exception.VolumeDriverNotFound(driver_type=driver_type)
1588 
1589         # Return the cached driver
1590         if driver_type in self.volume_drivers:
1591             return self.volume_drivers.get(driver_type)
1592 
1593         @utils.synchronized('cache_volume_driver')
1594         def _cache_volume_driver(driver_type):
1595             # Check if another request cached the driver while we waited
1596             if driver_type in self.volume_drivers:
1597                 return self.volume_drivers.get(driver_type)
1598 
1599             try:
1600                 driver_class = importutils.import_class(
1601                     VOLUME_DRIVERS.get(driver_type))
1602                 self.volume_drivers[driver_type] = driver_class(self._host)
1603                 return self.volume_drivers.get(driver_type)
1604             except brick_exception.InvalidConnectorProtocol:
1605                 LOG.debug('Unable to load volume driver %s. It is not '
1606                           'supported on this host.', driver_type)
1607                 # NOTE(lyarwood): This exception is a subclass of
1608                 # VolumeDriverNotFound to ensure no callers have to change
1609                 # their error handling code after the move to on-demand loading
1610                 # of the volume drivers and associated os-brick connectors.
1611                 raise exception.VolumeDriverNotSupported(
1612                     volume_driver=VOLUME_DRIVERS.get(driver_type))
1613 
1614         # Cache the volume driver if it hasn't already been
1615         return _cache_volume_driver(driver_type)
1616 
1617     def _connect_volume(self, context, connection_info, instance,
1618                         encryption=None):
1619         vol_driver = self._get_volume_driver(connection_info)
1620         vol_driver.connect_volume(connection_info, instance)
1621         try:
1622             self._attach_encryptor(context, connection_info, encryption)
1623         except Exception:
1624             # Encryption failed so rollback the volume connection.
1625             with excutils.save_and_reraise_exception(logger=LOG):
1626                 LOG.exception("Failure attaching encryptor; rolling back "
1627                               "volume connection", instance=instance)
1628                 vol_driver.disconnect_volume(connection_info, instance)
1629 
1630     def _should_disconnect_target(self, context, instance, multiattach,
1631                                   vol_driver, volume_id):
1632         # NOTE(jdg): Multiattach is a special case (not to be confused
1633         # with shared_targets). With multiattach we may have a single volume
1634         # attached multiple times to *this* compute node (ie Server-1 and
1635         # Server-2).  So, if we receive a call to delete the attachment for
1636         # Server-1 we need to take special care to make sure that the Volume
1637         # isn't also attached to another Server on this Node.  Otherwise we
1638         # will indiscriminantly delete the connection for all Server and that's
1639         # no good.  So check if it's attached multiple times on this node
1640         # if it is we skip the call to brick to delete the connection.
1641         if not multiattach:
1642             return True
1643 
1644         # NOTE(deiter): Volume drivers using _HostMountStateManager are another
1645         # special case. _HostMountStateManager ensures that the compute node
1646         # only attempts to mount a single mountpoint in use by multiple
1647         # attachments once, and that it is not unmounted until it is no longer
1648         # in use by any attachments. So we can skip the multiattach check for
1649         # volume drivers that based on LibvirtMountedFileSystemVolumeDriver.
1650         if isinstance(vol_driver, fs.LibvirtMountedFileSystemVolumeDriver):
1651             return True
1652 
1653         connection_count = 0
1654         volume = self._volume_api.get(context, volume_id)
1655         attachments = volume.get('attachments', {})
1656         if len(attachments) > 1:
1657             # First we get a list of all Server UUID's associated with
1658             # this Host (Compute Node).  We're going to use this to
1659             # determine if the Volume being detached is also in-use by
1660             # another Server on this Host, ie just check to see if more
1661             # than one attachment.server_id for this volume is in our
1662             # list of Server UUID's for this Host
1663             servers_this_host = objects.InstanceList.get_uuids_by_host(
1664                 context, instance.host)
1665 
1666             # NOTE(jdg): nova.volume.cinder translates the
1667             # volume['attachments'] response into a dict which includes
1668             # the Server UUID as the key, so we're using that
1669             # here to check against our server_this_host list
1670             for server_id, data in attachments.items():
1671                 if server_id in servers_this_host:
1672                     connection_count += 1
1673         return (False if connection_count > 1 else True)
1674 
1675     def _disconnect_volume(self, context, connection_info, instance,
1676                            encryption=None):
1677         self._detach_encryptor(context, connection_info, encryption=encryption)
1678         vol_driver = self._get_volume_driver(connection_info)
1679         volume_id = driver_block_device.get_volume_id(connection_info)
1680         multiattach = connection_info.get('multiattach', False)
1681         if self._should_disconnect_target(
1682                 context, instance, multiattach, vol_driver, volume_id):
1683             vol_driver.disconnect_volume(connection_info, instance)
1684         else:
1685             LOG.info('Detected multiple connections on this host for '
1686                      'volume: %(volume)s, skipping target disconnect.',
1687                      {'volume': volume_id})
1688 
1689     def _extend_volume(self, connection_info, instance, requested_size):
1690         vol_driver = self._get_volume_driver(connection_info)
1691         return vol_driver.extend_volume(connection_info, instance,
1692                                         requested_size)
1693 
1694     def _allow_native_luksv1(self, encryption=None):
1695         """Check if QEMU's native LUKSv1 decryption should be used.
1696         """
1697         # NOTE(lyarwood): Native LUKSv1 decryption can be disabled via a
1698         # workarounds configurable in order to aviod known performance issues
1699         # with the libgcrypt lib.
1700         if CONF.workarounds.disable_native_luksv1:
1701             return False
1702 
1703         # NOTE(lyarwood): Ensure the LUKSv1 provider is used.
1704         provider = None
1705         if encryption:
1706             provider = encryption.get('provider', None)
1707         if provider in encryptors.LEGACY_PROVIDER_CLASS_TO_FORMAT_MAP:
1708             provider = encryptors.LEGACY_PROVIDER_CLASS_TO_FORMAT_MAP[provider]
1709         return provider == encryptors.LUKS
1710 
1711     def _get_volume_config(self, connection_info, disk_info):
1712         vol_driver = self._get_volume_driver(connection_info)
1713         conf = vol_driver.get_config(connection_info, disk_info)
1714         self._set_cache_mode(conf)
1715         return conf
1716 
1717     def _get_volume_encryptor(self, connection_info, encryption):
1718         root_helper = utils.get_root_helper()
1719         return encryptors.get_volume_encryptor(root_helper=root_helper,
1720                                                keymgr=key_manager.API(CONF),
1721                                                connection_info=connection_info,
1722                                                **encryption)
1723 
1724     def _get_volume_encryption(self, context, connection_info):
1725         """Get the encryption metadata dict if it is not provided
1726         """
1727         encryption = {}
1728         volume_id = driver_block_device.get_volume_id(connection_info)
1729         if volume_id:
1730             encryption = encryptors.get_encryption_metadata(context,
1731                             self._volume_api, volume_id, connection_info)
1732         return encryption
1733 
1734     def _attach_encryptor(self, context, connection_info, encryption):
1735         """Attach the frontend encryptor if one is required by the volume.
1736 
1737         The request context is only used when an encryption metadata dict is
1738         not provided. The encryption metadata dict being populated is then used
1739         to determine if an attempt to attach the encryptor should be made.
1740 
1741         """
1742         # NOTE(lyarwood): Skip any attempt to fetch encryption metadata or the
1743         # actual passphrase from the key manager if a libvirt secert already
1744         # exists locally for the volume. This suggests that the instance was
1745         # only powered off or the underlying host rebooted.
1746         volume_id = driver_block_device.get_volume_id(connection_info)
1747         if self._host.find_secret('volume', volume_id):
1748             LOG.debug("A libvirt secret for volume %s has been found on the "
1749                       "host, skipping any attempt to create another or attach "
1750                       "an os-brick encryptor.", volume_id)
1751             return
1752 
1753         if encryption is None:
1754             encryption = self._get_volume_encryption(context, connection_info)
1755 
1756         if encryption and self._allow_native_luksv1(encryption=encryption):
1757             # NOTE(lyarwood): Fetch the associated key for the volume and
1758             # decode the passphrase from the key.
1759             # FIXME(lyarwood): c-vol currently creates symmetric keys for use
1760             # with volumes, leading to the binary to hex to string conversion
1761             # below.
1762             keymgr = key_manager.API(CONF)
1763             key = keymgr.get(context, encryption['encryption_key_id'])
1764             key_encoded = key.get_encoded()
1765             passphrase = binascii.hexlify(key_encoded).decode('utf-8')
1766 
1767             # NOTE(lyarwood): Retain the behaviour of the original os-brick
1768             # encryptors and format any volume that does not identify as
1769             # encrypted with LUKS.
1770             # FIXME(lyarwood): Remove this once c-vol correctly formats
1771             # encrypted volumes during their initial creation:
1772             # https://bugs.launchpad.net/cinder/+bug/1739442
1773             device_path = connection_info.get('data').get('device_path')
1774             if device_path:
1775                 root_helper = utils.get_root_helper()
1776                 if not luks_encryptor.is_luks(root_helper, device_path):
1777                     encryptor = self._get_volume_encryptor(connection_info,
1778                                                            encryption)
1779                     encryptor._format_volume(passphrase, **encryption)
1780 
1781             # NOTE(lyarwood): Store the passphrase as a libvirt secret locally
1782             # on the compute node. This secret is used later when generating
1783             # the volume config.
1784             self._host.create_secret('volume', volume_id, password=passphrase)
1785         elif encryption:
1786             encryptor = self._get_volume_encryptor(connection_info,
1787                                                    encryption)
1788             encryptor.attach_volume(context, **encryption)
1789 
1790     def _detach_encryptor(self, context, connection_info, encryption):
1791         """Detach the frontend encryptor if one is required by the volume.
1792 
1793         The request context is only used when an encryption metadata dict is
1794         not provided. The encryption metadata dict being populated is then used
1795         to determine if an attempt to detach the encryptor should be made.
1796 
1797         If native LUKS decryption is enabled then delete previously created
1798         Libvirt volume secret from the host.
1799         """
1800         volume_id = driver_block_device.get_volume_id(connection_info)
1801         if volume_id and self._host.find_secret('volume', volume_id):
1802             return self._host.delete_secret('volume', volume_id)
1803         if encryption is None:
1804             encryption = self._get_volume_encryption(context, connection_info)
1805         # NOTE(lyarwood): Handle bug #1821696 where volume secrets have been
1806         # removed manually by returning if a LUKS provider is being used
1807         # and device_path is not present in the connection_info. This avoids
1808         # VolumeEncryptionNotSupported being thrown when we incorrectly build
1809         # the encryptor below due to the secrets not being present above.
1810         if (encryption and self._allow_native_luksv1(encryption=encryption) and
1811             not connection_info['data'].get('device_path')):
1812             return
1813         if encryption:
1814             encryptor = self._get_volume_encryptor(connection_info,
1815                                                    encryption)
1816             encryptor.detach_volume(**encryption)
1817 
1818     def _check_discard_for_attach_volume(self, conf, instance):
1819         """Perform some checks for volumes configured for discard support.
1820 
1821         If discard is configured for the volume, and the guest is using a
1822         configuration known to not work, we will log a message explaining
1823         the reason why.
1824         """
1825         if conf.driver_discard == 'unmap' and conf.target_bus == 'virtio':
1826             LOG.debug('Attempting to attach volume %(id)s with discard '
1827                       'support enabled to an instance using an '
1828                       'unsupported configuration. target_bus = '
1829                       '%(bus)s. Trim commands will not be issued to '
1830                       'the storage device.',
1831                       {'bus': conf.target_bus,
1832                        'id': conf.serial},
1833                       instance=instance)
1834 
1835     def attach_volume(self, context, connection_info, instance, mountpoint,
1836                       disk_bus=None, device_type=None, encryption=None):
1837         guest = self._host.get_guest(instance)
1838 
1839         disk_dev = mountpoint.rpartition("/")[2]
1840         bdm = {
1841             'device_name': disk_dev,
1842             'disk_bus': disk_bus,
1843             'device_type': device_type}
1844 
1845         # Note(cfb): If the volume has a custom block size, check that
1846         #            that we are using QEMU/KVM and libvirt >= 0.10.2. The
1847         #            presence of a block size is considered mandatory by
1848         #            cinder so we fail if we can't honor the request.
1849         data = {}
1850         if ('data' in connection_info):
1851             data = connection_info['data']
1852         if ('logical_block_size' in data or 'physical_block_size' in data):
1853             if ((CONF.libvirt.virt_type != "kvm" and
1854                  CONF.libvirt.virt_type != "qemu")):
1855                 msg = _("Volume sets block size, but the current "
1856                         "libvirt hypervisor '%s' does not support custom "
1857                         "block size") % CONF.libvirt.virt_type
1858                 raise exception.InvalidHypervisorType(msg)
1859 
1860         self._connect_volume(context, connection_info, instance,
1861                              encryption=encryption)
1862         disk_info = blockinfo.get_info_from_bdm(
1863             instance, CONF.libvirt.virt_type, instance.image_meta, bdm)
1864         if disk_info['bus'] == 'scsi':
1865             disk_info['unit'] = self._get_scsi_controller_next_unit(guest)
1866 
1867         conf = self._get_volume_config(connection_info, disk_info)
1868 
1869         self._check_discard_for_attach_volume(conf, instance)
1870 
1871         try:
1872             state = guest.get_power_state(self._host)
1873             live = state in (power_state.RUNNING, power_state.PAUSED)
1874 
1875             guest.attach_device(conf, persistent=True, live=live)
1876             # NOTE(artom) If we're attaching with a device role tag, we need to
1877             # rebuild device_metadata. If we're attaching without a role
1878             # tag, we're rebuilding it here needlessly anyways. This isn't a
1879             # massive deal, and it helps reduce code complexity by not having
1880             # to indicate to the virt driver that the attach is tagged. The
1881             # really important optimization of not calling the database unless
1882             # device_metadata has actually changed is done for us by
1883             # instance.save().
1884             instance.device_metadata = self._build_device_metadata(
1885                 context, instance)
1886             instance.save()
1887         except Exception:
1888             LOG.exception('Failed to attach volume at mountpoint: %s',
1889                           mountpoint, instance=instance)
1890             with excutils.save_and_reraise_exception():
1891                 self._disconnect_volume(context, connection_info, instance,
1892                                         encryption=encryption)
1893 
1894     def _swap_volume(self, guest, disk_dev, conf, resize_to, hw_firmware_type):
1895         """Swap existing disk with a new block device.
1896 
1897         Call virDomainBlockRebase or virDomainBlockCopy with Libvirt >= 6.0.0
1898         to copy and then pivot to a new volume.
1899 
1900         :param: guest: Guest object representing the guest domain
1901         :param: disk_dev: Device within the domain that is being swapped
1902         :param: conf: LibvirtConfigGuestDisk object representing the new volume
1903         :param: resize_to: Size of the dst volume, 0 if the same as the src
1904         :param: hw_firmware_type: fields.FirmwareType if set in the imagemeta
1905         """
1906         dev = guest.get_block_device(disk_dev)
1907 
1908         # Save a copy of the domain's persistent XML file. We'll use this
1909         # to redefine the domain if anything fails during the volume swap.
1910         xml = guest.get_xml_desc(dump_inactive=True, dump_sensitive=True)
1911 
1912         # Abort is an idempotent operation, so make sure any block
1913         # jobs which may have failed are ended.
1914         try:
1915             dev.abort_job()
1916         except Exception:
1917             pass
1918 
1919         try:
1920             # NOTE (rmk): virDomainBlockRebase and virDomainBlockCopy cannot be
1921             # executed on persistent domains, so we need to temporarily
1922             # undefine it. If any part of this block fails, the domain is
1923             # re-defined regardless.
1924             if guest.has_persistent_configuration():
1925                 support_uefi = self._check_uefi_support(hw_firmware_type)
1926                 guest.delete_configuration(support_uefi)
1927 
1928             try:
1929                 # NOTE(lyarwood): Use virDomainBlockCopy from libvirt >= 6.0.0
1930                 # and QEMU >= 4.2.0 with -blockdev domains allowing QEMU to
1931                 # copy to remote disks.
1932                 if self._host.has_min_version(lv_ver=MIN_LIBVIRT_BLOCKDEV,
1933                                               hv_ver=MIN_QEMU_BLOCKDEV):
1934                     dev.copy(conf.to_xml(), reuse_ext=True)
1935                 else:
1936                     # TODO(lyarwood): Remove the following use of
1937                     # virDomainBlockRebase once MIN_LIBVIRT_VERSION hits >=
1938                     # 6.0.0 and MIN_QEMU_VERSION hits >= 4.2.0.
1939                     # Start copy with VIR_DOMAIN_BLOCK_REBASE_REUSE_EXT flag to
1940                     # allow writing to existing external volume file. Use
1941                     # VIR_DOMAIN_BLOCK_REBASE_COPY_DEV if it's a block device
1942                     # to make sure XML is generated correctly (bug 1691195)
1943                     copy_dev = conf.source_type == 'block'
1944                     dev.rebase(conf.source_path, copy=True, reuse_ext=True,
1945                                copy_dev=copy_dev)
1946                 while not dev.is_job_complete():
1947                     time.sleep(0.5)
1948 
1949                 dev.abort_job(pivot=True)
1950 
1951             except Exception as exc:
1952                 # NOTE(lyarwood): conf.source_path is not set for RBD disks so
1953                 # fallback to conf.target_dev when None.
1954                 new_path = conf.source_path or conf.target_dev
1955                 old_path = disk_dev
1956                 LOG.exception("Failure rebasing volume %(new_path)s on "
1957                     "%(old_path)s.", {'new_path': new_path,
1958                                       'old_path': old_path})
1959                 raise exception.VolumeRebaseFailed(reason=str(exc))
1960 
1961             if resize_to:
1962                 dev.resize(resize_to * units.Gi)
1963 
1964             # Make sure we will redefine the domain using the updated
1965             # configuration after the volume was swapped. The dump_inactive
1966             # keyword arg controls whether we pull the inactive (persistent)
1967             # or active (live) config from the domain. We want to pull the
1968             # live config after the volume was updated to use when we redefine
1969             # the domain.
1970             xml = guest.get_xml_desc(dump_inactive=False, dump_sensitive=True)
1971         finally:
1972             self._host.write_instance_config(xml)
1973 
1974     def swap_volume(self, context, old_connection_info,
1975                     new_connection_info, instance, mountpoint, resize_to):
1976 
1977         # NOTE(lyarwood): https://bugzilla.redhat.com/show_bug.cgi?id=760547
1978         old_encrypt = self._get_volume_encryption(context, old_connection_info)
1979         new_encrypt = self._get_volume_encryption(context, new_connection_info)
1980         if ((old_encrypt and self._allow_native_luksv1(old_encrypt)) or
1981             (new_encrypt and self._allow_native_luksv1(new_encrypt))):
1982             raise NotImplementedError(_("Swap volume is not supported for "
1983                 "encrypted volumes when native LUKS decryption is enabled."))
1984 
1985         guest = self._host.get_guest(instance)
1986 
1987         disk_dev = mountpoint.rpartition("/")[2]
1988         if not guest.get_disk(disk_dev):
1989             raise exception.DiskNotFound(location=disk_dev)
1990         disk_info = {
1991             'dev': disk_dev,
1992             'bus': blockinfo.get_disk_bus_for_disk_dev(
1993                 CONF.libvirt.virt_type, disk_dev),
1994             'type': 'disk',
1995             }
1996         # NOTE (lyarwood): new_connection_info will be modified by the
1997         # following _connect_volume call down into the volume drivers. The
1998         # majority of the volume drivers will add a device_path that is in turn
1999         # used by _get_volume_config to set the source_path of the
2000         # LibvirtConfigGuestDisk object it returns. We do not explicitly save
2001         # this to the BDM here as the upper compute swap_volume method will
2002         # eventually do this for us.
2003         self._connect_volume(context, new_connection_info, instance)
2004         conf = self._get_volume_config(new_connection_info, disk_info)
2005         if (not conf.source_path and not
2006             self._host.has_min_version(lv_ver=MIN_LIBVIRT_BLOCKDEV,
2007                                        hv_ver=MIN_QEMU_BLOCKDEV)):
2008             self._disconnect_volume(context, new_connection_info, instance)
2009             raise NotImplementedError(_("Swap only supports host devices and "
2010                                         "files with Libvirt < 6.0.0 or QEMU "
2011                                         "< 4.2.0"))
2012 
2013         hw_firmware_type = instance.image_meta.properties.get(
2014             'hw_firmware_type')
2015 
2016         try:
2017             self._swap_volume(guest, disk_dev, conf,
2018                               resize_to, hw_firmware_type)
2019         except exception.VolumeRebaseFailed:
2020             with excutils.save_and_reraise_exception():
2021                 self._disconnect_volume(context, new_connection_info, instance)
2022 
2023         self._disconnect_volume(context, old_connection_info, instance)
2024 
2025     def _get_existing_domain_xml(self, instance, network_info,
2026                                  block_device_info=None):
2027         try:
2028             guest = self._host.get_guest(instance)
2029             xml = guest.get_xml_desc()
2030         except exception.InstanceNotFound:
2031             disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
2032                                                 instance,
2033                                                 instance.image_meta,
2034                                                 block_device_info)
2035             xml = self._get_guest_xml(nova_context.get_admin_context(),
2036                                       instance, network_info, disk_info,
2037                                       instance.image_meta,
2038                                       block_device_info=block_device_info)
2039         return xml
2040 
2041     def detach_volume(self, context, connection_info, instance, mountpoint,
2042                       encryption=None):
2043         disk_dev = mountpoint.rpartition("/")[2]
2044         try:
2045             guest = self._host.get_guest(instance)
2046 
2047             state = guest.get_power_state(self._host)
2048             live = state in (power_state.RUNNING, power_state.PAUSED)
2049             # NOTE(lyarwood): The volume must be detached from the VM before
2050             # detaching any attached encryptors or disconnecting the underlying
2051             # volume in _disconnect_volume. Otherwise, the encryptor or volume
2052             # driver may report that the volume is still in use.
2053             wait_for_detach = guest.detach_device_with_retry(
2054                 guest.get_disk, disk_dev, live=live)
2055             wait_for_detach()
2056 
2057         except exception.InstanceNotFound:
2058             # NOTE(zhaoqin): If the instance does not exist, _lookup_by_name()
2059             #                will throw InstanceNotFound exception. Need to
2060             #                disconnect volume under this circumstance.
2061             LOG.warning("During detach_volume, instance disappeared.",
2062                         instance=instance)
2063         except exception.DeviceNotFound:
2064             # We should still try to disconnect logical device from
2065             # host, an error might have happened during a previous
2066             # call.
2067             LOG.info("Device %s not found in instance.",
2068                      disk_dev, instance=instance)
2069         except libvirt.libvirtError as ex:
2070             # NOTE(vish): This is called to cleanup volumes after live
2071             #             migration, so we should still disconnect even if
2072             #             the instance doesn't exist here anymore.
2073             error_code = ex.get_error_code()
2074             if error_code == libvirt.VIR_ERR_NO_DOMAIN:
2075                 # NOTE(vish):
2076                 LOG.warning("During detach_volume, instance disappeared.",
2077                             instance=instance)
2078             else:
2079                 raise
2080 
2081         self._disconnect_volume(context, connection_info, instance,
2082                                 encryption=encryption)
2083 
2084     def _resize_attached_volume(self, new_size, block_device, instance):
2085         LOG.debug('Resizing target device %(dev)s to %(size)u',
2086                   {'dev': block_device._disk, 'size': new_size},
2087                   instance=instance)
2088         block_device.resize(new_size)
2089 
2090     def _resize_attached_encrypted_volume(self, original_new_size,
2091                                           block_device, instance,
2092                                           connection_info, encryption):
2093         # TODO(lyarwood): Also handle the dm-crpyt encryption providers of
2094         # plain and LUKSv2, for now just use the original_new_size.
2095         decrypted_device_new_size = original_new_size
2096 
2097         # NOTE(lyarwood): original_new_size currently refers to the total size
2098         # of the extended volume in bytes. With natively decrypted LUKSv1
2099         # volumes we need to ensure this now takes the LUKSv1 header and key
2100         # material into account. Otherwise QEMU will attempt and fail to grow
2101         # host block devices and remote RBD volumes.
2102         if self._allow_native_luksv1(encryption):
2103             try:
2104                 # NOTE(lyarwood): Find the path to provide to qemu-img
2105                 if 'device_path' in connection_info['data']:
2106                     path = connection_info['data']['device_path']
2107                 elif connection_info['driver_volume_type'] == 'rbd':
2108                     path = 'rbd:%s' % (connection_info['data']['name'])
2109                 else:
2110                     path = 'unknown'
2111                     raise exception.DiskNotFound(location='unknown')
2112 
2113                 info = images.privileged_qemu_img_info(path)
2114                 format_specific_data = info.format_specific['data']
2115                 payload_offset = format_specific_data['payload-offset']
2116 
2117                 # NOTE(lyarwood): Ensure the underlying device is not resized
2118                 # by subtracting the LUKSv1 payload_offset (where the users
2119                 # encrypted data starts) from the original_new_size (the total
2120                 # size of the underlying volume). Both are reported in bytes.
2121                 decrypted_device_new_size = original_new_size - payload_offset
2122 
2123             except exception.DiskNotFound:
2124                 with excutils.save_and_reraise_exception():
2125                     LOG.exception('Unable to access the encrypted disk %s.',
2126                                   path, instance=instance)
2127             except Exception:
2128                 with excutils.save_and_reraise_exception():
2129                     LOG.exception('Unknown error when attempting to find the '
2130                                   'payload_offset for LUKSv1 encrypted disk '
2131                                   '%s.', path, instance=instance)
2132         # NOTE(lyarwood): Resize the decrypted device within the instance to
2133         # the calculated size as with normal volumes.
2134         self._resize_attached_volume(
2135             decrypted_device_new_size, block_device, instance)
2136 
2137     def extend_volume(self, context, connection_info, instance,
2138                       requested_size):
2139         try:
2140             new_size = self._extend_volume(connection_info, instance,
2141                                            requested_size)
2142         except NotImplementedError:
2143             raise exception.ExtendVolumeNotSupported()
2144 
2145         # Resize the device in QEMU so its size is updated and
2146         # detected by the instance without rebooting.
2147         try:
2148             guest = self._host.get_guest(instance)
2149             state = guest.get_power_state(self._host)
2150             volume_id = driver_block_device.get_volume_id(connection_info)
2151             active_state = state in (power_state.RUNNING, power_state.PAUSED)
2152             if active_state:
2153                 if 'device_path' in connection_info['data']:
2154                     disk_path = connection_info['data']['device_path']
2155                 else:
2156                     # Some drivers (eg. net) don't put the device_path
2157                     # into the connection_info. Match disks by their serial
2158                     # number instead
2159                     disk = next(iter([
2160                         d for d in guest.get_all_disks()
2161                         if d.serial == volume_id
2162                     ]), None)
2163                     if not disk:
2164                         raise exception.VolumeNotFound(volume_id=volume_id)
2165                     disk_path = disk.target_dev
2166                 dev = guest.get_block_device(disk_path)
2167                 encryption = encryptors.get_encryption_metadata(
2168                     context, self._volume_api, volume_id, connection_info)
2169                 if encryption:
2170                     self._resize_attached_encrypted_volume(
2171                         new_size, dev, instance,
2172                         connection_info, encryption)
2173                 else:
2174                     self._resize_attached_volume(
2175                         new_size, dev, instance)
2176             else:
2177                 LOG.debug('Skipping block device resize, guest is not running',
2178                           instance=instance)
2179         except exception.InstanceNotFound:
2180             with excutils.save_and_reraise_exception():
2181                 LOG.warning('During extend_volume, instance disappeared.',
2182                             instance=instance)
2183         except libvirt.libvirtError:
2184             with excutils.save_and_reraise_exception():
2185                 LOG.exception('resizing block device failed.',
2186                               instance=instance)
2187 
2188     def attach_interface(self, context, instance, image_meta, vif):
2189         guest = self._host.get_guest(instance)
2190 
2191         self.vif_driver.plug(instance, vif)
2192         cfg = self.vif_driver.get_config(instance, vif, image_meta,
2193                                          instance.flavor,
2194                                          CONF.libvirt.virt_type)
2195         try:
2196             state = guest.get_power_state(self._host)
2197             live = state in (power_state.RUNNING, power_state.PAUSED)
2198             guest.attach_device(cfg, persistent=True, live=live)
2199         except libvirt.libvirtError:
2200             LOG.error('attaching network adapter failed.',
2201                       instance=instance, exc_info=True)
2202             self.vif_driver.unplug(instance, vif)
2203             raise exception.InterfaceAttachFailed(
2204                     instance_uuid=instance.uuid)
2205         try:
2206             # NOTE(artom) If we're attaching with a device role tag, we need to
2207             # rebuild device_metadata. If we're attaching without a role
2208             # tag, we're rebuilding it here needlessly anyways. This isn't a
2209             # massive deal, and it helps reduce code complexity by not having
2210             # to indicate to the virt driver that the attach is tagged. The
2211             # really important optimization of not calling the database unless
2212             # device_metadata has actually changed is done for us by
2213             # instance.save().
2214             instance.device_metadata = self._build_device_metadata(
2215                 context, instance)
2216             instance.save()
2217         except Exception:
2218             # NOTE(artom) If we fail here it means the interface attached
2219             # successfully but building and/or saving the device metadata
2220             # failed. Just unplugging the vif is therefore not enough cleanup,
2221             # we need to detach the interface.
2222             with excutils.save_and_reraise_exception(reraise=False):
2223                 LOG.error('Interface attached successfully but building '
2224                           'and/or saving device metadata failed.',
2225                           instance=instance, exc_info=True)
2226                 self.detach_interface(context, instance, vif)
2227                 raise exception.InterfaceAttachFailed(
2228                     instance_uuid=instance.uuid)
2229         try:
2230             guest.set_metadata(
2231                 self._get_guest_config_meta(
2232                     instance, instance.get_network_info()))
2233         except libvirt.libvirtError:
2234             LOG.warning('updating libvirt metadata failed.', instance=instance)
2235 
2236     def detach_interface(self, context, instance, vif):
2237         guest = self._host.get_guest(instance)
2238         cfg = self.vif_driver.get_config(instance, vif,
2239                                          instance.image_meta,
2240                                          instance.flavor,
2241                                          CONF.libvirt.virt_type)
2242         interface = guest.get_interface_by_cfg(cfg)
2243         try:
2244             # NOTE(mriedem): When deleting an instance and using Neutron,
2245             # we can be racing against Neutron deleting the port and
2246             # sending the vif-deleted event which then triggers a call to
2247             # detach the interface, so if the interface is not found then
2248             # we can just log it as a warning.
2249             if not interface:
2250                 mac = vif.get('address')
2251                 # The interface is gone so just log it as a warning.
2252                 LOG.warning('Detaching interface %(mac)s failed because '
2253                             'the device is no longer found on the guest.',
2254                             {'mac': mac}, instance=instance)
2255                 return
2256 
2257             state = guest.get_power_state(self._host)
2258             live = state in (power_state.RUNNING, power_state.PAUSED)
2259             # Now we are going to loop until the interface is detached or we
2260             # timeout.
2261             wait_for_detach = guest.detach_device_with_retry(
2262                 guest.get_interface_by_cfg, cfg, live=live,
2263                 alternative_device_name=self.vif_driver.get_vif_devname(vif))
2264             wait_for_detach()
2265         except exception.DeviceDetachFailed:
2266             # We failed to detach the device even with the retry loop, so let's
2267             # dump some debug information to the logs before raising back up.
2268             with excutils.save_and_reraise_exception():
2269                 devname = self.vif_driver.get_vif_devname(vif)
2270                 interface = guest.get_interface_by_cfg(cfg)
2271                 if interface:
2272                     LOG.warning(
2273                         'Failed to detach interface %(devname)s after '
2274                         'repeated attempts. Final interface xml:\n'
2275                         '%(interface_xml)s\nFinal guest xml:\n%(guest_xml)s',
2276                         {'devname': devname,
2277                          'interface_xml': interface.to_xml(),
2278                          'guest_xml': guest.get_xml_desc()},
2279                         instance=instance)
2280         except exception.DeviceNotFound:
2281             # The interface is gone so just log it as a warning.
2282             LOG.warning('Detaching interface %(mac)s failed because '
2283                         'the device is no longer found on the guest.',
2284                         {'mac': vif.get('address')}, instance=instance)
2285         except libvirt.libvirtError as ex:
2286             error_code = ex.get_error_code()
2287             if error_code == libvirt.VIR_ERR_NO_DOMAIN:
2288                 LOG.warning("During detach_interface, instance disappeared.",
2289                             instance=instance)
2290             else:
2291                 # NOTE(mriedem): When deleting an instance and using Neutron,
2292                 # we can be racing against Neutron deleting the port and
2293                 # sending the vif-deleted event which then triggers a call to
2294                 # detach the interface, so we might have failed because the
2295                 # network device no longer exists. Libvirt will fail with
2296                 # "operation failed: no matching network device was found"
2297                 # which unfortunately does not have a unique error code so we
2298                 # need to look up the interface by config and if it's not found
2299                 # then we can just log it as a warning rather than tracing an
2300                 # error.
2301                 mac = vif.get('address')
2302                 # Get a fresh instance of the guest in case it is gone.
2303                 try:
2304                     guest = self._host.get_guest(instance)
2305                 except exception.InstanceNotFound:
2306                     LOG.info("Instance disappeared while detaching interface "
2307                              "%s", vif['id'], instance=instance)
2308                     return
2309                 interface = guest.get_interface_by_cfg(cfg)
2310                 if interface:
2311                     LOG.error('detaching network adapter failed.',
2312                               instance=instance, exc_info=True)
2313                     raise exception.InterfaceDetachFailed(
2314                             instance_uuid=instance.uuid)
2315 
2316                 # The interface is gone so just log it as a warning.
2317                 LOG.warning('Detaching interface %(mac)s failed because '
2318                             'the device is no longer found on the guest.',
2319                             {'mac': mac}, instance=instance)
2320         finally:
2321             # NOTE(gibi): we need to unplug the vif _after_ the detach is done
2322             # on the libvirt side as otherwise libvirt will still manage the
2323             # device that our unplug code trying to reset. This can cause a
2324             # race and leave the detached device configured. Also even if we
2325             # are failed to detach due to race conditions the unplug is
2326             # necessary for the same reason
2327             self.vif_driver.unplug(instance, vif)
2328         try:
2329             # NOTE(nmiki): In order for the interface to be removed from
2330             # network_info, the nova-compute process need to wait for
2331             # processing on the neutron side.
2332             # Here, I simply exclude the target VIF from metadata.
2333             network_info = list(filter(lambda info: info['id'] != vif['id'],
2334                                        instance.get_network_info()))
2335             guest.set_metadata(
2336                 self._get_guest_config_meta(instance, network_info))
2337         except libvirt.libvirtError:
2338             LOG.warning('updating libvirt metadata failed.', instance=instance)
2339 
2340     def _create_snapshot_metadata(self, image_meta, instance,
2341                                   img_fmt, snp_name):
2342         metadata = {'status': 'active',
2343                     'name': snp_name,
2344                     'properties': {
2345                                    'kernel_id': instance.kernel_id,
2346                                    'image_location': 'snapshot',
2347                                    'image_state': 'available',
2348                                    'owner_id': instance.project_id,
2349                                    'ramdisk_id': instance.ramdisk_id,
2350                                    }
2351                     }
2352         if instance.os_type:
2353             metadata['properties']['os_type'] = instance.os_type
2354 
2355         # NOTE(vish): glance forces ami disk format to be ami
2356         if image_meta.disk_format == 'ami':
2357             metadata['disk_format'] = 'ami'
2358         else:
2359             metadata['disk_format'] = img_fmt
2360 
2361         if image_meta.obj_attr_is_set("container_format"):
2362             metadata['container_format'] = image_meta.container_format
2363         else:
2364             metadata['container_format'] = "bare"
2365 
2366         return metadata
2367 
2368     def snapshot(self, context, instance, image_id, update_task_state):
2369         """Create snapshot from a running VM instance.
2370 
2371         This command only works with qemu 0.14+
2372         """
2373         try:
2374             guest = self._host.get_guest(instance)
2375         except exception.InstanceNotFound:
2376             raise exception.InstanceNotRunning(instance_id=instance.uuid)
2377 
2378         snapshot = self._image_api.get(context, image_id)
2379 
2380         # source_format is an on-disk format
2381         # source_type is a backend type
2382         disk_path, source_format = libvirt_utils.find_disk(guest)
2383         source_type = libvirt_utils.get_disk_type_from_path(disk_path)
2384 
2385         # We won't have source_type for raw or qcow2 disks, because we can't
2386         # determine that from the path. We should have it from the libvirt
2387         # xml, though.
2388         if source_type is None:
2389             source_type = source_format
2390         # For lxc instances we won't have it either from libvirt xml
2391         # (because we just gave libvirt the mounted filesystem), or the path,
2392         # so source_type is still going to be None. In this case,
2393         # root_disk is going to default to CONF.libvirt.images_type
2394         # below, which is still safe.
2395 
2396         image_format = CONF.libvirt.snapshot_image_format or source_type
2397 
2398         # NOTE(bfilippov): save lvm and rbd as raw
2399         if image_format == 'lvm' or image_format == 'rbd':
2400             image_format = 'raw'
2401 
2402         metadata = self._create_snapshot_metadata(instance.image_meta,
2403                                                   instance,
2404                                                   image_format,
2405                                                   snapshot['name'])
2406 
2407         snapshot_name = uuidutils.generate_uuid(dashed=False)
2408 
2409         # store current state so we know what to resume back to if we suspend
2410         original_power_state = guest.get_power_state(self._host)
2411 
2412         # NOTE(dgenin): Instances with LVM encrypted ephemeral storage require
2413         #               cold snapshots. Currently, checking for encryption is
2414         #               redundant because LVM supports only cold snapshots.
2415         #               It is necessary in case this situation changes in the
2416         #               future.
2417         if (
2418             self._host.has_min_version(hv_type=host.HV_DRIVER_QEMU) and
2419             source_type != 'lvm' and
2420             not CONF.ephemeral_storage_encryption.enabled and
2421             not CONF.workarounds.disable_libvirt_livesnapshot and
2422             # NOTE(stephenfin): Live snapshotting doesn't make sense for
2423             # shutdown instances
2424             original_power_state != power_state.SHUTDOWN
2425         ):
2426             live_snapshot = True
2427         else:
2428             live_snapshot = False
2429 
2430         self._suspend_guest_for_snapshot(
2431             context, live_snapshot, original_power_state, instance)
2432 
2433         root_disk = self.image_backend.by_libvirt_path(
2434             instance, disk_path, image_type=source_type)
2435 
2436         if live_snapshot:
2437             LOG.info("Beginning live snapshot process", instance=instance)
2438         else:
2439             LOG.info("Beginning cold snapshot process", instance=instance)
2440 
2441         update_task_state(task_state=task_states.IMAGE_PENDING_UPLOAD)
2442 
2443         update_task_state(task_state=task_states.IMAGE_UPLOADING,
2444                           expected_state=task_states.IMAGE_PENDING_UPLOAD)
2445 
2446         try:
2447             metadata['location'] = root_disk.direct_snapshot(
2448                 context, snapshot_name, image_format, image_id,
2449                 instance.image_ref)
2450             self._resume_guest_after_snapshot(
2451                 context, live_snapshot, original_power_state, instance, guest)
2452             self._image_api.update(context, image_id, metadata,
2453                                    purge_props=False)
2454         except (NotImplementedError, exception.ImageUnacceptable,
2455                 exception.Forbidden) as e:
2456             if type(e) != NotImplementedError:
2457                 LOG.warning('Performing standard snapshot because direct '
2458                             'snapshot failed: %(error)s',
2459                             {'error': encodeutils.exception_to_unicode(e)})
2460             failed_snap = metadata.pop('location', None)
2461             if failed_snap:
2462                 failed_snap = {'url': str(failed_snap)}
2463             root_disk.cleanup_direct_snapshot(failed_snap,
2464                                                   also_destroy_volume=True,
2465                                                   ignore_errors=True)
2466             update_task_state(task_state=task_states.IMAGE_PENDING_UPLOAD,
2467                               expected_state=task_states.IMAGE_UPLOADING)
2468 
2469             # TODO(nic): possibly abstract this out to the root_disk
2470             if source_type == 'rbd' and live_snapshot:
2471                 # Standard snapshot uses qemu-img convert from RBD which is
2472                 # not safe to run with live_snapshot.
2473                 live_snapshot = False
2474                 # Suspend the guest, so this is no longer a live snapshot
2475                 self._suspend_guest_for_snapshot(
2476                     context, live_snapshot, original_power_state, instance)
2477 
2478             snapshot_directory = CONF.libvirt.snapshots_directory
2479             fileutils.ensure_tree(snapshot_directory)
2480             with utils.tempdir(dir=snapshot_directory) as tmpdir:
2481                 try:
2482                     out_path = os.path.join(tmpdir, snapshot_name)
2483                     if live_snapshot:
2484                         # NOTE(xqueralt): libvirt needs o+x in the tempdir
2485                         os.chmod(tmpdir, 0o701)
2486                         self._live_snapshot(context, instance, guest,
2487                                             disk_path, out_path, source_format,
2488                                             image_format, instance.image_meta)
2489                     else:
2490                         root_disk.snapshot_extract(out_path, image_format)
2491                     LOG.info("Snapshot extracted, beginning image upload",
2492                              instance=instance)
2493                 except libvirt.libvirtError as ex:
2494                     error_code = ex.get_error_code()
2495                     if error_code == libvirt.VIR_ERR_NO_DOMAIN:
2496                         LOG.info('Instance %(instance_name)s disappeared '
2497                                  'while taking snapshot of it: [Error Code '
2498                                  '%(error_code)s] %(ex)s',
2499                                  {'instance_name': instance.name,
2500                                   'error_code': error_code,
2501                                   'ex': ex},
2502                                  instance=instance)
2503                         raise exception.InstanceNotFound(
2504                             instance_id=instance.uuid)
2505                     else:
2506                         raise
2507                 finally:
2508                     self._resume_guest_after_snapshot(
2509                         context, live_snapshot, original_power_state, instance,
2510                         guest)
2511 
2512                 # Upload that image to the image service
2513                 update_task_state(task_state=task_states.IMAGE_UPLOADING,
2514                         expected_state=task_states.IMAGE_PENDING_UPLOAD)
2515                 with libvirt_utils.file_open(out_path, 'rb') as image_file:
2516                     # execute operation with disk concurrency semaphore
2517                     with compute_utils.disk_ops_semaphore:
2518                         self._image_api.update(context,
2519                                                image_id,
2520                                                metadata,
2521                                                image_file)
2522         except Exception:
2523             with excutils.save_and_reraise_exception():
2524                 LOG.exception("Failed to snapshot image")
2525                 failed_snap = metadata.pop('location', None)
2526                 if failed_snap:
2527                     failed_snap = {'url': str(failed_snap)}
2528                 root_disk.cleanup_direct_snapshot(
2529                         failed_snap, also_destroy_volume=True,
2530                         ignore_errors=True)
2531 
2532         LOG.info("Snapshot image upload complete", instance=instance)
2533 
2534     def _needs_suspend_resume_for_snapshot(
2535         self,
2536         live_snapshot: bool,
2537         current_power_state: int,
2538     ):
2539         # NOTE(dkang): managedSave does not work for LXC
2540         if CONF.libvirt.virt_type == 'lxc':
2541             return False
2542 
2543         # Live snapshots do not necessitate suspending the domain
2544         if live_snapshot:
2545             return False
2546 
2547         # ...and neither does a non-running domain
2548         return current_power_state in (power_state.RUNNING, power_state.PAUSED)
2549 
2550     def _suspend_guest_for_snapshot(
2551         self,
2552         context: nova_context.RequestContext,
2553         live_snapshot: bool,
2554         current_power_state: int,
2555         instance: 'objects.Instance',
2556     ):
2557         if self._needs_suspend_resume_for_snapshot(
2558             live_snapshot, current_power_state,
2559         ):
2560             self.suspend(context, instance)
2561 
2562     def _resume_guest_after_snapshot(
2563         self,
2564         context: nova_context.RequestContext,
2565         live_snapshot: bool,
2566         original_power_state: int,
2567         instance: 'objects.Instance',
2568         guest: libvirt_guest.Guest,
2569     ):
2570         if not self._needs_suspend_resume_for_snapshot(
2571             live_snapshot, original_power_state,
2572         ):
2573             return
2574 
2575         current_power_state = guest.get_power_state(self._host)
2576 
2577         # TODO(stephenfin): Any reason we couldn't use 'self.resume' here?
2578         guest.launch(pause=current_power_state == power_state.PAUSED)
2579 
2580         self._attach_pci_devices(
2581             guest, pci_manager.get_instance_pci_devs(instance))
2582         self._attach_direct_passthrough_ports(context, instance, guest)
2583 
2584     def _can_set_admin_password(self, image_meta):
2585 
2586         if CONF.libvirt.virt_type in ('kvm', 'qemu'):
2587             if not image_meta.properties.get('hw_qemu_guest_agent', False):
2588                 raise exception.QemuGuestAgentNotEnabled()
2589         elif not CONF.libvirt.virt_type == 'parallels':
2590             raise exception.SetAdminPasswdNotSupported()
2591 
2592     def _save_instance_password_if_sshkey_present(self, instance, new_pass):
2593         sshkey = instance.key_data if 'key_data' in instance else None
2594         if sshkey and sshkey.startswith("ssh-rsa"):
2595             enc = crypto.ssh_encrypt_text(sshkey, new_pass)
2596             # NOTE(melwitt): The convert_password method doesn't actually do
2597             # anything with the context argument, so we can pass None.
2598             instance.system_metadata.update(
2599                 password.convert_password(None, base64.encode_as_text(enc)))
2600             instance.save()
2601 
2602     def set_admin_password(self, instance, new_pass):
2603         self._can_set_admin_password(instance.image_meta)
2604 
2605         guest = self._host.get_guest(instance)
2606         user = instance.image_meta.properties.get("os_admin_user")
2607         if not user:
2608             if instance.os_type == "windows":
2609                 user = "Administrator"
2610             else:
2611                 user = "root"
2612         try:
2613             guest.set_user_password(user, new_pass)
2614         except libvirt.libvirtError as ex:
2615             error_code = ex.get_error_code()
2616             if error_code == libvirt.VIR_ERR_AGENT_UNRESPONSIVE:
2617                 LOG.debug('Failed to set password: QEMU agent unresponsive',
2618                           instance_uuid=instance.uuid)
2619                 raise NotImplementedError()
2620 
2621             err_msg = encodeutils.exception_to_unicode(ex)
2622             msg = (_('Error from libvirt while set password for username '
2623                      '"%(user)s": [Error Code %(error_code)s] %(ex)s')
2624                    % {'user': user, 'error_code': error_code, 'ex': err_msg})
2625             raise exception.InternalError(msg)
2626         else:
2627             # Save the password in sysmeta so it may be retrieved from the
2628             # metadata service.
2629             self._save_instance_password_if_sshkey_present(instance, new_pass)
2630 
2631     def _can_quiesce(self, instance, image_meta):
2632         if CONF.libvirt.virt_type not in ('kvm', 'qemu'):
2633             raise exception.InstanceQuiesceNotSupported(
2634                 instance_id=instance.uuid)
2635 
2636         if not image_meta.properties.get('hw_qemu_guest_agent', False):
2637             raise exception.QemuGuestAgentNotEnabled()
2638 
2639     def _requires_quiesce(self, image_meta):
2640         return image_meta.properties.get('os_require_quiesce', False)
2641 
2642     def _set_quiesced(self, context, instance, image_meta, quiesced):
2643         self._can_quiesce(instance, image_meta)
2644         try:
2645             guest = self._host.get_guest(instance)
2646             if quiesced:
2647                 guest.freeze_filesystems()
2648             else:
2649                 guest.thaw_filesystems()
2650         except libvirt.libvirtError as ex:
2651             error_code = ex.get_error_code()
2652             err_msg = encodeutils.exception_to_unicode(ex)
2653             msg = (_('Error from libvirt while quiescing %(instance_name)s: '
2654                      '[Error Code %(error_code)s] %(ex)s')
2655                    % {'instance_name': instance.name,
2656                       'error_code': error_code, 'ex': err_msg})
2657             raise exception.InternalError(msg)
2658 
2659     def quiesce(self, context, instance, image_meta):
2660         """Freeze the guest filesystems to prepare for snapshot.
2661 
2662         The qemu-guest-agent must be setup to execute fsfreeze.
2663         """
2664         self._set_quiesced(context, instance, image_meta, True)
2665 
2666     def unquiesce(self, context, instance, image_meta):
2667         """Thaw the guest filesystems after snapshot."""
2668         self._set_quiesced(context, instance, image_meta, False)
2669 
2670     def _live_snapshot(self, context, instance, guest, disk_path, out_path,
2671                        source_format, image_format, image_meta):
2672         """Snapshot an instance without downtime."""
2673         dev = guest.get_block_device(disk_path)
2674 
2675         # Save a copy of the domain's persistent XML file
2676         xml = guest.get_xml_desc(dump_inactive=True, dump_sensitive=True)
2677 
2678         # Abort is an idempotent operation, so make sure any block
2679         # jobs which may have failed are ended.
2680         try:
2681             dev.abort_job()
2682         except Exception:
2683             pass
2684 
2685         # NOTE (rmk): We are using shallow rebases as a workaround to a bug
2686         #             in QEMU 1.3. In order to do this, we need to create
2687         #             a destination image with the original backing file
2688         #             and matching size of the instance root disk.
2689         src_disk_size = libvirt_utils.get_disk_size(disk_path,
2690                                                     format=source_format)
2691         src_back_path = libvirt_utils.get_disk_backing_file(disk_path,
2692                                                         format=source_format,
2693                                                         basename=False)
2694         disk_delta = out_path + '.delta'
2695         libvirt_utils.create_cow_image(src_back_path, disk_delta,
2696                                        src_disk_size)
2697 
2698         quiesced = False
2699         try:
2700             self._set_quiesced(context, instance, image_meta, True)
2701             quiesced = True
2702         except exception.NovaException as err:
2703             if self._requires_quiesce(image_meta):
2704                 raise
2705             LOG.info('Skipping quiescing instance: %(reason)s.',
2706                      {'reason': err}, instance=instance)
2707 
2708         try:
2709             # NOTE (rmk): blockRebase cannot be executed on persistent
2710             #             domains, so we need to temporarily undefine it.
2711             #             If any part of this block fails, the domain is
2712             #             re-defined regardless.
2713             if guest.has_persistent_configuration():
2714                 hw_firmware_type = image_meta.properties.get(
2715                     'hw_firmware_type')
2716                 support_uefi = self._check_uefi_support(hw_firmware_type)
2717                 guest.delete_configuration(support_uefi)
2718 
2719             # NOTE (rmk): Establish a temporary mirror of our root disk and
2720             #             issue an abort once we have a complete copy.
2721             dev.rebase(disk_delta, copy=True, reuse_ext=True, shallow=True)
2722 
2723             while not dev.is_job_complete():
2724                 time.sleep(0.5)
2725 
2726             dev.abort_job()
2727             nova.privsep.path.chown(disk_delta, uid=os.getuid())
2728         finally:
2729             self._host.write_instance_config(xml)
2730             if quiesced:
2731                 self._set_quiesced(context, instance, image_meta, False)
2732 
2733         # Convert the delta (CoW) image with a backing file to a flat
2734         # image with no backing file.
2735         libvirt_utils.extract_snapshot(disk_delta, 'qcow2',
2736                                        out_path, image_format)
2737 
2738         # Remove the disk_delta file once the snapshot extracted, so that
2739         # it doesn't hang around till the snapshot gets uploaded
2740         fileutils.delete_if_exists(disk_delta)
2741 
2742     def _volume_snapshot_update_status(self, context, snapshot_id, status):
2743         """Send a snapshot status update to Cinder.
2744 
2745         This method captures and logs exceptions that occur
2746         since callers cannot do anything useful with these exceptions.
2747 
2748         Operations on the Cinder side waiting for this will time out if
2749         a failure occurs sending the update.
2750 
2751         :param context: security context
2752         :param snapshot_id: id of snapshot being updated
2753         :param status: new status value
2754 
2755         """
2756 
2757         try:
2758             self._volume_api.update_snapshot_status(context,
2759                                                     snapshot_id,
2760                                                     status)
2761         except Exception:
2762             LOG.exception('Failed to send updated snapshot status '
2763                           'to volume service.')
2764 
2765     def _volume_snapshot_create(self, context, instance, guest,
2766                                 volume_id, new_file):
2767         """Perform volume snapshot.
2768 
2769            :param guest: VM that volume is attached to
2770            :param volume_id: volume UUID to snapshot
2771            :param new_file: relative path to new qcow2 file present on share
2772 
2773         """
2774         xml = guest.get_xml_desc()
2775         xml_doc = etree.fromstring(xml)
2776 
2777         device_info = vconfig.LibvirtConfigGuest()
2778         device_info.parse_dom(xml_doc)
2779 
2780         disks_to_snap = []          # to be snapshotted by libvirt
2781         network_disks_to_snap = []  # network disks (netfs, etc.)
2782         disks_to_skip = []          # local disks not snapshotted
2783 
2784         for guest_disk in device_info.devices:
2785             if (guest_disk.root_name != 'disk'):
2786                 continue
2787 
2788             if (guest_disk.target_dev is None):
2789                 continue
2790 
2791             if (guest_disk.serial is None or guest_disk.serial != volume_id):
2792                 disks_to_skip.append(guest_disk.target_dev)
2793                 continue
2794 
2795             # disk is a Cinder volume with the correct volume_id
2796 
2797             disk_info = {
2798                 'dev': guest_disk.target_dev,
2799                 'serial': guest_disk.serial,
2800                 'current_file': guest_disk.source_path,
2801                 'source_protocol': guest_disk.source_protocol,
2802                 'source_name': guest_disk.source_name,
2803                 'source_hosts': guest_disk.source_hosts,
2804                 'source_ports': guest_disk.source_ports
2805             }
2806 
2807             # Determine path for new_file based on current path
2808             if disk_info['current_file'] is not None:
2809                 current_file = disk_info['current_file']
2810                 new_file_path = os.path.join(os.path.dirname(current_file),
2811                                              new_file)
2812                 disks_to_snap.append((current_file, new_file_path))
2813             # NOTE(mriedem): This used to include a check for gluster in
2814             # addition to netfs since they were added together. Support for
2815             # gluster was removed in the 16.0.0 Pike release. It is unclear,
2816             # however, if other volume drivers rely on the netfs disk source
2817             # protocol.
2818             elif disk_info['source_protocol'] == 'netfs':
2819                 network_disks_to_snap.append((disk_info, new_file))
2820 
2821         if not disks_to_snap and not network_disks_to_snap:
2822             msg = _('Found no disk to snapshot.')
2823             raise exception.InternalError(msg)
2824 
2825         snapshot = vconfig.LibvirtConfigGuestSnapshot()
2826 
2827         for current_name, new_filename in disks_to_snap:
2828             snap_disk = vconfig.LibvirtConfigGuestSnapshotDisk()
2829             snap_disk.name = current_name
2830             snap_disk.source_path = new_filename
2831             snap_disk.source_type = 'file'
2832             snap_disk.snapshot = 'external'
2833             snap_disk.driver_name = 'qcow2'
2834 
2835             snapshot.add_disk(snap_disk)
2836 
2837         for disk_info, new_filename in network_disks_to_snap:
2838             snap_disk = vconfig.LibvirtConfigGuestSnapshotDisk()
2839             snap_disk.name = disk_info['dev']
2840             snap_disk.source_type = 'network'
2841             snap_disk.source_protocol = disk_info['source_protocol']
2842             snap_disk.snapshot = 'external'
2843             snap_disk.source_path = new_filename
2844             old_dir = disk_info['source_name'].split('/')[0]
2845             snap_disk.source_name = '%s/%s' % (old_dir, new_filename)
2846             snap_disk.source_hosts = disk_info['source_hosts']
2847             snap_disk.source_ports = disk_info['source_ports']
2848 
2849             snapshot.add_disk(snap_disk)
2850 
2851         for dev in disks_to_skip:
2852             snap_disk = vconfig.LibvirtConfigGuestSnapshotDisk()
2853             snap_disk.name = dev
2854             snap_disk.snapshot = 'no'
2855 
2856             snapshot.add_disk(snap_disk)
2857 
2858         snapshot_xml = snapshot.to_xml()
2859         LOG.debug("snap xml: %s", snapshot_xml, instance=instance)
2860 
2861         image_meta = instance.image_meta
2862         try:
2863             # Check to see if we can quiesce the guest before taking the
2864             # snapshot.
2865             self._can_quiesce(instance, image_meta)
2866             try:
2867                 guest.snapshot(snapshot, no_metadata=True, disk_only=True,
2868                                reuse_ext=True, quiesce=True)
2869                 return
2870             except libvirt.libvirtError:
2871                 # If the image says that quiesce is required then we fail.
2872                 if self._requires_quiesce(image_meta):
2873                     raise
2874                 LOG.exception('Unable to create quiesced VM snapshot, '
2875                               'attempting again with quiescing disabled.',
2876                               instance=instance)
2877         except (exception.InstanceQuiesceNotSupported,
2878                 exception.QemuGuestAgentNotEnabled) as err:
2879             # If the image says that quiesce is required then we need to fail.
2880             if self._requires_quiesce(image_meta):
2881                 raise
2882             LOG.info('Skipping quiescing instance: %(reason)s.',
2883                      {'reason': err}, instance=instance)
2884 
2885         try:
2886             guest.snapshot(snapshot, no_metadata=True, disk_only=True,
2887                            reuse_ext=True, quiesce=False)
2888         except libvirt.libvirtError:
2889             LOG.exception('Unable to create VM snapshot, '
2890                           'failing volume_snapshot operation.',
2891                           instance=instance)
2892 
2893             raise
2894 
2895     def _volume_refresh_connection_info(self, context, instance, volume_id):
2896         bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
2897                   context, volume_id, instance.uuid)
2898 
2899         driver_bdm = driver_block_device.convert_volume(bdm)
2900         if driver_bdm:
2901             driver_bdm.refresh_connection_info(context, instance,
2902                                                self._volume_api, self)
2903 
2904     def volume_snapshot_create(self, context, instance, volume_id,
2905                                create_info):
2906         """Create snapshots of a Cinder volume via libvirt.
2907 
2908         :param instance: VM instance object reference
2909         :param volume_id: id of volume being snapshotted
2910         :param create_info: dict of information used to create snapshots
2911                      - snapshot_id : ID of snapshot
2912                      - type : qcow2 / <other>
2913                      - new_file : qcow2 file created by Cinder which
2914                      becomes the VM's active image after
2915                      the snapshot is complete
2916         """
2917 
2918         LOG.debug("volume_snapshot_create: create_info: %(c_info)s",
2919                   {'c_info': create_info}, instance=instance)
2920 
2921         try:
2922             guest = self._host.get_guest(instance)
2923         except exception.InstanceNotFound:
2924             raise exception.InstanceNotRunning(instance_id=instance.uuid)
2925 
2926         if create_info['type'] != 'qcow2':
2927             msg = _('Unknown type: %s') % create_info['type']
2928             raise exception.InternalError(msg)
2929 
2930         snapshot_id = create_info.get('snapshot_id', None)
2931         if snapshot_id is None:
2932             msg = _('snapshot_id required in create_info')
2933             raise exception.InternalError(msg)
2934 
2935         try:
2936             self._volume_snapshot_create(context, instance, guest,
2937                                          volume_id, create_info['new_file'])
2938         except Exception:
2939             with excutils.save_and_reraise_exception():
2940                 LOG.exception('Error occurred during volume_snapshot_create, '
2941                               'sending error status to Cinder.',
2942                               instance=instance)
2943                 self._volume_snapshot_update_status(
2944                     context, snapshot_id, 'error')
2945 
2946         self._volume_snapshot_update_status(
2947             context, snapshot_id, 'creating')
2948 
2949         def _wait_for_snapshot():
2950             snapshot = self._volume_api.get_snapshot(context, snapshot_id)
2951 
2952             if snapshot.get('status') != 'creating':
2953                 self._volume_refresh_connection_info(context, instance,
2954                                                      volume_id)
2955                 raise loopingcall.LoopingCallDone()
2956 
2957         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_snapshot)
2958         timer.start(interval=0.5).wait()
2959 
2960     @staticmethod
2961     def _rebase_with_qemu_img(source_path, rebase_base):
2962         """Rebase a disk using qemu-img.
2963 
2964         :param source_path: the disk source path to rebase
2965         :type source_path: string
2966         :param rebase_base: the new parent in the backing chain
2967         :type rebase_base: None or string
2968         """
2969 
2970         if rebase_base is None:
2971             # If backing_file is specified as "" (the empty string), then
2972             # the image is rebased onto no backing file (i.e. it will exist
2973             # independently of any backing file).
2974             backing_file = ""
2975             qemu_img_extra_arg = []
2976         else:
2977             # If the rebased image is going to have a backing file then
2978             # explicitly set the backing file format to avoid any security
2979             # concerns related to file format auto detection.
2980             if os.path.isabs(rebase_base):
2981                 backing_file = rebase_base
2982             else:
2983                 # this is a probably a volume snapshot case where the
2984                 # rebase_base is relative. See bug
2985                 # https://bugs.launchpad.net/nova/+bug/1885528
2986                 backing_file_name = os.path.basename(rebase_base)
2987                 volume_path = os.path.dirname(source_path)
2988                 backing_file = os.path.join(volume_path, backing_file_name)
2989 
2990             b_file_fmt = images.qemu_img_info(backing_file).file_format
2991             qemu_img_extra_arg = ['-F', b_file_fmt]
2992 
2993         qemu_img_extra_arg.append(source_path)
2994         # execute operation with disk concurrency semaphore
2995         with compute_utils.disk_ops_semaphore:
2996             processutils.execute("qemu-img", "rebase", "-b", backing_file,
2997                                  *qemu_img_extra_arg)
2998 
2999     def _volume_snapshot_delete(self, context, instance, volume_id,
3000                                 snapshot_id, delete_info=None):
3001         """Note:
3002             if file being merged into == active image:
3003                 do a blockRebase (pull) operation
3004             else:
3005                 do a blockCommit operation
3006             Files must be adjacent in snap chain.
3007 
3008         :param instance: instance object reference
3009         :param volume_id: volume UUID
3010         :param snapshot_id: snapshot UUID (unused currently)
3011         :param delete_info: {
3012             'type':              'qcow2',
3013             'file_to_merge':     'a.img',
3014             'merge_target_file': 'b.img' or None (if merging file_to_merge into
3015                                                   active image)
3016           }
3017         """
3018 
3019         LOG.debug('volume_snapshot_delete: delete_info: %s', delete_info,
3020                   instance=instance)
3021 
3022         if delete_info['type'] != 'qcow2':
3023             msg = _('Unknown delete_info type %s') % delete_info['type']
3024             raise exception.InternalError(msg)
3025 
3026         try:
3027             guest = self._host.get_guest(instance)
3028         except exception.InstanceNotFound:
3029             raise exception.InstanceNotRunning(instance_id=instance.uuid)
3030 
3031         # Find dev name
3032         xml = guest.get_xml_desc()
3033         xml_doc = etree.fromstring(xml)
3034 
3035         device_info = vconfig.LibvirtConfigGuest()
3036         device_info.parse_dom(xml_doc)
3037 
3038         for guest_disk in device_info.devices:
3039             if (guest_disk.root_name != 'disk'):
3040                 continue
3041 
3042             if (guest_disk.target_dev is None or guest_disk.serial is None):
3043                 continue
3044 
3045             if (
3046                 guest_disk.source_path is None and
3047                 guest_disk.source_protocol is None
3048             ):
3049                 continue
3050 
3051             if guest_disk.serial == volume_id:
3052                 my_dev = guest_disk.target_dev
3053 
3054                 active_protocol = guest_disk.source_protocol
3055                 active_disk_object = guest_disk
3056                 break
3057         else:
3058             LOG.debug('Domain XML: %s', xml, instance=instance)
3059             msg = (_("Disk with id '%s' not found attached to instance.")
3060                    % volume_id)
3061             raise exception.InternalError(msg)
3062 
3063         LOG.debug("found device at %s", my_dev, instance=instance)
3064 
3065         def _get_snap_dev(filename, backing_store):
3066             if filename is None:
3067                 msg = _('filename cannot be None')
3068                 raise exception.InternalError(msg)
3069 
3070             # libgfapi delete
3071             LOG.debug("XML: %s", xml)
3072 
3073             LOG.debug("active disk object: %s", active_disk_object)
3074 
3075             # determine reference within backing store for desired image
3076             filename_to_merge = filename
3077             matched_name = None
3078             b = backing_store
3079             index = None
3080 
3081             current_filename = active_disk_object.source_name.split('/')[1]
3082             if current_filename == filename_to_merge:
3083                 return my_dev + '[0]'
3084 
3085             while b is not None:
3086                 source_filename = b.source_name.split('/')[1]
3087                 if source_filename == filename_to_merge:
3088                     LOG.debug('found match: %s', b.source_name)
3089                     matched_name = b.source_name
3090                     index = b.index
3091                     break
3092 
3093                 b = b.backing_store
3094 
3095             if matched_name is None:
3096                 msg = _('no match found for %s') % (filename_to_merge)
3097                 raise exception.InternalError(msg)
3098 
3099             LOG.debug('index of match (%s) is %s', b.source_name, index)
3100 
3101             my_snap_dev = '%s[%s]' % (my_dev, index)
3102             return my_snap_dev
3103 
3104         if delete_info['merge_target_file'] is None:
3105             # pull via blockRebase()
3106 
3107             # Merge the most recent snapshot into the active image
3108 
3109             rebase_disk = my_dev
3110             rebase_base = delete_info['file_to_merge']  # often None
3111             if (active_protocol is not None) and (rebase_base is not None):
3112                 rebase_base = _get_snap_dev(rebase_base,
3113                                             active_disk_object.backing_store)
3114 
3115             relative = rebase_base is not None
3116             LOG.debug(
3117                 'disk: %(disk)s, base: %(base)s, '
3118                 'bw: %(bw)s, relative: %(relative)s',
3119                 {'disk': rebase_disk,
3120                  'base': rebase_base,
3121                  'bw': libvirt_guest.BlockDevice.REBASE_DEFAULT_BANDWIDTH,
3122                  'relative': str(relative)}, instance=instance)
3123 
3124             dev = guest.get_block_device(rebase_disk)
3125             if guest.is_active():
3126                 result = dev.rebase(rebase_base, relative=relative)
3127                 if result == 0:
3128                     LOG.debug('blockRebase started successfully',
3129                               instance=instance)
3130 
3131                 while not dev.is_job_complete():
3132                     LOG.debug('waiting for blockRebase job completion',
3133                               instance=instance)
3134                     time.sleep(0.5)
3135 
3136             # If the guest is not running libvirt won't do a blockRebase.
3137             # In that case, let's ask qemu-img to rebase the disk.
3138             else:
3139                 LOG.debug('Guest is not running so doing a block rebase '
3140                           'using "qemu-img rebase"', instance=instance)
3141 
3142                 # It's unsure how well qemu-img handles network disks for
3143                 # every protocol. So let's be safe.
3144                 active_protocol = active_disk_object.source_protocol
3145                 if active_protocol is not None:
3146                     msg = _("Something went wrong when deleting a volume "
3147                             "snapshot: rebasing a %(protocol)s network disk "
3148                             "using qemu-img has not been fully tested"
3149                            ) % {'protocol': active_protocol}
3150                     LOG.error(msg)
3151                     raise exception.InternalError(msg)
3152                 self._rebase_with_qemu_img(active_disk_object.source_path,
3153                                            rebase_base)
3154 
3155         else:
3156             # commit with blockCommit()
3157             my_snap_base = None
3158             my_snap_top = None
3159             commit_disk = my_dev
3160 
3161             if active_protocol is not None:
3162                 my_snap_base = _get_snap_dev(delete_info['merge_target_file'],
3163                                              active_disk_object.backing_store)
3164                 my_snap_top = _get_snap_dev(delete_info['file_to_merge'],
3165                                             active_disk_object.backing_store)
3166 
3167             commit_base = my_snap_base or delete_info['merge_target_file']
3168             commit_top = my_snap_top or delete_info['file_to_merge']
3169 
3170             LOG.debug('will call blockCommit with commit_disk=%(commit_disk)s '
3171                       'commit_base=%(commit_base)s '
3172                       'commit_top=%(commit_top)s ',
3173                       {'commit_disk': commit_disk,
3174                        'commit_base': commit_base,
3175                        'commit_top': commit_top}, instance=instance)
3176 
3177             dev = guest.get_block_device(commit_disk)
3178             result = dev.commit(commit_base, commit_top, relative=True)
3179 
3180             if result == 0:
3181                 LOG.debug('blockCommit started successfully',
3182                           instance=instance)
3183 
3184             while not dev.is_job_complete():
3185                 LOG.debug('waiting for blockCommit job completion',
3186                           instance=instance)
3187                 time.sleep(0.5)
3188 
3189     def volume_snapshot_delete(self, context, instance, volume_id, snapshot_id,
3190                                delete_info):
3191         try:
3192             self._volume_snapshot_delete(context, instance, volume_id,
3193                                          snapshot_id, delete_info=delete_info)
3194         except Exception:
3195             with excutils.save_and_reraise_exception():
3196                 LOG.exception('Error occurred during volume_snapshot_delete, '
3197                               'sending error status to Cinder.',
3198                               instance=instance)
3199                 self._volume_snapshot_update_status(
3200                     context, snapshot_id, 'error_deleting')
3201 
3202         self._volume_snapshot_update_status(context, snapshot_id, 'deleting')
3203         self._volume_refresh_connection_info(context, instance, volume_id)
3204 
3205     def reboot(self, context, instance, network_info, reboot_type,
3206                block_device_info=None, bad_volumes_callback=None,
3207                accel_info=None):
3208         """Reboot a virtual machine, given an instance reference."""
3209         if reboot_type == 'SOFT':
3210             # NOTE(vish): This will attempt to do a graceful shutdown/restart.
3211             try:
3212                 soft_reboot_success = self._soft_reboot(instance)
3213             except libvirt.libvirtError as e:
3214                 LOG.debug("Instance soft reboot failed: %s",
3215                           encodeutils.exception_to_unicode(e),
3216                           instance=instance)
3217                 soft_reboot_success = False
3218 
3219             if soft_reboot_success:
3220                 LOG.info("Instance soft rebooted successfully.",
3221                          instance=instance)
3222                 return
3223             else:
3224                 LOG.warning("Failed to soft reboot instance. "
3225                             "Trying hard reboot.",
3226                             instance=instance)
3227         return self._hard_reboot(context, instance, network_info,
3228                                  block_device_info, accel_info)
3229 
3230     def _soft_reboot(self, instance):
3231         """Attempt to shutdown and restart the instance gracefully.
3232 
3233         We use shutdown and create here so we can return if the guest
3234         responded and actually rebooted. Note that this method only
3235         succeeds if the guest responds to acpi. Therefore we return
3236         success or failure so we can fall back to a hard reboot if
3237         necessary.
3238 
3239         :returns: True if the reboot succeeded
3240         """
3241         guest = self._host.get_guest(instance)
3242 
3243         state = guest.get_power_state(self._host)
3244         old_domid = guest.id
3245         # NOTE(vish): This check allows us to reboot an instance that
3246         #             is already shutdown.
3247         if state == power_state.RUNNING:
3248             guest.shutdown()
3249         # NOTE(vish): This actually could take slightly longer than the
3250         #             FLAG defines depending on how long the get_info
3251         #             call takes to return.
3252         self._prepare_pci_devices_for_use(
3253             pci_manager.get_instance_pci_devs(instance, 'all'))
3254         for x in range(CONF.libvirt.wait_soft_reboot_seconds):
3255             guest = self._host.get_guest(instance)
3256 
3257             state = guest.get_power_state(self._host)
3258             new_domid = guest.id
3259 
3260             # NOTE(ivoks): By checking domain IDs, we make sure we are
3261             #              not recreating domain that's already running.
3262             if old_domid != new_domid:
3263                 if state in (power_state.SHUTDOWN, power_state.CRASHED):
3264                     LOG.info("Instance shutdown successfully.",
3265                              instance=instance)
3266                     guest.launch()
3267                     timer = loopingcall.FixedIntervalLoopingCall(
3268                         self._wait_for_running, instance)
3269                     timer.start(interval=0.5).wait()
3270                     return True
3271                 else:
3272                     LOG.info("Instance may have been rebooted during soft "
3273                              "reboot, so return now.", instance=instance)
3274                     return True
3275             greenthread.sleep(1)
3276         return False
3277 
3278     def _hard_reboot(self, context, instance, network_info,
3279                      block_device_info=None, accel_info=None):
3280         """Reboot a virtual machine, given an instance reference.
3281 
3282         Performs a Libvirt reset (if supported) on the domain.
3283 
3284         If Libvirt reset is unavailable this method actually destroys and
3285         re-creates the domain to ensure the reboot happens, as the guest
3286         OS cannot ignore this action.
3287         """
3288         # NOTE(sbauza): Since we undefine the guest XML when destroying, we
3289         # need to remember the existing mdevs for reusing them.
3290         mdevs = self._get_all_assigned_mediated_devices(instance)
3291         mdevs = list(mdevs.keys())
3292         # NOTE(mdbooth): In addition to performing a hard reboot of the domain,
3293         # the hard reboot operation is relied upon by operators to be an
3294         # automated attempt to fix as many things as possible about a
3295         # non-functioning instance before resorting to manual intervention.
3296         # With this goal in mind, we tear down all the aspects of an instance
3297         # we can here without losing data. This allows us to re-initialise from
3298         # scratch, and hopefully fix, most aspects of a non-functioning guest.
3299         self.destroy(context, instance, network_info, destroy_disks=False,
3300                      block_device_info=block_device_info)
3301 
3302         # Convert the system metadata to image metadata
3303         # NOTE(mdbooth): This is a workaround for stateless Nova compute
3304         #                https://bugs.launchpad.net/nova/+bug/1349978
3305         instance_dir = libvirt_utils.get_instance_path(instance)
3306         fileutils.ensure_tree(instance_dir)
3307 
3308         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
3309                                             instance,
3310                                             instance.image_meta,
3311                                             block_device_info)
3312         # NOTE(vish): This could generate the wrong device_format if we are
3313         #             using the raw backend and the images don't exist yet.
3314         #             The create_images_and_backing below doesn't properly
3315         #             regenerate raw backend images, however, so when it
3316         #             does we need to (re)generate the xml after the images
3317         #             are in place.
3318         xml = self._get_guest_xml(context, instance, network_info, disk_info,
3319                                   instance.image_meta,
3320                                   block_device_info=block_device_info,
3321                                   mdevs=mdevs, accel_info=accel_info)
3322 
3323         # NOTE(mdbooth): context.auth_token will not be set when we call
3324         #                _hard_reboot from resume_state_on_host_boot()
3325         if context.auth_token is not None:
3326             # NOTE (rmk): Re-populate any missing backing files.
3327             config = vconfig.LibvirtConfigGuest()
3328             config.parse_str(xml)
3329             backing_disk_info = self._get_instance_disk_info_from_config(
3330                 config, block_device_info)
3331             self._create_images_and_backing(context, instance, instance_dir,
3332                                             backing_disk_info)
3333 
3334         # Initialize all the necessary networking, block devices and
3335         # start the instance.
3336         # NOTE(melwitt): Pass vifs_already_plugged=True here even though we've
3337         # unplugged vifs earlier. The behavior of neutron plug events depends
3338         # on which vif type we're using and we are working with a stale network
3339         # info cache here, so won't rely on waiting for neutron plug events.
3340         # vifs_already_plugged=True means "do not wait for neutron plug events"
3341         # NOTE(efried): The instance should already have a vtpm_secret_uuid
3342         # registered if appropriate.
3343         self._create_guest_with_network(
3344             context, xml, instance, network_info, block_device_info,
3345             vifs_already_plugged=True)
3346         self._prepare_pci_devices_for_use(
3347             pci_manager.get_instance_pci_devs(instance, 'all'))
3348 
3349         def _wait_for_reboot():
3350             """Called at an interval until the VM is running again."""
3351             state = self.get_info(instance).state
3352 
3353             if state == power_state.RUNNING:
3354                 LOG.info("Instance rebooted successfully.",
3355                          instance=instance)
3356                 raise loopingcall.LoopingCallDone()
3357 
3358         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_reboot)
3359         timer.start(interval=0.5).wait()
3360 
3361     def pause(self, instance):
3362         """Pause VM instance."""
3363         self._host.get_guest(instance).pause()
3364 
3365     def unpause(self, instance):
3366         """Unpause paused VM instance."""
3367         guest = self._host.get_guest(instance)
3368         guest.resume()
3369         guest.sync_guest_time()
3370 
3371     def _clean_shutdown(self, instance, timeout, retry_interval):
3372         """Attempt to shutdown the instance gracefully.
3373 
3374         :param instance: The instance to be shutdown
3375         :param timeout: How long to wait in seconds for the instance to
3376                         shutdown
3377         :param retry_interval: How often in seconds to signal the instance
3378                                to shutdown while waiting
3379 
3380         :returns: True if the shutdown succeeded
3381         """
3382 
3383         # List of states that represent a shutdown instance
3384         SHUTDOWN_STATES = [power_state.SHUTDOWN,
3385                            power_state.CRASHED]
3386 
3387         try:
3388             guest = self._host.get_guest(instance)
3389         except exception.InstanceNotFound:
3390             # If the instance has gone then we don't need to
3391             # wait for it to shutdown
3392             return True
3393 
3394         state = guest.get_power_state(self._host)
3395         if state in SHUTDOWN_STATES:
3396             LOG.info("Instance already shutdown.", instance=instance)
3397             return True
3398 
3399         LOG.debug("Shutting down instance from state %s", state,
3400                   instance=instance)
3401         guest.shutdown()
3402         retry_countdown = retry_interval
3403 
3404         for sec in range(timeout):
3405 
3406             guest = self._host.get_guest(instance)
3407             state = guest.get_power_state(self._host)
3408 
3409             if state in SHUTDOWN_STATES:
3410                 LOG.info("Instance shutdown successfully after %d seconds.",
3411                          sec, instance=instance)
3412                 return True
3413 
3414             # Note(PhilD): We can't assume that the Guest was able to process
3415             #              any previous shutdown signal (for example it may
3416             #              have still been startingup, so within the overall
3417             #              timeout we re-trigger the shutdown every
3418             #              retry_interval
3419             if retry_countdown == 0:
3420                 retry_countdown = retry_interval
3421                 # Instance could shutdown at any time, in which case we
3422                 # will get an exception when we call shutdown
3423                 try:
3424                     LOG.debug("Instance in state %s after %d seconds - "
3425                               "resending shutdown", state, sec,
3426                               instance=instance)
3427                     guest.shutdown()
3428                 except libvirt.libvirtError:
3429                     # Assume this is because its now shutdown, so loop
3430                     # one more time to clean up.
3431                     LOG.debug("Ignoring libvirt exception from shutdown "
3432                               "request.", instance=instance)
3433                     continue
3434             else:
3435                 retry_countdown -= 1
3436 
3437             time.sleep(1)
3438 
3439         LOG.info("Instance failed to shutdown in %d seconds.",
3440                  timeout, instance=instance)
3441         return False
3442 
3443     def power_off(self, instance, timeout=0, retry_interval=0):
3444         """Power off the specified instance."""
3445         if timeout:
3446             self._clean_shutdown(instance, timeout, retry_interval)
3447         self._destroy(instance)
3448 
3449     def power_on(self, context, instance, network_info,
3450                  block_device_info=None, accel_info=None):
3451         """Power on the specified instance."""
3452         # We use _hard_reboot here to ensure that all backing files,
3453         # network, and block device connections, etc. are established
3454         # and available before we attempt to start the instance.
3455         self._hard_reboot(context, instance, network_info, block_device_info,
3456                           accel_info)
3457 
3458     def trigger_crash_dump(self, instance):
3459 
3460         """Trigger crash dump by injecting an NMI to the specified instance."""
3461         try:
3462             self._host.get_guest(instance).inject_nmi()
3463         except libvirt.libvirtError as ex:
3464             error_code = ex.get_error_code()
3465 
3466             if error_code == libvirt.VIR_ERR_NO_SUPPORT:
3467                 raise exception.TriggerCrashDumpNotSupported()
3468             elif error_code == libvirt.VIR_ERR_OPERATION_INVALID:
3469                 raise exception.InstanceNotRunning(instance_id=instance.uuid)
3470 
3471             LOG.exception(
3472                 'Error from libvirt while injecting an NMI to '
3473                 '%(instance_uuid)s: [Error Code %(error_code)s] %(ex)s',
3474                 {'instance_uuid': instance.uuid,
3475                  'error_code': error_code, 'ex': ex})
3476             raise
3477 
3478     def suspend(self, context, instance):
3479         """Suspend the specified instance."""
3480         guest = self._host.get_guest(instance)
3481 
3482         self._detach_pci_devices(guest,
3483             pci_manager.get_instance_pci_devs(instance))
3484         self._detach_direct_passthrough_ports(context, instance, guest)
3485         self._detach_mediated_devices(guest)
3486         guest.save_memory_state()
3487 
3488     def resume(self, context, instance, network_info, block_device_info=None):
3489         """resume the specified instance."""
3490         xml = self._get_existing_domain_xml(instance, network_info,
3491                                             block_device_info)
3492         # NOTE(efried): The instance should already have a vtpm_secret_uuid
3493         # registered if appropriate.
3494         guest = self._create_guest_with_network(
3495             context, xml, instance, network_info, block_device_info,
3496             vifs_already_plugged=True)
3497         self._attach_pci_devices(guest,
3498             pci_manager.get_instance_pci_devs(instance))
3499         self._attach_direct_passthrough_ports(
3500             context, instance, guest, network_info)
3501         timer = loopingcall.FixedIntervalLoopingCall(self._wait_for_running,
3502                                                      instance)
3503         timer.start(interval=0.5).wait()
3504         guest.sync_guest_time()
3505 
3506     def resume_state_on_host_boot(self, context, instance, network_info,
3507                                   block_device_info=None):
3508         """resume guest state when a host is booted."""
3509         # Check if the instance is running already and avoid doing
3510         # anything if it is.
3511         try:
3512             guest = self._host.get_guest(instance)
3513             state = guest.get_power_state(self._host)
3514 
3515             ignored_states = (power_state.RUNNING,
3516                               power_state.SUSPENDED,
3517                               power_state.NOSTATE,
3518                               power_state.PAUSED)
3519 
3520             if state in ignored_states:
3521                 return
3522         except (exception.InternalError, exception.InstanceNotFound):
3523             pass
3524 
3525         # Instance is not up and could be in an unknown state.
3526         # Be as absolute as possible about getting it back into
3527         # a known and running state.
3528         self._hard_reboot(context, instance, network_info, block_device_info)
3529 
3530     def rescue(self, context, instance, network_info, image_meta,
3531                rescue_password, block_device_info):
3532         """Loads a VM using rescue images.
3533 
3534         A rescue is normally performed when something goes wrong with the
3535         primary images and data needs to be corrected/recovered. Rescuing
3536         should not edit or over-ride the original image, only allow for
3537         data recovery.
3538 
3539         Two modes are provided when rescuing an instance with this driver.
3540 
3541         The original and default rescue mode, where the rescue boot disk,
3542         original root disk and optional regenerated config drive are attached
3543         to the instance.
3544 
3545         A second stable device rescue mode is also provided where all of the
3546         original devices are attached to the instance during the rescue attempt
3547         with the addition of the rescue boot disk. This second mode is
3548         controlled by the hw_rescue_device and hw_rescue_bus image properties
3549         on the rescue image provided to this method via image_meta.
3550 
3551         :param nova.context.RequestContext context:
3552             The context for the rescue.
3553         :param nova.objects.instance.Instance instance:
3554             The instance being rescued.
3555         :param nova.network.model.NetworkInfo network_info:
3556             Necessary network information for the resume.
3557         :param nova.objects.ImageMeta image_meta:
3558             The metadata of the image of the instance.
3559         :param rescue_password: new root password to set for rescue.
3560         :param dict block_device_info:
3561             The block device mapping of the instance.
3562         """
3563         instance_dir = libvirt_utils.get_instance_path(instance)
3564         unrescue_xml = self._get_existing_domain_xml(instance, network_info)
3565         unrescue_xml_path = os.path.join(instance_dir, 'unrescue.xml')
3566         with open(unrescue_xml_path, 'w') as f:
3567             f.write(unrescue_xml)
3568 
3569         rescue_image_id = None
3570         rescue_image_meta = None
3571         if image_meta.obj_attr_is_set("id"):
3572             rescue_image_id = image_meta.id
3573 
3574         rescue_images = {
3575             'image_id': (rescue_image_id or
3576                         CONF.libvirt.rescue_image_id or instance.image_ref),
3577             'kernel_id': (CONF.libvirt.rescue_kernel_id or
3578                           instance.kernel_id),
3579             'ramdisk_id': (CONF.libvirt.rescue_ramdisk_id or
3580                            instance.ramdisk_id),
3581         }
3582 
3583         virt_type = CONF.libvirt.virt_type
3584         if hardware.check_hw_rescue_props(image_meta):
3585             LOG.info("Attempting a stable device rescue", instance=instance)
3586             # NOTE(lyarwood): Stable device rescue is not supported when using
3587             # the LXC and Xen virt_types as they do not support the required
3588             # <boot order=''> definitions allowing an instance to boot from the
3589             # rescue device added as a final device to the domain.
3590             if virt_type in ('lxc', 'xen'):
3591                 reason = ("Stable device rescue is not supported by virt_type "
3592                           "%s", virt_type)
3593                 raise exception.InstanceNotRescuable(instance_id=instance.uuid,
3594                                                      reason=reason)
3595             # NOTE(lyarwood): Stable device rescue provides the original disk
3596             # mapping of the instance with the rescue device appened to the
3597             # end. As a result we need to provide the original image_meta, the
3598             # new rescue_image_meta and block_device_info when calling
3599             # get_disk_info.
3600             rescue_image_meta = image_meta
3601             if instance.image_ref:
3602                 image_meta = objects.ImageMeta.from_image_ref(
3603                     context, self._image_api, instance.image_ref)
3604             else:
3605                 # NOTE(lyarwood): If instance.image_ref isn't set attempt to
3606                 # lookup the original image_meta from the bdms. This will
3607                 # return an empty dict if no valid image_meta is found.
3608                 image_meta_dict = block_device.get_bdm_image_metadata(
3609                     context, self._image_api, self._volume_api,
3610                     block_device_info['block_device_mapping'],
3611                     legacy_bdm=False)
3612                 image_meta = objects.ImageMeta.from_dict(image_meta_dict)
3613 
3614         else:
3615             LOG.info("Attempting rescue", instance=instance)
3616             # NOTE(lyarwood): A legacy rescue only provides the rescue device
3617             # and the original root device so we don't need to provide
3618             # block_device_info to the get_disk_info call.
3619             block_device_info = None
3620 
3621         disk_info = blockinfo.get_disk_info(virt_type, instance, image_meta,
3622             rescue=True, block_device_info=block_device_info,
3623             rescue_image_meta=rescue_image_meta)
3624         LOG.debug("rescue generated disk_info: %s", disk_info)
3625 
3626         injection_info = InjectionInfo(network_info=network_info,
3627                                        admin_pass=rescue_password,
3628                                        files=None)
3629         gen_confdrive = functools.partial(self._create_configdrive,
3630                                           context, instance, injection_info,
3631                                           rescue=True)
3632         # NOTE(sbauza): Since rescue recreates the guest XML, we need to
3633         # remember the existing mdevs for reusing them.
3634         mdevs = self._get_all_assigned_mediated_devices(instance)
3635         mdevs = list(mdevs.keys())
3636         self._create_image(context, instance, disk_info['mapping'],
3637                            injection_info=injection_info, suffix='.rescue',
3638                            disk_images=rescue_images)
3639         # NOTE(efried): The instance should already have a vtpm_secret_uuid
3640         # registered if appropriate.
3641         xml = self._get_guest_xml(context, instance, network_info, disk_info,
3642                                   image_meta, rescue=rescue_images,
3643                                   mdevs=mdevs,
3644                                   block_device_info=block_device_info)
3645         self._destroy(instance)
3646         self._create_guest(
3647             context, xml, instance, post_xml_callback=gen_confdrive,
3648         )
3649 
3650     def unrescue(
3651         self,
3652         context: nova_context.RequestContext,
3653         instance: 'objects.Instance',
3654     ):
3655         """Reboot the VM which is being rescued back into primary images."""
3656         instance_dir = libvirt_utils.get_instance_path(instance)
3657         unrescue_xml_path = os.path.join(instance_dir, 'unrescue.xml')
3658         # The xml should already contain the secret_uuid if relevant.
3659         xml = libvirt_utils.load_file(unrescue_xml_path)
3660 
3661         self._destroy(instance)
3662         self._create_guest(context, xml, instance)
3663         os.unlink(unrescue_xml_path)
3664         rescue_files = os.path.join(instance_dir, "*.rescue")
3665         for rescue_file in glob.iglob(rescue_files):
3666             if os.path.isdir(rescue_file):
3667                 shutil.rmtree(rescue_file)
3668             else:
3669                 os.unlink(rescue_file)
3670         # cleanup rescue volume
3671         lvm.remove_volumes([lvmdisk for lvmdisk in self._lvm_disks(instance)
3672                                 if lvmdisk.endswith('.rescue')])
3673         if CONF.libvirt.images_type == 'rbd':
3674             filter_fn = lambda disk: (disk.startswith(instance.uuid) and
3675                                       disk.endswith('.rescue'))
3676             rbd_utils.RBDDriver().cleanup_volumes(filter_fn)
3677 
3678     def poll_rebooting_instances(self, timeout, instances):
3679         pass
3680 
3681     def spawn(self, context, instance, image_meta, injected_files,
3682               admin_password, allocations, network_info=None,
3683               block_device_info=None, power_on=True, accel_info=None):
3684         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
3685                                             instance,
3686                                             image_meta,
3687                                             block_device_info)
3688         injection_info = InjectionInfo(network_info=network_info,
3689                                        files=injected_files,
3690                                        admin_pass=admin_password)
3691         gen_confdrive = functools.partial(self._create_configdrive,
3692                                           context, instance,
3693                                           injection_info)
3694         created_instance_dir, created_disks = self._create_image(
3695                 context, instance, disk_info['mapping'],
3696                 injection_info=injection_info,
3697                 block_device_info=block_device_info)
3698 
3699         # Required by Quobyte CI
3700         self._ensure_console_log_for_instance(instance)
3701 
3702         # Does the guest need to be assigned some vGPU mediated devices ?
3703         mdevs = self._allocate_mdevs(allocations)
3704 
3705         # If the guest needs a vTPM, _get_guest_xml needs its secret to exist
3706         # and its uuid to be registered in the instance prior to _get_guest_xml
3707         if CONF.libvirt.swtpm_enabled and hardware.get_vtpm_constraint(
3708             instance.flavor, image_meta
3709         ):
3710             if not instance.system_metadata.get('vtpm_secret_uuid'):
3711                 # Create the secret via the key manager service so that we have
3712                 # it to hand when generating the XML. This is slightly wasteful
3713                 # as we'll perform a redundant key manager API call later when
3714                 # we create the domain but the alternative is an ugly mess
3715                 crypto.ensure_vtpm_secret(context, instance)
3716 
3717         xml = self._get_guest_xml(context, instance, network_info,
3718                                   disk_info, image_meta,
3719                                   block_device_info=block_device_info,
3720                                   mdevs=mdevs, accel_info=accel_info)
3721         self._create_guest_with_network(
3722             context, xml, instance, network_info, block_device_info,
3723             post_xml_callback=gen_confdrive,
3724             power_on=power_on,
3725             cleanup_instance_dir=created_instance_dir,
3726             cleanup_instance_disks=created_disks)
3727         LOG.debug("Guest created on hypervisor", instance=instance)
3728 
3729         def _wait_for_boot():
3730             """Called at an interval until the VM is running."""
3731             state = self.get_info(instance).state
3732 
3733             if state == power_state.RUNNING:
3734                 LOG.info("Instance spawned successfully.", instance=instance)
3735                 raise loopingcall.LoopingCallDone()
3736 
3737         if power_on:
3738             timer = loopingcall.FixedIntervalLoopingCall(_wait_for_boot)
3739             timer.start(interval=0.5).wait()
3740         else:
3741             LOG.info("Instance spawned successfully.", instance=instance)
3742 
3743     def _get_console_output_file(self, instance, console_log):
3744         bytes_to_read = MAX_CONSOLE_BYTES
3745         log_data = b""  # The last N read bytes
3746         i = 0  # in case there is a log rotation (like "virtlogd")
3747         path = console_log
3748 
3749         while bytes_to_read > 0 and os.path.exists(path):
3750             read_log_data, remaining = nova.privsep.path.last_bytes(
3751                                         path, bytes_to_read)
3752             # We need the log file content in chronological order,
3753             # that's why we *prepend* the log data.
3754             log_data = read_log_data + log_data
3755 
3756             # Prep to read the next file in the chain
3757             bytes_to_read -= len(read_log_data)
3758             path = console_log + "." + str(i)
3759             i += 1
3760 
3761             if remaining > 0:
3762                 LOG.info('Truncated console log returned, '
3763                          '%d bytes ignored', remaining, instance=instance)
3764         return log_data
3765 
3766     def get_console_output(self, context, instance):
3767         guest = self._host.get_guest(instance)
3768 
3769         xml = guest.get_xml_desc()
3770         tree = etree.fromstring(xml)
3771 
3772         # check for different types of consoles
3773         path_sources = [
3774             ('file', "./devices/console[@type='file']/source[@path]", 'path'),
3775             ('tcp', "./devices/console[@type='tcp']/log[@file]", 'file'),
3776             ('pty', "./devices/console[@type='pty']/source[@path]", 'path')]
3777         console_type = ""
3778         console_path = ""
3779         for c_type, epath, attrib in path_sources:
3780             node = tree.find(epath)
3781             if (node is not None) and node.get(attrib):
3782                 console_type = c_type
3783                 console_path = node.get(attrib)
3784                 break
3785 
3786         # instance has no console at all
3787         if not console_path:
3788             raise exception.ConsoleNotAvailable()
3789 
3790         # instance has a console, but file doesn't exist (yet?)
3791         if not os.path.exists(console_path):
3792             LOG.info('console logfile for instance does not exist',
3793                       instance=instance)
3794             return ""
3795 
3796         # pty consoles need special handling
3797         if console_type == 'pty':
3798             console_log = self._get_console_log_path(instance)
3799             data = nova.privsep.libvirt.readpty(console_path)
3800 
3801             # NOTE(markus_z): The virt_types kvm and qemu are the only ones
3802             # which create a dedicated file device for the console logging.
3803             # Other virt_types like xen, lxc, uml, parallels depend on the
3804             # flush of that pty device into the "console.log" file to ensure
3805             # that a series of "get_console_output" calls return the complete
3806             # content even after rebooting a guest.
3807             nova.privsep.path.writefile(console_log, 'a+', data)
3808 
3809             # set console path to logfile, not to pty device
3810             console_path = console_log
3811 
3812         # return logfile content
3813         return self._get_console_output_file(instance, console_path)
3814 
3815     def get_host_ip_addr(self):
3816         return CONF.my_ip
3817 
3818     def get_vnc_console(self, context, instance):
3819         def get_vnc_port_for_instance(instance_name):
3820             guest = self._host.get_guest(instance)
3821 
3822             xml = guest.get_xml_desc()
3823             xml_dom = etree.fromstring(xml)
3824 
3825             graphic = xml_dom.find("./devices/graphics[@type='vnc']")
3826             if graphic is not None:
3827                 return graphic.get('port')
3828             # NOTE(rmk): We had VNC consoles enabled but the instance in
3829             # question is not actually listening for connections.
3830             raise exception.ConsoleTypeUnavailable(console_type='vnc')
3831 
3832         port = get_vnc_port_for_instance(instance.name)
3833         host = CONF.vnc.server_proxyclient_address
3834 
3835         return ctype.ConsoleVNC(host=host, port=port)
3836 
3837     def get_spice_console(self, context, instance):
3838         def get_spice_ports_for_instance(instance_name):
3839             guest = self._host.get_guest(instance)
3840 
3841             xml = guest.get_xml_desc()
3842             xml_dom = etree.fromstring(xml)
3843 
3844             graphic = xml_dom.find("./devices/graphics[@type='spice']")
3845             if graphic is not None:
3846                 return (graphic.get('port'), graphic.get('tlsPort'))
3847             # NOTE(rmk): We had Spice consoles enabled but the instance in
3848             # question is not actually listening for connections.
3849             raise exception.ConsoleTypeUnavailable(console_type='spice')
3850 
3851         ports = get_spice_ports_for_instance(instance.name)
3852         host = CONF.spice.server_proxyclient_address
3853 
3854         return ctype.ConsoleSpice(host=host, port=ports[0], tlsPort=ports[1])
3855 
3856     def get_serial_console(self, context, instance):
3857         guest = self._host.get_guest(instance)
3858         for hostname, port in self._get_serial_ports_from_guest(
3859                 guest, mode='bind'):
3860             return ctype.ConsoleSerial(host=hostname, port=port)
3861         raise exception.ConsoleTypeUnavailable(console_type='serial')
3862 
3863     @staticmethod
3864     def _create_ephemeral(target, ephemeral_size,
3865                           fs_label, os_type, is_block_dev=False,
3866                           context=None, specified_fs=None,
3867                           vm_mode=None):
3868         if not is_block_dev:
3869             if (CONF.libvirt.virt_type == "parallels" and
3870                     vm_mode == fields.VMMode.EXE):
3871 
3872                 libvirt_utils.create_ploop_image('expanded', target,
3873                                                  '%dG' % ephemeral_size,
3874                                                  specified_fs)
3875                 return
3876             libvirt_utils.create_image('raw', target, '%dG' % ephemeral_size)
3877 
3878         # Run as root only for block devices.
3879         disk_api.mkfs(os_type, fs_label, target, run_as_root=is_block_dev,
3880                       specified_fs=specified_fs)
3881 
3882     @staticmethod
3883     def _create_swap(target, swap_mb, context=None):
3884         """Create a swap file of specified size."""
3885         libvirt_utils.create_image('raw', target, '%dM' % swap_mb)
3886         nova.privsep.fs.unprivileged_mkfs('swap', target)
3887 
3888     @staticmethod
3889     def _get_console_log_path(instance):
3890         return os.path.join(libvirt_utils.get_instance_path(instance),
3891                             'console.log')
3892 
3893     def _ensure_console_log_for_instance(self, instance):
3894         # NOTE(mdbooth): Although libvirt will create this file for us
3895         # automatically when it starts, it will initially create it with
3896         # root ownership and then chown it depending on the configuration of
3897         # the domain it is launching. Quobyte CI explicitly disables the
3898         # chown by setting dynamic_ownership=0 in libvirt's config.
3899         # Consequently when the domain starts it is unable to write to its
3900         # console.log. See bug https://bugs.launchpad.net/nova/+bug/1597644
3901         #
3902         # To work around this, we create the file manually before starting
3903         # the domain so it has the same ownership as Nova. This works
3904         # for Quobyte CI because it is also configured to run qemu as the same
3905         # user as the Nova service. Installations which don't set
3906         # dynamic_ownership=0 are not affected because libvirt will always
3907         # correctly configure permissions regardless of initial ownership.
3908         #
3909         # Setting dynamic_ownership=0 is dubious and potentially broken in
3910         # more ways than console.log (see comment #22 on the above bug), so
3911         # Future Maintainer who finds this code problematic should check to see
3912         # if we still support it.
3913         console_file = self._get_console_log_path(instance)
3914         LOG.debug('Ensure instance console log exists: %s', console_file,
3915                   instance=instance)
3916         try:
3917             libvirt_utils.file_open(console_file, 'a').close()
3918         # NOTE(sfinucan): We can safely ignore permission issues here and
3919         # assume that it is libvirt that has taken ownership of this file.
3920         except IOError as ex:
3921             if ex.errno != errno.EACCES:
3922                 raise
3923             LOG.debug('Console file already exists: %s.', console_file)
3924 
3925     @staticmethod
3926     def _get_disk_config_image_type():
3927         # TODO(mikal): there is a bug here if images_type has
3928         # changed since creation of the instance, but I am pretty
3929         # sure that this bug already exists.
3930         return 'rbd' if CONF.libvirt.images_type == 'rbd' else 'raw'
3931 
3932     @staticmethod
3933     def _is_booted_from_volume(block_device_info):
3934         """Determines whether the VM is booting from volume
3935 
3936         Determines whether the block device info indicates that the VM
3937         is booting from a volume.
3938         """
3939         block_device_mapping = driver.block_device_info_get_mapping(
3940             block_device_info)
3941         return bool(block_device.get_root_bdm(block_device_mapping))
3942 
3943     def _inject_data(self, disk, instance, injection_info):
3944         """Injects data in a disk image
3945 
3946         Helper used for injecting data in a disk image file system.
3947 
3948         :param disk: The disk we're injecting into (an Image object)
3949         :param instance: The instance we're injecting into
3950         :param injection_info: Injection info
3951         """
3952         # Handles the partition need to be used.
3953         LOG.debug('Checking root disk injection %s',
3954                   str(injection_info), instance=instance)
3955         target_partition = None
3956         if not instance.kernel_id:
3957             target_partition = CONF.libvirt.inject_partition
3958             if target_partition == 0:
3959                 target_partition = None
3960         if CONF.libvirt.virt_type == 'lxc':
3961             target_partition = None
3962 
3963         # Handles the key injection.
3964         key = None
3965         if CONF.libvirt.inject_key and instance.get('key_data'):
3966             key = str(instance.key_data)
3967 
3968         # Handles the admin password injection.
3969         admin_pass = None
3970         if CONF.libvirt.inject_password:
3971             admin_pass = injection_info.admin_pass
3972 
3973         # Handles the network injection.
3974         net = netutils.get_injected_network_template(
3975             injection_info.network_info,
3976             libvirt_virt_type=CONF.libvirt.virt_type)
3977 
3978         # Handles the metadata injection
3979         metadata = instance.get('metadata')
3980 
3981         if any((key, net, metadata, admin_pass, injection_info.files)):
3982             LOG.debug('Injecting %s', str(injection_info),
3983                       instance=instance)
3984             img_id = instance.image_ref
3985             try:
3986                 disk_api.inject_data(disk.get_model(self._conn),
3987                                      key, net, metadata, admin_pass,
3988                                      injection_info.files,
3989                                      partition=target_partition,
3990                                      mandatory=('files',))
3991             except Exception as e:
3992                 with excutils.save_and_reraise_exception():
3993                     LOG.error('Error injecting data into image '
3994                               '%(img_id)s (%(e)s)',
3995                               {'img_id': img_id, 'e': e},
3996                               instance=instance)
3997 
3998     # NOTE(sileht): many callers of this method assume that this
3999     # method doesn't fail if an image already exists but instead
4000     # think that it will be reused (ie: (live)-migration/resize)
4001     def _create_image(self, context, instance,
4002                       disk_mapping, injection_info=None, suffix='',
4003                       disk_images=None, block_device_info=None,
4004                       fallback_from_host=None,
4005                       ignore_bdi_for_swap=False):
4006         booted_from_volume = self._is_booted_from_volume(block_device_info)
4007 
4008         def image(fname, image_type=CONF.libvirt.images_type):
4009             return self.image_backend.by_name(instance,
4010                                               fname + suffix, image_type)
4011 
4012         def raw(fname):
4013             return image(fname, image_type='raw')
4014 
4015         created_instance_dir = True
4016 
4017         # ensure directories exist and are writable
4018         instance_dir = libvirt_utils.get_instance_path(instance)
4019         if os.path.exists(instance_dir):
4020             LOG.debug("Instance directory exists: not creating",
4021                       instance=instance)
4022             created_instance_dir = False
4023         else:
4024             LOG.debug("Creating instance directory", instance=instance)
4025             fileutils.ensure_tree(libvirt_utils.get_instance_path(instance))
4026 
4027         LOG.info('Creating image', instance=instance)
4028 
4029         inst_type = instance.get_flavor()
4030         swap_mb = 0
4031         if 'disk.swap' in disk_mapping:
4032             mapping = disk_mapping['disk.swap']
4033 
4034             if ignore_bdi_for_swap:
4035                 # This is a workaround to support legacy swap resizing,
4036                 # which does not touch swap size specified in bdm,
4037                 # but works with flavor specified size only.
4038                 # In this case we follow the legacy logic and ignore block
4039                 # device info completely.
4040                 # NOTE(ft): This workaround must be removed when a correct
4041                 # implementation of resize operation changing sizes in bdms is
4042                 # developed. Also at that stage we probably may get rid of
4043                 # the direct usage of flavor swap size here,
4044                 # leaving the work with bdm only.
4045                 swap_mb = inst_type['swap']
4046             else:
4047                 swap = driver.block_device_info_get_swap(block_device_info)
4048                 if driver.swap_is_usable(swap):
4049                     swap_mb = swap['swap_size']
4050                 elif (inst_type['swap'] > 0 and
4051                       not block_device.volume_in_mapping(
4052                         mapping['dev'], block_device_info)):
4053                     swap_mb = inst_type['swap']
4054 
4055             if swap_mb > 0:
4056                 if (CONF.libvirt.virt_type == "parallels" and
4057                         instance.vm_mode == fields.VMMode.EXE):
4058                     msg = _("Swap disk is not supported "
4059                             "for Virtuozzo container")
4060                     raise exception.Invalid(msg)
4061 
4062         if not disk_images:
4063             disk_images = {'image_id': instance.image_ref,
4064                            'kernel_id': instance.kernel_id,
4065                            'ramdisk_id': instance.ramdisk_id}
4066 
4067         # NOTE(mdbooth): kernel and ramdisk, if they are defined, are hardcoded
4068         # to use raw, which means they will always be cleaned up with the
4069         # instance directory. We must not consider them for created_disks,
4070         # which may not be using the instance directory.
4071         if disk_images['kernel_id']:
4072             fname = imagecache.get_cache_fname(disk_images['kernel_id'])
4073             raw('kernel').cache(fetch_func=libvirt_utils.fetch_raw_image,
4074                                 context=context,
4075                                 filename=fname,
4076                                 image_id=disk_images['kernel_id'])
4077             if disk_images['ramdisk_id']:
4078                 fname = imagecache.get_cache_fname(disk_images['ramdisk_id'])
4079                 raw('ramdisk').cache(fetch_func=libvirt_utils.fetch_raw_image,
4080                                      context=context,
4081                                      filename=fname,
4082                                      image_id=disk_images['ramdisk_id'])
4083 
4084         if CONF.libvirt.virt_type == 'uml':
4085             # PONDERING(mikal): can I assume that root is UID zero in every
4086             # OS? Probably not.
4087             uid = pwd.getpwnam('root').pw_uid
4088             nova.privsep.path.chown(image('disk').path, uid=uid)
4089 
4090         created_disks = self._create_and_inject_local_root(
4091                 context, instance, booted_from_volume, suffix, disk_images,
4092                 injection_info, fallback_from_host)
4093 
4094         # Lookup the filesystem type if required
4095         os_type_with_default = nova.privsep.fs.get_fs_type_for_os_type(
4096             instance.os_type)
4097         # Generate a file extension based on the file system
4098         # type and the mkfs commands configured if any
4099         file_extension = nova.privsep.fs.get_file_extension_for_os_type(
4100             os_type_with_default, CONF.default_ephemeral_format)
4101 
4102         vm_mode = fields.VMMode.get_from_instance(instance)
4103         ephemeral_gb = instance.flavor.ephemeral_gb
4104         if 'disk.local' in disk_mapping:
4105             disk_image = image('disk.local')
4106             # Short circuit the exists() tests if we already created a disk
4107             created_disks = created_disks or not disk_image.exists()
4108 
4109             fn = functools.partial(self._create_ephemeral,
4110                                    fs_label='ephemeral0',
4111                                    os_type=instance.os_type,
4112                                    is_block_dev=disk_image.is_block_dev,
4113                                    vm_mode=vm_mode)
4114             fname = "ephemeral_%s_%s" % (ephemeral_gb, file_extension)
4115             size = ephemeral_gb * units.Gi
4116             disk_image.cache(fetch_func=fn,
4117                              context=context,
4118                              filename=fname,
4119                              size=size,
4120                              ephemeral_size=ephemeral_gb)
4121 
4122         for idx, eph in enumerate(driver.block_device_info_get_ephemerals(
4123                 block_device_info)):
4124             disk_image = image(blockinfo.get_eph_disk(idx))
4125             # Short circuit the exists() tests if we already created a disk
4126             created_disks = created_disks or not disk_image.exists()
4127 
4128             specified_fs = eph.get('guest_format')
4129             if specified_fs and not self.is_supported_fs_format(specified_fs):
4130                 msg = _("%s format is not supported") % specified_fs
4131                 raise exception.InvalidBDMFormat(details=msg)
4132 
4133             fn = functools.partial(self._create_ephemeral,
4134                                    fs_label='ephemeral%d' % idx,
4135                                    os_type=instance.os_type,
4136                                    is_block_dev=disk_image.is_block_dev,
4137                                    vm_mode=vm_mode)
4138             size = eph['size'] * units.Gi
4139             fname = "ephemeral_%s_%s" % (eph['size'], file_extension)
4140             disk_image.cache(fetch_func=fn,
4141                              context=context,
4142                              filename=fname,
4143                              size=size,
4144                              ephemeral_size=eph['size'],
4145                              specified_fs=specified_fs)
4146 
4147         if swap_mb > 0:
4148             size = swap_mb * units.Mi
4149             swap = image('disk.swap')
4150             # Short circuit the exists() tests if we already created a disk
4151             created_disks = created_disks or not swap.exists()
4152             swap.cache(fetch_func=self._create_swap, context=context,
4153                        filename="swap_%s" % swap_mb,
4154                        size=size, swap_mb=swap_mb)
4155 
4156         if created_disks:
4157             LOG.debug('Created local disks', instance=instance)
4158         else:
4159             LOG.debug('Did not create local disks', instance=instance)
4160 
4161         return (created_instance_dir, created_disks)
4162 
4163     def _create_and_inject_local_root(self, context, instance,
4164                                       booted_from_volume, suffix, disk_images,
4165                                       injection_info, fallback_from_host):
4166         created_disks = False
4167 
4168         # File injection only if needed
4169         need_inject = (not configdrive.required_by(instance) and
4170                        injection_info is not None and
4171                        CONF.libvirt.inject_partition != -2)
4172 
4173         # NOTE(ndipanov): Even if disk_mapping was passed in, which
4174         # currently happens only on rescue - we still don't want to
4175         # create a base image.
4176         if not booted_from_volume:
4177             root_fname = imagecache.get_cache_fname(disk_images['image_id'])
4178             size = instance.flavor.root_gb * units.Gi
4179 
4180             if size == 0 or suffix == '.rescue':
4181                 size = None
4182 
4183             backend = self.image_backend.by_name(instance, 'disk' + suffix,
4184                                                  CONF.libvirt.images_type)
4185             created_disks = not backend.exists()
4186 
4187             if instance.task_state == task_states.RESIZE_FINISH:
4188                 backend.create_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME)
4189             if backend.SUPPORTS_CLONE:
4190                 def clone_fallback_to_fetch(
4191                     context, target, image_id, trusted_certs=None,
4192                 ):
4193                     refuse_fetch = (
4194                         CONF.libvirt.images_type == 'rbd' and
4195                         CONF.workarounds.never_download_image_if_on_rbd)
4196                     try:
4197                         backend.clone(context, disk_images['image_id'])
4198                     except exception.ImageUnacceptable:
4199                         if refuse_fetch:
4200                             # Re-raise the exception from the failed
4201                             # ceph clone.  The compute manager expects
4202                             # ImageUnacceptable as a possible result
4203                             # of spawn(), from which this is called.
4204                             with excutils.save_and_reraise_exception():
4205                                 LOG.warning(
4206                                     'Image %s is not on my ceph and '
4207                                     '[workarounds]/'
4208                                     'never_download_image_if_on_rbd=True;'
4209                                     ' refusing to fetch and upload.',
4210                                     disk_images['image_id'])
4211                         libvirt_utils.fetch_image(
4212                             context, target, image_id, trusted_certs,
4213                         )
4214                 fetch_func = clone_fallback_to_fetch
4215             else:
4216                 fetch_func = libvirt_utils.fetch_image
4217 
4218             self._try_fetch_image_cache(backend, fetch_func, context,
4219                                         root_fname, disk_images['image_id'],
4220                                         instance, size, fallback_from_host)
4221 
4222             # During unshelve or cross cell resize on Qcow2 backend, we spawn()
4223             # using a snapshot image. Extra work is needed in order to rebase
4224             # disk image to its original image_ref. Disk backing file will
4225             # then represent back image_ref instead of snapshot image.
4226             self._rebase_original_qcow2_image(context, instance, backend)
4227 
4228             if need_inject:
4229                 self._inject_data(backend, instance, injection_info)
4230 
4231         elif need_inject:
4232             LOG.warning('File injection into a boot from volume '
4233                         'instance is not supported', instance=instance)
4234 
4235         return created_disks
4236 
4237     def _needs_rebase_original_qcow2_image(self, instance, backend):
4238         if not isinstance(backend, imagebackend.Qcow2):
4239             return False
4240         if instance.vm_state == vm_states.SHELVED_OFFLOADED:
4241             return True
4242         if instance.task_state == task_states.RESIZE_FINISH:
4243             # We need to distinguish between local versus cross cell resize.
4244             # Rebase is only needed in cross cell case because instance
4245             # is spawn from a snapshot.
4246             base_image_ref = instance.system_metadata.get(
4247                     'image_base_image_ref')
4248             if base_image_ref != instance.image_ref:
4249                 return True
4250         return False
4251 
4252     def _rebase_original_qcow2_image(self, context, instance, backend):
4253         # NOTE(aarents): During qcow2 instance unshelve/cross_cell_resize,
4254         # backing file represents a snapshot image, not original
4255         # instance.image_ref. We rebase here instance disk to original image.
4256         # This second fetch call does nothing except downloading original
4257         # backing file if missing, as image disk have already been
4258         # created/resized by first fetch call.
4259 
4260         if not self._needs_rebase_original_qcow2_image(instance, backend):
4261             return
4262 
4263         base_dir = self.image_cache_manager.cache_dir
4264         base_image_ref = instance.system_metadata.get('image_base_image_ref')
4265         root_fname = imagecache.get_cache_fname(base_image_ref)
4266         base_backing_fname = os.path.join(base_dir, root_fname)
4267 
4268         try:
4269             self._try_fetch_image_cache(backend, libvirt_utils.fetch_image,
4270                                         context, root_fname, base_image_ref,
4271                                         instance, None)
4272         except exception.ImageNotFound:
4273             # We must flatten here in order to remove dependency with an orphan
4274             # backing file (as snapshot image will be dropped once
4275             # unshelve/cross_cell_resize is successfull).
4276             LOG.warning('Current disk image is created on top of a snapshot '
4277                         'image and cannot be rebased to original image '
4278                         'because it is no longer available in the image '
4279                         'service, disk will be consequently flattened.',
4280                         instance=instance)
4281             base_backing_fname = None
4282 
4283         LOG.info('Rebasing disk image.', instance=instance)
4284         self._rebase_with_qemu_img(backend.path, base_backing_fname)
4285 
4286     def _create_configdrive(self, context, instance, injection_info,
4287                             rescue=False):
4288         # As this method being called right after the definition of a
4289         # domain, but before its actual launch, device metadata will be built
4290         # and saved in the instance for it to be used by the config drive and
4291         # the metadata service.
4292         instance.device_metadata = self._build_device_metadata(context,
4293                                                                instance)
4294         if configdrive.required_by(instance):
4295             LOG.info('Using config drive', instance=instance)
4296 
4297             name = 'disk.config'
4298             if rescue:
4299                 name += '.rescue'
4300 
4301             config_disk = self.image_backend.by_name(
4302                 instance, name, self._get_disk_config_image_type())
4303 
4304             # Don't overwrite an existing config drive
4305             if not config_disk.exists():
4306                 extra_md = {}
4307                 if injection_info.admin_pass:
4308                     extra_md['admin_pass'] = injection_info.admin_pass
4309 
4310                 inst_md = instance_metadata.InstanceMetadata(
4311                     instance, content=injection_info.files, extra_md=extra_md,
4312                     network_info=injection_info.network_info)
4313 
4314                 cdb = configdrive.ConfigDriveBuilder(instance_md=inst_md)
4315                 with cdb:
4316                     # NOTE(mdbooth): We're hardcoding here the path of the
4317                     # config disk when using the flat backend. This isn't
4318                     # good, but it's required because we need a local path we
4319                     # know we can write to in case we're subsequently
4320                     # importing into rbd. This will be cleaned up when we
4321                     # replace this with a call to create_from_func, but that
4322                     # can't happen until we've updated the backends and we
4323                     # teach them not to cache config disks. This isn't
4324                     # possible while we're still using cache() under the hood.
4325                     config_disk_local_path = os.path.join(
4326                         libvirt_utils.get_instance_path(instance), name)
4327                     LOG.info('Creating config drive at %(path)s',
4328                              {'path': config_disk_local_path},
4329                              instance=instance)
4330 
4331                     try:
4332                         cdb.make_drive(config_disk_local_path)
4333                     except processutils.ProcessExecutionError as e:
4334                         with excutils.save_and_reraise_exception():
4335                             LOG.error('Creating config drive failed with '
4336                                       'error: %s', e, instance=instance)
4337 
4338                 try:
4339                     config_disk.import_file(
4340                         instance, config_disk_local_path, name)
4341                 finally:
4342                     # NOTE(mikal): if the config drive was imported into RBD,
4343                     # then we no longer need the local copy
4344                     if CONF.libvirt.images_type == 'rbd':
4345                         LOG.info('Deleting local config drive %(path)s '
4346                                  'because it was imported into RBD.',
4347                                  {'path': config_disk_local_path},
4348                                  instance=instance)
4349                         os.unlink(config_disk_local_path)
4350 
4351     def _prepare_pci_devices_for_use(self, pci_devices):
4352         # kvm , qemu support managed mode
4353         # In managed mode, the configured device will be automatically
4354         # detached from the host OS drivers when the guest is started,
4355         # and then re-attached when the guest shuts down.
4356         if CONF.libvirt.virt_type != 'xen':
4357             # we do manual detach only for xen
4358             return
4359         try:
4360             for dev in pci_devices:
4361                 libvirt_dev_addr = dev['hypervisor_name']
4362                 libvirt_dev = \
4363                         self._host.device_lookup_by_name(libvirt_dev_addr)
4364                 # Note(yjiang5) Spelling for 'dettach' is correct, see
4365                 # http://libvirt.org/html/libvirt-libvirt.html.
4366                 libvirt_dev.dettach()
4367 
4368             # Note(yjiang5): A reset of one PCI device may impact other
4369             # devices on the same bus, thus we need two separated loops
4370             # to detach and then reset it.
4371             for dev in pci_devices:
4372                 libvirt_dev_addr = dev['hypervisor_name']
4373                 libvirt_dev = \
4374                         self._host.device_lookup_by_name(libvirt_dev_addr)
4375                 libvirt_dev.reset()
4376 
4377         except libvirt.libvirtError as exc:
4378             raise exception.PciDevicePrepareFailed(id=dev['id'],
4379                                                    instance_uuid=
4380                                                    dev['instance_uuid'],
4381                                                    reason=str(exc))
4382 
4383     def _detach_pci_devices(self, guest, pci_devs):
4384         try:
4385             for dev in pci_devs:
4386                 guest.detach_device(self._get_guest_pci_device(dev), live=True)
4387                 # after detachDeviceFlags returned, we should check the dom to
4388                 # ensure the detaching is finished
4389                 xml = guest.get_xml_desc()
4390                 xml_doc = etree.fromstring(xml)
4391                 guest_config = vconfig.LibvirtConfigGuest()
4392                 guest_config.parse_dom(xml_doc)
4393 
4394                 for hdev in [d for d in guest_config.devices
4395                     if isinstance(d, vconfig.LibvirtConfigGuestHostdevPCI)]:
4396                     hdbsf = [hdev.domain, hdev.bus, hdev.slot, hdev.function]
4397                     dbsf = pci_utils.parse_address(dev.address)
4398                     if [int(x, 16) for x in hdbsf] ==\
4399                             [int(x, 16) for x in dbsf]:
4400                         raise exception.PciDeviceDetachFailed(reason=
4401                                                               "timeout",
4402                                                               dev=dev)
4403 
4404         except libvirt.libvirtError as ex:
4405             error_code = ex.get_error_code()
4406             if error_code == libvirt.VIR_ERR_NO_DOMAIN:
4407                 LOG.warning("Instance disappeared while detaching "
4408                             "a PCI device from it.")
4409             else:
4410                 raise
4411 
4412     def _attach_pci_devices(self, guest, pci_devs):
4413         try:
4414             for dev in pci_devs:
4415                 guest.attach_device(self._get_guest_pci_device(dev))
4416 
4417         except libvirt.libvirtError:
4418             LOG.error('Attaching PCI devices %(dev)s to %(dom)s failed.',
4419                       {'dev': pci_devs, 'dom': guest.id})
4420             raise
4421 
4422     @staticmethod
4423     def _has_direct_passthrough_port(network_info):
4424         for vif in network_info:
4425             if (vif['vnic_type'] in
4426                 network_model.VNIC_TYPES_DIRECT_PASSTHROUGH):
4427                 return True
4428         return False
4429 
4430     def _attach_direct_passthrough_ports(
4431         self, context, instance, guest, network_info=None):
4432         if network_info is None:
4433             network_info = instance.info_cache.network_info
4434         if network_info is None:
4435             return
4436 
4437         if self._has_direct_passthrough_port(network_info):
4438             for vif in network_info:
4439                 if (vif['vnic_type'] in
4440                     network_model.VNIC_TYPES_DIRECT_PASSTHROUGH):
4441                     cfg = self.vif_driver.get_config(instance,
4442                                                      vif,
4443                                                      instance.image_meta,
4444                                                      instance.flavor,
4445                                                      CONF.libvirt.virt_type)
4446                     LOG.debug('Attaching direct passthrough port %(port)s '
4447                               'to %(dom)s', {'port': vif, 'dom': guest.id},
4448                               instance=instance)
4449                     guest.attach_device(cfg)
4450 
4451     def _detach_direct_passthrough_ports(self, context, instance, guest):
4452         network_info = instance.info_cache.network_info
4453         if network_info is None:
4454             return
4455 
4456         if self._has_direct_passthrough_port(network_info):
4457             # In case of VNIC_TYPES_DIRECT_PASSTHROUGH ports we create
4458             # pci request per direct passthrough port. Therefore we can trust
4459             # that pci_slot value in the vif is correct.
4460             direct_passthrough_pci_addresses = [
4461                 vif['profile']['pci_slot']
4462                 for vif in network_info
4463                 if (vif['vnic_type'] in
4464                     network_model.VNIC_TYPES_DIRECT_PASSTHROUGH and
4465                     vif['profile'].get('pci_slot') is not None)
4466             ]
4467 
4468             # use detach_pci_devices to avoid failure in case of
4469             # multiple guest direct passthrough ports with the same MAC
4470             # (protection use-case, ports are on different physical
4471             # interfaces)
4472             pci_devs = pci_manager.get_instance_pci_devs(instance, 'all')
4473             direct_passthrough_pci_addresses = (
4474                 [pci_dev for pci_dev in pci_devs
4475                  if pci_dev.address in direct_passthrough_pci_addresses])
4476             self._detach_pci_devices(guest, direct_passthrough_pci_addresses)
4477 
4478     def _update_compute_provider_status(self, context, service):
4479         """Calls the ComputeVirtAPI.update_compute_provider_status method
4480 
4481         :param context: nova auth RequestContext
4482         :param service: nova.objects.Service record for this host which is
4483             expected to only manage a single ComputeNode
4484         """
4485         rp_uuid = None
4486         try:
4487             rp_uuid = service.compute_node.uuid
4488             self.virtapi.update_compute_provider_status(
4489                 context, rp_uuid, enabled=not service.disabled)
4490         except Exception:
4491             # This is best effort so just log the exception but don't fail.
4492             # The update_available_resource periodic task will sync the trait.
4493             LOG.warning(
4494                 'An error occurred while updating compute node '
4495                 'resource provider status to "%s" for provider: %s',
4496                 'disabled' if service.disabled else 'enabled',
4497                 rp_uuid or service.host, exc_info=True)
4498 
4499     def _set_host_enabled(self, enabled,
4500                           disable_reason=DISABLE_REASON_UNDEFINED):
4501         """Enables / Disables the compute service on this host.
4502 
4503            This doesn't override non-automatic disablement with an automatic
4504            setting; thereby permitting operators to keep otherwise
4505            healthy hosts out of rotation.
4506         """
4507 
4508         status_name = {True: 'disabled',
4509                        False: 'enabled'}
4510 
4511         disable_service = not enabled
4512 
4513         ctx = nova_context.get_admin_context()
4514         try:
4515             service = objects.Service.get_by_compute_host(ctx, CONF.host)
4516 
4517             if service.disabled != disable_service:
4518                 # Note(jang): this is a quick fix to stop operator-
4519                 # disabled compute hosts from re-enabling themselves
4520                 # automatically. We prefix any automatic reason code
4521                 # with a fixed string. We only re-enable a host
4522                 # automatically if we find that string in place.
4523                 # This should probably be replaced with a separate flag.
4524                 if not service.disabled or (
4525                         service.disabled_reason and
4526                         service.disabled_reason.startswith(DISABLE_PREFIX)):
4527                     service.disabled = disable_service
4528                     service.disabled_reason = (
4529                        DISABLE_PREFIX + disable_reason
4530                        if disable_service and disable_reason else
4531                            DISABLE_REASON_UNDEFINED)
4532                     service.save()
4533                     LOG.debug('Updating compute service status to %s',
4534                               status_name[disable_service])
4535                     # Update the disabled trait status on the corresponding
4536                     # compute node resource provider in placement.
4537                     self._update_compute_provider_status(ctx, service)
4538                 else:
4539                     LOG.debug('Not overriding manual compute service '
4540                               'status with: %s',
4541                               status_name[disable_service])
4542         except exception.ComputeHostNotFound:
4543             LOG.warning('Cannot update service status on host "%s" '
4544                         'since it is not registered.', CONF.host)
4545         except Exception:
4546             LOG.warning('Cannot update service status on host "%s" '
4547                         'due to an unexpected exception.', CONF.host,
4548                         exc_info=True)
4549 
4550         if enabled:
4551             mount.get_manager().host_up(self._host)
4552         else:
4553             mount.get_manager().host_down()
4554 
4555     def _get_cpu_model_mapping(self, model):
4556         """Get the CPU model mapping
4557 
4558         The CPU models which admin configured are case-insensitive, libvirt is
4559         case-sensitive, therefore build a mapping to get the correct CPU model
4560         name.
4561 
4562         :param model: Case-insensitive CPU model name.
4563         :return: It will validate and return the case-sensitive CPU model name
4564                  if on a supported platform, otherwise it will just return
4565                  what was provided
4566         :raises: exception.InvalidCPUInfo if the CPU model is not supported.
4567         """
4568         cpu_info = self._get_cpu_info()
4569         if cpu_info['arch'] not in (fields.Architecture.I686,
4570                                     fields.Architecture.X86_64,
4571                                     fields.Architecture.PPC64,
4572                                     fields.Architecture.PPC64LE,
4573                                     fields.Architecture.PPC):
4574             return model
4575 
4576         if not self.cpu_models_mapping:
4577             cpu_models = self._host.get_cpu_model_names()
4578             for cpu_model in cpu_models:
4579                 self.cpu_models_mapping[cpu_model.lower()] = cpu_model
4580 
4581         if model.lower() not in self.cpu_models_mapping:
4582             msg = (_("Configured CPU model: %(model)s is not correct, "
4583                      "or your host CPU arch does not support this "
4584                      "model. Please correct your config and try "
4585                      "again.") % {'model': model})
4586             raise exception.InvalidCPUInfo(msg)
4587 
4588         return self.cpu_models_mapping.get(model.lower())
4589 
4590     def _get_guest_cpu_model_config(self, flavor=None):
4591         mode = CONF.libvirt.cpu_mode
4592         models = [self._get_cpu_model_mapping(model)
4593                   for model in CONF.libvirt.cpu_models]
4594         extra_flags = set([flag.lower() for flag in
4595             CONF.libvirt.cpu_model_extra_flags])
4596 
4597         if (CONF.libvirt.virt_type == "kvm" or
4598             CONF.libvirt.virt_type == "qemu"):
4599             caps = self._host.get_capabilities()
4600             if mode is None:
4601                 # AArch64 lacks 'host-model' support because neither libvirt
4602                 # nor QEMU are able to tell what the host CPU model exactly is.
4603                 # And there is no CPU description code for ARM(64) at this
4604                 # point.
4605 
4606                 # Also worth noting: 'host-passthrough' mode will completely
4607                 # break live migration, *unless* all the Compute nodes (running
4608                 # libvirtd) have *identical* CPUs.
4609                 if caps.host.cpu.arch == fields.Architecture.AARCH64:
4610                     mode = "host-passthrough"
4611                     LOG.info('CPU mode "host-passthrough" was chosen. Live '
4612                              'migration can break unless all compute nodes '
4613                              'have identical cpus. AArch64 does not support '
4614                              'other modes.')
4615                 else:
4616                     mode = "host-model"
4617             if mode == "none":
4618                 return vconfig.LibvirtConfigGuestCPU()
4619             # On AArch64 platform the return of _get_cpu_model_mapping will not
4620             # return the default CPU model.
4621             if mode == "custom":
4622                 if caps.host.cpu.arch == fields.Architecture.AARCH64:
4623                     if not models:
4624                         models = ['max']
4625 
4626         else:
4627             if mode is None or mode == "none":
4628                 return None
4629 
4630         cpu = vconfig.LibvirtConfigGuestCPU()
4631         cpu.mode = mode
4632         cpu.model = models[0] if models else None
4633 
4634         # compare flavor trait and cpu models, select the first mathched model
4635         if flavor and mode == "custom":
4636             flags = libvirt_utils.get_flags_by_flavor_specs(flavor)
4637             if flags:
4638                 cpu.model = self._match_cpu_model_by_flags(models, flags)
4639 
4640         LOG.debug("CPU mode '%(mode)s' models '%(models)s' was chosen, "
4641                   "with extra flags: '%(extra_flags)s'",
4642                   {'mode': mode,
4643                    'models': (cpu.model or ""),
4644                    'extra_flags': (extra_flags or "")})
4645 
4646         # NOTE (kchamart): Currently there's no existing way to ask if a
4647         # given CPU model + CPU flags combination is supported by KVM &
4648         # a specific QEMU binary.  However, libvirt runs the 'CPUID'
4649         # command upfront -- before even a Nova instance (a QEMU
4650         # process) is launched -- to construct CPU models and check
4651         # their validity; so we are good there.  In the long-term,
4652         # upstream libvirt intends to add an additional new API that can
4653         # do fine-grained validation of a certain CPU model + CPU flags
4654         # against a specific QEMU binary (the libvirt RFE bug for that:
4655         # https://bugzilla.redhat.com/show_bug.cgi?id=1559832).
4656         for flag in extra_flags:
4657             cpu.add_feature(vconfig.LibvirtConfigGuestCPUFeature(flag))
4658 
4659         return cpu
4660 
4661     def _match_cpu_model_by_flags(self, models, flags):
4662         for model in models:
4663             if flags.issubset(self.cpu_model_flag_mapping.get(model, set([]))):
4664                 return model
4665             cpu = vconfig.LibvirtConfigCPU()
4666             cpu.arch = self._host.get_capabilities().host.cpu.arch
4667             cpu.model = model
4668             features_xml = self._get_guest_baseline_cpu_features(cpu.to_xml())
4669             if features_xml:
4670                 cpu.parse_str(features_xml)
4671                 feature_names = [f.name for f in cpu.features]
4672                 self.cpu_model_flag_mapping[model] = feature_names
4673                 if flags.issubset(feature_names):
4674                     return model
4675 
4676         msg = ('No CPU model match traits, models: {models}, required '
4677                'flags: {flags}'.format(models=models, flags=flags))
4678         raise exception.InvalidCPUInfo(msg)
4679 
4680     def _get_guest_cpu_config(self, flavor, image_meta,
4681                               guest_cpu_numa_config, instance_numa_topology):
4682         cpu = self._get_guest_cpu_model_config(flavor)
4683 
4684         if cpu is None:
4685             return None
4686 
4687         topology = hardware.get_best_cpu_topology(
4688                 flavor, image_meta, numa_topology=instance_numa_topology)
4689 
4690         cpu.sockets = topology.sockets
4691         cpu.cores = topology.cores
4692         cpu.threads = topology.threads
4693         cpu.numa = guest_cpu_numa_config
4694 
4695         return cpu
4696 
4697     def _get_guest_disk_config(self, instance, name, disk_mapping, inst_type,
4698                                image_type=None, boot_order=None):
4699         disk_unit = None
4700         disk = self.image_backend.by_name(instance, name, image_type)
4701         if (name == 'disk.config' and image_type == 'rbd' and
4702                 not disk.exists()):
4703             # This is likely an older config drive that has not been migrated
4704             # to rbd yet. Try to fall back on 'flat' image type.
4705             # TODO(melwitt): Add online migration of some sort so we can
4706             # remove this fall back once we know all config drives are in rbd.
4707             # NOTE(vladikr): make sure that the flat image exist, otherwise
4708             # the image will be created after the domain definition.
4709             flat_disk = self.image_backend.by_name(instance, name, 'flat')
4710             if flat_disk.exists():
4711                 disk = flat_disk
4712                 LOG.debug('Config drive not found in RBD, falling back to the '
4713                           'instance directory', instance=instance)
4714         disk_info = disk_mapping[name]
4715         if 'unit' in disk_mapping and disk_info['bus'] == 'scsi':
4716             disk_unit = disk_mapping['unit']
4717             disk_mapping['unit'] += 1  # Increments for the next disk added
4718         conf = disk.libvirt_info(disk_info, self.disk_cachemode,
4719                                  inst_type['extra_specs'],
4720                                  self._host.get_version(),
4721                                  disk_unit=disk_unit,
4722                                  boot_order=boot_order)
4723         return conf
4724 
4725     def _get_guest_fs_config(self, instance, name, image_type=None):
4726         disk = self.image_backend.by_name(instance, name, image_type)
4727         return disk.libvirt_fs_info("/", "ploop")
4728 
4729     def _get_guest_storage_config(self, context, instance, image_meta,
4730                                   disk_info,
4731                                   rescue, block_device_info,
4732                                   inst_type, os_type):
4733         devices = []
4734         disk_mapping = disk_info['mapping']
4735 
4736         block_device_mapping = driver.block_device_info_get_mapping(
4737             block_device_info)
4738         mount_rootfs = CONF.libvirt.virt_type == "lxc"
4739         scsi_controller = self._get_scsi_controller(image_meta)
4740 
4741         if scsi_controller and scsi_controller.model == 'virtio-scsi':
4742             # The virtio-scsi can handle up to 256 devices but the
4743             # optional element "address" must be defined to describe
4744             # where the device is placed on the controller (see:
4745             # LibvirtConfigGuestDeviceAddressDrive).
4746             #
4747             # Note about why it's added in disk_mapping: It's not
4748             # possible to pass an 'int' by reference in Python, so we
4749             # use disk_mapping as container to keep reference of the
4750             # unit added and be able to increment it for each disk
4751             # added.
4752             #
4753             # NOTE(jaypipes,melwitt): If this is a boot-from-volume instance,
4754             # we need to start the disk mapping unit at 1 since we set the
4755             # bootable volume's unit to 0 for the bootable volume.
4756             disk_mapping['unit'] = 0
4757             if self._is_booted_from_volume(block_device_info):
4758                 disk_mapping['unit'] = 1
4759 
4760         def _get_ephemeral_devices():
4761             eph_devices = []
4762             for idx, eph in enumerate(
4763                 driver.block_device_info_get_ephemerals(
4764                     block_device_info)):
4765                 diskeph = self._get_guest_disk_config(
4766                     instance,
4767                     blockinfo.get_eph_disk(idx),
4768                     disk_mapping, inst_type)
4769                 eph_devices.append(diskeph)
4770             return eph_devices
4771 
4772         if mount_rootfs:
4773             fs = vconfig.LibvirtConfigGuestFilesys()
4774             fs.source_type = "mount"
4775             fs.source_dir = os.path.join(
4776                 libvirt_utils.get_instance_path(instance), 'rootfs')
4777             devices.append(fs)
4778         elif (os_type == fields.VMMode.EXE and
4779               CONF.libvirt.virt_type == "parallels"):
4780             if rescue:
4781                 fsrescue = self._get_guest_fs_config(instance, "disk.rescue")
4782                 devices.append(fsrescue)
4783 
4784                 fsos = self._get_guest_fs_config(instance, "disk")
4785                 fsos.target_dir = "/mnt/rescue"
4786                 devices.append(fsos)
4787             else:
4788                 if 'disk' in disk_mapping:
4789                     fs = self._get_guest_fs_config(instance, "disk")
4790                     devices.append(fs)
4791                 devices = devices + _get_ephemeral_devices()
4792         else:
4793 
4794             if rescue and disk_mapping['disk.rescue'] == disk_mapping['root']:
4795                 diskrescue = self._get_guest_disk_config(instance,
4796                                                          'disk.rescue',
4797                                                          disk_mapping,
4798                                                          inst_type)
4799                 devices.append(diskrescue)
4800 
4801                 diskos = self._get_guest_disk_config(instance,
4802                                                      'disk',
4803                                                      disk_mapping,
4804                                                      inst_type)
4805                 devices.append(diskos)
4806             else:
4807                 if 'disk' in disk_mapping:
4808                     diskos = self._get_guest_disk_config(instance,
4809                                                          'disk',
4810                                                          disk_mapping,
4811                                                          inst_type)
4812                     devices.append(diskos)
4813 
4814                 if 'disk.local' in disk_mapping:
4815                     disklocal = self._get_guest_disk_config(instance,
4816                                                             'disk.local',
4817                                                             disk_mapping,
4818                                                             inst_type)
4819                     devices.append(disklocal)
4820                     instance.default_ephemeral_device = (
4821                         block_device.prepend_dev(disklocal.target_dev))
4822 
4823                 devices = devices + _get_ephemeral_devices()
4824 
4825                 if 'disk.swap' in disk_mapping:
4826                     diskswap = self._get_guest_disk_config(instance,
4827                                                            'disk.swap',
4828                                                            disk_mapping,
4829                                                            inst_type)
4830                     devices.append(diskswap)
4831                     instance.default_swap_device = (
4832                         block_device.prepend_dev(diskswap.target_dev))
4833 
4834             config_name = 'disk.config'
4835             if rescue and disk_mapping['disk.rescue'] == disk_mapping['root']:
4836                 config_name = 'disk.config.rescue'
4837 
4838             if config_name in disk_mapping:
4839                 diskconfig = self._get_guest_disk_config(
4840                     instance, config_name, disk_mapping, inst_type,
4841                     self._get_disk_config_image_type())
4842                 devices.append(diskconfig)
4843 
4844         for vol in block_device.get_bdms_to_connect(block_device_mapping,
4845                                                    mount_rootfs):
4846             connection_info = vol['connection_info']
4847             vol_dev = block_device.prepend_dev(vol['mount_device'])
4848             info = disk_mapping[vol_dev]
4849             self._connect_volume(context, connection_info, instance)
4850             if scsi_controller and scsi_controller.model == 'virtio-scsi':
4851                 # Check if this is the bootable volume when in a
4852                 # boot-from-volume instance, and if so, ensure the unit
4853                 # attribute is 0.
4854                 if vol.get('boot_index') == 0:
4855                     info['unit'] = 0
4856                 else:
4857                     info['unit'] = disk_mapping['unit']
4858                     disk_mapping['unit'] += 1
4859             cfg = self._get_volume_config(connection_info, info)
4860             devices.append(cfg)
4861             vol['connection_info'] = connection_info
4862             vol.save()
4863 
4864         for d in devices:
4865             self._set_cache_mode(d)
4866 
4867         if scsi_controller:
4868             devices.append(scsi_controller)
4869 
4870         if rescue and disk_mapping['disk.rescue'] != disk_mapping['root']:
4871             diskrescue = self._get_guest_disk_config(instance, 'disk.rescue',
4872                                                      disk_mapping, inst_type,
4873                                                      boot_order='1')
4874             devices.append(diskrescue)
4875 
4876         return devices
4877 
4878     @staticmethod
4879     def _get_scsi_controller(image_meta):
4880         """Return scsi controller or None based on image meta"""
4881         if image_meta.properties.get('hw_scsi_model'):
4882             hw_scsi_model = image_meta.properties.hw_scsi_model
4883             scsi_controller = vconfig.LibvirtConfigGuestController()
4884             scsi_controller.type = 'scsi'
4885             scsi_controller.model = hw_scsi_model
4886             scsi_controller.index = 0
4887             return scsi_controller
4888 
4889     def _get_host_sysinfo_serial_hardware(self):
4890         """Get a UUID from the host hardware
4891 
4892         Get a UUID for the host hardware reported by libvirt.
4893         This is typically from the SMBIOS data, unless it has
4894         been overridden in /etc/libvirt/libvirtd.conf
4895         """
4896         caps = self._host.get_capabilities()
4897         return caps.host.uuid
4898 
4899     def _get_host_sysinfo_serial_os(self):
4900         """Get a UUID from the host operating system
4901 
4902         Get a UUID for the host operating system. Modern Linux
4903         distros based on systemd provide a /etc/machine-id
4904         file containing a UUID. This is also provided inside
4905         systemd based containers and can be provided by other
4906         init systems too, since it is just a plain text file.
4907         """
4908         if not os.path.exists("/etc/machine-id"):
4909             msg = _("Unable to get host UUID: /etc/machine-id does not exist")
4910             raise exception.InternalError(msg)
4911 
4912         with open("/etc/machine-id") as f:
4913             # We want to have '-' in the right place
4914             # so we parse & reformat the value
4915             lines = f.read().split()
4916             if not lines:
4917                 msg = _("Unable to get host UUID: /etc/machine-id is empty")
4918                 raise exception.InternalError(msg)
4919 
4920             return str(uuid.UUID(lines[0]))
4921 
4922     def _get_host_sysinfo_serial_auto(self):
4923         if os.path.exists("/etc/machine-id"):
4924             return self._get_host_sysinfo_serial_os()
4925         else:
4926             return self._get_host_sysinfo_serial_hardware()
4927 
4928     def _get_guest_config_sysinfo(self, instance):
4929         sysinfo = vconfig.LibvirtConfigGuestSysinfo()
4930 
4931         sysinfo.system_manufacturer = version.vendor_string()
4932         sysinfo.system_product = version.product_string()
4933         sysinfo.system_version = version.version_string_with_package()
4934 
4935         if CONF.libvirt.sysinfo_serial == 'unique':
4936             sysinfo.system_serial = instance.uuid
4937         else:
4938             sysinfo.system_serial = self._sysinfo_serial_func()
4939         sysinfo.system_uuid = instance.uuid
4940 
4941         sysinfo.system_family = "Virtual Machine"
4942 
4943         return sysinfo
4944 
4945     def _set_managed_mode(self, pcidev):
4946         # only kvm support managed mode
4947         if CONF.libvirt.virt_type in ('xen', 'parallels',):
4948             pcidev.managed = 'no'
4949         if CONF.libvirt.virt_type in ('kvm', 'qemu'):
4950             pcidev.managed = 'yes'
4951 
4952     def _get_guest_pci_device(self, pci_device):
4953 
4954         dbsf = pci_utils.parse_address(pci_device.address)
4955         dev = vconfig.LibvirtConfigGuestHostdevPCI()
4956         dev.domain, dev.bus, dev.slot, dev.function = dbsf
4957         self._set_managed_mode(dev)
4958 
4959         return dev
4960 
4961     def _get_guest_config_meta(self, instance, network_info):
4962         """Get metadata config for guest."""
4963 
4964         meta = vconfig.LibvirtConfigGuestMetaNovaInstance()
4965         meta.package = version.version_string_with_package()
4966         meta.name = instance.display_name
4967         meta.creationTime = time.time()
4968 
4969         if instance.image_ref not in ("", None):
4970             meta.roottype = "image"
4971             meta.rootid = instance.image_ref
4972 
4973         system_meta = instance.system_metadata
4974         ometa = vconfig.LibvirtConfigGuestMetaNovaOwner()
4975         ometa.userid = instance.user_id
4976         ometa.username = system_meta.get('owner_user_name', 'N/A')
4977         ometa.projectid = instance.project_id
4978         ometa.projectname = system_meta.get('owner_project_name', 'N/A')
4979         meta.owner = ometa
4980 
4981         fmeta = vconfig.LibvirtConfigGuestMetaNovaFlavor()
4982         flavor = instance.flavor
4983         fmeta.name = flavor.name
4984         fmeta.memory = flavor.memory_mb
4985         fmeta.vcpus = flavor.vcpus
4986         fmeta.ephemeral = flavor.ephemeral_gb
4987         fmeta.disk = flavor.root_gb
4988         fmeta.swap = flavor.swap
4989 
4990         meta.flavor = fmeta
4991 
4992         ports = []
4993         for vif in network_info:
4994             ips = []
4995             for subnet in vif.get('network', {}).get('subnets', []):
4996                 for ip in subnet.get('ips', []):
4997                     ips.append(vconfig.LibvirtConfigGuestMetaNovaIp(
4998                         ip.get('type'), ip.get('address'), ip.get('version')))
4999             ports.append(vconfig.LibvirtConfigGuestMetaNovaPort(
5000                 vif.get('id'), ips=ips))
5001 
5002         meta.ports = vconfig.LibvirtConfigGuestMetaNovaPorts(ports)
5003 
5004         return meta
5005 
5006     @staticmethod
5007     def _create_idmaps(klass, map_strings):
5008         idmaps = []
5009         if len(map_strings) > 5:
5010             map_strings = map_strings[0:5]
5011             LOG.warning("Too many id maps, only included first five.")
5012         for map_string in map_strings:
5013             try:
5014                 idmap = klass()
5015                 values = [int(i) for i in map_string.split(":")]
5016                 idmap.start = values[0]
5017                 idmap.target = values[1]
5018                 idmap.count = values[2]
5019                 idmaps.append(idmap)
5020             except (ValueError, IndexError):
5021                 LOG.warning("Invalid value for id mapping %s", map_string)
5022         return idmaps
5023 
5024     def _get_guest_idmaps(self):
5025         id_maps: ty.List[vconfig.LibvirtConfigGuestIDMap] = []
5026         if CONF.libvirt.virt_type == 'lxc' and CONF.libvirt.uid_maps:
5027             uid_maps = self._create_idmaps(vconfig.LibvirtConfigGuestUIDMap,
5028                                            CONF.libvirt.uid_maps)
5029             id_maps.extend(uid_maps)
5030         if CONF.libvirt.virt_type == 'lxc' and CONF.libvirt.gid_maps:
5031             gid_maps = self._create_idmaps(vconfig.LibvirtConfigGuestGIDMap,
5032                                            CONF.libvirt.gid_maps)
5033             id_maps.extend(gid_maps)
5034         return id_maps
5035 
5036     def _update_guest_cputune(self, guest, flavor, virt_type):
5037         is_able = self._host.is_cpu_control_policy_capable()
5038 
5039         cputuning = ['shares', 'period', 'quota']
5040         wants_cputune = any([k for k in cputuning
5041             if "quota:cpu_" + k in flavor.extra_specs.keys()])
5042 
5043         if wants_cputune and not is_able:
5044             raise exception.UnsupportedHostCPUControlPolicy()
5045 
5046         if not is_able or virt_type not in ('lxc', 'kvm', 'qemu'):
5047             return
5048 
5049         if guest.cputune is None:
5050             guest.cputune = vconfig.LibvirtConfigGuestCPUTune()
5051             # Setting the default cpu.shares value to be a value
5052             # dependent on the number of vcpus
5053         guest.cputune.shares = 1024 * guest.vcpus
5054 
5055         for name in cputuning:
5056             key = "quota:cpu_" + name
5057             if key in flavor.extra_specs:
5058                 setattr(guest.cputune, name,
5059                         int(flavor.extra_specs[key]))
5060 
5061     def _get_cpu_numa_config_from_instance(self, instance_numa_topology,
5062                                            wants_hugepages):
5063         if instance_numa_topology:
5064             guest_cpu_numa = vconfig.LibvirtConfigGuestCPUNUMA()
5065             for instance_cell in instance_numa_topology.cells:
5066                 guest_cell = vconfig.LibvirtConfigGuestCPUNUMACell()
5067                 guest_cell.id = instance_cell.id
5068                 guest_cell.cpus = instance_cell.total_cpus
5069                 guest_cell.memory = instance_cell.memory * units.Ki
5070 
5071                 # The vhost-user network backend requires file backed
5072                 # guest memory (ie huge pages) to be marked as shared
5073                 # access, not private, so an external process can read
5074                 # and write the pages.
5075                 #
5076                 # You can't change the shared vs private flag for an
5077                 # already running guest, and since we can't predict what
5078                 # types of NIC may be hotplugged, we have no choice but
5079                 # to unconditionally turn on the shared flag. This has
5080                 # no real negative functional effect on the guest, so
5081                 # is a reasonable approach to take
5082                 if wants_hugepages:
5083                     guest_cell.memAccess = "shared"
5084                 guest_cpu_numa.cells.append(guest_cell)
5085             return guest_cpu_numa
5086 
5087     def _wants_hugepages(self, host_topology, instance_topology):
5088         """Determine if the guest / host topology implies the
5089            use of huge pages for guest RAM backing
5090         """
5091 
5092         if host_topology is None or instance_topology is None:
5093             return False
5094 
5095         avail_pagesize = [page.size_kb
5096                           for page in host_topology.cells[0].mempages]
5097         avail_pagesize.sort()
5098         # Remove smallest page size as that's not classed as a largepage
5099         avail_pagesize = avail_pagesize[1:]
5100 
5101         # See if we have page size set
5102         for cell in instance_topology.cells:
5103             if (cell.pagesize is not None and
5104                 cell.pagesize in avail_pagesize):
5105                 return True
5106 
5107         return False
5108 
5109     def _get_cell_pairs(self, guest_cpu_numa_config, host_topology):
5110         """Returns the lists of pairs(tuple) of an instance cell and
5111         corresponding host cell:
5112             [(LibvirtConfigGuestCPUNUMACell, NUMACell), ...]
5113         """
5114         cell_pairs = []
5115         for guest_config_cell in guest_cpu_numa_config.cells:
5116             for host_cell in host_topology.cells:
5117                 if guest_config_cell.id == host_cell.id:
5118                     cell_pairs.append((guest_config_cell, host_cell))
5119         return cell_pairs
5120 
5121     def _get_pin_cpuset(self, vcpu, inst_cell, host_cell):
5122         """Returns the config object of LibvirtConfigGuestCPUTuneVCPUPin.
5123 
5124         Prepares vcpupin config for the guest with the following caveats:
5125 
5126             a) If the specified instance vCPU is intended to be pinned, we pin
5127                it to the previously selected host CPU.
5128             b) Otherwise we float over the whole host NUMA node
5129         """
5130         pin_cpuset = vconfig.LibvirtConfigGuestCPUTuneVCPUPin()
5131         pin_cpuset.id = vcpu
5132 
5133         # 'InstanceNUMACell.cpu_pinning' tracks the CPU pinning pair for guest
5134         # CPU and host CPU. If the guest CPU is in the keys of 'cpu_pinning',
5135         # fetch the host CPU from it and pin on it, otherwise, let the guest
5136         # CPU be floating on the sharing CPU set belonging to this NUMA cell.
5137         if inst_cell.cpu_pinning and vcpu in inst_cell.cpu_pinning:
5138             pin_cpuset.cpuset = set([inst_cell.cpu_pinning[vcpu]])
5139         else:
5140             pin_cpuset.cpuset = host_cell.cpuset
5141 
5142         return pin_cpuset
5143 
5144     def _get_emulatorpin_cpuset(self, vcpu, object_numa_cell, vcpus_rt,
5145                                 emulator_threads_policy,
5146                                 pin_cpuset):
5147         """Returns a set of cpu_ids to add to the cpuset for emulator threads
5148            with the following caveats:
5149 
5150             a) If emulator threads policy is isolated, we pin emulator threads
5151                to one cpu we have reserved for it.
5152             b) If emulator threads policy is shared and CONF.cpu_shared_set is
5153                defined, we pin emulator threads on the set of pCPUs defined by
5154                CONF.cpu_shared_set
5155             c) Otherwise;
5156                 c1) If realtime IS NOT enabled, the emulator threads are
5157                     allowed to float cross all the pCPUs associated with
5158                     the guest vCPUs.
5159                 c2) If realtime IS enabled, at least 1 vCPU is required
5160                     to be set aside for non-realtime usage. The emulator
5161                     threads are allowed to float across the pCPUs that
5162                     are associated with the non-realtime VCPUs.
5163         """
5164         emulatorpin_cpuset = set([])
5165         shared_ids = hardware.get_cpu_shared_set()
5166 
5167         if emulator_threads_policy == fields.CPUEmulatorThreadsPolicy.ISOLATE:
5168             if object_numa_cell.cpuset_reserved:
5169                 emulatorpin_cpuset = object_numa_cell.cpuset_reserved
5170         elif ((emulator_threads_policy ==
5171               fields.CPUEmulatorThreadsPolicy.SHARE) and
5172               shared_ids):
5173             online_pcpus = self._host.get_online_cpus()
5174             cpuset = shared_ids & online_pcpus
5175             if not cpuset:
5176                 msg = (_("Invalid cpu_shared_set config, one or more of the "
5177                          "specified cpuset is not online. Online cpuset(s): "
5178                          "%(online)s, requested cpuset(s): %(req)s"),
5179                        {'online': sorted(online_pcpus),
5180                         'req': sorted(shared_ids)})
5181                 raise exception.Invalid(msg)
5182             emulatorpin_cpuset = cpuset
5183         elif not vcpus_rt or vcpu not in vcpus_rt:
5184             emulatorpin_cpuset = pin_cpuset.cpuset
5185 
5186         return emulatorpin_cpuset
5187 
5188     def _get_guest_numa_config(self, instance_numa_topology, flavor,
5189                                image_meta):
5190         """Returns the config objects for the guest NUMA specs.
5191 
5192         Determines the CPUs that the guest can be pinned to if the guest
5193         specifies a cell topology and the host supports it. Constructs the
5194         libvirt XML config object representing the NUMA topology selected
5195         for the guest. Returns a tuple of:
5196 
5197             (cpu_set, guest_cpu_tune, guest_cpu_numa, guest_numa_tune)
5198 
5199         With the following caveats:
5200 
5201             a) If there is no specified guest NUMA topology, then
5202                all tuple elements except cpu_set shall be None. cpu_set
5203                will be populated with the chosen CPUs that the guest
5204                allowed CPUs fit within.
5205 
5206             b) If there is a specified guest NUMA topology, then
5207                cpu_set will be None and guest_cpu_numa will be the
5208                LibvirtConfigGuestCPUNUMA object representing the guest's
5209                NUMA topology. If the host supports NUMA, then guest_cpu_tune
5210                will contain a LibvirtConfigGuestCPUTune object representing
5211                the optimized chosen cells that match the host capabilities
5212                with the instance's requested topology. If the host does
5213                not support NUMA, then guest_cpu_tune and guest_numa_tune
5214                will be None.
5215         """
5216 
5217         if (not self._has_numa_support() and
5218                 instance_numa_topology is not None):
5219             # We should not get here, since we should have avoided
5220             # reporting NUMA topology from _get_host_numa_topology
5221             # in the first place. Just in case of a scheduler
5222             # mess up though, raise an exception
5223             raise exception.NUMATopologyUnsupported()
5224 
5225         # We only pin an instance to some host cores if the user has provided
5226         # configuration to suggest we should.
5227         shared_cpus = None
5228         if CONF.vcpu_pin_set or CONF.compute.cpu_shared_set:
5229             shared_cpus = self._get_vcpu_available()
5230 
5231         topology = self._get_host_numa_topology()
5232 
5233         # We have instance NUMA so translate it to the config class
5234         guest_cpu_numa_config = self._get_cpu_numa_config_from_instance(
5235                 instance_numa_topology,
5236                 self._wants_hugepages(topology, instance_numa_topology))
5237 
5238         if not guest_cpu_numa_config:
5239             # No NUMA topology defined for instance - let the host kernel deal
5240             # with the NUMA effects.
5241             # TODO(ndipanov): Attempt to spread the instance
5242             # across NUMA nodes and expose the topology to the
5243             # instance as an optimisation
5244             return GuestNumaConfig(shared_cpus, None, None, None)
5245 
5246         if not topology:
5247             # No NUMA topology defined for host - This will only happen with
5248             # some libvirt versions and certain platforms.
5249             return GuestNumaConfig(shared_cpus, None,
5250                                    guest_cpu_numa_config, None)
5251 
5252         # Now get configuration from the numa_topology
5253         # Init CPUTune configuration
5254         guest_cpu_tune = vconfig.LibvirtConfigGuestCPUTune()
5255         guest_cpu_tune.emulatorpin = (
5256             vconfig.LibvirtConfigGuestCPUTuneEmulatorPin())
5257         guest_cpu_tune.emulatorpin.cpuset = set([])
5258 
5259         # Init NUMATune configuration
5260         guest_numa_tune = vconfig.LibvirtConfigGuestNUMATune()
5261         guest_numa_tune.memory = vconfig.LibvirtConfigGuestNUMATuneMemory()
5262         guest_numa_tune.memnodes = []
5263 
5264         emulator_threads_policy = None
5265         if 'emulator_threads_policy' in instance_numa_topology:
5266             emulator_threads_policy = (
5267                 instance_numa_topology.emulator_threads_policy)
5268 
5269         # Set realtime scheduler for CPUTune
5270         vcpus_rt = hardware.get_realtime_cpu_constraint(flavor, image_meta)
5271         if vcpus_rt:
5272             vcpusched = vconfig.LibvirtConfigGuestCPUTuneVCPUSched()
5273             designer.set_vcpu_realtime_scheduler(
5274                 vcpusched, vcpus_rt, CONF.libvirt.realtime_scheduler_priority)
5275             guest_cpu_tune.vcpusched.append(vcpusched)
5276 
5277         cell_pairs = self._get_cell_pairs(guest_cpu_numa_config, topology)
5278         for guest_node_id, (guest_config_cell, host_cell) in enumerate(
5279                 cell_pairs):
5280             # set NUMATune for the cell
5281             tnode = vconfig.LibvirtConfigGuestNUMATuneMemNode()
5282             designer.set_numa_memnode(tnode, guest_node_id, host_cell.id)
5283             guest_numa_tune.memnodes.append(tnode)
5284             guest_numa_tune.memory.nodeset.append(host_cell.id)
5285 
5286             # set CPUTune for the cell
5287             object_numa_cell = instance_numa_topology.cells[guest_node_id]
5288             for cpu in guest_config_cell.cpus:
5289                 pin_cpuset = self._get_pin_cpuset(cpu, object_numa_cell,
5290                                                   host_cell)
5291                 guest_cpu_tune.vcpupin.append(pin_cpuset)
5292 
5293                 emu_pin_cpuset = self._get_emulatorpin_cpuset(
5294                     cpu, object_numa_cell, vcpus_rt,
5295                     emulator_threads_policy, pin_cpuset)
5296                 guest_cpu_tune.emulatorpin.cpuset.update(emu_pin_cpuset)
5297 
5298         # TODO(berrange) When the guest has >1 NUMA node, it will
5299         # span multiple host NUMA nodes. By pinning emulator threads
5300         # to the union of all nodes, we guarantee there will be
5301         # cross-node memory access by the emulator threads when
5302         # responding to guest I/O operations. The only way to avoid
5303         # this would be to pin emulator threads to a single node and
5304         # tell the guest OS to only do I/O from one of its virtual
5305         # NUMA nodes. This is not even remotely practical.
5306         #
5307         # The long term solution is to make use of a new QEMU feature
5308         # called "I/O Threads" which will let us configure an explicit
5309         # I/O thread for each guest vCPU or guest NUMA node. It is
5310         # still TBD how to make use of this feature though, especially
5311         # how to associate IO threads with guest devices to eliminate
5312         # cross NUMA node traffic. This is an area of investigation
5313         # for QEMU community devs.
5314 
5315         # Sort the vcpupin list per vCPU id for human-friendlier XML
5316         guest_cpu_tune.vcpupin.sort(key=operator.attrgetter("id"))
5317 
5318         # normalize cell.id
5319         for i, (cell, memnode) in enumerate(zip(guest_cpu_numa_config.cells,
5320                                                 guest_numa_tune.memnodes)):
5321             cell.id = i
5322             memnode.cellid = i
5323 
5324         return GuestNumaConfig(None, guest_cpu_tune, guest_cpu_numa_config,
5325                                guest_numa_tune)
5326 
5327     def _get_guest_os_type(self, virt_type):
5328         """Returns the guest OS type based on virt type."""
5329         if virt_type == "lxc":
5330             ret = fields.VMMode.EXE
5331         elif virt_type == "uml":
5332             ret = fields.VMMode.UML
5333         elif virt_type == "xen":
5334             ret = fields.VMMode.XEN
5335         else:
5336             ret = fields.VMMode.HVM
5337         return ret
5338 
5339     def _set_guest_for_rescue(self, rescue, guest, inst_path, virt_type,
5340                               root_device_name):
5341         if rescue.get('kernel_id'):
5342             guest.os_kernel = os.path.join(inst_path, "kernel.rescue")
5343             guest.os_cmdline = ("root=%s %s" % (root_device_name, CONSOLE))
5344             if virt_type == "qemu":
5345                 guest.os_cmdline += " no_timer_check"
5346         if rescue.get('ramdisk_id'):
5347             guest.os_initrd = os.path.join(inst_path, "ramdisk.rescue")
5348 
5349     def _set_guest_for_inst_kernel(self, instance, guest, inst_path, virt_type,
5350                                 root_device_name, image_meta):
5351         guest.os_kernel = os.path.join(inst_path, "kernel")
5352         guest.os_cmdline = ("root=%s %s" % (root_device_name, CONSOLE))
5353         if virt_type == "qemu":
5354             guest.os_cmdline += " no_timer_check"
5355         if instance.ramdisk_id:
5356             guest.os_initrd = os.path.join(inst_path, "ramdisk")
5357         # we only support os_command_line with images with an explicit
5358         # kernel set and don't want to break nova if there's an
5359         # os_command_line property without a specified kernel_id param
5360         if image_meta.properties.get("os_command_line"):
5361             guest.os_cmdline = image_meta.properties.os_command_line
5362 
5363     def _set_clock(self, guest, os_type, image_meta, virt_type):
5364         # NOTE(mikal): Microsoft Windows expects the clock to be in
5365         # "localtime". If the clock is set to UTC, then you can use a
5366         # registry key to let windows know, but Microsoft says this is
5367         # buggy in http://support.microsoft.com/kb/2687252
5368         clk = vconfig.LibvirtConfigGuestClock()
5369         if os_type == 'windows':
5370             LOG.info('Configuring timezone for windows instance to localtime')
5371             clk.offset = 'localtime'
5372         else:
5373             clk.offset = 'utc'
5374         guest.set_clock(clk)
5375 
5376         if virt_type == "kvm":
5377             self._set_kvm_timers(clk, os_type, image_meta)
5378 
5379     def _set_kvm_timers(self, clk, os_type, image_meta):
5380         # TODO(berrange) One day this should be per-guest
5381         # OS type configurable
5382         tmpit = vconfig.LibvirtConfigGuestTimer()
5383         tmpit.name = "pit"
5384         tmpit.tickpolicy = "delay"
5385 
5386         tmrtc = vconfig.LibvirtConfigGuestTimer()
5387         tmrtc.name = "rtc"
5388         tmrtc.tickpolicy = "catchup"
5389 
5390         clk.add_timer(tmpit)
5391         clk.add_timer(tmrtc)
5392 
5393         hpet = image_meta.properties.get('hw_time_hpet', False)
5394         guestarch = libvirt_utils.get_arch(image_meta)
5395         if guestarch in (fields.Architecture.I686,
5396                          fields.Architecture.X86_64):
5397             # NOTE(rfolco): HPET is a hardware timer for x86 arch.
5398             # qemu -no-hpet is not supported on non-x86 targets.
5399             tmhpet = vconfig.LibvirtConfigGuestTimer()
5400             tmhpet.name = "hpet"
5401             tmhpet.present = hpet
5402             clk.add_timer(tmhpet)
5403         else:
5404             if hpet:
5405                 LOG.warning('HPET is not turned on for non-x86 guests in image'
5406                             ' %s.', image_meta.id)
5407 
5408         # Provide Windows guests with the paravirtualized hyperv timer source.
5409         # This is the windows equiv of kvm-clock, allowing Windows
5410         # guests to accurately keep time.
5411         if os_type == 'windows':
5412             tmhyperv = vconfig.LibvirtConfigGuestTimer()
5413             tmhyperv.name = "hypervclock"
5414             tmhyperv.present = True
5415             clk.add_timer(tmhyperv)
5416 
5417     def _set_features(self, guest, os_type, caps, virt_type, image_meta,
5418             flavor):
5419         hide_hypervisor_id = (strutils.bool_from_string(
5420                 flavor.extra_specs.get('hide_hypervisor_id')) or
5421                 strutils.bool_from_string(
5422                     flavor.extra_specs.get('hw:hide_hypervisor_id')) or
5423                 image_meta.properties.get('img_hide_hypervisor_id'))
5424 
5425         if virt_type == "xen":
5426             # PAE only makes sense in X86
5427             if caps.host.cpu.arch in (fields.Architecture.I686,
5428                                       fields.Architecture.X86_64):
5429                 guest.features.append(vconfig.LibvirtConfigGuestFeaturePAE())
5430 
5431         if (virt_type not in ("lxc", "uml", "parallels", "xen") or
5432                 (virt_type == "xen" and guest.os_type == fields.VMMode.HVM)):
5433             guest.features.append(vconfig.LibvirtConfigGuestFeatureACPI())
5434             guest.features.append(vconfig.LibvirtConfigGuestFeatureAPIC())
5435 
5436         if (virt_type in ("qemu", "kvm") and
5437                 os_type == 'windows'):
5438             hv = vconfig.LibvirtConfigGuestFeatureHyperV()
5439             hv.relaxed = True
5440 
5441             hv.spinlocks = True
5442             # Increase spinlock retries - value recommended by
5443             # KVM maintainers who certify Windows guests
5444             # with Microsoft
5445             hv.spinlock_retries = 8191
5446             hv.vapic = True
5447 
5448             # NOTE(kosamara): Spoofing the vendor_id aims to allow the nvidia
5449             # driver to work on windows VMs. At the moment, the nvidia driver
5450             # checks for the hyperv vendorid, and if it doesn't find that, it
5451             # works. In the future, its behaviour could become more strict,
5452             # checking for the presence of other hyperv feature flags to
5453             # determine that it's loaded in a VM. If that happens, this
5454             # workaround will not be enough, and we'll need to drop the whole
5455             # hyperv element.
5456             # That would disable some optimizations, reducing the guest's
5457             # performance.
5458             if hide_hypervisor_id:
5459                 hv.vendorid_spoof = True
5460 
5461             guest.features.append(hv)
5462 
5463         if virt_type in ("qemu", "kvm"):
5464             if hide_hypervisor_id:
5465                 guest.features.append(
5466                     vconfig.LibvirtConfigGuestFeatureKvmHidden())
5467 
5468             # NOTE(sean-k-mooney): we validate that the image and flavor
5469             # cannot have conflicting values in the compute API
5470             # so we just use the values directly. If it is not set in
5471             # either the flavor or image pmu will be none and we should
5472             # not generate the element to allow qemu to decide if a vPMU
5473             # should be provided for backwards compatibility.
5474             pmu = (flavor.extra_specs.get('hw:pmu') or
5475                    image_meta.properties.get('hw_pmu'))
5476             if pmu is not None:
5477                 guest.features.append(
5478                     vconfig.LibvirtConfigGuestFeaturePMU(pmu))
5479 
5480     def _check_number_of_serial_console(self, num_ports):
5481         virt_type = CONF.libvirt.virt_type
5482         if (virt_type in ("kvm", "qemu") and
5483             num_ports > ALLOWED_QEMU_SERIAL_PORTS):
5484             raise exception.SerialPortNumberLimitExceeded(
5485                 allowed=ALLOWED_QEMU_SERIAL_PORTS, virt_type=virt_type)
5486 
5487     def _video_model_supported(self, model):
5488         return model in fields.VideoModel.ALL
5489 
5490     def _add_video_driver(self, guest, image_meta, flavor):
5491         video = vconfig.LibvirtConfigGuestVideo()
5492         # NOTE(ldbragst): The following logic sets the video.type
5493         # depending on supported defaults given the architecture,
5494         # virtualization type, and features. The video.type attribute can
5495         # be overridden by the user with image_meta.properties, which
5496         # is carried out in the next if statement below this one.
5497         guestarch = libvirt_utils.get_arch(image_meta)
5498         if guest.os_type == fields.VMMode.XEN:
5499             video.type = 'xen'
5500         elif CONF.libvirt.virt_type == 'parallels':
5501             video.type = 'vga'
5502         elif guestarch in (fields.Architecture.PPC,
5503                            fields.Architecture.PPC64,
5504                            fields.Architecture.PPC64LE):
5505             # NOTE(ldbragst): PowerKVM doesn't support 'cirrus' be default
5506             # so use 'vga' instead when running on Power hardware.
5507             video.type = 'vga'
5508         elif guestarch == fields.Architecture.AARCH64:
5509             # NOTE(kevinz): Only virtio device type is supported by AARCH64
5510             # so use 'virtio' instead when running on AArch64 hardware.
5511             video.type = 'virtio'
5512         elif CONF.spice.enabled:
5513             video.type = 'qxl'
5514         if image_meta.properties.get('hw_video_model'):
5515             video.type = image_meta.properties.hw_video_model
5516             if not self._video_model_supported(video.type):
5517                 raise exception.InvalidVideoMode(model=video.type)
5518 
5519         # Set video memory, only if the flavor's limit is set
5520         video_ram = image_meta.properties.get('hw_video_ram', 0)
5521         max_vram = int(flavor.extra_specs.get('hw_video:ram_max_mb', 0))
5522         if video_ram > max_vram:
5523             raise exception.RequestedVRamTooHigh(req_vram=video_ram,
5524                                                  max_vram=max_vram)
5525         if max_vram and video_ram:
5526             video.vram = video_ram * units.Mi // units.Ki
5527         guest.add_device(video)
5528 
5529         # NOTE(sean-k-mooney): return the video device we added
5530         # for simpler testing.
5531         return video
5532 
5533     def _add_qga_device(self, guest, instance):
5534         qga = vconfig.LibvirtConfigGuestChannel()
5535         qga.type = "unix"
5536         qga.target_name = "org.qemu.guest_agent.0"
5537         qga.source_path = ("/var/lib/libvirt/qemu/%s.%s.sock" %
5538                           ("org.qemu.guest_agent.0", instance.name))
5539         guest.add_device(qga)
5540 
5541     def _add_rng_device(self, guest, flavor, image_meta):
5542         rng_allowed_str = flavor.extra_specs.get('hw_rng:allowed', 'True')
5543         rng_allowed = strutils.bool_from_string(rng_allowed_str)
5544 
5545         if not rng_allowed:
5546             return
5547 
5548         rng_device = vconfig.LibvirtConfigGuestRng()
5549         rate_bytes = flavor.extra_specs.get('hw_rng:rate_bytes', 0)
5550         period = flavor.extra_specs.get('hw_rng:rate_period', 0)
5551         if rate_bytes:
5552             rng_device.rate_bytes = int(rate_bytes)
5553             rng_device.rate_period = int(period)
5554         rng_path = CONF.libvirt.rng_dev_path
5555         if (rng_path and not os.path.exists(rng_path)):
5556             raise exception.RngDeviceNotExist(path=rng_path)
5557         rng_device.backend = rng_path
5558         guest.add_device(rng_device)
5559 
5560     def _add_virtio_serial_controller(self, guest, instance):
5561         virtio_controller = vconfig.LibvirtConfigGuestController()
5562         virtio_controller.type = 'virtio-serial'
5563         guest.add_device(virtio_controller)
5564 
5565     def _add_vtpm_device(
5566         self,
5567         guest: libvirt_guest.Guest,
5568         flavor: 'objects.Flavor',
5569         instance: 'objects.Instance',
5570         image_meta: 'objects.ImageMeta',
5571     ):
5572         """Add a vTPM device to the guest, if requested."""
5573         # Enable virtual tpm support if required in the flavor or image.
5574         vtpm_config = hardware.get_vtpm_constraint(flavor, image_meta)
5575         if not vtpm_config:
5576             return
5577 
5578         vtpm_secret_uuid = instance.system_metadata.get('vtpm_secret_uuid')
5579         if not vtpm_secret_uuid:
5580             raise exception.Invalid(
5581                 'Refusing to create an emulated TPM with no secret!')
5582 
5583         vtpm = vconfig.LibvirtConfigGuestVTPM(vtpm_config, vtpm_secret_uuid)
5584         guest.add_device(vtpm)
5585 
5586     def _set_qemu_guest_agent(self, guest, flavor, instance, image_meta):
5587         # Enable qga only if the 'hw_qemu_guest_agent' is equal to yes
5588         if image_meta.properties.get('hw_qemu_guest_agent', False):
5589             # a virtio-serial controller is required for qga. If it is not
5590             # created explicitly, libvirt will do it by itself. But in case
5591             # of AMD SEV, any virtio device should use iommu driver, and
5592             # libvirt does not know about it. That is why the controller
5593             # should be created manually.
5594             if self._sev_enabled(flavor, image_meta):
5595                 self._add_virtio_serial_controller(guest, instance)
5596 
5597             LOG.debug("Qemu guest agent is enabled through image "
5598                       "metadata", instance=instance)
5599             self._add_qga_device(guest, instance)
5600 
5601     def _get_guest_memory_backing_config(
5602             self, inst_topology, numatune, flavor, image_meta):
5603         wantsrealtime = hardware.is_realtime_enabled(flavor)
5604         if (
5605             wantsrealtime and
5606             hardware.get_emulator_thread_policy_constraint(flavor) ==
5607                 fields.CPUEmulatorThreadsPolicy.SHARE and
5608             not CONF.compute.cpu_shared_set
5609         ):
5610             # NOTE(stephenfin) Yes, it's horrible that we're doing this here,
5611             # but the shared policy unfortunately has different behavior
5612             # depending on whether the '[compute] cpu_shared_set' is configured
5613             # or not and we need it to be configured. Also note that we have
5614             # already handled other conditions, such as no emulator thread
5615             # policy being configured whatsoever, at the API level.
5616             LOG.warning(
5617                 'Instance is requesting real-time CPUs with pooled '
5618                 'emulator threads, but a shared CPU pool has not been '
5619                 'configured on this host.'
5620             )
5621             raise exception.RealtimeMaskNotFoundOrInvalid()
5622 
5623         wantsmempages = False
5624         if inst_topology:
5625             for cell in inst_topology.cells:
5626                 if cell.pagesize:
5627                     wantsmempages = True
5628                     break
5629 
5630         wantsfilebacked = CONF.libvirt.file_backed_memory > 0
5631 
5632         if wantsmempages and wantsfilebacked:
5633             # Can't use file-backed memory with hugepages
5634             LOG.warning("Instance requested huge pages, but file-backed "
5635                     "memory is enabled, and incompatible with huge pages")
5636             raise exception.MemoryPagesUnsupported()
5637 
5638         membacking = None
5639         if wantsmempages:
5640             pages = self._get_memory_backing_hugepages_support(
5641                 inst_topology, numatune)
5642             if pages:
5643                 membacking = vconfig.LibvirtConfigGuestMemoryBacking()
5644                 membacking.hugepages = pages
5645         if wantsrealtime:
5646             if not membacking:
5647                 membacking = vconfig.LibvirtConfigGuestMemoryBacking()
5648             membacking.locked = True
5649             membacking.sharedpages = False
5650         if wantsfilebacked:
5651             if not membacking:
5652                 membacking = vconfig.LibvirtConfigGuestMemoryBacking()
5653             membacking.filesource = True
5654             membacking.sharedaccess = True
5655             membacking.allocateimmediate = True
5656             membacking.discard = True
5657         if self._sev_enabled(flavor, image_meta):
5658             if not membacking:
5659                 membacking = vconfig.LibvirtConfigGuestMemoryBacking()
5660             membacking.locked = True
5661 
5662         return membacking
5663 
5664     def _get_memory_backing_hugepages_support(self, inst_topology, numatune):
5665         if not self._has_numa_support():
5666             # We should not get here, since we should have avoided
5667             # reporting NUMA topology from _get_host_numa_topology
5668             # in the first place. Just in case of a scheduler
5669             # mess up though, raise an exception
5670             raise exception.MemoryPagesUnsupported()
5671 
5672         host_topology = self._get_host_numa_topology()
5673 
5674         if host_topology is None:
5675             # As above, we should not get here but just in case...
5676             raise exception.MemoryPagesUnsupported()
5677 
5678         # Currently libvirt does not support the smallest
5679         # pagesize set as a backend memory.
5680         # https://bugzilla.redhat.com/show_bug.cgi?id=1173507
5681         avail_pagesize = [page.size_kb
5682                           for page in host_topology.cells[0].mempages]
5683         avail_pagesize.sort()
5684         smallest = avail_pagesize[0]
5685 
5686         pages = []
5687         for guest_cellid, inst_cell in enumerate(inst_topology.cells):
5688             if inst_cell.pagesize and inst_cell.pagesize > smallest:
5689                 for memnode in numatune.memnodes:
5690                     if guest_cellid == memnode.cellid:
5691                         page = (
5692                             vconfig.LibvirtConfigGuestMemoryBackingPage())
5693                         page.nodeset = [guest_cellid]
5694                         page.size_kb = inst_cell.pagesize
5695                         pages.append(page)
5696                         break  # Quit early...
5697         return pages
5698 
5699     def _get_flavor(self, ctxt, instance, flavor):
5700         if flavor is not None:
5701             return flavor
5702         return instance.flavor
5703 
5704     def _has_uefi_support(self):
5705         # This means that the host can support UEFI booting for guests
5706         supported_archs = [fields.Architecture.X86_64,
5707                            fields.Architecture.AARCH64]
5708         caps = self._host.get_capabilities()
5709         # TODO(dmllr, kchamart): Get rid of probing the OVMF binary file
5710         # paths, it is not robust, because nothing but the binary's
5711         # filename is reported, which means you have to detect its
5712         # architecture and features by other means.  To solve this,
5713         # query the libvirt's getDomainCapabilities() to get the
5714         # firmware paths (as reported in the 'loader' value).  Nova now
5715         # has a wrapper method for this, get_domain_capabilities().
5716         # This is a more reliable way to detect UEFI boot support.
5717         #
5718         # Further, with libvirt 5.3 onwards, support for UEFI boot is
5719         # much more simplified by the "firmware auto-selection" feature.
5720         # When using this, Nova doesn't need to query OVMF file paths at
5721         # all; libvirt will take care of it.  This is done by taking
5722         # advantage of the so-called firmware "descriptor files" --
5723         # small JSON files (which will be shipped by Linux
5724         # distributions) that describe a UEFI firmware binary's
5725         # "characteristics", such as the binary's file path, its
5726         # features, architecture, supported machine type, NVRAM template
5727         # and so forth.
5728 
5729         return ((caps.host.cpu.arch in supported_archs) and
5730                 any((os.path.exists(p)
5731                      for p in DEFAULT_UEFI_LOADER_PATH[caps.host.cpu.arch])))
5732 
5733     def _check_uefi_support(self, hw_firmware_type):
5734         caps = self._host.get_capabilities()
5735         return (self._has_uefi_support() and
5736                 (hw_firmware_type == fields.FirmwareType.UEFI or
5737                  caps.host.cpu.arch == fields.Architecture.AARCH64))
5738 
5739     def _get_supported_perf_events(self):
5740         if not len(CONF.libvirt.enabled_perf_events):
5741             return []
5742 
5743         supported_events = []
5744         for event in CONF.libvirt.enabled_perf_events:
5745             libvirt_perf_event_name = LIBVIRT_PERF_EVENT_PREFIX + event.upper()
5746 
5747             if not hasattr(libvirt, libvirt_perf_event_name):
5748                 LOG.warning("Libvirt does not support event type '%s'.", event)
5749                 continue
5750 
5751             if event in ('cmt', 'mbml', 'mbmt'):
5752                 LOG.warning(
5753                     "Monitoring of Intel CMT `perf` event(s) '%s' is not "
5754                     "supported by recent Linux kernels; ignoring.",
5755                     event,
5756                 )
5757                 continue
5758 
5759             supported_events.append(event)
5760 
5761         return supported_events
5762 
5763     def _configure_guest_by_virt_type(self, guest, virt_type, caps, instance,
5764                                       image_meta, flavor, root_device_name,
5765                                       sev_enabled):
5766         if virt_type == "xen":
5767             if guest.os_type == fields.VMMode.HVM:
5768                 guest.os_loader = CONF.libvirt.xen_hvmloader_path
5769             else:
5770                 guest.os_cmdline = CONSOLE
5771         elif virt_type in ("kvm", "qemu"):
5772             if caps.host.cpu.arch in (fields.Architecture.I686,
5773                                       fields.Architecture.X86_64):
5774                 guest.sysinfo = self._get_guest_config_sysinfo(instance)
5775                 guest.os_smbios = vconfig.LibvirtConfigGuestSMBIOS()
5776             hw_firmware_type = image_meta.properties.get('hw_firmware_type')
5777             if caps.host.cpu.arch == fields.Architecture.AARCH64:
5778                 if not hw_firmware_type:
5779                     hw_firmware_type = fields.FirmwareType.UEFI
5780             if hw_firmware_type == fields.FirmwareType.UEFI:
5781                 if self._has_uefi_support():
5782                     global uefi_logged
5783                     if not uefi_logged:
5784                         LOG.warning("uefi support is without some kind of "
5785                                     "functional testing and therefore "
5786                                     "considered experimental.")
5787                         uefi_logged = True
5788                     for lpath in DEFAULT_UEFI_LOADER_PATH[caps.host.cpu.arch]:
5789                         if os.path.exists(lpath):
5790                             guest.os_loader = lpath
5791                     guest.os_loader_type = "pflash"
5792                 else:
5793                     raise exception.UEFINotSupported()
5794             guest.os_mach_type = libvirt_utils.get_machine_type(image_meta)
5795             if image_meta.properties.get('hw_boot_menu') is None:
5796                 guest.os_bootmenu = strutils.bool_from_string(
5797                     flavor.extra_specs.get('hw:boot_menu', 'no'))
5798             else:
5799                 guest.os_bootmenu = image_meta.properties.hw_boot_menu
5800 
5801         elif virt_type == "lxc":
5802             guest.os_init_path = "/sbin/init"
5803             guest.os_cmdline = CONSOLE
5804             guest.os_init_env["product_name"] = "OpenStack Nova"
5805         elif virt_type == "uml":
5806             guest.os_kernel = "/usr/bin/linux"
5807             guest.os_root = root_device_name
5808         elif virt_type == "parallels":
5809             if guest.os_type == fields.VMMode.EXE:
5810                 guest.os_init_path = "/sbin/init"
5811 
5812     def _conf_non_lxc_uml(self, virt_type, guest, root_device_name, rescue,
5813                     instance, inst_path, image_meta, disk_info):
5814         if rescue:
5815             self._set_guest_for_rescue(rescue, guest, inst_path, virt_type,
5816                                        root_device_name)
5817         elif instance.kernel_id:
5818             self._set_guest_for_inst_kernel(instance, guest, inst_path,
5819                                             virt_type, root_device_name,
5820                                             image_meta)
5821         else:
5822             guest.os_boot_dev = blockinfo.get_boot_order(disk_info)
5823 
5824     def _create_consoles(self, virt_type, guest_cfg, instance, flavor,
5825                          image_meta):
5826         # NOTE(markus_z): Beware! Below are so many conditionals that it is
5827         # easy to lose track. Use this chart to figure out your case:
5828         #
5829         # case | is serial | is qemu | resulting
5830         #      | enabled?  | or kvm? | devices
5831         # -------------------------------------------
5832         #    1 |        no |     no  | pty*
5833         #    2 |        no |     yes | pty with logd
5834         #    3 |       yes |      no | see case 1
5835         #    4 |       yes |     yes | tcp with logd
5836         #
5837         #    * exception: `virt_type=parallels` doesn't create a device
5838         if virt_type == 'parallels':
5839             pass
5840         elif virt_type not in ("qemu", "kvm"):
5841             log_path = self._get_console_log_path(instance)
5842             self._create_pty_device(guest_cfg,
5843                                     vconfig.LibvirtConfigGuestConsole,
5844                                     log_path=log_path)
5845         elif (virt_type in ("qemu", "kvm") and
5846                   self._is_s390x_guest(image_meta)):
5847             self._create_consoles_s390x(guest_cfg, instance,
5848                                         flavor, image_meta)
5849         elif virt_type in ("qemu", "kvm"):
5850             self._create_consoles_qemu_kvm(guest_cfg, instance,
5851                                         flavor, image_meta)
5852 
5853     def _is_s390x_guest(self, image_meta):
5854         s390x_archs = (fields.Architecture.S390, fields.Architecture.S390X)
5855         return libvirt_utils.get_arch(image_meta) in s390x_archs
5856 
5857     def _create_consoles_qemu_kvm(self, guest_cfg, instance, flavor,
5858                                   image_meta):
5859         char_dev_cls = vconfig.LibvirtConfigGuestSerial
5860         log_path = self._get_console_log_path(instance)
5861         if CONF.serial_console.enabled:
5862             if not self._serial_ports_already_defined(instance):
5863                 num_ports = hardware.get_number_of_serial_ports(flavor,
5864                                                                 image_meta)
5865                 self._check_number_of_serial_console(num_ports)
5866                 self._create_serial_consoles(guest_cfg, num_ports,
5867                                              char_dev_cls, log_path)
5868         else:
5869             self._create_pty_device(guest_cfg, char_dev_cls,
5870                                     log_path=log_path)
5871 
5872     def _create_consoles_s390x(self, guest_cfg, instance, flavor, image_meta):
5873         char_dev_cls = vconfig.LibvirtConfigGuestConsole
5874         log_path = self._get_console_log_path(instance)
5875         if CONF.serial_console.enabled:
5876             if not self._serial_ports_already_defined(instance):
5877                 num_ports = hardware.get_number_of_serial_ports(flavor,
5878                                                                 image_meta)
5879                 self._create_serial_consoles(guest_cfg, num_ports,
5880                                              char_dev_cls, log_path)
5881         else:
5882             self._create_pty_device(guest_cfg, char_dev_cls,
5883                                     "sclp", log_path)
5884 
5885     def _create_pty_device(self, guest_cfg, char_dev_cls, target_type=None,
5886                            log_path=None):
5887 
5888         consolepty = char_dev_cls()
5889         consolepty.target_type = target_type
5890         consolepty.type = "pty"
5891 
5892         log = vconfig.LibvirtConfigGuestCharDeviceLog()
5893         log.file = log_path
5894         consolepty.log = log
5895 
5896         guest_cfg.add_device(consolepty)
5897 
5898     def _serial_ports_already_defined(self, instance):
5899         try:
5900             guest = self._host.get_guest(instance)
5901             if list(self._get_serial_ports_from_guest(guest)):
5902                 # Serial port are already configured for instance that
5903                 # means we are in a context of migration.
5904                 return True
5905         except exception.InstanceNotFound:
5906             LOG.debug(
5907                 "Instance does not exist yet on libvirt, we can "
5908                 "safely pass on looking for already defined serial "
5909                 "ports in its domain XML", instance=instance)
5910         return False
5911 
5912     def _create_serial_consoles(self, guest_cfg, num_ports, char_dev_cls,
5913                                 log_path):
5914         for port in range(num_ports):
5915             console = char_dev_cls()
5916             console.port = port
5917             console.type = "tcp"
5918             console.listen_host = CONF.serial_console.proxyclient_address
5919             listen_port = serial_console.acquire_port(console.listen_host)
5920             console.listen_port = listen_port
5921             # NOTE: only the first serial console gets the boot messages,
5922             # that's why we attach the logd subdevice only to that.
5923             if port == 0:
5924                 log = vconfig.LibvirtConfigGuestCharDeviceLog()
5925                 log.file = log_path
5926                 console.log = log
5927             guest_cfg.add_device(console)
5928 
5929     def _cpu_config_to_vcpu_model(self, cpu_config, vcpu_model):
5930         """Update VirtCPUModel object according to libvirt CPU config.
5931 
5932         :param:cpu_config: vconfig.LibvirtConfigGuestCPU presenting the
5933                            instance's virtual cpu configuration.
5934         :param:vcpu_model: VirtCPUModel object. A new object will be created
5935                            if None.
5936 
5937         :return: Updated VirtCPUModel object, or None if cpu_config is None
5938 
5939         """
5940 
5941         if not cpu_config:
5942             return
5943         if not vcpu_model:
5944             vcpu_model = objects.VirtCPUModel()
5945 
5946         vcpu_model.arch = cpu_config.arch
5947         vcpu_model.vendor = cpu_config.vendor
5948         vcpu_model.model = cpu_config.model
5949         vcpu_model.mode = cpu_config.mode
5950         vcpu_model.match = cpu_config.match
5951 
5952         if cpu_config.sockets:
5953             vcpu_model.topology = objects.VirtCPUTopology(
5954                 sockets=cpu_config.sockets,
5955                 cores=cpu_config.cores,
5956                 threads=cpu_config.threads)
5957         else:
5958             vcpu_model.topology = None
5959 
5960         features = [objects.VirtCPUFeature(
5961             name=f.name,
5962             policy=f.policy) for f in cpu_config.features]
5963         vcpu_model.features = features
5964 
5965         return vcpu_model
5966 
5967     def _vcpu_model_to_cpu_config(self, vcpu_model):
5968         """Create libvirt CPU config according to VirtCPUModel object.
5969 
5970         :param:vcpu_model: VirtCPUModel object.
5971 
5972         :return: vconfig.LibvirtConfigGuestCPU.
5973 
5974         """
5975 
5976         cpu_config = vconfig.LibvirtConfigGuestCPU()
5977         cpu_config.arch = vcpu_model.arch
5978         cpu_config.model = vcpu_model.model
5979         cpu_config.mode = vcpu_model.mode
5980         cpu_config.match = vcpu_model.match
5981         cpu_config.vendor = vcpu_model.vendor
5982         if vcpu_model.topology:
5983             cpu_config.sockets = vcpu_model.topology.sockets
5984             cpu_config.cores = vcpu_model.topology.cores
5985             cpu_config.threads = vcpu_model.topology.threads
5986         if vcpu_model.features:
5987             for f in vcpu_model.features:
5988                 xf = vconfig.LibvirtConfigGuestCPUFeature()
5989                 xf.name = f.name
5990                 xf.policy = f.policy
5991                 cpu_config.features.add(xf)
5992         return cpu_config
5993 
5994     def _guest_needs_usb(self, guest):
5995         """Evaluate devices currently attached to the guest."""
5996         for dev in guest.devices:
5997             if isinstance(dev, vconfig.LibvirtConfigGuestDisk):
5998                 if dev.target_bus == 'usb':
5999                     return True
6000 
6001             if isinstance(dev, vconfig.LibvirtConfigGuestInput):
6002                 if dev.bus == 'usb':
6003                     return True
6004 
6005         return False
6006 
6007     def _guest_add_usb_root_controller(self, guest):
6008         """Add USB root controller, if necessary.
6009 
6010         Note that these are added by default on x86-64. We add the controller
6011         here explicitly so that we can _disable_ it (by setting the model to
6012         'none') if it's not necessary.
6013         """
6014         usbhost = vconfig.LibvirtConfigGuestUSBHostController()
6015         usbhost.index = 0
6016         # an unset model means autodetect, while 'none' means don't add a
6017         # controller (x86 gets one by default)
6018         usbhost.model = None if self._guest_needs_usb(guest) else 'none'
6019         guest.add_device(usbhost)
6020 
6021     def _guest_add_pcie_root_ports(self, guest):
6022         """Add PCI Express root ports.
6023 
6024         PCI Express machine can have as many PCIe devices as it has
6025         pcie-root-port controllers (slots in virtual motherboard).
6026 
6027         If we want to have more PCIe slots for hotplug then we need to create
6028         whole PCIe structure (libvirt limitation).
6029         """
6030 
6031         pcieroot = vconfig.LibvirtConfigGuestPCIeRootController()
6032         guest.add_device(pcieroot)
6033 
6034         for x in range(0, CONF.libvirt.num_pcie_ports):
6035             pcierootport = vconfig.LibvirtConfigGuestPCIeRootPortController()
6036             guest.add_device(pcierootport)
6037 
6038     def _guest_needs_pcie(self, guest, caps):
6039         """Check for prerequisites for adding PCIe root port
6040         controllers
6041         """
6042 
6043         # TODO(kchamart) In the third 'if' conditional below, for 'x86'
6044         # arch, we're assuming: when 'os_mach_type' is 'None', you'll
6045         # have "pc" machine type.  That assumption, although it is
6046         # correct for the "forseeable future", it will be invalid when
6047         # libvirt / QEMU changes the default machine types.
6048         #
6049         # From libvirt 4.7.0 onwards (September 2018), it will ensure
6050         # that *if* 'pc' is available, it will be used as the default --
6051         # to not break existing applications.  (Refer:
6052         # https://libvirt.org/git/?p=libvirt.git;a=commit;h=26cfb1a3
6053         # --"qemu: ensure default machine types don't change if QEMU
6054         # changes").
6055         #
6056         # But even if libvirt (>=v4.7.0) handled the default case,
6057         # relying on such assumptions is not robust.  Instead we should
6058         # get the default machine type for a given architecture reliably
6059         # -- by Nova setting it explicitly (we already do it for Arm /
6060         # AArch64 & s390x).  A part of this bug is being tracked here:
6061         # https://bugs.launchpad.net/nova/+bug/1780138).
6062 
6063         # Add PCIe root port controllers for PCI Express machines
6064         # but only if their amount is configured
6065 
6066         if not CONF.libvirt.num_pcie_ports:
6067             return False
6068         if (caps.host.cpu.arch == fields.Architecture.AARCH64 and
6069                 guest.os_mach_type.startswith('virt')):
6070             return True
6071         if (caps.host.cpu.arch == fields.Architecture.X86_64 and
6072                 guest.os_mach_type is not None and
6073                 'q35' in guest.os_mach_type):
6074             return True
6075         return False
6076 
6077     def _guest_add_usb_host_keyboard(self, guest):
6078         """Add USB Host controller and keyboard for graphical console use.
6079 
6080         Add USB keyboard as PS/2 support may not be present on non-x86
6081         architectures.
6082         """
6083         keyboard = vconfig.LibvirtConfigGuestInput()
6084         keyboard.type = "keyboard"
6085         keyboard.bus = "usb"
6086         guest.add_device(keyboard)
6087 
6088     def _get_guest_config(self, instance, network_info, image_meta,
6089                           disk_info, rescue=None, block_device_info=None,
6090                           context=None, mdevs=None, accel_info=None):
6091         """Get config data for parameters.
6092 
6093         :param rescue: optional dictionary that should contain the key
6094             'ramdisk_id' if a ramdisk is needed for the rescue image and
6095             'kernel_id' if a kernel is needed for the rescue image.
6096 
6097         :param mdevs: optional list of mediated devices to assign to the guest.
6098         :param accel_info: optional list of accelerator requests (ARQs)
6099         """
6100         flavor = instance.flavor
6101         inst_path = libvirt_utils.get_instance_path(instance)
6102         disk_mapping = disk_info['mapping']
6103         vpmems = self._get_ordered_vpmems(instance, flavor)
6104 
6105         virt_type = CONF.libvirt.virt_type
6106         guest = vconfig.LibvirtConfigGuest()
6107         guest.virt_type = virt_type
6108         guest.name = instance.name
6109         guest.uuid = instance.uuid
6110         # We are using default unit for memory: KiB
6111         guest.memory = flavor.memory_mb * units.Ki
6112         guest.vcpus = flavor.vcpus
6113 
6114         guest_numa_config = self._get_guest_numa_config(
6115             instance.numa_topology, flavor, image_meta)
6116 
6117         guest.cpuset = guest_numa_config.cpuset
6118         guest.cputune = guest_numa_config.cputune
6119         guest.numatune = guest_numa_config.numatune
6120 
6121         guest.membacking = self._get_guest_memory_backing_config(
6122             instance.numa_topology,
6123             guest_numa_config.numatune,
6124             flavor, image_meta)
6125 
6126         guest.metadata.append(self._get_guest_config_meta(
6127             instance, network_info))
6128         guest.idmaps = self._get_guest_idmaps()
6129 
6130         for event in self._supported_perf_events:
6131             guest.add_perf_event(event)
6132 
6133         self._update_guest_cputune(guest, flavor, virt_type)
6134 
6135         guest.cpu = self._get_guest_cpu_config(
6136             flavor, image_meta, guest_numa_config.numaconfig,
6137             instance.numa_topology)
6138 
6139         # Notes(yjiang5): we always sync the instance's vcpu model with
6140         # the corresponding config file.
6141         instance.vcpu_model = self._cpu_config_to_vcpu_model(
6142             guest.cpu, instance.vcpu_model)
6143 
6144         if 'root' in disk_mapping:
6145             root_device_name = block_device.prepend_dev(
6146                 disk_mapping['root']['dev'])
6147         else:
6148             root_device_name = None
6149 
6150         guest.os_type = (fields.VMMode.get_from_instance(instance) or
6151                 self._get_guest_os_type(virt_type))
6152         caps = self._host.get_capabilities()
6153 
6154         sev_enabled = self._sev_enabled(flavor, image_meta)
6155 
6156         self._configure_guest_by_virt_type(guest, virt_type, caps, instance,
6157                                            image_meta, flavor,
6158                                            root_device_name, sev_enabled)
6159         if virt_type not in ('lxc', 'uml'):
6160             self._conf_non_lxc_uml(virt_type, guest, root_device_name, rescue,
6161                     instance, inst_path, image_meta, disk_info)
6162 
6163         self._set_features(guest, instance.os_type, caps, virt_type,
6164                            image_meta, flavor)
6165         self._set_clock(guest, instance.os_type, image_meta, virt_type)
6166 
6167         storage_configs = self._get_guest_storage_config(context,
6168                 instance, image_meta, disk_info, rescue, block_device_info,
6169                 flavor, guest.os_type)
6170         for config in storage_configs:
6171             guest.add_device(config)
6172 
6173         for vif in network_info:
6174             config = self.vif_driver.get_config(
6175                 instance, vif, image_meta, flavor, virt_type,
6176             )
6177             guest.add_device(config)
6178 
6179         self._create_consoles(virt_type, guest, instance, flavor, image_meta)
6180 
6181         pointer = self._get_guest_pointer_model(guest.os_type, image_meta)
6182         if pointer:
6183             guest.add_device(pointer)
6184 
6185         self._guest_add_spice_channel(guest)
6186 
6187         if self._guest_add_video_device(guest):
6188             self._add_video_driver(guest, image_meta, flavor)
6189 
6190             # We want video == we want graphical console. Some architectures
6191             # do not have input devices attached in default configuration.
6192             # Let then add USB Host controller and USB keyboard.
6193             # x86(-64) and ppc64 have usb host controller and keyboard
6194             # s390x does not support USB
6195             if caps.host.cpu.arch == fields.Architecture.AARCH64:
6196                 self._guest_add_usb_host_keyboard(guest)
6197 
6198         # Some features are only supported 'qemu' and 'kvm' hypervisor
6199         if virt_type in ('qemu', 'kvm'):
6200             self._set_qemu_guest_agent(guest, flavor, instance, image_meta)
6201             self._add_rng_device(guest, flavor, image_meta)
6202             self._add_vtpm_device(guest, flavor, instance, image_meta)
6203 
6204         if self._guest_needs_pcie(guest, caps):
6205             self._guest_add_pcie_root_ports(guest)
6206 
6207         self._guest_add_usb_root_controller(guest)
6208 
6209         self._guest_add_pci_devices(guest, instance)
6210 
6211         pci_arq_list = []
6212         if accel_info:
6213             # NOTE(Sundar): We handle only the case where all attach handles
6214             # are of type 'PCI'. The Cyborg fake driver used for testing
6215             # returns attach handles of type 'TEST_PCI' and so its ARQs will
6216             # not get composed into the VM's domain XML. For now, we do not
6217             # expect a mixture of different attach handles for the same
6218             # instance; but that case also gets ignored by this logic.
6219             ah_types_set = {arq['attach_handle_type'] for arq in accel_info}
6220             supported_types_set = {'PCI'}
6221             if ah_types_set == supported_types_set:
6222                 pci_arq_list = accel_info
6223             else:
6224                 LOG.info('Ignoring accelerator requests for instance %s. '
6225                          'Supported Attach handle types: %s. '
6226                          'But got these unsupported types: %s.',
6227                          instance.uuid, supported_types_set,
6228                          ah_types_set.difference(supported_types_set))
6229 
6230         self._guest_add_accel_pci_devices(guest, pci_arq_list)
6231 
6232         self._guest_add_watchdog_action(guest, flavor, image_meta)
6233 
6234         self._guest_add_memory_balloon(guest)
6235 
6236         if mdevs:
6237             self._guest_add_mdevs(guest, mdevs)
6238 
6239         if sev_enabled:
6240             self._guest_configure_sev(guest, caps.host.cpu.arch,
6241                                       guest.os_mach_type)
6242 
6243         if vpmems:
6244             self._guest_add_vpmems(guest, vpmems)
6245 
6246         return guest
6247 
6248     def _get_ordered_vpmems(self, instance, flavor):
6249         resources = self._get_resources(instance)
6250         ordered_vpmem_resources = self._get_ordered_vpmem_resources(
6251             resources, flavor)
6252         ordered_vpmems = [self._vpmems_by_name[resource.identifier]
6253             for resource in ordered_vpmem_resources]
6254         return ordered_vpmems
6255 
6256     def _get_vpmems(self, instance, prefix=None):
6257         resources = self._get_resources(instance, prefix=prefix)
6258         vpmem_resources = self._get_vpmem_resources(resources)
6259         vpmems = [self._vpmems_by_name[resource.identifier]
6260             for resource in vpmem_resources]
6261         return vpmems
6262 
6263     def _guest_add_vpmems(self, guest, vpmems):
6264         guest.max_memory_size = guest.memory
6265         guest.max_memory_slots = 0
6266         for vpmem in vpmems:
6267             size_kb = vpmem.size // units.Ki
6268             align_kb = vpmem.align // units.Ki
6269 
6270             vpmem_config = vconfig.LibvirtConfigGuestVPMEM(
6271                 devpath=vpmem.devpath, size_kb=size_kb, align_kb=align_kb)
6272 
6273             # max memory size needs contain vpmem size
6274             guest.max_memory_size += size_kb
6275             # one vpmem will occupy one memory slot
6276             guest.max_memory_slots += 1
6277             guest.add_device(vpmem_config)
6278 
6279     def _sev_enabled(self, flavor, image_meta):
6280         """To enable AMD SEV, the following should be true:
6281 
6282         a) the supports_amd_sev instance variable in the host is
6283            true,
6284         b) the instance extra specs and/or image properties request
6285            memory encryption to be enabled, and
6286         c) there are no conflicts between extra specs, image properties
6287            and machine type selection.
6288 
6289         Most potential conflicts in c) should already be caught in the
6290         API layer.  However there is still one remaining case which
6291         needs to be handled here: when the image does not contain an
6292         hw_machine_type property, the machine type will be chosen from
6293         CONF.libvirt.hw_machine_type if configured, otherwise falling
6294         back to the hardcoded value which is currently 'pc'.  If it
6295         ends up being 'pc' or another value not in the q35 family, we
6296         need to raise an exception.  So calculate the machine type and
6297         pass it to be checked alongside the other sanity checks which
6298         are run while determining whether SEV is selected.
6299         """
6300         if not self._host.supports_amd_sev:
6301             return False
6302 
6303         mach_type = libvirt_utils.get_machine_type(image_meta)
6304         return hardware.get_mem_encryption_constraint(flavor, image_meta,
6305                                                       mach_type)
6306 
6307     def _guest_configure_sev(self, guest, arch, mach_type):
6308         sev = self._find_sev_feature(arch, mach_type)
6309         if sev is None:
6310             # In theory this should never happen because it should
6311             # only get called if SEV was requested, in which case the
6312             # guest should only get scheduled on this host if it
6313             # supports SEV, and SEV support is dependent on the
6314             # presence of this <sev> feature.  That said, it's
6315             # conceivable that something could get messed up along the
6316             # way, e.g. a mismatch in the choice of machine type.  So
6317             # make sure that if it ever does happen, we at least get a
6318             # helpful error rather than something cryptic like
6319             # "AttributeError: 'NoneType' object has no attribute 'cbitpos'
6320             raise exception.MissingDomainCapabilityFeatureException(
6321                 feature='sev')
6322 
6323         designer.set_driver_iommu_for_sev(guest)
6324         self._guest_add_launch_security(guest, sev)
6325 
6326     def _guest_add_launch_security(self, guest, sev):
6327         launch_security = vconfig.LibvirtConfigGuestSEVLaunchSecurity()
6328         launch_security.cbitpos = sev.cbitpos
6329         launch_security.reduced_phys_bits = sev.reduced_phys_bits
6330         guest.launch_security = launch_security
6331 
6332     def _find_sev_feature(self, arch, mach_type):
6333         """Search domain capabilities for the given arch and machine type
6334         for the <sev> element under <features>, and return it if found.
6335         """
6336         domain_caps = self._host.get_domain_capabilities()
6337         if arch not in domain_caps:
6338             LOG.warning(
6339                 "Wanted to add SEV to config for guest with arch %(arch)s "
6340                 "but only had domain capabilities for: %(archs)s",
6341                 {'arch': arch, 'archs': ' '.join(domain_caps)})
6342             return None
6343 
6344         if mach_type not in domain_caps[arch]:
6345             LOG.warning(
6346                 "Wanted to add SEV to config for guest with machine type "
6347                 "%(mtype)s but for arch %(arch)s only had domain capabilities "
6348                 "for machine types: %(mtypes)s",
6349                 {'mtype': mach_type, 'arch': arch,
6350                  'mtypes': ' '.join(domain_caps[arch])})
6351             return None
6352 
6353         for feature in domain_caps[arch][mach_type].features:
6354             if feature.root_name == 'sev':
6355                 return feature
6356 
6357         return None
6358 
6359     def _guest_add_mdevs(self, guest, chosen_mdevs):
6360         for chosen_mdev in chosen_mdevs:
6361             mdev = vconfig.LibvirtConfigGuestHostdevMDEV()
6362             mdev.uuid = chosen_mdev
6363             guest.add_device(mdev)
6364 
6365     @staticmethod
6366     def _guest_add_spice_channel(guest):
6367         if (CONF.spice.enabled and CONF.spice.agent_enabled and
6368                 guest.virt_type not in ('lxc', 'uml', 'xen')):
6369             channel = vconfig.LibvirtConfigGuestChannel()
6370             channel.type = 'spicevmc'
6371             channel.target_name = "com.redhat.spice.0"
6372             guest.add_device(channel)
6373 
6374     @staticmethod
6375     def _guest_add_memory_balloon(guest):
6376         virt_type = guest.virt_type
6377         # Memory balloon device only support 'qemu/kvm' and 'xen' hypervisor
6378         if (virt_type in ('xen', 'qemu', 'kvm') and
6379                     CONF.libvirt.mem_stats_period_seconds > 0):
6380             balloon = vconfig.LibvirtConfigMemoryBalloon()
6381             if virt_type in ('qemu', 'kvm'):
6382                 balloon.model = 'virtio'
6383             else:
6384                 balloon.model = 'xen'
6385             balloon.period = CONF.libvirt.mem_stats_period_seconds
6386             guest.add_device(balloon)
6387 
6388     @staticmethod
6389     def _guest_add_watchdog_action(guest, flavor, image_meta):
6390         # image meta takes precedence over flavor extra specs; disable the
6391         # watchdog action by default
6392         watchdog_action = (flavor.extra_specs.get('hw:watchdog_action') or
6393                            'disabled')
6394         watchdog_action = image_meta.properties.get('hw_watchdog_action',
6395                                                     watchdog_action)
6396         # NB(sross): currently only actually supported by KVM/QEmu
6397         if watchdog_action != 'disabled':
6398             if watchdog_action in fields.WatchdogAction.ALL:
6399                 bark = vconfig.LibvirtConfigGuestWatchdog()
6400                 bark.action = watchdog_action
6401                 guest.add_device(bark)
6402             else:
6403                 raise exception.InvalidWatchdogAction(action=watchdog_action)
6404 
6405     def _guest_add_pci_devices(self, guest, instance):
6406         virt_type = guest.virt_type
6407         if virt_type in ('xen', 'qemu', 'kvm'):
6408             # Get all generic PCI devices (non-SR-IOV).
6409             for pci_dev in pci_manager.get_instance_pci_devs(instance):
6410                 guest.add_device(self._get_guest_pci_device(pci_dev))
6411         else:
6412             # PCI devices is only supported for hypervisors
6413             #  'xen', 'qemu' and 'kvm'.
6414             if pci_manager.get_instance_pci_devs(instance, 'all'):
6415                 raise exception.PciDeviceUnsupportedHypervisor(type=virt_type)
6416 
6417     def _guest_add_accel_pci_devices(self, guest, accel_info):
6418         """Add all accelerator PCI functions from ARQ list."""
6419         for arq in accel_info:
6420             dev = vconfig.LibvirtConfigGuestHostdevPCI()
6421             pci_addr = arq['attach_handle_info']
6422             dev.domain, dev.bus, dev.slot, dev.function = (
6423                 pci_addr['domain'], pci_addr['bus'],
6424                 pci_addr['device'], pci_addr['function'])
6425             self._set_managed_mode(dev)
6426 
6427             guest.add_device(dev)
6428 
6429     @staticmethod
6430     def _guest_add_video_device(guest):
6431         # NB some versions of libvirt support both SPICE and VNC
6432         # at the same time. We're not trying to second guess which
6433         # those versions are. We'll just let libvirt report the
6434         # errors appropriately if the user enables both.
6435         add_video_driver = False
6436         if CONF.vnc.enabled and guest.virt_type not in ('lxc', 'uml'):
6437             graphics = vconfig.LibvirtConfigGuestGraphics()
6438             graphics.type = "vnc"
6439             graphics.listen = CONF.vnc.server_listen
6440             guest.add_device(graphics)
6441             add_video_driver = True
6442         if CONF.spice.enabled and guest.virt_type not in ('lxc', 'uml', 'xen'):
6443             graphics = vconfig.LibvirtConfigGuestGraphics()
6444             graphics.type = "spice"
6445             graphics.listen = CONF.spice.server_listen
6446             guest.add_device(graphics)
6447             add_video_driver = True
6448         return add_video_driver
6449 
6450     def _get_guest_pointer_model(self, os_type, image_meta):
6451         pointer_model = image_meta.properties.get(
6452             'hw_pointer_model', CONF.pointer_model)
6453 
6454         if pointer_model == "usbtablet":
6455             # We want a tablet if VNC is enabled, or SPICE is enabled and
6456             # the SPICE agent is disabled. If the SPICE agent is enabled
6457             # it provides a paravirt mouse which drastically reduces
6458             # overhead (by eliminating USB polling).
6459             if CONF.vnc.enabled or (
6460                     CONF.spice.enabled and not CONF.spice.agent_enabled):
6461                 return self._get_guest_usb_tablet(os_type)
6462             else:
6463                 if CONF.pointer_model:
6464                     # For backward compatibility We don't want to break
6465                     # process of booting an instance if host is configured
6466                     # to use USB tablet without VNC or SPICE and SPICE
6467                     # agent disable.
6468                     LOG.warning('USB tablet requested for guests by host '
6469                                 'configuration. In order to accept this '
6470                                 'request VNC should be enabled or SPICE '
6471                                 'and SPICE agent disabled on host.')
6472                 else:
6473                     raise exception.UnsupportedPointerModelRequested(
6474                         model="usbtablet")
6475 
6476     def _get_guest_usb_tablet(self, os_type):
6477         tablet = None
6478         if os_type == fields.VMMode.HVM:
6479             tablet = vconfig.LibvirtConfigGuestInput()
6480             tablet.type = "tablet"
6481             tablet.bus = "usb"
6482         else:
6483             if CONF.pointer_model:
6484                 # For backward compatibility We don't want to break
6485                 # process of booting an instance if virtual machine mode
6486                 # is not configured as HVM.
6487                 LOG.warning(
6488                     'USB tablet requested for guests by host configuration. '
6489                     'In order to accept this request the machine mode should '
6490                     'be configured as HVM.')
6491             else:
6492                 raise exception.UnsupportedPointerModelRequested(
6493                     model="usbtablet")
6494         return tablet
6495 
6496     def _get_guest_xml(self, context, instance, network_info, disk_info,
6497                        image_meta, rescue=None,
6498                        block_device_info=None,
6499                        mdevs=None, accel_info=None):
6500         # NOTE(danms): Stringifying a NetworkInfo will take a lock. Do
6501         # this ahead of time so that we don't acquire it while also
6502         # holding the logging lock.
6503         network_info_str = str(network_info)
6504         msg = ('Start _get_guest_xml '
6505                'network_info=%(network_info)s '
6506                'disk_info=%(disk_info)s '
6507                'image_meta=%(image_meta)s rescue=%(rescue)s '
6508                'block_device_info=%(block_device_info)s' %
6509                {'network_info': network_info_str, 'disk_info': disk_info,
6510                 'image_meta': image_meta, 'rescue': rescue,
6511                 'block_device_info': block_device_info})
6512         # NOTE(mriedem): block_device_info can contain auth_password so we
6513         # need to sanitize the password in the message.
6514         LOG.debug(strutils.mask_password(msg), instance=instance)
6515         conf = self._get_guest_config(instance, network_info, image_meta,
6516                                       disk_info, rescue, block_device_info,
6517                                       context, mdevs, accel_info)
6518         xml = conf.to_xml()
6519 
6520         LOG.debug('End _get_guest_xml xml=%(xml)s',
6521                   {'xml': xml}, instance=instance)
6522         return xml
6523 
6524     def get_info(self, instance, use_cache=True):
6525         """Retrieve information from libvirt for a specific instance.
6526 
6527         If a libvirt error is encountered during lookup, we might raise a
6528         NotFound exception or Error exception depending on how severe the
6529         libvirt error is.
6530 
6531         :param instance: nova.objects.instance.Instance object
6532         :param use_cache: unused in this driver
6533         :returns: An InstanceInfo object
6534         """
6535         guest = self._host.get_guest(instance)
6536         # Kind of ugly but we need to pass host to get_info as for a
6537         # workaround, see libvirt/compat.py
6538         return guest.get_info(self._host)
6539 
6540     def _create_domain_setup_lxc(self, context, instance, image_meta,
6541                                  block_device_info):
6542         inst_path = libvirt_utils.get_instance_path(instance)
6543         block_device_mapping = driver.block_device_info_get_mapping(
6544             block_device_info)
6545         root_disk = block_device.get_root_bdm(block_device_mapping)
6546         if root_disk:
6547             self._connect_volume(context, root_disk['connection_info'],
6548                                  instance)
6549             disk_path = root_disk['connection_info']['data']['device_path']
6550 
6551             # NOTE(apmelton) - Even though the instance is being booted from a
6552             # cinder volume, it is still presented as a local block device.
6553             # LocalBlockImage is used here to indicate that the instance's
6554             # disk is backed by a local block device.
6555             image_model = imgmodel.LocalBlockImage(disk_path)
6556         else:
6557             root_disk = self.image_backend.by_name(instance, 'disk')
6558             image_model = root_disk.get_model(self._conn)
6559 
6560         container_dir = os.path.join(inst_path, 'rootfs')
6561         fileutils.ensure_tree(container_dir)
6562         rootfs_dev = disk_api.setup_container(image_model,
6563                                               container_dir=container_dir)
6564 
6565         try:
6566             # Save rootfs device to disconnect it when deleting the instance
6567             if rootfs_dev:
6568                 instance.system_metadata['rootfs_device_name'] = rootfs_dev
6569             if CONF.libvirt.uid_maps or CONF.libvirt.gid_maps:
6570                 id_maps = self._get_guest_idmaps()
6571                 libvirt_utils.chown_for_id_maps(container_dir, id_maps)
6572         except Exception:
6573             with excutils.save_and_reraise_exception():
6574                 self._create_domain_cleanup_lxc(instance)
6575 
6576     def _create_domain_cleanup_lxc(self, instance):
6577         inst_path = libvirt_utils.get_instance_path(instance)
6578         container_dir = os.path.join(inst_path, 'rootfs')
6579 
6580         try:
6581             state = self.get_info(instance).state
6582         except exception.InstanceNotFound:
6583             # The domain may not be present if the instance failed to start
6584             state = None
6585 
6586         if state == power_state.RUNNING:
6587             # NOTE(uni): Now the container is running with its own private
6588             # mount namespace and so there is no need to keep the container
6589             # rootfs mounted in the host namespace
6590             LOG.debug('Attempting to unmount container filesystem: %s',
6591                       container_dir, instance=instance)
6592             disk_api.clean_lxc_namespace(container_dir=container_dir)
6593         else:
6594             disk_api.teardown_container(container_dir=container_dir)
6595 
6596     @contextlib.contextmanager
6597     def _lxc_disk_handler(self, context, instance, image_meta,
6598                           block_device_info):
6599         """Context manager to handle the pre and post instance boot,
6600            LXC specific disk operations.
6601 
6602            An image or a volume path will be prepared and setup to be
6603            used by the container, prior to starting it.
6604            The disk will be disconnected and unmounted if a container has
6605            failed to start.
6606         """
6607 
6608         if CONF.libvirt.virt_type != 'lxc':
6609             yield
6610             return
6611 
6612         self._create_domain_setup_lxc(context, instance, image_meta,
6613                                       block_device_info)
6614 
6615         try:
6616             yield
6617         finally:
6618             self._create_domain_cleanup_lxc(instance)
6619 
6620     def _create_guest(
6621         self,
6622         context: nova_context.RequestContext,
6623         xml: str,
6624         instance: 'objects.Instance',
6625         power_on: bool = True,
6626         pause: bool = False,
6627         post_xml_callback: ty.Callable = None,
6628     ) -> libvirt_guest.Guest:
6629         """Create a Guest from XML.
6630 
6631         Create a Guest, which in turn creates a libvirt domain, from XML,
6632         optionally starting it after creation.
6633 
6634         :returns guest.Guest: Created guest.
6635         """
6636         libvirt_secret = None
6637         # determine whether vTPM is in use and, if so, create the secret
6638         if CONF.libvirt.swtpm_enabled and hardware.get_vtpm_constraint(
6639             instance.flavor, instance.image_meta,
6640         ):
6641             secret_uuid, passphrase = crypto.ensure_vtpm_secret(
6642                 context, instance)
6643             libvirt_secret = self._host.create_secret(
6644                 'vtpm', instance.uuid, password=passphrase,
6645                 uuid=secret_uuid)
6646 
6647         try:
6648             guest = libvirt_guest.Guest.create(xml, self._host)
6649             if post_xml_callback is not None:
6650                 post_xml_callback()
6651 
6652             if power_on or pause:
6653                 guest.launch(pause=pause)
6654 
6655             return guest
6656         finally:
6657             if libvirt_secret is not None:
6658                 libvirt_secret.undefine()
6659 
6660     def _neutron_failed_callback(self, event_name, instance):
6661         LOG.error('Neutron Reported failure on event '
6662                   '%(event)s for instance %(uuid)s',
6663                   {'event': event_name, 'uuid': instance.uuid},
6664                   instance=instance)
6665         if CONF.vif_plugging_is_fatal:
6666             raise exception.VirtualInterfaceCreateException()
6667 
6668     def _get_neutron_events(self, network_info):
6669         # NOTE(danms): We need to collect any VIFs that are currently
6670         # down that we expect a down->up event for. Anything that is
6671         # already up will not undergo that transition, and for
6672         # anything that might be stale (cache-wise) assume it's
6673         # already up so we don't block on it.
6674         return [('network-vif-plugged', vif['id'])
6675                 for vif in network_info if vif.get('active', True) is False]
6676 
6677     def _cleanup_failed_start(self, context, instance, network_info,
6678                               block_device_info, guest,
6679                               cleanup_instance_dir=False,
6680                               cleanup_instance_disks=False):
6681         try:
6682             if guest and guest.is_active():
6683                 guest.poweroff()
6684         finally:
6685             self._cleanup(context, instance, network_info,
6686                           block_device_info=block_device_info,
6687                           destroy_vifs=True,
6688                           cleanup_instance_dir=cleanup_instance_dir,
6689                           cleanup_instance_disks=cleanup_instance_disks)
6690 
6691     def _create_guest_with_network(self, context, xml, instance, network_info,
6692                                    block_device_info, power_on=True,
6693                                    vifs_already_plugged=False,
6694                                    post_xml_callback=None,
6695                                    external_events=None,
6696                                    cleanup_instance_dir=False,
6697                                    cleanup_instance_disks=False):
6698         """Do required network setup and create domain."""
6699 
6700         timeout = CONF.vif_plugging_timeout
6701         if (
6702             CONF.libvirt.virt_type in ('kvm', 'qemu') and
6703             not vifs_already_plugged and power_on and timeout
6704         ):
6705             events = (external_events if external_events
6706                       else self._get_neutron_events(network_info))
6707         else:
6708             events = []
6709 
6710         pause = bool(events)
6711         guest: libvirt_guest.Guest = None
6712         try:
6713             with self.virtapi.wait_for_instance_event(
6714                     instance, events, deadline=timeout,
6715                     error_callback=self._neutron_failed_callback):
6716                 self.plug_vifs(instance, network_info)
6717                 with self._lxc_disk_handler(context, instance,
6718                                             instance.image_meta,
6719                                             block_device_info):
6720                     guest = self._create_guest(
6721                         context, xml, instance,
6722                         pause=pause, power_on=power_on,
6723                         post_xml_callback=post_xml_callback)
6724         except exception.VirtualInterfaceCreateException:
6725             # Neutron reported failure and we didn't swallow it, so
6726             # bail here
6727             with excutils.save_and_reraise_exception():
6728                 self._cleanup_failed_start(
6729                     context, instance, network_info, block_device_info, guest,
6730                     cleanup_instance_dir=cleanup_instance_dir,
6731                     cleanup_instance_disks=cleanup_instance_disks)
6732         except eventlet.timeout.Timeout:
6733             # We never heard from Neutron
6734             LOG.warning('Timeout waiting for %(events)s for '
6735                         'instance with vm_state %(vm_state)s and '
6736                         'task_state %(task_state)s.',
6737                         {'events': events,
6738                          'vm_state': instance.vm_state,
6739                          'task_state': instance.task_state},
6740                         instance=instance)
6741             if CONF.vif_plugging_is_fatal:
6742                 self._cleanup_failed_start(
6743                     context, instance, network_info, block_device_info, guest,
6744                     cleanup_instance_dir=cleanup_instance_dir,
6745                     cleanup_instance_disks=cleanup_instance_disks)
6746                 raise exception.VirtualInterfaceCreateException()
6747         except Exception:
6748             # Any other error, be sure to clean up
6749             LOG.error('Failed to start libvirt guest', instance=instance)
6750             with excutils.save_and_reraise_exception():
6751                 self._cleanup_failed_start(
6752                     context, instance, network_info, block_device_info, guest,
6753                     cleanup_instance_dir=cleanup_instance_dir,
6754                     cleanup_instance_disks=cleanup_instance_disks)
6755         # Resume only if domain has been paused
6756         if pause:
6757             guest.resume()
6758         return guest
6759 
6760     def _get_pcpu_available(self):
6761         """Get number of host cores to be used for PCPUs.
6762 
6763         :returns: The number of host cores to be used for PCPUs.
6764         """
6765         if not CONF.compute.cpu_dedicated_set:
6766             return set()
6767 
6768         online_cpus = self._host.get_online_cpus()
6769         dedicated_cpus = hardware.get_cpu_dedicated_set()
6770 
6771         if not dedicated_cpus.issubset(online_cpus):
6772             msg = _("Invalid '[compute] cpu_dedicated_set' config: one or "
6773                     "more of the configured CPUs is not online. Online "
6774                     "cpuset(s): %(online)s, configured cpuset(s): %(req)s")
6775             raise exception.Invalid(msg % {
6776                 'online': sorted(online_cpus),
6777                 'req': sorted(dedicated_cpus)})
6778 
6779         return dedicated_cpus
6780 
6781     def _get_vcpu_available(self):
6782         """Get host cores to be used for VCPUs.
6783 
6784         :returns: A list of host CPU cores that can be used for VCPUs.
6785         """
6786         online_cpus = self._host.get_online_cpus()
6787 
6788         # NOTE(stephenfin): The use of the legacy 'vcpu_pin_set' option happens
6789         # if it's defined, regardless of whether '[compute] cpu_shared_set' is
6790         # also configured. This is legacy behavior required for upgrades that
6791         # should be removed in the future, when we can rely exclusively on
6792         # '[compute] cpu_shared_set'.
6793         if CONF.vcpu_pin_set:
6794             # TODO(stephenfin): Remove this in U
6795             shared_cpus = hardware.get_vcpu_pin_set()
6796         elif CONF.compute.cpu_shared_set:
6797             shared_cpus = hardware.get_cpu_shared_set()
6798         elif CONF.compute.cpu_dedicated_set:
6799             return set()
6800         else:
6801             return online_cpus
6802 
6803         if not shared_cpus.issubset(online_cpus):
6804             msg = _("Invalid '%(config_opt)s' config: one or "
6805                     "more of the configured CPUs is not online. Online "
6806                     "cpuset(s): %(online)s, configured cpuset(s): %(req)s")
6807 
6808             if CONF.vcpu_pin_set:
6809                 config_opt = 'vcpu_pin_set'
6810             else:  # CONF.compute.cpu_shared_set
6811                 config_opt = '[compute] cpu_shared_set'
6812 
6813             raise exception.Invalid(msg % {
6814                 'config_opt': config_opt,
6815                 'online': sorted(online_cpus),
6816                 'req': sorted(shared_cpus)})
6817 
6818         return shared_cpus
6819 
6820     @staticmethod
6821     def _get_local_gb_info():
6822         """Get local storage info of the compute node in GB.
6823 
6824         :returns: A dict containing:
6825              :total: How big the overall usable filesystem is (in gigabytes)
6826              :free: How much space is free (in gigabytes)
6827              :used: How much space is used (in gigabytes)
6828         """
6829 
6830         if CONF.libvirt.images_type == 'lvm':
6831             info = lvm.get_volume_group_info(
6832                                CONF.libvirt.images_volume_group)
6833         elif CONF.libvirt.images_type == 'rbd':
6834             info = rbd_utils.RBDDriver().get_pool_info()
6835         else:
6836             info = libvirt_utils.get_fs_info(CONF.instances_path)
6837 
6838         for (k, v) in info.items():
6839             info[k] = v / units.Gi
6840 
6841         return info
6842 
6843     def _get_vcpu_used(self):
6844         """Get vcpu usage number of physical computer.
6845 
6846         :returns: The total number of vcpu(s) that are currently being used.
6847 
6848         """
6849 
6850         total = 0
6851 
6852         # Not all libvirt drivers will support the get_vcpus_info()
6853         #
6854         # For example, LXC does not have a concept of vCPUs, while
6855         # QEMU (TCG) traditionally handles all vCPUs in a single
6856         # thread. So both will report an exception when the vcpus()
6857         # API call is made. In such a case we should report the
6858         # guest as having 1 vCPU, since that lets us still do
6859         # CPU over commit calculations that apply as the total
6860         # guest count scales.
6861         #
6862         # It is also possible that we might see an exception if
6863         # the guest is just in middle of shutting down. Technically
6864         # we should report 0 for vCPU usage in this case, but we
6865         # we can't reliably distinguish the vcpu not supported
6866         # case from the just shutting down case. Thus we don't know
6867         # whether to report 1 or 0 for vCPU count.
6868         #
6869         # Under-reporting vCPUs is bad because it could conceivably
6870         # let the scheduler place too many guests on the host. Over-
6871         # reporting vCPUs is not a problem as it'll auto-correct on
6872         # the next refresh of usage data.
6873         #
6874         # Thus when getting an exception we always report 1 as the
6875         # vCPU count, as the least worst value.
6876         for guest in self._host.list_guests():
6877             try:
6878                 vcpus = guest.get_vcpus_info()
6879                 total += len(list(vcpus))
6880             except libvirt.libvirtError:
6881                 total += 1
6882             # NOTE(gtt116): give other tasks a chance.
6883             greenthread.sleep(0)
6884         return total
6885 
6886     def _get_supported_vgpu_types(self):
6887         if not CONF.devices.enabled_vgpu_types:
6888             return []
6889 
6890         # Make sure we register all the types as the compute service could
6891         # be calling this method before init_host()
6892         if len(CONF.devices.enabled_vgpu_types) > 1:
6893             nova.conf.devices.register_dynamic_opts(CONF)
6894 
6895         for vgpu_type in CONF.devices.enabled_vgpu_types:
6896             group = getattr(CONF, 'vgpu_%s' % vgpu_type, None)
6897             if group is None or not group.device_addresses:
6898                 first_type = CONF.devices.enabled_vgpu_types[0]
6899                 if len(CONF.devices.enabled_vgpu_types) > 1:
6900                     # Only provide the warning if the operator provided more
6901                     # than one type as it's not needed to provide groups
6902                     # if you only use one vGPU type.
6903                     msg = ("The vGPU type '%(type)s' was listed in '[devices] "
6904                            "enabled_vgpu_types' but no corresponding "
6905                            "'[vgpu_%(type)s]' group or "
6906                            "'[vgpu_%(type)s] device_addresses' "
6907                            "option was defined. Only the first type "
6908                            "'%(ftype)s' will be used." % {'type': vgpu_type,
6909                                                          'ftype': first_type})
6910                     LOG.warning(msg)
6911                 # We need to reset the mapping table that we started to provide
6912                 # keys and values from previously processed vGPUs but since
6913                 # there is a problem for this vGPU type, we only want to
6914                 # support only the first type.
6915                 self.pgpu_type_mapping.clear()
6916                 return [first_type]
6917             for device_address in group.device_addresses:
6918                 if device_address in self.pgpu_type_mapping:
6919                     raise exception.InvalidLibvirtGPUConfig(
6920                         reason="duplicate types for PCI ID %s" % device_address
6921                     )
6922                 # Just checking whether the operator fat-fingered the address.
6923                 # If it's wrong, it will return an exception
6924                 try:
6925                     pci_utils.parse_address(device_address)
6926                 except exception.PciDeviceWrongAddressFormat:
6927                     raise exception.InvalidLibvirtGPUConfig(
6928                         reason="incorrect PCI address: %s" % device_address
6929                     )
6930                 self.pgpu_type_mapping[device_address] = vgpu_type
6931         return CONF.devices.enabled_vgpu_types
6932 
6933     def _get_vgpu_type_per_pgpu(self, device_address):
6934         """Provides the vGPU type the pGPU supports.
6935 
6936         :param device_address: the libvirt PCI device name,
6937                                eg.'pci_0000_84_00_0'
6938         """
6939         # Bail out quickly if we don't support vGPUs
6940         if not self.supported_vgpu_types:
6941             return
6942 
6943         if len(self.supported_vgpu_types) == 1:
6944             # The operator wanted to only support one single type so we can
6945             # blindly return it for every single pGPU
6946             return self.supported_vgpu_types[0]
6947         # The libvirt name is like 'pci_0000_84_00_0'
6948         try:
6949             device_address = "{}:{}:{}.{}".format(
6950                 *device_address[4:].split('_'))
6951             # Validates whether it's a PCI ID...
6952             pci_utils.parse_address(device_address)
6953         # .format() can return IndexError
6954         except (exception.PciDeviceWrongAddressFormat, IndexError):
6955             # this is not a valid PCI address
6956             LOG.warning("The PCI address %s was invalid for getting the "
6957                         "related vGPU type", device_address)
6958             return
6959         try:
6960             return self.pgpu_type_mapping.get(device_address)
6961         except KeyError:
6962             LOG.warning("No vGPU type was configured for PCI address: %s",
6963                         device_address)
6964             # We accept to return None instead of raising an exception
6965             # because we prefer the callers to return the existing exceptions
6966             # in case we can't find a specific pGPU
6967             return
6968 
6969     def _count_mediated_devices(self, enabled_vgpu_types):
6970         """Counts the sysfs objects (handles) that represent a mediated device
6971         and filtered by $enabled_vgpu_types.
6972 
6973         Those handles can be in use by a libvirt guest or not.
6974 
6975         :param enabled_vgpu_types: list of enabled VGPU types on this host
6976         :returns: dict, keyed by parent GPU libvirt PCI device ID, of number of
6977         mdev device handles for that GPU
6978         """
6979 
6980         counts_per_parent: ty.Dict[str, int] = collections.defaultdict(int)
6981         mediated_devices = self._get_mediated_devices(types=enabled_vgpu_types)
6982         for mdev in mediated_devices:
6983             parent_vgpu_type = self._get_vgpu_type_per_pgpu(mdev['parent'])
6984             if mdev['type'] != parent_vgpu_type:
6985                 # Even if some mdev was created for another vGPU type, just
6986                 # verify all the mdevs related to the type that their pGPU
6987                 # has
6988                 continue
6989             counts_per_parent[mdev['parent']] += 1
6990         return counts_per_parent
6991 
6992     def _count_mdev_capable_devices(self, enabled_vgpu_types):
6993         """Counts the mdev-capable devices on this host filtered by
6994         $enabled_vgpu_types.
6995 
6996         :param enabled_vgpu_types: list of enabled VGPU types on this host
6997         :returns: dict, keyed by device name, to an integer count of available
6998             instances of each type per device
6999         """
7000         mdev_capable_devices = self._get_mdev_capable_devices(
7001             types=enabled_vgpu_types)
7002         counts_per_dev: ty.Dict[str, int] = collections.defaultdict(int)
7003         for dev in mdev_capable_devices:
7004             # dev_id is the libvirt name for the PCI device,
7005             # eg. pci_0000_84_00_0 which matches a PCI address of 0000:84:00.0
7006             dev_name = dev['dev_id']
7007             dev_supported_type = self._get_vgpu_type_per_pgpu(dev_name)
7008             for _type in dev['types']:
7009                 if _type != dev_supported_type:
7010                     # This is not the type the operator wanted to support for
7011                     # this physical GPU
7012                     continue
7013                 available = dev['types'][_type]['availableInstances']
7014                 # NOTE(sbauza): Even if we support multiple types, Nova will
7015                 # only use one per physical GPU.
7016                 counts_per_dev[dev_name] += available
7017         return counts_per_dev
7018 
7019     def _get_gpu_inventories(self):
7020         """Returns the inventories for each physical GPU for a specific type
7021         supported by the enabled_vgpu_types CONF option.
7022 
7023         :returns: dict, keyed by libvirt PCI name, of dicts like:
7024                 {'pci_0000_84_00_0':
7025                     {'total': $TOTAL,
7026                      'min_unit': 1,
7027                      'max_unit': $TOTAL,
7028                      'step_size': 1,
7029                      'reserved': 0,
7030                      'allocation_ratio': 1.0,
7031                     }
7032                 }
7033         """
7034 
7035         # Bail out early if operator doesn't care about providing vGPUs
7036         enabled_vgpu_types = self.supported_vgpu_types
7037         if not enabled_vgpu_types:
7038             return {}
7039         inventories = {}
7040         count_per_parent = self._count_mediated_devices(enabled_vgpu_types)
7041         for dev_name, count in count_per_parent.items():
7042             inventories[dev_name] = {'total': count}
7043         # Filter how many available mdevs we can create for all the supported
7044         # types.
7045         count_per_dev = self._count_mdev_capable_devices(enabled_vgpu_types)
7046         # Combine the counts into the dict that we return to the caller.
7047         for dev_name, count in count_per_dev.items():
7048             inv_per_parent = inventories.setdefault(
7049                 dev_name, {'total': 0})
7050             inv_per_parent['total'] += count
7051             inv_per_parent.update({
7052                 'min_unit': 1,
7053                 'step_size': 1,
7054                 'reserved': 0,
7055                 # NOTE(sbauza): There is no sense to have a ratio but 1.0
7056                 # since we can't overallocate vGPU resources
7057                 'allocation_ratio': 1.0,
7058                 # FIXME(sbauza): Some vendors could support only one
7059                 'max_unit': inv_per_parent['total'],
7060             })
7061 
7062         return inventories
7063 
7064     def _get_instance_capabilities(self):
7065         """Get hypervisor instance capabilities
7066 
7067         Returns a list of tuples that describe instances the
7068         hypervisor is capable of hosting.  Each tuple consists
7069         of the triplet (arch, hypervisor_type, vm_mode).
7070 
7071         :returns: List of tuples describing instance capabilities
7072         """
7073         caps = self._host.get_capabilities()
7074         instance_caps = list()
7075         for g in caps.guests:
7076             for domain_type in g.domains:
7077                 try:
7078                     instance_cap = (
7079                         fields.Architecture.canonicalize(g.arch),
7080                         fields.HVType.canonicalize(domain_type),
7081                         fields.VMMode.canonicalize(g.ostype))
7082                     instance_caps.append(instance_cap)
7083                 except exception.InvalidArchitectureName:
7084                     # NOTE(danms): Libvirt is exposing a guest arch that nova
7085                     # does not even know about. Avoid aborting here and
7086                     # continue to process the rest.
7087                     pass
7088 
7089         return instance_caps
7090 
7091     def _get_cpu_info(self):
7092         """Get cpuinfo information.
7093 
7094         Obtains cpu feature from virConnect.getCapabilities.
7095 
7096         :return: see above description
7097 
7098         """
7099 
7100         caps = self._host.get_capabilities()
7101         cpu_info = dict()
7102 
7103         cpu_info['arch'] = caps.host.cpu.arch
7104         cpu_info['model'] = caps.host.cpu.model
7105         cpu_info['vendor'] = caps.host.cpu.vendor
7106 
7107         topology = dict()
7108         topology['cells'] = len(getattr(caps.host.topology, 'cells', [1]))
7109         topology['sockets'] = caps.host.cpu.sockets
7110         topology['cores'] = caps.host.cpu.cores
7111         topology['threads'] = caps.host.cpu.threads
7112         cpu_info['topology'] = topology
7113 
7114         features = set()
7115         for f in caps.host.cpu.features:
7116             features.add(f.name)
7117         cpu_info['features'] = features
7118         return cpu_info
7119 
7120     def _get_pcinet_info(
7121         self,
7122         dev: 'libvirt.virNodeDevice',
7123         net_devs: ty.List['libvirt.virNodeDevice']
7124     ) -> ty.Optional[ty.List[str]]:
7125         """Returns a dict of NET device."""
7126         net_dev = {dev.parent(): dev for dev in net_devs}.get(dev.name(), None)
7127         if net_dev is None:
7128             return None
7129         xmlstr = net_dev.XMLDesc(0)
7130         cfgdev = vconfig.LibvirtConfigNodeDevice()
7131         cfgdev.parse_str(xmlstr)
7132         return cfgdev.pci_capability.features
7133 
7134     def _get_pcidev_info(
7135         self,
7136         devname: str,
7137         dev: 'libvirt.virNodeDevice',
7138         net_devs: ty.List['libvirt.virNodeDevice']
7139     ) -> ty.Dict[str, ty.Union[str, dict]]:
7140         """Returns a dict of PCI device."""
7141 
7142         def _get_device_type(
7143             cfgdev: vconfig.LibvirtConfigNodeDevice,
7144             pci_address: str,
7145             device: 'libvirt.virNodeDevice',
7146             net_devs: ty.List['libvirt.virNodeDevice']
7147         ) -> ty.Dict[str, str]:
7148             """Get a PCI device's device type.
7149 
7150             An assignable PCI device can be a normal PCI device,
7151             a SR-IOV Physical Function (PF), or a SR-IOV Virtual
7152             Function (VF).
7153             """
7154             net_dev_parents = {dev.parent() for dev in net_devs}
7155             for fun_cap in cfgdev.pci_capability.fun_capability:
7156                 if fun_cap.type == 'virt_functions':
7157                     return {
7158                         'dev_type': fields.PciDeviceType.SRIOV_PF,
7159                     }
7160                 if (fun_cap.type == 'phys_function' and
7161                     len(fun_cap.device_addrs) != 0):
7162                     phys_address = "%04x:%02x:%02x.%01x" % (
7163                         fun_cap.device_addrs[0][0],
7164                         fun_cap.device_addrs[0][1],
7165                         fun_cap.device_addrs[0][2],
7166                         fun_cap.device_addrs[0][3])
7167                     result = {
7168                         'dev_type': fields.PciDeviceType.SRIOV_VF,
7169                         'parent_addr': phys_address,
7170                     }
7171                     parent_ifname = None
7172                     # NOTE(sean-k-mooney): if the VF is a parent of a netdev
7173                     # the PF should also have a netdev.
7174                     if device.name() in net_dev_parents:
7175                         parent_ifname = pci_utils.get_ifname_by_pci_address(
7176                             pci_address, pf_interface=True)
7177                         result['parent_ifname'] = parent_ifname
7178                     return result
7179 
7180             return {'dev_type': fields.PciDeviceType.STANDARD}
7181 
7182         def _get_device_capabilities(
7183             device_dict: dict,
7184             device: 'libvirt.virNodeDevice',
7185             net_devs: ty.List['libvirt.virNodeDevice']
7186         ) -> ty.Dict[str, ty.Dict[str, ty.Optional[ty.List[str]]]]:
7187             """Get PCI VF device's additional capabilities.
7188 
7189             If a PCI device is a virtual function, this function reads the PCI
7190             parent's network capabilities (must be always a NIC device) and
7191             appends this information to the device's dictionary.
7192             """
7193             if device_dict.get('dev_type') == fields.PciDeviceType.SRIOV_VF:
7194                 pcinet_info = self._get_pcinet_info(device, net_devs)
7195                 if pcinet_info:
7196                     return {'capabilities': {'network': pcinet_info}}
7197             return {}
7198 
7199         xmlstr = dev.XMLDesc(0)
7200         cfgdev = vconfig.LibvirtConfigNodeDevice()
7201         cfgdev.parse_str(xmlstr)
7202 
7203         address = "%04x:%02x:%02x.%1x" % (
7204             cfgdev.pci_capability.domain,
7205             cfgdev.pci_capability.bus,
7206             cfgdev.pci_capability.slot,
7207             cfgdev.pci_capability.function)
7208 
7209         device = {
7210             "dev_id": cfgdev.name,
7211             "address": address,
7212             "product_id": "%04x" % cfgdev.pci_capability.product_id,
7213             "vendor_id": "%04x" % cfgdev.pci_capability.vendor_id,
7214             }
7215 
7216         device["numa_node"] = cfgdev.pci_capability.numa_node
7217 
7218         # requirement by DataBase Model
7219         device['label'] = 'label_%(vendor_id)s_%(product_id)s' % device
7220         device.update(_get_device_type(cfgdev, address, dev, net_devs))
7221         device.update(_get_device_capabilities(device, dev, net_devs))
7222         return device
7223 
7224     def _get_pci_passthrough_devices(self):
7225         """Get host PCI devices information.
7226 
7227         Obtains pci devices information from libvirt, and returns
7228         as a JSON string.
7229 
7230         Each device information is a dictionary, with mandatory keys
7231         of 'address', 'vendor_id', 'product_id', 'dev_type', 'dev_id',
7232         'label' and other optional device specific information.
7233 
7234         Refer to the objects/pci_device.py for more idea of these keys.
7235 
7236         :returns: a JSON string containing a list of the assignable PCI
7237                   devices information
7238         """
7239         dev_flags = (libvirt.VIR_CONNECT_LIST_NODE_DEVICES_CAP_NET |
7240                      libvirt.VIR_CONNECT_LIST_NODE_DEVICES_CAP_PCI_DEV)
7241         devices = {dev.name(): dev for dev in
7242                    self._host.list_all_devices(flags=dev_flags)}
7243         net_devs = [dev for dev in devices.values() if "net" in dev.listCaps()]
7244         pci_info = [self._get_pcidev_info(name, dev, net_devs) for name, dev
7245                     in devices.items() if "pci" in dev.listCaps()]
7246 
7247         return jsonutils.dumps(pci_info)
7248 
7249     def _get_mdev_capabilities_for_dev(self, devname, types=None):
7250         """Returns a dict of MDEV capable device with the ID as first key
7251         and then a list of supported types, each of them being a dict.
7252 
7253         :param types: Only return those specific types.
7254         """
7255         virtdev = self._host.device_lookup_by_name(devname)
7256         xmlstr = virtdev.XMLDesc(0)
7257         cfgdev = vconfig.LibvirtConfigNodeDevice()
7258         cfgdev.parse_str(xmlstr)
7259 
7260         device = {
7261             "dev_id": cfgdev.name,
7262             "types": {},
7263             "vendor_id": cfgdev.pci_capability.vendor_id,
7264         }
7265         for mdev_cap in cfgdev.pci_capability.mdev_capability:
7266             for cap in mdev_cap.mdev_types:
7267                 if not types or cap['type'] in types:
7268                     device["types"].update({cap['type']: {
7269                         'availableInstances': cap['availableInstances'],
7270                         # This attribute is optional
7271                         'name': cap.get('name'),
7272                         'deviceAPI': cap['deviceAPI']}})
7273         return device
7274 
7275     def _get_mdev_capable_devices(self, types=None):
7276         """Get host devices supporting mdev types.
7277 
7278         Obtain devices information from libvirt and returns a list of
7279         dictionaries.
7280 
7281         :param types: Filter only devices supporting those types.
7282         """
7283         dev_names = self._host.list_mdev_capable_devices() or []
7284         mdev_capable_devices = []
7285         for name in dev_names:
7286             device = self._get_mdev_capabilities_for_dev(name, types)
7287             if not device["types"]:
7288                 continue
7289             mdev_capable_devices.append(device)
7290         return mdev_capable_devices
7291 
7292     def _get_mediated_device_information(self, devname):
7293         """Returns a dict of a mediated device."""
7294         virtdev = self._host.device_lookup_by_name(devname)
7295         xmlstr = virtdev.XMLDesc(0)
7296         cfgdev = vconfig.LibvirtConfigNodeDevice()
7297         cfgdev.parse_str(xmlstr)
7298 
7299         device = {
7300             "dev_id": cfgdev.name,
7301             # name is like mdev_00ead764_fdc0_46b6_8db9_2963f5c815b4
7302             "uuid": libvirt_utils.mdev_name2uuid(cfgdev.name),
7303             # the physical GPU PCI device
7304             "parent": cfgdev.parent,
7305             "type": cfgdev.mdev_information.type,
7306             "iommu_group": cfgdev.mdev_information.iommu_group,
7307         }
7308         return device
7309 
7310     def _get_mediated_devices(self, types=None):
7311         """Get host mediated devices.
7312 
7313         Obtain devices information from libvirt and returns a list of
7314         dictionaries.
7315 
7316         :param types: Filter only devices supporting those types.
7317         """
7318         dev_names = self._host.list_mediated_devices() or []
7319         mediated_devices = []
7320         for name in dev_names:
7321             device = self._get_mediated_device_information(name)
7322             if not types or device["type"] in types:
7323                 mediated_devices.append(device)
7324         return mediated_devices
7325 
7326     def _get_all_assigned_mediated_devices(self, instance=None):
7327         """Lookup all instances from the host and return all the mediated
7328         devices that are assigned to a guest.
7329 
7330         :param instance: Only return mediated devices for that instance.
7331 
7332         :returns: A dictionary of keys being mediated device UUIDs and their
7333                   respective values the instance UUID of the guest using it.
7334                   Returns an empty dict if an instance is provided but not
7335                   found in the hypervisor.
7336         """
7337         allocated_mdevs = {}
7338         if instance:
7339             # NOTE(sbauza): In some cases (like a migration issue), the
7340             # instance can exist in the Nova database but libvirt doesn't know
7341             # about it. For such cases, the way to fix that is to hard reboot
7342             # the instance, which will recreate the libvirt guest.
7343             # For that reason, we need to support that case by making sure
7344             # we don't raise an exception if the libvirt guest doesn't exist.
7345             try:
7346                 guest = self._host.get_guest(instance)
7347             except exception.InstanceNotFound:
7348                 # Bail out early if libvirt doesn't know about it since we
7349                 # can't know the existing mediated devices
7350                 return {}
7351             guests = [guest]
7352         else:
7353             guests = self._host.list_guests(only_running=False)
7354         for guest in guests:
7355             cfg = guest.get_config()
7356             for device in cfg.devices:
7357                 if isinstance(device, vconfig.LibvirtConfigGuestHostdevMDEV):
7358                     allocated_mdevs[device.uuid] = guest.uuid
7359         return allocated_mdevs
7360 
7361     @staticmethod
7362     def _vgpu_allocations(allocations):
7363         """Filtering only the VGPU allocations from a list of allocations.
7364 
7365         :param allocations: Information about resources allocated to the
7366                             instance via placement, of the form returned by
7367                             SchedulerReportClient.get_allocations_for_consumer.
7368         """
7369         if not allocations:
7370             # If no allocations, there is no vGPU request.
7371             return {}
7372         RC_VGPU = orc.VGPU
7373         vgpu_allocations = {}
7374         for rp in allocations:
7375             res = allocations[rp]['resources']
7376             if RC_VGPU in res and res[RC_VGPU] > 0:
7377                 vgpu_allocations[rp] = {'resources': {RC_VGPU: res[RC_VGPU]}}
7378         return vgpu_allocations
7379 
7380     def _get_existing_mdevs_not_assigned(self, parent, requested_types=None):
7381         """Returns the already created mediated devices that are not assigned
7382         to a guest yet.
7383 
7384         :param parent: Filter out result for only mdevs from the parent device.
7385         :param requested_types: Filter out the result for only mediated devices
7386                                 having those types.
7387         """
7388         allocated_mdevs = self._get_all_assigned_mediated_devices()
7389         mdevs = self._get_mediated_devices(requested_types)
7390         available_mdevs = set()
7391         for mdev in mdevs:
7392             parent_vgpu_type = self._get_vgpu_type_per_pgpu(mdev['parent'])
7393             if mdev['type'] != parent_vgpu_type:
7394                 # This mdev is using a vGPU type that is not supported by the
7395                 # configuration option for its pGPU parent, so we can't use it.
7396                 continue
7397             # FIXME(sbauza): No longer accept the parent value to be nullable
7398             # once we fix the reshape functional test
7399             if parent is None or mdev['parent'] == parent:
7400                 available_mdevs.add(mdev["uuid"])
7401 
7402         available_mdevs -= set(allocated_mdevs)
7403         return available_mdevs
7404 
7405     def _create_new_mediated_device(self, parent, uuid=None):
7406         """Find a physical device that can support a new mediated device and
7407         create it.
7408 
7409         :param parent: The libvirt name of the parent GPU, eg. pci_0000_06_00_0
7410         :param uuid: The possible mdev UUID we want to create again
7411 
7412         :returns: the newly created mdev UUID or None if not possible
7413         """
7414         supported_types = self.supported_vgpu_types
7415         # Try to see if we can still create a new mediated device
7416         devices = self._get_mdev_capable_devices(supported_types)
7417         for device in devices:
7418             dev_name = device['dev_id']
7419             # FIXME(sbauza): No longer accept the parent value to be nullable
7420             # once we fix the reshape functional test
7421             if parent is not None and dev_name != parent:
7422                 # The device is not the one that was called, not creating
7423                 # the mdev
7424                 continue
7425             dev_supported_type = self._get_vgpu_type_per_pgpu(dev_name)
7426             if dev_supported_type and device['types'][
7427                     dev_supported_type]['availableInstances'] > 0:
7428                 # That physical GPU has enough room for a new mdev
7429                 # We need the PCI address, not the libvirt name
7430                 # The libvirt name is like 'pci_0000_84_00_0'
7431                 pci_addr = "{}:{}:{}.{}".format(*dev_name[4:].split('_'))
7432                 chosen_mdev = nova.privsep.libvirt.create_mdev(
7433                     pci_addr, dev_supported_type, uuid=uuid)
7434                 return chosen_mdev
7435 
7436     @utils.synchronized(VGPU_RESOURCE_SEMAPHORE)
7437     def _allocate_mdevs(self, allocations):
7438         """Returns a list of mediated device UUIDs corresponding to available
7439         resources we can assign to the guest(s) corresponding to the allocation
7440         requests passed as argument.
7441 
7442         That method can either find an existing but unassigned mediated device
7443         it can allocate, or create a new mediated device from a capable
7444         physical device if the latter has enough left capacity.
7445 
7446         :param allocations: Information about resources allocated to the
7447                             instance via placement, of the form returned by
7448                             SchedulerReportClient.get_allocations_for_consumer.
7449                             That code is supporting Placement API version 1.12
7450         """
7451         vgpu_allocations = self._vgpu_allocations(allocations)
7452         if not vgpu_allocations:
7453             return
7454         # TODO(sbauza): For the moment, we only support allocations for only
7455         # one pGPU.
7456         if len(vgpu_allocations) > 1:
7457             LOG.warning('More than one allocation was passed over to libvirt '
7458                         'while at the moment libvirt only supports one. Only '
7459                         'the first allocation will be looked up.')
7460         rp_uuid, alloc = next(iter(vgpu_allocations.items()))
7461         vgpus_asked = alloc['resources'][orc.VGPU]
7462 
7463         # Find if we allocated against a specific pGPU (and then the allocation
7464         # is made against a child RP) or any pGPU (in case the VGPU inventory
7465         # is still on the root RP)
7466         try:
7467             allocated_rp = self.provider_tree.data(rp_uuid)
7468         except ValueError:
7469             # The provider doesn't exist, return a better understandable
7470             # exception
7471             raise exception.ComputeResourcesUnavailable(
7472                 reason='vGPU resource is not available')
7473         # FIXME(sbauza): The functional reshape test assumes that we could
7474         # run _allocate_mdevs() against non-nested RPs but this is impossible
7475         # as all inventories have been reshaped *before now* since it's done
7476         # on init_host() (when the compute restarts or whatever else calls it).
7477         # That said, since fixing the functional test isn't easy yet, let's
7478         # assume we still support a non-nested RP for now.
7479         if allocated_rp.parent_uuid is None:
7480             # We are on a root RP
7481             parent_device = None
7482         else:
7483             rp_name = allocated_rp.name
7484             # There can be multiple roots, we need to find the root name
7485             # to guess the physical device name
7486             roots = list(self.provider_tree.roots)
7487             for root in roots:
7488                 if rp_name.startswith(root.name + '_'):
7489                     # The RP name convention is :
7490                     #    root_name + '_' + parent_device
7491                     parent_device = rp_name[len(root.name) + 1:]
7492                     break
7493             else:
7494                 LOG.warning(
7495                     "pGPU device name %(name)s can't be guessed from the "
7496                     "ProviderTree roots %(roots)s",
7497                     {'name': rp_name,
7498                      'roots': ', '.join([root.name for root in roots])})
7499                 # We f... have no idea what was the parent device
7500                 # If we can't find devices having available VGPUs, just raise
7501                 raise exception.ComputeResourcesUnavailable(
7502                     reason='vGPU resource is not available')
7503 
7504         supported_types = self.supported_vgpu_types
7505         # Which mediated devices are created but not assigned to a guest ?
7506         mdevs_available = self._get_existing_mdevs_not_assigned(
7507             parent_device, supported_types)
7508 
7509         chosen_mdevs = []
7510         for c in range(vgpus_asked):
7511             chosen_mdev = None
7512             if mdevs_available:
7513                 # Take the first available mdev
7514                 chosen_mdev = mdevs_available.pop()
7515             else:
7516                 chosen_mdev = self._create_new_mediated_device(parent_device)
7517             if not chosen_mdev:
7518                 # If we can't find devices having available VGPUs, just raise
7519                 raise exception.ComputeResourcesUnavailable(
7520                     reason='vGPU resource is not available')
7521             else:
7522                 chosen_mdevs.append(chosen_mdev)
7523         return chosen_mdevs
7524 
7525     def _detach_mediated_devices(self, guest):
7526         mdevs = guest.get_all_devices(
7527             devtype=vconfig.LibvirtConfigGuestHostdevMDEV)
7528         for mdev_cfg in mdevs:
7529             try:
7530                 guest.detach_device(mdev_cfg, live=True)
7531             except libvirt.libvirtError as ex:
7532                 error_code = ex.get_error_code()
7533                 # NOTE(sbauza): There is a pending issue with libvirt that
7534                 # doesn't allow to hot-unplug mediated devices. Let's
7535                 # short-circuit the suspend action and set the instance back
7536                 # to ACTIVE.
7537                 # TODO(sbauza): Once libvirt supports this, amend the resume()
7538                 # operation to support reallocating mediated devices.
7539                 if error_code == libvirt.VIR_ERR_CONFIG_UNSUPPORTED:
7540                     reason = _("Suspend is not supported for instances having "
7541                                "attached vGPUs.")
7542                     raise exception.InstanceFaultRollback(
7543                         exception.InstanceSuspendFailure(reason=reason))
7544                 else:
7545                     raise
7546 
7547     def _has_numa_support(self):
7548         # This means that the host can support LibvirtConfigGuestNUMATune
7549         # and the nodeset field in LibvirtConfigGuestMemoryBackingPage
7550         caps = self._host.get_capabilities()
7551 
7552         if (caps.host.cpu.arch in (fields.Architecture.I686,
7553                                    fields.Architecture.X86_64,
7554                                    fields.Architecture.AARCH64) and
7555                 self._host.has_min_version(hv_type=host.HV_DRIVER_QEMU)):
7556             return True
7557         elif (caps.host.cpu.arch in (fields.Architecture.PPC64,
7558                                      fields.Architecture.PPC64LE)):
7559             return True
7560 
7561         return False
7562 
7563     def _get_host_numa_topology(self):
7564         if not self._has_numa_support():
7565             return
7566 
7567         caps = self._host.get_capabilities()
7568         topology = caps.host.topology
7569 
7570         if topology is None or not topology.cells:
7571             return
7572 
7573         cells = []
7574 
7575         available_shared_cpus = self._get_vcpu_available()
7576         available_dedicated_cpus = self._get_pcpu_available()
7577 
7578         # NOTE(stephenfin): In an ideal world, if the operator had not
7579         # configured this host to report PCPUs using the '[compute]
7580         # cpu_dedicated_set' option, then we should not be able to used pinned
7581         # instances on this host. However, that would force operators to update
7582         # their configuration as part of the Stein -> Train upgrade or be
7583         # unable to schedule instances on the host. As a result, we need to
7584         # revert to legacy behavior and use 'vcpu_pin_set' for both VCPUs and
7585         # PCPUs.
7586         # TODO(stephenfin): Remove this in U
7587         if not available_dedicated_cpus and not (
7588                 CONF.compute.cpu_shared_set and not CONF.vcpu_pin_set):
7589             available_dedicated_cpus = available_shared_cpus
7590 
7591         def _get_reserved_memory_for_cell(self, cell_id, page_size):
7592             cell = self._reserved_hugepages.get(cell_id, {})
7593             return cell.get(page_size, 0)
7594 
7595         def _get_physnet_numa_affinity():
7596             affinities: ty.Dict[int, ty.Set[str]] = {
7597                 cell.id: set() for cell in topology.cells
7598             }
7599             for physnet in CONF.neutron.physnets:
7600                 # This will error out if the group is not registered, which is
7601                 # exactly what we want as that would be a bug
7602                 group = getattr(CONF, 'neutron_physnet_%s' % physnet)
7603 
7604                 if not group.numa_nodes:
7605                     msg = ("the physnet '%s' was listed in '[neutron] "
7606                            "physnets' but no corresponding "
7607                            "'[neutron_physnet_%s] numa_nodes' option was "
7608                            "defined." % (physnet, physnet))
7609                     raise exception.InvalidNetworkNUMAAffinity(reason=msg)
7610 
7611                 for node in group.numa_nodes:
7612                     if node not in affinities:
7613                         msg = ("node %d for physnet %s is not present in host "
7614                                "affinity set %r" % (node, physnet, affinities))
7615                         # The config option referenced an invalid node
7616                         raise exception.InvalidNetworkNUMAAffinity(reason=msg)
7617                     affinities[node].add(physnet)
7618 
7619             return affinities
7620 
7621         def _get_tunnel_numa_affinity():
7622             affinities = {cell.id: False for cell in topology.cells}
7623 
7624             for node in CONF.neutron_tunnel.numa_nodes:
7625                 if node not in affinities:
7626                     msg = ("node %d for tunneled networks is not present "
7627                            "in host affinity set %r" % (node, affinities))
7628                     # The config option referenced an invalid node
7629                     raise exception.InvalidNetworkNUMAAffinity(reason=msg)
7630                 affinities[node] = True
7631 
7632             return affinities
7633 
7634         physnet_affinities = _get_physnet_numa_affinity()
7635         tunnel_affinities = _get_tunnel_numa_affinity()
7636 
7637         for cell in topology.cells:
7638             cpus = set(cpu.id for cpu in cell.cpus)
7639 
7640             cpuset = cpus & available_shared_cpus
7641             pcpuset = cpus & available_dedicated_cpus
7642 
7643             # de-duplicate and sort the list of CPU sibling sets
7644             siblings = sorted(
7645                 set(x) for x in set(
7646                     tuple(cpu.siblings) or () for cpu in cell.cpus
7647                 )
7648             )
7649 
7650             cpus &= available_shared_cpus | available_dedicated_cpus
7651             siblings = [sib & cpus for sib in siblings]
7652             # Filter out empty sibling sets that may be left
7653             siblings = [sib for sib in siblings if len(sib) > 0]
7654 
7655             mempages = [
7656                 objects.NUMAPagesTopology(
7657                     size_kb=pages.size,
7658                     total=pages.total,
7659                     used=0,
7660                     reserved=_get_reserved_memory_for_cell(
7661                         self, cell.id, pages.size))
7662                 for pages in cell.mempages]
7663 
7664             network_metadata = objects.NetworkMetadata(
7665                 physnets=physnet_affinities[cell.id],
7666                 tunneled=tunnel_affinities[cell.id])
7667 
7668             # NOTE(stephenfin): Note that we don't actually return any usage
7669             # information here. This is because this is handled by the resource
7670             # tracker via the 'update_available_resource' periodic task, which
7671             # loops through all instances and calculated usage accordingly
7672             cell = objects.NUMACell(
7673                 id=cell.id,
7674                 cpuset=cpuset,
7675                 pcpuset=pcpuset,
7676                 memory=cell.memory / units.Ki,
7677                 cpu_usage=0,
7678                 pinned_cpus=set(),
7679                 memory_usage=0,
7680                 siblings=siblings,
7681                 mempages=mempages,
7682                 network_metadata=network_metadata)
7683             cells.append(cell)
7684 
7685         return objects.NUMATopology(cells=cells)
7686 
7687     def get_all_volume_usage(self, context, compute_host_bdms):
7688         """Return usage info for volumes attached to vms on
7689            a given host.
7690         """
7691         vol_usage = []
7692 
7693         for instance_bdms in compute_host_bdms:
7694             instance = instance_bdms['instance']
7695 
7696             for bdm in instance_bdms['instance_bdms']:
7697                 mountpoint = bdm['device_name']
7698                 if mountpoint.startswith('/dev/'):
7699                     mountpoint = mountpoint[5:]
7700                 volume_id = bdm['volume_id']
7701 
7702                 LOG.debug("Trying to get stats for the volume %s",
7703                           volume_id, instance=instance)
7704                 vol_stats = self.block_stats(instance, mountpoint)
7705 
7706                 if vol_stats:
7707                     stats = dict(volume=volume_id,
7708                                  instance=instance,
7709                                  rd_req=vol_stats[0],
7710                                  rd_bytes=vol_stats[1],
7711                                  wr_req=vol_stats[2],
7712                                  wr_bytes=vol_stats[3])
7713                     LOG.debug(
7714                         "Got volume usage stats for the volume=%(volume)s,"
7715                         " rd_req=%(rd_req)d, rd_bytes=%(rd_bytes)d, "
7716                         "wr_req=%(wr_req)d, wr_bytes=%(wr_bytes)d",
7717                         stats, instance=instance)
7718                     vol_usage.append(stats)
7719 
7720         return vol_usage
7721 
7722     def block_stats(self, instance, disk_id):
7723         """Note that this function takes an instance name."""
7724         try:
7725             guest = self._host.get_guest(instance)
7726             dev = guest.get_block_device(disk_id)
7727             return dev.blockStats()
7728         except libvirt.libvirtError as e:
7729             errcode = e.get_error_code()
7730             LOG.info('Getting block stats failed, device might have '
7731                      'been detached. Instance=%(instance_name)s '
7732                      'Disk=%(disk)s Code=%(errcode)s Error=%(e)s',
7733                      {'instance_name': instance.name, 'disk': disk_id,
7734                       'errcode': errcode, 'e': e},
7735                      instance=instance)
7736         except exception.InstanceNotFound:
7737             LOG.info('Could not find domain in libvirt for instance %s. '
7738                      'Cannot get block stats for device', instance.name,
7739                      instance=instance)
7740 
7741     def update_provider_tree(self, provider_tree, nodename, allocations=None):
7742         """Update a ProviderTree object with current resource provider,
7743         inventory information and CPU traits.
7744 
7745         :param nova.compute.provider_tree.ProviderTree provider_tree:
7746             A nova.compute.provider_tree.ProviderTree object representing all
7747             the providers in the tree associated with the compute node, and any
7748             sharing providers (those with the ``MISC_SHARES_VIA_AGGREGATE``
7749             trait) associated via aggregate with any of those providers (but
7750             not *their* tree- or aggregate-associated providers), as currently
7751             known by placement.
7752         :param nodename:
7753             String name of the compute node (i.e.
7754             ComputeNode.hypervisor_hostname) for which the caller is requesting
7755             updated provider information.
7756         :param allocations:
7757             Dict of allocation data of the form:
7758               { $CONSUMER_UUID: {
7759                     # The shape of each "allocations" dict below is identical
7760                     # to the return from GET /allocations/{consumer_uuid}
7761                     "allocations": {
7762                         $RP_UUID: {
7763                             "generation": $RP_GEN,
7764                             "resources": {
7765                                 $RESOURCE_CLASS: $AMOUNT,
7766                                 ...
7767                             },
7768                         },
7769                         ...
7770                     },
7771                     "project_id": $PROJ_ID,
7772                     "user_id": $USER_ID,
7773                     "consumer_generation": $CONSUMER_GEN,
7774                 },
7775                 ...
7776               }
7777             If None, and the method determines that any inventory needs to be
7778             moved (from one provider to another and/or to a different resource
7779             class), the ReshapeNeeded exception must be raised. Otherwise, this
7780             dict must be edited in place to indicate the desired final state of
7781             allocations.
7782         :raises ReshapeNeeded: If allocations is None and any inventory needs
7783             to be moved from one provider to another and/or to a different
7784             resource class.
7785         :raises: ReshapeFailed if the requested tree reshape fails for
7786             whatever reason.
7787         """
7788         disk_gb = int(self._get_local_gb_info()['total'])
7789         memory_mb = int(self._host.get_memory_mb_total())
7790         vcpus = len(self._get_vcpu_available())
7791         pcpus = len(self._get_pcpu_available())
7792         memory_enc_slots = self._get_memory_encrypted_slots()
7793 
7794         # NOTE(yikun): If the inv record does not exists, the allocation_ratio
7795         # will use the CONF.xxx_allocation_ratio value if xxx_allocation_ratio
7796         # is set, and fallback to use the initial_xxx_allocation_ratio
7797         # otherwise.
7798         inv = provider_tree.data(nodename).inventory
7799         ratios = self._get_allocation_ratios(inv)
7800         resources: ty.Dict[str, ty.Set['objects.Resource']] = (
7801             collections.defaultdict(set)
7802         )
7803 
7804         result = {}
7805         if memory_mb:
7806             result[orc.MEMORY_MB] = {
7807                 'total': memory_mb,
7808                 'min_unit': 1,
7809                 'max_unit': memory_mb,
7810                 'step_size': 1,
7811                 'allocation_ratio': ratios[orc.MEMORY_MB],
7812                 'reserved': CONF.reserved_host_memory_mb,
7813             }
7814 
7815         # NOTE(stephenfin): We have to optionally report these since placement
7816         # forbids reporting inventory with total=0
7817         if vcpus:
7818             result[orc.VCPU] = {
7819                 'total': vcpus,
7820                 'min_unit': 1,
7821                 'max_unit': vcpus,
7822                 'step_size': 1,
7823                 'allocation_ratio': ratios[orc.VCPU],
7824                 'reserved': CONF.reserved_host_cpus,
7825             }
7826 
7827         if pcpus:
7828             result[orc.PCPU] = {
7829                 'total': pcpus,
7830                 'min_unit': 1,
7831                 'max_unit': pcpus,
7832                 'step_size': 1,
7833                 'allocation_ratio': 1,
7834                 'reserved': 0,
7835             }
7836 
7837         if memory_enc_slots:
7838             result[orc.MEM_ENCRYPTION_CONTEXT] = {
7839                 'total': memory_enc_slots,
7840                 'min_unit': 1,
7841                 'max_unit': 1,
7842                 'step_size': 1,
7843                 'allocation_ratio': 1.0,
7844                 'reserved': 0,
7845             }
7846 
7847         # If a sharing DISK_GB provider exists in the provider tree, then our
7848         # storage is shared, and we should not report the DISK_GB inventory in
7849         # the compute node provider.
7850         # TODO(efried): Reinstate non-reporting of shared resource by the
7851         # compute RP once the issues from bug #1784020 have been resolved.
7852         if provider_tree.has_sharing_provider(orc.DISK_GB):
7853             LOG.debug('Ignoring sharing provider - see bug #1784020')
7854 
7855         if disk_gb:
7856             result[orc.DISK_GB] = {
7857                 'total': disk_gb,
7858                 'min_unit': 1,
7859                 'max_unit': disk_gb,
7860                 'step_size': 1,
7861                 'allocation_ratio': ratios[orc.DISK_GB],
7862                 'reserved': (self._get_reserved_host_disk_gb_from_config() +
7863                              self._get_disk_size_reserved_for_image_cache()),
7864             }
7865 
7866         # TODO(sbauza): Use traits to providing vGPU types. For the moment,
7867         # it will be only documentation support by explaining to use
7868         # osc-placement to create custom traits for each of the pGPU RPs.
7869         self._update_provider_tree_for_vgpu(
7870            provider_tree, nodename, allocations=allocations)
7871 
7872         self._update_provider_tree_for_pcpu(
7873             provider_tree, nodename, allocations=allocations)
7874 
7875         self._update_provider_tree_for_vpmems(
7876             provider_tree, nodename, result, resources)
7877 
7878         provider_tree.update_inventory(nodename, result)
7879         provider_tree.update_resources(nodename, resources)
7880 
7881         # Add supported traits i.e. those equal to True to provider tree while
7882         # removing the unsupported ones
7883         traits_to_add = [
7884             t for t in self.static_traits if self.static_traits[t]
7885         ]
7886         traits_to_remove = set(self.static_traits) - set(traits_to_add)
7887         provider_tree.add_traits(nodename, *traits_to_add)
7888         provider_tree.remove_traits(nodename, *traits_to_remove)
7889 
7890         # Now that we updated the ProviderTree, we want to store it locally
7891         # so that spawn() or other methods can access it thru a getter
7892         self.provider_tree = copy.deepcopy(provider_tree)
7893 
7894     def _update_provider_tree_for_vpmems(self, provider_tree, nodename,
7895                                          inventory, resources):
7896         """Update resources and inventory for vpmems in provider tree."""
7897         prov_data = provider_tree.data(nodename)
7898         for rc, vpmems in self._vpmems_by_rc.items():
7899             # Skip (and omit) inventories with total=0 because placement does
7900             # not allow setting total=0 for inventory.
7901             if not len(vpmems):
7902                 continue
7903             inventory[rc] = {
7904                 'total': len(vpmems),
7905                 'max_unit': len(vpmems),
7906                 'min_unit': 1,
7907                 'step_size': 1,
7908                 'allocation_ratio': 1.0,
7909                 'reserved': 0
7910             }
7911             for vpmem in vpmems:
7912                 resource_obj = objects.Resource(
7913                     provider_uuid=prov_data.uuid,
7914                     resource_class=rc,
7915                     identifier=vpmem.name,
7916                     metadata=vpmem)
7917                 resources[rc].add(resource_obj)
7918 
7919     def _get_memory_encrypted_slots(self):
7920         slots = CONF.libvirt.num_memory_encrypted_guests
7921         if not self._host.supports_amd_sev:
7922             if slots and slots > 0:
7923                 LOG.warning("Host is configured with "
7924                             "libvirt.num_memory_encrypted_guests set to "
7925                             "%d, but is not SEV-capable.", slots)
7926             return 0
7927 
7928         # NOTE(aspiers): Auto-detection of the number of available
7929         # slots for AMD SEV is not yet possible, so honor the
7930         # configured value, or impose no limit if this is not
7931         # specified.  This does incur a risk that if operators don't
7932         # read the instructions and configure the maximum correctly,
7933         # the maximum could be exceeded resulting in SEV guests
7934         # failing at launch-time.  However at least SEV guests will
7935         # launch until the maximum, and when auto-detection code is
7936         # added later, an upgrade will magically fix the issue.
7937         #
7938         # Note also that the configured value can be 0 on an
7939         # SEV-capable host, since there might conceivably be good
7940         # reasons for the operator to want to disable SEV even when
7941         # it's available (e.g. due to performance impact, or
7942         # implementation bugs which may surface later).
7943         if slots is not None:
7944             return slots
7945         else:
7946             return db_const.MAX_INT
7947 
7948     @property
7949     def static_traits(self) -> ty.Dict[str, bool]:
7950         if self._static_traits is not None:
7951             return self._static_traits
7952 
7953         traits: ty.Dict[str, bool] = {}
7954         traits.update(self._get_cpu_traits())
7955         traits.update(self._get_storage_bus_traits())
7956         traits.update(self._get_video_model_traits())
7957         traits.update(self._get_vif_model_traits())
7958         traits.update(self._get_tpm_traits())
7959 
7960         _, invalid_traits = ot.check_traits(traits)
7961         for invalid_trait in invalid_traits:
7962             LOG.debug("Trait '%s' is not valid; ignoring.", invalid_trait)
7963             del traits[invalid_trait]
7964 
7965         self._static_traits = traits
7966 
7967         return self._static_traits
7968 
7969     @staticmethod
7970     def _is_reshape_needed_vgpu_on_root(provider_tree, nodename):
7971         """Determine if root RP has VGPU inventories.
7972 
7973         Check to see if the root compute node provider in the tree for
7974         this host already has VGPU inventory because if it does, we either
7975         need to signal for a reshape (if _update_provider_tree_for_vgpu()
7976         has no allocations) or move the allocations within the ProviderTree if
7977         passed.
7978 
7979         :param provider_tree: The ProviderTree object for this host.
7980         :param nodename: The ComputeNode.hypervisor_hostname, also known as
7981             the name of the root node provider in the tree for this host.
7982         :returns: boolean, whether we have VGPU root inventory.
7983         """
7984         root_node = provider_tree.data(nodename)
7985         return orc.VGPU in root_node.inventory
7986 
7987     @staticmethod
7988     def _ensure_pgpu_providers(inventories_dict, provider_tree, nodename):
7989         """Ensures GPU inventory providers exist in the tree for $nodename.
7990 
7991         GPU providers are named $nodename_$gpu-device-id, e.g.
7992         ``somehost.foo.bar.com_pci_0000_84_00_0``.
7993 
7994         :param inventories_dict: Dictionary of inventories for VGPU class
7995             directly provided by _get_gpu_inventories() and which looks like:
7996                 {'pci_0000_84_00_0':
7997                     {'total': $TOTAL,
7998                      'min_unit': 1,
7999                      'max_unit': $MAX_UNIT, # defaults to $TOTAL
8000                      'step_size': 1,
8001                      'reserved': 0,
8002                      'allocation_ratio': 1.0,
8003                     }
8004                 }
8005         :param provider_tree: The ProviderTree to update.
8006         :param nodename: The ComputeNode.hypervisor_hostname, also known as
8007             the name of the root node provider in the tree for this host.
8008         :returns: dict, keyed by GPU device ID, to ProviderData object
8009             representing that resource provider in the tree
8010         """
8011         # Create the VGPU child providers if they do not already exist.
8012         # Dict of PGPU RPs keyed by their libvirt PCI name
8013         pgpu_rps = {}
8014         for pgpu_dev_id, inventory in inventories_dict.items():
8015             # Skip (and omit) inventories with total=0 because placement does
8016             # not allow setting total=0 for inventory.
8017             if not inventory['total']:
8018                 continue
8019             # For each physical GPU, we make sure to have a child provider
8020             pgpu_rp_name = '%s_%s' % (nodename, pgpu_dev_id)
8021             if not provider_tree.exists(pgpu_rp_name):
8022                 # This is the first time creating the child provider so add
8023                 # it to the tree under the root node provider.
8024                 provider_tree.new_child(pgpu_rp_name, nodename)
8025             # We want to idempotently return the resource providers with VGPUs
8026             pgpu_rp = provider_tree.data(pgpu_rp_name)
8027             pgpu_rps[pgpu_dev_id] = pgpu_rp
8028 
8029             # The VGPU inventory goes on a child provider of the given root
8030             # node, identified by $nodename.
8031             pgpu_inventory = {orc.VGPU: inventory}
8032             provider_tree.update_inventory(pgpu_rp_name, pgpu_inventory)
8033         return pgpu_rps
8034 
8035     @staticmethod
8036     def _assert_is_root_provider(
8037             rp_uuid, root_node, consumer_uuid, alloc_data):
8038         """Asserts during a reshape that rp_uuid is for the root node provider.
8039 
8040         When reshaping, inventory and allocations should be on the root node
8041         provider and then moved to child providers.
8042 
8043         :param rp_uuid: UUID of the provider that holds inventory/allocations.
8044         :param root_node: ProviderData object representing the root node in a
8045             provider tree.
8046         :param consumer_uuid: UUID of the consumer (instance) holding resource
8047             allocations against the given rp_uuid provider.
8048         :param alloc_data: dict of allocation data for the consumer.
8049         :raises: ReshapeFailed if rp_uuid is not the root node indicating a
8050             reshape was needed but the inventory/allocation structure is not
8051             expected.
8052         """
8053         if rp_uuid != root_node.uuid:
8054             # Something is wrong - VGPU inventory should
8055             # only be on the root node provider if we are
8056             # reshaping the tree.
8057             msg = (_('Unexpected VGPU resource allocation '
8058                      'on provider %(rp_uuid)s for consumer '
8059                      '%(consumer_uuid)s: %(alloc_data)s. '
8060                      'Expected VGPU allocation to be on root '
8061                      'compute node provider %(root_uuid)s.')
8062                    % {'rp_uuid': rp_uuid,
8063                       'consumer_uuid': consumer_uuid,
8064                       'alloc_data': alloc_data,
8065                       'root_uuid': root_node.uuid})
8066             raise exception.ReshapeFailed(error=msg)
8067 
8068     def _get_assigned_mdevs_for_reshape(
8069             self, instance_uuid, rp_uuid, alloc_data):
8070         """Gets the mediated devices assigned to the instance during a reshape.
8071 
8072         :param instance_uuid: UUID of the instance consuming VGPU resources
8073             on this host.
8074         :param rp_uuid: UUID of the resource provider with VGPU inventory being
8075             consumed by the instance.
8076         :param alloc_data: dict of allocation data for the instance consumer.
8077         :return: list of mediated device UUIDs assigned to the instance
8078         :raises: ReshapeFailed if the instance is not found in the hypervisor
8079             or no mediated devices were found to be assigned to the instance
8080             indicating VGPU allocations are out of sync with the hypervisor
8081         """
8082         # FIXME(sbauza): We don't really need an Instance
8083         # object, but given some libvirt.host logs needs
8084         # to have an instance name, just provide a fake one
8085         Instance = collections.namedtuple('Instance', ['uuid', 'name'])
8086         instance = Instance(uuid=instance_uuid, name=instance_uuid)
8087         mdevs = self._get_all_assigned_mediated_devices(instance)
8088         # _get_all_assigned_mediated_devices returns {} if the instance is
8089         # not found in the hypervisor
8090         if not mdevs:
8091             # If we found a VGPU allocation against a consumer
8092             # which is not an instance, the only left case for
8093             # Nova would be a migration but we don't support
8094             # this at the moment.
8095             msg = (_('Unexpected VGPU resource allocation on provider '
8096                      '%(rp_uuid)s for consumer %(consumer_uuid)s: '
8097                      '%(alloc_data)s. The allocation is made against a '
8098                      'non-existing instance or there are no devices assigned.')
8099                    % {'rp_uuid': rp_uuid, 'consumer_uuid': instance_uuid,
8100                       'alloc_data': alloc_data})
8101             raise exception.ReshapeFailed(error=msg)
8102         return mdevs
8103 
8104     def _count_vgpus_per_pgpu(self, mdev_uuids):
8105         """Count the number of VGPUs per physical GPU mediated device.
8106 
8107         :param mdev_uuids: List of physical GPU mediated device UUIDs.
8108         :return: dict, keyed by PGPU device ID, to count of VGPUs on that
8109             device
8110         """
8111         vgpu_count_per_pgpu: ty.Dict[str, int] = collections.defaultdict(int)
8112         for mdev_uuid in mdev_uuids:
8113             # libvirt name is like mdev_00ead764_fdc0_46b6_8db9_2963f5c815b4
8114             dev_name = libvirt_utils.mdev_uuid2name(mdev_uuid)
8115             # Count how many vGPUs are in use for this instance
8116             dev_info = self._get_mediated_device_information(dev_name)
8117             pgpu_dev_id = dev_info['parent']
8118             vgpu_count_per_pgpu[pgpu_dev_id] += 1
8119         return vgpu_count_per_pgpu
8120 
8121     @staticmethod
8122     def _check_vgpu_allocations_match_real_use(
8123             vgpu_count_per_pgpu, expected_usage, rp_uuid, consumer_uuid,
8124             alloc_data):
8125         """Checks that the number of GPU devices assigned to the consumer
8126         matches what is expected from the allocations in the placement service
8127         and logs a warning if there is a mismatch.
8128 
8129         :param vgpu_count_per_pgpu: dict, keyed by PGPU device ID, to count of
8130             VGPUs on that device where each device is assigned to the consumer
8131             (guest instance on this hypervisor)
8132         :param expected_usage: The expected usage from placement for the
8133             given resource provider and consumer
8134         :param rp_uuid: UUID of the resource provider with VGPU inventory being
8135             consumed by the instance
8136         :param consumer_uuid: UUID of the consumer (instance) holding resource
8137             allocations against the given rp_uuid provider
8138         :param alloc_data: dict of allocation data for the instance consumer
8139         """
8140         actual_usage = sum(vgpu_count_per_pgpu.values())
8141         if actual_usage != expected_usage:
8142             # Don't make it blocking, just make sure you actually correctly
8143             # allocate the existing resources
8144             LOG.warning(
8145                 'Unexpected VGPU resource allocation on provider %(rp_uuid)s '
8146                 'for consumer %(consumer_uuid)s: %(alloc_data)s. Allocations '
8147                 '(%(expected_usage)s) differ from actual use '
8148                 '(%(actual_usage)s).',
8149                 {'rp_uuid': rp_uuid, 'consumer_uuid': consumer_uuid,
8150                  'alloc_data': alloc_data, 'expected_usage': expected_usage,
8151                  'actual_usage': actual_usage})
8152 
8153     def _reshape_vgpu_allocations(
8154             self, rp_uuid, root_node, consumer_uuid, alloc_data, resources,
8155             pgpu_rps):
8156         """Update existing VGPU allocations by moving them from the root node
8157         provider to the child provider for the given VGPU provider.
8158 
8159         :param rp_uuid: UUID of the VGPU resource provider with allocations
8160             from consumer_uuid (should be the root node provider before
8161             reshaping occurs)
8162         :param root_node: ProviderData object for the root compute node
8163             resource provider in the provider tree
8164         :param consumer_uuid: UUID of the consumer (instance) with VGPU
8165             allocations against the resource provider represented by rp_uuid
8166         :param alloc_data: dict of allocation information for consumer_uuid
8167         :param resources: dict, keyed by resource class, of resources allocated
8168             to consumer_uuid from rp_uuid
8169         :param pgpu_rps: dict, keyed by GPU device ID, to ProviderData object
8170             representing that resource provider in the tree
8171         :raises: ReshapeFailed if the reshape fails for whatever reason
8172         """
8173         # We've found VGPU allocations on a provider. It should be the root
8174         # node provider.
8175         self._assert_is_root_provider(
8176             rp_uuid, root_node, consumer_uuid, alloc_data)
8177 
8178         # Find which physical GPU corresponds to this allocation.
8179         mdev_uuids = self._get_assigned_mdevs_for_reshape(
8180             consumer_uuid, rp_uuid, alloc_data)
8181 
8182         vgpu_count_per_pgpu = self._count_vgpus_per_pgpu(mdev_uuids)
8183 
8184         # We need to make sure we found all the mediated devices that
8185         # correspond to an allocation.
8186         self._check_vgpu_allocations_match_real_use(
8187             vgpu_count_per_pgpu, resources[orc.VGPU],
8188             rp_uuid, consumer_uuid, alloc_data)
8189 
8190         # Add the VGPU allocation for each VGPU provider.
8191         allocs = alloc_data['allocations']
8192         for pgpu_dev_id, pgpu_rp in pgpu_rps.items():
8193             vgpu_count = vgpu_count_per_pgpu[pgpu_dev_id]
8194             if vgpu_count:
8195                 allocs[pgpu_rp.uuid] = {
8196                     'resources': {
8197                         orc.VGPU: vgpu_count
8198                     }
8199                 }
8200         # And remove the VGPU allocation from the root node provider.
8201         del resources[orc.VGPU]
8202 
8203     def _reshape_gpu_resources(
8204             self, allocations, root_node, pgpu_rps):
8205         """Reshapes the provider tree moving VGPU inventory from root to child
8206 
8207         :param allocations:
8208             Dict of allocation data of the form:
8209               { $CONSUMER_UUID: {
8210                     # The shape of each "allocations" dict below is identical
8211                     # to the return from GET /allocations/{consumer_uuid}
8212                     "allocations": {
8213                         $RP_UUID: {
8214                             "generation": $RP_GEN,
8215                             "resources": {
8216                                 $RESOURCE_CLASS: $AMOUNT,
8217                                 ...
8218                             },
8219                         },
8220                         ...
8221                     },
8222                     "project_id": $PROJ_ID,
8223                     "user_id": $USER_ID,
8224                     "consumer_generation": $CONSUMER_GEN,
8225                 },
8226                 ...
8227               }
8228         :params root_node: The root node in the provider tree
8229         :params pgpu_rps: dict, keyed by GPU device ID, to ProviderData object
8230             representing that resource provider in the tree
8231         """
8232         LOG.info('Reshaping tree; moving VGPU allocations from root '
8233                  'provider %s to child providers %s.', root_node.uuid,
8234                  pgpu_rps.values())
8235         # For each consumer in the allocations dict, look for VGPU
8236         # allocations and move them to the VGPU provider.
8237         for consumer_uuid, alloc_data in allocations.items():
8238             # Copy and iterate over the current set of providers to avoid
8239             # modifying keys while iterating.
8240             allocs = alloc_data['allocations']
8241             for rp_uuid in list(allocs):
8242                 resources = allocs[rp_uuid]['resources']
8243                 if orc.VGPU in resources:
8244                     self._reshape_vgpu_allocations(
8245                         rp_uuid, root_node, consumer_uuid, alloc_data,
8246                         resources, pgpu_rps)
8247 
8248     def _update_provider_tree_for_vgpu(self, provider_tree, nodename,
8249                                        allocations=None):
8250         """Updates the provider tree for VGPU inventory.
8251 
8252         Before Stein, VGPU inventory and allocations were on the root compute
8253         node provider in the tree. Starting in Stein, the VGPU inventory is
8254         on a child provider in the tree. As a result, this method will
8255         "reshape" the tree if necessary on first start of this compute service
8256         in Stein.
8257 
8258         :param provider_tree: The ProviderTree to update.
8259         :param nodename: The ComputeNode.hypervisor_hostname, also known as
8260             the name of the root node provider in the tree for this host.
8261         :param allocations: If not None, indicates a reshape was requested and
8262             should be performed.
8263         :raises: nova.exception.ReshapeNeeded if ``allocations`` is None and
8264             the method determines a reshape of the tree is needed, i.e. VGPU
8265             inventory and allocations must be migrated from the root node
8266             provider to a child provider of VGPU resources in the tree.
8267         :raises: nova.exception.ReshapeFailed if the requested tree reshape
8268             fails for whatever reason.
8269         """
8270         # First, check if this host actually has vGPU to reshape
8271         inventories_dict = self._get_gpu_inventories()
8272         if not inventories_dict:
8273             return
8274 
8275         # Check to see if the root compute node provider in the tree for
8276         # this host already has VGPU inventory because if it does, and
8277         # we're not currently reshaping (allocations is None), we need
8278         # to indicate that a reshape is needed to move the VGPU inventory
8279         # onto a child provider in the tree.
8280 
8281         # Ensure GPU providers are in the ProviderTree for the given inventory.
8282         pgpu_rps = self._ensure_pgpu_providers(
8283             inventories_dict, provider_tree, nodename)
8284 
8285         if self._is_reshape_needed_vgpu_on_root(provider_tree, nodename):
8286             if allocations is None:
8287                 # We have old VGPU inventory on root RP, but we haven't yet
8288                 # allocations. That means we need to ask for a reshape.
8289                 LOG.info('Requesting provider tree reshape in order to move '
8290                          'VGPU inventory from the root compute node provider '
8291                          '%s to a child provider.', nodename)
8292                 raise exception.ReshapeNeeded()
8293             # We have allocations, that means we already asked for a reshape
8294             # and the Placement API returned us them. We now need to move
8295             # those from the root RP to the needed children RPs.
8296             root_node = provider_tree.data(nodename)
8297             # Reshape VGPU provider inventory and allocations, moving them
8298             # from the root node provider to the child providers.
8299             self._reshape_gpu_resources(allocations, root_node, pgpu_rps)
8300             # Only delete the root inventory once the reshape is done
8301             if orc.VGPU in root_node.inventory:
8302                 del root_node.inventory[orc.VGPU]
8303                 provider_tree.update_inventory(nodename, root_node.inventory)
8304 
8305     def _update_provider_tree_for_pcpu(self, provider_tree, nodename,
8306                                        allocations=None):
8307         """Updates the provider tree for PCPU inventory.
8308 
8309         Before Train, pinned instances consumed VCPU inventory just like
8310         unpinned instances. Starting in Train, these instances now consume PCPU
8311         inventory. The function can reshape the inventory, changing allocations
8312         of VCPUs to PCPUs.
8313 
8314         :param provider_tree: The ProviderTree to update.
8315         :param nodename: The ComputeNode.hypervisor_hostname, also known as
8316             the name of the root node provider in the tree for this host.
8317         :param allocations: A dict, keyed by consumer UUID, of allocation
8318             records, or None::
8319 
8320                 {
8321                     $CONSUMER_UUID: {
8322                         "allocations": {
8323                             $RP_UUID: {
8324                                 "generation": $RP_GEN,
8325                                 "resources": {
8326                                     $RESOURCE_CLASS: $AMOUNT,
8327                                     ...
8328                                 },
8329                             },
8330                             ...
8331                         },
8332                         "project_id": $PROJ_ID,
8333                         "user_id": $USER_ID,
8334                         "consumer_generation": $CONSUMER_GEN,
8335                     },
8336                     ...
8337                 }
8338 
8339             If provided, this indicates a reshape was requested and should be
8340             performed.
8341         :raises: nova.exception.ReshapeNeeded if ``allocations`` is None and
8342             the method determines a reshape of the tree is needed, i.e. VCPU
8343             inventory and allocations must be migrated to PCPU resources.
8344         :raises: nova.exception.ReshapeFailed if the requested tree reshape
8345             fails for whatever reason.
8346         """
8347         # If we're not configuring PCPUs, then we've nothing to worry about
8348         # (yet)
8349         if not CONF.compute.cpu_dedicated_set:
8350             return
8351 
8352         root_node = provider_tree.data(nodename)
8353 
8354         # Similarly, if PCPU inventories are already reported then there is no
8355         # need to reshape
8356         if orc.PCPU in root_node.inventory:
8357             return
8358 
8359         ctx = nova_context.get_admin_context()
8360         compute_node = objects.ComputeNode.get_by_nodename(ctx, nodename)
8361 
8362         # Finally, if the compute node doesn't appear to support NUMA, move
8363         # swiftly on
8364         if not compute_node.numa_topology:
8365             return
8366 
8367         # The ComputeNode.numa_topology is a StringField, deserialize
8368         numa = objects.NUMATopology.obj_from_db_obj(compute_node.numa_topology)
8369 
8370         # If the host doesn't know of any pinned CPUs, we can continue
8371         if not any(cell.pinned_cpus for cell in numa.cells):
8372             return
8373 
8374         # At this point, we know there's something to be migrated here but not
8375         # how much. If the allocations are None, we're at the startup of the
8376         # compute node and a Reshape is needed. Indicate this by raising the
8377         # ReshapeNeeded exception
8378 
8379         if allocations is None:
8380             LOG.info(
8381                 'Requesting provider tree reshape in order to move '
8382                 'VCPU to PCPU allocations to the compute node '
8383                 'provider %s', nodename)
8384             raise exception.ReshapeNeeded()
8385 
8386         # Go figure out how many VCPUs to migrate to PCPUs. We've been telling
8387         # people for years *not* to mix pinned and unpinned instances, meaning
8388         # we should be able to move all VCPUs to PCPUs, but we never actually
8389         # enforced this in code and there's an all-too-high chance someone
8390         # didn't get the memo
8391 
8392         allocations_needing_reshape = []
8393 
8394         # we need to tackle the allocations against instances on this host...
8395 
8396         instances = objects.InstanceList.get_by_host(
8397             ctx, compute_node.host, expected_attrs=['numa_topology'])
8398         for instance in instances:
8399             if not instance.numa_topology:
8400                 continue
8401 
8402             if instance.numa_topology.cpu_policy != (
8403                 fields.CPUAllocationPolicy.DEDICATED
8404             ):
8405                 continue
8406 
8407             allocations_needing_reshape.append(instance.uuid)
8408 
8409         # ...and those for any migrations
8410 
8411         migrations = objects.MigrationList.get_in_progress_by_host_and_node(
8412             ctx, compute_node.host, compute_node.hypervisor_hostname)
8413         for migration in migrations:
8414             # we don't care about migrations that have landed here, since we
8415             # already have those instances above
8416             if not migration.dest_compute or (
8417                     migration.dest_compute == compute_node.host):
8418                 continue
8419 
8420             instance = objects.Instance.get_by_uuid(
8421                 ctx, migration.instance_uuid, expected_attrs=['numa_topology'])
8422 
8423             if not instance.numa_topology:
8424                 continue
8425 
8426             if instance.numa_topology.cpu_policy != (
8427                 fields.CPUAllocationPolicy.DEDICATED
8428             ):
8429                 continue
8430 
8431             allocations_needing_reshape.append(migration.uuid)
8432 
8433         for allocation_uuid in allocations_needing_reshape:
8434             consumer_allocations = allocations.get(allocation_uuid, {}).get(
8435                 'allocations', {})
8436             # TODO(stephenfin): We can probably just check the allocations for
8437             # ComputeNode.uuid since compute nodes are the only (?) provider of
8438             # VCPU and PCPU resources
8439             for rp_uuid in consumer_allocations:
8440                 resources = consumer_allocations[rp_uuid]['resources']
8441 
8442                 if orc.PCPU in resources or orc.VCPU not in resources:
8443                     # Either this has been migrated or it's not a compute node
8444                     continue
8445 
8446                 # Switch stuff around. We can do a straight swap since an
8447                 # instance is either pinned or unpinned. By doing this, we're
8448                 # modifying the provided 'allocations' dict, which will
8449                 # eventually be used by the resource tracker to update
8450                 # placement
8451                 resources['PCPU'] = resources['VCPU']
8452                 del resources[orc.VCPU]
8453 
8454     def get_available_resource(self, nodename):
8455         """Retrieve resource information.
8456 
8457         This method is called when nova-compute launches, and
8458         as part of a periodic task that records the results in the DB.
8459 
8460         :param nodename: unused in this driver
8461         :returns: dictionary containing resource info
8462         """
8463 
8464         disk_info_dict = self._get_local_gb_info()
8465         data = {}
8466 
8467         # NOTE(dprince): calling capabilities before getVersion works around
8468         # an initialization issue with some versions of Libvirt (1.0.5.5).
8469         # See: https://bugzilla.redhat.com/show_bug.cgi?id=1000116
8470         # See: https://bugs.launchpad.net/nova/+bug/1215593
8471         data["supported_instances"] = self._get_instance_capabilities()
8472 
8473         data["vcpus"] = len(self._get_vcpu_available())
8474         data["memory_mb"] = self._host.get_memory_mb_total()
8475         data["local_gb"] = disk_info_dict['total']
8476         data["vcpus_used"] = self._get_vcpu_used()
8477         data["memory_mb_used"] = self._host.get_memory_mb_used()
8478         data["local_gb_used"] = disk_info_dict['used']
8479         data["hypervisor_type"] = self._host.get_driver_type()
8480         data["hypervisor_version"] = self._host.get_version()
8481         data["hypervisor_hostname"] = self._host.get_hostname()
8482         # TODO(berrange): why do we bother converting the
8483         # libvirt capabilities XML into a special JSON format ?
8484         # The data format is different across all the drivers
8485         # so we could just return the raw capabilities XML
8486         # which 'compare_cpu' could use directly
8487         #
8488         # That said, arch_filter.py now seems to rely on
8489         # the libvirt drivers format which suggests this
8490         # data format needs to be standardized across drivers
8491         data["cpu_info"] = jsonutils.dumps(self._get_cpu_info())
8492 
8493         disk_free_gb = disk_info_dict['free']
8494         disk_over_committed = self._get_disk_over_committed_size_total()
8495         available_least = disk_free_gb * units.Gi - disk_over_committed
8496         data['disk_available_least'] = available_least / units.Gi
8497 
8498         data['pci_passthrough_devices'] = self._get_pci_passthrough_devices()
8499 
8500         numa_topology = self._get_host_numa_topology()
8501         if numa_topology:
8502             data['numa_topology'] = numa_topology._to_json()
8503         else:
8504             data['numa_topology'] = None
8505 
8506         return data
8507 
8508     def check_instance_shared_storage_local(self, context, instance):
8509         """Check if instance files located on shared storage.
8510 
8511         This runs check on the destination host, and then calls
8512         back to the source host to check the results.
8513 
8514         :param context: security context
8515         :param instance: nova.objects.instance.Instance object
8516         :returns:
8517          - tempfile: A dict containing the tempfile info on the destination
8518                      host
8519          - None:
8520 
8521             1. If the instance path is not existing.
8522             2. If the image backend is shared block storage type.
8523         """
8524         if self.image_backend.backend().is_shared_block_storage():
8525             return None
8526 
8527         dirpath = libvirt_utils.get_instance_path(instance)
8528 
8529         if not os.path.exists(dirpath):
8530             return None
8531 
8532         fd, tmp_file = tempfile.mkstemp(dir=dirpath)
8533         LOG.debug("Creating tmpfile %s to verify with other "
8534                   "compute node that the instance is on "
8535                   "the same shared storage.",
8536                   tmp_file, instance=instance)
8537         os.close(fd)
8538         return {"filename": tmp_file}
8539 
8540     def check_instance_shared_storage_remote(self, context, data):
8541         return os.path.exists(data['filename'])
8542 
8543     def check_instance_shared_storage_cleanup(self, context, data):
8544         fileutils.delete_if_exists(data["filename"])
8545 
8546     def check_can_live_migrate_destination(self, context, instance,
8547                                            src_compute_info, dst_compute_info,
8548                                            block_migration=False,
8549                                            disk_over_commit=False):
8550         """Check if it is possible to execute live migration.
8551 
8552         This runs checks on the destination host, and then calls
8553         back to the source host to check the results.
8554 
8555         :param context: security context
8556         :param instance: nova.db.sqlalchemy.models.Instance
8557         :param block_migration: if true, prepare for block migration
8558         :param disk_over_commit: if true, allow disk over commit
8559         :returns: a LibvirtLiveMigrateData object
8560         """
8561         if disk_over_commit:
8562             disk_available_gb = dst_compute_info['free_disk_gb']
8563         else:
8564             disk_available_gb = dst_compute_info['disk_available_least']
8565         disk_available_mb = (
8566             (disk_available_gb * units.Ki) - CONF.reserved_host_disk_mb)
8567 
8568         # Compare CPU
8569         try:
8570             if not instance.vcpu_model or not instance.vcpu_model.model:
8571                 source_cpu_info = src_compute_info['cpu_info']
8572                 self._compare_cpu(None, source_cpu_info, instance)
8573             else:
8574                 self._compare_cpu(instance.vcpu_model, None, instance)
8575         except exception.InvalidCPUInfo as e:
8576             raise exception.MigrationPreCheckError(reason=e)
8577 
8578         # Create file on storage, to be checked on source host
8579         filename = self._create_shared_storage_test_file(instance)
8580 
8581         data = objects.LibvirtLiveMigrateData()
8582         data.filename = filename
8583         data.image_type = CONF.libvirt.images_type
8584         data.graphics_listen_addr_vnc = CONF.vnc.server_listen
8585         data.graphics_listen_addr_spice = CONF.spice.server_listen
8586         if CONF.serial_console.enabled:
8587             data.serial_listen_addr = CONF.serial_console.proxyclient_address
8588         else:
8589             data.serial_listen_addr = None
8590         # Notes(eliqiao): block_migration and disk_over_commit are not
8591         # nullable, so just don't set them if they are None
8592         if block_migration is not None:
8593             data.block_migration = block_migration
8594         if disk_over_commit is not None:
8595             data.disk_over_commit = disk_over_commit
8596         data.disk_available_mb = disk_available_mb
8597         data.dst_wants_file_backed_memory = CONF.libvirt.file_backed_memory > 0
8598 
8599         # TODO(artom) Set to indicate that the destination (us) can perform a
8600         # NUMA-aware live migration. NUMA-aware live migration will become
8601         # unconditionally supported in RPC 6.0, so this sentinel can be removed
8602         # then.
8603         if instance.numa_topology:
8604             data.dst_supports_numa_live_migration = True
8605 
8606         return data
8607 
8608     def post_claim_migrate_data(self, context, instance, migrate_data, claim):
8609         migrate_data.dst_numa_info = self._get_live_migrate_numa_info(
8610                 claim.claimed_numa_topology, claim.instance_type,
8611                 claim.image_meta)
8612         return migrate_data
8613 
8614     def _get_resources(self, instance, prefix=None):
8615         resources: 'objects.ResourceList' = []
8616         if prefix:
8617             migr_context = instance.migration_context
8618             attr_name = prefix + 'resources'
8619             if migr_context and attr_name in migr_context:
8620                 resources = getattr(migr_context, attr_name) or []
8621         else:
8622             resources = instance.resources or []
8623         return resources
8624 
8625     def _get_vpmem_resources(self, resources):
8626         vpmem_resources = []
8627         for resource in resources:
8628             if 'metadata' in resource and \
8629                 isinstance(resource.metadata, objects.LibvirtVPMEMDevice):
8630                 vpmem_resources.append(resource)
8631         return vpmem_resources
8632 
8633     def _get_ordered_vpmem_resources(self, resources, flavor):
8634         vpmem_resources = self._get_vpmem_resources(resources)
8635         ordered_vpmem_resources = []
8636         labels = hardware.get_vpmems(flavor)
8637         for label in labels:
8638             for vpmem_resource in vpmem_resources:
8639                 if vpmem_resource.metadata.label == label:
8640                     ordered_vpmem_resources.append(vpmem_resource)
8641                     vpmem_resources.remove(vpmem_resource)
8642                     break
8643         return ordered_vpmem_resources
8644 
8645     def _sorted_migrating_resources(self, instance, flavor):
8646         """This method is used to sort instance.migration_context.new_resources
8647         claimed on dest host, then the ordered new resources will be used to
8648         update resources info (e.g. vpmems) in the new xml which is used for
8649         live migration.
8650         """
8651         resources = self._get_resources(instance, prefix='new_')
8652         if not resources:
8653             return
8654         ordered_resources = []
8655         ordered_vpmem_resources = self._get_ordered_vpmem_resources(
8656                 resources, flavor)
8657         ordered_resources.extend(ordered_vpmem_resources)
8658         ordered_resources_obj = objects.ResourceList(objects=ordered_resources)
8659         return ordered_resources_obj
8660 
8661     def _get_live_migrate_numa_info(self, instance_numa_topology, flavor,
8662                                     image_meta):
8663         """Builds a LibvirtLiveMigrateNUMAInfo object to send to the source of
8664         a live migration, containing information about how the instance is to
8665         be pinned on the destination host.
8666 
8667         :param instance_numa_topology: The InstanceNUMATopology as fitted to
8668                                        the destination by the live migration
8669                                        Claim.
8670         :param flavor: The Flavor object for the instance.
8671         :param image_meta: The ImageMeta object for the instance.
8672         :returns: A LibvirtLiveMigrateNUMAInfo object indicating how to update
8673                   the XML for the destination host.
8674         """
8675         info = objects.LibvirtLiveMigrateNUMAInfo()
8676         cpu_set, guest_cpu_tune, guest_cpu_numa, guest_numa_tune = \
8677             self._get_guest_numa_config(instance_numa_topology, flavor,
8678                                         image_meta)
8679         # NOTE(artom) These two should always be either None together, or
8680         # truth-y together.
8681         if guest_cpu_tune and guest_numa_tune:
8682             info.cpu_pins = {}
8683             for pin in guest_cpu_tune.vcpupin:
8684                 info.cpu_pins[str(pin.id)] = pin.cpuset
8685 
8686             info.emulator_pins = guest_cpu_tune.emulatorpin.cpuset
8687 
8688             if guest_cpu_tune.vcpusched:
8689                 # NOTE(artom) vcpusched is a list, but there's only ever one
8690                 # element in it (see _get_guest_numa_config under
8691                 # wants_realtime)
8692                 info.sched_vcpus = guest_cpu_tune.vcpusched[0].vcpus
8693                 info.sched_priority = guest_cpu_tune.vcpusched[0].priority
8694 
8695             info.cell_pins = {}
8696             for node in guest_numa_tune.memnodes:
8697                 info.cell_pins[str(node.cellid)] = set(node.nodeset)
8698 
8699         LOG.debug('Built NUMA live migration info: %s', info)
8700         return info
8701 
8702     def cleanup_live_migration_destination_check(self, context,
8703                                                  dest_check_data):
8704         """Do required cleanup on dest host after check_can_live_migrate calls
8705 
8706         :param context: security context
8707         """
8708         filename = dest_check_data.filename
8709         self._cleanup_shared_storage_test_file(filename)
8710 
8711     def check_can_live_migrate_source(self, context, instance,
8712                                       dest_check_data,
8713                                       block_device_info=None):
8714         """Check if it is possible to execute live migration.
8715 
8716         This checks if the live migration can succeed, based on the
8717         results from check_can_live_migrate_destination.
8718 
8719         :param context: security context
8720         :param instance: nova.db.sqlalchemy.models.Instance
8721         :param dest_check_data: result of check_can_live_migrate_destination
8722         :param block_device_info: result of _get_instance_block_device_info
8723         :returns: a LibvirtLiveMigrateData object
8724         """
8725         # Checking shared storage connectivity
8726         # if block migration, instances_path should not be on shared storage.
8727         source = CONF.host
8728 
8729         dest_check_data.is_shared_instance_path = (
8730             self._check_shared_storage_test_file(
8731                 dest_check_data.filename, instance))
8732 
8733         dest_check_data.is_shared_block_storage = (
8734             self._is_shared_block_storage(instance, dest_check_data,
8735                                           block_device_info))
8736 
8737         if 'block_migration' not in dest_check_data:
8738             dest_check_data.block_migration = (
8739                 not dest_check_data.is_on_shared_storage())
8740 
8741         if dest_check_data.block_migration:
8742             # TODO(eliqiao): Once block_migration flag is removed from the API
8743             # we can safely remove the if condition
8744             if dest_check_data.is_on_shared_storage():
8745                 reason = _("Block migration can not be used "
8746                            "with shared storage.")
8747                 raise exception.InvalidLocalStorage(reason=reason, path=source)
8748             if 'disk_over_commit' in dest_check_data:
8749                 self._assert_dest_node_has_enough_disk(context, instance,
8750                                         dest_check_data.disk_available_mb,
8751                                         dest_check_data.disk_over_commit,
8752                                         block_device_info)
8753             if block_device_info:
8754                 bdm = block_device_info.get('block_device_mapping')
8755                 # NOTE(eliqiao): Selective disk migrations are not supported
8756                 # with tunnelled block migrations so we can block them early.
8757                 if (bdm and
8758                     (self._block_migration_flags &
8759                      libvirt.VIR_MIGRATE_TUNNELLED != 0)):
8760                     msg = (_('Cannot block migrate instance %(uuid)s with'
8761                              ' mapped volumes. Selective block device'
8762                              ' migration is not supported with tunnelled'
8763                              ' block migrations.') % {'uuid': instance.uuid})
8764                     LOG.error(msg, instance=instance)
8765                     raise exception.MigrationPreCheckError(reason=msg)
8766         elif not (dest_check_data.is_shared_block_storage or
8767                   dest_check_data.is_shared_instance_path):
8768             reason = _("Shared storage live-migration requires either shared "
8769                        "storage or boot-from-volume with no local disks.")
8770             raise exception.InvalidSharedStorage(reason=reason, path=source)
8771 
8772         # NOTE(mikal): include the instance directory name here because it
8773         # doesn't yet exist on the destination but we want to force that
8774         # same name to be used
8775         instance_path = libvirt_utils.get_instance_path(instance,
8776                                                         relative=True)
8777         dest_check_data.instance_relative_path = instance_path
8778 
8779         # TODO(artom) Set to indicate that the source (us) can perform a
8780         # NUMA-aware live migration. NUMA-aware live migration will become
8781         # unconditionally supported in RPC 6.0, so this sentinel can be removed
8782         # then.
8783         if instance.numa_topology:
8784             dest_check_data.src_supports_numa_live_migration = True
8785 
8786         return dest_check_data
8787 
8788     def _is_shared_block_storage(self, instance, dest_check_data,
8789                                  block_device_info=None):
8790         """Check if all block storage of an instance can be shared
8791         between source and destination of a live migration.
8792 
8793         Returns true if the instance is volume backed and has no local disks,
8794         or if the image backend is the same on source and destination and the
8795         backend shares block storage between compute nodes.
8796 
8797         :param instance: nova.objects.instance.Instance object
8798         :param dest_check_data: dict with boolean fields image_type,
8799                                 is_shared_instance_path, and is_volume_backed
8800         """
8801         if (dest_check_data.obj_attr_is_set('image_type') and
8802                 CONF.libvirt.images_type == dest_check_data.image_type and
8803                 self.image_backend.backend().is_shared_block_storage()):
8804             # NOTE(dgenin): currently true only for RBD image backend
8805             return True
8806 
8807         if (dest_check_data.is_shared_instance_path and
8808                 self.image_backend.backend().is_file_in_instance_path()):
8809             # NOTE(angdraug): file based image backends (Flat, Qcow2)
8810             # place block device files under the instance path
8811             return True
8812 
8813         if (dest_check_data.is_volume_backed and
8814                 not bool(self._get_instance_disk_info(instance,
8815                                                       block_device_info))):
8816             return True
8817 
8818         return False
8819 
8820     def _assert_dest_node_has_enough_disk(self, context, instance,
8821                                              available_mb, disk_over_commit,
8822                                              block_device_info):
8823         """Checks if destination has enough disk for block migration."""
8824         # Libvirt supports qcow2 disk format,which is usually compressed
8825         # on compute nodes.
8826         # Real disk image (compressed) may enlarged to "virtual disk size",
8827         # that is specified as the maximum disk size.
8828         # (See qemu-img -f path-to-disk)
8829         # Scheduler recognizes destination host still has enough disk space
8830         # if real disk size < available disk size
8831         # if disk_over_commit is True,
8832         #  otherwise virtual disk size < available disk size.
8833 
8834         available = 0
8835         if available_mb:
8836             available = available_mb * units.Mi
8837 
8838         disk_infos = self._get_instance_disk_info(instance, block_device_info)
8839 
8840         necessary = 0
8841         if disk_over_commit:
8842             for info in disk_infos:
8843                 necessary += int(info['disk_size'])
8844         else:
8845             for info in disk_infos:
8846                 necessary += int(info['virt_disk_size'])
8847 
8848         # Check that available disk > necessary disk
8849         if (available - necessary) < 0:
8850             reason = (_('Unable to migrate %(instance_uuid)s: '
8851                         'Disk of instance is too large(available'
8852                         ' on destination host:%(available)s '
8853                         '< need:%(necessary)s)') %
8854                       {'instance_uuid': instance.uuid,
8855                        'available': available,
8856                        'necessary': necessary})
8857             raise exception.MigrationPreCheckError(reason=reason)
8858 
8859     def _compare_cpu(self, guest_cpu, host_cpu_str, instance):
8860         """Check the host is compatible with the requested CPU
8861 
8862         :param guest_cpu: nova.objects.VirtCPUModel
8863             or nova.virt.libvirt.vconfig.LibvirtConfigGuestCPU or None.
8864         :param host_cpu_str: JSON from _get_cpu_info() method
8865 
8866         If the 'guest_cpu' parameter is not None, this will be
8867         validated for migration compatibility with the host.
8868         Otherwise the 'host_cpu_str' JSON string will be used for
8869         validation.
8870 
8871         :returns:
8872             None. if given cpu info is not compatible to this server,
8873             raise exception.
8874         """
8875 
8876         # NOTE(kchamart): Comparing host to guest CPU model for emulated
8877         # guests (<domain type='qemu'>) should not matter -- in this
8878         # mode (QEMU "TCG") the CPU is fully emulated in software and no
8879         # hardware acceleration, like KVM, is involved. So, skip the CPU
8880         # compatibility check for the QEMU domain type, and retain it for
8881         # KVM guests.
8882         if CONF.libvirt.virt_type not in ['kvm']:
8883             return
8884 
8885         if guest_cpu is None:
8886             info = jsonutils.loads(host_cpu_str)
8887             LOG.info('Instance launched has CPU info: %s', host_cpu_str)
8888             cpu = vconfig.LibvirtConfigCPU()
8889             cpu.arch = info['arch']
8890             cpu.model = info['model']
8891             cpu.vendor = info['vendor']
8892             cpu.sockets = info['topology']['sockets']
8893             cpu.cores = info['topology']['cores']
8894             cpu.threads = info['topology']['threads']
8895             for f in info['features']:
8896                 cpu.add_feature(vconfig.LibvirtConfigCPUFeature(f))
8897         elif isinstance(guest_cpu, vconfig.LibvirtConfigGuestCPU):
8898             cpu = guest_cpu
8899         else:
8900             cpu = self._vcpu_model_to_cpu_config(guest_cpu)
8901 
8902         # s390x doesn't support cpu model in host info, so compare
8903         # cpu info will raise an error anyway, thus have to avoid check
8904         # see bug 1854126 for more info
8905         if (
8906             cpu.arch in (arch.S390X, arch.S390) and
8907             not self._host.has_min_version(MIN_LIBVIRT_S390X_CPU_COMPARE)
8908         ):
8909             LOG.debug("on s390x platform, the min libvirt version "
8910                       "support cpu model compare is %s",
8911                       MIN_LIBVIRT_S390X_CPU_COMPARE)
8912             return
8913 
8914         u = ("http://libvirt.org/html/libvirt-libvirt-host.html#"
8915              "virCPUCompareResult")
8916         m = _("CPU doesn't have compatibility.\n\n%(ret)s\n\nRefer to %(u)s")
8917         # unknown character exists in xml, then libvirt complains
8918         try:
8919             cpu_xml = cpu.to_xml()
8920             LOG.debug("cpu compare xml: %s", cpu_xml, instance=instance)
8921             ret = self._host.compare_cpu(cpu_xml)
8922         except libvirt.libvirtError as e:
8923             if cpu.arch == fields.Architecture.AARCH64:
8924                 LOG.debug("Host CPU compatibility check does not make "
8925                           "sense on AArch64; skip CPU comparison")
8926                 return
8927             error_code = e.get_error_code()
8928             if error_code == libvirt.VIR_ERR_NO_SUPPORT:
8929                 LOG.debug("URI %(uri)s does not support cpu comparison. "
8930                           "It will be proceeded though. Error: %(error)s",
8931                           {'uri': self._uri(), 'error': e})
8932                 return
8933             else:
8934                 LOG.error(m, {'ret': e, 'u': u})
8935                 raise exception.InvalidCPUInfo(
8936                     reason=m % {'ret': e, 'u': u})
8937 
8938         if ret <= 0:
8939             LOG.error(m, {'ret': ret, 'u': u})
8940             raise exception.InvalidCPUInfo(reason=m % {'ret': ret, 'u': u})
8941 
8942     def _create_shared_storage_test_file(self, instance):
8943         """Makes tmpfile under CONF.instances_path."""
8944         dirpath = CONF.instances_path
8945         fd, tmp_file = tempfile.mkstemp(dir=dirpath)
8946         LOG.debug("Creating tmpfile %s to notify to other "
8947                   "compute nodes that they should mount "
8948                   "the same storage.", tmp_file, instance=instance)
8949         os.close(fd)
8950         return os.path.basename(tmp_file)
8951 
8952     def _check_shared_storage_test_file(self, filename, instance):
8953         """Confirms existence of the tmpfile under CONF.instances_path.
8954 
8955         Cannot confirm tmpfile return False.
8956         """
8957         # NOTE(tpatzig): if instances_path is a shared volume that is
8958         # under heavy IO (many instances on many compute nodes),
8959         # then checking the existence of the testfile fails,
8960         # just because it takes longer until the client refreshes and new
8961         # content gets visible.
8962         # os.utime (like touch) on the directory forces the client to refresh.
8963         os.utime(CONF.instances_path, None)
8964 
8965         tmp_file = os.path.join(CONF.instances_path, filename)
8966         if not os.path.exists(tmp_file):
8967             exists = False
8968         else:
8969             exists = True
8970         LOG.debug('Check if temp file %s exists to indicate shared storage '
8971                   'is being used for migration. Exists? %s', tmp_file, exists,
8972                   instance=instance)
8973         return exists
8974 
8975     def _cleanup_shared_storage_test_file(self, filename):
8976         """Removes existence of the tmpfile under CONF.instances_path."""
8977         tmp_file = os.path.join(CONF.instances_path, filename)
8978         os.remove(tmp_file)
8979 
8980     def live_migration(self, context, instance, dest,
8981                        post_method, recover_method, block_migration=False,
8982                        migrate_data=None):
8983         """Spawning live_migration operation for distributing high-load.
8984 
8985         :param context: security context
8986         :param instance:
8987             nova.db.sqlalchemy.models.Instance object
8988             instance object that is migrated.
8989         :param dest: destination host
8990         :param post_method:
8991             post operation method.
8992             expected nova.compute.manager._post_live_migration.
8993         :param recover_method:
8994             recovery method when any exception occurs.
8995             expected nova.compute.manager._rollback_live_migration.
8996         :param block_migration: if true, do block migration.
8997         :param migrate_data: a LibvirtLiveMigrateData object
8998 
8999         """
9000 
9001         # 'dest' will be substituted into 'migration_uri' so ensure
9002         # it does't contain any characters that could be used to
9003         # exploit the URI accepted by libvirt
9004         if not libvirt_utils.is_valid_hostname(dest):
9005             raise exception.InvalidHostname(hostname=dest)
9006 
9007         self._live_migration(context, instance, dest,
9008                              post_method, recover_method, block_migration,
9009                              migrate_data)
9010 
9011     def live_migration_abort(self, instance):
9012         """Aborting a running live-migration.
9013 
9014         :param instance: instance object that is in migration
9015 
9016         """
9017 
9018         guest = self._host.get_guest(instance)
9019         dom = guest._domain
9020 
9021         try:
9022             dom.abortJob()
9023         except libvirt.libvirtError as e:
9024             LOG.error("Failed to cancel migration %s",
9025                     encodeutils.exception_to_unicode(e), instance=instance)
9026             raise
9027 
9028     def _verify_serial_console_is_disabled(self):
9029         if CONF.serial_console.enabled:
9030 
9031             msg = _('Your destination node does not support'
9032                     ' retrieving listen addresses. In order'
9033                     ' for live migration to work properly you'
9034                     ' must disable serial console.')
9035             raise exception.MigrationError(reason=msg)
9036 
9037     def _detach_direct_passthrough_vifs(self, context,
9038                                         migrate_data, instance):
9039         """detaches passthrough vif to enable live migration
9040 
9041         :param context: security context
9042         :param migrate_data: a LibvirtLiveMigrateData object
9043         :param instance: instance object that is migrated.
9044         """
9045         # NOTE(sean-k-mooney): if we have vif data available we
9046         # loop over each vif and detach all direct passthrough
9047         # vifs to allow sriov live migration.
9048         direct_vnics = network_model.VNIC_TYPES_DIRECT_PASSTHROUGH
9049         vifs = [vif.source_vif for vif in migrate_data.vifs
9050                 if "source_vif" in vif and vif.source_vif]
9051         for vif in vifs:
9052             if vif['vnic_type'] in direct_vnics:
9053                 LOG.info("Detaching vif %s from instance "
9054                          "%s for live migration", vif['id'], instance.id)
9055                 self.detach_interface(context, instance, vif)
9056 
9057     def _live_migration_operation(self, context, instance, dest,
9058                                   block_migration, migrate_data, guest,
9059                                   device_names):
9060         """Invoke the live migration operation
9061 
9062         :param context: security context
9063         :param instance:
9064             nova.db.sqlalchemy.models.Instance object
9065             instance object that is migrated.
9066         :param dest: destination host
9067         :param block_migration: if true, do block migration.
9068         :param migrate_data: a LibvirtLiveMigrateData object
9069         :param guest: the guest domain object
9070         :param device_names: list of device names that are being migrated with
9071             instance
9072 
9073         This method is intended to be run in a background thread and will
9074         block that thread until the migration is finished or failed.
9075         """
9076         try:
9077             if migrate_data.block_migration:
9078                 migration_flags = self._block_migration_flags
9079             else:
9080                 migration_flags = self._live_migration_flags
9081 
9082             if not migrate_data.serial_listen_addr:
9083                 # In this context we want to ensure that serial console is
9084                 # disabled on source node. This is because nova couldn't
9085                 # retrieve serial listen address from destination node, so we
9086                 # consider that destination node might have serial console
9087                 # disabled as well.
9088                 self._verify_serial_console_is_disabled()
9089 
9090             # NOTE(aplanas) migrate_uri will have a value only in the
9091             # case that `live_migration_inbound_addr` parameter is
9092             # set, and we propose a non tunneled migration.
9093             migrate_uri = None
9094             if ('target_connect_addr' in migrate_data and
9095                     migrate_data.target_connect_addr is not None):
9096                 dest = migrate_data.target_connect_addr
9097                 if (migration_flags &
9098                     libvirt.VIR_MIGRATE_TUNNELLED == 0):
9099                     migrate_uri = self._migrate_uri(dest)
9100 
9101             new_xml_str = None
9102             if CONF.libvirt.virt_type != "parallels":
9103                 # If the migrate_data has port binding information for the
9104                 # destination host, we need to prepare the guest vif config
9105                 # for the destination before we start migrating the guest.
9106                 get_vif_config = None
9107                 if 'vifs' in migrate_data and migrate_data.vifs:
9108                     # NOTE(mriedem): The vif kwarg must be built on the fly
9109                     # within get_updated_guest_xml based on migrate_data.vifs.
9110                     # We could stash the virt_type from the destination host
9111                     # into LibvirtLiveMigrateData but the host kwarg is a
9112                     # nova.virt.libvirt.host.Host object and is used to check
9113                     # information like libvirt version on the destination.
9114                     # If this becomes a problem, what we could do is get the
9115                     # VIF configs while on the destination host during
9116                     # pre_live_migration() and store those in the
9117                     # LibvirtLiveMigrateData object. For now we just use the
9118                     # source host information for virt_type and
9119                     # host (version) since the conductor live_migrate method
9120                     # _check_compatible_with_source_hypervisor() ensures that
9121                     # the hypervisor types and versions are compatible.
9122                     get_vif_config = functools.partial(
9123                         self.vif_driver.get_config,
9124                         instance=instance,
9125                         image_meta=instance.image_meta,
9126                         inst_type=instance.flavor,
9127                         virt_type=CONF.libvirt.virt_type,
9128                     )
9129                     self._detach_direct_passthrough_vifs(context,
9130                         migrate_data, instance)
9131                 new_resources = None
9132                 if isinstance(instance, objects.Instance):
9133                     new_resources = self._sorted_migrating_resources(
9134                         instance, instance.flavor)
9135                 new_xml_str = libvirt_migrate.get_updated_guest_xml(
9136                     # TODO(sahid): It's not a really good idea to pass
9137                     # the method _get_volume_config and we should to find
9138                     # a way to avoid this in future.
9139                     guest, migrate_data, self._get_volume_config,
9140                     get_vif_config=get_vif_config, new_resources=new_resources)
9141 
9142             # NOTE(pkoniszewski): Because of precheck which blocks
9143             # tunnelled block live migration with mapped volumes we
9144             # can safely remove migrate_disks when tunnelling is on.
9145             # Otherwise we will block all tunnelled block migrations,
9146             # even when an instance does not have volumes mapped.
9147             # This is because selective disk migration is not
9148             # supported in tunnelled block live migration. Also we
9149             # cannot fallback to migrateToURI2 in this case because of
9150             # bug #1398999
9151             #
9152             # TODO(kchamart) Move the following bit to guest.migrate()
9153             if (migration_flags & libvirt.VIR_MIGRATE_TUNNELLED != 0):
9154                 device_names = []
9155 
9156             # TODO(sahid): This should be in
9157             # post_live_migration_at_source but no way to retrieve
9158             # ports acquired on the host for the guest at this
9159             # step. Since the domain is going to be removed from
9160             # libvird on source host after migration, we backup the
9161             # serial ports to release them if all went well.
9162             serial_ports = []
9163             if CONF.serial_console.enabled:
9164                 serial_ports = list(self._get_serial_ports_from_guest(guest))
9165 
9166             LOG.debug("About to invoke the migrate API", instance=instance)
9167             guest.migrate(self._live_migration_uri(dest),
9168                           migrate_uri=migrate_uri,
9169                           flags=migration_flags,
9170                           migrate_disks=device_names,
9171                           destination_xml=new_xml_str,
9172                           bandwidth=CONF.libvirt.live_migration_bandwidth)
9173             LOG.debug("Migrate API has completed", instance=instance)
9174 
9175             for hostname, port in serial_ports:
9176                 serial_console.release_port(host=hostname, port=port)
9177         except Exception as e:
9178             with excutils.save_and_reraise_exception():
9179                 LOG.error("Live Migration failure: %s", e, instance=instance)
9180 
9181         # If 'migrateToURI' fails we don't know what state the
9182         # VM instances on each host are in. Possibilities include
9183         #
9184         #  1. src==running, dst==none
9185         #
9186         #     Migration failed & rolled back, or never started
9187         #
9188         #  2. src==running, dst==paused
9189         #
9190         #     Migration started but is still ongoing
9191         #
9192         #  3. src==paused,  dst==paused
9193         #
9194         #     Migration data transfer completed, but switchover
9195         #     is still ongoing, or failed
9196         #
9197         #  4. src==paused,  dst==running
9198         #
9199         #     Migration data transfer completed, switchover
9200         #     happened but cleanup on source failed
9201         #
9202         #  5. src==none,    dst==running
9203         #
9204         #     Migration fully succeeded.
9205         #
9206         # Libvirt will aim to complete any migration operation
9207         # or roll it back. So even if the migrateToURI call has
9208         # returned an error, if the migration was not finished
9209         # libvirt should clean up.
9210         #
9211         # So we take the error raise here with a pinch of salt
9212         # and rely on the domain job info status to figure out
9213         # what really happened to the VM, which is a much more
9214         # reliable indicator.
9215         #
9216         # In particular we need to try very hard to ensure that
9217         # Nova does not "forget" about the guest. ie leaving it
9218         # running on a different host to the one recorded in
9219         # the database, as that would be a serious resource leak
9220 
9221         LOG.debug("Migration operation thread has finished",
9222                   instance=instance)
9223 
9224     def _live_migration_copy_disk_paths(self, context, instance, guest):
9225         '''Get list of disks to copy during migration
9226 
9227         :param context: security context
9228         :param instance: the instance being migrated
9229         :param guest: the Guest instance being migrated
9230 
9231         Get the list of disks to copy during migration.
9232 
9233         :returns: a list of local source paths and a list of device names to
9234             copy
9235         '''
9236 
9237         disk_paths = []
9238         device_names = []
9239         block_devices = []
9240 
9241         if (self._block_migration_flags &
9242                 libvirt.VIR_MIGRATE_TUNNELLED == 0):
9243             bdm_list = objects.BlockDeviceMappingList.get_by_instance_uuid(
9244                 context, instance.uuid)
9245             block_device_info = driver.get_block_device_info(instance,
9246                                                              bdm_list)
9247 
9248             block_device_mappings = driver.block_device_info_get_mapping(
9249                 block_device_info)
9250             for bdm in block_device_mappings:
9251                 device_name = str(bdm['mount_device'].rsplit('/', 1)[1])
9252                 block_devices.append(device_name)
9253 
9254         for dev in guest.get_all_disks():
9255             if dev.readonly or dev.shareable:
9256                 continue
9257             if dev.source_type not in ["file", "block"]:
9258                 continue
9259             if dev.target_dev in block_devices:
9260                 continue
9261             disk_paths.append(dev.source_path)
9262             device_names.append(dev.target_dev)
9263         return (disk_paths, device_names)
9264 
9265     def _live_migration_data_gb(self, instance, disk_paths):
9266         '''Calculate total amount of data to be transferred
9267 
9268         :param instance: the nova.objects.Instance being migrated
9269         :param disk_paths: list of disk paths that are being migrated
9270         with instance
9271 
9272         Calculates the total amount of data that needs to be
9273         transferred during the live migration. The actual
9274         amount copied will be larger than this, due to the
9275         guest OS continuing to dirty RAM while the migration
9276         is taking place. So this value represents the minimal
9277         data size possible.
9278 
9279         :returns: data size to be copied in GB
9280         '''
9281 
9282         ram_gb = instance.flavor.memory_mb * units.Mi / units.Gi
9283         if ram_gb < 2:
9284             ram_gb = 2
9285 
9286         disk_gb = 0
9287         for path in disk_paths:
9288             try:
9289                 size = os.stat(path).st_size
9290                 size_gb = (size / units.Gi)
9291                 if size_gb < 2:
9292                     size_gb = 2
9293                 disk_gb += size_gb
9294             except OSError as e:
9295                 LOG.warning("Unable to stat %(disk)s: %(ex)s",
9296                             {'disk': path, 'ex': e})
9297                 # Ignore error since we don't want to break
9298                 # the migration monitoring thread operation
9299 
9300         return ram_gb + disk_gb
9301 
9302     def _get_migration_flags(self, is_block_migration):
9303         if is_block_migration:
9304             return self._block_migration_flags
9305         return self._live_migration_flags
9306 
9307     def _live_migration_monitor(self, context, instance, guest,
9308                                 dest, post_method,
9309                                 recover_method, block_migration,
9310                                 migrate_data, finish_event,
9311                                 disk_paths):
9312 
9313         on_migration_failure: ty.Deque[str] = deque()
9314         data_gb = self._live_migration_data_gb(instance, disk_paths)
9315         downtime_steps = list(libvirt_migrate.downtime_steps(data_gb))
9316         migration = migrate_data.migration
9317         curdowntime = None
9318 
9319         migration_flags = self._get_migration_flags(
9320                                   migrate_data.block_migration)
9321 
9322         n = 0
9323         start = time.time()
9324         is_post_copy_enabled = self._is_post_copy_enabled(migration_flags)
9325         # vpmem does not support post copy
9326         is_post_copy_enabled &= not bool(self._get_vpmems(instance))
9327         while True:
9328             info = guest.get_job_info()
9329 
9330             if info.type == libvirt.VIR_DOMAIN_JOB_NONE:
9331                 # Either still running, or failed or completed,
9332                 # lets untangle the mess
9333                 if not finish_event.ready():
9334                     LOG.debug("Operation thread is still running",
9335                               instance=instance)
9336                 else:
9337                     info.type = libvirt_migrate.find_job_type(guest, instance)
9338                     LOG.debug("Fixed incorrect job type to be %d",
9339                               info.type, instance=instance)
9340 
9341             if info.type == libvirt.VIR_DOMAIN_JOB_NONE:
9342                 # Migration is not yet started
9343                 LOG.debug("Migration not running yet",
9344                           instance=instance)
9345             elif info.type == libvirt.VIR_DOMAIN_JOB_UNBOUNDED:
9346                 # Migration is still running
9347                 #
9348                 # This is where we wire up calls to change live
9349                 # migration status. eg change max downtime, cancel
9350                 # the operation, change max bandwidth
9351                 libvirt_migrate.run_tasks(guest, instance,
9352                                           self.active_migrations,
9353                                           on_migration_failure,
9354                                           migration,
9355                                           is_post_copy_enabled)
9356 
9357                 now = time.time()
9358                 elapsed = now - start
9359 
9360                 completion_timeout = int(
9361                     CONF.libvirt.live_migration_completion_timeout * data_gb)
9362                 # NOTE(yikun): Check the completion timeout to determine
9363                 # should trigger the timeout action, and there are two choices
9364                 # ``abort`` (default) or ``force_complete``. If the action is
9365                 # set to ``force_complete``, the post-copy will be triggered
9366                 # if available else the VM will be suspended, otherwise the
9367                 # live migrate operation will be aborted.
9368                 if libvirt_migrate.should_trigger_timeout_action(
9369                         instance, elapsed, completion_timeout,
9370                         migration.status):
9371                     timeout_act = CONF.libvirt.live_migration_timeout_action
9372                     if timeout_act == 'force_complete':
9373                         self.live_migration_force_complete(instance)
9374                     else:
9375                         # timeout action is 'abort'
9376                         try:
9377                             guest.abort_job()
9378                         except libvirt.libvirtError as e:
9379                             LOG.warning("Failed to abort migration %s",
9380                                     encodeutils.exception_to_unicode(e),
9381                                     instance=instance)
9382                             self._clear_empty_migration(instance)
9383                             raise
9384 
9385                 curdowntime = libvirt_migrate.update_downtime(
9386                     guest, instance, curdowntime,
9387                     downtime_steps, elapsed)
9388 
9389                 # We loop every 500ms, so don't log on every
9390                 # iteration to avoid spamming logs for long
9391                 # running migrations. Just once every 5 secs
9392                 # is sufficient for developers to debug problems.
9393                 # We log once every 30 seconds at info to help
9394                 # admins see slow running migration operations
9395                 # when debug logs are off.
9396                 if (n % 10) == 0:
9397                     # Ignoring memory_processed, as due to repeated
9398                     # dirtying of data, this can be way larger than
9399                     # memory_total. Best to just look at what's
9400                     # remaining to copy and ignore what's done already
9401                     #
9402                     # TODO(berrange) perhaps we could include disk
9403                     # transfer stats in the progress too, but it
9404                     # might make memory info more obscure as large
9405                     # disk sizes might dwarf memory size
9406                     remaining = 100
9407                     if info.memory_total != 0:
9408                         remaining = round(info.memory_remaining *
9409                                           100 / info.memory_total)
9410 
9411                     libvirt_migrate.save_stats(instance, migration,
9412                                                info, remaining)
9413 
9414                     # NOTE(fanzhang): do not include disk transfer stats in
9415                     # the progress percentage calculation but log them.
9416                     disk_remaining = 100
9417                     if info.disk_total != 0:
9418                         disk_remaining = round(info.disk_remaining *
9419                                                100 / info.disk_total)
9420 
9421                     lg = LOG.debug
9422                     if (n % 60) == 0:
9423                         lg = LOG.info
9424 
9425                     lg("Migration running for %(secs)d secs, "
9426                        "memory %(remaining)d%% remaining "
9427                        "(bytes processed=%(processed_memory)d, "
9428                        "remaining=%(remaining_memory)d, "
9429                        "total=%(total_memory)d); "
9430                        "disk %(disk_remaining)d%% remaining "
9431                        "(bytes processed=%(processed_disk)d, "
9432                        "remaining=%(remaining_disk)d, "
9433                        "total=%(total_disk)d).",
9434                        {"secs": n / 2, "remaining": remaining,
9435                         "processed_memory": info.memory_processed,
9436                         "remaining_memory": info.memory_remaining,
9437                         "total_memory": info.memory_total,
9438                         "disk_remaining": disk_remaining,
9439                         "processed_disk": info.disk_processed,
9440                         "remaining_disk": info.disk_remaining,
9441                         "total_disk": info.disk_total}, instance=instance)
9442 
9443                 n = n + 1
9444             elif info.type == libvirt.VIR_DOMAIN_JOB_COMPLETED:
9445                 # Migration is all done
9446                 LOG.info("Migration operation has completed",
9447                          instance=instance)
9448                 post_method(context, instance, dest, block_migration,
9449                             migrate_data)
9450                 break
9451             elif info.type == libvirt.VIR_DOMAIN_JOB_FAILED:
9452                 # Migration did not succeed
9453                 LOG.error("Migration operation has aborted", instance=instance)
9454                 libvirt_migrate.run_recover_tasks(self._host, guest, instance,
9455                                                   on_migration_failure)
9456                 recover_method(context, instance, dest, migrate_data)
9457                 break
9458             elif info.type == libvirt.VIR_DOMAIN_JOB_CANCELLED:
9459                 # Migration was stopped by admin
9460                 LOG.warning("Migration operation was cancelled",
9461                             instance=instance)
9462                 libvirt_migrate.run_recover_tasks(self._host, guest, instance,
9463                                                   on_migration_failure)
9464                 recover_method(context, instance, dest, migrate_data,
9465                                migration_status='cancelled')
9466                 break
9467             else:
9468                 LOG.warning("Unexpected migration job type: %d",
9469                             info.type, instance=instance)
9470 
9471             time.sleep(0.5)
9472         self._clear_empty_migration(instance)
9473 
9474     def _clear_empty_migration(self, instance):
9475         try:
9476             del self.active_migrations[instance.uuid]
9477         except KeyError:
9478             LOG.warning("There are no records in active migrations "
9479                         "for instance", instance=instance)
9480 
9481     def _live_migration(self, context, instance, dest, post_method,
9482                         recover_method, block_migration,
9483                         migrate_data):
9484         """Do live migration.
9485 
9486         :param context: security context
9487         :param instance:
9488             nova.db.sqlalchemy.models.Instance object
9489             instance object that is migrated.
9490         :param dest: destination host
9491         :param post_method:
9492             post operation method.
9493             expected nova.compute.manager._post_live_migration.
9494         :param recover_method:
9495             recovery method when any exception occurs.
9496             expected nova.compute.manager._rollback_live_migration.
9497         :param block_migration: if true, do block migration.
9498         :param migrate_data: a LibvirtLiveMigrateData object
9499 
9500         This fires off a new thread to run the blocking migration
9501         operation, and then this thread monitors the progress of
9502         migration and controls its operation
9503         """
9504 
9505         guest = self._host.get_guest(instance)
9506 
9507         disk_paths = []
9508         device_names = []
9509         if (migrate_data.block_migration and
9510                 CONF.libvirt.virt_type != "parallels"):
9511             disk_paths, device_names = self._live_migration_copy_disk_paths(
9512                 context, instance, guest)
9513 
9514         opthread = utils.spawn(self._live_migration_operation,
9515                                      context, instance, dest,
9516                                      block_migration,
9517                                      migrate_data, guest,
9518                                      device_names)
9519 
9520         finish_event = eventlet.event.Event()
9521         self.active_migrations[instance.uuid] = deque()
9522 
9523         def thread_finished(thread, event):
9524             LOG.debug("Migration operation thread notification",
9525                       instance=instance)
9526             event.send()
9527         opthread.link(thread_finished, finish_event)
9528 
9529         # Let eventlet schedule the new thread right away
9530         time.sleep(0)
9531 
9532         try:
9533             LOG.debug("Starting monitoring of live migration",
9534                       instance=instance)
9535             self._live_migration_monitor(context, instance, guest, dest,
9536                                          post_method, recover_method,
9537                                          block_migration, migrate_data,
9538                                          finish_event, disk_paths)
9539         except Exception as ex:
9540             LOG.warning("Error monitoring migration: %(ex)s",
9541                         {"ex": ex}, instance=instance, exc_info=True)
9542             raise
9543         finally:
9544             LOG.debug("Live migration monitoring is all done",
9545                       instance=instance)
9546 
9547     def _is_post_copy_enabled(self, migration_flags):
9548         return (migration_flags & libvirt.VIR_MIGRATE_POSTCOPY) != 0
9549 
9550     def live_migration_force_complete(self, instance):
9551         try:
9552             self.active_migrations[instance.uuid].append('force-complete')
9553         except KeyError:
9554             raise exception.NoActiveMigrationForInstance(
9555                 instance_id=instance.uuid)
9556 
9557     def _try_fetch_image(self, context, path, image_id, instance,
9558                          fallback_from_host=None):
9559         try:
9560             libvirt_utils.fetch_image(context, path, image_id,
9561                                       instance.trusted_certs)
9562         except exception.ImageNotFound:
9563             if not fallback_from_host:
9564                 raise
9565             LOG.debug("Image %(image_id)s doesn't exist anymore on "
9566                       "image service, attempting to copy image "
9567                       "from %(host)s",
9568                       {'image_id': image_id, 'host': fallback_from_host})
9569             libvirt_utils.copy_image(src=path, dest=path,
9570                                      host=fallback_from_host,
9571                                      receive=True)
9572 
9573     def _fetch_instance_kernel_ramdisk(self, context, instance,
9574                                        fallback_from_host=None):
9575         """Download kernel and ramdisk for instance in instance directory."""
9576         instance_dir = libvirt_utils.get_instance_path(instance)
9577         if instance.kernel_id:
9578             kernel_path = os.path.join(instance_dir, 'kernel')
9579             # NOTE(dsanders): only fetch image if it's not available at
9580             # kernel_path. This also avoids ImageNotFound exception if
9581             # the image has been deleted from glance
9582             if not os.path.exists(kernel_path):
9583                 self._try_fetch_image(context,
9584                                       kernel_path,
9585                                       instance.kernel_id,
9586                                       instance, fallback_from_host)
9587             if instance.ramdisk_id:
9588                 ramdisk_path = os.path.join(instance_dir, 'ramdisk')
9589                 # NOTE(dsanders): only fetch image if it's not available at
9590                 # ramdisk_path. This also avoids ImageNotFound exception if
9591                 # the image has been deleted from glance
9592                 if not os.path.exists(ramdisk_path):
9593                     self._try_fetch_image(context,
9594                                           ramdisk_path,
9595                                           instance.ramdisk_id,
9596                                           instance, fallback_from_host)
9597 
9598     def _reattach_instance_vifs(self, context, instance, network_info):
9599         guest = self._host.get_guest(instance)
9600         # validate that the guest has the expected number of interfaces
9601         # attached.
9602         guest_interfaces = guest.get_interfaces()
9603         # NOTE(sean-k-mooney): In general len(guest_interfaces) will
9604         # be equal to len(network_info) as interfaces will not be hot unplugged
9605         # unless they are SR-IOV direct mode interfaces. As such we do not
9606         # need an else block here as it would be a noop.
9607         if len(guest_interfaces) < len(network_info):
9608             # NOTE(sean-k-mooney): we are doing a post live migration
9609             # for a guest with sriov vif that were detached as part of
9610             # the migration. loop over the vifs and attach the missing
9611             # vif as part of the post live migration phase.
9612             direct_vnics = network_model.VNIC_TYPES_DIRECT_PASSTHROUGH
9613             for vif in network_info:
9614                 if vif['vnic_type'] in direct_vnics:
9615                     LOG.info("Attaching vif %s to instance %s",
9616                              vif['id'], instance.id)
9617                     self.attach_interface(context, instance,
9618                                           instance.image_meta, vif)
9619 
9620     def rollback_live_migration_at_source(self, context, instance,
9621                                           migrate_data):
9622         """reconnect sriov interfaces after failed live migration
9623         :param context: security context
9624         :param instance:  the instance being migrated
9625         :param migrate_date: a LibvirtLiveMigrateData object
9626         """
9627         network_info = network_model.NetworkInfo(
9628             [vif.source_vif for vif in migrate_data.vifs
9629                             if "source_vif" in vif and vif.source_vif])
9630         self._reattach_instance_vifs(context, instance, network_info)
9631 
9632     def rollback_live_migration_at_destination(self, context, instance,
9633                                                network_info,
9634                                                block_device_info,
9635                                                destroy_disks=True,
9636                                                migrate_data=None):
9637         """Clean up destination node after a failed live migration."""
9638         try:
9639             self.destroy(context, instance, network_info, block_device_info,
9640                          destroy_disks)
9641         finally:
9642             # NOTE(gcb): Failed block live migration may leave instance
9643             # directory at destination node, ensure it is always deleted.
9644             is_shared_instance_path = True
9645             if migrate_data:
9646                 is_shared_instance_path = migrate_data.is_shared_instance_path
9647                 if (migrate_data.obj_attr_is_set("serial_listen_ports") and
9648                         migrate_data.serial_listen_ports):
9649                     # Releases serial ports reserved.
9650                     for port in migrate_data.serial_listen_ports:
9651                         serial_console.release_port(
9652                             host=migrate_data.serial_listen_addr, port=port)
9653 
9654             if not is_shared_instance_path:
9655                 instance_dir = libvirt_utils.get_instance_path_at_destination(
9656                     instance, migrate_data)
9657                 if os.path.exists(instance_dir):
9658                     shutil.rmtree(instance_dir)
9659 
9660     def _pre_live_migration_plug_vifs(self, instance, network_info,
9661                                       migrate_data):
9662         if 'vifs' in migrate_data and migrate_data.vifs:
9663             LOG.debug('Plugging VIFs using destination host port bindings '
9664                       'before live migration.', instance=instance)
9665             vif_plug_nw_info = network_model.NetworkInfo([])
9666             for migrate_vif in migrate_data.vifs:
9667                 vif_plug_nw_info.append(migrate_vif.get_dest_vif())
9668         else:
9669             LOG.debug('Plugging VIFs before live migration.',
9670                       instance=instance)
9671             vif_plug_nw_info = network_info
9672         # Retry operation is necessary because continuous live migration
9673         # requests to the same host cause concurrent requests to iptables,
9674         # then it complains.
9675         max_retry = CONF.live_migration_retry_count
9676         for cnt in range(max_retry):
9677             try:
9678                 self.plug_vifs(instance, vif_plug_nw_info)
9679                 break
9680             except processutils.ProcessExecutionError:
9681                 if cnt == max_retry - 1:
9682                     raise
9683                 else:
9684                     LOG.warning('plug_vifs() failed %(cnt)d. Retry up to '
9685                                 '%(max_retry)d.',
9686                                 {'cnt': cnt, 'max_retry': max_retry},
9687                                 instance=instance)
9688                     greenthread.sleep(1)
9689 
9690     def pre_live_migration(self, context, instance, block_device_info,
9691                            network_info, disk_info, migrate_data):
9692         """Preparation live migration."""
9693         if disk_info is not None:
9694             disk_info = jsonutils.loads(disk_info)
9695 
9696         LOG.debug('migrate_data in pre_live_migration: %s', migrate_data,
9697                   instance=instance)
9698         is_shared_block_storage = migrate_data.is_shared_block_storage
9699         is_shared_instance_path = migrate_data.is_shared_instance_path
9700         is_block_migration = migrate_data.block_migration
9701 
9702         if not is_shared_instance_path:
9703             instance_dir = libvirt_utils.get_instance_path_at_destination(
9704                             instance, migrate_data)
9705 
9706             if os.path.exists(instance_dir):
9707                 raise exception.DestinationDiskExists(path=instance_dir)
9708 
9709             LOG.debug('Creating instance directory: %s', instance_dir,
9710                       instance=instance)
9711             os.mkdir(instance_dir)
9712 
9713             # Recreate the disk.info file and in doing so stop the
9714             # imagebackend from recreating it incorrectly by inspecting the
9715             # contents of each file when using the Raw backend.
9716             if disk_info:
9717                 image_disk_info = {}
9718                 for info in disk_info:
9719                     image_file = os.path.basename(info['path'])
9720                     image_path = os.path.join(instance_dir, image_file)
9721                     image_disk_info[image_path] = info['type']
9722 
9723                 LOG.debug('Creating disk.info with the contents: %s',
9724                           image_disk_info, instance=instance)
9725 
9726                 image_disk_info_path = os.path.join(instance_dir,
9727                                                     'disk.info')
9728                 with open(image_disk_info_path, 'w') as f:
9729                     f.write(jsonutils.dumps(image_disk_info))
9730 
9731             if not is_shared_block_storage:
9732                 # Ensure images and backing files are present.
9733                 LOG.debug('Checking to make sure images and backing files are '
9734                           'present before live migration.', instance=instance)
9735                 self._create_images_and_backing(
9736                     context, instance, instance_dir, disk_info,
9737                     fallback_from_host=instance.host)
9738                 if (configdrive.required_by(instance) and
9739                         CONF.config_drive_format == 'iso9660'):
9740                     # NOTE(pkoniszewski): Due to a bug in libvirt iso config
9741                     # drive needs to be copied to destination prior to
9742                     # migration when instance path is not shared and block
9743                     # storage is not shared. Files that are already present
9744                     # on destination are excluded from a list of files that
9745                     # need to be copied to destination. If we don't do that
9746                     # live migration will fail on copying iso config drive to
9747                     # destination and writing to read-only device.
9748                     # Please see bug/1246201 for more details.
9749                     src = "%s:%s/disk.config" % (instance.host, instance_dir)
9750                     self._remotefs.copy_file(src, instance_dir)
9751 
9752             if not is_block_migration:
9753                 # NOTE(angdraug): when block storage is shared between source
9754                 # and destination and instance path isn't (e.g. volume backed
9755                 # or rbd backed instance), instance path on destination has to
9756                 # be prepared
9757 
9758                 # Required by Quobyte CI
9759                 self._ensure_console_log_for_instance(instance)
9760 
9761                 # if image has kernel and ramdisk, just download
9762                 # following normal way.
9763                 self._fetch_instance_kernel_ramdisk(context, instance)
9764 
9765         # Establishing connection to volume server.
9766         block_device_mapping = driver.block_device_info_get_mapping(
9767             block_device_info)
9768 
9769         if len(block_device_mapping):
9770             LOG.debug('Connecting volumes before live migration.',
9771                       instance=instance)
9772 
9773         for bdm in block_device_mapping:
9774             connection_info = bdm['connection_info']
9775             self._connect_volume(context, connection_info, instance)
9776 
9777         self._pre_live_migration_plug_vifs(
9778             instance, network_info, migrate_data)
9779 
9780         # Store server_listen and latest disk device info
9781         if not migrate_data:
9782             migrate_data = objects.LibvirtLiveMigrateData(bdms=[])
9783         else:
9784             migrate_data.bdms = []
9785         # Store live_migration_inbound_addr
9786         migrate_data.target_connect_addr = \
9787             CONF.libvirt.live_migration_inbound_addr
9788         migrate_data.supported_perf_events = self._supported_perf_events
9789 
9790         migrate_data.serial_listen_ports = []
9791         if CONF.serial_console.enabled:
9792             num_ports = hardware.get_number_of_serial_ports(
9793                 instance.flavor, instance.image_meta)
9794             for port in range(num_ports):
9795                 migrate_data.serial_listen_ports.append(
9796                     serial_console.acquire_port(
9797                         migrate_data.serial_listen_addr))
9798 
9799         for vol in block_device_mapping:
9800             connection_info = vol['connection_info']
9801             if connection_info.get('serial'):
9802                 disk_info = blockinfo.get_info_from_bdm(
9803                     instance, CONF.libvirt.virt_type,
9804                     instance.image_meta, vol)
9805 
9806                 bdmi = objects.LibvirtLiveMigrateBDMInfo()
9807                 bdmi.serial = connection_info['serial']
9808                 bdmi.connection_info = connection_info
9809                 bdmi.bus = disk_info['bus']
9810                 bdmi.dev = disk_info['dev']
9811                 bdmi.type = disk_info['type']
9812                 bdmi.format = disk_info.get('format')
9813                 bdmi.boot_index = disk_info.get('boot_index')
9814                 volume_secret = self._host.find_secret('volume', vol.volume_id)
9815                 if volume_secret:
9816                     bdmi.encryption_secret_uuid = volume_secret.UUIDString()
9817 
9818                 migrate_data.bdms.append(bdmi)
9819 
9820         return migrate_data
9821 
9822     def _try_fetch_image_cache(self, image, fetch_func, context, filename,
9823                                image_id, instance, size,
9824                                fallback_from_host=None):
9825         try:
9826             image.cache(fetch_func=fetch_func,
9827                         context=context,
9828                         filename=filename,
9829                         image_id=image_id,
9830                         size=size,
9831                         trusted_certs=instance.trusted_certs)
9832         except exception.ImageNotFound:
9833             if not fallback_from_host:
9834                 raise
9835             LOG.debug("Image %(image_id)s doesn't exist anymore "
9836                       "on image service, attempting to copy "
9837                       "image from %(host)s",
9838                       {'image_id': image_id, 'host': fallback_from_host},
9839                       instance=instance)
9840 
9841             def copy_from_host(target):
9842                 libvirt_utils.copy_image(src=target,
9843                                          dest=target,
9844                                          host=fallback_from_host,
9845                                          receive=True)
9846             image.cache(fetch_func=copy_from_host, size=size,
9847                         filename=filename)
9848 
9849         # NOTE(lyarwood): If the instance vm_state is shelved offloaded then we
9850         # must be unshelving for _try_fetch_image_cache to be called.
9851         # NOTE(mriedem): Alternatively if we are doing a cross-cell move of a
9852         # non-volume-backed server and finishing (spawning) on the dest host,
9853         # we have to flatten the rbd image so we can delete the temporary
9854         # snapshot in the compute manager.
9855         mig_context = instance.migration_context
9856         cross_cell_move = (
9857                 mig_context and mig_context.is_cross_cell_move() or False)
9858         if instance.vm_state == vm_states.SHELVED_OFFLOADED or cross_cell_move:
9859             # NOTE(lyarwood): When using the rbd imagebackend the call to cache
9860             # above will attempt to clone from the shelved snapshot in Glance
9861             # if available from this compute. We then need to flatten the
9862             # resulting image to avoid it still referencing and ultimately
9863             # blocking the removal of the shelved snapshot at the end of the
9864             # unshelve. This is a no-op for all but the rbd imagebackend.
9865             action = (
9866                 'migrating instance across cells' if cross_cell_move
9867                 else 'unshelving instance')
9868             try:
9869                 image.flatten()
9870                 LOG.debug('Image %s flattened successfully while %s.',
9871                           image.path, action, instance=instance)
9872             except NotImplementedError:
9873                 # NOTE(lyarwood): There's an argument to be made for logging
9874                 # our inability to call flatten here, however given this isn't
9875                 # implemented for most of the backends it may do more harm than
9876                 # good, concerning operators etc so for now just pass.
9877                 pass
9878 
9879     def _create_images_and_backing(self, context, instance, instance_dir,
9880                                    disk_info, fallback_from_host=None):
9881         """:param context: security context
9882            :param instance:
9883                nova.db.sqlalchemy.models.Instance object
9884                instance object that is migrated.
9885            :param instance_dir:
9886                instance path to use, calculated externally to handle block
9887                migrating an instance with an old style instance path
9888            :param disk_info:
9889                disk info specified in _get_instance_disk_info_from_config
9890                (list of dicts)
9891            :param fallback_from_host:
9892                host where we can retrieve images if the glance images are
9893                not available.
9894 
9895         """
9896 
9897         # Virtuozzo containers don't use backing file
9898         if (CONF.libvirt.virt_type == "parallels" and
9899                 instance.vm_mode == fields.VMMode.EXE):
9900             return
9901 
9902         if not disk_info:
9903             disk_info = []
9904 
9905         for info in disk_info:
9906             base = os.path.basename(info['path'])
9907             # Get image type and create empty disk image, and
9908             # create backing file in case of qcow2.
9909             instance_disk = os.path.join(instance_dir, base)
9910             if not info['backing_file'] and not os.path.exists(instance_disk):
9911                 libvirt_utils.create_image(info['type'], instance_disk,
9912                                            info['virt_disk_size'])
9913             elif info['backing_file']:
9914                 # Creating backing file follows same way as spawning instances.
9915                 cache_name = os.path.basename(info['backing_file'])
9916 
9917                 disk = self.image_backend.by_name(instance, instance_disk,
9918                                                   CONF.libvirt.images_type)
9919                 if cache_name.startswith('ephemeral'):
9920                     # The argument 'size' is used by image.cache to
9921                     # validate disk size retrieved from cache against
9922                     # the instance disk size (should always return OK)
9923                     # and ephemeral_size is used by _create_ephemeral
9924                     # to build the image if the disk is not already
9925                     # cached.
9926                     disk.cache(
9927                         fetch_func=self._create_ephemeral,
9928                         fs_label=cache_name,
9929                         os_type=instance.os_type,
9930                         filename=cache_name,
9931                         size=info['virt_disk_size'],
9932                         ephemeral_size=info['virt_disk_size'] / units.Gi)
9933                 elif cache_name.startswith('swap'):
9934                     inst_type = instance.get_flavor()
9935                     swap_mb = inst_type.swap
9936                     disk.cache(fetch_func=self._create_swap,
9937                                 filename="swap_%s" % swap_mb,
9938                                 size=swap_mb * units.Mi,
9939                                 swap_mb=swap_mb)
9940                 else:
9941                     self._try_fetch_image_cache(disk,
9942                                                 libvirt_utils.fetch_image,
9943                                                 context, cache_name,
9944                                                 instance.image_ref,
9945                                                 instance,
9946                                                 info['virt_disk_size'],
9947                                                 fallback_from_host)
9948 
9949         # if disk has kernel and ramdisk, just download
9950         # following normal way.
9951         self._fetch_instance_kernel_ramdisk(
9952             context, instance, fallback_from_host=fallback_from_host)
9953 
9954     def post_live_migration(self, context, instance, block_device_info,
9955                             migrate_data=None):
9956         # NOTE(mdbooth): The block_device_info we were passed was initialized
9957         # with BDMs from the source host before they were updated to point to
9958         # the destination. We can safely use this to disconnect the source
9959         # without re-fetching.
9960         block_device_mapping = driver.block_device_info_get_mapping(
9961                 block_device_info)
9962 
9963         for vol in block_device_mapping:
9964             connection_info = vol['connection_info']
9965             # NOTE(lyarwood): Ignore exceptions here to avoid the instance
9966             # being left in an ERROR state and still marked on the source.
9967             try:
9968                 self._disconnect_volume(context, connection_info, instance)
9969             except Exception:
9970                 volume_id = driver_block_device.get_volume_id(connection_info)
9971                 LOG.exception("Ignoring exception while attempting to "
9972                               "disconnect volume %s from the source host "
9973                               "during post_live_migration", volume_id,
9974                               instance=instance)
9975 
9976     def post_live_migration_at_source(self, context, instance, network_info):
9977         """Unplug VIFs from networks at source.
9978 
9979         :param context: security context
9980         :param instance: instance object reference
9981         :param network_info: instance network information
9982         """
9983         self.unplug_vifs(instance, network_info)
9984 
9985     def post_live_migration_at_destination(self, context,
9986                                            instance,
9987                                            network_info,
9988                                            block_migration=False,
9989                                            block_device_info=None):
9990         """Post operation of live migration at destination host.
9991 
9992         :param context: security context
9993         :param instance:
9994             nova.db.sqlalchemy.models.Instance object
9995             instance object that is migrated.
9996         :param network_info: instance network information
9997         :param block_migration: if true, post operation of block_migration.
9998         """
9999         self._reattach_instance_vifs(context, instance, network_info)
10000 
10001     def _get_instance_disk_info_from_config(self, guest_config,
10002                                             block_device_info):
10003         """Get the non-volume disk information from the domain xml
10004 
10005         :param LibvirtConfigGuest guest_config: the libvirt domain config
10006                                                 for the instance
10007         :param dict block_device_info: block device info for BDMs
10008         :returns disk_info: list of dicts with keys:
10009 
10010           * 'type': the disk type (str)
10011           * 'path': the disk path (str)
10012           * 'virt_disk_size': the virtual disk size (int)
10013           * 'backing_file': backing file of a disk image (str)
10014           * 'disk_size': physical disk size (int)
10015           * 'over_committed_disk_size': virt_disk_size - disk_size or 0
10016         """
10017         block_device_mapping = driver.block_device_info_get_mapping(
10018             block_device_info)
10019 
10020         volume_devices = set()
10021         for vol in block_device_mapping:
10022             disk_dev = vol['mount_device'].rpartition("/")[2]
10023             volume_devices.add(disk_dev)
10024 
10025         disk_info = []
10026 
10027         if (guest_config.virt_type == 'parallels' and
10028                 guest_config.os_type == fields.VMMode.EXE):
10029             node_type = 'filesystem'
10030         else:
10031             node_type = 'disk'
10032 
10033         for device in guest_config.devices:
10034             if device.root_name != node_type:
10035                 continue
10036             disk_type = device.source_type
10037             if device.root_name == 'filesystem':
10038                 target = device.target_dir
10039                 if device.source_type == 'file':
10040                     path = device.source_file
10041                 elif device.source_type == 'block':
10042                     path = device.source_dev
10043                 else:
10044                     path = None
10045             else:
10046                 target = device.target_dev
10047                 path = device.source_path
10048 
10049             if not path:
10050                 LOG.debug('skipping disk for %s as it does not have a path',
10051                           guest_config.name)
10052                 continue
10053 
10054             if disk_type not in ['file', 'block']:
10055                 LOG.debug('skipping disk because it looks like a volume', path)
10056                 continue
10057 
10058             if target in volume_devices:
10059                 LOG.debug('skipping disk %(path)s (%(target)s) as it is a '
10060                           'volume', {'path': path, 'target': target})
10061                 continue
10062 
10063             if device.root_name == 'filesystem':
10064                 driver_type = device.driver_type
10065             else:
10066                 driver_type = device.driver_format
10067             # get the real disk size or
10068             # raise a localized error if image is unavailable
10069             if disk_type == 'file' and driver_type == 'ploop':
10070                 dk_size = 0
10071                 for dirpath, dirnames, filenames in os.walk(path):
10072                     for f in filenames:
10073                         fp = os.path.join(dirpath, f)
10074                         dk_size += os.path.getsize(fp)
10075                 qemu_img_info = disk_api.get_disk_info(path)
10076                 virt_size = qemu_img_info.virtual_size
10077                 backing_file = libvirt_utils.get_disk_backing_file(path)
10078                 over_commit_size = int(virt_size) - dk_size
10079 
10080             elif disk_type == 'file' and driver_type == 'qcow2':
10081                 qemu_img_info = disk_api.get_disk_info(path)
10082                 dk_size = qemu_img_info.disk_size
10083                 virt_size = qemu_img_info.virtual_size
10084                 backing_file = libvirt_utils.get_disk_backing_file(path)
10085                 over_commit_size = max(0, int(virt_size) - dk_size)
10086 
10087             elif disk_type == 'file':
10088                 dk_size = os.stat(path).st_blocks * 512
10089                 virt_size = os.path.getsize(path)
10090                 backing_file = ""
10091                 over_commit_size = int(virt_size) - dk_size
10092 
10093             elif disk_type == 'block' and block_device_info:
10094                 dk_size = lvm.get_volume_size(path)
10095                 virt_size = dk_size
10096                 backing_file = ""
10097                 over_commit_size = 0
10098 
10099             else:
10100                 LOG.debug('skipping disk %(path)s (%(target)s) - unable to '
10101                           'determine if volume',
10102                           {'path': path, 'target': target})
10103                 continue
10104 
10105             disk_info.append({'type': driver_type,
10106                               'path': path,
10107                               'virt_disk_size': virt_size,
10108                               'backing_file': backing_file,
10109                               'disk_size': dk_size,
10110                               'over_committed_disk_size': over_commit_size})
10111         return disk_info
10112 
10113     def _get_instance_disk_info(self, instance, block_device_info):
10114         try:
10115             guest = self._host.get_guest(instance)
10116             config = guest.get_config()
10117         except libvirt.libvirtError as ex:
10118             error_code = ex.get_error_code()
10119             LOG.warning('Error from libvirt while getting description of '
10120                         '%(instance_name)s: [Error Code %(error_code)s] '
10121                         '%(ex)s',
10122                         {'instance_name': instance.name,
10123                          'error_code': error_code,
10124                          'ex': encodeutils.exception_to_unicode(ex)},
10125                         instance=instance)
10126             raise exception.InstanceNotFound(instance_id=instance.uuid)
10127 
10128         return self._get_instance_disk_info_from_config(config,
10129                                                         block_device_info)
10130 
10131     def get_instance_disk_info(self, instance,
10132                                block_device_info=None):
10133         return jsonutils.dumps(
10134             self._get_instance_disk_info(instance, block_device_info))
10135 
10136     def _get_disk_over_committed_size_total(self):
10137         """Return total over committed disk size for all instances."""
10138         # Disk size that all instance uses : virtual_size - disk_size
10139         disk_over_committed_size = 0
10140         instance_domains = self._host.list_instance_domains(only_running=False)
10141         if not instance_domains:
10142             return disk_over_committed_size
10143 
10144         # Get all instance uuids
10145         instance_uuids = [dom.UUIDString() for dom in instance_domains]
10146         ctx = nova_context.get_admin_context()
10147         # Get instance object list by uuid filter
10148         filters = {'uuid': instance_uuids}
10149         # NOTE(ankit): objects.InstanceList.get_by_filters method is
10150         # getting called twice one is here and another in the
10151         # _update_available_resource method of resource_tracker. Since
10152         # _update_available_resource method is synchronized, there is a
10153         # possibility the instances list retrieved here to calculate
10154         # disk_over_committed_size would differ to the list you would get
10155         # in _update_available_resource method for calculating usages based
10156         # on instance utilization.
10157         local_instance_list = objects.InstanceList.get_by_filters(
10158             ctx, filters, use_slave=True)
10159         # Convert instance list to dictionary with instance uuid as key.
10160         local_instances = {inst.uuid: inst for inst in local_instance_list}
10161 
10162         # Get bdms by instance uuids
10163         bdms = objects.BlockDeviceMappingList.bdms_by_instance_uuid(
10164             ctx, instance_uuids)
10165 
10166         for dom in instance_domains:
10167             try:
10168                 guest = libvirt_guest.Guest(dom)
10169                 config = guest.get_config()
10170 
10171                 block_device_info = None
10172                 if guest.uuid in local_instances \
10173                         and (bdms and guest.uuid in bdms):
10174                     # Get block device info for instance
10175                     block_device_info = driver.get_block_device_info(
10176                         local_instances[guest.uuid], bdms[guest.uuid])
10177 
10178                 disk_infos = self._get_instance_disk_info_from_config(
10179                     config, block_device_info)
10180                 if not disk_infos:
10181                     continue
10182 
10183                 for info in disk_infos:
10184                     disk_over_committed_size += int(
10185                         info['over_committed_disk_size'])
10186             except libvirt.libvirtError as ex:
10187                 error_code = ex.get_error_code()
10188                 LOG.warning(
10189                     'Error from libvirt while getting description of '
10190                     '%(instance_name)s: [Error Code %(error_code)s] %(ex)s',
10191                     {'instance_name': guest.name,
10192                      'error_code': error_code,
10193                      'ex': encodeutils.exception_to_unicode(ex)})
10194             except OSError as e:
10195                 if e.errno in (errno.ENOENT, errno.ESTALE):
10196                     LOG.warning('Periodic task is updating the host stat, '
10197                                 'it is trying to get disk %(i_name)s, '
10198                                 'but disk file was removed by concurrent '
10199                                 'operations such as resize.',
10200                                 {'i_name': guest.name})
10201                 elif e.errno == errno.EACCES:
10202                     LOG.warning('Periodic task is updating the host stat, '
10203                                 'it is trying to get disk %(i_name)s, '
10204                                 'but access is denied. It is most likely '
10205                                 'due to a VM that exists on the compute '
10206                                 'node but is not managed by Nova.',
10207                                 {'i_name': guest.name})
10208                 else:
10209                     raise
10210             except (exception.VolumeBDMPathNotFound,
10211                     exception.DiskNotFound) as e:
10212                 if isinstance(e, exception.VolumeBDMPathNotFound):
10213                     thing = 'backing volume block device'
10214                 elif isinstance(e, exception.DiskNotFound):
10215                     thing = 'backing disk storage'
10216 
10217                 LOG.warning('Periodic task is updating the host stats, '
10218                             'it is trying to get disk info for %(i_name)s, '
10219                             'but the %(thing)s was removed by a concurrent '
10220                             'operation such as resize. Error: %(error)s',
10221                             {'i_name': guest.name, 'thing': thing, 'error': e})
10222 
10223             # NOTE(gtt116): give other tasks a chance.
10224             greenthread.sleep(0)
10225         return disk_over_committed_size
10226 
10227     def get_available_nodes(self, refresh=False):
10228         return [self._host.get_hostname()]
10229 
10230     def get_host_cpu_stats(self):
10231         """Return the current CPU state of the host."""
10232         return self._host.get_cpu_stats()
10233 
10234     def get_host_uptime(self):
10235         """Returns the result of calling "uptime"."""
10236         out, err = processutils.execute('env', 'LANG=C', 'uptime')
10237         return out
10238 
10239     def manage_image_cache(self, context, all_instances):
10240         """Manage the local cache of images."""
10241         self.image_cache_manager.update(context, all_instances)
10242 
10243     def _cleanup_remote_migration(self, dest, inst_base, inst_base_resize,
10244                                   shared_storage=False):
10245         """Used only for cleanup in case migrate_disk_and_power_off fails."""
10246         try:
10247             if os.path.exists(inst_base_resize):
10248                 shutil.rmtree(inst_base, ignore_errors=True)
10249                 os.rename(inst_base_resize, inst_base)
10250                 if not shared_storage:
10251                     self._remotefs.remove_dir(dest, inst_base)
10252         except Exception:
10253             pass
10254 
10255     def cache_image(self, context, image_id):
10256         cache_dir = os.path.join(CONF.instances_path,
10257                                  CONF.image_cache.subdirectory_name)
10258         path = os.path.join(cache_dir,
10259                             imagecache.get_cache_fname(image_id))
10260         if os.path.exists(path):
10261             LOG.info('Image %(image_id)s already cached; updating timestamp',
10262                      {'image_id': image_id})
10263             # NOTE(danms): The regular image cache routines use a wrapper
10264             # (_update_utime_ignore_eacces()) around this to avoid failing
10265             # on permissions (which may or may not be legit due to an NFS
10266             # race). However, since this is best-effort, errors are swallowed
10267             # by compute manager per-image, and we are compelled to report
10268             # errors up our stack, we use the raw method here to avoid the
10269             # silent ignore of the EACCESS.
10270             nova.privsep.path.utime(path)
10271             return False
10272         else:
10273             # NOTE(danms): In case we are running before the first boot, make
10274             # sure the cache directory is created
10275             if not os.path.isdir(cache_dir):
10276                 fileutils.ensure_tree(cache_dir)
10277             LOG.info('Caching image %(image_id)s by request',
10278                      {'image_id': image_id})
10279             # NOTE(danms): The imagebackend code, as called via spawn() where
10280             # images are normally cached, uses a lock on the root disk it is
10281             # creating at the time, but relies on the
10282             # compute_utils.disk_ops_semaphore for cache fetch mutual
10283             # exclusion, which is grabbed in images.fetch() (which is called
10284             # by images.fetch_to_raw() below). So, by calling fetch_to_raw(),
10285             # we are sharing the same locking for the cache fetch as the
10286             # rest of the code currently called only from spawn().
10287             images.fetch_to_raw(context, image_id, path)
10288             return True
10289 
10290     def _get_disk_size_reserved_for_image_cache(self):
10291         """Return the amount of DISK_GB resource need to be reserved for the
10292         image cache.
10293 
10294         :returns: The disk space in GB
10295         """
10296         if not CONF.workarounds.reserve_disk_resource_for_image_cache:
10297             return 0
10298 
10299         return compute_utils.convert_mb_to_ceil_gb(
10300             self.image_cache_manager.get_disk_usage() / 1024.0 / 1024.0)
10301 
10302     def _is_path_shared_with(self, dest, path):
10303         # NOTE (rmk): There are two methods of determining whether we are
10304         #             on the same filesystem: the source and dest IP are the
10305         #             same, or we create a file on the dest system via SSH
10306         #             and check whether the source system can also see it.
10307         shared_path = (dest == self.get_host_ip_addr())
10308         if not shared_path:
10309             tmp_file = uuidutils.generate_uuid(dashed=False) + '.tmp'
10310             tmp_path = os.path.join(path, tmp_file)
10311 
10312             try:
10313                 self._remotefs.create_file(dest, tmp_path)
10314                 if os.path.exists(tmp_path):
10315                     shared_path = True
10316                     os.unlink(tmp_path)
10317                 else:
10318                     self._remotefs.remove_file(dest, tmp_path)
10319             except Exception:
10320                 pass
10321         return shared_path
10322 
10323     def migrate_disk_and_power_off(self, context, instance, dest,
10324                                    flavor, network_info,
10325                                    block_device_info=None,
10326                                    timeout=0, retry_interval=0):
10327         LOG.debug("Starting migrate_disk_and_power_off",
10328                    instance=instance)
10329 
10330         ephemerals = driver.block_device_info_get_ephemerals(block_device_info)
10331 
10332         # get_bdm_ephemeral_disk_size() will return 0 if the new
10333         # instance's requested block device mapping contain no
10334         # ephemeral devices. However, we still want to check if
10335         # the original instance's ephemeral_gb property was set and
10336         # ensure that the new requested flavor ephemeral size is greater
10337         eph_size = (block_device.get_bdm_ephemeral_disk_size(ephemerals) or
10338                     instance.flavor.ephemeral_gb)
10339 
10340         # Checks if the migration needs a disk resize down.
10341         root_down = flavor.root_gb < instance.flavor.root_gb
10342         ephemeral_down = flavor.ephemeral_gb < eph_size
10343         booted_from_volume = self._is_booted_from_volume(block_device_info)
10344 
10345         if (root_down and not booted_from_volume) or ephemeral_down:
10346             reason = _("Unable to resize disk down.")
10347             raise exception.InstanceFaultRollback(
10348                 exception.ResizeError(reason=reason))
10349 
10350         # NOTE(dgenin): Migration is not implemented for LVM backed instances.
10351         if CONF.libvirt.images_type == 'lvm' and not booted_from_volume:
10352             reason = _("Migration is not supported for LVM backed instances")
10353             raise exception.InstanceFaultRollback(
10354                 exception.MigrationPreCheckError(reason=reason))
10355 
10356         # copy disks to destination
10357         # rename instance dir to +_resize at first for using
10358         # shared storage for instance dir (eg. NFS).
10359         inst_base = libvirt_utils.get_instance_path(instance)
10360         inst_base_resize = inst_base + "_resize"
10361         shared_instance_path = self._is_path_shared_with(dest, inst_base)
10362 
10363         # try to create the directory on the remote compute node
10364         # if this fails we pass the exception up the stack so we can catch
10365         # failures here earlier
10366         if not shared_instance_path:
10367             try:
10368                 self._remotefs.create_dir(dest, inst_base)
10369             except processutils.ProcessExecutionError as e:
10370                 reason = _("not able to execute ssh command: %s") % e
10371                 raise exception.InstanceFaultRollback(
10372                     exception.ResizeError(reason=reason))
10373 
10374         self.power_off(instance, timeout, retry_interval)
10375         self.unplug_vifs(instance, network_info)
10376         block_device_mapping = driver.block_device_info_get_mapping(
10377             block_device_info)
10378         for vol in block_device_mapping:
10379             connection_info = vol['connection_info']
10380             self._disconnect_volume(context, connection_info, instance)
10381 
10382         disk_info = self._get_instance_disk_info(instance, block_device_info)
10383 
10384         try:
10385             os.rename(inst_base, inst_base_resize)
10386             # if we are migrating the instance with shared instance path then
10387             # create the directory.  If it is a remote node the directory
10388             # has already been created
10389             if shared_instance_path:
10390                 dest = None
10391                 fileutils.ensure_tree(inst_base)
10392 
10393             on_execute = lambda process: \
10394                 self.job_tracker.add_job(instance, process.pid)
10395             on_completion = lambda process: \
10396                 self.job_tracker.remove_job(instance, process.pid)
10397 
10398             for info in disk_info:
10399                 # assume inst_base == dirname(info['path'])
10400                 img_path = info['path']
10401                 fname = os.path.basename(img_path)
10402                 from_path = os.path.join(inst_base_resize, fname)
10403 
10404                 # We will not copy over the swap disk here, and rely on
10405                 # finish_migration to re-create it for us. This is ok because
10406                 # the OS is shut down, and as recreating a swap disk is very
10407                 # cheap it is more efficient than copying either locally or
10408                 # over the network. This also means we don't have to resize it.
10409                 if fname == 'disk.swap':
10410                     continue
10411 
10412                 compression = info['type'] not in NO_COMPRESSION_TYPES
10413                 libvirt_utils.copy_image(from_path, img_path, host=dest,
10414                                          on_execute=on_execute,
10415                                          on_completion=on_completion,
10416                                          compression=compression)
10417 
10418             # Ensure disk.info is written to the new path to avoid disks being
10419             # reinspected and potentially changing format.
10420             src_disk_info_path = os.path.join(inst_base_resize, 'disk.info')
10421             if os.path.exists(src_disk_info_path):
10422                 dst_disk_info_path = os.path.join(inst_base, 'disk.info')
10423                 libvirt_utils.copy_image(src_disk_info_path,
10424                                          dst_disk_info_path,
10425                                          host=dest, on_execute=on_execute,
10426                                          on_completion=on_completion)
10427 
10428             # Handle migration of vTPM data if needed
10429             libvirt_utils.save_and_migrate_vtpm_dir(
10430                 instance.uuid, inst_base_resize, inst_base, dest,
10431                 on_execute, on_completion)
10432 
10433         except Exception:
10434             with excutils.save_and_reraise_exception():
10435                 self._cleanup_remote_migration(dest, inst_base,
10436                                                inst_base_resize,
10437                                                shared_instance_path)
10438 
10439         return jsonutils.dumps(disk_info)
10440 
10441     def _wait_for_running(self, instance):
10442         state = self.get_info(instance).state
10443 
10444         if state == power_state.RUNNING:
10445             LOG.info("Instance running successfully.", instance=instance)
10446             raise loopingcall.LoopingCallDone()
10447 
10448     @staticmethod
10449     def _disk_raw_to_qcow2(path):
10450         """Converts a raw disk to qcow2."""
10451         path_qcow = path + '_qcow'
10452         images.convert_image(path, path_qcow, 'raw', 'qcow2')
10453         os.rename(path_qcow, path)
10454 
10455     def _finish_migration_vtpm(
10456         self,
10457         context: nova_context.RequestContext,
10458         instance: 'objects.Instance',
10459     ) -> None:
10460         """Handle vTPM when migrating or resizing an instance.
10461 
10462         Handle the case where we're resizing between different versions of TPM,
10463         or enabling/disabling TPM.
10464         """
10465         old_vtpm_config = hardware.get_vtpm_constraint(
10466             instance.old_flavor, instance.image_meta)
10467         new_vtpm_config = hardware.get_vtpm_constraint(
10468             instance.new_flavor, instance.image_meta)
10469 
10470         if old_vtpm_config:
10471             # we had a vTPM in the old flavor; figure out if we need to do
10472             # anything with it
10473             inst_base = libvirt_utils.get_instance_path(instance)
10474             swtpm_dir = os.path.join(inst_base, 'swtpm', instance.uuid)
10475             copy_swtpm_dir = True
10476 
10477             if old_vtpm_config != new_vtpm_config:
10478                 # we had vTPM in the old flavor but the new flavor either
10479                 # doesn't or has different config; delete old TPM data and let
10480                 # libvirt create new data
10481                 if os.path.exists(swtpm_dir):
10482                     LOG.info(
10483                         'Old flavor and new flavor have different vTPM '
10484                         'configuration; removing existing vTPM data.')
10485                     copy_swtpm_dir = False
10486                     shutil.rmtree(swtpm_dir)
10487 
10488             # apparently shutil.rmtree() isn't reliable on NFS so don't rely
10489             # only on path existance here.
10490             if copy_swtpm_dir and os.path.exists(swtpm_dir):
10491                 libvirt_utils.restore_vtpm_dir(swtpm_dir)
10492         elif new_vtpm_config:
10493             # we've requested vTPM in the new flavor and didn't have one
10494             # previously so we need to create a new secret
10495             crypto.ensure_vtpm_secret(context, instance)
10496 
10497     def finish_migration(
10498         self,
10499         context: nova_context.RequestContext,
10500         migration: 'objects.Migration',
10501         instance: 'objects.Instance',
10502         disk_info: str,
10503         network_info: network_model.NetworkInfo,
10504         image_meta: 'objects.ImageMeta',
10505         resize_instance: bool,
10506         allocations: ty.Dict[str, ty.Any],
10507         block_device_info: ty.Optional[ty.Dict[str, ty.Any]] = None,
10508         power_on: bool = True,
10509     ) -> None:
10510         """Complete the migration process on the destination host."""
10511         LOG.debug("Starting finish_migration", instance=instance)
10512 
10513         block_disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
10514                                                   instance,
10515                                                   image_meta,
10516                                                   block_device_info)
10517         # assume _create_image does nothing if a target file exists.
10518         # NOTE: This has the intended side-effect of fetching a missing
10519         # backing file.
10520         self._create_image(context, instance, block_disk_info['mapping'],
10521                            block_device_info=block_device_info,
10522                            ignore_bdi_for_swap=True,
10523                            fallback_from_host=migration.source_compute)
10524 
10525         # Required by Quobyte CI
10526         self._ensure_console_log_for_instance(instance)
10527 
10528         gen_confdrive = functools.partial(
10529             self._create_configdrive, context, instance,
10530             InjectionInfo(admin_pass=None, network_info=network_info,
10531                           files=None))
10532 
10533         # Convert raw disks to qcow2 if migrating to host which uses
10534         # qcow2 from host which uses raw.
10535         for info in jsonutils.loads(disk_info):
10536             path = info['path']
10537             disk_name = os.path.basename(path)
10538 
10539             # NOTE(mdbooth): The code below looks wrong, but is actually
10540             # required to prevent a security hole when migrating from a host
10541             # with use_cow_images=False to one with use_cow_images=True.
10542             # Imagebackend uses use_cow_images to select between the
10543             # atrociously-named-Raw and Qcow2 backends. The Qcow2 backend
10544             # writes to disk.info, but does not read it as it assumes qcow2.
10545             # Therefore if we don't convert raw to qcow2 here, a raw disk will
10546             # be incorrectly assumed to be qcow2, which is a severe security
10547             # flaw. The reverse is not true, because the atrociously-named-Raw
10548             # backend supports both qcow2 and raw disks, and will choose
10549             # appropriately between them as long as disk.info exists and is
10550             # correctly populated, which it is because Qcow2 writes to
10551             # disk.info.
10552             #
10553             # In general, we do not yet support format conversion during
10554             # migration. For example:
10555             #   * Converting from use_cow_images=True to use_cow_images=False
10556             #     isn't handled. This isn't a security bug, but is almost
10557             #     certainly buggy in other cases, as the 'Raw' backend doesn't
10558             #     expect a backing file.
10559             #   * Converting to/from lvm and rbd backends is not supported.
10560             #
10561             # This behaviour is inconsistent, and therefore undesirable for
10562             # users. It is tightly-coupled to implementation quirks of 2
10563             # out of 5 backends in imagebackend and defends against a severe
10564             # security flaw which is not at all obvious without deep analysis,
10565             # and is therefore undesirable to developers. We should aim to
10566             # remove it. This will not be possible, though, until we can
10567             # represent the storage layout of a specific instance
10568             # independent of the default configuration of the local compute
10569             # host.
10570 
10571             # Config disks are hard-coded to be raw even when
10572             # use_cow_images=True (see _get_disk_config_image_type),so don't
10573             # need to be converted.
10574             if (disk_name != 'disk.config' and
10575                         info['type'] == 'raw' and CONF.use_cow_images):
10576                 self._disk_raw_to_qcow2(info['path'])
10577 
10578         # Does the guest need to be assigned some vGPU mediated devices ?
10579         mdevs = self._allocate_mdevs(allocations)
10580 
10581         # Handle the case where the guest has emulated TPM
10582         self._finish_migration_vtpm(context, instance)
10583 
10584         xml = self._get_guest_xml(context, instance, network_info,
10585                                   block_disk_info, image_meta,
10586                                   block_device_info=block_device_info,
10587                                   mdevs=mdevs)
10588         # NOTE(mriedem): vifs_already_plugged=True here, regardless of whether
10589         # or not we've migrated to another host, because we unplug VIFs locally
10590         # and the status change in the port might go undetected by the neutron
10591         # L2 agent (or neutron server) so neutron may not know that the VIF was
10592         # unplugged in the first place and never send an event.
10593         guest = self._create_guest_with_network(
10594             context, xml, instance, network_info, block_device_info,
10595             power_on=power_on, vifs_already_plugged=True,
10596             post_xml_callback=gen_confdrive)
10597         if power_on:
10598             timer = loopingcall.FixedIntervalLoopingCall(
10599                                                     self._wait_for_running,
10600                                                     instance)
10601             timer.start(interval=0.5).wait()
10602 
10603             # Sync guest time after migration.
10604             guest.sync_guest_time()
10605 
10606         LOG.debug("finish_migration finished successfully.", instance=instance)
10607 
10608     def _cleanup_failed_migration(self, inst_base):
10609         """Make sure that a failed migrate doesn't prevent us from rolling
10610         back in a revert.
10611         """
10612         try:
10613             shutil.rmtree(inst_base)
10614         except OSError as e:
10615             if e.errno != errno.ENOENT:
10616                 raise
10617 
10618     def _finish_revert_migration_vtpm(
10619         self,
10620         context: nova_context.RequestContext,
10621         instance: 'objects.Instance',
10622     ) -> None:
10623         """Handle vTPM differences when reverting a migration or resize.
10624 
10625         We should either restore any emulated vTPM persistent storage files or
10626         create new ones.
10627         """
10628         old_vtpm_config = hardware.get_vtpm_constraint(
10629             instance.old_flavor, instance.image_meta)
10630         new_vtpm_config = hardware.get_vtpm_constraint(
10631             instance.new_flavor, instance.image_meta)
10632 
10633         if old_vtpm_config:
10634             # the instance had a vTPM before resize and should have one again;
10635             # move the previously-saved vTPM data back to its proper location
10636             inst_base = libvirt_utils.get_instance_path(instance)
10637             swtpm_dir = os.path.join(inst_base, 'swtpm', instance.uuid)
10638             if os.path.exists(swtpm_dir):
10639                 libvirt_utils.restore_vtpm_dir(swtpm_dir)
10640         elif new_vtpm_config:
10641             # the instance gained a vTPM and must now lose it; delete the vTPM
10642             # secret, knowing that libvirt will take care of everything else on
10643             # the destination side
10644             crypto.delete_vtpm_secret(context, instance)
10645 
10646     def finish_revert_migration(
10647         self,
10648         context: nova.context.RequestContext,
10649         instance: 'objects.Instance',
10650         network_info: network_model.NetworkInfo,
10651         migration: 'objects.Migration',
10652         block_device_info: ty.Optional[ty.Dict[str, ty.Any]] = None,
10653         power_on: bool = True,
10654     ) -> None:
10655         """Finish the second half of reverting a resize on the source host."""
10656         LOG.debug('Starting finish_revert_migration', instance=instance)
10657 
10658         inst_base = libvirt_utils.get_instance_path(instance)
10659         inst_base_resize = inst_base + "_resize"
10660 
10661         # NOTE(danms): if we're recovering from a failed migration,
10662         # make sure we don't have a left-over same-host base directory
10663         # that would conflict. Also, don't fail on the rename if the
10664         # failure happened early.
10665         if os.path.exists(inst_base_resize):
10666             self._cleanup_failed_migration(inst_base)
10667             os.rename(inst_base_resize, inst_base)
10668 
10669         root_disk = self.image_backend.by_name(instance, 'disk')
10670         # Once we rollback, the snapshot is no longer needed, so remove it
10671         if root_disk.exists():
10672             root_disk.rollback_to_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME)
10673             root_disk.remove_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME)
10674 
10675         self._finish_revert_migration_vtpm(context, instance)
10676 
10677         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
10678                                             instance,
10679                                             instance.image_meta,
10680                                             block_device_info)
10681 
10682         # The guest could already have mediated devices, using them for
10683         # the new XML
10684         mdevs = list(self._get_all_assigned_mediated_devices(instance))
10685 
10686         xml = self._get_guest_xml(context, instance, network_info, disk_info,
10687                                   instance.image_meta,
10688                                   block_device_info=block_device_info,
10689                                   mdevs=mdevs)
10690         # NOTE(artom) In some Neutron or port configurations we've already
10691         # waited for vif-plugged events in the compute manager's
10692         # _finish_revert_resize_network_migrate_finish(), right after updating
10693         # the port binding. For any ports not covered by those "bind-time"
10694         # events, we wait for "plug-time" events here.
10695         events = network_info.get_plug_time_events(migration)
10696         if events:
10697             LOG.debug('Instance is using plug-time events: %s', events,
10698                       instance=instance)
10699         self._create_guest_with_network(
10700             context, xml, instance, network_info, block_device_info,
10701             power_on=power_on, external_events=events)
10702 
10703         if power_on:
10704             timer = loopingcall.FixedIntervalLoopingCall(
10705                                                     self._wait_for_running,
10706                                                     instance)
10707             timer.start(interval=0.5).wait()
10708 
10709         LOG.debug("finish_revert_migration finished successfully.",
10710                   instance=instance)
10711 
10712     def confirm_migration(self, context, migration, instance, network_info):
10713         """Confirms a resize, destroying the source VM."""
10714         self._cleanup_resize(context, instance, network_info)
10715 
10716     @staticmethod
10717     def _get_io_devices(xml_doc):
10718         """get the list of io devices from the xml document."""
10719         result: ty.Dict[str, ty.List[str]] = {"volumes": [], "ifaces": []}
10720         try:
10721             doc = etree.fromstring(xml_doc)
10722         except Exception:
10723             return result
10724         blocks = [('./devices/disk', 'volumes'),
10725             ('./devices/interface', 'ifaces')]
10726         for block, key in blocks:
10727             section = doc.findall(block)
10728             for node in section:
10729                 for child in node:
10730                     if child.tag == 'target' and child.get('dev'):
10731                         result[key].append(child.get('dev'))
10732         return result
10733 
10734     def get_diagnostics(self, instance):
10735         guest = self._host.get_guest(instance)
10736 
10737         # TODO(sahid): We are converting all calls from a
10738         # virDomain object to use nova.virt.libvirt.Guest.
10739         # We should be able to remove domain at the end.
10740         domain = guest._domain
10741         output = {}
10742         # get cpu time, might launch an exception if the method
10743         # is not supported by the underlying hypervisor being
10744         # used by libvirt
10745         try:
10746             for vcpu in guest.get_vcpus_info():
10747                 output["cpu" + str(vcpu.id) + "_time"] = vcpu.time
10748         except libvirt.libvirtError:
10749             pass
10750         # get io status
10751         xml = guest.get_xml_desc()
10752         dom_io = LibvirtDriver._get_io_devices(xml)
10753         for guest_disk in dom_io["volumes"]:
10754             try:
10755                 # blockStats might launch an exception if the method
10756                 # is not supported by the underlying hypervisor being
10757                 # used by libvirt
10758                 stats = domain.blockStats(guest_disk)
10759                 output[guest_disk + "_read_req"] = stats[0]
10760                 output[guest_disk + "_read"] = stats[1]
10761                 output[guest_disk + "_write_req"] = stats[2]
10762                 output[guest_disk + "_write"] = stats[3]
10763                 output[guest_disk + "_errors"] = stats[4]
10764             except libvirt.libvirtError:
10765                 pass
10766         for interface in dom_io["ifaces"]:
10767             try:
10768                 # interfaceStats might launch an exception if the method
10769                 # is not supported by the underlying hypervisor being
10770                 # used by libvirt
10771                 stats = domain.interfaceStats(interface)
10772                 output[interface + "_rx"] = stats[0]
10773                 output[interface + "_rx_packets"] = stats[1]
10774                 output[interface + "_rx_errors"] = stats[2]
10775                 output[interface + "_rx_drop"] = stats[3]
10776                 output[interface + "_tx"] = stats[4]
10777                 output[interface + "_tx_packets"] = stats[5]
10778                 output[interface + "_tx_errors"] = stats[6]
10779                 output[interface + "_tx_drop"] = stats[7]
10780             except libvirt.libvirtError:
10781                 pass
10782         output["memory"] = domain.maxMemory()
10783         # memoryStats might launch an exception if the method
10784         # is not supported by the underlying hypervisor being
10785         # used by libvirt
10786         try:
10787             mem = domain.memoryStats()
10788             for key in mem.keys():
10789                 output["memory-" + key] = mem[key]
10790         except (libvirt.libvirtError, AttributeError):
10791             pass
10792         return output
10793 
10794     def get_instance_diagnostics(self, instance):
10795         guest = self._host.get_guest(instance)
10796 
10797         # TODO(sahid): We are converting all calls from a
10798         # virDomain object to use nova.virt.libvirt.Guest.
10799         # We should be able to remove domain at the end.
10800         domain = guest._domain
10801 
10802         xml = guest.get_xml_desc()
10803         xml_doc = etree.fromstring(xml)
10804 
10805         # TODO(sahid): Needs to use get_info but more changes have to
10806         # be done since a mapping STATE_MAP LIBVIRT_POWER_STATE is
10807         # needed.
10808         state, max_mem, mem, num_cpu, cpu_time = guest._get_domain_info()
10809         config_drive = configdrive.required_by(instance)
10810         launched_at = timeutils.normalize_time(instance.launched_at)
10811         uptime = timeutils.delta_seconds(launched_at,
10812                                          timeutils.utcnow())
10813         diags = diagnostics_obj.Diagnostics(state=power_state.STATE_MAP[state],
10814                                         driver='libvirt',
10815                                         config_drive=config_drive,
10816                                         hypervisor=CONF.libvirt.virt_type,
10817                                         hypervisor_os='linux',
10818                                         uptime=uptime)
10819         diags.memory_details = diagnostics_obj.MemoryDiagnostics(
10820             maximum=max_mem / units.Mi,
10821             used=mem / units.Mi)
10822 
10823         # get cpu time, might launch an exception if the method
10824         # is not supported by the underlying hypervisor being
10825         # used by libvirt
10826         try:
10827             for vcpu in guest.get_vcpus_info():
10828                 diags.add_cpu(id=vcpu.id, time=vcpu.time)
10829         except libvirt.libvirtError:
10830             pass
10831         # get io status
10832         dom_io = LibvirtDriver._get_io_devices(xml)
10833         for guest_disk in dom_io["volumes"]:
10834             try:
10835                 # blockStats might launch an exception if the method
10836                 # is not supported by the underlying hypervisor being
10837                 # used by libvirt
10838                 stats = domain.blockStats(guest_disk)
10839                 diags.add_disk(read_bytes=stats[1],
10840                                read_requests=stats[0],
10841                                write_bytes=stats[3],
10842                                write_requests=stats[2],
10843                                errors_count=stats[4])
10844             except libvirt.libvirtError:
10845                 pass
10846 
10847         for interface in xml_doc.findall('./devices/interface'):
10848             mac_address = interface.find('mac').get('address')
10849             target = interface.find('./target')
10850 
10851             # add nic that has no target (therefore no stats)
10852             if target is None:
10853                 diags.add_nic(mac_address=mac_address)
10854                 continue
10855 
10856             # add nic with stats
10857             dev = target.get('dev')
10858             try:
10859                 if dev:
10860                     # interfaceStats might launch an exception if the
10861                     # method is not supported by the underlying hypervisor
10862                     # being used by libvirt
10863                     stats = domain.interfaceStats(dev)
10864                     diags.add_nic(mac_address=mac_address,
10865                                   rx_octets=stats[0],
10866                                   rx_errors=stats[2],
10867                                   rx_drop=stats[3],
10868                                   rx_packets=stats[1],
10869                                   tx_octets=stats[4],
10870                                   tx_errors=stats[6],
10871                                   tx_drop=stats[7],
10872                                   tx_packets=stats[5])
10873 
10874             except libvirt.libvirtError:
10875                 pass
10876 
10877         return diags
10878 
10879     @staticmethod
10880     def _prepare_device_bus(dev):
10881         """Determines the device bus and its hypervisor assigned address
10882         """
10883         bus = None
10884         address = (dev.device_addr.format_address() if
10885                    dev.device_addr else None)
10886         if isinstance(dev.device_addr,
10887                       vconfig.LibvirtConfigGuestDeviceAddressPCI):
10888             bus = objects.PCIDeviceBus()
10889         elif isinstance(dev, vconfig.LibvirtConfigGuestDisk):
10890             if dev.target_bus == 'scsi':
10891                 bus = objects.SCSIDeviceBus()
10892             elif dev.target_bus == 'ide':
10893                 bus = objects.IDEDeviceBus()
10894             elif dev.target_bus == 'usb':
10895                 bus = objects.USBDeviceBus()
10896         if address is not None and bus is not None:
10897             bus.address = address
10898         return bus
10899 
10900     def _build_interface_metadata(self, dev, vifs_to_expose, vlans_by_mac,
10901                                   trusted_by_mac):
10902         """Builds a metadata object for a network interface
10903 
10904         :param dev: The LibvirtConfigGuestInterface to build metadata for.
10905         :param vifs_to_expose: The list of tagged and/or vlan'ed
10906                                VirtualInterface objects.
10907         :param vlans_by_mac: A dictionary of mac address -> vlan associations.
10908         :param trusted_by_mac: A dictionary of mac address -> vf_trusted
10909                                associations.
10910         :return: A NetworkInterfaceMetadata object, or None.
10911         """
10912         vif = vifs_to_expose.get(dev.mac_addr)
10913         if not vif:
10914             LOG.debug('No VIF found with MAC %s, not building metadata',
10915                       dev.mac_addr)
10916             return None
10917         bus = self._prepare_device_bus(dev)
10918         device = objects.NetworkInterfaceMetadata(mac=vif.address)
10919         if 'tag' in vif and vif.tag:
10920             device.tags = [vif.tag]
10921         if bus:
10922             device.bus = bus
10923         vlan = vlans_by_mac.get(vif.address)
10924         if vlan:
10925             device.vlan = int(vlan)
10926         device.vf_trusted = trusted_by_mac.get(vif.address, False)
10927         return device
10928 
10929     def _build_disk_metadata(self, dev, tagged_bdms):
10930         """Builds a metadata object for a disk
10931 
10932         :param dev: The vconfig.LibvirtConfigGuestDisk to build metadata for.
10933         :param tagged_bdms: The list of tagged BlockDeviceMapping objects.
10934         :return: A DiskMetadata object, or None.
10935         """
10936         bdm = tagged_bdms.get(dev.target_dev)
10937         if not bdm:
10938             LOG.debug('No BDM found with device name %s, not building '
10939                       'metadata.', dev.target_dev)
10940             return None
10941         bus = self._prepare_device_bus(dev)
10942         device = objects.DiskMetadata(tags=[bdm.tag])
10943         # NOTE(artom) Setting the serial (which corresponds to
10944         # volume_id in BlockDeviceMapping) in DiskMetadata allows us to
10945         # find the disks's BlockDeviceMapping object when we detach the
10946         # volume and want to clean up its metadata.
10947         device.serial = bdm.volume_id
10948         if bus:
10949             device.bus = bus
10950         return device
10951 
10952     def _build_hostdev_metadata(self, dev, vifs_to_expose, vlans_by_mac):
10953         """Builds a metadata object for a hostdev. This can only be a PF, so we
10954         don't need trusted_by_mac like in _build_interface_metadata because
10955         only VFs can be trusted.
10956 
10957         :param dev: The LibvirtConfigGuestHostdevPCI to build metadata for.
10958         :param vifs_to_expose: The list of tagged and/or vlan'ed
10959                                VirtualInterface objects.
10960         :param vlans_by_mac: A dictionary of mac address -> vlan associations.
10961         :return: A NetworkInterfaceMetadata object, or None.
10962         """
10963         # Strip out the leading '0x'
10964         pci_address = pci_utils.get_pci_address(
10965             *[x[2:] for x in (dev.domain, dev.bus, dev.slot, dev.function)])
10966         try:
10967             mac = pci_utils.get_mac_by_pci_address(pci_address,
10968                                                    pf_interface=True)
10969         except exception.PciDeviceNotFoundById:
10970             LOG.debug('Not exposing metadata for not found PCI device %s',
10971                       pci_address)
10972             return None
10973 
10974         vif = vifs_to_expose.get(mac)
10975         if not vif:
10976             LOG.debug('No VIF found with MAC %s, not building metadata', mac)
10977             return None
10978 
10979         device = objects.NetworkInterfaceMetadata(mac=mac)
10980         device.bus = objects.PCIDeviceBus(address=pci_address)
10981         if 'tag' in vif and vif.tag:
10982             device.tags = [vif.tag]
10983         vlan = vlans_by_mac.get(mac)
10984         if vlan:
10985             device.vlan = int(vlan)
10986         return device
10987 
10988     def _build_device_metadata(self, context, instance):
10989         """Builds a metadata object for instance devices, that maps the user
10990            provided tag to the hypervisor assigned device address.
10991         """
10992         def _get_device_name(bdm):
10993             return block_device.strip_dev(bdm.device_name)
10994 
10995         network_info = instance.info_cache.network_info
10996         vlans_by_mac = netutils.get_cached_vifs_with_vlan(network_info)
10997         trusted_by_mac = netutils.get_cached_vifs_with_trusted(network_info)
10998         vifs = objects.VirtualInterfaceList.get_by_instance_uuid(context,
10999                                                                  instance.uuid)
11000         vifs_to_expose = {vif.address: vif for vif in vifs
11001                           if ('tag' in vif and vif.tag) or
11002                              vlans_by_mac.get(vif.address)}
11003         # TODO(mriedem): We should be able to avoid the DB query here by using
11004         # block_device_info['block_device_mapping'] which is passed into most
11005         # methods that call this function.
11006         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
11007             context, instance.uuid)
11008         tagged_bdms = {_get_device_name(bdm): bdm for bdm in bdms if bdm.tag}
11009 
11010         devices = []
11011         guest = self._host.get_guest(instance)
11012         xml = guest.get_xml_desc()
11013         xml_dom = etree.fromstring(xml)
11014         guest_config = vconfig.LibvirtConfigGuest()
11015         guest_config.parse_dom(xml_dom)
11016 
11017         for dev in guest_config.devices:
11018             device = None
11019             if isinstance(dev, vconfig.LibvirtConfigGuestInterface):
11020                 device = self._build_interface_metadata(dev, vifs_to_expose,
11021                                                         vlans_by_mac,
11022                                                         trusted_by_mac)
11023             if isinstance(dev, vconfig.LibvirtConfigGuestDisk):
11024                 device = self._build_disk_metadata(dev, tagged_bdms)
11025             if isinstance(dev, vconfig.LibvirtConfigGuestHostdevPCI):
11026                 device = self._build_hostdev_metadata(dev, vifs_to_expose,
11027                                                       vlans_by_mac)
11028             if device:
11029                 devices.append(device)
11030         if devices:
11031             dev_meta = objects.InstanceDeviceMetadata(devices=devices)
11032             return dev_meta
11033 
11034     def instance_on_disk(self, instance):
11035         # ensure directories exist and are writable
11036         instance_path = libvirt_utils.get_instance_path(instance)
11037         LOG.debug('Checking instance files accessibility %s', instance_path,
11038                   instance=instance)
11039         shared_instance_path = os.access(instance_path, os.W_OK)
11040         # NOTE(flwang): For shared block storage scenario, the file system is
11041         # not really shared by the two hosts, but the volume of evacuated
11042         # instance is reachable.
11043         shared_block_storage = (self.image_backend.backend().
11044                                 is_shared_block_storage())
11045         return shared_instance_path or shared_block_storage
11046 
11047     def inject_network_info(self, instance, nw_info):
11048         pass
11049 
11050     def delete_instance_files(self, instance):
11051         target = libvirt_utils.get_instance_path(instance)
11052         # A resize may be in progress
11053         target_resize = target + '_resize'
11054         # Other threads may attempt to rename the path, so renaming the path
11055         # to target + '_del' (because it is atomic) and iterating through
11056         # twice in the unlikely event that a concurrent rename occurs between
11057         # the two rename attempts in this method. In general this method
11058         # should be fairly thread-safe without these additional checks, since
11059         # other operations involving renames are not permitted when the task
11060         # state is not None and the task state should be set to something
11061         # other than None by the time this method is invoked.
11062         target_del = target + '_del'
11063         for i in range(2):
11064             try:
11065                 os.rename(target, target_del)
11066                 break
11067             except Exception:
11068                 pass
11069             try:
11070                 os.rename(target_resize, target_del)
11071                 break
11072             except Exception:
11073                 pass
11074         # Either the target or target_resize path may still exist if all
11075         # rename attempts failed.
11076         remaining_path = None
11077         for p in (target, target_resize):
11078             if os.path.exists(p):
11079                 remaining_path = p
11080                 break
11081 
11082         # A previous delete attempt may have been interrupted, so target_del
11083         # may exist even if all rename attempts during the present method
11084         # invocation failed due to the absence of both target and
11085         # target_resize.
11086         if not remaining_path and os.path.exists(target_del):
11087             self.job_tracker.terminate_jobs(instance)
11088 
11089             LOG.info('Deleting instance files %s', target_del,
11090                      instance=instance)
11091             remaining_path = target_del
11092             try:
11093                 shutil.rmtree(target_del)
11094             except OSError as e:
11095                 LOG.error('Failed to cleanup directory %(target)s: %(e)s',
11096                           {'target': target_del, 'e': e}, instance=instance)
11097 
11098         # It is possible that the delete failed, if so don't mark the instance
11099         # as cleaned.
11100         if remaining_path and os.path.exists(remaining_path):
11101             LOG.info('Deletion of %s failed', remaining_path,
11102                      instance=instance)
11103             return False
11104 
11105         LOG.info('Deletion of %s complete', target_del, instance=instance)
11106         return True
11107 
11108     @property
11109     def need_legacy_block_device_info(self):
11110         return False
11111 
11112     def default_root_device_name(self, instance, image_meta, root_bdm):
11113         disk_bus = blockinfo.get_disk_bus_for_device_type(
11114             instance, CONF.libvirt.virt_type, image_meta, "disk")
11115         cdrom_bus = blockinfo.get_disk_bus_for_device_type(
11116             instance, CONF.libvirt.virt_type, image_meta, "cdrom")
11117         root_info = blockinfo.get_root_info(
11118             instance, CONF.libvirt.virt_type, image_meta,
11119             root_bdm, disk_bus, cdrom_bus)
11120         return block_device.prepend_dev(root_info['dev'])
11121 
11122     def default_device_names_for_instance(self, instance, root_device_name,
11123                                           *block_device_lists):
11124         block_device_mapping = list(itertools.chain(*block_device_lists))
11125         # NOTE(ndipanov): Null out the device names so that blockinfo code
11126         #                 will assign them
11127         for bdm in block_device_mapping:
11128             if bdm.device_name is not None:
11129                 LOG.info(
11130                     "Ignoring supplied device name: %(device_name)s. "
11131                     "Libvirt can't honour user-supplied dev names",
11132                     {'device_name': bdm.device_name}, instance=instance)
11133                 bdm.device_name = None
11134         block_device_info = driver.get_block_device_info(instance,
11135                                                          block_device_mapping)
11136 
11137         blockinfo.default_device_names(CONF.libvirt.virt_type,
11138                                        nova_context.get_admin_context(),
11139                                        instance,
11140                                        block_device_info,
11141                                        instance.image_meta)
11142 
11143     def get_device_name_for_instance(self, instance, bdms, block_device_obj):
11144         block_device_info = driver.get_block_device_info(instance, bdms)
11145         instance_info = blockinfo.get_disk_info(
11146                 CONF.libvirt.virt_type, instance,
11147                 instance.image_meta, block_device_info=block_device_info)
11148 
11149         suggested_dev_name = block_device_obj.device_name
11150         if suggested_dev_name is not None:
11151             LOG.info(
11152                 'Ignoring supplied device name: %(suggested_dev)s',
11153                 {'suggested_dev': suggested_dev_name}, instance=instance)
11154 
11155         # NOTE(ndipanov): get_info_from_bdm will generate the new device name
11156         #                 only when it's actually not set on the bd object
11157         block_device_obj.device_name = None
11158         disk_info = blockinfo.get_info_from_bdm(
11159             instance, CONF.libvirt.virt_type, instance.image_meta,
11160             block_device_obj, mapping=instance_info['mapping'])
11161         return block_device.prepend_dev(disk_info['dev'])
11162 
11163     def is_supported_fs_format(self, fs_type):
11164         return fs_type in [nova.privsep.fs.FS_FORMAT_EXT2,
11165                            nova.privsep.fs.FS_FORMAT_EXT3,
11166                            nova.privsep.fs.FS_FORMAT_EXT4,
11167                            nova.privsep.fs.FS_FORMAT_XFS]
11168 
11169     def _get_tpm_traits(self) -> ty.Dict[str, bool]:
11170         # Assert or deassert TPM support traits
11171         return {
11172             ot.COMPUTE_SECURITY_TPM_2_0: CONF.libvirt.swtpm_enabled,
11173             ot.COMPUTE_SECURITY_TPM_1_2: CONF.libvirt.swtpm_enabled,
11174         }
11175 
11176     def _get_vif_model_traits(self) -> ty.Dict[str, bool]:
11177         """Get vif model traits based on the currently enabled virt_type.
11178 
11179         Not all traits generated by this function may be valid and the result
11180         should be validated.
11181 
11182         :return: A dict of trait names mapped to boolean values.
11183         """
11184         all_models = set(itertools.chain(
11185             *libvirt_vif.SUPPORTED_VIF_MODELS.values()
11186         ))
11187         supported_models = libvirt_vif.SUPPORTED_VIF_MODELS.get(
11188             CONF.libvirt.virt_type, []
11189         )
11190         # construct the corresponding standard trait from the VIF model name
11191         return {
11192             f'COMPUTE_NET_VIF_MODEL_{model.replace("-", "_").upper()}': model
11193             in supported_models for model in all_models
11194         }
11195 
11196     def _get_storage_bus_traits(self) -> ty.Dict[str, bool]:
11197         """Get storage bus traits based on the currently enabled virt_type.
11198 
11199         For QEMU and KVM this function uses the information returned by the
11200         libvirt domain capabilities API. For other virt types we generate the
11201         traits based on the static information in the blockinfo module.
11202 
11203         Not all traits generated by this function may be valid and the result
11204         should be validated.
11205 
11206         :return: A dict of trait names mapped to boolean values.
11207         """
11208         all_buses = set(itertools.chain(
11209             *blockinfo.SUPPORTED_DEVICE_BUSES.values()
11210         ))
11211 
11212         if CONF.libvirt.virt_type in ('qemu', 'kvm'):
11213             dom_caps = self._host.get_domain_capabilities()
11214             supported_buses: ty.Set[str] = set()
11215             for arch_type in dom_caps:
11216                 for machine_type in dom_caps[arch_type]:
11217                     supported_buses.update(
11218                         dom_caps[arch_type][machine_type].devices.disk.buses
11219                     )
11220         else:
11221             supported_buses = blockinfo.SUPPORTED_DEVICE_BUSES.get(
11222                 CONF.libvirt.virt_type, []
11223             )
11224 
11225         # construct the corresponding standard trait from the storage bus name
11226         return {
11227             f'COMPUTE_STORAGE_BUS_{bus.replace("-", "_").upper()}': bus in
11228             supported_buses for bus in all_buses
11229         }
11230 
11231     def _get_video_model_traits(self) -> ty.Dict[str, bool]:
11232         """Get video model traits from libvirt.
11233 
11234         Not all traits generated by this function may be valid and the result
11235         should be validated.
11236 
11237         :return: A dict of trait names mapped to boolean values.
11238         """
11239         all_models = fields.VideoModel.ALL
11240 
11241         dom_caps = self._host.get_domain_capabilities()
11242         supported_models: ty.Set[str] = set()
11243         for arch_type in dom_caps:
11244             for machine_type in dom_caps[arch_type]:
11245                 supported_models.update(
11246                     dom_caps[arch_type][machine_type].devices.video.models
11247                 )
11248 
11249         # construct the corresponding standard trait from the video model name
11250         return {
11251             f'COMPUTE_GRAPHICS_MODEL_{model.replace("-", "_").upper()}': model
11252             in supported_models for model in all_models
11253         }
11254 
11255     def _get_cpu_traits(self) -> ty.Dict[str, bool]:
11256         """Get CPU-related traits to be set and unset on the host's resource
11257         provider.
11258 
11259         :return: A dict of trait names mapped to boolean values.
11260         """
11261         traits = self._get_cpu_feature_traits()
11262         traits[ot.HW_CPU_X86_AMD_SEV] = self._host.supports_amd_sev
11263         traits[ot.HW_CPU_HYPERTHREADING] = self._host.has_hyperthreading
11264 
11265         return traits
11266 
11267     def _get_cpu_feature_traits(self) -> ty.Dict[str, bool]:
11268         """Get CPU traits of VMs based on guest CPU model config.
11269 
11270         1. If mode is 'host-model' or 'host-passthrough', use host's
11271            CPU features.
11272         2. If mode is None, choose a default CPU model based on CPU
11273            architecture.
11274         3. If mode is 'custom', use cpu_models to generate CPU features.
11275 
11276         The code also accounts for cpu_model_extra_flags configuration when
11277         cpu_mode is 'host-model', 'host-passthrough' or 'custom', this
11278         ensures user specified CPU feature flags to be included.
11279 
11280         :return: A dict of trait names mapped to boolean values.
11281         """
11282         cpu = self._get_guest_cpu_model_config()
11283         if not cpu:
11284             LOG.info('The current libvirt hypervisor %(virt_type)s '
11285                      'does not support reporting CPU traits.',
11286                      {'virt_type': CONF.libvirt.virt_type})
11287             return {}
11288 
11289         caps = deepcopy(self._host.get_capabilities())
11290         if cpu.mode in ('host-model', 'host-passthrough'):
11291             # Account for features in cpu_model_extra_flags conf
11292             host_features: ty.Set[str] = {
11293                 f.name for f in caps.host.cpu.features | cpu.features
11294             }
11295             return libvirt_utils.cpu_features_to_traits(host_features)
11296 
11297         def _resolve_features(cpu):
11298             xml_str = cpu.to_xml()
11299             features_xml = self._get_guest_baseline_cpu_features(xml_str)
11300             feature_names = []
11301             if features_xml:
11302                 cpu = vconfig.LibvirtConfigCPU()
11303                 cpu.parse_str(features_xml)
11304                 feature_names = [f.name for f in cpu.features]
11305             return feature_names
11306 
11307         features: ty.Set[str] = set()
11308         # Choose a default CPU model when cpu_mode is not specified
11309         if cpu.mode is None:
11310             caps.host.cpu.model = libvirt_utils.get_cpu_model_from_arch(
11311                 caps.host.cpu.arch)
11312             caps.host.cpu.features = set()
11313             features = features.union(_resolve_features(caps.host.cpu))
11314         else:
11315             models = [self._get_cpu_model_mapping(model)
11316                       for model in CONF.libvirt.cpu_models]
11317 
11318             # Aarch64 platform doesn't return the default CPU models
11319             if caps.host.cpu.arch == fields.Architecture.AARCH64:
11320                 if not models:
11321                     models = ['max']
11322             # For custom mode, iterate through cpu models
11323             for model in models:
11324                 caps.host.cpu.model = model
11325                 caps.host.cpu.features = set()
11326                 features = features.union(_resolve_features(caps.host.cpu))
11327             # Account for features in cpu_model_extra_flags conf
11328             features = features.union([f.name for f in cpu.features])
11329 
11330         return libvirt_utils.cpu_features_to_traits(features)
11331 
11332     def _get_guest_baseline_cpu_features(self, xml_str):
11333         """Calls libvirt's baselineCPU API to compute the biggest set of
11334         CPU features which is compatible with the given host CPU.
11335 
11336         :param xml_str: XML description of host CPU
11337         :return: An XML string of the computed CPU, or None on error
11338         """
11339         LOG.debug("Libvirt baseline CPU %s", xml_str)
11340         # TODO(lei-zh): baselineCPU is not supported on all platforms.
11341         # There is some work going on in the libvirt community to replace the
11342         # baseline call. Consider using the new apis when they are ready. See
11343         # https://www.redhat.com/archives/libvir-list/2018-May/msg01204.html.
11344         try:
11345             if hasattr(libvirt, 'VIR_CONNECT_BASELINE_CPU_EXPAND_FEATURES'):
11346                 return self._host.get_connection().baselineCPU(
11347                     [xml_str],
11348                     libvirt.VIR_CONNECT_BASELINE_CPU_EXPAND_FEATURES)
11349             else:
11350                 return self._host.get_connection().baselineCPU([xml_str])
11351         except libvirt.libvirtError as ex:
11352             with excutils.save_and_reraise_exception() as ctxt:
11353                 error_code = ex.get_error_code()
11354                 if error_code == libvirt.VIR_ERR_NO_SUPPORT:
11355                     ctxt.reraise = False
11356                     LOG.debug('URI %(uri)s does not support full set'
11357                               ' of host capabilities: %(error)s',
11358                               {'uri': self._host._uri, 'error': ex})
11359                     return None
