Based on the given code from a commit, please generate supplementary code files according to the commit message.
####commit message
Transform instance.resize.error notifications

The instance.resize.error notification
is transformed to the versioned framework.

Change-Id: Ieb4ae4605fee8fbf58de4c5efb3c00083b4bd62c
Implements: bp versioned-notification-transformation-queens

####code 
1 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
2 #    not use this file except in compliance with the License. You may obtain
3 #    a copy of the License at
4 #
5 #         http://www.apache.org/licenses/LICENSE-2.0
6 #
7 #    Unless required by applicable law or agreed to in writing, software
8 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
9 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
10 #    License for the specific language governing permissions and limitations
11 #    under the License.
12 
13 import time
14 
15 import mock
16 
17 from nova import context
18 from nova import exception
19 from nova.tests import fixtures
20 from nova.tests.functional.notification_sample_tests \
21     import notification_sample_base
22 from nova.tests.unit import fake_notifier
23 
24 
25 class TestInstanceNotificationSampleWithMultipleCompute(
26         notification_sample_base.NotificationSampleTestBase):
27 
28     def setUp(self):
29         self.flags(use_neutron=True)
30         self.flags(bdms_in_notifications='True', group='notifications')
31         super(TestInstanceNotificationSampleWithMultipleCompute, self).setUp()
32         self.neutron = fixtures.NeutronFixture(self)
33         self.useFixture(self.neutron)
34         self.cinder = fixtures.CinderFixture(self)
35         self.useFixture(self.cinder)
36 
37     def test_live_migration_actions(self):
38         server = self._boot_a_server(
39             extra_params={'networks': [{'port': self.neutron.port_1['id']}]})
40         self._wait_for_notification('instance.create.end')
41         self._attach_volume_to_server(server, self.cinder.SWAP_OLD_VOL)
42         # server will boot on host1
43         self.useFixture(fixtures.ConfPatcher(host='host2'))
44         self.compute2 = self.start_service('compute', host='host2')
45 
46         actions = [
47             self._test_live_migration_rollback,
48         ]
49 
50         for action in actions:
51             fake_notifier.reset()
52             action(server)
53             # Ensure that instance is in active state after an action
54             self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
55 
56     @mock.patch('nova.compute.rpcapi.ComputeAPI.pre_live_migration',
57                 side_effect=exception.LiveMigrationWithOldNovaNotSupported())
58     def _test_live_migration_rollback(self, server, mock_migration):
59         post = {
60             'os-migrateLive': {
61                 'host': 'host2',
62                 'block_migration': True,
63                 'force': True,
64             }
65         }
66         self.admin_api.post_server_action(server['id'], post)
67         self._wait_for_notification('instance.live_migration_rollback.start')
68         self._wait_for_notification('instance.live_migration_rollback.end')
69 
70         self.assertEqual(2, len(fake_notifier.VERSIONED_NOTIFICATIONS))
71         self._verify_notification(
72             'instance-live_migration_rollback-start',
73             replacements={
74                 'reservation_id': server['reservation_id'],
75                 'uuid': server['id']},
76             actual=fake_notifier.VERSIONED_NOTIFICATIONS[0])
77         self._verify_notification(
78             'instance-live_migration_rollback-end',
79             replacements={
80                 'reservation_id': server['reservation_id'],
81                 'uuid': server['id']},
82             actual=fake_notifier.VERSIONED_NOTIFICATIONS[1])
83 
84 
85 class TestInstanceNotificationSample(
86         notification_sample_base.NotificationSampleTestBase):
87 
88     def setUp(self):
89         self.flags(use_neutron=True)
90         self.flags(bdms_in_notifications='True', group='notifications')
91         super(TestInstanceNotificationSample, self).setUp()
92         self.neutron = fixtures.NeutronFixture(self)
93         self.useFixture(self.neutron)
94         self.cinder = fixtures.CinderFixture(self)
95         self.useFixture(self.cinder)
96 
97     def _wait_until_swap_volume(self, server, volume_id):
98         for i in range(50):
99             volume_attachments = self.api.get_server_volumes(server['id'])
100             if len(volume_attachments) > 0:
101                 for volume_attachment in volume_attachments:
102                     if volume_attachment['volumeId'] == volume_id:
103                         return
104             time.sleep(0.5)
105         self.fail('Volume swap operation failed.')
106 
107     def _wait_until_swap_volume_error(self):
108         for i in range(50):
109             if self.cinder.swap_error:
110                 return
111             time.sleep(0.5)
112         self.fail("Timed out waiting for volume swap error to occur.")
113 
114     def test_instance_action(self):
115         # A single test case is used to test most of the instance action
116         # notifications to avoid booting up an instance for every action
117         # separately.
118         # Every instance action test function shall make sure that after the
119         # function the instance is in active state and usable by other actions.
120         # Therefore some action especially delete cannot be used here as
121         # recovering from that action would mean to recreate the instance and
122         # that would go against the whole purpose of this optimization
123 
124         server = self._boot_a_server(
125             extra_params={'networks': [{'port': self.neutron.port_1['id']}]})
126 
127         self._attach_volume_to_server(server, self.cinder.SWAP_OLD_VOL)
128 
129         actions = [
130             self._test_power_off_on_server,
131             self._test_restore_server,
132             self._test_suspend_resume_server,
133             self._test_pause_unpause_server,
134             self._test_shelve_server,
135             self._test_shelve_offload_server,
136             self._test_unshelve_server,
137             self._test_resize_server,
138             self._test_revert_server,
139             self._test_resize_confirm_server,
140             self._test_snapshot_server,
141             self._test_reboot_server,
142             self._test_reboot_server_error,
143             self._test_trigger_crash_dump,
144             self._test_volume_detach_attach_server,
145             self._test_rescue_server,
146             self._test_unrescue_server,
147             self._test_soft_delete_server,
148             self._test_attach_volume_error,
149         ]
150 
151         for action in actions:
152             fake_notifier.reset()
153             action(server)
154             # Ensure that instance is in active state after an action
155             self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
156 
157     def test_create_delete_server(self):
158         server = self._boot_a_server(
159             extra_params={'networks': [{'port': self.neutron.port_1['id']}],
160                           'tags': ['tag']})
161         self._attach_volume_to_server(server, self.cinder.SWAP_OLD_VOL)
162         self.api.delete_server(server['id'])
163         self._wait_until_deleted(server)
164         # NOTE(gibi): The wait_unit_deleted() call polls the REST API to see if
165         # the instance is disappeared however the _delete_instance() in
166         # compute/manager destroys the instance first then send the
167         # instance.delete.end notification. So to avoid race condition the test
168         # needs to wait for the notification as well here.
169         self._wait_for_notification('instance.delete.end')
170         self.assertEqual(9, len(fake_notifier.VERSIONED_NOTIFICATIONS),
171                          fake_notifier.VERSIONED_NOTIFICATIONS)
172 
173         # This list needs to be in order.
174         expected_notifications = [
175             'instance-create-start',
176             'instance-create-end',
177             'instance-update-tags-action',
178             'instance-volume_attach-start',
179             'instance-volume_attach-end',
180             'instance-delete-start',
181             'instance-shutdown-start',
182             'instance-shutdown-end',
183             'instance-delete-end'
184         ]
185         for idx, notification in enumerate(expected_notifications):
186             self._verify_notification(
187                 notification,
188                 replacements={
189                     'reservation_id': server['reservation_id'],
190                     'uuid': server['id']},
191                 actual=fake_notifier.VERSIONED_NOTIFICATIONS[idx])
192 
193     @mock.patch('nova.compute.manager.ComputeManager._build_resources')
194     def test_create_server_error(self, mock_build):
195         def _build_resources(*args, **kwargs):
196             raise exception.FlavorDiskTooSmall()
197 
198         mock_build.side_effect = _build_resources
199 
200         server = self._boot_a_server(
201             expected_status='ERROR',
202             extra_params={'networks': [{'port': self.neutron.port_1['id']}],
203                           'tags': ['tag']})
204 
205         self.assertEqual(2, len(fake_notifier.VERSIONED_NOTIFICATIONS))
206 
207         self._verify_notification(
208             'instance-create-start',
209             replacements={
210                 'reservation_id': server['reservation_id'],
211                 'uuid': server['id']},
212             actual=fake_notifier.VERSIONED_NOTIFICATIONS[0])
213         self._verify_notification(
214             'instance-create-error',
215             replacements={
216                 'reservation_id': server['reservation_id'],
217                 'uuid': server['id']},
218             actual=fake_notifier.VERSIONED_NOTIFICATIONS[1])
219 
220     def _verify_instance_update_steps(self, steps, notifications,
221                                       initial=None):
222         replacements = {}
223         if initial:
224             replacements = initial
225         for i, step in enumerate(steps):
226             replacements.update(step)
227             self._verify_notification(
228                 'instance-update',
229                 replacements=replacements,
230                 actual=notifications[i])
231         return replacements
232 
233     def test_create_delete_server_with_instance_update(self):
234         # This makes server network creation synchronous which is necessary
235         # for notification samples that expect instance.info_cache.network_info
236         # to be set.
237         self.useFixture(fixtures.SpawnIsSynchronousFixture())
238         self.flags(notify_on_state_change='vm_and_task_state',
239                    group='notifications')
240 
241         server = self._boot_a_server(
242             extra_params={'networks': [{'port': self.neutron.port_1['id']}]})
243         self._attach_volume_to_server(server, self.cinder.SWAP_OLD_VOL)
244 
245         instance_updates = self._wait_for_notifications('instance.update', 8)
246 
247         # The first notification comes from the nova-conductor, the
248         # eighth notification comes from nova-api the
249         # rest is from the nova-compute. To keep the test simpler
250         # assert this fact and then modify the publisher_id of the
251         # first and eighth notification to match the template
252         self.assertEqual('nova-conductor:fake-mini',
253                          instance_updates[0]['publisher_id'])
254         self.assertEqual('nova-api:fake-mini',
255                          instance_updates[7]['publisher_id'])
256         instance_updates[0]['publisher_id'] = 'nova-compute:fake-mini'
257         instance_updates[7]['publisher_id'] = 'nova-compute:fake-mini'
258 
259         create_steps = [
260             # nothing -> scheduling
261             {'reservation_id': server['reservation_id'],
262              'uuid': server['id'],
263              'host': None,
264              'node': None,
265              'state_update.new_task_state': 'scheduling',
266              'state_update.old_task_state': 'scheduling',
267              'state_update.state': 'building',
268              'state_update.old_state': 'building',
269              'state': 'building'},
270 
271             # scheduling -> building
272             {
273              'state_update.new_task_state': None,
274              'state_update.old_task_state': 'scheduling',
275              'task_state': None},
276 
277             # scheduled
278             {'host': 'compute',
279              'node': 'fake-mini',
280              'state_update.old_task_state': None,
281              'updated_at': '2012-10-29T13:42:11Z'},
282 
283             # building -> networking
284             {'state_update.new_task_state': 'networking',
285              'state_update.old_task_state': 'networking',
286              'task_state': 'networking'},
287 
288             # networking -> block_device_mapping
289             {'state_update.new_task_state': 'block_device_mapping',
290              'state_update.old_task_state': 'networking',
291              'task_state': 'block_device_mapping',
292              'ip_addresses': [{
293                  "nova_object.name": "IpPayload",
294                  "nova_object.namespace": "nova",
295                  "nova_object.version": "1.0",
296                  "nova_object.data": {
297                      "mac": "fa:16:3e:4c:2c:30",
298                      "address": "192.168.1.3",
299                      "port_uuid": "ce531f90-199f-48c0-816c-13e38010b442",
300                      "meta": {},
301                      "version": 4,
302                      "label": "private-network",
303                      "device_name": "tapce531f90-19"
304                  }}]
305             },
306 
307             # block_device_mapping -> spawning
308             {'state_update.new_task_state': 'spawning',
309              'state_update.old_task_state': 'block_device_mapping',
310              'task_state': 'spawning',
311              },
312 
313             # spawning -> active
314             {'state_update.new_task_state': None,
315              'state_update.old_task_state': 'spawning',
316              'state_update.state': 'active',
317              'launched_at': '2012-10-29T13:42:11Z',
318              'state': 'active',
319              'task_state': None,
320              'power_state': 'running'},
321 
322             # tag added
323             {'state_update.old_task_state': None,
324              'state_update.old_state': 'active',
325              'tags': ['tag1']},
326         ]
327 
328         replacements = self._verify_instance_update_steps(
329                 create_steps, instance_updates)
330 
331         fake_notifier.reset()
332 
333         # Let's generate some bandwidth usage data.
334         # Just call the periodic task directly for simplicity
335         self.compute.manager._poll_bandwidth_usage(context.get_admin_context())
336 
337         self.api.delete_server(server['id'])
338         self._wait_until_deleted(server)
339 
340         instance_updates = self._get_notifications('instance.update')
341         self.assertEqual(2, len(instance_updates))
342 
343         delete_steps = [
344             # active -> deleting
345             {'state_update.new_task_state': 'deleting',
346              'state_update.old_task_state': 'deleting',
347              'state_update.old_state': 'active',
348              'state': 'active',
349              'task_state': 'deleting',
350              'bandwidth': [
351                  {'nova_object.namespace': 'nova',
352                   'nova_object.name': 'BandwidthPayload',
353                   'nova_object.data':
354                       {'network_name': 'private-network',
355                        'out_bytes': 0,
356                        'in_bytes': 0},
357                   'nova_object.version': '1.0'}],
358              'tags': ["tag1"],
359              'block_devices': [{
360                 "nova_object.data": {
361                     "boot_index": None,
362                     "delete_on_termination": False,
363                     "device_name": "/dev/sdb",
364                     "tag": None,
365                     "volume_id": "a07f71dc-8151-4e7d-a0cc-cd24a3f11113"
366                 },
367                 "nova_object.name": "BlockDevicePayload",
368                 "nova_object.namespace": "nova",
369                 "nova_object.version": "1.0"
370               }]
371             },
372 
373             # deleting -> deleted
374             {'state_update.new_task_state': None,
375              'state_update.old_task_state': 'deleting',
376              'state_update.old_state': 'active',
377              'state_update.state': 'deleted',
378              'state': 'deleted',
379              'task_state': None,
380              'terminated_at': '2012-10-29T13:42:11Z',
381              'ip_addresses': [],
382              'power_state': 'pending',
383              'bandwidth': [],
384              'tags': ["tag1"],
385              'block_devices': [{
386                 "nova_object.data": {
387                     "boot_index": None,
388                     "delete_on_termination": False,
389                     "device_name": "/dev/sdb",
390                     "tag": None,
391                     "volume_id": "a07f71dc-8151-4e7d-a0cc-cd24a3f11113"
392                 },
393                 "nova_object.name": "BlockDevicePayload",
394                 "nova_object.namespace": "nova",
395                 "nova_object.version": "1.0"
396               }]
397             },
398         ]
399 
400         self._verify_instance_update_steps(delete_steps, instance_updates,
401                                            initial=replacements)
402 
403     def _test_power_off_on_server(self, server):
404         self.api.post_server_action(server['id'], {'os-stop': {}})
405         self._wait_for_state_change(self.api, server,
406                                     expected_status='SHUTOFF')
407         self.api.post_server_action(server['id'], {'os-start': {}})
408         self._wait_for_state_change(self.api, server,
409                                     expected_status='ACTIVE')
410 
411         self.assertEqual(4, len(fake_notifier.VERSIONED_NOTIFICATIONS))
412         self._verify_notification(
413             'instance-power_off-start',
414             replacements={
415                 'reservation_id': server['reservation_id'],
416                 'uuid': server['id']},
417             actual=fake_notifier.VERSIONED_NOTIFICATIONS[0])
418         self._verify_notification(
419             'instance-power_off-end',
420             replacements={
421                 'reservation_id': server['reservation_id'],
422                 'power_state': 'running',
423                 'uuid': server['id']},
424             actual=fake_notifier.VERSIONED_NOTIFICATIONS[1])
425 
426         self._verify_notification(
427             'instance-power_on-start',
428             replacements={
429                 'reservation_id': server['reservation_id'],
430                 'uuid': server['id']},
431             actual=fake_notifier.VERSIONED_NOTIFICATIONS[2])
432         self._verify_notification(
433             'instance-power_on-end',
434             replacements={
435                 'reservation_id': server['reservation_id'],
436                 'uuid': server['id']},
437             actual=fake_notifier.VERSIONED_NOTIFICATIONS[3])
438 
439     def _test_shelve_server(self, server):
440         self.flags(shelved_offload_time = -1)
441 
442         self.api.post_server_action(server['id'], {'shelve': {}})
443         self._wait_for_state_change(self.api, server,
444                                     expected_status='SHELVED')
445 
446         self.assertEqual(2, len(fake_notifier.VERSIONED_NOTIFICATIONS))
447         self._verify_notification(
448             'instance-shelve-start',
449             replacements={
450                 'reservation_id': server['reservation_id'],
451                 'uuid': server['id']},
452             actual=fake_notifier.VERSIONED_NOTIFICATIONS[0])
453         self._verify_notification(
454             'instance-shelve-end',
455             replacements={
456                 'reservation_id': server['reservation_id'],
457                 'uuid': server['id']},
458             actual=fake_notifier.VERSIONED_NOTIFICATIONS[1])
459 
460         post = {'unshelve': None}
461         self.api.post_server_action(server['id'], post)
462 
463     def _test_shelve_offload_server(self, server):
464         self.flags(shelved_offload_time=-1)
465         self.api.post_server_action(server['id'], {'shelve': {}})
466         self._wait_for_state_change(self.api, server,
467                                     expected_status='SHELVED')
468         self.api.post_server_action(server['id'], {'shelveOffload': {}})
469         # we need to wait for the instance.host to become None as well before
470         # we can unshelve to make sure that the unshelve.start notification
471         # payload is stable as the compute manager first sets the instance
472         # state then a bit later sets the instance.host to None.
473         self._wait_for_server_parameter(self.api, server,
474                                         {'status': 'SHELVED_OFFLOADED',
475                                          'OS-EXT-SRV-ATTR:host': None})
476 
477         self.assertEqual(4, len(fake_notifier.VERSIONED_NOTIFICATIONS))
478         self._verify_notification(
479             'instance-shelve-start',
480             replacements={
481                 'reservation_id': server['reservation_id'],
482                 'uuid': server['id']},
483             actual=fake_notifier.VERSIONED_NOTIFICATIONS[0])
484         self._verify_notification(
485             'instance-shelve-end',
486             replacements={
487                 'reservation_id': server['reservation_id'],
488                 'uuid': server['id']},
489             actual=fake_notifier.VERSIONED_NOTIFICATIONS[1])
490 
491         self._verify_notification(
492             'instance-shelve_offload-start',
493             replacements={
494                 'reservation_id': server['reservation_id'],
495                 'uuid': server['id']},
496             actual=fake_notifier.VERSIONED_NOTIFICATIONS[2])
497         self._verify_notification(
498             'instance-shelve_offload-end',
499             replacements={
500                 'reservation_id': server['reservation_id'],
501                 'uuid': server['id']},
502             actual=fake_notifier.VERSIONED_NOTIFICATIONS[3])
503 
504         self.api.post_server_action(server['id'], {'unshelve': None})
505 
506     def _test_unshelve_server(self, server):
507         # setting the shelved_offload_time to 0 should set the
508         # instance status to 'SHELVED_OFFLOADED'
509         self.flags(shelved_offload_time = 0)
510         self.api.post_server_action(server['id'], {'shelve': {}})
511         # we need to wait for the instance.host to become None as well before
512         # we can unshelve to make sure that the unshelve.start notification
513         # payload is stable as the compute manager first sets the instance
514         # state then a bit later sets the instance.host to None.
515         self._wait_for_server_parameter(self.api, server,
516                                         {'status': 'SHELVED_OFFLOADED',
517                                          'OS-EXT-SRV-ATTR:host': None})
518 
519         post = {'unshelve': None}
520         self.api.post_server_action(server['id'], post)
521         self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
522         self.assertEqual(6, len(fake_notifier.VERSIONED_NOTIFICATIONS))
523         self._verify_notification(
524             'instance-unshelve-start',
525             replacements={
526                 'reservation_id': server['reservation_id'],
527                 'uuid': server['id']},
528             actual=fake_notifier.VERSIONED_NOTIFICATIONS[4])
529         self._verify_notification(
530             'instance-unshelve-end',
531             replacements={
532                 'reservation_id': server['reservation_id'],
533                 'uuid': server['id']},
534             actual=fake_notifier.VERSIONED_NOTIFICATIONS[5])
535 
536     def _test_suspend_resume_server(self, server):
537         post = {'suspend': {}}
538         self.api.post_server_action(server['id'], post)
539         self._wait_for_state_change(self.admin_api, server, 'SUSPENDED')
540 
541         post = {'resume': None}
542         self.api.post_server_action(server['id'], post)
543         self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
544 
545         # Four versioned notification are generated.
546         # 0. instance-suspend-start
547         # 1. instance-suspend-end
548         # 2. instance-resume-start
549         # 3. instance-resume-end
550         self.assertEqual(4, len(fake_notifier.VERSIONED_NOTIFICATIONS))
551         self._verify_notification(
552             'instance-suspend-start',
553             replacements={
554                 'reservation_id': server['reservation_id'],
555                 'uuid': server['id']},
556             actual=fake_notifier.VERSIONED_NOTIFICATIONS[0])
557         self._verify_notification(
558             'instance-suspend-end',
559             replacements={
560                 'reservation_id': server['reservation_id'],
561                 'uuid': server['id']},
562             actual=fake_notifier.VERSIONED_NOTIFICATIONS[1])
563 
564         self._verify_notification(
565             'instance-resume-start',
566             replacements={
567                 'reservation_id': server['reservation_id'],
568                 'uuid': server['id']},
569             actual=fake_notifier.VERSIONED_NOTIFICATIONS[2])
570         self._verify_notification(
571             'instance-resume-end',
572             replacements={
573                 'reservation_id': server['reservation_id'],
574                 'uuid': server['id']},
575             actual=fake_notifier.VERSIONED_NOTIFICATIONS[3])
576 
577         self.flags(reclaim_instance_interval=0)
578 
579     def _test_pause_unpause_server(self, server):
580         self.api.post_server_action(server['id'], {'pause': {}})
581         self._wait_for_state_change(self.api, server, 'PAUSED')
582 
583         self.api.post_server_action(server['id'], {'unpause': {}})
584         self._wait_for_state_change(self.api, server, 'ACTIVE')
585 
586         # Four versioned notifications are generated
587         # 0. instance-pause-start
588         # 1. instance-pause-end
589         # 2. instance-unpause-start
590         # 3. instance-unpause-end
591         self.assertEqual(4, len(fake_notifier.VERSIONED_NOTIFICATIONS))
592         self._verify_notification(
593             'instance-pause-start',
594             replacements={
595                 'reservation_id': server['reservation_id'],
596                 'uuid': server['id']},
597             actual=fake_notifier.VERSIONED_NOTIFICATIONS[0])
598         self._verify_notification(
599             'instance-pause-end',
600             replacements={
601                 'reservation_id': server['reservation_id'],
602                 'uuid': server['id']},
603             actual=fake_notifier.VERSIONED_NOTIFICATIONS[1])
604         self._verify_notification(
605             'instance-unpause-start',
606             replacements={
607                 'reservation_id': server['reservation_id'],
608                 'uuid': server['id']},
609             actual=fake_notifier.VERSIONED_NOTIFICATIONS[2])
610         self._verify_notification(
611             'instance-unpause-end',
612             replacements={
613                 'reservation_id': server['reservation_id'],
614                 'uuid': server['id']},
615             actual=fake_notifier.VERSIONED_NOTIFICATIONS[3])
616 
617     def _test_resize_server(self, server):
618         self.flags(allow_resize_to_same_host=True)
619         other_flavor_body = {
620             'flavor': {
621                 'name': 'other_flavor',
622                 'ram': 256,
623                 'vcpus': 1,
624                 'disk': 1,
625                 'id': 'd5a8bb54-365a-45ae-abdb-38d249df7845'
626             }
627         }
628         other_flavor_id = self.api.post_flavor(other_flavor_body)['id']
629         extra_specs = {
630             "extra_specs": {
631                 "hw:watchdog_action": "reset"}}
632         self.admin_api.post_extra_spec(other_flavor_id, extra_specs)
633 
634         # Ignore the create flavor notification
635         fake_notifier.reset()
636 
637         post = {
638             'resize': {
639                 'flavorRef': other_flavor_id
640             }
641         }
642         self.api.post_server_action(server['id'], post)
643         self._wait_for_state_change(self.api, server, 'VERIFY_RESIZE')
644 
645         self.assertEqual(4, len(fake_notifier.VERSIONED_NOTIFICATIONS))
646         # This list needs to be in order.
647         expected_notifications = [
648             'instance-resize-start',
649             'instance-resize-end',
650             'instance-resize_finish-start',
651             'instance-resize_finish-end'
652         ]
653         for idx, notification in enumerate(expected_notifications):
654             self._verify_notification(
655                 notification,
656                 replacements={
657                     'reservation_id': server['reservation_id'],
658                     'uuid': server['id']},
659                 actual=fake_notifier.VERSIONED_NOTIFICATIONS[idx])
660 
661         post = {'revertResize': None}
662         self.api.post_server_action(server['id'], post)
663 
664     @mock.patch('nova.compute.manager.ComputeManager._reschedule')
665     @mock.patch('nova.compute.manager.ComputeManager._prep_resize')
666     def test_resize_server_error_but_reschedule_was_success(
667             self, mock_prep_resize, mock_reschedule):
668         """Test it, when the prep_resize method raise an exception,
669         but the reschedule_resize_or_reraise was successful and
670         scheduled the resize. In this case we get a notification
671         about the exception, which caused the prep_resize error.
672         """
673         def _build_resources(*args, **kwargs):
674             raise exception.FlavorDiskTooSmall()
675         server = self._boot_a_server(
676             extra_params={'networks': [{'port': self.neutron.port_1['id']}]})
677         self.flags(allow_resize_to_same_host=True)
678         other_flavor_body = {
679             'flavor': {
680                 'name': 'other_flavor_error',
681                 'ram': 512,
682                 'vcpus': 1,
683                 'disk': 1,
684                 'id': 'a22d5517-147c-4147-a0d1-e698df5cd4e9'
685             }
686         }
687         other_flavor_id = self.api.post_flavor(other_flavor_body)['id']
688 
689         post = {
690             'resize': {
691                 'flavorRef': other_flavor_id
692             }
693         }
694         fake_notifier.reset()
695         self.api.post_server_action(server['id'], post)
696         mock_prep_resize.side_effect = _build_resources
697         self._wait_for_notification('instance.resize.error')
698         self.assertEqual(1, len(fake_notifier.VERSIONED_NOTIFICATIONS))
699         self._verify_notification('instance-resize-error',
700             replacements={
701                 'reservation_id': server['reservation_id'],
702                 'uuid': server['id']
703             },
704             actual=fake_notifier.VERSIONED_NOTIFICATIONS[0])
705 
706     @mock.patch('nova.compute.manager.ComputeManager._reschedule')
707     @mock.patch('nova.compute.manager.ComputeManager._prep_resize')
708     def test_resize_server_error_and_reschedule_was_failed(
709             self, mock_prep_resize, mock_reschedule):
710         """Test it, when the prep_resize method raise an exception,
711         after trying again with the reschedule_resize_or_reraise method
712         call, but the rescheduled also was unsuccessful. In this
713         case called the exception block.
714         In the exception block send a notification about error.
715         At end called the six.reraise(*exc_info), which not
716         send another error.
717         """
718         def _build_resources(*args, **kwargs):
719             raise exception.FlavorDiskTooSmall()
720 
721         server = self._boot_a_server(
722             extra_params={'networks': [{'port': self.neutron.port_1['id']}]})
723         self.flags(allow_resize_to_same_host=True)
724         other_flavor_body = {
725             'flavor': {
726                 'name': 'other_flavor_error',
727                 'ram': 512,
728                 'vcpus': 1,
729                 'disk': 1,
730                 'id': 'a22d5517-147c-4147-a0d1-e698df5cd4e9'
731             }
732         }
733         other_flavor_id = self.api.post_flavor(other_flavor_body)['id']
734 
735         post = {
736             'resize': {
737                 'flavorRef': other_flavor_id
738             }
739         }
740         fake_notifier.reset()
741         mock_prep_resize.side_effect = _build_resources
742         mock_reschedule.side_effect = _build_resources
743         self.api.post_server_action(server['id'], post)
744         self._wait_for_state_change(self.api, server, expected_status='ERROR')
745         self.assertEqual(2, len(fake_notifier.VERSIONED_NOTIFICATIONS))
746         self._verify_notification('instance-resize-error',
747             replacements={
748                 'reservation_id': server['reservation_id'],
749                 'uuid': server['id']
750             },
751             actual=fake_notifier.VERSIONED_NOTIFICATIONS[0])
752 
753     def _test_snapshot_server(self, server):
754         post = {'createImage': {'name': 'test-snap'}}
755         self.api.post_server_action(server['id'], post)
756         self._wait_for_notification('instance.snapshot.end')
757 
758         self.assertEqual(2, len(fake_notifier.VERSIONED_NOTIFICATIONS))
759         self._verify_notification(
760             'instance-snapshot-start',
761             replacements={
762                 'reservation_id': server['reservation_id'],
763                 'uuid': server['id']},
764                     actual=fake_notifier.VERSIONED_NOTIFICATIONS[0])
765         self._verify_notification(
766             'instance-snapshot-end',
767             replacements={
768                 'reservation_id': server['reservation_id'],
769                 'uuid': server['id']},
770             actual=fake_notifier.VERSIONED_NOTIFICATIONS[1])
771 
772     def test_rebuild_server(self):
773         # NOTE(gabor_antal): Rebuild changes the image used by the instance,
774         # therefore the actions tested in test_instance_action had to be in
775         # specific order. To avoid this problem, rebuild was moved from
776         # test_instance_action to its own method.
777 
778         server = self._boot_a_server(
779             extra_params={'networks': [{'port': self.neutron.port_1['id']}]})
780         self._attach_volume_to_server(server, self.cinder.SWAP_OLD_VOL)
781 
782         fake_notifier.reset()
783 
784         post = {
785             'rebuild': {
786                 'imageRef': 'a2459075-d96c-40d5-893e-577ff92e721c',
787                 'metadata': {}
788             }
789         }
790         self.api.post_server_action(server['id'], post)
791         # Before going back to ACTIVE state
792         # server state need to be changed to REBUILD state
793         self._wait_for_state_change(self.api, server,
794                                     expected_status='REBUILD')
795         self._wait_for_state_change(self.api, server,
796                                     expected_status='ACTIVE')
797 
798         # The compute/manager will detach every volume during rebuild
799         self.assertEqual(4, len(fake_notifier.VERSIONED_NOTIFICATIONS))
800         self._verify_notification(
801             'instance-rebuild-start',
802             replacements={
803                 'reservation_id': server['reservation_id'],
804                 'uuid': server['id']},
805             actual=fake_notifier.VERSIONED_NOTIFICATIONS[0])
806         self._verify_notification(
807             'instance-volume_detach-start',
808             replacements={
809                 'reservation_id': server['reservation_id'],
810                 'task_state': 'rebuilding',
811                 'architecture': None,
812                 'image_uuid': 'a2459075-d96c-40d5-893e-577ff92e721c',
813                 'uuid': server['id']},
814             actual=fake_notifier.VERSIONED_NOTIFICATIONS[1])
815         self._verify_notification(
816             'instance-volume_detach-end',
817             replacements={
818                 'reservation_id': server['reservation_id'],
819                 'task_state': 'rebuilding',
820                 'architecture': None,
821                 'image_uuid': 'a2459075-d96c-40d5-893e-577ff92e721c',
822                 'uuid': server['id']},
823             actual=fake_notifier.VERSIONED_NOTIFICATIONS[2])
824         self._verify_notification(
825             'instance-rebuild-end',
826             replacements={
827                 'reservation_id': server['reservation_id'],
828                 'uuid': server['id']},
829             actual=fake_notifier.VERSIONED_NOTIFICATIONS[3])
830 
831     @mock.patch('nova.compute.manager.ComputeManager.'
832                 '_do_rebuild_instance_with_claim')
833     def test_rebuild_server_exc(self, mock_rebuild):
834         def _compute_resources_unavailable(*args, **kwargs):
835             raise exception.ComputeResourcesUnavailable(
836                 reason="fake-resource")
837 
838         server = self._boot_a_server(
839             extra_params={'networks': [{'port': self.neutron.port_1['id']}]})
840         self._attach_volume_to_server(server, self.cinder.SWAP_OLD_VOL)
841 
842         fake_notifier.reset()
843 
844         post = {
845             'rebuild': {
846                 'imageRef': 'a2459075-d96c-40d5-893e-577ff92e721c',
847                 'metadata': {}
848             }
849         }
850         self.api.post_server_action(server['id'], post)
851         mock_rebuild.side_effect = _compute_resources_unavailable
852         self._wait_for_state_change(self.api, server, expected_status='ERROR')
853         notification = self._get_notifications('instance.rebuild.error')
854         self.assertEqual(1, len(notification))
855         self._verify_notification(
856             'instance-rebuild-error',
857             replacements={
858                 'reservation_id': server['reservation_id'],
859                 'uuid': server['id']},
860             actual=notification[0])
861 
862     def _test_restore_server(self, server):
863         self.flags(reclaim_instance_interval=30)
864         self.api.delete_server(server['id'])
865         self._wait_for_state_change(self.api, server, 'SOFT_DELETED')
866         # we don't want to test soft_delete here
867         fake_notifier.reset()
868         self.api.post_server_action(server['id'], {'restore': {}})
869         self._wait_for_state_change(self.api, server, 'ACTIVE')
870 
871         self.assertEqual(2, len(fake_notifier.VERSIONED_NOTIFICATIONS))
872         self._verify_notification(
873             'instance-restore-start',
874             replacements={
875                 'reservation_id': server['reservation_id'],
876                 'uuid': server['id']},
877             actual=fake_notifier.VERSIONED_NOTIFICATIONS[0])
878         self._verify_notification(
879             'instance-restore-end',
880             replacements={
881                 'reservation_id': server['reservation_id'],
882                 'uuid': server['id']},
883             actual=fake_notifier.VERSIONED_NOTIFICATIONS[1])
884 
885     def _test_reboot_server(self, server):
886         post = {'reboot': {'type': 'HARD'}}
887         self.api.post_server_action(server['id'], post)
888         self._wait_for_notification('instance.reboot.start')
889         self._wait_for_notification('instance.reboot.end')
890 
891         self.assertEqual(2, len(fake_notifier.VERSIONED_NOTIFICATIONS))
892         self._verify_notification(
893             'instance-reboot-start',
894             replacements={
895                 'reservation_id': server['reservation_id'],
896                 'uuid': server['id']},
897             actual=fake_notifier.VERSIONED_NOTIFICATIONS[0])
898         self._verify_notification(
899             'instance-reboot-end',
900             replacements={
901                 'reservation_id': server['reservation_id'],
902                 'uuid': server['id']},
903             actual=fake_notifier.VERSIONED_NOTIFICATIONS[1])
904 
905     @mock.patch('nova.virt.fake.SmallFakeDriver.reboot')
906     def _test_reboot_server_error(self, server, mock_reboot):
907         def _hard_reboot(*args, **kwargs):
908             raise exception.UnsupportedVirtType(virt="FakeVirt")
909         mock_reboot.side_effect = _hard_reboot
910         post = {'reboot': {'type': 'HARD'}}
911         self.api.post_server_action(server['id'], post)
912         self._wait_for_notification('instance.reboot.start')
913         self._wait_for_notification('instance.reboot.error')
914         self.assertEqual(2, len(fake_notifier.VERSIONED_NOTIFICATIONS))
915         self._verify_notification(
916             'instance-reboot-start',
917             replacements={
918                 'reservation_id': server['reservation_id'],
919                 'uuid': server['id']},
920             actual=fake_notifier.VERSIONED_NOTIFICATIONS[0])
921         self._verify_notification(
922             'instance-reboot-error',
923             replacements={
924                 'reservation_id': server['reservation_id'],
925                 'uuid': server['id']},
926             actual=fake_notifier.VERSIONED_NOTIFICATIONS[1])
927 
928     def _detach_volume_from_server(self, server, volume_id):
929         self.api.delete_server_volume(server['id'], volume_id)
930         self._wait_for_notification('instance.volume_detach.end')
931 
932     def _volume_swap_server(self, server, attachement_id, volume_id):
933         self.api.put_server_volume(server['id'], attachement_id, volume_id)
934 
935     def test_volume_swap_server(self):
936         server = self._boot_a_server(
937             extra_params={'networks':
938                           [{'port': self.neutron.port_1['id']}]})
939 
940         self._attach_volume_to_server(server, self.cinder.SWAP_OLD_VOL)
941         self.cinder.swap_volume_instance_uuid = server['id']
942 
943         self._volume_swap_server(server, self.cinder.SWAP_OLD_VOL,
944                                  self.cinder.SWAP_NEW_VOL)
945         self._wait_until_swap_volume(server, self.cinder.SWAP_NEW_VOL)
946         # NOTE(gibi): the new volume id can appear on the API earlier than the
947         # volume_swap.end notification emitted. So to make the test stable
948         # we have to wait for the volume_swap.end notification directly.
949         self._wait_for_notification('instance.volume_swap.end')
950 
951         self.assertEqual(7, len(fake_notifier.VERSIONED_NOTIFICATIONS),
952                          'Unexpected number of versioned notifications. '
953                          'Got: %s' % fake_notifier.VERSIONED_NOTIFICATIONS)
954         self._verify_notification(
955             'instance-volume_swap-start',
956             replacements={
957                 'reservation_id': server['reservation_id'],
958                 'uuid': server['id']},
959             actual=fake_notifier.VERSIONED_NOTIFICATIONS[5])
960         self._verify_notification(
961             'instance-volume_swap-end',
962             replacements={
963                 'reservation_id': server['reservation_id'],
964                 'uuid': server['id']},
965             actual=fake_notifier.VERSIONED_NOTIFICATIONS[6])
966 
967     def test_volume_swap_server_with_error(self):
968         server = self._boot_a_server(
969             extra_params={'networks': [{'port': self.neutron.port_1['id']}]})
970 
971         self._attach_volume_to_server(server, self.cinder.SWAP_ERR_OLD_VOL)
972         self.cinder.swap_volume_instance_error_uuid = server['id']
973 
974         self._volume_swap_server(server, self.cinder.SWAP_ERR_OLD_VOL,
975                                  self.cinder.SWAP_ERR_NEW_VOL)
976         self._wait_until_swap_volume_error()
977 
978         # Seven versioned notifications are generated. We only rely on the
979         # first six because _wait_until_swap_volume_error will return True
980         # after volume_api.unreserve is called on the cinder fixture, and that
981         # happens before the instance fault is handled in the compute manager
982         # which generates the last notification (compute.exception).
983         # 0. instance-create-start
984         # 1. instance-create-end
985         # 2. instance-update
986         # 3. instance-volume_attach-start
987         # 4. instance-volume_attach-end
988         # 5. instance-volume_swap-start
989         # 6. instance-volume_swap-error
990         # 7. compute.exception
991         self.assertLessEqual(7, len(fake_notifier.VERSIONED_NOTIFICATIONS),
992                              'Unexpected number of versioned notifications. '
993                              'Got: %s' % fake_notifier.VERSIONED_NOTIFICATIONS)
994         block_devices = [{
995             "nova_object.data": {
996                 "boot_index": None,
997                 "delete_on_termination": False,
998                 "device_name": "/dev/sdb",
999                 "tag": None,
1000                 "volume_id": self.cinder.SWAP_ERR_OLD_VOL
1001             },
1002             "nova_object.name": "BlockDevicePayload",
1003             "nova_object.namespace": "nova",
1004             "nova_object.version": "1.0"
1005         }]
1006         self._verify_notification(
1007             'instance-volume_swap-start',
1008             replacements={
1009                 'new_volume_id': self.cinder.SWAP_ERR_NEW_VOL,
1010                 'old_volume_id': self.cinder.SWAP_ERR_OLD_VOL,
1011                 'block_devices': block_devices,
1012                 'reservation_id': server['reservation_id'],
1013                 'uuid': server['id']},
1014             actual=fake_notifier.VERSIONED_NOTIFICATIONS[5])
1015         self._verify_notification(
1016             'instance-volume_swap-error',
1017             replacements={
1018                 'reservation_id': server['reservation_id'],
1019                 'block_devices': block_devices,
1020                 'uuid': server['id']},
1021             actual=fake_notifier.VERSIONED_NOTIFICATIONS[6])
1022 
1023     def _test_revert_server(self, server):
1024         pass
1025 
1026     def _test_resize_confirm_server(self, server):
1027         pass
1028 
1029     def _test_trigger_crash_dump(self, server):
1030         pass
1031 
1032     def _test_volume_detach_attach_server(self, server):
1033         self._detach_volume_from_server(server, self.cinder.SWAP_OLD_VOL)
1034 
1035         # 0. volume_detach-start
1036         # 1. volume_detach-end
1037         self.assertEqual(2, len(fake_notifier.VERSIONED_NOTIFICATIONS))
1038         self._verify_notification(
1039             'instance-volume_detach-start',
1040             replacements={
1041                 'reservation_id': server['reservation_id'],
1042                 'uuid': server['id']},
1043             actual=fake_notifier.VERSIONED_NOTIFICATIONS[0])
1044         self._verify_notification(
1045             'instance-volume_detach-end',
1046             replacements={
1047                 'reservation_id': server['reservation_id'],
1048                 'uuid': server['id']},
1049             actual=fake_notifier.VERSIONED_NOTIFICATIONS[1])
1050 
1051         fake_notifier.reset()
1052         self._attach_volume_to_server(server, self.cinder.SWAP_OLD_VOL)
1053 
1054         # 0. volume_attach-start
1055         # 1. volume_attach-end
1056         self.assertEqual(2, len(fake_notifier.VERSIONED_NOTIFICATIONS))
1057         self._verify_notification(
1058             'instance-volume_attach-start',
1059             replacements={
1060                 'reservation_id': server['reservation_id'],
1061                 'uuid': server['id']},
1062             actual=fake_notifier.VERSIONED_NOTIFICATIONS[0])
1063         self._verify_notification(
1064             'instance-volume_attach-end',
1065             replacements={
1066                 'reservation_id': server['reservation_id'],
1067                 'uuid': server['id']},
1068             actual=fake_notifier.VERSIONED_NOTIFICATIONS[1])
1069 
1070     def _test_rescue_server(self, server):
1071         pass
1072 
1073     def _test_unrescue_server(self, server):
1074         pass
1075 
1076     def _test_soft_delete_server(self, server):
1077         self.flags(reclaim_instance_interval=30)
1078         self.api.delete_server(server['id'])
1079         self._wait_for_state_change(self.api, server, 'SOFT_DELETED')
1080 
1081         self.assertEqual(2, len(fake_notifier.VERSIONED_NOTIFICATIONS))
1082         self._verify_notification(
1083             'instance-soft_delete-start',
1084             replacements={
1085                 'reservation_id': server['reservation_id'],
1086                 'uuid': server['id']},
1087             actual=fake_notifier.VERSIONED_NOTIFICATIONS[0])
1088         self._verify_notification(
1089             'instance-soft_delete-end',
1090             replacements={
1091                 'reservation_id': server['reservation_id'],
1092                 'uuid': server['id']},
1093             actual=fake_notifier.VERSIONED_NOTIFICATIONS[1])
1094         self.flags(reclaim_instance_interval=0)
1095         # Leave instance in normal, active state
1096         self.api.post_server_action(server['id'], {'restore': {}})
1097 
1098     @mock.patch('nova.volume.cinder.API.attach')
1099     def _test_attach_volume_error(self, server, mock_attach):
1100         def attach_volume(*args, **kwargs):
1101             raise exception.CinderConnectionFailed(
1102                 reason="Connection timed out")
1103         mock_attach.side_effect = attach_volume
1104 
1105         post = {"volumeAttachment": {"volumeId": self.cinder.SWAP_NEW_VOL}}
1106         self.api.post_server_volume(server['id'], post)
1107 
1108         self._wait_for_notification('instance.volume_attach.error')
1109 
1110         block_devices = [
1111             # Add by default at boot
1112             {'nova_object.data': {'boot_index': None,
1113                                   'delete_on_termination': False,
1114                                   'tag': None,
1115                                   'device_name': '/dev/sdb',
1116                                   'volume_id': self.cinder.SWAP_OLD_VOL},
1117              'nova_object.name': 'BlockDevicePayload',
1118              'nova_object.namespace': 'nova',
1119              'nova_object.version': '1.0'},
1120             # Attaching it right now
1121             {'nova_object.data': {'boot_index': None,
1122                                   'delete_on_termination': False,
1123                                   'tag': None,
1124                                   'device_name': '/dev/sdc',
1125                                   'volume_id': self.cinder.SWAP_NEW_VOL},
1126              'nova_object.name': 'BlockDevicePayload',
1127              'nova_object.namespace': 'nova',
1128              'nova_object.version': '1.0'}]
1129 
1130         # 0. volume_attach-start
1131         # 1. volume_attach-error
1132         # 2. compute.exception
1133         # We only rely on the first 2 notifications, in this case we don't
1134         # care about the exception notification.
1135         self.assertLessEqual(2, len(fake_notifier.VERSIONED_NOTIFICATIONS))
1136         self._verify_notification(
1137             'instance-volume_attach-start',
1138             replacements={
1139                 'reservation_id': server['reservation_id'],
1140                 'block_devices': block_devices,
1141                 'volume_id': self.cinder.SWAP_NEW_VOL,
1142                 'uuid': server['id']},
1143             actual=fake_notifier.VERSIONED_NOTIFICATIONS[0])
1144         self._verify_notification(
1145             'instance-volume_attach-error',
1146             replacements={
1147                 'reservation_id': server['reservation_id'],
1148                 'block_devices': block_devices,
1149                 'volume_id': self.cinder.SWAP_NEW_VOL,
1150                 'uuid': server['id']},
1151             actual=fake_notifier.VERSIONED_NOTIFICATIONS[1])
