Based on the given code from a commit, please generate supplementary code files according to the commit message.
####commit message
libvirt: Enable driver configures PMEM namespace

Suppose that the configuration option CONF.libvirt.pmem_namespaces
is introduced. Libvirt driver will get and update PMEM namespaces info
from host and configuration.

Change-Id: I146cc9d4caafd0abd6b2642edb6eb59cd7e339b7
Partially-Implements: blueprint virtual-persistent-memory
Co-Authored-By: He Jie Xu <hejie.xu@intel.com>

####code 
1 # Copyright 2010 United States Government as represented by the
2 # Administrator of the National Aeronautics and Space Administration.
3 # All Rights Reserved.
4 # Copyright (c) 2010 Citrix Systems, Inc.
5 # Copyright (c) 2011 Piston Cloud Computing, Inc
6 # Copyright (c) 2012 University Of Minho
7 # (c) Copyright 2013 Hewlett-Packard Development Company, L.P.
8 #
9 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
10 #    not use this file except in compliance with the License. You may obtain
11 #    a copy of the License at
12 #
13 #         http://www.apache.org/licenses/LICENSE-2.0
14 #
15 #    Unless required by applicable law or agreed to in writing, software
16 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
17 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
18 #    License for the specific language governing permissions and limitations
19 #    under the License.
20 
21 """
22 A connection to a hypervisor through libvirt.
23 
24 Supports KVM, LXC, QEMU, UML, XEN and Parallels.
25 
26 """
27 
28 import binascii
29 import collections
30 from collections import deque
31 import contextlib
32 import copy
33 import errno
34 import functools
35 import glob
36 import itertools
37 import operator
38 import os
39 import pwd
40 import random
41 import shutil
42 import tempfile
43 import time
44 import uuid
45 
46 from castellan import key_manager
47 from copy import deepcopy
48 import eventlet
49 from eventlet import greenthread
50 from eventlet import tpool
51 from lxml import etree
52 from os_brick import encryptors
53 from os_brick.encryptors import luks as luks_encryptor
54 from os_brick import exception as brick_exception
55 from os_brick.initiator import connector
56 import os_resource_classes as orc
57 from oslo_concurrency import processutils
58 from oslo_log import log as logging
59 from oslo_serialization import base64
60 from oslo_serialization import jsonutils
61 from oslo_service import loopingcall
62 from oslo_utils import encodeutils
63 from oslo_utils import excutils
64 from oslo_utils import fileutils
65 from oslo_utils import importutils
66 from oslo_utils import netutils as oslo_netutils
67 from oslo_utils import strutils
68 from oslo_utils import timeutils
69 from oslo_utils import units
70 from oslo_utils import uuidutils
71 import six
72 from six.moves import range
73 
74 from nova.api.metadata import base as instance_metadata
75 from nova.api.metadata import password
76 from nova import block_device
77 from nova.compute import power_state
78 from nova.compute import task_states
79 from nova.compute import utils as compute_utils
80 from nova.compute import vm_states
81 import nova.conf
82 from nova.console import serial as serial_console
83 from nova.console import type as ctype
84 from nova import context as nova_context
85 from nova import crypto
86 from nova import exception
87 from nova.i18n import _
88 from nova import image
89 from nova.network import model as network_model
90 from nova import objects
91 from nova.objects import diagnostics as diagnostics_obj
92 from nova.objects import fields
93 from nova.pci import manager as pci_manager
94 from nova.pci import utils as pci_utils
95 import nova.privsep.libvirt
96 import nova.privsep.path
97 import nova.privsep.utils
98 from nova import utils
99 from nova import version
100 from nova.virt import block_device as driver_block_device
101 from nova.virt import configdrive
102 from nova.virt.disk import api as disk_api
103 from nova.virt.disk.vfs import guestfs
104 from nova.virt import driver
105 from nova.virt import firewall
106 from nova.virt import hardware
107 from nova.virt.image import model as imgmodel
108 from nova.virt import images
109 from nova.virt.libvirt import blockinfo
110 from nova.virt.libvirt import config as vconfig
111 from nova.virt.libvirt import designer
112 from nova.virt.libvirt import firewall as libvirt_firewall
113 from nova.virt.libvirt import guest as libvirt_guest
114 from nova.virt.libvirt import host
115 from nova.virt.libvirt import imagebackend
116 from nova.virt.libvirt import imagecache
117 from nova.virt.libvirt import instancejobtracker
118 from nova.virt.libvirt import migration as libvirt_migrate
119 from nova.virt.libvirt.storage import dmcrypt
120 from nova.virt.libvirt.storage import lvm
121 from nova.virt.libvirt.storage import rbd_utils
122 from nova.virt.libvirt import utils as libvirt_utils
123 from nova.virt.libvirt import vif as libvirt_vif
124 from nova.virt.libvirt.volume import mount
125 from nova.virt.libvirt.volume import remotefs
126 from nova.virt import netutils
127 from nova.volume import cinder
128 
129 libvirt = None
130 
131 uefi_logged = False
132 
133 LOG = logging.getLogger(__name__)
134 
135 CONF = nova.conf.CONF
136 
137 DEFAULT_FIREWALL_DRIVER = "%s.%s" % (
138     libvirt_firewall.__name__,
139     libvirt_firewall.IptablesFirewallDriver.__name__)
140 
141 DEFAULT_UEFI_LOADER_PATH = {
142     "x86_64": "/usr/share/OVMF/OVMF_CODE.fd",
143     "aarch64": "/usr/share/AAVMF/AAVMF_CODE.fd"
144 }
145 
146 MAX_CONSOLE_BYTES = 100 * units.Ki
147 
148 # The libvirt driver will prefix any disable reason codes with this string.
149 DISABLE_PREFIX = 'AUTO: '
150 # Disable reason for the service which was enabled or disabled without reason
151 DISABLE_REASON_UNDEFINED = None
152 
153 # Guest config console string
154 CONSOLE = "console=tty0 console=ttyS0 console=hvc0"
155 
156 GuestNumaConfig = collections.namedtuple(
157     'GuestNumaConfig', ['cpuset', 'cputune', 'numaconfig', 'numatune'])
158 
159 
160 class InjectionInfo(collections.namedtuple(
161         'InjectionInfo', ['network_info', 'files', 'admin_pass'])):
162     __slots__ = ()
163 
164     def __repr__(self):
165         return ('InjectionInfo(network_info=%r, files=%r, '
166                 'admin_pass=<SANITIZED>)') % (self.network_info, self.files)
167 
168 
169 libvirt_volume_drivers = [
170     'iscsi=nova.virt.libvirt.volume.iscsi.LibvirtISCSIVolumeDriver',
171     'iser=nova.virt.libvirt.volume.iser.LibvirtISERVolumeDriver',
172     'local=nova.virt.libvirt.volume.volume.LibvirtVolumeDriver',
173     'drbd=nova.virt.libvirt.volume.drbd.LibvirtDRBDVolumeDriver',
174     'fake=nova.virt.libvirt.volume.volume.LibvirtFakeVolumeDriver',
175     'rbd=nova.virt.libvirt.volume.net.LibvirtNetVolumeDriver',
176     'sheepdog=nova.virt.libvirt.volume.net.LibvirtNetVolumeDriver',
177     'nfs=nova.virt.libvirt.volume.nfs.LibvirtNFSVolumeDriver',
178     'smbfs=nova.virt.libvirt.volume.smbfs.LibvirtSMBFSVolumeDriver',
179     'aoe=nova.virt.libvirt.volume.aoe.LibvirtAOEVolumeDriver',
180     'fibre_channel='
181         'nova.virt.libvirt.volume.fibrechannel.'
182         'LibvirtFibreChannelVolumeDriver',
183     'gpfs=nova.virt.libvirt.volume.gpfs.LibvirtGPFSVolumeDriver',
184     'quobyte=nova.virt.libvirt.volume.quobyte.LibvirtQuobyteVolumeDriver',
185     'hgst=nova.virt.libvirt.volume.hgst.LibvirtHGSTVolumeDriver',
186     'scaleio=nova.virt.libvirt.volume.scaleio.LibvirtScaleIOVolumeDriver',
187     'disco=nova.virt.libvirt.volume.disco.LibvirtDISCOVolumeDriver',
188     'vzstorage='
189         'nova.virt.libvirt.volume.vzstorage.LibvirtVZStorageVolumeDriver',
190     'veritas_hyperscale='
191         'nova.virt.libvirt.volume.vrtshyperscale.'
192         'LibvirtHyperScaleVolumeDriver',
193     'storpool=nova.virt.libvirt.volume.storpool.LibvirtStorPoolVolumeDriver',
194     'nvmeof=nova.virt.libvirt.volume.nvme.LibvirtNVMEVolumeDriver',
195 ]
196 
197 
198 def patch_tpool_proxy():
199     """eventlet.tpool.Proxy doesn't work with old-style class in __str__()
200     or __repr__() calls. See bug #962840 for details.
201     We perform a monkey patch to replace those two instance methods.
202     """
203     def str_method(self):
204         return str(self._obj)
205 
206     def repr_method(self):
207         return repr(self._obj)
208 
209     tpool.Proxy.__str__ = str_method
210     tpool.Proxy.__repr__ = repr_method
211 
212 
213 patch_tpool_proxy()
214 
215 # For information about when MIN_LIBVIRT_VERSION and
216 # NEXT_MIN_LIBVIRT_VERSION can be changed, consult
217 #
218 #   https://wiki.openstack.org/wiki/LibvirtDistroSupportMatrix
219 #
220 # Currently this is effectively the min version for i686/x86_64
221 # + KVM/QEMU, as other architectures/hypervisors require newer
222 # versions. Over time, this will become a common min version
223 # for all architectures/hypervisors, as this value rises to
224 # meet them.
225 MIN_LIBVIRT_VERSION = (3, 0, 0)
226 MIN_QEMU_VERSION = (2, 8, 0)
227 # TODO(berrange): Re-evaluate this at start of each release cycle
228 # to decide if we want to plan a future min version bump.
229 # MIN_LIBVIRT_VERSION can be updated to match this after
230 # NEXT_MIN_LIBVIRT_VERSION  has been at a higher value for
231 # one cycle
232 NEXT_MIN_LIBVIRT_VERSION = (4, 0, 0)
233 NEXT_MIN_QEMU_VERSION = (2, 11, 0)
234 
235 
236 # Virtuozzo driver support
237 MIN_VIRTUOZZO_VERSION = (7, 0, 0)
238 
239 # aarch64 architecture with KVM
240 # 'chardev' support got sorted out in 3.6.0
241 MIN_LIBVIRT_KVM_AARCH64_VERSION = (3, 6, 0)
242 
243 # Names of the types that do not get compressed during migration
244 NO_COMPRESSION_TYPES = ('qcow2',)
245 
246 
247 # number of serial console limit
248 QEMU_MAX_SERIAL_PORTS = 4
249 # Qemu supports 4 serial consoles, we remove 1 because of the PTY one defined
250 ALLOWED_QEMU_SERIAL_PORTS = QEMU_MAX_SERIAL_PORTS - 1
251 
252 MIN_LIBVIRT_OTHER_ARCH = {
253     fields.Architecture.AARCH64: MIN_LIBVIRT_KVM_AARCH64_VERSION,
254 }
255 
256 LIBVIRT_PERF_EVENT_PREFIX = 'VIR_PERF_PARAM_'
257 
258 PERF_EVENTS_CPU_FLAG_MAPPING = {'cmt': 'cmt',
259                                 'mbml': 'mbm_local',
260                                 'mbmt': 'mbm_total',
261                                }
262 
263 # Mediated devices support
264 MIN_LIBVIRT_MDEV_SUPPORT = (3, 4, 0)
265 
266 # libvirt>=3.10 is required for volume multiattach unless qemu<2.10.
267 # See https://bugzilla.redhat.com/show_bug.cgi?id=1378242
268 # for details.
269 MIN_LIBVIRT_MULTIATTACH = (3, 10, 0)
270 
271 MIN_LIBVIRT_FILE_BACKED_VERSION = (4, 0, 0)
272 MIN_QEMU_FILE_BACKED_VERSION = (2, 6, 0)
273 
274 MIN_LIBVIRT_FILE_BACKED_DISCARD_VERSION = (4, 4, 0)
275 MIN_QEMU_FILE_BACKED_DISCARD_VERSION = (2, 10, 0)
276 
277 MIN_LIBVIRT_NATIVE_TLS_VERSION = (4, 4, 0)
278 MIN_QEMU_NATIVE_TLS_VERSION = (2, 11, 0)
279 
280 # If the host has this libvirt version, then we skip the retry loop of
281 # instance destroy() call, as libvirt itself increased the wait time
282 # before the SIGKILL signal takes effect.
283 MIN_LIBVIRT_BETTER_SIGKILL_HANDLING = (4, 7, 0)
284 
285 VGPU_RESOURCE_SEMAPHORE = "vgpu_resources"
286 
287 # see https://libvirt.org/formatdomain.html#elementsVideo
288 MIN_LIBVIRT_VIDEO_MODEL_VERSIONS = {
289     fields.VideoModel.GOP: (3, 2, 0),
290     fields.VideoModel.NONE: (4, 6, 0),
291 }
292 
293 
294 # This is used to save the pmem namespace info temporarily
295 PMEMNamespace = collections.namedtuple('PMEMNamespace',
296         ['label', 'name', 'dev', 'size', 'align'])
297 
298 
299 class LibvirtDriver(driver.ComputeDriver):
300     def __init__(self, virtapi, read_only=False):
301         # NOTE(aspiers) Some of these are dynamic, so putting
302         # capabilities on the instance rather than on the class.
303         # This prevents the risk of one test setting a capability
304         # which bleeds over into other tests.
305 
306         # LVM and RBD require raw images. If we are not configured to
307         # force convert images into raw format, then we _require_ raw
308         # images only.
309         raw_only = ('rbd', 'lvm')
310         requires_raw_image = (CONF.libvirt.images_type in raw_only and
311                               not CONF.force_raw_images)
312 
313         self.capabilities = {
314             "has_imagecache": True,
315             "supports_evacuate": True,
316             "supports_migrate_to_same_host": False,
317             "supports_attach_interface": True,
318             "supports_device_tagging": True,
319             "supports_tagged_attach_interface": True,
320             "supports_tagged_attach_volume": True,
321             "supports_extend_volume": True,
322             # Multiattach support is conditional on qemu and libvirt versions
323             # determined in init_host.
324             "supports_multiattach": False,
325             "supports_trusted_certs": True,
326             # Supported image types
327             "supports_image_type_aki": True,
328             "supports_image_type_ari": True,
329             "supports_image_type_ami": True,
330             # FIXME(danms): I can see a future where people might want to
331             # configure certain compute nodes to not allow giant raw images
332             # to be booted (like nodes that are across a WAN). Thus, at some
333             # point we may want to be able to _not_ expose "supports raw" on
334             # some nodes by policy. Until then, raw is always supported.
335             "supports_image_type_raw": True,
336             "supports_image_type_iso": True,
337             # NOTE(danms): Certain backends do not work with complex image
338             # formats. If we are configured for those backends, then we
339             # should not expose the corresponding support traits.
340             "supports_image_type_qcow2": not requires_raw_image,
341         }
342         super(LibvirtDriver, self).__init__(virtapi)
343 
344         global libvirt
345         if libvirt is None:
346             libvirt = importutils.import_module('libvirt')
347             libvirt_migrate.libvirt = libvirt
348 
349         self._host = host.Host(self._uri(), read_only,
350                                lifecycle_event_handler=self.emit_event,
351                                conn_event_handler=self._handle_conn_event)
352         self._initiator = None
353         self._fc_wwnns = None
354         self._fc_wwpns = None
355         self._caps = None
356         self._supported_perf_events = []
357         self.firewall_driver = firewall.load_driver(
358             DEFAULT_FIREWALL_DRIVER,
359             host=self._host)
360 
361         self.vif_driver = libvirt_vif.LibvirtGenericVIFDriver()
362 
363         # TODO(mriedem): Long-term we should load up the volume drivers on
364         # demand as needed rather than doing this on startup, as there might
365         # be unsupported volume drivers in this list based on the underlying
366         # platform.
367         self.volume_drivers = self._get_volume_drivers()
368 
369         self._disk_cachemode = None
370         self.image_cache_manager = imagecache.ImageCacheManager()
371         self.image_backend = imagebackend.Backend(CONF.use_cow_images)
372 
373         self.disk_cachemodes = {}
374 
375         self.valid_cachemodes = ["default",
376                                  "none",
377                                  "writethrough",
378                                  "writeback",
379                                  "directsync",
380                                  "unsafe",
381                                 ]
382         self._conn_supports_start_paused = CONF.libvirt.virt_type in ('kvm',
383                                                                       'qemu')
384 
385         for mode_str in CONF.libvirt.disk_cachemodes:
386             disk_type, sep, cache_mode = mode_str.partition('=')
387             if cache_mode not in self.valid_cachemodes:
388                 LOG.warning('Invalid cachemode %(cache_mode)s specified '
389                             'for disk type %(disk_type)s.',
390                             {'cache_mode': cache_mode, 'disk_type': disk_type})
391                 continue
392             self.disk_cachemodes[disk_type] = cache_mode
393 
394         self._volume_api = cinder.API()
395         self._image_api = image.API()
396 
397         # The default choice for the sysinfo_serial config option is "unique"
398         # which does not have a special function since the value is just the
399         # instance.uuid.
400         sysinfo_serial_funcs = {
401             'none': lambda: None,
402             'hardware': self._get_host_sysinfo_serial_hardware,
403             'os': self._get_host_sysinfo_serial_os,
404             'auto': self._get_host_sysinfo_serial_auto,
405         }
406 
407         self._sysinfo_serial_func = sysinfo_serial_funcs.get(
408             CONF.libvirt.sysinfo_serial)
409 
410         self.job_tracker = instancejobtracker.InstanceJobTracker()
411         self._remotefs = remotefs.RemoteFilesystem()
412 
413         self._live_migration_flags = self._block_migration_flags = 0
414         self.active_migrations = {}
415 
416         # Compute reserved hugepages from conf file at the very
417         # beginning to ensure any syntax error will be reported and
418         # avoid any re-calculation when computing resources.
419         self._reserved_hugepages = hardware.numa_get_reserved_huge_pages()
420 
421         # Copy of the compute service ProviderTree object that is updated
422         # every time update_provider_tree() is called.
423         # NOTE(sbauza): We only want a read-only cache, this attribute is not
424         # intended to be updatable directly
425         self.provider_tree = None
426 
427         # get pmem namespaces info
428         self._pmem_namespaces = self._get_pmem_namespaces()
429 
430     def _get_pmem_namespaces(self):
431         if 'pmem_namespaces' not in CONF.libvirt or \
432                 not CONF.libvirt.pmem_namespaces:
433             return {}, {}
434         # pmem namespace dict {ns_name:PMEMNamespace,...}
435         pmems_by_name = {}
436         # pmem names group by label
437         pmem_names_by_label = collections.defaultdict(set)
438         pmems_host = self._get_pmem_namespaces_on_host()
439         for ns_conf in CONF.libvirt.pmem_namespaces:
440             try:
441                 ns_label, ns_names = ns_conf.split(":", 1)
442             except ValueError:
443                 reason = _("The configuration doesn't follow the format")
444                 raise exception.PMEMNamespaceConfigInvalid(
445                         reason=reason)
446             ns_names = ns_names.split("|")
447             for ns_name in ns_names:
448                 if ns_name not in pmems_host:
449                     reason = _("The PMEM namespace %s isn't on host") % ns_name
450                     raise exception.PMEMNamespaceConfigInvalid(
451                             reason=reason)
452                 if ns_name in pmems_by_name:
453                     reason = _("Duplicated pmem namespaces configured")
454                     raise exception.PMEMNamespaceConfigInvalid(
455                             reason=reason)
456                 pmem_ns_updated = pmems_host[ns_name]._replace(
457                                     label=ns_label)
458                 pmems_by_name[ns_name] = pmem_ns_updated
459                 pmem_names_by_label[ns_label].add(ns_name)
460         return pmems_by_name, pmem_names_by_label
461 
462     def _get_pmem_namespaces_on_host(self):
463         pmems_host = {}
464         nss = jsonutils.loads(nova.privsep.libvirt.get_pmem_namespaces())
465         for ns in nss:
466             if not ns.get('name'):
467                 continue
468             pmems_host[ns['name']] = PMEMNamespace(
469                     label=None,
470                     name=ns['name'],
471                     dev=ns['daxregion']['devices'][0]['chardev'],
472                     size=ns['size'],
473                     align=ns['daxregion']['align'])
474         return pmems_host
475 
476     def _get_assigned_pmem_names_by_label(self, context, instances):
477         assigned_pmem_names_by_label = collections.defaultdict(set)
478         vpmems = []
479         for item in instances:
480             inst = objects.Instance.get_by_uuid(
481                     context, item, expected_attrs=['vpmems'])
482             if 'vpmems' in inst and inst.vpmems:
483                 vpmems.extend(inst.vpmems.vpmems)
484         for vpmem in vpmems:
485             if 'ns_name' in vpmem and vpmem.ns_name:
486                 assigned_pmem_names_by_label[vpmem.label].add(vpmem.ns_name)
487         return assigned_pmem_names_by_label
488 
489     def _get_volume_drivers(self):
490         driver_registry = dict()
491 
492         for driver_str in libvirt_volume_drivers:
493             driver_type, _sep, driver = driver_str.partition('=')
494             driver_class = importutils.import_class(driver)
495             try:
496                 driver_registry[driver_type] = driver_class(self._host)
497             except brick_exception.InvalidConnectorProtocol:
498                 LOG.debug('Unable to load volume driver %s. It is not '
499                           'supported on this host.', driver)
500 
501         return driver_registry
502 
503     @property
504     def disk_cachemode(self):
505         # It can be confusing to understand the QEMU cache mode
506         # behaviour, because each cache=$MODE is a convenient shorthand
507         # to toggle _three_ cache.* booleans.  Consult the below table
508         # (quoting from the QEMU man page):
509         #
510         #              | cache.writeback | cache.direct | cache.no-flush
511         # --------------------------------------------------------------
512         # writeback    | on              | off          | off
513         # none         | on              | on           | off
514         # writethrough | off             | off          | off
515         # directsync   | off             | on           | off
516         # unsafe       | on              | off          | on
517         #
518         # Where:
519         #
520         #  - 'cache.writeback=off' means: QEMU adds an automatic fsync()
521         #    after each write request.
522         #
523         #  - 'cache.direct=on' means: Use Linux's O_DIRECT, i.e. bypass
524         #    the kernel page cache.  Caches in any other layer (disk
525         #    cache, QEMU metadata caches, etc.) can still be present.
526         #
527         #  - 'cache.no-flush=on' means: Ignore flush requests, i.e.
528         #    never call fsync(), even if the guest explicitly requested
529         #    it.
530         #
531         # Use cache mode "none" (cache.writeback=on, cache.direct=on,
532         # cache.no-flush=off) for consistent performance and
533         # migration correctness.  Some filesystems don't support
534         # O_DIRECT, though.  For those we fallback to the next
535         # reasonable option that is "writeback" (cache.writeback=on,
536         # cache.direct=off, cache.no-flush=off).
537 
538         if self._disk_cachemode is None:
539             self._disk_cachemode = "none"
540             if not nova.privsep.utils.supports_direct_io(CONF.instances_path):
541                 self._disk_cachemode = "writeback"
542         return self._disk_cachemode
543 
544     def _set_cache_mode(self, conf):
545         """Set cache mode on LibvirtConfigGuestDisk object."""
546         try:
547             source_type = conf.source_type
548             driver_cache = conf.driver_cache
549         except AttributeError:
550             return
551 
552         # Shareable disks like for a multi-attach volume need to have the
553         # driver cache disabled.
554         if getattr(conf, 'shareable', False):
555             conf.driver_cache = 'none'
556         else:
557             cache_mode = self.disk_cachemodes.get(source_type,
558                                                   driver_cache)
559             conf.driver_cache = cache_mode
560 
561     def _do_quality_warnings(self):
562         """Warn about potential configuration issues.
563 
564         This will log a warning message for things such as untested driver or
565         host arch configurations in order to indicate potential issues to
566         administrators.
567         """
568         caps = self._host.get_capabilities()
569         hostarch = caps.host.cpu.arch
570         if (CONF.libvirt.virt_type not in ('qemu', 'kvm') or
571             hostarch not in (fields.Architecture.I686,
572                              fields.Architecture.X86_64)):
573             LOG.warning('The libvirt driver is not tested on '
574                         '%(type)s/%(arch)s by the OpenStack project and '
575                         'thus its quality can not be ensured. For more '
576                         'information, see: https://docs.openstack.org/'
577                         'nova/latest/user/support-matrix.html',
578                         {'type': CONF.libvirt.virt_type, 'arch': hostarch})
579 
580         if CONF.vnc.keymap:
581             LOG.warning('The option "[vnc] keymap" has been deprecated '
582                         'in favor of configuration within the guest. '
583                         'Update nova.conf to address this change and '
584                         'refer to bug #1682020 for more information.')
585 
586         if CONF.spice.keymap:
587             LOG.warning('The option "[spice] keymap" has been deprecated '
588                         'in favor of configuration within the guest. '
589                         'Update nova.conf to address this change and '
590                         'refer to bug #1682020 for more information.')
591 
592     def _handle_conn_event(self, enabled, reason):
593         LOG.info("Connection event '%(enabled)d' reason '%(reason)s'",
594                  {'enabled': enabled, 'reason': reason})
595         self._set_host_enabled(enabled, reason)
596 
597     def _check_pmem_namespaces(self, context):
598         """This is making sure that, for every label:ns in every
599         existing Instance, a corresponding label:ns existed in the CONF.
600         """
601         instance_domains = self._host.list_instance_domains(
602                 only_running=False)
603         instances = [dom.UUIDString() for dom in instance_domains]
604         assigned_pmems_by_label = self._get_assigned_pmem_names_by_label(
605                 context, instances)
606         pmems, pmems_by_label = self._pmem_namespaces
607         for label, namespaces in assigned_pmems_by_label.items():
608             if label not in pmems_by_label:
609                 reason = _("some PMEM namespaces with label %(label)s are "
610                            "assigned, but the label isn't configured") % label
611                 raise exception.PMEMNamespaceConfigInvalid(reason=reason)
612             for ns in namespaces:
613                 if ns not in pmems_by_label[label]:
614                     reason = (_("PMEM namespace %(ns)s with label %(label)s"
615                                 "is assigned, but it isn't configured") %
616                               {'ns': ns, 'label': label})
617                     raise exception.PMEMNamespaceConfigInvalid(reason=reason)
618 
619     def init_host(self, host):
620         self._host.initialize()
621 
622         self._do_quality_warnings()
623 
624         self._parse_migration_flags()
625 
626         self._supported_perf_events = self._get_supported_perf_events()
627 
628         self._set_multiattach_support()
629 
630         self._check_file_backed_memory_support()
631 
632         self._check_pmem_namespaces(nova_context.get_admin_context())
633 
634         if (CONF.libvirt.virt_type == 'lxc' and
635                 not (CONF.libvirt.uid_maps and CONF.libvirt.gid_maps)):
636             LOG.warning("Running libvirt-lxc without user namespaces is "
637                         "dangerous. Containers spawned by Nova will be run "
638                         "as the host's root user. It is highly suggested "
639                         "that user namespaces be used in a public or "
640                         "multi-tenant environment.")
641 
642         # Stop libguestfs using KVM unless we're also configured
643         # to use this. This solves problem where people need to
644         # stop Nova use of KVM because nested-virt is broken
645         if CONF.libvirt.virt_type != "kvm":
646             guestfs.force_tcg()
647 
648         if not self._host.has_min_version(MIN_LIBVIRT_VERSION):
649             raise exception.InternalError(
650                 _('Nova requires libvirt version %s or greater.') %
651                 libvirt_utils.version_to_string(MIN_LIBVIRT_VERSION))
652 
653         if CONF.libvirt.virt_type in ("qemu", "kvm"):
654             if self._host.has_min_version(hv_ver=MIN_QEMU_VERSION):
655                 # "qemu-img info" calls are version dependent, so we need to
656                 # store the version in the images module.
657                 images.QEMU_VERSION = self._host.get_connection().getVersion()
658             else:
659                 raise exception.InternalError(
660                     _('Nova requires QEMU version %s or greater.') %
661                     libvirt_utils.version_to_string(MIN_QEMU_VERSION))
662 
663         if CONF.libvirt.virt_type == 'parallels':
664             if not self._host.has_min_version(hv_ver=MIN_VIRTUOZZO_VERSION):
665                 raise exception.InternalError(
666                     _('Nova requires Virtuozzo version %s or greater.') %
667                     libvirt_utils.version_to_string(MIN_VIRTUOZZO_VERSION))
668 
669         # Give the cloud admin a heads up if we are intending to
670         # change the MIN_LIBVIRT_VERSION in the next release.
671         if not self._host.has_min_version(NEXT_MIN_LIBVIRT_VERSION):
672             LOG.warning('Running Nova with a libvirt version less than '
673                         '%(version)s is deprecated. The required minimum '
674                         'version of libvirt will be raised to %(version)s '
675                         'in the next release.',
676                         {'version': libvirt_utils.version_to_string(
677                             NEXT_MIN_LIBVIRT_VERSION)})
678         if (CONF.libvirt.virt_type in ("qemu", "kvm") and
679             not self._host.has_min_version(hv_ver=NEXT_MIN_QEMU_VERSION)):
680             LOG.warning('Running Nova with a QEMU version less than '
681                         '%(version)s is deprecated. The required minimum '
682                         'version of QEMU will be raised to %(version)s '
683                         'in the next release.',
684                         {'version': libvirt_utils.version_to_string(
685                             NEXT_MIN_QEMU_VERSION)})
686 
687         kvm_arch = fields.Architecture.from_host()
688         if (CONF.libvirt.virt_type in ('kvm', 'qemu') and
689             kvm_arch in MIN_LIBVIRT_OTHER_ARCH and
690                 not self._host.has_min_version(
691                     MIN_LIBVIRT_OTHER_ARCH.get(kvm_arch))):
692             raise exception.InternalError(
693                 _('Running Nova with qemu/kvm virt_type on %(arch)s '
694                   'requires libvirt version %(libvirt_ver)s or greater') %
695                 {'arch': kvm_arch,
696                  'libvirt_ver': libvirt_utils.version_to_string(
697                      MIN_LIBVIRT_OTHER_ARCH.get(kvm_arch))})
698 
699         # Allowing both "tunnelling via libvirtd" (which will be
700         # deprecated once the MIN_{LIBVIRT,QEMU}_VERSION is sufficiently
701         # new enough) and "native TLS" options at the same time is
702         # nonsensical.
703         if (CONF.libvirt.live_migration_tunnelled and
704                 CONF.libvirt.live_migration_with_native_tls):
705             msg = _("Setting both 'live_migration_tunnelled' and "
706                     "'live_migration_with_native_tls' at the same "
707                     "time is invalid. If you have the relevant "
708                     "libvirt and QEMU versions, and TLS configured "
709                     "in your environment, pick "
710                     "'live_migration_with_native_tls'.")
711             raise exception.Invalid(msg)
712 
713         # Some imagebackends are only able to import raw disk images,
714         # and will fail if given any other format. See the bug
715         # https://bugs.launchpad.net/nova/+bug/1816686 for more details.
716         if CONF.libvirt.images_type in ('rbd',):
717             if not CONF.force_raw_images:
718                 msg = _("'[DEFAULT]/force_raw_images = False' is not "
719                         "allowed with '[libvirt]/images_type = rbd'. "
720                         "Please check the two configs and if you really "
721                         "do want to use rbd as images_type, set "
722                         "force_raw_images to True.")
723                 raise exception.InvalidConfiguration(msg)
724 
725         # TODO(sbauza): Remove this code once mediated devices are persisted
726         # across reboots.
727         if self._host.has_min_version(MIN_LIBVIRT_MDEV_SUPPORT):
728             self._recreate_assigned_mediated_devices()
729 
730     @staticmethod
731     def _is_existing_mdev(uuid):
732         # FIXME(sbauza): Some kernel can have a uevent race meaning that the
733         # libvirt daemon won't know when a mediated device is created unless
734         # you restart that daemon. Until all kernels we support are not having
735         # that possible race, check the sysfs directly instead of asking the
736         # libvirt API.
737         # See https://bugzilla.redhat.com/show_bug.cgi?id=1376907 for ref.
738         return os.path.exists('/sys/bus/mdev/devices/{0}'.format(uuid))
739 
740     def _recreate_assigned_mediated_devices(self):
741         """Recreate assigned mdevs that could have disappeared if we reboot
742         the host.
743         """
744         # FIXME(sbauza): We blindly recreate mediated devices without checking
745         # which ResourceProvider was allocated for the instance so it would use
746         # another pGPU.
747         # TODO(sbauza): Pass all instances' allocations here.
748         mdevs = self._get_all_assigned_mediated_devices()
749         requested_types = self._get_supported_vgpu_types()
750         for (mdev_uuid, instance_uuid) in six.iteritems(mdevs):
751             if not self._is_existing_mdev(mdev_uuid):
752                 self._create_new_mediated_device(requested_types, mdev_uuid)
753 
754     def _set_multiattach_support(self):
755         # Check to see if multiattach is supported. Based on bugzilla
756         # https://bugzilla.redhat.com/show_bug.cgi?id=1378242 and related
757         # clones, the shareable flag on a disk device will only work with
758         # qemu<2.10 or libvirt>=3.10. So check those versions here and set
759         # the capability appropriately.
760         if (self._host.has_min_version(lv_ver=MIN_LIBVIRT_MULTIATTACH) or
761                 not self._host.has_min_version(hv_ver=(2, 10, 0))):
762             self.capabilities['supports_multiattach'] = True
763         else:
764             LOG.debug('Volume multiattach is not supported based on current '
765                       'versions of QEMU and libvirt. QEMU must be less than '
766                       '2.10 or libvirt must be greater than or equal to 3.10.')
767 
768     def _check_file_backed_memory_support(self):
769         if CONF.libvirt.file_backed_memory:
770             # file_backed_memory is only compatible with qemu/kvm virts
771             if CONF.libvirt.virt_type not in ("qemu", "kvm"):
772                 raise exception.InternalError(
773                     _('Running Nova with file_backed_memory and virt_type '
774                       '%(type)s is not supported. file_backed_memory is only '
775                       'supported with qemu and kvm types.') %
776                     {'type': CONF.libvirt.virt_type})
777 
778             # Check needed versions for file_backed_memory
779             if not self._host.has_min_version(
780                     MIN_LIBVIRT_FILE_BACKED_VERSION,
781                     MIN_QEMU_FILE_BACKED_VERSION):
782                 raise exception.InternalError(
783                     _('Running Nova with file_backed_memory requires libvirt '
784                       'version %(libvirt)s and qemu version %(qemu)s') %
785                     {'libvirt': libvirt_utils.version_to_string(
786                         MIN_LIBVIRT_FILE_BACKED_VERSION),
787                     'qemu': libvirt_utils.version_to_string(
788                         MIN_QEMU_FILE_BACKED_VERSION)})
789 
790             # file-backed memory doesn't work with memory overcommit.
791             # Block service startup if file-backed memory is enabled and
792             # ram_allocation_ratio is not 1.0
793             if CONF.ram_allocation_ratio != 1.0:
794                 raise exception.InternalError(
795                     'Running Nova with file_backed_memory requires '
796                     'ram_allocation_ratio configured to 1.0')
797 
798     def _prepare_migration_flags(self):
799         migration_flags = 0
800 
801         migration_flags |= libvirt.VIR_MIGRATE_LIVE
802 
803         # Adding p2p flag only if xen is not in use, because xen does not
804         # support p2p migrations
805         if CONF.libvirt.virt_type != 'xen':
806             migration_flags |= libvirt.VIR_MIGRATE_PEER2PEER
807 
808         # Adding VIR_MIGRATE_UNDEFINE_SOURCE because, without it, migrated
809         # instance will remain defined on the source host
810         migration_flags |= libvirt.VIR_MIGRATE_UNDEFINE_SOURCE
811 
812         # Adding VIR_MIGRATE_PERSIST_DEST to persist the VM on the
813         # destination host
814         migration_flags |= libvirt.VIR_MIGRATE_PERSIST_DEST
815 
816         live_migration_flags = block_migration_flags = migration_flags
817 
818         # Adding VIR_MIGRATE_NON_SHARED_INC, otherwise all block-migrations
819         # will be live-migrations instead
820         block_migration_flags |= libvirt.VIR_MIGRATE_NON_SHARED_INC
821 
822         return (live_migration_flags, block_migration_flags)
823 
824     # TODO(kchamart) Once the MIN_LIBVIRT_VERSION and MIN_QEMU_VERSION
825     # reach 4.4.0 and 2.11.0, which provide "native TLS" support by
826     # default, deprecate and remove the support for "tunnelled live
827     # migration" (and related config attribute), because:
828     #
829     #  (a) it cannot handle live migration of disks in a non-shared
830     #      storage setup (a.k.a. "block migration");
831     #
832     #  (b) has a huge performance overhead and latency, because it burns
833     #      more CPU and memory bandwidth due to increased number of data
834     #      copies on both source and destination hosts.
835     #
836     # Both the above limitations are addressed by the QEMU-native TLS
837     # support (`live_migration_with_native_tls`).
838     def _handle_live_migration_tunnelled(self, migration_flags):
839         if CONF.libvirt.live_migration_tunnelled:
840             migration_flags |= libvirt.VIR_MIGRATE_TUNNELLED
841         return migration_flags
842 
843     def _is_native_tls_available(self):
844         return self._host.has_min_version(MIN_LIBVIRT_NATIVE_TLS_VERSION,
845                                           MIN_QEMU_NATIVE_TLS_VERSION)
846 
847     def _handle_native_tls(self, migration_flags):
848         if (CONF.libvirt.live_migration_with_native_tls and
849                 self._is_native_tls_available()):
850             migration_flags |= libvirt.VIR_MIGRATE_TLS
851         return migration_flags
852 
853     def _handle_live_migration_post_copy(self, migration_flags):
854         if CONF.libvirt.live_migration_permit_post_copy:
855             migration_flags |= libvirt.VIR_MIGRATE_POSTCOPY
856         return migration_flags
857 
858     def _handle_live_migration_auto_converge(self, migration_flags):
859         if self._is_post_copy_enabled(migration_flags):
860             LOG.info('The live_migration_permit_post_copy is set to '
861                      'True and post copy live migration is available '
862                      'so auto-converge will not be in use.')
863         elif CONF.libvirt.live_migration_permit_auto_converge:
864             migration_flags |= libvirt.VIR_MIGRATE_AUTO_CONVERGE
865         return migration_flags
866 
867     def _parse_migration_flags(self):
868         (live_migration_flags,
869             block_migration_flags) = self._prepare_migration_flags()
870 
871         live_migration_flags = self._handle_live_migration_tunnelled(
872             live_migration_flags)
873         block_migration_flags = self._handle_live_migration_tunnelled(
874             block_migration_flags)
875 
876         live_migration_flags = self._handle_native_tls(
877             live_migration_flags)
878         block_migration_flags = self._handle_native_tls(
879             block_migration_flags)
880 
881         live_migration_flags = self._handle_live_migration_post_copy(
882             live_migration_flags)
883         block_migration_flags = self._handle_live_migration_post_copy(
884             block_migration_flags)
885 
886         live_migration_flags = self._handle_live_migration_auto_converge(
887             live_migration_flags)
888         block_migration_flags = self._handle_live_migration_auto_converge(
889             block_migration_flags)
890 
891         self._live_migration_flags = live_migration_flags
892         self._block_migration_flags = block_migration_flags
893 
894     # TODO(sahid): This method is targeted for removal when the tests
895     # have been updated to avoid its use
896     #
897     # All libvirt API calls on the libvirt.Connect object should be
898     # encapsulated by methods on the nova.virt.libvirt.host.Host
899     # object, rather than directly invoking the libvirt APIs. The goal
900     # is to avoid a direct dependency on the libvirt API from the
901     # driver.py file.
902     def _get_connection(self):
903         return self._host.get_connection()
904 
905     _conn = property(_get_connection)
906 
907     @staticmethod
908     def _uri():
909         if CONF.libvirt.virt_type == 'uml':
910             uri = CONF.libvirt.connection_uri or 'uml:///system'
911         elif CONF.libvirt.virt_type == 'xen':
912             uri = CONF.libvirt.connection_uri or 'xen:///'
913         elif CONF.libvirt.virt_type == 'lxc':
914             uri = CONF.libvirt.connection_uri or 'lxc:///'
915         elif CONF.libvirt.virt_type == 'parallels':
916             uri = CONF.libvirt.connection_uri or 'parallels:///system'
917         else:
918             uri = CONF.libvirt.connection_uri or 'qemu:///system'
919         return uri
920 
921     @staticmethod
922     def _live_migration_uri(dest):
923         uris = {
924             'kvm': 'qemu+%s://%s/system',
925             'qemu': 'qemu+%s://%s/system',
926             'xen': 'xenmigr://%s/system',
927             'parallels': 'parallels+tcp://%s/system',
928         }
929         dest = oslo_netutils.escape_ipv6(dest)
930 
931         virt_type = CONF.libvirt.virt_type
932         # TODO(pkoniszewski): Remove fetching live_migration_uri in Pike
933         uri = CONF.libvirt.live_migration_uri
934         if uri:
935             return uri % dest
936 
937         uri = uris.get(virt_type)
938         if uri is None:
939             raise exception.LiveMigrationURINotAvailable(virt_type=virt_type)
940 
941         str_format = (dest,)
942         if virt_type in ('kvm', 'qemu'):
943             scheme = CONF.libvirt.live_migration_scheme or 'tcp'
944             str_format = (scheme, dest)
945         return uris.get(virt_type) % str_format
946 
947     @staticmethod
948     def _migrate_uri(dest):
949         uri = None
950         dest = oslo_netutils.escape_ipv6(dest)
951 
952         # Only QEMU live migrations supports migrate-uri parameter
953         virt_type = CONF.libvirt.virt_type
954         if virt_type in ('qemu', 'kvm'):
955             # QEMU accept two schemes: tcp and rdma.  By default
956             # libvirt build the URI using the remote hostname and the
957             # tcp schema.
958             uri = 'tcp://%s' % dest
959         # Because dest might be of type unicode, here we might return value of
960         # type unicode as well which is not acceptable by libvirt python
961         # binding when Python 2.7 is in use, so let's convert it explicitly
962         # back to string. When Python 3.x is in use, libvirt python binding
963         # accepts unicode type so it is completely fine to do a no-op str(uri)
964         # conversion which will return value of type unicode.
965         return uri and str(uri)
966 
967     def instance_exists(self, instance):
968         """Efficient override of base instance_exists method."""
969         try:
970             self._host.get_guest(instance)
971             return True
972         except (exception.InternalError, exception.InstanceNotFound):
973             return False
974 
975     def estimate_instance_overhead(self, instance_info):
976         overhead = super(LibvirtDriver, self).estimate_instance_overhead(
977             instance_info)
978         if isinstance(instance_info, objects.Flavor):
979             # A flavor object is passed during case of migrate
980             emu_policy = hardware.get_emulator_thread_policy_constraint(
981                 instance_info)
982             if emu_policy == fields.CPUEmulatorThreadsPolicy.ISOLATE:
983                 overhead['vcpus'] += 1
984         else:
985             # An instance object is passed during case of spawing or a
986             # dict is passed when computing resource for an instance
987             numa_topology = hardware.instance_topology_from_instance(
988                 instance_info)
989             if numa_topology and numa_topology.emulator_threads_isolated:
990                 overhead['vcpus'] += 1
991         return overhead
992 
993     def list_instances(self):
994         names = []
995         for guest in self._host.list_guests(only_running=False):
996             names.append(guest.name)
997 
998         return names
999 
1000     def list_instance_uuids(self):
1001         uuids = []
1002         for guest in self._host.list_guests(only_running=False):
1003             uuids.append(guest.uuid)
1004 
1005         return uuids
1006 
1007     def plug_vifs(self, instance, network_info):
1008         """Plug VIFs into networks."""
1009         for vif in network_info:
1010             self.vif_driver.plug(instance, vif)
1011 
1012     def _unplug_vifs(self, instance, network_info, ignore_errors):
1013         """Unplug VIFs from networks."""
1014         for vif in network_info:
1015             try:
1016                 self.vif_driver.unplug(instance, vif)
1017             except exception.NovaException:
1018                 if not ignore_errors:
1019                     raise
1020 
1021     def unplug_vifs(self, instance, network_info):
1022         self._unplug_vifs(instance, network_info, False)
1023 
1024     def _teardown_container(self, instance):
1025         inst_path = libvirt_utils.get_instance_path(instance)
1026         container_dir = os.path.join(inst_path, 'rootfs')
1027         rootfs_dev = instance.system_metadata.get('rootfs_device_name')
1028         LOG.debug('Attempting to teardown container at path %(dir)s with '
1029                   'root device: %(rootfs_dev)s',
1030                   {'dir': container_dir, 'rootfs_dev': rootfs_dev},
1031                   instance=instance)
1032         disk_api.teardown_container(container_dir, rootfs_dev)
1033 
1034     def _destroy(self, instance, attempt=1):
1035         try:
1036             guest = self._host.get_guest(instance)
1037             if CONF.serial_console.enabled:
1038                 # This method is called for several events: destroy,
1039                 # rebuild, hard-reboot, power-off - For all of these
1040                 # events we want to release the serial ports acquired
1041                 # for the guest before destroying it.
1042                 serials = self._get_serial_ports_from_guest(guest)
1043                 for hostname, port in serials:
1044                     serial_console.release_port(host=hostname, port=port)
1045         except exception.InstanceNotFound:
1046             guest = None
1047 
1048         # If the instance is already terminated, we're still happy
1049         # Otherwise, destroy it
1050         old_domid = -1
1051         if guest is not None:
1052             try:
1053                 old_domid = guest.id
1054                 guest.poweroff()
1055 
1056             except libvirt.libvirtError as e:
1057                 is_okay = False
1058                 errcode = e.get_error_code()
1059                 if errcode == libvirt.VIR_ERR_NO_DOMAIN:
1060                     # Domain already gone. This can safely be ignored.
1061                     is_okay = True
1062                 elif errcode == libvirt.VIR_ERR_OPERATION_INVALID:
1063                     # If the instance is already shut off, we get this:
1064                     # Code=55 Error=Requested operation is not valid:
1065                     # domain is not running
1066 
1067                     state = guest.get_power_state(self._host)
1068                     if state == power_state.SHUTDOWN:
1069                         is_okay = True
1070                 elif errcode == libvirt.VIR_ERR_INTERNAL_ERROR:
1071                     errmsg = e.get_error_message()
1072                     if (CONF.libvirt.virt_type == 'lxc' and
1073                         errmsg == 'internal error: '
1074                                   'Some processes refused to die'):
1075                         # Some processes in the container didn't die
1076                         # fast enough for libvirt. The container will
1077                         # eventually die. For now, move on and let
1078                         # the wait_for_destroy logic take over.
1079                         is_okay = True
1080                 elif errcode == libvirt.VIR_ERR_OPERATION_TIMEOUT:
1081                     LOG.warning("Cannot destroy instance, operation time out",
1082                                 instance=instance)
1083                     reason = _("operation time out")
1084                     raise exception.InstancePowerOffFailure(reason=reason)
1085                 elif errcode == libvirt.VIR_ERR_SYSTEM_ERROR:
1086                     if e.get_int1() == errno.EBUSY:
1087                         # NOTE(danpb): When libvirt kills a process it sends it
1088                         # SIGTERM first and waits 10 seconds. If it hasn't gone
1089                         # it sends SIGKILL and waits another 5 seconds. If it
1090                         # still hasn't gone then you get this EBUSY error.
1091                         # Usually when a QEMU process fails to go away upon
1092                         # SIGKILL it is because it is stuck in an
1093                         # uninterruptible kernel sleep waiting on I/O from
1094                         # some non-responsive server.
1095                         # Given the CPU load of the gate tests though, it is
1096                         # conceivable that the 15 second timeout is too short,
1097                         # particularly if the VM running tempest has a high
1098                         # steal time from the cloud host. ie 15 wallclock
1099                         # seconds may have passed, but the VM might have only
1100                         # have a few seconds of scheduled run time.
1101                         #
1102                         # TODO(kchamart): Once MIN_LIBVIRT_VERSION
1103                         # reaches v4.7.0, (a) rewrite the above note,
1104                         # and (b) remove the following code that retries
1105                         # _destroy() API call (which gives SIGKILL 30
1106                         # seconds to take effect) -- because from v4.7.0
1107                         # onwards, libvirt _automatically_ increases the
1108                         # timeout to 30 seconds.  This was added in the
1109                         # following libvirt commits:
1110                         #
1111                         #   - 9a4e4b942 (process: wait longer 5->30s on
1112                         #     hard shutdown)
1113                         #
1114                         #   - be2ca0444 (process: wait longer on kill
1115                         #     per assigned Hostdev)
1116                         with excutils.save_and_reraise_exception() as ctxt:
1117                             if not self._host.has_min_version(
1118                                     MIN_LIBVIRT_BETTER_SIGKILL_HANDLING):
1119                                 LOG.warning('Error from libvirt during '
1120                                             'destroy. Code=%(errcode)s '
1121                                             'Error=%(e)s; attempt '
1122                                             '%(attempt)d of 6 ',
1123                                             {'errcode': errcode, 'e': e,
1124                                              'attempt': attempt},
1125                                             instance=instance)
1126                                 # Try up to 6 times before giving up.
1127                                 if attempt < 6:
1128                                     ctxt.reraise = False
1129                                     self._destroy(instance, attempt + 1)
1130                                     return
1131 
1132                 if not is_okay:
1133                     with excutils.save_and_reraise_exception():
1134                         LOG.error('Error from libvirt during destroy. '
1135                                   'Code=%(errcode)s Error=%(e)s',
1136                                   {'errcode': errcode, 'e': e},
1137                                   instance=instance)
1138 
1139         def _wait_for_destroy(expected_domid):
1140             """Called at an interval until the VM is gone."""
1141             # NOTE(vish): If the instance disappears during the destroy
1142             #             we ignore it so the cleanup can still be
1143             #             attempted because we would prefer destroy to
1144             #             never fail.
1145             try:
1146                 dom_info = self.get_info(instance)
1147                 state = dom_info.state
1148                 new_domid = dom_info.internal_id
1149             except exception.InstanceNotFound:
1150                 LOG.debug("During wait destroy, instance disappeared.",
1151                           instance=instance)
1152                 state = power_state.SHUTDOWN
1153 
1154             if state == power_state.SHUTDOWN:
1155                 LOG.info("Instance destroyed successfully.", instance=instance)
1156                 raise loopingcall.LoopingCallDone()
1157 
1158             # NOTE(wangpan): If the instance was booted again after destroy,
1159             #                this may be an endless loop, so check the id of
1160             #                domain here, if it changed and the instance is
1161             #                still running, we should destroy it again.
1162             # see https://bugs.launchpad.net/nova/+bug/1111213 for more details
1163             if new_domid != expected_domid:
1164                 LOG.info("Instance may be started again.", instance=instance)
1165                 kwargs['is_running'] = True
1166                 raise loopingcall.LoopingCallDone()
1167 
1168         kwargs = {'is_running': False}
1169         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_destroy,
1170                                                      old_domid)
1171         timer.start(interval=0.5).wait()
1172         if kwargs['is_running']:
1173             LOG.info("Going to destroy instance again.", instance=instance)
1174             self._destroy(instance)
1175         else:
1176             # NOTE(GuanQiang): teardown container to avoid resource leak
1177             if CONF.libvirt.virt_type == 'lxc':
1178                 self._teardown_container(instance)
1179 
1180     def destroy(self, context, instance, network_info, block_device_info=None,
1181                 destroy_disks=True):
1182         self._destroy(instance)
1183         self.cleanup(context, instance, network_info, block_device_info,
1184                      destroy_disks)
1185 
1186     def _undefine_domain(self, instance):
1187         try:
1188             guest = self._host.get_guest(instance)
1189             try:
1190                 support_uefi = self._has_uefi_support()
1191                 guest.delete_configuration(support_uefi)
1192             except libvirt.libvirtError as e:
1193                 with excutils.save_and_reraise_exception() as ctxt:
1194                     errcode = e.get_error_code()
1195                     if errcode == libvirt.VIR_ERR_NO_DOMAIN:
1196                         LOG.debug("Called undefine, but domain already gone.",
1197                                   instance=instance)
1198                         ctxt.reraise = False
1199                     else:
1200                         LOG.error('Error from libvirt during undefine. '
1201                                   'Code=%(errcode)s Error=%(e)s',
1202                                   {'errcode': errcode,
1203                                    'e': encodeutils.exception_to_unicode(e)},
1204                                   instance=instance)
1205         except exception.InstanceNotFound:
1206             pass
1207 
1208     def cleanup(self, context, instance, network_info, block_device_info=None,
1209                 destroy_disks=True, migrate_data=None, destroy_vifs=True):
1210         if destroy_vifs:
1211             self._unplug_vifs(instance, network_info, True)
1212 
1213         # Continue attempting to remove firewall filters for the instance
1214         # until it's done or there is a failure to remove the filters. If
1215         # unfilter fails because the instance is not yet shutdown, try to
1216         # destroy the guest again and then retry the unfilter.
1217         while True:
1218             try:
1219                 self.unfilter_instance(instance, network_info)
1220                 break
1221             except libvirt.libvirtError as e:
1222                 try:
1223                     state = self.get_info(instance).state
1224                 except exception.InstanceNotFound:
1225                     state = power_state.SHUTDOWN
1226 
1227                 if state != power_state.SHUTDOWN:
1228                     LOG.warning("Instance may be still running, destroy "
1229                                 "it again.", instance=instance)
1230                     self._destroy(instance)
1231                 else:
1232                     errcode = e.get_error_code()
1233                     LOG.exception(_('Error from libvirt during unfilter. '
1234                                     'Code=%(errcode)s Error=%(e)s'),
1235                                   {'errcode': errcode, 'e': e},
1236                                   instance=instance)
1237                     reason = _("Error unfiltering instance.")
1238                     raise exception.InstanceTerminationFailure(reason=reason)
1239             except Exception:
1240                 raise
1241 
1242         # FIXME(wangpan): if the instance is booted again here, such as the
1243         #                 soft reboot operation boot it here, it will become
1244         #                 "running deleted", should we check and destroy it
1245         #                 at the end of this method?
1246 
1247         # NOTE(vish): we disconnect from volumes regardless
1248         block_device_mapping = driver.block_device_info_get_mapping(
1249             block_device_info)
1250         for vol in block_device_mapping:
1251             connection_info = vol['connection_info']
1252             disk_dev = vol['mount_device']
1253             if disk_dev is not None:
1254                 disk_dev = disk_dev.rpartition("/")[2]
1255             try:
1256                 self._disconnect_volume(context, connection_info, instance)
1257             except Exception as exc:
1258                 with excutils.save_and_reraise_exception() as ctxt:
1259                     if destroy_disks:
1260                         # Don't block on Volume errors if we're trying to
1261                         # delete the instance as we may be partially created
1262                         # or deleted
1263                         ctxt.reraise = False
1264                         LOG.warning(
1265                             "Ignoring Volume Error on vol %(vol_id)s "
1266                             "during delete %(exc)s",
1267                             {'vol_id': vol.get('volume_id'),
1268                              'exc': encodeutils.exception_to_unicode(exc)},
1269                             instance=instance)
1270 
1271         if destroy_disks:
1272             # NOTE(haomai): destroy volumes if needed
1273             if CONF.libvirt.images_type == 'lvm':
1274                 self._cleanup_lvm(instance, block_device_info)
1275             if CONF.libvirt.images_type == 'rbd':
1276                 self._cleanup_rbd(instance)
1277 
1278         is_shared_block_storage = False
1279         if migrate_data and 'is_shared_block_storage' in migrate_data:
1280             is_shared_block_storage = migrate_data.is_shared_block_storage
1281         # NOTE(lyarwood): The following workaround allows operators to ensure
1282         # that non-shared instance directories are removed after an evacuation
1283         # or revert resize when using the shared RBD imagebackend. This
1284         # workaround is not required when cleaning up migrations that provide
1285         # migrate_data to this method as the existing is_shared_block_storage
1286         # conditional will cause the instance directory to be removed.
1287         if ((destroy_disks or is_shared_block_storage) or
1288             (CONF.workarounds.ensure_libvirt_rbd_instance_dir_cleanup and
1289              CONF.libvirt.images_type == 'rbd')):
1290 
1291             attempts = int(instance.system_metadata.get('clean_attempts',
1292                                                         '0'))
1293             success = self.delete_instance_files(instance)
1294             # NOTE(mriedem): This is used in the _run_pending_deletes periodic
1295             # task in the compute manager. The tight coupling is not great...
1296             instance.system_metadata['clean_attempts'] = str(attempts + 1)
1297             if success:
1298                 instance.cleaned = True
1299             instance.save()
1300 
1301         self._undefine_domain(instance)
1302 
1303     def _detach_encrypted_volumes(self, instance, block_device_info):
1304         """Detaches encrypted volumes attached to instance."""
1305         disks = self._get_instance_disk_info(instance, block_device_info)
1306         encrypted_volumes = filter(dmcrypt.is_encrypted,
1307                                    [disk['path'] for disk in disks])
1308         for path in encrypted_volumes:
1309             dmcrypt.delete_volume(path)
1310 
1311     def _get_serial_ports_from_guest(self, guest, mode=None):
1312         """Returns an iterator over serial port(s) configured on guest.
1313 
1314         :param mode: Should be a value in (None, bind, connect)
1315         """
1316         xml = guest.get_xml_desc()
1317         tree = etree.fromstring(xml)
1318 
1319         # The 'serial' device is the base for x86 platforms. Other platforms
1320         # (e.g. kvm on system z = S390X) can only use 'console' devices.
1321         xpath_mode = "[@mode='%s']" % mode if mode else ""
1322         serial_tcp = "./devices/serial[@type='tcp']/source" + xpath_mode
1323         console_tcp = "./devices/console[@type='tcp']/source" + xpath_mode
1324 
1325         tcp_devices = tree.findall(serial_tcp)
1326         if len(tcp_devices) == 0:
1327             tcp_devices = tree.findall(console_tcp)
1328         for source in tcp_devices:
1329             yield (source.get("host"), int(source.get("service")))
1330 
1331     def _get_scsi_controller_max_unit(self, guest):
1332         """Returns the max disk unit used by scsi controller"""
1333         xml = guest.get_xml_desc()
1334         tree = etree.fromstring(xml)
1335         addrs = "./devices/disk[@device='disk']/address[@type='drive']"
1336 
1337         ret = []
1338         for obj in tree.findall(addrs):
1339             ret.append(int(obj.get('unit', 0)))
1340         return max(ret)
1341 
1342     @staticmethod
1343     def _get_rbd_driver():
1344         return rbd_utils.RBDDriver(
1345                 pool=CONF.libvirt.images_rbd_pool,
1346                 ceph_conf=CONF.libvirt.images_rbd_ceph_conf,
1347                 rbd_user=CONF.libvirt.rbd_user,
1348                 rbd_connect_timeout=CONF.libvirt.rbd_connect_timeout)
1349 
1350     def _cleanup_rbd(self, instance):
1351         # NOTE(nic): On revert_resize, the cleanup steps for the root
1352         # volume are handled with an "rbd snap rollback" command,
1353         # and none of this is needed (and is, in fact, harmful) so
1354         # filter out non-ephemerals from the list
1355         if instance.task_state == task_states.RESIZE_REVERTING:
1356             filter_fn = lambda disk: (disk.startswith(instance.uuid) and
1357                                       disk.endswith('disk.local'))
1358         else:
1359             filter_fn = lambda disk: disk.startswith(instance.uuid)
1360         LibvirtDriver._get_rbd_driver().cleanup_volumes(filter_fn)
1361 
1362     def _cleanup_lvm(self, instance, block_device_info):
1363         """Delete all LVM disks for given instance object."""
1364         if instance.get('ephemeral_key_uuid') is not None:
1365             self._detach_encrypted_volumes(instance, block_device_info)
1366 
1367         disks = self._lvm_disks(instance)
1368         if disks:
1369             lvm.remove_volumes(disks)
1370 
1371     def _lvm_disks(self, instance):
1372         """Returns all LVM disks for given instance object."""
1373         if CONF.libvirt.images_volume_group:
1374             vg = os.path.join('/dev', CONF.libvirt.images_volume_group)
1375             if not os.path.exists(vg):
1376                 return []
1377             pattern = '%s_' % instance.uuid
1378 
1379             def belongs_to_instance(disk):
1380                 return disk.startswith(pattern)
1381 
1382             def fullpath(name):
1383                 return os.path.join(vg, name)
1384 
1385             logical_volumes = lvm.list_volumes(vg)
1386 
1387             disks = [fullpath(disk) for disk in logical_volumes
1388                      if belongs_to_instance(disk)]
1389             return disks
1390         return []
1391 
1392     def get_volume_connector(self, instance):
1393         root_helper = utils.get_root_helper()
1394         return connector.get_connector_properties(
1395             root_helper, CONF.my_block_storage_ip,
1396             CONF.libvirt.volume_use_multipath,
1397             enforce_multipath=True,
1398             host=CONF.host)
1399 
1400     def _cleanup_resize(self, context, instance, network_info):
1401         inst_base = libvirt_utils.get_instance_path(instance)
1402         target = inst_base + '_resize'
1403 
1404         # Deletion can fail over NFS, so retry the deletion as required.
1405         # Set maximum attempt as 5, most test can remove the directory
1406         # for the second time.
1407         attempts = 0
1408         while(os.path.exists(target) and attempts < 5):
1409             shutil.rmtree(target, ignore_errors=True)
1410             if os.path.exists(target):
1411                 time.sleep(random.randint(20, 200) / 100.0)
1412             attempts += 1
1413 
1414         # NOTE(mriedem): Some image backends will recreate the instance path
1415         # and disk.info during init, and all we need the root disk for
1416         # here is removing cloned snapshots which is backend-specific, so
1417         # check that first before initializing the image backend object. If
1418         # there is ever an image type that supports clone *and* re-creates
1419         # the instance directory and disk.info on init, this condition will
1420         # need to be re-visited to make sure that backend doesn't re-create
1421         # the disk. Refer to bugs: 1666831 1728603 1769131
1422         if self.image_backend.backend(CONF.libvirt.images_type).SUPPORTS_CLONE:
1423             root_disk = self.image_backend.by_name(instance, 'disk')
1424             if root_disk.exists():
1425                 root_disk.remove_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME)
1426 
1427         if instance.host != CONF.host:
1428             self._undefine_domain(instance)
1429             self.unplug_vifs(instance, network_info)
1430             self.unfilter_instance(instance, network_info)
1431 
1432     def _get_volume_driver(self, connection_info):
1433         driver_type = connection_info.get('driver_volume_type')
1434         if driver_type not in self.volume_drivers:
1435             raise exception.VolumeDriverNotFound(driver_type=driver_type)
1436         return self.volume_drivers[driver_type]
1437 
1438     def _connect_volume(self, context, connection_info, instance,
1439                         encryption=None, allow_native_luks=True):
1440         vol_driver = self._get_volume_driver(connection_info)
1441         vol_driver.connect_volume(connection_info, instance)
1442         try:
1443             self._attach_encryptor(
1444                 context, connection_info, encryption, allow_native_luks)
1445         except Exception:
1446             # Encryption failed so rollback the volume connection.
1447             with excutils.save_and_reraise_exception(logger=LOG):
1448                 LOG.exception("Failure attaching encryptor; rolling back "
1449                               "volume connection", instance=instance)
1450                 vol_driver.disconnect_volume(connection_info, instance)
1451 
1452     def _should_disconnect_target(self, context, connection_info, instance):
1453         connection_count = 0
1454 
1455         # NOTE(jdg): Multiattach is a special case (not to be confused
1456         # with shared_targets). With multiattach we may have a single volume
1457         # attached multiple times to *this* compute node (ie Server-1 and
1458         # Server-2).  So, if we receive a call to delete the attachment for
1459         # Server-1 we need to take special care to make sure that the Volume
1460         # isn't also attached to another Server on this Node.  Otherwise we
1461         # will indiscriminantly delete the connection for all Server and that's
1462         # no good.  So check if it's attached multiple times on this node
1463         # if it is we skip the call to brick to delete the connection.
1464         if connection_info.get('multiattach', False):
1465             volume = self._volume_api.get(
1466                 context,
1467                 driver_block_device.get_volume_id(connection_info))
1468             attachments = volume.get('attachments', {})
1469             if len(attachments) > 1:
1470                 # First we get a list of all Server UUID's associated with
1471                 # this Host (Compute Node).  We're going to use this to
1472                 # determine if the Volume being detached is also in-use by
1473                 # another Server on this Host, ie just check to see if more
1474                 # than one attachment.server_id for this volume is in our
1475                 # list of Server UUID's for this Host
1476                 servers_this_host = objects.InstanceList.get_uuids_by_host(
1477                     context, instance.host)
1478 
1479                 # NOTE(jdg): nova.volume.cinder translates the
1480                 # volume['attachments'] response into a dict which includes
1481                 # the Server UUID as the key, so we're using that
1482                 # here to check against our server_this_host list
1483                 for server_id, data in attachments.items():
1484                     if server_id in servers_this_host:
1485                         connection_count += 1
1486         return (False if connection_count > 1 else True)
1487 
1488     def _disconnect_volume(self, context, connection_info, instance,
1489                            encryption=None):
1490         self._detach_encryptor(context, connection_info, encryption=encryption)
1491         if self._should_disconnect_target(context, connection_info, instance):
1492             vol_driver = self._get_volume_driver(connection_info)
1493             vol_driver.disconnect_volume(connection_info, instance)
1494         else:
1495             LOG.info("Detected multiple connections on this host for volume: "
1496                      "%s, skipping target disconnect.",
1497                      driver_block_device.get_volume_id(connection_info),
1498                      instance=instance)
1499 
1500     def _extend_volume(self, connection_info, instance, requested_size):
1501         vol_driver = self._get_volume_driver(connection_info)
1502         return vol_driver.extend_volume(connection_info, instance,
1503                                         requested_size)
1504 
1505     def _use_native_luks(self, encryption=None):
1506         """Check if LUKS is the required 'provider'
1507         """
1508         provider = None
1509         if encryption:
1510             provider = encryption.get('provider', None)
1511         if provider in encryptors.LEGACY_PROVIDER_CLASS_TO_FORMAT_MAP:
1512             provider = encryptors.LEGACY_PROVIDER_CLASS_TO_FORMAT_MAP[provider]
1513         return provider == encryptors.LUKS
1514 
1515     def _get_volume_config(self, connection_info, disk_info):
1516         vol_driver = self._get_volume_driver(connection_info)
1517         conf = vol_driver.get_config(connection_info, disk_info)
1518         self._set_cache_mode(conf)
1519         return conf
1520 
1521     def _get_volume_encryptor(self, connection_info, encryption):
1522         root_helper = utils.get_root_helper()
1523         return encryptors.get_volume_encryptor(root_helper=root_helper,
1524                                                keymgr=key_manager.API(CONF),
1525                                                connection_info=connection_info,
1526                                                **encryption)
1527 
1528     def _get_volume_encryption(self, context, connection_info):
1529         """Get the encryption metadata dict if it is not provided
1530         """
1531         encryption = {}
1532         volume_id = driver_block_device.get_volume_id(connection_info)
1533         if volume_id:
1534             encryption = encryptors.get_encryption_metadata(context,
1535                             self._volume_api, volume_id, connection_info)
1536         return encryption
1537 
1538     def _attach_encryptor(self, context, connection_info, encryption,
1539                           allow_native_luks):
1540         """Attach the frontend encryptor if one is required by the volume.
1541 
1542         The request context is only used when an encryption metadata dict is
1543         not provided. The encryption metadata dict being populated is then used
1544         to determine if an attempt to attach the encryptor should be made.
1545 
1546         If native LUKS decryption is enabled then create a Libvirt volume
1547         secret containing the LUKS passphrase for the volume.
1548         """
1549         if encryption is None:
1550             encryption = self._get_volume_encryption(context, connection_info)
1551 
1552         if (encryption and allow_native_luks and
1553             self._use_native_luks(encryption)):
1554             # NOTE(lyarwood): Fetch the associated key for the volume and
1555             # decode the passphrase from the key.
1556             # FIXME(lyarwood): c-vol currently creates symmetric keys for use
1557             # with volumes, leading to the binary to hex to string conversion
1558             # below.
1559             keymgr = key_manager.API(CONF)
1560             key = keymgr.get(context, encryption['encryption_key_id'])
1561             key_encoded = key.get_encoded()
1562             passphrase = binascii.hexlify(key_encoded).decode('utf-8')
1563 
1564             # NOTE(lyarwood): Retain the behaviour of the original os-brick
1565             # encryptors and format any volume that does not identify as
1566             # encrypted with LUKS.
1567             # FIXME(lyarwood): Remove this once c-vol correctly formats
1568             # encrypted volumes during their initial creation:
1569             # https://bugs.launchpad.net/cinder/+bug/1739442
1570             device_path = connection_info.get('data').get('device_path')
1571             if device_path:
1572                 root_helper = utils.get_root_helper()
1573                 if not luks_encryptor.is_luks(root_helper, device_path):
1574                     encryptor = self._get_volume_encryptor(connection_info,
1575                                                            encryption)
1576                     encryptor._format_volume(passphrase, **encryption)
1577 
1578             # NOTE(lyarwood): Store the passphrase as a libvirt secret locally
1579             # on the compute node. This secret is used later when generating
1580             # the volume config.
1581             volume_id = driver_block_device.get_volume_id(connection_info)
1582             self._host.create_secret('volume', volume_id, password=passphrase)
1583         elif encryption:
1584             encryptor = self._get_volume_encryptor(connection_info,
1585                                                    encryption)
1586             encryptor.attach_volume(context, **encryption)
1587 
1588     def _detach_encryptor(self, context, connection_info, encryption):
1589         """Detach the frontend encryptor if one is required by the volume.
1590 
1591         The request context is only used when an encryption metadata dict is
1592         not provided. The encryption metadata dict being populated is then used
1593         to determine if an attempt to detach the encryptor should be made.
1594 
1595         If native LUKS decryption is enabled then delete previously created
1596         Libvirt volume secret from the host.
1597         """
1598         volume_id = driver_block_device.get_volume_id(connection_info)
1599         if volume_id and self._host.find_secret('volume', volume_id):
1600             return self._host.delete_secret('volume', volume_id)
1601         if encryption is None:
1602             encryption = self._get_volume_encryption(context, connection_info)
1603         # NOTE(lyarwood): Handle bug #1821696 where volume secrets have been
1604         # removed manually by returning if native LUKS decryption is available
1605         # and device_path is not present in the connection_info. This avoids
1606         # VolumeEncryptionNotSupported being thrown when we incorrectly build
1607         # the encryptor below due to the secrets not being present above.
1608         if (encryption and self._use_native_luks(encryption) and
1609             not connection_info['data'].get('device_path')):
1610             return
1611         if encryption:
1612             encryptor = self._get_volume_encryptor(connection_info,
1613                                                    encryption)
1614             encryptor.detach_volume(**encryption)
1615 
1616     def _check_discard_for_attach_volume(self, conf, instance):
1617         """Perform some checks for volumes configured for discard support.
1618 
1619         If discard is configured for the volume, and the guest is using a
1620         configuration known to not work, we will log a message explaining
1621         the reason why.
1622         """
1623         if conf.driver_discard == 'unmap' and conf.target_bus == 'virtio':
1624             LOG.debug('Attempting to attach volume %(id)s with discard '
1625                       'support enabled to an instance using an '
1626                       'unsupported configuration. target_bus = '
1627                       '%(bus)s. Trim commands will not be issued to '
1628                       'the storage device.',
1629                       {'bus': conf.target_bus,
1630                        'id': conf.serial},
1631                       instance=instance)
1632 
1633     def attach_volume(self, context, connection_info, instance, mountpoint,
1634                       disk_bus=None, device_type=None, encryption=None):
1635         guest = self._host.get_guest(instance)
1636 
1637         disk_dev = mountpoint.rpartition("/")[2]
1638         bdm = {
1639             'device_name': disk_dev,
1640             'disk_bus': disk_bus,
1641             'device_type': device_type}
1642 
1643         # Note(cfb): If the volume has a custom block size, check that
1644         #            that we are using QEMU/KVM and libvirt >= 0.10.2. The
1645         #            presence of a block size is considered mandatory by
1646         #            cinder so we fail if we can't honor the request.
1647         data = {}
1648         if ('data' in connection_info):
1649             data = connection_info['data']
1650         if ('logical_block_size' in data or 'physical_block_size' in data):
1651             if ((CONF.libvirt.virt_type != "kvm" and
1652                  CONF.libvirt.virt_type != "qemu")):
1653                 msg = _("Volume sets block size, but the current "
1654                         "libvirt hypervisor '%s' does not support custom "
1655                         "block size") % CONF.libvirt.virt_type
1656                 raise exception.InvalidHypervisorType(msg)
1657 
1658         self._connect_volume(context, connection_info, instance,
1659                              encryption=encryption)
1660         disk_info = blockinfo.get_info_from_bdm(
1661             instance, CONF.libvirt.virt_type, instance.image_meta, bdm)
1662         if disk_info['bus'] == 'scsi':
1663             disk_info['unit'] = self._get_scsi_controller_max_unit(guest) + 1
1664 
1665         conf = self._get_volume_config(connection_info, disk_info)
1666 
1667         self._check_discard_for_attach_volume(conf, instance)
1668 
1669         try:
1670             state = guest.get_power_state(self._host)
1671             live = state in (power_state.RUNNING, power_state.PAUSED)
1672 
1673             guest.attach_device(conf, persistent=True, live=live)
1674             # NOTE(artom) If we're attaching with a device role tag, we need to
1675             # rebuild device_metadata. If we're attaching without a role
1676             # tag, we're rebuilding it here needlessly anyways. This isn't a
1677             # massive deal, and it helps reduce code complexity by not having
1678             # to indicate to the virt driver that the attach is tagged. The
1679             # really important optimization of not calling the database unless
1680             # device_metadata has actually changed is done for us by
1681             # instance.save().
1682             instance.device_metadata = self._build_device_metadata(
1683                 context, instance)
1684             instance.save()
1685 
1686         # TODO(lyarwood) Remove the following breadcrumb once all supported
1687         # distributions provide Libvirt 3.3.0 or earlier with
1688         # https://libvirt.org/git/?p=libvirt.git;a=commit;h=7189099 applied.
1689         except libvirt.libvirtError as ex:
1690             with excutils.save_and_reraise_exception():
1691                 if 'Incorrect number of padding bytes' in six.text_type(ex):
1692                     LOG.warning(_('Failed to attach encrypted volume due to a '
1693                                   'known Libvirt issue, see the following bug '
1694                                   'for details: '
1695                                   'https://bugzilla.redhat.com/1447297'))
1696                 else:
1697                     LOG.exception(_('Failed to attach volume at mountpoint: '
1698                                     '%s'), mountpoint, instance=instance)
1699                 self._disconnect_volume(context, connection_info, instance,
1700                                         encryption=encryption)
1701         except Exception:
1702             LOG.exception(_('Failed to attach volume at mountpoint: %s'),
1703                           mountpoint, instance=instance)
1704             with excutils.save_and_reraise_exception():
1705                 self._disconnect_volume(context, connection_info, instance,
1706                                         encryption=encryption)
1707 
1708     def _swap_volume(self, guest, disk_path, conf, resize_to):
1709         """Swap existing disk with a new block device."""
1710         dev = guest.get_block_device(disk_path)
1711 
1712         # Save a copy of the domain's persistent XML file. We'll use this
1713         # to redefine the domain if anything fails during the volume swap.
1714         xml = guest.get_xml_desc(dump_inactive=True, dump_sensitive=True)
1715 
1716         # Abort is an idempotent operation, so make sure any block
1717         # jobs which may have failed are ended.
1718         try:
1719             dev.abort_job()
1720         except Exception:
1721             pass
1722 
1723         try:
1724             # NOTE (rmk): blockRebase cannot be executed on persistent
1725             #             domains, so we need to temporarily undefine it.
1726             #             If any part of this block fails, the domain is
1727             #             re-defined regardless.
1728             if guest.has_persistent_configuration():
1729                 support_uefi = self._has_uefi_support()
1730                 guest.delete_configuration(support_uefi)
1731 
1732             try:
1733                 # Start copy with VIR_DOMAIN_BLOCK_REBASE_REUSE_EXT flag to
1734                 # allow writing to existing external volume file. Use
1735                 # VIR_DOMAIN_BLOCK_REBASE_COPY_DEV if it's a block device to
1736                 # make sure XML is generated correctly (bug 1691195)
1737                 copy_dev = conf.source_type == 'block'
1738                 dev.rebase(conf.source_path, copy=True, reuse_ext=True,
1739                            copy_dev=copy_dev)
1740                 while not dev.is_job_complete():
1741                     time.sleep(0.5)
1742 
1743                 dev.abort_job(pivot=True)
1744 
1745             except Exception as exc:
1746                 LOG.exception("Failure rebasing volume %(new_path)s on "
1747                     "%(old_path)s.", {'new_path': conf.source_path,
1748                                       'old_path': disk_path})
1749                 raise exception.VolumeRebaseFailed(reason=six.text_type(exc))
1750 
1751             if resize_to:
1752                 dev.resize(resize_to * units.Gi / units.Ki)
1753 
1754             # Make sure we will redefine the domain using the updated
1755             # configuration after the volume was swapped. The dump_inactive
1756             # keyword arg controls whether we pull the inactive (persistent)
1757             # or active (live) config from the domain. We want to pull the
1758             # live config after the volume was updated to use when we redefine
1759             # the domain.
1760             xml = guest.get_xml_desc(dump_inactive=False, dump_sensitive=True)
1761         finally:
1762             self._host.write_instance_config(xml)
1763 
1764     def swap_volume(self, context, old_connection_info,
1765                     new_connection_info, instance, mountpoint, resize_to):
1766 
1767         # NOTE(lyarwood): https://bugzilla.redhat.com/show_bug.cgi?id=760547
1768         old_encrypt = self._get_volume_encryption(context, old_connection_info)
1769         new_encrypt = self._get_volume_encryption(context, new_connection_info)
1770         if ((old_encrypt and self._use_native_luks(old_encrypt)) or
1771             (new_encrypt and self._use_native_luks(new_encrypt))):
1772             raise NotImplementedError(_("Swap volume is not supported for "
1773                 "encrypted volumes when native LUKS decryption is enabled."))
1774 
1775         guest = self._host.get_guest(instance)
1776 
1777         disk_dev = mountpoint.rpartition("/")[2]
1778         if not guest.get_disk(disk_dev):
1779             raise exception.DiskNotFound(location=disk_dev)
1780         disk_info = {
1781             'dev': disk_dev,
1782             'bus': blockinfo.get_disk_bus_for_disk_dev(
1783                 CONF.libvirt.virt_type, disk_dev),
1784             'type': 'disk',
1785             }
1786         # NOTE (lyarwood): new_connection_info will be modified by the
1787         # following _connect_volume call down into the volume drivers. The
1788         # majority of the volume drivers will add a device_path that is in turn
1789         # used by _get_volume_config to set the source_path of the
1790         # LibvirtConfigGuestDisk object it returns. We do not explicitly save
1791         # this to the BDM here as the upper compute swap_volume method will
1792         # eventually do this for us.
1793         self._connect_volume(context, new_connection_info, instance)
1794         conf = self._get_volume_config(new_connection_info, disk_info)
1795         if not conf.source_path:
1796             self._disconnect_volume(context, new_connection_info, instance)
1797             raise NotImplementedError(_("Swap only supports host devices"))
1798 
1799         try:
1800             self._swap_volume(guest, disk_dev, conf, resize_to)
1801         except exception.VolumeRebaseFailed:
1802             with excutils.save_and_reraise_exception():
1803                 self._disconnect_volume(context, new_connection_info, instance)
1804 
1805         self._disconnect_volume(context, old_connection_info, instance)
1806 
1807     def _get_existing_domain_xml(self, instance, network_info,
1808                                  block_device_info=None):
1809         try:
1810             guest = self._host.get_guest(instance)
1811             xml = guest.get_xml_desc()
1812         except exception.InstanceNotFound:
1813             disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
1814                                                 instance,
1815                                                 instance.image_meta,
1816                                                 block_device_info)
1817             xml = self._get_guest_xml(nova_context.get_admin_context(),
1818                                       instance, network_info, disk_info,
1819                                       instance.image_meta,
1820                                       block_device_info=block_device_info)
1821         return xml
1822 
1823     def detach_volume(self, context, connection_info, instance, mountpoint,
1824                       encryption=None):
1825         disk_dev = mountpoint.rpartition("/")[2]
1826         try:
1827             guest = self._host.get_guest(instance)
1828 
1829             state = guest.get_power_state(self._host)
1830             live = state in (power_state.RUNNING, power_state.PAUSED)
1831             # NOTE(lyarwood): The volume must be detached from the VM before
1832             # detaching any attached encryptors or disconnecting the underlying
1833             # volume in _disconnect_volume. Otherwise, the encryptor or volume
1834             # driver may report that the volume is still in use.
1835             wait_for_detach = guest.detach_device_with_retry(guest.get_disk,
1836                                                              disk_dev,
1837                                                              live=live)
1838             wait_for_detach()
1839 
1840         except exception.InstanceNotFound:
1841             # NOTE(zhaoqin): If the instance does not exist, _lookup_by_name()
1842             #                will throw InstanceNotFound exception. Need to
1843             #                disconnect volume under this circumstance.
1844             LOG.warning("During detach_volume, instance disappeared.",
1845                         instance=instance)
1846         except exception.DeviceNotFound:
1847             # We should still try to disconnect logical device from
1848             # host, an error might have happened during a previous
1849             # call.
1850             LOG.info("Device %s not found in instance.",
1851                      disk_dev, instance=instance)
1852         except libvirt.libvirtError as ex:
1853             # NOTE(vish): This is called to cleanup volumes after live
1854             #             migration, so we should still disconnect even if
1855             #             the instance doesn't exist here anymore.
1856             error_code = ex.get_error_code()
1857             if error_code == libvirt.VIR_ERR_NO_DOMAIN:
1858                 # NOTE(vish):
1859                 LOG.warning("During detach_volume, instance disappeared.",
1860                             instance=instance)
1861             else:
1862                 raise
1863 
1864         self._disconnect_volume(context, connection_info, instance,
1865                                 encryption=encryption)
1866 
1867     def extend_volume(self, connection_info, instance, requested_size):
1868         try:
1869             new_size = self._extend_volume(connection_info, instance,
1870                                            requested_size)
1871         except NotImplementedError:
1872             raise exception.ExtendVolumeNotSupported()
1873 
1874         # Resize the device in QEMU so its size is updated and
1875         # detected by the instance without rebooting.
1876         try:
1877             guest = self._host.get_guest(instance)
1878             state = guest.get_power_state(self._host)
1879             active_state = state in (power_state.RUNNING, power_state.PAUSED)
1880             if active_state:
1881                 if 'device_path' in connection_info['data']:
1882                     disk_path = connection_info['data']['device_path']
1883                 else:
1884                     # Some drivers (eg. net) don't put the device_path
1885                     # into the connection_info. Match disks by their serial
1886                     # number instead
1887                     volume_id = driver_block_device.get_volume_id(
1888                         connection_info)
1889                     disk = next(iter([
1890                         d for d in guest.get_all_disks()
1891                         if d.serial == volume_id
1892                     ]), None)
1893                     if not disk:
1894                         raise exception.VolumeNotFound(volume_id=volume_id)
1895                     disk_path = disk.target_dev
1896 
1897                 LOG.debug('resizing block device %(dev)s to %(size)u kb',
1898                           {'dev': disk_path, 'size': new_size})
1899                 dev = guest.get_block_device(disk_path)
1900                 dev.resize(new_size // units.Ki)
1901             else:
1902                 LOG.debug('Skipping block device resize, guest is not running',
1903                           instance=instance)
1904         except exception.InstanceNotFound:
1905             with excutils.save_and_reraise_exception():
1906                 LOG.warning('During extend_volume, instance disappeared.',
1907                             instance=instance)
1908         except libvirt.libvirtError:
1909             with excutils.save_and_reraise_exception():
1910                 LOG.exception('resizing block device failed.',
1911                               instance=instance)
1912 
1913     def attach_interface(self, context, instance, image_meta, vif):
1914         guest = self._host.get_guest(instance)
1915 
1916         self.vif_driver.plug(instance, vif)
1917         self.firewall_driver.setup_basic_filtering(instance, [vif])
1918         cfg = self.vif_driver.get_config(instance, vif, image_meta,
1919                                          instance.flavor,
1920                                          CONF.libvirt.virt_type,
1921                                          self._host)
1922         try:
1923             state = guest.get_power_state(self._host)
1924             live = state in (power_state.RUNNING, power_state.PAUSED)
1925             guest.attach_device(cfg, persistent=True, live=live)
1926         except libvirt.libvirtError:
1927             LOG.error('attaching network adapter failed.',
1928                       instance=instance, exc_info=True)
1929             self.vif_driver.unplug(instance, vif)
1930             raise exception.InterfaceAttachFailed(
1931                     instance_uuid=instance.uuid)
1932         try:
1933             # NOTE(artom) If we're attaching with a device role tag, we need to
1934             # rebuild device_metadata. If we're attaching without a role
1935             # tag, we're rebuilding it here needlessly anyways. This isn't a
1936             # massive deal, and it helps reduce code complexity by not having
1937             # to indicate to the virt driver that the attach is tagged. The
1938             # really important optimization of not calling the database unless
1939             # device_metadata has actually changed is done for us by
1940             # instance.save().
1941             instance.device_metadata = self._build_device_metadata(
1942                 context, instance)
1943             instance.save()
1944         except Exception:
1945             # NOTE(artom) If we fail here it means the interface attached
1946             # successfully but building and/or saving the device metadata
1947             # failed. Just unplugging the vif is therefore not enough cleanup,
1948             # we need to detach the interface.
1949             with excutils.save_and_reraise_exception(reraise=False):
1950                 LOG.error('Interface attached successfully but building '
1951                           'and/or saving device metadata failed.',
1952                           instance=instance, exc_info=True)
1953                 self.detach_interface(context, instance, vif)
1954                 raise exception.InterfaceAttachFailed(
1955                     instance_uuid=instance.uuid)
1956 
1957     def detach_interface(self, context, instance, vif):
1958         guest = self._host.get_guest(instance)
1959         cfg = self.vif_driver.get_config(instance, vif,
1960                                          instance.image_meta,
1961                                          instance.flavor,
1962                                          CONF.libvirt.virt_type, self._host)
1963         interface = guest.get_interface_by_cfg(cfg)
1964         try:
1965             self.vif_driver.unplug(instance, vif)
1966             # NOTE(mriedem): When deleting an instance and using Neutron,
1967             # we can be racing against Neutron deleting the port and
1968             # sending the vif-deleted event which then triggers a call to
1969             # detach the interface, so if the interface is not found then
1970             # we can just log it as a warning.
1971             if not interface:
1972                 mac = vif.get('address')
1973                 # The interface is gone so just log it as a warning.
1974                 LOG.warning('Detaching interface %(mac)s failed because '
1975                             'the device is no longer found on the guest.',
1976                             {'mac': mac}, instance=instance)
1977                 return
1978 
1979             state = guest.get_power_state(self._host)
1980             live = state in (power_state.RUNNING, power_state.PAUSED)
1981             # Now we are going to loop until the interface is detached or we
1982             # timeout.
1983             wait_for_detach = guest.detach_device_with_retry(
1984                 guest.get_interface_by_cfg, cfg, live=live,
1985                 alternative_device_name=self.vif_driver.get_vif_devname(vif))
1986             wait_for_detach()
1987         except exception.DeviceDetachFailed:
1988             # We failed to detach the device even with the retry loop, so let's
1989             # dump some debug information to the logs before raising back up.
1990             with excutils.save_and_reraise_exception():
1991                 devname = self.vif_driver.get_vif_devname(vif)
1992                 interface = guest.get_interface_by_cfg(cfg)
1993                 if interface:
1994                     LOG.warning(
1995                         'Failed to detach interface %(devname)s after '
1996                         'repeated attempts. Final interface xml:\n'
1997                         '%(interface_xml)s\nFinal guest xml:\n%(guest_xml)s',
1998                         {'devname': devname,
1999                          'interface_xml': interface.to_xml(),
2000                          'guest_xml': guest.get_xml_desc()},
2001                         instance=instance)
2002         except exception.DeviceNotFound:
2003             # The interface is gone so just log it as a warning.
2004             LOG.warning('Detaching interface %(mac)s failed because '
2005                         'the device is no longer found on the guest.',
2006                         {'mac': vif.get('address')}, instance=instance)
2007         except libvirt.libvirtError as ex:
2008             error_code = ex.get_error_code()
2009             if error_code == libvirt.VIR_ERR_NO_DOMAIN:
2010                 LOG.warning("During detach_interface, instance disappeared.",
2011                             instance=instance)
2012             else:
2013                 # NOTE(mriedem): When deleting an instance and using Neutron,
2014                 # we can be racing against Neutron deleting the port and
2015                 # sending the vif-deleted event which then triggers a call to
2016                 # detach the interface, so we might have failed because the
2017                 # network device no longer exists. Libvirt will fail with
2018                 # "operation failed: no matching network device was found"
2019                 # which unfortunately does not have a unique error code so we
2020                 # need to look up the interface by config and if it's not found
2021                 # then we can just log it as a warning rather than tracing an
2022                 # error.
2023                 mac = vif.get('address')
2024                 interface = guest.get_interface_by_cfg(cfg)
2025                 if interface:
2026                     LOG.error('detaching network adapter failed.',
2027                               instance=instance, exc_info=True)
2028                     raise exception.InterfaceDetachFailed(
2029                             instance_uuid=instance.uuid)
2030 
2031                 # The interface is gone so just log it as a warning.
2032                 LOG.warning('Detaching interface %(mac)s failed because '
2033                             'the device is no longer found on the guest.',
2034                             {'mac': mac}, instance=instance)
2035 
2036     def _create_snapshot_metadata(self, image_meta, instance,
2037                                   img_fmt, snp_name):
2038         metadata = {'status': 'active',
2039                     'name': snp_name,
2040                     'properties': {
2041                                    'kernel_id': instance.kernel_id,
2042                                    'image_location': 'snapshot',
2043                                    'image_state': 'available',
2044                                    'owner_id': instance.project_id,
2045                                    'ramdisk_id': instance.ramdisk_id,
2046                                    }
2047                     }
2048         if instance.os_type:
2049             metadata['properties']['os_type'] = instance.os_type
2050 
2051         # NOTE(vish): glance forces ami disk format to be ami
2052         if image_meta.disk_format == 'ami':
2053             metadata['disk_format'] = 'ami'
2054         else:
2055             metadata['disk_format'] = img_fmt
2056 
2057         if image_meta.obj_attr_is_set("container_format"):
2058             metadata['container_format'] = image_meta.container_format
2059         else:
2060             metadata['container_format'] = "bare"
2061 
2062         return metadata
2063 
2064     def snapshot(self, context, instance, image_id, update_task_state):
2065         """Create snapshot from a running VM instance.
2066 
2067         This command only works with qemu 0.14+
2068         """
2069         try:
2070             guest = self._host.get_guest(instance)
2071 
2072             # TODO(sahid): We are converting all calls from a
2073             # virDomain object to use nova.virt.libvirt.Guest.
2074             # We should be able to remove virt_dom at the end.
2075             virt_dom = guest._domain
2076         except exception.InstanceNotFound:
2077             raise exception.InstanceNotRunning(instance_id=instance.uuid)
2078 
2079         snapshot = self._image_api.get(context, image_id)
2080 
2081         # source_format is an on-disk format
2082         # source_type is a backend type
2083         disk_path, source_format = libvirt_utils.find_disk(guest)
2084         source_type = libvirt_utils.get_disk_type_from_path(disk_path)
2085 
2086         # We won't have source_type for raw or qcow2 disks, because we can't
2087         # determine that from the path. We should have it from the libvirt
2088         # xml, though.
2089         if source_type is None:
2090             source_type = source_format
2091         # For lxc instances we won't have it either from libvirt xml
2092         # (because we just gave libvirt the mounted filesystem), or the path,
2093         # so source_type is still going to be None. In this case,
2094         # root_disk is going to default to CONF.libvirt.images_type
2095         # below, which is still safe.
2096 
2097         image_format = CONF.libvirt.snapshot_image_format or source_type
2098 
2099         # NOTE(bfilippov): save lvm and rbd as raw
2100         if image_format == 'lvm' or image_format == 'rbd':
2101             image_format = 'raw'
2102 
2103         metadata = self._create_snapshot_metadata(instance.image_meta,
2104                                                   instance,
2105                                                   image_format,
2106                                                   snapshot['name'])
2107 
2108         snapshot_name = uuidutils.generate_uuid(dashed=False)
2109 
2110         state = guest.get_power_state(self._host)
2111 
2112         # NOTE(dgenin): Instances with LVM encrypted ephemeral storage require
2113         #               cold snapshots. Currently, checking for encryption is
2114         #               redundant because LVM supports only cold snapshots.
2115         #               It is necessary in case this situation changes in the
2116         #               future.
2117         if (self._host.has_min_version(hv_type=host.HV_DRIVER_QEMU) and
2118                 source_type != 'lvm' and
2119                 not CONF.ephemeral_storage_encryption.enabled and
2120                 not CONF.workarounds.disable_libvirt_livesnapshot and
2121                 # NOTE(rmk): We cannot perform live snapshots when a
2122                 # managedSave file is present, so we will use the cold/legacy
2123                 # method for instances which are shutdown or paused.
2124                 # NOTE(mriedem): Live snapshot doesn't work with paused
2125                 # instances on older versions of libvirt/qemu. We can likely
2126                 # remove the restriction on PAUSED once we require
2127                 # libvirt>=3.6.0 and qemu>=2.10 since that works with the
2128                 # Pike Ubuntu Cloud Archive testing in Queens.
2129                 state not in (power_state.SHUTDOWN, power_state.PAUSED)):
2130             live_snapshot = True
2131             # Abort is an idempotent operation, so make sure any block
2132             # jobs which may have failed are ended. This operation also
2133             # confirms the running instance, as opposed to the system as a
2134             # whole, has a new enough version of the hypervisor (bug 1193146).
2135             try:
2136                 guest.get_block_device(disk_path).abort_job()
2137             except libvirt.libvirtError as ex:
2138                 error_code = ex.get_error_code()
2139                 if error_code == libvirt.VIR_ERR_CONFIG_UNSUPPORTED:
2140                     live_snapshot = False
2141                 else:
2142                     pass
2143         else:
2144             live_snapshot = False
2145 
2146         self._prepare_domain_for_snapshot(context, live_snapshot, state,
2147                                           instance)
2148 
2149         root_disk = self.image_backend.by_libvirt_path(
2150             instance, disk_path, image_type=source_type)
2151 
2152         if live_snapshot:
2153             LOG.info("Beginning live snapshot process", instance=instance)
2154         else:
2155             LOG.info("Beginning cold snapshot process", instance=instance)
2156 
2157         update_task_state(task_state=task_states.IMAGE_PENDING_UPLOAD)
2158 
2159         update_task_state(task_state=task_states.IMAGE_UPLOADING,
2160                           expected_state=task_states.IMAGE_PENDING_UPLOAD)
2161 
2162         try:
2163             metadata['location'] = root_disk.direct_snapshot(
2164                 context, snapshot_name, image_format, image_id,
2165                 instance.image_ref)
2166             self._snapshot_domain(context, live_snapshot, virt_dom, state,
2167                                   instance)
2168             self._image_api.update(context, image_id, metadata,
2169                                    purge_props=False)
2170         except (NotImplementedError, exception.ImageUnacceptable,
2171                 exception.Forbidden) as e:
2172             if type(e) != NotImplementedError:
2173                 LOG.warning('Performing standard snapshot because direct '
2174                             'snapshot failed: %(error)s',
2175                             {'error': encodeutils.exception_to_unicode(e)})
2176             failed_snap = metadata.pop('location', None)
2177             if failed_snap:
2178                 failed_snap = {'url': str(failed_snap)}
2179             root_disk.cleanup_direct_snapshot(failed_snap,
2180                                                   also_destroy_volume=True,
2181                                                   ignore_errors=True)
2182             update_task_state(task_state=task_states.IMAGE_PENDING_UPLOAD,
2183                               expected_state=task_states.IMAGE_UPLOADING)
2184 
2185             # TODO(nic): possibly abstract this out to the root_disk
2186             if source_type == 'rbd' and live_snapshot:
2187                 # Standard snapshot uses qemu-img convert from RBD which is
2188                 # not safe to run with live_snapshot.
2189                 live_snapshot = False
2190                 # Suspend the guest, so this is no longer a live snapshot
2191                 self._prepare_domain_for_snapshot(context, live_snapshot,
2192                                                   state, instance)
2193 
2194             snapshot_directory = CONF.libvirt.snapshots_directory
2195             fileutils.ensure_tree(snapshot_directory)
2196             with utils.tempdir(dir=snapshot_directory) as tmpdir:
2197                 try:
2198                     out_path = os.path.join(tmpdir, snapshot_name)
2199                     if live_snapshot:
2200                         # NOTE(xqueralt): libvirt needs o+x in the tempdir
2201                         os.chmod(tmpdir, 0o701)
2202                         self._live_snapshot(context, instance, guest,
2203                                             disk_path, out_path, source_format,
2204                                             image_format, instance.image_meta)
2205                     else:
2206                         root_disk.snapshot_extract(out_path, image_format)
2207                     LOG.info("Snapshot extracted, beginning image upload",
2208                              instance=instance)
2209                 except libvirt.libvirtError as ex:
2210                     error_code = ex.get_error_code()
2211                     if error_code == libvirt.VIR_ERR_NO_DOMAIN:
2212                         LOG.info('Instance %(instance_name)s disappeared '
2213                                  'while taking snapshot of it: [Error Code '
2214                                  '%(error_code)s] %(ex)s',
2215                                  {'instance_name': instance.name,
2216                                   'error_code': error_code,
2217                                   'ex': ex},
2218                                  instance=instance)
2219                         raise exception.InstanceNotFound(
2220                             instance_id=instance.uuid)
2221                     else:
2222                         raise
2223                 finally:
2224                     self._snapshot_domain(context, live_snapshot, virt_dom,
2225                                           state, instance)
2226 
2227                 # Upload that image to the image service
2228                 update_task_state(task_state=task_states.IMAGE_UPLOADING,
2229                         expected_state=task_states.IMAGE_PENDING_UPLOAD)
2230                 with libvirt_utils.file_open(out_path, 'rb') as image_file:
2231                     # execute operation with disk concurrency semaphore
2232                     with compute_utils.disk_ops_semaphore:
2233                         self._image_api.update(context,
2234                                                image_id,
2235                                                metadata,
2236                                                image_file)
2237         except Exception:
2238             with excutils.save_and_reraise_exception():
2239                 LOG.exception(_("Failed to snapshot image"))
2240                 failed_snap = metadata.pop('location', None)
2241                 if failed_snap:
2242                     failed_snap = {'url': str(failed_snap)}
2243                 root_disk.cleanup_direct_snapshot(
2244                         failed_snap, also_destroy_volume=True,
2245                         ignore_errors=True)
2246 
2247         LOG.info("Snapshot image upload complete", instance=instance)
2248 
2249     def _prepare_domain_for_snapshot(self, context, live_snapshot, state,
2250                                      instance):
2251         # NOTE(dkang): managedSave does not work for LXC
2252         if CONF.libvirt.virt_type != 'lxc' and not live_snapshot:
2253             if state == power_state.RUNNING or state == power_state.PAUSED:
2254                 self.suspend(context, instance)
2255 
2256     def _snapshot_domain(self, context, live_snapshot, virt_dom, state,
2257                          instance):
2258         guest = None
2259         # NOTE(dkang): because previous managedSave is not called
2260         #              for LXC, _create_domain must not be called.
2261         if CONF.libvirt.virt_type != 'lxc' and not live_snapshot:
2262             if state == power_state.RUNNING:
2263                 guest = self._create_domain(domain=virt_dom)
2264             elif state == power_state.PAUSED:
2265                 guest = self._create_domain(domain=virt_dom, pause=True)
2266 
2267             if guest is not None:
2268                 self._attach_pci_devices(
2269                     guest, pci_manager.get_instance_pci_devs(instance))
2270                 self._attach_direct_passthrough_ports(
2271                     context, instance, guest)
2272 
2273     def _can_set_admin_password(self, image_meta):
2274 
2275         if CONF.libvirt.virt_type in ('kvm', 'qemu'):
2276             if not image_meta.properties.get('hw_qemu_guest_agent', False):
2277                 raise exception.QemuGuestAgentNotEnabled()
2278         elif not CONF.libvirt.virt_type == 'parallels':
2279             raise exception.SetAdminPasswdNotSupported()
2280 
2281     # TODO(melwitt): Combine this with the similar xenapi code at some point.
2282     def _save_instance_password_if_sshkey_present(self, instance, new_pass):
2283         sshkey = instance.key_data if 'key_data' in instance else None
2284         if sshkey and sshkey.startswith("ssh-rsa"):
2285             enc = crypto.ssh_encrypt_text(sshkey, new_pass)
2286             # NOTE(melwitt): The convert_password method doesn't actually do
2287             # anything with the context argument, so we can pass None.
2288             instance.system_metadata.update(
2289                 password.convert_password(None, base64.encode_as_text(enc)))
2290             instance.save()
2291 
2292     def set_admin_password(self, instance, new_pass):
2293         self._can_set_admin_password(instance.image_meta)
2294 
2295         guest = self._host.get_guest(instance)
2296         user = instance.image_meta.properties.get("os_admin_user")
2297         if not user:
2298             if instance.os_type == "windows":
2299                 user = "Administrator"
2300             else:
2301                 user = "root"
2302         try:
2303             guest.set_user_password(user, new_pass)
2304         except libvirt.libvirtError as ex:
2305             error_code = ex.get_error_code()
2306             if error_code == libvirt.VIR_ERR_AGENT_UNRESPONSIVE:
2307                 LOG.debug('Failed to set password: QEMU agent unresponsive',
2308                           instance_uuid=instance.uuid)
2309                 raise NotImplementedError()
2310 
2311             err_msg = encodeutils.exception_to_unicode(ex)
2312             msg = (_('Error from libvirt while set password for username '
2313                      '"%(user)s": [Error Code %(error_code)s] %(ex)s')
2314                    % {'user': user, 'error_code': error_code, 'ex': err_msg})
2315             raise exception.InternalError(msg)
2316         else:
2317             # Save the password in sysmeta so it may be retrieved from the
2318             # metadata service.
2319             self._save_instance_password_if_sshkey_present(instance, new_pass)
2320 
2321     def _can_quiesce(self, instance, image_meta):
2322         if CONF.libvirt.virt_type not in ('kvm', 'qemu'):
2323             raise exception.InstanceQuiesceNotSupported(
2324                 instance_id=instance.uuid)
2325 
2326         if not image_meta.properties.get('hw_qemu_guest_agent', False):
2327             raise exception.QemuGuestAgentNotEnabled()
2328 
2329     def _requires_quiesce(self, image_meta):
2330         return image_meta.properties.get('os_require_quiesce', False)
2331 
2332     def _set_quiesced(self, context, instance, image_meta, quiesced):
2333         self._can_quiesce(instance, image_meta)
2334         try:
2335             guest = self._host.get_guest(instance)
2336             if quiesced:
2337                 guest.freeze_filesystems()
2338             else:
2339                 guest.thaw_filesystems()
2340         except libvirt.libvirtError as ex:
2341             error_code = ex.get_error_code()
2342             err_msg = encodeutils.exception_to_unicode(ex)
2343             msg = (_('Error from libvirt while quiescing %(instance_name)s: '
2344                      '[Error Code %(error_code)s] %(ex)s')
2345                    % {'instance_name': instance.name,
2346                       'error_code': error_code, 'ex': err_msg})
2347             raise exception.InternalError(msg)
2348 
2349     def quiesce(self, context, instance, image_meta):
2350         """Freeze the guest filesystems to prepare for snapshot.
2351 
2352         The qemu-guest-agent must be setup to execute fsfreeze.
2353         """
2354         self._set_quiesced(context, instance, image_meta, True)
2355 
2356     def unquiesce(self, context, instance, image_meta):
2357         """Thaw the guest filesystems after snapshot."""
2358         self._set_quiesced(context, instance, image_meta, False)
2359 
2360     def _live_snapshot(self, context, instance, guest, disk_path, out_path,
2361                        source_format, image_format, image_meta):
2362         """Snapshot an instance without downtime."""
2363         dev = guest.get_block_device(disk_path)
2364 
2365         # Save a copy of the domain's persistent XML file
2366         xml = guest.get_xml_desc(dump_inactive=True, dump_sensitive=True)
2367 
2368         # Abort is an idempotent operation, so make sure any block
2369         # jobs which may have failed are ended.
2370         try:
2371             dev.abort_job()
2372         except Exception:
2373             pass
2374 
2375         # NOTE (rmk): We are using shallow rebases as a workaround to a bug
2376         #             in QEMU 1.3. In order to do this, we need to create
2377         #             a destination image with the original backing file
2378         #             and matching size of the instance root disk.
2379         src_disk_size = libvirt_utils.get_disk_size(disk_path,
2380                                                     format=source_format)
2381         src_back_path = libvirt_utils.get_disk_backing_file(disk_path,
2382                                                         format=source_format,
2383                                                         basename=False)
2384         disk_delta = out_path + '.delta'
2385         libvirt_utils.create_cow_image(src_back_path, disk_delta,
2386                                        src_disk_size)
2387 
2388         quiesced = False
2389         try:
2390             self._set_quiesced(context, instance, image_meta, True)
2391             quiesced = True
2392         except exception.NovaException as err:
2393             if self._requires_quiesce(image_meta):
2394                 raise
2395             LOG.info('Skipping quiescing instance: %(reason)s.',
2396                      {'reason': err}, instance=instance)
2397 
2398         try:
2399             # NOTE (rmk): blockRebase cannot be executed on persistent
2400             #             domains, so we need to temporarily undefine it.
2401             #             If any part of this block fails, the domain is
2402             #             re-defined regardless.
2403             if guest.has_persistent_configuration():
2404                 support_uefi = self._has_uefi_support()
2405                 guest.delete_configuration(support_uefi)
2406 
2407             # NOTE (rmk): Establish a temporary mirror of our root disk and
2408             #             issue an abort once we have a complete copy.
2409             dev.rebase(disk_delta, copy=True, reuse_ext=True, shallow=True)
2410 
2411             while not dev.is_job_complete():
2412                 time.sleep(0.5)
2413 
2414             dev.abort_job()
2415             nova.privsep.path.chown(disk_delta, uid=os.getuid())
2416         finally:
2417             self._host.write_instance_config(xml)
2418             if quiesced:
2419                 self._set_quiesced(context, instance, image_meta, False)
2420 
2421         # Convert the delta (CoW) image with a backing file to a flat
2422         # image with no backing file.
2423         libvirt_utils.extract_snapshot(disk_delta, 'qcow2',
2424                                        out_path, image_format)
2425 
2426     def _volume_snapshot_update_status(self, context, snapshot_id, status):
2427         """Send a snapshot status update to Cinder.
2428 
2429         This method captures and logs exceptions that occur
2430         since callers cannot do anything useful with these exceptions.
2431 
2432         Operations on the Cinder side waiting for this will time out if
2433         a failure occurs sending the update.
2434 
2435         :param context: security context
2436         :param snapshot_id: id of snapshot being updated
2437         :param status: new status value
2438 
2439         """
2440 
2441         try:
2442             self._volume_api.update_snapshot_status(context,
2443                                                     snapshot_id,
2444                                                     status)
2445         except Exception:
2446             LOG.exception(_('Failed to send updated snapshot status '
2447                             'to volume service.'))
2448 
2449     def _volume_snapshot_create(self, context, instance, guest,
2450                                 volume_id, new_file):
2451         """Perform volume snapshot.
2452 
2453            :param guest: VM that volume is attached to
2454            :param volume_id: volume UUID to snapshot
2455            :param new_file: relative path to new qcow2 file present on share
2456 
2457         """
2458         xml = guest.get_xml_desc()
2459         xml_doc = etree.fromstring(xml)
2460 
2461         device_info = vconfig.LibvirtConfigGuest()
2462         device_info.parse_dom(xml_doc)
2463 
2464         disks_to_snap = []          # to be snapshotted by libvirt
2465         network_disks_to_snap = []  # network disks (netfs, etc.)
2466         disks_to_skip = []          # local disks not snapshotted
2467 
2468         for guest_disk in device_info.devices:
2469             if (guest_disk.root_name != 'disk'):
2470                 continue
2471 
2472             if (guest_disk.target_dev is None):
2473                 continue
2474 
2475             if (guest_disk.serial is None or guest_disk.serial != volume_id):
2476                 disks_to_skip.append(guest_disk.target_dev)
2477                 continue
2478 
2479             # disk is a Cinder volume with the correct volume_id
2480 
2481             disk_info = {
2482                 'dev': guest_disk.target_dev,
2483                 'serial': guest_disk.serial,
2484                 'current_file': guest_disk.source_path,
2485                 'source_protocol': guest_disk.source_protocol,
2486                 'source_name': guest_disk.source_name,
2487                 'source_hosts': guest_disk.source_hosts,
2488                 'source_ports': guest_disk.source_ports
2489             }
2490 
2491             # Determine path for new_file based on current path
2492             if disk_info['current_file'] is not None:
2493                 current_file = disk_info['current_file']
2494                 new_file_path = os.path.join(os.path.dirname(current_file),
2495                                              new_file)
2496                 disks_to_snap.append((current_file, new_file_path))
2497             # NOTE(mriedem): This used to include a check for gluster in
2498             # addition to netfs since they were added together. Support for
2499             # gluster was removed in the 16.0.0 Pike release. It is unclear,
2500             # however, if other volume drivers rely on the netfs disk source
2501             # protocol.
2502             elif disk_info['source_protocol'] == 'netfs':
2503                 network_disks_to_snap.append((disk_info, new_file))
2504 
2505         if not disks_to_snap and not network_disks_to_snap:
2506             msg = _('Found no disk to snapshot.')
2507             raise exception.InternalError(msg)
2508 
2509         snapshot = vconfig.LibvirtConfigGuestSnapshot()
2510 
2511         for current_name, new_filename in disks_to_snap:
2512             snap_disk = vconfig.LibvirtConfigGuestSnapshotDisk()
2513             snap_disk.name = current_name
2514             snap_disk.source_path = new_filename
2515             snap_disk.source_type = 'file'
2516             snap_disk.snapshot = 'external'
2517             snap_disk.driver_name = 'qcow2'
2518 
2519             snapshot.add_disk(snap_disk)
2520 
2521         for disk_info, new_filename in network_disks_to_snap:
2522             snap_disk = vconfig.LibvirtConfigGuestSnapshotDisk()
2523             snap_disk.name = disk_info['dev']
2524             snap_disk.source_type = 'network'
2525             snap_disk.source_protocol = disk_info['source_protocol']
2526             snap_disk.snapshot = 'external'
2527             snap_disk.source_path = new_filename
2528             old_dir = disk_info['source_name'].split('/')[0]
2529             snap_disk.source_name = '%s/%s' % (old_dir, new_filename)
2530             snap_disk.source_hosts = disk_info['source_hosts']
2531             snap_disk.source_ports = disk_info['source_ports']
2532 
2533             snapshot.add_disk(snap_disk)
2534 
2535         for dev in disks_to_skip:
2536             snap_disk = vconfig.LibvirtConfigGuestSnapshotDisk()
2537             snap_disk.name = dev
2538             snap_disk.snapshot = 'no'
2539 
2540             snapshot.add_disk(snap_disk)
2541 
2542         snapshot_xml = snapshot.to_xml()
2543         LOG.debug("snap xml: %s", snapshot_xml, instance=instance)
2544 
2545         image_meta = instance.image_meta
2546         try:
2547             # Check to see if we can quiesce the guest before taking the
2548             # snapshot.
2549             self._can_quiesce(instance, image_meta)
2550             try:
2551                 guest.snapshot(snapshot, no_metadata=True, disk_only=True,
2552                                reuse_ext=True, quiesce=True)
2553                 return
2554             except libvirt.libvirtError:
2555                 # If the image says that quiesce is required then we fail.
2556                 if self._requires_quiesce(image_meta):
2557                     raise
2558                 LOG.exception(_('Unable to create quiesced VM snapshot, '
2559                                 'attempting again with quiescing disabled.'),
2560                               instance=instance)
2561         except (exception.InstanceQuiesceNotSupported,
2562                 exception.QemuGuestAgentNotEnabled) as err:
2563             # If the image says that quiesce is required then we need to fail.
2564             if self._requires_quiesce(image_meta):
2565                 raise
2566             LOG.info('Skipping quiescing instance: %(reason)s.',
2567                      {'reason': err}, instance=instance)
2568 
2569         try:
2570             guest.snapshot(snapshot, no_metadata=True, disk_only=True,
2571                            reuse_ext=True, quiesce=False)
2572         except libvirt.libvirtError:
2573             LOG.exception(_('Unable to create VM snapshot, '
2574                             'failing volume_snapshot operation.'),
2575                           instance=instance)
2576 
2577             raise
2578 
2579     def _volume_refresh_connection_info(self, context, instance, volume_id):
2580         bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
2581                   context, volume_id, instance.uuid)
2582 
2583         driver_bdm = driver_block_device.convert_volume(bdm)
2584         if driver_bdm:
2585             driver_bdm.refresh_connection_info(context, instance,
2586                                                self._volume_api, self)
2587 
2588     def volume_snapshot_create(self, context, instance, volume_id,
2589                                create_info):
2590         """Create snapshots of a Cinder volume via libvirt.
2591 
2592         :param instance: VM instance object reference
2593         :param volume_id: id of volume being snapshotted
2594         :param create_info: dict of information used to create snapshots
2595                      - snapshot_id : ID of snapshot
2596                      - type : qcow2 / <other>
2597                      - new_file : qcow2 file created by Cinder which
2598                      becomes the VM's active image after
2599                      the snapshot is complete
2600         """
2601 
2602         LOG.debug("volume_snapshot_create: create_info: %(c_info)s",
2603                   {'c_info': create_info}, instance=instance)
2604 
2605         try:
2606             guest = self._host.get_guest(instance)
2607         except exception.InstanceNotFound:
2608             raise exception.InstanceNotRunning(instance_id=instance.uuid)
2609 
2610         if create_info['type'] != 'qcow2':
2611             msg = _('Unknown type: %s') % create_info['type']
2612             raise exception.InternalError(msg)
2613 
2614         snapshot_id = create_info.get('snapshot_id', None)
2615         if snapshot_id is None:
2616             msg = _('snapshot_id required in create_info')
2617             raise exception.InternalError(msg)
2618 
2619         try:
2620             self._volume_snapshot_create(context, instance, guest,
2621                                          volume_id, create_info['new_file'])
2622         except Exception:
2623             with excutils.save_and_reraise_exception():
2624                 LOG.exception(_('Error occurred during '
2625                                 'volume_snapshot_create, '
2626                                 'sending error status to Cinder.'),
2627                               instance=instance)
2628                 self._volume_snapshot_update_status(
2629                     context, snapshot_id, 'error')
2630 
2631         self._volume_snapshot_update_status(
2632             context, snapshot_id, 'creating')
2633 
2634         def _wait_for_snapshot():
2635             snapshot = self._volume_api.get_snapshot(context, snapshot_id)
2636 
2637             if snapshot.get('status') != 'creating':
2638                 self._volume_refresh_connection_info(context, instance,
2639                                                      volume_id)
2640                 raise loopingcall.LoopingCallDone()
2641 
2642         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_snapshot)
2643         timer.start(interval=0.5).wait()
2644 
2645     @staticmethod
2646     def _rebase_with_qemu_img(guest, device, active_disk_object,
2647                               rebase_base):
2648         """Rebase a device tied to a guest using qemu-img.
2649 
2650         :param guest:the Guest which owns the device being rebased
2651         :type guest: nova.virt.libvirt.guest.Guest
2652         :param device: the guest block device to rebase
2653         :type device: nova.virt.libvirt.guest.BlockDevice
2654         :param active_disk_object: the guest block device to rebase
2655         :type active_disk_object: nova.virt.libvirt.config.\
2656                                     LibvirtConfigGuestDisk
2657         :param rebase_base: the new parent in the backing chain
2658         :type rebase_base: None or string
2659         """
2660 
2661         # It's unsure how well qemu-img handles network disks for
2662         # every protocol. So let's be safe.
2663         active_protocol = active_disk_object.source_protocol
2664         if active_protocol is not None:
2665             msg = _("Something went wrong when deleting a volume snapshot: "
2666                     "rebasing a %(protocol)s network disk using qemu-img "
2667                     "has not been fully tested") % {'protocol':
2668                     active_protocol}
2669             LOG.error(msg)
2670             raise exception.InternalError(msg)
2671 
2672         if rebase_base is None:
2673             # If backing_file is specified as "" (the empty string), then
2674             # the image is rebased onto no backing file (i.e. it will exist
2675             # independently of any backing file).
2676             backing_file = ""
2677             qemu_img_extra_arg = []
2678         else:
2679             # If the rebased image is going to have a backing file then
2680             # explicitly set the backing file format to avoid any security
2681             # concerns related to file format auto detection.
2682             backing_file = rebase_base
2683             b_file_fmt = images.qemu_img_info(backing_file).file_format
2684             qemu_img_extra_arg = ['-F', b_file_fmt]
2685 
2686         qemu_img_extra_arg.append(active_disk_object.source_path)
2687         # execute operation with disk concurrency semaphore
2688         with compute_utils.disk_ops_semaphore:
2689             processutils.execute("qemu-img", "rebase", "-b", backing_file,
2690                                  *qemu_img_extra_arg)
2691 
2692     def _volume_snapshot_delete(self, context, instance, volume_id,
2693                                 snapshot_id, delete_info=None):
2694         """Note:
2695             if file being merged into == active image:
2696                 do a blockRebase (pull) operation
2697             else:
2698                 do a blockCommit operation
2699             Files must be adjacent in snap chain.
2700 
2701         :param instance: instance object reference
2702         :param volume_id: volume UUID
2703         :param snapshot_id: snapshot UUID (unused currently)
2704         :param delete_info: {
2705             'type':              'qcow2',
2706             'file_to_merge':     'a.img',
2707             'merge_target_file': 'b.img' or None (if merging file_to_merge into
2708                                                   active image)
2709           }
2710         """
2711 
2712         LOG.debug('volume_snapshot_delete: delete_info: %s', delete_info,
2713                   instance=instance)
2714 
2715         if delete_info['type'] != 'qcow2':
2716             msg = _('Unknown delete_info type %s') % delete_info['type']
2717             raise exception.InternalError(msg)
2718 
2719         try:
2720             guest = self._host.get_guest(instance)
2721         except exception.InstanceNotFound:
2722             raise exception.InstanceNotRunning(instance_id=instance.uuid)
2723 
2724         # Find dev name
2725         my_dev = None
2726         active_disk = None
2727 
2728         xml = guest.get_xml_desc()
2729         xml_doc = etree.fromstring(xml)
2730 
2731         device_info = vconfig.LibvirtConfigGuest()
2732         device_info.parse_dom(xml_doc)
2733 
2734         active_disk_object = None
2735 
2736         for guest_disk in device_info.devices:
2737             if (guest_disk.root_name != 'disk'):
2738                 continue
2739 
2740             if (guest_disk.target_dev is None or guest_disk.serial is None):
2741                 continue
2742 
2743             if guest_disk.serial == volume_id:
2744                 my_dev = guest_disk.target_dev
2745 
2746                 active_disk = guest_disk.source_path
2747                 active_protocol = guest_disk.source_protocol
2748                 active_disk_object = guest_disk
2749                 break
2750 
2751         if my_dev is None or (active_disk is None and active_protocol is None):
2752             LOG.debug('Domain XML: %s', xml, instance=instance)
2753             msg = (_('Disk with id: %s not found attached to instance.')
2754                    % volume_id)
2755             raise exception.InternalError(msg)
2756 
2757         LOG.debug("found device at %s", my_dev, instance=instance)
2758 
2759         def _get_snap_dev(filename, backing_store):
2760             if filename is None:
2761                 msg = _('filename cannot be None')
2762                 raise exception.InternalError(msg)
2763 
2764             # libgfapi delete
2765             LOG.debug("XML: %s", xml)
2766 
2767             LOG.debug("active disk object: %s", active_disk_object)
2768 
2769             # determine reference within backing store for desired image
2770             filename_to_merge = filename
2771             matched_name = None
2772             b = backing_store
2773             index = None
2774 
2775             current_filename = active_disk_object.source_name.split('/')[1]
2776             if current_filename == filename_to_merge:
2777                 return my_dev + '[0]'
2778 
2779             while b is not None:
2780                 source_filename = b.source_name.split('/')[1]
2781                 if source_filename == filename_to_merge:
2782                     LOG.debug('found match: %s', b.source_name)
2783                     matched_name = b.source_name
2784                     index = b.index
2785                     break
2786 
2787                 b = b.backing_store
2788 
2789             if matched_name is None:
2790                 msg = _('no match found for %s') % (filename_to_merge)
2791                 raise exception.InternalError(msg)
2792 
2793             LOG.debug('index of match (%s) is %s', b.source_name, index)
2794 
2795             my_snap_dev = '%s[%s]' % (my_dev, index)
2796             return my_snap_dev
2797 
2798         if delete_info['merge_target_file'] is None:
2799             # pull via blockRebase()
2800 
2801             # Merge the most recent snapshot into the active image
2802 
2803             rebase_disk = my_dev
2804             rebase_base = delete_info['file_to_merge']  # often None
2805             if (active_protocol is not None) and (rebase_base is not None):
2806                 rebase_base = _get_snap_dev(rebase_base,
2807                                             active_disk_object.backing_store)
2808 
2809             # NOTE(deepakcs): libvirt added support for _RELATIVE in v1.2.7,
2810             # and when available this flag _must_ be used to ensure backing
2811             # paths are maintained relative by qemu.
2812             #
2813             # If _RELATIVE flag not found, continue with old behaviour
2814             # (relative backing path seems to work for this case)
2815             try:
2816                 libvirt.VIR_DOMAIN_BLOCK_REBASE_RELATIVE
2817                 relative = rebase_base is not None
2818             except AttributeError:
2819                 LOG.warning(
2820                     "Relative blockrebase support was not detected. "
2821                     "Continuing with old behaviour.")
2822                 relative = False
2823 
2824             LOG.debug(
2825                 'disk: %(disk)s, base: %(base)s, '
2826                 'bw: %(bw)s, relative: %(relative)s',
2827                 {'disk': rebase_disk,
2828                  'base': rebase_base,
2829                  'bw': libvirt_guest.BlockDevice.REBASE_DEFAULT_BANDWIDTH,
2830                  'relative': str(relative)}, instance=instance)
2831 
2832             dev = guest.get_block_device(rebase_disk)
2833             if guest.is_active():
2834                 result = dev.rebase(rebase_base, relative=relative)
2835                 if result == 0:
2836                     LOG.debug('blockRebase started successfully',
2837                               instance=instance)
2838 
2839                 while not dev.is_job_complete():
2840                     LOG.debug('waiting for blockRebase job completion',
2841                               instance=instance)
2842                     time.sleep(0.5)
2843 
2844             # If the guest is not running libvirt won't do a blockRebase.
2845             # In that case, let's ask qemu-img to rebase the disk.
2846             else:
2847                 LOG.debug('Guest is not running so doing a block rebase '
2848                           'using "qemu-img rebase"', instance=instance)
2849                 self._rebase_with_qemu_img(guest, dev, active_disk_object,
2850                                            rebase_base)
2851 
2852         else:
2853             # commit with blockCommit()
2854             my_snap_base = None
2855             my_snap_top = None
2856             commit_disk = my_dev
2857 
2858             if active_protocol is not None:
2859                 my_snap_base = _get_snap_dev(delete_info['merge_target_file'],
2860                                              active_disk_object.backing_store)
2861                 my_snap_top = _get_snap_dev(delete_info['file_to_merge'],
2862                                             active_disk_object.backing_store)
2863 
2864             commit_base = my_snap_base or delete_info['merge_target_file']
2865             commit_top = my_snap_top or delete_info['file_to_merge']
2866 
2867             LOG.debug('will call blockCommit with commit_disk=%(commit_disk)s '
2868                       'commit_base=%(commit_base)s '
2869                       'commit_top=%(commit_top)s ',
2870                       {'commit_disk': commit_disk,
2871                        'commit_base': commit_base,
2872                        'commit_top': commit_top}, instance=instance)
2873 
2874             dev = guest.get_block_device(commit_disk)
2875             result = dev.commit(commit_base, commit_top, relative=True)
2876 
2877             if result == 0:
2878                 LOG.debug('blockCommit started successfully',
2879                           instance=instance)
2880 
2881             while not dev.is_job_complete():
2882                 LOG.debug('waiting for blockCommit job completion',
2883                           instance=instance)
2884                 time.sleep(0.5)
2885 
2886     def volume_snapshot_delete(self, context, instance, volume_id, snapshot_id,
2887                                delete_info):
2888         try:
2889             self._volume_snapshot_delete(context, instance, volume_id,
2890                                          snapshot_id, delete_info=delete_info)
2891         except Exception:
2892             with excutils.save_and_reraise_exception():
2893                 LOG.exception(_('Error occurred during '
2894                                 'volume_snapshot_delete, '
2895                                 'sending error status to Cinder.'),
2896                               instance=instance)
2897                 self._volume_snapshot_update_status(
2898                     context, snapshot_id, 'error_deleting')
2899 
2900         self._volume_snapshot_update_status(context, snapshot_id, 'deleting')
2901         self._volume_refresh_connection_info(context, instance, volume_id)
2902 
2903     def reboot(self, context, instance, network_info, reboot_type,
2904                block_device_info=None, bad_volumes_callback=None):
2905         """Reboot a virtual machine, given an instance reference."""
2906         if reboot_type == 'SOFT':
2907             # NOTE(vish): This will attempt to do a graceful shutdown/restart.
2908             try:
2909                 soft_reboot_success = self._soft_reboot(instance)
2910             except libvirt.libvirtError as e:
2911                 LOG.debug("Instance soft reboot failed: %s",
2912                           encodeutils.exception_to_unicode(e),
2913                           instance=instance)
2914                 soft_reboot_success = False
2915 
2916             if soft_reboot_success:
2917                 LOG.info("Instance soft rebooted successfully.",
2918                          instance=instance)
2919                 return
2920             else:
2921                 LOG.warning("Failed to soft reboot instance. "
2922                             "Trying hard reboot.",
2923                             instance=instance)
2924         return self._hard_reboot(context, instance, network_info,
2925                                  block_device_info)
2926 
2927     def _soft_reboot(self, instance):
2928         """Attempt to shutdown and restart the instance gracefully.
2929 
2930         We use shutdown and create here so we can return if the guest
2931         responded and actually rebooted. Note that this method only
2932         succeeds if the guest responds to acpi. Therefore we return
2933         success or failure so we can fall back to a hard reboot if
2934         necessary.
2935 
2936         :returns: True if the reboot succeeded
2937         """
2938         guest = self._host.get_guest(instance)
2939 
2940         state = guest.get_power_state(self._host)
2941         old_domid = guest.id
2942         # NOTE(vish): This check allows us to reboot an instance that
2943         #             is already shutdown.
2944         if state == power_state.RUNNING:
2945             guest.shutdown()
2946         # NOTE(vish): This actually could take slightly longer than the
2947         #             FLAG defines depending on how long the get_info
2948         #             call takes to return.
2949         self._prepare_pci_devices_for_use(
2950             pci_manager.get_instance_pci_devs(instance, 'all'))
2951         for x in range(CONF.libvirt.wait_soft_reboot_seconds):
2952             guest = self._host.get_guest(instance)
2953 
2954             state = guest.get_power_state(self._host)
2955             new_domid = guest.id
2956 
2957             # NOTE(ivoks): By checking domain IDs, we make sure we are
2958             #              not recreating domain that's already running.
2959             if old_domid != new_domid:
2960                 if state in [power_state.SHUTDOWN,
2961                              power_state.CRASHED]:
2962                     LOG.info("Instance shutdown successfully.",
2963                              instance=instance)
2964                     self._create_domain(domain=guest._domain)
2965                     timer = loopingcall.FixedIntervalLoopingCall(
2966                         self._wait_for_running, instance)
2967                     timer.start(interval=0.5).wait()
2968                     return True
2969                 else:
2970                     LOG.info("Instance may have been rebooted during soft "
2971                              "reboot, so return now.", instance=instance)
2972                     return True
2973             greenthread.sleep(1)
2974         return False
2975 
2976     def _hard_reboot(self, context, instance, network_info,
2977                      block_device_info=None):
2978         """Reboot a virtual machine, given an instance reference.
2979 
2980         Performs a Libvirt reset (if supported) on the domain.
2981 
2982         If Libvirt reset is unavailable this method actually destroys and
2983         re-creates the domain to ensure the reboot happens, as the guest
2984         OS cannot ignore this action.
2985         """
2986         # NOTE(sbauza): Since we undefine the guest XML when destroying, we
2987         # need to remember the existing mdevs for reusing them.
2988         mdevs = self._get_all_assigned_mediated_devices(instance)
2989         mdevs = list(mdevs.keys())
2990         # NOTE(mdbooth): In addition to performing a hard reboot of the domain,
2991         # the hard reboot operation is relied upon by operators to be an
2992         # automated attempt to fix as many things as possible about a
2993         # non-functioning instance before resorting to manual intervention.
2994         # With this goal in mind, we tear down all the aspects of an instance
2995         # we can here without losing data. This allows us to re-initialise from
2996         # scratch, and hopefully fix, most aspects of a non-functioning guest.
2997         self.destroy(context, instance, network_info, destroy_disks=False,
2998                      block_device_info=block_device_info)
2999 
3000         # Convert the system metadata to image metadata
3001         # NOTE(mdbooth): This is a workaround for stateless Nova compute
3002         #                https://bugs.launchpad.net/nova/+bug/1349978
3003         instance_dir = libvirt_utils.get_instance_path(instance)
3004         fileutils.ensure_tree(instance_dir)
3005 
3006         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
3007                                             instance,
3008                                             instance.image_meta,
3009                                             block_device_info)
3010         # NOTE(vish): This could generate the wrong device_format if we are
3011         #             using the raw backend and the images don't exist yet.
3012         #             The create_images_and_backing below doesn't properly
3013         #             regenerate raw backend images, however, so when it
3014         #             does we need to (re)generate the xml after the images
3015         #             are in place.
3016         xml = self._get_guest_xml(context, instance, network_info, disk_info,
3017                                   instance.image_meta,
3018                                   block_device_info=block_device_info,
3019                                   mdevs=mdevs)
3020 
3021         # NOTE(mdbooth): context.auth_token will not be set when we call
3022         #                _hard_reboot from resume_state_on_host_boot()
3023         if context.auth_token is not None:
3024             # NOTE (rmk): Re-populate any missing backing files.
3025             config = vconfig.LibvirtConfigGuest()
3026             config.parse_str(xml)
3027             backing_disk_info = self._get_instance_disk_info_from_config(
3028                 config, block_device_info)
3029             self._create_images_and_backing(context, instance, instance_dir,
3030                                             backing_disk_info)
3031 
3032         # Initialize all the necessary networking, block devices and
3033         # start the instance.
3034         # NOTE(melwitt): Pass vifs_already_plugged=True here even though we've
3035         # unplugged vifs earlier. The behavior of neutron plug events depends
3036         # on which vif type we're using and we are working with a stale network
3037         # info cache here, so won't rely on waiting for neutron plug events.
3038         # vifs_already_plugged=True means "do not wait for neutron plug events"
3039         self._create_domain_and_network(context, xml, instance, network_info,
3040                                         block_device_info=block_device_info,
3041                                         vifs_already_plugged=True)
3042         self._prepare_pci_devices_for_use(
3043             pci_manager.get_instance_pci_devs(instance, 'all'))
3044 
3045         def _wait_for_reboot():
3046             """Called at an interval until the VM is running again."""
3047             state = self.get_info(instance).state
3048 
3049             if state == power_state.RUNNING:
3050                 LOG.info("Instance rebooted successfully.",
3051                          instance=instance)
3052                 raise loopingcall.LoopingCallDone()
3053 
3054         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_reboot)
3055         timer.start(interval=0.5).wait()
3056 
3057     def pause(self, instance):
3058         """Pause VM instance."""
3059         self._host.get_guest(instance).pause()
3060 
3061     def unpause(self, instance):
3062         """Unpause paused VM instance."""
3063         guest = self._host.get_guest(instance)
3064         guest.resume()
3065         guest.sync_guest_time()
3066 
3067     def _clean_shutdown(self, instance, timeout, retry_interval):
3068         """Attempt to shutdown the instance gracefully.
3069 
3070         :param instance: The instance to be shutdown
3071         :param timeout: How long to wait in seconds for the instance to
3072                         shutdown
3073         :param retry_interval: How often in seconds to signal the instance
3074                                to shutdown while waiting
3075 
3076         :returns: True if the shutdown succeeded
3077         """
3078 
3079         # List of states that represent a shutdown instance
3080         SHUTDOWN_STATES = [power_state.SHUTDOWN,
3081                            power_state.CRASHED]
3082 
3083         try:
3084             guest = self._host.get_guest(instance)
3085         except exception.InstanceNotFound:
3086             # If the instance has gone then we don't need to
3087             # wait for it to shutdown
3088             return True
3089 
3090         state = guest.get_power_state(self._host)
3091         if state in SHUTDOWN_STATES:
3092             LOG.info("Instance already shutdown.", instance=instance)
3093             return True
3094 
3095         LOG.debug("Shutting down instance from state %s", state,
3096                   instance=instance)
3097         guest.shutdown()
3098         retry_countdown = retry_interval
3099 
3100         for sec in range(timeout):
3101 
3102             guest = self._host.get_guest(instance)
3103             state = guest.get_power_state(self._host)
3104 
3105             if state in SHUTDOWN_STATES:
3106                 LOG.info("Instance shutdown successfully after %d seconds.",
3107                          sec, instance=instance)
3108                 return True
3109 
3110             # Note(PhilD): We can't assume that the Guest was able to process
3111             #              any previous shutdown signal (for example it may
3112             #              have still been startingup, so within the overall
3113             #              timeout we re-trigger the shutdown every
3114             #              retry_interval
3115             if retry_countdown == 0:
3116                 retry_countdown = retry_interval
3117                 # Instance could shutdown at any time, in which case we
3118                 # will get an exception when we call shutdown
3119                 try:
3120                     LOG.debug("Instance in state %s after %d seconds - "
3121                               "resending shutdown", state, sec,
3122                               instance=instance)
3123                     guest.shutdown()
3124                 except libvirt.libvirtError:
3125                     # Assume this is because its now shutdown, so loop
3126                     # one more time to clean up.
3127                     LOG.debug("Ignoring libvirt exception from shutdown "
3128                               "request.", instance=instance)
3129                     continue
3130             else:
3131                 retry_countdown -= 1
3132 
3133             time.sleep(1)
3134 
3135         LOG.info("Instance failed to shutdown in %d seconds.",
3136                  timeout, instance=instance)
3137         return False
3138 
3139     def power_off(self, instance, timeout=0, retry_interval=0):
3140         """Power off the specified instance."""
3141         if timeout:
3142             self._clean_shutdown(instance, timeout, retry_interval)
3143         self._destroy(instance)
3144 
3145     def power_on(self, context, instance, network_info,
3146                  block_device_info=None):
3147         """Power on the specified instance."""
3148         # We use _hard_reboot here to ensure that all backing files,
3149         # network, and block device connections, etc. are established
3150         # and available before we attempt to start the instance.
3151         self._hard_reboot(context, instance, network_info, block_device_info)
3152 
3153     def trigger_crash_dump(self, instance):
3154 
3155         """Trigger crash dump by injecting an NMI to the specified instance."""
3156         try:
3157             self._host.get_guest(instance).inject_nmi()
3158         except libvirt.libvirtError as ex:
3159             error_code = ex.get_error_code()
3160 
3161             if error_code == libvirt.VIR_ERR_NO_SUPPORT:
3162                 raise exception.TriggerCrashDumpNotSupported()
3163             elif error_code == libvirt.VIR_ERR_OPERATION_INVALID:
3164                 raise exception.InstanceNotRunning(instance_id=instance.uuid)
3165 
3166             LOG.exception(_('Error from libvirt while injecting an NMI to '
3167                             '%(instance_uuid)s: '
3168                             '[Error Code %(error_code)s] %(ex)s'),
3169                           {'instance_uuid': instance.uuid,
3170                            'error_code': error_code, 'ex': ex})
3171             raise
3172 
3173     def suspend(self, context, instance):
3174         """Suspend the specified instance."""
3175         guest = self._host.get_guest(instance)
3176 
3177         self._detach_pci_devices(guest,
3178             pci_manager.get_instance_pci_devs(instance))
3179         self._detach_direct_passthrough_ports(context, instance, guest)
3180         self._detach_mediated_devices(guest)
3181         guest.save_memory_state()
3182 
3183     def resume(self, context, instance, network_info, block_device_info=None):
3184         """resume the specified instance."""
3185         xml = self._get_existing_domain_xml(instance, network_info,
3186                                             block_device_info)
3187         guest = self._create_domain_and_network(context, xml, instance,
3188                            network_info, block_device_info=block_device_info,
3189                            vifs_already_plugged=True)
3190         self._attach_pci_devices(guest,
3191             pci_manager.get_instance_pci_devs(instance))
3192         self._attach_direct_passthrough_ports(
3193             context, instance, guest, network_info)
3194         timer = loopingcall.FixedIntervalLoopingCall(self._wait_for_running,
3195                                                      instance)
3196         timer.start(interval=0.5).wait()
3197         guest.sync_guest_time()
3198 
3199     def resume_state_on_host_boot(self, context, instance, network_info,
3200                                   block_device_info=None):
3201         """resume guest state when a host is booted."""
3202         # Check if the instance is running already and avoid doing
3203         # anything if it is.
3204         try:
3205             guest = self._host.get_guest(instance)
3206             state = guest.get_power_state(self._host)
3207 
3208             ignored_states = (power_state.RUNNING,
3209                               power_state.SUSPENDED,
3210                               power_state.NOSTATE,
3211                               power_state.PAUSED)
3212 
3213             if state in ignored_states:
3214                 return
3215         except (exception.InternalError, exception.InstanceNotFound):
3216             pass
3217 
3218         # Instance is not up and could be in an unknown state.
3219         # Be as absolute as possible about getting it back into
3220         # a known and running state.
3221         self._hard_reboot(context, instance, network_info, block_device_info)
3222 
3223     def rescue(self, context, instance, network_info, image_meta,
3224                rescue_password):
3225         """Loads a VM using rescue images.
3226 
3227         A rescue is normally performed when something goes wrong with the
3228         primary images and data needs to be corrected/recovered. Rescuing
3229         should not edit or over-ride the original image, only allow for
3230         data recovery.
3231 
3232         """
3233         instance_dir = libvirt_utils.get_instance_path(instance)
3234         unrescue_xml = self._get_existing_domain_xml(instance, network_info)
3235         unrescue_xml_path = os.path.join(instance_dir, 'unrescue.xml')
3236         libvirt_utils.write_to_file(unrescue_xml_path, unrescue_xml)
3237 
3238         rescue_image_id = None
3239         if image_meta.obj_attr_is_set("id"):
3240             rescue_image_id = image_meta.id
3241 
3242         rescue_images = {
3243             'image_id': (rescue_image_id or
3244                         CONF.libvirt.rescue_image_id or instance.image_ref),
3245             'kernel_id': (CONF.libvirt.rescue_kernel_id or
3246                           instance.kernel_id),
3247             'ramdisk_id': (CONF.libvirt.rescue_ramdisk_id or
3248                            instance.ramdisk_id),
3249         }
3250         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
3251                                             instance,
3252                                             image_meta,
3253                                             rescue=True)
3254         injection_info = InjectionInfo(network_info=network_info,
3255                                        admin_pass=rescue_password,
3256                                        files=None)
3257         gen_confdrive = functools.partial(self._create_configdrive,
3258                                           context, instance, injection_info,
3259                                           rescue=True)
3260         # NOTE(sbauza): Since rescue recreates the guest XML, we need to
3261         # remember the existing mdevs for reusing them.
3262         mdevs = self._get_all_assigned_mediated_devices(instance)
3263         mdevs = list(mdevs.keys())
3264         self._create_image(context, instance, disk_info['mapping'],
3265                            injection_info=injection_info, suffix='.rescue',
3266                            disk_images=rescue_images)
3267         xml = self._get_guest_xml(context, instance, network_info, disk_info,
3268                                   image_meta, rescue=rescue_images,
3269                                   mdevs=mdevs)
3270         self._destroy(instance)
3271         self._create_domain(xml, post_xml_callback=gen_confdrive)
3272 
3273     def unrescue(self, instance, network_info):
3274         """Reboot the VM which is being rescued back into primary images.
3275         """
3276         instance_dir = libvirt_utils.get_instance_path(instance)
3277         unrescue_xml_path = os.path.join(instance_dir, 'unrescue.xml')
3278         xml = libvirt_utils.load_file(unrescue_xml_path)
3279         guest = self._host.get_guest(instance)
3280 
3281         # TODO(sahid): We are converting all calls from a
3282         # virDomain object to use nova.virt.libvirt.Guest.
3283         # We should be able to remove virt_dom at the end.
3284         virt_dom = guest._domain
3285         self._destroy(instance)
3286         self._create_domain(xml, virt_dom)
3287         os.unlink(unrescue_xml_path)
3288         rescue_files = os.path.join(instance_dir, "*.rescue")
3289         for rescue_file in glob.iglob(rescue_files):
3290             if os.path.isdir(rescue_file):
3291                 shutil.rmtree(rescue_file)
3292             else:
3293                 os.unlink(rescue_file)
3294         # cleanup rescue volume
3295         lvm.remove_volumes([lvmdisk for lvmdisk in self._lvm_disks(instance)
3296                                 if lvmdisk.endswith('.rescue')])
3297         if CONF.libvirt.images_type == 'rbd':
3298             filter_fn = lambda disk: (disk.startswith(instance.uuid) and
3299                                       disk.endswith('.rescue'))
3300             LibvirtDriver._get_rbd_driver().cleanup_volumes(filter_fn)
3301 
3302     def poll_rebooting_instances(self, timeout, instances):
3303         pass
3304 
3305     def spawn(self, context, instance, image_meta, injected_files,
3306               admin_password, allocations, network_info=None,
3307               block_device_info=None):
3308         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
3309                                             instance,
3310                                             image_meta,
3311                                             block_device_info)
3312         injection_info = InjectionInfo(network_info=network_info,
3313                                        files=injected_files,
3314                                        admin_pass=admin_password)
3315         gen_confdrive = functools.partial(self._create_configdrive,
3316                                           context, instance,
3317                                           injection_info)
3318         self._create_image(context, instance, disk_info['mapping'],
3319                            injection_info=injection_info,
3320                            block_device_info=block_device_info)
3321 
3322         # Required by Quobyte CI
3323         self._ensure_console_log_for_instance(instance)
3324 
3325         # Does the guest need to be assigned some vGPU mediated devices ?
3326         mdevs = self._allocate_mdevs(allocations)
3327 
3328         xml = self._get_guest_xml(context, instance, network_info,
3329                                   disk_info, image_meta,
3330                                   block_device_info=block_device_info,
3331                                   mdevs=mdevs)
3332         self._create_domain_and_network(
3333             context, xml, instance, network_info,
3334             block_device_info=block_device_info,
3335             post_xml_callback=gen_confdrive,
3336             destroy_disks_on_failure=True)
3337         LOG.debug("Guest created on hypervisor", instance=instance)
3338 
3339         def _wait_for_boot():
3340             """Called at an interval until the VM is running."""
3341             state = self.get_info(instance).state
3342 
3343             if state == power_state.RUNNING:
3344                 LOG.info("Instance spawned successfully.", instance=instance)
3345                 raise loopingcall.LoopingCallDone()
3346 
3347         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_boot)
3348         timer.start(interval=0.5).wait()
3349 
3350     def _get_console_output_file(self, instance, console_log):
3351         bytes_to_read = MAX_CONSOLE_BYTES
3352         log_data = b""  # The last N read bytes
3353         i = 0  # in case there is a log rotation (like "virtlogd")
3354         path = console_log
3355 
3356         while bytes_to_read > 0 and os.path.exists(path):
3357             read_log_data, remaining = nova.privsep.path.last_bytes(
3358                                         path, bytes_to_read)
3359             # We need the log file content in chronological order,
3360             # that's why we *prepend* the log data.
3361             log_data = read_log_data + log_data
3362 
3363             # Prep to read the next file in the chain
3364             bytes_to_read -= len(read_log_data)
3365             path = console_log + "." + str(i)
3366             i += 1
3367 
3368             if remaining > 0:
3369                 LOG.info('Truncated console log returned, '
3370                          '%d bytes ignored', remaining, instance=instance)
3371         return log_data
3372 
3373     def get_console_output(self, context, instance):
3374         guest = self._host.get_guest(instance)
3375 
3376         xml = guest.get_xml_desc()
3377         tree = etree.fromstring(xml)
3378 
3379         # check for different types of consoles
3380         path_sources = [
3381             ('file', "./devices/console[@type='file']/source[@path]", 'path'),
3382             ('tcp', "./devices/console[@type='tcp']/log[@file]", 'file'),
3383             ('pty', "./devices/console[@type='pty']/source[@path]", 'path')]
3384         console_type = ""
3385         console_path = ""
3386         for c_type, epath, attrib in path_sources:
3387             node = tree.find(epath)
3388             if (node is not None) and node.get(attrib):
3389                 console_type = c_type
3390                 console_path = node.get(attrib)
3391                 break
3392 
3393         # instance has no console at all
3394         if not console_path:
3395             raise exception.ConsoleNotAvailable()
3396 
3397         # instance has a console, but file doesn't exist (yet?)
3398         if not os.path.exists(console_path):
3399             LOG.info('console logfile for instance does not exist',
3400                       instance=instance)
3401             return ""
3402 
3403         # pty consoles need special handling
3404         if console_type == 'pty':
3405             console_log = self._get_console_log_path(instance)
3406             data = nova.privsep.libvirt.readpty(console_path)
3407 
3408             # NOTE(markus_z): The virt_types kvm and qemu are the only ones
3409             # which create a dedicated file device for the console logging.
3410             # Other virt_types like xen, lxc, uml, parallels depend on the
3411             # flush of that pty device into the "console.log" file to ensure
3412             # that a series of "get_console_output" calls return the complete
3413             # content even after rebooting a guest.
3414             nova.privsep.path.writefile(console_log, 'a+', data)
3415 
3416             # set console path to logfile, not to pty device
3417             console_path = console_log
3418 
3419         # return logfile content
3420         return self._get_console_output_file(instance, console_path)
3421 
3422     def get_host_ip_addr(self):
3423         ips = compute_utils.get_machine_ips()
3424         if CONF.my_ip not in ips:
3425             LOG.warning('my_ip address (%(my_ip)s) was not found on '
3426                         'any of the interfaces: %(ifaces)s',
3427                         {'my_ip': CONF.my_ip, 'ifaces': ", ".join(ips)})
3428         return CONF.my_ip
3429 
3430     def get_vnc_console(self, context, instance):
3431         def get_vnc_port_for_instance(instance_name):
3432             guest = self._host.get_guest(instance)
3433 
3434             xml = guest.get_xml_desc()
3435             xml_dom = etree.fromstring(xml)
3436 
3437             graphic = xml_dom.find("./devices/graphics[@type='vnc']")
3438             if graphic is not None:
3439                 return graphic.get('port')
3440             # NOTE(rmk): We had VNC consoles enabled but the instance in
3441             # question is not actually listening for connections.
3442             raise exception.ConsoleTypeUnavailable(console_type='vnc')
3443 
3444         port = get_vnc_port_for_instance(instance.name)
3445         host = CONF.vnc.server_proxyclient_address
3446 
3447         return ctype.ConsoleVNC(host=host, port=port)
3448 
3449     def get_spice_console(self, context, instance):
3450         def get_spice_ports_for_instance(instance_name):
3451             guest = self._host.get_guest(instance)
3452 
3453             xml = guest.get_xml_desc()
3454             xml_dom = etree.fromstring(xml)
3455 
3456             graphic = xml_dom.find("./devices/graphics[@type='spice']")
3457             if graphic is not None:
3458                 return (graphic.get('port'), graphic.get('tlsPort'))
3459             # NOTE(rmk): We had Spice consoles enabled but the instance in
3460             # question is not actually listening for connections.
3461             raise exception.ConsoleTypeUnavailable(console_type='spice')
3462 
3463         ports = get_spice_ports_for_instance(instance.name)
3464         host = CONF.spice.server_proxyclient_address
3465 
3466         return ctype.ConsoleSpice(host=host, port=ports[0], tlsPort=ports[1])
3467 
3468     def get_serial_console(self, context, instance):
3469         guest = self._host.get_guest(instance)
3470         for hostname, port in self._get_serial_ports_from_guest(
3471                 guest, mode='bind'):
3472             return ctype.ConsoleSerial(host=hostname, port=port)
3473         raise exception.ConsoleTypeUnavailable(console_type='serial')
3474 
3475     @staticmethod
3476     def _create_ephemeral(target, ephemeral_size,
3477                           fs_label, os_type, is_block_dev=False,
3478                           context=None, specified_fs=None,
3479                           vm_mode=None):
3480         if not is_block_dev:
3481             if (CONF.libvirt.virt_type == "parallels" and
3482                     vm_mode == fields.VMMode.EXE):
3483 
3484                 libvirt_utils.create_ploop_image('expanded', target,
3485                                                  '%dG' % ephemeral_size,
3486                                                  specified_fs)
3487                 return
3488             libvirt_utils.create_image('raw', target, '%dG' % ephemeral_size)
3489 
3490         # Run as root only for block devices.
3491         disk_api.mkfs(os_type, fs_label, target, run_as_root=is_block_dev,
3492                       specified_fs=specified_fs)
3493 
3494     @staticmethod
3495     def _create_swap(target, swap_mb, context=None):
3496         """Create a swap file of specified size."""
3497         libvirt_utils.create_image('raw', target, '%dM' % swap_mb)
3498         nova.privsep.fs.unprivileged_mkfs('swap', target)
3499 
3500     @staticmethod
3501     def _get_console_log_path(instance):
3502         return os.path.join(libvirt_utils.get_instance_path(instance),
3503                             'console.log')
3504 
3505     def _ensure_console_log_for_instance(self, instance):
3506         # NOTE(mdbooth): Although libvirt will create this file for us
3507         # automatically when it starts, it will initially create it with
3508         # root ownership and then chown it depending on the configuration of
3509         # the domain it is launching. Quobyte CI explicitly disables the
3510         # chown by setting dynamic_ownership=0 in libvirt's config.
3511         # Consequently when the domain starts it is unable to write to its
3512         # console.log. See bug https://bugs.launchpad.net/nova/+bug/1597644
3513         #
3514         # To work around this, we create the file manually before starting
3515         # the domain so it has the same ownership as Nova. This works
3516         # for Quobyte CI because it is also configured to run qemu as the same
3517         # user as the Nova service. Installations which don't set
3518         # dynamic_ownership=0 are not affected because libvirt will always
3519         # correctly configure permissions regardless of initial ownership.
3520         #
3521         # Setting dynamic_ownership=0 is dubious and potentially broken in
3522         # more ways than console.log (see comment #22 on the above bug), so
3523         # Future Maintainer who finds this code problematic should check to see
3524         # if we still support it.
3525         console_file = self._get_console_log_path(instance)
3526         LOG.debug('Ensure instance console log exists: %s', console_file,
3527                   instance=instance)
3528         try:
3529             libvirt_utils.file_open(console_file, 'a').close()
3530         # NOTE(sfinucan): We can safely ignore permission issues here and
3531         # assume that it is libvirt that has taken ownership of this file.
3532         except IOError as ex:
3533             if ex.errno != errno.EACCES:
3534                 raise
3535             LOG.debug('Console file already exists: %s.', console_file)
3536 
3537     @staticmethod
3538     def _get_disk_config_image_type():
3539         # TODO(mikal): there is a bug here if images_type has
3540         # changed since creation of the instance, but I am pretty
3541         # sure that this bug already exists.
3542         return 'rbd' if CONF.libvirt.images_type == 'rbd' else 'raw'
3543 
3544     @staticmethod
3545     def _is_booted_from_volume(block_device_info):
3546         """Determines whether the VM is booting from volume
3547 
3548         Determines whether the block device info indicates that the VM
3549         is booting from a volume.
3550         """
3551         block_device_mapping = driver.block_device_info_get_mapping(
3552             block_device_info)
3553         return bool(block_device.get_root_bdm(block_device_mapping))
3554 
3555     def _inject_data(self, disk, instance, injection_info):
3556         """Injects data in a disk image
3557 
3558         Helper used for injecting data in a disk image file system.
3559 
3560         :param disk: The disk we're injecting into (an Image object)
3561         :param instance: The instance we're injecting into
3562         :param injection_info: Injection info
3563         """
3564         # Handles the partition need to be used.
3565         LOG.debug('Checking root disk injection %s',
3566                   str(injection_info), instance=instance)
3567         target_partition = None
3568         if not instance.kernel_id:
3569             target_partition = CONF.libvirt.inject_partition
3570             if target_partition == 0:
3571                 target_partition = None
3572         if CONF.libvirt.virt_type == 'lxc':
3573             target_partition = None
3574 
3575         # Handles the key injection.
3576         if CONF.libvirt.inject_key and instance.get('key_data'):
3577             key = str(instance.key_data)
3578         else:
3579             key = None
3580 
3581         # Handles the admin password injection.
3582         if not CONF.libvirt.inject_password:
3583             admin_pass = None
3584         else:
3585             admin_pass = injection_info.admin_pass
3586 
3587         # Handles the network injection.
3588         net = netutils.get_injected_network_template(
3589             injection_info.network_info,
3590             libvirt_virt_type=CONF.libvirt.virt_type)
3591 
3592         # Handles the metadata injection
3593         metadata = instance.get('metadata')
3594 
3595         if any((key, net, metadata, admin_pass, injection_info.files)):
3596             LOG.debug('Injecting %s', str(injection_info),
3597                       instance=instance)
3598             img_id = instance.image_ref
3599             try:
3600                 disk_api.inject_data(disk.get_model(self._conn),
3601                                      key, net, metadata, admin_pass,
3602                                      injection_info.files,
3603                                      partition=target_partition,
3604                                      mandatory=('files',))
3605             except Exception as e:
3606                 with excutils.save_and_reraise_exception():
3607                     LOG.error('Error injecting data into image '
3608                               '%(img_id)s (%(e)s)',
3609                               {'img_id': img_id, 'e': e},
3610                               instance=instance)
3611 
3612     # NOTE(sileht): many callers of this method assume that this
3613     # method doesn't fail if an image already exists but instead
3614     # think that it will be reused (ie: (live)-migration/resize)
3615     def _create_image(self, context, instance,
3616                       disk_mapping, injection_info=None, suffix='',
3617                       disk_images=None, block_device_info=None,
3618                       fallback_from_host=None,
3619                       ignore_bdi_for_swap=False):
3620         booted_from_volume = self._is_booted_from_volume(block_device_info)
3621 
3622         def image(fname, image_type=CONF.libvirt.images_type):
3623             return self.image_backend.by_name(instance,
3624                                               fname + suffix, image_type)
3625 
3626         def raw(fname):
3627             return image(fname, image_type='raw')
3628 
3629         # ensure directories exist and are writable
3630         fileutils.ensure_tree(libvirt_utils.get_instance_path(instance))
3631 
3632         LOG.info('Creating image', instance=instance)
3633 
3634         inst_type = instance.get_flavor()
3635         swap_mb = 0
3636         if 'disk.swap' in disk_mapping:
3637             mapping = disk_mapping['disk.swap']
3638 
3639             if ignore_bdi_for_swap:
3640                 # This is a workaround to support legacy swap resizing,
3641                 # which does not touch swap size specified in bdm,
3642                 # but works with flavor specified size only.
3643                 # In this case we follow the legacy logic and ignore block
3644                 # device info completely.
3645                 # NOTE(ft): This workaround must be removed when a correct
3646                 # implementation of resize operation changing sizes in bdms is
3647                 # developed. Also at that stage we probably may get rid of
3648                 # the direct usage of flavor swap size here,
3649                 # leaving the work with bdm only.
3650                 swap_mb = inst_type['swap']
3651             else:
3652                 swap = driver.block_device_info_get_swap(block_device_info)
3653                 if driver.swap_is_usable(swap):
3654                     swap_mb = swap['swap_size']
3655                 elif (inst_type['swap'] > 0 and
3656                       not block_device.volume_in_mapping(
3657                         mapping['dev'], block_device_info)):
3658                     swap_mb = inst_type['swap']
3659 
3660             if swap_mb > 0:
3661                 if (CONF.libvirt.virt_type == "parallels" and
3662                         instance.vm_mode == fields.VMMode.EXE):
3663                     msg = _("Swap disk is not supported "
3664                             "for Virtuozzo container")
3665                     raise exception.Invalid(msg)
3666 
3667         if not disk_images:
3668             disk_images = {'image_id': instance.image_ref,
3669                            'kernel_id': instance.kernel_id,
3670                            'ramdisk_id': instance.ramdisk_id}
3671 
3672         if disk_images['kernel_id']:
3673             fname = imagecache.get_cache_fname(disk_images['kernel_id'])
3674             raw('kernel').cache(fetch_func=libvirt_utils.fetch_raw_image,
3675                                 context=context,
3676                                 filename=fname,
3677                                 image_id=disk_images['kernel_id'])
3678             if disk_images['ramdisk_id']:
3679                 fname = imagecache.get_cache_fname(disk_images['ramdisk_id'])
3680                 raw('ramdisk').cache(fetch_func=libvirt_utils.fetch_raw_image,
3681                                      context=context,
3682                                      filename=fname,
3683                                      image_id=disk_images['ramdisk_id'])
3684 
3685         if CONF.libvirt.virt_type == 'uml':
3686             # PONDERING(mikal): can I assume that root is UID zero in every
3687             # OS? Probably not.
3688             uid = pwd.getpwnam('root').pw_uid
3689             nova.privsep.path.chown(image('disk').path, uid=uid)
3690 
3691         self._create_and_inject_local_root(context, instance,
3692                                            booted_from_volume, suffix,
3693                                            disk_images, injection_info,
3694                                            fallback_from_host)
3695 
3696         # Lookup the filesystem type if required
3697         os_type_with_default = nova.privsep.fs.get_fs_type_for_os_type(
3698             instance.os_type)
3699         # Generate a file extension based on the file system
3700         # type and the mkfs commands configured if any
3701         file_extension = nova.privsep.fs.get_file_extension_for_os_type(
3702             os_type_with_default, CONF.default_ephemeral_format)
3703 
3704         vm_mode = fields.VMMode.get_from_instance(instance)
3705         ephemeral_gb = instance.flavor.ephemeral_gb
3706         if 'disk.local' in disk_mapping:
3707             disk_image = image('disk.local')
3708             fn = functools.partial(self._create_ephemeral,
3709                                    fs_label='ephemeral0',
3710                                    os_type=instance.os_type,
3711                                    is_block_dev=disk_image.is_block_dev,
3712                                    vm_mode=vm_mode)
3713             fname = "ephemeral_%s_%s" % (ephemeral_gb, file_extension)
3714             size = ephemeral_gb * units.Gi
3715             disk_image.cache(fetch_func=fn,
3716                              context=context,
3717                              filename=fname,
3718                              size=size,
3719                              ephemeral_size=ephemeral_gb)
3720 
3721         for idx, eph in enumerate(driver.block_device_info_get_ephemerals(
3722                 block_device_info)):
3723             disk_image = image(blockinfo.get_eph_disk(idx))
3724 
3725             specified_fs = eph.get('guest_format')
3726             if specified_fs and not self.is_supported_fs_format(specified_fs):
3727                 msg = _("%s format is not supported") % specified_fs
3728                 raise exception.InvalidBDMFormat(details=msg)
3729 
3730             fn = functools.partial(self._create_ephemeral,
3731                                    fs_label='ephemeral%d' % idx,
3732                                    os_type=instance.os_type,
3733                                    is_block_dev=disk_image.is_block_dev,
3734                                    vm_mode=vm_mode)
3735             size = eph['size'] * units.Gi
3736             fname = "ephemeral_%s_%s" % (eph['size'], file_extension)
3737             disk_image.cache(fetch_func=fn,
3738                              context=context,
3739                              filename=fname,
3740                              size=size,
3741                              ephemeral_size=eph['size'],
3742                              specified_fs=specified_fs)
3743 
3744         if swap_mb > 0:
3745             size = swap_mb * units.Mi
3746             image('disk.swap').cache(fetch_func=self._create_swap,
3747                                      context=context,
3748                                      filename="swap_%s" % swap_mb,
3749                                      size=size,
3750                                      swap_mb=swap_mb)
3751 
3752     def _create_and_inject_local_root(self, context, instance,
3753                                       booted_from_volume, suffix, disk_images,
3754                                       injection_info, fallback_from_host):
3755         # File injection only if needed
3756         need_inject = (not configdrive.required_by(instance) and
3757                        injection_info is not None and
3758                        CONF.libvirt.inject_partition != -2)
3759 
3760         # NOTE(ndipanov): Even if disk_mapping was passed in, which
3761         # currently happens only on rescue - we still don't want to
3762         # create a base image.
3763         if not booted_from_volume:
3764             root_fname = imagecache.get_cache_fname(disk_images['image_id'])
3765             size = instance.flavor.root_gb * units.Gi
3766 
3767             if size == 0 or suffix == '.rescue':
3768                 size = None
3769 
3770             backend = self.image_backend.by_name(instance, 'disk' + suffix,
3771                                                  CONF.libvirt.images_type)
3772             if instance.task_state == task_states.RESIZE_FINISH:
3773                 backend.create_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME)
3774             if backend.SUPPORTS_CLONE:
3775                 def clone_fallback_to_fetch(*args, **kwargs):
3776                     try:
3777                         backend.clone(context, disk_images['image_id'])
3778                     except exception.ImageUnacceptable:
3779                         libvirt_utils.fetch_image(*args, **kwargs)
3780                 fetch_func = clone_fallback_to_fetch
3781             else:
3782                 fetch_func = libvirt_utils.fetch_image
3783             self._try_fetch_image_cache(backend, fetch_func, context,
3784                                         root_fname, disk_images['image_id'],
3785                                         instance, size, fallback_from_host)
3786 
3787             if need_inject:
3788                 self._inject_data(backend, instance, injection_info)
3789 
3790         elif need_inject:
3791             LOG.warning('File injection into a boot from volume '
3792                         'instance is not supported', instance=instance)
3793 
3794     def _create_configdrive(self, context, instance, injection_info,
3795                             rescue=False):
3796         # As this method being called right after the definition of a
3797         # domain, but before its actual launch, device metadata will be built
3798         # and saved in the instance for it to be used by the config drive and
3799         # the metadata service.
3800         instance.device_metadata = self._build_device_metadata(context,
3801                                                                instance)
3802         if configdrive.required_by(instance):
3803             LOG.info('Using config drive', instance=instance)
3804 
3805             name = 'disk.config'
3806             if rescue:
3807                 name += '.rescue'
3808 
3809             config_disk = self.image_backend.by_name(
3810                 instance, name, self._get_disk_config_image_type())
3811 
3812             # Don't overwrite an existing config drive
3813             if not config_disk.exists():
3814                 extra_md = {}
3815                 if injection_info.admin_pass:
3816                     extra_md['admin_pass'] = injection_info.admin_pass
3817 
3818                 inst_md = instance_metadata.InstanceMetadata(
3819                     instance, content=injection_info.files, extra_md=extra_md,
3820                     network_info=injection_info.network_info,
3821                     request_context=context)
3822 
3823                 cdb = configdrive.ConfigDriveBuilder(instance_md=inst_md)
3824                 with cdb:
3825                     # NOTE(mdbooth): We're hardcoding here the path of the
3826                     # config disk when using the flat backend. This isn't
3827                     # good, but it's required because we need a local path we
3828                     # know we can write to in case we're subsequently
3829                     # importing into rbd. This will be cleaned up when we
3830                     # replace this with a call to create_from_func, but that
3831                     # can't happen until we've updated the backends and we
3832                     # teach them not to cache config disks. This isn't
3833                     # possible while we're still using cache() under the hood.
3834                     config_disk_local_path = os.path.join(
3835                         libvirt_utils.get_instance_path(instance), name)
3836                     LOG.info('Creating config drive at %(path)s',
3837                              {'path': config_disk_local_path},
3838                              instance=instance)
3839 
3840                     try:
3841                         cdb.make_drive(config_disk_local_path)
3842                     except processutils.ProcessExecutionError as e:
3843                         with excutils.save_and_reraise_exception():
3844                             LOG.error('Creating config drive failed with '
3845                                       'error: %s', e, instance=instance)
3846 
3847                 try:
3848                     config_disk.import_file(
3849                         instance, config_disk_local_path, name)
3850                 finally:
3851                     # NOTE(mikal): if the config drive was imported into RBD,
3852                     # then we no longer need the local copy
3853                     if CONF.libvirt.images_type == 'rbd':
3854                         LOG.info('Deleting local config drive %(path)s '
3855                                  'because it was imported into RBD.',
3856                                  {'path': config_disk_local_path},
3857                                  instance=instance)
3858                         os.unlink(config_disk_local_path)
3859 
3860     def _prepare_pci_devices_for_use(self, pci_devices):
3861         # kvm , qemu support managed mode
3862         # In managed mode, the configured device will be automatically
3863         # detached from the host OS drivers when the guest is started,
3864         # and then re-attached when the guest shuts down.
3865         if CONF.libvirt.virt_type != 'xen':
3866             # we do manual detach only for xen
3867             return
3868         try:
3869             for dev in pci_devices:
3870                 libvirt_dev_addr = dev['hypervisor_name']
3871                 libvirt_dev = \
3872                         self._host.device_lookup_by_name(libvirt_dev_addr)
3873                 # Note(yjiang5) Spelling for 'dettach' is correct, see
3874                 # http://libvirt.org/html/libvirt-libvirt.html.
3875                 libvirt_dev.dettach()
3876 
3877             # Note(yjiang5): A reset of one PCI device may impact other
3878             # devices on the same bus, thus we need two separated loops
3879             # to detach and then reset it.
3880             for dev in pci_devices:
3881                 libvirt_dev_addr = dev['hypervisor_name']
3882                 libvirt_dev = \
3883                         self._host.device_lookup_by_name(libvirt_dev_addr)
3884                 libvirt_dev.reset()
3885 
3886         except libvirt.libvirtError as exc:
3887             raise exception.PciDevicePrepareFailed(id=dev['id'],
3888                                                    instance_uuid=
3889                                                    dev['instance_uuid'],
3890                                                    reason=six.text_type(exc))
3891 
3892     def _detach_pci_devices(self, guest, pci_devs):
3893         try:
3894             for dev in pci_devs:
3895                 guest.detach_device(self._get_guest_pci_device(dev), live=True)
3896                 # after detachDeviceFlags returned, we should check the dom to
3897                 # ensure the detaching is finished
3898                 xml = guest.get_xml_desc()
3899                 xml_doc = etree.fromstring(xml)
3900                 guest_config = vconfig.LibvirtConfigGuest()
3901                 guest_config.parse_dom(xml_doc)
3902 
3903                 for hdev in [d for d in guest_config.devices
3904                     if isinstance(d, vconfig.LibvirtConfigGuestHostdevPCI)]:
3905                     hdbsf = [hdev.domain, hdev.bus, hdev.slot, hdev.function]
3906                     dbsf = pci_utils.parse_address(dev.address)
3907                     if [int(x, 16) for x in hdbsf] ==\
3908                             [int(x, 16) for x in dbsf]:
3909                         raise exception.PciDeviceDetachFailed(reason=
3910                                                               "timeout",
3911                                                               dev=dev)
3912 
3913         except libvirt.libvirtError as ex:
3914             error_code = ex.get_error_code()
3915             if error_code == libvirt.VIR_ERR_NO_DOMAIN:
3916                 LOG.warning("Instance disappeared while detaching "
3917                             "a PCI device from it.")
3918             else:
3919                 raise
3920 
3921     def _attach_pci_devices(self, guest, pci_devs):
3922         try:
3923             for dev in pci_devs:
3924                 guest.attach_device(self._get_guest_pci_device(dev))
3925 
3926         except libvirt.libvirtError:
3927             LOG.error('Attaching PCI devices %(dev)s to %(dom)s failed.',
3928                       {'dev': pci_devs, 'dom': guest.id})
3929             raise
3930 
3931     @staticmethod
3932     def _has_direct_passthrough_port(network_info):
3933         for vif in network_info:
3934             if (vif['vnic_type'] in
3935                 network_model.VNIC_TYPES_DIRECT_PASSTHROUGH):
3936                 return True
3937         return False
3938 
3939     def _attach_direct_passthrough_ports(
3940         self, context, instance, guest, network_info=None):
3941         if network_info is None:
3942             network_info = instance.info_cache.network_info
3943         if network_info is None:
3944             return
3945 
3946         if self._has_direct_passthrough_port(network_info):
3947             for vif in network_info:
3948                 if (vif['vnic_type'] in
3949                     network_model.VNIC_TYPES_DIRECT_PASSTHROUGH):
3950                     cfg = self.vif_driver.get_config(instance,
3951                                                      vif,
3952                                                      instance.image_meta,
3953                                                      instance.flavor,
3954                                                      CONF.libvirt.virt_type,
3955                                                      self._host)
3956                     LOG.debug('Attaching direct passthrough port %(port)s '
3957                               'to %(dom)s', {'port': vif, 'dom': guest.id},
3958                               instance=instance)
3959                     guest.attach_device(cfg)
3960 
3961     def _detach_direct_passthrough_ports(self, context, instance, guest):
3962         network_info = instance.info_cache.network_info
3963         if network_info is None:
3964             return
3965 
3966         if self._has_direct_passthrough_port(network_info):
3967             # In case of VNIC_TYPES_DIRECT_PASSTHROUGH ports we create
3968             # pci request per direct passthrough port. Therefore we can trust
3969             # that pci_slot value in the vif is correct.
3970             direct_passthrough_pci_addresses = [
3971                 vif['profile']['pci_slot']
3972                 for vif in network_info
3973                 if (vif['vnic_type'] in
3974                     network_model.VNIC_TYPES_DIRECT_PASSTHROUGH and
3975                     vif['profile'].get('pci_slot') is not None)
3976             ]
3977 
3978             # use detach_pci_devices to avoid failure in case of
3979             # multiple guest direct passthrough ports with the same MAC
3980             # (protection use-case, ports are on different physical
3981             # interfaces)
3982             pci_devs = pci_manager.get_instance_pci_devs(instance, 'all')
3983             direct_passthrough_pci_addresses = (
3984                 [pci_dev for pci_dev in pci_devs
3985                  if pci_dev.address in direct_passthrough_pci_addresses])
3986             self._detach_pci_devices(guest, direct_passthrough_pci_addresses)
3987 
3988     def _update_compute_provider_status(self, context, service):
3989         """Calls the ComputeVirtAPI.update_compute_provider_status method
3990 
3991         :param context: nova auth RequestContext
3992         :param service: nova.objects.Service record for this host which is
3993             expected to only manage a single ComputeNode
3994         """
3995         rp_uuid = None
3996         try:
3997             rp_uuid = service.compute_node.uuid
3998             self.virtapi.update_compute_provider_status(
3999                 context, rp_uuid, enabled=not service.disabled)
4000         except Exception:
4001             LOG.warning(
4002                 'An error occurred while updating compute node '
4003                 'resource provider status to "%s" for provider: %s',
4004                 'disabled' if service.disabled else 'enabled',
4005                 rp_uuid or service.host, exc_info=True)
4006 
4007     def _set_host_enabled(self, enabled,
4008                           disable_reason=DISABLE_REASON_UNDEFINED):
4009         """Enables / Disables the compute service on this host.
4010 
4011            This doesn't override non-automatic disablement with an automatic
4012            setting; thereby permitting operators to keep otherwise
4013            healthy hosts out of rotation.
4014         """
4015 
4016         status_name = {True: 'disabled',
4017                        False: 'enabled'}
4018 
4019         disable_service = not enabled
4020 
4021         ctx = nova_context.get_admin_context()
4022         try:
4023             service = objects.Service.get_by_compute_host(ctx, CONF.host)
4024 
4025             if service.disabled != disable_service:
4026                 # Note(jang): this is a quick fix to stop operator-
4027                 # disabled compute hosts from re-enabling themselves
4028                 # automatically. We prefix any automatic reason code
4029                 # with a fixed string. We only re-enable a host
4030                 # automatically if we find that string in place.
4031                 # This should probably be replaced with a separate flag.
4032                 if not service.disabled or (
4033                         service.disabled_reason and
4034                         service.disabled_reason.startswith(DISABLE_PREFIX)):
4035                     service.disabled = disable_service
4036                     service.disabled_reason = (
4037                        DISABLE_PREFIX + disable_reason
4038                        if disable_service and disable_reason else
4039                            DISABLE_REASON_UNDEFINED)
4040                     service.save()
4041                     LOG.debug('Updating compute service status to %s',
4042                               status_name[disable_service])
4043                     # Update the disabled trait status on the corresponding
4044                     # compute node resource provider in placement.
4045                     self._update_compute_provider_status(ctx, service)
4046                 else:
4047                     LOG.debug('Not overriding manual compute service '
4048                               'status with: %s',
4049                               status_name[disable_service])
4050         except exception.ComputeHostNotFound:
4051             LOG.warning('Cannot update service status on host "%s" '
4052                         'since it is not registered.', CONF.host)
4053         except Exception:
4054             LOG.warning('Cannot update service status on host "%s" '
4055                         'due to an unexpected exception.', CONF.host,
4056                         exc_info=True)
4057 
4058         if enabled:
4059             mount.get_manager().host_up(self._host)
4060         else:
4061             mount.get_manager().host_down()
4062 
4063     def _get_guest_cpu_model_config(self):
4064         mode = CONF.libvirt.cpu_mode
4065         model = CONF.libvirt.cpu_model
4066         extra_flags = set([flag.lower() for flag in
4067             CONF.libvirt.cpu_model_extra_flags])
4068 
4069         if (CONF.libvirt.virt_type == "kvm" or
4070             CONF.libvirt.virt_type == "qemu"):
4071             if mode is None:
4072                 caps = self._host.get_capabilities()
4073                 # AArch64 lacks 'host-model' support because neither libvirt
4074                 # nor QEMU are able to tell what the host CPU model exactly is.
4075                 # And there is no CPU description code for ARM(64) at this
4076                 # point.
4077 
4078                 # Also worth noting: 'host-passthrough' mode will completely
4079                 # break live migration, *unless* all the Compute nodes (running
4080                 # libvirtd) have *identical* CPUs.
4081                 if caps.host.cpu.arch == fields.Architecture.AARCH64:
4082                     mode = "host-passthrough"
4083                     LOG.info('CPU mode "host-passthrough" was chosen. Live '
4084                              'migration can break unless all compute nodes '
4085                              'have identical cpus. AArch64 does not support '
4086                              'other modes.')
4087                 else:
4088                     mode = "host-model"
4089             if mode == "none":
4090                 return vconfig.LibvirtConfigGuestCPU()
4091         else:
4092             if mode is None or mode == "none":
4093                 return None
4094 
4095         if ((CONF.libvirt.virt_type != "kvm" and
4096              CONF.libvirt.virt_type != "qemu")):
4097             msg = _("Config requested an explicit CPU model, but "
4098                     "the current libvirt hypervisor '%s' does not "
4099                     "support selecting CPU models") % CONF.libvirt.virt_type
4100             raise exception.Invalid(msg)
4101 
4102         if mode == "custom" and model is None:
4103             msg = _("Config requested a custom CPU model, but no "
4104                     "model name was provided")
4105             raise exception.Invalid(msg)
4106         elif mode != "custom" and model is not None:
4107             msg = _("A CPU model name should not be set when a "
4108                     "host CPU model is requested")
4109             raise exception.Invalid(msg)
4110 
4111         LOG.debug("CPU mode '%(mode)s' model '%(model)s' was chosen, "
4112                   "with extra flags: '%(extra_flags)s'",
4113                   {'mode': mode,
4114                    'model': (model or ""),
4115                    'extra_flags': (extra_flags or "")})
4116 
4117         cpu = vconfig.LibvirtConfigGuestCPU()
4118         cpu.mode = mode
4119         cpu.model = model
4120 
4121         # NOTE (kchamart): Currently there's no existing way to ask if a
4122         # given CPU model + CPU flags combination is supported by KVM &
4123         # a specific QEMU binary.  However, libvirt runs the 'CPUID'
4124         # command upfront -- before even a Nova instance (a QEMU
4125         # process) is launched -- to construct CPU models and check
4126         # their validity; so we are good there.  In the long-term,
4127         # upstream libvirt intends to add an additional new API that can
4128         # do fine-grained validation of a certain CPU model + CPU flags
4129         # against a specific QEMU binary (the libvirt RFE bug for that:
4130         # https://bugzilla.redhat.com/show_bug.cgi?id=1559832).
4131         for flag in extra_flags:
4132             cpu.add_feature(vconfig.LibvirtConfigGuestCPUFeature(flag))
4133 
4134         return cpu
4135 
4136     def _get_guest_cpu_config(self, flavor, image_meta,
4137                               guest_cpu_numa_config, instance_numa_topology):
4138         cpu = self._get_guest_cpu_model_config()
4139 
4140         if cpu is None:
4141             return None
4142 
4143         topology = hardware.get_best_cpu_topology(
4144                 flavor, image_meta, numa_topology=instance_numa_topology)
4145 
4146         cpu.sockets = topology.sockets
4147         cpu.cores = topology.cores
4148         cpu.threads = topology.threads
4149         cpu.numa = guest_cpu_numa_config
4150 
4151         return cpu
4152 
4153     def _get_guest_disk_config(self, instance, name, disk_mapping, inst_type,
4154                                image_type=None):
4155         disk_unit = None
4156         disk = self.image_backend.by_name(instance, name, image_type)
4157         if (name == 'disk.config' and image_type == 'rbd' and
4158                 not disk.exists()):
4159             # This is likely an older config drive that has not been migrated
4160             # to rbd yet. Try to fall back on 'flat' image type.
4161             # TODO(melwitt): Add online migration of some sort so we can
4162             # remove this fall back once we know all config drives are in rbd.
4163             # NOTE(vladikr): make sure that the flat image exist, otherwise
4164             # the image will be created after the domain definition.
4165             flat_disk = self.image_backend.by_name(instance, name, 'flat')
4166             if flat_disk.exists():
4167                 disk = flat_disk
4168                 LOG.debug('Config drive not found in RBD, falling back to the '
4169                           'instance directory', instance=instance)
4170         disk_info = disk_mapping[name]
4171         if 'unit' in disk_mapping and disk_info['bus'] == 'scsi':
4172             disk_unit = disk_mapping['unit']
4173             disk_mapping['unit'] += 1  # Increments for the next disk added
4174         conf = disk.libvirt_info(disk_info, self.disk_cachemode,
4175                                  inst_type['extra_specs'],
4176                                  self._host.get_version(),
4177                                  disk_unit=disk_unit)
4178         return conf
4179 
4180     def _get_guest_fs_config(self, instance, name, image_type=None):
4181         disk = self.image_backend.by_name(instance, name, image_type)
4182         return disk.libvirt_fs_info("/", "ploop")
4183 
4184     def _get_guest_storage_config(self, context, instance, image_meta,
4185                                   disk_info,
4186                                   rescue, block_device_info,
4187                                   inst_type, os_type):
4188         devices = []
4189         disk_mapping = disk_info['mapping']
4190 
4191         block_device_mapping = driver.block_device_info_get_mapping(
4192             block_device_info)
4193         mount_rootfs = CONF.libvirt.virt_type == "lxc"
4194         scsi_controller = self._get_scsi_controller(image_meta)
4195 
4196         if scsi_controller and scsi_controller.model == 'virtio-scsi':
4197             # The virtio-scsi can handle up to 256 devices but the
4198             # optional element "address" must be defined to describe
4199             # where the device is placed on the controller (see:
4200             # LibvirtConfigGuestDeviceAddressDrive).
4201             #
4202             # Note about why it's added in disk_mapping: It's not
4203             # possible to pass an 'int' by reference in Python, so we
4204             # use disk_mapping as container to keep reference of the
4205             # unit added and be able to increment it for each disk
4206             # added.
4207             #
4208             # NOTE(jaypipes,melwitt): If this is a boot-from-volume instance,
4209             # we need to start the disk mapping unit at 1 since we set the
4210             # bootable volume's unit to 0 for the bootable volume.
4211             disk_mapping['unit'] = 0
4212             if self._is_booted_from_volume(block_device_info):
4213                 disk_mapping['unit'] = 1
4214 
4215         def _get_ephemeral_devices():
4216             eph_devices = []
4217             for idx, eph in enumerate(
4218                 driver.block_device_info_get_ephemerals(
4219                     block_device_info)):
4220                 diskeph = self._get_guest_disk_config(
4221                     instance,
4222                     blockinfo.get_eph_disk(idx),
4223                     disk_mapping, inst_type)
4224                 eph_devices.append(diskeph)
4225             return eph_devices
4226 
4227         if mount_rootfs:
4228             fs = vconfig.LibvirtConfigGuestFilesys()
4229             fs.source_type = "mount"
4230             fs.source_dir = os.path.join(
4231                 libvirt_utils.get_instance_path(instance), 'rootfs')
4232             devices.append(fs)
4233         elif (os_type == fields.VMMode.EXE and
4234               CONF.libvirt.virt_type == "parallels"):
4235             if rescue:
4236                 fsrescue = self._get_guest_fs_config(instance, "disk.rescue")
4237                 devices.append(fsrescue)
4238 
4239                 fsos = self._get_guest_fs_config(instance, "disk")
4240                 fsos.target_dir = "/mnt/rescue"
4241                 devices.append(fsos)
4242             else:
4243                 if 'disk' in disk_mapping:
4244                     fs = self._get_guest_fs_config(instance, "disk")
4245                     devices.append(fs)
4246                 devices = devices + _get_ephemeral_devices()
4247         else:
4248 
4249             if rescue:
4250                 diskrescue = self._get_guest_disk_config(instance,
4251                                                          'disk.rescue',
4252                                                          disk_mapping,
4253                                                          inst_type)
4254                 devices.append(diskrescue)
4255 
4256                 diskos = self._get_guest_disk_config(instance,
4257                                                      'disk',
4258                                                      disk_mapping,
4259                                                      inst_type)
4260                 devices.append(diskos)
4261             else:
4262                 if 'disk' in disk_mapping:
4263                     diskos = self._get_guest_disk_config(instance,
4264                                                          'disk',
4265                                                          disk_mapping,
4266                                                          inst_type)
4267                     devices.append(diskos)
4268 
4269                 if 'disk.local' in disk_mapping:
4270                     disklocal = self._get_guest_disk_config(instance,
4271                                                             'disk.local',
4272                                                             disk_mapping,
4273                                                             inst_type)
4274                     devices.append(disklocal)
4275                     instance.default_ephemeral_device = (
4276                         block_device.prepend_dev(disklocal.target_dev))
4277 
4278                 devices = devices + _get_ephemeral_devices()
4279 
4280                 if 'disk.swap' in disk_mapping:
4281                     diskswap = self._get_guest_disk_config(instance,
4282                                                            'disk.swap',
4283                                                            disk_mapping,
4284                                                            inst_type)
4285                     devices.append(diskswap)
4286                     instance.default_swap_device = (
4287                         block_device.prepend_dev(diskswap.target_dev))
4288 
4289             config_name = 'disk.config.rescue' if rescue else 'disk.config'
4290             if config_name in disk_mapping:
4291                 diskconfig = self._get_guest_disk_config(
4292                     instance, config_name, disk_mapping, inst_type,
4293                     self._get_disk_config_image_type())
4294                 devices.append(diskconfig)
4295 
4296         for vol in block_device.get_bdms_to_connect(block_device_mapping,
4297                                                    mount_rootfs):
4298             connection_info = vol['connection_info']
4299             vol_dev = block_device.prepend_dev(vol['mount_device'])
4300             info = disk_mapping[vol_dev]
4301             self._connect_volume(context, connection_info, instance)
4302             if scsi_controller and scsi_controller.model == 'virtio-scsi':
4303                 # Check if this is the bootable volume when in a
4304                 # boot-from-volume instance, and if so, ensure the unit
4305                 # attribute is 0.
4306                 if vol.get('boot_index') == 0:
4307                     info['unit'] = 0
4308                 else:
4309                     info['unit'] = disk_mapping['unit']
4310                     disk_mapping['unit'] += 1
4311             cfg = self._get_volume_config(connection_info, info)
4312             devices.append(cfg)
4313             vol['connection_info'] = connection_info
4314             vol.save()
4315 
4316         for d in devices:
4317             self._set_cache_mode(d)
4318 
4319         if scsi_controller:
4320             devices.append(scsi_controller)
4321 
4322         return devices
4323 
4324     @staticmethod
4325     def _get_scsi_controller(image_meta):
4326         """Return scsi controller or None based on image meta"""
4327         if image_meta.properties.get('hw_scsi_model'):
4328             hw_scsi_model = image_meta.properties.hw_scsi_model
4329             scsi_controller = vconfig.LibvirtConfigGuestController()
4330             scsi_controller.type = 'scsi'
4331             scsi_controller.model = hw_scsi_model
4332             scsi_controller.index = 0
4333             return scsi_controller
4334 
4335     def _get_host_sysinfo_serial_hardware(self):
4336         """Get a UUID from the host hardware
4337 
4338         Get a UUID for the host hardware reported by libvirt.
4339         This is typically from the SMBIOS data, unless it has
4340         been overridden in /etc/libvirt/libvirtd.conf
4341         """
4342         caps = self._host.get_capabilities()
4343         return caps.host.uuid
4344 
4345     def _get_host_sysinfo_serial_os(self):
4346         """Get a UUID from the host operating system
4347 
4348         Get a UUID for the host operating system. Modern Linux
4349         distros based on systemd provide a /etc/machine-id
4350         file containing a UUID. This is also provided inside
4351         systemd based containers and can be provided by other
4352         init systems too, since it is just a plain text file.
4353         """
4354         if not os.path.exists("/etc/machine-id"):
4355             msg = _("Unable to get host UUID: /etc/machine-id does not exist")
4356             raise exception.InternalError(msg)
4357 
4358         with open("/etc/machine-id") as f:
4359             # We want to have '-' in the right place
4360             # so we parse & reformat the value
4361             lines = f.read().split()
4362             if not lines:
4363                 msg = _("Unable to get host UUID: /etc/machine-id is empty")
4364                 raise exception.InternalError(msg)
4365 
4366             return str(uuid.UUID(lines[0]))
4367 
4368     def _get_host_sysinfo_serial_auto(self):
4369         if os.path.exists("/etc/machine-id"):
4370             return self._get_host_sysinfo_serial_os()
4371         else:
4372             return self._get_host_sysinfo_serial_hardware()
4373 
4374     def _get_guest_config_sysinfo(self, instance):
4375         sysinfo = vconfig.LibvirtConfigGuestSysinfo()
4376 
4377         sysinfo.system_manufacturer = version.vendor_string()
4378         sysinfo.system_product = version.product_string()
4379         sysinfo.system_version = version.version_string_with_package()
4380 
4381         if CONF.libvirt.sysinfo_serial == 'unique':
4382             sysinfo.system_serial = instance.uuid
4383         else:
4384             sysinfo.system_serial = self._sysinfo_serial_func()
4385         sysinfo.system_uuid = instance.uuid
4386 
4387         sysinfo.system_family = "Virtual Machine"
4388 
4389         return sysinfo
4390 
4391     def _get_guest_pci_device(self, pci_device):
4392 
4393         dbsf = pci_utils.parse_address(pci_device.address)
4394         dev = vconfig.LibvirtConfigGuestHostdevPCI()
4395         dev.domain, dev.bus, dev.slot, dev.function = dbsf
4396 
4397         # only kvm support managed mode
4398         if CONF.libvirt.virt_type in ('xen', 'parallels',):
4399             dev.managed = 'no'
4400         if CONF.libvirt.virt_type in ('kvm', 'qemu'):
4401             dev.managed = 'yes'
4402 
4403         return dev
4404 
4405     def _get_guest_config_meta(self, instance):
4406         """Get metadata config for guest."""
4407 
4408         meta = vconfig.LibvirtConfigGuestMetaNovaInstance()
4409         meta.package = version.version_string_with_package()
4410         meta.name = instance.display_name
4411         meta.creationTime = time.time()
4412 
4413         if instance.image_ref not in ("", None):
4414             meta.roottype = "image"
4415             meta.rootid = instance.image_ref
4416 
4417         system_meta = instance.system_metadata
4418         ometa = vconfig.LibvirtConfigGuestMetaNovaOwner()
4419         ometa.userid = instance.user_id
4420         ometa.username = system_meta.get('owner_user_name', 'N/A')
4421         ometa.projectid = instance.project_id
4422         ometa.projectname = system_meta.get('owner_project_name', 'N/A')
4423         meta.owner = ometa
4424 
4425         fmeta = vconfig.LibvirtConfigGuestMetaNovaFlavor()
4426         flavor = instance.flavor
4427         fmeta.name = flavor.name
4428         fmeta.memory = flavor.memory_mb
4429         fmeta.vcpus = flavor.vcpus
4430         fmeta.ephemeral = flavor.ephemeral_gb
4431         fmeta.disk = flavor.root_gb
4432         fmeta.swap = flavor.swap
4433 
4434         meta.flavor = fmeta
4435 
4436         return meta
4437 
4438     @staticmethod
4439     def _create_idmaps(klass, map_strings):
4440         idmaps = []
4441         if len(map_strings) > 5:
4442             map_strings = map_strings[0:5]
4443             LOG.warning("Too many id maps, only included first five.")
4444         for map_string in map_strings:
4445             try:
4446                 idmap = klass()
4447                 values = [int(i) for i in map_string.split(":")]
4448                 idmap.start = values[0]
4449                 idmap.target = values[1]
4450                 idmap.count = values[2]
4451                 idmaps.append(idmap)
4452             except (ValueError, IndexError):
4453                 LOG.warning("Invalid value for id mapping %s", map_string)
4454         return idmaps
4455 
4456     def _get_guest_idmaps(self):
4457         id_maps = []
4458         if CONF.libvirt.virt_type == 'lxc' and CONF.libvirt.uid_maps:
4459             uid_maps = self._create_idmaps(vconfig.LibvirtConfigGuestUIDMap,
4460                                            CONF.libvirt.uid_maps)
4461             id_maps.extend(uid_maps)
4462         if CONF.libvirt.virt_type == 'lxc' and CONF.libvirt.gid_maps:
4463             gid_maps = self._create_idmaps(vconfig.LibvirtConfigGuestGIDMap,
4464                                            CONF.libvirt.gid_maps)
4465             id_maps.extend(gid_maps)
4466         return id_maps
4467 
4468     def _update_guest_cputune(self, guest, flavor, virt_type):
4469         is_able = self._host.is_cpu_control_policy_capable()
4470 
4471         cputuning = ['shares', 'period', 'quota']
4472         wants_cputune = any([k for k in cputuning
4473             if "quota:cpu_" + k in flavor.extra_specs.keys()])
4474 
4475         if wants_cputune and not is_able:
4476             raise exception.UnsupportedHostCPUControlPolicy()
4477 
4478         if not is_able or virt_type not in ('lxc', 'kvm', 'qemu'):
4479             return
4480 
4481         if guest.cputune is None:
4482             guest.cputune = vconfig.LibvirtConfigGuestCPUTune()
4483             # Setting the default cpu.shares value to be a value
4484             # dependent on the number of vcpus
4485         guest.cputune.shares = 1024 * guest.vcpus
4486 
4487         for name in cputuning:
4488             key = "quota:cpu_" + name
4489             if key in flavor.extra_specs:
4490                 setattr(guest.cputune, name,
4491                         int(flavor.extra_specs[key]))
4492 
4493     def _get_cpu_numa_config_from_instance(self, instance_numa_topology,
4494                                            wants_hugepages):
4495         if instance_numa_topology:
4496             guest_cpu_numa = vconfig.LibvirtConfigGuestCPUNUMA()
4497             for instance_cell in instance_numa_topology.cells:
4498                 guest_cell = vconfig.LibvirtConfigGuestCPUNUMACell()
4499                 guest_cell.id = instance_cell.id
4500                 guest_cell.cpus = instance_cell.cpuset
4501                 guest_cell.memory = instance_cell.memory * units.Ki
4502 
4503                 # The vhost-user network backend requires file backed
4504                 # guest memory (ie huge pages) to be marked as shared
4505                 # access, not private, so an external process can read
4506                 # and write the pages.
4507                 #
4508                 # You can't change the shared vs private flag for an
4509                 # already running guest, and since we can't predict what
4510                 # types of NIC may be hotplugged, we have no choice but
4511                 # to unconditionally turn on the shared flag. This has
4512                 # no real negative functional effect on the guest, so
4513                 # is a reasonable approach to take
4514                 if wants_hugepages:
4515                     guest_cell.memAccess = "shared"
4516                 guest_cpu_numa.cells.append(guest_cell)
4517             return guest_cpu_numa
4518 
4519     def _wants_hugepages(self, host_topology, instance_topology):
4520         """Determine if the guest / host topology implies the
4521            use of huge pages for guest RAM backing
4522         """
4523 
4524         if host_topology is None or instance_topology is None:
4525             return False
4526 
4527         avail_pagesize = [page.size_kb
4528                           for page in host_topology.cells[0].mempages]
4529         avail_pagesize.sort()
4530         # Remove smallest page size as that's not classed as a largepage
4531         avail_pagesize = avail_pagesize[1:]
4532 
4533         # See if we have page size set
4534         for cell in instance_topology.cells:
4535             if (cell.pagesize is not None and
4536                 cell.pagesize in avail_pagesize):
4537                 return True
4538 
4539         return False
4540 
4541     def _get_cell_pairs(self, guest_cpu_numa_config, host_topology):
4542         """Returns the lists of pairs(tuple) of an instance cell and
4543         corresponding host cell:
4544             [(LibvirtConfigGuestCPUNUMACell, NUMACell), ...]
4545         """
4546         cell_pairs = []
4547         for guest_config_cell in guest_cpu_numa_config.cells:
4548             for host_cell in host_topology.cells:
4549                 if guest_config_cell.id == host_cell.id:
4550                     cell_pairs.append((guest_config_cell, host_cell))
4551         return cell_pairs
4552 
4553     def _get_pin_cpuset(self, vcpu, object_numa_cell, host_cell):
4554         """Returns the config object of LibvirtConfigGuestCPUTuneVCPUPin.
4555         Prepares vcpupin config for the guest with the following caveats:
4556 
4557             a) If there is pinning information in the cell, we pin vcpus to
4558                individual CPUs
4559             b) Otherwise we float over the whole host NUMA node
4560         """
4561         pin_cpuset = vconfig.LibvirtConfigGuestCPUTuneVCPUPin()
4562         pin_cpuset.id = vcpu
4563 
4564         if object_numa_cell.cpu_pinning:
4565             pin_cpuset.cpuset = set([object_numa_cell.cpu_pinning[vcpu]])
4566         else:
4567             pin_cpuset.cpuset = host_cell.cpuset
4568 
4569         return pin_cpuset
4570 
4571     def _get_emulatorpin_cpuset(self, vcpu, object_numa_cell, vcpus_rt,
4572                                 emulator_threads_policy, wants_realtime,
4573                                 pin_cpuset):
4574         """Returns a set of cpu_ids to add to the cpuset for emulator threads
4575            with the following caveats:
4576 
4577             a) If emulator threads policy is isolated, we pin emulator threads
4578                to one cpu we have reserved for it.
4579             b) If emulator threads policy is shared and CONF.cpu_shared_set is
4580                defined, we pin emulator threads on the set of pCPUs defined by
4581                CONF.cpu_shared_set
4582             c) Otherwise;
4583                 c1) If realtime IS NOT enabled, the emulator threads are
4584                     allowed to float cross all the pCPUs associated with
4585                     the guest vCPUs.
4586                 c2) If realtime IS enabled, at least 1 vCPU is required
4587                     to be set aside for non-realtime usage. The emulator
4588                     threads are allowed to float across the pCPUs that
4589                     are associated with the non-realtime VCPUs.
4590         """
4591         emulatorpin_cpuset = set([])
4592         shared_ids = hardware.get_cpu_shared_set()
4593 
4594         if emulator_threads_policy == fields.CPUEmulatorThreadsPolicy.ISOLATE:
4595             if object_numa_cell.cpuset_reserved:
4596                 emulatorpin_cpuset = object_numa_cell.cpuset_reserved
4597         elif ((emulator_threads_policy ==
4598               fields.CPUEmulatorThreadsPolicy.SHARE) and
4599               shared_ids):
4600             online_pcpus = self._host.get_online_cpus()
4601             cpuset = shared_ids & online_pcpus
4602             if not cpuset:
4603                 msg = (_("Invalid cpu_shared_set config, one or more of the "
4604                          "specified cpuset is not online. Online cpuset(s): "
4605                          "%(online)s, requested cpuset(s): %(req)s"),
4606                        {'online': sorted(online_pcpus),
4607                         'req': sorted(shared_ids)})
4608                 raise exception.Invalid(msg)
4609             emulatorpin_cpuset = cpuset
4610         elif not wants_realtime or vcpu not in vcpus_rt:
4611             emulatorpin_cpuset = pin_cpuset.cpuset
4612 
4613         return emulatorpin_cpuset
4614 
4615     def _get_guest_numa_config(self, instance_numa_topology, flavor,
4616                                allowed_cpus=None, image_meta=None):
4617         """Returns the config objects for the guest NUMA specs.
4618 
4619         Determines the CPUs that the guest can be pinned to if the guest
4620         specifies a cell topology and the host supports it. Constructs the
4621         libvirt XML config object representing the NUMA topology selected
4622         for the guest. Returns a tuple of:
4623 
4624             (cpu_set, guest_cpu_tune, guest_cpu_numa, guest_numa_tune)
4625 
4626         With the following caveats:
4627 
4628             a) If there is no specified guest NUMA topology, then
4629                all tuple elements except cpu_set shall be None. cpu_set
4630                will be populated with the chosen CPUs that the guest
4631                allowed CPUs fit within, which could be the supplied
4632                allowed_cpus value if the host doesn't support NUMA
4633                topologies.
4634 
4635             b) If there is a specified guest NUMA topology, then
4636                cpu_set will be None and guest_cpu_numa will be the
4637                LibvirtConfigGuestCPUNUMA object representing the guest's
4638                NUMA topology. If the host supports NUMA, then guest_cpu_tune
4639                will contain a LibvirtConfigGuestCPUTune object representing
4640                the optimized chosen cells that match the host capabilities
4641                with the instance's requested topology. If the host does
4642                not support NUMA, then guest_cpu_tune and guest_numa_tune
4643                will be None.
4644         """
4645 
4646         if (not self._has_numa_support() and
4647                 instance_numa_topology is not None):
4648             # We should not get here, since we should have avoided
4649             # reporting NUMA topology from _get_host_numa_topology
4650             # in the first place. Just in case of a scheduler
4651             # mess up though, raise an exception
4652             raise exception.NUMATopologyUnsupported()
4653 
4654         topology = self._get_host_numa_topology()
4655 
4656         # We have instance NUMA so translate it to the config class
4657         guest_cpu_numa_config = self._get_cpu_numa_config_from_instance(
4658                 instance_numa_topology,
4659                 self._wants_hugepages(topology, instance_numa_topology))
4660 
4661         if not guest_cpu_numa_config:
4662             # No NUMA topology defined for instance - let the host kernel deal
4663             # with the NUMA effects.
4664             # TODO(ndipanov): Attempt to spread the instance
4665             # across NUMA nodes and expose the topology to the
4666             # instance as an optimisation
4667             return GuestNumaConfig(allowed_cpus, None, None, None)
4668 
4669         if not topology:
4670             # No NUMA topology defined for host - This will only happen with
4671             # some libvirt versions and certain platforms.
4672             return GuestNumaConfig(allowed_cpus, None,
4673                                    guest_cpu_numa_config, None)
4674 
4675         # Now get configuration from the numa_topology
4676         # Init CPUTune configuration
4677         guest_cpu_tune = vconfig.LibvirtConfigGuestCPUTune()
4678         guest_cpu_tune.emulatorpin = (
4679             vconfig.LibvirtConfigGuestCPUTuneEmulatorPin())
4680         guest_cpu_tune.emulatorpin.cpuset = set([])
4681 
4682         # Init NUMATune configuration
4683         guest_numa_tune = vconfig.LibvirtConfigGuestNUMATune()
4684         guest_numa_tune.memory = vconfig.LibvirtConfigGuestNUMATuneMemory()
4685         guest_numa_tune.memnodes = []
4686 
4687         emulator_threads_policy = None
4688         if 'emulator_threads_policy' in instance_numa_topology:
4689             emulator_threads_policy = (
4690                 instance_numa_topology.emulator_threads_policy)
4691 
4692         # Set realtime scheduler for CPUTune
4693         vcpus_rt = set([])
4694         wants_realtime = hardware.is_realtime_enabled(flavor)
4695         if wants_realtime:
4696             vcpus_rt = hardware.vcpus_realtime_topology(flavor, image_meta)
4697             vcpusched = vconfig.LibvirtConfigGuestCPUTuneVCPUSched()
4698             designer.set_vcpu_realtime_scheduler(
4699                 vcpusched, vcpus_rt, CONF.libvirt.realtime_scheduler_priority)
4700             guest_cpu_tune.vcpusched.append(vcpusched)
4701 
4702         cell_pairs = self._get_cell_pairs(guest_cpu_numa_config, topology)
4703         for guest_node_id, (guest_config_cell, host_cell) in enumerate(
4704                 cell_pairs):
4705             # set NUMATune for the cell
4706             tnode = vconfig.LibvirtConfigGuestNUMATuneMemNode()
4707             designer.set_numa_memnode(tnode, guest_node_id, host_cell.id)
4708             guest_numa_tune.memnodes.append(tnode)
4709             guest_numa_tune.memory.nodeset.append(host_cell.id)
4710 
4711             # set CPUTune for the cell
4712             object_numa_cell = instance_numa_topology.cells[guest_node_id]
4713             for cpu in guest_config_cell.cpus:
4714                 pin_cpuset = self._get_pin_cpuset(cpu, object_numa_cell,
4715                                                   host_cell)
4716                 guest_cpu_tune.vcpupin.append(pin_cpuset)
4717 
4718                 emu_pin_cpuset = self._get_emulatorpin_cpuset(
4719                     cpu, object_numa_cell, vcpus_rt,
4720                     emulator_threads_policy, wants_realtime, pin_cpuset)
4721                 guest_cpu_tune.emulatorpin.cpuset.update(emu_pin_cpuset)
4722 
4723         # TODO(berrange) When the guest has >1 NUMA node, it will
4724         # span multiple host NUMA nodes. By pinning emulator threads
4725         # to the union of all nodes, we guarantee there will be
4726         # cross-node memory access by the emulator threads when
4727         # responding to guest I/O operations. The only way to avoid
4728         # this would be to pin emulator threads to a single node and
4729         # tell the guest OS to only do I/O from one of its virtual
4730         # NUMA nodes. This is not even remotely practical.
4731         #
4732         # The long term solution is to make use of a new QEMU feature
4733         # called "I/O Threads" which will let us configure an explicit
4734         # I/O thread for each guest vCPU or guest NUMA node. It is
4735         # still TBD how to make use of this feature though, especially
4736         # how to associate IO threads with guest devices to eliminate
4737         # cross NUMA node traffic. This is an area of investigation
4738         # for QEMU community devs.
4739 
4740         # Sort the vcpupin list per vCPU id for human-friendlier XML
4741         guest_cpu_tune.vcpupin.sort(key=operator.attrgetter("id"))
4742 
4743         # normalize cell.id
4744         for i, (cell, memnode) in enumerate(zip(guest_cpu_numa_config.cells,
4745                                                 guest_numa_tune.memnodes)):
4746             cell.id = i
4747             memnode.cellid = i
4748 
4749         return GuestNumaConfig(None, guest_cpu_tune, guest_cpu_numa_config,
4750                                guest_numa_tune)
4751 
4752     def _get_guest_os_type(self, virt_type):
4753         """Returns the guest OS type based on virt type."""
4754         if virt_type == "lxc":
4755             ret = fields.VMMode.EXE
4756         elif virt_type == "uml":
4757             ret = fields.VMMode.UML
4758         elif virt_type == "xen":
4759             ret = fields.VMMode.XEN
4760         else:
4761             ret = fields.VMMode.HVM
4762         return ret
4763 
4764     def _set_guest_for_rescue(self, rescue, guest, inst_path, virt_type,
4765                               root_device_name):
4766         if rescue.get('kernel_id'):
4767             guest.os_kernel = os.path.join(inst_path, "kernel.rescue")
4768             guest.os_cmdline = ("root=%s %s" % (root_device_name, CONSOLE))
4769             if virt_type == "qemu":
4770                 guest.os_cmdline += " no_timer_check"
4771         if rescue.get('ramdisk_id'):
4772             guest.os_initrd = os.path.join(inst_path, "ramdisk.rescue")
4773 
4774     def _set_guest_for_inst_kernel(self, instance, guest, inst_path, virt_type,
4775                                 root_device_name, image_meta):
4776         guest.os_kernel = os.path.join(inst_path, "kernel")
4777         guest.os_cmdline = ("root=%s %s" % (root_device_name, CONSOLE))
4778         if virt_type == "qemu":
4779             guest.os_cmdline += " no_timer_check"
4780         if instance.ramdisk_id:
4781             guest.os_initrd = os.path.join(inst_path, "ramdisk")
4782         # we only support os_command_line with images with an explicit
4783         # kernel set and don't want to break nova if there's an
4784         # os_command_line property without a specified kernel_id param
4785         if image_meta.properties.get("os_command_line"):
4786             guest.os_cmdline = image_meta.properties.os_command_line
4787 
4788     def _set_clock(self, guest, os_type, image_meta, virt_type):
4789         # NOTE(mikal): Microsoft Windows expects the clock to be in
4790         # "localtime". If the clock is set to UTC, then you can use a
4791         # registry key to let windows know, but Microsoft says this is
4792         # buggy in http://support.microsoft.com/kb/2687252
4793         clk = vconfig.LibvirtConfigGuestClock()
4794         if os_type == 'windows':
4795             LOG.info('Configuring timezone for windows instance to localtime')
4796             clk.offset = 'localtime'
4797         else:
4798             clk.offset = 'utc'
4799         guest.set_clock(clk)
4800 
4801         if virt_type == "kvm":
4802             self._set_kvm_timers(clk, os_type, image_meta)
4803 
4804     def _set_kvm_timers(self, clk, os_type, image_meta):
4805         # TODO(berrange) One day this should be per-guest
4806         # OS type configurable
4807         tmpit = vconfig.LibvirtConfigGuestTimer()
4808         tmpit.name = "pit"
4809         tmpit.tickpolicy = "delay"
4810 
4811         tmrtc = vconfig.LibvirtConfigGuestTimer()
4812         tmrtc.name = "rtc"
4813         tmrtc.tickpolicy = "catchup"
4814 
4815         clk.add_timer(tmpit)
4816         clk.add_timer(tmrtc)
4817 
4818         hpet = image_meta.properties.get('hw_time_hpet', False)
4819         guestarch = libvirt_utils.get_arch(image_meta)
4820         if guestarch in (fields.Architecture.I686,
4821                          fields.Architecture.X86_64):
4822             # NOTE(rfolco): HPET is a hardware timer for x86 arch.
4823             # qemu -no-hpet is not supported on non-x86 targets.
4824             tmhpet = vconfig.LibvirtConfigGuestTimer()
4825             tmhpet.name = "hpet"
4826             tmhpet.present = hpet
4827             clk.add_timer(tmhpet)
4828         else:
4829             if hpet:
4830                 LOG.warning('HPET is not turned on for non-x86 guests in image'
4831                             ' %s.', image_meta.id)
4832 
4833         # Provide Windows guests with the paravirtualized hyperv timer source.
4834         # This is the windows equiv of kvm-clock, allowing Windows
4835         # guests to accurately keep time.
4836         if os_type == 'windows':
4837             tmhyperv = vconfig.LibvirtConfigGuestTimer()
4838             tmhyperv.name = "hypervclock"
4839             tmhyperv.present = True
4840             clk.add_timer(tmhyperv)
4841 
4842     def _set_features(self, guest, os_type, caps, virt_type, image_meta,
4843             flavor):
4844         hide_hypervisor_id = (strutils.bool_from_string(
4845                 flavor.extra_specs.get('hide_hypervisor_id')) or
4846             image_meta.properties.get('img_hide_hypervisor_id'))
4847 
4848         if virt_type == "xen":
4849             # PAE only makes sense in X86
4850             if caps.host.cpu.arch in (fields.Architecture.I686,
4851                                       fields.Architecture.X86_64):
4852                 guest.features.append(vconfig.LibvirtConfigGuestFeaturePAE())
4853 
4854         if (virt_type not in ("lxc", "uml", "parallels", "xen") or
4855                 (virt_type == "xen" and guest.os_type == fields.VMMode.HVM)):
4856             guest.features.append(vconfig.LibvirtConfigGuestFeatureACPI())
4857             guest.features.append(vconfig.LibvirtConfigGuestFeatureAPIC())
4858 
4859         if (virt_type in ("qemu", "kvm") and
4860                 os_type == 'windows'):
4861             hv = vconfig.LibvirtConfigGuestFeatureHyperV()
4862             hv.relaxed = True
4863 
4864             hv.spinlocks = True
4865             # Increase spinlock retries - value recommended by
4866             # KVM maintainers who certify Windows guests
4867             # with Microsoft
4868             hv.spinlock_retries = 8191
4869             hv.vapic = True
4870 
4871             # NOTE(kosamara): Spoofing the vendor_id aims to allow the nvidia
4872             # driver to work on windows VMs. At the moment, the nvidia driver
4873             # checks for the hyperv vendorid, and if it doesn't find that, it
4874             # works. In the future, its behaviour could become more strict,
4875             # checking for the presence of other hyperv feature flags to
4876             # determine that it's loaded in a VM. If that happens, this
4877             # workaround will not be enough, and we'll need to drop the whole
4878             # hyperv element.
4879             # That would disable some optimizations, reducing the guest's
4880             # performance.
4881             if hide_hypervisor_id:
4882                 hv.vendorid_spoof = True
4883 
4884             guest.features.append(hv)
4885 
4886         if (virt_type in ("qemu", "kvm") and hide_hypervisor_id):
4887             guest.features.append(vconfig.LibvirtConfigGuestFeatureKvmHidden())
4888 
4889     def _check_number_of_serial_console(self, num_ports):
4890         virt_type = CONF.libvirt.virt_type
4891         if (virt_type in ("kvm", "qemu") and
4892             num_ports > ALLOWED_QEMU_SERIAL_PORTS):
4893             raise exception.SerialPortNumberLimitExceeded(
4894                 allowed=ALLOWED_QEMU_SERIAL_PORTS, virt_type=virt_type)
4895 
4896     def _video_model_supported(self, model):
4897         if model not in fields.VideoModel.ALL:
4898             return False
4899         min_ver = MIN_LIBVIRT_VIDEO_MODEL_VERSIONS.get(model)
4900         if min_ver and not self._host.has_min_version(lv_ver=min_ver):
4901             return False
4902         return True
4903 
4904     def _add_video_driver(self, guest, image_meta, flavor):
4905         video = vconfig.LibvirtConfigGuestVideo()
4906         # NOTE(ldbragst): The following logic sets the video.type
4907         # depending on supported defaults given the architecture,
4908         # virtualization type, and features. The video.type attribute can
4909         # be overridden by the user with image_meta.properties, which
4910         # is carried out in the next if statement below this one.
4911         guestarch = libvirt_utils.get_arch(image_meta)
4912         if guest.os_type == fields.VMMode.XEN:
4913             video.type = 'xen'
4914         elif CONF.libvirt.virt_type == 'parallels':
4915             video.type = 'vga'
4916         elif guestarch in (fields.Architecture.PPC,
4917                            fields.Architecture.PPC64,
4918                            fields.Architecture.PPC64LE):
4919             # NOTE(ldbragst): PowerKVM doesn't support 'cirrus' be default
4920             # so use 'vga' instead when running on Power hardware.
4921             video.type = 'vga'
4922         elif guestarch == fields.Architecture.AARCH64:
4923             # NOTE(kevinz): Only virtio device type is supported by AARCH64
4924             # so use 'virtio' instead when running on AArch64 hardware.
4925             video.type = 'virtio'
4926         elif CONF.spice.enabled:
4927             video.type = 'qxl'
4928         if image_meta.properties.get('hw_video_model'):
4929             video.type = image_meta.properties.hw_video_model
4930             if not self._video_model_supported(video.type):
4931                 raise exception.InvalidVideoMode(model=video.type)
4932 
4933         # Set video memory, only if the flavor's limit is set
4934         video_ram = image_meta.properties.get('hw_video_ram', 0)
4935         max_vram = int(flavor.extra_specs.get('hw_video:ram_max_mb', 0))
4936         if video_ram > max_vram:
4937             raise exception.RequestedVRamTooHigh(req_vram=video_ram,
4938                                                  max_vram=max_vram)
4939         if max_vram and video_ram:
4940             video.vram = video_ram * units.Mi / units.Ki
4941         guest.add_device(video)
4942 
4943         # NOTE(sean-k-mooney): return the video device we added
4944         # for simpler testing.
4945         return video
4946 
4947     def _add_qga_device(self, guest, instance):
4948         qga = vconfig.LibvirtConfigGuestChannel()
4949         qga.type = "unix"
4950         qga.target_name = "org.qemu.guest_agent.0"
4951         qga.source_path = ("/var/lib/libvirt/qemu/%s.%s.sock" %
4952                           ("org.qemu.guest_agent.0", instance.name))
4953         guest.add_device(qga)
4954 
4955     def _add_rng_device(self, guest, flavor):
4956         rng_device = vconfig.LibvirtConfigGuestRng()
4957         rate_bytes = flavor.extra_specs.get('hw_rng:rate_bytes', 0)
4958         period = flavor.extra_specs.get('hw_rng:rate_period', 0)
4959         if rate_bytes:
4960             rng_device.rate_bytes = int(rate_bytes)
4961             rng_device.rate_period = int(period)
4962         rng_path = CONF.libvirt.rng_dev_path
4963         if (rng_path and not os.path.exists(rng_path)):
4964             raise exception.RngDeviceNotExist(path=rng_path)
4965         rng_device.backend = rng_path
4966         guest.add_device(rng_device)
4967 
4968     def _set_qemu_guest_agent(self, guest, flavor, instance, image_meta):
4969         # Enable qga only if the 'hw_qemu_guest_agent' is equal to yes
4970         if image_meta.properties.get('hw_qemu_guest_agent', False):
4971             LOG.debug("Qemu guest agent is enabled through image "
4972                       "metadata", instance=instance)
4973             self._add_qga_device(guest, instance)
4974         rng_is_virtio = image_meta.properties.get('hw_rng_model') == 'virtio'
4975         rng_allowed_str = flavor.extra_specs.get('hw_rng:allowed', '')
4976         rng_allowed = strutils.bool_from_string(rng_allowed_str)
4977         if rng_is_virtio and rng_allowed:
4978             self._add_rng_device(guest, flavor)
4979 
4980     def _get_guest_memory_backing_config(
4981             self, inst_topology, numatune, flavor):
4982         wantsmempages = False
4983         if inst_topology:
4984             for cell in inst_topology.cells:
4985                 if cell.pagesize:
4986                     wantsmempages = True
4987                     break
4988 
4989         wantsrealtime = hardware.is_realtime_enabled(flavor)
4990 
4991         wantsfilebacked = CONF.libvirt.file_backed_memory > 0
4992 
4993         if wantsmempages and wantsfilebacked:
4994             # Can't use file-backed memory with hugepages
4995             LOG.warning("Instance requested huge pages, but file-backed "
4996                     "memory is enabled, and incompatible with huge pages")
4997             raise exception.MemoryPagesUnsupported()
4998 
4999         membacking = None
5000         if wantsmempages:
5001             pages = self._get_memory_backing_hugepages_support(
5002                 inst_topology, numatune)
5003             if pages:
5004                 membacking = vconfig.LibvirtConfigGuestMemoryBacking()
5005                 membacking.hugepages = pages
5006         if wantsrealtime:
5007             if not membacking:
5008                 membacking = vconfig.LibvirtConfigGuestMemoryBacking()
5009             membacking.locked = True
5010             membacking.sharedpages = False
5011         if wantsfilebacked:
5012             if not membacking:
5013                 membacking = vconfig.LibvirtConfigGuestMemoryBacking()
5014             membacking.filesource = True
5015             membacking.sharedaccess = True
5016             membacking.allocateimmediate = True
5017             if self._host.has_min_version(
5018                     MIN_LIBVIRT_FILE_BACKED_DISCARD_VERSION,
5019                     MIN_QEMU_FILE_BACKED_DISCARD_VERSION):
5020                 membacking.discard = True
5021 
5022         return membacking
5023 
5024     def _get_memory_backing_hugepages_support(self, inst_topology, numatune):
5025         if not self._has_numa_support():
5026             # We should not get here, since we should have avoided
5027             # reporting NUMA topology from _get_host_numa_topology
5028             # in the first place. Just in case of a scheduler
5029             # mess up though, raise an exception
5030             raise exception.MemoryPagesUnsupported()
5031 
5032         host_topology = self._get_host_numa_topology()
5033 
5034         if host_topology is None:
5035             # As above, we should not get here but just in case...
5036             raise exception.MemoryPagesUnsupported()
5037 
5038         # Currently libvirt does not support the smallest
5039         # pagesize set as a backend memory.
5040         # https://bugzilla.redhat.com/show_bug.cgi?id=1173507
5041         avail_pagesize = [page.size_kb
5042                           for page in host_topology.cells[0].mempages]
5043         avail_pagesize.sort()
5044         smallest = avail_pagesize[0]
5045 
5046         pages = []
5047         for guest_cellid, inst_cell in enumerate(inst_topology.cells):
5048             if inst_cell.pagesize and inst_cell.pagesize > smallest:
5049                 for memnode in numatune.memnodes:
5050                     if guest_cellid == memnode.cellid:
5051                         page = (
5052                             vconfig.LibvirtConfigGuestMemoryBackingPage())
5053                         page.nodeset = [guest_cellid]
5054                         page.size_kb = inst_cell.pagesize
5055                         pages.append(page)
5056                         break  # Quit early...
5057         return pages
5058 
5059     def _get_flavor(self, ctxt, instance, flavor):
5060         if flavor is not None:
5061             return flavor
5062         return instance.flavor
5063 
5064     def _has_uefi_support(self):
5065         # This means that the host can support uefi booting for guests
5066         supported_archs = [fields.Architecture.X86_64,
5067                            fields.Architecture.AARCH64]
5068         caps = self._host.get_capabilities()
5069         return ((caps.host.cpu.arch in supported_archs) and
5070                 os.path.exists(DEFAULT_UEFI_LOADER_PATH[caps.host.cpu.arch]))
5071 
5072     def _get_supported_perf_events(self):
5073 
5074         if (len(CONF.libvirt.enabled_perf_events) == 0):
5075             return []
5076 
5077         supported_events = []
5078         host_cpu_info = self._get_cpu_info()
5079         for event in CONF.libvirt.enabled_perf_events:
5080             if self._supported_perf_event(event, host_cpu_info['features']):
5081                 supported_events.append(event)
5082         return supported_events
5083 
5084     def _supported_perf_event(self, event, cpu_features):
5085 
5086         libvirt_perf_event_name = LIBVIRT_PERF_EVENT_PREFIX + event.upper()
5087 
5088         if not hasattr(libvirt, libvirt_perf_event_name):
5089             LOG.warning("Libvirt doesn't support event type %s.", event)
5090             return False
5091 
5092         if event in PERF_EVENTS_CPU_FLAG_MAPPING:
5093             LOG.warning('Monitoring Intel CMT `perf` event(s) %s is '
5094                         'deprecated and will be removed in the "Stein" '
5095                         'release.  It was broken by design in the '
5096                         'Linux kernel, so support for Intel CMT was '
5097                         'removed from Linux 4.14 onwards. Therefore '
5098                         'it is recommended to not enable them.',
5099                         event)
5100             if PERF_EVENTS_CPU_FLAG_MAPPING[event] not in cpu_features:
5101                 LOG.warning("Host does not support event type %s.", event)
5102                 return False
5103         return True
5104 
5105     def _configure_guest_by_virt_type(self, guest, virt_type, caps, instance,
5106                                       image_meta, flavor, root_device_name):
5107         if virt_type == "xen":
5108             if guest.os_type == fields.VMMode.HVM:
5109                 guest.os_loader = CONF.libvirt.xen_hvmloader_path
5110             else:
5111                 guest.os_cmdline = CONSOLE
5112         elif virt_type in ("kvm", "qemu"):
5113             if caps.host.cpu.arch in (fields.Architecture.I686,
5114                                       fields.Architecture.X86_64):
5115                 guest.sysinfo = self._get_guest_config_sysinfo(instance)
5116                 guest.os_smbios = vconfig.LibvirtConfigGuestSMBIOS()
5117             hw_firmware_type = image_meta.properties.get('hw_firmware_type')
5118             if caps.host.cpu.arch == fields.Architecture.AARCH64:
5119                 if not hw_firmware_type:
5120                     hw_firmware_type = fields.FirmwareType.UEFI
5121             if hw_firmware_type == fields.FirmwareType.UEFI:
5122                 if self._has_uefi_support():
5123                     global uefi_logged
5124                     if not uefi_logged:
5125                         LOG.warning("uefi support is without some kind of "
5126                                     "functional testing and therefore "
5127                                     "considered experimental.")
5128                         uefi_logged = True
5129                     guest.os_loader = DEFAULT_UEFI_LOADER_PATH[
5130                         caps.host.cpu.arch]
5131                     guest.os_loader_type = "pflash"
5132                 else:
5133                     raise exception.UEFINotSupported()
5134             guest.os_mach_type = libvirt_utils.get_machine_type(image_meta)
5135             if image_meta.properties.get('hw_boot_menu') is None:
5136                 guest.os_bootmenu = strutils.bool_from_string(
5137                     flavor.extra_specs.get('hw:boot_menu', 'no'))
5138             else:
5139                 guest.os_bootmenu = image_meta.properties.hw_boot_menu
5140 
5141         elif virt_type == "lxc":
5142             guest.os_init_path = "/sbin/init"
5143             guest.os_cmdline = CONSOLE
5144         elif virt_type == "uml":
5145             guest.os_kernel = "/usr/bin/linux"
5146             guest.os_root = root_device_name
5147         elif virt_type == "parallels":
5148             if guest.os_type == fields.VMMode.EXE:
5149                 guest.os_init_path = "/sbin/init"
5150 
5151     def _conf_non_lxc_uml(self, virt_type, guest, root_device_name, rescue,
5152                     instance, inst_path, image_meta, disk_info):
5153         if rescue:
5154             self._set_guest_for_rescue(rescue, guest, inst_path, virt_type,
5155                                        root_device_name)
5156         elif instance.kernel_id:
5157             self._set_guest_for_inst_kernel(instance, guest, inst_path,
5158                                             virt_type, root_device_name,
5159                                             image_meta)
5160         else:
5161             guest.os_boot_dev = blockinfo.get_boot_order(disk_info)
5162 
5163     def _create_consoles(self, virt_type, guest_cfg, instance, flavor,
5164                          image_meta):
5165         # NOTE(markus_z): Beware! Below are so many conditionals that it is
5166         # easy to lose track. Use this chart to figure out your case:
5167         #
5168         # case | is serial | is qemu | resulting
5169         #      | enabled?  | or kvm? | devices
5170         # -------------------------------------------
5171         #    1 |        no |     no  | pty*
5172         #    2 |        no |     yes | pty with logd
5173         #    3 |       yes |      no | see case 1
5174         #    4 |       yes |     yes | tcp with logd
5175         #
5176         #    * exception: `virt_type=parallels` doesn't create a device
5177         if virt_type == 'parallels':
5178             pass
5179         elif virt_type not in ("qemu", "kvm"):
5180             log_path = self._get_console_log_path(instance)
5181             self._create_pty_device(guest_cfg,
5182                                     vconfig.LibvirtConfigGuestConsole,
5183                                     log_path=log_path)
5184         elif (virt_type in ("qemu", "kvm") and
5185                   self._is_s390x_guest(image_meta)):
5186             self._create_consoles_s390x(guest_cfg, instance,
5187                                         flavor, image_meta)
5188         elif virt_type in ("qemu", "kvm"):
5189             self._create_consoles_qemu_kvm(guest_cfg, instance,
5190                                         flavor, image_meta)
5191 
5192     def _is_s390x_guest(self, image_meta):
5193         s390x_archs = (fields.Architecture.S390, fields.Architecture.S390X)
5194         return libvirt_utils.get_arch(image_meta) in s390x_archs
5195 
5196     def _create_consoles_qemu_kvm(self, guest_cfg, instance, flavor,
5197                                   image_meta):
5198         char_dev_cls = vconfig.LibvirtConfigGuestSerial
5199         log_path = self._get_console_log_path(instance)
5200         if CONF.serial_console.enabled:
5201             if not self._serial_ports_already_defined(instance):
5202                 num_ports = hardware.get_number_of_serial_ports(flavor,
5203                                                                 image_meta)
5204                 self._check_number_of_serial_console(num_ports)
5205                 self._create_serial_consoles(guest_cfg, num_ports,
5206                                              char_dev_cls, log_path)
5207         else:
5208             self._create_pty_device(guest_cfg, char_dev_cls,
5209                                     log_path=log_path)
5210 
5211     def _create_consoles_s390x(self, guest_cfg, instance, flavor, image_meta):
5212         char_dev_cls = vconfig.LibvirtConfigGuestConsole
5213         log_path = self._get_console_log_path(instance)
5214         if CONF.serial_console.enabled:
5215             if not self._serial_ports_already_defined(instance):
5216                 num_ports = hardware.get_number_of_serial_ports(flavor,
5217                                                                 image_meta)
5218                 self._create_serial_consoles(guest_cfg, num_ports,
5219                                              char_dev_cls, log_path)
5220         else:
5221             self._create_pty_device(guest_cfg, char_dev_cls,
5222                                     "sclp", log_path)
5223 
5224     def _create_pty_device(self, guest_cfg, char_dev_cls, target_type=None,
5225                            log_path=None):
5226 
5227         consolepty = char_dev_cls()
5228         consolepty.target_type = target_type
5229         consolepty.type = "pty"
5230 
5231         log = vconfig.LibvirtConfigGuestCharDeviceLog()
5232         log.file = log_path
5233         consolepty.log = log
5234 
5235         guest_cfg.add_device(consolepty)
5236 
5237     def _serial_ports_already_defined(self, instance):
5238         try:
5239             guest = self._host.get_guest(instance)
5240             if list(self._get_serial_ports_from_guest(guest)):
5241                 # Serial port are already configured for instance that
5242                 # means we are in a context of migration.
5243                 return True
5244         except exception.InstanceNotFound:
5245             LOG.debug(
5246                 "Instance does not exist yet on libvirt, we can "
5247                 "safely pass on looking for already defined serial "
5248                 "ports in its domain XML", instance=instance)
5249         return False
5250 
5251     def _create_serial_consoles(self, guest_cfg, num_ports, char_dev_cls,
5252                                 log_path):
5253         for port in six.moves.range(num_ports):
5254             console = char_dev_cls()
5255             console.port = port
5256             console.type = "tcp"
5257             console.listen_host = CONF.serial_console.proxyclient_address
5258             listen_port = serial_console.acquire_port(console.listen_host)
5259             console.listen_port = listen_port
5260             # NOTE: only the first serial console gets the boot messages,
5261             # that's why we attach the logd subdevice only to that.
5262             if port == 0:
5263                 log = vconfig.LibvirtConfigGuestCharDeviceLog()
5264                 log.file = log_path
5265                 console.log = log
5266             guest_cfg.add_device(console)
5267 
5268     def _cpu_config_to_vcpu_model(self, cpu_config, vcpu_model):
5269         """Update VirtCPUModel object according to libvirt CPU config.
5270 
5271         :param:cpu_config: vconfig.LibvirtConfigGuestCPU presenting the
5272                            instance's virtual cpu configuration.
5273         :param:vcpu_model: VirtCPUModel object. A new object will be created
5274                            if None.
5275 
5276         :return: Updated VirtCPUModel object, or None if cpu_config is None
5277 
5278         """
5279 
5280         if not cpu_config:
5281             return
5282         if not vcpu_model:
5283             vcpu_model = objects.VirtCPUModel()
5284 
5285         vcpu_model.arch = cpu_config.arch
5286         vcpu_model.vendor = cpu_config.vendor
5287         vcpu_model.model = cpu_config.model
5288         vcpu_model.mode = cpu_config.mode
5289         vcpu_model.match = cpu_config.match
5290 
5291         if cpu_config.sockets:
5292             vcpu_model.topology = objects.VirtCPUTopology(
5293                 sockets=cpu_config.sockets,
5294                 cores=cpu_config.cores,
5295                 threads=cpu_config.threads)
5296         else:
5297             vcpu_model.topology = None
5298 
5299         features = [objects.VirtCPUFeature(
5300             name=f.name,
5301             policy=f.policy) for f in cpu_config.features]
5302         vcpu_model.features = features
5303 
5304         return vcpu_model
5305 
5306     def _vcpu_model_to_cpu_config(self, vcpu_model):
5307         """Create libvirt CPU config according to VirtCPUModel object.
5308 
5309         :param:vcpu_model: VirtCPUModel object.
5310 
5311         :return: vconfig.LibvirtConfigGuestCPU.
5312 
5313         """
5314 
5315         cpu_config = vconfig.LibvirtConfigGuestCPU()
5316         cpu_config.arch = vcpu_model.arch
5317         cpu_config.model = vcpu_model.model
5318         cpu_config.mode = vcpu_model.mode
5319         cpu_config.match = vcpu_model.match
5320         cpu_config.vendor = vcpu_model.vendor
5321         if vcpu_model.topology:
5322             cpu_config.sockets = vcpu_model.topology.sockets
5323             cpu_config.cores = vcpu_model.topology.cores
5324             cpu_config.threads = vcpu_model.topology.threads
5325         if vcpu_model.features:
5326             for f in vcpu_model.features:
5327                 xf = vconfig.LibvirtConfigGuestCPUFeature()
5328                 xf.name = f.name
5329                 xf.policy = f.policy
5330                 cpu_config.features.add(xf)
5331         return cpu_config
5332 
5333     def _guest_add_pcie_root_ports(self, guest):
5334         """Add PCI Express root ports.
5335 
5336         PCI Express machine can have as many PCIe devices as it has
5337         pcie-root-port controllers (slots in virtual motherboard).
5338 
5339         If we want to have more PCIe slots for hotplug then we need to create
5340         whole PCIe structure (libvirt limitation).
5341         """
5342 
5343         pcieroot = vconfig.LibvirtConfigGuestPCIeRootController()
5344         guest.add_device(pcieroot)
5345 
5346         for x in range(0, CONF.libvirt.num_pcie_ports):
5347             pcierootport = vconfig.LibvirtConfigGuestPCIeRootPortController()
5348             guest.add_device(pcierootport)
5349 
5350     def _guest_needs_pcie(self, guest, caps):
5351         """Check for prerequisites for adding PCIe root port
5352         controllers
5353         """
5354 
5355         # TODO(kchamart) In the third 'if' conditional below, for 'x86'
5356         # arch, we're assuming: when 'os_mach_type' is 'None', you'll
5357         # have "pc" machine type.  That assumption, although it is
5358         # correct for the "forseeable future", it will be invalid when
5359         # libvirt / QEMU changes the default machine types.
5360         #
5361         # From libvirt 4.7.0 onwards (September 2018), it will ensure
5362         # that *if* 'pc' is available, it will be used as the default --
5363         # to not break existing applications.  (Refer:
5364         # https://libvirt.org/git/?p=libvirt.git;a=commit;h=26cfb1a3
5365         # --"qemu: ensure default machine types don't change if QEMU
5366         # changes").
5367         #
5368         # But even if libvirt (>=v4.7.0) handled the default case,
5369         # relying on such assumptions is not robust.  Instead we should
5370         # get the default machine type for a given architecture reliably
5371         # -- by Nova setting it explicitly (we already do it for Arm /
5372         # AArch64 & s390x).  A part of this bug is being tracked here:
5373         # https://bugs.launchpad.net/nova/+bug/1780138).
5374 
5375         # Add PCIe root port controllers for PCI Express machines
5376         # but only if their amount is configured
5377 
5378         if not CONF.libvirt.num_pcie_ports:
5379             return False
5380         if (caps.host.cpu.arch == fields.Architecture.AARCH64 and
5381                 guest.os_mach_type.startswith('virt')):
5382             return True
5383         if (caps.host.cpu.arch == fields.Architecture.X86_64 and
5384                 guest.os_mach_type is not None and
5385                 'q35' in guest.os_mach_type):
5386             return True
5387         return False
5388 
5389     def _guest_add_usb_host_keyboard(self, guest):
5390         """Add USB Host controller and keyboard for graphical console use.
5391 
5392         Add USB keyboard as PS/2 support may not be present on non-x86
5393         architectures.
5394         """
5395         keyboard = vconfig.LibvirtConfigGuestInput()
5396         keyboard.type = "keyboard"
5397         keyboard.bus = "usb"
5398         guest.add_device(keyboard)
5399 
5400         usbhost = vconfig.LibvirtConfigGuestUSBHostController()
5401         usbhost.index = 0
5402         guest.add_device(usbhost)
5403 
5404     def _get_guest_config(self, instance, network_info, image_meta,
5405                           disk_info, rescue=None, block_device_info=None,
5406                           context=None, mdevs=None):
5407         """Get config data for parameters.
5408 
5409         :param rescue: optional dictionary that should contain the key
5410             'ramdisk_id' if a ramdisk is needed for the rescue image and
5411             'kernel_id' if a kernel is needed for the rescue image.
5412 
5413         :param mdevs: optional list of mediated devices to assign to the guest.
5414         """
5415         flavor = instance.flavor
5416         inst_path = libvirt_utils.get_instance_path(instance)
5417         disk_mapping = disk_info['mapping']
5418 
5419         virt_type = CONF.libvirt.virt_type
5420         guest = vconfig.LibvirtConfigGuest()
5421         guest.virt_type = virt_type
5422         guest.name = instance.name
5423         guest.uuid = instance.uuid
5424         # We are using default unit for memory: KiB
5425         guest.memory = flavor.memory_mb * units.Ki
5426         guest.vcpus = flavor.vcpus
5427         allowed_cpus = hardware.get_vcpu_pin_set()
5428 
5429         guest_numa_config = self._get_guest_numa_config(
5430             instance.numa_topology, flavor, allowed_cpus, image_meta)
5431 
5432         guest.cpuset = guest_numa_config.cpuset
5433         guest.cputune = guest_numa_config.cputune
5434         guest.numatune = guest_numa_config.numatune
5435 
5436         guest.membacking = self._get_guest_memory_backing_config(
5437             instance.numa_topology,
5438             guest_numa_config.numatune,
5439             flavor)
5440 
5441         guest.metadata.append(self._get_guest_config_meta(instance))
5442         guest.idmaps = self._get_guest_idmaps()
5443 
5444         for event in self._supported_perf_events:
5445             guest.add_perf_event(event)
5446 
5447         self._update_guest_cputune(guest, flavor, virt_type)
5448 
5449         guest.cpu = self._get_guest_cpu_config(
5450             flavor, image_meta, guest_numa_config.numaconfig,
5451             instance.numa_topology)
5452 
5453         # Notes(yjiang5): we always sync the instance's vcpu model with
5454         # the corresponding config file.
5455         instance.vcpu_model = self._cpu_config_to_vcpu_model(
5456             guest.cpu, instance.vcpu_model)
5457 
5458         if 'root' in disk_mapping:
5459             root_device_name = block_device.prepend_dev(
5460                 disk_mapping['root']['dev'])
5461         else:
5462             root_device_name = None
5463 
5464         if root_device_name:
5465             instance.root_device_name = root_device_name
5466 
5467         guest.os_type = (fields.VMMode.get_from_instance(instance) or
5468                 self._get_guest_os_type(virt_type))
5469         caps = self._host.get_capabilities()
5470 
5471         self._configure_guest_by_virt_type(guest, virt_type, caps, instance,
5472                                            image_meta, flavor,
5473                                            root_device_name)
5474         if virt_type not in ('lxc', 'uml'):
5475             self._conf_non_lxc_uml(virt_type, guest, root_device_name, rescue,
5476                     instance, inst_path, image_meta, disk_info)
5477 
5478         self._set_features(guest, instance.os_type, caps, virt_type,
5479                            image_meta, flavor)
5480         self._set_clock(guest, instance.os_type, image_meta, virt_type)
5481 
5482         storage_configs = self._get_guest_storage_config(context,
5483                 instance, image_meta, disk_info, rescue, block_device_info,
5484                 flavor, guest.os_type)
5485         for config in storage_configs:
5486             guest.add_device(config)
5487 
5488         for vif in network_info:
5489             config = self.vif_driver.get_config(
5490                 instance, vif, image_meta,
5491                 flavor, virt_type, self._host)
5492             guest.add_device(config)
5493 
5494         self._create_consoles(virt_type, guest, instance, flavor, image_meta)
5495 
5496         pointer = self._get_guest_pointer_model(guest.os_type, image_meta)
5497         if pointer:
5498             guest.add_device(pointer)
5499 
5500         self._guest_add_spice_channel(guest)
5501 
5502         if self._guest_add_video_device(guest):
5503             self._add_video_driver(guest, image_meta, flavor)
5504 
5505             # We want video == we want graphical console. Some architectures
5506             # do not have input devices attached in default configuration.
5507             # Let then add USB Host controller and USB keyboard.
5508             # x86(-64) and ppc64 have usb host controller and keyboard
5509             # s390x does not support USB
5510             if caps.host.cpu.arch == fields.Architecture.AARCH64:
5511                 self._guest_add_usb_host_keyboard(guest)
5512 
5513         # Qemu guest agent only support 'qemu' and 'kvm' hypervisor
5514         if virt_type in ('qemu', 'kvm'):
5515             self._set_qemu_guest_agent(guest, flavor, instance, image_meta)
5516 
5517         if self._guest_needs_pcie(guest, caps):
5518             self._guest_add_pcie_root_ports(guest)
5519 
5520         self._guest_add_pci_devices(guest, instance)
5521 
5522         self._guest_add_watchdog_action(guest, flavor, image_meta)
5523 
5524         self._guest_add_memory_balloon(guest)
5525 
5526         if mdevs:
5527             self._guest_add_mdevs(guest, mdevs)
5528 
5529         return guest
5530 
5531     def _guest_add_mdevs(self, guest, chosen_mdevs):
5532         for chosen_mdev in chosen_mdevs:
5533             mdev = vconfig.LibvirtConfigGuestHostdevMDEV()
5534             mdev.uuid = chosen_mdev
5535             guest.add_device(mdev)
5536 
5537     @staticmethod
5538     def _guest_add_spice_channel(guest):
5539         if (CONF.spice.enabled and CONF.spice.agent_enabled and
5540                 guest.virt_type not in ('lxc', 'uml', 'xen')):
5541             channel = vconfig.LibvirtConfigGuestChannel()
5542             channel.type = 'spicevmc'
5543             channel.target_name = "com.redhat.spice.0"
5544             guest.add_device(channel)
5545 
5546     @staticmethod
5547     def _guest_add_memory_balloon(guest):
5548         virt_type = guest.virt_type
5549         # Memory balloon device only support 'qemu/kvm' and 'xen' hypervisor
5550         if (virt_type in ('xen', 'qemu', 'kvm') and
5551                     CONF.libvirt.mem_stats_period_seconds > 0):
5552             balloon = vconfig.LibvirtConfigMemoryBalloon()
5553             if virt_type in ('qemu', 'kvm'):
5554                 balloon.model = 'virtio'
5555             else:
5556                 balloon.model = 'xen'
5557             balloon.period = CONF.libvirt.mem_stats_period_seconds
5558             guest.add_device(balloon)
5559 
5560     @staticmethod
5561     def _guest_add_watchdog_action(guest, flavor, image_meta):
5562         # image meta takes precedence over flavor extra specs; disable the
5563         # watchdog action by default
5564         watchdog_action = (flavor.extra_specs.get('hw:watchdog_action') or
5565                            'disabled')
5566         watchdog_action = image_meta.properties.get('hw_watchdog_action',
5567                                                     watchdog_action)
5568         # NB(sross): currently only actually supported by KVM/QEmu
5569         if watchdog_action != 'disabled':
5570             if watchdog_action in fields.WatchdogAction.ALL:
5571                 bark = vconfig.LibvirtConfigGuestWatchdog()
5572                 bark.action = watchdog_action
5573                 guest.add_device(bark)
5574             else:
5575                 raise exception.InvalidWatchdogAction(action=watchdog_action)
5576 
5577     def _guest_add_pci_devices(self, guest, instance):
5578         virt_type = guest.virt_type
5579         if virt_type in ('xen', 'qemu', 'kvm'):
5580             # Get all generic PCI devices (non-SR-IOV).
5581             for pci_dev in pci_manager.get_instance_pci_devs(instance):
5582                 guest.add_device(self._get_guest_pci_device(pci_dev))
5583         else:
5584             # PCI devices is only supported for hypervisors
5585             #  'xen', 'qemu' and 'kvm'.
5586             if pci_manager.get_instance_pci_devs(instance, 'all'):
5587                 raise exception.PciDeviceUnsupportedHypervisor(type=virt_type)
5588 
5589     @staticmethod
5590     def _guest_add_video_device(guest):
5591         # NB some versions of libvirt support both SPICE and VNC
5592         # at the same time. We're not trying to second guess which
5593         # those versions are. We'll just let libvirt report the
5594         # errors appropriately if the user enables both.
5595         add_video_driver = False
5596         if CONF.vnc.enabled and guest.virt_type not in ('lxc', 'uml'):
5597             graphics = vconfig.LibvirtConfigGuestGraphics()
5598             graphics.type = "vnc"
5599             if CONF.vnc.keymap:
5600                 graphics.keymap = CONF.vnc.keymap
5601             graphics.listen = CONF.vnc.server_listen
5602             guest.add_device(graphics)
5603             add_video_driver = True
5604         if CONF.spice.enabled and guest.virt_type not in ('lxc', 'uml', 'xen'):
5605             graphics = vconfig.LibvirtConfigGuestGraphics()
5606             graphics.type = "spice"
5607             if CONF.spice.keymap:
5608                 graphics.keymap = CONF.spice.keymap
5609             graphics.listen = CONF.spice.server_listen
5610             guest.add_device(graphics)
5611             add_video_driver = True
5612         return add_video_driver
5613 
5614     def _get_guest_pointer_model(self, os_type, image_meta):
5615         pointer_model = image_meta.properties.get(
5616             'hw_pointer_model', CONF.pointer_model)
5617         if pointer_model is None and CONF.libvirt.use_usb_tablet:
5618             # TODO(sahid): We set pointer_model to keep compatibility
5619             # until the next release O*. It means operators can continue
5620             # to use the deprecated option "use_usb_tablet" or set a
5621             # specific device to use
5622             pointer_model = "usbtablet"
5623             LOG.warning('The option "use_usb_tablet" has been '
5624                         'deprecated for Newton in favor of the more '
5625                         'generic "pointer_model". Please update '
5626                         'nova.conf to address this change.')
5627 
5628         if pointer_model == "usbtablet":
5629             # We want a tablet if VNC is enabled, or SPICE is enabled and
5630             # the SPICE agent is disabled. If the SPICE agent is enabled
5631             # it provides a paravirt mouse which drastically reduces
5632             # overhead (by eliminating USB polling).
5633             if CONF.vnc.enabled or (
5634                     CONF.spice.enabled and not CONF.spice.agent_enabled):
5635                 return self._get_guest_usb_tablet(os_type)
5636             else:
5637                 if CONF.pointer_model or CONF.libvirt.use_usb_tablet:
5638                     # For backward compatibility We don't want to break
5639                     # process of booting an instance if host is configured
5640                     # to use USB tablet without VNC or SPICE and SPICE
5641                     # agent disable.
5642                     LOG.warning('USB tablet requested for guests by host '
5643                                 'configuration. In order to accept this '
5644                                 'request VNC should be enabled or SPICE '
5645                                 'and SPICE agent disabled on host.')
5646                 else:
5647                     raise exception.UnsupportedPointerModelRequested(
5648                         model="usbtablet")
5649 
5650     def _get_guest_usb_tablet(self, os_type):
5651         tablet = None
5652         if os_type == fields.VMMode.HVM:
5653             tablet = vconfig.LibvirtConfigGuestInput()
5654             tablet.type = "tablet"
5655             tablet.bus = "usb"
5656         else:
5657             if CONF.pointer_model or CONF.libvirt.use_usb_tablet:
5658                 # For backward compatibility We don't want to break
5659                 # process of booting an instance if virtual machine mode
5660                 # is not configured as HVM.
5661                 LOG.warning('USB tablet requested for guests by host '
5662                             'configuration. In order to accept this '
5663                             'request the machine mode should be '
5664                             'configured as HVM.')
5665             else:
5666                 raise exception.UnsupportedPointerModelRequested(
5667                     model="usbtablet")
5668         return tablet
5669 
5670     def _get_guest_xml(self, context, instance, network_info, disk_info,
5671                        image_meta, rescue=None,
5672                        block_device_info=None,
5673                        mdevs=None):
5674         # NOTE(danms): Stringifying a NetworkInfo will take a lock. Do
5675         # this ahead of time so that we don't acquire it while also
5676         # holding the logging lock.
5677         network_info_str = str(network_info)
5678         msg = ('Start _get_guest_xml '
5679                'network_info=%(network_info)s '
5680                'disk_info=%(disk_info)s '
5681                'image_meta=%(image_meta)s rescue=%(rescue)s '
5682                'block_device_info=%(block_device_info)s' %
5683                {'network_info': network_info_str, 'disk_info': disk_info,
5684                 'image_meta': image_meta, 'rescue': rescue,
5685                 'block_device_info': block_device_info})
5686         # NOTE(mriedem): block_device_info can contain auth_password so we
5687         # need to sanitize the password in the message.
5688         LOG.debug(strutils.mask_password(msg), instance=instance)
5689         conf = self._get_guest_config(instance, network_info, image_meta,
5690                                       disk_info, rescue, block_device_info,
5691                                       context, mdevs)
5692         xml = conf.to_xml()
5693 
5694         LOG.debug('End _get_guest_xml xml=%(xml)s',
5695                   {'xml': xml}, instance=instance)
5696         return xml
5697 
5698     def get_info(self, instance, use_cache=True):
5699         """Retrieve information from libvirt for a specific instance.
5700 
5701         If a libvirt error is encountered during lookup, we might raise a
5702         NotFound exception or Error exception depending on how severe the
5703         libvirt error is.
5704 
5705         :param instance: nova.objects.instance.Instance object
5706         :param use_cache: unused in this driver
5707         :returns: An InstanceInfo object
5708         """
5709         guest = self._host.get_guest(instance)
5710         # Kind of ugly but we need to pass host to get_info as for a
5711         # workaround, see libvirt/compat.py
5712         return guest.get_info(self._host)
5713 
5714     def _create_domain_setup_lxc(self, context, instance, image_meta,
5715                                  block_device_info):
5716         inst_path = libvirt_utils.get_instance_path(instance)
5717         block_device_mapping = driver.block_device_info_get_mapping(
5718             block_device_info)
5719         root_disk = block_device.get_root_bdm(block_device_mapping)
5720         if root_disk:
5721             self._connect_volume(context, root_disk['connection_info'],
5722                                  instance)
5723             disk_path = root_disk['connection_info']['data']['device_path']
5724 
5725             # NOTE(apmelton) - Even though the instance is being booted from a
5726             # cinder volume, it is still presented as a local block device.
5727             # LocalBlockImage is used here to indicate that the instance's
5728             # disk is backed by a local block device.
5729             image_model = imgmodel.LocalBlockImage(disk_path)
5730         else:
5731             root_disk = self.image_backend.by_name(instance, 'disk')
5732             image_model = root_disk.get_model(self._conn)
5733 
5734         container_dir = os.path.join(inst_path, 'rootfs')
5735         fileutils.ensure_tree(container_dir)
5736         rootfs_dev = disk_api.setup_container(image_model,
5737                                               container_dir=container_dir)
5738 
5739         try:
5740             # Save rootfs device to disconnect it when deleting the instance
5741             if rootfs_dev:
5742                 instance.system_metadata['rootfs_device_name'] = rootfs_dev
5743             if CONF.libvirt.uid_maps or CONF.libvirt.gid_maps:
5744                 id_maps = self._get_guest_idmaps()
5745                 libvirt_utils.chown_for_id_maps(container_dir, id_maps)
5746         except Exception:
5747             with excutils.save_and_reraise_exception():
5748                 self._create_domain_cleanup_lxc(instance)
5749 
5750     def _create_domain_cleanup_lxc(self, instance):
5751         inst_path = libvirt_utils.get_instance_path(instance)
5752         container_dir = os.path.join(inst_path, 'rootfs')
5753 
5754         try:
5755             state = self.get_info(instance).state
5756         except exception.InstanceNotFound:
5757             # The domain may not be present if the instance failed to start
5758             state = None
5759 
5760         if state == power_state.RUNNING:
5761             # NOTE(uni): Now the container is running with its own private
5762             # mount namespace and so there is no need to keep the container
5763             # rootfs mounted in the host namespace
5764             LOG.debug('Attempting to unmount container filesystem: %s',
5765                       container_dir, instance=instance)
5766             disk_api.clean_lxc_namespace(container_dir=container_dir)
5767         else:
5768             disk_api.teardown_container(container_dir=container_dir)
5769 
5770     @contextlib.contextmanager
5771     def _lxc_disk_handler(self, context, instance, image_meta,
5772                           block_device_info):
5773         """Context manager to handle the pre and post instance boot,
5774            LXC specific disk operations.
5775 
5776            An image or a volume path will be prepared and setup to be
5777            used by the container, prior to starting it.
5778            The disk will be disconnected and unmounted if a container has
5779            failed to start.
5780         """
5781 
5782         if CONF.libvirt.virt_type != 'lxc':
5783             yield
5784             return
5785 
5786         self._create_domain_setup_lxc(context, instance, image_meta,
5787                                       block_device_info)
5788 
5789         try:
5790             yield
5791         finally:
5792             self._create_domain_cleanup_lxc(instance)
5793 
5794     # TODO(sahid): Consider renaming this to _create_guest.
5795     def _create_domain(self, xml=None, domain=None,
5796                        power_on=True, pause=False, post_xml_callback=None):
5797         """Create a domain.
5798 
5799         Either domain or xml must be passed in. If both are passed, then
5800         the domain definition is overwritten from the xml.
5801 
5802         :returns guest.Guest: Guest just created
5803         """
5804         if xml:
5805             guest = libvirt_guest.Guest.create(xml, self._host)
5806             if post_xml_callback is not None:
5807                 post_xml_callback()
5808         else:
5809             guest = libvirt_guest.Guest(domain)
5810 
5811         if power_on or pause:
5812             guest.launch(pause=pause)
5813 
5814         if not utils.is_neutron():
5815             guest.enable_hairpin()
5816 
5817         return guest
5818 
5819     def _neutron_failed_callback(self, event_name, instance):
5820         LOG.error('Neutron Reported failure on event '
5821                   '%(event)s for instance %(uuid)s',
5822                   {'event': event_name, 'uuid': instance.uuid},
5823                   instance=instance)
5824         if CONF.vif_plugging_is_fatal:
5825             raise exception.VirtualInterfaceCreateException()
5826 
5827     def _get_neutron_events(self, network_info):
5828         # NOTE(danms): We need to collect any VIFs that are currently
5829         # down that we expect a down->up event for. Anything that is
5830         # already up will not undergo that transition, and for
5831         # anything that might be stale (cache-wise) assume it's
5832         # already up so we don't block on it.
5833         return [('network-vif-plugged', vif['id'])
5834                 for vif in network_info if vif.get('active', True) is False]
5835 
5836     def _cleanup_failed_start(self, context, instance, network_info,
5837                               block_device_info, guest, destroy_disks):
5838         try:
5839             if guest and guest.is_active():
5840                 guest.poweroff()
5841         finally:
5842             self.cleanup(context, instance, network_info=network_info,
5843                          block_device_info=block_device_info,
5844                          destroy_disks=destroy_disks)
5845 
5846     def _create_domain_and_network(self, context, xml, instance, network_info,
5847                                    block_device_info=None, power_on=True,
5848                                    vifs_already_plugged=False,
5849                                    post_xml_callback=None,
5850                                    destroy_disks_on_failure=False):
5851 
5852         """Do required network setup and create domain."""
5853         timeout = CONF.vif_plugging_timeout
5854         if (self._conn_supports_start_paused and
5855             utils.is_neutron() and not
5856             vifs_already_plugged and power_on and timeout):
5857             events = self._get_neutron_events(network_info)
5858         else:
5859             events = []
5860 
5861         pause = bool(events)
5862         guest = None
5863         try:
5864             with self.virtapi.wait_for_instance_event(
5865                     instance, events, deadline=timeout,
5866                     error_callback=self._neutron_failed_callback):
5867                 self.plug_vifs(instance, network_info)
5868                 self.firewall_driver.setup_basic_filtering(instance,
5869                                                            network_info)
5870                 self.firewall_driver.prepare_instance_filter(instance,
5871                                                              network_info)
5872                 with self._lxc_disk_handler(context, instance,
5873                                             instance.image_meta,
5874                                             block_device_info):
5875                     guest = self._create_domain(
5876                         xml, pause=pause, power_on=power_on,
5877                         post_xml_callback=post_xml_callback)
5878 
5879                 self.firewall_driver.apply_instance_filter(instance,
5880                                                            network_info)
5881         except exception.VirtualInterfaceCreateException:
5882             # Neutron reported failure and we didn't swallow it, so
5883             # bail here
5884             with excutils.save_and_reraise_exception():
5885                 self._cleanup_failed_start(context, instance, network_info,
5886                                            block_device_info, guest,
5887                                            destroy_disks_on_failure)
5888         except eventlet.timeout.Timeout:
5889             # We never heard from Neutron
5890             LOG.warning('Timeout waiting for %(events)s for '
5891                         'instance with vm_state %(vm_state)s and '
5892                         'task_state %(task_state)s.',
5893                         {'events': events,
5894                          'vm_state': instance.vm_state,
5895                          'task_state': instance.task_state},
5896                         instance=instance)
5897             if CONF.vif_plugging_is_fatal:
5898                 self._cleanup_failed_start(context, instance, network_info,
5899                                            block_device_info, guest,
5900                                            destroy_disks_on_failure)
5901                 raise exception.VirtualInterfaceCreateException()
5902         except Exception:
5903             # Any other error, be sure to clean up
5904             LOG.error('Failed to start libvirt guest', instance=instance)
5905             with excutils.save_and_reraise_exception():
5906                 self._cleanup_failed_start(context, instance, network_info,
5907                                            block_device_info, guest,
5908                                            destroy_disks_on_failure)
5909 
5910         # Resume only if domain has been paused
5911         if pause:
5912             guest.resume()
5913         return guest
5914 
5915     def _get_vcpu_total(self):
5916         """Get available vcpu number of physical computer.
5917 
5918         :returns: the number of cpu core instances can be used.
5919 
5920         """
5921         try:
5922             total_pcpus = self._host.get_cpu_count()
5923         except libvirt.libvirtError:
5924             LOG.warning("Cannot get the number of cpu, because this "
5925                         "function is not implemented for this platform.")
5926             return 0
5927 
5928         if not CONF.vcpu_pin_set:
5929             return total_pcpus
5930 
5931         available_ids = hardware.get_vcpu_pin_set()
5932         # We get the list of online CPUs on the host and see if the requested
5933         # set falls under these. If not, we retain the old behavior.
5934         online_pcpus = None
5935         try:
5936             online_pcpus = self._host.get_online_cpus()
5937         except libvirt.libvirtError as ex:
5938             error_code = ex.get_error_code()
5939             err_msg = encodeutils.exception_to_unicode(ex)
5940             LOG.warning(
5941                 "Couldn't retrieve the online CPUs due to a Libvirt "
5942                 "error: %(error)s with error code: %(error_code)s",
5943                 {'error': err_msg, 'error_code': error_code})
5944         if online_pcpus:
5945             if not (available_ids <= online_pcpus):
5946                 msg = (_("Invalid vcpu_pin_set config, one or more of the "
5947                          "specified cpuset is not online. Online cpuset(s): "
5948                          "%(online)s, requested cpuset(s): %(req)s"),
5949                        {'online': sorted(online_pcpus),
5950                         'req': sorted(available_ids)})
5951                 raise exception.Invalid(msg)
5952         elif sorted(available_ids)[-1] >= total_pcpus:
5953             raise exception.Invalid(_("Invalid vcpu_pin_set config, "
5954                                       "out of hypervisor cpu range."))
5955         return len(available_ids)
5956 
5957     @staticmethod
5958     def _get_local_gb_info():
5959         """Get local storage info of the compute node in GB.
5960 
5961         :returns: A dict containing:
5962              :total: How big the overall usable filesystem is (in gigabytes)
5963              :free: How much space is free (in gigabytes)
5964              :used: How much space is used (in gigabytes)
5965         """
5966 
5967         if CONF.libvirt.images_type == 'lvm':
5968             info = lvm.get_volume_group_info(
5969                                CONF.libvirt.images_volume_group)
5970         elif CONF.libvirt.images_type == 'rbd':
5971             info = LibvirtDriver._get_rbd_driver().get_pool_info()
5972         else:
5973             info = libvirt_utils.get_fs_info(CONF.instances_path)
5974 
5975         for (k, v) in info.items():
5976             info[k] = v / units.Gi
5977 
5978         return info
5979 
5980     def _get_vcpu_used(self):
5981         """Get vcpu usage number of physical computer.
5982 
5983         :returns: The total number of vcpu(s) that are currently being used.
5984 
5985         """
5986 
5987         total = 0
5988 
5989         # Not all libvirt drivers will support the get_vcpus_info()
5990         #
5991         # For example, LXC does not have a concept of vCPUs, while
5992         # QEMU (TCG) traditionally handles all vCPUs in a single
5993         # thread. So both will report an exception when the vcpus()
5994         # API call is made. In such a case we should report the
5995         # guest as having 1 vCPU, since that lets us still do
5996         # CPU over commit calculations that apply as the total
5997         # guest count scales.
5998         #
5999         # It is also possible that we might see an exception if
6000         # the guest is just in middle of shutting down. Technically
6001         # we should report 0 for vCPU usage in this case, but we
6002         # we can't reliably distinguish the vcpu not supported
6003         # case from the just shutting down case. Thus we don't know
6004         # whether to report 1 or 0 for vCPU count.
6005         #
6006         # Under-reporting vCPUs is bad because it could conceivably
6007         # let the scheduler place too many guests on the host. Over-
6008         # reporting vCPUs is not a problem as it'll auto-correct on
6009         # the next refresh of usage data.
6010         #
6011         # Thus when getting an exception we always report 1 as the
6012         # vCPU count, as the least worst value.
6013         for guest in self._host.list_guests():
6014             try:
6015                 vcpus = guest.get_vcpus_info()
6016                 total += len(list(vcpus))
6017             except libvirt.libvirtError:
6018                 total += 1
6019             # NOTE(gtt116): give other tasks a chance.
6020             greenthread.sleep(0)
6021         return total
6022 
6023     def _get_supported_vgpu_types(self):
6024         if not CONF.devices.enabled_vgpu_types:
6025             return []
6026         # TODO(sbauza): Move this check up to compute_manager.init_host
6027         if len(CONF.devices.enabled_vgpu_types) > 1:
6028             LOG.warning('libvirt only supports one GPU type per compute node,'
6029                         ' only first type will be used.')
6030         requested_types = CONF.devices.enabled_vgpu_types[:1]
6031         return requested_types
6032 
6033     def _count_mediated_devices(self, enabled_vgpu_types):
6034         """Counts the sysfs objects (handles) that represent a mediated device
6035         and filtered by $enabled_vgpu_types.
6036 
6037         Those handles can be in use by a libvirt guest or not.
6038 
6039         :param enabled_vgpu_types: list of enabled VGPU types on this host
6040         :returns: dict, keyed by parent GPU libvirt PCI device ID, of number of
6041         mdev device handles for that GPU
6042         """
6043 
6044         counts_per_parent = collections.defaultdict(int)
6045         mediated_devices = self._get_mediated_devices(types=enabled_vgpu_types)
6046         for mdev in mediated_devices:
6047             counts_per_parent[mdev['parent']] += 1
6048         return counts_per_parent
6049 
6050     def _count_mdev_capable_devices(self, enabled_vgpu_types):
6051         """Counts the mdev-capable devices on this host filtered by
6052         $enabled_vgpu_types.
6053 
6054         :param enabled_vgpu_types: list of enabled VGPU types on this host
6055         :returns: dict, keyed by device name, to an integer count of available
6056             instances of each type per device
6057         """
6058         mdev_capable_devices = self._get_mdev_capable_devices(
6059             types=enabled_vgpu_types)
6060         counts_per_dev = collections.defaultdict(int)
6061         for dev in mdev_capable_devices:
6062             # dev_id is the libvirt name for the PCI device,
6063             # eg. pci_0000_84_00_0 which matches a PCI address of 0000:84:00.0
6064             dev_name = dev['dev_id']
6065             for _type in dev['types']:
6066                 available = dev['types'][_type]['availableInstances']
6067                 # TODO(sbauza): Once we support multiple types, check which
6068                 # PCI devices are set for this type
6069                 # NOTE(sbauza): Even if we support multiple types, Nova will
6070                 # only use one per physical GPU.
6071                 counts_per_dev[dev_name] += available
6072         return counts_per_dev
6073 
6074     def _get_gpu_inventories(self):
6075         """Returns the inventories for each physical GPU for a specific type
6076         supported by the enabled_vgpu_types CONF option.
6077 
6078         :returns: dict, keyed by libvirt PCI name, of dicts like:
6079                 {'pci_0000_84_00_0':
6080                     {'total': $TOTAL,
6081                      'min_unit': 1,
6082                      'max_unit': $TOTAL,
6083                      'step_size': 1,
6084                      'reserved': 0,
6085                      'allocation_ratio': 1.0,
6086                     }
6087                 }
6088         """
6089 
6090         # Bail out early if operator doesn't care about providing vGPUs
6091         enabled_vgpu_types = self._get_supported_vgpu_types()
6092         if not enabled_vgpu_types:
6093             return {}
6094         inventories = {}
6095         count_per_parent = self._count_mediated_devices(enabled_vgpu_types)
6096         for dev_name, count in count_per_parent.items():
6097             inventories[dev_name] = {'total': count}
6098         # Filter how many available mdevs we can create for all the supported
6099         # types.
6100         count_per_dev = self._count_mdev_capable_devices(enabled_vgpu_types)
6101         # Combine the counts into the dict that we return to the caller.
6102         for dev_name, count in count_per_dev.items():
6103             inv_per_parent = inventories.setdefault(
6104                 dev_name, {'total': 0})
6105             inv_per_parent['total'] += count
6106             inv_per_parent.update({
6107                 'min_unit': 1,
6108                 'step_size': 1,
6109                 'reserved': 0,
6110                 # NOTE(sbauza): There is no sense to have a ratio but 1.0
6111                 # since we can't overallocate vGPU resources
6112                 'allocation_ratio': 1.0,
6113                 # FIXME(sbauza): Some vendors could support only one
6114                 'max_unit': inv_per_parent['total'],
6115             })
6116 
6117         return inventories
6118 
6119     def _get_instance_capabilities(self):
6120         """Get hypervisor instance capabilities
6121 
6122         Returns a list of tuples that describe instances the
6123         hypervisor is capable of hosting.  Each tuple consists
6124         of the triplet (arch, hypervisor_type, vm_mode).
6125 
6126         Supported hypervisor_type is filtered by virt_type,
6127         a parameter set by operators via `nova.conf`.
6128 
6129         :returns: List of tuples describing instance capabilities
6130         """
6131         caps = self._host.get_capabilities()
6132         instance_caps = list()
6133         for g in caps.guests:
6134             for dt in g.domtype:
6135                 if dt != CONF.libvirt.virt_type:
6136                     continue
6137                 try:
6138                     instance_cap = (
6139                         fields.Architecture.canonicalize(g.arch),
6140                         fields.HVType.canonicalize(dt),
6141                         fields.VMMode.canonicalize(g.ostype))
6142                     instance_caps.append(instance_cap)
6143                 except exception.InvalidArchitectureName:
6144                     # NOTE(danms): Libvirt is exposing a guest arch that nova
6145                     # does not even know about. Avoid aborting here and
6146                     # continue to process the rest.
6147                     pass
6148 
6149         return instance_caps
6150 
6151     def _get_cpu_info(self):
6152         """Get cpuinfo information.
6153 
6154         Obtains cpu feature from virConnect.getCapabilities.
6155 
6156         :return: see above description
6157 
6158         """
6159 
6160         caps = self._host.get_capabilities()
6161         cpu_info = dict()
6162 
6163         cpu_info['arch'] = caps.host.cpu.arch
6164         cpu_info['model'] = caps.host.cpu.model
6165         cpu_info['vendor'] = caps.host.cpu.vendor
6166 
6167         topology = dict()
6168         topology['cells'] = len(getattr(caps.host.topology, 'cells', [1]))
6169         topology['sockets'] = caps.host.cpu.sockets
6170         topology['cores'] = caps.host.cpu.cores
6171         topology['threads'] = caps.host.cpu.threads
6172         cpu_info['topology'] = topology
6173 
6174         features = set()
6175         for f in caps.host.cpu.features:
6176             features.add(f.name)
6177         cpu_info['features'] = features
6178         return cpu_info
6179 
6180     def _get_pcinet_info(self, vf_address):
6181         """Returns a dict of NET device."""
6182         devname = pci_utils.get_net_name_by_vf_pci_address(vf_address)
6183         if not devname:
6184             return
6185 
6186         virtdev = self._host.device_lookup_by_name(devname)
6187         xmlstr = virtdev.XMLDesc(0)
6188         cfgdev = vconfig.LibvirtConfigNodeDevice()
6189         cfgdev.parse_str(xmlstr)
6190         return {'name': cfgdev.name,
6191                 'capabilities': cfgdev.pci_capability.features}
6192 
6193     def _get_pcidev_info(self, devname):
6194         """Returns a dict of PCI device."""
6195 
6196         def _get_device_type(cfgdev, pci_address):
6197             """Get a PCI device's device type.
6198 
6199             An assignable PCI device can be a normal PCI device,
6200             a SR-IOV Physical Function (PF), or a SR-IOV Virtual
6201             Function (VF). Only normal PCI devices or SR-IOV VFs
6202             are assignable, while SR-IOV PFs are always owned by
6203             hypervisor.
6204             """
6205             for fun_cap in cfgdev.pci_capability.fun_capability:
6206                 if fun_cap.type == 'virt_functions':
6207                     return {
6208                         'dev_type': fields.PciDeviceType.SRIOV_PF,
6209                     }
6210                 if (fun_cap.type == 'phys_function' and
6211                     len(fun_cap.device_addrs) != 0):
6212                     phys_address = "%04x:%02x:%02x.%01x" % (
6213                         fun_cap.device_addrs[0][0],
6214                         fun_cap.device_addrs[0][1],
6215                         fun_cap.device_addrs[0][2],
6216                         fun_cap.device_addrs[0][3])
6217                     result = {
6218                         'dev_type': fields.PciDeviceType.SRIOV_VF,
6219                         'parent_addr': phys_address,
6220                     }
6221                     parent_ifname = None
6222                     try:
6223                         parent_ifname = pci_utils.get_ifname_by_pci_address(
6224                             pci_address, pf_interface=True)
6225                     except exception.PciDeviceNotFoundById:
6226                         # NOTE(sean-k-mooney): we ignore this error as it
6227                         # is expected when the virtual function is not a NIC.
6228                         pass
6229                     if parent_ifname:
6230                         result['parent_ifname'] = parent_ifname
6231                     return result
6232 
6233             return {'dev_type': fields.PciDeviceType.STANDARD}
6234 
6235         def _get_device_capabilities(device, address):
6236             """Get PCI VF device's additional capabilities.
6237 
6238             If a PCI device is a virtual function, this function reads the PCI
6239             parent's network capabilities (must be always a NIC device) and
6240             appends this information to the device's dictionary.
6241             """
6242             if device.get('dev_type') == fields.PciDeviceType.SRIOV_VF:
6243                 pcinet_info = self._get_pcinet_info(address)
6244                 if pcinet_info:
6245                     return {'capabilities':
6246                                 {'network': pcinet_info.get('capabilities')}}
6247             return {}
6248 
6249         virtdev = self._host.device_lookup_by_name(devname)
6250         xmlstr = virtdev.XMLDesc(0)
6251         cfgdev = vconfig.LibvirtConfigNodeDevice()
6252         cfgdev.parse_str(xmlstr)
6253 
6254         address = "%04x:%02x:%02x.%1x" % (
6255             cfgdev.pci_capability.domain,
6256             cfgdev.pci_capability.bus,
6257             cfgdev.pci_capability.slot,
6258             cfgdev.pci_capability.function)
6259 
6260         device = {
6261             "dev_id": cfgdev.name,
6262             "address": address,
6263             "product_id": "%04x" % cfgdev.pci_capability.product_id,
6264             "vendor_id": "%04x" % cfgdev.pci_capability.vendor_id,
6265             }
6266 
6267         device["numa_node"] = cfgdev.pci_capability.numa_node
6268 
6269         # requirement by DataBase Model
6270         device['label'] = 'label_%(vendor_id)s_%(product_id)s' % device
6271         device.update(_get_device_type(cfgdev, address))
6272         device.update(_get_device_capabilities(device, address))
6273         return device
6274 
6275     def _get_pci_passthrough_devices(self):
6276         """Get host PCI devices information.
6277 
6278         Obtains pci devices information from libvirt, and returns
6279         as a JSON string.
6280 
6281         Each device information is a dictionary, with mandatory keys
6282         of 'address', 'vendor_id', 'product_id', 'dev_type', 'dev_id',
6283         'label' and other optional device specific information.
6284 
6285         Refer to the objects/pci_device.py for more idea of these keys.
6286 
6287         :returns: a JSON string containing a list of the assignable PCI
6288                   devices information
6289         """
6290         # Bail early if we know we can't support `listDevices` to avoid
6291         # repeated warnings within a periodic task
6292         if not getattr(self, '_list_devices_supported', True):
6293             return jsonutils.dumps([])
6294 
6295         try:
6296             dev_names = self._host.list_pci_devices() or []
6297         except libvirt.libvirtError as ex:
6298             error_code = ex.get_error_code()
6299             if error_code == libvirt.VIR_ERR_NO_SUPPORT:
6300                 self._list_devices_supported = False
6301                 LOG.warning("URI %(uri)s does not support "
6302                             "listDevices: %(error)s",
6303                             {'uri': self._uri(),
6304                              'error': encodeutils.exception_to_unicode(ex)})
6305                 return jsonutils.dumps([])
6306             else:
6307                 raise
6308 
6309         pci_info = []
6310         for name in dev_names:
6311             pci_info.append(self._get_pcidev_info(name))
6312 
6313         return jsonutils.dumps(pci_info)
6314 
6315     def _get_mdev_capabilities_for_dev(self, devname, types=None):
6316         """Returns a dict of MDEV capable device with the ID as first key
6317         and then a list of supported types, each of them being a dict.
6318 
6319         :param types: Only return those specific types.
6320         """
6321         virtdev = self._host.device_lookup_by_name(devname)
6322         xmlstr = virtdev.XMLDesc(0)
6323         cfgdev = vconfig.LibvirtConfigNodeDevice()
6324         cfgdev.parse_str(xmlstr)
6325 
6326         device = {
6327             "dev_id": cfgdev.name,
6328             "types": {},
6329             "vendor_id": cfgdev.pci_capability.vendor_id,
6330         }
6331         for mdev_cap in cfgdev.pci_capability.mdev_capability:
6332             for cap in mdev_cap.mdev_types:
6333                 if not types or cap['type'] in types:
6334                     device["types"].update({cap['type']: {
6335                         'availableInstances': cap['availableInstances'],
6336                         'name': cap['name'],
6337                         'deviceAPI': cap['deviceAPI']}})
6338         return device
6339 
6340     def _get_mdev_capable_devices(self, types=None):
6341         """Get host devices supporting mdev types.
6342 
6343         Obtain devices information from libvirt and returns a list of
6344         dictionaries.
6345 
6346         :param types: Filter only devices supporting those types.
6347         """
6348         if not self._host.has_min_version(MIN_LIBVIRT_MDEV_SUPPORT):
6349             return []
6350         dev_names = self._host.list_mdev_capable_devices() or []
6351         mdev_capable_devices = []
6352         for name in dev_names:
6353             device = self._get_mdev_capabilities_for_dev(name, types)
6354             if not device["types"]:
6355                 continue
6356             mdev_capable_devices.append(device)
6357         return mdev_capable_devices
6358 
6359     def _get_mediated_device_information(self, devname):
6360         """Returns a dict of a mediated device."""
6361         virtdev = self._host.device_lookup_by_name(devname)
6362         xmlstr = virtdev.XMLDesc(0)
6363         cfgdev = vconfig.LibvirtConfigNodeDevice()
6364         cfgdev.parse_str(xmlstr)
6365 
6366         device = {
6367             "dev_id": cfgdev.name,
6368             # name is like mdev_00ead764_fdc0_46b6_8db9_2963f5c815b4
6369             "uuid": libvirt_utils.mdev_name2uuid(cfgdev.name),
6370             # the physical GPU PCI device
6371             "parent": cfgdev.parent,
6372             "type": cfgdev.mdev_information.type,
6373             "iommu_group": cfgdev.mdev_information.iommu_group,
6374         }
6375         return device
6376 
6377     def _get_mediated_devices(self, types=None):
6378         """Get host mediated devices.
6379 
6380         Obtain devices information from libvirt and returns a list of
6381         dictionaries.
6382 
6383         :param types: Filter only devices supporting those types.
6384         """
6385         if not self._host.has_min_version(MIN_LIBVIRT_MDEV_SUPPORT):
6386             return []
6387         dev_names = self._host.list_mediated_devices() or []
6388         mediated_devices = []
6389         for name in dev_names:
6390             device = self._get_mediated_device_information(name)
6391             if not types or device["type"] in types:
6392                 mediated_devices.append(device)
6393         return mediated_devices
6394 
6395     def _get_all_assigned_mediated_devices(self, instance=None):
6396         """Lookup all instances from the host and return all the mediated
6397         devices that are assigned to a guest.
6398 
6399         :param instance: Only return mediated devices for that instance.
6400 
6401         :returns: A dictionary of keys being mediated device UUIDs and their
6402                   respective values the instance UUID of the guest using it.
6403                   Returns an empty dict if an instance is provided but not
6404                   found in the hypervisor.
6405         """
6406         allocated_mdevs = {}
6407         if instance:
6408             # NOTE(sbauza): In some cases (like a migration issue), the
6409             # instance can exist in the Nova database but libvirt doesn't know
6410             # about it. For such cases, the way to fix that is to hard reboot
6411             # the instance, which will recreate the libvirt guest.
6412             # For that reason, we need to support that case by making sure
6413             # we don't raise an exception if the libvirt guest doesn't exist.
6414             try:
6415                 guest = self._host.get_guest(instance)
6416             except exception.InstanceNotFound:
6417                 # Bail out early if libvirt doesn't know about it since we
6418                 # can't know the existing mediated devices
6419                 return {}
6420             guests = [guest]
6421         else:
6422             guests = self._host.list_guests(only_running=False)
6423         for guest in guests:
6424             cfg = guest.get_config()
6425             for device in cfg.devices:
6426                 if isinstance(device, vconfig.LibvirtConfigGuestHostdevMDEV):
6427                     allocated_mdevs[device.uuid] = guest.uuid
6428         return allocated_mdevs
6429 
6430     @staticmethod
6431     def _vgpu_allocations(allocations):
6432         """Filtering only the VGPU allocations from a list of allocations.
6433 
6434         :param allocations: Information about resources allocated to the
6435                             instance via placement, of the form returned by
6436                             SchedulerReportClient.get_allocations_for_consumer.
6437         """
6438         if not allocations:
6439             # If no allocations, there is no vGPU request.
6440             return {}
6441         RC_VGPU = orc.VGPU
6442         vgpu_allocations = {}
6443         for rp in allocations:
6444             res = allocations[rp]['resources']
6445             if RC_VGPU in res and res[RC_VGPU] > 0:
6446                 vgpu_allocations[rp] = {'resources': {RC_VGPU: res[RC_VGPU]}}
6447         return vgpu_allocations
6448 
6449     def _get_existing_mdevs_not_assigned(self, requested_types=None,
6450                                          parent=None):
6451         """Returns the already created mediated devices that are not assigned
6452         to a guest yet.
6453 
6454         :param requested_types: Filter out the result for only mediated devices
6455                                 having those types.
6456         :param parent: Filter out result for only mdevs from the parent device.
6457         """
6458         allocated_mdevs = self._get_all_assigned_mediated_devices()
6459         mdevs = self._get_mediated_devices(requested_types)
6460         available_mdevs = set()
6461         for mdev in mdevs:
6462             if parent is None or mdev['parent'] == parent:
6463                 available_mdevs.add(mdev["uuid"])
6464 
6465         available_mdevs -= set(allocated_mdevs)
6466         return available_mdevs
6467 
6468     def _create_new_mediated_device(self, requested_types, uuid=None,
6469                                     parent=None):
6470         """Find a physical device that can support a new mediated device and
6471         create it.
6472 
6473         :param requested_types: Filter only capable devices supporting those
6474                                 types.
6475         :param uuid: The possible mdev UUID we want to create again
6476         :param parent: Only create a mdev for this device
6477 
6478         :returns: the newly created mdev UUID or None if not possible
6479         """
6480         # Try to see if we can still create a new mediated device
6481         devices = self._get_mdev_capable_devices(requested_types)
6482         for device in devices:
6483             # For the moment, the libvirt driver only supports one
6484             # type per host
6485             # TODO(sbauza): Once we support more than one type, make
6486             # sure we look at the flavor/trait for the asked type.
6487             asked_type = requested_types[0]
6488             if device['types'][asked_type]['availableInstances'] > 0:
6489                 # That physical GPU has enough room for a new mdev
6490                 dev_name = device['dev_id']
6491                 # the parent attribute can be None
6492                 if parent is not None and dev_name != parent:
6493                     # The device is not the one that was called, not creating
6494                     # the mdev
6495                     continue
6496                 # We need the PCI address, not the libvirt name
6497                 # The libvirt name is like 'pci_0000_84_00_0'
6498                 pci_addr = "{}:{}:{}.{}".format(*dev_name[4:].split('_'))
6499                 chosen_mdev = nova.privsep.libvirt.create_mdev(pci_addr,
6500                                                                asked_type,
6501                                                                uuid=uuid)
6502                 return chosen_mdev
6503 
6504     @utils.synchronized(VGPU_RESOURCE_SEMAPHORE)
6505     def _allocate_mdevs(self, allocations):
6506         """Returns a list of mediated device UUIDs corresponding to available
6507         resources we can assign to the guest(s) corresponding to the allocation
6508         requests passed as argument.
6509 
6510         That method can either find an existing but unassigned mediated device
6511         it can allocate, or create a new mediated device from a capable
6512         physical device if the latter has enough left capacity.
6513 
6514         :param allocations: Information about resources allocated to the
6515                             instance via placement, of the form returned by
6516                             SchedulerReportClient.get_allocations_for_consumer.
6517                             That code is supporting Placement API version 1.12
6518         """
6519         vgpu_allocations = self._vgpu_allocations(allocations)
6520         if not vgpu_allocations:
6521             return
6522         # TODO(sbauza): Once we have nested resource providers, find which one
6523         # is having the related allocation for the specific VGPU type.
6524         # For the moment, we should only have one allocation for
6525         # ResourceProvider.
6526         # TODO(sbauza): Iterate over all the allocations once we have
6527         # nested Resource Providers. For the moment, just take the first.
6528         if len(vgpu_allocations) > 1:
6529             LOG.warning('More than one allocation was passed over to libvirt '
6530                         'while at the moment libvirt only supports one. Only '
6531                         'the first allocation will be looked up.')
6532         rp_uuid, alloc = six.next(six.iteritems(vgpu_allocations))
6533         vgpus_asked = alloc['resources'][orc.VGPU]
6534 
6535         # Find if we allocated against a specific pGPU (and then the allocation
6536         # is made against a child RP) or any pGPU (in case the VGPU inventory
6537         # is still on the root RP)
6538         try:
6539             allocated_rp = self.provider_tree.data(rp_uuid)
6540         except ValueError:
6541             # The provider doesn't exist, return a better understandable
6542             # exception
6543             raise exception.ComputeResourcesUnavailable(
6544                 reason='vGPU resource is not available')
6545         # TODO(sbauza): Remove this conditional in Train once all VGPU
6546         # inventories are related to a child RP
6547         if allocated_rp.parent_uuid is None:
6548             # We are on a root RP
6549             parent_device = None
6550         else:
6551             rp_name = allocated_rp.name
6552             # There can be multiple roots, we need to find the root name
6553             # to guess the physical device name
6554             roots = self.provider_tree.roots
6555             for root in roots:
6556                 if rp_name.startswith(root.name + '_'):
6557                     # The RP name convention is :
6558                     #    root_name + '_' + parent_device
6559                     parent_device = rp_name[len(root.name) + 1:]
6560                     break
6561             else:
6562                 LOG.warning("pGPU device name %(name)s can't be guessed from "
6563                             "the ProviderTree "
6564                             "roots %(roots)s", {'name': rp_name,
6565                                                  'roots': roots})
6566                 # We f... have no idea what was the parent device
6567                 # If we can't find devices having available VGPUs, just raise
6568                 raise exception.ComputeResourcesUnavailable(
6569                     reason='vGPU resource is not available')
6570 
6571         requested_types = self._get_supported_vgpu_types()
6572         # Which mediated devices are created but not assigned to a guest ?
6573         mdevs_available = self._get_existing_mdevs_not_assigned(
6574             requested_types, parent_device)
6575 
6576         chosen_mdevs = []
6577         for c in six.moves.range(vgpus_asked):
6578             chosen_mdev = None
6579             if mdevs_available:
6580                 # Take the first available mdev
6581                 chosen_mdev = mdevs_available.pop()
6582             else:
6583                 chosen_mdev = self._create_new_mediated_device(
6584                     requested_types, parent=parent_device)
6585             if not chosen_mdev:
6586                 # If we can't find devices having available VGPUs, just raise
6587                 raise exception.ComputeResourcesUnavailable(
6588                     reason='vGPU resource is not available')
6589             else:
6590                 chosen_mdevs.append(chosen_mdev)
6591         return chosen_mdevs
6592 
6593     def _detach_mediated_devices(self, guest):
6594         mdevs = guest.get_all_devices(
6595             devtype=vconfig.LibvirtConfigGuestHostdevMDEV)
6596         for mdev_cfg in mdevs:
6597             try:
6598                 guest.detach_device(mdev_cfg, live=True)
6599             except libvirt.libvirtError as ex:
6600                 error_code = ex.get_error_code()
6601                 # NOTE(sbauza): There is a pending issue with libvirt that
6602                 # doesn't allow to hot-unplug mediated devices. Let's
6603                 # short-circuit the suspend action and set the instance back
6604                 # to ACTIVE.
6605                 # TODO(sbauza): Once libvirt supports this, amend the resume()
6606                 # operation to support reallocating mediated devices.
6607                 if error_code == libvirt.VIR_ERR_CONFIG_UNSUPPORTED:
6608                     reason = _("Suspend is not supported for instances having "
6609                                "attached vGPUs.")
6610                     raise exception.InstanceFaultRollback(
6611                         exception.InstanceSuspendFailure(reason=reason))
6612                 else:
6613                     raise
6614 
6615     def _has_numa_support(self):
6616         # This means that the host can support LibvirtConfigGuestNUMATune
6617         # and the nodeset field in LibvirtConfigGuestMemoryBackingPage
6618         caps = self._host.get_capabilities()
6619 
6620         if (caps.host.cpu.arch in (fields.Architecture.I686,
6621                                    fields.Architecture.X86_64,
6622                                    fields.Architecture.AARCH64) and
6623                 self._host.has_min_version(hv_type=host.HV_DRIVER_QEMU)):
6624             return True
6625         elif (caps.host.cpu.arch in (fields.Architecture.PPC64,
6626                                      fields.Architecture.PPC64LE)):
6627             return True
6628 
6629         return False
6630 
6631     def _get_host_numa_topology(self):
6632         if not self._has_numa_support():
6633             return
6634 
6635         caps = self._host.get_capabilities()
6636         topology = caps.host.topology
6637 
6638         if topology is None or not topology.cells:
6639             return
6640 
6641         cells = []
6642         allowed_cpus = hardware.get_vcpu_pin_set()
6643         online_cpus = self._host.get_online_cpus()
6644         if allowed_cpus:
6645             allowed_cpus &= online_cpus
6646         else:
6647             allowed_cpus = online_cpus
6648 
6649         def _get_reserved_memory_for_cell(self, cell_id, page_size):
6650             cell = self._reserved_hugepages.get(cell_id, {})
6651             return cell.get(page_size, 0)
6652 
6653         def _get_physnet_numa_affinity():
6654             affinities = {cell.id: set() for cell in topology.cells}
6655             for physnet in CONF.neutron.physnets:
6656                 # This will error out if the group is not registered, which is
6657                 # exactly what we want as that would be a bug
6658                 group = getattr(CONF, 'neutron_physnet_%s' % physnet)
6659 
6660                 if not group.numa_nodes:
6661                     msg = ("the physnet '%s' was listed in '[neutron] "
6662                            "physnets' but no corresponding "
6663                            "'[neutron_physnet_%s] numa_nodes' option was "
6664                            "defined." % (physnet, physnet))
6665                     raise exception.InvalidNetworkNUMAAffinity(reason=msg)
6666 
6667                 for node in group.numa_nodes:
6668                     if node not in affinities:
6669                         msg = ("node %d for physnet %s is not present in host "
6670                                "affinity set %r" % (node, physnet, affinities))
6671                         # The config option referenced an invalid node
6672                         raise exception.InvalidNetworkNUMAAffinity(reason=msg)
6673                     affinities[node].add(physnet)
6674 
6675             return affinities
6676 
6677         def _get_tunnel_numa_affinity():
6678             affinities = {cell.id: False for cell in topology.cells}
6679 
6680             for node in CONF.neutron_tunnel.numa_nodes:
6681                 if node not in affinities:
6682                     msg = ("node %d for tunneled networks is not present "
6683                            "in host affinity set %r" % (node, affinities))
6684                     # The config option referenced an invalid node
6685                     raise exception.InvalidNetworkNUMAAffinity(reason=msg)
6686                 affinities[node] = True
6687 
6688             return affinities
6689 
6690         physnet_affinities = _get_physnet_numa_affinity()
6691         tunnel_affinities = _get_tunnel_numa_affinity()
6692 
6693         for cell in topology.cells:
6694             cpuset = set(cpu.id for cpu in cell.cpus)
6695             siblings = sorted(map(set,
6696                                   set(tuple(cpu.siblings)
6697                                         if cpu.siblings else ()
6698                                       for cpu in cell.cpus)
6699                                   ))
6700             cpuset &= allowed_cpus
6701             siblings = [sib & allowed_cpus for sib in siblings]
6702             # Filter out empty sibling sets that may be left
6703             siblings = [sib for sib in siblings if len(sib) > 0]
6704 
6705             mempages = [
6706                 objects.NUMAPagesTopology(
6707                     size_kb=pages.size,
6708                     total=pages.total,
6709                     used=0,
6710                     reserved=_get_reserved_memory_for_cell(
6711                         self, cell.id, pages.size))
6712                 for pages in cell.mempages]
6713 
6714             network_metadata = objects.NetworkMetadata(
6715                 physnets=physnet_affinities[cell.id],
6716                 tunneled=tunnel_affinities[cell.id])
6717 
6718             cell = objects.NUMACell(id=cell.id, cpuset=cpuset,
6719                                     memory=cell.memory / units.Ki,
6720                                     cpu_usage=0, memory_usage=0,
6721                                     siblings=siblings,
6722                                     pinned_cpus=set([]),
6723                                     mempages=mempages,
6724                                     network_metadata=network_metadata)
6725             cells.append(cell)
6726 
6727         return objects.NUMATopology(cells=cells)
6728 
6729     def get_all_volume_usage(self, context, compute_host_bdms):
6730         """Return usage info for volumes attached to vms on
6731            a given host.
6732         """
6733         vol_usage = []
6734 
6735         for instance_bdms in compute_host_bdms:
6736             instance = instance_bdms['instance']
6737 
6738             for bdm in instance_bdms['instance_bdms']:
6739                 mountpoint = bdm['device_name']
6740                 if mountpoint.startswith('/dev/'):
6741                     mountpoint = mountpoint[5:]
6742                 volume_id = bdm['volume_id']
6743 
6744                 LOG.debug("Trying to get stats for the volume %s",
6745                           volume_id, instance=instance)
6746                 vol_stats = self.block_stats(instance, mountpoint)
6747 
6748                 if vol_stats:
6749                     stats = dict(volume=volume_id,
6750                                  instance=instance,
6751                                  rd_req=vol_stats[0],
6752                                  rd_bytes=vol_stats[1],
6753                                  wr_req=vol_stats[2],
6754                                  wr_bytes=vol_stats[3])
6755                     LOG.debug(
6756                         "Got volume usage stats for the volume=%(volume)s,"
6757                         " rd_req=%(rd_req)d, rd_bytes=%(rd_bytes)d, "
6758                         "wr_req=%(wr_req)d, wr_bytes=%(wr_bytes)d",
6759                         stats, instance=instance)
6760                     vol_usage.append(stats)
6761 
6762         return vol_usage
6763 
6764     def block_stats(self, instance, disk_id):
6765         """Note that this function takes an instance name."""
6766         try:
6767             guest = self._host.get_guest(instance)
6768             dev = guest.get_block_device(disk_id)
6769             return dev.blockStats()
6770         except libvirt.libvirtError as e:
6771             errcode = e.get_error_code()
6772             LOG.info('Getting block stats failed, device might have '
6773                      'been detached. Instance=%(instance_name)s '
6774                      'Disk=%(disk)s Code=%(errcode)s Error=%(e)s',
6775                      {'instance_name': instance.name, 'disk': disk_id,
6776                       'errcode': errcode, 'e': e},
6777                      instance=instance)
6778         except exception.InstanceNotFound:
6779             LOG.info('Could not find domain in libvirt for instance %s. '
6780                      'Cannot get block stats for device', instance.name,
6781                      instance=instance)
6782 
6783     def get_console_pool_info(self, console_type):
6784         # TODO(mdragon): console proxy should be implemented for libvirt,
6785         #                in case someone wants to use it with kvm or
6786         #                such. For now return fake data.
6787         return {'address': '127.0.0.1',
6788                 'username': 'fakeuser',
6789                 'password': 'fakepassword'}
6790 
6791     def refresh_security_group_rules(self, security_group_id):
6792         self.firewall_driver.refresh_security_group_rules(security_group_id)
6793 
6794     def refresh_instance_security_rules(self, instance):
6795         self.firewall_driver.refresh_instance_security_rules(instance)
6796 
6797     def update_provider_tree(self, provider_tree, nodename, allocations=None):
6798         """Update a ProviderTree object with current resource provider,
6799         inventory information and CPU traits.
6800 
6801         :param nova.compute.provider_tree.ProviderTree provider_tree:
6802             A nova.compute.provider_tree.ProviderTree object representing all
6803             the providers in the tree associated with the compute node, and any
6804             sharing providers (those with the ``MISC_SHARES_VIA_AGGREGATE``
6805             trait) associated via aggregate with any of those providers (but
6806             not *their* tree- or aggregate-associated providers), as currently
6807             known by placement.
6808         :param nodename:
6809             String name of the compute node (i.e.
6810             ComputeNode.hypervisor_hostname) for which the caller is requesting
6811             updated provider information.
6812         :param allocations:
6813             Dict of allocation data of the form:
6814               { $CONSUMER_UUID: {
6815                     # The shape of each "allocations" dict below is identical
6816                     # to the return from GET /allocations/{consumer_uuid}
6817                     "allocations": {
6818                         $RP_UUID: {
6819                             "generation": $RP_GEN,
6820                             "resources": {
6821                                 $RESOURCE_CLASS: $AMOUNT,
6822                                 ...
6823                             },
6824                         },
6825                         ...
6826                     },
6827                     "project_id": $PROJ_ID,
6828                     "user_id": $USER_ID,
6829                     "consumer_generation": $CONSUMER_GEN,
6830                 },
6831                 ...
6832               }
6833             If None, and the method determines that any inventory needs to be
6834             moved (from one provider to another and/or to a different resource
6835             class), the ReshapeNeeded exception must be raised. Otherwise, this
6836             dict must be edited in place to indicate the desired final state of
6837             allocations.
6838         :raises ReshapeNeeded: If allocations is None and any inventory needs
6839             to be moved from one provider to another and/or to a different
6840             resource class.
6841         :raises: ReshapeFailed if the requested tree reshape fails for
6842             whatever reason.
6843         """
6844         disk_gb = int(self._get_local_gb_info()['total'])
6845         memory_mb = int(self._host.get_memory_mb_total())
6846         vcpus = self._get_vcpu_total()
6847 
6848         # NOTE(yikun): If the inv record does not exists, the allocation_ratio
6849         # will use the CONF.xxx_allocation_ratio value if xxx_allocation_ratio
6850         # is set, and fallback to use the initial_xxx_allocation_ratio
6851         # otherwise.
6852         inv = provider_tree.data(nodename).inventory
6853         ratios = self._get_allocation_ratios(inv)
6854         result = {
6855             orc.VCPU: {
6856                 'total': vcpus,
6857                 'min_unit': 1,
6858                 'max_unit': vcpus,
6859                 'step_size': 1,
6860                 'allocation_ratio': ratios[orc.VCPU],
6861                 'reserved': CONF.reserved_host_cpus,
6862             },
6863             orc.MEMORY_MB: {
6864                 'total': memory_mb,
6865                 'min_unit': 1,
6866                 'max_unit': memory_mb,
6867                 'step_size': 1,
6868                 'allocation_ratio': ratios[orc.MEMORY_MB],
6869                 'reserved': CONF.reserved_host_memory_mb,
6870             },
6871         }
6872 
6873         # If a sharing DISK_GB provider exists in the provider tree, then our
6874         # storage is shared, and we should not report the DISK_GB inventory in
6875         # the compute node provider.
6876         # TODO(efried): Reinstate non-reporting of shared resource by the
6877         # compute RP once the issues from bug #1784020 have been resolved.
6878         if provider_tree.has_sharing_provider(orc.DISK_GB):
6879             LOG.debug('Ignoring sharing provider - see bug #1784020')
6880         result[orc.DISK_GB] = {
6881             'total': disk_gb,
6882             'min_unit': 1,
6883             'max_unit': disk_gb,
6884             'step_size': 1,
6885             'allocation_ratio': ratios[orc.DISK_GB],
6886             'reserved': self._get_reserved_host_disk_gb_from_config(),
6887         }
6888 
6889         # NOTE(sbauza): For the moment, the libvirt driver only supports
6890         # providing the total number of virtual GPUs for a single GPU type. If
6891         # you have multiple physical GPUs, each of them providing multiple GPU
6892         # types, only one type will be used for each of the physical GPUs.
6893         # If one of the pGPUs doesn't support this type, it won't be used.
6894         # TODO(sbauza): Use traits to make a better world.
6895         inventories_dict = self._get_gpu_inventories()
6896         if inventories_dict:
6897             self._update_provider_tree_for_vgpu(
6898                 inventories_dict, provider_tree, nodename,
6899                 allocations=allocations)
6900 
6901         provider_tree.update_inventory(nodename, result)
6902 
6903         traits = self._get_cpu_traits()
6904         if traits is not None:
6905             # _get_cpu_traits returns a dict of trait names mapped to boolean
6906             # values. Add traits equal to True to provider tree, remove
6907             # those False traits from provider tree.
6908             traits_to_add = [t for t in traits if traits[t]]
6909             traits_to_remove = set(traits) - set(traits_to_add)
6910             provider_tree.add_traits(nodename, *traits_to_add)
6911             provider_tree.remove_traits(nodename, *traits_to_remove)
6912 
6913         # Now that we updated the ProviderTree, we want to store it locally
6914         # so that spawn() or other methods can access it thru a getter
6915         self.provider_tree = copy.deepcopy(provider_tree)
6916 
6917     @staticmethod
6918     def _is_reshape_needed_vgpu_on_root(provider_tree, nodename):
6919         """Determine if root RP has VGPU inventories.
6920 
6921         Check to see if the root compute node provider in the tree for
6922         this host already has VGPU inventory because if it does, we either
6923         need to signal for a reshape (if _update_provider_tree_for_vgpu()
6924         has no allocations) or move the allocations within the ProviderTree if
6925         passed.
6926 
6927         :param provider_tree: The ProviderTree object for this host.
6928         :param nodename: The ComputeNode.hypervisor_hostname, also known as
6929             the name of the root node provider in the tree for this host.
6930         :returns: boolean, whether we have VGPU root inventory.
6931         """
6932         root_node = provider_tree.data(nodename)
6933         return orc.VGPU in root_node.inventory
6934 
6935     @staticmethod
6936     def _ensure_pgpu_providers(inventories_dict, provider_tree, nodename):
6937         """Ensures GPU inventory providers exist in the tree for $nodename.
6938 
6939         GPU providers are named $nodename_$gpu-device-id, e.g.
6940         ``somehost.foo.bar.com_pci_0000_84_00_0``.
6941 
6942         :param inventories_dict: Dictionary of inventories for VGPU class
6943             directly provided by _get_gpu_inventories() and which looks like:
6944                 {'pci_0000_84_00_0':
6945                     {'total': $TOTAL,
6946                      'min_unit': 1,
6947                      'max_unit': $MAX_UNIT, # defaults to $TOTAL
6948                      'step_size': 1,
6949                      'reserved': 0,
6950                      'allocation_ratio': 1.0,
6951                     }
6952                 }
6953         :param provider_tree: The ProviderTree to update.
6954         :param nodename: The ComputeNode.hypervisor_hostname, also known as
6955             the name of the root node provider in the tree for this host.
6956         :returns: dict, keyed by GPU device ID, to ProviderData object
6957             representing that resource provider in the tree
6958         """
6959         # Create the VGPU child providers if they do not already exist.
6960         # TODO(mriedem): For the moment, _get_supported_vgpu_types() only
6961         # returns one single type but that will be changed once we support
6962         # multiple types.
6963         # Note that we can't support multiple vgpu types until a reshape has
6964         # been performed on the vgpu resources provided by the root provider,
6965         # if any.
6966 
6967         # Dict of PGPU RPs keyed by their libvirt PCI name
6968         pgpu_rps = {}
6969         for pgpu_dev_id, inventory in inventories_dict.items():
6970             # For each physical GPU, we make sure to have a child provider
6971             pgpu_rp_name = '%s_%s' % (nodename, pgpu_dev_id)
6972             if not provider_tree.exists(pgpu_rp_name):
6973                 # This is the first time creating the child provider so add
6974                 # it to the tree under the root node provider.
6975                 provider_tree.new_child(pgpu_rp_name, nodename)
6976             # We want to idempotently return the resource providers with VGPUs
6977             pgpu_rp = provider_tree.data(pgpu_rp_name)
6978             pgpu_rps[pgpu_dev_id] = pgpu_rp
6979 
6980             # The VGPU inventory goes on a child provider of the given root
6981             # node, identified by $nodename.
6982             pgpu_inventory = {orc.VGPU: inventory}
6983             provider_tree.update_inventory(pgpu_rp_name, pgpu_inventory)
6984         return pgpu_rps
6985 
6986     @staticmethod
6987     def _assert_is_root_provider(
6988             rp_uuid, root_node, consumer_uuid, alloc_data):
6989         """Asserts during a reshape that rp_uuid is for the root node provider.
6990 
6991         When reshaping, inventory and allocations should be on the root node
6992         provider and then moved to child providers.
6993 
6994         :param rp_uuid: UUID of the provider that holds inventory/allocations.
6995         :param root_node: ProviderData object representing the root node in a
6996             provider tree.
6997         :param consumer_uuid: UUID of the consumer (instance) holding resource
6998             allocations against the given rp_uuid provider.
6999         :param alloc_data: dict of allocation data for the consumer.
7000         :raises: ReshapeFailed if rp_uuid is not the root node indicating a
7001             reshape was needed but the inventory/allocation structure is not
7002             expected.
7003         """
7004         if rp_uuid != root_node.uuid:
7005             # Something is wrong - VGPU inventory should
7006             # only be on the root node provider if we are
7007             # reshaping the tree.
7008             msg = (_('Unexpected VGPU resource allocation '
7009                      'on provider %(rp_uuid)s for consumer '
7010                      '%(consumer_uuid)s: %(alloc_data)s. '
7011                      'Expected VGPU allocation to be on root '
7012                      'compute node provider %(root_uuid)s.')
7013                    % {'rp_uuid': rp_uuid,
7014                       'consumer_uuid': consumer_uuid,
7015                       'alloc_data': alloc_data,
7016                       'root_uuid': root_node.uuid})
7017             raise exception.ReshapeFailed(error=msg)
7018 
7019     def _get_assigned_mdevs_for_reshape(
7020             self, instance_uuid, rp_uuid, alloc_data):
7021         """Gets the mediated devices assigned to the instance during a reshape.
7022 
7023         :param instance_uuid: UUID of the instance consuming VGPU resources
7024             on this host.
7025         :param rp_uuid: UUID of the resource provider with VGPU inventory being
7026             consumed by the instance.
7027         :param alloc_data: dict of allocation data for the instance consumer.
7028         :return: list of mediated device UUIDs assigned to the instance
7029         :raises: ReshapeFailed if the instance is not found in the hypervisor
7030             or no mediated devices were found to be assigned to the instance
7031             indicating VGPU allocations are out of sync with the hypervisor
7032         """
7033         # FIXME(sbauza): We don't really need an Instance
7034         # object, but given some libvirt.host logs needs
7035         # to have an instance name, just provide a fake one
7036         Instance = collections.namedtuple('Instance', ['uuid', 'name'])
7037         instance = Instance(uuid=instance_uuid, name=instance_uuid)
7038         mdevs = self._get_all_assigned_mediated_devices(instance)
7039         # _get_all_assigned_mediated_devices returns {} if the instance is
7040         # not found in the hypervisor
7041         if not mdevs:
7042             # If we found a VGPU allocation against a consumer
7043             # which is not an instance, the only left case for
7044             # Nova would be a migration but we don't support
7045             # this at the moment.
7046             msg = (_('Unexpected VGPU resource allocation on provider '
7047                      '%(rp_uuid)s for consumer %(consumer_uuid)s: '
7048                      '%(alloc_data)s. The allocation is made against a '
7049                      'non-existing instance or there are no devices assigned.')
7050                    % {'rp_uuid': rp_uuid, 'consumer_uuid': instance_uuid,
7051                       'alloc_data': alloc_data})
7052             raise exception.ReshapeFailed(error=msg)
7053         return mdevs
7054 
7055     def _count_vgpus_per_pgpu(self, mdev_uuids):
7056         """Count the number of VGPUs per physical GPU mediated device.
7057 
7058         :param mdev_uuids: List of physical GPU mediated device UUIDs.
7059         :return: dict, keyed by PGPU device ID, to count of VGPUs on that
7060             device
7061         """
7062         vgpu_count_per_pgpu = collections.defaultdict(int)
7063         for mdev_uuid in mdev_uuids:
7064             # libvirt name is like mdev_00ead764_fdc0_46b6_8db9_2963f5c815b4
7065             dev_name = libvirt_utils.mdev_uuid2name(mdev_uuid)
7066             # Count how many vGPUs are in use for this instance
7067             dev_info = self._get_mediated_device_information(dev_name)
7068             pgpu_dev_id = dev_info['parent']
7069             vgpu_count_per_pgpu[pgpu_dev_id] += 1
7070         return vgpu_count_per_pgpu
7071 
7072     @staticmethod
7073     def _check_vgpu_allocations_match_real_use(
7074             vgpu_count_per_pgpu, expected_usage, rp_uuid, consumer_uuid,
7075             alloc_data):
7076         """Checks that the number of GPU devices assigned to the consumer
7077         matches what is expected from the allocations in the placement service
7078         and logs a warning if there is a mismatch.
7079 
7080         :param vgpu_count_per_pgpu: dict, keyed by PGPU device ID, to count of
7081             VGPUs on that device where each device is assigned to the consumer
7082             (guest instance on this hypervisor)
7083         :param expected_usage: The expected usage from placement for the
7084             given resource provider and consumer
7085         :param rp_uuid: UUID of the resource provider with VGPU inventory being
7086             consumed by the instance
7087         :param consumer_uuid: UUID of the consumer (instance) holding resource
7088             allocations against the given rp_uuid provider
7089         :param alloc_data: dict of allocation data for the instance consumer
7090         """
7091         actual_usage = sum(vgpu_count_per_pgpu.values())
7092         if actual_usage != expected_usage:
7093             # Don't make it blocking, just make sure you actually correctly
7094             # allocate the existing resources
7095             LOG.warning(
7096                 'Unexpected VGPU resource allocation on provider %(rp_uuid)s '
7097                 'for consumer %(consumer_uuid)s: %(alloc_data)s. Allocations '
7098                 '(%(expected_usage)s) differ from actual use '
7099                 '(%(actual_usage)s).',
7100                 {'rp_uuid': rp_uuid, 'consumer_uuid': consumer_uuid,
7101                  'alloc_data': alloc_data, 'expected_usage': expected_usage,
7102                  'actual_usage': actual_usage})
7103 
7104     def _reshape_vgpu_allocations(
7105             self, rp_uuid, root_node, consumer_uuid, alloc_data, resources,
7106             pgpu_rps):
7107         """Update existing VGPU allocations by moving them from the root node
7108         provider to the child provider for the given VGPU provider.
7109 
7110         :param rp_uuid: UUID of the VGPU resource provider with allocations
7111             from consumer_uuid (should be the root node provider before
7112             reshaping occurs)
7113         :param root_node: ProviderData object for the root compute node
7114             resource provider in the provider tree
7115         :param consumer_uuid: UUID of the consumer (instance) with VGPU
7116             allocations against the resource provider represented by rp_uuid
7117         :param alloc_data: dict of allocation information for consumer_uuid
7118         :param resources: dict, keyed by resource class, of resources allocated
7119             to consumer_uuid from rp_uuid
7120         :param pgpu_rps: dict, keyed by GPU device ID, to ProviderData object
7121             representing that resource provider in the tree
7122         :raises: ReshapeFailed if the reshape fails for whatever reason
7123         """
7124         # We've found VGPU allocations on a provider. It should be the root
7125         # node provider.
7126         self._assert_is_root_provider(
7127             rp_uuid, root_node, consumer_uuid, alloc_data)
7128 
7129         # Find which physical GPU corresponds to this allocation.
7130         mdev_uuids = self._get_assigned_mdevs_for_reshape(
7131             consumer_uuid, rp_uuid, alloc_data)
7132 
7133         vgpu_count_per_pgpu = self._count_vgpus_per_pgpu(mdev_uuids)
7134 
7135         # We need to make sure we found all the mediated devices that
7136         # correspond to an allocation.
7137         self._check_vgpu_allocations_match_real_use(
7138             vgpu_count_per_pgpu, resources[orc.VGPU],
7139             rp_uuid, consumer_uuid, alloc_data)
7140 
7141         # Add the VGPU allocation for each VGPU provider.
7142         allocs = alloc_data['allocations']
7143         for pgpu_dev_id, pgpu_rp in pgpu_rps.items():
7144             vgpu_count = vgpu_count_per_pgpu[pgpu_dev_id]
7145             if vgpu_count:
7146                 allocs[pgpu_rp.uuid] = {
7147                     'resources': {
7148                         orc.VGPU: vgpu_count
7149                     }
7150                 }
7151         # And remove the VGPU allocation from the root node provider.
7152         del resources[orc.VGPU]
7153 
7154     def _reshape_gpu_resources(
7155             self, allocations, root_node, pgpu_rps):
7156         """Reshapes the provider tree moving VGPU inventory from root to child
7157 
7158         :param allocations:
7159             Dict of allocation data of the form:
7160               { $CONSUMER_UUID: {
7161                     # The shape of each "allocations" dict below is identical
7162                     # to the return from GET /allocations/{consumer_uuid}
7163                     "allocations": {
7164                         $RP_UUID: {
7165                             "generation": $RP_GEN,
7166                             "resources": {
7167                                 $RESOURCE_CLASS: $AMOUNT,
7168                                 ...
7169                             },
7170                         },
7171                         ...
7172                     },
7173                     "project_id": $PROJ_ID,
7174                     "user_id": $USER_ID,
7175                     "consumer_generation": $CONSUMER_GEN,
7176                 },
7177                 ...
7178               }
7179         :params root_node: The root node in the provider tree
7180         :params pgpu_rps: dict, keyed by GPU device ID, to ProviderData object
7181             representing that resource provider in the tree
7182         """
7183         LOG.info('Reshaping tree; moving VGPU allocations from root '
7184                  'provider %s to child providers %s.', root_node.uuid,
7185                  pgpu_rps.values())
7186         # For each consumer in the allocations dict, look for VGPU
7187         # allocations and move them to the VGPU provider.
7188         for consumer_uuid, alloc_data in allocations.items():
7189             # Copy and iterate over the current set of providers to avoid
7190             # modifying keys while iterating.
7191             allocs = alloc_data['allocations']
7192             for rp_uuid in list(allocs):
7193                 resources = allocs[rp_uuid]['resources']
7194                 if orc.VGPU in resources:
7195                     self._reshape_vgpu_allocations(
7196                         rp_uuid, root_node, consumer_uuid, alloc_data,
7197                         resources, pgpu_rps)
7198 
7199     def _update_provider_tree_for_vgpu(self, inventories_dict, provider_tree,
7200                                        nodename, allocations=None):
7201         """Updates the provider tree for VGPU inventory.
7202 
7203         Before Stein, VGPU inventory and allocations were on the root compute
7204         node provider in the tree. Starting in Stein, the VGPU inventory is
7205         on a child provider in the tree. As a result, this method will
7206         "reshape" the tree if necessary on first start of this compute service
7207         in Stein.
7208 
7209         :param inventories_dict: Dictionary of inventories for VGPU class
7210             directly provided by _get_gpu_inventories() and which looks like:
7211                 {'pci_0000_84_00_0':
7212                     {'total': $TOTAL,
7213                      'min_unit': 1,
7214                      'max_unit': $MAX_UNIT, # defaults to $TOTAL
7215                      'step_size': 1,
7216                      'reserved': 0,
7217                      'allocation_ratio': 1.0,
7218                     }
7219                 }
7220         :param provider_tree: The ProviderTree to update.
7221         :param nodename: The ComputeNode.hypervisor_hostname, also known as
7222             the name of the root node provider in the tree for this host.
7223         :param allocations: If not None, indicates a reshape was requested and
7224             should be performed.
7225         :raises: nova.exception.ReshapeNeeded if ``allocations`` is None and
7226             the method determines a reshape of the tree is needed, i.e. VGPU
7227             inventory and allocations must be migrated from the root node
7228             provider to a child provider of VGPU resources in the tree.
7229         :raises: nova.exception.ReshapeFailed if the requested tree reshape
7230             fails for whatever reason.
7231         """
7232         # Check to see if the root compute node provider in the tree for
7233         # this host already has VGPU inventory because if it does, and
7234         # we're not currently reshaping (allocations is None), we need
7235         # to indicate that a reshape is needed to move the VGPU inventory
7236         # onto a child provider in the tree.
7237 
7238         # Ensure GPU providers are in the ProviderTree for the given inventory.
7239         pgpu_rps = self._ensure_pgpu_providers(
7240             inventories_dict, provider_tree, nodename)
7241 
7242         if self._is_reshape_needed_vgpu_on_root(provider_tree, nodename):
7243             if allocations is None:
7244                 # We have old VGPU inventory on root RP, but we haven't yet
7245                 # allocations. That means we need to ask for a reshape.
7246                 LOG.info('Requesting provider tree reshape in order to move '
7247                          'VGPU inventory from the root compute node provider '
7248                          '%s to a child provider.', nodename)
7249                 raise exception.ReshapeNeeded()
7250             # We have allocations, that means we already asked for a reshape
7251             # and the Placement API returned us them. We now need to move
7252             # those from the root RP to the needed children RPs.
7253             root_node = provider_tree.data(nodename)
7254             # Reshape VGPU provider inventory and allocations, moving them
7255             # from the root node provider to the child providers.
7256             self._reshape_gpu_resources(allocations, root_node, pgpu_rps)
7257             # Only delete the root inventory once the reshape is done
7258             if orc.VGPU in root_node.inventory:
7259                 del root_node.inventory[orc.VGPU]
7260                 provider_tree.update_inventory(nodename, root_node.inventory)
7261 
7262     def get_available_resource(self, nodename):
7263         """Retrieve resource information.
7264 
7265         This method is called when nova-compute launches, and
7266         as part of a periodic task that records the results in the DB.
7267 
7268         :param nodename: unused in this driver
7269         :returns: dictionary containing resource info
7270         """
7271 
7272         disk_info_dict = self._get_local_gb_info()
7273         data = {}
7274 
7275         # NOTE(dprince): calling capabilities before getVersion works around
7276         # an initialization issue with some versions of Libvirt (1.0.5.5).
7277         # See: https://bugzilla.redhat.com/show_bug.cgi?id=1000116
7278         # See: https://bugs.launchpad.net/nova/+bug/1215593
7279         data["supported_instances"] = self._get_instance_capabilities()
7280 
7281         data["vcpus"] = self._get_vcpu_total()
7282         data["memory_mb"] = self._host.get_memory_mb_total()
7283         data["local_gb"] = disk_info_dict['total']
7284         data["vcpus_used"] = self._get_vcpu_used()
7285         data["memory_mb_used"] = self._host.get_memory_mb_used()
7286         data["local_gb_used"] = disk_info_dict['used']
7287         data["hypervisor_type"] = self._host.get_driver_type()
7288         data["hypervisor_version"] = self._host.get_version()
7289         data["hypervisor_hostname"] = self._host.get_hostname()
7290         # TODO(berrange): why do we bother converting the
7291         # libvirt capabilities XML into a special JSON format ?
7292         # The data format is different across all the drivers
7293         # so we could just return the raw capabilities XML
7294         # which 'compare_cpu' could use directly
7295         #
7296         # That said, arch_filter.py now seems to rely on
7297         # the libvirt drivers format which suggests this
7298         # data format needs to be standardized across drivers
7299         data["cpu_info"] = jsonutils.dumps(self._get_cpu_info())
7300 
7301         disk_free_gb = disk_info_dict['free']
7302         disk_over_committed = self._get_disk_over_committed_size_total()
7303         available_least = disk_free_gb * units.Gi - disk_over_committed
7304         data['disk_available_least'] = available_least / units.Gi
7305 
7306         data['pci_passthrough_devices'] = self._get_pci_passthrough_devices()
7307 
7308         numa_topology = self._get_host_numa_topology()
7309         if numa_topology:
7310             data['numa_topology'] = numa_topology._to_json()
7311         else:
7312             data['numa_topology'] = None
7313 
7314         return data
7315 
7316     def check_instance_shared_storage_local(self, context, instance):
7317         """Check if instance files located on shared storage.
7318 
7319         This runs check on the destination host, and then calls
7320         back to the source host to check the results.
7321 
7322         :param context: security context
7323         :param instance: nova.objects.instance.Instance object
7324         :returns:
7325          - tempfile: A dict containing the tempfile info on the destination
7326                      host
7327          - None:
7328 
7329             1. If the instance path is not existing.
7330             2. If the image backend is shared block storage type.
7331         """
7332         if self.image_backend.backend().is_shared_block_storage():
7333             return None
7334 
7335         dirpath = libvirt_utils.get_instance_path(instance)
7336 
7337         if not os.path.exists(dirpath):
7338             return None
7339 
7340         fd, tmp_file = tempfile.mkstemp(dir=dirpath)
7341         LOG.debug("Creating tmpfile %s to verify with other "
7342                   "compute node that the instance is on "
7343                   "the same shared storage.",
7344                   tmp_file, instance=instance)
7345         os.close(fd)
7346         return {"filename": tmp_file}
7347 
7348     def check_instance_shared_storage_remote(self, context, data):
7349         return os.path.exists(data['filename'])
7350 
7351     def check_instance_shared_storage_cleanup(self, context, data):
7352         fileutils.delete_if_exists(data["filename"])
7353 
7354     def check_can_live_migrate_destination(self, context, instance,
7355                                            src_compute_info, dst_compute_info,
7356                                            block_migration=False,
7357                                            disk_over_commit=False):
7358         """Check if it is possible to execute live migration.
7359 
7360         This runs checks on the destination host, and then calls
7361         back to the source host to check the results.
7362 
7363         :param context: security context
7364         :param instance: nova.db.sqlalchemy.models.Instance
7365         :param block_migration: if true, prepare for block migration
7366         :param disk_over_commit: if true, allow disk over commit
7367         :returns: a LibvirtLiveMigrateData object
7368         """
7369         if disk_over_commit:
7370             disk_available_gb = dst_compute_info['free_disk_gb']
7371         else:
7372             disk_available_gb = dst_compute_info['disk_available_least']
7373         disk_available_mb = (
7374             (disk_available_gb * units.Ki) - CONF.reserved_host_disk_mb)
7375 
7376         # Compare CPU
7377         if not instance.vcpu_model or not instance.vcpu_model.model:
7378             source_cpu_info = src_compute_info['cpu_info']
7379             self._compare_cpu(None, source_cpu_info, instance)
7380         else:
7381             self._compare_cpu(instance.vcpu_model, None, instance)
7382 
7383         # Create file on storage, to be checked on source host
7384         filename = self._create_shared_storage_test_file(instance)
7385 
7386         data = objects.LibvirtLiveMigrateData()
7387         data.filename = filename
7388         data.image_type = CONF.libvirt.images_type
7389         data.graphics_listen_addr_vnc = CONF.vnc.server_listen
7390         data.graphics_listen_addr_spice = CONF.spice.server_listen
7391         if CONF.serial_console.enabled:
7392             data.serial_listen_addr = CONF.serial_console.proxyclient_address
7393         else:
7394             data.serial_listen_addr = None
7395         # Notes(eliqiao): block_migration and disk_over_commit are not
7396         # nullable, so just don't set them if they are None
7397         if block_migration is not None:
7398             data.block_migration = block_migration
7399         if disk_over_commit is not None:
7400             data.disk_over_commit = disk_over_commit
7401         data.disk_available_mb = disk_available_mb
7402         data.dst_wants_file_backed_memory = CONF.libvirt.file_backed_memory > 0
7403         data.file_backed_memory_discard = (CONF.libvirt.file_backed_memory and
7404             self._host.has_min_version(MIN_LIBVIRT_FILE_BACKED_DISCARD_VERSION,
7405                                        MIN_QEMU_FILE_BACKED_DISCARD_VERSION))
7406 
7407         return data
7408 
7409     def cleanup_live_migration_destination_check(self, context,
7410                                                  dest_check_data):
7411         """Do required cleanup on dest host after check_can_live_migrate calls
7412 
7413         :param context: security context
7414         """
7415         filename = dest_check_data.filename
7416         self._cleanup_shared_storage_test_file(filename)
7417 
7418     def check_can_live_migrate_source(self, context, instance,
7419                                       dest_check_data,
7420                                       block_device_info=None):
7421         """Check if it is possible to execute live migration.
7422 
7423         This checks if the live migration can succeed, based on the
7424         results from check_can_live_migrate_destination.
7425 
7426         :param context: security context
7427         :param instance: nova.db.sqlalchemy.models.Instance
7428         :param dest_check_data: result of check_can_live_migrate_destination
7429         :param block_device_info: result of _get_instance_block_device_info
7430         :returns: a LibvirtLiveMigrateData object
7431         """
7432         # Checking shared storage connectivity
7433         # if block migration, instances_path should not be on shared storage.
7434         source = CONF.host
7435 
7436         dest_check_data.is_shared_instance_path = (
7437             self._check_shared_storage_test_file(
7438                 dest_check_data.filename, instance))
7439 
7440         dest_check_data.is_shared_block_storage = (
7441             self._is_shared_block_storage(instance, dest_check_data,
7442                                           block_device_info))
7443 
7444         if 'block_migration' not in dest_check_data:
7445             dest_check_data.block_migration = (
7446                 not dest_check_data.is_on_shared_storage())
7447 
7448         if dest_check_data.block_migration:
7449             # TODO(eliqiao): Once block_migration flag is removed from the API
7450             # we can safely remove the if condition
7451             if dest_check_data.is_on_shared_storage():
7452                 reason = _("Block migration can not be used "
7453                            "with shared storage.")
7454                 raise exception.InvalidLocalStorage(reason=reason, path=source)
7455             if 'disk_over_commit' in dest_check_data:
7456                 self._assert_dest_node_has_enough_disk(context, instance,
7457                                         dest_check_data.disk_available_mb,
7458                                         dest_check_data.disk_over_commit,
7459                                         block_device_info)
7460             if block_device_info:
7461                 bdm = block_device_info.get('block_device_mapping')
7462                 # NOTE(eliqiao): Selective disk migrations are not supported
7463                 # with tunnelled block migrations so we can block them early.
7464                 if (bdm and
7465                     (self._block_migration_flags &
7466                      libvirt.VIR_MIGRATE_TUNNELLED != 0)):
7467                     msg = (_('Cannot block migrate instance %(uuid)s with'
7468                              ' mapped volumes. Selective block device'
7469                              ' migration is not supported with tunnelled'
7470                              ' block migrations.') % {'uuid': instance.uuid})
7471                     LOG.error(msg, instance=instance)
7472                     raise exception.MigrationPreCheckError(reason=msg)
7473         elif not (dest_check_data.is_shared_block_storage or
7474                   dest_check_data.is_shared_instance_path):
7475             reason = _("Shared storage live-migration requires either shared "
7476                        "storage or boot-from-volume with no local disks.")
7477             raise exception.InvalidSharedStorage(reason=reason, path=source)
7478 
7479         # NOTE(mikal): include the instance directory name here because it
7480         # doesn't yet exist on the destination but we want to force that
7481         # same name to be used
7482         instance_path = libvirt_utils.get_instance_path(instance,
7483                                                         relative=True)
7484         dest_check_data.instance_relative_path = instance_path
7485 
7486         # NOTE(lyarwood): Used to indicate to the dest that the src is capable
7487         # of wiring up the encrypted disk configuration for the domain.
7488         # Note that this does not require the QEMU and Libvirt versions to
7489         # decrypt LUKS to be installed on the source node. Only the Nova
7490         # utility code to generate the correct XML is required, so we can
7491         # default to True here for all computes >= Queens.
7492         dest_check_data.src_supports_native_luks = True
7493 
7494         return dest_check_data
7495 
7496     def _is_shared_block_storage(self, instance, dest_check_data,
7497                                  block_device_info=None):
7498         """Check if all block storage of an instance can be shared
7499         between source and destination of a live migration.
7500 
7501         Returns true if the instance is volume backed and has no local disks,
7502         or if the image backend is the same on source and destination and the
7503         backend shares block storage between compute nodes.
7504 
7505         :param instance: nova.objects.instance.Instance object
7506         :param dest_check_data: dict with boolean fields image_type,
7507                                 is_shared_instance_path, and is_volume_backed
7508         """
7509         if (dest_check_data.obj_attr_is_set('image_type') and
7510                 CONF.libvirt.images_type == dest_check_data.image_type and
7511                 self.image_backend.backend().is_shared_block_storage()):
7512             # NOTE(dgenin): currently true only for RBD image backend
7513             return True
7514 
7515         if (dest_check_data.is_shared_instance_path and
7516                 self.image_backend.backend().is_file_in_instance_path()):
7517             # NOTE(angdraug): file based image backends (Flat, Qcow2)
7518             # place block device files under the instance path
7519             return True
7520 
7521         if (dest_check_data.is_volume_backed and
7522                 not bool(self._get_instance_disk_info(instance,
7523                                                       block_device_info))):
7524             return True
7525 
7526         return False
7527 
7528     def _assert_dest_node_has_enough_disk(self, context, instance,
7529                                              available_mb, disk_over_commit,
7530                                              block_device_info):
7531         """Checks if destination has enough disk for block migration."""
7532         # Libvirt supports qcow2 disk format,which is usually compressed
7533         # on compute nodes.
7534         # Real disk image (compressed) may enlarged to "virtual disk size",
7535         # that is specified as the maximum disk size.
7536         # (See qemu-img -f path-to-disk)
7537         # Scheduler recognizes destination host still has enough disk space
7538         # if real disk size < available disk size
7539         # if disk_over_commit is True,
7540         #  otherwise virtual disk size < available disk size.
7541 
7542         available = 0
7543         if available_mb:
7544             available = available_mb * units.Mi
7545 
7546         disk_infos = self._get_instance_disk_info(instance, block_device_info)
7547 
7548         necessary = 0
7549         if disk_over_commit:
7550             for info in disk_infos:
7551                 necessary += int(info['disk_size'])
7552         else:
7553             for info in disk_infos:
7554                 necessary += int(info['virt_disk_size'])
7555 
7556         # Check that available disk > necessary disk
7557         if (available - necessary) < 0:
7558             reason = (_('Unable to migrate %(instance_uuid)s: '
7559                         'Disk of instance is too large(available'
7560                         ' on destination host:%(available)s '
7561                         '< need:%(necessary)s)') %
7562                       {'instance_uuid': instance.uuid,
7563                        'available': available,
7564                        'necessary': necessary})
7565             raise exception.MigrationPreCheckError(reason=reason)
7566 
7567     def _compare_cpu(self, guest_cpu, host_cpu_str, instance):
7568         """Check the host is compatible with the requested CPU
7569 
7570         :param guest_cpu: nova.objects.VirtCPUModel or None
7571         :param host_cpu_str: JSON from _get_cpu_info() method
7572 
7573         If the 'guest_cpu' parameter is not None, this will be
7574         validated for migration compatibility with the host.
7575         Otherwise the 'host_cpu_str' JSON string will be used for
7576         validation.
7577 
7578         :returns:
7579             None. if given cpu info is not compatible to this server,
7580             raise exception.
7581         """
7582 
7583         # NOTE(kchamart): Comparing host to guest CPU model for emulated
7584         # guests (<domain type='qemu'>) should not matter -- in this
7585         # mode (QEMU "TCG") the CPU is fully emulated in software and no
7586         # hardware acceleration, like KVM, is involved. So, skip the CPU
7587         # compatibility check for the QEMU domain type, and retain it for
7588         # KVM guests.
7589         if CONF.libvirt.virt_type not in ['kvm']:
7590             return
7591 
7592         if guest_cpu is None:
7593             info = jsonutils.loads(host_cpu_str)
7594             LOG.info('Instance launched has CPU info: %s', host_cpu_str)
7595             cpu = vconfig.LibvirtConfigCPU()
7596             cpu.arch = info['arch']
7597             cpu.model = info['model']
7598             cpu.vendor = info['vendor']
7599             cpu.sockets = info['topology']['sockets']
7600             cpu.cores = info['topology']['cores']
7601             cpu.threads = info['topology']['threads']
7602             for f in info['features']:
7603                 cpu.add_feature(vconfig.LibvirtConfigCPUFeature(f))
7604         else:
7605             cpu = self._vcpu_model_to_cpu_config(guest_cpu)
7606 
7607         u = ("http://libvirt.org/html/libvirt-libvirt-host.html#"
7608              "virCPUCompareResult")
7609         m = _("CPU doesn't have compatibility.\n\n%(ret)s\n\nRefer to %(u)s")
7610         # unknown character exists in xml, then libvirt complains
7611         try:
7612             cpu_xml = cpu.to_xml()
7613             LOG.debug("cpu compare xml: %s", cpu_xml, instance=instance)
7614             ret = self._host.compare_cpu(cpu_xml)
7615         except libvirt.libvirtError as e:
7616             error_code = e.get_error_code()
7617             if error_code == libvirt.VIR_ERR_NO_SUPPORT:
7618                 LOG.debug("URI %(uri)s does not support cpu comparison. "
7619                           "It will be proceeded though. Error: %(error)s",
7620                           {'uri': self._uri(), 'error': e})
7621                 return
7622             else:
7623                 LOG.error(m, {'ret': e, 'u': u})
7624                 raise exception.MigrationPreCheckError(
7625                     reason=m % {'ret': e, 'u': u})
7626 
7627         if ret <= 0:
7628             LOG.error(m, {'ret': ret, 'u': u})
7629             raise exception.InvalidCPUInfo(reason=m % {'ret': ret, 'u': u})
7630 
7631     def _create_shared_storage_test_file(self, instance):
7632         """Makes tmpfile under CONF.instances_path."""
7633         dirpath = CONF.instances_path
7634         fd, tmp_file = tempfile.mkstemp(dir=dirpath)
7635         LOG.debug("Creating tmpfile %s to notify to other "
7636                   "compute nodes that they should mount "
7637                   "the same storage.", tmp_file, instance=instance)
7638         os.close(fd)
7639         return os.path.basename(tmp_file)
7640 
7641     def _check_shared_storage_test_file(self, filename, instance):
7642         """Confirms existence of the tmpfile under CONF.instances_path.
7643 
7644         Cannot confirm tmpfile return False.
7645         """
7646         # NOTE(tpatzig): if instances_path is a shared volume that is
7647         # under heavy IO (many instances on many compute nodes),
7648         # then checking the existence of the testfile fails,
7649         # just because it takes longer until the client refreshes and new
7650         # content gets visible.
7651         # os.utime (like touch) on the directory forces the client to refresh.
7652         os.utime(CONF.instances_path, None)
7653 
7654         tmp_file = os.path.join(CONF.instances_path, filename)
7655         if not os.path.exists(tmp_file):
7656             exists = False
7657         else:
7658             exists = True
7659         LOG.debug('Check if temp file %s exists to indicate shared storage '
7660                   'is being used for migration. Exists? %s', tmp_file, exists,
7661                   instance=instance)
7662         return exists
7663 
7664     def _cleanup_shared_storage_test_file(self, filename):
7665         """Removes existence of the tmpfile under CONF.instances_path."""
7666         tmp_file = os.path.join(CONF.instances_path, filename)
7667         os.remove(tmp_file)
7668 
7669     def ensure_filtering_rules_for_instance(self, instance, network_info):
7670         """Ensure that an instance's filtering rules are enabled.
7671 
7672         When migrating an instance, we need the filtering rules to
7673         be configured on the destination host before starting the
7674         migration.
7675 
7676         Also, when restarting the compute service, we need to ensure
7677         that filtering rules exist for all running services.
7678         """
7679 
7680         self.firewall_driver.setup_basic_filtering(instance, network_info)
7681         self.firewall_driver.prepare_instance_filter(instance,
7682                 network_info)
7683 
7684         # nwfilters may be defined in a separate thread in the case
7685         # of libvirt non-blocking mode, so we wait for completion
7686         timeout_count = list(range(CONF.live_migration_retry_count))
7687         while timeout_count:
7688             if self.firewall_driver.instance_filter_exists(instance,
7689                                                            network_info):
7690                 break
7691             timeout_count.pop()
7692             if len(timeout_count) == 0:
7693                 msg = _('The firewall filter for %s does not exist')
7694                 raise exception.InternalError(msg % instance.name)
7695             greenthread.sleep(1)
7696 
7697     def filter_defer_apply_on(self):
7698         self.firewall_driver.filter_defer_apply_on()
7699 
7700     def filter_defer_apply_off(self):
7701         self.firewall_driver.filter_defer_apply_off()
7702 
7703     def live_migration(self, context, instance, dest,
7704                        post_method, recover_method, block_migration=False,
7705                        migrate_data=None):
7706         """Spawning live_migration operation for distributing high-load.
7707 
7708         :param context: security context
7709         :param instance:
7710             nova.db.sqlalchemy.models.Instance object
7711             instance object that is migrated.
7712         :param dest: destination host
7713         :param post_method:
7714             post operation method.
7715             expected nova.compute.manager._post_live_migration.
7716         :param recover_method:
7717             recovery method when any exception occurs.
7718             expected nova.compute.manager._rollback_live_migration.
7719         :param block_migration: if true, do block migration.
7720         :param migrate_data: a LibvirtLiveMigrateData object
7721 
7722         """
7723 
7724         # 'dest' will be substituted into 'migration_uri' so ensure
7725         # it does't contain any characters that could be used to
7726         # exploit the URI accepted by libvirt
7727         if not libvirt_utils.is_valid_hostname(dest):
7728             raise exception.InvalidHostname(hostname=dest)
7729 
7730         self._live_migration(context, instance, dest,
7731                              post_method, recover_method, block_migration,
7732                              migrate_data)
7733 
7734     def live_migration_abort(self, instance):
7735         """Aborting a running live-migration.
7736 
7737         :param instance: instance object that is in migration
7738 
7739         """
7740 
7741         guest = self._host.get_guest(instance)
7742         dom = guest._domain
7743 
7744         try:
7745             dom.abortJob()
7746         except libvirt.libvirtError as e:
7747             LOG.error("Failed to cancel migration %s",
7748                     encodeutils.exception_to_unicode(e), instance=instance)
7749             raise
7750 
7751     def _verify_serial_console_is_disabled(self):
7752         if CONF.serial_console.enabled:
7753 
7754             msg = _('Your destination node does not support'
7755                     ' retrieving listen addresses. In order'
7756                     ' for live migration to work properly you'
7757                     ' must disable serial console.')
7758             raise exception.MigrationError(reason=msg)
7759 
7760     def _detach_direct_passthrough_vifs(self, context,
7761                                         migrate_data, instance):
7762         """detaches passthrough vif to enable live migration
7763 
7764         :param context: security context
7765         :param migrate_data: a LibvirtLiveMigrateData object
7766         :param instance: instance object that is migrated.
7767         """
7768         # NOTE(sean-k-mooney): if we have vif data available we
7769         # loop over each vif and detach all direct passthrough
7770         # vifs to allow sriov live migration.
7771         direct_vnics = network_model.VNIC_TYPES_DIRECT_PASSTHROUGH
7772         vifs = [vif.source_vif for vif in migrate_data.vifs
7773                 if "source_vif" in vif and vif.source_vif]
7774         for vif in vifs:
7775             if vif['vnic_type'] in direct_vnics:
7776                 LOG.info("Detaching vif %s from instnace "
7777                          "%s for live migration", vif['id'], instance.id)
7778                 self.detach_interface(context, instance, vif)
7779 
7780     def _live_migration_operation(self, context, instance, dest,
7781                                   block_migration, migrate_data, guest,
7782                                   device_names):
7783         """Invoke the live migration operation
7784 
7785         :param context: security context
7786         :param instance:
7787             nova.db.sqlalchemy.models.Instance object
7788             instance object that is migrated.
7789         :param dest: destination host
7790         :param block_migration: if true, do block migration.
7791         :param migrate_data: a LibvirtLiveMigrateData object
7792         :param guest: the guest domain object
7793         :param device_names: list of device names that are being migrated with
7794             instance
7795 
7796         This method is intended to be run in a background thread and will
7797         block that thread until the migration is finished or failed.
7798         """
7799         try:
7800             if migrate_data.block_migration:
7801                 migration_flags = self._block_migration_flags
7802             else:
7803                 migration_flags = self._live_migration_flags
7804 
7805             serial_listen_addr = libvirt_migrate.serial_listen_addr(
7806                 migrate_data)
7807             if not serial_listen_addr:
7808                 # In this context we want to ensure that serial console is
7809                 # disabled on source node. This is because nova couldn't
7810                 # retrieve serial listen address from destination node, so we
7811                 # consider that destination node might have serial console
7812                 # disabled as well.
7813                 self._verify_serial_console_is_disabled()
7814 
7815             # NOTE(aplanas) migrate_uri will have a value only in the
7816             # case that `live_migration_inbound_addr` parameter is
7817             # set, and we propose a non tunneled migration.
7818             migrate_uri = None
7819             if ('target_connect_addr' in migrate_data and
7820                     migrate_data.target_connect_addr is not None):
7821                 dest = migrate_data.target_connect_addr
7822                 if (migration_flags &
7823                     libvirt.VIR_MIGRATE_TUNNELLED == 0):
7824                     migrate_uri = self._migrate_uri(dest)
7825 
7826             new_xml_str = None
7827             if CONF.libvirt.virt_type != "parallels":
7828                 # If the migrate_data has port binding information for the
7829                 # destination host, we need to prepare the guest vif config
7830                 # for the destination before we start migrating the guest.
7831                 get_vif_config = None
7832                 if 'vifs' in migrate_data and migrate_data.vifs:
7833                     # NOTE(mriedem): The vif kwarg must be built on the fly
7834                     # within get_updated_guest_xml based on migrate_data.vifs.
7835                     # We could stash the virt_type from the destination host
7836                     # into LibvirtLiveMigrateData but the host kwarg is a
7837                     # nova.virt.libvirt.host.Host object and is used to check
7838                     # information like libvirt version on the destination.
7839                     # If this becomes a problem, what we could do is get the
7840                     # VIF configs while on the destination host during
7841                     # pre_live_migration() and store those in the
7842                     # LibvirtLiveMigrateData object. For now we just use the
7843                     # source host information for virt_type and
7844                     # host (version) since the conductor live_migrate method
7845                     # _check_compatible_with_source_hypervisor() ensures that
7846                     # the hypervisor types and versions are compatible.
7847                     get_vif_config = functools.partial(
7848                         self.vif_driver.get_config,
7849                         instance=instance,
7850                         image_meta=instance.image_meta,
7851                         inst_type=instance.flavor,
7852                         virt_type=CONF.libvirt.virt_type,
7853                         host=self._host)
7854                     self._detach_direct_passthrough_vifs(context,
7855                         migrate_data, instance)
7856                 new_xml_str = libvirt_migrate.get_updated_guest_xml(
7857                     # TODO(sahid): It's not a really good idea to pass
7858                     # the method _get_volume_config and we should to find
7859                     # a way to avoid this in future.
7860                     guest, migrate_data, self._get_volume_config,
7861                     get_vif_config=get_vif_config)
7862 
7863             # NOTE(pkoniszewski): Because of precheck which blocks
7864             # tunnelled block live migration with mapped volumes we
7865             # can safely remove migrate_disks when tunnelling is on.
7866             # Otherwise we will block all tunnelled block migrations,
7867             # even when an instance does not have volumes mapped.
7868             # This is because selective disk migration is not
7869             # supported in tunnelled block live migration. Also we
7870             # cannot fallback to migrateToURI2 in this case because of
7871             # bug #1398999
7872             #
7873             # TODO(kchamart) Move the following bit to guest.migrate()
7874             if (migration_flags & libvirt.VIR_MIGRATE_TUNNELLED != 0):
7875                 device_names = []
7876 
7877             # TODO(sahid): This should be in
7878             # post_live_migration_at_source but no way to retrieve
7879             # ports acquired on the host for the guest at this
7880             # step. Since the domain is going to be removed from
7881             # libvird on source host after migration, we backup the
7882             # serial ports to release them if all went well.
7883             serial_ports = []
7884             if CONF.serial_console.enabled:
7885                 serial_ports = list(self._get_serial_ports_from_guest(guest))
7886 
7887             LOG.debug("About to invoke the migrate API", instance=instance)
7888             guest.migrate(self._live_migration_uri(dest),
7889                           migrate_uri=migrate_uri,
7890                           flags=migration_flags,
7891                           migrate_disks=device_names,
7892                           destination_xml=new_xml_str,
7893                           bandwidth=CONF.libvirt.live_migration_bandwidth)
7894             LOG.debug("Migrate API has completed", instance=instance)
7895 
7896             for hostname, port in serial_ports:
7897                 serial_console.release_port(host=hostname, port=port)
7898         except Exception as e:
7899             with excutils.save_and_reraise_exception():
7900                 LOG.error("Live Migration failure: %s", e, instance=instance)
7901 
7902         # If 'migrateToURI' fails we don't know what state the
7903         # VM instances on each host are in. Possibilities include
7904         #
7905         #  1. src==running, dst==none
7906         #
7907         #     Migration failed & rolled back, or never started
7908         #
7909         #  2. src==running, dst==paused
7910         #
7911         #     Migration started but is still ongoing
7912         #
7913         #  3. src==paused,  dst==paused
7914         #
7915         #     Migration data transfer completed, but switchover
7916         #     is still ongoing, or failed
7917         #
7918         #  4. src==paused,  dst==running
7919         #
7920         #     Migration data transfer completed, switchover
7921         #     happened but cleanup on source failed
7922         #
7923         #  5. src==none,    dst==running
7924         #
7925         #     Migration fully succeeded.
7926         #
7927         # Libvirt will aim to complete any migration operation
7928         # or roll it back. So even if the migrateToURI call has
7929         # returned an error, if the migration was not finished
7930         # libvirt should clean up.
7931         #
7932         # So we take the error raise here with a pinch of salt
7933         # and rely on the domain job info status to figure out
7934         # what really happened to the VM, which is a much more
7935         # reliable indicator.
7936         #
7937         # In particular we need to try very hard to ensure that
7938         # Nova does not "forget" about the guest. ie leaving it
7939         # running on a different host to the one recorded in
7940         # the database, as that would be a serious resource leak
7941 
7942         LOG.debug("Migration operation thread has finished",
7943                   instance=instance)
7944 
7945     def _live_migration_copy_disk_paths(self, context, instance, guest):
7946         '''Get list of disks to copy during migration
7947 
7948         :param context: security context
7949         :param instance: the instance being migrated
7950         :param guest: the Guest instance being migrated
7951 
7952         Get the list of disks to copy during migration.
7953 
7954         :returns: a list of local source paths and a list of device names to
7955             copy
7956         '''
7957 
7958         disk_paths = []
7959         device_names = []
7960         block_devices = []
7961 
7962         if (self._block_migration_flags &
7963                 libvirt.VIR_MIGRATE_TUNNELLED == 0):
7964             bdm_list = objects.BlockDeviceMappingList.get_by_instance_uuid(
7965                 context, instance.uuid)
7966             block_device_info = driver.get_block_device_info(instance,
7967                                                              bdm_list)
7968 
7969             block_device_mappings = driver.block_device_info_get_mapping(
7970                 block_device_info)
7971             for bdm in block_device_mappings:
7972                 device_name = str(bdm['mount_device'].rsplit('/', 1)[1])
7973                 block_devices.append(device_name)
7974 
7975         for dev in guest.get_all_disks():
7976             if dev.readonly or dev.shareable:
7977                 continue
7978             if dev.source_type not in ["file", "block"]:
7979                 continue
7980             if dev.target_dev in block_devices:
7981                 continue
7982             disk_paths.append(dev.source_path)
7983             device_names.append(dev.target_dev)
7984         return (disk_paths, device_names)
7985 
7986     def _live_migration_data_gb(self, instance, disk_paths):
7987         '''Calculate total amount of data to be transferred
7988 
7989         :param instance: the nova.objects.Instance being migrated
7990         :param disk_paths: list of disk paths that are being migrated
7991         with instance
7992 
7993         Calculates the total amount of data that needs to be
7994         transferred during the live migration. The actual
7995         amount copied will be larger than this, due to the
7996         guest OS continuing to dirty RAM while the migration
7997         is taking place. So this value represents the minimal
7998         data size possible.
7999 
8000         :returns: data size to be copied in GB
8001         '''
8002 
8003         ram_gb = instance.flavor.memory_mb * units.Mi / units.Gi
8004         if ram_gb < 2:
8005             ram_gb = 2
8006 
8007         disk_gb = 0
8008         for path in disk_paths:
8009             try:
8010                 size = os.stat(path).st_size
8011                 size_gb = (size / units.Gi)
8012                 if size_gb < 2:
8013                     size_gb = 2
8014                 disk_gb += size_gb
8015             except OSError as e:
8016                 LOG.warning("Unable to stat %(disk)s: %(ex)s",
8017                             {'disk': path, 'ex': e})
8018                 # Ignore error since we don't want to break
8019                 # the migration monitoring thread operation
8020 
8021         return ram_gb + disk_gb
8022 
8023     def _get_migration_flags(self, is_block_migration):
8024         if is_block_migration:
8025             return self._block_migration_flags
8026         return self._live_migration_flags
8027 
8028     def _live_migration_monitor(self, context, instance, guest,
8029                                 dest, post_method,
8030                                 recover_method, block_migration,
8031                                 migrate_data, finish_event,
8032                                 disk_paths):
8033         on_migration_failure = deque()
8034         data_gb = self._live_migration_data_gb(instance, disk_paths)
8035         downtime_steps = list(libvirt_migrate.downtime_steps(data_gb))
8036         migration = migrate_data.migration
8037         curdowntime = None
8038 
8039         migration_flags = self._get_migration_flags(
8040                                   migrate_data.block_migration)
8041 
8042         n = 0
8043         start = time.time()
8044         is_post_copy_enabled = self._is_post_copy_enabled(migration_flags)
8045         while True:
8046             info = guest.get_job_info()
8047 
8048             if info.type == libvirt.VIR_DOMAIN_JOB_NONE:
8049                 # Either still running, or failed or completed,
8050                 # lets untangle the mess
8051                 if not finish_event.ready():
8052                     LOG.debug("Operation thread is still running",
8053                               instance=instance)
8054                 else:
8055                     info.type = libvirt_migrate.find_job_type(guest, instance)
8056                     LOG.debug("Fixed incorrect job type to be %d",
8057                               info.type, instance=instance)
8058 
8059             if info.type == libvirt.VIR_DOMAIN_JOB_NONE:
8060                 # Migration is not yet started
8061                 LOG.debug("Migration not running yet",
8062                           instance=instance)
8063             elif info.type == libvirt.VIR_DOMAIN_JOB_UNBOUNDED:
8064                 # Migration is still running
8065                 #
8066                 # This is where we wire up calls to change live
8067                 # migration status. eg change max downtime, cancel
8068                 # the operation, change max bandwidth
8069                 libvirt_migrate.run_tasks(guest, instance,
8070                                           self.active_migrations,
8071                                           on_migration_failure,
8072                                           migration,
8073                                           is_post_copy_enabled)
8074 
8075                 now = time.time()
8076                 elapsed = now - start
8077 
8078                 completion_timeout = int(
8079                     CONF.libvirt.live_migration_completion_timeout * data_gb)
8080                 # NOTE(yikun): Check the completion timeout to determine
8081                 # should trigger the timeout action, and there are two choices
8082                 # ``abort`` (default) or ``force_complete``. If the action is
8083                 # set to ``force_complete``, the post-copy will be triggered
8084                 # if available else the VM will be suspended, otherwise the
8085                 # live migrate operation will be aborted.
8086                 if libvirt_migrate.should_trigger_timeout_action(
8087                         instance, elapsed, completion_timeout,
8088                         migration.status):
8089                     timeout_act = CONF.libvirt.live_migration_timeout_action
8090                     if timeout_act == 'force_complete':
8091                         self.live_migration_force_complete(instance)
8092                     else:
8093                         # timeout action is 'abort'
8094                         try:
8095                             guest.abort_job()
8096                         except libvirt.libvirtError as e:
8097                             LOG.warning("Failed to abort migration %s",
8098                                     encodeutils.exception_to_unicode(e),
8099                                     instance=instance)
8100                             self._clear_empty_migration(instance)
8101                             raise
8102 
8103                 curdowntime = libvirt_migrate.update_downtime(
8104                     guest, instance, curdowntime,
8105                     downtime_steps, elapsed)
8106 
8107                 # We loop every 500ms, so don't log on every
8108                 # iteration to avoid spamming logs for long
8109                 # running migrations. Just once every 5 secs
8110                 # is sufficient for developers to debug problems.
8111                 # We log once every 30 seconds at info to help
8112                 # admins see slow running migration operations
8113                 # when debug logs are off.
8114                 if (n % 10) == 0:
8115                     # Ignoring memory_processed, as due to repeated
8116                     # dirtying of data, this can be way larger than
8117                     # memory_total. Best to just look at what's
8118                     # remaining to copy and ignore what's done already
8119                     #
8120                     # TODO(berrange) perhaps we could include disk
8121                     # transfer stats in the progress too, but it
8122                     # might make memory info more obscure as large
8123                     # disk sizes might dwarf memory size
8124                     remaining = 100
8125                     if info.memory_total != 0:
8126                         remaining = round(info.memory_remaining *
8127                                           100 / info.memory_total)
8128 
8129                     libvirt_migrate.save_stats(instance, migration,
8130                                                info, remaining)
8131 
8132                     # NOTE(fanzhang): do not include disk transfer stats in
8133                     # the progress percentage calculation but log them.
8134                     disk_remaining = 100
8135                     if info.disk_total != 0:
8136                         disk_remaining = round(info.disk_remaining *
8137                                                100 / info.disk_total)
8138 
8139                     lg = LOG.debug
8140                     if (n % 60) == 0:
8141                         lg = LOG.info
8142 
8143                     lg("Migration running for %(secs)d secs, "
8144                        "memory %(remaining)d%% remaining "
8145                        "(bytes processed=%(processed_memory)d, "
8146                        "remaining=%(remaining_memory)d, "
8147                        "total=%(total_memory)d); "
8148                        "disk %(disk_remaining)d%% remaining "
8149                        "(bytes processed=%(processed_disk)d, "
8150                        "remaining=%(remaining_disk)d, "
8151                        "total=%(total_disk)d).",
8152                        {"secs": n / 2, "remaining": remaining,
8153                         "processed_memory": info.memory_processed,
8154                         "remaining_memory": info.memory_remaining,
8155                         "total_memory": info.memory_total,
8156                         "disk_remaining": disk_remaining,
8157                         "processed_disk": info.disk_processed,
8158                         "remaining_disk": info.disk_remaining,
8159                         "total_disk": info.disk_total}, instance=instance)
8160 
8161                 n = n + 1
8162             elif info.type == libvirt.VIR_DOMAIN_JOB_COMPLETED:
8163                 # Migration is all done
8164                 LOG.info("Migration operation has completed",
8165                          instance=instance)
8166                 post_method(context, instance, dest, block_migration,
8167                             migrate_data)
8168                 break
8169             elif info.type == libvirt.VIR_DOMAIN_JOB_FAILED:
8170                 # Migration did not succeed
8171                 LOG.error("Migration operation has aborted", instance=instance)
8172                 libvirt_migrate.run_recover_tasks(self._host, guest, instance,
8173                                                   on_migration_failure)
8174                 recover_method(context, instance, dest, migrate_data)
8175                 break
8176             elif info.type == libvirt.VIR_DOMAIN_JOB_CANCELLED:
8177                 # Migration was stopped by admin
8178                 LOG.warning("Migration operation was cancelled",
8179                             instance=instance)
8180                 libvirt_migrate.run_recover_tasks(self._host, guest, instance,
8181                                                   on_migration_failure)
8182                 recover_method(context, instance, dest, migrate_data,
8183                                migration_status='cancelled')
8184                 break
8185             else:
8186                 LOG.warning("Unexpected migration job type: %d",
8187                             info.type, instance=instance)
8188 
8189             time.sleep(0.5)
8190         self._clear_empty_migration(instance)
8191 
8192     def _clear_empty_migration(self, instance):
8193         try:
8194             del self.active_migrations[instance.uuid]
8195         except KeyError:
8196             LOG.warning("There are no records in active migrations "
8197                         "for instance", instance=instance)
8198 
8199     def _live_migration(self, context, instance, dest, post_method,
8200                         recover_method, block_migration,
8201                         migrate_data):
8202         """Do live migration.
8203 
8204         :param context: security context
8205         :param instance:
8206             nova.db.sqlalchemy.models.Instance object
8207             instance object that is migrated.
8208         :param dest: destination host
8209         :param post_method:
8210             post operation method.
8211             expected nova.compute.manager._post_live_migration.
8212         :param recover_method:
8213             recovery method when any exception occurs.
8214             expected nova.compute.manager._rollback_live_migration.
8215         :param block_migration: if true, do block migration.
8216         :param migrate_data: a LibvirtLiveMigrateData object
8217 
8218         This fires off a new thread to run the blocking migration
8219         operation, and then this thread monitors the progress of
8220         migration and controls its operation
8221         """
8222 
8223         guest = self._host.get_guest(instance)
8224 
8225         disk_paths = []
8226         device_names = []
8227         if (migrate_data.block_migration and
8228                 CONF.libvirt.virt_type != "parallels"):
8229             disk_paths, device_names = self._live_migration_copy_disk_paths(
8230                 context, instance, guest)
8231 
8232         opthread = utils.spawn(self._live_migration_operation,
8233                                      context, instance, dest,
8234                                      block_migration,
8235                                      migrate_data, guest,
8236                                      device_names)
8237 
8238         finish_event = eventlet.event.Event()
8239         self.active_migrations[instance.uuid] = deque()
8240 
8241         def thread_finished(thread, event):
8242             LOG.debug("Migration operation thread notification",
8243                       instance=instance)
8244             event.send()
8245         opthread.link(thread_finished, finish_event)
8246 
8247         # Let eventlet schedule the new thread right away
8248         time.sleep(0)
8249 
8250         try:
8251             LOG.debug("Starting monitoring of live migration",
8252                       instance=instance)
8253             self._live_migration_monitor(context, instance, guest, dest,
8254                                          post_method, recover_method,
8255                                          block_migration, migrate_data,
8256                                          finish_event, disk_paths)
8257         except Exception as ex:
8258             LOG.warning("Error monitoring migration: %(ex)s",
8259                         {"ex": ex}, instance=instance, exc_info=True)
8260             raise
8261         finally:
8262             LOG.debug("Live migration monitoring is all done",
8263                       instance=instance)
8264 
8265     def _is_post_copy_enabled(self, migration_flags):
8266         return (migration_flags & libvirt.VIR_MIGRATE_POSTCOPY) != 0
8267 
8268     def live_migration_force_complete(self, instance):
8269         try:
8270             self.active_migrations[instance.uuid].append('force-complete')
8271         except KeyError:
8272             raise exception.NoActiveMigrationForInstance(
8273                 instance_id=instance.uuid)
8274 
8275     def _try_fetch_image(self, context, path, image_id, instance,
8276                          fallback_from_host=None):
8277         try:
8278             libvirt_utils.fetch_image(context, path, image_id,
8279                                       instance.trusted_certs)
8280         except exception.ImageNotFound:
8281             if not fallback_from_host:
8282                 raise
8283             LOG.debug("Image %(image_id)s doesn't exist anymore on "
8284                       "image service, attempting to copy image "
8285                       "from %(host)s",
8286                       {'image_id': image_id, 'host': fallback_from_host})
8287             libvirt_utils.copy_image(src=path, dest=path,
8288                                      host=fallback_from_host,
8289                                      receive=True)
8290 
8291     def _fetch_instance_kernel_ramdisk(self, context, instance,
8292                                        fallback_from_host=None):
8293         """Download kernel and ramdisk for instance in instance directory."""
8294         instance_dir = libvirt_utils.get_instance_path(instance)
8295         if instance.kernel_id:
8296             kernel_path = os.path.join(instance_dir, 'kernel')
8297             # NOTE(dsanders): only fetch image if it's not available at
8298             # kernel_path. This also avoids ImageNotFound exception if
8299             # the image has been deleted from glance
8300             if not os.path.exists(kernel_path):
8301                 self._try_fetch_image(context,
8302                                       kernel_path,
8303                                       instance.kernel_id,
8304                                       instance, fallback_from_host)
8305             if instance.ramdisk_id:
8306                 ramdisk_path = os.path.join(instance_dir, 'ramdisk')
8307                 # NOTE(dsanders): only fetch image if it's not available at
8308                 # ramdisk_path. This also avoids ImageNotFound exception if
8309                 # the image has been deleted from glance
8310                 if not os.path.exists(ramdisk_path):
8311                     self._try_fetch_image(context,
8312                                           ramdisk_path,
8313                                           instance.ramdisk_id,
8314                                           instance, fallback_from_host)
8315 
8316     def _reattach_instance_vifs(self, context, instance, network_info):
8317         guest = self._host.get_guest(instance)
8318         # validate that the guest has the expected number of interfaces
8319         # attached.
8320         guest_interfaces = guest.get_interfaces()
8321         # NOTE(sean-k-mooney): In general len(guest_interfaces) will
8322         # be equal to len(network_info) as interfaces will not be hot unplugged
8323         # unless they are SR-IOV direct mode interfaces. As such we do not
8324         # need an else block here as it would be a noop.
8325         if len(guest_interfaces) < len(network_info):
8326             # NOTE(sean-k-mooney): we are doing a post live migration
8327             # for a guest with sriov vif that were detached as part of
8328             # the migration. loop over the vifs and attach the missing
8329             # vif as part of the post live migration phase.
8330             direct_vnics = network_model.VNIC_TYPES_DIRECT_PASSTHROUGH
8331             for vif in network_info:
8332                 if vif['vnic_type'] in direct_vnics:
8333                     LOG.info("Attaching vif %s to instance %s",
8334                              vif['id'], instance.id)
8335                     self.attach_interface(context, instance,
8336                                           instance.image_meta, vif)
8337 
8338     def rollback_live_migration_at_source(self, context, instance,
8339                                           migrate_data):
8340         """reconnect sriov interfaces after failed live migration
8341         :param context: security context
8342         :param instance:  the instance being migrated
8343         :param migrate_date: a LibvirtLiveMigrateData object
8344         """
8345         network_info = network_model.NetworkInfo(
8346             [vif.source_vif for vif in migrate_data.vifs
8347                             if "source_vif" in vif and vif.source_vif])
8348         self._reattach_instance_vifs(context, instance, network_info)
8349 
8350     def rollback_live_migration_at_destination(self, context, instance,
8351                                                network_info,
8352                                                block_device_info,
8353                                                destroy_disks=True,
8354                                                migrate_data=None):
8355         """Clean up destination node after a failed live migration."""
8356         try:
8357             self.destroy(context, instance, network_info, block_device_info,
8358                          destroy_disks)
8359         finally:
8360             # NOTE(gcb): Failed block live migration may leave instance
8361             # directory at destination node, ensure it is always deleted.
8362             is_shared_instance_path = True
8363             if migrate_data:
8364                 is_shared_instance_path = migrate_data.is_shared_instance_path
8365                 if (migrate_data.obj_attr_is_set("serial_listen_ports") and
8366                         migrate_data.serial_listen_ports):
8367                     # Releases serial ports reserved.
8368                     for port in migrate_data.serial_listen_ports:
8369                         serial_console.release_port(
8370                             host=migrate_data.serial_listen_addr, port=port)
8371 
8372             if not is_shared_instance_path:
8373                 instance_dir = libvirt_utils.get_instance_path_at_destination(
8374                     instance, migrate_data)
8375                 if os.path.exists(instance_dir):
8376                     shutil.rmtree(instance_dir)
8377 
8378     def _pre_live_migration_plug_vifs(self, instance, network_info,
8379                                       migrate_data):
8380         if 'vifs' in migrate_data and migrate_data.vifs:
8381             LOG.debug('Plugging VIFs using destination host port bindings '
8382                       'before live migration.', instance=instance)
8383             vif_plug_nw_info = network_model.NetworkInfo([])
8384             for migrate_vif in migrate_data.vifs:
8385                 vif_plug_nw_info.append(migrate_vif.get_dest_vif())
8386         else:
8387             LOG.debug('Plugging VIFs before live migration.',
8388                       instance=instance)
8389             vif_plug_nw_info = network_info
8390         # Retry operation is necessary because continuous live migration
8391         # requests to the same host cause concurrent requests to iptables,
8392         # then it complains.
8393         max_retry = CONF.live_migration_retry_count
8394         for cnt in range(max_retry):
8395             try:
8396                 self.plug_vifs(instance, vif_plug_nw_info)
8397                 break
8398             except processutils.ProcessExecutionError:
8399                 if cnt == max_retry - 1:
8400                     raise
8401                 else:
8402                     LOG.warning('plug_vifs() failed %(cnt)d. Retry up to '
8403                                 '%(max_retry)d.',
8404                                 {'cnt': cnt, 'max_retry': max_retry},
8405                                 instance=instance)
8406                     greenthread.sleep(1)
8407 
8408     def pre_live_migration(self, context, instance, block_device_info,
8409                            network_info, disk_info, migrate_data):
8410         """Preparation live migration."""
8411         if disk_info is not None:
8412             disk_info = jsonutils.loads(disk_info)
8413 
8414         LOG.debug('migrate_data in pre_live_migration: %s', migrate_data,
8415                   instance=instance)
8416         is_shared_block_storage = migrate_data.is_shared_block_storage
8417         is_shared_instance_path = migrate_data.is_shared_instance_path
8418         is_block_migration = migrate_data.block_migration
8419 
8420         if not is_shared_instance_path:
8421             instance_dir = libvirt_utils.get_instance_path_at_destination(
8422                             instance, migrate_data)
8423 
8424             if os.path.exists(instance_dir):
8425                 raise exception.DestinationDiskExists(path=instance_dir)
8426 
8427             LOG.debug('Creating instance directory: %s', instance_dir,
8428                       instance=instance)
8429             os.mkdir(instance_dir)
8430 
8431             # Recreate the disk.info file and in doing so stop the
8432             # imagebackend from recreating it incorrectly by inspecting the
8433             # contents of each file when using the Raw backend.
8434             if disk_info:
8435                 image_disk_info = {}
8436                 for info in disk_info:
8437                     image_file = os.path.basename(info['path'])
8438                     image_path = os.path.join(instance_dir, image_file)
8439                     image_disk_info[image_path] = info['type']
8440 
8441                 LOG.debug('Creating disk.info with the contents: %s',
8442                           image_disk_info, instance=instance)
8443 
8444                 image_disk_info_path = os.path.join(instance_dir,
8445                                                     'disk.info')
8446                 libvirt_utils.write_to_file(image_disk_info_path,
8447                                             jsonutils.dumps(image_disk_info))
8448 
8449             if not is_shared_block_storage:
8450                 # Ensure images and backing files are present.
8451                 LOG.debug('Checking to make sure images and backing files are '
8452                           'present before live migration.', instance=instance)
8453                 self._create_images_and_backing(
8454                     context, instance, instance_dir, disk_info,
8455                     fallback_from_host=instance.host)
8456                 if (configdrive.required_by(instance) and
8457                         CONF.config_drive_format == 'iso9660'):
8458                     # NOTE(pkoniszewski): Due to a bug in libvirt iso config
8459                     # drive needs to be copied to destination prior to
8460                     # migration when instance path is not shared and block
8461                     # storage is not shared. Files that are already present
8462                     # on destination are excluded from a list of files that
8463                     # need to be copied to destination. If we don't do that
8464                     # live migration will fail on copying iso config drive to
8465                     # destination and writing to read-only device.
8466                     # Please see bug/1246201 for more details.
8467                     src = "%s:%s/disk.config" % (instance.host, instance_dir)
8468                     self._remotefs.copy_file(src, instance_dir)
8469 
8470             if not is_block_migration:
8471                 # NOTE(angdraug): when block storage is shared between source
8472                 # and destination and instance path isn't (e.g. volume backed
8473                 # or rbd backed instance), instance path on destination has to
8474                 # be prepared
8475 
8476                 # Required by Quobyte CI
8477                 self._ensure_console_log_for_instance(instance)
8478 
8479                 # if image has kernel and ramdisk, just download
8480                 # following normal way.
8481                 self._fetch_instance_kernel_ramdisk(context, instance)
8482 
8483         # Establishing connection to volume server.
8484         block_device_mapping = driver.block_device_info_get_mapping(
8485             block_device_info)
8486 
8487         if len(block_device_mapping):
8488             LOG.debug('Connecting volumes before live migration.',
8489                       instance=instance)
8490 
8491         for bdm in block_device_mapping:
8492             connection_info = bdm['connection_info']
8493             # NOTE(lyarwood): Handle the P to Q LM during upgrade use case
8494             # where an instance has encrypted volumes attached using the
8495             # os-brick encryptors. Do not attempt to attach the encrypted
8496             # volume using native LUKS decryption on the destionation.
8497             src_native_luks = False
8498             if migrate_data.obj_attr_is_set('src_supports_native_luks'):
8499                 src_native_luks = migrate_data.src_supports_native_luks
8500             self._connect_volume(context, connection_info, instance,
8501                                  allow_native_luks=src_native_luks)
8502 
8503         self._pre_live_migration_plug_vifs(
8504             instance, network_info, migrate_data)
8505 
8506         # Store server_listen and latest disk device info
8507         if not migrate_data:
8508             migrate_data = objects.LibvirtLiveMigrateData(bdms=[])
8509         else:
8510             migrate_data.bdms = []
8511         # Store live_migration_inbound_addr
8512         migrate_data.target_connect_addr = \
8513             CONF.libvirt.live_migration_inbound_addr
8514         migrate_data.supported_perf_events = self._supported_perf_events
8515 
8516         migrate_data.serial_listen_ports = []
8517         if CONF.serial_console.enabled:
8518             num_ports = hardware.get_number_of_serial_ports(
8519                 instance.flavor, instance.image_meta)
8520             for port in six.moves.range(num_ports):
8521                 migrate_data.serial_listen_ports.append(
8522                     serial_console.acquire_port(
8523                         migrate_data.serial_listen_addr))
8524 
8525         for vol in block_device_mapping:
8526             connection_info = vol['connection_info']
8527             if connection_info.get('serial'):
8528                 disk_info = blockinfo.get_info_from_bdm(
8529                     instance, CONF.libvirt.virt_type,
8530                     instance.image_meta, vol)
8531 
8532                 bdmi = objects.LibvirtLiveMigrateBDMInfo()
8533                 bdmi.serial = connection_info['serial']
8534                 bdmi.connection_info = connection_info
8535                 bdmi.bus = disk_info['bus']
8536                 bdmi.dev = disk_info['dev']
8537                 bdmi.type = disk_info['type']
8538                 bdmi.format = disk_info.get('format')
8539                 bdmi.boot_index = disk_info.get('boot_index')
8540                 volume_secret = self._host.find_secret('volume', vol.volume_id)
8541                 if volume_secret:
8542                     bdmi.encryption_secret_uuid = volume_secret.UUIDString()
8543 
8544                 migrate_data.bdms.append(bdmi)
8545 
8546         return migrate_data
8547 
8548     def _try_fetch_image_cache(self, image, fetch_func, context, filename,
8549                                image_id, instance, size,
8550                                fallback_from_host=None):
8551         try:
8552             image.cache(fetch_func=fetch_func,
8553                         context=context,
8554                         filename=filename,
8555                         image_id=image_id,
8556                         size=size,
8557                         trusted_certs=instance.trusted_certs)
8558         except exception.ImageNotFound:
8559             if not fallback_from_host:
8560                 raise
8561             LOG.debug("Image %(image_id)s doesn't exist anymore "
8562                       "on image service, attempting to copy "
8563                       "image from %(host)s",
8564                       {'image_id': image_id, 'host': fallback_from_host},
8565                       instance=instance)
8566 
8567             def copy_from_host(target):
8568                 libvirt_utils.copy_image(src=target,
8569                                          dest=target,
8570                                          host=fallback_from_host,
8571                                          receive=True)
8572             image.cache(fetch_func=copy_from_host, size=size,
8573                         filename=filename)
8574 
8575         # NOTE(lyarwood): If the instance vm_state is shelved offloaded then we
8576         # must be unshelving for _try_fetch_image_cache to be called.
8577         if instance.vm_state == vm_states.SHELVED_OFFLOADED:
8578             # NOTE(lyarwood): When using the rbd imagebackend the call to cache
8579             # above will attempt to clone from the shelved snapshot in Glance
8580             # if available from this compute. We then need to flatten the
8581             # resulting image to avoid it still referencing and ultimately
8582             # blocking the removal of the shelved snapshot at the end of the
8583             # unshelve. This is a no-op for all but the rbd imagebackend.
8584             try:
8585                 image.flatten()
8586                 LOG.debug('Image %s flattened successfully while unshelving '
8587                           'instance.', image.path, instance=instance)
8588             except NotImplementedError:
8589                 # NOTE(lyarwood): There's an argument to be made for logging
8590                 # our inability to call flatten here, however given this isn't
8591                 # implemented for most of the backends it may do more harm than
8592                 # good, concerning operators etc so for now just pass.
8593                 pass
8594 
8595     def _create_images_and_backing(self, context, instance, instance_dir,
8596                                    disk_info, fallback_from_host=None):
8597         """:param context: security context
8598            :param instance:
8599                nova.db.sqlalchemy.models.Instance object
8600                instance object that is migrated.
8601            :param instance_dir:
8602                instance path to use, calculated externally to handle block
8603                migrating an instance with an old style instance path
8604            :param disk_info:
8605                disk info specified in _get_instance_disk_info_from_config
8606                (list of dicts)
8607            :param fallback_from_host:
8608                host where we can retrieve images if the glance images are
8609                not available.
8610 
8611         """
8612 
8613         # Virtuozzo containers don't use backing file
8614         if (CONF.libvirt.virt_type == "parallels" and
8615                 instance.vm_mode == fields.VMMode.EXE):
8616             return
8617 
8618         if not disk_info:
8619             disk_info = []
8620 
8621         for info in disk_info:
8622             base = os.path.basename(info['path'])
8623             # Get image type and create empty disk image, and
8624             # create backing file in case of qcow2.
8625             instance_disk = os.path.join(instance_dir, base)
8626             if not info['backing_file'] and not os.path.exists(instance_disk):
8627                 libvirt_utils.create_image(info['type'], instance_disk,
8628                                            info['virt_disk_size'])
8629             elif info['backing_file']:
8630                 # Creating backing file follows same way as spawning instances.
8631                 cache_name = os.path.basename(info['backing_file'])
8632 
8633                 disk = self.image_backend.by_name(instance, instance_disk,
8634                                                   CONF.libvirt.images_type)
8635                 if cache_name.startswith('ephemeral'):
8636                     # The argument 'size' is used by image.cache to
8637                     # validate disk size retrieved from cache against
8638                     # the instance disk size (should always return OK)
8639                     # and ephemeral_size is used by _create_ephemeral
8640                     # to build the image if the disk is not already
8641                     # cached.
8642                     disk.cache(
8643                         fetch_func=self._create_ephemeral,
8644                         fs_label=cache_name,
8645                         os_type=instance.os_type,
8646                         filename=cache_name,
8647                         size=info['virt_disk_size'],
8648                         ephemeral_size=info['virt_disk_size'] / units.Gi)
8649                 elif cache_name.startswith('swap'):
8650                     inst_type = instance.get_flavor()
8651                     swap_mb = inst_type.swap
8652                     disk.cache(fetch_func=self._create_swap,
8653                                 filename="swap_%s" % swap_mb,
8654                                 size=swap_mb * units.Mi,
8655                                 swap_mb=swap_mb)
8656                 else:
8657                     self._try_fetch_image_cache(disk,
8658                                                 libvirt_utils.fetch_image,
8659                                                 context, cache_name,
8660                                                 instance.image_ref,
8661                                                 instance,
8662                                                 info['virt_disk_size'],
8663                                                 fallback_from_host)
8664 
8665         # if disk has kernel and ramdisk, just download
8666         # following normal way.
8667         self._fetch_instance_kernel_ramdisk(
8668             context, instance, fallback_from_host=fallback_from_host)
8669 
8670     def post_live_migration(self, context, instance, block_device_info,
8671                             migrate_data=None):
8672         # Disconnect from volume server
8673         block_device_mapping = driver.block_device_info_get_mapping(
8674                 block_device_info)
8675         for vol in block_device_mapping:
8676             # NOTE(mdbooth): The block_device_info we were passed was
8677             # initialized with BDMs from the source host before they were
8678             # updated to point to the destination. We can safely use this to
8679             # disconnect the source without re-fetching.
8680             self._disconnect_volume(context, vol['connection_info'], instance)
8681 
8682     def post_live_migration_at_source(self, context, instance, network_info):
8683         """Unplug VIFs from networks at source.
8684 
8685         :param context: security context
8686         :param instance: instance object reference
8687         :param network_info: instance network information
8688         """
8689         self.unplug_vifs(instance, network_info)
8690 
8691     def post_live_migration_at_destination(self, context,
8692                                            instance,
8693                                            network_info,
8694                                            block_migration=False,
8695                                            block_device_info=None):
8696         """Post operation of live migration at destination host.
8697 
8698         :param context: security context
8699         :param instance:
8700             nova.db.sqlalchemy.models.Instance object
8701             instance object that is migrated.
8702         :param network_info: instance network information
8703         :param block_migration: if true, post operation of block_migration.
8704         """
8705         self._reattach_instance_vifs(context, instance, network_info)
8706 
8707     def _get_instance_disk_info_from_config(self, guest_config,
8708                                             block_device_info):
8709         """Get the non-volume disk information from the domain xml
8710 
8711         :param LibvirtConfigGuest guest_config: the libvirt domain config
8712                                                 for the instance
8713         :param dict block_device_info: block device info for BDMs
8714         :returns disk_info: list of dicts with keys:
8715 
8716           * 'type': the disk type (str)
8717           * 'path': the disk path (str)
8718           * 'virt_disk_size': the virtual disk size (int)
8719           * 'backing_file': backing file of a disk image (str)
8720           * 'disk_size': physical disk size (int)
8721           * 'over_committed_disk_size': virt_disk_size - disk_size or 0
8722         """
8723         block_device_mapping = driver.block_device_info_get_mapping(
8724             block_device_info)
8725 
8726         volume_devices = set()
8727         for vol in block_device_mapping:
8728             disk_dev = vol['mount_device'].rpartition("/")[2]
8729             volume_devices.add(disk_dev)
8730 
8731         disk_info = []
8732 
8733         if (guest_config.virt_type == 'parallels' and
8734                 guest_config.os_type == fields.VMMode.EXE):
8735             node_type = 'filesystem'
8736         else:
8737             node_type = 'disk'
8738 
8739         for device in guest_config.devices:
8740             if device.root_name != node_type:
8741                 continue
8742             disk_type = device.source_type
8743             if device.root_name == 'filesystem':
8744                 target = device.target_dir
8745                 if device.source_type == 'file':
8746                     path = device.source_file
8747                 elif device.source_type == 'block':
8748                     path = device.source_dev
8749                 else:
8750                     path = None
8751             else:
8752                 target = device.target_dev
8753                 path = device.source_path
8754 
8755             if not path:
8756                 LOG.debug('skipping disk for %s as it does not have a path',
8757                           guest_config.name)
8758                 continue
8759 
8760             if disk_type not in ['file', 'block']:
8761                 LOG.debug('skipping disk because it looks like a volume', path)
8762                 continue
8763 
8764             if target in volume_devices:
8765                 LOG.debug('skipping disk %(path)s (%(target)s) as it is a '
8766                           'volume', {'path': path, 'target': target})
8767                 continue
8768 
8769             if device.root_name == 'filesystem':
8770                 driver_type = device.driver_type
8771             else:
8772                 driver_type = device.driver_format
8773             # get the real disk size or
8774             # raise a localized error if image is unavailable
8775             if disk_type == 'file' and driver_type == 'ploop':
8776                 dk_size = 0
8777                 for dirpath, dirnames, filenames in os.walk(path):
8778                     for f in filenames:
8779                         fp = os.path.join(dirpath, f)
8780                         dk_size += os.path.getsize(fp)
8781                 qemu_img_info = disk_api.get_disk_info(path)
8782                 virt_size = qemu_img_info.virtual_size
8783                 backing_file = libvirt_utils.get_disk_backing_file(path)
8784                 over_commit_size = int(virt_size) - dk_size
8785 
8786             elif disk_type == 'file' and driver_type == 'qcow2':
8787                 qemu_img_info = disk_api.get_disk_info(path)
8788                 dk_size = qemu_img_info.disk_size
8789                 virt_size = qemu_img_info.virtual_size
8790                 backing_file = libvirt_utils.get_disk_backing_file(path)
8791                 over_commit_size = int(virt_size) - dk_size
8792 
8793             elif disk_type == 'file':
8794                 dk_size = os.stat(path).st_blocks * 512
8795                 virt_size = os.path.getsize(path)
8796                 backing_file = ""
8797                 over_commit_size = 0
8798 
8799             elif disk_type == 'block' and block_device_info:
8800                 dk_size = lvm.get_volume_size(path)
8801                 virt_size = dk_size
8802                 backing_file = ""
8803                 over_commit_size = 0
8804 
8805             else:
8806                 LOG.debug('skipping disk %(path)s (%(target)s) - unable to '
8807                           'determine if volume',
8808                           {'path': path, 'target': target})
8809                 continue
8810 
8811             disk_info.append({'type': driver_type,
8812                               'path': path,
8813                               'virt_disk_size': virt_size,
8814                               'backing_file': backing_file,
8815                               'disk_size': dk_size,
8816                               'over_committed_disk_size': over_commit_size})
8817         return disk_info
8818 
8819     def _get_instance_disk_info(self, instance, block_device_info):
8820         try:
8821             guest = self._host.get_guest(instance)
8822             config = guest.get_config()
8823         except libvirt.libvirtError as ex:
8824             error_code = ex.get_error_code()
8825             LOG.warning('Error from libvirt while getting description of '
8826                         '%(instance_name)s: [Error Code %(error_code)s] '
8827                         '%(ex)s',
8828                         {'instance_name': instance.name,
8829                          'error_code': error_code,
8830                          'ex': encodeutils.exception_to_unicode(ex)},
8831                         instance=instance)
8832             raise exception.InstanceNotFound(instance_id=instance.uuid)
8833 
8834         return self._get_instance_disk_info_from_config(config,
8835                                                         block_device_info)
8836 
8837     def get_instance_disk_info(self, instance,
8838                                block_device_info=None):
8839         return jsonutils.dumps(
8840             self._get_instance_disk_info(instance, block_device_info))
8841 
8842     def _get_disk_over_committed_size_total(self):
8843         """Return total over committed disk size for all instances."""
8844         # Disk size that all instance uses : virtual_size - disk_size
8845         disk_over_committed_size = 0
8846         instance_domains = self._host.list_instance_domains(only_running=False)
8847         if not instance_domains:
8848             return disk_over_committed_size
8849 
8850         # Get all instance uuids
8851         instance_uuids = [dom.UUIDString() for dom in instance_domains]
8852         ctx = nova_context.get_admin_context()
8853         # Get instance object list by uuid filter
8854         filters = {'uuid': instance_uuids}
8855         # NOTE(ankit): objects.InstanceList.get_by_filters method is
8856         # getting called twice one is here and another in the
8857         # _update_available_resource method of resource_tracker. Since
8858         # _update_available_resource method is synchronized, there is a
8859         # possibility the instances list retrieved here to calculate
8860         # disk_over_committed_size would differ to the list you would get
8861         # in _update_available_resource method for calculating usages based
8862         # on instance utilization.
8863         local_instance_list = objects.InstanceList.get_by_filters(
8864             ctx, filters, use_slave=True)
8865         # Convert instance list to dictionary with instance uuid as key.
8866         local_instances = {inst.uuid: inst for inst in local_instance_list}
8867 
8868         # Get bdms by instance uuids
8869         bdms = objects.BlockDeviceMappingList.bdms_by_instance_uuid(
8870             ctx, instance_uuids)
8871 
8872         for dom in instance_domains:
8873             try:
8874                 guest = libvirt_guest.Guest(dom)
8875                 config = guest.get_config()
8876 
8877                 block_device_info = None
8878                 if guest.uuid in local_instances \
8879                         and (bdms and guest.uuid in bdms):
8880                     # Get block device info for instance
8881                     block_device_info = driver.get_block_device_info(
8882                         local_instances[guest.uuid], bdms[guest.uuid])
8883 
8884                 disk_infos = self._get_instance_disk_info_from_config(
8885                     config, block_device_info)
8886                 if not disk_infos:
8887                     continue
8888 
8889                 for info in disk_infos:
8890                     disk_over_committed_size += int(
8891                         info['over_committed_disk_size'])
8892             except libvirt.libvirtError as ex:
8893                 error_code = ex.get_error_code()
8894                 LOG.warning(
8895                     'Error from libvirt while getting description of '
8896                     '%(instance_name)s: [Error Code %(error_code)s] %(ex)s',
8897                     {'instance_name': guest.name,
8898                      'error_code': error_code,
8899                      'ex': encodeutils.exception_to_unicode(ex)})
8900             except OSError as e:
8901                 if e.errno in (errno.ENOENT, errno.ESTALE):
8902                     LOG.warning('Periodic task is updating the host stat, '
8903                                 'it is trying to get disk %(i_name)s, '
8904                                 'but disk file was removed by concurrent '
8905                                 'operations such as resize.',
8906                                 {'i_name': guest.name})
8907                 elif e.errno == errno.EACCES:
8908                     LOG.warning('Periodic task is updating the host stat, '
8909                                 'it is trying to get disk %(i_name)s, '
8910                                 'but access is denied. It is most likely '
8911                                 'due to a VM that exists on the compute '
8912                                 'node but is not managed by Nova.',
8913                                 {'i_name': guest.name})
8914                 else:
8915                     raise
8916             except exception.VolumeBDMPathNotFound as e:
8917                 LOG.warning('Periodic task is updating the host stats, '
8918                             'it is trying to get disk info for %(i_name)s, '
8919                             'but the backing volume block device was removed '
8920                             'by concurrent operations such as resize. '
8921                             'Error: %(error)s',
8922                             {'i_name': guest.name, 'error': e})
8923             except exception.DiskNotFound:
8924                 with excutils.save_and_reraise_exception() as err_ctxt:
8925                     # If the instance is undergoing a task state transition,
8926                     # like moving to another host or is being deleted, we
8927                     # should ignore this instance and move on.
8928                     if guest.uuid in local_instances:
8929                         inst = local_instances[guest.uuid]
8930                         # bug 1774249 indicated when instance is in RESIZED
8931                         # state it might also can't find back disk
8932                         if (inst.task_state is not None or
8933                             inst.vm_state == vm_states.RESIZED):
8934                             LOG.info('Periodic task is updating the host '
8935                                      'stats; it is trying to get disk info '
8936                                      'for %(i_name)s, but the backing disk '
8937                                      'was removed by a concurrent operation '
8938                                      '(task_state=%(task_state)s) and '
8939                                      '(vm_state=%(vm_state)s)',
8940                                      {'i_name': guest.name,
8941                                       'task_state': inst.task_state,
8942                                       'vm_state': inst.vm_state},
8943                                      instance=inst)
8944                             err_ctxt.reraise = False
8945 
8946             # NOTE(gtt116): give other tasks a chance.
8947             greenthread.sleep(0)
8948         return disk_over_committed_size
8949 
8950     def unfilter_instance(self, instance, network_info):
8951         """See comments of same method in firewall_driver."""
8952         self.firewall_driver.unfilter_instance(instance,
8953                                                network_info=network_info)
8954 
8955     def get_available_nodes(self, refresh=False):
8956         return [self._host.get_hostname()]
8957 
8958     def get_host_cpu_stats(self):
8959         """Return the current CPU state of the host."""
8960         return self._host.get_cpu_stats()
8961 
8962     def get_host_uptime(self):
8963         """Returns the result of calling "uptime"."""
8964         out, err = processutils.execute('env', 'LANG=C', 'uptime')
8965         return out
8966 
8967     def manage_image_cache(self, context, all_instances):
8968         """Manage the local cache of images."""
8969         self.image_cache_manager.update(context, all_instances)
8970 
8971     def _cleanup_remote_migration(self, dest, inst_base, inst_base_resize,
8972                                   shared_storage=False):
8973         """Used only for cleanup in case migrate_disk_and_power_off fails."""
8974         try:
8975             if os.path.exists(inst_base_resize):
8976                 shutil.rmtree(inst_base, ignore_errors=True)
8977                 os.rename(inst_base_resize, inst_base)
8978                 if not shared_storage:
8979                     self._remotefs.remove_dir(dest, inst_base)
8980         except Exception:
8981             pass
8982 
8983     def _is_storage_shared_with(self, dest, inst_base):
8984         # NOTE (rmk): There are two methods of determining whether we are
8985         #             on the same filesystem: the source and dest IP are the
8986         #             same, or we create a file on the dest system via SSH
8987         #             and check whether the source system can also see it.
8988         # NOTE (drwahl): Actually, there is a 3rd way: if images_type is rbd,
8989         #                it will always be shared storage
8990         if CONF.libvirt.images_type == 'rbd':
8991             return True
8992         shared_storage = (dest == self.get_host_ip_addr())
8993         if not shared_storage:
8994             tmp_file = uuidutils.generate_uuid(dashed=False) + '.tmp'
8995             tmp_path = os.path.join(inst_base, tmp_file)
8996 
8997             try:
8998                 self._remotefs.create_file(dest, tmp_path)
8999                 if os.path.exists(tmp_path):
9000                     shared_storage = True
9001                     os.unlink(tmp_path)
9002                 else:
9003                     self._remotefs.remove_file(dest, tmp_path)
9004             except Exception:
9005                 pass
9006         return shared_storage
9007 
9008     def migrate_disk_and_power_off(self, context, instance, dest,
9009                                    flavor, network_info,
9010                                    block_device_info=None,
9011                                    timeout=0, retry_interval=0):
9012         LOG.debug("Starting migrate_disk_and_power_off",
9013                    instance=instance)
9014 
9015         ephemerals = driver.block_device_info_get_ephemerals(block_device_info)
9016 
9017         # get_bdm_ephemeral_disk_size() will return 0 if the new
9018         # instance's requested block device mapping contain no
9019         # ephemeral devices. However, we still want to check if
9020         # the original instance's ephemeral_gb property was set and
9021         # ensure that the new requested flavor ephemeral size is greater
9022         eph_size = (block_device.get_bdm_ephemeral_disk_size(ephemerals) or
9023                     instance.flavor.ephemeral_gb)
9024 
9025         # Checks if the migration needs a disk resize down.
9026         root_down = flavor.root_gb < instance.flavor.root_gb
9027         ephemeral_down = flavor.ephemeral_gb < eph_size
9028         booted_from_volume = self._is_booted_from_volume(block_device_info)
9029 
9030         if (root_down and not booted_from_volume) or ephemeral_down:
9031             reason = _("Unable to resize disk down.")
9032             raise exception.InstanceFaultRollback(
9033                 exception.ResizeError(reason=reason))
9034 
9035         # NOTE(dgenin): Migration is not implemented for LVM backed instances.
9036         if CONF.libvirt.images_type == 'lvm' and not booted_from_volume:
9037             reason = _("Migration is not supported for LVM backed instances")
9038             raise exception.InstanceFaultRollback(
9039                 exception.MigrationPreCheckError(reason=reason))
9040 
9041         # copy disks to destination
9042         # rename instance dir to +_resize at first for using
9043         # shared storage for instance dir (eg. NFS).
9044         inst_base = libvirt_utils.get_instance_path(instance)
9045         inst_base_resize = inst_base + "_resize"
9046         shared_storage = self._is_storage_shared_with(dest, inst_base)
9047 
9048         # try to create the directory on the remote compute node
9049         # if this fails we pass the exception up the stack so we can catch
9050         # failures here earlier
9051         if not shared_storage:
9052             try:
9053                 self._remotefs.create_dir(dest, inst_base)
9054             except processutils.ProcessExecutionError as e:
9055                 reason = _("not able to execute ssh command: %s") % e
9056                 raise exception.InstanceFaultRollback(
9057                     exception.ResizeError(reason=reason))
9058 
9059         self.power_off(instance, timeout, retry_interval)
9060 
9061         block_device_mapping = driver.block_device_info_get_mapping(
9062             block_device_info)
9063         for vol in block_device_mapping:
9064             connection_info = vol['connection_info']
9065             self._disconnect_volume(context, connection_info, instance)
9066 
9067         disk_info = self._get_instance_disk_info(instance, block_device_info)
9068 
9069         try:
9070             os.rename(inst_base, inst_base_resize)
9071             # if we are migrating the instance with shared storage then
9072             # create the directory.  If it is a remote node the directory
9073             # has already been created
9074             if shared_storage:
9075                 dest = None
9076                 fileutils.ensure_tree(inst_base)
9077 
9078             on_execute = lambda process: \
9079                 self.job_tracker.add_job(instance, process.pid)
9080             on_completion = lambda process: \
9081                 self.job_tracker.remove_job(instance, process.pid)
9082 
9083             for info in disk_info:
9084                 # assume inst_base == dirname(info['path'])
9085                 img_path = info['path']
9086                 fname = os.path.basename(img_path)
9087                 from_path = os.path.join(inst_base_resize, fname)
9088 
9089                 # We will not copy over the swap disk here, and rely on
9090                 # finish_migration to re-create it for us. This is ok because
9091                 # the OS is shut down, and as recreating a swap disk is very
9092                 # cheap it is more efficient than copying either locally or
9093                 # over the network. This also means we don't have to resize it.
9094                 if fname == 'disk.swap':
9095                     continue
9096 
9097                 compression = info['type'] not in NO_COMPRESSION_TYPES
9098                 libvirt_utils.copy_image(from_path, img_path, host=dest,
9099                                          on_execute=on_execute,
9100                                          on_completion=on_completion,
9101                                          compression=compression)
9102 
9103             # Ensure disk.info is written to the new path to avoid disks being
9104             # reinspected and potentially changing format.
9105             src_disk_info_path = os.path.join(inst_base_resize, 'disk.info')
9106             if os.path.exists(src_disk_info_path):
9107                 dst_disk_info_path = os.path.join(inst_base, 'disk.info')
9108                 libvirt_utils.copy_image(src_disk_info_path,
9109                                          dst_disk_info_path,
9110                                          host=dest, on_execute=on_execute,
9111                                          on_completion=on_completion)
9112         except Exception:
9113             with excutils.save_and_reraise_exception():
9114                 self._cleanup_remote_migration(dest, inst_base,
9115                                                inst_base_resize,
9116                                                shared_storage)
9117 
9118         return jsonutils.dumps(disk_info)
9119 
9120     def _wait_for_running(self, instance):
9121         state = self.get_info(instance).state
9122 
9123         if state == power_state.RUNNING:
9124             LOG.info("Instance running successfully.", instance=instance)
9125             raise loopingcall.LoopingCallDone()
9126 
9127     @staticmethod
9128     def _disk_raw_to_qcow2(path):
9129         """Converts a raw disk to qcow2."""
9130         path_qcow = path + '_qcow'
9131         images.convert_image(path, path_qcow, 'raw', 'qcow2')
9132         os.rename(path_qcow, path)
9133 
9134     def finish_migration(self, context, migration, instance, disk_info,
9135                          network_info, image_meta, resize_instance,
9136                          block_device_info=None, power_on=True):
9137         LOG.debug("Starting finish_migration", instance=instance)
9138 
9139         block_disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
9140                                                   instance,
9141                                                   image_meta,
9142                                                   block_device_info)
9143         # assume _create_image does nothing if a target file exists.
9144         # NOTE: This has the intended side-effect of fetching a missing
9145         # backing file.
9146         self._create_image(context, instance, block_disk_info['mapping'],
9147                            block_device_info=block_device_info,
9148                            ignore_bdi_for_swap=True,
9149                            fallback_from_host=migration.source_compute)
9150 
9151         # Required by Quobyte CI
9152         self._ensure_console_log_for_instance(instance)
9153 
9154         gen_confdrive = functools.partial(
9155             self._create_configdrive, context, instance,
9156             InjectionInfo(admin_pass=None, network_info=network_info,
9157                           files=None))
9158 
9159         # Convert raw disks to qcow2 if migrating to host which uses
9160         # qcow2 from host which uses raw.
9161         disk_info = jsonutils.loads(disk_info)
9162         for info in disk_info:
9163             path = info['path']
9164             disk_name = os.path.basename(path)
9165 
9166             # NOTE(mdbooth): The code below looks wrong, but is actually
9167             # required to prevent a security hole when migrating from a host
9168             # with use_cow_images=False to one with use_cow_images=True.
9169             # Imagebackend uses use_cow_images to select between the
9170             # atrociously-named-Raw and Qcow2 backends. The Qcow2 backend
9171             # writes to disk.info, but does not read it as it assumes qcow2.
9172             # Therefore if we don't convert raw to qcow2 here, a raw disk will
9173             # be incorrectly assumed to be qcow2, which is a severe security
9174             # flaw. The reverse is not true, because the atrociously-named-Raw
9175             # backend supports both qcow2 and raw disks, and will choose
9176             # appropriately between them as long as disk.info exists and is
9177             # correctly populated, which it is because Qcow2 writes to
9178             # disk.info.
9179             #
9180             # In general, we do not yet support format conversion during
9181             # migration. For example:
9182             #   * Converting from use_cow_images=True to use_cow_images=False
9183             #     isn't handled. This isn't a security bug, but is almost
9184             #     certainly buggy in other cases, as the 'Raw' backend doesn't
9185             #     expect a backing file.
9186             #   * Converting to/from lvm and rbd backends is not supported.
9187             #
9188             # This behaviour is inconsistent, and therefore undesirable for
9189             # users. It is tightly-coupled to implementation quirks of 2
9190             # out of 5 backends in imagebackend and defends against a severe
9191             # security flaw which is not at all obvious without deep analysis,
9192             # and is therefore undesirable to developers. We should aim to
9193             # remove it. This will not be possible, though, until we can
9194             # represent the storage layout of a specific instance
9195             # independent of the default configuration of the local compute
9196             # host.
9197 
9198             # Config disks are hard-coded to be raw even when
9199             # use_cow_images=True (see _get_disk_config_image_type),so don't
9200             # need to be converted.
9201             if (disk_name != 'disk.config' and
9202                         info['type'] == 'raw' and CONF.use_cow_images):
9203                 self._disk_raw_to_qcow2(info['path'])
9204 
9205         xml = self._get_guest_xml(context, instance, network_info,
9206                                   block_disk_info, image_meta,
9207                                   block_device_info=block_device_info)
9208         # NOTE(mriedem): vifs_already_plugged=True here, regardless of whether
9209         # or not we've migrated to another host, because we unplug VIFs locally
9210         # and the status change in the port might go undetected by the neutron
9211         # L2 agent (or neutron server) so neutron may not know that the VIF was
9212         # unplugged in the first place and never send an event.
9213         guest = self._create_domain_and_network(context, xml, instance,
9214                                         network_info,
9215                                         block_device_info=block_device_info,
9216                                         power_on=power_on,
9217                                         vifs_already_plugged=True,
9218                                         post_xml_callback=gen_confdrive)
9219         if power_on:
9220             timer = loopingcall.FixedIntervalLoopingCall(
9221                                                     self._wait_for_running,
9222                                                     instance)
9223             timer.start(interval=0.5).wait()
9224 
9225             # Sync guest time after migration.
9226             guest.sync_guest_time()
9227 
9228         LOG.debug("finish_migration finished successfully.", instance=instance)
9229 
9230     def _cleanup_failed_migration(self, inst_base):
9231         """Make sure that a failed migrate doesn't prevent us from rolling
9232         back in a revert.
9233         """
9234         try:
9235             shutil.rmtree(inst_base)
9236         except OSError as e:
9237             if e.errno != errno.ENOENT:
9238                 raise
9239 
9240     def finish_revert_migration(self, context, instance, network_info,
9241                                 block_device_info=None, power_on=True):
9242         LOG.debug("Starting finish_revert_migration",
9243                   instance=instance)
9244 
9245         inst_base = libvirt_utils.get_instance_path(instance)
9246         inst_base_resize = inst_base + "_resize"
9247 
9248         # NOTE(danms): if we're recovering from a failed migration,
9249         # make sure we don't have a left-over same-host base directory
9250         # that would conflict. Also, don't fail on the rename if the
9251         # failure happened early.
9252         if os.path.exists(inst_base_resize):
9253             self._cleanup_failed_migration(inst_base)
9254             os.rename(inst_base_resize, inst_base)
9255 
9256         root_disk = self.image_backend.by_name(instance, 'disk')
9257         # Once we rollback, the snapshot is no longer needed, so remove it
9258         if root_disk.exists():
9259             root_disk.rollback_to_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME)
9260             root_disk.remove_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME)
9261 
9262         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
9263                                             instance,
9264                                             instance.image_meta,
9265                                             block_device_info)
9266         xml = self._get_guest_xml(context, instance, network_info, disk_info,
9267                                   instance.image_meta,
9268                                   block_device_info=block_device_info)
9269         self._create_domain_and_network(context, xml, instance, network_info,
9270                                         block_device_info=block_device_info,
9271                                         power_on=power_on)
9272 
9273         if power_on:
9274             timer = loopingcall.FixedIntervalLoopingCall(
9275                                                     self._wait_for_running,
9276                                                     instance)
9277             timer.start(interval=0.5).wait()
9278 
9279         LOG.debug("finish_revert_migration finished successfully.",
9280                   instance=instance)
9281 
9282     def confirm_migration(self, context, migration, instance, network_info):
9283         """Confirms a resize, destroying the source VM."""
9284         self._cleanup_resize(context, instance, network_info)
9285 
9286     @staticmethod
9287     def _get_io_devices(xml_doc):
9288         """get the list of io devices from the xml document."""
9289         result = {"volumes": [], "ifaces": []}
9290         try:
9291             doc = etree.fromstring(xml_doc)
9292         except Exception:
9293             return result
9294         blocks = [('./devices/disk', 'volumes'),
9295             ('./devices/interface', 'ifaces')]
9296         for block, key in blocks:
9297             section = doc.findall(block)
9298             for node in section:
9299                 for child in node.getchildren():
9300                     if child.tag == 'target' and child.get('dev'):
9301                         result[key].append(child.get('dev'))
9302         return result
9303 
9304     def get_diagnostics(self, instance):
9305         guest = self._host.get_guest(instance)
9306 
9307         # TODO(sahid): We are converting all calls from a
9308         # virDomain object to use nova.virt.libvirt.Guest.
9309         # We should be able to remove domain at the end.
9310         domain = guest._domain
9311         output = {}
9312         # get cpu time, might launch an exception if the method
9313         # is not supported by the underlying hypervisor being
9314         # used by libvirt
9315         try:
9316             for vcpu in guest.get_vcpus_info():
9317                 output["cpu" + str(vcpu.id) + "_time"] = vcpu.time
9318         except libvirt.libvirtError:
9319             pass
9320         # get io status
9321         xml = guest.get_xml_desc()
9322         dom_io = LibvirtDriver._get_io_devices(xml)
9323         for guest_disk in dom_io["volumes"]:
9324             try:
9325                 # blockStats might launch an exception if the method
9326                 # is not supported by the underlying hypervisor being
9327                 # used by libvirt
9328                 stats = domain.blockStats(guest_disk)
9329                 output[guest_disk + "_read_req"] = stats[0]
9330                 output[guest_disk + "_read"] = stats[1]
9331                 output[guest_disk + "_write_req"] = stats[2]
9332                 output[guest_disk + "_write"] = stats[3]
9333                 output[guest_disk + "_errors"] = stats[4]
9334             except libvirt.libvirtError:
9335                 pass
9336         for interface in dom_io["ifaces"]:
9337             try:
9338                 # interfaceStats might launch an exception if the method
9339                 # is not supported by the underlying hypervisor being
9340                 # used by libvirt
9341                 stats = domain.interfaceStats(interface)
9342                 output[interface + "_rx"] = stats[0]
9343                 output[interface + "_rx_packets"] = stats[1]
9344                 output[interface + "_rx_errors"] = stats[2]
9345                 output[interface + "_rx_drop"] = stats[3]
9346                 output[interface + "_tx"] = stats[4]
9347                 output[interface + "_tx_packets"] = stats[5]
9348                 output[interface + "_tx_errors"] = stats[6]
9349                 output[interface + "_tx_drop"] = stats[7]
9350             except libvirt.libvirtError:
9351                 pass
9352         output["memory"] = domain.maxMemory()
9353         # memoryStats might launch an exception if the method
9354         # is not supported by the underlying hypervisor being
9355         # used by libvirt
9356         try:
9357             mem = domain.memoryStats()
9358             for key in mem.keys():
9359                 output["memory-" + key] = mem[key]
9360         except (libvirt.libvirtError, AttributeError):
9361             pass
9362         return output
9363 
9364     def get_instance_diagnostics(self, instance):
9365         guest = self._host.get_guest(instance)
9366 
9367         # TODO(sahid): We are converting all calls from a
9368         # virDomain object to use nova.virt.libvirt.Guest.
9369         # We should be able to remove domain at the end.
9370         domain = guest._domain
9371 
9372         xml = guest.get_xml_desc()
9373         xml_doc = etree.fromstring(xml)
9374 
9375         # TODO(sahid): Needs to use get_info but more changes have to
9376         # be done since a mapping STATE_MAP LIBVIRT_POWER_STATE is
9377         # needed.
9378         (state, max_mem, mem, num_cpu, cpu_time) = \
9379             guest._get_domain_info(self._host)
9380         config_drive = configdrive.required_by(instance)
9381         launched_at = timeutils.normalize_time(instance.launched_at)
9382         uptime = timeutils.delta_seconds(launched_at,
9383                                          timeutils.utcnow())
9384         diags = diagnostics_obj.Diagnostics(state=power_state.STATE_MAP[state],
9385                                         driver='libvirt',
9386                                         config_drive=config_drive,
9387                                         hypervisor=CONF.libvirt.virt_type,
9388                                         hypervisor_os='linux',
9389                                         uptime=uptime)
9390         diags.memory_details = diagnostics_obj.MemoryDiagnostics(
9391             maximum=max_mem / units.Mi,
9392             used=mem / units.Mi)
9393 
9394         # get cpu time, might launch an exception if the method
9395         # is not supported by the underlying hypervisor being
9396         # used by libvirt
9397         try:
9398             for vcpu in guest.get_vcpus_info():
9399                 diags.add_cpu(id=vcpu.id, time=vcpu.time)
9400         except libvirt.libvirtError:
9401             pass
9402         # get io status
9403         dom_io = LibvirtDriver._get_io_devices(xml)
9404         for guest_disk in dom_io["volumes"]:
9405             try:
9406                 # blockStats might launch an exception if the method
9407                 # is not supported by the underlying hypervisor being
9408                 # used by libvirt
9409                 stats = domain.blockStats(guest_disk)
9410                 diags.add_disk(read_bytes=stats[1],
9411                                read_requests=stats[0],
9412                                write_bytes=stats[3],
9413                                write_requests=stats[2],
9414                                errors_count=stats[4])
9415             except libvirt.libvirtError:
9416                 pass
9417 
9418         for interface in xml_doc.findall('./devices/interface'):
9419             mac_address = interface.find('mac').get('address')
9420             target = interface.find('./target')
9421 
9422             # add nic that has no target (therefore no stats)
9423             if target is None:
9424                 diags.add_nic(mac_address=mac_address)
9425                 continue
9426 
9427             # add nic with stats
9428             dev = target.get('dev')
9429             try:
9430                 if dev:
9431                     # interfaceStats might launch an exception if the
9432                     # method is not supported by the underlying hypervisor
9433                     # being used by libvirt
9434                     stats = domain.interfaceStats(dev)
9435                     diags.add_nic(mac_address=mac_address,
9436                                   rx_octets=stats[0],
9437                                   rx_errors=stats[2],
9438                                   rx_drop=stats[3],
9439                                   rx_packets=stats[1],
9440                                   tx_octets=stats[4],
9441                                   tx_errors=stats[6],
9442                                   tx_drop=stats[7],
9443                                   tx_packets=stats[5])
9444 
9445             except libvirt.libvirtError:
9446                 pass
9447 
9448         return diags
9449 
9450     @staticmethod
9451     def _prepare_device_bus(dev):
9452         """Determines the device bus and its hypervisor assigned address
9453         """
9454         bus = None
9455         address = (dev.device_addr.format_address() if
9456                    dev.device_addr else None)
9457         if isinstance(dev.device_addr,
9458                       vconfig.LibvirtConfigGuestDeviceAddressPCI):
9459             bus = objects.PCIDeviceBus()
9460         elif isinstance(dev, vconfig.LibvirtConfigGuestDisk):
9461             if dev.target_bus == 'scsi':
9462                 bus = objects.SCSIDeviceBus()
9463             elif dev.target_bus == 'ide':
9464                 bus = objects.IDEDeviceBus()
9465             elif dev.target_bus == 'usb':
9466                 bus = objects.USBDeviceBus()
9467         if address is not None and bus is not None:
9468             bus.address = address
9469         return bus
9470 
9471     def _build_interface_metadata(self, dev, vifs_to_expose, vlans_by_mac,
9472                                   trusted_by_mac):
9473         """Builds a metadata object for a network interface
9474 
9475         :param dev: The LibvirtConfigGuestInterface to build metadata for.
9476         :param vifs_to_expose: The list of tagged and/or vlan'ed
9477                                VirtualInterface objects.
9478         :param vlans_by_mac: A dictionary of mac address -> vlan associations.
9479         :param trusted_by_mac: A dictionary of mac address -> vf_trusted
9480                                associations.
9481         :return: A NetworkInterfaceMetadata object, or None.
9482         """
9483         vif = vifs_to_expose.get(dev.mac_addr)
9484         if not vif:
9485             LOG.debug('No VIF found with MAC %s, not building metadata',
9486                       dev.mac_addr)
9487             return None
9488         bus = self._prepare_device_bus(dev)
9489         device = objects.NetworkInterfaceMetadata(mac=vif.address)
9490         if 'tag' in vif and vif.tag:
9491             device.tags = [vif.tag]
9492         if bus:
9493             device.bus = bus
9494         vlan = vlans_by_mac.get(vif.address)
9495         if vlan:
9496             device.vlan = int(vlan)
9497         device.vf_trusted = trusted_by_mac.get(vif.address, False)
9498         return device
9499 
9500     def _build_disk_metadata(self, dev, tagged_bdms):
9501         """Builds a metadata object for a disk
9502 
9503         :param dev: The vconfig.LibvirtConfigGuestDisk to build metadata for.
9504         :param tagged_bdms: The list of tagged BlockDeviceMapping objects.
9505         :return: A DiskMetadata object, or None.
9506         """
9507         bdm = tagged_bdms.get(dev.target_dev)
9508         if not bdm:
9509             LOG.debug('No BDM found with device name %s, not building '
9510                       'metadata.', dev.target_dev)
9511             return None
9512         bus = self._prepare_device_bus(dev)
9513         device = objects.DiskMetadata(tags=[bdm.tag])
9514         # NOTE(artom) Setting the serial (which corresponds to
9515         # volume_id in BlockDeviceMapping) in DiskMetadata allows us to
9516         # find the disks's BlockDeviceMapping object when we detach the
9517         # volume and want to clean up its metadata.
9518         device.serial = bdm.volume_id
9519         if bus:
9520             device.bus = bus
9521         return device
9522 
9523     def _build_hostdev_metadata(self, dev, vifs_to_expose, vlans_by_mac):
9524         """Builds a metadata object for a hostdev. This can only be a PF, so we
9525         don't need trusted_by_mac like in _build_interface_metadata because
9526         only VFs can be trusted.
9527 
9528         :param dev: The LibvirtConfigGuestHostdevPCI to build metadata for.
9529         :param vifs_to_expose: The list of tagged and/or vlan'ed
9530                                VirtualInterface objects.
9531         :param vlans_by_mac: A dictionary of mac address -> vlan associations.
9532         :return: A NetworkInterfaceMetadata object, or None.
9533         """
9534         # Strip out the leading '0x'
9535         pci_address = pci_utils.get_pci_address(
9536             *[x[2:] for x in (dev.domain, dev.bus, dev.slot, dev.function)])
9537         try:
9538             mac = pci_utils.get_mac_by_pci_address(pci_address,
9539                                                    pf_interface=True)
9540         except exception.PciDeviceNotFoundById:
9541             LOG.debug('Not exposing metadata for not found PCI device %s',
9542                       pci_address)
9543             return None
9544 
9545         vif = vifs_to_expose.get(mac)
9546         if not vif:
9547             LOG.debug('No VIF found with MAC %s, not building metadata', mac)
9548             return None
9549 
9550         device = objects.NetworkInterfaceMetadata(mac=mac)
9551         device.bus = objects.PCIDeviceBus(address=pci_address)
9552         if 'tag' in vif and vif.tag:
9553             device.tags = [vif.tag]
9554         vlan = vlans_by_mac.get(mac)
9555         if vlan:
9556             device.vlan = int(vlan)
9557         return device
9558 
9559     def _build_device_metadata(self, context, instance):
9560         """Builds a metadata object for instance devices, that maps the user
9561            provided tag to the hypervisor assigned device address.
9562         """
9563         def _get_device_name(bdm):
9564             return block_device.strip_dev(bdm.device_name)
9565 
9566         network_info = instance.info_cache.network_info
9567         vlans_by_mac = netutils.get_cached_vifs_with_vlan(network_info)
9568         trusted_by_mac = netutils.get_cached_vifs_with_trusted(network_info)
9569         vifs = objects.VirtualInterfaceList.get_by_instance_uuid(context,
9570                                                                  instance.uuid)
9571         vifs_to_expose = {vif.address: vif for vif in vifs
9572                           if ('tag' in vif and vif.tag) or
9573                              vlans_by_mac.get(vif.address)}
9574         # TODO(mriedem): We should be able to avoid the DB query here by using
9575         # block_device_info['block_device_mapping'] which is passed into most
9576         # methods that call this function.
9577         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
9578             context, instance.uuid)
9579         tagged_bdms = {_get_device_name(bdm): bdm for bdm in bdms if bdm.tag}
9580 
9581         devices = []
9582         guest = self._host.get_guest(instance)
9583         xml = guest.get_xml_desc()
9584         xml_dom = etree.fromstring(xml)
9585         guest_config = vconfig.LibvirtConfigGuest()
9586         guest_config.parse_dom(xml_dom)
9587 
9588         for dev in guest_config.devices:
9589             device = None
9590             if isinstance(dev, vconfig.LibvirtConfigGuestInterface):
9591                 device = self._build_interface_metadata(dev, vifs_to_expose,
9592                                                         vlans_by_mac,
9593                                                         trusted_by_mac)
9594             if isinstance(dev, vconfig.LibvirtConfigGuestDisk):
9595                 device = self._build_disk_metadata(dev, tagged_bdms)
9596             if isinstance(dev, vconfig.LibvirtConfigGuestHostdevPCI):
9597                 device = self._build_hostdev_metadata(dev, vifs_to_expose,
9598                                                       vlans_by_mac)
9599             if device:
9600                 devices.append(device)
9601         if devices:
9602             dev_meta = objects.InstanceDeviceMetadata(devices=devices)
9603             return dev_meta
9604 
9605     def instance_on_disk(self, instance):
9606         # ensure directories exist and are writable
9607         instance_path = libvirt_utils.get_instance_path(instance)
9608         LOG.debug('Checking instance files accessibility %s', instance_path,
9609                   instance=instance)
9610         shared_instance_path = os.access(instance_path, os.W_OK)
9611         # NOTE(flwang): For shared block storage scenario, the file system is
9612         # not really shared by the two hosts, but the volume of evacuated
9613         # instance is reachable.
9614         shared_block_storage = (self.image_backend.backend().
9615                                 is_shared_block_storage())
9616         return shared_instance_path or shared_block_storage
9617 
9618     def inject_network_info(self, instance, nw_info):
9619         self.firewall_driver.setup_basic_filtering(instance, nw_info)
9620 
9621     def delete_instance_files(self, instance):
9622         target = libvirt_utils.get_instance_path(instance)
9623         # A resize may be in progress
9624         target_resize = target + '_resize'
9625         # Other threads may attempt to rename the path, so renaming the path
9626         # to target + '_del' (because it is atomic) and iterating through
9627         # twice in the unlikely event that a concurrent rename occurs between
9628         # the two rename attempts in this method. In general this method
9629         # should be fairly thread-safe without these additional checks, since
9630         # other operations involving renames are not permitted when the task
9631         # state is not None and the task state should be set to something
9632         # other than None by the time this method is invoked.
9633         target_del = target + '_del'
9634         for i in range(2):
9635             try:
9636                 os.rename(target, target_del)
9637                 break
9638             except Exception:
9639                 pass
9640             try:
9641                 os.rename(target_resize, target_del)
9642                 break
9643             except Exception:
9644                 pass
9645         # Either the target or target_resize path may still exist if all
9646         # rename attempts failed.
9647         remaining_path = None
9648         for p in (target, target_resize):
9649             if os.path.exists(p):
9650                 remaining_path = p
9651                 break
9652 
9653         # A previous delete attempt may have been interrupted, so target_del
9654         # may exist even if all rename attempts during the present method
9655         # invocation failed due to the absence of both target and
9656         # target_resize.
9657         if not remaining_path and os.path.exists(target_del):
9658             self.job_tracker.terminate_jobs(instance)
9659 
9660             LOG.info('Deleting instance files %s', target_del,
9661                      instance=instance)
9662             remaining_path = target_del
9663             try:
9664                 shutil.rmtree(target_del)
9665             except OSError as e:
9666                 LOG.error('Failed to cleanup directory %(target)s: %(e)s',
9667                           {'target': target_del, 'e': e}, instance=instance)
9668 
9669         # It is possible that the delete failed, if so don't mark the instance
9670         # as cleaned.
9671         if remaining_path and os.path.exists(remaining_path):
9672             LOG.info('Deletion of %s failed', remaining_path,
9673                      instance=instance)
9674             return False
9675 
9676         LOG.info('Deletion of %s complete', target_del, instance=instance)
9677         return True
9678 
9679     @property
9680     def need_legacy_block_device_info(self):
9681         return False
9682 
9683     def default_root_device_name(self, instance, image_meta, root_bdm):
9684         disk_bus = blockinfo.get_disk_bus_for_device_type(
9685             instance, CONF.libvirt.virt_type, image_meta, "disk")
9686         cdrom_bus = blockinfo.get_disk_bus_for_device_type(
9687             instance, CONF.libvirt.virt_type, image_meta, "cdrom")
9688         root_info = blockinfo.get_root_info(
9689             instance, CONF.libvirt.virt_type, image_meta,
9690             root_bdm, disk_bus, cdrom_bus)
9691         return block_device.prepend_dev(root_info['dev'])
9692 
9693     def default_device_names_for_instance(self, instance, root_device_name,
9694                                           *block_device_lists):
9695         block_device_mapping = list(itertools.chain(*block_device_lists))
9696         # NOTE(ndipanov): Null out the device names so that blockinfo code
9697         #                 will assign them
9698         for bdm in block_device_mapping:
9699             if bdm.device_name is not None:
9700                 LOG.info(
9701                     "Ignoring supplied device name: %(device_name)s. "
9702                     "Libvirt can't honour user-supplied dev names",
9703                     {'device_name': bdm.device_name}, instance=instance)
9704                 bdm.device_name = None
9705         block_device_info = driver.get_block_device_info(instance,
9706                                                          block_device_mapping)
9707 
9708         blockinfo.default_device_names(CONF.libvirt.virt_type,
9709                                        nova_context.get_admin_context(),
9710                                        instance,
9711                                        block_device_info,
9712                                        instance.image_meta)
9713 
9714     def get_device_name_for_instance(self, instance, bdms, block_device_obj):
9715         block_device_info = driver.get_block_device_info(instance, bdms)
9716         instance_info = blockinfo.get_disk_info(
9717                 CONF.libvirt.virt_type, instance,
9718                 instance.image_meta, block_device_info=block_device_info)
9719 
9720         suggested_dev_name = block_device_obj.device_name
9721         if suggested_dev_name is not None:
9722             LOG.info(
9723                 'Ignoring supplied device name: %(suggested_dev)s',
9724                 {'suggested_dev': suggested_dev_name}, instance=instance)
9725 
9726         # NOTE(ndipanov): get_info_from_bdm will generate the new device name
9727         #                 only when it's actually not set on the bd object
9728         block_device_obj.device_name = None
9729         disk_info = blockinfo.get_info_from_bdm(
9730             instance, CONF.libvirt.virt_type, instance.image_meta,
9731             block_device_obj, mapping=instance_info['mapping'])
9732         return block_device.prepend_dev(disk_info['dev'])
9733 
9734     def is_supported_fs_format(self, fs_type):
9735         return fs_type in [nova.privsep.fs.FS_FORMAT_EXT2,
9736                            nova.privsep.fs.FS_FORMAT_EXT3,
9737                            nova.privsep.fs.FS_FORMAT_EXT4,
9738                            nova.privsep.fs.FS_FORMAT_XFS]
9739 
9740     def _get_cpu_traits(self):
9741         """Get CPU traits of VMs based on guest CPU model config:
9742         1. if mode is 'host-model' or 'host-passthrough', use host's
9743         CPU features.
9744         2. if mode is None, choose a default CPU model based on CPU
9745         architecture.
9746         3. if mode is 'custom', use cpu_model to generate CPU features.
9747         The code also accounts for cpu_model_extra_flags configuration when
9748         cpu_mode is 'host-model', 'host-passthrough' or 'custom', this
9749         ensures user specified CPU feature flags to be included.
9750         :return: A dict of trait names mapped to boolean values or None.
9751         """
9752         cpu = self._get_guest_cpu_model_config()
9753         if not cpu:
9754             LOG.info('The current libvirt hypervisor %(virt_type)s '
9755                      'does not support reporting CPU traits.',
9756                      {'virt_type': CONF.libvirt.virt_type})
9757             return
9758 
9759         caps = deepcopy(self._host.get_capabilities())
9760         if cpu.mode in ('host-model', 'host-passthrough'):
9761             # Account for features in cpu_model_extra_flags conf
9762             host_features = [f.name for f in
9763                              caps.host.cpu.features | cpu.features]
9764             return libvirt_utils.cpu_features_to_traits(host_features)
9765 
9766         # Choose a default CPU model when cpu_mode is not specified
9767         if cpu.mode is None:
9768             caps.host.cpu.model = libvirt_utils.get_cpu_model_from_arch(
9769                 caps.host.cpu.arch)
9770             caps.host.cpu.features = set()
9771         else:
9772             # For custom mode, set model to guest CPU model
9773             caps.host.cpu.model = cpu.model
9774             caps.host.cpu.features = set()
9775             # Account for features in cpu_model_extra_flags conf
9776             for f in cpu.features:
9777                 caps.host.cpu.add_feature(
9778                     vconfig.LibvirtConfigCPUFeature(name=f.name))
9779 
9780         xml_str = caps.host.cpu.to_xml()
9781         features_xml = self._get_guest_baseline_cpu_features(xml_str)
9782         feature_names = []
9783         if features_xml:
9784             cpu.parse_str(features_xml)
9785             feature_names = [f.name for f in cpu.features]
9786         return libvirt_utils.cpu_features_to_traits(feature_names)
9787 
9788     def _get_guest_baseline_cpu_features(self, xml_str):
9789         """Calls libvirt's baselineCPU API to compute the biggest set of
9790         CPU features which is compatible with the given host CPU.
9791 
9792         :param xml_str: XML description of host CPU
9793         :return: An XML string of the computed CPU, or None on error
9794         """
9795         LOG.debug("Libvirt baseline CPU %s", xml_str)
9796         # TODO(lei-zh): baselineCPU is not supported on all platforms.
9797         # There is some work going on in the libvirt community to replace the
9798         # baseline call. Consider using the new apis when they are ready. See
9799         # https://www.redhat.com/archives/libvir-list/2018-May/msg01204.html.
9800         try:
9801             if hasattr(libvirt, 'VIR_CONNECT_BASELINE_CPU_EXPAND_FEATURES'):
9802                 return self._host.get_connection().baselineCPU(
9803                     [xml_str],
9804                     libvirt.VIR_CONNECT_BASELINE_CPU_EXPAND_FEATURES)
9805             else:
9806                 return self._host.get_connection().baselineCPU([xml_str])
9807         except libvirt.libvirtError as ex:
9808             with excutils.save_and_reraise_exception() as ctxt:
9809                 error_code = ex.get_error_code()
9810                 if error_code == libvirt.VIR_ERR_NO_SUPPORT:
9811                     ctxt.reraise = False
9812                     LOG.info('URI %(uri)s does not support full set'
9813                              ' of host capabilities: %(error)s',
9814                              {'uri': self._host._uri, 'error': ex})
9815                     return None
