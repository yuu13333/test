Based on the given code from a commit, please generate supplementary code files according to the commit message.
####commit message
libvirt: create vGPU for instance

If an allocation is asking for a VGPU, then libvirt will look at the
available mediated devices and call the sysfs to create one of them if
needed.

Please note I commented in the relnote all the caveats we currently have
with mediated devices in libvirt, but I'll provide some workarounds for
those in the next changes.

Change-Id: Ibf210dd27972fed2651d6c9bd73a0bcf352c8bab
Partially-Implements: blueprint add-support-for-vgpu

####code 
1 # Copyright 2010 United States Government as represented by the
2 # Administrator of the National Aeronautics and Space Administration.
3 # All Rights Reserved.
4 # Copyright (c) 2010 Citrix Systems, Inc.
5 # Copyright (c) 2011 Piston Cloud Computing, Inc
6 # Copyright (c) 2012 University Of Minho
7 # (c) Copyright 2013 Hewlett-Packard Development Company, L.P.
8 #
9 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
10 #    not use this file except in compliance with the License. You may obtain
11 #    a copy of the License at
12 #
13 #         http://www.apache.org/licenses/LICENSE-2.0
14 #
15 #    Unless required by applicable law or agreed to in writing, software
16 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
17 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
18 #    License for the specific language governing permissions and limitations
19 #    under the License.
20 
21 """
22 A connection to a hypervisor through libvirt.
23 
24 Supports KVM, LXC, QEMU, UML, XEN and Parallels.
25 
26 """
27 
28 import collections
29 from collections import deque
30 import contextlib
31 import errno
32 import functools
33 import glob
34 import itertools
35 import operator
36 import os
37 import pwd
38 import shutil
39 import tempfile
40 import time
41 import uuid
42 
43 from castellan import key_manager
44 import eventlet
45 from eventlet import greenthread
46 from eventlet import tpool
47 from lxml import etree
48 from os_brick import encryptors
49 from os_brick import exception as brick_exception
50 from os_brick.initiator import connector
51 from oslo_concurrency import processutils
52 from oslo_log import log as logging
53 from oslo_serialization import jsonutils
54 from oslo_service import loopingcall
55 from oslo_utils import encodeutils
56 from oslo_utils import excutils
57 from oslo_utils import fileutils
58 from oslo_utils import importutils
59 from oslo_utils import strutils
60 from oslo_utils import timeutils
61 from oslo_utils import units
62 from oslo_utils import uuidutils
63 import six
64 from six.moves import range
65 
66 from nova.api.metadata import base as instance_metadata
67 from nova import block_device
68 from nova.compute import power_state
69 from nova.compute import task_states
70 from nova.compute import utils as compute_utils
71 import nova.conf
72 from nova.console import serial as serial_console
73 from nova.console import type as ctype
74 from nova import context as nova_context
75 from nova import exception
76 from nova.i18n import _
77 from nova import image
78 from nova.network import model as network_model
79 from nova import objects
80 from nova.objects import diagnostics as diagnostics_obj
81 from nova.objects import fields
82 from nova.objects import migrate_data as migrate_data_obj
83 from nova.pci import manager as pci_manager
84 from nova.pci import utils as pci_utils
85 import nova.privsep.libvirt
86 import nova.privsep.path
87 from nova import utils
88 from nova import version
89 from nova.virt import block_device as driver_block_device
90 from nova.virt import configdrive
91 from nova.virt.disk import api as disk_api
92 from nova.virt.disk.vfs import guestfs
93 from nova.virt import driver
94 from nova.virt import firewall
95 from nova.virt import hardware
96 from nova.virt.image import model as imgmodel
97 from nova.virt import images
98 from nova.virt.libvirt import blockinfo
99 from nova.virt.libvirt import config as vconfig
100 from nova.virt.libvirt import firewall as libvirt_firewall
101 from nova.virt.libvirt import guest as libvirt_guest
102 from nova.virt.libvirt import host
103 from nova.virt.libvirt import imagebackend
104 from nova.virt.libvirt import imagecache
105 from nova.virt.libvirt import instancejobtracker
106 from nova.virt.libvirt import migration as libvirt_migrate
107 from nova.virt.libvirt.storage import dmcrypt
108 from nova.virt.libvirt.storage import lvm
109 from nova.virt.libvirt.storage import rbd_utils
110 from nova.virt.libvirt import utils as libvirt_utils
111 from nova.virt.libvirt import vif as libvirt_vif
112 from nova.virt.libvirt.volume import mount
113 from nova.virt.libvirt.volume import remotefs
114 from nova.virt import netutils
115 from nova.volume import cinder
116 
117 libvirt = None
118 
119 uefi_logged = False
120 
121 LOG = logging.getLogger(__name__)
122 
123 CONF = nova.conf.CONF
124 
125 DEFAULT_FIREWALL_DRIVER = "%s.%s" % (
126     libvirt_firewall.__name__,
127     libvirt_firewall.IptablesFirewallDriver.__name__)
128 
129 DEFAULT_UEFI_LOADER_PATH = {
130     "x86_64": "/usr/share/OVMF/OVMF_CODE.fd",
131     "aarch64": "/usr/share/AAVMF/AAVMF_CODE.fd"
132 }
133 
134 MAX_CONSOLE_BYTES = 100 * units.Ki
135 
136 # The libvirt driver will prefix any disable reason codes with this string.
137 DISABLE_PREFIX = 'AUTO: '
138 # Disable reason for the service which was enabled or disabled without reason
139 DISABLE_REASON_UNDEFINED = None
140 
141 # Guest config console string
142 CONSOLE = "console=tty0 console=ttyS0 console=hvc0"
143 
144 GuestNumaConfig = collections.namedtuple(
145     'GuestNumaConfig', ['cpuset', 'cputune', 'numaconfig', 'numatune'])
146 
147 InjectionInfo = collections.namedtuple(
148     'InjectionInfo', ['network_info', 'files', 'admin_pass'])
149 
150 libvirt_volume_drivers = [
151     'iscsi=nova.virt.libvirt.volume.iscsi.LibvirtISCSIVolumeDriver',
152     'iser=nova.virt.libvirt.volume.iser.LibvirtISERVolumeDriver',
153     'local=nova.virt.libvirt.volume.volume.LibvirtVolumeDriver',
154     'drbd=nova.virt.libvirt.volume.drbd.LibvirtDRBDVolumeDriver',
155     'fake=nova.virt.libvirt.volume.volume.LibvirtFakeVolumeDriver',
156     'rbd=nova.virt.libvirt.volume.net.LibvirtNetVolumeDriver',
157     'sheepdog=nova.virt.libvirt.volume.net.LibvirtNetVolumeDriver',
158     'nfs=nova.virt.libvirt.volume.nfs.LibvirtNFSVolumeDriver',
159     'smbfs=nova.virt.libvirt.volume.smbfs.LibvirtSMBFSVolumeDriver',
160     'aoe=nova.virt.libvirt.volume.aoe.LibvirtAOEVolumeDriver',
161     'fibre_channel='
162         'nova.virt.libvirt.volume.fibrechannel.'
163         'LibvirtFibreChannelVolumeDriver',
164     'gpfs=nova.virt.libvirt.volume.gpfs.LibvirtGPFSVolumeDriver',
165     'quobyte=nova.virt.libvirt.volume.quobyte.LibvirtQuobyteVolumeDriver',
166     'hgst=nova.virt.libvirt.volume.hgst.LibvirtHGSTVolumeDriver',
167     'scaleio=nova.virt.libvirt.volume.scaleio.LibvirtScaleIOVolumeDriver',
168     'disco=nova.virt.libvirt.volume.disco.LibvirtDISCOVolumeDriver',
169     'vzstorage='
170         'nova.virt.libvirt.volume.vzstorage.LibvirtVZStorageVolumeDriver',
171     'veritas_hyperscale='
172         'nova.virt.libvirt.volume.vrtshyperscale.'
173         'LibvirtHyperScaleVolumeDriver',
174 ]
175 
176 
177 def patch_tpool_proxy():
178     """eventlet.tpool.Proxy doesn't work with old-style class in __str__()
179     or __repr__() calls. See bug #962840 for details.
180     We perform a monkey patch to replace those two instance methods.
181     """
182     def str_method(self):
183         return str(self._obj)
184 
185     def repr_method(self):
186         return repr(self._obj)
187 
188     tpool.Proxy.__str__ = str_method
189     tpool.Proxy.__repr__ = repr_method
190 
191 
192 patch_tpool_proxy()
193 
194 # For information about when MIN_LIBVIRT_VERSION and
195 # NEXT_MIN_LIBVIRT_VERSION can be changed, consult
196 #
197 #   https://wiki.openstack.org/wiki/LibvirtDistroSupportMatrix
198 #
199 # Currently this is effectively the min version for i686/x86_64
200 # + KVM/QEMU, as other architectures/hypervisors require newer
201 # versions. Over time, this will become a common min version
202 # for all architectures/hypervisors, as this value rises to
203 # meet them.
204 MIN_LIBVIRT_VERSION = (1, 2, 9)
205 MIN_QEMU_VERSION = (2, 1, 0)
206 # TODO(berrange): Re-evaluate this at start of each release cycle
207 # to decide if we want to plan a future min version bump.
208 # MIN_LIBVIRT_VERSION can be updated to match this after
209 # NEXT_MIN_LIBVIRT_VERSION  has been at a higher value for
210 # one cycle
211 NEXT_MIN_LIBVIRT_VERSION = (1, 3, 1)
212 NEXT_MIN_QEMU_VERSION = (2, 5, 0)
213 
214 # When the above version matches/exceeds this version
215 # delete it & corresponding code using it
216 # Libvirt version 1.2.17 is required for successful block live migration
217 # of vm booted from image with attached devices
218 MIN_LIBVIRT_BLOCK_LM_WITH_VOLUMES_VERSION = (1, 2, 17)
219 # PowerPC based hosts that support NUMA using libvirt
220 MIN_LIBVIRT_NUMA_VERSION_PPC = (1, 2, 19)
221 # Versions of libvirt with known NUMA topology issues
222 # See bug #1449028
223 BAD_LIBVIRT_NUMA_VERSIONS = [(1, 2, 9, 2)]
224 # Versions of libvirt with broken cpu pinning support. This excludes
225 # versions of libvirt with broken NUMA support since pinning needs
226 # NUMA
227 # See bug #1438226
228 BAD_LIBVIRT_CPU_POLICY_VERSIONS = [(1, 2, 10)]
229 
230 # Virtuozzo driver support
231 MIN_VIRTUOZZO_VERSION = (7, 0, 0)
232 MIN_LIBVIRT_VIRTUOZZO_VERSION = (1, 2, 12)
233 
234 # Ability to set the user guest password with Qemu
235 MIN_LIBVIRT_SET_ADMIN_PASSWD = (1, 2, 16)
236 
237 # Ability to set the user guest password with parallels
238 MIN_LIBVIRT_PARALLELS_SET_ADMIN_PASSWD = (2, 0, 0)
239 
240 # s/390 & s/390x architectures with KVM
241 MIN_LIBVIRT_KVM_S390_VERSION = (1, 2, 13)
242 MIN_QEMU_S390_VERSION = (2, 3, 0)
243 
244 # libvirt < 1.3 reported virt_functions capability
245 # only when VFs are enabled.
246 # libvirt 1.3 fix f391889f4e942e22b9ef8ecca492de05106ce41e
247 MIN_LIBVIRT_PF_WITH_NO_VFS_CAP_VERSION = (1, 3, 0)
248 
249 # Use the "logd" backend for handling stdout/stderr from QEMU processes.
250 MIN_LIBVIRT_VIRTLOGD = (1, 3, 3)
251 MIN_QEMU_VIRTLOGD = (2, 7, 0)
252 
253 # ppc64/ppc64le architectures with KVM
254 # NOTE(rfolco): Same levels for Libvirt/Qemu on Big Endian and Little
255 # Endian giving the nuance around guest vs host architectures
256 MIN_LIBVIRT_KVM_PPC64_VERSION = (1, 2, 12)
257 
258 # Names of the types that do not get compressed during migration
259 NO_COMPRESSION_TYPES = ('qcow2',)
260 
261 
262 # number of serial console limit
263 QEMU_MAX_SERIAL_PORTS = 4
264 # Qemu supports 4 serial consoles, we remove 1 because of the PTY one defined
265 ALLOWED_QEMU_SERIAL_PORTS = QEMU_MAX_SERIAL_PORTS - 1
266 
267 # realtime support
268 MIN_LIBVIRT_REALTIME_VERSION = (1, 2, 13)
269 
270 # libvirt postcopy support
271 MIN_LIBVIRT_POSTCOPY_VERSION = (1, 3, 3)
272 
273 # qemu postcopy support
274 MIN_QEMU_POSTCOPY_VERSION = (2, 5, 0)
275 
276 MIN_LIBVIRT_OTHER_ARCH = {
277     fields.Architecture.S390: MIN_LIBVIRT_KVM_S390_VERSION,
278     fields.Architecture.S390X: MIN_LIBVIRT_KVM_S390_VERSION,
279     fields.Architecture.PPC: MIN_LIBVIRT_KVM_PPC64_VERSION,
280     fields.Architecture.PPC64: MIN_LIBVIRT_KVM_PPC64_VERSION,
281     fields.Architecture.PPC64LE: MIN_LIBVIRT_KVM_PPC64_VERSION,
282 }
283 
284 MIN_QEMU_OTHER_ARCH = {
285     fields.Architecture.S390: MIN_QEMU_S390_VERSION,
286     fields.Architecture.S390X: MIN_QEMU_S390_VERSION,
287 }
288 
289 # perf events support
290 MIN_LIBVIRT_PERF_VERSION = (2, 0, 0)
291 LIBVIRT_PERF_EVENT_PREFIX = 'VIR_PERF_PARAM_'
292 
293 PERF_EVENTS_CPU_FLAG_MAPPING = {'cmt': 'cmt',
294                                 'mbml': 'mbm_local',
295                                 'mbmt': 'mbm_total',
296                                }
297 
298 # Mediated devices support
299 MIN_LIBVIRT_MDEV_SUPPORT = (3, 4, 0)
300 
301 
302 class LibvirtDriver(driver.ComputeDriver):
303     capabilities = {
304         "has_imagecache": True,
305         "supports_recreate": True,
306         "supports_migrate_to_same_host": False,
307         "supports_attach_interface": True,
308         "supports_device_tagging": True,
309         "supports_tagged_attach_interface": True,
310         "supports_tagged_attach_volume": True,
311         "supports_extend_volume": True,
312     }
313 
314     def __init__(self, virtapi, read_only=False):
315         super(LibvirtDriver, self).__init__(virtapi)
316 
317         global libvirt
318         if libvirt is None:
319             libvirt = importutils.import_module('libvirt')
320             libvirt_migrate.libvirt = libvirt
321 
322         self._host = host.Host(self._uri(), read_only,
323                                lifecycle_event_handler=self.emit_event,
324                                conn_event_handler=self._handle_conn_event)
325         self._initiator = None
326         self._fc_wwnns = None
327         self._fc_wwpns = None
328         self._caps = None
329         self._supported_perf_events = []
330         self.firewall_driver = firewall.load_driver(
331             DEFAULT_FIREWALL_DRIVER,
332             host=self._host)
333 
334         self.vif_driver = libvirt_vif.LibvirtGenericVIFDriver()
335 
336         # TODO(mriedem): Long-term we should load up the volume drivers on
337         # demand as needed rather than doing this on startup, as there might
338         # be unsupported volume drivers in this list based on the underlying
339         # platform.
340         self.volume_drivers = self._get_volume_drivers()
341 
342         self._disk_cachemode = None
343         self.image_cache_manager = imagecache.ImageCacheManager()
344         self.image_backend = imagebackend.Backend(CONF.use_cow_images)
345 
346         self.disk_cachemodes = {}
347 
348         self.valid_cachemodes = ["default",
349                                  "none",
350                                  "writethrough",
351                                  "writeback",
352                                  "directsync",
353                                  "unsafe",
354                                 ]
355         self._conn_supports_start_paused = CONF.libvirt.virt_type in ('kvm',
356                                                                       'qemu')
357 
358         for mode_str in CONF.libvirt.disk_cachemodes:
359             disk_type, sep, cache_mode = mode_str.partition('=')
360             if cache_mode not in self.valid_cachemodes:
361                 LOG.warning('Invalid cachemode %(cache_mode)s specified '
362                             'for disk type %(disk_type)s.',
363                             {'cache_mode': cache_mode, 'disk_type': disk_type})
364                 continue
365             self.disk_cachemodes[disk_type] = cache_mode
366 
367         self._volume_api = cinder.API()
368         self._image_api = image.API()
369 
370         sysinfo_serial_funcs = {
371             'none': lambda: None,
372             'hardware': self._get_host_sysinfo_serial_hardware,
373             'os': self._get_host_sysinfo_serial_os,
374             'auto': self._get_host_sysinfo_serial_auto,
375         }
376 
377         self._sysinfo_serial_func = sysinfo_serial_funcs.get(
378             CONF.libvirt.sysinfo_serial)
379 
380         self.job_tracker = instancejobtracker.InstanceJobTracker()
381         self._remotefs = remotefs.RemoteFilesystem()
382 
383         self._live_migration_flags = self._block_migration_flags = 0
384         self.active_migrations = {}
385 
386         # Compute reserved hugepages from conf file at the very
387         # beginning to ensure any syntax error will be reported and
388         # avoid any re-calculation when computing resources.
389         self._reserved_hugepages = hardware.numa_get_reserved_huge_pages()
390 
391     def _get_volume_drivers(self):
392         driver_registry = dict()
393 
394         for driver_str in libvirt_volume_drivers:
395             driver_type, _sep, driver = driver_str.partition('=')
396             driver_class = importutils.import_class(driver)
397             try:
398                 driver_registry[driver_type] = driver_class(self._host)
399             except brick_exception.InvalidConnectorProtocol:
400                 LOG.debug('Unable to load volume driver %s. It is not '
401                           'supported on this host.', driver)
402 
403         return driver_registry
404 
405     @property
406     def disk_cachemode(self):
407         if self._disk_cachemode is None:
408             # We prefer 'none' for consistent performance, host crash
409             # safety & migration correctness by avoiding host page cache.
410             # Some filesystems don't support O_DIRECT though. For those we
411             # fallback to 'writethrough' which gives host crash safety, and
412             # is safe for migration provided the filesystem is cache coherent
413             # (cluster filesystems typically are, but things like NFS are not).
414             self._disk_cachemode = "none"
415             if not utils.supports_direct_io(CONF.instances_path):
416                 self._disk_cachemode = "writethrough"
417         return self._disk_cachemode
418 
419     def _set_cache_mode(self, conf):
420         """Set cache mode on LibvirtConfigGuestDisk object."""
421         try:
422             source_type = conf.source_type
423             driver_cache = conf.driver_cache
424         except AttributeError:
425             return
426 
427         cache_mode = self.disk_cachemodes.get(source_type,
428                                               driver_cache)
429         conf.driver_cache = cache_mode
430 
431     def _do_quality_warnings(self):
432         """Warn about untested driver configurations.
433 
434         This will log a warning message about untested driver or host arch
435         configurations to indicate to administrators that the quality is
436         unknown. Currently, only qemu or kvm on intel 32- or 64-bit systems
437         is tested upstream.
438         """
439         caps = self._host.get_capabilities()
440         hostarch = caps.host.cpu.arch
441         if (CONF.libvirt.virt_type not in ('qemu', 'kvm') or
442             hostarch not in (fields.Architecture.I686,
443                              fields.Architecture.X86_64)):
444             LOG.warning('The libvirt driver is not tested on '
445                         '%(type)s/%(arch)s by the OpenStack project and '
446                         'thus its quality can not be ensured. For more '
447                         'information, see: https://docs.openstack.org/'
448                         'nova/latest/user/support-matrix.html',
449                         {'type': CONF.libvirt.virt_type, 'arch': hostarch})
450 
451     def _handle_conn_event(self, enabled, reason):
452         LOG.info("Connection event '%(enabled)d' reason '%(reason)s'",
453                  {'enabled': enabled, 'reason': reason})
454         self._set_host_enabled(enabled, reason)
455 
456     def _version_to_string(self, version):
457         return '.'.join([str(x) for x in version])
458 
459     def init_host(self, host):
460         self._host.initialize()
461 
462         self._do_quality_warnings()
463 
464         self._parse_migration_flags()
465 
466         self._supported_perf_events = self._get_supported_perf_events()
467 
468         if (CONF.libvirt.virt_type == 'lxc' and
469                 not (CONF.libvirt.uid_maps and CONF.libvirt.gid_maps)):
470             LOG.warning("Running libvirt-lxc without user namespaces is "
471                         "dangerous. Containers spawned by Nova will be run "
472                         "as the host's root user. It is highly suggested "
473                         "that user namespaces be used in a public or "
474                         "multi-tenant environment.")
475 
476         # Stop libguestfs using KVM unless we're also configured
477         # to use this. This solves problem where people need to
478         # stop Nova use of KVM because nested-virt is broken
479         if CONF.libvirt.virt_type != "kvm":
480             guestfs.force_tcg()
481 
482         if not self._host.has_min_version(MIN_LIBVIRT_VERSION):
483             raise exception.InternalError(
484                 _('Nova requires libvirt version %s or greater.') %
485                 self._version_to_string(MIN_LIBVIRT_VERSION))
486 
487         if CONF.libvirt.virt_type in ("qemu", "kvm"):
488             if self._host.has_min_version(hv_ver=MIN_QEMU_VERSION):
489                 # "qemu-img info" calls are version dependent, so we need to
490                 # store the version in the images module.
491                 images.QEMU_VERSION = self._host.get_connection().getVersion()
492             else:
493                 raise exception.InternalError(
494                     _('Nova requires QEMU version %s or greater.') %
495                     self._version_to_string(MIN_QEMU_VERSION))
496 
497         if CONF.libvirt.virt_type == 'parallels':
498             if not self._host.has_min_version(hv_ver=MIN_VIRTUOZZO_VERSION):
499                 raise exception.InternalError(
500                     _('Nova requires Virtuozzo version %s or greater.') %
501                     self._version_to_string(MIN_VIRTUOZZO_VERSION))
502             if not self._host.has_min_version(MIN_LIBVIRT_VIRTUOZZO_VERSION):
503                 raise exception.InternalError(
504                     _('Running Nova with parallels virt_type requires '
505                       'libvirt version %s') %
506                     self._version_to_string(MIN_LIBVIRT_VIRTUOZZO_VERSION))
507 
508         # Give the cloud admin a heads up if we are intending to
509         # change the MIN_LIBVIRT_VERSION in the next release.
510         if not self._host.has_min_version(NEXT_MIN_LIBVIRT_VERSION):
511             LOG.warning('Running Nova with a libvirt version less than '
512                         '%(version)s is deprecated. The required minimum '
513                         'version of libvirt will be raised to %(version)s '
514                         'in the next release.',
515                         {'version': self._version_to_string(
516                             NEXT_MIN_LIBVIRT_VERSION)})
517         if (CONF.libvirt.virt_type in ("qemu", "kvm") and
518             not self._host.has_min_version(hv_ver=NEXT_MIN_QEMU_VERSION)):
519             LOG.warning('Running Nova with a QEMU version less than '
520                         '%(version)s is deprecated. The required minimum '
521                         'version of QEMU will be raised to %(version)s '
522                         'in the next release.',
523                         {'version': self._version_to_string(
524                             NEXT_MIN_QEMU_VERSION)})
525 
526         kvm_arch = fields.Architecture.from_host()
527         if (CONF.libvirt.virt_type in ('kvm', 'qemu') and
528             kvm_arch in MIN_LIBVIRT_OTHER_ARCH and
529                 not self._host.has_min_version(
530                                         MIN_LIBVIRT_OTHER_ARCH.get(kvm_arch),
531                                         MIN_QEMU_OTHER_ARCH.get(kvm_arch))):
532             if MIN_QEMU_OTHER_ARCH.get(kvm_arch):
533                 raise exception.InternalError(
534                     _('Running Nova with qemu/kvm virt_type on %(arch)s '
535                       'requires libvirt version %(libvirt_ver)s and '
536                       'qemu version %(qemu_ver)s, or greater') %
537                     {'arch': kvm_arch,
538                      'libvirt_ver': self._version_to_string(
539                          MIN_LIBVIRT_OTHER_ARCH.get(kvm_arch)),
540                      'qemu_ver': self._version_to_string(
541                          MIN_QEMU_OTHER_ARCH.get(kvm_arch))})
542             # no qemu version in the error message
543             raise exception.InternalError(
544                 _('Running Nova with qemu/kvm virt_type on %(arch)s '
545                   'requires libvirt version %(libvirt_ver)s or greater') %
546                 {'arch': kvm_arch,
547                  'libvirt_ver': self._version_to_string(
548                      MIN_LIBVIRT_OTHER_ARCH.get(kvm_arch))})
549 
550     def _prepare_migration_flags(self):
551         migration_flags = 0
552 
553         migration_flags |= libvirt.VIR_MIGRATE_LIVE
554 
555         # Adding p2p flag only if xen is not in use, because xen does not
556         # support p2p migrations
557         if CONF.libvirt.virt_type != 'xen':
558             migration_flags |= libvirt.VIR_MIGRATE_PEER2PEER
559 
560         # Adding VIR_MIGRATE_UNDEFINE_SOURCE because, without it, migrated
561         # instance will remain defined on the source host
562         migration_flags |= libvirt.VIR_MIGRATE_UNDEFINE_SOURCE
563 
564         # Adding VIR_MIGRATE_PERSIST_DEST to persist the VM on the
565         # destination host
566         migration_flags |= libvirt.VIR_MIGRATE_PERSIST_DEST
567 
568         live_migration_flags = block_migration_flags = migration_flags
569 
570         # Adding VIR_MIGRATE_NON_SHARED_INC, otherwise all block-migrations
571         # will be live-migrations instead
572         block_migration_flags |= libvirt.VIR_MIGRATE_NON_SHARED_INC
573 
574         return (live_migration_flags, block_migration_flags)
575 
576     def _handle_live_migration_tunnelled(self, migration_flags):
577         if (CONF.libvirt.live_migration_tunnelled is None or
578                 CONF.libvirt.live_migration_tunnelled):
579             migration_flags |= libvirt.VIR_MIGRATE_TUNNELLED
580         return migration_flags
581 
582     def _is_post_copy_available(self):
583         if self._host.has_min_version(lv_ver=MIN_LIBVIRT_POSTCOPY_VERSION,
584                                       hv_ver=MIN_QEMU_POSTCOPY_VERSION):
585             return True
586         return False
587 
588     def _is_virtlogd_available(self):
589         return self._host.has_min_version(MIN_LIBVIRT_VIRTLOGD,
590                                           MIN_QEMU_VIRTLOGD)
591 
592     def _handle_live_migration_post_copy(self, migration_flags):
593         if CONF.libvirt.live_migration_permit_post_copy:
594             if self._is_post_copy_available():
595                 migration_flags |= libvirt.VIR_MIGRATE_POSTCOPY
596             else:
597                 LOG.info('The live_migration_permit_post_copy is set '
598                          'to True, but it is not supported.')
599         return migration_flags
600 
601     def _handle_live_migration_auto_converge(self, migration_flags):
602         if (self._is_post_copy_available() and
603                 (migration_flags & libvirt.VIR_MIGRATE_POSTCOPY) != 0):
604             LOG.info('The live_migration_permit_post_copy is set to '
605                      'True and post copy live migration is available '
606                      'so auto-converge will not be in use.')
607         elif CONF.libvirt.live_migration_permit_auto_converge:
608             migration_flags |= libvirt.VIR_MIGRATE_AUTO_CONVERGE
609         return migration_flags
610 
611     def _parse_migration_flags(self):
612         (live_migration_flags,
613             block_migration_flags) = self._prepare_migration_flags()
614 
615         live_migration_flags = self._handle_live_migration_tunnelled(
616             live_migration_flags)
617         block_migration_flags = self._handle_live_migration_tunnelled(
618             block_migration_flags)
619 
620         live_migration_flags = self._handle_live_migration_post_copy(
621             live_migration_flags)
622         block_migration_flags = self._handle_live_migration_post_copy(
623             block_migration_flags)
624 
625         live_migration_flags = self._handle_live_migration_auto_converge(
626             live_migration_flags)
627         block_migration_flags = self._handle_live_migration_auto_converge(
628             block_migration_flags)
629 
630         self._live_migration_flags = live_migration_flags
631         self._block_migration_flags = block_migration_flags
632 
633     # TODO(sahid): This method is targeted for removal when the tests
634     # have been updated to avoid its use
635     #
636     # All libvirt API calls on the libvirt.Connect object should be
637     # encapsulated by methods on the nova.virt.libvirt.host.Host
638     # object, rather than directly invoking the libvirt APIs. The goal
639     # is to avoid a direct dependency on the libvirt API from the
640     # driver.py file.
641     def _get_connection(self):
642         return self._host.get_connection()
643 
644     _conn = property(_get_connection)
645 
646     @staticmethod
647     def _uri():
648         if CONF.libvirt.virt_type == 'uml':
649             uri = CONF.libvirt.connection_uri or 'uml:///system'
650         elif CONF.libvirt.virt_type == 'xen':
651             uri = CONF.libvirt.connection_uri or 'xen:///'
652         elif CONF.libvirt.virt_type == 'lxc':
653             uri = CONF.libvirt.connection_uri or 'lxc:///'
654         elif CONF.libvirt.virt_type == 'parallels':
655             uri = CONF.libvirt.connection_uri or 'parallels:///system'
656         else:
657             uri = CONF.libvirt.connection_uri or 'qemu:///system'
658         return uri
659 
660     @staticmethod
661     def _live_migration_uri(dest):
662         uris = {
663             'kvm': 'qemu+%s://%s/system',
664             'qemu': 'qemu+%s://%s/system',
665             'xen': 'xenmigr://%s/system',
666             'parallels': 'parallels+tcp://%s/system',
667         }
668         virt_type = CONF.libvirt.virt_type
669         # TODO(pkoniszewski): Remove fetching live_migration_uri in Pike
670         uri = CONF.libvirt.live_migration_uri
671         if uri:
672             return uri % dest
673 
674         uri = uris.get(virt_type)
675         if uri is None:
676             raise exception.LiveMigrationURINotAvailable(virt_type=virt_type)
677 
678         str_format = (dest,)
679         if virt_type in ('kvm', 'qemu'):
680             scheme = CONF.libvirt.live_migration_scheme or 'tcp'
681             str_format = (scheme, dest)
682         return uris.get(virt_type) % str_format
683 
684     @staticmethod
685     def _migrate_uri(dest):
686         uri = None
687         # Only QEMU live migrations supports migrate-uri parameter
688         virt_type = CONF.libvirt.virt_type
689         if virt_type in ('qemu', 'kvm'):
690             # QEMU accept two schemes: tcp and rdma.  By default
691             # libvirt build the URI using the remote hostname and the
692             # tcp schema.
693             uri = 'tcp://%s' % dest
694         # Because dest might be of type unicode, here we might return value of
695         # type unicode as well which is not acceptable by libvirt python
696         # binding when Python 2.7 is in use, so let's convert it explicitly
697         # back to string. When Python 3.x is in use, libvirt python binding
698         # accepts unicode type so it is completely fine to do a no-op str(uri)
699         # conversion which will return value of type unicode.
700         return uri and str(uri)
701 
702     def instance_exists(self, instance):
703         """Efficient override of base instance_exists method."""
704         try:
705             self._host.get_guest(instance)
706             return True
707         except (exception.InternalError, exception.InstanceNotFound):
708             return False
709 
710     def estimate_instance_overhead(self, instance_info):
711         overhead = super(LibvirtDriver, self).estimate_instance_overhead(
712             instance_info)
713         if isinstance(instance_info, objects.Flavor):
714             # A flavor object is passed during case of migrate
715             # TODO(sahid): We do not have any way to retrieve the
716             # image meta related to the instance so if the cpu_policy
717             # has been set in image_meta we will get an
718             # exception. Until we fix it we specifically set the
719             # cpu_policy in dedicated in an ImageMeta object so if the
720             # emulator threads has been requested nothing is going to
721             # fail.
722             image_meta = objects.ImageMeta.from_dict({"properties": {
723                 "hw_cpu_policy": fields.CPUAllocationPolicy.DEDICATED,
724             }})
725             if (hardware.get_emulator_threads_constraint(
726                     instance_info, image_meta)
727                 == fields.CPUEmulatorThreadsPolicy.ISOLATE):
728                 overhead['vcpus'] += 1
729         else:
730             # An instance object is passed during case of spawing or a
731             # dict is passed when computing resource for an instance
732             numa_topology = hardware.instance_topology_from_instance(
733                 instance_info)
734             if numa_topology and numa_topology.emulator_threads_isolated:
735                 overhead['vcpus'] += 1
736         return overhead
737 
738     def list_instances(self):
739         names = []
740         for guest in self._host.list_guests(only_running=False):
741             names.append(guest.name)
742 
743         return names
744 
745     def list_instance_uuids(self):
746         uuids = []
747         for guest in self._host.list_guests(only_running=False):
748             uuids.append(guest.uuid)
749 
750         return uuids
751 
752     def plug_vifs(self, instance, network_info):
753         """Plug VIFs into networks."""
754         for vif in network_info:
755             self.vif_driver.plug(instance, vif)
756 
757     def _unplug_vifs(self, instance, network_info, ignore_errors):
758         """Unplug VIFs from networks."""
759         for vif in network_info:
760             try:
761                 self.vif_driver.unplug(instance, vif)
762             except exception.NovaException:
763                 if not ignore_errors:
764                     raise
765 
766     def unplug_vifs(self, instance, network_info):
767         self._unplug_vifs(instance, network_info, False)
768 
769     def _teardown_container(self, instance):
770         inst_path = libvirt_utils.get_instance_path(instance)
771         container_dir = os.path.join(inst_path, 'rootfs')
772         rootfs_dev = instance.system_metadata.get('rootfs_device_name')
773         LOG.debug('Attempting to teardown container at path %(dir)s with '
774                   'root device: %(rootfs_dev)s',
775                   {'dir': container_dir, 'rootfs_dev': rootfs_dev},
776                   instance=instance)
777         disk_api.teardown_container(container_dir, rootfs_dev)
778 
779     def _destroy(self, instance, attempt=1):
780         try:
781             guest = self._host.get_guest(instance)
782             if CONF.serial_console.enabled:
783                 # This method is called for several events: destroy,
784                 # rebuild, hard-reboot, power-off - For all of these
785                 # events we want to release the serial ports acquired
786                 # for the guest before destroying it.
787                 serials = self._get_serial_ports_from_guest(guest)
788                 for hostname, port in serials:
789                     serial_console.release_port(host=hostname, port=port)
790         except exception.InstanceNotFound:
791             guest = None
792 
793         # If the instance is already terminated, we're still happy
794         # Otherwise, destroy it
795         old_domid = -1
796         if guest is not None:
797             try:
798                 old_domid = guest.id
799                 guest.poweroff()
800 
801             except libvirt.libvirtError as e:
802                 is_okay = False
803                 errcode = e.get_error_code()
804                 if errcode == libvirt.VIR_ERR_NO_DOMAIN:
805                     # Domain already gone. This can safely be ignored.
806                     is_okay = True
807                 elif errcode == libvirt.VIR_ERR_OPERATION_INVALID:
808                     # If the instance is already shut off, we get this:
809                     # Code=55 Error=Requested operation is not valid:
810                     # domain is not running
811 
812                     state = guest.get_power_state(self._host)
813                     if state == power_state.SHUTDOWN:
814                         is_okay = True
815                 elif errcode == libvirt.VIR_ERR_INTERNAL_ERROR:
816                     errmsg = e.get_error_message()
817                     if (CONF.libvirt.virt_type == 'lxc' and
818                         errmsg == 'internal error: '
819                                   'Some processes refused to die'):
820                         # Some processes in the container didn't die
821                         # fast enough for libvirt. The container will
822                         # eventually die. For now, move on and let
823                         # the wait_for_destroy logic take over.
824                         is_okay = True
825                 elif errcode == libvirt.VIR_ERR_OPERATION_TIMEOUT:
826                     LOG.warning("Cannot destroy instance, operation time out",
827                                 instance=instance)
828                     reason = _("operation time out")
829                     raise exception.InstancePowerOffFailure(reason=reason)
830                 elif errcode == libvirt.VIR_ERR_SYSTEM_ERROR:
831                     if e.get_int1() == errno.EBUSY:
832                         # NOTE(danpb): When libvirt kills a process it sends it
833                         # SIGTERM first and waits 10 seconds. If it hasn't gone
834                         # it sends SIGKILL and waits another 5 seconds. If it
835                         # still hasn't gone then you get this EBUSY error.
836                         # Usually when a QEMU process fails to go away upon
837                         # SIGKILL it is because it is stuck in an
838                         # uninterruptible kernel sleep waiting on I/O from
839                         # some non-responsive server.
840                         # Given the CPU load of the gate tests though, it is
841                         # conceivable that the 15 second timeout is too short,
842                         # particularly if the VM running tempest has a high
843                         # steal time from the cloud host. ie 15 wallclock
844                         # seconds may have passed, but the VM might have only
845                         # have a few seconds of scheduled run time.
846                         LOG.warning('Error from libvirt during destroy. '
847                                     'Code=%(errcode)s Error=%(e)s; '
848                                     'attempt %(attempt)d of 3',
849                                     {'errcode': errcode, 'e': e,
850                                      'attempt': attempt},
851                                     instance=instance)
852                         with excutils.save_and_reraise_exception() as ctxt:
853                             # Try up to 3 times before giving up.
854                             if attempt < 3:
855                                 ctxt.reraise = False
856                                 self._destroy(instance, attempt + 1)
857                                 return
858 
859                 if not is_okay:
860                     with excutils.save_and_reraise_exception():
861                         LOG.error('Error from libvirt during destroy. '
862                                   'Code=%(errcode)s Error=%(e)s',
863                                   {'errcode': errcode, 'e': e},
864                                   instance=instance)
865 
866         def _wait_for_destroy(expected_domid):
867             """Called at an interval until the VM is gone."""
868             # NOTE(vish): If the instance disappears during the destroy
869             #             we ignore it so the cleanup can still be
870             #             attempted because we would prefer destroy to
871             #             never fail.
872             try:
873                 dom_info = self.get_info(instance)
874                 state = dom_info.state
875                 new_domid = dom_info.internal_id
876             except exception.InstanceNotFound:
877                 LOG.debug("During wait destroy, instance disappeared.",
878                           instance=instance)
879                 state = power_state.SHUTDOWN
880 
881             if state == power_state.SHUTDOWN:
882                 LOG.info("Instance destroyed successfully.", instance=instance)
883                 raise loopingcall.LoopingCallDone()
884 
885             # NOTE(wangpan): If the instance was booted again after destroy,
886             #                this may be an endless loop, so check the id of
887             #                domain here, if it changed and the instance is
888             #                still running, we should destroy it again.
889             # see https://bugs.launchpad.net/nova/+bug/1111213 for more details
890             if new_domid != expected_domid:
891                 LOG.info("Instance may be started again.", instance=instance)
892                 kwargs['is_running'] = True
893                 raise loopingcall.LoopingCallDone()
894 
895         kwargs = {'is_running': False}
896         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_destroy,
897                                                      old_domid)
898         timer.start(interval=0.5).wait()
899         if kwargs['is_running']:
900             LOG.info("Going to destroy instance again.", instance=instance)
901             self._destroy(instance)
902         else:
903             # NOTE(GuanQiang): teardown container to avoid resource leak
904             if CONF.libvirt.virt_type == 'lxc':
905                 self._teardown_container(instance)
906 
907     def destroy(self, context, instance, network_info, block_device_info=None,
908                 destroy_disks=True):
909         self._destroy(instance)
910         self.cleanup(context, instance, network_info, block_device_info,
911                      destroy_disks)
912 
913     def _undefine_domain(self, instance):
914         try:
915             guest = self._host.get_guest(instance)
916             try:
917                 support_uefi = self._has_uefi_support()
918                 guest.delete_configuration(support_uefi)
919             except libvirt.libvirtError as e:
920                 with excutils.save_and_reraise_exception() as ctxt:
921                     errcode = e.get_error_code()
922                     if errcode == libvirt.VIR_ERR_NO_DOMAIN:
923                         LOG.debug("Called undefine, but domain already gone.",
924                                   instance=instance)
925                         ctxt.reraise = False
926                     else:
927                         LOG.error('Error from libvirt during undefine. '
928                                   'Code=%(errcode)s Error=%(e)s',
929                                   {'errcode': errcode,
930                                    'e': encodeutils.exception_to_unicode(e)},
931                                   instance=instance)
932         except exception.InstanceNotFound:
933             pass
934 
935     def cleanup(self, context, instance, network_info, block_device_info=None,
936                 destroy_disks=True, migrate_data=None, destroy_vifs=True):
937         if destroy_vifs:
938             self._unplug_vifs(instance, network_info, True)
939 
940         # Continue attempting to remove firewall filters for the instance
941         # until it's done or there is a failure to remove the filters. If
942         # unfilter fails because the instance is not yet shutdown, try to
943         # destroy the guest again and then retry the unfilter.
944         while True:
945             try:
946                 self.unfilter_instance(instance, network_info)
947                 break
948             except libvirt.libvirtError as e:
949                 try:
950                     state = self.get_info(instance).state
951                 except exception.InstanceNotFound:
952                     state = power_state.SHUTDOWN
953 
954                 if state != power_state.SHUTDOWN:
955                     LOG.warning("Instance may be still running, destroy "
956                                 "it again.", instance=instance)
957                     self._destroy(instance)
958                 else:
959                     errcode = e.get_error_code()
960                     LOG.exception(_('Error from libvirt during unfilter. '
961                                     'Code=%(errcode)s Error=%(e)s'),
962                                   {'errcode': errcode, 'e': e},
963                                   instance=instance)
964                     reason = _("Error unfiltering instance.")
965                     raise exception.InstanceTerminationFailure(reason=reason)
966             except Exception:
967                 raise
968 
969         # FIXME(wangpan): if the instance is booted again here, such as the
970         #                 soft reboot operation boot it here, it will become
971         #                 "running deleted", should we check and destroy it
972         #                 at the end of this method?
973 
974         # NOTE(vish): we disconnect from volumes regardless
975         block_device_mapping = driver.block_device_info_get_mapping(
976             block_device_info)
977         for vol in block_device_mapping:
978             connection_info = vol['connection_info']
979             disk_dev = vol['mount_device']
980             if disk_dev is not None:
981                 disk_dev = disk_dev.rpartition("/")[2]
982 
983             if ('data' in connection_info and
984                     'volume_id' in connection_info['data']):
985                 volume_id = connection_info['data']['volume_id']
986                 encryption = encryptors.get_encryption_metadata(
987                     context, self._volume_api, volume_id, connection_info)
988 
989                 if encryption:
990                     # The volume must be detached from the VM before
991                     # disconnecting it from its encryptor. Otherwise, the
992                     # encryptor may report that the volume is still in use.
993                     encryptor = self._get_volume_encryptor(connection_info,
994                                                            encryption)
995                     encryptor.detach_volume(**encryption)
996 
997             try:
998                 self._disconnect_volume(connection_info, instance)
999             except Exception as exc:
1000                 with excutils.save_and_reraise_exception() as ctxt:
1001                     if destroy_disks:
1002                         # Don't block on Volume errors if we're trying to
1003                         # delete the instance as we may be partially created
1004                         # or deleted
1005                         ctxt.reraise = False
1006                         LOG.warning(
1007                             "Ignoring Volume Error on vol %(vol_id)s "
1008                             "during delete %(exc)s",
1009                             {'vol_id': vol.get('volume_id'),
1010                              'exc': encodeutils.exception_to_unicode(exc)},
1011                             instance=instance)
1012 
1013         if destroy_disks:
1014             # NOTE(haomai): destroy volumes if needed
1015             if CONF.libvirt.images_type == 'lvm':
1016                 self._cleanup_lvm(instance, block_device_info)
1017             if CONF.libvirt.images_type == 'rbd':
1018                 self._cleanup_rbd(instance)
1019 
1020         is_shared_block_storage = False
1021         if migrate_data and 'is_shared_block_storage' in migrate_data:
1022             is_shared_block_storage = migrate_data.is_shared_block_storage
1023         if destroy_disks or is_shared_block_storage:
1024             attempts = int(instance.system_metadata.get('clean_attempts',
1025                                                         '0'))
1026             success = self.delete_instance_files(instance)
1027             # NOTE(mriedem): This is used in the _run_pending_deletes periodic
1028             # task in the compute manager. The tight coupling is not great...
1029             instance.system_metadata['clean_attempts'] = str(attempts + 1)
1030             if success:
1031                 instance.cleaned = True
1032             instance.save()
1033 
1034         self._undefine_domain(instance)
1035 
1036     def _detach_encrypted_volumes(self, instance, block_device_info):
1037         """Detaches encrypted volumes attached to instance."""
1038         disks = self._get_instance_disk_info(instance, block_device_info)
1039         encrypted_volumes = filter(dmcrypt.is_encrypted,
1040                                    [disk['path'] for disk in disks])
1041         for path in encrypted_volumes:
1042             dmcrypt.delete_volume(path)
1043 
1044     def _get_serial_ports_from_guest(self, guest, mode=None):
1045         """Returns an iterator over serial port(s) configured on guest.
1046 
1047         :param mode: Should be a value in (None, bind, connect)
1048         """
1049         xml = guest.get_xml_desc()
1050         tree = etree.fromstring(xml)
1051 
1052         # The 'serial' device is the base for x86 platforms. Other platforms
1053         # (e.g. kvm on system z = S390X) can only use 'console' devices.
1054         xpath_mode = "[@mode='%s']" % mode if mode else ""
1055         serial_tcp = "./devices/serial[@type='tcp']/source" + xpath_mode
1056         console_tcp = "./devices/console[@type='tcp']/source" + xpath_mode
1057 
1058         tcp_devices = tree.findall(serial_tcp)
1059         if len(tcp_devices) == 0:
1060             tcp_devices = tree.findall(console_tcp)
1061         for source in tcp_devices:
1062             yield (source.get("host"), int(source.get("service")))
1063 
1064     def _get_scsi_controller_max_unit(self, guest):
1065         """Returns the max disk unit used by scsi controller"""
1066         xml = guest.get_xml_desc()
1067         tree = etree.fromstring(xml)
1068         addrs = "./devices/disk[@device='disk']/address[@type='drive']"
1069 
1070         ret = []
1071         for obj in tree.findall(addrs):
1072             ret.append(int(obj.get('unit', 0)))
1073         return max(ret)
1074 
1075     @staticmethod
1076     def _get_rbd_driver():
1077         return rbd_utils.RBDDriver(
1078                 pool=CONF.libvirt.images_rbd_pool,
1079                 ceph_conf=CONF.libvirt.images_rbd_ceph_conf,
1080                 rbd_user=CONF.libvirt.rbd_user)
1081 
1082     def _cleanup_rbd(self, instance):
1083         # NOTE(nic): On revert_resize, the cleanup steps for the root
1084         # volume are handled with an "rbd snap rollback" command,
1085         # and none of this is needed (and is, in fact, harmful) so
1086         # filter out non-ephemerals from the list
1087         if instance.task_state == task_states.RESIZE_REVERTING:
1088             filter_fn = lambda disk: (disk.startswith(instance.uuid) and
1089                                       disk.endswith('disk.local'))
1090         else:
1091             filter_fn = lambda disk: disk.startswith(instance.uuid)
1092         LibvirtDriver._get_rbd_driver().cleanup_volumes(filter_fn)
1093 
1094     def _cleanup_lvm(self, instance, block_device_info):
1095         """Delete all LVM disks for given instance object."""
1096         if instance.get('ephemeral_key_uuid') is not None:
1097             self._detach_encrypted_volumes(instance, block_device_info)
1098 
1099         disks = self._lvm_disks(instance)
1100         if disks:
1101             lvm.remove_volumes(disks)
1102 
1103     def _lvm_disks(self, instance):
1104         """Returns all LVM disks for given instance object."""
1105         if CONF.libvirt.images_volume_group:
1106             vg = os.path.join('/dev', CONF.libvirt.images_volume_group)
1107             if not os.path.exists(vg):
1108                 return []
1109             pattern = '%s_' % instance.uuid
1110 
1111             def belongs_to_instance(disk):
1112                 return disk.startswith(pattern)
1113 
1114             def fullpath(name):
1115                 return os.path.join(vg, name)
1116 
1117             logical_volumes = lvm.list_volumes(vg)
1118 
1119             disks = [fullpath(disk) for disk in logical_volumes
1120                      if belongs_to_instance(disk)]
1121             return disks
1122         return []
1123 
1124     def get_volume_connector(self, instance):
1125         root_helper = utils.get_root_helper()
1126         return connector.get_connector_properties(
1127             root_helper, CONF.my_block_storage_ip,
1128             CONF.libvirt.volume_use_multipath,
1129             enforce_multipath=True,
1130             host=CONF.host)
1131 
1132     def _cleanup_resize(self, context, instance, network_info):
1133         inst_base = libvirt_utils.get_instance_path(instance)
1134         target = inst_base + '_resize'
1135 
1136         if os.path.exists(target):
1137             # Deletion can fail over NFS, so retry the deletion as required.
1138             # Set maximum attempt as 5, most test can remove the directory
1139             # for the second time.
1140             utils.execute('rm', '-rf', target, delay_on_retry=True,
1141                           attempts=5)
1142 
1143         root_disk = self.image_backend.by_name(instance, 'disk')
1144         # TODO(nic): Set ignore_errors=False in a future release.
1145         # It is set to True here to avoid any upgrade issues surrounding
1146         # instances being in pending resize state when the software is updated;
1147         # in that case there will be no snapshot to remove.  Once it can be
1148         # reasonably assumed that no such instances exist in the wild
1149         # anymore, it should be set back to False (the default) so it will
1150         # throw errors, like it should.
1151         if root_disk.exists():
1152             root_disk.remove_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME,
1153                                   ignore_errors=True)
1154 
1155         # NOTE(mjozefcz):
1156         # self.image_backend.image for some backends recreates instance
1157         # directory and image disk.info - remove it here if exists
1158         # Do not remove inst_base for volume-backed instances since that
1159         # could potentially remove the files on the destination host
1160         # if using shared storage.
1161         if (os.path.exists(inst_base) and not root_disk.exists() and
1162                 not compute_utils.is_volume_backed_instance(
1163                     context, instance)):
1164             try:
1165                 shutil.rmtree(inst_base)
1166             except OSError as e:
1167                 if e.errno != errno.ENOENT:
1168                     raise
1169 
1170         if instance.host != CONF.host:
1171             self._undefine_domain(instance)
1172             self.unplug_vifs(instance, network_info)
1173             self.unfilter_instance(instance, network_info)
1174 
1175     def _get_volume_driver(self, connection_info):
1176         driver_type = connection_info.get('driver_volume_type')
1177         if driver_type not in self.volume_drivers:
1178             raise exception.VolumeDriverNotFound(driver_type=driver_type)
1179         return self.volume_drivers[driver_type]
1180 
1181     def _connect_volume(self, connection_info, instance):
1182         vol_driver = self._get_volume_driver(connection_info)
1183         vol_driver.connect_volume(connection_info, instance)
1184 
1185     def _disconnect_volume(self, connection_info, instance):
1186         vol_driver = self._get_volume_driver(connection_info)
1187         vol_driver.disconnect_volume(connection_info, instance)
1188 
1189     def _extend_volume(self, connection_info, instance):
1190         vol_driver = self._get_volume_driver(connection_info)
1191         return vol_driver.extend_volume(connection_info, instance)
1192 
1193     def _get_volume_config(self, connection_info, disk_info):
1194         vol_driver = self._get_volume_driver(connection_info)
1195         conf = vol_driver.get_config(connection_info, disk_info)
1196         self._set_cache_mode(conf)
1197         return conf
1198 
1199     def _get_volume_encryptor(self, connection_info, encryption):
1200         root_helper = utils.get_root_helper()
1201         return encryptors.get_volume_encryptor(root_helper=root_helper,
1202                                                keymgr=key_manager.API(CONF),
1203                                                connection_info=connection_info,
1204                                                **encryption)
1205 
1206     def _check_discard_for_attach_volume(self, conf, instance):
1207         """Perform some checks for volumes configured for discard support.
1208 
1209         If discard is configured for the volume, and the guest is using a
1210         configuration known to not work, we will log a message explaining
1211         the reason why.
1212         """
1213         if conf.driver_discard == 'unmap' and conf.target_bus == 'virtio':
1214             LOG.debug('Attempting to attach volume %(id)s with discard '
1215                       'support enabled to an instance using an '
1216                       'unsupported configuration. target_bus = '
1217                       '%(bus)s. Trim commands will not be issued to '
1218                       'the storage device.',
1219                       {'bus': conf.target_bus,
1220                        'id': conf.serial},
1221                       instance=instance)
1222 
1223     def attach_volume(self, context, connection_info, instance, mountpoint,
1224                       disk_bus=None, device_type=None, encryption=None):
1225         guest = self._host.get_guest(instance)
1226 
1227         disk_dev = mountpoint.rpartition("/")[2]
1228         bdm = {
1229             'device_name': disk_dev,
1230             'disk_bus': disk_bus,
1231             'device_type': device_type}
1232 
1233         # Note(cfb): If the volume has a custom block size, check that
1234         #            that we are using QEMU/KVM and libvirt >= 0.10.2. The
1235         #            presence of a block size is considered mandatory by
1236         #            cinder so we fail if we can't honor the request.
1237         data = {}
1238         if ('data' in connection_info):
1239             data = connection_info['data']
1240         if ('logical_block_size' in data or 'physical_block_size' in data):
1241             if ((CONF.libvirt.virt_type != "kvm" and
1242                  CONF.libvirt.virt_type != "qemu")):
1243                 msg = _("Volume sets block size, but the current "
1244                         "libvirt hypervisor '%s' does not support custom "
1245                         "block size") % CONF.libvirt.virt_type
1246                 raise exception.InvalidHypervisorType(msg)
1247 
1248         self._connect_volume(connection_info, instance)
1249         disk_info = blockinfo.get_info_from_bdm(
1250             instance, CONF.libvirt.virt_type, instance.image_meta, bdm)
1251         if disk_info['bus'] == 'scsi':
1252             disk_info['unit'] = self._get_scsi_controller_max_unit(guest) + 1
1253 
1254         conf = self._get_volume_config(connection_info, disk_info)
1255 
1256         self._check_discard_for_attach_volume(conf, instance)
1257 
1258         try:
1259             state = guest.get_power_state(self._host)
1260             live = state in (power_state.RUNNING, power_state.PAUSED)
1261 
1262             if encryption:
1263                 encryptor = self._get_volume_encryptor(connection_info,
1264                                                        encryption)
1265                 encryptor.attach_volume(context, **encryption)
1266 
1267             guest.attach_device(conf, persistent=True, live=live)
1268             # NOTE(artom) If we're attaching with a device role tag, we need to
1269             # rebuild device_metadata. If we're attaching without a role
1270             # tag, we're rebuilding it here needlessly anyways. This isn't a
1271             # massive deal, and it helps reduce code complexity by not having
1272             # to indicate to the virt driver that the attach is tagged. The
1273             # really important optimization of not calling the database unless
1274             # device_metadata has actually changed is done for us by
1275             # instance.save().
1276             instance.device_metadata = self._build_device_metadata(
1277                 context, instance)
1278             instance.save()
1279         except Exception:
1280             LOG.exception(_('Failed to attach volume at mountpoint: %s'),
1281                           mountpoint, instance=instance)
1282             with excutils.save_and_reraise_exception():
1283                 self._disconnect_volume(connection_info, instance)
1284 
1285     def _swap_volume(self, guest, disk_path, conf, resize_to):
1286         """Swap existing disk with a new block device."""
1287         dev = guest.get_block_device(disk_path)
1288 
1289         # Save a copy of the domain's persistent XML file. We'll use this
1290         # to redefine the domain if anything fails during the volume swap.
1291         xml = guest.get_xml_desc(dump_inactive=True, dump_sensitive=True)
1292 
1293         # Abort is an idempotent operation, so make sure any block
1294         # jobs which may have failed are ended.
1295         try:
1296             dev.abort_job()
1297         except Exception:
1298             pass
1299 
1300         try:
1301             # NOTE (rmk): blockRebase cannot be executed on persistent
1302             #             domains, so we need to temporarily undefine it.
1303             #             If any part of this block fails, the domain is
1304             #             re-defined regardless.
1305             if guest.has_persistent_configuration():
1306                 support_uefi = self._has_uefi_support()
1307                 guest.delete_configuration(support_uefi)
1308 
1309             try:
1310                 # Start copy with VIR_DOMAIN_BLOCK_REBASE_REUSE_EXT flag to
1311                 # allow writing to existing external volume file. Use
1312                 # VIR_DOMAIN_BLOCK_REBASE_COPY_DEV if it's a block device to
1313                 # make sure XML is generated correctly (bug 1691195)
1314                 copy_dev = conf.source_type == 'block'
1315                 dev.rebase(conf.source_path, copy=True, reuse_ext=True,
1316                            copy_dev=copy_dev)
1317                 while not dev.is_job_complete():
1318                     time.sleep(0.5)
1319 
1320                 dev.abort_job(pivot=True)
1321 
1322             except Exception as exc:
1323                 LOG.exception("Failure rebasing volume %(new_path)s on "
1324                     "%(old_path)s.", {'new_path': conf.source_path,
1325                                       'old_path': disk_path})
1326                 raise exception.VolumeRebaseFailed(reason=six.text_type(exc))
1327 
1328             if resize_to:
1329                 dev.resize(resize_to * units.Gi / units.Ki)
1330 
1331             # Make sure we will redefine the domain using the updated
1332             # configuration after the volume was swapped. The dump_inactive
1333             # keyword arg controls whether we pull the inactive (persistent)
1334             # or active (live) config from the domain. We want to pull the
1335             # live config after the volume was updated to use when we redefine
1336             # the domain.
1337             xml = guest.get_xml_desc(dump_inactive=False, dump_sensitive=True)
1338         finally:
1339             self._host.write_instance_config(xml)
1340 
1341     def swap_volume(self, old_connection_info,
1342                     new_connection_info, instance, mountpoint, resize_to):
1343 
1344         guest = self._host.get_guest(instance)
1345 
1346         disk_dev = mountpoint.rpartition("/")[2]
1347         if not guest.get_disk(disk_dev):
1348             raise exception.DiskNotFound(location=disk_dev)
1349         disk_info = {
1350             'dev': disk_dev,
1351             'bus': blockinfo.get_disk_bus_for_disk_dev(
1352                 CONF.libvirt.virt_type, disk_dev),
1353             'type': 'disk',
1354             }
1355         # NOTE (lyarwood): new_connection_info will be modified by the
1356         # following _connect_volume call down into the volume drivers. The
1357         # majority of the volume drivers will add a device_path that is in turn
1358         # used by _get_volume_config to set the source_path of the
1359         # LibvirtConfigGuestDisk object it returns. We do not explicitly save
1360         # this to the BDM here as the upper compute swap_volume method will
1361         # eventually do this for us.
1362         self._connect_volume(new_connection_info, instance)
1363         conf = self._get_volume_config(new_connection_info, disk_info)
1364         if not conf.source_path:
1365             self._disconnect_volume(new_connection_info, instance)
1366             raise NotImplementedError(_("Swap only supports host devices"))
1367 
1368         try:
1369             self._swap_volume(guest, disk_dev, conf, resize_to)
1370         except exception.VolumeRebaseFailed:
1371             with excutils.save_and_reraise_exception():
1372                 self._disconnect_volume(new_connection_info, instance)
1373 
1374         self._disconnect_volume(old_connection_info, instance)
1375 
1376     def _get_existing_domain_xml(self, instance, network_info,
1377                                  block_device_info=None):
1378         try:
1379             guest = self._host.get_guest(instance)
1380             xml = guest.get_xml_desc()
1381         except exception.InstanceNotFound:
1382             disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
1383                                                 instance,
1384                                                 instance.image_meta,
1385                                                 block_device_info)
1386             xml = self._get_guest_xml(nova_context.get_admin_context(),
1387                                       instance, network_info, disk_info,
1388                                       instance.image_meta,
1389                                       block_device_info=block_device_info)
1390         return xml
1391 
1392     def detach_volume(self, connection_info, instance, mountpoint,
1393                       encryption=None):
1394         disk_dev = mountpoint.rpartition("/")[2]
1395         try:
1396             guest = self._host.get_guest(instance)
1397 
1398             state = guest.get_power_state(self._host)
1399             live = state in (power_state.RUNNING, power_state.PAUSED)
1400 
1401             # The volume must be detached from the VM before disconnecting it
1402             # from its encryptor. Otherwise, the encryptor may report that the
1403             # volume is still in use.
1404             wait_for_detach = guest.detach_device_with_retry(guest.get_disk,
1405                                                              disk_dev,
1406                                                              live=live)
1407             wait_for_detach()
1408 
1409             if encryption:
1410                 encryptor = self._get_volume_encryptor(connection_info,
1411                                                        encryption)
1412                 encryptor.detach_volume(**encryption)
1413 
1414         except exception.InstanceNotFound:
1415             # NOTE(zhaoqin): If the instance does not exist, _lookup_by_name()
1416             #                will throw InstanceNotFound exception. Need to
1417             #                disconnect volume under this circumstance.
1418             LOG.warning("During detach_volume, instance disappeared.",
1419                         instance=instance)
1420         except exception.DeviceNotFound:
1421             raise exception.DiskNotFound(location=disk_dev)
1422         except libvirt.libvirtError as ex:
1423             # NOTE(vish): This is called to cleanup volumes after live
1424             #             migration, so we should still disconnect even if
1425             #             the instance doesn't exist here anymore.
1426             error_code = ex.get_error_code()
1427             if error_code == libvirt.VIR_ERR_NO_DOMAIN:
1428                 # NOTE(vish):
1429                 LOG.warning("During detach_volume, instance disappeared.",
1430                             instance=instance)
1431             else:
1432                 raise
1433 
1434         self._disconnect_volume(connection_info, instance)
1435 
1436     def extend_volume(self, connection_info, instance):
1437         try:
1438             new_size = self._extend_volume(connection_info, instance)
1439         except NotImplementedError:
1440             raise exception.ExtendVolumeNotSupported()
1441 
1442         # Resize the device in QEMU so its size is updated and
1443         # detected by the instance without rebooting.
1444         try:
1445             guest = self._host.get_guest(instance)
1446             state = guest.get_power_state(self._host)
1447             active_state = state in (power_state.RUNNING, power_state.PAUSED)
1448             if active_state:
1449                 disk_path = connection_info['data']['device_path']
1450                 LOG.debug('resizing block device %(dev)s to %(size)u kb',
1451                           {'dev': disk_path, 'size': new_size})
1452                 dev = guest.get_block_device(disk_path)
1453                 dev.resize(new_size // units.Ki)
1454             else:
1455                 LOG.debug('Skipping block device resize, guest is not running',
1456                           instance=instance)
1457         except exception.InstanceNotFound:
1458             with excutils.save_and_reraise_exception():
1459                 LOG.warning('During extend_volume, instance disappeared.',
1460                             instance=instance)
1461         except libvirt.libvirtError:
1462             with excutils.save_and_reraise_exception():
1463                 LOG.exception('resizing block device failed.',
1464                               instance=instance)
1465 
1466     def attach_interface(self, context, instance, image_meta, vif):
1467         guest = self._host.get_guest(instance)
1468 
1469         self.vif_driver.plug(instance, vif)
1470         self.firewall_driver.setup_basic_filtering(instance, [vif])
1471         cfg = self.vif_driver.get_config(instance, vif, image_meta,
1472                                          instance.flavor,
1473                                          CONF.libvirt.virt_type,
1474                                          self._host)
1475         try:
1476             state = guest.get_power_state(self._host)
1477             live = state in (power_state.RUNNING, power_state.PAUSED)
1478             guest.attach_device(cfg, persistent=True, live=live)
1479         except libvirt.libvirtError:
1480             LOG.error('attaching network adapter failed.',
1481                       instance=instance, exc_info=True)
1482             self.vif_driver.unplug(instance, vif)
1483             raise exception.InterfaceAttachFailed(
1484                     instance_uuid=instance.uuid)
1485         try:
1486             # NOTE(artom) If we're attaching with a device role tag, we need to
1487             # rebuild device_metadata. If we're attaching without a role
1488             # tag, we're rebuilding it here needlessly anyways. This isn't a
1489             # massive deal, and it helps reduce code complexity by not having
1490             # to indicate to the virt driver that the attach is tagged. The
1491             # really important optimization of not calling the database unless
1492             # device_metadata has actually changed is done for us by
1493             # instance.save().
1494             instance.device_metadata = self._build_device_metadata(
1495                 context, instance)
1496             instance.save()
1497         except Exception:
1498             # NOTE(artom) If we fail here it means the interface attached
1499             # successfully but building and/or saving the device metadata
1500             # failed. Just unplugging the vif is therefore not enough cleanup,
1501             # we need to detach the interface.
1502             with excutils.save_and_reraise_exception(reraise=False):
1503                 LOG.error('Interface attached successfully but building '
1504                           'and/or saving device metadata failed.',
1505                           instance=instance, exc_info=True)
1506                 self.detach_interface(context, instance, vif)
1507                 raise exception.InterfaceAttachFailed(
1508                     instance_uuid=instance.uuid)
1509 
1510     def detach_interface(self, context, instance, vif):
1511         guest = self._host.get_guest(instance)
1512         cfg = self.vif_driver.get_config(instance, vif,
1513                                          instance.image_meta,
1514                                          instance.flavor,
1515                                          CONF.libvirt.virt_type, self._host)
1516         interface = guest.get_interface_by_cfg(cfg)
1517         try:
1518             self.vif_driver.unplug(instance, vif)
1519             # NOTE(mriedem): When deleting an instance and using Neutron,
1520             # we can be racing against Neutron deleting the port and
1521             # sending the vif-deleted event which then triggers a call to
1522             # detach the interface, so if the interface is not found then
1523             # we can just log it as a warning.
1524             if not interface:
1525                 mac = vif.get('address')
1526                 # The interface is gone so just log it as a warning.
1527                 LOG.warning('Detaching interface %(mac)s failed because '
1528                             'the device is no longer found on the guest.',
1529                             {'mac': mac}, instance=instance)
1530                 return
1531 
1532             state = guest.get_power_state(self._host)
1533             live = state in (power_state.RUNNING, power_state.PAUSED)
1534             # Now we are going to loop until the interface is detached or we
1535             # timeout.
1536             wait_for_detach = guest.detach_device_with_retry(
1537                 guest.get_interface_by_cfg, cfg, live=live,
1538                 alternative_device_name=self.vif_driver.get_vif_devname(vif))
1539             wait_for_detach()
1540         except exception.DeviceDetachFailed:
1541             # We failed to detach the device even with the retry loop, so let's
1542             # dump some debug information to the logs before raising back up.
1543             with excutils.save_and_reraise_exception():
1544                 devname = self.vif_driver.get_vif_devname(vif)
1545                 interface = guest.get_interface_by_cfg(cfg)
1546                 if interface:
1547                     LOG.warning(
1548                         'Failed to detach interface %(devname)s after '
1549                         'repeated attempts. Final interface xml:\n'
1550                         '%(interface_xml)s\nFinal guest xml:\n%(guest_xml)s',
1551                         {'devname': devname,
1552                          'interface_xml': interface.to_xml(),
1553                          'guest_xml': guest.get_xml_desc()},
1554                         instance=instance)
1555         except exception.DeviceNotFound:
1556             # The interface is gone so just log it as a warning.
1557             LOG.warning('Detaching interface %(mac)s failed because '
1558                         'the device is no longer found on the guest.',
1559                         {'mac': vif.get('address')}, instance=instance)
1560         except libvirt.libvirtError as ex:
1561             error_code = ex.get_error_code()
1562             if error_code == libvirt.VIR_ERR_NO_DOMAIN:
1563                 LOG.warning("During detach_interface, instance disappeared.",
1564                             instance=instance)
1565             else:
1566                 # NOTE(mriedem): When deleting an instance and using Neutron,
1567                 # we can be racing against Neutron deleting the port and
1568                 # sending the vif-deleted event which then triggers a call to
1569                 # detach the interface, so we might have failed because the
1570                 # network device no longer exists. Libvirt will fail with
1571                 # "operation failed: no matching network device was found"
1572                 # which unfortunately does not have a unique error code so we
1573                 # need to look up the interface by config and if it's not found
1574                 # then we can just log it as a warning rather than tracing an
1575                 # error.
1576                 mac = vif.get('address')
1577                 interface = guest.get_interface_by_cfg(cfg)
1578                 if interface:
1579                     LOG.error('detaching network adapter failed.',
1580                               instance=instance, exc_info=True)
1581                     raise exception.InterfaceDetachFailed(
1582                             instance_uuid=instance.uuid)
1583 
1584                 # The interface is gone so just log it as a warning.
1585                 LOG.warning('Detaching interface %(mac)s failed because '
1586                             'the device is no longer found on the guest.',
1587                             {'mac': mac}, instance=instance)
1588 
1589     def _create_snapshot_metadata(self, image_meta, instance,
1590                                   img_fmt, snp_name):
1591         metadata = {'is_public': False,
1592                     'status': 'active',
1593                     'name': snp_name,
1594                     'properties': {
1595                                    'kernel_id': instance.kernel_id,
1596                                    'image_location': 'snapshot',
1597                                    'image_state': 'available',
1598                                    'owner_id': instance.project_id,
1599                                    'ramdisk_id': instance.ramdisk_id,
1600                                    }
1601                     }
1602         if instance.os_type:
1603             metadata['properties']['os_type'] = instance.os_type
1604 
1605         # NOTE(vish): glance forces ami disk format to be ami
1606         if image_meta.disk_format == 'ami':
1607             metadata['disk_format'] = 'ami'
1608         else:
1609             metadata['disk_format'] = img_fmt
1610 
1611         if image_meta.obj_attr_is_set("container_format"):
1612             metadata['container_format'] = image_meta.container_format
1613         else:
1614             metadata['container_format'] = "bare"
1615 
1616         return metadata
1617 
1618     def snapshot(self, context, instance, image_id, update_task_state):
1619         """Create snapshot from a running VM instance.
1620 
1621         This command only works with qemu 0.14+
1622         """
1623         try:
1624             guest = self._host.get_guest(instance)
1625 
1626             # TODO(sahid): We are converting all calls from a
1627             # virDomain object to use nova.virt.libvirt.Guest.
1628             # We should be able to remove virt_dom at the end.
1629             virt_dom = guest._domain
1630         except exception.InstanceNotFound:
1631             raise exception.InstanceNotRunning(instance_id=instance.uuid)
1632 
1633         snapshot = self._image_api.get(context, image_id)
1634 
1635         # source_format is an on-disk format
1636         # source_type is a backend type
1637         disk_path, source_format = libvirt_utils.find_disk(guest)
1638         source_type = libvirt_utils.get_disk_type_from_path(disk_path)
1639 
1640         # We won't have source_type for raw or qcow2 disks, because we can't
1641         # determine that from the path. We should have it from the libvirt
1642         # xml, though.
1643         if source_type is None:
1644             source_type = source_format
1645         # For lxc instances we won't have it either from libvirt xml
1646         # (because we just gave libvirt the mounted filesystem), or the path,
1647         # so source_type is still going to be None. In this case,
1648         # root_disk is going to default to CONF.libvirt.images_type
1649         # below, which is still safe.
1650 
1651         image_format = CONF.libvirt.snapshot_image_format or source_type
1652 
1653         # NOTE(bfilippov): save lvm and rbd as raw
1654         if image_format == 'lvm' or image_format == 'rbd':
1655             image_format = 'raw'
1656 
1657         metadata = self._create_snapshot_metadata(instance.image_meta,
1658                                                   instance,
1659                                                   image_format,
1660                                                   snapshot['name'])
1661 
1662         snapshot_name = uuidutils.generate_uuid(dashed=False)
1663 
1664         state = guest.get_power_state(self._host)
1665 
1666         # NOTE(dgenin): Instances with LVM encrypted ephemeral storage require
1667         #               cold snapshots. Currently, checking for encryption is
1668         #               redundant because LVM supports only cold snapshots.
1669         #               It is necessary in case this situation changes in the
1670         #               future.
1671         if (self._host.has_min_version(hv_type=host.HV_DRIVER_QEMU)
1672              and source_type not in ('lvm')
1673              and not CONF.ephemeral_storage_encryption.enabled
1674              and not CONF.workarounds.disable_libvirt_livesnapshot):
1675             live_snapshot = True
1676             # Abort is an idempotent operation, so make sure any block
1677             # jobs which may have failed are ended. This operation also
1678             # confirms the running instance, as opposed to the system as a
1679             # whole, has a new enough version of the hypervisor (bug 1193146).
1680             try:
1681                 guest.get_block_device(disk_path).abort_job()
1682             except libvirt.libvirtError as ex:
1683                 error_code = ex.get_error_code()
1684                 if error_code == libvirt.VIR_ERR_CONFIG_UNSUPPORTED:
1685                     live_snapshot = False
1686                 else:
1687                     pass
1688         else:
1689             live_snapshot = False
1690 
1691         # NOTE(rmk): We cannot perform live snapshots when a managedSave
1692         #            file is present, so we will use the cold/legacy method
1693         #            for instances which are shutdown.
1694         if state == power_state.SHUTDOWN:
1695             live_snapshot = False
1696 
1697         self._prepare_domain_for_snapshot(context, live_snapshot, state,
1698                                           instance)
1699 
1700         root_disk = self.image_backend.by_libvirt_path(
1701             instance, disk_path, image_type=source_type)
1702 
1703         if live_snapshot:
1704             LOG.info("Beginning live snapshot process", instance=instance)
1705         else:
1706             LOG.info("Beginning cold snapshot process", instance=instance)
1707 
1708         update_task_state(task_state=task_states.IMAGE_PENDING_UPLOAD)
1709 
1710         try:
1711             update_task_state(task_state=task_states.IMAGE_UPLOADING,
1712                               expected_state=task_states.IMAGE_PENDING_UPLOAD)
1713             metadata['location'] = root_disk.direct_snapshot(
1714                 context, snapshot_name, image_format, image_id,
1715                 instance.image_ref)
1716             self._snapshot_domain(context, live_snapshot, virt_dom, state,
1717                                   instance)
1718             self._image_api.update(context, image_id, metadata,
1719                                    purge_props=False)
1720         except (NotImplementedError, exception.ImageUnacceptable,
1721                 exception.Forbidden) as e:
1722             if type(e) != NotImplementedError:
1723                 LOG.warning('Performing standard snapshot because direct '
1724                             'snapshot failed: %(error)s',
1725                             {'error': encodeutils.exception_to_unicode(e)})
1726             failed_snap = metadata.pop('location', None)
1727             if failed_snap:
1728                 failed_snap = {'url': str(failed_snap)}
1729             root_disk.cleanup_direct_snapshot(failed_snap,
1730                                                   also_destroy_volume=True,
1731                                                   ignore_errors=True)
1732             update_task_state(task_state=task_states.IMAGE_PENDING_UPLOAD,
1733                               expected_state=task_states.IMAGE_UPLOADING)
1734 
1735             # TODO(nic): possibly abstract this out to the root_disk
1736             if source_type == 'rbd' and live_snapshot:
1737                 # Standard snapshot uses qemu-img convert from RBD which is
1738                 # not safe to run with live_snapshot.
1739                 live_snapshot = False
1740                 # Suspend the guest, so this is no longer a live snapshot
1741                 self._prepare_domain_for_snapshot(context, live_snapshot,
1742                                                   state, instance)
1743 
1744             snapshot_directory = CONF.libvirt.snapshots_directory
1745             fileutils.ensure_tree(snapshot_directory)
1746             with utils.tempdir(dir=snapshot_directory) as tmpdir:
1747                 try:
1748                     out_path = os.path.join(tmpdir, snapshot_name)
1749                     if live_snapshot:
1750                         # NOTE(xqueralt): libvirt needs o+x in the tempdir
1751                         os.chmod(tmpdir, 0o701)
1752                         self._live_snapshot(context, instance, guest,
1753                                             disk_path, out_path, source_format,
1754                                             image_format, instance.image_meta)
1755                     else:
1756                         root_disk.snapshot_extract(out_path, image_format)
1757                     LOG.info("Snapshot extracted, beginning image upload",
1758                              instance=instance)
1759                 finally:
1760                     self._snapshot_domain(context, live_snapshot, virt_dom,
1761                                           state, instance)
1762 
1763                 # Upload that image to the image service
1764                 update_task_state(task_state=task_states.IMAGE_UPLOADING,
1765                         expected_state=task_states.IMAGE_PENDING_UPLOAD)
1766                 with libvirt_utils.file_open(out_path, 'rb') as image_file:
1767                     self._image_api.update(context,
1768                                            image_id,
1769                                            metadata,
1770                                            image_file)
1771         except Exception:
1772             with excutils.save_and_reraise_exception():
1773                 LOG.exception(_("Failed to snapshot image"))
1774                 failed_snap = metadata.pop('location', None)
1775                 if failed_snap:
1776                     failed_snap = {'url': str(failed_snap)}
1777                 root_disk.cleanup_direct_snapshot(
1778                         failed_snap, also_destroy_volume=True,
1779                         ignore_errors=True)
1780 
1781         LOG.info("Snapshot image upload complete", instance=instance)
1782 
1783     def _prepare_domain_for_snapshot(self, context, live_snapshot, state,
1784                                      instance):
1785         # NOTE(dkang): managedSave does not work for LXC
1786         if CONF.libvirt.virt_type != 'lxc' and not live_snapshot:
1787             if state == power_state.RUNNING or state == power_state.PAUSED:
1788                 self.suspend(context, instance)
1789 
1790     def _snapshot_domain(self, context, live_snapshot, virt_dom, state,
1791                          instance):
1792         guest = None
1793         # NOTE(dkang): because previous managedSave is not called
1794         #              for LXC, _create_domain must not be called.
1795         if CONF.libvirt.virt_type != 'lxc' and not live_snapshot:
1796             if state == power_state.RUNNING:
1797                 guest = self._create_domain(domain=virt_dom)
1798             elif state == power_state.PAUSED:
1799                 guest = self._create_domain(domain=virt_dom, pause=True)
1800 
1801             if guest is not None:
1802                 self._attach_pci_devices(
1803                     guest, pci_manager.get_instance_pci_devs(instance))
1804                 self._attach_direct_passthrough_ports(
1805                     context, instance, guest)
1806 
1807     def _can_set_admin_password(self, image_meta):
1808 
1809         if CONF.libvirt.virt_type == 'parallels':
1810             if not self._host.has_min_version(
1811                    MIN_LIBVIRT_PARALLELS_SET_ADMIN_PASSWD):
1812                 raise exception.SetAdminPasswdNotSupported()
1813         elif CONF.libvirt.virt_type in ('kvm', 'qemu'):
1814             if not self._host.has_min_version(
1815                    MIN_LIBVIRT_SET_ADMIN_PASSWD):
1816                 raise exception.SetAdminPasswdNotSupported()
1817             if not image_meta.properties.get('hw_qemu_guest_agent', False):
1818                 raise exception.QemuGuestAgentNotEnabled()
1819         else:
1820             raise exception.SetAdminPasswdNotSupported()
1821 
1822     def set_admin_password(self, instance, new_pass):
1823         self._can_set_admin_password(instance.image_meta)
1824 
1825         guest = self._host.get_guest(instance)
1826         user = instance.image_meta.properties.get("os_admin_user")
1827         if not user:
1828             if instance.os_type == "windows":
1829                 user = "Administrator"
1830             else:
1831                 user = "root"
1832         try:
1833             guest.set_user_password(user, new_pass)
1834         except libvirt.libvirtError as ex:
1835             error_code = ex.get_error_code()
1836             if error_code == libvirt.VIR_ERR_AGENT_UNRESPONSIVE:
1837                 LOG.debug('Failed to set password: QEMU agent unresponsive',
1838                           instance_uuid=instance.uuid)
1839                 raise NotImplementedError()
1840 
1841             err_msg = encodeutils.exception_to_unicode(ex)
1842             msg = (_('Error from libvirt while set password for username '
1843                      '"%(user)s": [Error Code %(error_code)s] %(ex)s')
1844                    % {'user': user, 'error_code': error_code, 'ex': err_msg})
1845             raise exception.InternalError(msg)
1846 
1847     def _can_quiesce(self, instance, image_meta):
1848         if CONF.libvirt.virt_type not in ('kvm', 'qemu'):
1849             raise exception.InstanceQuiesceNotSupported(
1850                 instance_id=instance.uuid)
1851 
1852         if not image_meta.properties.get('hw_qemu_guest_agent', False):
1853             raise exception.QemuGuestAgentNotEnabled()
1854 
1855     def _requires_quiesce(self, image_meta):
1856         return image_meta.properties.get('os_require_quiesce', False)
1857 
1858     def _set_quiesced(self, context, instance, image_meta, quiesced):
1859         self._can_quiesce(instance, image_meta)
1860         try:
1861             guest = self._host.get_guest(instance)
1862             if quiesced:
1863                 guest.freeze_filesystems()
1864             else:
1865                 guest.thaw_filesystems()
1866         except libvirt.libvirtError as ex:
1867             error_code = ex.get_error_code()
1868             err_msg = encodeutils.exception_to_unicode(ex)
1869             msg = (_('Error from libvirt while quiescing %(instance_name)s: '
1870                      '[Error Code %(error_code)s] %(ex)s')
1871                    % {'instance_name': instance.name,
1872                       'error_code': error_code, 'ex': err_msg})
1873             raise exception.InternalError(msg)
1874 
1875     def quiesce(self, context, instance, image_meta):
1876         """Freeze the guest filesystems to prepare for snapshot.
1877 
1878         The qemu-guest-agent must be setup to execute fsfreeze.
1879         """
1880         self._set_quiesced(context, instance, image_meta, True)
1881 
1882     def unquiesce(self, context, instance, image_meta):
1883         """Thaw the guest filesystems after snapshot."""
1884         self._set_quiesced(context, instance, image_meta, False)
1885 
1886     def _live_snapshot(self, context, instance, guest, disk_path, out_path,
1887                        source_format, image_format, image_meta):
1888         """Snapshot an instance without downtime."""
1889         dev = guest.get_block_device(disk_path)
1890 
1891         # Save a copy of the domain's persistent XML file
1892         xml = guest.get_xml_desc(dump_inactive=True, dump_sensitive=True)
1893 
1894         # Abort is an idempotent operation, so make sure any block
1895         # jobs which may have failed are ended.
1896         try:
1897             dev.abort_job()
1898         except Exception:
1899             pass
1900 
1901         # NOTE (rmk): We are using shallow rebases as a workaround to a bug
1902         #             in QEMU 1.3. In order to do this, we need to create
1903         #             a destination image with the original backing file
1904         #             and matching size of the instance root disk.
1905         src_disk_size = libvirt_utils.get_disk_size(disk_path,
1906                                                     format=source_format)
1907         src_back_path = libvirt_utils.get_disk_backing_file(disk_path,
1908                                                         format=source_format,
1909                                                         basename=False)
1910         disk_delta = out_path + '.delta'
1911         libvirt_utils.create_cow_image(src_back_path, disk_delta,
1912                                        src_disk_size)
1913 
1914         quiesced = False
1915         try:
1916             self._set_quiesced(context, instance, image_meta, True)
1917             quiesced = True
1918         except exception.NovaException as err:
1919             if self._requires_quiesce(image_meta):
1920                 raise
1921             LOG.info('Skipping quiescing instance: %(reason)s.',
1922                      {'reason': err}, instance=instance)
1923 
1924         try:
1925             # NOTE (rmk): blockRebase cannot be executed on persistent
1926             #             domains, so we need to temporarily undefine it.
1927             #             If any part of this block fails, the domain is
1928             #             re-defined regardless.
1929             if guest.has_persistent_configuration():
1930                 support_uefi = self._has_uefi_support()
1931                 guest.delete_configuration(support_uefi)
1932 
1933             # NOTE (rmk): Establish a temporary mirror of our root disk and
1934             #             issue an abort once we have a complete copy.
1935             dev.rebase(disk_delta, copy=True, reuse_ext=True, shallow=True)
1936 
1937             while not dev.is_job_complete():
1938                 time.sleep(0.5)
1939 
1940             dev.abort_job()
1941             nova.privsep.path.chown(disk_delta, uid=os.getuid())
1942         finally:
1943             self._host.write_instance_config(xml)
1944             if quiesced:
1945                 self._set_quiesced(context, instance, image_meta, False)
1946 
1947         # Convert the delta (CoW) image with a backing file to a flat
1948         # image with no backing file.
1949         libvirt_utils.extract_snapshot(disk_delta, 'qcow2',
1950                                        out_path, image_format)
1951 
1952     def _volume_snapshot_update_status(self, context, snapshot_id, status):
1953         """Send a snapshot status update to Cinder.
1954 
1955         This method captures and logs exceptions that occur
1956         since callers cannot do anything useful with these exceptions.
1957 
1958         Operations on the Cinder side waiting for this will time out if
1959         a failure occurs sending the update.
1960 
1961         :param context: security context
1962         :param snapshot_id: id of snapshot being updated
1963         :param status: new status value
1964 
1965         """
1966 
1967         try:
1968             self._volume_api.update_snapshot_status(context,
1969                                                     snapshot_id,
1970                                                     status)
1971         except Exception:
1972             LOG.exception(_('Failed to send updated snapshot status '
1973                             'to volume service.'))
1974 
1975     def _volume_snapshot_create(self, context, instance, guest,
1976                                 volume_id, new_file):
1977         """Perform volume snapshot.
1978 
1979            :param guest: VM that volume is attached to
1980            :param volume_id: volume UUID to snapshot
1981            :param new_file: relative path to new qcow2 file present on share
1982 
1983         """
1984         xml = guest.get_xml_desc()
1985         xml_doc = etree.fromstring(xml)
1986 
1987         device_info = vconfig.LibvirtConfigGuest()
1988         device_info.parse_dom(xml_doc)
1989 
1990         disks_to_snap = []          # to be snapshotted by libvirt
1991         network_disks_to_snap = []  # network disks (netfs, etc.)
1992         disks_to_skip = []          # local disks not snapshotted
1993 
1994         for guest_disk in device_info.devices:
1995             if (guest_disk.root_name != 'disk'):
1996                 continue
1997 
1998             if (guest_disk.target_dev is None):
1999                 continue
2000 
2001             if (guest_disk.serial is None or guest_disk.serial != volume_id):
2002                 disks_to_skip.append(guest_disk.target_dev)
2003                 continue
2004 
2005             # disk is a Cinder volume with the correct volume_id
2006 
2007             disk_info = {
2008                 'dev': guest_disk.target_dev,
2009                 'serial': guest_disk.serial,
2010                 'current_file': guest_disk.source_path,
2011                 'source_protocol': guest_disk.source_protocol,
2012                 'source_name': guest_disk.source_name,
2013                 'source_hosts': guest_disk.source_hosts,
2014                 'source_ports': guest_disk.source_ports
2015             }
2016 
2017             # Determine path for new_file based on current path
2018             if disk_info['current_file'] is not None:
2019                 current_file = disk_info['current_file']
2020                 new_file_path = os.path.join(os.path.dirname(current_file),
2021                                              new_file)
2022                 disks_to_snap.append((current_file, new_file_path))
2023             # NOTE(mriedem): This used to include a check for gluster in
2024             # addition to netfs since they were added together. Support for
2025             # gluster was removed in the 16.0.0 Pike release. It is unclear,
2026             # however, if other volume drivers rely on the netfs disk source
2027             # protocol.
2028             elif disk_info['source_protocol'] == 'netfs':
2029                 network_disks_to_snap.append((disk_info, new_file))
2030 
2031         if not disks_to_snap and not network_disks_to_snap:
2032             msg = _('Found no disk to snapshot.')
2033             raise exception.InternalError(msg)
2034 
2035         snapshot = vconfig.LibvirtConfigGuestSnapshot()
2036 
2037         for current_name, new_filename in disks_to_snap:
2038             snap_disk = vconfig.LibvirtConfigGuestSnapshotDisk()
2039             snap_disk.name = current_name
2040             snap_disk.source_path = new_filename
2041             snap_disk.source_type = 'file'
2042             snap_disk.snapshot = 'external'
2043             snap_disk.driver_name = 'qcow2'
2044 
2045             snapshot.add_disk(snap_disk)
2046 
2047         for disk_info, new_filename in network_disks_to_snap:
2048             snap_disk = vconfig.LibvirtConfigGuestSnapshotDisk()
2049             snap_disk.name = disk_info['dev']
2050             snap_disk.source_type = 'network'
2051             snap_disk.source_protocol = disk_info['source_protocol']
2052             snap_disk.snapshot = 'external'
2053             snap_disk.source_path = new_filename
2054             old_dir = disk_info['source_name'].split('/')[0]
2055             snap_disk.source_name = '%s/%s' % (old_dir, new_filename)
2056             snap_disk.source_hosts = disk_info['source_hosts']
2057             snap_disk.source_ports = disk_info['source_ports']
2058 
2059             snapshot.add_disk(snap_disk)
2060 
2061         for dev in disks_to_skip:
2062             snap_disk = vconfig.LibvirtConfigGuestSnapshotDisk()
2063             snap_disk.name = dev
2064             snap_disk.snapshot = 'no'
2065 
2066             snapshot.add_disk(snap_disk)
2067 
2068         snapshot_xml = snapshot.to_xml()
2069         LOG.debug("snap xml: %s", snapshot_xml, instance=instance)
2070 
2071         image_meta = instance.image_meta
2072         try:
2073             # Check to see if we can quiesce the guest before taking the
2074             # snapshot.
2075             self._can_quiesce(instance, image_meta)
2076             try:
2077                 guest.snapshot(snapshot, no_metadata=True, disk_only=True,
2078                                reuse_ext=True, quiesce=True)
2079                 return
2080             except libvirt.libvirtError:
2081                 # If the image says that quiesce is required then we fail.
2082                 if self._requires_quiesce(image_meta):
2083                     raise
2084                 LOG.exception(_('Unable to create quiesced VM snapshot, '
2085                                 'attempting again with quiescing disabled.'),
2086                               instance=instance)
2087         except (exception.InstanceQuiesceNotSupported,
2088                 exception.QemuGuestAgentNotEnabled) as err:
2089             # If the image says that quiesce is required then we need to fail.
2090             if self._requires_quiesce(image_meta):
2091                 raise
2092             LOG.info('Skipping quiescing instance: %(reason)s.',
2093                      {'reason': err}, instance=instance)
2094 
2095         try:
2096             guest.snapshot(snapshot, no_metadata=True, disk_only=True,
2097                            reuse_ext=True, quiesce=False)
2098         except libvirt.libvirtError:
2099             LOG.exception(_('Unable to create VM snapshot, '
2100                             'failing volume_snapshot operation.'),
2101                           instance=instance)
2102 
2103             raise
2104 
2105     def _volume_refresh_connection_info(self, context, instance, volume_id):
2106         bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
2107                   context, volume_id, instance.uuid)
2108 
2109         driver_bdm = driver_block_device.convert_volume(bdm)
2110         if driver_bdm:
2111             driver_bdm.refresh_connection_info(context, instance,
2112                                                self._volume_api, self)
2113 
2114     def volume_snapshot_create(self, context, instance, volume_id,
2115                                create_info):
2116         """Create snapshots of a Cinder volume via libvirt.
2117 
2118         :param instance: VM instance object reference
2119         :param volume_id: id of volume being snapshotted
2120         :param create_info: dict of information used to create snapshots
2121                      - snapshot_id : ID of snapshot
2122                      - type : qcow2 / <other>
2123                      - new_file : qcow2 file created by Cinder which
2124                      becomes the VM's active image after
2125                      the snapshot is complete
2126         """
2127 
2128         LOG.debug("volume_snapshot_create: create_info: %(c_info)s",
2129                   {'c_info': create_info}, instance=instance)
2130 
2131         try:
2132             guest = self._host.get_guest(instance)
2133         except exception.InstanceNotFound:
2134             raise exception.InstanceNotRunning(instance_id=instance.uuid)
2135 
2136         if create_info['type'] != 'qcow2':
2137             msg = _('Unknown type: %s') % create_info['type']
2138             raise exception.InternalError(msg)
2139 
2140         snapshot_id = create_info.get('snapshot_id', None)
2141         if snapshot_id is None:
2142             msg = _('snapshot_id required in create_info')
2143             raise exception.InternalError(msg)
2144 
2145         try:
2146             self._volume_snapshot_create(context, instance, guest,
2147                                          volume_id, create_info['new_file'])
2148         except Exception:
2149             with excutils.save_and_reraise_exception():
2150                 LOG.exception(_('Error occurred during '
2151                                 'volume_snapshot_create, '
2152                                 'sending error status to Cinder.'),
2153                               instance=instance)
2154                 self._volume_snapshot_update_status(
2155                     context, snapshot_id, 'error')
2156 
2157         self._volume_snapshot_update_status(
2158             context, snapshot_id, 'creating')
2159 
2160         def _wait_for_snapshot():
2161             snapshot = self._volume_api.get_snapshot(context, snapshot_id)
2162 
2163             if snapshot.get('status') != 'creating':
2164                 self._volume_refresh_connection_info(context, instance,
2165                                                      volume_id)
2166                 raise loopingcall.LoopingCallDone()
2167 
2168         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_snapshot)
2169         timer.start(interval=0.5).wait()
2170 
2171     @staticmethod
2172     def _rebase_with_qemu_img(guest, device, active_disk_object,
2173                               rebase_base):
2174         """Rebase a device tied to a guest using qemu-img.
2175 
2176         :param guest:the Guest which owns the device being rebased
2177         :type guest: nova.virt.libvirt.guest.Guest
2178         :param device: the guest block device to rebase
2179         :type device: nova.virt.libvirt.guest.BlockDevice
2180         :param active_disk_object: the guest block device to rebase
2181         :type active_disk_object: nova.virt.libvirt.config.\
2182                                     LibvirtConfigGuestDisk
2183         :param rebase_base: the new parent in the backing chain
2184         :type rebase_base: None or string
2185         """
2186 
2187         # It's unsure how well qemu-img handles network disks for
2188         # every protocol. So let's be safe.
2189         active_protocol = active_disk_object.source_protocol
2190         if active_protocol is not None:
2191             msg = _("Something went wrong when deleting a volume snapshot: "
2192                     "rebasing a %(protocol)s network disk using qemu-img "
2193                     "has not been fully tested") % {'protocol':
2194                     active_protocol}
2195             LOG.error(msg)
2196             raise exception.InternalError(msg)
2197 
2198         if rebase_base is None:
2199             # If backing_file is specified as "" (the empty string), then
2200             # the image is rebased onto no backing file (i.e. it will exist
2201             # independently of any backing file).
2202             backing_file = ""
2203             qemu_img_extra_arg = []
2204         else:
2205             # If the rebased image is going to have a backing file then
2206             # explicitly set the backing file format to avoid any security
2207             # concerns related to file format auto detection.
2208             backing_file = rebase_base
2209             b_file_fmt = images.qemu_img_info(backing_file).file_format
2210             qemu_img_extra_arg = ['-F', b_file_fmt]
2211 
2212         qemu_img_extra_arg.append(active_disk_object.source_path)
2213         utils.execute("qemu-img", "rebase", "-b", backing_file,
2214                       *qemu_img_extra_arg)
2215 
2216     def _volume_snapshot_delete(self, context, instance, volume_id,
2217                                 snapshot_id, delete_info=None):
2218         """Note:
2219             if file being merged into == active image:
2220                 do a blockRebase (pull) operation
2221             else:
2222                 do a blockCommit operation
2223             Files must be adjacent in snap chain.
2224 
2225         :param instance: instance object reference
2226         :param volume_id: volume UUID
2227         :param snapshot_id: snapshot UUID (unused currently)
2228         :param delete_info: {
2229             'type':              'qcow2',
2230             'file_to_merge':     'a.img',
2231             'merge_target_file': 'b.img' or None (if merging file_to_merge into
2232                                                   active image)
2233           }
2234         """
2235 
2236         LOG.debug('volume_snapshot_delete: delete_info: %s', delete_info,
2237                   instance=instance)
2238 
2239         if delete_info['type'] != 'qcow2':
2240             msg = _('Unknown delete_info type %s') % delete_info['type']
2241             raise exception.InternalError(msg)
2242 
2243         try:
2244             guest = self._host.get_guest(instance)
2245         except exception.InstanceNotFound:
2246             raise exception.InstanceNotRunning(instance_id=instance.uuid)
2247 
2248         # Find dev name
2249         my_dev = None
2250         active_disk = None
2251 
2252         xml = guest.get_xml_desc()
2253         xml_doc = etree.fromstring(xml)
2254 
2255         device_info = vconfig.LibvirtConfigGuest()
2256         device_info.parse_dom(xml_doc)
2257 
2258         active_disk_object = None
2259 
2260         for guest_disk in device_info.devices:
2261             if (guest_disk.root_name != 'disk'):
2262                 continue
2263 
2264             if (guest_disk.target_dev is None or guest_disk.serial is None):
2265                 continue
2266 
2267             if guest_disk.serial == volume_id:
2268                 my_dev = guest_disk.target_dev
2269 
2270                 active_disk = guest_disk.source_path
2271                 active_protocol = guest_disk.source_protocol
2272                 active_disk_object = guest_disk
2273                 break
2274 
2275         if my_dev is None or (active_disk is None and active_protocol is None):
2276             LOG.debug('Domain XML: %s', xml, instance=instance)
2277             msg = (_('Disk with id: %s not found attached to instance.')
2278                    % volume_id)
2279             raise exception.InternalError(msg)
2280 
2281         LOG.debug("found device at %s", my_dev, instance=instance)
2282 
2283         def _get_snap_dev(filename, backing_store):
2284             if filename is None:
2285                 msg = _('filename cannot be None')
2286                 raise exception.InternalError(msg)
2287 
2288             # libgfapi delete
2289             LOG.debug("XML: %s", xml)
2290 
2291             LOG.debug("active disk object: %s", active_disk_object)
2292 
2293             # determine reference within backing store for desired image
2294             filename_to_merge = filename
2295             matched_name = None
2296             b = backing_store
2297             index = None
2298 
2299             current_filename = active_disk_object.source_name.split('/')[1]
2300             if current_filename == filename_to_merge:
2301                 return my_dev + '[0]'
2302 
2303             while b is not None:
2304                 source_filename = b.source_name.split('/')[1]
2305                 if source_filename == filename_to_merge:
2306                     LOG.debug('found match: %s', b.source_name)
2307                     matched_name = b.source_name
2308                     index = b.index
2309                     break
2310 
2311                 b = b.backing_store
2312 
2313             if matched_name is None:
2314                 msg = _('no match found for %s') % (filename_to_merge)
2315                 raise exception.InternalError(msg)
2316 
2317             LOG.debug('index of match (%s) is %s', b.source_name, index)
2318 
2319             my_snap_dev = '%s[%s]' % (my_dev, index)
2320             return my_snap_dev
2321 
2322         if delete_info['merge_target_file'] is None:
2323             # pull via blockRebase()
2324 
2325             # Merge the most recent snapshot into the active image
2326 
2327             rebase_disk = my_dev
2328             rebase_base = delete_info['file_to_merge']  # often None
2329             if (active_protocol is not None) and (rebase_base is not None):
2330                 rebase_base = _get_snap_dev(rebase_base,
2331                                             active_disk_object.backing_store)
2332 
2333             # NOTE(deepakcs): libvirt added support for _RELATIVE in v1.2.7,
2334             # and when available this flag _must_ be used to ensure backing
2335             # paths are maintained relative by qemu.
2336             #
2337             # If _RELATIVE flag not found, continue with old behaviour
2338             # (relative backing path seems to work for this case)
2339             try:
2340                 libvirt.VIR_DOMAIN_BLOCK_REBASE_RELATIVE
2341                 relative = rebase_base is not None
2342             except AttributeError:
2343                 LOG.warning(
2344                     "Relative blockrebase support was not detected. "
2345                     "Continuing with old behaviour.")
2346                 relative = False
2347 
2348             LOG.debug(
2349                 'disk: %(disk)s, base: %(base)s, '
2350                 'bw: %(bw)s, relative: %(relative)s',
2351                 {'disk': rebase_disk,
2352                  'base': rebase_base,
2353                  'bw': libvirt_guest.BlockDevice.REBASE_DEFAULT_BANDWIDTH,
2354                  'relative': str(relative)}, instance=instance)
2355 
2356             dev = guest.get_block_device(rebase_disk)
2357             if guest.is_active():
2358                 result = dev.rebase(rebase_base, relative=relative)
2359                 if result == 0:
2360                     LOG.debug('blockRebase started successfully',
2361                               instance=instance)
2362 
2363                 while not dev.is_job_complete():
2364                     LOG.debug('waiting for blockRebase job completion',
2365                               instance=instance)
2366                     time.sleep(0.5)
2367 
2368             # If the guest is not running libvirt won't do a blockRebase.
2369             # In that case, let's ask qemu-img to rebase the disk.
2370             else:
2371                 LOG.debug('Guest is not running so doing a block rebase '
2372                           'using "qemu-img rebase"', instance=instance)
2373                 self._rebase_with_qemu_img(guest, dev, active_disk_object,
2374                                            rebase_base)
2375 
2376         else:
2377             # commit with blockCommit()
2378             my_snap_base = None
2379             my_snap_top = None
2380             commit_disk = my_dev
2381 
2382             if active_protocol is not None:
2383                 my_snap_base = _get_snap_dev(delete_info['merge_target_file'],
2384                                              active_disk_object.backing_store)
2385                 my_snap_top = _get_snap_dev(delete_info['file_to_merge'],
2386                                             active_disk_object.backing_store)
2387 
2388             commit_base = my_snap_base or delete_info['merge_target_file']
2389             commit_top = my_snap_top or delete_info['file_to_merge']
2390 
2391             LOG.debug('will call blockCommit with commit_disk=%(commit_disk)s '
2392                       'commit_base=%(commit_base)s '
2393                       'commit_top=%(commit_top)s ',
2394                       {'commit_disk': commit_disk,
2395                        'commit_base': commit_base,
2396                        'commit_top': commit_top}, instance=instance)
2397 
2398             dev = guest.get_block_device(commit_disk)
2399             result = dev.commit(commit_base, commit_top, relative=True)
2400 
2401             if result == 0:
2402                 LOG.debug('blockCommit started successfully',
2403                           instance=instance)
2404 
2405             while not dev.is_job_complete():
2406                 LOG.debug('waiting for blockCommit job completion',
2407                           instance=instance)
2408                 time.sleep(0.5)
2409 
2410     def volume_snapshot_delete(self, context, instance, volume_id, snapshot_id,
2411                                delete_info):
2412         try:
2413             self._volume_snapshot_delete(context, instance, volume_id,
2414                                          snapshot_id, delete_info=delete_info)
2415         except Exception:
2416             with excutils.save_and_reraise_exception():
2417                 LOG.exception(_('Error occurred during '
2418                                 'volume_snapshot_delete, '
2419                                 'sending error status to Cinder.'),
2420                               instance=instance)
2421                 self._volume_snapshot_update_status(
2422                     context, snapshot_id, 'error_deleting')
2423 
2424         self._volume_snapshot_update_status(context, snapshot_id, 'deleting')
2425         self._volume_refresh_connection_info(context, instance, volume_id)
2426 
2427     def reboot(self, context, instance, network_info, reboot_type,
2428                block_device_info=None, bad_volumes_callback=None):
2429         """Reboot a virtual machine, given an instance reference."""
2430         if reboot_type == 'SOFT':
2431             # NOTE(vish): This will attempt to do a graceful shutdown/restart.
2432             try:
2433                 soft_reboot_success = self._soft_reboot(instance)
2434             except libvirt.libvirtError as e:
2435                 LOG.debug("Instance soft reboot failed: %s",
2436                           encodeutils.exception_to_unicode(e),
2437                           instance=instance)
2438                 soft_reboot_success = False
2439 
2440             if soft_reboot_success:
2441                 LOG.info("Instance soft rebooted successfully.",
2442                          instance=instance)
2443                 return
2444             else:
2445                 LOG.warning("Failed to soft reboot instance. "
2446                             "Trying hard reboot.",
2447                             instance=instance)
2448         return self._hard_reboot(context, instance, network_info,
2449                                  block_device_info)
2450 
2451     def _soft_reboot(self, instance):
2452         """Attempt to shutdown and restart the instance gracefully.
2453 
2454         We use shutdown and create here so we can return if the guest
2455         responded and actually rebooted. Note that this method only
2456         succeeds if the guest responds to acpi. Therefore we return
2457         success or failure so we can fall back to a hard reboot if
2458         necessary.
2459 
2460         :returns: True if the reboot succeeded
2461         """
2462         guest = self._host.get_guest(instance)
2463 
2464         state = guest.get_power_state(self._host)
2465         old_domid = guest.id
2466         # NOTE(vish): This check allows us to reboot an instance that
2467         #             is already shutdown.
2468         if state == power_state.RUNNING:
2469             guest.shutdown()
2470         # NOTE(vish): This actually could take slightly longer than the
2471         #             FLAG defines depending on how long the get_info
2472         #             call takes to return.
2473         self._prepare_pci_devices_for_use(
2474             pci_manager.get_instance_pci_devs(instance, 'all'))
2475         for x in range(CONF.libvirt.wait_soft_reboot_seconds):
2476             guest = self._host.get_guest(instance)
2477 
2478             state = guest.get_power_state(self._host)
2479             new_domid = guest.id
2480 
2481             # NOTE(ivoks): By checking domain IDs, we make sure we are
2482             #              not recreating domain that's already running.
2483             if old_domid != new_domid:
2484                 if state in [power_state.SHUTDOWN,
2485                              power_state.CRASHED]:
2486                     LOG.info("Instance shutdown successfully.",
2487                              instance=instance)
2488                     self._create_domain(domain=guest._domain)
2489                     timer = loopingcall.FixedIntervalLoopingCall(
2490                         self._wait_for_running, instance)
2491                     timer.start(interval=0.5).wait()
2492                     return True
2493                 else:
2494                     LOG.info("Instance may have been rebooted during soft "
2495                              "reboot, so return now.", instance=instance)
2496                     return True
2497             greenthread.sleep(1)
2498         return False
2499 
2500     def _hard_reboot(self, context, instance, network_info,
2501                      block_device_info=None):
2502         """Reboot a virtual machine, given an instance reference.
2503 
2504         Performs a Libvirt reset (if supported) on the domain.
2505 
2506         If Libvirt reset is unavailable this method actually destroys and
2507         re-creates the domain to ensure the reboot happens, as the guest
2508         OS cannot ignore this action.
2509         """
2510         # NOTE(mdbooth): In addition to performing a hard reboot of the domain,
2511         # the hard reboot operation is relied upon by operators to be an
2512         # automated attempt to fix as many things as possible about a
2513         # non-functioning instance before resorting to manual intervention.
2514         # With this goal in mind, we tear down all the aspects of an instance
2515         # we can here without losing data. This allows us to re-initialise from
2516         # scratch, and hopefully fix, most aspects of a non-functioning guest.
2517         self.destroy(context, instance, network_info, destroy_disks=False,
2518                      block_device_info=block_device_info)
2519 
2520         # Convert the system metadata to image metadata
2521         # NOTE(mdbooth): This is a workaround for stateless Nova compute
2522         #                https://bugs.launchpad.net/nova/+bug/1349978
2523         instance_dir = libvirt_utils.get_instance_path(instance)
2524         fileutils.ensure_tree(instance_dir)
2525 
2526         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
2527                                             instance,
2528                                             instance.image_meta,
2529                                             block_device_info)
2530         # NOTE(vish): This could generate the wrong device_format if we are
2531         #             using the raw backend and the images don't exist yet.
2532         #             The create_images_and_backing below doesn't properly
2533         #             regenerate raw backend images, however, so when it
2534         #             does we need to (re)generate the xml after the images
2535         #             are in place.
2536         xml = self._get_guest_xml(context, instance, network_info, disk_info,
2537                                   instance.image_meta,
2538                                   block_device_info=block_device_info)
2539 
2540         # NOTE(mdbooth): context.auth_token will not be set when we call
2541         #                _hard_reboot from resume_state_on_host_boot()
2542         if context.auth_token is not None:
2543             # NOTE (rmk): Re-populate any missing backing files.
2544             config = vconfig.LibvirtConfigGuest()
2545             config.parse_str(xml)
2546             backing_disk_info = self._get_instance_disk_info_from_config(
2547                 config, block_device_info)
2548             self._create_images_and_backing(context, instance, instance_dir,
2549                                             backing_disk_info)
2550 
2551         # Initialize all the necessary networking, block devices and
2552         # start the instance.
2553         self._create_domain_and_network(context, xml, instance, network_info,
2554                                         block_device_info=block_device_info)
2555         self._prepare_pci_devices_for_use(
2556             pci_manager.get_instance_pci_devs(instance, 'all'))
2557 
2558         def _wait_for_reboot():
2559             """Called at an interval until the VM is running again."""
2560             state = self.get_info(instance).state
2561 
2562             if state == power_state.RUNNING:
2563                 LOG.info("Instance rebooted successfully.",
2564                          instance=instance)
2565                 raise loopingcall.LoopingCallDone()
2566 
2567         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_reboot)
2568         timer.start(interval=0.5).wait()
2569 
2570     def pause(self, instance):
2571         """Pause VM instance."""
2572         self._host.get_guest(instance).pause()
2573 
2574     def unpause(self, instance):
2575         """Unpause paused VM instance."""
2576         guest = self._host.get_guest(instance)
2577         guest.resume()
2578         guest.sync_guest_time()
2579 
2580     def _clean_shutdown(self, instance, timeout, retry_interval):
2581         """Attempt to shutdown the instance gracefully.
2582 
2583         :param instance: The instance to be shutdown
2584         :param timeout: How long to wait in seconds for the instance to
2585                         shutdown
2586         :param retry_interval: How often in seconds to signal the instance
2587                                to shutdown while waiting
2588 
2589         :returns: True if the shutdown succeeded
2590         """
2591 
2592         # List of states that represent a shutdown instance
2593         SHUTDOWN_STATES = [power_state.SHUTDOWN,
2594                            power_state.CRASHED]
2595 
2596         try:
2597             guest = self._host.get_guest(instance)
2598         except exception.InstanceNotFound:
2599             # If the instance has gone then we don't need to
2600             # wait for it to shutdown
2601             return True
2602 
2603         state = guest.get_power_state(self._host)
2604         if state in SHUTDOWN_STATES:
2605             LOG.info("Instance already shutdown.", instance=instance)
2606             return True
2607 
2608         LOG.debug("Shutting down instance from state %s", state,
2609                   instance=instance)
2610         guest.shutdown()
2611         retry_countdown = retry_interval
2612 
2613         for sec in range(timeout):
2614 
2615             guest = self._host.get_guest(instance)
2616             state = guest.get_power_state(self._host)
2617 
2618             if state in SHUTDOWN_STATES:
2619                 LOG.info("Instance shutdown successfully after %d seconds.",
2620                          sec, instance=instance)
2621                 return True
2622 
2623             # Note(PhilD): We can't assume that the Guest was able to process
2624             #              any previous shutdown signal (for example it may
2625             #              have still been startingup, so within the overall
2626             #              timeout we re-trigger the shutdown every
2627             #              retry_interval
2628             if retry_countdown == 0:
2629                 retry_countdown = retry_interval
2630                 # Instance could shutdown at any time, in which case we
2631                 # will get an exception when we call shutdown
2632                 try:
2633                     LOG.debug("Instance in state %s after %d seconds - "
2634                               "resending shutdown", state, sec,
2635                               instance=instance)
2636                     guest.shutdown()
2637                 except libvirt.libvirtError:
2638                     # Assume this is because its now shutdown, so loop
2639                     # one more time to clean up.
2640                     LOG.debug("Ignoring libvirt exception from shutdown "
2641                               "request.", instance=instance)
2642                     continue
2643             else:
2644                 retry_countdown -= 1
2645 
2646             time.sleep(1)
2647 
2648         LOG.info("Instance failed to shutdown in %d seconds.",
2649                  timeout, instance=instance)
2650         return False
2651 
2652     def power_off(self, instance, timeout=0, retry_interval=0):
2653         """Power off the specified instance."""
2654         if timeout:
2655             self._clean_shutdown(instance, timeout, retry_interval)
2656         self._destroy(instance)
2657 
2658     def power_on(self, context, instance, network_info,
2659                  block_device_info=None):
2660         """Power on the specified instance."""
2661         # We use _hard_reboot here to ensure that all backing files,
2662         # network, and block device connections, etc. are established
2663         # and available before we attempt to start the instance.
2664         self._hard_reboot(context, instance, network_info, block_device_info)
2665 
2666     def trigger_crash_dump(self, instance):
2667 
2668         """Trigger crash dump by injecting an NMI to the specified instance."""
2669         try:
2670             self._host.get_guest(instance).inject_nmi()
2671         except libvirt.libvirtError as ex:
2672             error_code = ex.get_error_code()
2673 
2674             if error_code == libvirt.VIR_ERR_NO_SUPPORT:
2675                 raise exception.TriggerCrashDumpNotSupported()
2676             elif error_code == libvirt.VIR_ERR_OPERATION_INVALID:
2677                 raise exception.InstanceNotRunning(instance_id=instance.uuid)
2678 
2679             LOG.exception(_('Error from libvirt while injecting an NMI to '
2680                             '%(instance_uuid)s: '
2681                             '[Error Code %(error_code)s] %(ex)s'),
2682                           {'instance_uuid': instance.uuid,
2683                            'error_code': error_code, 'ex': ex})
2684             raise
2685 
2686     def suspend(self, context, instance):
2687         """Suspend the specified instance."""
2688         guest = self._host.get_guest(instance)
2689 
2690         self._detach_pci_devices(guest,
2691             pci_manager.get_instance_pci_devs(instance))
2692         self._detach_direct_passthrough_ports(context, instance, guest)
2693         guest.save_memory_state()
2694 
2695     def resume(self, context, instance, network_info, block_device_info=None):
2696         """resume the specified instance."""
2697         xml = self._get_existing_domain_xml(instance, network_info,
2698                                             block_device_info)
2699         guest = self._create_domain_and_network(context, xml, instance,
2700                            network_info, block_device_info=block_device_info,
2701                            vifs_already_plugged=True)
2702         self._attach_pci_devices(guest,
2703             pci_manager.get_instance_pci_devs(instance))
2704         self._attach_direct_passthrough_ports(
2705             context, instance, guest, network_info)
2706         timer = loopingcall.FixedIntervalLoopingCall(self._wait_for_running,
2707                                                      instance)
2708         timer.start(interval=0.5).wait()
2709         guest.sync_guest_time()
2710 
2711     def resume_state_on_host_boot(self, context, instance, network_info,
2712                                   block_device_info=None):
2713         """resume guest state when a host is booted."""
2714         # Check if the instance is running already and avoid doing
2715         # anything if it is.
2716         try:
2717             guest = self._host.get_guest(instance)
2718             state = guest.get_power_state(self._host)
2719 
2720             ignored_states = (power_state.RUNNING,
2721                               power_state.SUSPENDED,
2722                               power_state.NOSTATE,
2723                               power_state.PAUSED)
2724 
2725             if state in ignored_states:
2726                 return
2727         except (exception.InternalError, exception.InstanceNotFound):
2728             pass
2729 
2730         # Instance is not up and could be in an unknown state.
2731         # Be as absolute as possible about getting it back into
2732         # a known and running state.
2733         self._hard_reboot(context, instance, network_info, block_device_info)
2734 
2735     def rescue(self, context, instance, network_info, image_meta,
2736                rescue_password):
2737         """Loads a VM using rescue images.
2738 
2739         A rescue is normally performed when something goes wrong with the
2740         primary images and data needs to be corrected/recovered. Rescuing
2741         should not edit or over-ride the original image, only allow for
2742         data recovery.
2743 
2744         """
2745         instance_dir = libvirt_utils.get_instance_path(instance)
2746         unrescue_xml = self._get_existing_domain_xml(instance, network_info)
2747         unrescue_xml_path = os.path.join(instance_dir, 'unrescue.xml')
2748         libvirt_utils.write_to_file(unrescue_xml_path, unrescue_xml)
2749 
2750         rescue_image_id = None
2751         if image_meta.obj_attr_is_set("id"):
2752             rescue_image_id = image_meta.id
2753 
2754         rescue_images = {
2755             'image_id': (rescue_image_id or
2756                         CONF.libvirt.rescue_image_id or instance.image_ref),
2757             'kernel_id': (CONF.libvirt.rescue_kernel_id or
2758                           instance.kernel_id),
2759             'ramdisk_id': (CONF.libvirt.rescue_ramdisk_id or
2760                            instance.ramdisk_id),
2761         }
2762         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
2763                                             instance,
2764                                             image_meta,
2765                                             rescue=True)
2766         injection_info = InjectionInfo(network_info=network_info,
2767                                        admin_pass=rescue_password,
2768                                        files=None)
2769         gen_confdrive = functools.partial(self._create_configdrive,
2770                                           context, instance, injection_info,
2771                                           rescue=True)
2772         self._create_image(context, instance, disk_info['mapping'],
2773                            injection_info=injection_info, suffix='.rescue',
2774                            disk_images=rescue_images)
2775         xml = self._get_guest_xml(context, instance, network_info, disk_info,
2776                                   image_meta, rescue=rescue_images)
2777         self._destroy(instance)
2778         self._create_domain(xml, post_xml_callback=gen_confdrive)
2779 
2780     def unrescue(self, instance, network_info):
2781         """Reboot the VM which is being rescued back into primary images.
2782         """
2783         instance_dir = libvirt_utils.get_instance_path(instance)
2784         unrescue_xml_path = os.path.join(instance_dir, 'unrescue.xml')
2785         xml = libvirt_utils.load_file(unrescue_xml_path)
2786         guest = self._host.get_guest(instance)
2787 
2788         # TODO(sahid): We are converting all calls from a
2789         # virDomain object to use nova.virt.libvirt.Guest.
2790         # We should be able to remove virt_dom at the end.
2791         virt_dom = guest._domain
2792         self._destroy(instance)
2793         self._create_domain(xml, virt_dom)
2794         os.unlink(unrescue_xml_path)
2795         rescue_files = os.path.join(instance_dir, "*.rescue")
2796         for rescue_file in glob.iglob(rescue_files):
2797             if os.path.isdir(rescue_file):
2798                 shutil.rmtree(rescue_file)
2799             else:
2800                 os.unlink(rescue_file)
2801         # cleanup rescue volume
2802         lvm.remove_volumes([lvmdisk for lvmdisk in self._lvm_disks(instance)
2803                                 if lvmdisk.endswith('.rescue')])
2804         if CONF.libvirt.images_type == 'rbd':
2805             filter_fn = lambda disk: (disk.startswith(instance.uuid) and
2806                                       disk.endswith('.rescue'))
2807             LibvirtDriver._get_rbd_driver().cleanup_volumes(filter_fn)
2808 
2809     def poll_rebooting_instances(self, timeout, instances):
2810         pass
2811 
2812     # NOTE(ilyaalekseyev): Implementation like in multinics
2813     # for xenapi(tr3buchet)
2814     def spawn(self, context, instance, image_meta, injected_files,
2815               admin_password, allocations, network_info=None,
2816               block_device_info=None):
2817         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
2818                                             instance,
2819                                             image_meta,
2820                                             block_device_info)
2821         injection_info = InjectionInfo(network_info=network_info,
2822                                        files=injected_files,
2823                                        admin_pass=admin_password)
2824         gen_confdrive = functools.partial(self._create_configdrive,
2825                                           context, instance,
2826                                           injection_info)
2827         self._create_image(context, instance, disk_info['mapping'],
2828                            injection_info=injection_info,
2829                            block_device_info=block_device_info)
2830 
2831         # Required by Quobyte CI
2832         self._ensure_console_log_for_instance(instance)
2833 
2834         xml = self._get_guest_xml(context, instance, network_info,
2835                                   disk_info, image_meta,
2836                                   block_device_info=block_device_info,
2837                                   allocations=allocations)
2838         self._create_domain_and_network(
2839             context, xml, instance, network_info,
2840             block_device_info=block_device_info,
2841             post_xml_callback=gen_confdrive,
2842             destroy_disks_on_failure=True)
2843         LOG.debug("Instance is running", instance=instance)
2844 
2845         def _wait_for_boot():
2846             """Called at an interval until the VM is running."""
2847             state = self.get_info(instance).state
2848 
2849             if state == power_state.RUNNING:
2850                 LOG.info("Instance spawned successfully.", instance=instance)
2851                 raise loopingcall.LoopingCallDone()
2852 
2853         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_boot)
2854         timer.start(interval=0.5).wait()
2855 
2856     def _get_console_output_file(self, instance, console_log):
2857         bytes_to_read = MAX_CONSOLE_BYTES
2858         log_data = b""  # The last N read bytes
2859         i = 0  # in case there is a log rotation (like "virtlogd")
2860         path = console_log
2861 
2862         while bytes_to_read > 0 and os.path.exists(path):
2863             read_log_data, remaining = nova.privsep.path.last_bytes(
2864                                         path, bytes_to_read)
2865             # We need the log file content in chronological order,
2866             # that's why we *prepend* the log data.
2867             log_data = read_log_data + log_data
2868 
2869             # Prep to read the next file in the chain
2870             bytes_to_read -= len(read_log_data)
2871             path = console_log + "." + str(i)
2872             i += 1
2873 
2874             if remaining > 0:
2875                 LOG.info('Truncated console log returned, '
2876                          '%d bytes ignored', remaining, instance=instance)
2877         return log_data
2878 
2879     def get_console_output(self, context, instance):
2880         guest = self._host.get_guest(instance)
2881 
2882         xml = guest.get_xml_desc()
2883         tree = etree.fromstring(xml)
2884 
2885         # If the guest has a console logging to a file prefer to use that
2886         file_consoles = tree.findall("./devices/console[@type='file']")
2887         if file_consoles:
2888             for file_console in file_consoles:
2889                 source_node = file_console.find('./source')
2890                 if source_node is None:
2891                     continue
2892                 path = source_node.get("path")
2893                 if not path:
2894                     continue
2895 
2896                 if not os.path.exists(path):
2897                     LOG.info('Instance is configured with a file console, '
2898                              'but the backing file is not (yet?) present',
2899                              instance=instance)
2900                     return ""
2901 
2902                 return self._get_console_output_file(instance, path)
2903 
2904         # Try 'pty' types
2905         pty_consoles = tree.findall("./devices/console[@type='pty']")
2906         if pty_consoles:
2907             for pty_console in pty_consoles:
2908                 source_node = pty_console.find('./source')
2909                 if source_node is None:
2910                     continue
2911                 pty = source_node.get("path")
2912                 if not pty:
2913                     continue
2914                 break
2915             else:
2916                 raise exception.ConsoleNotAvailable()
2917         else:
2918             raise exception.ConsoleNotAvailable()
2919 
2920         console_log = self._get_console_log_path(instance)
2921         data = nova.privsep.libvirt.readpty(pty)
2922 
2923         # NOTE(markus_z): The virt_types kvm and qemu are the only ones
2924         # which create a dedicated file device for the console logging.
2925         # Other virt_types like xen, lxc, uml, parallels depend on the
2926         # flush of that pty device into the "console.log" file to ensure
2927         # that a series of "get_console_output" calls return the complete
2928         # content even after rebooting a guest.
2929         nova.privsep.path.writefile(console_log, 'a+', data)
2930         return self._get_console_output_file(instance, console_log)
2931 
2932     def get_host_ip_addr(self):
2933         ips = compute_utils.get_machine_ips()
2934         if CONF.my_ip not in ips:
2935             LOG.warning('my_ip address (%(my_ip)s) was not found on '
2936                         'any of the interfaces: %(ifaces)s',
2937                         {'my_ip': CONF.my_ip, 'ifaces': ", ".join(ips)})
2938         return CONF.my_ip
2939 
2940     def get_vnc_console(self, context, instance):
2941         def get_vnc_port_for_instance(instance_name):
2942             guest = self._host.get_guest(instance)
2943 
2944             xml = guest.get_xml_desc()
2945             xml_dom = etree.fromstring(xml)
2946 
2947             graphic = xml_dom.find("./devices/graphics[@type='vnc']")
2948             if graphic is not None:
2949                 return graphic.get('port')
2950             # NOTE(rmk): We had VNC consoles enabled but the instance in
2951             # question is not actually listening for connections.
2952             raise exception.ConsoleTypeUnavailable(console_type='vnc')
2953 
2954         port = get_vnc_port_for_instance(instance.name)
2955         host = CONF.vnc.server_proxyclient_address
2956 
2957         return ctype.ConsoleVNC(host=host, port=port)
2958 
2959     def get_spice_console(self, context, instance):
2960         def get_spice_ports_for_instance(instance_name):
2961             guest = self._host.get_guest(instance)
2962 
2963             xml = guest.get_xml_desc()
2964             xml_dom = etree.fromstring(xml)
2965 
2966             graphic = xml_dom.find("./devices/graphics[@type='spice']")
2967             if graphic is not None:
2968                 return (graphic.get('port'), graphic.get('tlsPort'))
2969             # NOTE(rmk): We had Spice consoles enabled but the instance in
2970             # question is not actually listening for connections.
2971             raise exception.ConsoleTypeUnavailable(console_type='spice')
2972 
2973         ports = get_spice_ports_for_instance(instance.name)
2974         host = CONF.spice.server_proxyclient_address
2975 
2976         return ctype.ConsoleSpice(host=host, port=ports[0], tlsPort=ports[1])
2977 
2978     def get_serial_console(self, context, instance):
2979         guest = self._host.get_guest(instance)
2980         for hostname, port in self._get_serial_ports_from_guest(
2981                 guest, mode='bind'):
2982             return ctype.ConsoleSerial(host=hostname, port=port)
2983         raise exception.ConsoleTypeUnavailable(console_type='serial')
2984 
2985     @staticmethod
2986     def _create_ephemeral(target, ephemeral_size,
2987                           fs_label, os_type, is_block_dev=False,
2988                           context=None, specified_fs=None,
2989                           vm_mode=None):
2990         if not is_block_dev:
2991             if (CONF.libvirt.virt_type == "parallels" and
2992                     vm_mode == fields.VMMode.EXE):
2993 
2994                 libvirt_utils.create_ploop_image('expanded', target,
2995                                                  '%dG' % ephemeral_size,
2996                                                  specified_fs)
2997                 return
2998             libvirt_utils.create_image('raw', target, '%dG' % ephemeral_size)
2999 
3000         # Run as root only for block devices.
3001         disk_api.mkfs(os_type, fs_label, target, run_as_root=is_block_dev,
3002                       specified_fs=specified_fs)
3003 
3004     @staticmethod
3005     def _create_swap(target, swap_mb, context=None):
3006         """Create a swap file of specified size."""
3007         libvirt_utils.create_image('raw', target, '%dM' % swap_mb)
3008         utils.mkfs('swap', target)
3009 
3010     @staticmethod
3011     def _get_console_log_path(instance):
3012         return os.path.join(libvirt_utils.get_instance_path(instance),
3013                             'console.log')
3014 
3015     def _ensure_console_log_for_instance(self, instance):
3016         # NOTE(mdbooth): Although libvirt will create this file for us
3017         # automatically when it starts, it will initially create it with
3018         # root ownership and then chown it depending on the configuration of
3019         # the domain it is launching. Quobyte CI explicitly disables the
3020         # chown by setting dynamic_ownership=0 in libvirt's config.
3021         # Consequently when the domain starts it is unable to write to its
3022         # console.log. See bug https://bugs.launchpad.net/nova/+bug/1597644
3023         #
3024         # To work around this, we create the file manually before starting
3025         # the domain so it has the same ownership as Nova. This works
3026         # for Quobyte CI because it is also configured to run qemu as the same
3027         # user as the Nova service. Installations which don't set
3028         # dynamic_ownership=0 are not affected because libvirt will always
3029         # correctly configure permissions regardless of initial ownership.
3030         #
3031         # Setting dynamic_ownership=0 is dubious and potentially broken in
3032         # more ways than console.log (see comment #22 on the above bug), so
3033         # Future Maintainer who finds this code problematic should check to see
3034         # if we still support it.
3035         console_file = self._get_console_log_path(instance)
3036         LOG.debug('Ensure instance console log exists: %s', console_file,
3037                   instance=instance)
3038         try:
3039             libvirt_utils.file_open(console_file, 'a').close()
3040         # NOTE(sfinucan): We can safely ignore permission issues here and
3041         # assume that it is libvirt that has taken ownership of this file.
3042         except IOError as ex:
3043             if ex.errno != errno.EACCES:
3044                 raise
3045             LOG.debug('Console file already exists: %s.', console_file)
3046 
3047     @staticmethod
3048     def _get_disk_config_image_type():
3049         # TODO(mikal): there is a bug here if images_type has
3050         # changed since creation of the instance, but I am pretty
3051         # sure that this bug already exists.
3052         return 'rbd' if CONF.libvirt.images_type == 'rbd' else 'raw'
3053 
3054     @staticmethod
3055     def _is_booted_from_volume(block_device_info):
3056         """Determines whether the VM is booting from volume
3057 
3058         Determines whether the block device info indicates that the VM
3059         is booting from a volume.
3060         """
3061         block_device_mapping = driver.block_device_info_get_mapping(
3062             block_device_info)
3063         return bool(block_device.get_root_bdm(block_device_mapping))
3064 
3065     def _inject_data(self, disk, instance, injection_info):
3066         """Injects data in a disk image
3067 
3068         Helper used for injecting data in a disk image file system.
3069 
3070         :param disk: The disk we're injecting into (an Image object)
3071         :param instance: The instance we're injecting into
3072         :param injection_info: Injection info
3073         """
3074         # Handles the partition need to be used.
3075         LOG.debug('Checking root disk injection %s',
3076                   str(injection_info), instance=instance)
3077         target_partition = None
3078         if not instance.kernel_id:
3079             target_partition = CONF.libvirt.inject_partition
3080             if target_partition == 0:
3081                 target_partition = None
3082         if CONF.libvirt.virt_type == 'lxc':
3083             target_partition = None
3084 
3085         # Handles the key injection.
3086         if CONF.libvirt.inject_key and instance.get('key_data'):
3087             key = str(instance.key_data)
3088         else:
3089             key = None
3090 
3091         # Handles the admin password injection.
3092         if not CONF.libvirt.inject_password:
3093             admin_pass = None
3094         else:
3095             admin_pass = injection_info.admin_pass
3096 
3097         # Handles the network injection.
3098         net = netutils.get_injected_network_template(
3099             injection_info.network_info,
3100             libvirt_virt_type=CONF.libvirt.virt_type)
3101 
3102         # Handles the metadata injection
3103         metadata = instance.get('metadata')
3104 
3105         if any((key, net, metadata, admin_pass, injection_info.files)):
3106             LOG.debug('Injecting %s', str(injection_info),
3107                       instance=instance)
3108             img_id = instance.image_ref
3109             try:
3110                 disk_api.inject_data(disk.get_model(self._conn),
3111                                      key, net, metadata, admin_pass,
3112                                      injection_info.files,
3113                                      partition=target_partition,
3114                                      mandatory=('files',))
3115             except Exception as e:
3116                 with excutils.save_and_reraise_exception():
3117                     LOG.error('Error injecting data into image '
3118                               '%(img_id)s (%(e)s)',
3119                               {'img_id': img_id, 'e': e},
3120                               instance=instance)
3121 
3122     # NOTE(sileht): many callers of this method assume that this
3123     # method doesn't fail if an image already exists but instead
3124     # think that it will be reused (ie: (live)-migration/resize)
3125     def _create_image(self, context, instance,
3126                       disk_mapping, injection_info=None, suffix='',
3127                       disk_images=None, block_device_info=None,
3128                       fallback_from_host=None,
3129                       ignore_bdi_for_swap=False):
3130         booted_from_volume = self._is_booted_from_volume(block_device_info)
3131 
3132         def image(fname, image_type=CONF.libvirt.images_type):
3133             return self.image_backend.by_name(instance,
3134                                               fname + suffix, image_type)
3135 
3136         def raw(fname):
3137             return image(fname, image_type='raw')
3138 
3139         # ensure directories exist and are writable
3140         fileutils.ensure_tree(libvirt_utils.get_instance_path(instance))
3141 
3142         LOG.info('Creating image', instance=instance)
3143 
3144         inst_type = instance.get_flavor()
3145         swap_mb = 0
3146         if 'disk.swap' in disk_mapping:
3147             mapping = disk_mapping['disk.swap']
3148 
3149             if ignore_bdi_for_swap:
3150                 # This is a workaround to support legacy swap resizing,
3151                 # which does not touch swap size specified in bdm,
3152                 # but works with flavor specified size only.
3153                 # In this case we follow the legacy logic and ignore block
3154                 # device info completely.
3155                 # NOTE(ft): This workaround must be removed when a correct
3156                 # implementation of resize operation changing sizes in bdms is
3157                 # developed. Also at that stage we probably may get rid of
3158                 # the direct usage of flavor swap size here,
3159                 # leaving the work with bdm only.
3160                 swap_mb = inst_type['swap']
3161             else:
3162                 swap = driver.block_device_info_get_swap(block_device_info)
3163                 if driver.swap_is_usable(swap):
3164                     swap_mb = swap['swap_size']
3165                 elif (inst_type['swap'] > 0 and
3166                       not block_device.volume_in_mapping(
3167                         mapping['dev'], block_device_info)):
3168                     swap_mb = inst_type['swap']
3169 
3170             if swap_mb > 0:
3171                 if (CONF.libvirt.virt_type == "parallels" and
3172                         instance.vm_mode == fields.VMMode.EXE):
3173                     msg = _("Swap disk is not supported "
3174                             "for Virtuozzo container")
3175                     raise exception.Invalid(msg)
3176 
3177         if not disk_images:
3178             disk_images = {'image_id': instance.image_ref,
3179                            'kernel_id': instance.kernel_id,
3180                            'ramdisk_id': instance.ramdisk_id}
3181 
3182         if disk_images['kernel_id']:
3183             fname = imagecache.get_cache_fname(disk_images['kernel_id'])
3184             raw('kernel').cache(fetch_func=libvirt_utils.fetch_raw_image,
3185                                 context=context,
3186                                 filename=fname,
3187                                 image_id=disk_images['kernel_id'])
3188             if disk_images['ramdisk_id']:
3189                 fname = imagecache.get_cache_fname(disk_images['ramdisk_id'])
3190                 raw('ramdisk').cache(fetch_func=libvirt_utils.fetch_raw_image,
3191                                      context=context,
3192                                      filename=fname,
3193                                      image_id=disk_images['ramdisk_id'])
3194 
3195         if CONF.libvirt.virt_type == 'uml':
3196             # PONDERING(mikal): can I assume that root is UID zero in every
3197             # OS? Probably not.
3198             uid = pwd.getpwnam('root').pw_uid
3199             nova.privsep.path.chown(image('disk').path, uid=uid)
3200 
3201         self._create_and_inject_local_root(context, instance,
3202                                            booted_from_volume, suffix,
3203                                            disk_images, injection_info,
3204                                            fallback_from_host)
3205 
3206         # Lookup the filesystem type if required
3207         os_type_with_default = disk_api.get_fs_type_for_os_type(
3208             instance.os_type)
3209         # Generate a file extension based on the file system
3210         # type and the mkfs commands configured if any
3211         file_extension = disk_api.get_file_extension_for_os_type(
3212                                                           os_type_with_default)
3213 
3214         vm_mode = fields.VMMode.get_from_instance(instance)
3215         ephemeral_gb = instance.flavor.ephemeral_gb
3216         if 'disk.local' in disk_mapping:
3217             disk_image = image('disk.local')
3218             fn = functools.partial(self._create_ephemeral,
3219                                    fs_label='ephemeral0',
3220                                    os_type=instance.os_type,
3221                                    is_block_dev=disk_image.is_block_dev,
3222                                    vm_mode=vm_mode)
3223             fname = "ephemeral_%s_%s" % (ephemeral_gb, file_extension)
3224             size = ephemeral_gb * units.Gi
3225             disk_image.cache(fetch_func=fn,
3226                              context=context,
3227                              filename=fname,
3228                              size=size,
3229                              ephemeral_size=ephemeral_gb)
3230 
3231         for idx, eph in enumerate(driver.block_device_info_get_ephemerals(
3232                 block_device_info)):
3233             disk_image = image(blockinfo.get_eph_disk(idx))
3234 
3235             specified_fs = eph.get('guest_format')
3236             if specified_fs and not self.is_supported_fs_format(specified_fs):
3237                 msg = _("%s format is not supported") % specified_fs
3238                 raise exception.InvalidBDMFormat(details=msg)
3239 
3240             fn = functools.partial(self._create_ephemeral,
3241                                    fs_label='ephemeral%d' % idx,
3242                                    os_type=instance.os_type,
3243                                    is_block_dev=disk_image.is_block_dev,
3244                                    vm_mode=vm_mode)
3245             size = eph['size'] * units.Gi
3246             fname = "ephemeral_%s_%s" % (eph['size'], file_extension)
3247             disk_image.cache(fetch_func=fn,
3248                              context=context,
3249                              filename=fname,
3250                              size=size,
3251                              ephemeral_size=eph['size'],
3252                              specified_fs=specified_fs)
3253 
3254         if swap_mb > 0:
3255             size = swap_mb * units.Mi
3256             image('disk.swap').cache(fetch_func=self._create_swap,
3257                                      context=context,
3258                                      filename="swap_%s" % swap_mb,
3259                                      size=size,
3260                                      swap_mb=swap_mb)
3261 
3262     def _create_and_inject_local_root(self, context, instance,
3263                                       booted_from_volume, suffix, disk_images,
3264                                       injection_info, fallback_from_host):
3265         # File injection only if needed
3266         need_inject = (not configdrive.required_by(instance) and
3267                        injection_info is not None and
3268                        CONF.libvirt.inject_partition != -2)
3269 
3270         # NOTE(ndipanov): Even if disk_mapping was passed in, which
3271         # currently happens only on rescue - we still don't want to
3272         # create a base image.
3273         if not booted_from_volume:
3274             root_fname = imagecache.get_cache_fname(disk_images['image_id'])
3275             size = instance.flavor.root_gb * units.Gi
3276 
3277             if size == 0 or suffix == '.rescue':
3278                 size = None
3279 
3280             backend = self.image_backend.by_name(instance, 'disk' + suffix,
3281                                                  CONF.libvirt.images_type)
3282             if instance.task_state == task_states.RESIZE_FINISH:
3283                 backend.create_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME)
3284             if backend.SUPPORTS_CLONE:
3285                 def clone_fallback_to_fetch(*args, **kwargs):
3286                     try:
3287                         backend.clone(context, disk_images['image_id'])
3288                     except exception.ImageUnacceptable:
3289                         libvirt_utils.fetch_image(*args, **kwargs)
3290                 fetch_func = clone_fallback_to_fetch
3291             else:
3292                 fetch_func = libvirt_utils.fetch_image
3293             self._try_fetch_image_cache(backend, fetch_func, context,
3294                                         root_fname, disk_images['image_id'],
3295                                         instance, size, fallback_from_host)
3296 
3297             if need_inject:
3298                 self._inject_data(backend, instance, injection_info)
3299 
3300         elif need_inject:
3301             LOG.warning('File injection into a boot from volume '
3302                         'instance is not supported', instance=instance)
3303 
3304     def _create_configdrive(self, context, instance, injection_info,
3305                             rescue=False):
3306         # As this method being called right after the definition of a
3307         # domain, but before its actual launch, device metadata will be built
3308         # and saved in the instance for it to be used by the config drive and
3309         # the metadata service.
3310         instance.device_metadata = self._build_device_metadata(context,
3311                                                                instance)
3312         if configdrive.required_by(instance):
3313             LOG.info('Using config drive', instance=instance)
3314 
3315             name = 'disk.config'
3316             if rescue:
3317                 name += '.rescue'
3318 
3319             config_disk = self.image_backend.by_name(
3320                 instance, name, self._get_disk_config_image_type())
3321 
3322             # Don't overwrite an existing config drive
3323             if not config_disk.exists():
3324                 extra_md = {}
3325                 if injection_info.admin_pass:
3326                     extra_md['admin_pass'] = injection_info.admin_pass
3327 
3328                 inst_md = instance_metadata.InstanceMetadata(
3329                     instance, content=injection_info.files, extra_md=extra_md,
3330                     network_info=injection_info.network_info,
3331                     request_context=context)
3332 
3333                 cdb = configdrive.ConfigDriveBuilder(instance_md=inst_md)
3334                 with cdb:
3335                     # NOTE(mdbooth): We're hardcoding here the path of the
3336                     # config disk when using the flat backend. This isn't
3337                     # good, but it's required because we need a local path we
3338                     # know we can write to in case we're subsequently
3339                     # importing into rbd. This will be cleaned up when we
3340                     # replace this with a call to create_from_func, but that
3341                     # can't happen until we've updated the backends and we
3342                     # teach them not to cache config disks. This isn't
3343                     # possible while we're still using cache() under the hood.
3344                     config_disk_local_path = os.path.join(
3345                         libvirt_utils.get_instance_path(instance), name)
3346                     LOG.info('Creating config drive at %(path)s',
3347                              {'path': config_disk_local_path},
3348                              instance=instance)
3349 
3350                     try:
3351                         cdb.make_drive(config_disk_local_path)
3352                     except processutils.ProcessExecutionError as e:
3353                         with excutils.save_and_reraise_exception():
3354                             LOG.error('Creating config drive failed with '
3355                                       'error: %s', e, instance=instance)
3356 
3357                 try:
3358                     config_disk.import_file(
3359                         instance, config_disk_local_path, name)
3360                 finally:
3361                     # NOTE(mikal): if the config drive was imported into RBD,
3362                     # then we no longer need the local copy
3363                     if CONF.libvirt.images_type == 'rbd':
3364                         LOG.info('Deleting local config drive %(path)s '
3365                                  'because it was imported into RBD.',
3366                                  {'path': config_disk_local_path},
3367                                  instance=instance)
3368                         os.unlink(config_disk_local_path)
3369 
3370     def _prepare_pci_devices_for_use(self, pci_devices):
3371         # kvm , qemu support managed mode
3372         # In managed mode, the configured device will be automatically
3373         # detached from the host OS drivers when the guest is started,
3374         # and then re-attached when the guest shuts down.
3375         if CONF.libvirt.virt_type != 'xen':
3376             # we do manual detach only for xen
3377             return
3378         try:
3379             for dev in pci_devices:
3380                 libvirt_dev_addr = dev['hypervisor_name']
3381                 libvirt_dev = \
3382                         self._host.device_lookup_by_name(libvirt_dev_addr)
3383                 # Note(yjiang5) Spelling for 'dettach' is correct, see
3384                 # http://libvirt.org/html/libvirt-libvirt.html.
3385                 libvirt_dev.dettach()
3386 
3387             # Note(yjiang5): A reset of one PCI device may impact other
3388             # devices on the same bus, thus we need two separated loops
3389             # to detach and then reset it.
3390             for dev in pci_devices:
3391                 libvirt_dev_addr = dev['hypervisor_name']
3392                 libvirt_dev = \
3393                         self._host.device_lookup_by_name(libvirt_dev_addr)
3394                 libvirt_dev.reset()
3395 
3396         except libvirt.libvirtError as exc:
3397             raise exception.PciDevicePrepareFailed(id=dev['id'],
3398                                                    instance_uuid=
3399                                                    dev['instance_uuid'],
3400                                                    reason=six.text_type(exc))
3401 
3402     def _detach_pci_devices(self, guest, pci_devs):
3403         try:
3404             for dev in pci_devs:
3405                 guest.detach_device(self._get_guest_pci_device(dev), live=True)
3406                 # after detachDeviceFlags returned, we should check the dom to
3407                 # ensure the detaching is finished
3408                 xml = guest.get_xml_desc()
3409                 xml_doc = etree.fromstring(xml)
3410                 guest_config = vconfig.LibvirtConfigGuest()
3411                 guest_config.parse_dom(xml_doc)
3412 
3413                 for hdev in [d for d in guest_config.devices
3414                     if isinstance(d, vconfig.LibvirtConfigGuestHostdevPCI)]:
3415                     hdbsf = [hdev.domain, hdev.bus, hdev.slot, hdev.function]
3416                     dbsf = pci_utils.parse_address(dev.address)
3417                     if [int(x, 16) for x in hdbsf] ==\
3418                             [int(x, 16) for x in dbsf]:
3419                         raise exception.PciDeviceDetachFailed(reason=
3420                                                               "timeout",
3421                                                               dev=dev)
3422 
3423         except libvirt.libvirtError as ex:
3424             error_code = ex.get_error_code()
3425             if error_code == libvirt.VIR_ERR_NO_DOMAIN:
3426                 LOG.warning("Instance disappeared while detaching "
3427                             "a PCI device from it.")
3428             else:
3429                 raise
3430 
3431     def _attach_pci_devices(self, guest, pci_devs):
3432         try:
3433             for dev in pci_devs:
3434                 guest.attach_device(self._get_guest_pci_device(dev))
3435 
3436         except libvirt.libvirtError:
3437             LOG.error('Attaching PCI devices %(dev)s to %(dom)s failed.',
3438                       {'dev': pci_devs, 'dom': guest.id})
3439             raise
3440 
3441     @staticmethod
3442     def _has_direct_passthrough_port(network_info):
3443         for vif in network_info:
3444             if (vif['vnic_type'] in
3445                 network_model.VNIC_TYPES_DIRECT_PASSTHROUGH):
3446                 return True
3447         return False
3448 
3449     def _attach_direct_passthrough_ports(
3450         self, context, instance, guest, network_info=None):
3451         if network_info is None:
3452             network_info = instance.info_cache.network_info
3453         if network_info is None:
3454             return
3455 
3456         if self._has_direct_passthrough_port(network_info):
3457             for vif in network_info:
3458                 if (vif['vnic_type'] in
3459                     network_model.VNIC_TYPES_DIRECT_PASSTHROUGH):
3460                     cfg = self.vif_driver.get_config(instance,
3461                                                      vif,
3462                                                      instance.image_meta,
3463                                                      instance.flavor,
3464                                                      CONF.libvirt.virt_type,
3465                                                      self._host)
3466                     LOG.debug('Attaching direct passthrough port %(port)s '
3467                               'to %(dom)s', {'port': vif, 'dom': guest.id},
3468                               instance=instance)
3469                     guest.attach_device(cfg)
3470 
3471     def _detach_direct_passthrough_ports(self, context, instance, guest):
3472         network_info = instance.info_cache.network_info
3473         if network_info is None:
3474             return
3475 
3476         if self._has_direct_passthrough_port(network_info):
3477             # In case of VNIC_TYPES_DIRECT_PASSTHROUGH ports we create
3478             # pci request per direct passthrough port. Therefore we can trust
3479             # that pci_slot value in the vif is correct.
3480             direct_passthrough_pci_addresses = [
3481                 vif['profile']['pci_slot']
3482                 for vif in network_info
3483                 if (vif['vnic_type'] in
3484                     network_model.VNIC_TYPES_DIRECT_PASSTHROUGH and
3485                     vif['profile'].get('pci_slot') is not None)
3486             ]
3487 
3488             # use detach_pci_devices to avoid failure in case of
3489             # multiple guest direct passthrough ports with the same MAC
3490             # (protection use-case, ports are on different physical
3491             # interfaces)
3492             pci_devs = pci_manager.get_instance_pci_devs(instance, 'all')
3493             direct_passthrough_pci_addresses = (
3494                 [pci_dev for pci_dev in pci_devs
3495                  if pci_dev.address in direct_passthrough_pci_addresses])
3496             self._detach_pci_devices(guest, direct_passthrough_pci_addresses)
3497 
3498     def _set_host_enabled(self, enabled,
3499                           disable_reason=DISABLE_REASON_UNDEFINED):
3500         """Enables / Disables the compute service on this host.
3501 
3502            This doesn't override non-automatic disablement with an automatic
3503            setting; thereby permitting operators to keep otherwise
3504            healthy hosts out of rotation.
3505         """
3506 
3507         status_name = {True: 'disabled',
3508                        False: 'enabled'}
3509 
3510         disable_service = not enabled
3511 
3512         ctx = nova_context.get_admin_context()
3513         try:
3514             service = objects.Service.get_by_compute_host(ctx, CONF.host)
3515 
3516             if service.disabled != disable_service:
3517                 # Note(jang): this is a quick fix to stop operator-
3518                 # disabled compute hosts from re-enabling themselves
3519                 # automatically. We prefix any automatic reason code
3520                 # with a fixed string. We only re-enable a host
3521                 # automatically if we find that string in place.
3522                 # This should probably be replaced with a separate flag.
3523                 if not service.disabled or (
3524                         service.disabled_reason and
3525                         service.disabled_reason.startswith(DISABLE_PREFIX)):
3526                     service.disabled = disable_service
3527                     service.disabled_reason = (
3528                        DISABLE_PREFIX + disable_reason
3529                        if disable_service and disable_reason else
3530                            DISABLE_REASON_UNDEFINED)
3531                     service.save()
3532                     LOG.debug('Updating compute service status to %s',
3533                               status_name[disable_service])
3534                 else:
3535                     LOG.debug('Not overriding manual compute service '
3536                               'status with: %s',
3537                               status_name[disable_service])
3538         except exception.ComputeHostNotFound:
3539             LOG.warning('Cannot update service status on host "%s" '
3540                         'since it is not registered.', CONF.host)
3541         except Exception:
3542             LOG.warning('Cannot update service status on host "%s" '
3543                         'due to an unexpected exception.', CONF.host,
3544                         exc_info=True)
3545 
3546         if enabled:
3547             mount.get_manager().host_up(self._host)
3548         else:
3549             mount.get_manager().host_down()
3550 
3551     def _get_guest_cpu_model_config(self):
3552         mode = CONF.libvirt.cpu_mode
3553         model = CONF.libvirt.cpu_model
3554 
3555         if (CONF.libvirt.virt_type == "kvm" or
3556             CONF.libvirt.virt_type == "qemu"):
3557             if mode is None:
3558                 caps = self._host.get_capabilities()
3559                 # AArch64 lacks 'host-model' support because neither libvirt
3560                 # nor QEMU are able to tell what the host CPU model exactly is.
3561                 # And there is no CPU description code for ARM(64) at this
3562                 # point.
3563 
3564                 # Also worth noting: 'host-passthrough' mode will completely
3565                 # break live migration, *unless* all the Compute nodes (running
3566                 # libvirtd) have *identical* CPUs.
3567                 if caps.host.cpu.arch == fields.Architecture.AARCH64:
3568                     mode = "host-passthrough"
3569                     LOG.info('CPU mode "host-passthrough" was chosen. Live '
3570                              'migration can break unless all compute nodes '
3571                              'have identical cpus. AArch64 does not support '
3572                              'other modes.')
3573                 else:
3574                     mode = "host-model"
3575             if mode == "none":
3576                 return vconfig.LibvirtConfigGuestCPU()
3577         else:
3578             if mode is None or mode == "none":
3579                 return None
3580 
3581         if ((CONF.libvirt.virt_type != "kvm" and
3582              CONF.libvirt.virt_type != "qemu")):
3583             msg = _("Config requested an explicit CPU model, but "
3584                     "the current libvirt hypervisor '%s' does not "
3585                     "support selecting CPU models") % CONF.libvirt.virt_type
3586             raise exception.Invalid(msg)
3587 
3588         if mode == "custom" and model is None:
3589             msg = _("Config requested a custom CPU model, but no "
3590                     "model name was provided")
3591             raise exception.Invalid(msg)
3592         elif mode != "custom" and model is not None:
3593             msg = _("A CPU model name should not be set when a "
3594                     "host CPU model is requested")
3595             raise exception.Invalid(msg)
3596 
3597         LOG.debug("CPU mode '%(mode)s' model '%(model)s' was chosen",
3598                   {'mode': mode, 'model': (model or "")})
3599 
3600         cpu = vconfig.LibvirtConfigGuestCPU()
3601         cpu.mode = mode
3602         cpu.model = model
3603 
3604         return cpu
3605 
3606     def _get_guest_cpu_config(self, flavor, image_meta,
3607                               guest_cpu_numa_config, instance_numa_topology):
3608         cpu = self._get_guest_cpu_model_config()
3609 
3610         if cpu is None:
3611             return None
3612 
3613         topology = hardware.get_best_cpu_topology(
3614                 flavor, image_meta, numa_topology=instance_numa_topology)
3615 
3616         cpu.sockets = topology.sockets
3617         cpu.cores = topology.cores
3618         cpu.threads = topology.threads
3619         cpu.numa = guest_cpu_numa_config
3620 
3621         return cpu
3622 
3623     def _get_guest_disk_config(self, instance, name, disk_mapping, inst_type,
3624                                image_type=None):
3625         disk_unit = None
3626         disk = self.image_backend.by_name(instance, name, image_type)
3627         if (name == 'disk.config' and image_type == 'rbd' and
3628                 not disk.exists()):
3629             # This is likely an older config drive that has not been migrated
3630             # to rbd yet. Try to fall back on 'flat' image type.
3631             # TODO(melwitt): Add online migration of some sort so we can
3632             # remove this fall back once we know all config drives are in rbd.
3633             # NOTE(vladikr): make sure that the flat image exist, otherwise
3634             # the image will be created after the domain definition.
3635             flat_disk = self.image_backend.by_name(instance, name, 'flat')
3636             if flat_disk.exists():
3637                 disk = flat_disk
3638                 LOG.debug('Config drive not found in RBD, falling back to the '
3639                           'instance directory', instance=instance)
3640         disk_info = disk_mapping[name]
3641         if 'unit' in disk_mapping:
3642             disk_unit = disk_mapping['unit']
3643             disk_mapping['unit'] += 1  # Increments for the next disk added
3644         conf = disk.libvirt_info(disk_info['bus'],
3645                                  disk_info['dev'],
3646                                  disk_info['type'],
3647                                  self.disk_cachemode,
3648                                  inst_type['extra_specs'],
3649                                  self._host.get_version(),
3650                                  disk_unit=disk_unit)
3651         return conf
3652 
3653     def _get_guest_fs_config(self, instance, name, image_type=None):
3654         disk = self.image_backend.by_name(instance, name, image_type)
3655         return disk.libvirt_fs_info("/", "ploop")
3656 
3657     def _get_guest_storage_config(self, instance, image_meta,
3658                                   disk_info,
3659                                   rescue, block_device_info,
3660                                   inst_type, os_type):
3661         devices = []
3662         disk_mapping = disk_info['mapping']
3663 
3664         block_device_mapping = driver.block_device_info_get_mapping(
3665             block_device_info)
3666         mount_rootfs = CONF.libvirt.virt_type == "lxc"
3667         scsi_controller = self._get_scsi_controller(image_meta)
3668 
3669         if scsi_controller and scsi_controller.model == 'virtio-scsi':
3670             # The virtio-scsi can handle up to 256 devices but the
3671             # optional element "address" must be defined to describe
3672             # where the device is placed on the controller (see:
3673             # LibvirtConfigGuestDeviceAddressDrive).
3674             #
3675             # Note about why it's added in disk_mapping: It's not
3676             # possible to pass an 'int' by reference in Python, so we
3677             # use disk_mapping as container to keep reference of the
3678             # unit added and be able to increment it for each disk
3679             # added.
3680             disk_mapping['unit'] = 0
3681 
3682         def _get_ephemeral_devices():
3683             eph_devices = []
3684             for idx, eph in enumerate(
3685                 driver.block_device_info_get_ephemerals(
3686                     block_device_info)):
3687                 diskeph = self._get_guest_disk_config(
3688                     instance,
3689                     blockinfo.get_eph_disk(idx),
3690                     disk_mapping, inst_type)
3691                 eph_devices.append(diskeph)
3692             return eph_devices
3693 
3694         if mount_rootfs:
3695             fs = vconfig.LibvirtConfigGuestFilesys()
3696             fs.source_type = "mount"
3697             fs.source_dir = os.path.join(
3698                 libvirt_utils.get_instance_path(instance), 'rootfs')
3699             devices.append(fs)
3700         elif (os_type == fields.VMMode.EXE and
3701               CONF.libvirt.virt_type == "parallels"):
3702             if rescue:
3703                 fsrescue = self._get_guest_fs_config(instance, "disk.rescue")
3704                 devices.append(fsrescue)
3705 
3706                 fsos = self._get_guest_fs_config(instance, "disk")
3707                 fsos.target_dir = "/mnt/rescue"
3708                 devices.append(fsos)
3709             else:
3710                 if 'disk' in disk_mapping:
3711                     fs = self._get_guest_fs_config(instance, "disk")
3712                     devices.append(fs)
3713                 devices = devices + _get_ephemeral_devices()
3714         else:
3715 
3716             if rescue:
3717                 diskrescue = self._get_guest_disk_config(instance,
3718                                                          'disk.rescue',
3719                                                          disk_mapping,
3720                                                          inst_type)
3721                 devices.append(diskrescue)
3722 
3723                 diskos = self._get_guest_disk_config(instance,
3724                                                      'disk',
3725                                                      disk_mapping,
3726                                                      inst_type)
3727                 devices.append(diskos)
3728             else:
3729                 if 'disk' in disk_mapping:
3730                     diskos = self._get_guest_disk_config(instance,
3731                                                          'disk',
3732                                                          disk_mapping,
3733                                                          inst_type)
3734                     devices.append(diskos)
3735 
3736                 if 'disk.local' in disk_mapping:
3737                     disklocal = self._get_guest_disk_config(instance,
3738                                                             'disk.local',
3739                                                             disk_mapping,
3740                                                             inst_type)
3741                     devices.append(disklocal)
3742                     instance.default_ephemeral_device = (
3743                         block_device.prepend_dev(disklocal.target_dev))
3744 
3745                 devices = devices + _get_ephemeral_devices()
3746 
3747                 if 'disk.swap' in disk_mapping:
3748                     diskswap = self._get_guest_disk_config(instance,
3749                                                            'disk.swap',
3750                                                            disk_mapping,
3751                                                            inst_type)
3752                     devices.append(diskswap)
3753                     instance.default_swap_device = (
3754                         block_device.prepend_dev(diskswap.target_dev))
3755 
3756             config_name = 'disk.config.rescue' if rescue else 'disk.config'
3757             if config_name in disk_mapping:
3758                 diskconfig = self._get_guest_disk_config(
3759                     instance, config_name, disk_mapping, inst_type,
3760                     self._get_disk_config_image_type())
3761                 devices.append(diskconfig)
3762 
3763         for vol in block_device.get_bdms_to_connect(block_device_mapping,
3764                                                    mount_rootfs):
3765             connection_info = vol['connection_info']
3766             vol_dev = block_device.prepend_dev(vol['mount_device'])
3767             info = disk_mapping[vol_dev]
3768             self._connect_volume(connection_info, instance)
3769             if scsi_controller and scsi_controller.model == 'virtio-scsi':
3770                 info['unit'] = disk_mapping['unit']
3771                 disk_mapping['unit'] += 1
3772             cfg = self._get_volume_config(connection_info, info)
3773             devices.append(cfg)
3774             vol['connection_info'] = connection_info
3775             vol.save()
3776 
3777         for d in devices:
3778             self._set_cache_mode(d)
3779 
3780         if scsi_controller:
3781             devices.append(scsi_controller)
3782 
3783         return devices
3784 
3785     @staticmethod
3786     def _get_scsi_controller(image_meta):
3787         """Return scsi controller or None based on image meta"""
3788         # TODO(sahid): should raise an exception for an invalid controller
3789         if image_meta.properties.get('hw_scsi_model'):
3790             hw_scsi_model = image_meta.properties.hw_scsi_model
3791             scsi_controller = vconfig.LibvirtConfigGuestController()
3792             scsi_controller.type = 'scsi'
3793             scsi_controller.model = hw_scsi_model
3794             scsi_controller.index = 0
3795             return scsi_controller
3796 
3797     def _get_host_sysinfo_serial_hardware(self):
3798         """Get a UUID from the host hardware
3799 
3800         Get a UUID for the host hardware reported by libvirt.
3801         This is typically from the SMBIOS data, unless it has
3802         been overridden in /etc/libvirt/libvirtd.conf
3803         """
3804         caps = self._host.get_capabilities()
3805         return caps.host.uuid
3806 
3807     def _get_host_sysinfo_serial_os(self):
3808         """Get a UUID from the host operating system
3809 
3810         Get a UUID for the host operating system. Modern Linux
3811         distros based on systemd provide a /etc/machine-id
3812         file containing a UUID. This is also provided inside
3813         systemd based containers and can be provided by other
3814         init systems too, since it is just a plain text file.
3815         """
3816         if not os.path.exists("/etc/machine-id"):
3817             msg = _("Unable to get host UUID: /etc/machine-id does not exist")
3818             raise exception.InternalError(msg)
3819 
3820         with open("/etc/machine-id") as f:
3821             # We want to have '-' in the right place
3822             # so we parse & reformat the value
3823             lines = f.read().split()
3824             if not lines:
3825                 msg = _("Unable to get host UUID: /etc/machine-id is empty")
3826                 raise exception.InternalError(msg)
3827 
3828             return str(uuid.UUID(lines[0]))
3829 
3830     def _get_host_sysinfo_serial_auto(self):
3831         if os.path.exists("/etc/machine-id"):
3832             return self._get_host_sysinfo_serial_os()
3833         else:
3834             return self._get_host_sysinfo_serial_hardware()
3835 
3836     def _get_guest_config_sysinfo(self, instance):
3837         sysinfo = vconfig.LibvirtConfigGuestSysinfo()
3838 
3839         sysinfo.system_manufacturer = version.vendor_string()
3840         sysinfo.system_product = version.product_string()
3841         sysinfo.system_version = version.version_string_with_package()
3842 
3843         sysinfo.system_serial = self._sysinfo_serial_func()
3844         sysinfo.system_uuid = instance.uuid
3845 
3846         sysinfo.system_family = "Virtual Machine"
3847 
3848         return sysinfo
3849 
3850     def _get_guest_pci_device(self, pci_device):
3851 
3852         dbsf = pci_utils.parse_address(pci_device.address)
3853         dev = vconfig.LibvirtConfigGuestHostdevPCI()
3854         dev.domain, dev.bus, dev.slot, dev.function = dbsf
3855 
3856         # only kvm support managed mode
3857         if CONF.libvirt.virt_type in ('xen', 'parallels',):
3858             dev.managed = 'no'
3859         if CONF.libvirt.virt_type in ('kvm', 'qemu'):
3860             dev.managed = 'yes'
3861 
3862         return dev
3863 
3864     def _get_guest_config_meta(self, instance):
3865         """Get metadata config for guest."""
3866 
3867         meta = vconfig.LibvirtConfigGuestMetaNovaInstance()
3868         meta.package = version.version_string_with_package()
3869         meta.name = instance.display_name
3870         meta.creationTime = time.time()
3871 
3872         if instance.image_ref not in ("", None):
3873             meta.roottype = "image"
3874             meta.rootid = instance.image_ref
3875 
3876         system_meta = instance.system_metadata
3877         ometa = vconfig.LibvirtConfigGuestMetaNovaOwner()
3878         ometa.userid = instance.user_id
3879         ometa.username = system_meta.get('owner_user_name', 'N/A')
3880         ometa.projectid = instance.project_id
3881         ometa.projectname = system_meta.get('owner_project_name', 'N/A')
3882         meta.owner = ometa
3883 
3884         fmeta = vconfig.LibvirtConfigGuestMetaNovaFlavor()
3885         flavor = instance.flavor
3886         fmeta.name = flavor.name
3887         fmeta.memory = flavor.memory_mb
3888         fmeta.vcpus = flavor.vcpus
3889         fmeta.ephemeral = flavor.ephemeral_gb
3890         fmeta.disk = flavor.root_gb
3891         fmeta.swap = flavor.swap
3892 
3893         meta.flavor = fmeta
3894 
3895         return meta
3896 
3897     def _machine_type_mappings(self):
3898         mappings = {}
3899         for mapping in CONF.libvirt.hw_machine_type:
3900             host_arch, _, machine_type = mapping.partition('=')
3901             mappings[host_arch] = machine_type
3902         return mappings
3903 
3904     def _get_machine_type(self, image_meta, caps):
3905         # The underlying machine type can be set as an image attribute,
3906         # or otherwise based on some architecture specific defaults
3907 
3908         mach_type = None
3909 
3910         if image_meta.properties.get('hw_machine_type') is not None:
3911             mach_type = image_meta.properties.hw_machine_type
3912         else:
3913             # For ARM systems we will default to vexpress-a15 for armv7
3914             # and virt for aarch64
3915             if caps.host.cpu.arch == fields.Architecture.ARMV7:
3916                 mach_type = "vexpress-a15"
3917 
3918             if caps.host.cpu.arch == fields.Architecture.AARCH64:
3919                 mach_type = "virt"
3920 
3921             if caps.host.cpu.arch in (fields.Architecture.S390,
3922                                       fields.Architecture.S390X):
3923                 mach_type = 's390-ccw-virtio'
3924 
3925             # If set in the config, use that as the default.
3926             if CONF.libvirt.hw_machine_type:
3927                 mappings = self._machine_type_mappings()
3928                 mach_type = mappings.get(caps.host.cpu.arch)
3929 
3930         return mach_type
3931 
3932     @staticmethod
3933     def _create_idmaps(klass, map_strings):
3934         idmaps = []
3935         if len(map_strings) > 5:
3936             map_strings = map_strings[0:5]
3937             LOG.warning("Too many id maps, only included first five.")
3938         for map_string in map_strings:
3939             try:
3940                 idmap = klass()
3941                 values = [int(i) for i in map_string.split(":")]
3942                 idmap.start = values[0]
3943                 idmap.target = values[1]
3944                 idmap.count = values[2]
3945                 idmaps.append(idmap)
3946             except (ValueError, IndexError):
3947                 LOG.warning("Invalid value for id mapping %s", map_string)
3948         return idmaps
3949 
3950     def _get_guest_idmaps(self):
3951         id_maps = []
3952         if CONF.libvirt.virt_type == 'lxc' and CONF.libvirt.uid_maps:
3953             uid_maps = self._create_idmaps(vconfig.LibvirtConfigGuestUIDMap,
3954                                            CONF.libvirt.uid_maps)
3955             id_maps.extend(uid_maps)
3956         if CONF.libvirt.virt_type == 'lxc' and CONF.libvirt.gid_maps:
3957             gid_maps = self._create_idmaps(vconfig.LibvirtConfigGuestGIDMap,
3958                                            CONF.libvirt.gid_maps)
3959             id_maps.extend(gid_maps)
3960         return id_maps
3961 
3962     def _update_guest_cputune(self, guest, flavor, virt_type):
3963         is_able = self._host.is_cpu_control_policy_capable()
3964 
3965         cputuning = ['shares', 'period', 'quota']
3966         wants_cputune = any([k for k in cputuning
3967             if "quota:cpu_" + k in flavor.extra_specs.keys()])
3968 
3969         if wants_cputune and not is_able:
3970             raise exception.UnsupportedHostCPUControlPolicy()
3971 
3972         if not is_able or virt_type not in ('lxc', 'kvm', 'qemu'):
3973             return
3974 
3975         if guest.cputune is None:
3976             guest.cputune = vconfig.LibvirtConfigGuestCPUTune()
3977             # Setting the default cpu.shares value to be a value
3978             # dependent on the number of vcpus
3979         guest.cputune.shares = 1024 * guest.vcpus
3980 
3981         for name in cputuning:
3982             key = "quota:cpu_" + name
3983             if key in flavor.extra_specs:
3984                 setattr(guest.cputune, name,
3985                         int(flavor.extra_specs[key]))
3986 
3987     def _get_cpu_numa_config_from_instance(self, instance_numa_topology,
3988                                            wants_hugepages):
3989         if instance_numa_topology:
3990             guest_cpu_numa = vconfig.LibvirtConfigGuestCPUNUMA()
3991             for instance_cell in instance_numa_topology.cells:
3992                 guest_cell = vconfig.LibvirtConfigGuestCPUNUMACell()
3993                 guest_cell.id = instance_cell.id
3994                 guest_cell.cpus = instance_cell.cpuset
3995                 guest_cell.memory = instance_cell.memory * units.Ki
3996 
3997                 # The vhost-user network backend requires file backed
3998                 # guest memory (ie huge pages) to be marked as shared
3999                 # access, not private, so an external process can read
4000                 # and write the pages.
4001                 #
4002                 # You can't change the shared vs private flag for an
4003                 # already running guest, and since we can't predict what
4004                 # types of NIC may be hotplugged, we have no choice but
4005                 # to unconditionally turn on the shared flag. This has
4006                 # no real negative functional effect on the guest, so
4007                 # is a reasonable approach to take
4008                 if wants_hugepages:
4009                     guest_cell.memAccess = "shared"
4010                 guest_cpu_numa.cells.append(guest_cell)
4011             return guest_cpu_numa
4012 
4013     def _has_cpu_policy_support(self):
4014         for ver in BAD_LIBVIRT_CPU_POLICY_VERSIONS:
4015             if self._host.has_version(ver):
4016                 ver_ = self._version_to_string(ver)
4017                 raise exception.CPUPinningNotSupported(reason=_(
4018                     'Invalid libvirt version %(version)s') % {'version': ver_})
4019         return True
4020 
4021     def _wants_hugepages(self, host_topology, instance_topology):
4022         """Determine if the guest / host topology implies the
4023            use of huge pages for guest RAM backing
4024         """
4025 
4026         if host_topology is None or instance_topology is None:
4027             return False
4028 
4029         avail_pagesize = [page.size_kb
4030                           for page in host_topology.cells[0].mempages]
4031         avail_pagesize.sort()
4032         # Remove smallest page size as that's not classed as a largepage
4033         avail_pagesize = avail_pagesize[1:]
4034 
4035         # See if we have page size set
4036         for cell in instance_topology.cells:
4037             if (cell.pagesize is not None and
4038                 cell.pagesize in avail_pagesize):
4039                 return True
4040 
4041         return False
4042 
4043     def _get_guest_numa_config(self, instance_numa_topology, flavor,
4044                                allowed_cpus=None, image_meta=None):
4045         """Returns the config objects for the guest NUMA specs.
4046 
4047         Determines the CPUs that the guest can be pinned to if the guest
4048         specifies a cell topology and the host supports it. Constructs the
4049         libvirt XML config object representing the NUMA topology selected
4050         for the guest. Returns a tuple of:
4051 
4052             (cpu_set, guest_cpu_tune, guest_cpu_numa, guest_numa_tune)
4053 
4054         With the following caveats:
4055 
4056             a) If there is no specified guest NUMA topology, then
4057                all tuple elements except cpu_set shall be None. cpu_set
4058                will be populated with the chosen CPUs that the guest
4059                allowed CPUs fit within, which could be the supplied
4060                allowed_cpus value if the host doesn't support NUMA
4061                topologies.
4062 
4063             b) If there is a specified guest NUMA topology, then
4064                cpu_set will be None and guest_cpu_numa will be the
4065                LibvirtConfigGuestCPUNUMA object representing the guest's
4066                NUMA topology. If the host supports NUMA, then guest_cpu_tune
4067                will contain a LibvirtConfigGuestCPUTune object representing
4068                the optimized chosen cells that match the host capabilities
4069                with the instance's requested topology. If the host does
4070                not support NUMA, then guest_cpu_tune and guest_numa_tune
4071                will be None.
4072         """
4073 
4074         if (not self._has_numa_support() and
4075                 instance_numa_topology is not None):
4076             # We should not get here, since we should have avoided
4077             # reporting NUMA topology from _get_host_numa_topology
4078             # in the first place. Just in case of a scheduler
4079             # mess up though, raise an exception
4080             raise exception.NUMATopologyUnsupported()
4081 
4082         topology = self._get_host_numa_topology()
4083 
4084         # We have instance NUMA so translate it to the config class
4085         guest_cpu_numa_config = self._get_cpu_numa_config_from_instance(
4086                 instance_numa_topology,
4087                 self._wants_hugepages(topology, instance_numa_topology))
4088 
4089         if not guest_cpu_numa_config:
4090             # No NUMA topology defined for instance - let the host kernel deal
4091             # with the NUMA effects.
4092             # TODO(ndipanov): Attempt to spread the instance
4093             # across NUMA nodes and expose the topology to the
4094             # instance as an optimisation
4095             return GuestNumaConfig(allowed_cpus, None, None, None)
4096         else:
4097             if topology:
4098                 # Now get the CpuTune configuration from the numa_topology
4099                 guest_cpu_tune = vconfig.LibvirtConfigGuestCPUTune()
4100                 guest_numa_tune = vconfig.LibvirtConfigGuestNUMATune()
4101                 emupcpus = []
4102 
4103                 numa_mem = vconfig.LibvirtConfigGuestNUMATuneMemory()
4104                 numa_memnodes = [vconfig.LibvirtConfigGuestNUMATuneMemNode()
4105                                  for _ in guest_cpu_numa_config.cells]
4106 
4107                 emulator_threads_isolated = (
4108                     instance_numa_topology.emulator_threads_isolated)
4109 
4110                 vcpus_rt = set([])
4111                 wants_realtime = hardware.is_realtime_enabled(flavor)
4112                 if wants_realtime:
4113                     if not self._host.has_min_version(
4114                             MIN_LIBVIRT_REALTIME_VERSION):
4115                         raise exception.RealtimePolicyNotSupported()
4116                     # Prepare realtime config for libvirt
4117                     vcpus_rt = hardware.vcpus_realtime_topology(
4118                         flavor, image_meta)
4119                     vcpusched = vconfig.LibvirtConfigGuestCPUTuneVCPUSched()
4120                     vcpusched.vcpus = vcpus_rt
4121                     vcpusched.scheduler = "fifo"
4122                     vcpusched.priority = (
4123                         CONF.libvirt.realtime_scheduler_priority)
4124                     guest_cpu_tune.vcpusched.append(vcpusched)
4125 
4126                 # TODO(sahid): Defining domain topology should be
4127                 # refactored.
4128                 for host_cell in topology.cells:
4129                     for guest_node_id, guest_config_cell in enumerate(
4130                             guest_cpu_numa_config.cells):
4131                         if guest_config_cell.id == host_cell.id:
4132                             node = numa_memnodes[guest_node_id]
4133                             node.cellid = guest_node_id
4134                             node.nodeset = [host_cell.id]
4135                             node.mode = "strict"
4136 
4137                             numa_mem.nodeset.append(host_cell.id)
4138 
4139                             object_numa_cell = (
4140                                     instance_numa_topology.cells[guest_node_id]
4141                                 )
4142                             for cpu in guest_config_cell.cpus:
4143                                 pin_cpuset = (
4144                                     vconfig.LibvirtConfigGuestCPUTuneVCPUPin())
4145                                 pin_cpuset.id = cpu
4146                                 # If there is pinning information in the cell
4147                                 # we pin to individual CPUs, otherwise we float
4148                                 # over the whole host NUMA node
4149 
4150                                 if (object_numa_cell.cpu_pinning and
4151                                         self._has_cpu_policy_support()):
4152                                     pcpu = object_numa_cell.cpu_pinning[cpu]
4153                                     pin_cpuset.cpuset = set([pcpu])
4154                                 else:
4155                                     pin_cpuset.cpuset = host_cell.cpuset
4156                                 if emulator_threads_isolated:
4157                                     emupcpus.extend(
4158                                         object_numa_cell.cpuset_reserved)
4159                                 elif not wants_realtime or cpu not in vcpus_rt:
4160                                     # - If realtime IS NOT enabled, the
4161                                     #   emulator threads are allowed to float
4162                                     #   across all the pCPUs associated with
4163                                     #   the guest vCPUs ("not wants_realtime"
4164                                     #   is true, so we add all pcpus)
4165                                     # - If realtime IS enabled, then at least
4166                                     #   1 vCPU is required to be set aside for
4167                                     #   non-realtime usage. The emulator
4168                                     #   threads are allowed to float acros the
4169                                     #   pCPUs that are associated with the
4170                                     #   non-realtime VCPUs (the "cpu not in
4171                                     #   vcpu_rt" check deals with this
4172                                     #   filtering)
4173                                     emupcpus.extend(pin_cpuset.cpuset)
4174                                 guest_cpu_tune.vcpupin.append(pin_cpuset)
4175 
4176                 # TODO(berrange) When the guest has >1 NUMA node, it will
4177                 # span multiple host NUMA nodes. By pinning emulator threads
4178                 # to the union of all nodes, we guarantee there will be
4179                 # cross-node memory access by the emulator threads when
4180                 # responding to guest I/O operations. The only way to avoid
4181                 # this would be to pin emulator threads to a single node and
4182                 # tell the guest OS to only do I/O from one of its virtual
4183                 # NUMA nodes. This is not even remotely practical.
4184                 #
4185                 # The long term solution is to make use of a new QEMU feature
4186                 # called "I/O Threads" which will let us configure an explicit
4187                 # I/O thread for each guest vCPU or guest NUMA node. It is
4188                 # still TBD how to make use of this feature though, especially
4189                 # how to associate IO threads with guest devices to eliminate
4190                 # cross NUMA node traffic. This is an area of investigation
4191                 # for QEMU community devs.
4192                 emulatorpin = vconfig.LibvirtConfigGuestCPUTuneEmulatorPin()
4193                 emulatorpin.cpuset = set(emupcpus)
4194                 guest_cpu_tune.emulatorpin = emulatorpin
4195                 # Sort the vcpupin list per vCPU id for human-friendlier XML
4196                 guest_cpu_tune.vcpupin.sort(key=operator.attrgetter("id"))
4197 
4198                 guest_numa_tune.memory = numa_mem
4199                 guest_numa_tune.memnodes = numa_memnodes
4200 
4201                 # normalize cell.id
4202                 for i, (cell, memnode) in enumerate(
4203                                             zip(guest_cpu_numa_config.cells,
4204                                                 guest_numa_tune.memnodes)):
4205                     cell.id = i
4206                     memnode.cellid = i
4207 
4208                 return GuestNumaConfig(None, guest_cpu_tune,
4209                                        guest_cpu_numa_config,
4210                                        guest_numa_tune)
4211             else:
4212                 return GuestNumaConfig(allowed_cpus, None,
4213                                        guest_cpu_numa_config, None)
4214 
4215     def _get_guest_os_type(self, virt_type):
4216         """Returns the guest OS type based on virt type."""
4217         if virt_type == "lxc":
4218             ret = fields.VMMode.EXE
4219         elif virt_type == "uml":
4220             ret = fields.VMMode.UML
4221         elif virt_type == "xen":
4222             ret = fields.VMMode.XEN
4223         else:
4224             ret = fields.VMMode.HVM
4225         return ret
4226 
4227     def _set_guest_for_rescue(self, rescue, guest, inst_path, virt_type,
4228                               root_device_name):
4229         if rescue.get('kernel_id'):
4230             guest.os_kernel = os.path.join(inst_path, "kernel.rescue")
4231             guest.os_cmdline = ("root=%s %s" % (root_device_name, CONSOLE))
4232             if virt_type == "qemu":
4233                 guest.os_cmdline += " no_timer_check"
4234         if rescue.get('ramdisk_id'):
4235             guest.os_initrd = os.path.join(inst_path, "ramdisk.rescue")
4236 
4237     def _set_guest_for_inst_kernel(self, instance, guest, inst_path, virt_type,
4238                                 root_device_name, image_meta):
4239         guest.os_kernel = os.path.join(inst_path, "kernel")
4240         guest.os_cmdline = ("root=%s %s" % (root_device_name, CONSOLE))
4241         if virt_type == "qemu":
4242             guest.os_cmdline += " no_timer_check"
4243         if instance.ramdisk_id:
4244             guest.os_initrd = os.path.join(inst_path, "ramdisk")
4245         # we only support os_command_line with images with an explicit
4246         # kernel set and don't want to break nova if there's an
4247         # os_command_line property without a specified kernel_id param
4248         if image_meta.properties.get("os_command_line"):
4249             guest.os_cmdline = image_meta.properties.os_command_line
4250 
4251     def _set_clock(self, guest, os_type, image_meta, virt_type):
4252         # NOTE(mikal): Microsoft Windows expects the clock to be in
4253         # "localtime". If the clock is set to UTC, then you can use a
4254         # registry key to let windows know, but Microsoft says this is
4255         # buggy in http://support.microsoft.com/kb/2687252
4256         clk = vconfig.LibvirtConfigGuestClock()
4257         if os_type == 'windows':
4258             LOG.info('Configuring timezone for windows instance to localtime')
4259             clk.offset = 'localtime'
4260         else:
4261             clk.offset = 'utc'
4262         guest.set_clock(clk)
4263 
4264         if virt_type == "kvm":
4265             self._set_kvm_timers(clk, os_type, image_meta)
4266 
4267     def _set_kvm_timers(self, clk, os_type, image_meta):
4268         # TODO(berrange) One day this should be per-guest
4269         # OS type configurable
4270         tmpit = vconfig.LibvirtConfigGuestTimer()
4271         tmpit.name = "pit"
4272         tmpit.tickpolicy = "delay"
4273 
4274         tmrtc = vconfig.LibvirtConfigGuestTimer()
4275         tmrtc.name = "rtc"
4276         tmrtc.tickpolicy = "catchup"
4277 
4278         clk.add_timer(tmpit)
4279         clk.add_timer(tmrtc)
4280 
4281         guestarch = libvirt_utils.get_arch(image_meta)
4282         if guestarch in (fields.Architecture.I686,
4283                          fields.Architecture.X86_64):
4284             # NOTE(rfolco): HPET is a hardware timer for x86 arch.
4285             # qemu -no-hpet is not supported on non-x86 targets.
4286             tmhpet = vconfig.LibvirtConfigGuestTimer()
4287             tmhpet.name = "hpet"
4288             tmhpet.present = False
4289             clk.add_timer(tmhpet)
4290 
4291         # Provide Windows guests with the paravirtualized hyperv timer source.
4292         # This is the windows equiv of kvm-clock, allowing Windows
4293         # guests to accurately keep time.
4294         if os_type == 'windows':
4295             tmhyperv = vconfig.LibvirtConfigGuestTimer()
4296             tmhyperv.name = "hypervclock"
4297             tmhyperv.present = True
4298             clk.add_timer(tmhyperv)
4299 
4300     def _set_features(self, guest, os_type, caps, virt_type, image_meta):
4301         if virt_type == "xen":
4302             # PAE only makes sense in X86
4303             if caps.host.cpu.arch in (fields.Architecture.I686,
4304                                       fields.Architecture.X86_64):
4305                 guest.features.append(vconfig.LibvirtConfigGuestFeaturePAE())
4306 
4307         if (virt_type not in ("lxc", "uml", "parallels", "xen") or
4308                 (virt_type == "xen" and guest.os_type == fields.VMMode.HVM)):
4309             guest.features.append(vconfig.LibvirtConfigGuestFeatureACPI())
4310             guest.features.append(vconfig.LibvirtConfigGuestFeatureAPIC())
4311 
4312         if (virt_type in ("qemu", "kvm") and
4313                 os_type == 'windows'):
4314             hv = vconfig.LibvirtConfigGuestFeatureHyperV()
4315             hv.relaxed = True
4316 
4317             hv.spinlocks = True
4318             # Increase spinlock retries - value recommended by
4319             # KVM maintainers who certify Windows guests
4320             # with Microsoft
4321             hv.spinlock_retries = 8191
4322             hv.vapic = True
4323             guest.features.append(hv)
4324 
4325         if (virt_type in ("qemu", "kvm") and
4326                 image_meta.properties.get('img_hide_hypervisor_id')):
4327             guest.features.append(vconfig.LibvirtConfigGuestFeatureKvmHidden())
4328 
4329     def _check_number_of_serial_console(self, num_ports):
4330         virt_type = CONF.libvirt.virt_type
4331         if (virt_type in ("kvm", "qemu") and
4332             num_ports > ALLOWED_QEMU_SERIAL_PORTS):
4333             raise exception.SerialPortNumberLimitExceeded(
4334                 allowed=ALLOWED_QEMU_SERIAL_PORTS, virt_type=virt_type)
4335 
4336     def _add_video_driver(self, guest, image_meta, flavor):
4337         VALID_VIDEO_DEVICES = ("vga", "cirrus", "vmvga",
4338                                "xen", "qxl", "virtio")
4339         video = vconfig.LibvirtConfigGuestVideo()
4340         # NOTE(ldbragst): The following logic sets the video.type
4341         # depending on supported defaults given the architecture,
4342         # virtualization type, and features. The video.type attribute can
4343         # be overridden by the user with image_meta.properties, which
4344         # is carried out in the next if statement below this one.
4345         guestarch = libvirt_utils.get_arch(image_meta)
4346         if guest.os_type == fields.VMMode.XEN:
4347             video.type = 'xen'
4348         elif CONF.libvirt.virt_type == 'parallels':
4349             video.type = 'vga'
4350         elif guestarch in (fields.Architecture.PPC,
4351                            fields.Architecture.PPC64,
4352                            fields.Architecture.PPC64LE):
4353             # NOTE(ldbragst): PowerKVM doesn't support 'cirrus' be default
4354             # so use 'vga' instead when running on Power hardware.
4355             video.type = 'vga'
4356         elif guestarch in (fields.Architecture.AARCH64):
4357             # NOTE(kevinz): Only virtio device type is supported by AARCH64
4358             # so use 'virtio' instead when running on AArch64 hardware.
4359             video.type = 'virtio'
4360         elif CONF.spice.enabled:
4361             video.type = 'qxl'
4362         if image_meta.properties.get('hw_video_model'):
4363             video.type = image_meta.properties.hw_video_model
4364             if (video.type not in VALID_VIDEO_DEVICES):
4365                 raise exception.InvalidVideoMode(model=video.type)
4366 
4367         # Set video memory, only if the flavor's limit is set
4368         video_ram = image_meta.properties.get('hw_video_ram', 0)
4369         max_vram = int(flavor.extra_specs.get('hw_video:ram_max_mb', 0))
4370         if video_ram > max_vram:
4371             raise exception.RequestedVRamTooHigh(req_vram=video_ram,
4372                                                  max_vram=max_vram)
4373         if max_vram and video_ram:
4374             video.vram = video_ram * units.Mi / units.Ki
4375         guest.add_device(video)
4376 
4377     def _add_qga_device(self, guest, instance):
4378         qga = vconfig.LibvirtConfigGuestChannel()
4379         qga.type = "unix"
4380         qga.target_name = "org.qemu.guest_agent.0"
4381         qga.source_path = ("/var/lib/libvirt/qemu/%s.%s.sock" %
4382                           ("org.qemu.guest_agent.0", instance.name))
4383         guest.add_device(qga)
4384 
4385     def _add_rng_device(self, guest, flavor):
4386         rng_device = vconfig.LibvirtConfigGuestRng()
4387         rate_bytes = flavor.extra_specs.get('hw_rng:rate_bytes', 0)
4388         period = flavor.extra_specs.get('hw_rng:rate_period', 0)
4389         if rate_bytes:
4390             rng_device.rate_bytes = int(rate_bytes)
4391             rng_device.rate_period = int(period)
4392         rng_path = CONF.libvirt.rng_dev_path
4393         if (rng_path and not os.path.exists(rng_path)):
4394             raise exception.RngDeviceNotExist(path=rng_path)
4395         rng_device.backend = rng_path
4396         guest.add_device(rng_device)
4397 
4398     def _set_qemu_guest_agent(self, guest, flavor, instance, image_meta):
4399         # Enable qga only if the 'hw_qemu_guest_agent' is equal to yes
4400         if image_meta.properties.get('hw_qemu_guest_agent', False):
4401             LOG.debug("Qemu guest agent is enabled through image "
4402                       "metadata", instance=instance)
4403             self._add_qga_device(guest, instance)
4404         rng_is_virtio = image_meta.properties.get('hw_rng_model') == 'virtio'
4405         rng_allowed_str = flavor.extra_specs.get('hw_rng:allowed', '')
4406         rng_allowed = strutils.bool_from_string(rng_allowed_str)
4407         if rng_is_virtio and rng_allowed:
4408             self._add_rng_device(guest, flavor)
4409 
4410     def _get_guest_memory_backing_config(
4411             self, inst_topology, numatune, flavor):
4412         wantsmempages = False
4413         if inst_topology:
4414             for cell in inst_topology.cells:
4415                 if cell.pagesize:
4416                     wantsmempages = True
4417                     break
4418 
4419         wantsrealtime = hardware.is_realtime_enabled(flavor)
4420 
4421         membacking = None
4422         if wantsmempages:
4423             pages = self._get_memory_backing_hugepages_support(
4424                 inst_topology, numatune)
4425             if pages:
4426                 membacking = vconfig.LibvirtConfigGuestMemoryBacking()
4427                 membacking.hugepages = pages
4428         if wantsrealtime:
4429             if not membacking:
4430                 membacking = vconfig.LibvirtConfigGuestMemoryBacking()
4431             membacking.locked = True
4432             membacking.sharedpages = False
4433 
4434         return membacking
4435 
4436     def _get_memory_backing_hugepages_support(self, inst_topology, numatune):
4437         if not self._has_numa_support():
4438             # We should not get here, since we should have avoided
4439             # reporting NUMA topology from _get_host_numa_topology
4440             # in the first place. Just in case of a scheduler
4441             # mess up though, raise an exception
4442             raise exception.MemoryPagesUnsupported()
4443 
4444         host_topology = self._get_host_numa_topology()
4445 
4446         if host_topology is None:
4447             # As above, we should not get here but just in case...
4448             raise exception.MemoryPagesUnsupported()
4449 
4450         # Currently libvirt does not support the smallest
4451         # pagesize set as a backend memory.
4452         # https://bugzilla.redhat.com/show_bug.cgi?id=1173507
4453         avail_pagesize = [page.size_kb
4454                           for page in host_topology.cells[0].mempages]
4455         avail_pagesize.sort()
4456         smallest = avail_pagesize[0]
4457 
4458         pages = []
4459         for guest_cellid, inst_cell in enumerate(inst_topology.cells):
4460             if inst_cell.pagesize and inst_cell.pagesize > smallest:
4461                 for memnode in numatune.memnodes:
4462                     if guest_cellid == memnode.cellid:
4463                         page = (
4464                             vconfig.LibvirtConfigGuestMemoryBackingPage())
4465                         page.nodeset = [guest_cellid]
4466                         page.size_kb = inst_cell.pagesize
4467                         pages.append(page)
4468                         break  # Quit early...
4469         return pages
4470 
4471     def _get_flavor(self, ctxt, instance, flavor):
4472         if flavor is not None:
4473             return flavor
4474         return instance.flavor
4475 
4476     def _has_uefi_support(self):
4477         # This means that the host can support uefi booting for guests
4478         supported_archs = [fields.Architecture.X86_64,
4479                            fields.Architecture.AARCH64]
4480         caps = self._host.get_capabilities()
4481         return ((caps.host.cpu.arch in supported_archs) and
4482                 os.path.exists(DEFAULT_UEFI_LOADER_PATH[caps.host.cpu.arch]))
4483 
4484     def _get_supported_perf_events(self):
4485 
4486         if (len(CONF.libvirt.enabled_perf_events) == 0 or
4487              not self._host.has_min_version(MIN_LIBVIRT_PERF_VERSION)):
4488             return []
4489 
4490         supported_events = []
4491         host_cpu_info = self._get_cpu_info()
4492         for event in CONF.libvirt.enabled_perf_events:
4493             if self._supported_perf_event(event, host_cpu_info['features']):
4494                 supported_events.append(event)
4495         return supported_events
4496 
4497     def _supported_perf_event(self, event, cpu_features):
4498 
4499         libvirt_perf_event_name = LIBVIRT_PERF_EVENT_PREFIX + event.upper()
4500 
4501         if not hasattr(libvirt, libvirt_perf_event_name):
4502             LOG.warning("Libvirt doesn't support event type %s.", event)
4503             return False
4504 
4505         if (event in PERF_EVENTS_CPU_FLAG_MAPPING
4506             and PERF_EVENTS_CPU_FLAG_MAPPING[event] not in cpu_features):
4507             LOG.warning("Host does not support event type %s.", event)
4508             return False
4509 
4510         return True
4511 
4512     def _configure_guest_by_virt_type(self, guest, virt_type, caps, instance,
4513                                       image_meta, flavor, root_device_name):
4514         if virt_type == "xen":
4515             if guest.os_type == fields.VMMode.HVM:
4516                 guest.os_loader = CONF.libvirt.xen_hvmloader_path
4517             else:
4518                 guest.os_cmdline = CONSOLE
4519         elif virt_type in ("kvm", "qemu"):
4520             if caps.host.cpu.arch in (fields.Architecture.I686,
4521                                       fields.Architecture.X86_64):
4522                 guest.sysinfo = self._get_guest_config_sysinfo(instance)
4523                 guest.os_smbios = vconfig.LibvirtConfigGuestSMBIOS()
4524             hw_firmware_type = image_meta.properties.get('hw_firmware_type')
4525             if caps.host.cpu.arch == fields.Architecture.AARCH64:
4526                 if not hw_firmware_type:
4527                     hw_firmware_type = fields.FirmwareType.UEFI
4528             if hw_firmware_type == fields.FirmwareType.UEFI:
4529                 if self._has_uefi_support():
4530                     global uefi_logged
4531                     if not uefi_logged:
4532                         LOG.warning("uefi support is without some kind of "
4533                                     "functional testing and therefore "
4534                                     "considered experimental.")
4535                         uefi_logged = True
4536                     guest.os_loader = DEFAULT_UEFI_LOADER_PATH[
4537                         caps.host.cpu.arch]
4538                     guest.os_loader_type = "pflash"
4539                 else:
4540                     raise exception.UEFINotSupported()
4541             guest.os_mach_type = self._get_machine_type(image_meta, caps)
4542             if image_meta.properties.get('hw_boot_menu') is None:
4543                 guest.os_bootmenu = strutils.bool_from_string(
4544                     flavor.extra_specs.get('hw:boot_menu', 'no'))
4545             else:
4546                 guest.os_bootmenu = image_meta.properties.hw_boot_menu
4547 
4548         elif virt_type == "lxc":
4549             guest.os_init_path = "/sbin/init"
4550             guest.os_cmdline = CONSOLE
4551         elif virt_type == "uml":
4552             guest.os_kernel = "/usr/bin/linux"
4553             guest.os_root = root_device_name
4554         elif virt_type == "parallels":
4555             if guest.os_type == fields.VMMode.EXE:
4556                 guest.os_init_path = "/sbin/init"
4557 
4558     def _conf_non_lxc_uml(self, virt_type, guest, root_device_name, rescue,
4559                     instance, inst_path, image_meta, disk_info):
4560         if rescue:
4561             self._set_guest_for_rescue(rescue, guest, inst_path, virt_type,
4562                                        root_device_name)
4563         elif instance.kernel_id:
4564             self._set_guest_for_inst_kernel(instance, guest, inst_path,
4565                                             virt_type, root_device_name,
4566                                             image_meta)
4567         else:
4568             guest.os_boot_dev = blockinfo.get_boot_order(disk_info)
4569 
4570     def _create_consoles(self, virt_type, guest_cfg, instance, flavor,
4571                          image_meta):
4572         # NOTE(markus_z): Beware! Below are so many conditionals that it is
4573         # easy to lose track. Use this chart to figure out your case:
4574         #
4575         # case | is serial | has       | is qemu | resulting
4576         #      | enabled?  | virtlogd? | or kvm? | devices
4577         # --------------------------------------------------
4578         #    1 |        no |        no |     no  | pty*
4579         #    2 |        no |        no |     yes | file + pty
4580         #    3 |        no |       yes |      no | see case 1
4581         #    4 |        no |       yes |     yes | pty with logd
4582         #    5 |       yes |        no |      no | see case 1
4583         #    6 |       yes |        no |     yes | tcp + pty
4584         #    7 |       yes |       yes |      no | see case 1
4585         #    8 |       yes |       yes |     yes | tcp with logd
4586         #    * exception: virt_type "parallels" doesn't create a device
4587         if virt_type == 'parallels':
4588             pass
4589         elif virt_type not in ("qemu", "kvm"):
4590             log_path = self._get_console_log_path(instance)
4591             self._create_pty_device(guest_cfg,
4592                                     vconfig.LibvirtConfigGuestConsole,
4593                                     log_path=log_path)
4594         elif (virt_type in ("qemu", "kvm") and
4595                   self._is_s390x_guest(image_meta)):
4596             self._create_consoles_s390x(guest_cfg, instance,
4597                                         flavor, image_meta)
4598         elif virt_type in ("qemu", "kvm"):
4599             self._create_consoles_qemu_kvm(guest_cfg, instance,
4600                                         flavor, image_meta)
4601 
4602     def _is_s390x_guest(self, image_meta):
4603         s390x_archs = (fields.Architecture.S390, fields.Architecture.S390X)
4604         return libvirt_utils.get_arch(image_meta) in s390x_archs
4605 
4606     def _create_consoles_qemu_kvm(self, guest_cfg, instance, flavor,
4607                                   image_meta):
4608         char_dev_cls = vconfig.LibvirtConfigGuestSerial
4609         log_path = self._get_console_log_path(instance)
4610         if CONF.serial_console.enabled:
4611             if not self._serial_ports_already_defined(instance):
4612                 num_ports = hardware.get_number_of_serial_ports(flavor,
4613                                                                 image_meta)
4614                 self._check_number_of_serial_console(num_ports)
4615                 self._create_serial_consoles(guest_cfg, num_ports,
4616                                              char_dev_cls, log_path)
4617         else:
4618             self._create_file_device(guest_cfg, instance, char_dev_cls)
4619         self._create_pty_device(guest_cfg, char_dev_cls, log_path=log_path)
4620 
4621     def _create_consoles_s390x(self, guest_cfg, instance, flavor, image_meta):
4622         char_dev_cls = vconfig.LibvirtConfigGuestConsole
4623         log_path = self._get_console_log_path(instance)
4624         if CONF.serial_console.enabled:
4625             if not self._serial_ports_already_defined(instance):
4626                 num_ports = hardware.get_number_of_serial_ports(flavor,
4627                                                                 image_meta)
4628                 self._create_serial_consoles(guest_cfg, num_ports,
4629                                              char_dev_cls, log_path)
4630         else:
4631             self._create_file_device(guest_cfg, instance, char_dev_cls,
4632                                      "sclplm")
4633         self._create_pty_device(guest_cfg, char_dev_cls, "sclp", log_path)
4634 
4635     def _create_pty_device(self, guest_cfg, char_dev_cls, target_type=None,
4636                            log_path=None):
4637         def _create_base_dev():
4638             consolepty = char_dev_cls()
4639             consolepty.target_type = target_type
4640             consolepty.type = "pty"
4641             return consolepty
4642 
4643         def _create_logd_dev():
4644             consolepty = _create_base_dev()
4645             log = vconfig.LibvirtConfigGuestCharDeviceLog()
4646             log.file = log_path
4647             consolepty.log = log
4648             return consolepty
4649 
4650         if CONF.serial_console.enabled:
4651             if self._is_virtlogd_available():
4652                 return
4653             else:
4654                 # NOTE(markus_z): You may wonder why this is necessary and
4655                 # so do I. I'm certain that this is *not* needed in any
4656                 # real use case. It is, however, useful if you want to
4657                 # pypass the Nova API and use "virsh console <guest>" on
4658                 # an hypervisor, as this CLI command doesn't work with TCP
4659                 # devices (like the serial console is).
4660                 #     https://bugzilla.redhat.com/show_bug.cgi?id=781467
4661                 # Pypassing the Nova API however is a thing we don't want.
4662                 # Future changes should remove this and fix the unit tests
4663                 # which ask for the existence.
4664                 guest_cfg.add_device(_create_base_dev())
4665         else:
4666             if self._is_virtlogd_available():
4667                 guest_cfg.add_device(_create_logd_dev())
4668             else:
4669                 guest_cfg.add_device(_create_base_dev())
4670 
4671     def _create_file_device(self, guest_cfg, instance, char_dev_cls,
4672                             target_type=None):
4673         if self._is_virtlogd_available():
4674             return
4675 
4676         consolelog = char_dev_cls()
4677         consolelog.target_type = target_type
4678         consolelog.type = "file"
4679         consolelog.source_path = self._get_console_log_path(instance)
4680         guest_cfg.add_device(consolelog)
4681 
4682     def _serial_ports_already_defined(self, instance):
4683         try:
4684             guest = self._host.get_guest(instance)
4685             if list(self._get_serial_ports_from_guest(guest)):
4686                 # Serial port are already configured for instance that
4687                 # means we are in a context of migration.
4688                 return True
4689         except exception.InstanceNotFound:
4690             LOG.debug(
4691                 "Instance does not exist yet on libvirt, we can "
4692                 "safely pass on looking for already defined serial "
4693                 "ports in its domain XML", instance=instance)
4694         return False
4695 
4696     def _create_serial_consoles(self, guest_cfg, num_ports, char_dev_cls,
4697                                 log_path):
4698         for port in six.moves.range(num_ports):
4699             console = char_dev_cls()
4700             console.port = port
4701             console.type = "tcp"
4702             console.listen_host = CONF.serial_console.proxyclient_address
4703             listen_port = serial_console.acquire_port(console.listen_host)
4704             console.listen_port = listen_port
4705             # NOTE: only the first serial console gets the boot messages,
4706             # that's why we attach the logd subdevice only to that.
4707             if port == 0 and self._is_virtlogd_available():
4708                 log = vconfig.LibvirtConfigGuestCharDeviceLog()
4709                 log.file = log_path
4710                 console.log = log
4711             guest_cfg.add_device(console)
4712 
4713     def _cpu_config_to_vcpu_model(self, cpu_config, vcpu_model):
4714         """Update VirtCPUModel object according to libvirt CPU config.
4715 
4716         :param:cpu_config: vconfig.LibvirtConfigGuestCPU presenting the
4717                            instance's virtual cpu configuration.
4718         :param:vcpu_model: VirtCPUModel object. A new object will be created
4719                            if None.
4720 
4721         :return: Updated VirtCPUModel object, or None if cpu_config is None
4722 
4723         """
4724 
4725         if not cpu_config:
4726             return
4727         if not vcpu_model:
4728             vcpu_model = objects.VirtCPUModel()
4729 
4730         vcpu_model.arch = cpu_config.arch
4731         vcpu_model.vendor = cpu_config.vendor
4732         vcpu_model.model = cpu_config.model
4733         vcpu_model.mode = cpu_config.mode
4734         vcpu_model.match = cpu_config.match
4735 
4736         if cpu_config.sockets:
4737             vcpu_model.topology = objects.VirtCPUTopology(
4738                 sockets=cpu_config.sockets,
4739                 cores=cpu_config.cores,
4740                 threads=cpu_config.threads)
4741         else:
4742             vcpu_model.topology = None
4743 
4744         features = [objects.VirtCPUFeature(
4745             name=f.name,
4746             policy=f.policy) for f in cpu_config.features]
4747         vcpu_model.features = features
4748 
4749         return vcpu_model
4750 
4751     def _vcpu_model_to_cpu_config(self, vcpu_model):
4752         """Create libvirt CPU config according to VirtCPUModel object.
4753 
4754         :param:vcpu_model: VirtCPUModel object.
4755 
4756         :return: vconfig.LibvirtConfigGuestCPU.
4757 
4758         """
4759 
4760         cpu_config = vconfig.LibvirtConfigGuestCPU()
4761         cpu_config.arch = vcpu_model.arch
4762         cpu_config.model = vcpu_model.model
4763         cpu_config.mode = vcpu_model.mode
4764         cpu_config.match = vcpu_model.match
4765         cpu_config.vendor = vcpu_model.vendor
4766         if vcpu_model.topology:
4767             cpu_config.sockets = vcpu_model.topology.sockets
4768             cpu_config.cores = vcpu_model.topology.cores
4769             cpu_config.threads = vcpu_model.topology.threads
4770         if vcpu_model.features:
4771             for f in vcpu_model.features:
4772                 xf = vconfig.LibvirtConfigGuestCPUFeature()
4773                 xf.name = f.name
4774                 xf.policy = f.policy
4775                 cpu_config.features.add(xf)
4776         return cpu_config
4777 
4778     def _get_guest_config(self, instance, network_info, image_meta,
4779                           disk_info, rescue=None, block_device_info=None,
4780                           context=None, allocations=None):
4781         """Get config data for parameters.
4782 
4783         :param rescue: optional dictionary that should contain the key
4784             'ramdisk_id' if a ramdisk is needed for the rescue image and
4785             'kernel_id' if a kernel is needed for the rescue image.
4786         """
4787         flavor = instance.flavor
4788         inst_path = libvirt_utils.get_instance_path(instance)
4789         disk_mapping = disk_info['mapping']
4790 
4791         virt_type = CONF.libvirt.virt_type
4792         guest = vconfig.LibvirtConfigGuest()
4793         guest.virt_type = virt_type
4794         guest.name = instance.name
4795         guest.uuid = instance.uuid
4796         # We are using default unit for memory: KiB
4797         guest.memory = flavor.memory_mb * units.Ki
4798         guest.vcpus = flavor.vcpus
4799         allowed_cpus = hardware.get_vcpu_pin_set()
4800 
4801         guest_numa_config = self._get_guest_numa_config(
4802             instance.numa_topology, flavor, allowed_cpus, image_meta)
4803 
4804         guest.cpuset = guest_numa_config.cpuset
4805         guest.cputune = guest_numa_config.cputune
4806         guest.numatune = guest_numa_config.numatune
4807 
4808         guest.membacking = self._get_guest_memory_backing_config(
4809             instance.numa_topology,
4810             guest_numa_config.numatune,
4811             flavor)
4812 
4813         guest.metadata.append(self._get_guest_config_meta(instance))
4814         guest.idmaps = self._get_guest_idmaps()
4815 
4816         for event in self._supported_perf_events:
4817             guest.add_perf_event(event)
4818 
4819         self._update_guest_cputune(guest, flavor, virt_type)
4820 
4821         guest.cpu = self._get_guest_cpu_config(
4822             flavor, image_meta, guest_numa_config.numaconfig,
4823             instance.numa_topology)
4824 
4825         # Notes(yjiang5): we always sync the instance's vcpu model with
4826         # the corresponding config file.
4827         instance.vcpu_model = self._cpu_config_to_vcpu_model(
4828             guest.cpu, instance.vcpu_model)
4829 
4830         if 'root' in disk_mapping:
4831             root_device_name = block_device.prepend_dev(
4832                 disk_mapping['root']['dev'])
4833         else:
4834             root_device_name = None
4835 
4836         if root_device_name:
4837             # NOTE(yamahata):
4838             # for nova.api.ec2.cloud.CloudController.get_metadata()
4839             instance.root_device_name = root_device_name
4840 
4841         guest.os_type = (fields.VMMode.get_from_instance(instance) or
4842                 self._get_guest_os_type(virt_type))
4843         caps = self._host.get_capabilities()
4844 
4845         self._configure_guest_by_virt_type(guest, virt_type, caps, instance,
4846                                            image_meta, flavor,
4847                                            root_device_name)
4848         if virt_type not in ('lxc', 'uml'):
4849             self._conf_non_lxc_uml(virt_type, guest, root_device_name, rescue,
4850                     instance, inst_path, image_meta, disk_info)
4851 
4852         self._set_features(guest, instance.os_type, caps, virt_type,
4853                            image_meta)
4854         self._set_clock(guest, instance.os_type, image_meta, virt_type)
4855 
4856         storage_configs = self._get_guest_storage_config(
4857                 instance, image_meta, disk_info, rescue, block_device_info,
4858                 flavor, guest.os_type)
4859         for config in storage_configs:
4860             guest.add_device(config)
4861 
4862         for vif in network_info:
4863             config = self.vif_driver.get_config(
4864                 instance, vif, image_meta,
4865                 flavor, virt_type, self._host)
4866             guest.add_device(config)
4867 
4868         self._create_consoles(virt_type, guest, instance, flavor, image_meta)
4869 
4870         pointer = self._get_guest_pointer_model(guest.os_type, image_meta)
4871         if pointer:
4872             guest.add_device(pointer)
4873 
4874         self._guest_add_spice_channel(guest)
4875 
4876         if self._guest_add_video_device(guest):
4877             self._add_video_driver(guest, image_meta, flavor)
4878 
4879         # Qemu guest agent only support 'qemu' and 'kvm' hypervisor
4880         if virt_type in ('qemu', 'kvm'):
4881             self._set_qemu_guest_agent(guest, flavor, instance, image_meta)
4882 
4883         self._guest_add_pci_devices(guest, instance)
4884 
4885         self._guest_add_watchdog_action(guest, flavor, image_meta)
4886 
4887         self._guest_add_memory_balloon(guest)
4888 
4889         self._guest_add_vgpus(guest, allocations)
4890 
4891         return guest
4892 
4893     @staticmethod
4894     def _vgpu_allocations(allocations):
4895         """Filtering only the VGPU allocations from a list of allocations.
4896 
4897         :param allocations: Information about resources allocated to the
4898                             instance via placement, of the form returned by
4899                             SchedulerReportClient.get_allocations_for_consumer.
4900         """
4901         if not allocations:
4902             # If no allocations, there is no vGPU request.
4903             return {}
4904         RC_VGPU = fields.ResourceClass.VGPU
4905         vgpu_allocations = {}
4906         for rp in allocations:
4907             res = allocations[rp]['resources']
4908             if RC_VGPU in res and res[RC_VGPU] > 0:
4909                 vgpu_allocations[rp] = {'resources': {RC_VGPU: res[RC_VGPU]}}
4910         return vgpu_allocations
4911 
4912     def _get_existing_mdevs_not_assigned(self, requested_types=None):
4913         """Returns the already created mediated devices that are not assigned
4914         to a guest yet.
4915 
4916         :param requested_types: Filter out the result for only mediated devices
4917                                 having those types.
4918         """
4919         allocated_mdevs = self._get_all_assigned_mediated_devices()
4920         mdevs = self._get_mediated_devices(requested_types)
4921         available_mdevs = set([mdev["uuid"]
4922                                for mdev in mdevs]) - set(allocated_mdevs)
4923         return available_mdevs
4924 
4925     def _guest_add_vgpus(self, guest, allocations):
4926         """Add virtual GPU(s) to a guest if the list of allocations contains
4927         at least one specific resource class allocation for VGPU.
4928 
4929         Note that if allocations is None (like when rebooting or rescuing), we
4930         don't assign a new vGPU obviously.
4931 
4932         :param guest: the libvirt Guest information related to the instance.
4933         :param allocations: Information about resources allocated to the
4934                             instance via placement, of the form returned by
4935                             SchedulerReportClient.get_allocations_for_consumer.
4936         """
4937         vgpu_allocations = self._vgpu_allocations(allocations)
4938         if not vgpu_allocations:
4939             return
4940         # TODO(sbauza): Once we have nested resource providers, find which one
4941         # is having the related allocation for the specific VGPU type.
4942         # For the moment, we should only have one allocation for only one
4943         # ResourceProvider.
4944         for (rp, alloc) in six.iteritems(vgpu_allocations):
4945             vgpus_asked = alloc['resources'][fields.ResourceClass.VGPU]
4946             # TODO(sbauza): Iterate over all the allocations once we have
4947             # nested Resource Providers. For the moment, just stop here.
4948             break
4949 
4950         requested_types = self._get_supported_vgpu_types()
4951         # Which mediated devices are created but not assigned to a guest ?
4952         mdevs_available = self._get_existing_mdevs_not_assigned(
4953             requested_types)
4954 
4955         for c in six.moves.range(vgpus_asked):
4956             chosen_mdev = None
4957             if mdevs_available:
4958                 # Take the first available mdev
4959                 chosen_mdev = mdevs_available.pop()
4960             else:
4961                 # Try to see if we can still create a new mediated device
4962                 # TODO(sbauza): Not use that conditional branch if a
4963                 # workaround config opt asks to not create mdevs by Nova
4964                 # because of a kernel race.
4965                 devices = self._get_mdev_capable_devices(requested_types)
4966                 for device in devices:
4967                     # For the moment, the libvirt driver only supports one
4968                     # type per host
4969                     # TODO(sbauza): Once we support more than one type, make
4970                     # sure we look at the flavor/trait for the asked type.
4971                     asked_type = requested_types[0]
4972                     if device['types'][asked_type]['availableInstances'] > 0:
4973                         # That physical GPU has enough room for a new mdev
4974                         chosen_mdev = nova.privsep.libvirt.create_mdev(
4975                             device['dev_id'], asked_type)
4976                         break
4977             if not chosen_mdev:
4978                 # If we can't find devices having available VGPUs, just raise
4979                 raise exception.ComputeResourcesUnavailable(
4980                     reason='vGPU resource is not available')
4981             mdev = vconfig.LibvirtConfigGuestHostdevMDEV()
4982             mdev.uuid = chosen_mdev
4983             guest.add_device(mdev)
4984 
4985     @staticmethod
4986     def _guest_add_spice_channel(guest):
4987         if (CONF.spice.enabled and CONF.spice.agent_enabled
4988                 and guest.virt_type not in ('lxc', 'uml', 'xen')):
4989             channel = vconfig.LibvirtConfigGuestChannel()
4990             channel.type = 'spicevmc'
4991             channel.target_name = "com.redhat.spice.0"
4992             guest.add_device(channel)
4993 
4994     @staticmethod
4995     def _guest_add_memory_balloon(guest):
4996         virt_type = guest.virt_type
4997         # Memory balloon device only support 'qemu/kvm' and 'xen' hypervisor
4998         if (virt_type in ('xen', 'qemu', 'kvm') and
4999                     CONF.libvirt.mem_stats_period_seconds > 0):
5000             balloon = vconfig.LibvirtConfigMemoryBalloon()
5001             if virt_type in ('qemu', 'kvm'):
5002                 balloon.model = 'virtio'
5003             else:
5004                 balloon.model = 'xen'
5005             balloon.period = CONF.libvirt.mem_stats_period_seconds
5006             guest.add_device(balloon)
5007 
5008     @staticmethod
5009     def _guest_add_watchdog_action(guest, flavor, image_meta):
5010         # image meta takes precedence over flavor extra specs; disable the
5011         # watchdog action by default
5012         watchdog_action = (flavor.extra_specs.get('hw:watchdog_action')
5013                            or 'disabled')
5014         watchdog_action = image_meta.properties.get('hw_watchdog_action',
5015                                                     watchdog_action)
5016         # NB(sross): currently only actually supported by KVM/QEmu
5017         if watchdog_action != 'disabled':
5018             if watchdog_action in fields.WatchdogAction.ALL:
5019                 bark = vconfig.LibvirtConfigGuestWatchdog()
5020                 bark.action = watchdog_action
5021                 guest.add_device(bark)
5022             else:
5023                 raise exception.InvalidWatchdogAction(action=watchdog_action)
5024 
5025     def _guest_add_pci_devices(self, guest, instance):
5026         virt_type = guest.virt_type
5027         if virt_type in ('xen', 'qemu', 'kvm'):
5028             # Get all generic PCI devices (non-SR-IOV).
5029             for pci_dev in pci_manager.get_instance_pci_devs(instance):
5030                 guest.add_device(self._get_guest_pci_device(pci_dev))
5031         else:
5032             # PCI devices is only supported for hypervisors
5033             #  'xen', 'qemu' and 'kvm'.
5034             if pci_manager.get_instance_pci_devs(instance, 'all'):
5035                 raise exception.PciDeviceUnsupportedHypervisor(type=virt_type)
5036 
5037     @staticmethod
5038     def _guest_add_video_device(guest):
5039         # NB some versions of libvirt support both SPICE and VNC
5040         # at the same time. We're not trying to second guess which
5041         # those versions are. We'll just let libvirt report the
5042         # errors appropriately if the user enables both.
5043         add_video_driver = False
5044         if CONF.vnc.enabled and guest.virt_type not in ('lxc', 'uml'):
5045             graphics = vconfig.LibvirtConfigGuestGraphics()
5046             graphics.type = "vnc"
5047             if CONF.vnc.keymap:
5048                 # TODO(stephenfin): There are some issues here that may
5049                 # necessitate deprecating this option entirely in the future.
5050                 # Refer to bug #1682020 for more information.
5051                 graphics.keymap = CONF.vnc.keymap
5052             graphics.listen = CONF.vnc.server_listen
5053             guest.add_device(graphics)
5054             add_video_driver = True
5055         if CONF.spice.enabled and guest.virt_type not in ('lxc', 'uml', 'xen'):
5056             graphics = vconfig.LibvirtConfigGuestGraphics()
5057             graphics.type = "spice"
5058             if CONF.spice.keymap:
5059                 # TODO(stephenfin): There are some issues here that may
5060                 # necessitate deprecating this option entirely in the future.
5061                 # Refer to bug #1682020 for more information.
5062                 graphics.keymap = CONF.spice.keymap
5063             graphics.listen = CONF.spice.server_listen
5064             guest.add_device(graphics)
5065             add_video_driver = True
5066         return add_video_driver
5067 
5068     def _get_guest_pointer_model(self, os_type, image_meta):
5069         pointer_model = image_meta.properties.get(
5070             'hw_pointer_model', CONF.pointer_model)
5071         if pointer_model is None and CONF.libvirt.use_usb_tablet:
5072             # TODO(sahid): We set pointer_model to keep compatibility
5073             # until the next release O*. It means operators can continue
5074             # to use the deprecated option "use_usb_tablet" or set a
5075             # specific device to use
5076             pointer_model = "usbtablet"
5077             LOG.warning('The option "use_usb_tablet" has been '
5078                         'deprecated for Newton in favor of the more '
5079                         'generic "pointer_model". Please update '
5080                         'nova.conf to address this change.')
5081 
5082         if pointer_model == "usbtablet":
5083             # We want a tablet if VNC is enabled, or SPICE is enabled and
5084             # the SPICE agent is disabled. If the SPICE agent is enabled
5085             # it provides a paravirt mouse which drastically reduces
5086             # overhead (by eliminating USB polling).
5087             if CONF.vnc.enabled or (
5088                     CONF.spice.enabled and not CONF.spice.agent_enabled):
5089                 return self._get_guest_usb_tablet(os_type)
5090             else:
5091                 if CONF.pointer_model or CONF.libvirt.use_usb_tablet:
5092                     # For backward compatibility We don't want to break
5093                     # process of booting an instance if host is configured
5094                     # to use USB tablet without VNC or SPICE and SPICE
5095                     # agent disable.
5096                     LOG.warning('USB tablet requested for guests by host '
5097                                 'configuration. In order to accept this '
5098                                 'request VNC should be enabled or SPICE '
5099                                 'and SPICE agent disabled on host.')
5100                 else:
5101                     raise exception.UnsupportedPointerModelRequested(
5102                         model="usbtablet")
5103 
5104     def _get_guest_usb_tablet(self, os_type):
5105         tablet = None
5106         if os_type == fields.VMMode.HVM:
5107             tablet = vconfig.LibvirtConfigGuestInput()
5108             tablet.type = "tablet"
5109             tablet.bus = "usb"
5110         else:
5111             if CONF.pointer_model or CONF.libvirt.use_usb_tablet:
5112                 # For backward compatibility We don't want to break
5113                 # process of booting an instance if virtual machine mode
5114                 # is not configured as HVM.
5115                 LOG.warning('USB tablet requested for guests by host '
5116                             'configuration. In order to accept this '
5117                             'request the machine mode should be '
5118                             'configured as HVM.')
5119             else:
5120                 raise exception.UnsupportedPointerModelRequested(
5121                     model="usbtablet")
5122         return tablet
5123 
5124     def _get_guest_xml(self, context, instance, network_info, disk_info,
5125                        image_meta, rescue=None,
5126                        block_device_info=None,
5127                        allocations=None):
5128         # NOTE(danms): Stringifying a NetworkInfo will take a lock. Do
5129         # this ahead of time so that we don't acquire it while also
5130         # holding the logging lock.
5131         network_info_str = str(network_info)
5132         msg = ('Start _get_guest_xml '
5133                'network_info=%(network_info)s '
5134                'disk_info=%(disk_info)s '
5135                'image_meta=%(image_meta)s rescue=%(rescue)s '
5136                'block_device_info=%(block_device_info)s' %
5137                {'network_info': network_info_str, 'disk_info': disk_info,
5138                 'image_meta': image_meta, 'rescue': rescue,
5139                 'block_device_info': block_device_info})
5140         # NOTE(mriedem): block_device_info can contain auth_password so we
5141         # need to sanitize the password in the message.
5142         LOG.debug(strutils.mask_password(msg), instance=instance)
5143         conf = self._get_guest_config(instance, network_info, image_meta,
5144                                       disk_info, rescue, block_device_info,
5145                                       context, allocations)
5146         xml = conf.to_xml()
5147 
5148         LOG.debug('End _get_guest_xml xml=%(xml)s',
5149                   {'xml': xml}, instance=instance)
5150         return xml
5151 
5152     def get_info(self, instance):
5153         """Retrieve information from libvirt for a specific instance.
5154 
5155         If a libvirt error is encountered during lookup, we might raise a
5156         NotFound exception or Error exception depending on how severe the
5157         libvirt error is.
5158 
5159         :param instance: nova.objects.instance.Instance object
5160         :returns: An InstanceInfo object
5161         """
5162         guest = self._host.get_guest(instance)
5163         # Kind of ugly but we need to pass host to get_info as for a
5164         # workaround, see libvirt/compat.py
5165         return guest.get_info(self._host)
5166 
5167     def _create_domain_setup_lxc(self, instance, image_meta,
5168                                  block_device_info):
5169         inst_path = libvirt_utils.get_instance_path(instance)
5170         block_device_mapping = driver.block_device_info_get_mapping(
5171             block_device_info)
5172         root_disk = block_device.get_root_bdm(block_device_mapping)
5173         if root_disk:
5174             self._connect_volume(root_disk['connection_info'], instance)
5175             disk_path = root_disk['connection_info']['data']['device_path']
5176 
5177             # NOTE(apmelton) - Even though the instance is being booted from a
5178             # cinder volume, it is still presented as a local block device.
5179             # LocalBlockImage is used here to indicate that the instance's
5180             # disk is backed by a local block device.
5181             image_model = imgmodel.LocalBlockImage(disk_path)
5182         else:
5183             root_disk = self.image_backend.by_name(instance, 'disk')
5184             image_model = root_disk.get_model(self._conn)
5185 
5186         container_dir = os.path.join(inst_path, 'rootfs')
5187         fileutils.ensure_tree(container_dir)
5188         rootfs_dev = disk_api.setup_container(image_model,
5189                                               container_dir=container_dir)
5190 
5191         try:
5192             # Save rootfs device to disconnect it when deleting the instance
5193             if rootfs_dev:
5194                 instance.system_metadata['rootfs_device_name'] = rootfs_dev
5195             if CONF.libvirt.uid_maps or CONF.libvirt.gid_maps:
5196                 id_maps = self._get_guest_idmaps()
5197                 libvirt_utils.chown_for_id_maps(container_dir, id_maps)
5198         except Exception:
5199             with excutils.save_and_reraise_exception():
5200                 self._create_domain_cleanup_lxc(instance)
5201 
5202     def _create_domain_cleanup_lxc(self, instance):
5203         inst_path = libvirt_utils.get_instance_path(instance)
5204         container_dir = os.path.join(inst_path, 'rootfs')
5205 
5206         try:
5207             state = self.get_info(instance).state
5208         except exception.InstanceNotFound:
5209             # The domain may not be present if the instance failed to start
5210             state = None
5211 
5212         if state == power_state.RUNNING:
5213             # NOTE(uni): Now the container is running with its own private
5214             # mount namespace and so there is no need to keep the container
5215             # rootfs mounted in the host namespace
5216             LOG.debug('Attempting to unmount container filesystem: %s',
5217                       container_dir, instance=instance)
5218             disk_api.clean_lxc_namespace(container_dir=container_dir)
5219         else:
5220             disk_api.teardown_container(container_dir=container_dir)
5221 
5222     @contextlib.contextmanager
5223     def _lxc_disk_handler(self, instance, image_meta, block_device_info):
5224         """Context manager to handle the pre and post instance boot,
5225            LXC specific disk operations.
5226 
5227            An image or a volume path will be prepared and setup to be
5228            used by the container, prior to starting it.
5229            The disk will be disconnected and unmounted if a container has
5230            failed to start.
5231         """
5232 
5233         if CONF.libvirt.virt_type != 'lxc':
5234             yield
5235             return
5236 
5237         self._create_domain_setup_lxc(instance, image_meta, block_device_info)
5238 
5239         try:
5240             yield
5241         finally:
5242             self._create_domain_cleanup_lxc(instance)
5243 
5244     # TODO(sahid): Consider renaming this to _create_guest.
5245     def _create_domain(self, xml=None, domain=None,
5246                        power_on=True, pause=False, post_xml_callback=None):
5247         """Create a domain.
5248 
5249         Either domain or xml must be passed in. If both are passed, then
5250         the domain definition is overwritten from the xml.
5251 
5252         :returns guest.Guest: Guest just created
5253         """
5254         if xml:
5255             guest = libvirt_guest.Guest.create(xml, self._host)
5256             if post_xml_callback is not None:
5257                 post_xml_callback()
5258         else:
5259             guest = libvirt_guest.Guest(domain)
5260 
5261         if power_on or pause:
5262             guest.launch(pause=pause)
5263 
5264         if not utils.is_neutron():
5265             guest.enable_hairpin()
5266 
5267         return guest
5268 
5269     def _neutron_failed_callback(self, event_name, instance):
5270         LOG.error('Neutron Reported failure on event '
5271                   '%(event)s for instance %(uuid)s',
5272                   {'event': event_name, 'uuid': instance.uuid},
5273                   instance=instance)
5274         if CONF.vif_plugging_is_fatal:
5275             raise exception.VirtualInterfaceCreateException()
5276 
5277     def _get_neutron_events(self, network_info):
5278         # NOTE(danms): We need to collect any VIFs that are currently
5279         # down that we expect a down->up event for. Anything that is
5280         # already up will not undergo that transition, and for
5281         # anything that might be stale (cache-wise) assume it's
5282         # already up so we don't block on it.
5283         return [('network-vif-plugged', vif['id'])
5284                 for vif in network_info if vif.get('active', True) is False]
5285 
5286     def _cleanup_failed_start(self, context, instance, network_info,
5287                               block_device_info, guest, destroy_disks):
5288         try:
5289             if guest and guest.is_active():
5290                 guest.poweroff()
5291         finally:
5292             self.cleanup(context, instance, network_info=network_info,
5293                          block_device_info=block_device_info,
5294                          destroy_disks=destroy_disks)
5295 
5296     def _create_domain_and_network(self, context, xml, instance, network_info,
5297                                    block_device_info=None, power_on=True,
5298                                    vifs_already_plugged=False,
5299                                    post_xml_callback=None,
5300                                    destroy_disks_on_failure=False):
5301 
5302         """Do required network setup and create domain."""
5303         block_device_mapping = driver.block_device_info_get_mapping(
5304             block_device_info)
5305 
5306         for vol in block_device_mapping:
5307             connection_info = vol['connection_info']
5308 
5309             if ('data' in connection_info and
5310                     'volume_id' in connection_info['data']):
5311                 volume_id = connection_info['data']['volume_id']
5312                 encryption = encryptors.get_encryption_metadata(
5313                     context, self._volume_api, volume_id, connection_info)
5314 
5315                 if encryption:
5316                     encryptor = self._get_volume_encryptor(connection_info,
5317                                                            encryption)
5318                     encryptor.attach_volume(context, **encryption)
5319 
5320         timeout = CONF.vif_plugging_timeout
5321         if (self._conn_supports_start_paused and
5322             utils.is_neutron() and not
5323             vifs_already_plugged and power_on and timeout):
5324             events = self._get_neutron_events(network_info)
5325         else:
5326             events = []
5327 
5328         pause = bool(events)
5329         guest = None
5330         try:
5331             with self.virtapi.wait_for_instance_event(
5332                     instance, events, deadline=timeout,
5333                     error_callback=self._neutron_failed_callback):
5334                 self.plug_vifs(instance, network_info)
5335                 self.firewall_driver.setup_basic_filtering(instance,
5336                                                            network_info)
5337                 self.firewall_driver.prepare_instance_filter(instance,
5338                                                              network_info)
5339                 with self._lxc_disk_handler(instance, instance.image_meta,
5340                                             block_device_info):
5341                     guest = self._create_domain(
5342                         xml, pause=pause, power_on=power_on,
5343                         post_xml_callback=post_xml_callback)
5344 
5345                 self.firewall_driver.apply_instance_filter(instance,
5346                                                            network_info)
5347         except exception.VirtualInterfaceCreateException:
5348             # Neutron reported failure and we didn't swallow it, so
5349             # bail here
5350             with excutils.save_and_reraise_exception():
5351                 self._cleanup_failed_start(context, instance, network_info,
5352                                            block_device_info, guest,
5353                                            destroy_disks_on_failure)
5354         except eventlet.timeout.Timeout:
5355             # We never heard from Neutron
5356             LOG.warning('Timeout waiting for vif plugging callback for '
5357                         'instance with vm_state %(vm_state)s and '
5358                         'task_state %(task_state)s.',
5359                         {'vm_state': instance.vm_state,
5360                          'task_state': instance.task_state},
5361                         instance=instance)
5362             if CONF.vif_plugging_is_fatal:
5363                 self._cleanup_failed_start(context, instance, network_info,
5364                                            block_device_info, guest,
5365                                            destroy_disks_on_failure)
5366                 raise exception.VirtualInterfaceCreateException()
5367         except Exception:
5368             # Any other error, be sure to clean up
5369             LOG.error('Failed to start libvirt guest', instance=instance)
5370             with excutils.save_and_reraise_exception():
5371                 self._cleanup_failed_start(context, instance, network_info,
5372                                            block_device_info, guest,
5373                                            destroy_disks_on_failure)
5374 
5375         # Resume only if domain has been paused
5376         if pause:
5377             guest.resume()
5378         return guest
5379 
5380     def _get_vcpu_total(self):
5381         """Get available vcpu number of physical computer.
5382 
5383         :returns: the number of cpu core instances can be used.
5384 
5385         """
5386         try:
5387             total_pcpus = self._host.get_cpu_count()
5388         except libvirt.libvirtError:
5389             LOG.warning("Cannot get the number of cpu, because this "
5390                         "function is not implemented for this platform. ")
5391             return 0
5392 
5393         if not CONF.vcpu_pin_set:
5394             return total_pcpus
5395 
5396         available_ids = hardware.get_vcpu_pin_set()
5397         # We get the list of online CPUs on the host and see if the requested
5398         # set falls under these. If not, we retain the old behavior.
5399         online_pcpus = None
5400         try:
5401             online_pcpus = self._host.get_online_cpus()
5402         except libvirt.libvirtError as ex:
5403             error_code = ex.get_error_code()
5404             err_msg = encodeutils.exception_to_unicode(ex)
5405             LOG.warning(
5406                 "Couldn't retrieve the online CPUs due to a Libvirt "
5407                 "error: %(error)s with error code: %(error_code)s",
5408                 {'error': err_msg, 'error_code': error_code})
5409         if online_pcpus:
5410             if not (available_ids <= online_pcpus):
5411                 msg = (_("Invalid vcpu_pin_set config, one or more of the "
5412                          "specified cpuset is not online. Online cpuset(s): "
5413                          "%(online)s, requested cpuset(s): %(req)s"),
5414                        {'online': sorted(online_pcpus),
5415                         'req': sorted(available_ids)})
5416                 raise exception.Invalid(msg)
5417         elif sorted(available_ids)[-1] >= total_pcpus:
5418             raise exception.Invalid(_("Invalid vcpu_pin_set config, "
5419                                       "out of hypervisor cpu range."))
5420         return len(available_ids)
5421 
5422     @staticmethod
5423     def _get_local_gb_info():
5424         """Get local storage info of the compute node in GB.
5425 
5426         :returns: A dict containing:
5427              :total: How big the overall usable filesystem is (in gigabytes)
5428              :free: How much space is free (in gigabytes)
5429              :used: How much space is used (in gigabytes)
5430         """
5431 
5432         if CONF.libvirt.images_type == 'lvm':
5433             info = lvm.get_volume_group_info(
5434                                CONF.libvirt.images_volume_group)
5435         elif CONF.libvirt.images_type == 'rbd':
5436             info = LibvirtDriver._get_rbd_driver().get_pool_info()
5437         else:
5438             info = libvirt_utils.get_fs_info(CONF.instances_path)
5439 
5440         for (k, v) in info.items():
5441             info[k] = v / units.Gi
5442 
5443         return info
5444 
5445     def _get_vcpu_used(self):
5446         """Get vcpu usage number of physical computer.
5447 
5448         :returns: The total number of vcpu(s) that are currently being used.
5449 
5450         """
5451 
5452         total = 0
5453 
5454         # Not all libvirt drivers will support the get_vcpus_info()
5455         #
5456         # For example, LXC does not have a concept of vCPUs, while
5457         # QEMU (TCG) traditionally handles all vCPUs in a single
5458         # thread. So both will report an exception when the vcpus()
5459         # API call is made. In such a case we should report the
5460         # guest as having 1 vCPU, since that lets us still do
5461         # CPU over commit calculations that apply as the total
5462         # guest count scales.
5463         #
5464         # It is also possible that we might see an exception if
5465         # the guest is just in middle of shutting down. Technically
5466         # we should report 0 for vCPU usage in this case, but we
5467         # we can't reliably distinguish the vcpu not supported
5468         # case from the just shutting down case. Thus we don't know
5469         # whether to report 1 or 0 for vCPU count.
5470         #
5471         # Under-reporting vCPUs is bad because it could conceivably
5472         # let the scheduler place too many guests on the host. Over-
5473         # reporting vCPUs is not a problem as it'll auto-correct on
5474         # the next refresh of usage data.
5475         #
5476         # Thus when getting an exception we always report 1 as the
5477         # vCPU count, as the least worst value.
5478         for guest in self._host.list_guests():
5479             try:
5480                 vcpus = guest.get_vcpus_info()
5481                 total += len(list(vcpus))
5482             except libvirt.libvirtError:
5483                 total += 1
5484             # NOTE(gtt116): give other tasks a chance.
5485             greenthread.sleep(0)
5486         return total
5487 
5488     def _get_supported_vgpu_types(self):
5489         if not CONF.devices.enabled_vgpu_types:
5490             return []
5491         # TODO(sbauza): Move this check up to compute_manager.init_host
5492         if len(CONF.devices.enabled_vgpu_types) > 1:
5493             LOG.warning('libvirt only supports one GPU type per compute node,'
5494                         ' only first type will be used.')
5495         requested_types = CONF.devices.enabled_vgpu_types[:1]
5496         return requested_types
5497 
5498     def _get_vgpu_total(self):
5499         """Returns the number of total available vGPUs for any GPU type that is
5500         enabled with the enabled_vgpu_types CONF option.
5501         """
5502         requested_types = self._get_supported_vgpu_types()
5503         # Bail out early if operator doesn't care about providing vGPUs
5504         if not requested_types:
5505             return 0
5506         # Filter how many available mdevs we can create for all the supported
5507         # types.
5508         mdev_capable_devices = self._get_mdev_capable_devices(requested_types)
5509         vgpus = 0
5510         for dev in mdev_capable_devices:
5511             for _type in dev['types']:
5512                 vgpus += dev['types'][_type]['availableInstances']
5513         # Count the already created (but possibly not assigned to a guest)
5514         # mdevs for all the supported types
5515         mediated_devices = self._get_mediated_devices(requested_types)
5516         vgpus += len(mediated_devices)
5517         return vgpus
5518 
5519     def _get_instance_capabilities(self):
5520         """Get hypervisor instance capabilities
5521 
5522         Returns a list of tuples that describe instances the
5523         hypervisor is capable of hosting.  Each tuple consists
5524         of the triplet (arch, hypervisor_type, vm_mode).
5525 
5526         :returns: List of tuples describing instance capabilities
5527         """
5528         caps = self._host.get_capabilities()
5529         instance_caps = list()
5530         for g in caps.guests:
5531             for dt in g.domtype:
5532                 instance_cap = (
5533                     fields.Architecture.canonicalize(g.arch),
5534                     fields.HVType.canonicalize(dt),
5535                     fields.VMMode.canonicalize(g.ostype))
5536                 instance_caps.append(instance_cap)
5537 
5538         return instance_caps
5539 
5540     def _get_cpu_info(self):
5541         """Get cpuinfo information.
5542 
5543         Obtains cpu feature from virConnect.getCapabilities.
5544 
5545         :return: see above description
5546 
5547         """
5548 
5549         caps = self._host.get_capabilities()
5550         cpu_info = dict()
5551 
5552         cpu_info['arch'] = caps.host.cpu.arch
5553         cpu_info['model'] = caps.host.cpu.model
5554         cpu_info['vendor'] = caps.host.cpu.vendor
5555 
5556         topology = dict()
5557         topology['cells'] = len(getattr(caps.host.topology, 'cells', [1]))
5558         topology['sockets'] = caps.host.cpu.sockets
5559         topology['cores'] = caps.host.cpu.cores
5560         topology['threads'] = caps.host.cpu.threads
5561         cpu_info['topology'] = topology
5562 
5563         features = set()
5564         for f in caps.host.cpu.features:
5565             features.add(f.name)
5566         cpu_info['features'] = features
5567         return cpu_info
5568 
5569     def _get_pcinet_info(self, vf_address):
5570         """Returns a dict of NET device."""
5571         devname = pci_utils.get_net_name_by_vf_pci_address(vf_address)
5572         if not devname:
5573             return
5574 
5575         virtdev = self._host.device_lookup_by_name(devname)
5576         xmlstr = virtdev.XMLDesc(0)
5577         cfgdev = vconfig.LibvirtConfigNodeDevice()
5578         cfgdev.parse_str(xmlstr)
5579         return {'name': cfgdev.name,
5580                 'capabilities': cfgdev.pci_capability.features}
5581 
5582     def _get_pcidev_info(self, devname):
5583         """Returns a dict of PCI device."""
5584 
5585         def _get_device_type(cfgdev, pci_address):
5586             """Get a PCI device's device type.
5587 
5588             An assignable PCI device can be a normal PCI device,
5589             a SR-IOV Physical Function (PF), or a SR-IOV Virtual
5590             Function (VF). Only normal PCI devices or SR-IOV VFs
5591             are assignable, while SR-IOV PFs are always owned by
5592             hypervisor.
5593             """
5594             for fun_cap in cfgdev.pci_capability.fun_capability:
5595                 if fun_cap.type == 'virt_functions':
5596                     return {
5597                         'dev_type': fields.PciDeviceType.SRIOV_PF,
5598                     }
5599                 if (fun_cap.type == 'phys_function' and
5600                     len(fun_cap.device_addrs) != 0):
5601                     phys_address = "%04x:%02x:%02x.%01x" % (
5602                         fun_cap.device_addrs[0][0],
5603                         fun_cap.device_addrs[0][1],
5604                         fun_cap.device_addrs[0][2],
5605                         fun_cap.device_addrs[0][3])
5606                     return {
5607                         'dev_type': fields.PciDeviceType.SRIOV_VF,
5608                         'parent_addr': phys_address,
5609                     }
5610 
5611             # Note(moshele): libvirt < 1.3 reported virt_functions capability
5612             # only when VFs are enabled. The check below is a workaround
5613             # to get the correct report regardless of whether or not any
5614             # VFs are enabled for the device.
5615             if not self._host.has_min_version(
5616                 MIN_LIBVIRT_PF_WITH_NO_VFS_CAP_VERSION):
5617                 is_physical_function = pci_utils.is_physical_function(
5618                     *pci_utils.get_pci_address_fields(pci_address))
5619                 if is_physical_function:
5620                     return {'dev_type': fields.PciDeviceType.SRIOV_PF}
5621 
5622             return {'dev_type': fields.PciDeviceType.STANDARD}
5623 
5624         def _get_device_capabilities(device, address):
5625             """Get PCI VF device's additional capabilities.
5626 
5627             If a PCI device is a virtual function, this function reads the PCI
5628             parent's network capabilities (must be always a NIC device) and
5629             appends this information to the device's dictionary.
5630             """
5631             if device.get('dev_type') == fields.PciDeviceType.SRIOV_VF:
5632                 pcinet_info = self._get_pcinet_info(address)
5633                 if pcinet_info:
5634                     return {'capabilities':
5635                                 {'network': pcinet_info.get('capabilities')}}
5636             return {}
5637 
5638         virtdev = self._host.device_lookup_by_name(devname)
5639         xmlstr = virtdev.XMLDesc(0)
5640         cfgdev = vconfig.LibvirtConfigNodeDevice()
5641         cfgdev.parse_str(xmlstr)
5642 
5643         address = "%04x:%02x:%02x.%1x" % (
5644             cfgdev.pci_capability.domain,
5645             cfgdev.pci_capability.bus,
5646             cfgdev.pci_capability.slot,
5647             cfgdev.pci_capability.function)
5648 
5649         device = {
5650             "dev_id": cfgdev.name,
5651             "address": address,
5652             "product_id": "%04x" % cfgdev.pci_capability.product_id,
5653             "vendor_id": "%04x" % cfgdev.pci_capability.vendor_id,
5654             }
5655 
5656         device["numa_node"] = cfgdev.pci_capability.numa_node
5657 
5658         # requirement by DataBase Model
5659         device['label'] = 'label_%(vendor_id)s_%(product_id)s' % device
5660         device.update(_get_device_type(cfgdev, address))
5661         device.update(_get_device_capabilities(device, address))
5662         return device
5663 
5664     def _get_pci_passthrough_devices(self):
5665         """Get host PCI devices information.
5666 
5667         Obtains pci devices information from libvirt, and returns
5668         as a JSON string.
5669 
5670         Each device information is a dictionary, with mandatory keys
5671         of 'address', 'vendor_id', 'product_id', 'dev_type', 'dev_id',
5672         'label' and other optional device specific information.
5673 
5674         Refer to the objects/pci_device.py for more idea of these keys.
5675 
5676         :returns: a JSON string containing a list of the assignable PCI
5677                   devices information
5678         """
5679         # Bail early if we know we can't support `listDevices` to avoid
5680         # repeated warnings within a periodic task
5681         if not getattr(self, '_list_devices_supported', True):
5682             return jsonutils.dumps([])
5683 
5684         try:
5685             dev_names = self._host.list_pci_devices() or []
5686         except libvirt.libvirtError as ex:
5687             error_code = ex.get_error_code()
5688             if error_code == libvirt.VIR_ERR_NO_SUPPORT:
5689                 self._list_devices_supported = False
5690                 LOG.warning("URI %(uri)s does not support "
5691                             "listDevices: %(error)s",
5692                             {'uri': self._uri(),
5693                              'error': encodeutils.exception_to_unicode(ex)})
5694                 return jsonutils.dumps([])
5695             else:
5696                 raise
5697 
5698         pci_info = []
5699         for name in dev_names:
5700             pci_info.append(self._get_pcidev_info(name))
5701 
5702         return jsonutils.dumps(pci_info)
5703 
5704     def _get_mdev_capabilities_for_dev(self, devname, types=None):
5705         """Returns a dict of MDEV capable device with the ID as first key
5706         and then a list of supported types, each of them being a dict.
5707 
5708         :param types: Only return those specific types.
5709         """
5710         virtdev = self._host.device_lookup_by_name(devname)
5711         xmlstr = virtdev.XMLDesc(0)
5712         cfgdev = vconfig.LibvirtConfigNodeDevice()
5713         cfgdev.parse_str(xmlstr)
5714 
5715         device = {
5716             "dev_id": cfgdev.name,
5717             "types": {},
5718         }
5719         for mdev_cap in cfgdev.pci_capability.mdev_capability:
5720             for cap in mdev_cap.mdev_types:
5721                 if not types or cap['type'] in types:
5722                     device["types"].update({cap['type']: {
5723                         'availableInstances': cap['availableInstances'],
5724                         'name': cap['name'],
5725                         'deviceAPI': cap['deviceAPI']}})
5726         return device
5727 
5728     def _get_mdev_capable_devices(self, types=None):
5729         """Get host devices supporting mdev types.
5730 
5731         Obtain devices information from libvirt and returns a list of
5732         dictionaries.
5733 
5734         :param types: Filter only devices supporting those types.
5735         """
5736         if not self._host.has_min_version(MIN_LIBVIRT_MDEV_SUPPORT):
5737             return []
5738         dev_names = self._host.list_mdev_capable_devices() or []
5739         mdev_capable_devices = []
5740         for name in dev_names:
5741             device = self._get_mdev_capabilities_for_dev(name, types)
5742             if not device["types"]:
5743                 continue
5744             mdev_capable_devices.append(device)
5745         return mdev_capable_devices
5746 
5747     def _get_mediated_device_information(self, devname):
5748         """Returns a dict of a mediated device."""
5749         virtdev = self._host.device_lookup_by_name(devname)
5750         xmlstr = virtdev.XMLDesc(0)
5751         cfgdev = vconfig.LibvirtConfigNodeDevice()
5752         cfgdev.parse_str(xmlstr)
5753 
5754         device = {
5755             "dev_id": cfgdev.name,
5756             # name is like mdev_00ead764_fdc0_46b6_8db9_2963f5c815b4
5757             "uuid": str(uuid.UUID(cfgdev.name[5:].replace('_', '-'))),
5758             "type": cfgdev.mdev_information.type,
5759             "iommu_group": cfgdev.mdev_information.iommu_group,
5760         }
5761         return device
5762 
5763     def _get_mediated_devices(self, types=None):
5764         """Get host mediated devices.
5765 
5766         Obtain devices information from libvirt and returns a list of
5767         dictionaries.
5768 
5769         :param types: Filter only devices supporting those types.
5770         """
5771         if not self._host.has_min_version(MIN_LIBVIRT_MDEV_SUPPORT):
5772             return []
5773         dev_names = self._host.list_mediated_devices() or []
5774         mediated_devices = []
5775         for name in dev_names:
5776             device = self._get_mediated_device_information(name)
5777             if not types or device["type"] in types:
5778                 mediated_devices.append(device)
5779         return mediated_devices
5780 
5781     def _get_all_assigned_mediated_devices(self):
5782         """Lookup all instances from the host and return all the mediated
5783         devices that are assigned to a guest.
5784 
5785         :returns: A dictionary of keys being mediated device UUIDs and their
5786                   respective values the instance UUID of the guest using it.
5787         """
5788         allocated_mdevs = {}
5789         for guest in self._host.list_guests(only_running=False):
5790             cfg = guest.get_config()
5791             for device in cfg.devices:
5792                 if isinstance(device, vconfig.LibvirtConfigGuestHostdevMDEV):
5793                     allocated_mdevs[device.uuid] = guest.uuid
5794         return allocated_mdevs
5795 
5796     def _has_numa_support(self):
5797         # This means that the host can support LibvirtConfigGuestNUMATune
5798         # and the nodeset field in LibvirtConfigGuestMemoryBackingPage
5799         for ver in BAD_LIBVIRT_NUMA_VERSIONS:
5800             if self._host.has_version(ver):
5801                 if not getattr(self, '_bad_libvirt_numa_version_warn', False):
5802                     LOG.warning('You are running with libvirt version %s '
5803                                 'which is known to have broken NUMA support. '
5804                                 'Consider patching or updating libvirt on '
5805                                 'this host if you need NUMA support.',
5806                                 self._version_to_string(ver))
5807                     self._bad_libvirt_numa_version_warn = True
5808                 return False
5809 
5810         caps = self._host.get_capabilities()
5811 
5812         if (caps.host.cpu.arch in (fields.Architecture.I686,
5813                                    fields.Architecture.X86_64,
5814                                    fields.Architecture.AARCH64) and
5815                 self._host.has_min_version(hv_type=host.HV_DRIVER_QEMU)):
5816             return True
5817         elif (caps.host.cpu.arch in (fields.Architecture.PPC64,
5818                                      fields.Architecture.PPC64LE) and
5819                 self._host.has_min_version(MIN_LIBVIRT_NUMA_VERSION_PPC,
5820                                            hv_type=host.HV_DRIVER_QEMU)):
5821             return True
5822 
5823         return False
5824 
5825     def _get_host_numa_topology(self):
5826         if not self._has_numa_support():
5827             return
5828 
5829         caps = self._host.get_capabilities()
5830         topology = caps.host.topology
5831 
5832         if topology is None or not topology.cells:
5833             return
5834 
5835         cells = []
5836         allowed_cpus = hardware.get_vcpu_pin_set()
5837         online_cpus = self._host.get_online_cpus()
5838         if allowed_cpus:
5839             allowed_cpus &= online_cpus
5840         else:
5841             allowed_cpus = online_cpus
5842 
5843         def _get_reserved_memory_for_cell(self, cell_id, page_size):
5844             cell = self._reserved_hugepages.get(cell_id, {})
5845             return cell.get(page_size, 0)
5846 
5847         for cell in topology.cells:
5848             cpuset = set(cpu.id for cpu in cell.cpus)
5849             siblings = sorted(map(set,
5850                                   set(tuple(cpu.siblings)
5851                                         if cpu.siblings else ()
5852                                       for cpu in cell.cpus)
5853                                   ))
5854             cpuset &= allowed_cpus
5855             siblings = [sib & allowed_cpus for sib in siblings]
5856             # Filter out singles and empty sibling sets that may be left
5857             siblings = [sib for sib in siblings if len(sib) > 1]
5858 
5859             mempages = [
5860                 objects.NUMAPagesTopology(
5861                     size_kb=pages.size,
5862                     total=pages.total,
5863                     used=0,
5864                     reserved=_get_reserved_memory_for_cell(
5865                         self, cell.id, pages.size))
5866                 for pages in cell.mempages]
5867 
5868             cell = objects.NUMACell(id=cell.id, cpuset=cpuset,
5869                                     memory=cell.memory / units.Ki,
5870                                     cpu_usage=0, memory_usage=0,
5871                                     siblings=siblings,
5872                                     pinned_cpus=set([]),
5873                                     mempages=mempages)
5874             cells.append(cell)
5875 
5876         return objects.NUMATopology(cells=cells)
5877 
5878     def get_all_volume_usage(self, context, compute_host_bdms):
5879         """Return usage info for volumes attached to vms on
5880            a given host.
5881         """
5882         vol_usage = []
5883 
5884         for instance_bdms in compute_host_bdms:
5885             instance = instance_bdms['instance']
5886 
5887             for bdm in instance_bdms['instance_bdms']:
5888                 mountpoint = bdm['device_name']
5889                 if mountpoint.startswith('/dev/'):
5890                     mountpoint = mountpoint[5:]
5891                 volume_id = bdm['volume_id']
5892 
5893                 LOG.debug("Trying to get stats for the volume %s",
5894                           volume_id, instance=instance)
5895                 vol_stats = self.block_stats(instance, mountpoint)
5896 
5897                 if vol_stats:
5898                     stats = dict(volume=volume_id,
5899                                  instance=instance,
5900                                  rd_req=vol_stats[0],
5901                                  rd_bytes=vol_stats[1],
5902                                  wr_req=vol_stats[2],
5903                                  wr_bytes=vol_stats[3])
5904                     LOG.debug(
5905                         "Got volume usage stats for the volume=%(volume)s,"
5906                         " rd_req=%(rd_req)d, rd_bytes=%(rd_bytes)d, "
5907                         "wr_req=%(wr_req)d, wr_bytes=%(wr_bytes)d",
5908                         stats, instance=instance)
5909                     vol_usage.append(stats)
5910 
5911         return vol_usage
5912 
5913     def block_stats(self, instance, disk_id):
5914         """Note that this function takes an instance name."""
5915         try:
5916             guest = self._host.get_guest(instance)
5917 
5918             # TODO(sahid): We are converting all calls from a
5919             # virDomain object to use nova.virt.libvirt.Guest.
5920             # We should be able to remove domain at the end.
5921             domain = guest._domain
5922             return domain.blockStats(disk_id)
5923         except libvirt.libvirtError as e:
5924             errcode = e.get_error_code()
5925             LOG.info('Getting block stats failed, device might have '
5926                      'been detached. Instance=%(instance_name)s '
5927                      'Disk=%(disk)s Code=%(errcode)s Error=%(e)s',
5928                      {'instance_name': instance.name, 'disk': disk_id,
5929                       'errcode': errcode, 'e': e},
5930                      instance=instance)
5931         except exception.InstanceNotFound:
5932             LOG.info('Could not find domain in libvirt for instance %s. '
5933                      'Cannot get block stats for device', instance.name,
5934                      instance=instance)
5935 
5936     def get_console_pool_info(self, console_type):
5937         # TODO(mdragon): console proxy should be implemented for libvirt,
5938         #                in case someone wants to use it with kvm or
5939         #                such. For now return fake data.
5940         return {'address': '127.0.0.1',
5941                 'username': 'fakeuser',
5942                 'password': 'fakepassword'}
5943 
5944     def refresh_security_group_rules(self, security_group_id):
5945         self.firewall_driver.refresh_security_group_rules(security_group_id)
5946 
5947     def refresh_instance_security_rules(self, instance):
5948         self.firewall_driver.refresh_instance_security_rules(instance)
5949 
5950     def get_inventory(self, nodename):
5951         """Return a dict, keyed by resource class, of inventory information for
5952         the supplied node.
5953         """
5954         disk_gb = int(self._get_local_gb_info()['total'])
5955         memory_mb = int(self._host.get_memory_mb_total())
5956         vcpus = self._get_vcpu_total()
5957 
5958         # NOTE(sbauza): For the moment, the libvirt driver only supports
5959         # providing the total number of virtual GPUs for a single GPU type. If
5960         # you have multiple physical GPUs, each of them providing multiple GPU
5961         # types, libvirt will return the total sum of virtual GPUs
5962         # corresponding to the single type passed in enabled_vgpu_types
5963         # configuration option. Eg. if you have 2 pGPUs supporting 'nvidia-35',
5964         # each of them having 16 available instances, the total here will be
5965         # 32.
5966         # If one of the 2 pGPUs doesn't support 'nvidia-35', it won't be used.
5967         # TODO(sbauza): Use ProviderTree and traits to make a better world.
5968         vgpus = self._get_vgpu_total()
5969 
5970         # NOTE(jaypipes): We leave some fields like allocation_ratio and
5971         # reserved out of the returned dicts here because, for now at least,
5972         # the RT injects those values into the inventory dict based on the
5973         # compute_nodes record values.
5974         result = {
5975             fields.ResourceClass.VCPU: {
5976                 'total': vcpus,
5977                 'min_unit': 1,
5978                 'max_unit': vcpus,
5979                 'step_size': 1,
5980             },
5981             fields.ResourceClass.MEMORY_MB: {
5982                 'total': memory_mb,
5983                 'min_unit': 1,
5984                 'max_unit': memory_mb,
5985                 'step_size': 1,
5986             },
5987             fields.ResourceClass.DISK_GB: {
5988                 'total': disk_gb,
5989                 'min_unit': 1,
5990                 'max_unit': disk_gb,
5991                 'step_size': 1,
5992             },
5993         }
5994 
5995         if vgpus > 0:
5996             # Only provide VGPU resource classes if the driver supports it.
5997             result[fields.ResourceClass.VGPU] = {
5998                 'total': vgpus,
5999                 'min_unit': 1,
6000                 'max_unit': vgpus,
6001                 'step_size': 1,
6002                 }
6003 
6004         return result
6005 
6006     def get_available_resource(self, nodename):
6007         """Retrieve resource information.
6008 
6009         This method is called when nova-compute launches, and
6010         as part of a periodic task that records the results in the DB.
6011 
6012         :param nodename: unused in this driver
6013         :returns: dictionary containing resource info
6014         """
6015 
6016         disk_info_dict = self._get_local_gb_info()
6017         data = {}
6018 
6019         # NOTE(dprince): calling capabilities before getVersion works around
6020         # an initialization issue with some versions of Libvirt (1.0.5.5).
6021         # See: https://bugzilla.redhat.com/show_bug.cgi?id=1000116
6022         # See: https://bugs.launchpad.net/nova/+bug/1215593
6023         data["supported_instances"] = self._get_instance_capabilities()
6024 
6025         data["vcpus"] = self._get_vcpu_total()
6026         data["memory_mb"] = self._host.get_memory_mb_total()
6027         data["local_gb"] = disk_info_dict['total']
6028         data["vcpus_used"] = self._get_vcpu_used()
6029         data["memory_mb_used"] = self._host.get_memory_mb_used()
6030         data["local_gb_used"] = disk_info_dict['used']
6031         data["hypervisor_type"] = self._host.get_driver_type()
6032         data["hypervisor_version"] = self._host.get_version()
6033         data["hypervisor_hostname"] = self._host.get_hostname()
6034         # TODO(berrange): why do we bother converting the
6035         # libvirt capabilities XML into a special JSON format ?
6036         # The data format is different across all the drivers
6037         # so we could just return the raw capabilities XML
6038         # which 'compare_cpu' could use directly
6039         #
6040         # That said, arch_filter.py now seems to rely on
6041         # the libvirt drivers format which suggests this
6042         # data format needs to be standardized across drivers
6043         data["cpu_info"] = jsonutils.dumps(self._get_cpu_info())
6044 
6045         disk_free_gb = disk_info_dict['free']
6046         disk_over_committed = self._get_disk_over_committed_size_total()
6047         available_least = disk_free_gb * units.Gi - disk_over_committed
6048         data['disk_available_least'] = available_least / units.Gi
6049 
6050         data['pci_passthrough_devices'] = \
6051             self._get_pci_passthrough_devices()
6052 
6053         numa_topology = self._get_host_numa_topology()
6054         if numa_topology:
6055             data['numa_topology'] = numa_topology._to_json()
6056         else:
6057             data['numa_topology'] = None
6058 
6059         return data
6060 
6061     def check_instance_shared_storage_local(self, context, instance):
6062         """Check if instance files located on shared storage.
6063 
6064         This runs check on the destination host, and then calls
6065         back to the source host to check the results.
6066 
6067         :param context: security context
6068         :param instance: nova.objects.instance.Instance object
6069         :returns:
6070          - tempfile: A dict containing the tempfile info on the destination
6071                      host
6072          - None:
6073 
6074             1. If the instance path is not existing.
6075             2. If the image backend is shared block storage type.
6076         """
6077         if self.image_backend.backend().is_shared_block_storage():
6078             return None
6079 
6080         dirpath = libvirt_utils.get_instance_path(instance)
6081 
6082         if not os.path.exists(dirpath):
6083             return None
6084 
6085         fd, tmp_file = tempfile.mkstemp(dir=dirpath)
6086         LOG.debug("Creating tmpfile %s to verify with other "
6087                   "compute node that the instance is on "
6088                   "the same shared storage.",
6089                   tmp_file, instance=instance)
6090         os.close(fd)
6091         return {"filename": tmp_file}
6092 
6093     def check_instance_shared_storage_remote(self, context, data):
6094         return os.path.exists(data['filename'])
6095 
6096     def check_instance_shared_storage_cleanup(self, context, data):
6097         fileutils.delete_if_exists(data["filename"])
6098 
6099     def check_can_live_migrate_destination(self, context, instance,
6100                                            src_compute_info, dst_compute_info,
6101                                            block_migration=False,
6102                                            disk_over_commit=False):
6103         """Check if it is possible to execute live migration.
6104 
6105         This runs checks on the destination host, and then calls
6106         back to the source host to check the results.
6107 
6108         :param context: security context
6109         :param instance: nova.db.sqlalchemy.models.Instance
6110         :param block_migration: if true, prepare for block migration
6111         :param disk_over_commit: if true, allow disk over commit
6112         :returns: a LibvirtLiveMigrateData object
6113         """
6114         if disk_over_commit:
6115             disk_available_gb = dst_compute_info['local_gb']
6116         else:
6117             disk_available_gb = dst_compute_info['disk_available_least']
6118         disk_available_mb = (
6119             (disk_available_gb * units.Ki) - CONF.reserved_host_disk_mb)
6120 
6121         # Compare CPU
6122         if not instance.vcpu_model or not instance.vcpu_model.model:
6123             source_cpu_info = src_compute_info['cpu_info']
6124             self._compare_cpu(None, source_cpu_info, instance)
6125         else:
6126             self._compare_cpu(instance.vcpu_model, None, instance)
6127 
6128         # Create file on storage, to be checked on source host
6129         filename = self._create_shared_storage_test_file(instance)
6130 
6131         data = objects.LibvirtLiveMigrateData()
6132         data.filename = filename
6133         data.image_type = CONF.libvirt.images_type
6134         data.graphics_listen_addr_vnc = CONF.vnc.server_listen
6135         data.graphics_listen_addr_spice = CONF.spice.server_listen
6136         if CONF.serial_console.enabled:
6137             data.serial_listen_addr = CONF.serial_console.proxyclient_address
6138         else:
6139             data.serial_listen_addr = None
6140         # Notes(eliqiao): block_migration and disk_over_commit are not
6141         # nullable, so just don't set them if they are None
6142         if block_migration is not None:
6143             data.block_migration = block_migration
6144         if disk_over_commit is not None:
6145             data.disk_over_commit = disk_over_commit
6146         data.disk_available_mb = disk_available_mb
6147         return data
6148 
6149     def cleanup_live_migration_destination_check(self, context,
6150                                                  dest_check_data):
6151         """Do required cleanup on dest host after check_can_live_migrate calls
6152 
6153         :param context: security context
6154         """
6155         filename = dest_check_data.filename
6156         self._cleanup_shared_storage_test_file(filename)
6157 
6158     def check_can_live_migrate_source(self, context, instance,
6159                                       dest_check_data,
6160                                       block_device_info=None):
6161         """Check if it is possible to execute live migration.
6162 
6163         This checks if the live migration can succeed, based on the
6164         results from check_can_live_migrate_destination.
6165 
6166         :param context: security context
6167         :param instance: nova.db.sqlalchemy.models.Instance
6168         :param dest_check_data: result of check_can_live_migrate_destination
6169         :param block_device_info: result of _get_instance_block_device_info
6170         :returns: a LibvirtLiveMigrateData object
6171         """
6172         if not isinstance(dest_check_data, migrate_data_obj.LiveMigrateData):
6173             md_obj = objects.LibvirtLiveMigrateData()
6174             md_obj.from_legacy_dict(dest_check_data)
6175             dest_check_data = md_obj
6176 
6177         # Checking shared storage connectivity
6178         # if block migration, instances_path should not be on shared storage.
6179         source = CONF.host
6180 
6181         dest_check_data.is_shared_instance_path = (
6182             self._check_shared_storage_test_file(
6183                 dest_check_data.filename, instance))
6184 
6185         dest_check_data.is_shared_block_storage = (
6186             self._is_shared_block_storage(instance, dest_check_data,
6187                                           block_device_info))
6188 
6189         if 'block_migration' not in dest_check_data:
6190             dest_check_data.block_migration = (
6191                 not dest_check_data.is_on_shared_storage())
6192 
6193         if dest_check_data.block_migration:
6194             # TODO(eliqiao): Once block_migration flag is removed from the API
6195             # we can safely remove the if condition
6196             if dest_check_data.is_on_shared_storage():
6197                 reason = _("Block migration can not be used "
6198                            "with shared storage.")
6199                 raise exception.InvalidLocalStorage(reason=reason, path=source)
6200             if 'disk_over_commit' in dest_check_data:
6201                 self._assert_dest_node_has_enough_disk(context, instance,
6202                                         dest_check_data.disk_available_mb,
6203                                         dest_check_data.disk_over_commit,
6204                                         block_device_info)
6205             if block_device_info:
6206                 bdm = block_device_info.get('block_device_mapping')
6207                 # NOTE(pkoniszewski): libvirt from version 1.2.17 upwards
6208                 # supports selective block device migration. It means that it
6209                 # is possible to define subset of block devices to be copied
6210                 # during migration. If they are not specified - block devices
6211                 # won't be migrated. However, it does not work when live
6212                 # migration is tunnelled through libvirt.
6213                 if bdm and not self._host.has_min_version(
6214                         MIN_LIBVIRT_BLOCK_LM_WITH_VOLUMES_VERSION):
6215                     # NOTE(stpierre): if this instance has mapped volumes,
6216                     # we can't do a block migration, since that will result
6217                     # in volumes being copied from themselves to themselves,
6218                     # which is a recipe for disaster.
6219                     ver = ".".join([str(x) for x in
6220                                     MIN_LIBVIRT_BLOCK_LM_WITH_VOLUMES_VERSION])
6221                     msg = (_('Cannot block migrate instance %(uuid)s with'
6222                              ' mapped volumes. Selective block device'
6223                              ' migration feature requires libvirt version'
6224                              ' %(libvirt_ver)s') %
6225                            {'uuid': instance.uuid, 'libvirt_ver': ver})
6226                     LOG.error(msg, instance=instance)
6227                     raise exception.MigrationPreCheckError(reason=msg)
6228                 # NOTE(eliqiao): Selective disk migrations are not supported
6229                 # with tunnelled block migrations so we can block them early.
6230                 if (bdm and
6231                     (self._block_migration_flags &
6232                      libvirt.VIR_MIGRATE_TUNNELLED != 0)):
6233                     msg = (_('Cannot block migrate instance %(uuid)s with'
6234                              ' mapped volumes. Selective block device'
6235                              ' migration is not supported with tunnelled'
6236                              ' block migrations.') % {'uuid': instance.uuid})
6237                     LOG.error(msg, instance=instance)
6238                     raise exception.MigrationPreCheckError(reason=msg)
6239         elif not (dest_check_data.is_shared_block_storage or
6240                   dest_check_data.is_shared_instance_path):
6241             reason = _("Shared storage live-migration requires either shared "
6242                        "storage or boot-from-volume with no local disks.")
6243             raise exception.InvalidSharedStorage(reason=reason, path=source)
6244 
6245         # NOTE(mikal): include the instance directory name here because it
6246         # doesn't yet exist on the destination but we want to force that
6247         # same name to be used
6248         instance_path = libvirt_utils.get_instance_path(instance,
6249                                                         relative=True)
6250         dest_check_data.instance_relative_path = instance_path
6251 
6252         return dest_check_data
6253 
6254     def _is_shared_block_storage(self, instance, dest_check_data,
6255                                  block_device_info=None):
6256         """Check if all block storage of an instance can be shared
6257         between source and destination of a live migration.
6258 
6259         Returns true if the instance is volume backed and has no local disks,
6260         or if the image backend is the same on source and destination and the
6261         backend shares block storage between compute nodes.
6262 
6263         :param instance: nova.objects.instance.Instance object
6264         :param dest_check_data: dict with boolean fields image_type,
6265                                 is_shared_instance_path, and is_volume_backed
6266         """
6267         if (dest_check_data.obj_attr_is_set('image_type') and
6268                 CONF.libvirt.images_type == dest_check_data.image_type and
6269                 self.image_backend.backend().is_shared_block_storage()):
6270             # NOTE(dgenin): currently true only for RBD image backend
6271             return True
6272 
6273         if (dest_check_data.is_shared_instance_path and
6274                 self.image_backend.backend().is_file_in_instance_path()):
6275             # NOTE(angdraug): file based image backends (Flat, Qcow2)
6276             # place block device files under the instance path
6277             return True
6278 
6279         if (dest_check_data.is_volume_backed and
6280                 not bool(self._get_instance_disk_info(instance,
6281                                                       block_device_info))):
6282             return True
6283 
6284         return False
6285 
6286     def _assert_dest_node_has_enough_disk(self, context, instance,
6287                                              available_mb, disk_over_commit,
6288                                              block_device_info):
6289         """Checks if destination has enough disk for block migration."""
6290         # Libvirt supports qcow2 disk format,which is usually compressed
6291         # on compute nodes.
6292         # Real disk image (compressed) may enlarged to "virtual disk size",
6293         # that is specified as the maximum disk size.
6294         # (See qemu-img -f path-to-disk)
6295         # Scheduler recognizes destination host still has enough disk space
6296         # if real disk size < available disk size
6297         # if disk_over_commit is True,
6298         #  otherwise virtual disk size < available disk size.
6299 
6300         available = 0
6301         if available_mb:
6302             available = available_mb * units.Mi
6303 
6304         disk_infos = self._get_instance_disk_info(instance, block_device_info)
6305 
6306         necessary = 0
6307         if disk_over_commit:
6308             for info in disk_infos:
6309                 necessary += int(info['disk_size'])
6310         else:
6311             for info in disk_infos:
6312                 necessary += int(info['virt_disk_size'])
6313 
6314         # Check that available disk > necessary disk
6315         if (available - necessary) < 0:
6316             reason = (_('Unable to migrate %(instance_uuid)s: '
6317                         'Disk of instance is too large(available'
6318                         ' on destination host:%(available)s '
6319                         '< need:%(necessary)s)') %
6320                       {'instance_uuid': instance.uuid,
6321                        'available': available,
6322                        'necessary': necessary})
6323             raise exception.MigrationPreCheckError(reason=reason)
6324 
6325     def _compare_cpu(self, guest_cpu, host_cpu_str, instance):
6326         """Check the host is compatible with the requested CPU
6327 
6328         :param guest_cpu: nova.objects.VirtCPUModel or None
6329         :param host_cpu_str: JSON from _get_cpu_info() method
6330 
6331         If the 'guest_cpu' parameter is not None, this will be
6332         validated for migration compatibility with the host.
6333         Otherwise the 'host_cpu_str' JSON string will be used for
6334         validation.
6335 
6336         :returns:
6337             None. if given cpu info is not compatible to this server,
6338             raise exception.
6339         """
6340 
6341         # NOTE(kchamart): Comparing host to guest CPU model for emulated
6342         # guests (<domain type='qemu'>) should not matter -- in this
6343         # mode (QEMU "TCG") the CPU is fully emulated in software and no
6344         # hardware acceleration, like KVM, is involved. So, skip the CPU
6345         # compatibility check for the QEMU domain type, and retain it for
6346         # KVM guests.
6347         if CONF.libvirt.virt_type not in ['kvm']:
6348             return
6349 
6350         if guest_cpu is None:
6351             info = jsonutils.loads(host_cpu_str)
6352             LOG.info('Instance launched has CPU info: %s', host_cpu_str)
6353             cpu = vconfig.LibvirtConfigCPU()
6354             cpu.arch = info['arch']
6355             cpu.model = info['model']
6356             cpu.vendor = info['vendor']
6357             cpu.sockets = info['topology']['sockets']
6358             cpu.cores = info['topology']['cores']
6359             cpu.threads = info['topology']['threads']
6360             for f in info['features']:
6361                 cpu.add_feature(vconfig.LibvirtConfigCPUFeature(f))
6362         else:
6363             cpu = self._vcpu_model_to_cpu_config(guest_cpu)
6364 
6365         u = ("http://libvirt.org/html/libvirt-libvirt-host.html#"
6366              "virCPUCompareResult")
6367         m = _("CPU doesn't have compatibility.\n\n%(ret)s\n\nRefer to %(u)s")
6368         # unknown character exists in xml, then libvirt complains
6369         try:
6370             cpu_xml = cpu.to_xml()
6371             LOG.debug("cpu compare xml: %s", cpu_xml, instance=instance)
6372             ret = self._host.compare_cpu(cpu_xml)
6373         except libvirt.libvirtError as e:
6374             error_code = e.get_error_code()
6375             if error_code == libvirt.VIR_ERR_NO_SUPPORT:
6376                 LOG.debug("URI %(uri)s does not support cpu comparison. "
6377                           "It will be proceeded though. Error: %(error)s",
6378                           {'uri': self._uri(), 'error': e})
6379                 return
6380             else:
6381                 LOG.error(m, {'ret': e, 'u': u})
6382                 raise exception.MigrationPreCheckError(
6383                     reason=m % {'ret': e, 'u': u})
6384 
6385         if ret <= 0:
6386             LOG.error(m, {'ret': ret, 'u': u})
6387             raise exception.InvalidCPUInfo(reason=m % {'ret': ret, 'u': u})
6388 
6389     def _create_shared_storage_test_file(self, instance):
6390         """Makes tmpfile under CONF.instances_path."""
6391         dirpath = CONF.instances_path
6392         fd, tmp_file = tempfile.mkstemp(dir=dirpath)
6393         LOG.debug("Creating tmpfile %s to notify to other "
6394                   "compute nodes that they should mount "
6395                   "the same storage.", tmp_file, instance=instance)
6396         os.close(fd)
6397         return os.path.basename(tmp_file)
6398 
6399     def _check_shared_storage_test_file(self, filename, instance):
6400         """Confirms existence of the tmpfile under CONF.instances_path.
6401 
6402         Cannot confirm tmpfile return False.
6403         """
6404         # NOTE(tpatzig): if instances_path is a shared volume that is
6405         # under heavy IO (many instances on many compute nodes),
6406         # then checking the existence of the testfile fails,
6407         # just because it takes longer until the client refreshes and new
6408         # content gets visible.
6409         # os.utime (like touch) on the directory forces the client to refresh.
6410         os.utime(CONF.instances_path, None)
6411 
6412         tmp_file = os.path.join(CONF.instances_path, filename)
6413         if not os.path.exists(tmp_file):
6414             exists = False
6415         else:
6416             exists = True
6417         LOG.debug('Check if temp file %s exists to indicate shared storage '
6418                   'is being used for migration. Exists? %s', tmp_file, exists,
6419                   instance=instance)
6420         return exists
6421 
6422     def _cleanup_shared_storage_test_file(self, filename):
6423         """Removes existence of the tmpfile under CONF.instances_path."""
6424         tmp_file = os.path.join(CONF.instances_path, filename)
6425         os.remove(tmp_file)
6426 
6427     def ensure_filtering_rules_for_instance(self, instance, network_info):
6428         """Ensure that an instance's filtering rules are enabled.
6429 
6430         When migrating an instance, we need the filtering rules to
6431         be configured on the destination host before starting the
6432         migration.
6433 
6434         Also, when restarting the compute service, we need to ensure
6435         that filtering rules exist for all running services.
6436         """
6437 
6438         self.firewall_driver.setup_basic_filtering(instance, network_info)
6439         self.firewall_driver.prepare_instance_filter(instance,
6440                 network_info)
6441 
6442         # nwfilters may be defined in a separate thread in the case
6443         # of libvirt non-blocking mode, so we wait for completion
6444         timeout_count = list(range(CONF.live_migration_retry_count))
6445         while timeout_count:
6446             if self.firewall_driver.instance_filter_exists(instance,
6447                                                            network_info):
6448                 break
6449             timeout_count.pop()
6450             if len(timeout_count) == 0:
6451                 msg = _('The firewall filter for %s does not exist')
6452                 raise exception.InternalError(msg % instance.name)
6453             greenthread.sleep(1)
6454 
6455     def filter_defer_apply_on(self):
6456         self.firewall_driver.filter_defer_apply_on()
6457 
6458     def filter_defer_apply_off(self):
6459         self.firewall_driver.filter_defer_apply_off()
6460 
6461     def live_migration(self, context, instance, dest,
6462                        post_method, recover_method, block_migration=False,
6463                        migrate_data=None):
6464         """Spawning live_migration operation for distributing high-load.
6465 
6466         :param context: security context
6467         :param instance:
6468             nova.db.sqlalchemy.models.Instance object
6469             instance object that is migrated.
6470         :param dest: destination host
6471         :param post_method:
6472             post operation method.
6473             expected nova.compute.manager._post_live_migration.
6474         :param recover_method:
6475             recovery method when any exception occurs.
6476             expected nova.compute.manager._rollback_live_migration.
6477         :param block_migration: if true, do block migration.
6478         :param migrate_data: a LibvirtLiveMigrateData object
6479 
6480         """
6481 
6482         # 'dest' will be substituted into 'migration_uri' so ensure
6483         # it does't contain any characters that could be used to
6484         # exploit the URI accepted by libivrt
6485         if not libvirt_utils.is_valid_hostname(dest):
6486             raise exception.InvalidHostname(hostname=dest)
6487 
6488         self._live_migration(context, instance, dest,
6489                              post_method, recover_method, block_migration,
6490                              migrate_data)
6491 
6492     def live_migration_abort(self, instance):
6493         """Aborting a running live-migration.
6494 
6495         :param instance: instance object that is in migration
6496 
6497         """
6498 
6499         guest = self._host.get_guest(instance)
6500         dom = guest._domain
6501 
6502         try:
6503             dom.abortJob()
6504         except libvirt.libvirtError as e:
6505             LOG.error("Failed to cancel migration %s",
6506                     encodeutils.exception_to_unicode(e), instance=instance)
6507             raise
6508 
6509     def _verify_serial_console_is_disabled(self):
6510         if CONF.serial_console.enabled:
6511 
6512             msg = _('Your destination node does not support'
6513                     ' retrieving listen addresses.  In order'
6514                     ' for live migration to work properly you'
6515                     ' must disable serial console.')
6516             raise exception.MigrationError(reason=msg)
6517 
6518     def _live_migration_operation(self, context, instance, dest,
6519                                   block_migration, migrate_data, guest,
6520                                   device_names):
6521         """Invoke the live migration operation
6522 
6523         :param context: security context
6524         :param instance:
6525             nova.db.sqlalchemy.models.Instance object
6526             instance object that is migrated.
6527         :param dest: destination host
6528         :param block_migration: if true, do block migration.
6529         :param migrate_data: a LibvirtLiveMigrateData object
6530         :param guest: the guest domain object
6531         :param device_names: list of device names that are being migrated with
6532             instance
6533 
6534         This method is intended to be run in a background thread and will
6535         block that thread until the migration is finished or failed.
6536         """
6537         try:
6538             if migrate_data.block_migration:
6539                 migration_flags = self._block_migration_flags
6540             else:
6541                 migration_flags = self._live_migration_flags
6542 
6543             serial_listen_addr = libvirt_migrate.serial_listen_addr(
6544                 migrate_data)
6545             if not serial_listen_addr:
6546                 # In this context we want to ensure that serial console is
6547                 # disabled on source node. This is because nova couldn't
6548                 # retrieve serial listen address from destination node, so we
6549                 # consider that destination node might have serial console
6550                 # disabled as well.
6551                 self._verify_serial_console_is_disabled()
6552 
6553             # NOTE(aplanas) migrate_uri will have a value only in the
6554             # case that `live_migration_inbound_addr` parameter is
6555             # set, and we propose a non tunneled migration.
6556             migrate_uri = None
6557             if ('target_connect_addr' in migrate_data and
6558                     migrate_data.target_connect_addr is not None):
6559                 dest = migrate_data.target_connect_addr
6560                 if (migration_flags &
6561                     libvirt.VIR_MIGRATE_TUNNELLED == 0):
6562                     migrate_uri = self._migrate_uri(dest)
6563 
6564             params = None
6565             new_xml_str = None
6566             if CONF.libvirt.virt_type != "parallels":
6567                 new_xml_str = libvirt_migrate.get_updated_guest_xml(
6568                     # TODO(sahid): It's not a really good idea to pass
6569                     # the method _get_volume_config and we should to find
6570                     # a way to avoid this in future.
6571                     guest, migrate_data, self._get_volume_config)
6572             if self._host.has_min_version(
6573                     MIN_LIBVIRT_BLOCK_LM_WITH_VOLUMES_VERSION):
6574                 params = {
6575                     'destination_xml': new_xml_str,
6576                     'migrate_disks': device_names,
6577                 }
6578                 # NOTE(pkoniszewski): Because of precheck which blocks
6579                 # tunnelled block live migration with mapped volumes we
6580                 # can safely remove migrate_disks when tunnelling is on.
6581                 # Otherwise we will block all tunnelled block migrations,
6582                 # even when an instance does not have volumes mapped.
6583                 # This is because selective disk migration is not
6584                 # supported in tunnelled block live migration. Also we
6585                 # cannot fallback to migrateToURI2 in this case because of
6586                 # bug #1398999
6587                 if (migration_flags &
6588                     libvirt.VIR_MIGRATE_TUNNELLED != 0):
6589                     params.pop('migrate_disks')
6590 
6591             # TODO(sahid): This should be in
6592             # post_live_migration_at_source but no way to retrieve
6593             # ports acquired on the host for the guest at this
6594             # step. Since the domain is going to be removed from
6595             # libvird on source host after migration, we backup the
6596             # serial ports to release them if all went well.
6597             serial_ports = []
6598             if CONF.serial_console.enabled:
6599                 serial_ports = list(self._get_serial_ports_from_guest(guest))
6600 
6601             guest.migrate(self._live_migration_uri(dest),
6602                           migrate_uri=migrate_uri,
6603                           flags=migration_flags,
6604                           params=params,
6605                           domain_xml=new_xml_str,
6606                           bandwidth=CONF.libvirt.live_migration_bandwidth)
6607 
6608             for hostname, port in serial_ports:
6609                 serial_console.release_port(host=hostname, port=port)
6610         except Exception as e:
6611             with excutils.save_and_reraise_exception():
6612                 LOG.error("Live Migration failure: %s", e, instance=instance)
6613 
6614         # If 'migrateToURI' fails we don't know what state the
6615         # VM instances on each host are in. Possibilities include
6616         #
6617         #  1. src==running, dst==none
6618         #
6619         #     Migration failed & rolled back, or never started
6620         #
6621         #  2. src==running, dst==paused
6622         #
6623         #     Migration started but is still ongoing
6624         #
6625         #  3. src==paused,  dst==paused
6626         #
6627         #     Migration data transfer completed, but switchover
6628         #     is still ongoing, or failed
6629         #
6630         #  4. src==paused,  dst==running
6631         #
6632         #     Migration data transfer completed, switchover
6633         #     happened but cleanup on source failed
6634         #
6635         #  5. src==none,    dst==running
6636         #
6637         #     Migration fully succeeded.
6638         #
6639         # Libvirt will aim to complete any migration operation
6640         # or roll it back. So even if the migrateToURI call has
6641         # returned an error, if the migration was not finished
6642         # libvirt should clean up.
6643         #
6644         # So we take the error raise here with a pinch of salt
6645         # and rely on the domain job info status to figure out
6646         # what really happened to the VM, which is a much more
6647         # reliable indicator.
6648         #
6649         # In particular we need to try very hard to ensure that
6650         # Nova does not "forget" about the guest. ie leaving it
6651         # running on a different host to the one recorded in
6652         # the database, as that would be a serious resource leak
6653 
6654         LOG.debug("Migration operation thread has finished",
6655                   instance=instance)
6656 
6657     def _live_migration_copy_disk_paths(self, context, instance, guest):
6658         '''Get list of disks to copy during migration
6659 
6660         :param context: security context
6661         :param instance: the instance being migrated
6662         :param guest: the Guest instance being migrated
6663 
6664         Get the list of disks to copy during migration.
6665 
6666         :returns: a list of local source paths and a list of device names to
6667             copy
6668         '''
6669 
6670         disk_paths = []
6671         device_names = []
6672         block_devices = []
6673 
6674         # TODO(pkoniszewski): Remove version check when we bump min libvirt
6675         # version to >= 1.2.17.
6676         if (self._block_migration_flags &
6677                 libvirt.VIR_MIGRATE_TUNNELLED == 0 and
6678                 self._host.has_min_version(
6679                     MIN_LIBVIRT_BLOCK_LM_WITH_VOLUMES_VERSION)):
6680             bdm_list = objects.BlockDeviceMappingList.get_by_instance_uuid(
6681                 context, instance.uuid)
6682             block_device_info = driver.get_block_device_info(instance,
6683                                                              bdm_list)
6684 
6685             block_device_mappings = driver.block_device_info_get_mapping(
6686                 block_device_info)
6687             for bdm in block_device_mappings:
6688                 device_name = str(bdm['mount_device'].rsplit('/', 1)[1])
6689                 block_devices.append(device_name)
6690 
6691         for dev in guest.get_all_disks():
6692             if dev.readonly or dev.shareable:
6693                 continue
6694             if dev.source_type not in ["file", "block"]:
6695                 continue
6696             if dev.target_dev in block_devices:
6697                 continue
6698             disk_paths.append(dev.source_path)
6699             device_names.append(dev.target_dev)
6700         return (disk_paths, device_names)
6701 
6702     def _live_migration_data_gb(self, instance, disk_paths):
6703         '''Calculate total amount of data to be transferred
6704 
6705         :param instance: the nova.objects.Instance being migrated
6706         :param disk_paths: list of disk paths that are being migrated
6707         with instance
6708 
6709         Calculates the total amount of data that needs to be
6710         transferred during the live migration. The actual
6711         amount copied will be larger than this, due to the
6712         guest OS continuing to dirty RAM while the migration
6713         is taking place. So this value represents the minimal
6714         data size possible.
6715 
6716         :returns: data size to be copied in GB
6717         '''
6718 
6719         ram_gb = instance.flavor.memory_mb * units.Mi / units.Gi
6720         if ram_gb < 2:
6721             ram_gb = 2
6722 
6723         disk_gb = 0
6724         for path in disk_paths:
6725             try:
6726                 size = os.stat(path).st_size
6727                 size_gb = (size / units.Gi)
6728                 if size_gb < 2:
6729                     size_gb = 2
6730                 disk_gb += size_gb
6731             except OSError as e:
6732                 LOG.warning("Unable to stat %(disk)s: %(ex)s",
6733                             {'disk': path, 'ex': e})
6734                 # Ignore error since we don't want to break
6735                 # the migration monitoring thread operation
6736 
6737         return ram_gb + disk_gb
6738 
6739     def _get_migration_flags(self, is_block_migration):
6740         if is_block_migration:
6741             return self._block_migration_flags
6742         return self._live_migration_flags
6743 
6744     def _live_migration_monitor(self, context, instance, guest,
6745                                 dest, post_method,
6746                                 recover_method, block_migration,
6747                                 migrate_data, finish_event,
6748                                 disk_paths):
6749         on_migration_failure = deque()
6750         data_gb = self._live_migration_data_gb(instance, disk_paths)
6751         downtime_steps = list(libvirt_migrate.downtime_steps(data_gb))
6752         migration = migrate_data.migration
6753         curdowntime = None
6754 
6755         migration_flags = self._get_migration_flags(
6756                                   migrate_data.block_migration)
6757 
6758         n = 0
6759         start = time.time()
6760         progress_time = start
6761         progress_watermark = None
6762         previous_data_remaining = -1
6763         is_post_copy_enabled = self._is_post_copy_enabled(migration_flags)
6764         while True:
6765             info = guest.get_job_info()
6766 
6767             if info.type == libvirt.VIR_DOMAIN_JOB_NONE:
6768                 # Either still running, or failed or completed,
6769                 # lets untangle the mess
6770                 if not finish_event.ready():
6771                     LOG.debug("Operation thread is still running",
6772                               instance=instance)
6773                 else:
6774                     info.type = libvirt_migrate.find_job_type(guest, instance)
6775                     LOG.debug("Fixed incorrect job type to be %d",
6776                               info.type, instance=instance)
6777 
6778             if info.type == libvirt.VIR_DOMAIN_JOB_NONE:
6779                 # Migration is not yet started
6780                 LOG.debug("Migration not running yet",
6781                           instance=instance)
6782             elif info.type == libvirt.VIR_DOMAIN_JOB_UNBOUNDED:
6783                 # Migration is still running
6784                 #
6785                 # This is where we wire up calls to change live
6786                 # migration status. eg change max downtime, cancel
6787                 # the operation, change max bandwidth
6788                 libvirt_migrate.run_tasks(guest, instance,
6789                                           self.active_migrations,
6790                                           on_migration_failure,
6791                                           migration,
6792                                           is_post_copy_enabled)
6793 
6794                 now = time.time()
6795                 elapsed = now - start
6796 
6797                 if ((progress_watermark is None) or
6798                     (progress_watermark == 0) or
6799                     (progress_watermark > info.data_remaining)):
6800                     progress_watermark = info.data_remaining
6801                     progress_time = now
6802 
6803                 progress_timeout = CONF.libvirt.live_migration_progress_timeout
6804                 completion_timeout = int(
6805                     CONF.libvirt.live_migration_completion_timeout * data_gb)
6806                 if libvirt_migrate.should_abort(instance, now, progress_time,
6807                                                 progress_timeout, elapsed,
6808                                                 completion_timeout,
6809                                                 migration.status):
6810                     try:
6811                         guest.abort_job()
6812                     except libvirt.libvirtError as e:
6813                         LOG.warning("Failed to abort migration %s",
6814                                 encodeutils.exception_to_unicode(e),
6815                                 instance=instance)
6816                         self._clear_empty_migration(instance)
6817                         raise
6818 
6819                 if (is_post_copy_enabled and
6820                     libvirt_migrate.should_switch_to_postcopy(
6821                     info.memory_iteration, info.data_remaining,
6822                     previous_data_remaining, migration.status)):
6823                     libvirt_migrate.trigger_postcopy_switch(guest,
6824                                                             instance,
6825                                                             migration)
6826                 previous_data_remaining = info.data_remaining
6827 
6828                 curdowntime = libvirt_migrate.update_downtime(
6829                     guest, instance, curdowntime,
6830                     downtime_steps, elapsed)
6831 
6832                 # We loop every 500ms, so don't log on every
6833                 # iteration to avoid spamming logs for long
6834                 # running migrations. Just once every 5 secs
6835                 # is sufficient for developers to debug problems.
6836                 # We log once every 30 seconds at info to help
6837                 # admins see slow running migration operations
6838                 # when debug logs are off.
6839                 if (n % 10) == 0:
6840                     # Ignoring memory_processed, as due to repeated
6841                     # dirtying of data, this can be way larger than
6842                     # memory_total. Best to just look at what's
6843                     # remaining to copy and ignore what's done already
6844                     #
6845                     # TODO(berrange) perhaps we could include disk
6846                     # transfer stats in the progress too, but it
6847                     # might make memory info more obscure as large
6848                     # disk sizes might dwarf memory size
6849                     remaining = 100
6850                     if info.memory_total != 0:
6851                         remaining = round(info.memory_remaining *
6852                                           100 / info.memory_total)
6853 
6854                     libvirt_migrate.save_stats(instance, migration,
6855                                                info, remaining)
6856 
6857                     lg = LOG.debug
6858                     if (n % 60) == 0:
6859                         lg = LOG.info
6860 
6861                     lg("Migration running for %(secs)d secs, "
6862                        "memory %(remaining)d%% remaining; "
6863                        "(bytes processed=%(processed_memory)d, "
6864                        "remaining=%(remaining_memory)d, "
6865                        "total=%(total_memory)d)",
6866                        {"secs": n / 2, "remaining": remaining,
6867                         "processed_memory": info.memory_processed,
6868                         "remaining_memory": info.memory_remaining,
6869                         "total_memory": info.memory_total}, instance=instance)
6870                     if info.data_remaining > progress_watermark:
6871                         lg("Data remaining %(remaining)d bytes, "
6872                            "low watermark %(watermark)d bytes "
6873                            "%(last)d seconds ago",
6874                            {"remaining": info.data_remaining,
6875                             "watermark": progress_watermark,
6876                             "last": (now - progress_time)}, instance=instance)
6877 
6878                 n = n + 1
6879             elif info.type == libvirt.VIR_DOMAIN_JOB_COMPLETED:
6880                 # Migration is all done
6881                 LOG.info("Migration operation has completed",
6882                          instance=instance)
6883                 post_method(context, instance, dest, block_migration,
6884                             migrate_data)
6885                 break
6886             elif info.type == libvirt.VIR_DOMAIN_JOB_FAILED:
6887                 # Migration did not succeed
6888                 LOG.error("Migration operation has aborted", instance=instance)
6889                 libvirt_migrate.run_recover_tasks(self._host, guest, instance,
6890                                                   on_migration_failure)
6891                 recover_method(context, instance, dest, migrate_data)
6892                 break
6893             elif info.type == libvirt.VIR_DOMAIN_JOB_CANCELLED:
6894                 # Migration was stopped by admin
6895                 LOG.warning("Migration operation was cancelled",
6896                             instance=instance)
6897                 libvirt_migrate.run_recover_tasks(self._host, guest, instance,
6898                                                   on_migration_failure)
6899                 recover_method(context, instance, dest, migrate_data,
6900                                migration_status='cancelled')
6901                 break
6902             else:
6903                 LOG.warning("Unexpected migration job type: %d",
6904                             info.type, instance=instance)
6905 
6906             time.sleep(0.5)
6907         self._clear_empty_migration(instance)
6908 
6909     def _clear_empty_migration(self, instance):
6910         try:
6911             del self.active_migrations[instance.uuid]
6912         except KeyError:
6913             LOG.warning("There are no records in active migrations "
6914                         "for instance", instance=instance)
6915 
6916     def _live_migration(self, context, instance, dest, post_method,
6917                         recover_method, block_migration,
6918                         migrate_data):
6919         """Do live migration.
6920 
6921         :param context: security context
6922         :param instance:
6923             nova.db.sqlalchemy.models.Instance object
6924             instance object that is migrated.
6925         :param dest: destination host
6926         :param post_method:
6927             post operation method.
6928             expected nova.compute.manager._post_live_migration.
6929         :param recover_method:
6930             recovery method when any exception occurs.
6931             expected nova.compute.manager._rollback_live_migration.
6932         :param block_migration: if true, do block migration.
6933         :param migrate_data: a LibvirtLiveMigrateData object
6934 
6935         This fires off a new thread to run the blocking migration
6936         operation, and then this thread monitors the progress of
6937         migration and controls its operation
6938         """
6939 
6940         guest = self._host.get_guest(instance)
6941 
6942         disk_paths = []
6943         device_names = []
6944         if (migrate_data.block_migration and
6945                 CONF.libvirt.virt_type != "parallels"):
6946             disk_paths, device_names = self._live_migration_copy_disk_paths(
6947                 context, instance, guest)
6948 
6949         opthread = utils.spawn(self._live_migration_operation,
6950                                      context, instance, dest,
6951                                      block_migration,
6952                                      migrate_data, guest,
6953                                      device_names)
6954 
6955         finish_event = eventlet.event.Event()
6956         self.active_migrations[instance.uuid] = deque()
6957 
6958         def thread_finished(thread, event):
6959             LOG.debug("Migration operation thread notification",
6960                       instance=instance)
6961             event.send()
6962         opthread.link(thread_finished, finish_event)
6963 
6964         # Let eventlet schedule the new thread right away
6965         time.sleep(0)
6966 
6967         try:
6968             LOG.debug("Starting monitoring of live migration",
6969                       instance=instance)
6970             self._live_migration_monitor(context, instance, guest, dest,
6971                                          post_method, recover_method,
6972                                          block_migration, migrate_data,
6973                                          finish_event, disk_paths)
6974         except Exception as ex:
6975             LOG.warning("Error monitoring migration: %(ex)s",
6976                         {"ex": ex}, instance=instance, exc_info=True)
6977             raise
6978         finally:
6979             LOG.debug("Live migration monitoring is all done",
6980                       instance=instance)
6981 
6982     def _is_post_copy_enabled(self, migration_flags):
6983         if self._is_post_copy_available():
6984             if (migration_flags & libvirt.VIR_MIGRATE_POSTCOPY) != 0:
6985                 return True
6986         return False
6987 
6988     def live_migration_force_complete(self, instance):
6989         try:
6990             self.active_migrations[instance.uuid].append('force-complete')
6991         except KeyError:
6992             raise exception.NoActiveMigrationForInstance(
6993                 instance_id=instance.uuid)
6994 
6995     def _try_fetch_image(self, context, path, image_id, instance,
6996                          fallback_from_host=None):
6997         try:
6998             libvirt_utils.fetch_image(context, path, image_id)
6999         except exception.ImageNotFound:
7000             if not fallback_from_host:
7001                 raise
7002             LOG.debug("Image %(image_id)s doesn't exist anymore on "
7003                       "image service, attempting to copy image "
7004                       "from %(host)s",
7005                       {'image_id': image_id, 'host': fallback_from_host})
7006             libvirt_utils.copy_image(src=path, dest=path,
7007                                      host=fallback_from_host,
7008                                      receive=True)
7009 
7010     def _fetch_instance_kernel_ramdisk(self, context, instance,
7011                                        fallback_from_host=None):
7012         """Download kernel and ramdisk for instance in instance directory."""
7013         instance_dir = libvirt_utils.get_instance_path(instance)
7014         if instance.kernel_id:
7015             kernel_path = os.path.join(instance_dir, 'kernel')
7016             # NOTE(dsanders): only fetch image if it's not available at
7017             # kernel_path. This also avoids ImageNotFound exception if
7018             # the image has been deleted from glance
7019             if not os.path.exists(kernel_path):
7020                 self._try_fetch_image(context,
7021                                       kernel_path,
7022                                       instance.kernel_id,
7023                                       instance, fallback_from_host)
7024             if instance.ramdisk_id:
7025                 ramdisk_path = os.path.join(instance_dir, 'ramdisk')
7026                 # NOTE(dsanders): only fetch image if it's not available at
7027                 # ramdisk_path. This also avoids ImageNotFound exception if
7028                 # the image has been deleted from glance
7029                 if not os.path.exists(ramdisk_path):
7030                     self._try_fetch_image(context,
7031                                           ramdisk_path,
7032                                           instance.ramdisk_id,
7033                                           instance, fallback_from_host)
7034 
7035     def rollback_live_migration_at_destination(self, context, instance,
7036                                                network_info,
7037                                                block_device_info,
7038                                                destroy_disks=True,
7039                                                migrate_data=None):
7040         """Clean up destination node after a failed live migration."""
7041         try:
7042             self.destroy(context, instance, network_info, block_device_info,
7043                          destroy_disks)
7044         finally:
7045             # NOTE(gcb): Failed block live migration may leave instance
7046             # directory at destination node, ensure it is always deleted.
7047             is_shared_instance_path = True
7048             if migrate_data:
7049                 is_shared_instance_path = migrate_data.is_shared_instance_path
7050                 if (migrate_data.obj_attr_is_set("serial_listen_ports")
7051                     and migrate_data.serial_listen_ports):
7052                     # Releases serial ports reserved.
7053                     for port in migrate_data.serial_listen_ports:
7054                         serial_console.release_port(
7055                             host=migrate_data.serial_listen_addr, port=port)
7056 
7057             if not is_shared_instance_path:
7058                 instance_dir = libvirt_utils.get_instance_path_at_destination(
7059                     instance, migrate_data)
7060                 if os.path.exists(instance_dir):
7061                     shutil.rmtree(instance_dir)
7062 
7063     def pre_live_migration(self, context, instance, block_device_info,
7064                            network_info, disk_info, migrate_data):
7065         """Preparation live migration."""
7066         if disk_info is not None:
7067             disk_info = jsonutils.loads(disk_info)
7068 
7069         LOG.debug('migrate_data in pre_live_migration: %s', migrate_data,
7070                   instance=instance)
7071         is_shared_block_storage = migrate_data.is_shared_block_storage
7072         is_shared_instance_path = migrate_data.is_shared_instance_path
7073         is_block_migration = migrate_data.block_migration
7074 
7075         if not is_shared_instance_path:
7076             instance_dir = libvirt_utils.get_instance_path_at_destination(
7077                             instance, migrate_data)
7078 
7079             if os.path.exists(instance_dir):
7080                 raise exception.DestinationDiskExists(path=instance_dir)
7081 
7082             LOG.debug('Creating instance directory: %s', instance_dir,
7083                       instance=instance)
7084             os.mkdir(instance_dir)
7085 
7086             # Recreate the disk.info file and in doing so stop the
7087             # imagebackend from recreating it incorrectly by inspecting the
7088             # contents of each file when using the Raw backend.
7089             if disk_info:
7090                 image_disk_info = {}
7091                 for info in disk_info:
7092                     image_file = os.path.basename(info['path'])
7093                     image_path = os.path.join(instance_dir, image_file)
7094                     image_disk_info[image_path] = info['type']
7095 
7096                 LOG.debug('Creating disk.info with the contents: %s',
7097                           image_disk_info, instance=instance)
7098 
7099                 image_disk_info_path = os.path.join(instance_dir,
7100                                                     'disk.info')
7101                 libvirt_utils.write_to_file(image_disk_info_path,
7102                                             jsonutils.dumps(image_disk_info))
7103 
7104             if not is_shared_block_storage:
7105                 # Ensure images and backing files are present.
7106                 LOG.debug('Checking to make sure images and backing files are '
7107                           'present before live migration.', instance=instance)
7108                 self._create_images_and_backing(
7109                     context, instance, instance_dir, disk_info,
7110                     fallback_from_host=instance.host)
7111                 if (configdrive.required_by(instance) and
7112                         CONF.config_drive_format == 'iso9660'):
7113                     # NOTE(pkoniszewski): Due to a bug in libvirt iso config
7114                     # drive needs to be copied to destination prior to
7115                     # migration when instance path is not shared and block
7116                     # storage is not shared. Files that are already present
7117                     # on destination are excluded from a list of files that
7118                     # need to be copied to destination. If we don't do that
7119                     # live migration will fail on copying iso config drive to
7120                     # destination and writing to read-only device.
7121                     # Please see bug/1246201 for more details.
7122                     src = "%s:%s/disk.config" % (instance.host, instance_dir)
7123                     self._remotefs.copy_file(src, instance_dir)
7124 
7125             if not is_block_migration:
7126                 # NOTE(angdraug): when block storage is shared between source
7127                 # and destination and instance path isn't (e.g. volume backed
7128                 # or rbd backed instance), instance path on destination has to
7129                 # be prepared
7130 
7131                 # Required by Quobyte CI
7132                 self._ensure_console_log_for_instance(instance)
7133 
7134                 # if image has kernel and ramdisk, just download
7135                 # following normal way.
7136                 self._fetch_instance_kernel_ramdisk(context, instance)
7137 
7138         # Establishing connection to volume server.
7139         block_device_mapping = driver.block_device_info_get_mapping(
7140             block_device_info)
7141 
7142         if len(block_device_mapping):
7143             LOG.debug('Connecting volumes before live migration.',
7144                       instance=instance)
7145 
7146         for bdm in block_device_mapping:
7147             connection_info = bdm['connection_info']
7148             self._connect_volume(connection_info, instance)
7149 
7150         # We call plug_vifs before the compute manager calls
7151         # ensure_filtering_rules_for_instance, to ensure bridge is set up
7152         # Retry operation is necessary because continuously request comes,
7153         # concurrent request occurs to iptables, then it complains.
7154         LOG.debug('Plugging VIFs before live migration.', instance=instance)
7155         max_retry = CONF.live_migration_retry_count
7156         for cnt in range(max_retry):
7157             try:
7158                 self.plug_vifs(instance, network_info)
7159                 break
7160             except processutils.ProcessExecutionError:
7161                 if cnt == max_retry - 1:
7162                     raise
7163                 else:
7164                     LOG.warning('plug_vifs() failed %(cnt)d. Retry up to '
7165                                 '%(max_retry)d.',
7166                                 {'cnt': cnt, 'max_retry': max_retry},
7167                                 instance=instance)
7168                     greenthread.sleep(1)
7169 
7170         # Store server_listen and latest disk device info
7171         if not migrate_data:
7172             migrate_data = objects.LibvirtLiveMigrateData(bdms=[])
7173         else:
7174             migrate_data.bdms = []
7175         # Store live_migration_inbound_addr
7176         migrate_data.target_connect_addr = \
7177             CONF.libvirt.live_migration_inbound_addr
7178         migrate_data.supported_perf_events = self._supported_perf_events
7179 
7180         migrate_data.serial_listen_ports = []
7181         if CONF.serial_console.enabled:
7182             num_ports = hardware.get_number_of_serial_ports(
7183                 instance.flavor, instance.image_meta)
7184             for port in six.moves.range(num_ports):
7185                 migrate_data.serial_listen_ports.append(
7186                     serial_console.acquire_port(
7187                         migrate_data.serial_listen_addr))
7188 
7189         for vol in block_device_mapping:
7190             connection_info = vol['connection_info']
7191             if connection_info.get('serial'):
7192                 disk_info = blockinfo.get_info_from_bdm(
7193                     instance, CONF.libvirt.virt_type,
7194                     instance.image_meta, vol)
7195 
7196                 bdmi = objects.LibvirtLiveMigrateBDMInfo()
7197                 bdmi.serial = connection_info['serial']
7198                 bdmi.connection_info = connection_info
7199                 bdmi.bus = disk_info['bus']
7200                 bdmi.dev = disk_info['dev']
7201                 bdmi.type = disk_info['type']
7202                 bdmi.format = disk_info.get('format')
7203                 bdmi.boot_index = disk_info.get('boot_index')
7204                 migrate_data.bdms.append(bdmi)
7205 
7206         return migrate_data
7207 
7208     def _try_fetch_image_cache(self, image, fetch_func, context, filename,
7209                                image_id, instance, size,
7210                                fallback_from_host=None):
7211         try:
7212             image.cache(fetch_func=fetch_func,
7213                         context=context,
7214                         filename=filename,
7215                         image_id=image_id,
7216                         size=size)
7217         except exception.ImageNotFound:
7218             if not fallback_from_host:
7219                 raise
7220             LOG.debug("Image %(image_id)s doesn't exist anymore "
7221                       "on image service, attempting to copy "
7222                       "image from %(host)s",
7223                       {'image_id': image_id, 'host': fallback_from_host},
7224                       instance=instance)
7225 
7226             def copy_from_host(target):
7227                 libvirt_utils.copy_image(src=target,
7228                                          dest=target,
7229                                          host=fallback_from_host,
7230                                          receive=True)
7231             image.cache(fetch_func=copy_from_host,
7232                         filename=filename)
7233 
7234     def _create_images_and_backing(self, context, instance, instance_dir,
7235                                    disk_info, fallback_from_host=None):
7236         """:param context: security context
7237            :param instance:
7238                nova.db.sqlalchemy.models.Instance object
7239                instance object that is migrated.
7240            :param instance_dir:
7241                instance path to use, calculated externally to handle block
7242                migrating an instance with an old style instance path
7243            :param disk_info:
7244                disk info specified in _get_instance_disk_info_from_config
7245                (list of dicts)
7246            :param fallback_from_host:
7247                host where we can retrieve images if the glance images are
7248                not available.
7249 
7250         """
7251 
7252         # Virtuozzo containers don't use backing file
7253         if (CONF.libvirt.virt_type == "parallels" and
7254                 instance.vm_mode == fields.VMMode.EXE):
7255             return
7256 
7257         if not disk_info:
7258             disk_info = []
7259 
7260         for info in disk_info:
7261             base = os.path.basename(info['path'])
7262             # Get image type and create empty disk image, and
7263             # create backing file in case of qcow2.
7264             instance_disk = os.path.join(instance_dir, base)
7265             if not info['backing_file'] and not os.path.exists(instance_disk):
7266                 libvirt_utils.create_image(info['type'], instance_disk,
7267                                            info['virt_disk_size'])
7268             elif info['backing_file']:
7269                 # Creating backing file follows same way as spawning instances.
7270                 cache_name = os.path.basename(info['backing_file'])
7271 
7272                 disk = self.image_backend.by_name(instance, instance_disk,
7273                                                   CONF.libvirt.images_type)
7274                 if cache_name.startswith('ephemeral'):
7275                     # The argument 'size' is used by image.cache to
7276                     # validate disk size retrieved from cache against
7277                     # the instance disk size (should always return OK)
7278                     # and ephemeral_size is used by _create_ephemeral
7279                     # to build the image if the disk is not already
7280                     # cached.
7281                     disk.cache(
7282                         fetch_func=self._create_ephemeral,
7283                         fs_label=cache_name,
7284                         os_type=instance.os_type,
7285                         filename=cache_name,
7286                         size=info['virt_disk_size'],
7287                         ephemeral_size=info['virt_disk_size'] / units.Gi)
7288                 elif cache_name.startswith('swap'):
7289                     inst_type = instance.get_flavor()
7290                     swap_mb = inst_type.swap
7291                     disk.cache(fetch_func=self._create_swap,
7292                                 filename="swap_%s" % swap_mb,
7293                                 size=swap_mb * units.Mi,
7294                                 swap_mb=swap_mb)
7295                 else:
7296                     self._try_fetch_image_cache(disk,
7297                                                 libvirt_utils.fetch_image,
7298                                                 context, cache_name,
7299                                                 instance.image_ref,
7300                                                 instance,
7301                                                 info['virt_disk_size'],
7302                                                 fallback_from_host)
7303 
7304         # if disk has kernel and ramdisk, just download
7305         # following normal way.
7306         self._fetch_instance_kernel_ramdisk(
7307             context, instance, fallback_from_host=fallback_from_host)
7308 
7309     def post_live_migration(self, context, instance, block_device_info,
7310                             migrate_data=None):
7311         # Disconnect from volume server
7312         block_device_mapping = driver.block_device_info_get_mapping(
7313                 block_device_info)
7314         volume_api = self._volume_api
7315         for vol in block_device_mapping:
7316             volume_id = vol['connection_info']['serial']
7317             if vol['attachment_id'] is None:
7318                 # Cinder v2 api flow: Retrieve connection info from Cinder's
7319                 # initialize_connection API. The info returned will be
7320                 # accurate for the source server.
7321                 connector = self.get_volume_connector(instance)
7322                 connection_info = volume_api.initialize_connection(
7323                     context, volume_id, connector)
7324             else:
7325                 # cinder v3.44 api flow: Retrieve the connection_info for
7326                 # the old attachment from cinder.
7327                 old_attachment_id = \
7328                     migrate_data.old_vol_attachment_ids[volume_id]
7329                 old_attachment = volume_api.attachment_get(
7330                     context, old_attachment_id)
7331                 connection_info = old_attachment['connection_info']
7332 
7333             # TODO(leeantho) The following multipath_id logic is temporary
7334             # and will be removed in the future once os-brick is updated
7335             # to handle multipath for drivers in a more efficient way.
7336             # For now this logic is needed to ensure the connection info
7337             # data is correct.
7338 
7339             # Pull out multipath_id from the bdm information. The
7340             # multipath_id can be placed into the connection info
7341             # because it is based off of the volume and will be the
7342             # same on the source and destination hosts.
7343             if 'multipath_id' in vol['connection_info']['data']:
7344                 multipath_id = vol['connection_info']['data']['multipath_id']
7345                 connection_info['data']['multipath_id'] = multipath_id
7346 
7347             self._disconnect_volume(connection_info, instance)
7348 
7349     def post_live_migration_at_source(self, context, instance, network_info):
7350         """Unplug VIFs from networks at source.
7351 
7352         :param context: security context
7353         :param instance: instance object reference
7354         :param network_info: instance network information
7355         """
7356         self.unplug_vifs(instance, network_info)
7357 
7358     def post_live_migration_at_destination(self, context,
7359                                            instance,
7360                                            network_info,
7361                                            block_migration=False,
7362                                            block_device_info=None):
7363         """Post operation of live migration at destination host.
7364 
7365         :param context: security context
7366         :param instance:
7367             nova.db.sqlalchemy.models.Instance object
7368             instance object that is migrated.
7369         :param network_info: instance network information
7370         :param block_migration: if true, post operation of block_migration.
7371         """
7372         # The source node set the VIR_MIGRATE_PERSIST_DEST flag when live
7373         # migrating so the guest xml should already be persisted on the
7374         # destination host, so just perform a sanity check to make sure it
7375         # made it as expected.
7376         self._host.get_guest(instance)
7377 
7378     def _get_instance_disk_info_from_config(self, guest_config,
7379                                             block_device_info):
7380         """Get the non-volume disk information from the domain xml
7381 
7382         :param LibvirtConfigGuest guest_config: the libvirt domain config
7383                                                 for the instance
7384         :param dict block_device_info: block device info for BDMs
7385         :returns disk_info: list of dicts with keys:
7386 
7387           * 'type': the disk type (str)
7388           * 'path': the disk path (str)
7389           * 'virt_disk_size': the virtual disk size (int)
7390           * 'backing_file': backing file of a disk image (str)
7391           * 'disk_size': physical disk size (int)
7392           * 'over_committed_disk_size': virt_disk_size - disk_size or 0
7393         """
7394         block_device_mapping = driver.block_device_info_get_mapping(
7395             block_device_info)
7396 
7397         volume_devices = set()
7398         for vol in block_device_mapping:
7399             disk_dev = vol['mount_device'].rpartition("/")[2]
7400             volume_devices.add(disk_dev)
7401 
7402         disk_info = []
7403 
7404         if (guest_config.virt_type == 'parallels' and
7405                 guest_config.os_type == fields.VMMode.EXE):
7406             node_type = 'filesystem'
7407         else:
7408             node_type = 'disk'
7409 
7410         for device in guest_config.devices:
7411             if device.root_name != node_type:
7412                 continue
7413             disk_type = device.source_type
7414             if device.root_name == 'filesystem':
7415                 target = device.target_dir
7416                 if device.source_type == 'file':
7417                     path = device.source_file
7418                 elif device.source_type == 'block':
7419                     path = device.source_dev
7420                 else:
7421                     path = None
7422             else:
7423                 target = device.target_dev
7424                 path = device.source_path
7425 
7426             if not path:
7427                 LOG.debug('skipping disk for %s as it does not have a path',
7428                           guest_config.name)
7429                 continue
7430 
7431             if disk_type not in ['file', 'block']:
7432                 LOG.debug('skipping disk because it looks like a volume', path)
7433                 continue
7434 
7435             if target in volume_devices:
7436                 LOG.debug('skipping disk %(path)s (%(target)s) as it is a '
7437                           'volume', {'path': path, 'target': target})
7438                 continue
7439 
7440             if device.root_name == 'filesystem':
7441                 driver_type = device.driver_type
7442             else:
7443                 driver_type = device.driver_format
7444             # get the real disk size or
7445             # raise a localized error if image is unavailable
7446             if disk_type == 'file':
7447                 if driver_type == 'ploop':
7448                     dk_size = 0
7449                     for dirpath, dirnames, filenames in os.walk(path):
7450                         for f in filenames:
7451                             fp = os.path.join(dirpath, f)
7452                             dk_size += os.path.getsize(fp)
7453                 else:
7454                     dk_size = int(os.path.getsize(path))
7455             elif disk_type == 'block' and block_device_info:
7456                 dk_size = lvm.get_volume_size(path)
7457             else:
7458                 LOG.debug('skipping disk %(path)s (%(target)s) - unable to '
7459                           'determine if volume',
7460                           {'path': path, 'target': target})
7461                 continue
7462 
7463             if driver_type in ("qcow2", "ploop"):
7464                 backing_file = libvirt_utils.get_disk_backing_file(path)
7465                 virt_size = disk_api.get_disk_size(path)
7466                 over_commit_size = int(virt_size) - dk_size
7467             else:
7468                 backing_file = ""
7469                 virt_size = dk_size
7470                 over_commit_size = 0
7471 
7472             disk_info.append({'type': driver_type,
7473                               'path': path,
7474                               'virt_disk_size': virt_size,
7475                               'backing_file': backing_file,
7476                               'disk_size': dk_size,
7477                               'over_committed_disk_size': over_commit_size})
7478         return disk_info
7479 
7480     def _get_instance_disk_info(self, instance, block_device_info):
7481         try:
7482             guest = self._host.get_guest(instance)
7483             config = guest.get_config()
7484         except libvirt.libvirtError as ex:
7485             error_code = ex.get_error_code()
7486             LOG.warning('Error from libvirt while getting description of '
7487                         '%(instance_name)s: [Error Code %(error_code)s] '
7488                         '%(ex)s',
7489                         {'instance_name': instance.name,
7490                          'error_code': error_code,
7491                          'ex': encodeutils.exception_to_unicode(ex)},
7492                         instance=instance)
7493             raise exception.InstanceNotFound(instance_id=instance.uuid)
7494 
7495         return self._get_instance_disk_info_from_config(config,
7496                                                         block_device_info)
7497 
7498     def get_instance_disk_info(self, instance,
7499                                block_device_info=None):
7500         return jsonutils.dumps(
7501             self._get_instance_disk_info(instance, block_device_info))
7502 
7503     def _get_disk_over_committed_size_total(self):
7504         """Return total over committed disk size for all instances."""
7505         # Disk size that all instance uses : virtual_size - disk_size
7506         disk_over_committed_size = 0
7507         instance_domains = self._host.list_instance_domains(only_running=False)
7508         if not instance_domains:
7509             return disk_over_committed_size
7510 
7511         # Get all instance uuids
7512         instance_uuids = [dom.UUIDString() for dom in instance_domains]
7513         ctx = nova_context.get_admin_context()
7514         # Get instance object list by uuid filter
7515         filters = {'uuid': instance_uuids}
7516         # NOTE(ankit): objects.InstanceList.get_by_filters method is
7517         # getting called twice one is here and another in the
7518         # _update_available_resource method of resource_tracker. Since
7519         # _update_available_resource method is synchronized, there is a
7520         # possibility the instances list retrieved here to calculate
7521         # disk_over_committed_size would differ to the list you would get
7522         # in _update_available_resource method for calculating usages based
7523         # on instance utilization.
7524         local_instance_list = objects.InstanceList.get_by_filters(
7525             ctx, filters, use_slave=True)
7526         # Convert instance list to dictionary with instance uuid as key.
7527         local_instances = {inst.uuid: inst for inst in local_instance_list}
7528 
7529         # Get bdms by instance uuids
7530         bdms = objects.BlockDeviceMappingList.bdms_by_instance_uuid(
7531             ctx, instance_uuids)
7532 
7533         for dom in instance_domains:
7534             try:
7535                 guest = libvirt_guest.Guest(dom)
7536                 config = guest.get_config()
7537 
7538                 block_device_info = None
7539                 if guest.uuid in local_instances \
7540                         and (bdms and guest.uuid in bdms):
7541                     # Get block device info for instance
7542                     block_device_info = driver.get_block_device_info(
7543                         local_instances[guest.uuid], bdms[guest.uuid])
7544 
7545                 disk_infos = self._get_instance_disk_info_from_config(
7546                     config, block_device_info)
7547                 if not disk_infos:
7548                     continue
7549 
7550                 for info in disk_infos:
7551                     disk_over_committed_size += int(
7552                         info['over_committed_disk_size'])
7553             except libvirt.libvirtError as ex:
7554                 error_code = ex.get_error_code()
7555                 LOG.warning(
7556                     'Error from libvirt while getting description of '
7557                     '%(instance_name)s: [Error Code %(error_code)s] %(ex)s',
7558                     {'instance_name': guest.name,
7559                      'error_code': error_code,
7560                      'ex': encodeutils.exception_to_unicode(ex)})
7561             except OSError as e:
7562                 if e.errno in (errno.ENOENT, errno.ESTALE):
7563                     LOG.warning('Periodic task is updating the host stat, '
7564                                 'it is trying to get disk %(i_name)s, '
7565                                 'but disk file was removed by concurrent '
7566                                 'operations such as resize.',
7567                                 {'i_name': guest.name})
7568                 elif e.errno == errno.EACCES:
7569                     LOG.warning('Periodic task is updating the host stat, '
7570                                 'it is trying to get disk %(i_name)s, '
7571                                 'but access is denied. It is most likely '
7572                                 'due to a VM that exists on the compute '
7573                                 'node but is not managed by Nova.',
7574                                 {'i_name': guest.name})
7575                 else:
7576                     raise
7577             except exception.VolumeBDMPathNotFound as e:
7578                 LOG.warning('Periodic task is updating the host stats, '
7579                             'it is trying to get disk info for %(i_name)s, '
7580                             'but the backing volume block device was removed '
7581                             'by concurrent operations such as resize. '
7582                             'Error: %(error)s',
7583                             {'i_name': guest.name, 'error': e})
7584             # NOTE(gtt116): give other tasks a chance.
7585             greenthread.sleep(0)
7586         return disk_over_committed_size
7587 
7588     def unfilter_instance(self, instance, network_info):
7589         """See comments of same method in firewall_driver."""
7590         self.firewall_driver.unfilter_instance(instance,
7591                                                network_info=network_info)
7592 
7593     def get_available_nodes(self, refresh=False):
7594         return [self._host.get_hostname()]
7595 
7596     def get_host_cpu_stats(self):
7597         """Return the current CPU state of the host."""
7598         return self._host.get_cpu_stats()
7599 
7600     def get_host_uptime(self):
7601         """Returns the result of calling "uptime"."""
7602         out, err = utils.execute('env', 'LANG=C', 'uptime')
7603         return out
7604 
7605     def manage_image_cache(self, context, all_instances):
7606         """Manage the local cache of images."""
7607         self.image_cache_manager.update(context, all_instances)
7608 
7609     def _cleanup_remote_migration(self, dest, inst_base, inst_base_resize,
7610                                   shared_storage=False):
7611         """Used only for cleanup in case migrate_disk_and_power_off fails."""
7612         try:
7613             if os.path.exists(inst_base_resize):
7614                 utils.execute('rm', '-rf', inst_base)
7615                 utils.execute('mv', inst_base_resize, inst_base)
7616                 if not shared_storage:
7617                     self._remotefs.remove_dir(dest, inst_base)
7618         except Exception:
7619             pass
7620 
7621     def _is_storage_shared_with(self, dest, inst_base):
7622         # NOTE (rmk): There are two methods of determining whether we are
7623         #             on the same filesystem: the source and dest IP are the
7624         #             same, or we create a file on the dest system via SSH
7625         #             and check whether the source system can also see it.
7626         # NOTE (drwahl): Actually, there is a 3rd way: if images_type is rbd,
7627         #                it will always be shared storage
7628         if CONF.libvirt.images_type == 'rbd':
7629             return True
7630         shared_storage = (dest == self.get_host_ip_addr())
7631         if not shared_storage:
7632             tmp_file = uuidutils.generate_uuid(dashed=False) + '.tmp'
7633             tmp_path = os.path.join(inst_base, tmp_file)
7634 
7635             try:
7636                 self._remotefs.create_file(dest, tmp_path)
7637                 if os.path.exists(tmp_path):
7638                     shared_storage = True
7639                     os.unlink(tmp_path)
7640                 else:
7641                     self._remotefs.remove_file(dest, tmp_path)
7642             except Exception:
7643                 pass
7644         return shared_storage
7645 
7646     def migrate_disk_and_power_off(self, context, instance, dest,
7647                                    flavor, network_info,
7648                                    block_device_info=None,
7649                                    timeout=0, retry_interval=0):
7650         LOG.debug("Starting migrate_disk_and_power_off",
7651                    instance=instance)
7652 
7653         ephemerals = driver.block_device_info_get_ephemerals(block_device_info)
7654 
7655         # get_bdm_ephemeral_disk_size() will return 0 if the new
7656         # instance's requested block device mapping contain no
7657         # ephemeral devices. However, we still want to check if
7658         # the original instance's ephemeral_gb property was set and
7659         # ensure that the new requested flavor ephemeral size is greater
7660         eph_size = (block_device.get_bdm_ephemeral_disk_size(ephemerals) or
7661                     instance.flavor.ephemeral_gb)
7662 
7663         # Checks if the migration needs a disk resize down.
7664         root_down = flavor.root_gb < instance.flavor.root_gb
7665         ephemeral_down = flavor.ephemeral_gb < eph_size
7666         booted_from_volume = self._is_booted_from_volume(block_device_info)
7667 
7668         if (root_down and not booted_from_volume) or ephemeral_down:
7669             reason = _("Unable to resize disk down.")
7670             raise exception.InstanceFaultRollback(
7671                 exception.ResizeError(reason=reason))
7672 
7673         # NOTE(dgenin): Migration is not implemented for LVM backed instances.
7674         if CONF.libvirt.images_type == 'lvm' and not booted_from_volume:
7675             reason = _("Migration is not supported for LVM backed instances")
7676             raise exception.InstanceFaultRollback(
7677                 exception.MigrationPreCheckError(reason=reason))
7678 
7679         # copy disks to destination
7680         # rename instance dir to +_resize at first for using
7681         # shared storage for instance dir (eg. NFS).
7682         inst_base = libvirt_utils.get_instance_path(instance)
7683         inst_base_resize = inst_base + "_resize"
7684         shared_storage = self._is_storage_shared_with(dest, inst_base)
7685 
7686         # try to create the directory on the remote compute node
7687         # if this fails we pass the exception up the stack so we can catch
7688         # failures here earlier
7689         if not shared_storage:
7690             try:
7691                 self._remotefs.create_dir(dest, inst_base)
7692             except processutils.ProcessExecutionError as e:
7693                 reason = _("not able to execute ssh command: %s") % e
7694                 raise exception.InstanceFaultRollback(
7695                     exception.ResizeError(reason=reason))
7696 
7697         self.power_off(instance, timeout, retry_interval)
7698 
7699         block_device_mapping = driver.block_device_info_get_mapping(
7700             block_device_info)
7701         for vol in block_device_mapping:
7702             connection_info = vol['connection_info']
7703             self._disconnect_volume(connection_info, instance)
7704 
7705         disk_info = self._get_instance_disk_info(instance, block_device_info)
7706 
7707         try:
7708             utils.execute('mv', inst_base, inst_base_resize)
7709             # if we are migrating the instance with shared storage then
7710             # create the directory.  If it is a remote node the directory
7711             # has already been created
7712             if shared_storage:
7713                 dest = None
7714                 fileutils.ensure_tree(inst_base)
7715 
7716             on_execute = lambda process: \
7717                 self.job_tracker.add_job(instance, process.pid)
7718             on_completion = lambda process: \
7719                 self.job_tracker.remove_job(instance, process.pid)
7720 
7721             for info in disk_info:
7722                 # assume inst_base == dirname(info['path'])
7723                 img_path = info['path']
7724                 fname = os.path.basename(img_path)
7725                 from_path = os.path.join(inst_base_resize, fname)
7726 
7727                 # We will not copy over the swap disk here, and rely on
7728                 # finish_migration to re-create it for us. This is ok because
7729                 # the OS is shut down, and as recreating a swap disk is very
7730                 # cheap it is more efficient than copying either locally or
7731                 # over the network. This also means we don't have to resize it.
7732                 if fname == 'disk.swap':
7733                     continue
7734 
7735                 compression = info['type'] not in NO_COMPRESSION_TYPES
7736                 libvirt_utils.copy_image(from_path, img_path, host=dest,
7737                                          on_execute=on_execute,
7738                                          on_completion=on_completion,
7739                                          compression=compression)
7740 
7741             # Ensure disk.info is written to the new path to avoid disks being
7742             # reinspected and potentially changing format.
7743             src_disk_info_path = os.path.join(inst_base_resize, 'disk.info')
7744             if os.path.exists(src_disk_info_path):
7745                 dst_disk_info_path = os.path.join(inst_base, 'disk.info')
7746                 libvirt_utils.copy_image(src_disk_info_path,
7747                                          dst_disk_info_path,
7748                                          host=dest, on_execute=on_execute,
7749                                          on_completion=on_completion)
7750         except Exception:
7751             with excutils.save_and_reraise_exception():
7752                 self._cleanup_remote_migration(dest, inst_base,
7753                                                inst_base_resize,
7754                                                shared_storage)
7755 
7756         return jsonutils.dumps(disk_info)
7757 
7758     def _wait_for_running(self, instance):
7759         state = self.get_info(instance).state
7760 
7761         if state == power_state.RUNNING:
7762             LOG.info("Instance running successfully.", instance=instance)
7763             raise loopingcall.LoopingCallDone()
7764 
7765     @staticmethod
7766     def _disk_raw_to_qcow2(path):
7767         """Converts a raw disk to qcow2."""
7768         path_qcow = path + '_qcow'
7769         utils.execute('qemu-img', 'convert', '-f', 'raw',
7770                       '-O', 'qcow2', path, path_qcow)
7771         utils.execute('mv', path_qcow, path)
7772 
7773     @staticmethod
7774     def _disk_qcow2_to_raw(path):
7775         """Converts a qcow2 disk to raw."""
7776         path_raw = path + '_raw'
7777         utils.execute('qemu-img', 'convert', '-f', 'qcow2',
7778                       '-O', 'raw', path, path_raw)
7779         utils.execute('mv', path_raw, path)
7780 
7781     def finish_migration(self, context, migration, instance, disk_info,
7782                          network_info, image_meta, resize_instance,
7783                          block_device_info=None, power_on=True):
7784         LOG.debug("Starting finish_migration", instance=instance)
7785 
7786         block_disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
7787                                                   instance,
7788                                                   image_meta,
7789                                                   block_device_info)
7790         # assume _create_image does nothing if a target file exists.
7791         # NOTE: This has the intended side-effect of fetching a missing
7792         # backing file.
7793         self._create_image(context, instance, block_disk_info['mapping'],
7794                            block_device_info=block_device_info,
7795                            ignore_bdi_for_swap=True,
7796                            fallback_from_host=migration.source_compute)
7797 
7798         # Required by Quobyte CI
7799         self._ensure_console_log_for_instance(instance)
7800 
7801         gen_confdrive = functools.partial(
7802             self._create_configdrive, context, instance,
7803             InjectionInfo(admin_pass=None, network_info=network_info,
7804                           files=None))
7805 
7806         # Convert raw disks to qcow2 if migrating to host which uses
7807         # qcow2 from host which uses raw.
7808         disk_info = jsonutils.loads(disk_info)
7809         for info in disk_info:
7810             path = info['path']
7811             disk_name = os.path.basename(path)
7812 
7813             # NOTE(mdbooth): The code below looks wrong, but is actually
7814             # required to prevent a security hole when migrating from a host
7815             # with use_cow_images=False to one with use_cow_images=True.
7816             # Imagebackend uses use_cow_images to select between the
7817             # atrociously-named-Raw and Qcow2 backends. The Qcow2 backend
7818             # writes to disk.info, but does not read it as it assumes qcow2.
7819             # Therefore if we don't convert raw to qcow2 here, a raw disk will
7820             # be incorrectly assumed to be qcow2, which is a severe security
7821             # flaw. The reverse is not true, because the atrociously-named-Raw
7822             # backend supports both qcow2 and raw disks, and will choose
7823             # appropriately between them as long as disk.info exists and is
7824             # correctly populated, which it is because Qcow2 writes to
7825             # disk.info.
7826             #
7827             # In general, we do not yet support format conversion during
7828             # migration. For example:
7829             #   * Converting from use_cow_images=True to use_cow_images=False
7830             #     isn't handled. This isn't a security bug, but is almost
7831             #     certainly buggy in other cases, as the 'Raw' backend doesn't
7832             #     expect a backing file.
7833             #   * Converting to/from lvm and rbd backends is not supported.
7834             #
7835             # This behaviour is inconsistent, and therefore undesirable for
7836             # users. It is tightly-coupled to implementation quirks of 2
7837             # out of 5 backends in imagebackend and defends against a severe
7838             # security flaw which is not at all obvious without deep analysis,
7839             # and is therefore undesirable to developers. We should aim to
7840             # remove it. This will not be possible, though, until we can
7841             # represent the storage layout of a specific instance
7842             # independent of the default configuration of the local compute
7843             # host.
7844 
7845             # Config disks are hard-coded to be raw even when
7846             # use_cow_images=True (see _get_disk_config_image_type),so don't
7847             # need to be converted.
7848             if (disk_name != 'disk.config' and
7849                         info['type'] == 'raw' and CONF.use_cow_images):
7850                 self._disk_raw_to_qcow2(info['path'])
7851 
7852         xml = self._get_guest_xml(context, instance, network_info,
7853                                   block_disk_info, image_meta,
7854                                   block_device_info=block_device_info)
7855         # NOTE(mriedem): vifs_already_plugged=True here, regardless of whether
7856         # or not we've migrated to another host, because we unplug VIFs locally
7857         # and the status change in the port might go undetected by the neutron
7858         # L2 agent (or neutron server) so neutron may not know that the VIF was
7859         # unplugged in the first place and never send an event.
7860         guest = self._create_domain_and_network(context, xml, instance,
7861                                         network_info,
7862                                         block_device_info=block_device_info,
7863                                         power_on=power_on,
7864                                         vifs_already_plugged=True,
7865                                         post_xml_callback=gen_confdrive)
7866         if power_on:
7867             timer = loopingcall.FixedIntervalLoopingCall(
7868                                                     self._wait_for_running,
7869                                                     instance)
7870             timer.start(interval=0.5).wait()
7871 
7872             # Sync guest time after migration.
7873             guest.sync_guest_time()
7874 
7875         LOG.debug("finish_migration finished successfully.", instance=instance)
7876 
7877     def _cleanup_failed_migration(self, inst_base):
7878         """Make sure that a failed migrate doesn't prevent us from rolling
7879         back in a revert.
7880         """
7881         try:
7882             shutil.rmtree(inst_base)
7883         except OSError as e:
7884             if e.errno != errno.ENOENT:
7885                 raise
7886 
7887     def finish_revert_migration(self, context, instance, network_info,
7888                                 block_device_info=None, power_on=True):
7889         LOG.debug("Starting finish_revert_migration",
7890                   instance=instance)
7891 
7892         inst_base = libvirt_utils.get_instance_path(instance)
7893         inst_base_resize = inst_base + "_resize"
7894 
7895         # NOTE(danms): if we're recovering from a failed migration,
7896         # make sure we don't have a left-over same-host base directory
7897         # that would conflict. Also, don't fail on the rename if the
7898         # failure happened early.
7899         if os.path.exists(inst_base_resize):
7900             self._cleanup_failed_migration(inst_base)
7901             utils.execute('mv', inst_base_resize, inst_base)
7902 
7903         root_disk = self.image_backend.by_name(instance, 'disk')
7904         # Once we rollback, the snapshot is no longer needed, so remove it
7905         # TODO(nic): Remove the try/except/finally in a future release
7906         # To avoid any upgrade issues surrounding instances being in pending
7907         # resize state when the software is updated, this portion of the
7908         # method logs exceptions rather than failing on them.  Once it can be
7909         # reasonably assumed that no such instances exist in the wild
7910         # anymore, the try/except/finally should be removed,
7911         # and ignore_errors should be set back to False (the default) so
7912         # that problems throw errors, like they should.
7913         if root_disk.exists():
7914             try:
7915                 root_disk.rollback_to_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME)
7916             except exception.SnapshotNotFound:
7917                 LOG.warning("Failed to rollback snapshot (%s)",
7918                             libvirt_utils.RESIZE_SNAPSHOT_NAME)
7919             finally:
7920                 root_disk.remove_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME,
7921                                       ignore_errors=True)
7922 
7923         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
7924                                             instance,
7925                                             instance.image_meta,
7926                                             block_device_info)
7927         xml = self._get_guest_xml(context, instance, network_info, disk_info,
7928                                   instance.image_meta,
7929                                   block_device_info=block_device_info)
7930         self._create_domain_and_network(context, xml, instance, network_info,
7931                                         block_device_info=block_device_info,
7932                                         power_on=power_on,
7933                                         vifs_already_plugged=True)
7934 
7935         if power_on:
7936             timer = loopingcall.FixedIntervalLoopingCall(
7937                                                     self._wait_for_running,
7938                                                     instance)
7939             timer.start(interval=0.5).wait()
7940 
7941         LOG.debug("finish_revert_migration finished successfully.",
7942                   instance=instance)
7943 
7944     def confirm_migration(self, context, migration, instance, network_info):
7945         """Confirms a resize, destroying the source VM."""
7946         self._cleanup_resize(context, instance, network_info)
7947 
7948     @staticmethod
7949     def _get_io_devices(xml_doc):
7950         """get the list of io devices from the xml document."""
7951         result = {"volumes": [], "ifaces": []}
7952         try:
7953             doc = etree.fromstring(xml_doc)
7954         except Exception:
7955             return result
7956         blocks = [('./devices/disk', 'volumes'),
7957             ('./devices/interface', 'ifaces')]
7958         for block, key in blocks:
7959             section = doc.findall(block)
7960             for node in section:
7961                 for child in node.getchildren():
7962                     if child.tag == 'target' and child.get('dev'):
7963                         result[key].append(child.get('dev'))
7964         return result
7965 
7966     def get_diagnostics(self, instance):
7967         guest = self._host.get_guest(instance)
7968 
7969         # TODO(sahid): We are converting all calls from a
7970         # virDomain object to use nova.virt.libvirt.Guest.
7971         # We should be able to remove domain at the end.
7972         domain = guest._domain
7973         output = {}
7974         # get cpu time, might launch an exception if the method
7975         # is not supported by the underlying hypervisor being
7976         # used by libvirt
7977         try:
7978             for vcpu in guest.get_vcpus_info():
7979                 output["cpu" + str(vcpu.id) + "_time"] = vcpu.time
7980         except libvirt.libvirtError:
7981             pass
7982         # get io status
7983         xml = guest.get_xml_desc()
7984         dom_io = LibvirtDriver._get_io_devices(xml)
7985         for guest_disk in dom_io["volumes"]:
7986             try:
7987                 # blockStats might launch an exception if the method
7988                 # is not supported by the underlying hypervisor being
7989                 # used by libvirt
7990                 stats = domain.blockStats(guest_disk)
7991                 output[guest_disk + "_read_req"] = stats[0]
7992                 output[guest_disk + "_read"] = stats[1]
7993                 output[guest_disk + "_write_req"] = stats[2]
7994                 output[guest_disk + "_write"] = stats[3]
7995                 output[guest_disk + "_errors"] = stats[4]
7996             except libvirt.libvirtError:
7997                 pass
7998         for interface in dom_io["ifaces"]:
7999             try:
8000                 # interfaceStats might launch an exception if the method
8001                 # is not supported by the underlying hypervisor being
8002                 # used by libvirt
8003                 stats = domain.interfaceStats(interface)
8004                 output[interface + "_rx"] = stats[0]
8005                 output[interface + "_rx_packets"] = stats[1]
8006                 output[interface + "_rx_errors"] = stats[2]
8007                 output[interface + "_rx_drop"] = stats[3]
8008                 output[interface + "_tx"] = stats[4]
8009                 output[interface + "_tx_packets"] = stats[5]
8010                 output[interface + "_tx_errors"] = stats[6]
8011                 output[interface + "_tx_drop"] = stats[7]
8012             except libvirt.libvirtError:
8013                 pass
8014         output["memory"] = domain.maxMemory()
8015         # memoryStats might launch an exception if the method
8016         # is not supported by the underlying hypervisor being
8017         # used by libvirt
8018         try:
8019             mem = domain.memoryStats()
8020             for key in mem.keys():
8021                 output["memory-" + key] = mem[key]
8022         except (libvirt.libvirtError, AttributeError):
8023             pass
8024         return output
8025 
8026     def get_instance_diagnostics(self, instance):
8027         guest = self._host.get_guest(instance)
8028 
8029         # TODO(sahid): We are converting all calls from a
8030         # virDomain object to use nova.virt.libvirt.Guest.
8031         # We should be able to remove domain at the end.
8032         domain = guest._domain
8033 
8034         xml = guest.get_xml_desc()
8035         xml_doc = etree.fromstring(xml)
8036 
8037         # TODO(sahid): Needs to use get_info but more changes have to
8038         # be done since a mapping STATE_MAP LIBVIRT_POWER_STATE is
8039         # needed.
8040         (state, max_mem, mem, num_cpu, cpu_time) = \
8041             guest._get_domain_info(self._host)
8042         config_drive = configdrive.required_by(instance)
8043         launched_at = timeutils.normalize_time(instance.launched_at)
8044         uptime = timeutils.delta_seconds(launched_at,
8045                                          timeutils.utcnow())
8046         diags = diagnostics_obj.Diagnostics(state=power_state.STATE_MAP[state],
8047                                         driver='libvirt',
8048                                         config_drive=config_drive,
8049                                         hypervisor=CONF.libvirt.virt_type,
8050                                         hypervisor_os='linux',
8051                                         uptime=uptime)
8052         diags.memory_details = diagnostics_obj.MemoryDiagnostics(
8053             maximum=max_mem / units.Mi,
8054             used=mem / units.Mi)
8055 
8056         # get cpu time, might launch an exception if the method
8057         # is not supported by the underlying hypervisor being
8058         # used by libvirt
8059         try:
8060             for vcpu in guest.get_vcpus_info():
8061                 diags.add_cpu(id=vcpu.id, time=vcpu.time)
8062         except libvirt.libvirtError:
8063             pass
8064         # get io status
8065         dom_io = LibvirtDriver._get_io_devices(xml)
8066         for guest_disk in dom_io["volumes"]:
8067             try:
8068                 # blockStats might launch an exception if the method
8069                 # is not supported by the underlying hypervisor being
8070                 # used by libvirt
8071                 stats = domain.blockStats(guest_disk)
8072                 diags.add_disk(read_bytes=stats[1],
8073                                read_requests=stats[0],
8074                                write_bytes=stats[3],
8075                                write_requests=stats[2],
8076                                errors_count=stats[4])
8077             except libvirt.libvirtError:
8078                 pass
8079         for interface in dom_io["ifaces"]:
8080             try:
8081                 # interfaceStats might launch an exception if the method
8082                 # is not supported by the underlying hypervisor being
8083                 # used by libvirt
8084                 stats = domain.interfaceStats(interface)
8085                 diags.add_nic(rx_octets=stats[0],
8086                               rx_errors=stats[2],
8087                               rx_drop=stats[3],
8088                               rx_packets=stats[1],
8089                               tx_octets=stats[4],
8090                               tx_errors=stats[6],
8091                               tx_drop=stats[7],
8092                               tx_packets=stats[5])
8093             except libvirt.libvirtError:
8094                 pass
8095 
8096         # Update mac addresses of interface if stats have been reported
8097         if diags.nic_details:
8098             nodes = xml_doc.findall('./devices/interface/mac')
8099             for index, node in enumerate(nodes):
8100                 diags.nic_details[index].mac_address = node.get('address')
8101         return diags
8102 
8103     @staticmethod
8104     def _prepare_device_bus(dev):
8105         """Determines the device bus and its hypervisor assigned address
8106         """
8107         bus = None
8108         address = (dev.device_addr.format_address() if
8109                    dev.device_addr else None)
8110         if isinstance(dev.device_addr,
8111                       vconfig.LibvirtConfigGuestDeviceAddressPCI):
8112             bus = objects.PCIDeviceBus()
8113         elif isinstance(dev, vconfig.LibvirtConfigGuestDisk):
8114             if dev.target_bus == 'scsi':
8115                 bus = objects.SCSIDeviceBus()
8116             elif dev.target_bus == 'ide':
8117                 bus = objects.IDEDeviceBus()
8118             elif dev.target_bus == 'usb':
8119                 bus = objects.USBDeviceBus()
8120         if address is not None and bus is not None:
8121             bus.address = address
8122         return bus
8123 
8124     def _build_device_metadata(self, context, instance):
8125         """Builds a metadata object for instance devices, that maps the user
8126            provided tag to the hypervisor assigned device address.
8127         """
8128         def _get_device_name(bdm):
8129             return block_device.strip_dev(bdm.device_name)
8130 
8131         network_info = instance.info_cache.network_info
8132         vlans_by_mac = netutils.get_cached_vifs_with_vlan(network_info)
8133         vifs = objects.VirtualInterfaceList.get_by_instance_uuid(context,
8134                                                                  instance.uuid)
8135         vifs_to_expose = {vif.address: vif for vif in vifs
8136                           if ('tag' in vif and vif.tag) or
8137                              vlans_by_mac.get(vif.address)}
8138         # TODO(mriedem): We should be able to avoid the DB query here by using
8139         # block_device_info['block_device_mapping'] which is passed into most
8140         # methods that call this function.
8141         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
8142             context, instance.uuid)
8143         tagged_bdms = {_get_device_name(bdm): bdm for bdm in bdms if bdm.tag}
8144 
8145         devices = []
8146         guest = self._host.get_guest(instance)
8147         xml = guest.get_xml_desc()
8148         xml_dom = etree.fromstring(xml)
8149         guest_config = vconfig.LibvirtConfigGuest()
8150         guest_config.parse_dom(xml_dom)
8151 
8152         for dev in guest_config.devices:
8153             # Build network interfaces related metadata
8154             if isinstance(dev, vconfig.LibvirtConfigGuestInterface):
8155                 vif = vifs_to_expose.get(dev.mac_addr)
8156                 if not vif:
8157                     continue
8158                 bus = self._prepare_device_bus(dev)
8159                 device = objects.NetworkInterfaceMetadata(mac=vif.address)
8160                 if 'tag' in vif and vif.tag:
8161                     device.tags = [vif.tag]
8162                 if bus:
8163                     device.bus = bus
8164                 vlan = vlans_by_mac.get(vif.address)
8165                 if vlan:
8166                     device.vlan = int(vlan)
8167                 devices.append(device)
8168 
8169             # Build disks related metadata
8170             if isinstance(dev, vconfig.LibvirtConfigGuestDisk):
8171                 bdm = tagged_bdms.get(dev.target_dev)
8172                 if not bdm:
8173                     continue
8174                 bus = self._prepare_device_bus(dev)
8175                 device = objects.DiskMetadata(tags=[bdm.tag])
8176                 # NOTE(artom) Setting the serial (which corresponds to
8177                 # volume_id in BlockDeviceMapping) in DiskMetadata allows us to
8178                 # find the disks's BlockDeviceMapping object when we detach the
8179                 # volume and want to clean up its metadata.
8180                 device.serial = bdm.volume_id
8181                 if bus:
8182                     device.bus = bus
8183                 devices.append(device)
8184         if devices:
8185             dev_meta = objects.InstanceDeviceMetadata(devices=devices)
8186             return dev_meta
8187 
8188     def instance_on_disk(self, instance):
8189         # ensure directories exist and are writable
8190         instance_path = libvirt_utils.get_instance_path(instance)
8191         LOG.debug('Checking instance files accessibility %s', instance_path,
8192                   instance=instance)
8193         shared_instance_path = os.access(instance_path, os.W_OK)
8194         # NOTE(flwang): For shared block storage scenario, the file system is
8195         # not really shared by the two hosts, but the volume of evacuated
8196         # instance is reachable.
8197         shared_block_storage = (self.image_backend.backend().
8198                                 is_shared_block_storage())
8199         return shared_instance_path or shared_block_storage
8200 
8201     def inject_network_info(self, instance, nw_info):
8202         self.firewall_driver.setup_basic_filtering(instance, nw_info)
8203 
8204     def delete_instance_files(self, instance):
8205         target = libvirt_utils.get_instance_path(instance)
8206         # A resize may be in progress
8207         target_resize = target + '_resize'
8208         # Other threads may attempt to rename the path, so renaming the path
8209         # to target + '_del' (because it is atomic) and iterating through
8210         # twice in the unlikely event that a concurrent rename occurs between
8211         # the two rename attempts in this method. In general this method
8212         # should be fairly thread-safe without these additional checks, since
8213         # other operations involving renames are not permitted when the task
8214         # state is not None and the task state should be set to something
8215         # other than None by the time this method is invoked.
8216         target_del = target + '_del'
8217         for i in range(2):
8218             try:
8219                 utils.execute('mv', target, target_del)
8220                 break
8221             except Exception:
8222                 pass
8223             try:
8224                 utils.execute('mv', target_resize, target_del)
8225                 break
8226             except Exception:
8227                 pass
8228         # Either the target or target_resize path may still exist if all
8229         # rename attempts failed.
8230         remaining_path = None
8231         for p in (target, target_resize):
8232             if os.path.exists(p):
8233                 remaining_path = p
8234                 break
8235 
8236         # A previous delete attempt may have been interrupted, so target_del
8237         # may exist even if all rename attempts during the present method
8238         # invocation failed due to the absence of both target and
8239         # target_resize.
8240         if not remaining_path and os.path.exists(target_del):
8241             self.job_tracker.terminate_jobs(instance)
8242 
8243             LOG.info('Deleting instance files %s', target_del,
8244                      instance=instance)
8245             remaining_path = target_del
8246             try:
8247                 shutil.rmtree(target_del)
8248             except OSError as e:
8249                 LOG.error('Failed to cleanup directory %(target)s: %(e)s',
8250                           {'target': target_del, 'e': e}, instance=instance)
8251 
8252         # It is possible that the delete failed, if so don't mark the instance
8253         # as cleaned.
8254         if remaining_path and os.path.exists(remaining_path):
8255             LOG.info('Deletion of %s failed', remaining_path,
8256                      instance=instance)
8257             return False
8258 
8259         LOG.info('Deletion of %s complete', target_del, instance=instance)
8260         return True
8261 
8262     @property
8263     def need_legacy_block_device_info(self):
8264         return False
8265 
8266     def default_root_device_name(self, instance, image_meta, root_bdm):
8267         disk_bus = blockinfo.get_disk_bus_for_device_type(
8268             instance, CONF.libvirt.virt_type, image_meta, "disk")
8269         cdrom_bus = blockinfo.get_disk_bus_for_device_type(
8270             instance, CONF.libvirt.virt_type, image_meta, "cdrom")
8271         root_info = blockinfo.get_root_info(
8272             instance, CONF.libvirt.virt_type, image_meta,
8273             root_bdm, disk_bus, cdrom_bus)
8274         return block_device.prepend_dev(root_info['dev'])
8275 
8276     def default_device_names_for_instance(self, instance, root_device_name,
8277                                           *block_device_lists):
8278         block_device_mapping = list(itertools.chain(*block_device_lists))
8279         # NOTE(ndipanov): Null out the device names so that blockinfo code
8280         #                 will assign them
8281         for bdm in block_device_mapping:
8282             if bdm.device_name is not None:
8283                 LOG.warning(
8284                     "Ignoring supplied device name: %(device_name)s. "
8285                     "Libvirt can't honour user-supplied dev names",
8286                     {'device_name': bdm.device_name}, instance=instance)
8287                 bdm.device_name = None
8288         block_device_info = driver.get_block_device_info(instance,
8289                                                          block_device_mapping)
8290 
8291         blockinfo.default_device_names(CONF.libvirt.virt_type,
8292                                        nova_context.get_admin_context(),
8293                                        instance,
8294                                        block_device_info,
8295                                        instance.image_meta)
8296 
8297     def get_device_name_for_instance(self, instance, bdms, block_device_obj):
8298         block_device_info = driver.get_block_device_info(instance, bdms)
8299         instance_info = blockinfo.get_disk_info(
8300                 CONF.libvirt.virt_type, instance,
8301                 instance.image_meta, block_device_info=block_device_info)
8302 
8303         suggested_dev_name = block_device_obj.device_name
8304         if suggested_dev_name is not None:
8305             LOG.warning(
8306                 'Ignoring supplied device name: %(suggested_dev)s',
8307                 {'suggested_dev': suggested_dev_name}, instance=instance)
8308 
8309         # NOTE(ndipanov): get_info_from_bdm will generate the new device name
8310         #                 only when it's actually not set on the bd object
8311         block_device_obj.device_name = None
8312         disk_info = blockinfo.get_info_from_bdm(
8313             instance, CONF.libvirt.virt_type, instance.image_meta,
8314             block_device_obj, mapping=instance_info['mapping'])
8315         return block_device.prepend_dev(disk_info['dev'])
8316 
8317     def is_supported_fs_format(self, fs_type):
8318         return fs_type in [disk_api.FS_FORMAT_EXT2, disk_api.FS_FORMAT_EXT3,
8319                            disk_api.FS_FORMAT_EXT4, disk_api.FS_FORMAT_XFS]
