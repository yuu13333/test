Based on the given code from a commit, please generate supplementary code files according to the commit message.
####commit message
Do not reserve device before reserving the volume

Reserving a device means that we have to create a BDM record on the
compute node, as part of an RPC call. If we can't actually attach the
volume - it's pointless to create a BDM record which we will just delete
if the reservation in Cinder fails, but this also opens us up to leaking
BDM records, as there is a window where the API worker dying or RPC
timing out leaves us with a leaked BDM.

Change-Id: I0a7eacd0751d604b932e65a2671e2b88010b3560
Closes-bug: 1427060

####code 
1 # Copyright 2010 United States Government as represented by the
2 # Administrator of the National Aeronautics and Space Administration.
3 # Copyright 2011 Piston Cloud Computing, Inc.
4 # Copyright 2012-2013 Red Hat, Inc.
5 # All Rights Reserved.
6 #
7 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
8 #    not use this file except in compliance with the License. You may obtain
9 #    a copy of the License at
10 #
11 #         http://www.apache.org/licenses/LICENSE-2.0
12 #
13 #    Unless required by applicable law or agreed to in writing, software
14 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
15 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
16 #    License for the specific language governing permissions and limitations
17 #    under the License.
18 
19 """Handles all requests relating to compute resources (e.g. guest VMs,
20 networking and storage of VMs, and compute hosts on which they run)."""
21 
22 import base64
23 import contextlib
24 import copy
25 import functools
26 import re
27 import string
28 import uuid
29 
30 from oslo_log import log as logging
31 from oslo_serialization import jsonutils
32 from oslo_utils import excutils
33 from oslo_utils import strutils
34 from oslo_utils import timeutils
35 from oslo_utils import units
36 from oslo_utils import uuidutils
37 import six
38 from six.moves import range
39 
40 from nova import availability_zones
41 from nova import block_device
42 from nova.cells import opts as cells_opts
43 from nova.compute import flavors
44 from nova.compute import instance_actions
45 from nova.compute import power_state
46 from nova.compute import rpcapi as compute_rpcapi
47 from nova.compute import task_states
48 from nova.compute import utils as compute_utils
49 from nova.compute import vm_states
50 from nova import conductor
51 import nova.conf
52 from nova.consoleauth import rpcapi as consoleauth_rpcapi
53 from nova import crypto
54 from nova.db import base
55 from nova import exception
56 from nova import hooks
57 from nova.i18n import _
58 from nova.i18n import _LE
59 from nova.i18n import _LI
60 from nova.i18n import _LW
61 from nova import image
62 from nova import keymgr
63 from nova import network
64 from nova.network import model as network_model
65 from nova.network.security_group import openstack_driver
66 from nova.network.security_group import security_group_base
67 from nova import notifications
68 from nova import objects
69 from nova.objects import base as obj_base
70 from nova.objects import block_device as block_device_obj
71 from nova.objects import fields as fields_obj
72 from nova.objects import keypair as keypair_obj
73 from nova.objects import quotas as quotas_obj
74 from nova.objects import security_group as security_group_obj
75 from nova.pci import request as pci_request
76 import nova.policy
77 from nova import rpc
78 from nova.scheduler import client as scheduler_client
79 from nova.scheduler import utils as scheduler_utils
80 from nova import servicegroup
81 from nova import utils
82 from nova.virt import hardware
83 from nova import volume
84 
85 LOG = logging.getLogger(__name__)
86 
87 get_notifier = functools.partial(rpc.get_notifier, service='compute')
88 wrap_exception = functools.partial(exception.wrap_exception,
89                                    get_notifier=get_notifier)
90 
91 CONF = nova.conf.CONF
92 CONF.import_opt('compute_topic', 'nova.compute.rpcapi')
93 
94 MAX_USERDATA_SIZE = 65535
95 RO_SECURITY_GROUPS = ['default']
96 VIDEO_RAM = 'hw_video:ram_max_mb'
97 
98 AGGREGATE_ACTION_UPDATE = 'Update'
99 AGGREGATE_ACTION_UPDATE_META = 'UpdateMeta'
100 AGGREGATE_ACTION_DELETE = 'Delete'
101 AGGREGATE_ACTION_ADD = 'Add'
102 
103 
104 def check_instance_state(vm_state=None, task_state=(None,),
105                          must_have_launched=True):
106     """Decorator to check VM and/or task state before entry to API functions.
107 
108     If the instance is in the wrong state, or has not been successfully
109     started at least once the wrapper will raise an exception.
110     """
111 
112     if vm_state is not None and not isinstance(vm_state, set):
113         vm_state = set(vm_state)
114     if task_state is not None and not isinstance(task_state, set):
115         task_state = set(task_state)
116 
117     def outer(f):
118         @functools.wraps(f)
119         def inner(self, context, instance, *args, **kw):
120             if vm_state is not None and instance.vm_state not in vm_state:
121                 raise exception.InstanceInvalidState(
122                     attr='vm_state',
123                     instance_uuid=instance.uuid,
124                     state=instance.vm_state,
125                     method=f.__name__)
126             if (task_state is not None and
127                     instance.task_state not in task_state):
128                 raise exception.InstanceInvalidState(
129                     attr='task_state',
130                     instance_uuid=instance.uuid,
131                     state=instance.task_state,
132                     method=f.__name__)
133             if must_have_launched and not instance.launched_at:
134                 raise exception.InstanceInvalidState(
135                     attr='launched_at',
136                     instance_uuid=instance.uuid,
137                     state=instance.launched_at,
138                     method=f.__name__)
139 
140             return f(self, context, instance, *args, **kw)
141         return inner
142     return outer
143 
144 
145 def check_instance_host(function):
146     @functools.wraps(function)
147     def wrapped(self, context, instance, *args, **kwargs):
148         if not instance.host:
149             raise exception.InstanceNotReady(instance_id=instance.uuid)
150         return function(self, context, instance, *args, **kwargs)
151     return wrapped
152 
153 
154 def check_instance_lock(function):
155     @functools.wraps(function)
156     def inner(self, context, instance, *args, **kwargs):
157         if instance.locked and not context.is_admin:
158             raise exception.InstanceIsLocked(instance_uuid=instance.uuid)
159         return function(self, context, instance, *args, **kwargs)
160     return inner
161 
162 
163 def policy_decorator(scope):
164     """Check corresponding policy prior of wrapped method to execution."""
165     def outer(func):
166         @functools.wraps(func)
167         def wrapped(self, context, target, *args, **kwargs):
168             if not self.skip_policy_check:
169                 check_policy(context, func.__name__, target, scope)
170             return func(self, context, target, *args, **kwargs)
171         return wrapped
172     return outer
173 
174 wrap_check_policy = policy_decorator(scope='compute')
175 wrap_check_security_groups_policy = policy_decorator(
176                                     scope='compute:security_groups')
177 
178 
179 def check_policy(context, action, target, scope='compute'):
180     _action = '%s:%s' % (scope, action)
181     nova.policy.enforce(context, _action, target)
182 
183 
184 def check_instance_cell(fn):
185     def _wrapped(self, context, instance, *args, **kwargs):
186         self._validate_cell(instance)
187         return fn(self, context, instance, *args, **kwargs)
188     _wrapped.__name__ = fn.__name__
189     return _wrapped
190 
191 
192 def _diff_dict(orig, new):
193     """Return a dict describing how to change orig to new.  The keys
194     correspond to values that have changed; the value will be a list
195     of one or two elements.  The first element of the list will be
196     either '+' or '-', indicating whether the key was updated or
197     deleted; if the key was updated, the list will contain a second
198     element, giving the updated value.
199     """
200     # Figure out what keys went away
201     result = {k: ['-'] for k in set(orig.keys()) - set(new.keys())}
202     # Compute the updates
203     for key, value in new.items():
204         if key not in orig or value != orig[key]:
205             result[key] = ['+', value]
206     return result
207 
208 
209 class API(base.Base):
210     """API for interacting with the compute manager."""
211 
212     def __init__(self, image_api=None, network_api=None, volume_api=None,
213                  security_group_api=None, skip_policy_check=False, **kwargs):
214         self.skip_policy_check = skip_policy_check
215         self.image_api = image_api or image.API()
216         self.network_api = network_api or network.API(
217             skip_policy_check=skip_policy_check)
218         self.volume_api = volume_api or volume.API()
219         self.security_group_api = (security_group_api or
220             openstack_driver.get_openstack_security_group_driver(
221                 skip_policy_check=skip_policy_check))
222         self.consoleauth_rpcapi = consoleauth_rpcapi.ConsoleAuthAPI()
223         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
224         self.compute_task_api = conductor.ComputeTaskAPI()
225         self.servicegroup_api = servicegroup.API()
226         self.notifier = rpc.get_notifier('compute', CONF.host)
227         if CONF.ephemeral_storage_encryption.enabled:
228             self.key_manager = keymgr.API()
229 
230         super(API, self).__init__(**kwargs)
231 
232     @property
233     def cell_type(self):
234         try:
235             return getattr(self, '_cell_type')
236         except AttributeError:
237             self._cell_type = cells_opts.get_cell_type()
238             return self._cell_type
239 
240     def _validate_cell(self, instance):
241         if self.cell_type != 'api':
242             return
243         cell_name = instance.cell_name
244         if not cell_name:
245             raise exception.InstanceUnknownCell(
246                     instance_uuid=instance.uuid)
247 
248     def _record_action_start(self, context, instance, action):
249         objects.InstanceAction.action_start(context, instance.uuid,
250                                             action, want_result=False)
251 
252     def _check_injected_file_quota(self, context, injected_files):
253         """Enforce quota limits on injected files.
254 
255         Raises a QuotaError if any limit is exceeded.
256         """
257         if injected_files is None:
258             return
259 
260         # Check number of files first
261         try:
262             objects.Quotas.limit_check(context,
263                                        injected_files=len(injected_files))
264         except exception.OverQuota:
265             raise exception.OnsetFileLimitExceeded()
266 
267         # OK, now count path and content lengths; we're looking for
268         # the max...
269         max_path = 0
270         max_content = 0
271         for path, content in injected_files:
272             max_path = max(max_path, len(path))
273             max_content = max(max_content, len(content))
274 
275         try:
276             objects.Quotas.limit_check(context,
277                                        injected_file_path_bytes=max_path,
278                                        injected_file_content_bytes=max_content)
279         except exception.OverQuota as exc:
280             # Favor path limit over content limit for reporting
281             # purposes
282             if 'injected_file_path_bytes' in exc.kwargs['overs']:
283                 raise exception.OnsetFilePathLimitExceeded()
284             else:
285                 raise exception.OnsetFileContentLimitExceeded()
286 
287     def _get_headroom(self, quotas, usages, deltas):
288         headroom = {res: quotas[res] -
289                          (usages[res]['in_use'] + usages[res]['reserved'])
290                     for res in quotas.keys()}
291         # If quota_cores is unlimited [-1]:
292         # - set cores headroom based on instances headroom:
293         if quotas.get('cores') == -1:
294             if deltas.get('cores'):
295                 hc = headroom['instances'] * deltas['cores']
296                 headroom['cores'] = hc / deltas.get('instances', 1)
297             else:
298                 headroom['cores'] = headroom['instances']
299 
300         # If quota_ram is unlimited [-1]:
301         # - set ram headroom based on instances headroom:
302         if quotas.get('ram') == -1:
303             if deltas.get('ram'):
304                 hr = headroom['instances'] * deltas['ram']
305                 headroom['ram'] = hr / deltas.get('instances', 1)
306             else:
307                 headroom['ram'] = headroom['instances']
308 
309         return headroom
310 
311     def _check_num_instances_quota(self, context, instance_type, min_count,
312                                    max_count, project_id=None, user_id=None):
313         """Enforce quota limits on number of instances created."""
314 
315         # Determine requested cores and ram
316         req_cores = max_count * instance_type['vcpus']
317         vram_mb = int(instance_type.get('extra_specs', {}).get(VIDEO_RAM, 0))
318         req_ram = max_count * (instance_type['memory_mb'] + vram_mb)
319 
320         # Check the quota
321         try:
322             quotas = objects.Quotas(context=context)
323             quotas.reserve(instances=max_count,
324                            cores=req_cores, ram=req_ram,
325                            project_id=project_id, user_id=user_id)
326         except exception.OverQuota as exc:
327             # OK, we exceeded quota; let's figure out why...
328             quotas = exc.kwargs['quotas']
329             overs = exc.kwargs['overs']
330             usages = exc.kwargs['usages']
331             deltas = {'instances': max_count,
332                       'cores': req_cores, 'ram': req_ram}
333             headroom = self._get_headroom(quotas, usages, deltas)
334 
335             allowed = headroom['instances']
336             # Reduce 'allowed' instances in line with the cores & ram headroom
337             if instance_type['vcpus']:
338                 allowed = min(allowed,
339                               headroom['cores'] // instance_type['vcpus'])
340             if instance_type['memory_mb']:
341                 allowed = min(allowed,
342                               headroom['ram'] // (instance_type['memory_mb'] +
343                                                   vram_mb))
344 
345             # Convert to the appropriate exception message
346             if allowed <= 0:
347                 msg = _("Cannot run any more instances of this type.")
348             elif min_count <= allowed <= max_count:
349                 # We're actually OK, but still need reservations
350                 return self._check_num_instances_quota(context, instance_type,
351                                                        min_count, allowed)
352             else:
353                 msg = (_("Can only run %s more instances of this type.") %
354                        allowed)
355 
356             num_instances = (str(min_count) if min_count == max_count else
357                 "%s-%s" % (min_count, max_count))
358             requested = dict(instances=num_instances, cores=req_cores,
359                              ram=req_ram)
360             (overs, reqs, total_alloweds, useds) = self._get_over_quota_detail(
361                 headroom, overs, quotas, requested)
362             params = {'overs': overs, 'pid': context.project_id,
363                       'min_count': min_count, 'max_count': max_count,
364                       'msg': msg}
365 
366             if min_count == max_count:
367                 LOG.debug(("%(overs)s quota exceeded for %(pid)s,"
368                            " tried to run %(min_count)d instances. "
369                            "%(msg)s"), params)
370             else:
371                 LOG.debug(("%(overs)s quota exceeded for %(pid)s,"
372                            " tried to run between %(min_count)d and"
373                            " %(max_count)d instances. %(msg)s"),
374                           params)
375             raise exception.TooManyInstances(overs=overs,
376                                              req=reqs,
377                                              used=useds,
378                                              allowed=total_alloweds)
379 
380         return max_count, quotas
381 
382     def _get_over_quota_detail(self, headroom, overs, quotas, requested):
383         reqs = []
384         useds = []
385         total_alloweds = []
386         for resource in overs:
387             reqs.append(str(requested[resource]))
388             useds.append(str(quotas[resource] - headroom[resource]))
389             total_alloweds.append(str(quotas[resource]))
390         (overs, reqs, useds, total_alloweds) = map(', '.join, (
391             overs, reqs, useds, total_alloweds))
392         return overs, reqs, total_alloweds, useds
393 
394     def _check_metadata_properties_quota(self, context, metadata=None):
395         """Enforce quota limits on metadata properties."""
396         if not metadata:
397             metadata = {}
398         if not isinstance(metadata, dict):
399             msg = (_("Metadata type should be dict."))
400             raise exception.InvalidMetadata(reason=msg)
401         num_metadata = len(metadata)
402         try:
403             objects.Quotas.limit_check(context, metadata_items=num_metadata)
404         except exception.OverQuota as exc:
405             quota_metadata = exc.kwargs['quotas']['metadata_items']
406             raise exception.MetadataLimitExceeded(allowed=quota_metadata)
407 
408         # Because metadata is stored in the DB, we hard-code the size limits
409         # In future, we may support more variable length strings, so we act
410         #  as if this is quota-controlled for forwards compatibility.
411         # Those are only used in V2 API, from V2.1 API, those checks are
412         # validated at API layer schema validation.
413         for k, v in six.iteritems(metadata):
414             try:
415                 utils.check_string_length(v)
416                 utils.check_string_length(k, min_length=1)
417             except exception.InvalidInput as e:
418                 raise exception.InvalidMetadata(reason=e.format_message())
419 
420             if len(k) > 255:
421                 msg = _("Metadata property key greater than 255 characters")
422                 raise exception.InvalidMetadataSize(reason=msg)
423             if len(v) > 255:
424                 msg = _("Metadata property value greater than 255 characters")
425                 raise exception.InvalidMetadataSize(reason=msg)
426 
427     def _check_requested_secgroups(self, context, secgroups):
428         """Check if the security group requested exists and belongs to
429         the project.
430         """
431         for secgroup in secgroups:
432             # NOTE(sdague): default is handled special
433             if secgroup == "default":
434                 continue
435             if not self.security_group_api.get(context, secgroup):
436                 raise exception.SecurityGroupNotFoundForProject(
437                     project_id=context.project_id, security_group_id=secgroup)
438 
439     def _check_requested_networks(self, context, requested_networks,
440                                   max_count):
441         """Check if the networks requested belongs to the project
442         and the fixed IP address for each network provided is within
443         same the network block
444         """
445         if requested_networks is not None:
446             # NOTE(danms): Temporary transition
447             requested_networks = requested_networks.as_tuples()
448         return self.network_api.validate_networks(context, requested_networks,
449                                                   max_count)
450 
451     def _handle_kernel_and_ramdisk(self, context, kernel_id, ramdisk_id,
452                                    image):
453         """Choose kernel and ramdisk appropriate for the instance.
454 
455         The kernel and ramdisk can be chosen in one of three ways:
456 
457             1. Passed in with create-instance request.
458 
459             2. Inherited from image.
460 
461             3. Forced to None by using `null_kernel` FLAG.
462         """
463         # Inherit from image if not specified
464         image_properties = image.get('properties', {})
465 
466         if kernel_id is None:
467             kernel_id = image_properties.get('kernel_id')
468 
469         if ramdisk_id is None:
470             ramdisk_id = image_properties.get('ramdisk_id')
471 
472         # Force to None if using null_kernel
473         if kernel_id == str(CONF.null_kernel):
474             kernel_id = None
475             ramdisk_id = None
476 
477         # Verify kernel and ramdisk exist (fail-fast)
478         if kernel_id is not None:
479             kernel_image = self.image_api.get(context, kernel_id)
480             # kernel_id could have been a URI, not a UUID, so to keep behaviour
481             # from before, which leaked that implementation detail out to the
482             # caller, we return the image UUID of the kernel image and ramdisk
483             # image (below) and not any image URIs that might have been
484             # supplied.
485             # TODO(jaypipes): Get rid of this silliness once we move to a real
486             # Image object and hide all of that stuff within nova.image.api.
487             kernel_id = kernel_image['id']
488 
489         if ramdisk_id is not None:
490             ramdisk_image = self.image_api.get(context, ramdisk_id)
491             ramdisk_id = ramdisk_image['id']
492 
493         return kernel_id, ramdisk_id
494 
495     @staticmethod
496     def parse_availability_zone(context, availability_zone):
497         # NOTE(vish): We have a legacy hack to allow admins to specify hosts
498         #             via az using az:host:node. It might be nice to expose an
499         #             api to specify specific hosts to force onto, but for
500         #             now it just supports this legacy hack.
501         # NOTE(deva): It is also possible to specify az::node, in which case
502         #             the host manager will determine the correct host.
503         forced_host = None
504         forced_node = None
505         if availability_zone and ':' in availability_zone:
506             c = availability_zone.count(':')
507             if c == 1:
508                 availability_zone, forced_host = availability_zone.split(':')
509             elif c == 2:
510                 if '::' in availability_zone:
511                     availability_zone, forced_node = \
512                             availability_zone.split('::')
513                 else:
514                     availability_zone, forced_host, forced_node = \
515                             availability_zone.split(':')
516             else:
517                 raise exception.InvalidInput(
518                         reason="Unable to parse availability_zone")
519 
520         if not availability_zone:
521             availability_zone = CONF.default_schedule_zone
522 
523         return availability_zone, forced_host, forced_node
524 
525     def _ensure_auto_disk_config_is_valid(self, auto_disk_config_img,
526                                           auto_disk_config, image):
527         auto_disk_config_disabled = \
528                 utils.is_auto_disk_config_disabled(auto_disk_config_img)
529         if auto_disk_config_disabled and auto_disk_config:
530             raise exception.AutoDiskConfigDisabledByImage(image=image)
531 
532     def _inherit_properties_from_image(self, image, auto_disk_config):
533         image_properties = image.get('properties', {})
534         auto_disk_config_img = \
535                 utils.get_auto_disk_config_from_image_props(image_properties)
536         self._ensure_auto_disk_config_is_valid(auto_disk_config_img,
537                                                auto_disk_config,
538                                                image.get("id"))
539         if auto_disk_config is None:
540             auto_disk_config = strutils.bool_from_string(auto_disk_config_img)
541 
542         return {
543             'os_type': image_properties.get('os_type'),
544             'architecture': image_properties.get('architecture'),
545             'vm_mode': image_properties.get('vm_mode'),
546             'auto_disk_config': auto_disk_config
547         }
548 
549     def _new_instance_name_from_template(self, uuid, display_name, index):
550         params = {
551             'uuid': uuid,
552             'name': display_name,
553             'count': index + 1,
554         }
555         try:
556             new_name = (CONF.multi_instance_display_name_template %
557                         params)
558         except (KeyError, TypeError):
559             LOG.exception(_LE('Failed to set instance name using '
560                               'multi_instance_display_name_template.'))
561             new_name = display_name
562         return new_name
563 
564     def _apply_instance_name_template(self, context, instance, index):
565         original_name = instance.display_name
566         new_name = self._new_instance_name_from_template(instance.uuid,
567                 instance.display_name, index)
568         instance.display_name = new_name
569         if not instance.get('hostname', None):
570             if utils.sanitize_hostname(original_name) == "":
571                 instance.hostname = self._default_host_name(instance.uuid)
572             else:
573                 instance.hostname = utils.sanitize_hostname(new_name)
574         instance.save()
575         return instance
576 
577     def _check_config_drive(self, config_drive):
578         if config_drive:
579             try:
580                 bool_val = strutils.bool_from_string(config_drive,
581                                                      strict=True)
582             except ValueError:
583                 raise exception.ConfigDriveInvalidValue(option=config_drive)
584         else:
585             bool_val = False
586         # FIXME(comstud):  Bug ID 1193438 filed for this. This looks silly,
587         # but this is because the config drive column is a String.  False
588         # is represented by using an empty string.  And for whatever
589         # reason, we rely on the DB to cast True to a String.
590         return True if bool_val else ''
591 
592     def _check_requested_image(self, context, image_id, image,
593                                instance_type, root_bdm):
594         if not image:
595             return
596 
597         if image['status'] != 'active':
598             raise exception.ImageNotActive(image_id=image_id)
599 
600         image_properties = image.get('properties', {})
601         config_drive_option = image_properties.get(
602             'img_config_drive', 'optional')
603         if config_drive_option not in ['optional', 'mandatory']:
604             raise exception.InvalidImageConfigDrive(
605                 config_drive=config_drive_option)
606 
607         if instance_type['memory_mb'] < int(image.get('min_ram') or 0):
608             raise exception.FlavorMemoryTooSmall()
609 
610         # Image min_disk is in gb, size is in bytes. For sanity, have them both
611         # in bytes.
612         image_min_disk = int(image.get('min_disk') or 0) * units.Gi
613         image_size = int(image.get('size') or 0)
614 
615         # Target disk is a volume. Don't check flavor disk size because it
616         # doesn't make sense, and check min_disk against the volume size.
617         if (root_bdm is not None and root_bdm.is_volume):
618             # There are 2 possibilities here: either the target volume already
619             # exists, or it doesn't, in which case the bdm will contain the
620             # intended volume size.
621             #
622             # Cinder does its own check against min_disk, so if the target
623             # volume already exists this has already been done and we don't
624             # need to check it again here. In this case, volume_size may not be
625             # set on the bdm.
626             #
627             # If we're going to create the volume, the bdm will contain
628             # volume_size. Therefore we should check it if it exists. This will
629             # still be checked again by cinder when the volume is created, but
630             # that will not happen until the request reaches a host. By
631             # checking it here, the user gets an immediate and useful failure
632             # indication.
633             #
634             # The third possibility is that we have failed to consider
635             # something, and there are actually more than 2 possibilities. In
636             # this case cinder will still do the check at volume creation time.
637             # The behaviour will still be correct, but the user will not get an
638             # immediate failure from the api, and will instead have to
639             # determine why the instance is in an error state with a task of
640             # block_device_mapping.
641             #
642             # We could reasonably refactor this check into _validate_bdm at
643             # some future date, as the various size logic is already split out
644             # in there.
645             dest_size = root_bdm.volume_size
646             if dest_size is not None:
647                 dest_size *= units.Gi
648 
649                 if image_min_disk > dest_size:
650                     raise exception.VolumeSmallerThanMinDisk(
651                         volume_size=dest_size, image_min_disk=image_min_disk)
652 
653         # Target disk is a local disk whose size is taken from the flavor
654         else:
655             dest_size = instance_type['root_gb'] * units.Gi
656 
657             # NOTE(johannes): root_gb is allowed to be 0 for legacy reasons
658             # since libvirt interpreted the value differently than other
659             # drivers. A value of 0 means don't check size.
660             if dest_size != 0:
661                 if image_size > dest_size:
662                     raise exception.FlavorDiskSmallerThanImage(
663                         flavor_size=dest_size, image_size=image_size)
664 
665                 if image_min_disk > dest_size:
666                     raise exception.FlavorDiskSmallerThanMinDisk(
667                         flavor_size=dest_size, image_min_disk=image_min_disk)
668 
669     def _get_image_defined_bdms(self, base_options, instance_type, image_meta,
670                                 root_device_name):
671         image_properties = image_meta.get('properties', {})
672 
673         # Get the block device mappings defined by the image.
674         image_defined_bdms = image_properties.get('block_device_mapping', [])
675         legacy_image_defined = not image_properties.get('bdm_v2', False)
676 
677         image_mapping = image_properties.get('mappings', [])
678 
679         if legacy_image_defined:
680             image_defined_bdms = block_device.from_legacy_mapping(
681                 image_defined_bdms, None, root_device_name)
682         else:
683             image_defined_bdms = list(map(block_device.BlockDeviceDict,
684                                           image_defined_bdms))
685 
686         if image_mapping:
687             image_mapping = self._prepare_image_mapping(instance_type,
688                                                         image_mapping)
689             image_defined_bdms = self._merge_bdms_lists(
690                 image_mapping, image_defined_bdms)
691 
692         return image_defined_bdms
693 
694     def _get_flavor_defined_bdms(self, instance_type, block_device_mapping):
695         flavor_defined_bdms = []
696 
697         have_ephemeral_bdms = any(filter(
698             block_device.new_format_is_ephemeral, block_device_mapping))
699         have_swap_bdms = any(filter(
700             block_device.new_format_is_swap, block_device_mapping))
701 
702         if instance_type.get('ephemeral_gb') and not have_ephemeral_bdms:
703             flavor_defined_bdms.append(
704                 block_device.create_blank_bdm(instance_type['ephemeral_gb']))
705         if instance_type.get('swap') and not have_swap_bdms:
706             flavor_defined_bdms.append(
707                 block_device.create_blank_bdm(instance_type['swap'], 'swap'))
708 
709         return flavor_defined_bdms
710 
711     def _merge_bdms_lists(self, overrideable_mappings, overrider_mappings):
712         """Override any block devices from the first list by device name
713 
714         :param overridable_mappings: list which items are overriden
715         :param overrider_mappings: list which items override
716 
717         :returns: A merged list of bdms
718         """
719         device_names = set(bdm['device_name'] for bdm in overrider_mappings
720                            if bdm['device_name'])
721         return (overrider_mappings +
722                 [bdm for bdm in overrideable_mappings
723                  if bdm['device_name'] not in device_names])
724 
725     def _check_and_transform_bdm(self, context, base_options, instance_type,
726                                  image_meta, min_count, max_count,
727                                  block_device_mapping, legacy_bdm):
728         # NOTE (ndipanov): Assume root dev name is 'vda' if not supplied.
729         #                  It's needed for legacy conversion to work.
730         root_device_name = (base_options.get('root_device_name') or 'vda')
731         image_ref = base_options.get('image_ref', '')
732         # If the instance is booted by image and has a volume attached,
733         # the volume cannot have the same device name as root_device_name
734         if image_ref:
735             for bdm in block_device_mapping:
736                 if (bdm.get('destination_type') == 'volume' and
737                     block_device.strip_dev(bdm.get(
738                     'device_name')) == root_device_name):
739                     msg = _('The volume cannot be assigned the same device'
740                             ' name as the root device %s') % root_device_name
741                     raise exception.InvalidRequest(msg)
742 
743         image_defined_bdms = self._get_image_defined_bdms(
744             base_options, instance_type, image_meta, root_device_name)
745         root_in_image_bdms = (
746             block_device.get_root_bdm(image_defined_bdms) is not None)
747 
748         if legacy_bdm:
749             block_device_mapping = block_device.from_legacy_mapping(
750                 block_device_mapping, image_ref, root_device_name,
751                 no_root=root_in_image_bdms)
752         elif root_in_image_bdms:
753             # NOTE (ndipanov): client will insert an image mapping into the v2
754             # block_device_mapping, but if there is a bootable device in image
755             # mappings - we need to get rid of the inserted image
756             # NOTE (gibi): another case is when a server is booted with an
757             # image to bdm mapping where the image only contains a bdm to a
758             # snapshot. In this case the other image to bdm mapping
759             # contains an unnecessary device with boot_index == 0.
760             # Also in this case the image_ref is None as we are booting from
761             # an image to volume bdm.
762             def not_image_and_root_bdm(bdm):
763                 return not (bdm.get('boot_index') == 0 and
764                             bdm.get('source_type') == 'image')
765 
766             block_device_mapping = list(
767                 filter(not_image_and_root_bdm, block_device_mapping))
768 
769         block_device_mapping = self._merge_bdms_lists(
770             image_defined_bdms, block_device_mapping)
771 
772         if min_count > 1 or max_count > 1:
773             if any(map(lambda bdm: bdm['source_type'] == 'volume',
774                        block_device_mapping)):
775                 msg = _('Cannot attach one or more volumes to multiple'
776                         ' instances')
777                 raise exception.InvalidRequest(msg)
778 
779         block_device_mapping += self._get_flavor_defined_bdms(
780             instance_type, block_device_mapping)
781 
782         return block_device_obj.block_device_make_list_from_dicts(
783                 context, block_device_mapping)
784 
785     def _get_image(self, context, image_href):
786         if not image_href:
787             return None, {}
788 
789         image = self.image_api.get(context, image_href)
790         return image['id'], image
791 
792     def _checks_for_create_and_rebuild(self, context, image_id, image,
793                                        instance_type, metadata,
794                                        files_to_inject, root_bdm):
795         self._check_metadata_properties_quota(context, metadata)
796         self._check_injected_file_quota(context, files_to_inject)
797         self._check_requested_image(context, image_id, image,
798                                     instance_type, root_bdm)
799 
800     def _validate_and_build_base_options(self, context, instance_type,
801                                          boot_meta, image_href, image_id,
802                                          kernel_id, ramdisk_id, display_name,
803                                          display_description, key_name,
804                                          key_data, security_groups,
805                                          availability_zone, user_data,
806                                          metadata, access_ip_v4, access_ip_v6,
807                                          requested_networks, config_drive,
808                                          auto_disk_config, reservation_id,
809                                          max_count):
810         """Verify all the input parameters regardless of the provisioning
811         strategy being performed.
812         """
813         if instance_type['disabled']:
814             raise exception.FlavorNotFound(flavor_id=instance_type['id'])
815 
816         if user_data:
817             l = len(user_data)
818             if l > MAX_USERDATA_SIZE:
819                 # NOTE(mikal): user_data is stored in a text column, and
820                 # the database might silently truncate if its over length.
821                 raise exception.InstanceUserDataTooLarge(
822                     length=l, maxsize=MAX_USERDATA_SIZE)
823 
824             try:
825                 base64.decodestring(user_data)
826             except base64.binascii.Error:
827                 raise exception.InstanceUserDataMalformed()
828 
829         self._check_requested_secgroups(context, security_groups)
830 
831         # Note:  max_count is the number of instances requested by the user,
832         # max_network_count is the maximum number of instances taking into
833         # account any network quotas
834         max_network_count = self._check_requested_networks(context,
835                                      requested_networks, max_count)
836 
837         kernel_id, ramdisk_id = self._handle_kernel_and_ramdisk(
838                 context, kernel_id, ramdisk_id, boot_meta)
839 
840         config_drive = self._check_config_drive(config_drive)
841 
842         if key_data is None and key_name is not None:
843             key_pair = objects.KeyPair.get_by_name(context,
844                                                    context.user_id,
845                                                    key_name)
846             key_data = key_pair.public_key
847 
848         root_device_name = block_device.prepend_dev(
849                 block_device.properties_root_device_name(
850                     boot_meta.get('properties', {})))
851 
852         image_meta = objects.ImageMeta.from_dict(boot_meta)
853         numa_topology = hardware.numa_get_constraints(
854                 instance_type, image_meta)
855 
856         system_metadata = {}
857 
858         # PCI requests come from two sources: instance flavor and
859         # requested_networks. The first call in below returns an
860         # InstancePCIRequests object which is a list of InstancePCIRequest
861         # objects. The second call in below creates an InstancePCIRequest
862         # object for each SR-IOV port, and append it to the list in the
863         # InstancePCIRequests object
864         pci_request_info = pci_request.get_pci_requests_from_flavor(
865             instance_type)
866         self.network_api.create_pci_requests_for_sriov_ports(context,
867             pci_request_info, requested_networks)
868 
869         base_options = {
870             'reservation_id': reservation_id,
871             'image_ref': image_href,
872             'kernel_id': kernel_id or '',
873             'ramdisk_id': ramdisk_id or '',
874             'power_state': power_state.NOSTATE,
875             'vm_state': vm_states.BUILDING,
876             'config_drive': config_drive,
877             'user_id': context.user_id,
878             'project_id': context.project_id,
879             'instance_type_id': instance_type['id'],
880             'memory_mb': instance_type['memory_mb'],
881             'vcpus': instance_type['vcpus'],
882             'root_gb': instance_type['root_gb'],
883             'ephemeral_gb': instance_type['ephemeral_gb'],
884             'display_name': display_name,
885             'display_description': display_description,
886             'user_data': user_data,
887             'key_name': key_name,
888             'key_data': key_data,
889             'locked': False,
890             'metadata': metadata or {},
891             'access_ip_v4': access_ip_v4,
892             'access_ip_v6': access_ip_v6,
893             'availability_zone': availability_zone,
894             'root_device_name': root_device_name,
895             'progress': 0,
896             'pci_requests': pci_request_info,
897             'numa_topology': numa_topology,
898             'system_metadata': system_metadata}
899 
900         options_from_image = self._inherit_properties_from_image(
901                 boot_meta, auto_disk_config)
902 
903         base_options.update(options_from_image)
904 
905         # return the validated options and maximum number of instances allowed
906         # by the network quotas
907         return base_options, max_network_count
908 
909     def _create_build_request(self, context, instance_uuid, base_options,
910             request_spec, security_groups, num_instances, index):
911         # Store the BuildRequest that will help populate an instance
912         # object for a list/show request
913         info_cache = objects.InstanceInfoCache()
914         info_cache.instance_uuid = instance_uuid
915         info_cache.network_info = network_model.NetworkInfo()
916         # NOTE: base_options['config_drive'] is either True or '' due
917         # to how it's represented in the instances table in the db.
918         # BuildRequest needs a boolean.
919         bool_config_drive = strutils.bool_from_string(
920                 base_options['config_drive'], default=False)
921         # display_name follows a whole set of rules
922         display_name = base_options['display_name']
923         if display_name is None:
924             display_name = self._default_display_name(instance_uuid)
925         if num_instances > 1:
926             display_name = self._new_instance_name_from_template(instance_uuid,
927                     display_name, index)
928         build_request = objects.BuildRequest(context,
929                 request_spec=request_spec,
930                 project_id=context.project_id,
931                 user_id=context.user_id,
932                 display_name=display_name,
933                 instance_metadata=base_options['metadata'],
934                 progress=0,
935                 vm_state=vm_states.BUILDING,
936                 task_state=task_states.SCHEDULING,
937                 image_ref=base_options['image_ref'],
938                 access_ip_v4=base_options['access_ip_v4'],
939                 access_ip_v6=base_options['access_ip_v6'],
940                 info_cache=info_cache,
941                 security_groups=security_groups,
942                 config_drive=bool_config_drive,
943                 key_name=base_options['key_name'],
944                 locked_by=None)
945         build_request.create()
946         return build_request
947 
948     def _provision_instances(self, context, instance_type, min_count,
949             max_count, base_options, boot_meta, security_groups,
950             block_device_mapping, shutdown_terminate,
951             instance_group, check_server_group_quota, filter_properties):
952         # Reserve quotas
953         num_instances, quotas = self._check_num_instances_quota(
954                 context, instance_type, min_count, max_count)
955         security_groups = self.security_group_api.populate_security_groups(
956                 security_groups)
957         LOG.debug("Going to run %s instances...", num_instances)
958         instances = []
959         try:
960             for i in range(num_instances):
961                 # Create a uuid for the instance so we can store the
962                 # RequestSpec before the instance is created.
963                 instance_uuid = str(uuid.uuid4())
964                 # Store the RequestSpec that will be used for scheduling.
965                 req_spec = objects.RequestSpec.from_components(context,
966                         instance_uuid, boot_meta, instance_type,
967                         base_options['numa_topology'],
968                         base_options['pci_requests'], filter_properties,
969                         instance_group, base_options['availability_zone'])
970                 req_spec.create()
971                 build_request = self._create_build_request(context,
972                         instance_uuid, base_options, req_spec, security_groups,
973                         num_instances, i)
974                 # TODO(alaski): Cast to conductor here which will call the
975                 # scheduler and defer instance creation until the scheduler
976                 # has picked a cell/host.
977                 instance = objects.Instance(context=context)
978                 instance.uuid = instance_uuid
979                 instance.update(base_options)
980                 instance = self.create_db_entry_for_new_instance(
981                         context, instance_type, boot_meta, instance,
982                         security_groups, block_device_mapping,
983                         num_instances, i, shutdown_terminate)
984                 instances.append(instance)
985                 # The BuildRequest needs to be stored until the instance is in
986                 # an instance table. At that point it will never be used again
987                 # and should be deleted. As instance creation is pushed back,
988                 # as noted above, this will move with it.
989                 # TODO(alaski): Sync API updates to the build_request to the
990                 # instance before it is destroyed. Right now only locked_by can
991                 # be updated before this is destroyed.
992                 build_request.destroy()
993 
994                 if instance_group:
995                     if check_server_group_quota:
996                         count = objects.Quotas.count(context,
997                                              'server_group_members',
998                                              instance_group,
999                                              context.user_id)
1000                         try:
1001                             objects.Quotas.limit_check(context,
1002                                                server_group_members=count + 1)
1003                         except exception.OverQuota:
1004                             msg = _("Quota exceeded, too many servers in "
1005                                     "group")
1006                             raise exception.QuotaError(msg)
1007 
1008                     objects.InstanceGroup.add_members(context,
1009                                                       instance_group.uuid,
1010                                                       [instance.uuid])
1011 
1012                 # send a state update notification for the initial create to
1013                 # show it going from non-existent to BUILDING
1014                 notifications.send_update_with_states(context, instance, None,
1015                         vm_states.BUILDING, None, None, service="api")
1016 
1017         # In the case of any exceptions, attempt DB cleanup and rollback the
1018         # quota reservations.
1019         except Exception:
1020             with excutils.save_and_reraise_exception():
1021                 try:
1022                     for instance in instances:
1023                         try:
1024                             instance.destroy()
1025                         except exception.ObjectActionError:
1026                             pass
1027                 finally:
1028                     quotas.rollback()
1029 
1030         # Commit the reservations
1031         quotas.commit()
1032         return instances
1033 
1034     def _get_bdm_image_metadata(self, context, block_device_mapping,
1035                                 legacy_bdm=True):
1036         """If we are booting from a volume, we need to get the
1037         volume details from Cinder and make sure we pass the
1038         metadata back accordingly.
1039         """
1040         if not block_device_mapping:
1041             return {}
1042 
1043         for bdm in block_device_mapping:
1044             if (legacy_bdm and
1045                     block_device.get_device_letter(
1046                        bdm.get('device_name', '')) != 'a'):
1047                 continue
1048             elif not legacy_bdm and bdm.get('boot_index') != 0:
1049                 continue
1050 
1051             volume_id = bdm.get('volume_id')
1052             snapshot_id = bdm.get('snapshot_id')
1053             if snapshot_id:
1054                 # NOTE(alaski): A volume snapshot inherits metadata from the
1055                 # originating volume, but the API does not expose metadata
1056                 # on the snapshot itself.  So we query the volume for it below.
1057                 snapshot = self.volume_api.get_snapshot(context, snapshot_id)
1058                 volume_id = snapshot['volume_id']
1059 
1060             if bdm.get('image_id'):
1061                 try:
1062                     image_id = bdm['image_id']
1063                     image_meta = self.image_api.get(context, image_id)
1064                     return image_meta
1065                 except Exception:
1066                     raise exception.InvalidBDMImage(id=image_id)
1067             elif volume_id:
1068                 try:
1069                     volume = self.volume_api.get(context, volume_id)
1070                 except exception.CinderConnectionFailed:
1071                     raise
1072                 except Exception:
1073                     raise exception.InvalidBDMVolume(id=volume_id)
1074 
1075                 if not volume.get('bootable', True):
1076                     raise exception.InvalidBDMVolumeNotBootable(id=volume_id)
1077 
1078                 return utils.get_image_metadata_from_volume(volume)
1079         return {}
1080 
1081     @staticmethod
1082     def _get_requested_instance_group(context, filter_properties,
1083                                       check_quota):
1084         if (not filter_properties or
1085                 not filter_properties.get('scheduler_hints')):
1086             return
1087 
1088         group_hint = filter_properties.get('scheduler_hints').get('group')
1089         if not group_hint:
1090             return
1091 
1092         # TODO(gibi): We need to remove the following validation code when
1093         # removing legacy v2 code.
1094         if not uuidutils.is_uuid_like(group_hint):
1095             msg = _('Server group scheduler hint must be a UUID.')
1096             raise exception.InvalidInput(reason=msg)
1097 
1098         return objects.InstanceGroup.get_by_uuid(context, group_hint)
1099 
1100     def _create_instance(self, context, instance_type,
1101                image_href, kernel_id, ramdisk_id,
1102                min_count, max_count,
1103                display_name, display_description,
1104                key_name, key_data, security_groups,
1105                availability_zone, user_data, metadata, injected_files,
1106                admin_password, access_ip_v4, access_ip_v6,
1107                requested_networks, config_drive,
1108                block_device_mapping, auto_disk_config, filter_properties,
1109                reservation_id=None, legacy_bdm=True, shutdown_terminate=False,
1110                check_server_group_quota=False):
1111         """Verify all the input parameters regardless of the provisioning
1112         strategy being performed and schedule the instance(s) for
1113         creation.
1114         """
1115 
1116         # Normalize and setup some parameters
1117         if reservation_id is None:
1118             reservation_id = utils.generate_uid('r')
1119         security_groups = security_groups or ['default']
1120         min_count = min_count or 1
1121         max_count = max_count or min_count
1122         block_device_mapping = block_device_mapping or []
1123 
1124         if image_href:
1125             image_id, boot_meta = self._get_image(context, image_href)
1126         else:
1127             image_id = None
1128             boot_meta = self._get_bdm_image_metadata(
1129                 context, block_device_mapping, legacy_bdm)
1130 
1131         self._check_auto_disk_config(image=boot_meta,
1132                                      auto_disk_config=auto_disk_config)
1133 
1134         base_options, max_net_count = self._validate_and_build_base_options(
1135                 context, instance_type, boot_meta, image_href, image_id,
1136                 kernel_id, ramdisk_id, display_name, display_description,
1137                 key_name, key_data, security_groups, availability_zone,
1138                 user_data, metadata, access_ip_v4, access_ip_v6,
1139                 requested_networks, config_drive, auto_disk_config,
1140                 reservation_id, max_count)
1141 
1142         # max_net_count is the maximum number of instances requested by the
1143         # user adjusted for any network quota constraints, including
1144         # consideration of connections to each requested network
1145         if max_net_count < min_count:
1146             raise exception.PortLimitExceeded()
1147         elif max_net_count < max_count:
1148             LOG.info(_LI("max count reduced from %(max_count)d to "
1149                          "%(max_net_count)d due to network port quota"),
1150                         {'max_count': max_count,
1151                          'max_net_count': max_net_count})
1152             max_count = max_net_count
1153 
1154         block_device_mapping = self._check_and_transform_bdm(context,
1155             base_options, instance_type, boot_meta, min_count, max_count,
1156             block_device_mapping, legacy_bdm)
1157 
1158         # We can't do this check earlier because we need bdms from all sources
1159         # to have been merged in order to get the root bdm.
1160         self._checks_for_create_and_rebuild(context, image_id, boot_meta,
1161                 instance_type, metadata, injected_files,
1162                 block_device_mapping.root_bdm())
1163 
1164         instance_group = self._get_requested_instance_group(context,
1165                                    filter_properties, check_server_group_quota)
1166 
1167         instances = self._provision_instances(context, instance_type,
1168                 min_count, max_count, base_options, boot_meta, security_groups,
1169                 block_device_mapping, shutdown_terminate,
1170                 instance_group, check_server_group_quota, filter_properties)
1171 
1172         for instance in instances:
1173             self._record_action_start(context, instance,
1174                                       instance_actions.CREATE)
1175 
1176         self.compute_task_api.build_instances(context,
1177                 instances=instances, image=boot_meta,
1178                 filter_properties=filter_properties,
1179                 admin_password=admin_password,
1180                 injected_files=injected_files,
1181                 requested_networks=requested_networks,
1182                 security_groups=security_groups,
1183                 block_device_mapping=block_device_mapping,
1184                 legacy_bdm=False)
1185 
1186         return (instances, reservation_id)
1187 
1188     @staticmethod
1189     def _volume_size(instance_type, bdm):
1190         size = bdm.get('volume_size')
1191         # NOTE (ndipanov): inherit flavor size only for swap and ephemeral
1192         if (size is None and bdm.get('source_type') == 'blank' and
1193                 bdm.get('destination_type') == 'local'):
1194             if bdm.get('guest_format') == 'swap':
1195                 size = instance_type.get('swap', 0)
1196             else:
1197                 size = instance_type.get('ephemeral_gb', 0)
1198         return size
1199 
1200     def _prepare_image_mapping(self, instance_type, mappings):
1201         """Extract and format blank devices from image mappings."""
1202 
1203         prepared_mappings = []
1204 
1205         for bdm in block_device.mappings_prepend_dev(mappings):
1206             LOG.debug("Image bdm %s", bdm)
1207 
1208             virtual_name = bdm['virtual']
1209             if virtual_name == 'ami' or virtual_name == 'root':
1210                 continue
1211 
1212             if not block_device.is_swap_or_ephemeral(virtual_name):
1213                 continue
1214 
1215             guest_format = bdm.get('guest_format')
1216             if virtual_name == 'swap':
1217                 guest_format = 'swap'
1218             if not guest_format:
1219                 guest_format = CONF.default_ephemeral_format
1220 
1221             values = block_device.BlockDeviceDict({
1222                 'device_name': bdm['device'],
1223                 'source_type': 'blank',
1224                 'destination_type': 'local',
1225                 'device_type': 'disk',
1226                 'guest_format': guest_format,
1227                 'delete_on_termination': True,
1228                 'boot_index': -1})
1229 
1230             values['volume_size'] = self._volume_size(
1231                 instance_type, values)
1232             if values['volume_size'] == 0:
1233                 continue
1234 
1235             prepared_mappings.append(values)
1236 
1237         return prepared_mappings
1238 
1239     def _create_block_device_mapping(self, instance_type, instance_uuid,
1240                                      block_device_mapping):
1241         """Create the BlockDeviceMapping objects in the db.
1242 
1243         This method makes a copy of the list in order to avoid using the same
1244         id field in case this is called for multiple instances.
1245         """
1246         LOG.debug("block_device_mapping %s", list(block_device_mapping),
1247                   instance_uuid=instance_uuid)
1248         instance_block_device_mapping = copy.deepcopy(block_device_mapping)
1249         for bdm in instance_block_device_mapping:
1250             bdm.volume_size = self._volume_size(instance_type, bdm)
1251             if bdm.volume_size == 0:
1252                 continue
1253 
1254             bdm.instance_uuid = instance_uuid
1255             bdm.update_or_create()
1256 
1257     def _validate_bdm(self, context, instance, instance_type,
1258                       block_device_mappings):
1259         def _subsequent_list(l):
1260             # Each device which is capable of being used as boot device should
1261             # be given a unique boot index, starting from 0 in ascending order.
1262             return all(el + 1 == l[i + 1] for i, el in enumerate(l[:-1]))
1263 
1264         # Make sure that the boot indexes make sense.
1265         # Setting a negative value or None indicates that the device should not
1266         # be used for booting.
1267         boot_indexes = sorted([bdm.boot_index
1268                                for bdm in block_device_mappings
1269                                if bdm.boot_index is not None
1270                                and bdm.boot_index >= 0])
1271 
1272         if 0 not in boot_indexes or not _subsequent_list(boot_indexes):
1273             # Convert the BlockDeviceMappingList to a list for repr details.
1274             LOG.debug('Invalid block device mapping boot sequence for '
1275                       'instance: %s', list(block_device_mappings),
1276                       instance=instance)
1277             raise exception.InvalidBDMBootSequence()
1278 
1279         for bdm in block_device_mappings:
1280             # NOTE(vish): For now, just make sure the volumes are accessible.
1281             # Additionally, check that the volume can be attached to this
1282             # instance.
1283             snapshot_id = bdm.snapshot_id
1284             volume_id = bdm.volume_id
1285             image_id = bdm.image_id
1286             if (image_id is not None and
1287                     image_id != instance.get('image_ref')):
1288                 try:
1289                     self._get_image(context, image_id)
1290                 except Exception:
1291                     raise exception.InvalidBDMImage(id=image_id)
1292                 if (bdm.source_type == 'image' and
1293                         bdm.destination_type == 'volume' and
1294                         not bdm.volume_size):
1295                     raise exception.InvalidBDM(message=_("Images with "
1296                         "destination_type 'volume' need to have a non-zero "
1297                         "size specified"))
1298             elif volume_id is not None:
1299                 try:
1300                     volume = self.volume_api.get(context, volume_id)
1301                     self.volume_api.check_attach(context,
1302                                                  volume,
1303                                                  instance=instance)
1304                     bdm.volume_size = volume.get('size')
1305                 except (exception.CinderConnectionFailed,
1306                         exception.InvalidVolume):
1307                     raise
1308                 except Exception:
1309                     raise exception.InvalidBDMVolume(id=volume_id)
1310             elif snapshot_id is not None:
1311                 try:
1312                     snap = self.volume_api.get_snapshot(context, snapshot_id)
1313                     bdm.volume_size = bdm.volume_size or snap.get('size')
1314                 except exception.CinderConnectionFailed:
1315                     raise
1316                 except Exception:
1317                     raise exception.InvalidBDMSnapshot(id=snapshot_id)
1318             elif (bdm.source_type == 'blank' and
1319                     bdm.destination_type == 'volume' and
1320                     not bdm.volume_size):
1321                 raise exception.InvalidBDM(message=_("Blank volumes "
1322                     "(source: 'blank', dest: 'volume') need to have non-zero "
1323                     "size"))
1324 
1325         ephemeral_size = sum(bdm.volume_size or 0
1326                 for bdm in block_device_mappings
1327                 if block_device.new_format_is_ephemeral(bdm))
1328         if ephemeral_size > instance_type['ephemeral_gb']:
1329             raise exception.InvalidBDMEphemeralSize()
1330 
1331         # There should be only one swap
1332         swap_list = block_device.get_bdm_swap_list(block_device_mappings)
1333         if len(swap_list) > 1:
1334             msg = _("More than one swap drive requested.")
1335             raise exception.InvalidBDMFormat(details=msg)
1336 
1337         if swap_list:
1338             swap_size = swap_list[0].volume_size or 0
1339             if swap_size > instance_type['swap']:
1340                 raise exception.InvalidBDMSwapSize()
1341 
1342         max_local = CONF.max_local_block_devices
1343         if max_local >= 0:
1344             num_local = len([bdm for bdm in block_device_mappings
1345                              if bdm.destination_type == 'local'])
1346             if num_local > max_local:
1347                 raise exception.InvalidBDMLocalsLimit()
1348 
1349     def _populate_instance_names(self, instance, num_instances):
1350         """Populate instance display_name and hostname."""
1351         display_name = instance.get('display_name')
1352         if instance.obj_attr_is_set('hostname'):
1353             hostname = instance.get('hostname')
1354         else:
1355             hostname = None
1356 
1357         if display_name is None:
1358             display_name = self._default_display_name(instance.uuid)
1359             instance.display_name = display_name
1360 
1361         if hostname is None and num_instances == 1:
1362             # NOTE(russellb) In the multi-instance case, we're going to
1363             # overwrite the display_name using the
1364             # multi_instance_display_name_template.  We need the default
1365             # display_name set so that it can be used in the template, though.
1366             # Only set the hostname here if we're only creating one instance.
1367             # Otherwise, it will be built after the template based
1368             # display_name.
1369             hostname = display_name
1370             default_hostname = self._default_host_name(instance.uuid)
1371             instance.hostname = utils.sanitize_hostname(hostname,
1372                                                         default_hostname)
1373 
1374     def _default_display_name(self, instance_uuid):
1375         return "Server %s" % instance_uuid
1376 
1377     def _default_host_name(self, instance_uuid):
1378         return "Server-%s" % instance_uuid
1379 
1380     def _populate_instance_for_create(self, context, instance, image,
1381                                       index, security_groups, instance_type):
1382         """Build the beginning of a new instance."""
1383 
1384         instance.launch_index = index
1385         instance.vm_state = vm_states.BUILDING
1386         instance.task_state = task_states.SCHEDULING
1387         info_cache = objects.InstanceInfoCache()
1388         info_cache.instance_uuid = instance.uuid
1389         info_cache.network_info = network_model.NetworkInfo()
1390         instance.info_cache = info_cache
1391         instance.flavor = instance_type
1392         instance.old_flavor = None
1393         instance.new_flavor = None
1394         if CONF.ephemeral_storage_encryption.enabled:
1395             instance.ephemeral_key_uuid = self.key_manager.create_key(
1396                 context,
1397                 length=CONF.ephemeral_storage_encryption.key_size)
1398         else:
1399             instance.ephemeral_key_uuid = None
1400 
1401         # Store image properties so we can use them later
1402         # (for notifications, etc).  Only store what we can.
1403         if not instance.obj_attr_is_set('system_metadata'):
1404             instance.system_metadata = {}
1405         # Make sure we have the dict form that we need for instance_update.
1406         instance.system_metadata = utils.instance_sys_meta(instance)
1407 
1408         system_meta = utils.get_system_metadata_from_image(
1409             image, instance_type)
1410 
1411         # In case we couldn't find any suitable base_image
1412         system_meta.setdefault('image_base_image_ref', instance.image_ref)
1413 
1414         instance.system_metadata.update(system_meta)
1415 
1416         instance.security_groups = security_groups
1417 
1418         return instance
1419 
1420     # NOTE(bcwaldon): No policy check since this is only used by scheduler and
1421     # the compute api. That should probably be cleaned up, though.
1422     def create_db_entry_for_new_instance(self, context, instance_type, image,
1423             instance, security_group, block_device_mapping, num_instances,
1424             index, shutdown_terminate=False):
1425         """Create an entry in the DB for this new instance,
1426         including any related table updates (such as security group,
1427         etc).
1428 
1429         This is called by the scheduler after a location for the
1430         instance has been determined.
1431         """
1432         self._populate_instance_for_create(context, instance, image, index,
1433                                            security_group, instance_type)
1434 
1435         self._populate_instance_names(instance, num_instances)
1436 
1437         instance.shutdown_terminate = shutdown_terminate
1438 
1439         self.security_group_api.ensure_default(context)
1440         instance.create()
1441 
1442         if num_instances > 1:
1443             # NOTE(russellb) We wait until this spot to handle
1444             # multi_instance_display_name_template, because we need
1445             # the UUID from the instance.
1446             instance = self._apply_instance_name_template(context, instance,
1447                                                           index)
1448 
1449         # NOTE (ndipanov): This can now raise exceptions but the instance
1450         #                  has been created, so delete it and re-raise so
1451         #                  that other cleanup can happen.
1452         try:
1453             self._validate_bdm(
1454                 context, instance, instance_type, block_device_mapping)
1455         except (exception.CinderConnectionFailed, exception.InvalidBDM,
1456                 exception.InvalidVolume):
1457             with excutils.save_and_reraise_exception():
1458                 instance.destroy()
1459 
1460         self._create_block_device_mapping(
1461                 instance_type, instance.uuid, block_device_mapping)
1462 
1463         return instance
1464 
1465     def _check_create_policies(self, context, availability_zone,
1466             requested_networks, block_device_mapping, forced_host,
1467             forced_node):
1468         """Check policies for create()."""
1469         target = {'project_id': context.project_id,
1470                   'user_id': context.user_id,
1471                   'availability_zone': availability_zone}
1472         if not self.skip_policy_check:
1473             check_policy(context, 'create', target)
1474 
1475             if requested_networks and len(requested_networks):
1476                 check_policy(context, 'create:attach_network', target)
1477 
1478             if block_device_mapping:
1479                 check_policy(context, 'create:attach_volume', target)
1480 
1481             if forced_host or forced_node:
1482                 check_policy(context, 'create:forced_host', {})
1483 
1484     def _check_multiple_instances_neutron_ports(self, requested_networks):
1485         """Check whether multiple instances are created from port id(s)."""
1486         for requested_net in requested_networks:
1487             if requested_net.port_id:
1488                 msg = _("Unable to launch multiple instances with"
1489                         " a single configured port ID. Please launch your"
1490                         " instance one by one with different ports.")
1491                 raise exception.MultiplePortsNotApplicable(reason=msg)
1492 
1493     def _check_multiple_instances_and_specified_ip(self, requested_networks):
1494         """Check whether multiple instances are created with specified ip."""
1495 
1496         for requested_net in requested_networks:
1497             if requested_net.network_id and requested_net.address:
1498                 msg = _("max_count cannot be greater than 1 if an fixed_ip "
1499                         "is specified.")
1500                 raise exception.InvalidFixedIpAndMaxCountRequest(reason=msg)
1501 
1502     @hooks.add_hook("create_instance")
1503     def create(self, context, instance_type,
1504                image_href, kernel_id=None, ramdisk_id=None,
1505                min_count=None, max_count=None,
1506                display_name=None, display_description=None,
1507                key_name=None, key_data=None, security_group=None,
1508                availability_zone=None, forced_host=None, forced_node=None,
1509                user_data=None, metadata=None, injected_files=None,
1510                admin_password=None, block_device_mapping=None,
1511                access_ip_v4=None, access_ip_v6=None, requested_networks=None,
1512                config_drive=None, auto_disk_config=None, scheduler_hints=None,
1513                legacy_bdm=True, shutdown_terminate=False,
1514                check_server_group_quota=False):
1515         """Provision instances, sending instance information to the
1516         scheduler.  The scheduler will determine where the instance(s)
1517         go and will handle creating the DB entries.
1518 
1519         Returns a tuple of (instances, reservation_id)
1520         """
1521         # Check policies up front to fail before performing more expensive work
1522         self._check_create_policies(context, availability_zone,
1523                 requested_networks, block_device_mapping, forced_host,
1524                 forced_node)
1525 
1526         if requested_networks and max_count > 1:
1527             self._check_multiple_instances_and_specified_ip(requested_networks)
1528             if utils.is_neutron():
1529                 self._check_multiple_instances_neutron_ports(
1530                     requested_networks)
1531 
1532         if availability_zone:
1533             available_zones = availability_zones.\
1534                 get_availability_zones(context.elevated(), True)
1535             if forced_host is None and availability_zone not in \
1536                     available_zones:
1537                 msg = _('The requested availability zone is not available')
1538                 raise exception.InvalidRequest(msg)
1539 
1540         filter_properties = scheduler_utils.build_filter_properties(
1541                 scheduler_hints, forced_host, forced_node, instance_type)
1542 
1543         return self._create_instance(
1544                        context, instance_type,
1545                        image_href, kernel_id, ramdisk_id,
1546                        min_count, max_count,
1547                        display_name, display_description,
1548                        key_name, key_data, security_group,
1549                        availability_zone, user_data, metadata,
1550                        injected_files, admin_password,
1551                        access_ip_v4, access_ip_v6,
1552                        requested_networks, config_drive,
1553                        block_device_mapping, auto_disk_config,
1554                        filter_properties=filter_properties,
1555                        legacy_bdm=legacy_bdm,
1556                        shutdown_terminate=shutdown_terminate,
1557                        check_server_group_quota=check_server_group_quota)
1558 
1559     def _check_auto_disk_config(self, instance=None, image=None,
1560                                 **extra_instance_updates):
1561         auto_disk_config = extra_instance_updates.get("auto_disk_config")
1562         if auto_disk_config is None:
1563             return
1564         if not image and not instance:
1565             return
1566 
1567         if image:
1568             image_props = image.get("properties", {})
1569             auto_disk_config_img = \
1570                 utils.get_auto_disk_config_from_image_props(image_props)
1571             image_ref = image.get("id")
1572         else:
1573             sys_meta = utils.instance_sys_meta(instance)
1574             image_ref = sys_meta.get('image_base_image_ref')
1575             auto_disk_config_img = \
1576                 utils.get_auto_disk_config_from_instance(sys_meta=sys_meta)
1577 
1578         self._ensure_auto_disk_config_is_valid(auto_disk_config_img,
1579                                                auto_disk_config,
1580                                                image_ref)
1581 
1582     def _delete(self, context, instance, delete_type, cb, **instance_attrs):
1583         if instance.disable_terminate:
1584             LOG.info(_LI('instance termination disabled'),
1585                      instance=instance)
1586             return
1587 
1588         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
1589                 context, instance.uuid)
1590 
1591         project_id, user_id = quotas_obj.ids_from_instance(context, instance)
1592 
1593         # At these states an instance has a snapshot associate.
1594         if instance.vm_state in (vm_states.SHELVED,
1595                                  vm_states.SHELVED_OFFLOADED):
1596             snapshot_id = instance.system_metadata.get('shelved_image_id')
1597             LOG.info(_LI("Working on deleting snapshot %s "
1598                          "from shelved instance..."),
1599                      snapshot_id, instance=instance)
1600             try:
1601                 self.image_api.delete(context, snapshot_id)
1602             except (exception.ImageNotFound,
1603                     exception.ImageNotAuthorized) as exc:
1604                 LOG.warning(_LW("Failed to delete snapshot "
1605                                 "from shelved instance (%s)."),
1606                             exc.format_message(), instance=instance)
1607             except Exception:
1608                 LOG.exception(_LE("Something wrong happened when trying to "
1609                                   "delete snapshot from shelved instance."),
1610                               instance=instance)
1611 
1612         original_task_state = instance.task_state
1613         quotas = None
1614         try:
1615             # NOTE(maoy): no expected_task_state needs to be set
1616             instance.update(instance_attrs)
1617             instance.progress = 0
1618             instance.save()
1619 
1620             # NOTE(comstud): If we delete the instance locally, we'll
1621             # commit the reservations here.  Otherwise, the manager side
1622             # will commit or rollback the reservations based on success.
1623             quotas = self._create_reservations(context,
1624                                                instance,
1625                                                original_task_state,
1626                                                project_id, user_id)
1627 
1628             if self.cell_type == 'api':
1629                 # NOTE(comstud): If we're in the API cell, we need to
1630                 # skip all remaining logic and just call the callback,
1631                 # which will cause a cast to the child cell.  Also,
1632                 # commit reservations here early until we have a better
1633                 # way to deal with quotas with cells.
1634                 cb(context, instance, bdms, reservations=None)
1635                 quotas.commit()
1636                 return
1637             shelved_offloaded = (instance.vm_state
1638                                  == vm_states.SHELVED_OFFLOADED)
1639             if not instance.host and not shelved_offloaded:
1640                 try:
1641                     compute_utils.notify_about_instance_usage(
1642                             self.notifier, context, instance,
1643                             "%s.start" % delete_type)
1644                     instance.destroy()
1645                     compute_utils.notify_about_instance_usage(
1646                             self.notifier, context, instance,
1647                             "%s.end" % delete_type,
1648                             system_metadata=instance.system_metadata)
1649                     quotas.commit()
1650                     return
1651                 except exception.ObjectActionError:
1652                     instance.refresh()
1653 
1654             if instance.vm_state == vm_states.RESIZED:
1655                 self._confirm_resize_on_deleting(context, instance)
1656 
1657             is_local_delete = True
1658             try:
1659                 if not shelved_offloaded:
1660                     service = objects.Service.get_by_compute_host(
1661                         context.elevated(), instance.host)
1662                     is_local_delete = not self.servicegroup_api.service_is_up(
1663                         service)
1664                 if not is_local_delete:
1665                     if original_task_state in (task_states.DELETING,
1666                                                   task_states.SOFT_DELETING):
1667                         LOG.info(_LI('Instance is already in deleting state, '
1668                                      'ignoring this request'),
1669                                  instance=instance)
1670                         quotas.rollback()
1671                         return
1672                     self._record_action_start(context, instance,
1673                                               instance_actions.DELETE)
1674 
1675                     # NOTE(snikitin): If instance's vm_state is 'soft-delete',
1676                     # we should not count reservations here, because instance
1677                     # in soft-delete vm_state have already had quotas
1678                     # decremented. More details:
1679                     # https://bugs.launchpad.net/nova/+bug/1333145
1680                     if instance.vm_state == vm_states.SOFT_DELETED:
1681                         quotas.rollback()
1682 
1683                     cb(context, instance, bdms,
1684                        reservations=quotas.reservations)
1685             except exception.ComputeHostNotFound:
1686                 pass
1687 
1688             if is_local_delete:
1689                 # If instance is in shelved_offloaded state or compute node
1690                 # isn't up, delete instance from db and clean bdms info and
1691                 # network info
1692                 self._local_delete(context, instance, bdms, delete_type, cb)
1693                 quotas.commit()
1694 
1695         except exception.InstanceNotFound:
1696             # NOTE(comstud): Race condition. Instance already gone.
1697             if quotas:
1698                 quotas.rollback()
1699         except Exception:
1700             with excutils.save_and_reraise_exception():
1701                 if quotas:
1702                     quotas.rollback()
1703 
1704     def _confirm_resize_on_deleting(self, context, instance):
1705         # If in the middle of a resize, use confirm_resize to
1706         # ensure the original instance is cleaned up too
1707         migration = None
1708         for status in ('finished', 'confirming'):
1709             try:
1710                 migration = objects.Migration.get_by_instance_and_status(
1711                         context.elevated(), instance.uuid, status)
1712                 LOG.info(_LI('Found an unconfirmed migration during delete, '
1713                              'id: %(id)s, status: %(status)s'),
1714                          {'id': migration.id,
1715                           'status': migration.status},
1716                          context=context, instance=instance)
1717                 break
1718             except exception.MigrationNotFoundByStatus:
1719                 pass
1720 
1721         if not migration:
1722             LOG.info(_LI('Instance may have been confirmed during delete'),
1723                      context=context, instance=instance)
1724             return
1725 
1726         src_host = migration.source_compute
1727         # Call since this can race with the terminate_instance.
1728         # The resize is done but awaiting confirmation/reversion,
1729         # so there are two cases:
1730         # 1. up-resize: here -instance['vcpus'/'memory_mb'] match
1731         #    the quota usages accounted for this instance,
1732         #    so no further quota adjustment is needed
1733         # 2. down-resize: here -instance['vcpus'/'memory_mb'] are
1734         #    shy by delta(old, new) from the quota usages accounted
1735         #    for this instance, so we must adjust
1736         try:
1737             deltas = compute_utils.downsize_quota_delta(context, instance)
1738         except KeyError:
1739             LOG.info(_LI('Migration %s may have been confirmed during '
1740                          'delete'),
1741                      migration.id, context=context, instance=instance)
1742             return
1743         quotas = compute_utils.reserve_quota_delta(context, deltas, instance)
1744 
1745         self._record_action_start(context, instance,
1746                                   instance_actions.CONFIRM_RESIZE)
1747 
1748         self.compute_rpcapi.confirm_resize(context,
1749                 instance, migration,
1750                 src_host, quotas.reservations,
1751                 cast=False)
1752 
1753     def _create_reservations(self, context, instance, original_task_state,
1754                              project_id, user_id):
1755         # NOTE(wangpan): if the instance is resizing, and the resources
1756         #                are updated to new instance type, we should use
1757         #                the old instance type to create reservation.
1758         # see https://bugs.launchpad.net/nova/+bug/1099729 for more details
1759         if original_task_state in (task_states.RESIZE_MIGRATED,
1760                                    task_states.RESIZE_FINISH):
1761             old_flavor = instance.old_flavor
1762             instance_vcpus = old_flavor.vcpus
1763             vram_mb = old_flavor.extra_specs.get('hw_video:ram_max_mb', 0)
1764             instance_memory_mb = old_flavor.memory_mb + vram_mb
1765         else:
1766             instance_vcpus = instance.vcpus
1767             instance_memory_mb = instance.memory_mb
1768 
1769         quotas = objects.Quotas(context=context)
1770         quotas.reserve(project_id=project_id,
1771                        user_id=user_id,
1772                        instances=-1,
1773                        cores=-instance_vcpus,
1774                        ram=-instance_memory_mb)
1775         return quotas
1776 
1777     def _local_cleanup_bdm_volumes(self, bdms, instance, context):
1778         """The method deletes the bdm records and, if a bdm is a volume, call
1779         the terminate connection and the detach volume via the Volume API.
1780         Note that at this point we do not have the information about the
1781         correct connector so we pass a fake one.
1782         """
1783         elevated = context.elevated()
1784         for bdm in bdms:
1785             if bdm.is_volume:
1786                 # NOTE(vish): We don't have access to correct volume
1787                 #             connector info, so just pass a fake
1788                 #             connector. This can be improved when we
1789                 #             expose get_volume_connector to rpc.
1790                 connector = {'ip': '127.0.0.1', 'initiator': 'iqn.fake'}
1791                 try:
1792                     self.volume_api.terminate_connection(context,
1793                                                          bdm.volume_id,
1794                                                          connector)
1795                     self.volume_api.detach(elevated, bdm.volume_id,
1796                                            instance.uuid)
1797                     if bdm.delete_on_termination:
1798                         self.volume_api.delete(context, bdm.volume_id)
1799                 except Exception as exc:
1800                     err_str = _LW("Ignoring volume cleanup failure due to %s")
1801                     LOG.warn(err_str % exc, instance=instance)
1802             bdm.destroy()
1803 
1804     def _local_delete(self, context, instance, bdms, delete_type, cb):
1805         if instance.vm_state == vm_states.SHELVED_OFFLOADED:
1806             LOG.info(_LI("instance is in SHELVED_OFFLOADED state, cleanup"
1807                          " the instance's info from database."),
1808                      instance=instance)
1809         else:
1810             LOG.warning(_LW("instance's host %s is down, deleting from "
1811                             "database"), instance.host, instance=instance)
1812         compute_utils.notify_about_instance_usage(
1813             self.notifier, context, instance, "%s.start" % delete_type)
1814 
1815         elevated = context.elevated()
1816         if self.cell_type != 'api':
1817             # NOTE(liusheng): In nova-network multi_host scenario,deleting
1818             # network info of the instance may need instance['host'] as
1819             # destination host of RPC call. If instance in SHELVED_OFFLOADED
1820             # state, instance['host'] is None, here, use shelved_host as host
1821             # to deallocate network info and reset instance['host'] after that.
1822             # Here we shouldn't use instance.save(), because this will mislead
1823             # user who may think the instance's host has been changed, and
1824             # actually, the instance.host is always None.
1825             orig_host = instance.host
1826             try:
1827                 if instance.vm_state == vm_states.SHELVED_OFFLOADED:
1828                     sysmeta = getattr(instance,
1829                                       obj_base.get_attrname('system_metadata'))
1830                     instance.host = sysmeta.get('shelved_host')
1831                 self.network_api.deallocate_for_instance(elevated,
1832                                                          instance)
1833             finally:
1834                 instance.host = orig_host
1835 
1836         # cleanup volumes
1837         self._local_cleanup_bdm_volumes(bdms, instance, context)
1838         cb(context, instance, bdms, local=True)
1839         sys_meta = instance.system_metadata
1840         instance.destroy()
1841         compute_utils.notify_about_instance_usage(
1842             self.notifier, context, instance, "%s.end" % delete_type,
1843             system_metadata=sys_meta)
1844 
1845     def _do_delete(self, context, instance, bdms, reservations=None,
1846                    local=False):
1847         if local:
1848             instance.vm_state = vm_states.DELETED
1849             instance.task_state = None
1850             instance.terminated_at = timeutils.utcnow()
1851             instance.save()
1852         else:
1853             self.compute_rpcapi.terminate_instance(context, instance, bdms,
1854                                                    reservations=reservations,
1855                                                    delete_type='delete')
1856 
1857     def _do_force_delete(self, context, instance, bdms, reservations=None,
1858                          local=False):
1859         if local:
1860             instance.vm_state = vm_states.DELETED
1861             instance.task_state = None
1862             instance.terminated_at = timeutils.utcnow()
1863             instance.save()
1864         else:
1865             self.compute_rpcapi.terminate_instance(context, instance, bdms,
1866                                                    reservations=reservations,
1867                                                    delete_type='force_delete')
1868 
1869     def _do_soft_delete(self, context, instance, bdms, reservations=None,
1870                         local=False):
1871         if local:
1872             instance.vm_state = vm_states.SOFT_DELETED
1873             instance.task_state = None
1874             instance.terminated_at = timeutils.utcnow()
1875             instance.save()
1876         else:
1877             self.compute_rpcapi.soft_delete_instance(context, instance,
1878                                                      reservations=reservations)
1879 
1880     # NOTE(maoy): we allow delete to be called no matter what vm_state says.
1881     @wrap_check_policy
1882     @check_instance_lock
1883     @check_instance_cell
1884     @check_instance_state(vm_state=None, task_state=None,
1885                           must_have_launched=True)
1886     def soft_delete(self, context, instance):
1887         """Terminate an instance."""
1888         LOG.debug('Going to try to soft delete instance',
1889                   instance=instance)
1890 
1891         self._delete(context, instance, 'soft_delete', self._do_soft_delete,
1892                      task_state=task_states.SOFT_DELETING,
1893                      deleted_at=timeutils.utcnow())
1894 
1895     def _delete_instance(self, context, instance):
1896         self._delete(context, instance, 'delete', self._do_delete,
1897                      task_state=task_states.DELETING)
1898 
1899     @wrap_check_policy
1900     @check_instance_lock
1901     @check_instance_cell
1902     @check_instance_state(vm_state=None, task_state=None,
1903                           must_have_launched=False)
1904     def delete(self, context, instance):
1905         """Terminate an instance."""
1906         LOG.debug("Going to try to terminate instance", instance=instance)
1907         self._delete_instance(context, instance)
1908 
1909     @wrap_check_policy
1910     @check_instance_lock
1911     @check_instance_state(vm_state=[vm_states.SOFT_DELETED])
1912     def restore(self, context, instance):
1913         """Restore a previously deleted (but not reclaimed) instance."""
1914         # Reserve quotas
1915         flavor = instance.get_flavor()
1916         project_id, user_id = quotas_obj.ids_from_instance(context, instance)
1917         num_instances, quotas = self._check_num_instances_quota(
1918                 context, flavor, 1, 1,
1919                 project_id=project_id, user_id=user_id)
1920 
1921         self._record_action_start(context, instance, instance_actions.RESTORE)
1922 
1923         try:
1924             if instance.host:
1925                 instance.task_state = task_states.RESTORING
1926                 instance.deleted_at = None
1927                 instance.save(expected_task_state=[None])
1928                 self.compute_rpcapi.restore_instance(context, instance)
1929             else:
1930                 instance.vm_state = vm_states.ACTIVE
1931                 instance.task_state = None
1932                 instance.deleted_at = None
1933                 instance.save(expected_task_state=[None])
1934 
1935             quotas.commit()
1936         except Exception:
1937             with excutils.save_and_reraise_exception():
1938                 quotas.rollback()
1939 
1940     @wrap_check_policy
1941     @check_instance_lock
1942     @check_instance_state(must_have_launched=False)
1943     def force_delete(self, context, instance):
1944         """Force delete an instance in any vm_state/task_state."""
1945         self._delete(context, instance, 'force_delete', self._do_force_delete,
1946                      task_state=task_states.DELETING)
1947 
1948     def force_stop(self, context, instance, do_cast=True, clean_shutdown=True):
1949         LOG.debug("Going to try to stop instance", instance=instance)
1950 
1951         instance.task_state = task_states.POWERING_OFF
1952         instance.progress = 0
1953         instance.save(expected_task_state=[None])
1954 
1955         self._record_action_start(context, instance, instance_actions.STOP)
1956 
1957         self.compute_rpcapi.stop_instance(context, instance, do_cast=do_cast,
1958                                           clean_shutdown=clean_shutdown)
1959 
1960     @check_instance_lock
1961     @check_instance_host
1962     @check_instance_cell
1963     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.ERROR])
1964     def stop(self, context, instance, do_cast=True, clean_shutdown=True):
1965         """Stop an instance."""
1966         self.force_stop(context, instance, do_cast, clean_shutdown)
1967 
1968     @check_instance_lock
1969     @check_instance_host
1970     @check_instance_cell
1971     @check_instance_state(vm_state=[vm_states.STOPPED])
1972     def start(self, context, instance):
1973         """Start an instance."""
1974         LOG.debug("Going to try to start instance", instance=instance)
1975 
1976         instance.task_state = task_states.POWERING_ON
1977         instance.save(expected_task_state=[None])
1978 
1979         self._record_action_start(context, instance, instance_actions.START)
1980         # TODO(yamahata): injected_files isn't supported right now.
1981         #                 It is used only for osapi. not for ec2 api.
1982         #                 availability_zone isn't used by run_instance.
1983         self.compute_rpcapi.start_instance(context, instance)
1984 
1985     @check_instance_lock
1986     @check_instance_host
1987     @check_instance_cell
1988     @check_instance_state(vm_state=vm_states.ALLOW_TRIGGER_CRASH_DUMP)
1989     def trigger_crash_dump(self, context, instance):
1990         """Trigger crash dump in an instance."""
1991         LOG.debug("Try to trigger crash dump", instance=instance)
1992 
1993         self._record_action_start(context, instance,
1994                                   instance_actions.TRIGGER_CRASH_DUMP)
1995 
1996         self.compute_rpcapi.trigger_crash_dump(context, instance)
1997 
1998     def get(self, context, instance_id, want_objects=False,
1999             expected_attrs=None):
2000         """Get a single instance with the given instance_id."""
2001         if not expected_attrs:
2002             expected_attrs = []
2003         expected_attrs.extend(['metadata', 'system_metadata',
2004                                'security_groups', 'info_cache'])
2005         # NOTE(ameade): we still need to support integer ids for ec2
2006         try:
2007             if uuidutils.is_uuid_like(instance_id):
2008                 LOG.debug("Fetching instance by UUID",
2009                            instance_uuid=instance_id)
2010                 instance = objects.Instance.get_by_uuid(
2011                     context, instance_id, expected_attrs=expected_attrs)
2012             else:
2013                 LOG.debug("Failed to fetch instance by id %s", instance_id)
2014                 raise exception.InstanceNotFound(instance_id=instance_id)
2015         except exception.InvalidID:
2016             LOG.debug("Invalid instance id %s", instance_id)
2017             raise exception.InstanceNotFound(instance_id=instance_id)
2018 
2019         if not self.skip_policy_check:
2020             check_policy(context, 'get', instance)
2021 
2022         if not want_objects:
2023             instance = obj_base.obj_to_primitive(instance)
2024         return instance
2025 
2026     def get_all(self, context, search_opts=None, limit=None, marker=None,
2027                 want_objects=False, expected_attrs=None, sort_keys=None,
2028                 sort_dirs=None):
2029         """Get all instances filtered by one of the given parameters.
2030 
2031         If there is no filter and the context is an admin, it will retrieve
2032         all instances in the system.
2033 
2034         Deleted instances will be returned by default, unless there is a
2035         search option that says otherwise.
2036 
2037         The results will be sorted based on the list of sort keys in the
2038         'sort_keys' parameter (first value is primary sort key, second value is
2039         secondary sort ket, etc.). For each sort key, the associated sort
2040         direction is based on the list of sort directions in the 'sort_dirs'
2041         parameter.
2042         """
2043 
2044         # TODO(bcwaldon): determine the best argument for target here
2045         target = {
2046             'project_id': context.project_id,
2047             'user_id': context.user_id,
2048         }
2049 
2050         if not self.skip_policy_check:
2051             check_policy(context, "get_all", target)
2052 
2053         if search_opts is None:
2054             search_opts = {}
2055 
2056         LOG.debug("Searching by: %s", str(search_opts))
2057 
2058         # Fixups for the DB call
2059         filters = {}
2060 
2061         def _remap_flavor_filter(flavor_id):
2062             flavor = objects.Flavor.get_by_flavor_id(context, flavor_id)
2063             filters['instance_type_id'] = flavor.id
2064 
2065         def _remap_fixed_ip_filter(fixed_ip):
2066             # Turn fixed_ip into a regexp match. Since '.' matches
2067             # any character, we need to use regexp escaping for it.
2068             filters['ip'] = '^%s$' % fixed_ip.replace('.', '\\.')
2069 
2070         def _remap_metadata_filter(metadata):
2071             filters['metadata'] = jsonutils.loads(metadata)
2072 
2073         def _remap_system_metadata_filter(metadata):
2074             filters['system_metadata'] = jsonutils.loads(metadata)
2075 
2076         # search_option to filter_name mapping.
2077         filter_mapping = {
2078                 'image': 'image_ref',
2079                 'name': 'display_name',
2080                 'tenant_id': 'project_id',
2081                 'flavor': _remap_flavor_filter,
2082                 'fixed_ip': _remap_fixed_ip_filter,
2083                 'metadata': _remap_metadata_filter,
2084                 'system_metadata': _remap_system_metadata_filter}
2085 
2086         # copy from search_opts, doing various remappings as necessary
2087         for opt, value in six.iteritems(search_opts):
2088             # Do remappings.
2089             # Values not in the filter_mapping table are copied as-is.
2090             # If remapping is None, option is not copied
2091             # If the remapping is a string, it is the filter_name to use
2092             try:
2093                 remap_object = filter_mapping[opt]
2094             except KeyError:
2095                 filters[opt] = value
2096             else:
2097                 # Remaps are strings to translate to, or functions to call
2098                 # to do the translating as defined by the table above.
2099                 if isinstance(remap_object, six.string_types):
2100                     filters[remap_object] = value
2101                 else:
2102                     try:
2103                         remap_object(value)
2104 
2105                     # We already know we can't match the filter, so
2106                     # return an empty list
2107                     except ValueError:
2108                         if want_objects:
2109                             return objects.InstanceList()
2110                         else:
2111                             return []
2112 
2113         # IP address filtering cannot be applied at the DB layer, remove any DB
2114         # limit so that it can be applied after the IP filter.
2115         filter_ip = 'ip6' in filters or 'ip' in filters
2116         orig_limit = limit
2117         if filter_ip and limit:
2118             LOG.debug('Removing limit for DB query due to IP filter')
2119             limit = None
2120 
2121         inst_models = self._get_instances_by_filters(context, filters,
2122                 limit=limit, marker=marker, expected_attrs=expected_attrs,
2123                 sort_keys=sort_keys, sort_dirs=sort_dirs)
2124 
2125         if filter_ip:
2126             inst_models = self._ip_filter(inst_models, filters, orig_limit)
2127 
2128         if want_objects:
2129             return inst_models
2130 
2131         # Convert the models to dictionaries
2132         instances = []
2133         for inst_model in inst_models:
2134             instances.append(obj_base.obj_to_primitive(inst_model))
2135 
2136         return instances
2137 
2138     @staticmethod
2139     def _ip_filter(inst_models, filters, limit):
2140         ipv4_f = re.compile(str(filters.get('ip')))
2141         ipv6_f = re.compile(str(filters.get('ip6')))
2142 
2143         def _match_instance(instance):
2144             nw_info = compute_utils.get_nw_info_for_instance(instance)
2145             for vif in nw_info:
2146                 for fixed_ip in vif.fixed_ips():
2147                     address = fixed_ip.get('address')
2148                     if not address:
2149                         continue
2150                     version = fixed_ip.get('version')
2151                     if ((version == 4 and ipv4_f.match(address)) or
2152                         (version == 6 and ipv6_f.match(address))):
2153                         return True
2154             return False
2155 
2156         result_objs = []
2157         for instance in inst_models:
2158             if _match_instance(instance):
2159                 result_objs.append(instance)
2160                 if limit and len(result_objs) == limit:
2161                     break
2162         return objects.InstanceList(objects=result_objs)
2163 
2164     def _get_instances_by_filters(self, context, filters,
2165                                   limit=None, marker=None, expected_attrs=None,
2166                                   sort_keys=None, sort_dirs=None):
2167         fields = ['metadata', 'system_metadata', 'info_cache',
2168                   'security_groups']
2169         if expected_attrs:
2170             fields.extend(expected_attrs)
2171         return objects.InstanceList.get_by_filters(
2172             context, filters=filters, limit=limit, marker=marker,
2173             expected_attrs=fields, sort_keys=sort_keys, sort_dirs=sort_dirs)
2174 
2175     # NOTE(melwitt): We don't check instance lock for backup because lock is
2176     #                intended to prevent accidental change/delete of instances
2177     @wrap_check_policy
2178     @check_instance_cell
2179     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
2180                                     vm_states.PAUSED, vm_states.SUSPENDED])
2181     def backup(self, context, instance, name, backup_type, rotation,
2182                extra_properties=None):
2183         """Backup the given instance
2184 
2185         :param instance: nova.objects.instance.Instance object
2186         :param name: name of the backup
2187         :param backup_type: 'daily' or 'weekly'
2188         :param rotation: int representing how many backups to keep around;
2189             None if rotation shouldn't be used (as in the case of snapshots)
2190         :param extra_properties: dict of extra image properties to include
2191                                  when creating the image.
2192         :returns: A dict containing image metadata
2193         """
2194         props_copy = dict(extra_properties, backup_type=backup_type)
2195 
2196         if self.is_volume_backed_instance(context, instance):
2197             LOG.info(_LI("It's not supported to backup volume backed "
2198                          "instance."), context=context, instance=instance)
2199             raise exception.InvalidRequest()
2200         else:
2201             image_meta = self._create_image(context, instance,
2202                                             name, 'backup',
2203                                             extra_properties=props_copy)
2204 
2205         # NOTE(comstud): Any changes to this method should also be made
2206         # to the backup_instance() method in nova/cells/messaging.py
2207 
2208         instance.task_state = task_states.IMAGE_BACKUP
2209         instance.save(expected_task_state=[None])
2210 
2211         self.compute_rpcapi.backup_instance(context, instance,
2212                                             image_meta['id'],
2213                                             backup_type,
2214                                             rotation)
2215         return image_meta
2216 
2217     # NOTE(melwitt): We don't check instance lock for snapshot because lock is
2218     #                intended to prevent accidental change/delete of instances
2219     @wrap_check_policy
2220     @check_instance_cell
2221     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
2222                                     vm_states.PAUSED, vm_states.SUSPENDED])
2223     def snapshot(self, context, instance, name, extra_properties=None):
2224         """Snapshot the given instance.
2225 
2226         :param instance: nova.objects.instance.Instance object
2227         :param name: name of the snapshot
2228         :param extra_properties: dict of extra image properties to include
2229                                  when creating the image.
2230         :returns: A dict containing image metadata
2231         """
2232         image_meta = self._create_image(context, instance, name,
2233                                         'snapshot',
2234                                         extra_properties=extra_properties)
2235 
2236         # NOTE(comstud): Any changes to this method should also be made
2237         # to the snapshot_instance() method in nova/cells/messaging.py
2238         instance.task_state = task_states.IMAGE_SNAPSHOT_PENDING
2239         instance.save(expected_task_state=[None])
2240 
2241         self.compute_rpcapi.snapshot_instance(context, instance,
2242                                               image_meta['id'])
2243 
2244         return image_meta
2245 
2246     def _create_image(self, context, instance, name, image_type,
2247                       extra_properties=None):
2248         """Create new image entry in the image service.  This new image
2249         will be reserved for the compute manager to upload a snapshot
2250         or backup.
2251 
2252         :param context: security context
2253         :param instance: nova.objects.instance.Instance object
2254         :param name: string for name of the snapshot
2255         :param image_type: snapshot | backup
2256         :param extra_properties: dict of extra image properties to include
2257 
2258         """
2259         properties = {
2260             'instance_uuid': instance.uuid,
2261             'user_id': str(context.user_id),
2262             'image_type': image_type,
2263         }
2264         properties.update(extra_properties or {})
2265 
2266         image_meta = self._initialize_instance_snapshot_metadata(
2267             instance, name, properties)
2268         return self.image_api.create(context, image_meta)
2269 
2270     def _initialize_instance_snapshot_metadata(self, instance, name,
2271                                                extra_properties=None):
2272         """Initialize new metadata for a snapshot of the given instance.
2273 
2274         :param instance: nova.objects.instance.Instance object
2275         :param name: string for name of the snapshot
2276         :param extra_properties: dict of extra metadata properties to include
2277 
2278         :returns: the new instance snapshot metadata
2279         """
2280         image_meta = utils.get_image_from_system_metadata(
2281             instance.system_metadata)
2282         image_meta.update({'name': name,
2283                            'is_public': False})
2284 
2285         # Delete properties that are non-inheritable
2286         properties = image_meta['properties']
2287         for key in CONF.non_inheritable_image_properties:
2288             properties.pop(key, None)
2289 
2290         # The properties in extra_properties have precedence
2291         properties.update(extra_properties or {})
2292 
2293         return image_meta
2294 
2295     # NOTE(melwitt): We don't check instance lock for snapshot because lock is
2296     #                intended to prevent accidental change/delete of instances
2297     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED])
2298     def snapshot_volume_backed(self, context, instance, name,
2299                                extra_properties=None):
2300         """Snapshot the given volume-backed instance.
2301 
2302         :param instance: nova.objects.instance.Instance object
2303         :param name: name of the backup or snapshot
2304         :param extra_properties: dict of extra image properties to include
2305 
2306         :returns: the new image metadata
2307         """
2308         image_meta = self._initialize_instance_snapshot_metadata(
2309             instance, name, extra_properties)
2310         # the new image is simply a bucket of properties (particularly the
2311         # block device mapping, kernel and ramdisk IDs) with no image data,
2312         # hence the zero size
2313         image_meta['size'] = 0
2314         for attr in ('container_format', 'disk_format'):
2315             image_meta.pop(attr, None)
2316         properties = image_meta['properties']
2317         # clean properties before filling
2318         for key in ('block_device_mapping', 'bdm_v2', 'root_device_name'):
2319             properties.pop(key, None)
2320         if instance.root_device_name:
2321             properties['root_device_name'] = instance.root_device_name
2322 
2323         quiesced = False
2324         if instance.vm_state == vm_states.ACTIVE:
2325             try:
2326                 self.compute_rpcapi.quiesce_instance(context, instance)
2327                 quiesced = True
2328             except (exception.InstanceQuiesceNotSupported,
2329                     exception.QemuGuestAgentNotEnabled,
2330                     exception.NovaException, NotImplementedError) as err:
2331                 if strutils.bool_from_string(instance.system_metadata.get(
2332                         'image_os_require_quiesce')):
2333                     raise
2334                 else:
2335                     LOG.info(_LI('Skipping quiescing instance: '
2336                                  '%(reason)s.'), {'reason': err},
2337                              context=context, instance=instance)
2338 
2339         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
2340                 context, instance.uuid)
2341 
2342         mapping = []
2343         for bdm in bdms:
2344             if bdm.no_device:
2345                 continue
2346 
2347             if bdm.is_volume:
2348                 # create snapshot based on volume_id
2349                 volume = self.volume_api.get(context, bdm.volume_id)
2350                 # NOTE(yamahata): Should we wait for snapshot creation?
2351                 #                 Linux LVM snapshot creation completes in
2352                 #                 short time, it doesn't matter for now.
2353                 name = _('snapshot for %s') % image_meta['name']
2354                 LOG.debug('Creating snapshot from volume %s.', volume['id'],
2355                           instance=instance)
2356                 snapshot = self.volume_api.create_snapshot_force(
2357                     context, volume['id'], name, volume['display_description'])
2358                 mapping_dict = block_device.snapshot_from_bdm(snapshot['id'],
2359                                                               bdm)
2360                 mapping_dict = mapping_dict.get_image_mapping()
2361             else:
2362                 mapping_dict = bdm.get_image_mapping()
2363 
2364             mapping.append(mapping_dict)
2365 
2366         if quiesced:
2367             self.compute_rpcapi.unquiesce_instance(context, instance, mapping)
2368 
2369         if mapping:
2370             properties['block_device_mapping'] = mapping
2371             properties['bdm_v2'] = True
2372 
2373         return self.image_api.create(context, image_meta)
2374 
2375     @wrap_check_policy
2376     @check_instance_lock
2377     def reboot(self, context, instance, reboot_type):
2378         """Reboot the given instance."""
2379         if reboot_type == 'SOFT':
2380             self._soft_reboot(context, instance)
2381         else:
2382             self._hard_reboot(context, instance)
2383 
2384     @check_instance_state(vm_state=set(vm_states.ALLOW_SOFT_REBOOT),
2385                           task_state=[None])
2386     def _soft_reboot(self, context, instance):
2387         expected_task_state = [None]
2388         instance.task_state = task_states.REBOOTING
2389         instance.save(expected_task_state=expected_task_state)
2390 
2391         self._record_action_start(context, instance, instance_actions.REBOOT)
2392 
2393         self.compute_rpcapi.reboot_instance(context, instance=instance,
2394                                             block_device_info=None,
2395                                             reboot_type='SOFT')
2396 
2397     @check_instance_state(vm_state=set(vm_states.ALLOW_HARD_REBOOT),
2398                           task_state=task_states.ALLOW_REBOOT)
2399     def _hard_reboot(self, context, instance):
2400         instance.task_state = task_states.REBOOTING_HARD
2401         expected_task_state = [None,
2402                                task_states.REBOOTING,
2403                                task_states.REBOOT_PENDING,
2404                                task_states.REBOOT_STARTED,
2405                                task_states.REBOOTING_HARD,
2406                                task_states.RESUMING,
2407                                task_states.UNPAUSING,
2408                                task_states.SUSPENDING]
2409         instance.save(expected_task_state = expected_task_state)
2410 
2411         self._record_action_start(context, instance, instance_actions.REBOOT)
2412 
2413         self.compute_rpcapi.reboot_instance(context, instance=instance,
2414                                             block_device_info=None,
2415                                             reboot_type='HARD')
2416 
2417     @wrap_check_policy
2418     @check_instance_lock
2419     @check_instance_cell
2420     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
2421                                     vm_states.ERROR])
2422     def rebuild(self, context, instance, image_href, admin_password,
2423                 files_to_inject=None, **kwargs):
2424         """Rebuild the given instance with the provided attributes."""
2425         orig_image_ref = instance.image_ref or ''
2426         files_to_inject = files_to_inject or []
2427         metadata = kwargs.get('metadata', {})
2428         preserve_ephemeral = kwargs.get('preserve_ephemeral', False)
2429         auto_disk_config = kwargs.get('auto_disk_config')
2430 
2431         image_id, image = self._get_image(context, image_href)
2432         self._check_auto_disk_config(image=image, **kwargs)
2433 
2434         flavor = instance.get_flavor()
2435         root_bdm = self._get_root_bdm(context, instance)
2436         self._checks_for_create_and_rebuild(context, image_id, image,
2437                 flavor, metadata, files_to_inject, root_bdm)
2438 
2439         kernel_id, ramdisk_id = self._handle_kernel_and_ramdisk(
2440                 context, None, None, image)
2441 
2442         def _reset_image_metadata():
2443             """Remove old image properties that we're storing as instance
2444             system metadata.  These properties start with 'image_'.
2445             Then add the properties for the new image.
2446             """
2447             # FIXME(comstud): There's a race condition here in that if
2448             # the system_metadata for this instance is updated after
2449             # we do the previous save() and before we update.. those
2450             # other updates will be lost. Since this problem exists in
2451             # a lot of other places, I think it should be addressed in
2452             # a DB layer overhaul.
2453 
2454             orig_sys_metadata = dict(instance.system_metadata)
2455             # Remove the old keys
2456             for key in list(instance.system_metadata.keys()):
2457                 if key.startswith(utils.SM_IMAGE_PROP_PREFIX):
2458                     del instance.system_metadata[key]
2459 
2460             # Add the new ones
2461             new_sys_metadata = utils.get_system_metadata_from_image(
2462                 image, flavor)
2463 
2464             instance.system_metadata.update(new_sys_metadata)
2465             instance.save()
2466             return orig_sys_metadata
2467 
2468         # Since image might have changed, we may have new values for
2469         # os_type, vm_mode, etc
2470         options_from_image = self._inherit_properties_from_image(
2471                 image, auto_disk_config)
2472         instance.update(options_from_image)
2473 
2474         instance.task_state = task_states.REBUILDING
2475         instance.image_ref = image_href
2476         instance.kernel_id = kernel_id or ""
2477         instance.ramdisk_id = ramdisk_id or ""
2478         instance.progress = 0
2479         instance.update(kwargs)
2480         instance.save(expected_task_state=[None])
2481 
2482         # On a rebuild, since we're potentially changing images, we need to
2483         # wipe out the old image properties that we're storing as instance
2484         # system metadata... and copy in the properties for the new image.
2485         orig_sys_metadata = _reset_image_metadata()
2486 
2487         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
2488                 context, instance.uuid)
2489 
2490         self._record_action_start(context, instance, instance_actions.REBUILD)
2491 
2492         self.compute_task_api.rebuild_instance(context, instance=instance,
2493                 new_pass=admin_password, injected_files=files_to_inject,
2494                 image_ref=image_href, orig_image_ref=orig_image_ref,
2495                 orig_sys_metadata=orig_sys_metadata, bdms=bdms,
2496                 preserve_ephemeral=preserve_ephemeral, host=instance.host,
2497                 kwargs=kwargs)
2498 
2499     @wrap_check_policy
2500     @check_instance_lock
2501     @check_instance_cell
2502     @check_instance_state(vm_state=[vm_states.RESIZED])
2503     def revert_resize(self, context, instance):
2504         """Reverts a resize, deleting the 'new' instance in the process."""
2505         elevated = context.elevated()
2506         migration = objects.Migration.get_by_instance_and_status(
2507             elevated, instance.uuid, 'finished')
2508 
2509         # reverse quota reservation for increased resource usage
2510         deltas = compute_utils.reverse_upsize_quota_delta(context, migration)
2511         quotas = compute_utils.reserve_quota_delta(context, deltas, instance)
2512 
2513         instance.task_state = task_states.RESIZE_REVERTING
2514         try:
2515             instance.save(expected_task_state=[None])
2516         except Exception:
2517             with excutils.save_and_reraise_exception():
2518                 quotas.rollback()
2519 
2520         migration.status = 'reverting'
2521         migration.save()
2522         # With cells, the best we can do right now is commit the reservations
2523         # immediately...
2524         if CONF.cells.enable:
2525             quotas.commit()
2526 
2527         self._record_action_start(context, instance,
2528                                   instance_actions.REVERT_RESIZE)
2529 
2530         self.compute_rpcapi.revert_resize(context, instance,
2531                                           migration,
2532                                           migration.dest_compute,
2533                                           quotas.reservations or [])
2534 
2535     @wrap_check_policy
2536     @check_instance_lock
2537     @check_instance_cell
2538     @check_instance_state(vm_state=[vm_states.RESIZED])
2539     def confirm_resize(self, context, instance, migration=None):
2540         """Confirms a migration/resize and deletes the 'old' instance."""
2541         elevated = context.elevated()
2542         if migration is None:
2543             migration = objects.Migration.get_by_instance_and_status(
2544                 elevated, instance.uuid, 'finished')
2545 
2546         # reserve quota only for any decrease in resource usage
2547         deltas = compute_utils.downsize_quota_delta(context, instance)
2548         quotas = compute_utils.reserve_quota_delta(context, deltas, instance)
2549 
2550         migration.status = 'confirming'
2551         migration.save()
2552         # With cells, the best we can do right now is commit the reservations
2553         # immediately...
2554         if CONF.cells.enable:
2555             quotas.commit()
2556 
2557         self._record_action_start(context, instance,
2558                                   instance_actions.CONFIRM_RESIZE)
2559 
2560         self.compute_rpcapi.confirm_resize(context,
2561                                            instance,
2562                                            migration,
2563                                            migration.source_compute,
2564                                            quotas.reservations or [])
2565 
2566     @staticmethod
2567     def _resize_cells_support(context, quotas, instance,
2568                               current_instance_type, new_instance_type):
2569         """Special API cell logic for resize."""
2570         # With cells, the best we can do right now is commit the
2571         # reservations immediately...
2572         quotas.commit()
2573         # NOTE(johannes/comstud): The API cell needs a local migration
2574         # record for later resize_confirm and resize_reverts to deal
2575         # with quotas.  We don't need source and/or destination
2576         # information, just the old and new flavors. Status is set to
2577         # 'finished' since nothing else will update the status along
2578         # the way.
2579         mig = objects.Migration(context=context.elevated())
2580         mig.instance_uuid = instance.uuid
2581         mig.old_instance_type_id = current_instance_type['id']
2582         mig.new_instance_type_id = new_instance_type['id']
2583         mig.status = 'finished'
2584         mig.migration_type = (
2585             mig.old_instance_type_id != mig.new_instance_type_id and
2586             'resize' or 'migration')
2587         mig.create()
2588 
2589     @wrap_check_policy
2590     @check_instance_lock
2591     @check_instance_cell
2592     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED])
2593     def resize(self, context, instance, flavor_id=None, clean_shutdown=True,
2594                **extra_instance_updates):
2595         """Resize (ie, migrate) a running instance.
2596 
2597         If flavor_id is None, the process is considered a migration, keeping
2598         the original flavor_id. If flavor_id is not None, the instance should
2599         be migrated to a new host and resized to the new flavor_id.
2600         """
2601         self._check_auto_disk_config(instance, **extra_instance_updates)
2602 
2603         current_instance_type = instance.get_flavor()
2604 
2605         # If flavor_id is not provided, only migrate the instance.
2606         if not flavor_id:
2607             LOG.debug("flavor_id is None. Assuming migration.",
2608                       instance=instance)
2609             new_instance_type = current_instance_type
2610         else:
2611             new_instance_type = flavors.get_flavor_by_flavor_id(
2612                     flavor_id, read_deleted="no")
2613             if (new_instance_type.get('root_gb') == 0 and
2614                 current_instance_type.get('root_gb') != 0 and
2615                 not self.is_volume_backed_instance(context, instance)):
2616                 reason = _('Resize to zero disk flavor is not allowed.')
2617                 raise exception.CannotResizeDisk(reason=reason)
2618 
2619         if not new_instance_type:
2620             raise exception.FlavorNotFound(flavor_id=flavor_id)
2621 
2622         current_instance_type_name = current_instance_type['name']
2623         new_instance_type_name = new_instance_type['name']
2624         LOG.debug("Old instance type %(current_instance_type_name)s, "
2625                   "new instance type %(new_instance_type_name)s",
2626                   {'current_instance_type_name': current_instance_type_name,
2627                    'new_instance_type_name': new_instance_type_name},
2628                   instance=instance)
2629 
2630         same_instance_type = (current_instance_type['id'] ==
2631                               new_instance_type['id'])
2632 
2633         # NOTE(sirp): We don't want to force a customer to change their flavor
2634         # when Ops is migrating off of a failed host.
2635         if not same_instance_type and new_instance_type.get('disabled'):
2636             raise exception.FlavorNotFound(flavor_id=flavor_id)
2637 
2638         if same_instance_type and flavor_id and self.cell_type != 'compute':
2639             raise exception.CannotResizeToSameFlavor()
2640 
2641         # ensure there is sufficient headroom for upsizes
2642         if flavor_id:
2643             deltas = compute_utils.upsize_quota_delta(context,
2644                                                       new_instance_type,
2645                                                       current_instance_type)
2646             try:
2647                 quotas = compute_utils.reserve_quota_delta(context, deltas,
2648                                                            instance)
2649             except exception.OverQuota as exc:
2650                 quotas = exc.kwargs['quotas']
2651                 overs = exc.kwargs['overs']
2652                 usages = exc.kwargs['usages']
2653                 headroom = self._get_headroom(quotas, usages, deltas)
2654                 (overs, reqs, total_alloweds,
2655                  useds) = self._get_over_quota_detail(headroom, overs, quotas,
2656                                                       deltas)
2657                 LOG.warning(_LW("%(overs)s quota exceeded for %(pid)s,"
2658                                 " tried to resize instance."),
2659                             {'overs': overs, 'pid': context.project_id})
2660                 raise exception.TooManyInstances(overs=overs,
2661                                                  req=reqs,
2662                                                  used=useds,
2663                                                  allowed=total_alloweds)
2664         else:
2665             quotas = objects.Quotas(context=context)
2666 
2667         instance.task_state = task_states.RESIZE_PREP
2668         instance.progress = 0
2669         instance.update(extra_instance_updates)
2670         instance.save(expected_task_state=[None])
2671 
2672         filter_properties = {'ignore_hosts': []}
2673 
2674         if not CONF.allow_resize_to_same_host:
2675             filter_properties['ignore_hosts'].append(instance.host)
2676 
2677         if self.cell_type == 'api':
2678             # Commit reservations early and create migration record.
2679             self._resize_cells_support(context, quotas, instance,
2680                                        current_instance_type,
2681                                        new_instance_type)
2682 
2683         if not flavor_id:
2684             self._record_action_start(context, instance,
2685                                       instance_actions.MIGRATE)
2686         else:
2687             self._record_action_start(context, instance,
2688                                       instance_actions.RESIZE)
2689 
2690         scheduler_hint = {'filter_properties': filter_properties}
2691         self.compute_task_api.resize_instance(context, instance,
2692                 extra_instance_updates, scheduler_hint=scheduler_hint,
2693                 flavor=new_instance_type,
2694                 reservations=quotas.reservations or [],
2695                 clean_shutdown=clean_shutdown)
2696 
2697     @wrap_check_policy
2698     @check_instance_lock
2699     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
2700                                     vm_states.PAUSED, vm_states.SUSPENDED])
2701     def shelve(self, context, instance, clean_shutdown=True):
2702         """Shelve an instance.
2703 
2704         Shuts down an instance and frees it up to be removed from the
2705         hypervisor.
2706         """
2707         instance.task_state = task_states.SHELVING
2708         instance.save(expected_task_state=[None])
2709 
2710         self._record_action_start(context, instance, instance_actions.SHELVE)
2711 
2712         if not self.is_volume_backed_instance(context, instance):
2713             name = '%s-shelved' % instance.display_name
2714             image_meta = self._create_image(context, instance, name,
2715                     'snapshot')
2716             image_id = image_meta['id']
2717             self.compute_rpcapi.shelve_instance(context, instance=instance,
2718                     image_id=image_id, clean_shutdown=clean_shutdown)
2719         else:
2720             self.compute_rpcapi.shelve_offload_instance(context,
2721                     instance=instance, clean_shutdown=clean_shutdown)
2722 
2723     @wrap_check_policy
2724     @check_instance_lock
2725     @check_instance_state(vm_state=[vm_states.SHELVED])
2726     def shelve_offload(self, context, instance, clean_shutdown=True):
2727         """Remove a shelved instance from the hypervisor."""
2728         instance.task_state = task_states.SHELVING_OFFLOADING
2729         instance.save(expected_task_state=[None])
2730 
2731         self.compute_rpcapi.shelve_offload_instance(context, instance=instance,
2732             clean_shutdown=clean_shutdown)
2733 
2734     @wrap_check_policy
2735     @check_instance_lock
2736     @check_instance_state(vm_state=[vm_states.SHELVED,
2737         vm_states.SHELVED_OFFLOADED])
2738     def unshelve(self, context, instance):
2739         """Restore a shelved instance."""
2740         instance.task_state = task_states.UNSHELVING
2741         instance.save(expected_task_state=[None])
2742 
2743         self._record_action_start(context, instance, instance_actions.UNSHELVE)
2744 
2745         try:
2746             request_spec = objects.RequestSpec.get_by_instance_uuid(
2747                 context, instance.uuid)
2748         except exception.RequestSpecNotFound:
2749             # Some old instances can still have no RequestSpec object attached
2750             # to them, we need to support the old way
2751             request_spec = None
2752         self.compute_task_api.unshelve_instance(context, instance,
2753                                                 request_spec)
2754 
2755     @wrap_check_policy
2756     @check_instance_lock
2757     def add_fixed_ip(self, context, instance, network_id):
2758         """Add fixed_ip from specified network to given instance."""
2759         self.compute_rpcapi.add_fixed_ip_to_instance(context,
2760                 instance=instance, network_id=network_id)
2761 
2762     @wrap_check_policy
2763     @check_instance_lock
2764     def remove_fixed_ip(self, context, instance, address):
2765         """Remove fixed_ip from specified network to given instance."""
2766         self.compute_rpcapi.remove_fixed_ip_from_instance(context,
2767                 instance=instance, address=address)
2768 
2769     @wrap_check_policy
2770     @check_instance_lock
2771     @check_instance_cell
2772     @check_instance_state(vm_state=[vm_states.ACTIVE])
2773     def pause(self, context, instance):
2774         """Pause the given instance."""
2775         instance.task_state = task_states.PAUSING
2776         instance.save(expected_task_state=[None])
2777         self._record_action_start(context, instance, instance_actions.PAUSE)
2778         self.compute_rpcapi.pause_instance(context, instance)
2779 
2780     @wrap_check_policy
2781     @check_instance_lock
2782     @check_instance_cell
2783     @check_instance_state(vm_state=[vm_states.PAUSED])
2784     def unpause(self, context, instance):
2785         """Unpause the given instance."""
2786         instance.task_state = task_states.UNPAUSING
2787         instance.save(expected_task_state=[None])
2788         self._record_action_start(context, instance, instance_actions.UNPAUSE)
2789         self.compute_rpcapi.unpause_instance(context, instance)
2790 
2791     @wrap_check_policy
2792     def get_diagnostics(self, context, instance):
2793         """Retrieve diagnostics for the given instance."""
2794         return self.compute_rpcapi.get_diagnostics(context, instance=instance)
2795 
2796     @wrap_check_policy
2797     def get_instance_diagnostics(self, context, instance):
2798         """Retrieve diagnostics for the given instance."""
2799         return self.compute_rpcapi.get_instance_diagnostics(context,
2800                                                             instance=instance)
2801 
2802     @wrap_check_policy
2803     @check_instance_lock
2804     @check_instance_cell
2805     @check_instance_state(vm_state=[vm_states.ACTIVE])
2806     def suspend(self, context, instance):
2807         """Suspend the given instance."""
2808         instance.task_state = task_states.SUSPENDING
2809         instance.save(expected_task_state=[None])
2810         self._record_action_start(context, instance, instance_actions.SUSPEND)
2811         self.compute_rpcapi.suspend_instance(context, instance)
2812 
2813     @wrap_check_policy
2814     @check_instance_lock
2815     @check_instance_cell
2816     @check_instance_state(vm_state=[vm_states.SUSPENDED])
2817     def resume(self, context, instance):
2818         """Resume the given instance."""
2819         instance.task_state = task_states.RESUMING
2820         instance.save(expected_task_state=[None])
2821         self._record_action_start(context, instance, instance_actions.RESUME)
2822         self.compute_rpcapi.resume_instance(context, instance)
2823 
2824     @wrap_check_policy
2825     @check_instance_lock
2826     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
2827                                     vm_states.ERROR])
2828     def rescue(self, context, instance, rescue_password=None,
2829                rescue_image_ref=None, clean_shutdown=True):
2830         """Rescue the given instance."""
2831 
2832         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
2833                     context, instance.uuid)
2834         for bdm in bdms:
2835             if bdm.volume_id:
2836                 vol = self.volume_api.get(context, bdm.volume_id)
2837                 self.volume_api.check_attached(context, vol)
2838         if self.is_volume_backed_instance(context, instance, bdms):
2839             reason = _("Cannot rescue a volume-backed instance")
2840             raise exception.InstanceNotRescuable(instance_id=instance.uuid,
2841                                                  reason=reason)
2842 
2843         instance.task_state = task_states.RESCUING
2844         instance.save(expected_task_state=[None])
2845 
2846         self._record_action_start(context, instance, instance_actions.RESCUE)
2847 
2848         self.compute_rpcapi.rescue_instance(context, instance=instance,
2849             rescue_password=rescue_password, rescue_image_ref=rescue_image_ref,
2850             clean_shutdown=clean_shutdown)
2851 
2852     @wrap_check_policy
2853     @check_instance_lock
2854     @check_instance_state(vm_state=[vm_states.RESCUED])
2855     def unrescue(self, context, instance):
2856         """Unrescue the given instance."""
2857         instance.task_state = task_states.UNRESCUING
2858         instance.save(expected_task_state=[None])
2859 
2860         self._record_action_start(context, instance, instance_actions.UNRESCUE)
2861 
2862         self.compute_rpcapi.unrescue_instance(context, instance=instance)
2863 
2864     @wrap_check_policy
2865     @check_instance_lock
2866     @check_instance_cell
2867     @check_instance_state(vm_state=[vm_states.ACTIVE])
2868     def set_admin_password(self, context, instance, password=None):
2869         """Set the root/admin password for the given instance.
2870 
2871         @param context: Nova auth context.
2872         @param instance: Nova instance object.
2873         @param password: The admin password for the instance.
2874         """
2875         instance.task_state = task_states.UPDATING_PASSWORD
2876         instance.save(expected_task_state=[None])
2877 
2878         self._record_action_start(context, instance,
2879                                   instance_actions.CHANGE_PASSWORD)
2880 
2881         self.compute_rpcapi.set_admin_password(context,
2882                                                instance=instance,
2883                                                new_pass=password)
2884 
2885     @wrap_check_policy
2886     @check_instance_host
2887     def get_vnc_console(self, context, instance, console_type):
2888         """Get a url to an instance Console."""
2889         connect_info = self.compute_rpcapi.get_vnc_console(context,
2890                 instance=instance, console_type=console_type)
2891 
2892         self.consoleauth_rpcapi.authorize_console(context,
2893                 connect_info['token'], console_type,
2894                 connect_info['host'], connect_info['port'],
2895                 connect_info['internal_access_path'], instance.uuid,
2896                 access_url=connect_info['access_url'])
2897 
2898         return {'url': connect_info['access_url']}
2899 
2900     @check_instance_host
2901     def get_vnc_connect_info(self, context, instance, console_type):
2902         """Used in a child cell to get console info."""
2903         connect_info = self.compute_rpcapi.get_vnc_console(context,
2904                 instance=instance, console_type=console_type)
2905         return connect_info
2906 
2907     @wrap_check_policy
2908     @check_instance_host
2909     def get_spice_console(self, context, instance, console_type):
2910         """Get a url to an instance Console."""
2911         connect_info = self.compute_rpcapi.get_spice_console(context,
2912                 instance=instance, console_type=console_type)
2913         self.consoleauth_rpcapi.authorize_console(context,
2914                 connect_info['token'], console_type,
2915                 connect_info['host'], connect_info['port'],
2916                 connect_info['internal_access_path'], instance.uuid,
2917                 access_url=connect_info['access_url'])
2918 
2919         return {'url': connect_info['access_url']}
2920 
2921     @check_instance_host
2922     def get_spice_connect_info(self, context, instance, console_type):
2923         """Used in a child cell to get console info."""
2924         connect_info = self.compute_rpcapi.get_spice_console(context,
2925                 instance=instance, console_type=console_type)
2926         return connect_info
2927 
2928     @wrap_check_policy
2929     @check_instance_host
2930     def get_rdp_console(self, context, instance, console_type):
2931         """Get a url to an instance Console."""
2932         connect_info = self.compute_rpcapi.get_rdp_console(context,
2933                 instance=instance, console_type=console_type)
2934         self.consoleauth_rpcapi.authorize_console(context,
2935                 connect_info['token'], console_type,
2936                 connect_info['host'], connect_info['port'],
2937                 connect_info['internal_access_path'], instance.uuid,
2938                 access_url=connect_info['access_url'])
2939 
2940         return {'url': connect_info['access_url']}
2941 
2942     @check_instance_host
2943     def get_rdp_connect_info(self, context, instance, console_type):
2944         """Used in a child cell to get console info."""
2945         connect_info = self.compute_rpcapi.get_rdp_console(context,
2946                 instance=instance, console_type=console_type)
2947         return connect_info
2948 
2949     @wrap_check_policy
2950     @check_instance_host
2951     def get_serial_console(self, context, instance, console_type):
2952         """Get a url to a serial console."""
2953         connect_info = self.compute_rpcapi.get_serial_console(context,
2954                 instance=instance, console_type=console_type)
2955 
2956         self.consoleauth_rpcapi.authorize_console(context,
2957                 connect_info['token'], console_type,
2958                 connect_info['host'], connect_info['port'],
2959                 connect_info['internal_access_path'], instance.uuid,
2960                 access_url=connect_info['access_url'])
2961         return {'url': connect_info['access_url']}
2962 
2963     @check_instance_host
2964     def get_serial_console_connect_info(self, context, instance, console_type):
2965         """Used in a child cell to get serial console."""
2966         connect_info = self.compute_rpcapi.get_serial_console(context,
2967                 instance=instance, console_type=console_type)
2968         return connect_info
2969 
2970     @wrap_check_policy
2971     @check_instance_host
2972     def get_mks_console(self, context, instance, console_type):
2973         """Get a url to a MKS console."""
2974         connect_info = self.compute_rpcapi.get_mks_console(context,
2975                 instance=instance, console_type=console_type)
2976         self.consoleauth_rpcapi.authorize_console(context,
2977                 connect_info['token'], console_type,
2978                 connect_info['host'], connect_info['port'],
2979                 connect_info['internal_access_path'], instance.uuid,
2980                 access_url=connect_info['access_url'])
2981         return {'url': connect_info['access_url']}
2982 
2983     @wrap_check_policy
2984     @check_instance_host
2985     def get_console_output(self, context, instance, tail_length=None):
2986         """Get console output for an instance."""
2987         return self.compute_rpcapi.get_console_output(context,
2988                 instance=instance, tail_length=tail_length)
2989 
2990     @wrap_check_policy
2991     def lock(self, context, instance):
2992         """Lock the given instance."""
2993         # Only update the lock if we are an admin (non-owner)
2994         is_owner = instance.project_id == context.project_id
2995         if instance.locked and is_owner:
2996             return
2997 
2998         context = context.elevated()
2999         LOG.debug('Locking', context=context, instance=instance)
3000         instance.locked = True
3001         instance.locked_by = 'owner' if is_owner else 'admin'
3002         instance.save()
3003 
3004     def is_expected_locked_by(self, context, instance):
3005         is_owner = instance.project_id == context.project_id
3006         expect_locked_by = 'owner' if is_owner else 'admin'
3007         locked_by = instance.locked_by
3008         if locked_by and locked_by != expect_locked_by:
3009             return False
3010         return True
3011 
3012     @wrap_check_policy
3013     def unlock(self, context, instance):
3014         """Unlock the given instance."""
3015         # If the instance was locked by someone else, check
3016         # that we're allowed to override the lock
3017         if not self.skip_policy_check and not self.is_expected_locked_by(
3018             context, instance):
3019             check_policy(context, 'unlock_override', instance)
3020 
3021         context = context.elevated()
3022         LOG.debug('Unlocking', context=context, instance=instance)
3023         instance.locked = False
3024         instance.locked_by = None
3025         instance.save()
3026 
3027     @wrap_check_policy
3028     @check_instance_lock
3029     @check_instance_cell
3030     def reset_network(self, context, instance):
3031         """Reset networking on the instance."""
3032         self.compute_rpcapi.reset_network(context, instance=instance)
3033 
3034     @wrap_check_policy
3035     @check_instance_lock
3036     @check_instance_cell
3037     def inject_network_info(self, context, instance):
3038         """Inject network info for the instance."""
3039         self.compute_rpcapi.inject_network_info(context, instance=instance)
3040 
3041     def _create_volume_bdm(self, context, instance, device, volume_id,
3042                            disk_bus, device_type, is_local_creation=False):
3043         if is_local_creation:
3044             # when the creation is done locally we can't specify the device
3045             # name as we do not have a way to check that the name specified is
3046             # a valid one.
3047             # We leave the setting of that value when the actual attach
3048             # happens on the compute manager
3049             volume_bdm = objects.BlockDeviceMapping(
3050                 context=context,
3051                 source_type='volume', destination_type='volume',
3052                 instance_uuid=instance.uuid, boot_index=None,
3053                 volume_id=volume_id or 'reserved',
3054                 device_name=None, guest_format=None,
3055                 disk_bus=disk_bus, device_type=device_type)
3056             volume_bdm.create()
3057         else:
3058             # NOTE(vish): This is done on the compute host because we want
3059             #             to avoid a race where two devices are requested at
3060             #             the same time. When db access is removed from
3061             #             compute, the bdm will be created here and we will
3062             #             have to make sure that they are assigned atomically.
3063             volume_bdm = self.compute_rpcapi.reserve_block_device_name(
3064                 context, instance, device, volume_id, disk_bus=disk_bus,
3065                 device_type=device_type)
3066         return volume_bdm
3067 
3068     @contextlib.contextmanager
3069     def _check_attach_and_reserve_volume(self, context, volume_id, instance):
3070         volume = self.volume_api.get(context, volume_id)
3071         self.volume_api.check_attach(context, volume, instance=instance)
3072         self.volume_api.reserve_volume(context, volume_id)
3073         try:
3074             yield
3075         except Exception:
3076             with excutils.save_and_reraise_exception():
3077                 LOG.info(_LI("Unreserving a reserved volume %(vol)s, due to "
3078                              "an exception."), {'vol': volume_id})
3079                 self.volume_api.unreserve_volume(context, volume_id)
3080 
3081     def _attach_volume(self, context, instance, volume_id, device,
3082                        disk_bus, device_type):
3083         """Attach an existing volume to an existing instance.
3084 
3085         This method is separated to make it possible for cells version
3086         to override it.
3087         """
3088         with self._check_attach_and_reserve_volume(
3089                 context, volume_id, instance):
3090             volume_bdm = self._create_volume_bdm(
3091                 context, instance, device, volume_id, disk_bus=disk_bus,
3092                 device_type=device_type)
3093         try:
3094             self.compute_rpcapi.attach_volume(context, instance, volume_bdm)
3095         except Exception:
3096             with excutils.save_and_reraise_exception():
3097                 volume_bdm.destroy()
3098 
3099         return volume_bdm.device_name
3100 
3101     def _attach_volume_shelved_offloaded(self, context, instance, volume_id,
3102                                          device, disk_bus, device_type):
3103         """Attach an existing volume to an instance in shelved offloaded state.
3104 
3105         Attaching a volume for an instance in shelved offloaded state requires
3106         to perform the regular check to see if we can attach and reserve the
3107         volume then we need to call the attach method on the volume API
3108         to mark the volume as 'in-use'.
3109         The instance at this stage is not managed by a compute manager
3110         therefore the actual attachment will be performed once the
3111         instance will be unshelved.
3112         """
3113 
3114         with self._check_attach_and_reserve_volume(
3115                 context, volume_id, instance):
3116             volume_bdm = self._create_volume_bdm(
3117                 context, instance, device, volume_id, disk_bus=disk_bus,
3118                 device_type=device_type, is_local_creation=True)
3119         try:
3120             self.volume_api.attach(context,
3121                                    volume_id,
3122                                    instance.uuid,
3123                                    device)
3124         except Exception:
3125             with excutils.save_and_reraise_exception():
3126                 volume_bdm.destroy()
3127 
3128         return volume_bdm.device_name
3129 
3130     @wrap_check_policy
3131     @check_instance_lock
3132     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
3133                                     vm_states.STOPPED, vm_states.RESIZED,
3134                                     vm_states.SOFT_DELETED, vm_states.SHELVED,
3135                                     vm_states.SHELVED_OFFLOADED])
3136     def attach_volume(self, context, instance, volume_id, device=None,
3137                        disk_bus=None, device_type=None):
3138         """Attach an existing volume to an existing instance."""
3139         # NOTE(vish): Fail fast if the device is not going to pass. This
3140         #             will need to be removed along with the test if we
3141         #             change the logic in the manager for what constitutes
3142         #             a valid device.
3143         if device and not block_device.match_device(device):
3144             raise exception.InvalidDevicePath(path=device)
3145 
3146         is_shelved_offloaded = instance.vm_state == vm_states.SHELVED_OFFLOADED
3147         if is_shelved_offloaded:
3148             return self._attach_volume_shelved_offloaded(context,
3149                                                          instance,
3150                                                          volume_id,
3151                                                          device,
3152                                                          disk_bus,
3153                                                          device_type)
3154 
3155         return self._attach_volume(context, instance, volume_id, device,
3156                                    disk_bus, device_type)
3157 
3158     def _check_and_begin_detach(self, context, volume, instance):
3159         self.volume_api.check_detach(context, volume, instance=instance)
3160         self.volume_api.begin_detaching(context, volume['id'])
3161 
3162     def _detach_volume(self, context, instance, volume):
3163         """Detach volume from instance.
3164 
3165         This method is separated to make it easier for cells version
3166         to override.
3167         """
3168         self._check_and_begin_detach(context, volume, instance)
3169         attachments = volume.get('attachments', {})
3170         attachment_id = None
3171         if attachments and instance.uuid in attachments:
3172             attachment_id = attachments[instance.uuid]['attachment_id']
3173         self.compute_rpcapi.detach_volume(context, instance=instance,
3174                 volume_id=volume['id'], attachment_id=attachment_id)
3175 
3176     def _detach_volume_shelved_offloaded(self, context, instance, volume):
3177         """Detach a volume from an instance in shelved offloaded state.
3178 
3179         If the instance is shelved offloaded we just need to cleanup volume
3180         calling the volume api detach, the volume api terminte_connection
3181         and delete the bdm record.
3182         If the volume has delete_on_termination option set then we call the
3183         volume api delete as well.
3184         """
3185         self._check_and_begin_detach(context, volume, instance)
3186         bdms = [objects.BlockDeviceMapping.get_by_volume_id(
3187                 context, volume['id'], instance.uuid)]
3188         self._local_cleanup_bdm_volumes(bdms, instance, context)
3189 
3190     @wrap_check_policy
3191     @check_instance_lock
3192     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
3193                                     vm_states.STOPPED, vm_states.RESIZED,
3194                                     vm_states.SOFT_DELETED, vm_states.SHELVED,
3195                                     vm_states.SHELVED_OFFLOADED])
3196     def detach_volume(self, context, instance, volume):
3197         """Detach a volume from an instance."""
3198         if instance.vm_state == vm_states.SHELVED_OFFLOADED:
3199             self._detach_volume_shelved_offloaded(context, instance, volume)
3200         else:
3201             self._detach_volume(context, instance, volume)
3202 
3203     @wrap_check_policy
3204     @check_instance_lock
3205     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
3206                                     vm_states.SUSPENDED, vm_states.STOPPED,
3207                                     vm_states.RESIZED, vm_states.SOFT_DELETED])
3208     def swap_volume(self, context, instance, old_volume, new_volume):
3209         """Swap volume attached to an instance."""
3210         if old_volume['attach_status'] == 'detached':
3211             raise exception.VolumeUnattached(volume_id=old_volume['id'])
3212         # The caller likely got the instance from volume['attachments']
3213         # in the first place, but let's sanity check.
3214         if not old_volume.get('attachments', {}).get(instance.uuid):
3215             msg = _("Old volume is attached to a different instance.")
3216             raise exception.InvalidVolume(reason=msg)
3217         if new_volume['attach_status'] == 'attached':
3218             msg = _("New volume must be detached in order to swap.")
3219             raise exception.InvalidVolume(reason=msg)
3220         if int(new_volume['size']) < int(old_volume['size']):
3221             msg = _("New volume must be the same size or larger.")
3222             raise exception.InvalidVolume(reason=msg)
3223         self.volume_api.check_detach(context, old_volume)
3224         self.volume_api.check_attach(context, new_volume, instance=instance)
3225         self.volume_api.begin_detaching(context, old_volume['id'])
3226         self.volume_api.reserve_volume(context, new_volume['id'])
3227         try:
3228             self.compute_rpcapi.swap_volume(
3229                     context, instance=instance,
3230                     old_volume_id=old_volume['id'],
3231                     new_volume_id=new_volume['id'])
3232         except Exception:
3233             with excutils.save_and_reraise_exception():
3234                 self.volume_api.roll_detaching(context, old_volume['id'])
3235                 self.volume_api.unreserve_volume(context, new_volume['id'])
3236 
3237     @wrap_check_policy
3238     @check_instance_lock
3239     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
3240                                     vm_states.STOPPED],
3241                           task_state=[None])
3242     def attach_interface(self, context, instance, network_id, port_id,
3243                          requested_ip):
3244         """Use hotplug to add an network adapter to an instance."""
3245         return self.compute_rpcapi.attach_interface(context,
3246             instance=instance, network_id=network_id, port_id=port_id,
3247             requested_ip=requested_ip)
3248 
3249     @wrap_check_policy
3250     @check_instance_lock
3251     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
3252                                     vm_states.STOPPED],
3253                           task_state=[None])
3254     def detach_interface(self, context, instance, port_id):
3255         """Detach an network adapter from an instance."""
3256         self.compute_rpcapi.detach_interface(context, instance=instance,
3257             port_id=port_id)
3258 
3259     @wrap_check_policy
3260     def get_instance_metadata(self, context, instance):
3261         """Get all metadata associated with an instance."""
3262         return self.db.instance_metadata_get(context, instance.uuid)
3263 
3264     def get_all_instance_metadata(self, context, search_filts):
3265         return self._get_all_instance_metadata(
3266             context, search_filts, metadata_type='metadata')
3267 
3268     def get_all_system_metadata(self, context, search_filts):
3269         return self._get_all_instance_metadata(
3270             context, search_filts, metadata_type='system_metadata')
3271 
3272     def _get_all_instance_metadata(self, context, search_filts, metadata_type):
3273         """Get all metadata."""
3274         instances = self._get_instances_by_filters(context, filters={},
3275                                                    sort_keys=['created_at'],
3276                                                    sort_dirs=['desc'])
3277         for instance in instances:
3278             try:
3279                 check_policy(context, 'get_all_instance_%s' % metadata_type,
3280                              instance)
3281             except exception.PolicyNotAuthorized:
3282                 # failed policy check - not allowed to
3283                 # read this metadata
3284                 continue
3285 
3286         return utils.filter_and_format_resource_metadata('instance', instances,
3287                 search_filts, metadata_type)
3288 
3289     @wrap_check_policy
3290     @check_instance_lock
3291     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
3292                                     vm_states.SUSPENDED, vm_states.STOPPED],
3293                           task_state=None)
3294     def delete_instance_metadata(self, context, instance, key):
3295         """Delete the given metadata item from an instance."""
3296         instance.delete_metadata_key(key)
3297         self.compute_rpcapi.change_instance_metadata(context,
3298                                                      instance=instance,
3299                                                      diff={key: ['-']})
3300 
3301     @wrap_check_policy
3302     @check_instance_lock
3303     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
3304                                     vm_states.SUSPENDED, vm_states.STOPPED],
3305                           task_state=None)
3306     def update_instance_metadata(self, context, instance,
3307                                  metadata, delete=False):
3308         """Updates or creates instance metadata.
3309 
3310         If delete is True, metadata items that are not specified in the
3311         `metadata` argument will be deleted.
3312 
3313         """
3314         orig = dict(instance.metadata)
3315         if delete:
3316             _metadata = metadata
3317         else:
3318             _metadata = dict(instance.metadata)
3319             _metadata.update(metadata)
3320 
3321         self._check_metadata_properties_quota(context, _metadata)
3322         instance.metadata = _metadata
3323         instance.save()
3324         diff = _diff_dict(orig, instance.metadata)
3325         self.compute_rpcapi.change_instance_metadata(context,
3326                                                      instance=instance,
3327                                                      diff=diff)
3328         return _metadata
3329 
3330     def _get_root_bdm(self, context, instance, bdms=None):
3331         if bdms is None:
3332             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
3333                     context, instance.uuid)
3334 
3335         return bdms.root_bdm()
3336 
3337     def is_volume_backed_instance(self, context, instance, bdms=None):
3338         root_bdm = self._get_root_bdm(context, instance, bdms)
3339         if root_bdm is not None:
3340             return root_bdm.is_volume
3341         # in case we hit a very old instance without root bdm, we _assume_ that
3342         # instance is backed by a volume, if and only if image_ref is not set
3343         return not instance.image_ref
3344 
3345     @check_instance_lock
3346     @check_instance_cell
3347     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED])
3348     def live_migrate(self, context, instance, block_migration,
3349                      disk_over_commit, host_name):
3350         """Migrate a server lively to a new host."""
3351         LOG.debug("Going to try to live migrate instance to %s",
3352                   host_name or "another host", instance=instance)
3353 
3354         instance.task_state = task_states.MIGRATING
3355         instance.save(expected_task_state=[None])
3356 
3357         self._record_action_start(context, instance,
3358                                   instance_actions.LIVE_MIGRATION)
3359         try:
3360             request_spec = objects.RequestSpec.get_by_instance_uuid(
3361                 context, instance.uuid)
3362         except exception.RequestSpecNotFound:
3363             # Some old instances can still have no RequestSpec object attached
3364             # to them, we need to support the old way
3365             request_spec = None
3366         self.compute_task_api.live_migrate_instance(context, instance,
3367                 host_name, block_migration=block_migration,
3368                 disk_over_commit=disk_over_commit,
3369                 request_spec=request_spec)
3370 
3371     @check_instance_lock
3372     @check_instance_cell
3373     @check_instance_state(vm_state=[vm_states.ACTIVE],
3374                           task_state=[task_states.MIGRATING])
3375     def live_migrate_force_complete(self, context, instance, migration_id):
3376         """Force live migration to complete.
3377 
3378         :param context: Security context
3379         :param instance: The instance that is being migrated
3380         :param migration_id: ID of ongoing migration
3381 
3382         """
3383         LOG.debug("Going to try to force live migration to complete",
3384                   instance=instance)
3385 
3386         # NOTE(pkoniszewski): Get migration object to check if there is ongoing
3387         # live migration for particular instance. Also pass migration id to
3388         # compute to double check and avoid possible race condition.
3389         migration = objects.Migration.get_by_id_and_instance(
3390             context, migration_id, instance.uuid)
3391         if migration.status != 'running':
3392             raise exception.InvalidMigrationState(migration_id=migration_id,
3393                                                   instance_uuid=instance.uuid,
3394                                                   state=migration.status,
3395                                                   method='force complete')
3396 
3397         self._record_action_start(
3398             context, instance, instance_actions.LIVE_MIGRATION_FORCE_COMPLETE)
3399 
3400         self.compute_rpcapi.live_migration_force_complete(
3401             context, instance, migration.id)
3402 
3403     @check_instance_lock
3404     @check_instance_cell
3405     @check_instance_state(task_state=[task_states.MIGRATING])
3406     def live_migrate_abort(self, context, instance, migration_id):
3407         """Abort an in-progress live migration.
3408 
3409         :param context: Security context
3410         :param instance: The instance that is being migrated
3411         :param migration_id: ID of in-progress live migration
3412 
3413         """
3414         migration = objects.Migration.get_by_id_and_instance(context,
3415                     migration_id, instance.uuid)
3416         LOG.debug("Going to cancel live migration %s",
3417                   migration.id, instance=instance)
3418 
3419         if migration.status != 'running':
3420             raise exception.InvalidMigrationState(migration_id=migration_id,
3421                     instance_uuid=instance.uuid,
3422                     state=migration.status,
3423                     method='abort live migration')
3424         self._record_action_start(context, instance,
3425                                   instance_actions.LIVE_MIGRATION_CANCEL)
3426 
3427         self.compute_rpcapi.live_migration_abort(context,
3428                 instance, migration.id)
3429 
3430     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
3431                                     vm_states.ERROR])
3432     def evacuate(self, context, instance, host, on_shared_storage,
3433                  admin_password=None):
3434         """Running evacuate to target host.
3435 
3436         Checking vm compute host state, if the host not in expected_state,
3437         raising an exception.
3438 
3439         :param instance: The instance to evacuate
3440         :param host: Target host. if not set, the scheduler will pick up one
3441         :param on_shared_storage: True if instance files on shared storage
3442         :param admin_password: password to set on rebuilt instance
3443 
3444         """
3445         LOG.debug('vm evacuation scheduled', instance=instance)
3446         inst_host = instance.host
3447         service = objects.Service.get_by_compute_host(context, inst_host)
3448         if self.servicegroup_api.service_is_up(service):
3449             LOG.error(_LE('Instance compute service state on %s '
3450                           'expected to be down, but it was up.'), inst_host)
3451             raise exception.ComputeServiceInUse(host=inst_host)
3452 
3453         instance.task_state = task_states.REBUILDING
3454         instance.save(expected_task_state=[None])
3455         self._record_action_start(context, instance, instance_actions.EVACUATE)
3456 
3457         # NOTE(danms): Create this as a tombstone for the source compute
3458         # to find and cleanup. No need to pass it anywhere else.
3459         migration = objects.Migration(context,
3460                                       source_compute=instance.host,
3461                                       source_node=instance.node,
3462                                       instance_uuid=instance.uuid,
3463                                       status='accepted',
3464                                       migration_type='evacuation')
3465         if host:
3466             migration.dest_compute = host
3467         migration.create()
3468 
3469         compute_utils.notify_about_instance_usage(
3470             self.notifier, context, instance, "evacuate")
3471 
3472         try:
3473             request_spec = objects.RequestSpec.get_by_instance_uuid(
3474                 context, instance.uuid)
3475         except exception.RequestSpecNotFound:
3476             # Some old instances can still have no RequestSpec object attached
3477             # to them, we need to support the old way
3478             request_spec = None
3479         return self.compute_task_api.rebuild_instance(context,
3480                        instance=instance,
3481                        new_pass=admin_password,
3482                        injected_files=None,
3483                        image_ref=None,
3484                        orig_image_ref=None,
3485                        orig_sys_metadata=None,
3486                        bdms=None,
3487                        recreate=True,
3488                        on_shared_storage=on_shared_storage,
3489                        host=host,
3490                        request_spec=request_spec,
3491                        )
3492 
3493     def get_migrations(self, context, filters):
3494         """Get all migrations for the given filters."""
3495         return objects.MigrationList.get_by_filters(context, filters)
3496 
3497     def get_migrations_in_progress_by_instance(self, context, instance_uuid,
3498                                                migration_type=None):
3499         """Get all migrations of an instance in progress."""
3500         return objects.MigrationList.get_in_progress_by_instance(
3501                 context, instance_uuid, migration_type)
3502 
3503     def get_migration_by_id_and_instance(self, context,
3504                                          migration_id, instance_uuid):
3505         """Get the migration of an instance by id."""
3506         return objects.Migration.get_by_id_and_instance(
3507                 context, migration_id, instance_uuid)
3508 
3509     @wrap_check_policy
3510     def volume_snapshot_create(self, context, volume_id, create_info):
3511         bdm = objects.BlockDeviceMapping.get_by_volume(
3512                 context, volume_id, expected_attrs=['instance'])
3513         self.compute_rpcapi.volume_snapshot_create(context, bdm.instance,
3514                 volume_id, create_info)
3515         snapshot = {
3516             'snapshot': {
3517                 'id': create_info.get('id'),
3518                 'volumeId': volume_id
3519             }
3520         }
3521         return snapshot
3522 
3523     @wrap_check_policy
3524     def volume_snapshot_delete(self, context, volume_id, snapshot_id,
3525                                delete_info):
3526         bdm = objects.BlockDeviceMapping.get_by_volume(
3527                 context, volume_id, expected_attrs=['instance'])
3528         self.compute_rpcapi.volume_snapshot_delete(context, bdm.instance,
3529                 volume_id, snapshot_id, delete_info)
3530 
3531     def external_instance_event(self, context, instances, events):
3532         # NOTE(danms): The external API consumer just provides events,
3533         # but doesn't know where they go. We need to collate lists
3534         # by the host the affected instance is on and dispatch them
3535         # according to host
3536         instances_by_host = {}
3537         events_by_host = {}
3538         hosts_by_instance = {}
3539         for instance in instances:
3540             instances_on_host = instances_by_host.get(instance.host, [])
3541             instances_on_host.append(instance)
3542             instances_by_host[instance.host] = instances_on_host
3543             hosts_by_instance[instance.uuid] = instance.host
3544 
3545         for event in events:
3546             host = hosts_by_instance[event.instance_uuid]
3547             events_on_host = events_by_host.get(host, [])
3548             events_on_host.append(event)
3549             events_by_host[host] = events_on_host
3550 
3551         for host in instances_by_host:
3552             # TODO(salv-orlando): Handle exceptions raised by the rpc api layer
3553             # in order to ensure that a failure in processing events on a host
3554             # will not prevent processing events on other hosts
3555             self.compute_rpcapi.external_instance_event(
3556                 context, instances_by_host[host], events_by_host[host])
3557 
3558     def get_instance_host_status(self, instance):
3559         if instance.host:
3560             try:
3561                 service = [service for service in instance.services if
3562                            service.binary == 'nova-compute'][0]
3563                 if service.forced_down:
3564                     host_status = fields_obj.HostStatus.DOWN
3565                 elif service.disabled:
3566                     host_status = fields_obj.HostStatus.MAINTENANCE
3567                 else:
3568                     alive = self.servicegroup_api.service_is_up(service)
3569                     host_status = ((alive and fields_obj.HostStatus.UP) or
3570                                    fields_obj.HostStatus.UNKNOWN)
3571             except IndexError:
3572                 host_status = fields_obj.HostStatus.NONE
3573         else:
3574             host_status = fields_obj.HostStatus.NONE
3575         return host_status
3576 
3577     def get_instances_host_statuses(self, instance_list):
3578         host_status_dict = dict()
3579         host_statuses = dict()
3580         for instance in instance_list:
3581             if instance.host:
3582                 if instance.host not in host_status_dict:
3583                     host_status = self.get_instance_host_status(instance)
3584                     host_status_dict[instance.host] = host_status
3585                 else:
3586                     host_status = host_status_dict[instance.host]
3587             else:
3588                 host_status = fields_obj.HostStatus.NONE
3589             host_statuses[instance.uuid] = host_status
3590         return host_statuses
3591 
3592 
3593 class HostAPI(base.Base):
3594     """Sub-set of the Compute Manager API for managing host operations."""
3595 
3596     def __init__(self, rpcapi=None):
3597         self.rpcapi = rpcapi or compute_rpcapi.ComputeAPI()
3598         self.servicegroup_api = servicegroup.API()
3599         super(HostAPI, self).__init__()
3600 
3601     def _assert_host_exists(self, context, host_name, must_be_up=False):
3602         """Raise HostNotFound if compute host doesn't exist."""
3603         service = objects.Service.get_by_compute_host(context, host_name)
3604         if not service:
3605             raise exception.HostNotFound(host=host_name)
3606         if must_be_up and not self.servicegroup_api.service_is_up(service):
3607             raise exception.ComputeServiceUnavailable(host=host_name)
3608         return service['host']
3609 
3610     @wrap_exception()
3611     def set_host_enabled(self, context, host_name, enabled):
3612         """Sets the specified host's ability to accept new instances."""
3613         host_name = self._assert_host_exists(context, host_name)
3614         payload = {'host_name': host_name, 'enabled': enabled}
3615         compute_utils.notify_about_host_update(context,
3616                                                'set_enabled.start',
3617                                                payload)
3618         result = self.rpcapi.set_host_enabled(context, enabled=enabled,
3619                 host=host_name)
3620         compute_utils.notify_about_host_update(context,
3621                                                'set_enabled.end',
3622                                                payload)
3623         return result
3624 
3625     def get_host_uptime(self, context, host_name):
3626         """Returns the result of calling "uptime" on the target host."""
3627         host_name = self._assert_host_exists(context, host_name,
3628                          must_be_up=True)
3629         return self.rpcapi.get_host_uptime(context, host=host_name)
3630 
3631     @wrap_exception()
3632     def host_power_action(self, context, host_name, action):
3633         """Reboots, shuts down or powers up the host."""
3634         host_name = self._assert_host_exists(context, host_name)
3635         payload = {'host_name': host_name, 'action': action}
3636         compute_utils.notify_about_host_update(context,
3637                                                'power_action.start',
3638                                                payload)
3639         result = self.rpcapi.host_power_action(context, action=action,
3640                 host=host_name)
3641         compute_utils.notify_about_host_update(context,
3642                                                'power_action.end',
3643                                                payload)
3644         return result
3645 
3646     @wrap_exception()
3647     def set_host_maintenance(self, context, host_name, mode):
3648         """Start/Stop host maintenance window. On start, it triggers
3649         guest VMs evacuation.
3650         """
3651         host_name = self._assert_host_exists(context, host_name)
3652         payload = {'host_name': host_name, 'mode': mode}
3653         compute_utils.notify_about_host_update(context,
3654                                                'set_maintenance.start',
3655                                                payload)
3656         result = self.rpcapi.host_maintenance_mode(context,
3657                 host_param=host_name, mode=mode, host=host_name)
3658         compute_utils.notify_about_host_update(context,
3659                                                'set_maintenance.end',
3660                                                payload)
3661         return result
3662 
3663     def service_get_all(self, context, filters=None, set_zones=False):
3664         """Returns a list of services, optionally filtering the results.
3665 
3666         If specified, 'filters' should be a dictionary containing services
3667         attributes and matching values.  Ie, to get a list of services for
3668         the 'compute' topic, use filters={'topic': 'compute'}.
3669         """
3670         if filters is None:
3671             filters = {}
3672         disabled = filters.pop('disabled', None)
3673         if 'availability_zone' in filters:
3674             set_zones = True
3675         services = objects.ServiceList.get_all(context, disabled,
3676                                                set_zones=set_zones)
3677         ret_services = []
3678         for service in services:
3679             for key, val in six.iteritems(filters):
3680                 if service[key] != val:
3681                     break
3682             else:
3683                 # All filters matched.
3684                 ret_services.append(service)
3685         return ret_services
3686 
3687     def service_get_by_compute_host(self, context, host_name):
3688         """Get service entry for the given compute hostname."""
3689         return objects.Service.get_by_compute_host(context, host_name)
3690 
3691     def _service_update(self, context, host_name, binary, params_to_update):
3692         """Performs the actual service update operation."""
3693         service = objects.Service.get_by_args(context, host_name, binary)
3694         service.update(params_to_update)
3695         service.save()
3696         return service
3697 
3698     def service_update(self, context, host_name, binary, params_to_update):
3699         """Enable / Disable a service.
3700 
3701         For compute services, this stops new builds and migrations going to
3702         the host.
3703         """
3704         return self._service_update(context, host_name, binary,
3705                                     params_to_update)
3706 
3707     def _service_delete(self, context, service_id):
3708         """Performs the actual Service deletion operation."""
3709         objects.Service.get_by_id(context, service_id).destroy()
3710 
3711     def service_delete(self, context, service_id):
3712         """Deletes the specified service."""
3713         self._service_delete(context, service_id)
3714 
3715     def instance_get_all_by_host(self, context, host_name):
3716         """Return all instances on the given host."""
3717         return objects.InstanceList.get_by_host(context, host_name)
3718 
3719     def task_log_get_all(self, context, task_name, period_beginning,
3720                          period_ending, host=None, state=None):
3721         """Return the task logs within a given range, optionally
3722         filtering by host and/or state.
3723         """
3724         return self.db.task_log_get_all(context, task_name,
3725                                         period_beginning,
3726                                         period_ending,
3727                                         host=host,
3728                                         state=state)
3729 
3730     def compute_node_get(self, context, compute_id):
3731         """Return compute node entry for particular integer ID."""
3732         return objects.ComputeNode.get_by_id(context, int(compute_id))
3733 
3734     def compute_node_get_all(self, context):
3735         return objects.ComputeNodeList.get_all(context)
3736 
3737     def compute_node_search_by_hypervisor(self, context, hypervisor_match):
3738         return objects.ComputeNodeList.get_by_hypervisor(context,
3739                                                          hypervisor_match)
3740 
3741     def compute_node_statistics(self, context):
3742         return self.db.compute_node_statistics(context)
3743 
3744 
3745 class InstanceActionAPI(base.Base):
3746     """Sub-set of the Compute Manager API for managing instance actions."""
3747 
3748     def actions_get(self, context, instance):
3749         return objects.InstanceActionList.get_by_instance_uuid(
3750             context, instance.uuid)
3751 
3752     def action_get_by_request_id(self, context, instance, request_id):
3753         return objects.InstanceAction.get_by_request_id(
3754             context, instance.uuid, request_id)
3755 
3756     def action_events_get(self, context, instance, action_id):
3757         return objects.InstanceActionEventList.get_by_action(
3758             context, action_id)
3759 
3760 
3761 class AggregateAPI(base.Base):
3762     """Sub-set of the Compute Manager API for managing host aggregates."""
3763     def __init__(self, **kwargs):
3764         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
3765         self.scheduler_client = scheduler_client.SchedulerClient()
3766         super(AggregateAPI, self).__init__(**kwargs)
3767 
3768     @wrap_exception()
3769     def create_aggregate(self, context, aggregate_name, availability_zone):
3770         """Creates the model for the aggregate."""
3771 
3772         aggregate = objects.Aggregate(context=context)
3773         aggregate.name = aggregate_name
3774         if availability_zone:
3775             aggregate.metadata = {'availability_zone': availability_zone}
3776         aggregate.create()
3777         self.scheduler_client.update_aggregates(context, [aggregate])
3778         return aggregate
3779 
3780     def get_aggregate(self, context, aggregate_id):
3781         """Get an aggregate by id."""
3782         return objects.Aggregate.get_by_id(context, aggregate_id)
3783 
3784     def get_aggregate_list(self, context):
3785         """Get all the aggregates."""
3786         return objects.AggregateList.get_all(context)
3787 
3788     @wrap_exception()
3789     def update_aggregate(self, context, aggregate_id, values):
3790         """Update the properties of an aggregate."""
3791         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
3792         if 'name' in values:
3793             aggregate.name = values.pop('name')
3794             aggregate.save()
3795         self.is_safe_to_update_az(context, values, aggregate=aggregate,
3796                                   action_name=AGGREGATE_ACTION_UPDATE)
3797         if values:
3798             aggregate.update_metadata(values)
3799             aggregate.updated_at = timeutils.utcnow()
3800         self.scheduler_client.update_aggregates(context, [aggregate])
3801         # If updated values include availability_zones, then the cache
3802         # which stored availability_zones and host need to be reset
3803         if values.get('availability_zone'):
3804             availability_zones.reset_cache()
3805         return aggregate
3806 
3807     @wrap_exception()
3808     def update_aggregate_metadata(self, context, aggregate_id, metadata):
3809         """Updates the aggregate metadata."""
3810         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
3811         self.is_safe_to_update_az(context, metadata, aggregate=aggregate,
3812                                   action_name=AGGREGATE_ACTION_UPDATE_META)
3813         aggregate.update_metadata(metadata)
3814         self.scheduler_client.update_aggregates(context, [aggregate])
3815         # If updated metadata include availability_zones, then the cache
3816         # which stored availability_zones and host need to be reset
3817         if metadata and metadata.get('availability_zone'):
3818             availability_zones.reset_cache()
3819         aggregate.updated_at = timeutils.utcnow()
3820         return aggregate
3821 
3822     @wrap_exception()
3823     def delete_aggregate(self, context, aggregate_id):
3824         """Deletes the aggregate."""
3825         aggregate_payload = {'aggregate_id': aggregate_id}
3826         compute_utils.notify_about_aggregate_update(context,
3827                                                     "delete.start",
3828                                                     aggregate_payload)
3829         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
3830         if len(aggregate.hosts) > 0:
3831             msg = _("Host aggregate is not empty")
3832             raise exception.InvalidAggregateActionDelete(
3833                 aggregate_id=aggregate_id, reason=msg)
3834         aggregate.destroy()
3835         self.scheduler_client.delete_aggregate(context, aggregate)
3836         compute_utils.notify_about_aggregate_update(context,
3837                                                     "delete.end",
3838                                                     aggregate_payload)
3839 
3840     def is_safe_to_update_az(self, context, metadata, aggregate,
3841                              hosts=None,
3842                              action_name=AGGREGATE_ACTION_ADD):
3843         """Determine if updates alter an aggregate's availability zone.
3844 
3845             :param context: local context
3846             :param metadata: Target metadata for updating aggregate
3847             :param aggregate: Aggregate to update
3848             :param hosts: Hosts to check. If None, aggregate.hosts is used
3849             :type hosts: list
3850             :action_name: Calling method for logging purposes
3851 
3852         """
3853         if 'availability_zone' in metadata:
3854             if not metadata['availability_zone']:
3855                 msg = _("Aggregate %s does not support empty named "
3856                         "availability zone") % aggregate.name
3857                 self._raise_invalid_aggregate_exc(action_name, aggregate.id,
3858                                                   msg)
3859             _hosts = hosts or aggregate.hosts
3860             host_aggregates = objects.AggregateList.get_by_metadata_key(
3861                 context, 'availability_zone', hosts=_hosts)
3862             conflicting_azs = [
3863                 agg.availability_zone for agg in host_aggregates
3864                 if agg.availability_zone != metadata['availability_zone']
3865                 and agg.id != aggregate.id]
3866             if conflicting_azs:
3867                 msg = _("One or more hosts already in availability zone(s) "
3868                         "%s") % conflicting_azs
3869                 self._raise_invalid_aggregate_exc(action_name, aggregate.id,
3870                                                   msg)
3871 
3872     def _raise_invalid_aggregate_exc(self, action_name, aggregate_id, reason):
3873         if action_name == AGGREGATE_ACTION_ADD:
3874             raise exception.InvalidAggregateActionAdd(
3875                 aggregate_id=aggregate_id, reason=reason)
3876         elif action_name == AGGREGATE_ACTION_UPDATE:
3877             raise exception.InvalidAggregateActionUpdate(
3878                 aggregate_id=aggregate_id, reason=reason)
3879         elif action_name == AGGREGATE_ACTION_UPDATE_META:
3880             raise exception.InvalidAggregateActionUpdateMeta(
3881                 aggregate_id=aggregate_id, reason=reason)
3882         elif action_name == AGGREGATE_ACTION_DELETE:
3883             raise exception.InvalidAggregateActionDelete(
3884                 aggregate_id=aggregate_id, reason=reason)
3885 
3886         raise exception.NovaException(
3887             _("Unexpected aggregate action %s") % action_name)
3888 
3889     def _update_az_cache_for_host(self, context, host_name, aggregate_meta):
3890         # Update the availability_zone cache to avoid getting wrong
3891         # availability_zone in cache retention time when add/remove
3892         # host to/from aggregate.
3893         if aggregate_meta and aggregate_meta.get('availability_zone'):
3894             availability_zones.update_host_availability_zone_cache(context,
3895                                                                    host_name)
3896 
3897     @wrap_exception()
3898     def add_host_to_aggregate(self, context, aggregate_id, host_name):
3899         """Adds the host to an aggregate."""
3900         aggregate_payload = {'aggregate_id': aggregate_id,
3901                              'host_name': host_name}
3902         compute_utils.notify_about_aggregate_update(context,
3903                                                     "addhost.start",
3904                                                     aggregate_payload)
3905         # validates the host; ComputeHostNotFound is raised if invalid
3906         objects.Service.get_by_compute_host(context, host_name)
3907 
3908         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
3909         self.is_safe_to_update_az(context, aggregate.metadata,
3910                                   hosts=[host_name], aggregate=aggregate)
3911 
3912         aggregate.add_host(host_name)
3913         self.scheduler_client.update_aggregates(context, [aggregate])
3914         self._update_az_cache_for_host(context, host_name, aggregate.metadata)
3915         # NOTE(jogo): Send message to host to support resource pools
3916         self.compute_rpcapi.add_aggregate_host(context,
3917                 aggregate=aggregate, host_param=host_name, host=host_name)
3918         aggregate_payload.update({'name': aggregate.name})
3919         compute_utils.notify_about_aggregate_update(context,
3920                                                     "addhost.end",
3921                                                     aggregate_payload)
3922         return aggregate
3923 
3924     @wrap_exception()
3925     def remove_host_from_aggregate(self, context, aggregate_id, host_name):
3926         """Removes host from the aggregate."""
3927         aggregate_payload = {'aggregate_id': aggregate_id,
3928                              'host_name': host_name}
3929         compute_utils.notify_about_aggregate_update(context,
3930                                                     "removehost.start",
3931                                                     aggregate_payload)
3932         # validates the host; ComputeHostNotFound is raised if invalid
3933         objects.Service.get_by_compute_host(context, host_name)
3934         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
3935         aggregate.delete_host(host_name)
3936         self.scheduler_client.update_aggregates(context, [aggregate])
3937         self._update_az_cache_for_host(context, host_name, aggregate.metadata)
3938         self.compute_rpcapi.remove_aggregate_host(context,
3939                 aggregate=aggregate, host_param=host_name, host=host_name)
3940         compute_utils.notify_about_aggregate_update(context,
3941                                                     "removehost.end",
3942                                                     aggregate_payload)
3943         return aggregate
3944 
3945 
3946 class KeypairAPI(base.Base):
3947     """Subset of the Compute Manager API for managing key pairs."""
3948 
3949     get_notifier = functools.partial(rpc.get_notifier, service='api')
3950     wrap_exception = functools.partial(exception.wrap_exception,
3951                                        get_notifier=get_notifier)
3952 
3953     def _notify(self, context, event_suffix, keypair_name):
3954         payload = {
3955             'tenant_id': context.project_id,
3956             'user_id': context.user_id,
3957             'key_name': keypair_name,
3958         }
3959         notify = self.get_notifier()
3960         notify.info(context, 'keypair.%s' % event_suffix, payload)
3961 
3962     def _validate_new_key_pair(self, context, user_id, key_name, key_type):
3963         safe_chars = "_- " + string.digits + string.ascii_letters
3964         clean_value = "".join(x for x in key_name if x in safe_chars)
3965         if clean_value != key_name:
3966             raise exception.InvalidKeypair(
3967                 reason=_("Keypair name contains unsafe characters"))
3968 
3969         try:
3970             utils.check_string_length(key_name, min_length=1, max_length=255)
3971         except exception.InvalidInput:
3972             raise exception.InvalidKeypair(
3973                 reason=_('Keypair name must be string and between '
3974                          '1 and 255 characters long'))
3975 
3976         count = objects.Quotas.count(context, 'key_pairs', user_id)
3977 
3978         try:
3979             objects.Quotas.limit_check(context, key_pairs=count + 1)
3980         except exception.OverQuota:
3981             raise exception.KeypairLimitExceeded()
3982 
3983     @wrap_exception()
3984     def import_key_pair(self, context, user_id, key_name, public_key,
3985                         key_type=keypair_obj.KEYPAIR_TYPE_SSH):
3986         """Import a key pair using an existing public key."""
3987         self._validate_new_key_pair(context, user_id, key_name, key_type)
3988 
3989         self._notify(context, 'import.start', key_name)
3990 
3991         fingerprint = self._generate_fingerprint(public_key, key_type)
3992 
3993         keypair = objects.KeyPair(context)
3994         keypair.user_id = user_id
3995         keypair.name = key_name
3996         keypair.type = key_type
3997         keypair.fingerprint = fingerprint
3998         keypair.public_key = public_key
3999         keypair.create()
4000 
4001         self._notify(context, 'import.end', key_name)
4002 
4003         return keypair
4004 
4005     @wrap_exception()
4006     def create_key_pair(self, context, user_id, key_name,
4007                         key_type=keypair_obj.KEYPAIR_TYPE_SSH):
4008         """Create a new key pair."""
4009         self._validate_new_key_pair(context, user_id, key_name, key_type)
4010 
4011         self._notify(context, 'create.start', key_name)
4012 
4013         private_key, public_key, fingerprint = self._generate_key_pair(
4014             user_id, key_type)
4015 
4016         keypair = objects.KeyPair(context)
4017         keypair.user_id = user_id
4018         keypair.name = key_name
4019         keypair.type = key_type
4020         keypair.fingerprint = fingerprint
4021         keypair.public_key = public_key
4022         keypair.create()
4023 
4024         self._notify(context, 'create.end', key_name)
4025 
4026         return keypair, private_key
4027 
4028     def _generate_fingerprint(self, public_key, key_type):
4029         if key_type == keypair_obj.KEYPAIR_TYPE_SSH:
4030             return crypto.generate_fingerprint(public_key)
4031         elif key_type == keypair_obj.KEYPAIR_TYPE_X509:
4032             return crypto.generate_x509_fingerprint(public_key)
4033 
4034     def _generate_key_pair(self, user_id, key_type):
4035         if key_type == keypair_obj.KEYPAIR_TYPE_SSH:
4036             return crypto.generate_key_pair()
4037         elif key_type == keypair_obj.KEYPAIR_TYPE_X509:
4038             return crypto.generate_winrm_x509_cert(user_id)
4039 
4040     @wrap_exception()
4041     def delete_key_pair(self, context, user_id, key_name):
4042         """Delete a keypair by name."""
4043         self._notify(context, 'delete.start', key_name)
4044         objects.KeyPair.destroy_by_name(context, user_id, key_name)
4045         self._notify(context, 'delete.end', key_name)
4046 
4047     def get_key_pairs(self, context, user_id):
4048         """List key pairs."""
4049         return objects.KeyPairList.get_by_user(context, user_id)
4050 
4051     def get_key_pair(self, context, user_id, key_name):
4052         """Get a keypair by name."""
4053         return objects.KeyPair.get_by_name(context, user_id, key_name)
4054 
4055 
4056 class SecurityGroupAPI(base.Base, security_group_base.SecurityGroupBase):
4057     """Sub-set of the Compute API related to managing security groups
4058     and security group rules
4059     """
4060 
4061     # The nova security group api does not use a uuid for the id.
4062     id_is_uuid = False
4063 
4064     def __init__(self, skip_policy_check=False, **kwargs):
4065         super(SecurityGroupAPI, self).__init__(**kwargs)
4066         self.skip_policy_check = skip_policy_check
4067         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
4068 
4069     def validate_property(self, value, property, allowed):
4070         """Validate given security group property.
4071 
4072         :param value:          the value to validate, as a string or unicode
4073         :param property:       the property, either 'name' or 'description'
4074         :param allowed:        the range of characters allowed
4075         """
4076 
4077         try:
4078             val = value.strip()
4079         except AttributeError:
4080             msg = _("Security group %s is not a string or unicode") % property
4081             self.raise_invalid_property(msg)
4082         utils.check_string_length(val, name=property, min_length=1,
4083                                   max_length=255)
4084 
4085         if allowed and not re.match(allowed, val):
4086             # Some validation to ensure that values match API spec.
4087             # - Alphanumeric characters, spaces, dashes, and underscores.
4088             # TODO(Daviey): LP: #813685 extend beyond group_name checking, and
4089             #  probably create a param validator that can be used elsewhere.
4090             msg = (_("Value (%(value)s) for parameter Group%(property)s is "
4091                      "invalid. Content limited to '%(allowed)s'.") %
4092                    {'value': value, 'allowed': allowed,
4093                     'property': property.capitalize()})
4094             self.raise_invalid_property(msg)
4095 
4096     def ensure_default(self, context):
4097         """Ensure that a context has a security group.
4098 
4099         Creates a security group for the security context if it does not
4100         already exist.
4101 
4102         :param context: the security context
4103         """
4104         self.db.security_group_ensure_default(context)
4105 
4106     def create_security_group(self, context, name, description):
4107         quotas = objects.Quotas(context=context)
4108         try:
4109             quotas.reserve(security_groups=1)
4110         except exception.OverQuota:
4111             msg = _("Quota exceeded, too many security groups.")
4112             self.raise_over_quota(msg)
4113 
4114         LOG.info(_LI("Create Security Group %s"), name, context=context)
4115 
4116         try:
4117             self.ensure_default(context)
4118 
4119             group = {'user_id': context.user_id,
4120                      'project_id': context.project_id,
4121                      'name': name,
4122                      'description': description}
4123             try:
4124                 group_ref = self.db.security_group_create(context, group)
4125             except exception.SecurityGroupExists:
4126                 msg = _('Security group %s already exists') % name
4127                 self.raise_group_already_exists(msg)
4128             # Commit the reservation
4129             quotas.commit()
4130         except Exception:
4131             with excutils.save_and_reraise_exception():
4132                 quotas.rollback()
4133 
4134         return group_ref
4135 
4136     def update_security_group(self, context, security_group,
4137                                 name, description):
4138         if security_group['name'] in RO_SECURITY_GROUPS:
4139             msg = (_("Unable to update system group '%s'") %
4140                     security_group['name'])
4141             self.raise_invalid_group(msg)
4142 
4143         group = {'name': name,
4144                  'description': description}
4145 
4146         columns_to_join = ['rules.grantee_group']
4147         group_ref = self.db.security_group_update(context,
4148                 security_group['id'],
4149                 group,
4150                 columns_to_join=columns_to_join)
4151         return group_ref
4152 
4153     def get(self, context, name=None, id=None, map_exception=False):
4154         self.ensure_default(context)
4155         try:
4156             if name:
4157                 return self.db.security_group_get_by_name(context,
4158                                                           context.project_id,
4159                                                           name)
4160             elif id:
4161                 return self.db.security_group_get(context, id)
4162         except exception.NotFound as exp:
4163             if map_exception:
4164                 msg = exp.format_message()
4165                 self.raise_not_found(msg)
4166             else:
4167                 raise
4168 
4169     def list(self, context, names=None, ids=None, project=None,
4170              search_opts=None):
4171         self.ensure_default(context)
4172 
4173         groups = []
4174         if names or ids:
4175             if names:
4176                 for name in names:
4177                     groups.append(self.db.security_group_get_by_name(context,
4178                                                                      project,
4179                                                                      name))
4180             if ids:
4181                 for id in ids:
4182                     groups.append(self.db.security_group_get(context, id))
4183 
4184         elif context.is_admin:
4185             # TODO(eglynn): support a wider set of search options than just
4186             # all_tenants, at least include the standard filters defined for
4187             # the EC2 DescribeSecurityGroups API for the non-admin case also
4188             if (search_opts and 'all_tenants' in search_opts):
4189                 groups = self.db.security_group_get_all(context)
4190             else:
4191                 groups = self.db.security_group_get_by_project(context,
4192                                                                project)
4193 
4194         elif project:
4195             groups = self.db.security_group_get_by_project(context, project)
4196 
4197         return groups
4198 
4199     def destroy(self, context, security_group):
4200         if security_group['name'] in RO_SECURITY_GROUPS:
4201             msg = _("Unable to delete system group '%s'") % \
4202                     security_group['name']
4203             self.raise_invalid_group(msg)
4204 
4205         if self.db.security_group_in_use(context, security_group['id']):
4206             msg = _("Security group is still in use")
4207             self.raise_invalid_group(msg)
4208 
4209         quotas = objects.Quotas(context=context)
4210         quota_project, quota_user = quotas_obj.ids_from_security_group(
4211                                 context, security_group)
4212         try:
4213             quotas.reserve(project_id=quota_project,
4214                            user_id=quota_user, security_groups=-1)
4215         except Exception:
4216             LOG.exception(_LE("Failed to update usages deallocating "
4217                               "security group"))
4218 
4219         LOG.info(_LI("Delete security group %s"), security_group['name'],
4220                   context=context)
4221         self.db.security_group_destroy(context, security_group['id'])
4222 
4223         # Commit the reservations
4224         quotas.commit()
4225 
4226     def is_associated_with_server(self, security_group, instance_uuid):
4227         """Check if the security group is already associated
4228            with the instance. If Yes, return True.
4229         """
4230 
4231         if not security_group:
4232             return False
4233 
4234         instances = security_group.get('instances')
4235         if not instances:
4236             return False
4237 
4238         for inst in instances:
4239             if (instance_uuid == inst['uuid']):
4240                 return True
4241 
4242         return False
4243 
4244     @wrap_check_security_groups_policy
4245     def add_to_instance(self, context, instance, security_group_name):
4246         """Add security group to the instance."""
4247         security_group = self.db.security_group_get_by_name(context,
4248                 context.project_id,
4249                 security_group_name)
4250 
4251         instance_uuid = instance.uuid
4252 
4253         # check if the security group is associated with the server
4254         if self.is_associated_with_server(security_group, instance_uuid):
4255             raise exception.SecurityGroupExistsForInstance(
4256                                         security_group_id=security_group['id'],
4257                                         instance_id=instance_uuid)
4258 
4259         self.db.instance_add_security_group(context.elevated(),
4260                                             instance_uuid,
4261                                             security_group['id'])
4262         if instance.host:
4263             self.compute_rpcapi.refresh_instance_security_rules(
4264                     context, instance.host, instance)
4265 
4266     @wrap_check_security_groups_policy
4267     def remove_from_instance(self, context, instance, security_group_name):
4268         """Remove the security group associated with the instance."""
4269         security_group = self.db.security_group_get_by_name(context,
4270                 context.project_id,
4271                 security_group_name)
4272 
4273         instance_uuid = instance.uuid
4274 
4275         # check if the security group is associated with the server
4276         if not self.is_associated_with_server(security_group, instance_uuid):
4277             raise exception.SecurityGroupNotExistsForInstance(
4278                                     security_group_id=security_group['id'],
4279                                     instance_id=instance_uuid)
4280 
4281         self.db.instance_remove_security_group(context.elevated(),
4282                                                instance_uuid,
4283                                                security_group['id'])
4284         if instance.host:
4285             self.compute_rpcapi.refresh_instance_security_rules(
4286                     context, instance.host, instance)
4287 
4288     def get_rule(self, context, id):
4289         self.ensure_default(context)
4290         try:
4291             return self.db.security_group_rule_get(context, id)
4292         except exception.NotFound:
4293             msg = _("Rule (%s) not found") % id
4294             self.raise_not_found(msg)
4295 
4296     def add_rules(self, context, id, name, vals):
4297         """Add security group rule(s) to security group.
4298 
4299         Note: the Nova security group API doesn't support adding multiple
4300         security group rules at once but the EC2 one does. Therefore,
4301         this function is written to support both.
4302         """
4303 
4304         count = objects.Quotas.count(context, 'security_group_rules', id)
4305         try:
4306             projected = count + len(vals)
4307             objects.Quotas.limit_check(context, security_group_rules=projected)
4308         except exception.OverQuota:
4309             msg = _("Quota exceeded, too many security group rules.")
4310             self.raise_over_quota(msg)
4311 
4312         msg = _("Security group %(name)s added %(protocol)s ingress "
4313                 "(%(from_port)s:%(to_port)s)")
4314         rules = []
4315         for v in vals:
4316             rule = self.db.security_group_rule_create(context, v)
4317             rules.append(rule)
4318             LOG.info(msg, {'name': name,
4319                            'protocol': rule.protocol,
4320                            'from_port': rule.from_port,
4321                            'to_port': rule.to_port})
4322 
4323         self.trigger_rules_refresh(context, id=id)
4324         return rules
4325 
4326     def remove_rules(self, context, security_group, rule_ids):
4327         msg = _("Security group %(name)s removed %(protocol)s ingress "
4328                 "(%(from_port)s:%(to_port)s)")
4329         for rule_id in rule_ids:
4330             rule = self.get_rule(context, rule_id)
4331             LOG.info(msg, {'name': security_group['name'],
4332                            'protocol': rule.protocol,
4333                            'from_port': rule.from_port,
4334                            'to_port': rule.to_port})
4335 
4336             self.db.security_group_rule_destroy(context, rule_id)
4337 
4338         # NOTE(vish): we removed some rules, so refresh
4339         self.trigger_rules_refresh(context, id=security_group['id'])
4340 
4341     def remove_default_rules(self, context, rule_ids):
4342         for rule_id in rule_ids:
4343             self.db.security_group_default_rule_destroy(context, rule_id)
4344 
4345     def add_default_rules(self, context, vals):
4346         rules = [self.db.security_group_default_rule_create(context, v)
4347                  for v in vals]
4348         return rules
4349 
4350     def default_rule_exists(self, context, values):
4351         """Indicates whether the specified rule values are already
4352            defined in the default security group rules.
4353         """
4354         for rule in self.db.security_group_default_rule_list(context):
4355             keys = ('cidr', 'from_port', 'to_port', 'protocol')
4356             for key in keys:
4357                 if rule.get(key) != values.get(key):
4358                     break
4359             else:
4360                 return rule.get('id') or True
4361         return False
4362 
4363     def get_all_default_rules(self, context):
4364         try:
4365             rules = self.db.security_group_default_rule_list(context)
4366         except Exception:
4367             msg = 'cannot get default security group rules'
4368             raise exception.SecurityGroupDefaultRuleNotFound(msg)
4369 
4370         return rules
4371 
4372     def get_default_rule(self, context, id):
4373         return self.db.security_group_default_rule_get(context, id)
4374 
4375     def validate_id(self, id):
4376         try:
4377             return int(id)
4378         except ValueError:
4379             msg = _("Security group id should be integer")
4380             self.raise_invalid_property(msg)
4381 
4382     def _refresh_instance_security_rules(self, context, instances):
4383         for instance in instances:
4384             if instance.host is not None:
4385                 self.compute_rpcapi.refresh_instance_security_rules(
4386                         context, instance.host, instance)
4387 
4388     def trigger_rules_refresh(self, context, id):
4389         """Called when a rule is added to or removed from a security_group."""
4390         instances = objects.InstanceList.get_by_security_group_id(context, id)
4391         self._refresh_instance_security_rules(context, instances)
4392 
4393     def trigger_members_refresh(self, context, group_ids):
4394         """Called when a security group gains a new or loses a member.
4395 
4396         Sends an update request to each compute node for each instance for
4397         which this is relevant.
4398         """
4399         instances = objects.InstanceList.get_by_grantee_security_group_ids(
4400             context, group_ids)
4401         self._refresh_instance_security_rules(context, instances)
4402 
4403     def get_instance_security_groups(self, context, instance_uuid,
4404                                      detailed=False):
4405         if detailed:
4406             return self.db.security_group_get_by_instance(context,
4407                                                           instance_uuid)
4408         instance = objects.Instance(uuid=instance_uuid)
4409         groups = objects.SecurityGroupList.get_by_instance(context, instance)
4410         return [{'name': group.name} for group in groups]
4411 
4412     def populate_security_groups(self, security_groups):
4413         if not security_groups:
4414             # Make sure it's an empty SecurityGroupList and not None
4415             return objects.SecurityGroupList()
4416         return security_group_obj.make_secgroup_list(security_groups)
