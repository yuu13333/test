Based on the given code from a commit, please generate supplementary code files according to the commit message.
####commit message
compute: Remove stale BDMs on reserve_block_device_name failure

Previously errors encountered when creating BDMs on a remote compute
host via reserve_block_device_name could result in stale BDM entries
being left in the database.

This change adds some simple cleanup logic to reserve_block_device_name
to lookup and remove any such BDMs when an exception is encountered
before reraising.

Closes-Bug: #1844296
Change-Id: I3c45b3e9c8a5a9ce1e0e47abb8f95089f8c212c3

####code 
1 # Copyright 2010 United States Government as represented by the
2 # Administrator of the National Aeronautics and Space Administration.
3 # Copyright 2011 Piston Cloud Computing, Inc.
4 # Copyright 2012-2013 Red Hat, Inc.
5 # All Rights Reserved.
6 #
7 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
8 #    not use this file except in compliance with the License. You may obtain
9 #    a copy of the License at
10 #
11 #         http://www.apache.org/licenses/LICENSE-2.0
12 #
13 #    Unless required by applicable law or agreed to in writing, software
14 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
15 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
16 #    License for the specific language governing permissions and limitations
17 #    under the License.
18 
19 """Handles all requests relating to compute resources (e.g. guest VMs,
20 networking and storage of VMs, and compute hosts on which they run)."""
21 
22 import collections
23 import functools
24 import re
25 import string
26 
27 from castellan import key_manager
28 from oslo_log import log as logging
29 from oslo_messaging import exceptions as oslo_exceptions
30 from oslo_serialization import base64 as base64utils
31 from oslo_utils import excutils
32 from oslo_utils import strutils
33 from oslo_utils import timeutils
34 from oslo_utils import units
35 from oslo_utils import uuidutils
36 import six
37 from six.moves import range
38 
39 from nova import availability_zones
40 from nova import block_device
41 from nova.compute import flavors
42 from nova.compute import instance_actions
43 from nova.compute import instance_list
44 from nova.compute import migration_list
45 from nova.compute import power_state
46 from nova.compute import rpcapi as compute_rpcapi
47 from nova.compute import task_states
48 from nova.compute import utils as compute_utils
49 from nova.compute.utils import wrap_instance_event
50 from nova.compute import vm_states
51 from nova import conductor
52 import nova.conf
53 from nova import context as nova_context
54 from nova import crypto
55 from nova.db import base
56 from nova.db.sqlalchemy import api as db_api
57 from nova import exception
58 from nova import exception_wrapper
59 from nova import hooks
60 from nova.i18n import _
61 from nova import image
62 from nova import network
63 from nova.network import model as network_model
64 from nova.network.neutronv2 import constants
65 from nova.network.security_group import openstack_driver
66 from nova.network.security_group import security_group_base
67 from nova import objects
68 from nova.objects import base as obj_base
69 from nova.objects import block_device as block_device_obj
70 from nova.objects import external_event as external_event_obj
71 from nova.objects import fields as fields_obj
72 from nova.objects import keypair as keypair_obj
73 from nova.objects import quotas as quotas_obj
74 from nova.pci import request as pci_request
75 from nova.policies import servers as servers_policies
76 import nova.policy
77 from nova import profiler
78 from nova import rpc
79 from nova.scheduler.client import query
80 from nova.scheduler.client import report
81 from nova.scheduler import utils as scheduler_utils
82 from nova import servicegroup
83 from nova import utils
84 from nova.virt import hardware
85 from nova.volume import cinder
86 
87 LOG = logging.getLogger(__name__)
88 
89 get_notifier = functools.partial(rpc.get_notifier, service='compute')
90 # NOTE(gibi): legacy notification used compute as a service but these
91 # calls still run on the client side of the compute service which is
92 # nova-api. By setting the binary to nova-api below, we can make sure
93 # that the new versioned notifications has the right publisher_id but the
94 # legacy notifications does not change.
95 wrap_exception = functools.partial(exception_wrapper.wrap_exception,
96                                    get_notifier=get_notifier,
97                                    binary='nova-api')
98 CONF = nova.conf.CONF
99 
100 RO_SECURITY_GROUPS = ['default']
101 
102 AGGREGATE_ACTION_UPDATE = 'Update'
103 AGGREGATE_ACTION_UPDATE_META = 'UpdateMeta'
104 AGGREGATE_ACTION_DELETE = 'Delete'
105 AGGREGATE_ACTION_ADD = 'Add'
106 MIN_COMPUTE_ABORT_QUEUED_LIVE_MIGRATION = 34
107 MIN_COMPUTE_VOLUME_TYPE = 36
108 MIN_COMPUTE_SYNC_COMPUTE_STATUS_DISABLED = 38
109 
110 # FIXME(danms): Keep a global cache of the cells we find the
111 # first time we look. This needs to be refreshed on a timer or
112 # trigger.
113 CELLS = []
114 
115 
116 def check_instance_state(vm_state=None, task_state=(None,),
117                          must_have_launched=True):
118     """Decorator to check VM and/or task state before entry to API functions.
119 
120     If the instance is in the wrong state, or has not been successfully
121     started at least once the wrapper will raise an exception.
122     """
123 
124     if vm_state is not None and not isinstance(vm_state, set):
125         vm_state = set(vm_state)
126     if task_state is not None and not isinstance(task_state, set):
127         task_state = set(task_state)
128 
129     def outer(f):
130         @six.wraps(f)
131         def inner(self, context, instance, *args, **kw):
132             if vm_state is not None and instance.vm_state not in vm_state:
133                 raise exception.InstanceInvalidState(
134                     attr='vm_state',
135                     instance_uuid=instance.uuid,
136                     state=instance.vm_state,
137                     method=f.__name__)
138             if (task_state is not None and
139                     instance.task_state not in task_state):
140                 raise exception.InstanceInvalidState(
141                     attr='task_state',
142                     instance_uuid=instance.uuid,
143                     state=instance.task_state,
144                     method=f.__name__)
145             if must_have_launched and not instance.launched_at:
146                 raise exception.InstanceInvalidState(
147                     attr='launched_at',
148                     instance_uuid=instance.uuid,
149                     state=instance.launched_at,
150                     method=f.__name__)
151 
152             return f(self, context, instance, *args, **kw)
153         return inner
154     return outer
155 
156 
157 def _set_or_none(q):
158     return q if q is None or isinstance(q, set) else set(q)
159 
160 
161 def reject_instance_state(vm_state=None, task_state=None):
162     """Decorator.  Raise InstanceInvalidState if instance is in any of the
163     given states.
164     """
165 
166     vm_state = _set_or_none(vm_state)
167     task_state = _set_or_none(task_state)
168 
169     def outer(f):
170         @six.wraps(f)
171         def inner(self, context, instance, *args, **kw):
172             _InstanceInvalidState = functools.partial(
173                 exception.InstanceInvalidState,
174                 instance_uuid=instance.uuid,
175                 method=f.__name__)
176 
177             if vm_state is not None and instance.vm_state in vm_state:
178                 raise _InstanceInvalidState(
179                     attr='vm_state', state=instance.vm_state)
180 
181             if task_state is not None and instance.task_state in task_state:
182                 raise _InstanceInvalidState(
183                     attr='task_state', state=instance.task_state)
184 
185             return f(self, context, instance, *args, **kw)
186         return inner
187     return outer
188 
189 
190 def check_instance_host(function):
191     @six.wraps(function)
192     def wrapped(self, context, instance, *args, **kwargs):
193         if not instance.host:
194             raise exception.InstanceNotReady(instance_id=instance.uuid)
195         return function(self, context, instance, *args, **kwargs)
196     return wrapped
197 
198 
199 def check_instance_lock(function):
200     @six.wraps(function)
201     def inner(self, context, instance, *args, **kwargs):
202         if instance.locked and not context.is_admin:
203             raise exception.InstanceIsLocked(instance_uuid=instance.uuid)
204         return function(self, context, instance, *args, **kwargs)
205     return inner
206 
207 
208 def reject_sev_instances(operation):
209     """Decorator.  Raise OperationNotSupportedForSEV if instance has SEV
210     enabled.
211     """
212 
213     def outer(f):
214         @six.wraps(f)
215         def inner(self, context, instance, *args, **kw):
216             if hardware.get_mem_encryption_constraint(instance.flavor,
217                                                       instance.image_meta):
218                 raise exception.OperationNotSupportedForSEV(
219                     instance_uuid=instance.uuid,
220                     operation=operation)
221             return f(self, context, instance, *args, **kw)
222         return inner
223     return outer
224 
225 
226 def _diff_dict(orig, new):
227     """Return a dict describing how to change orig to new.  The keys
228     correspond to values that have changed; the value will be a list
229     of one or two elements.  The first element of the list will be
230     either '+' or '-', indicating whether the key was updated or
231     deleted; if the key was updated, the list will contain a second
232     element, giving the updated value.
233     """
234     # Figure out what keys went away
235     result = {k: ['-'] for k in set(orig.keys()) - set(new.keys())}
236     # Compute the updates
237     for key, value in new.items():
238         if key not in orig or value != orig[key]:
239             result[key] = ['+', value]
240     return result
241 
242 
243 def load_cells():
244     global CELLS
245     if not CELLS:
246         CELLS = objects.CellMappingList.get_all(
247             nova_context.get_admin_context())
248         LOG.debug('Found %(count)i cells: %(cells)s',
249                   dict(count=len(CELLS),
250                        cells=','.join([c.identity for c in CELLS])))
251 
252     if not CELLS:
253         LOG.error('No cells are configured, unable to continue')
254 
255 
256 def _get_image_meta_obj(image_meta_dict):
257     try:
258         image_meta = objects.ImageMeta.from_dict(image_meta_dict)
259     except ValueError as e:
260         # there must be invalid values in the image meta properties so
261         # consider this an invalid request
262         msg = _('Invalid image metadata. Error: %s') % six.text_type(e)
263         raise exception.InvalidRequest(msg)
264     return image_meta
265 
266 
267 @profiler.trace_cls("compute_api")
268 class API(base.Base):
269     """API for interacting with the compute manager."""
270 
271     def __init__(self, image_api=None, network_api=None, volume_api=None,
272                  security_group_api=None, **kwargs):
273         self.image_api = image_api or image.API()
274         self.network_api = network_api or network.API()
275         self.volume_api = volume_api or cinder.API()
276         self._placementclient = None  # Lazy-load on first access.
277         self.security_group_api = (security_group_api or
278             openstack_driver.get_openstack_security_group_driver())
279         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
280         self.compute_task_api = conductor.ComputeTaskAPI()
281         self.servicegroup_api = servicegroup.API()
282         self.host_api = HostAPI(self.compute_rpcapi, self.servicegroup_api)
283         self.notifier = rpc.get_notifier('compute', CONF.host)
284         if CONF.ephemeral_storage_encryption.enabled:
285             self.key_manager = key_manager.API()
286         # Help us to record host in EventReporter
287         self.host = CONF.host
288         super(API, self).__init__(**kwargs)
289 
290     def _record_action_start(self, context, instance, action):
291         objects.InstanceAction.action_start(context, instance.uuid,
292                                             action, want_result=False)
293 
294     def _check_injected_file_quota(self, context, injected_files):
295         """Enforce quota limits on injected files.
296 
297         Raises a QuotaError if any limit is exceeded.
298         """
299         if injected_files is None:
300             return
301 
302         # Check number of files first
303         try:
304             objects.Quotas.limit_check(context,
305                                        injected_files=len(injected_files))
306         except exception.OverQuota:
307             raise exception.OnsetFileLimitExceeded()
308 
309         # OK, now count path and content lengths; we're looking for
310         # the max...
311         max_path = 0
312         max_content = 0
313         for path, content in injected_files:
314             max_path = max(max_path, len(path))
315             max_content = max(max_content, len(content))
316 
317         try:
318             objects.Quotas.limit_check(context,
319                                        injected_file_path_bytes=max_path,
320                                        injected_file_content_bytes=max_content)
321         except exception.OverQuota as exc:
322             # Favor path limit over content limit for reporting
323             # purposes
324             if 'injected_file_path_bytes' in exc.kwargs['overs']:
325                 raise exception.OnsetFilePathLimitExceeded(
326                       allowed=exc.kwargs['quotas']['injected_file_path_bytes'])
327             else:
328                 raise exception.OnsetFileContentLimitExceeded(
329                    allowed=exc.kwargs['quotas']['injected_file_content_bytes'])
330 
331     def _check_metadata_properties_quota(self, context, metadata=None):
332         """Enforce quota limits on metadata properties."""
333         if not metadata:
334             metadata = {}
335         if not isinstance(metadata, dict):
336             msg = (_("Metadata type should be dict."))
337             raise exception.InvalidMetadata(reason=msg)
338         num_metadata = len(metadata)
339         try:
340             objects.Quotas.limit_check(context, metadata_items=num_metadata)
341         except exception.OverQuota as exc:
342             quota_metadata = exc.kwargs['quotas']['metadata_items']
343             raise exception.MetadataLimitExceeded(allowed=quota_metadata)
344 
345         # Because metadata is stored in the DB, we hard-code the size limits
346         # In future, we may support more variable length strings, so we act
347         #  as if this is quota-controlled for forwards compatibility.
348         # Those are only used in V2 API, from V2.1 API, those checks are
349         # validated at API layer schema validation.
350         for k, v in metadata.items():
351             try:
352                 utils.check_string_length(v)
353                 utils.check_string_length(k, min_length=1)
354             except exception.InvalidInput as e:
355                 raise exception.InvalidMetadata(reason=e.format_message())
356 
357             if len(k) > 255:
358                 msg = _("Metadata property key greater than 255 characters")
359                 raise exception.InvalidMetadataSize(reason=msg)
360             if len(v) > 255:
361                 msg = _("Metadata property value greater than 255 characters")
362                 raise exception.InvalidMetadataSize(reason=msg)
363 
364     def _check_requested_secgroups(self, context, secgroups):
365         """Check if the security group requested exists and belongs to
366         the project.
367 
368         :param context: The nova request context.
369         :type context: nova.context.RequestContext
370         :param secgroups: list of requested security group names, or uuids in
371             the case of Neutron.
372         :type secgroups: list
373         :returns: list of requested security group names unmodified if using
374             nova-network. If using Neutron, the list returned is all uuids.
375             Note that 'default' is a special case and will be unmodified if
376             it's requested.
377         """
378         security_groups = []
379         for secgroup in secgroups:
380             # NOTE(sdague): default is handled special
381             if secgroup == "default":
382                 security_groups.append(secgroup)
383                 continue
384             secgroup_dict = self.security_group_api.get(context, secgroup)
385             if not secgroup_dict:
386                 raise exception.SecurityGroupNotFoundForProject(
387                     project_id=context.project_id, security_group_id=secgroup)
388 
389             # Check to see if it's a nova-network or neutron type.
390             if isinstance(secgroup_dict['id'], int):
391                 # This is nova-network so just return the requested name.
392                 security_groups.append(secgroup)
393             else:
394                 # The id for neutron is a uuid, so we return the id (uuid).
395                 security_groups.append(secgroup_dict['id'])
396 
397         return security_groups
398 
399     def _check_requested_networks(self, context, requested_networks,
400                                   max_count):
401         """Check if the networks requested belongs to the project
402         and the fixed IP address for each network provided is within
403         same the network block
404         """
405         if requested_networks is not None:
406             if requested_networks.no_allocate:
407                 # If the network request was specifically 'none' meaning don't
408                 # allocate any networks, we just return the number of requested
409                 # instances since quotas don't change at all.
410                 return max_count
411 
412             # NOTE(danms): Temporary transition
413             requested_networks = requested_networks.as_tuples()
414 
415         return self.network_api.validate_networks(context, requested_networks,
416                                                   max_count)
417 
418     def _handle_kernel_and_ramdisk(self, context, kernel_id, ramdisk_id,
419                                    image):
420         """Choose kernel and ramdisk appropriate for the instance.
421 
422         The kernel and ramdisk can be chosen in one of two ways:
423 
424             1. Passed in with create-instance request.
425 
426             2. Inherited from image metadata.
427 
428         If inherited from image metadata, and if that image metadata value is
429         set to 'nokernel', both kernel and ramdisk will default to None.
430         """
431         # Inherit from image if not specified
432         image_properties = image.get('properties', {})
433 
434         if kernel_id is None:
435             kernel_id = image_properties.get('kernel_id')
436 
437         if ramdisk_id is None:
438             ramdisk_id = image_properties.get('ramdisk_id')
439 
440         # Force to None if kernel_id indicates that a kernel is not to be used
441         if kernel_id == 'nokernel':
442             kernel_id = None
443             ramdisk_id = None
444 
445         # Verify kernel and ramdisk exist (fail-fast)
446         if kernel_id is not None:
447             kernel_image = self.image_api.get(context, kernel_id)
448             # kernel_id could have been a URI, not a UUID, so to keep behaviour
449             # from before, which leaked that implementation detail out to the
450             # caller, we return the image UUID of the kernel image and ramdisk
451             # image (below) and not any image URIs that might have been
452             # supplied.
453             # TODO(jaypipes): Get rid of this silliness once we move to a real
454             # Image object and hide all of that stuff within nova.image.api.
455             kernel_id = kernel_image['id']
456 
457         if ramdisk_id is not None:
458             ramdisk_image = self.image_api.get(context, ramdisk_id)
459             ramdisk_id = ramdisk_image['id']
460 
461         return kernel_id, ramdisk_id
462 
463     @staticmethod
464     def parse_availability_zone(context, availability_zone):
465         # NOTE(vish): We have a legacy hack to allow admins to specify hosts
466         #             via az using az:host:node. It might be nice to expose an
467         #             api to specify specific hosts to force onto, but for
468         #             now it just supports this legacy hack.
469         # NOTE(deva): It is also possible to specify az::node, in which case
470         #             the host manager will determine the correct host.
471         forced_host = None
472         forced_node = None
473         if availability_zone and ':' in availability_zone:
474             c = availability_zone.count(':')
475             if c == 1:
476                 availability_zone, forced_host = availability_zone.split(':')
477             elif c == 2:
478                 if '::' in availability_zone:
479                     availability_zone, forced_node = \
480                             availability_zone.split('::')
481                 else:
482                     availability_zone, forced_host, forced_node = \
483                             availability_zone.split(':')
484             else:
485                 raise exception.InvalidInput(
486                         reason="Unable to parse availability_zone")
487 
488         if not availability_zone:
489             availability_zone = CONF.default_schedule_zone
490 
491         return availability_zone, forced_host, forced_node
492 
493     def _ensure_auto_disk_config_is_valid(self, auto_disk_config_img,
494                                           auto_disk_config, image):
495         auto_disk_config_disabled = \
496                 utils.is_auto_disk_config_disabled(auto_disk_config_img)
497         if auto_disk_config_disabled and auto_disk_config:
498             raise exception.AutoDiskConfigDisabledByImage(image=image)
499 
500     def _inherit_properties_from_image(self, image, auto_disk_config):
501         image_properties = image.get('properties', {})
502         auto_disk_config_img = \
503                 utils.get_auto_disk_config_from_image_props(image_properties)
504         self._ensure_auto_disk_config_is_valid(auto_disk_config_img,
505                                                auto_disk_config,
506                                                image.get("id"))
507         if auto_disk_config is None:
508             auto_disk_config = strutils.bool_from_string(auto_disk_config_img)
509 
510         return {
511             'os_type': image_properties.get('os_type'),
512             'architecture': image_properties.get('architecture'),
513             'vm_mode': image_properties.get('vm_mode'),
514             'auto_disk_config': auto_disk_config
515         }
516 
517     def _check_config_drive(self, config_drive):
518         if config_drive:
519             try:
520                 bool_val = strutils.bool_from_string(config_drive,
521                                                      strict=True)
522             except ValueError:
523                 raise exception.ConfigDriveInvalidValue(option=config_drive)
524         else:
525             bool_val = False
526         # FIXME(comstud):  Bug ID 1193438 filed for this. This looks silly,
527         # but this is because the config drive column is a String.  False
528         # is represented by using an empty string.  And for whatever
529         # reason, we rely on the DB to cast True to a String.
530         return True if bool_val else ''
531 
532     def _validate_flavor_image(self, context, image_id, image,
533                                instance_type, root_bdm, validate_numa=True):
534         """Validate the flavor and image.
535 
536         This is called from the API service to ensure that the flavor
537         extra-specs and image properties are self-consistent and compatible
538         with each other.
539 
540         :param context: A context.RequestContext
541         :param image_id: UUID of the image
542         :param image: a dict representation of the image including properties,
543                       enforces the image status is active.
544         :param instance_type: Flavor object
545         :param root_bdm: BlockDeviceMapping for root disk.  Will be None for
546                the resize case.
547         :param validate_numa: Flag to indicate whether or not to validate
548                the NUMA-related metadata.
549         :raises: Many different possible exceptions.  See
550                  api.openstack.compute.servers.INVALID_FLAVOR_IMAGE_EXCEPTIONS
551                  for the full list.
552         """
553         if image and image['status'] != 'active':
554             raise exception.ImageNotActive(image_id=image_id)
555         self._validate_flavor_image_nostatus(context, image, instance_type,
556                                              root_bdm, validate_numa)
557 
558     @staticmethod
559     def _validate_flavor_image_nostatus(context, image, instance_type,
560                                         root_bdm, validate_numa=True,
561                                         validate_pci=False):
562         """Validate the flavor and image.
563 
564         This is called from the API service to ensure that the flavor
565         extra-specs and image properties are self-consistent and compatible
566         with each other.
567 
568         :param context: A context.RequestContext
569         :param image: a dict representation of the image including properties
570         :param instance_type: Flavor object
571         :param root_bdm: BlockDeviceMapping for root disk.  Will be None for
572                the resize case.
573         :param validate_numa: Flag to indicate whether or not to validate
574                the NUMA-related metadata.
575         :param validate_pci: Flag to indicate whether or not to validate
576                the PCI-related metadata.
577         :raises: Many different possible exceptions.  See
578                  api.openstack.compute.servers.INVALID_FLAVOR_IMAGE_EXCEPTIONS
579                  for the full list.
580         """
581         if not image:
582             return
583 
584         image_properties = image.get('properties', {})
585         config_drive_option = image_properties.get(
586             'img_config_drive', 'optional')
587         if config_drive_option not in ['optional', 'mandatory']:
588             raise exception.InvalidImageConfigDrive(
589                 config_drive=config_drive_option)
590 
591         if instance_type['memory_mb'] < int(image.get('min_ram') or 0):
592             raise exception.FlavorMemoryTooSmall()
593 
594         # Image min_disk is in gb, size is in bytes. For sanity, have them both
595         # in bytes.
596         image_min_disk = int(image.get('min_disk') or 0) * units.Gi
597         image_size = int(image.get('size') or 0)
598 
599         # Target disk is a volume. Don't check flavor disk size because it
600         # doesn't make sense, and check min_disk against the volume size.
601         if root_bdm is not None and root_bdm.is_volume:
602             # There are 2 possibilities here:
603             #
604             # 1. The target volume already exists but bdm.volume_size is not
605             #    yet set because this method is called before
606             #    _bdm_validate_set_size_and_instance during server create.
607             # 2. The target volume doesn't exist, in which case the bdm will
608             #    contain the intended volume size
609             #
610             # Note that rebuild also calls this method with potentially a new
611             # image but you can't rebuild a volume-backed server with a new
612             # image (yet).
613             #
614             # Cinder does its own check against min_disk, so if the target
615             # volume already exists this has already been done and we don't
616             # need to check it again here. In this case, volume_size may not be
617             # set on the bdm.
618             #
619             # If we're going to create the volume, the bdm will contain
620             # volume_size. Therefore we should check it if it exists. This will
621             # still be checked again by cinder when the volume is created, but
622             # that will not happen until the request reaches a host. By
623             # checking it here, the user gets an immediate and useful failure
624             # indication.
625             #
626             # The third possibility is that we have failed to consider
627             # something, and there are actually more than 2 possibilities. In
628             # this case cinder will still do the check at volume creation time.
629             # The behaviour will still be correct, but the user will not get an
630             # immediate failure from the api, and will instead have to
631             # determine why the instance is in an error state with a task of
632             # block_device_mapping.
633             #
634             # We could reasonably refactor this check into _validate_bdm at
635             # some future date, as the various size logic is already split out
636             # in there.
637             dest_size = root_bdm.volume_size
638             if dest_size is not None:
639                 dest_size *= units.Gi
640 
641                 if image_min_disk > dest_size:
642                     raise exception.VolumeSmallerThanMinDisk(
643                         volume_size=dest_size, image_min_disk=image_min_disk)
644 
645         # Target disk is a local disk whose size is taken from the flavor
646         else:
647             dest_size = instance_type['root_gb'] * units.Gi
648 
649             # NOTE(johannes): root_gb is allowed to be 0 for legacy reasons
650             # since libvirt interpreted the value differently than other
651             # drivers. A value of 0 means don't check size.
652             if dest_size != 0:
653                 if image_size > dest_size:
654                     raise exception.FlavorDiskSmallerThanImage(
655                         flavor_size=dest_size, image_size=image_size)
656 
657                 if image_min_disk > dest_size:
658                     raise exception.FlavorDiskSmallerThanMinDisk(
659                         flavor_size=dest_size, image_min_disk=image_min_disk)
660             else:
661                 # The user is attempting to create a server with a 0-disk
662                 # image-backed flavor, which can lead to issues with a large
663                 # image consuming an unexpectedly large amount of local disk
664                 # on the compute host. Check to see if the deployment will
665                 # allow that.
666                 if not context.can(
667                         servers_policies.ZERO_DISK_FLAVOR, fatal=False):
668                     raise exception.BootFromVolumeRequiredForZeroDiskFlavor()
669 
670         API._validate_flavor_image_numa_pci(
671             image, instance_type, validate_numa=validate_numa,
672             validate_pci=validate_pci)
673 
674     @staticmethod
675     def _validate_flavor_image_numa_pci(image, instance_type,
676                                         validate_numa=True,
677                                         validate_pci=False):
678         """Validate the flavor and image NUMA/PCI values.
679 
680         This is called from the API service to ensure that the flavor
681         extra-specs and image properties are self-consistent and compatible
682         with each other.
683 
684         :param image: a dict representation of the image including properties
685         :param instance_type: Flavor object
686         :param validate_numa: Flag to indicate whether or not to validate
687                the NUMA-related metadata.
688         :param validate_pci: Flag to indicate whether or not to validate
689                the PCI-related metadata.
690         :raises: Many different possible exceptions.  See
691                  api.openstack.compute.servers.INVALID_FLAVOR_IMAGE_EXCEPTIONS
692                  for the full list.
693         """
694         image_meta = _get_image_meta_obj(image)
695 
696         API._validate_flavor_image_mem_encryption(instance_type, image_meta)
697 
698         # validate PMU extra spec and image metadata
699         flavor_pmu = instance_type.extra_specs.get('hw:pmu')
700         image_pmu = image_meta.properties.get('hw_pmu')
701         if (flavor_pmu is not None and image_pmu is not None and
702                 image_pmu != strutils.bool_from_string(flavor_pmu)):
703             raise exception.ImagePMUConflict()
704 
705         # Only validate values of flavor/image so the return results of
706         # following 'get' functions are not used.
707         hardware.get_number_of_serial_ports(instance_type, image_meta)
708         if hardware.is_realtime_enabled(instance_type):
709             hardware.vcpus_realtime_topology(instance_type, image_meta)
710         hardware.get_cpu_topology_constraints(instance_type, image_meta)
711         if validate_numa:
712             hardware.numa_get_constraints(instance_type, image_meta)
713         if validate_pci:
714             pci_request.get_pci_requests_from_flavor(instance_type)
715 
716     @staticmethod
717     def _validate_flavor_image_mem_encryption(instance_type, image):
718         """Validate that the flavor and image don't make contradictory
719         requests regarding memory encryption.
720 
721         :param instance_type: Flavor object
722         :param image: an ImageMeta object
723         :raises: nova.exception.FlavorImageConflict
724         """
725         # This library function will raise the exception for us if
726         # necessary; if not, we can ignore the result returned.
727         hardware.get_mem_encryption_constraint(instance_type, image)
728 
729     def _get_image_defined_bdms(self, instance_type, image_meta,
730                                 root_device_name):
731         image_properties = image_meta.get('properties', {})
732 
733         # Get the block device mappings defined by the image.
734         image_defined_bdms = image_properties.get('block_device_mapping', [])
735         legacy_image_defined = not image_properties.get('bdm_v2', False)
736 
737         image_mapping = image_properties.get('mappings', [])
738 
739         if legacy_image_defined:
740             image_defined_bdms = block_device.from_legacy_mapping(
741                 image_defined_bdms, None, root_device_name)
742         else:
743             image_defined_bdms = list(map(block_device.BlockDeviceDict,
744                                           image_defined_bdms))
745 
746         if image_mapping:
747             image_mapping = self._prepare_image_mapping(instance_type,
748                                                         image_mapping)
749             image_defined_bdms = self._merge_bdms_lists(
750                 image_mapping, image_defined_bdms)
751 
752         return image_defined_bdms
753 
754     def _get_flavor_defined_bdms(self, instance_type, block_device_mapping):
755         flavor_defined_bdms = []
756 
757         have_ephemeral_bdms = any(filter(
758             block_device.new_format_is_ephemeral, block_device_mapping))
759         have_swap_bdms = any(filter(
760             block_device.new_format_is_swap, block_device_mapping))
761 
762         if instance_type.get('ephemeral_gb') and not have_ephemeral_bdms:
763             flavor_defined_bdms.append(
764                 block_device.create_blank_bdm(instance_type['ephemeral_gb']))
765         if instance_type.get('swap') and not have_swap_bdms:
766             flavor_defined_bdms.append(
767                 block_device.create_blank_bdm(instance_type['swap'], 'swap'))
768 
769         return flavor_defined_bdms
770 
771     def _merge_bdms_lists(self, overridable_mappings, overrider_mappings):
772         """Override any block devices from the first list by device name
773 
774         :param overridable_mappings: list which items are overridden
775         :param overrider_mappings: list which items override
776 
777         :returns: A merged list of bdms
778         """
779         device_names = set(bdm['device_name'] for bdm in overrider_mappings
780                            if bdm['device_name'])
781         return (overrider_mappings +
782                 [bdm for bdm in overridable_mappings
783                  if bdm['device_name'] not in device_names])
784 
785     def _check_and_transform_bdm(self, context, base_options, instance_type,
786                                  image_meta, min_count, max_count,
787                                  block_device_mapping, legacy_bdm):
788         # NOTE (ndipanov): Assume root dev name is 'vda' if not supplied.
789         #                  It's needed for legacy conversion to work.
790         root_device_name = (base_options.get('root_device_name') or 'vda')
791         image_ref = base_options.get('image_ref', '')
792         # If the instance is booted by image and has a volume attached,
793         # the volume cannot have the same device name as root_device_name
794         if image_ref:
795             for bdm in block_device_mapping:
796                 if (bdm.get('destination_type') == 'volume' and
797                     block_device.strip_dev(bdm.get(
798                     'device_name')) == root_device_name):
799                     msg = _('The volume cannot be assigned the same device'
800                             ' name as the root device %s') % root_device_name
801                     raise exception.InvalidRequest(msg)
802 
803         image_defined_bdms = self._get_image_defined_bdms(
804             instance_type, image_meta, root_device_name)
805         root_in_image_bdms = (
806             block_device.get_root_bdm(image_defined_bdms) is not None)
807 
808         if legacy_bdm:
809             block_device_mapping = block_device.from_legacy_mapping(
810                 block_device_mapping, image_ref, root_device_name,
811                 no_root=root_in_image_bdms)
812         elif root_in_image_bdms:
813             # NOTE (ndipanov): client will insert an image mapping into the v2
814             # block_device_mapping, but if there is a bootable device in image
815             # mappings - we need to get rid of the inserted image
816             # NOTE (gibi): another case is when a server is booted with an
817             # image to bdm mapping where the image only contains a bdm to a
818             # snapshot. In this case the other image to bdm mapping
819             # contains an unnecessary device with boot_index == 0.
820             # Also in this case the image_ref is None as we are booting from
821             # an image to volume bdm.
822             def not_image_and_root_bdm(bdm):
823                 return not (bdm.get('boot_index') == 0 and
824                             bdm.get('source_type') == 'image')
825 
826             block_device_mapping = list(
827                 filter(not_image_and_root_bdm, block_device_mapping))
828 
829         block_device_mapping = self._merge_bdms_lists(
830             image_defined_bdms, block_device_mapping)
831 
832         if min_count > 1 or max_count > 1:
833             if any(map(lambda bdm: bdm['source_type'] == 'volume',
834                        block_device_mapping)):
835                 msg = _('Cannot attach one or more volumes to multiple'
836                         ' instances')
837                 raise exception.InvalidRequest(msg)
838 
839         block_device_mapping += self._get_flavor_defined_bdms(
840             instance_type, block_device_mapping)
841 
842         return block_device_obj.block_device_make_list_from_dicts(
843                 context, block_device_mapping)
844 
845     def _get_image(self, context, image_href):
846         if not image_href:
847             return None, {}
848 
849         image = self.image_api.get(context, image_href)
850         return image['id'], image
851 
852     def _checks_for_create_and_rebuild(self, context, image_id, image,
853                                        instance_type, metadata,
854                                        files_to_inject, root_bdm,
855                                        validate_numa=True):
856         self._check_metadata_properties_quota(context, metadata)
857         self._check_injected_file_quota(context, files_to_inject)
858         self._validate_flavor_image(context, image_id, image,
859                                     instance_type, root_bdm,
860                                     validate_numa=validate_numa)
861 
862     def _validate_and_build_base_options(self, context, instance_type,
863                                          boot_meta, image_href, image_id,
864                                          kernel_id, ramdisk_id, display_name,
865                                          display_description, key_name,
866                                          key_data, security_groups,
867                                          availability_zone, user_data,
868                                          metadata, access_ip_v4, access_ip_v6,
869                                          requested_networks, config_drive,
870                                          auto_disk_config, reservation_id,
871                                          max_count,
872                                          supports_port_resource_request):
873         """Verify all the input parameters regardless of the provisioning
874         strategy being performed.
875         """
876         if instance_type['disabled']:
877             raise exception.FlavorNotFound(flavor_id=instance_type['id'])
878 
879         if user_data:
880             try:
881                 base64utils.decode_as_bytes(user_data)
882             except TypeError:
883                 raise exception.InstanceUserDataMalformed()
884 
885         # When using Neutron, _check_requested_secgroups will translate and
886         # return any requested security group names to uuids.
887         security_groups = (
888             self._check_requested_secgroups(context, security_groups))
889 
890         # Note:  max_count is the number of instances requested by the user,
891         # max_network_count is the maximum number of instances taking into
892         # account any network quotas
893         max_network_count = self._check_requested_networks(context,
894                                      requested_networks, max_count)
895 
896         kernel_id, ramdisk_id = self._handle_kernel_and_ramdisk(
897                 context, kernel_id, ramdisk_id, boot_meta)
898 
899         config_drive = self._check_config_drive(config_drive)
900 
901         if key_data is None and key_name is not None:
902             key_pair = objects.KeyPair.get_by_name(context,
903                                                    context.user_id,
904                                                    key_name)
905             key_data = key_pair.public_key
906         else:
907             key_pair = None
908 
909         root_device_name = block_device.prepend_dev(
910                 block_device.properties_root_device_name(
911                     boot_meta.get('properties', {})))
912 
913         image_meta = _get_image_meta_obj(boot_meta)
914         numa_topology = hardware.numa_get_constraints(
915                 instance_type, image_meta)
916 
917         system_metadata = {}
918 
919         # PCI requests come from two sources: instance flavor and
920         # requested_networks. The first call in below returns an
921         # InstancePCIRequests object which is a list of InstancePCIRequest
922         # objects. The second call in below creates an InstancePCIRequest
923         # object for each SR-IOV port, and append it to the list in the
924         # InstancePCIRequests object
925         pci_request_info = pci_request.get_pci_requests_from_flavor(
926             instance_type)
927         result = self.network_api.create_resource_requests(
928             context, requested_networks, pci_request_info)
929         network_metadata, port_resource_requests = result
930 
931         # Creating servers with ports that have resource requests, like QoS
932         # minimum bandwidth rules, is only supported in a requested minimum
933         # microversion.
934         if port_resource_requests and not supports_port_resource_request:
935             raise exception.CreateWithPortResourceRequestOldVersion()
936 
937         base_options = {
938             'reservation_id': reservation_id,
939             'image_ref': image_href,
940             'kernel_id': kernel_id or '',
941             'ramdisk_id': ramdisk_id or '',
942             'power_state': power_state.NOSTATE,
943             'vm_state': vm_states.BUILDING,
944             'config_drive': config_drive,
945             'user_id': context.user_id,
946             'project_id': context.project_id,
947             'instance_type_id': instance_type['id'],
948             'memory_mb': instance_type['memory_mb'],
949             'vcpus': instance_type['vcpus'],
950             'root_gb': instance_type['root_gb'],
951             'ephemeral_gb': instance_type['ephemeral_gb'],
952             'display_name': display_name,
953             'display_description': display_description,
954             'user_data': user_data,
955             'key_name': key_name,
956             'key_data': key_data,
957             'locked': False,
958             'metadata': metadata or {},
959             'access_ip_v4': access_ip_v4,
960             'access_ip_v6': access_ip_v6,
961             'availability_zone': availability_zone,
962             'root_device_name': root_device_name,
963             'progress': 0,
964             'pci_requests': pci_request_info,
965             'numa_topology': numa_topology,
966             'system_metadata': system_metadata,
967             'port_resource_requests': port_resource_requests}
968 
969         options_from_image = self._inherit_properties_from_image(
970                 boot_meta, auto_disk_config)
971 
972         base_options.update(options_from_image)
973 
974         # return the validated options and maximum number of instances allowed
975         # by the network quotas
976         return (base_options, max_network_count, key_pair, security_groups,
977                 network_metadata)
978 
979     @staticmethod
980     @db_api.api_context_manager.writer
981     def _create_reqspec_buildreq_instmapping(context, rs, br, im):
982         """Create the request spec, build request, and instance mapping in a
983         single database transaction.
984 
985         The RequestContext must be passed in to this method so that the
986         database transaction context manager decorator will nest properly and
987         include each create() into the same transaction context.
988         """
989         rs.create()
990         br.create()
991         im.create()
992 
993     def _validate_host_or_node(self, context, host, hypervisor_hostname):
994         """Check whether compute nodes exist by validating the host
995         and/or the hypervisor_hostname. There are three cases:
996         1. If only host is supplied, we can lookup the HostMapping in
997         the API DB.
998         2. If only node is supplied, we can query a resource provider
999         with that name in placement.
1000         3. If both host and node are supplied, we can get the cell from
1001         HostMapping and from that lookup the ComputeNode with the
1002         given cell.
1003 
1004         :param context: The API request context.
1005         :param host: Target host.
1006         :param hypervisor_hostname: Target node.
1007         :raises: ComputeHostNotFound if we find no compute nodes with host
1008                  and/or hypervisor_hostname.
1009         """
1010 
1011         if host:
1012             # When host is specified.
1013             try:
1014                 host_mapping = objects.HostMapping.get_by_host(context, host)
1015             except exception.HostMappingNotFound:
1016                 LOG.warning('No host-to-cell mapping found for host '
1017                             '%(host)s.', {'host': host})
1018                 raise exception.ComputeHostNotFound(host=host)
1019             # When both host and node are specified.
1020             if hypervisor_hostname:
1021                 cell = host_mapping.cell_mapping
1022                 with nova_context.target_cell(context, cell) as cctxt:
1023                     # Here we only do an existence check, so we don't
1024                     # need to store the return value into a variable.
1025                     objects.ComputeNode.get_by_host_and_nodename(
1026                         cctxt, host, hypervisor_hostname)
1027         elif hypervisor_hostname:
1028             # When only node is specified.
1029             try:
1030                 self.placementclient.get_provider_by_name(
1031                     context, hypervisor_hostname)
1032             except exception.ResourceProviderNotFound:
1033                 raise exception.ComputeHostNotFound(host=hypervisor_hostname)
1034 
1035     def _provision_instances(self, context, instance_type, min_count,
1036             max_count, base_options, boot_meta, security_groups,
1037             block_device_mapping, shutdown_terminate,
1038             instance_group, check_server_group_quota, filter_properties,
1039             key_pair, tags, trusted_certs, supports_multiattach,
1040             network_metadata=None, requested_host=None,
1041             requested_hypervisor_hostname=None):
1042         # NOTE(boxiang): Check whether compute nodes exist by validating
1043         # the host and/or the hypervisor_hostname. Pass the destination
1044         # to the scheduler with host and/or hypervisor_hostname(node).
1045         destination = None
1046         if requested_host or requested_hypervisor_hostname:
1047             self._validate_host_or_node(context, requested_host,
1048                                         requested_hypervisor_hostname)
1049             destination = objects.Destination()
1050             if requested_host:
1051                 destination.host = requested_host
1052             destination.node = requested_hypervisor_hostname
1053         # Check quotas
1054         num_instances = compute_utils.check_num_instances_quota(
1055                 context, instance_type, min_count, max_count)
1056         security_groups = self.security_group_api.populate_security_groups(
1057                 security_groups)
1058         self.security_group_api.ensure_default(context)
1059         port_resource_requests = base_options.pop('port_resource_requests')
1060         LOG.debug("Going to run %s instances...", num_instances)
1061         instances_to_build = []
1062         try:
1063             for i in range(num_instances):
1064                 # Create a uuid for the instance so we can store the
1065                 # RequestSpec before the instance is created.
1066                 instance_uuid = uuidutils.generate_uuid()
1067                 # Store the RequestSpec that will be used for scheduling.
1068                 req_spec = objects.RequestSpec.from_components(context,
1069                         instance_uuid, boot_meta, instance_type,
1070                         base_options['numa_topology'],
1071                         base_options['pci_requests'], filter_properties,
1072                         instance_group, base_options['availability_zone'],
1073                         security_groups=security_groups,
1074                         port_resource_requests=port_resource_requests)
1075 
1076                 if block_device_mapping:
1077                     # Record whether or not we are a BFV instance
1078                     root = block_device_mapping.root_bdm()
1079                     req_spec.is_bfv = bool(root and root.is_volume)
1080                 else:
1081                     # If we have no BDMs, we're clearly not BFV
1082                     req_spec.is_bfv = False
1083 
1084                 # NOTE(danms): We need to record num_instances on the request
1085                 # spec as this is how the conductor knows how many were in this
1086                 # batch.
1087                 req_spec.num_instances = num_instances
1088 
1089                 # NOTE(stephenfin): The network_metadata field is not persisted
1090                 # inside RequestSpec object.
1091                 if network_metadata:
1092                     req_spec.network_metadata = network_metadata
1093 
1094                 if destination:
1095                     req_spec.requested_destination = destination
1096 
1097                 # Create an instance object, but do not store in db yet.
1098                 instance = objects.Instance(context=context)
1099                 instance.uuid = instance_uuid
1100                 instance.update(base_options)
1101                 instance.keypairs = objects.KeyPairList(objects=[])
1102                 if key_pair:
1103                     instance.keypairs.objects.append(key_pair)
1104 
1105                 instance.trusted_certs = self._retrieve_trusted_certs_object(
1106                     context, trusted_certs)
1107 
1108                 instance = self.create_db_entry_for_new_instance(context,
1109                         instance_type, boot_meta, instance, security_groups,
1110                         block_device_mapping, num_instances, i,
1111                         shutdown_terminate, create_instance=False)
1112                 block_device_mapping = (
1113                     self._bdm_validate_set_size_and_instance(context,
1114                         instance, instance_type, block_device_mapping,
1115                         supports_multiattach))
1116                 instance_tags = self._transform_tags(tags, instance.uuid)
1117 
1118                 build_request = objects.BuildRequest(context,
1119                         instance=instance, instance_uuid=instance.uuid,
1120                         project_id=instance.project_id,
1121                         block_device_mappings=block_device_mapping,
1122                         tags=instance_tags)
1123 
1124                 # Create an instance_mapping.  The null cell_mapping indicates
1125                 # that the instance doesn't yet exist in a cell, and lookups
1126                 # for it need to instead look for the RequestSpec.
1127                 # cell_mapping will be populated after scheduling, with a
1128                 # scheduling failure using the cell_mapping for the special
1129                 # cell0.
1130                 inst_mapping = objects.InstanceMapping(context=context)
1131                 inst_mapping.instance_uuid = instance_uuid
1132                 inst_mapping.project_id = context.project_id
1133                 inst_mapping.user_id = context.user_id
1134                 inst_mapping.cell_mapping = None
1135 
1136                 # Create the request spec, build request, and instance mapping
1137                 # records in a single transaction so that if a DBError is
1138                 # raised from any of them, all INSERTs will be rolled back and
1139                 # no orphaned records will be left behind.
1140                 self._create_reqspec_buildreq_instmapping(context, req_spec,
1141                                                           build_request,
1142                                                           inst_mapping)
1143 
1144                 instances_to_build.append(
1145                     (req_spec, build_request, inst_mapping))
1146 
1147                 if instance_group:
1148                     if check_server_group_quota:
1149                         try:
1150                             objects.Quotas.check_deltas(
1151                                 context, {'server_group_members': 1},
1152                                 instance_group, context.user_id)
1153                         except exception.OverQuota:
1154                             msg = _("Quota exceeded, too many servers in "
1155                                     "group")
1156                             raise exception.QuotaError(msg)
1157 
1158                     members = objects.InstanceGroup.add_members(
1159                         context, instance_group.uuid, [instance.uuid])
1160 
1161                     # NOTE(melwitt): We recheck the quota after creating the
1162                     # object to prevent users from allocating more resources
1163                     # than their allowed quota in the event of a race. This is
1164                     # configurable because it can be expensive if strict quota
1165                     # limits are not required in a deployment.
1166                     if CONF.quota.recheck_quota and check_server_group_quota:
1167                         try:
1168                             objects.Quotas.check_deltas(
1169                                 context, {'server_group_members': 0},
1170                                 instance_group, context.user_id)
1171                         except exception.OverQuota:
1172                             objects.InstanceGroup._remove_members_in_db(
1173                                 context, instance_group.id, [instance.uuid])
1174                             msg = _("Quota exceeded, too many servers in "
1175                                     "group")
1176                             raise exception.QuotaError(msg)
1177                     # list of members added to servers group in this iteration
1178                     # is needed to check quota of server group during add next
1179                     # instance
1180                     instance_group.members.extend(members)
1181 
1182         # In the case of any exceptions, attempt DB cleanup
1183         except Exception:
1184             with excutils.save_and_reraise_exception():
1185                 self._cleanup_build_artifacts(None, instances_to_build)
1186 
1187         return instances_to_build
1188 
1189     @staticmethod
1190     def _retrieve_trusted_certs_object(context, trusted_certs, rebuild=False):
1191         """Convert user-requested trusted cert IDs to TrustedCerts object
1192 
1193         Also validates that the deployment is new enough to support trusted
1194         image certification validation.
1195 
1196         :param context: The user request auth context
1197         :param trusted_certs: list of user-specified trusted cert string IDs,
1198             may be None
1199         :param rebuild: True if rebuilding the server, False if creating a
1200             new server
1201         :returns: nova.objects.TrustedCerts object or None if no user-specified
1202             trusted cert IDs were given and nova is not configured with
1203             default trusted cert IDs
1204         """
1205         # Retrieve trusted_certs parameter, or use CONF value if certificate
1206         # validation is enabled
1207         if trusted_certs:
1208             certs_to_return = objects.TrustedCerts(ids=trusted_certs)
1209         elif (CONF.glance.verify_glance_signatures and
1210               CONF.glance.enable_certificate_validation and
1211               CONF.glance.default_trusted_certificate_ids):
1212             certs_to_return = objects.TrustedCerts(
1213                 ids=CONF.glance.default_trusted_certificate_ids)
1214         else:
1215             return None
1216 
1217         return certs_to_return
1218 
1219     def _get_bdm_image_metadata(self, context, block_device_mapping,
1220                                 legacy_bdm=True):
1221         """If we are booting from a volume, we need to get the
1222         volume details from Cinder and make sure we pass the
1223         metadata back accordingly.
1224         """
1225         if not block_device_mapping:
1226             return {}
1227 
1228         for bdm in block_device_mapping:
1229             if (legacy_bdm and
1230                     block_device.get_device_letter(
1231                        bdm.get('device_name', '')) != 'a'):
1232                 continue
1233             elif not legacy_bdm and bdm.get('boot_index') != 0:
1234                 continue
1235 
1236             volume_id = bdm.get('volume_id')
1237             snapshot_id = bdm.get('snapshot_id')
1238             if snapshot_id:
1239                 # NOTE(alaski): A volume snapshot inherits metadata from the
1240                 # originating volume, but the API does not expose metadata
1241                 # on the snapshot itself.  So we query the volume for it below.
1242                 snapshot = self.volume_api.get_snapshot(context, snapshot_id)
1243                 volume_id = snapshot['volume_id']
1244 
1245             if bdm.get('image_id'):
1246                 try:
1247                     image_id = bdm['image_id']
1248                     image_meta = self.image_api.get(context, image_id)
1249                     return image_meta
1250                 except Exception:
1251                     raise exception.InvalidBDMImage(id=image_id)
1252             elif volume_id:
1253                 try:
1254                     volume = self.volume_api.get(context, volume_id)
1255                 except exception.CinderConnectionFailed:
1256                     raise
1257                 except Exception:
1258                     raise exception.InvalidBDMVolume(id=volume_id)
1259 
1260                 if not volume.get('bootable', True):
1261                     raise exception.InvalidBDMVolumeNotBootable(id=volume_id)
1262 
1263                 return utils.get_image_metadata_from_volume(volume)
1264         return {}
1265 
1266     @staticmethod
1267     def _get_requested_instance_group(context, filter_properties):
1268         if (not filter_properties or
1269                 not filter_properties.get('scheduler_hints')):
1270             return
1271 
1272         group_hint = filter_properties.get('scheduler_hints').get('group')
1273         if not group_hint:
1274             return
1275 
1276         return objects.InstanceGroup.get_by_uuid(context, group_hint)
1277 
1278     def _create_instance(self, context, instance_type,
1279                image_href, kernel_id, ramdisk_id,
1280                min_count, max_count,
1281                display_name, display_description,
1282                key_name, key_data, security_groups,
1283                availability_zone, user_data, metadata, injected_files,
1284                admin_password, access_ip_v4, access_ip_v6,
1285                requested_networks, config_drive,
1286                block_device_mapping, auto_disk_config, filter_properties,
1287                reservation_id=None, legacy_bdm=True, shutdown_terminate=False,
1288                check_server_group_quota=False, tags=None,
1289                supports_multiattach=False, trusted_certs=None,
1290                supports_port_resource_request=False,
1291                requested_host=None, requested_hypervisor_hostname=None):
1292         """Verify all the input parameters regardless of the provisioning
1293         strategy being performed and schedule the instance(s) for
1294         creation.
1295         """
1296 
1297         # Normalize and setup some parameters
1298         if reservation_id is None:
1299             reservation_id = utils.generate_uid('r')
1300         security_groups = security_groups or ['default']
1301         min_count = min_count or 1
1302         max_count = max_count or min_count
1303         block_device_mapping = block_device_mapping or []
1304         tags = tags or []
1305 
1306         if image_href:
1307             image_id, boot_meta = self._get_image(context, image_href)
1308         else:
1309             # This is similar to the logic in _retrieve_trusted_certs_object.
1310             if (trusted_certs or
1311                 (CONF.glance.verify_glance_signatures and
1312                  CONF.glance.enable_certificate_validation and
1313                  CONF.glance.default_trusted_certificate_ids)):
1314                 msg = _("Image certificate validation is not supported "
1315                         "when booting from volume")
1316                 raise exception.CertificateValidationFailed(message=msg)
1317             image_id = None
1318             boot_meta = self._get_bdm_image_metadata(
1319                 context, block_device_mapping, legacy_bdm)
1320 
1321         self._check_auto_disk_config(image=boot_meta,
1322                                      auto_disk_config=auto_disk_config)
1323 
1324         base_options, max_net_count, key_pair, security_groups, \
1325             network_metadata = self._validate_and_build_base_options(
1326                     context, instance_type, boot_meta, image_href, image_id,
1327                     kernel_id, ramdisk_id, display_name, display_description,
1328                     key_name, key_data, security_groups, availability_zone,
1329                     user_data, metadata, access_ip_v4, access_ip_v6,
1330                     requested_networks, config_drive, auto_disk_config,
1331                     reservation_id, max_count, supports_port_resource_request)
1332 
1333         # max_net_count is the maximum number of instances requested by the
1334         # user adjusted for any network quota constraints, including
1335         # consideration of connections to each requested network
1336         if max_net_count < min_count:
1337             raise exception.PortLimitExceeded()
1338         elif max_net_count < max_count:
1339             LOG.info("max count reduced from %(max_count)d to "
1340                      "%(max_net_count)d due to network port quota",
1341                      {'max_count': max_count,
1342                       'max_net_count': max_net_count})
1343             max_count = max_net_count
1344 
1345         block_device_mapping = self._check_and_transform_bdm(context,
1346             base_options, instance_type, boot_meta, min_count, max_count,
1347             block_device_mapping, legacy_bdm)
1348 
1349         # We can't do this check earlier because we need bdms from all sources
1350         # to have been merged in order to get the root bdm.
1351         # Set validate_numa=False since numa validation is already done by
1352         # _validate_and_build_base_options().
1353         self._checks_for_create_and_rebuild(context, image_id, boot_meta,
1354                 instance_type, metadata, injected_files,
1355                 block_device_mapping.root_bdm(), validate_numa=False)
1356 
1357         instance_group = self._get_requested_instance_group(context,
1358                                    filter_properties)
1359 
1360         tags = self._create_tag_list_obj(context, tags)
1361 
1362         instances_to_build = self._provision_instances(
1363             context, instance_type, min_count, max_count, base_options,
1364             boot_meta, security_groups, block_device_mapping,
1365             shutdown_terminate, instance_group, check_server_group_quota,
1366             filter_properties, key_pair, tags, trusted_certs,
1367             supports_multiattach, network_metadata,
1368             requested_host, requested_hypervisor_hostname)
1369 
1370         instances = []
1371         request_specs = []
1372         build_requests = []
1373         for rs, build_request, im in instances_to_build:
1374             build_requests.append(build_request)
1375             instance = build_request.get_new_instance(context)
1376             instances.append(instance)
1377             request_specs.append(rs)
1378 
1379         self.compute_task_api.schedule_and_build_instances(
1380             context,
1381             build_requests=build_requests,
1382             request_spec=request_specs,
1383             image=boot_meta,
1384             admin_password=admin_password,
1385             injected_files=injected_files,
1386             requested_networks=requested_networks,
1387             block_device_mapping=block_device_mapping,
1388             tags=tags)
1389 
1390         return instances, reservation_id
1391 
1392     @staticmethod
1393     def _cleanup_build_artifacts(instances, instances_to_build):
1394         # instances_to_build is a list of tuples:
1395         # (RequestSpec, BuildRequest, InstanceMapping)
1396 
1397         # Be paranoid about artifacts being deleted underneath us.
1398         for instance in instances or []:
1399             try:
1400                 instance.destroy()
1401             except exception.InstanceNotFound:
1402                 pass
1403         for rs, build_request, im in instances_to_build or []:
1404             try:
1405                 rs.destroy()
1406             except exception.RequestSpecNotFound:
1407                 pass
1408             try:
1409                 build_request.destroy()
1410             except exception.BuildRequestNotFound:
1411                 pass
1412             try:
1413                 im.destroy()
1414             except exception.InstanceMappingNotFound:
1415                 pass
1416 
1417     @staticmethod
1418     def _volume_size(instance_type, bdm):
1419         size = bdm.get('volume_size')
1420         # NOTE (ndipanov): inherit flavor size only for swap and ephemeral
1421         if (size is None and bdm.get('source_type') == 'blank' and
1422                 bdm.get('destination_type') == 'local'):
1423             if bdm.get('guest_format') == 'swap':
1424                 size = instance_type.get('swap', 0)
1425             else:
1426                 size = instance_type.get('ephemeral_gb', 0)
1427         return size
1428 
1429     def _prepare_image_mapping(self, instance_type, mappings):
1430         """Extract and format blank devices from image mappings."""
1431 
1432         prepared_mappings = []
1433 
1434         for bdm in block_device.mappings_prepend_dev(mappings):
1435             LOG.debug("Image bdm %s", bdm)
1436 
1437             virtual_name = bdm['virtual']
1438             if virtual_name == 'ami' or virtual_name == 'root':
1439                 continue
1440 
1441             if not block_device.is_swap_or_ephemeral(virtual_name):
1442                 continue
1443 
1444             guest_format = bdm.get('guest_format')
1445             if virtual_name == 'swap':
1446                 guest_format = 'swap'
1447             if not guest_format:
1448                 guest_format = CONF.default_ephemeral_format
1449 
1450             values = block_device.BlockDeviceDict({
1451                 'device_name': bdm['device'],
1452                 'source_type': 'blank',
1453                 'destination_type': 'local',
1454                 'device_type': 'disk',
1455                 'guest_format': guest_format,
1456                 'delete_on_termination': True,
1457                 'boot_index': -1})
1458 
1459             values['volume_size'] = self._volume_size(
1460                 instance_type, values)
1461             if values['volume_size'] == 0:
1462                 continue
1463 
1464             prepared_mappings.append(values)
1465 
1466         return prepared_mappings
1467 
1468     def _bdm_validate_set_size_and_instance(self, context, instance,
1469                                             instance_type,
1470                                             block_device_mapping,
1471                                             supports_multiattach=False):
1472         """Ensure the bdms are valid, then set size and associate with instance
1473 
1474         Because this method can be called multiple times when more than one
1475         instance is booted in a single request it makes a copy of the bdm list.
1476         """
1477         LOG.debug("block_device_mapping %s", list(block_device_mapping),
1478                   instance_uuid=instance.uuid)
1479         self._validate_bdm(
1480             context, instance, instance_type, block_device_mapping,
1481             supports_multiattach)
1482         instance_block_device_mapping = block_device_mapping.obj_clone()
1483         for bdm in instance_block_device_mapping:
1484             bdm.volume_size = self._volume_size(instance_type, bdm)
1485             bdm.instance_uuid = instance.uuid
1486         return instance_block_device_mapping
1487 
1488     @staticmethod
1489     def _check_requested_volume_type(bdm, volume_type_id_or_name,
1490                                      volume_types):
1491         """If we are specifying a volume type, we need to get the
1492         volume type details from Cinder and make sure the ``volume_type``
1493         is available.
1494         """
1495 
1496         # NOTE(brinzhang): Verify that the specified volume type exists.
1497         # And save the volume type name internally for consistency in the
1498         # BlockDeviceMapping object.
1499         for vol_type in volume_types:
1500             if (volume_type_id_or_name == vol_type['id'] or
1501                         volume_type_id_or_name == vol_type['name']):
1502                 bdm.volume_type = vol_type['name']
1503                 break
1504         else:
1505             raise exception.VolumeTypeNotFound(
1506                 id_or_name=volume_type_id_or_name)
1507 
1508     @staticmethod
1509     def _check_compute_supports_volume_type(context):
1510         # NOTE(brinzhang): Checking the minimum nova-compute service
1511         # version across the deployment. Just make sure the volume
1512         # type can be supported when the bdm.volume_type is requested.
1513         min_compute_version = objects.service.get_minimum_version_all_cells(
1514             context, ['nova-compute'])
1515         if min_compute_version < MIN_COMPUTE_VOLUME_TYPE:
1516             raise exception.VolumeTypeSupportNotYetAvailable()
1517 
1518     def _validate_bdm(self, context, instance, instance_type,
1519                       block_device_mappings, supports_multiattach=False):
1520         # Make sure that the boot indexes make sense.
1521         # Setting a negative value or None indicates that the device should not
1522         # be used for booting.
1523         boot_indexes = sorted([bdm.boot_index
1524                                for bdm in block_device_mappings
1525                                if bdm.boot_index is not None and
1526                                bdm.boot_index >= 0])
1527 
1528         # Each device which is capable of being used as boot device should
1529         # be given a unique boot index, starting from 0 in ascending order, and
1530         # there needs to be at least one boot device.
1531         if not boot_indexes or any(i != v for i, v in enumerate(boot_indexes)):
1532             # Convert the BlockDeviceMappingList to a list for repr details.
1533             LOG.debug('Invalid block device mapping boot sequence for '
1534                       'instance: %s', list(block_device_mappings),
1535                       instance=instance)
1536             raise exception.InvalidBDMBootSequence()
1537 
1538         volume_types = None
1539         volume_type_is_supported = False
1540         for bdm in block_device_mappings:
1541             volume_type = bdm.volume_type
1542             if volume_type:
1543                 if not volume_type_is_supported:
1544                     # The following method raises
1545                     # VolumeTypeSupportNotYetAvailable if the minimum
1546                     # nova-compute service version across the deployment is
1547                     # not new enough to support creating volumes with a
1548                     # specific type.
1549                     self._check_compute_supports_volume_type(context)
1550                     # Set the flag to avoid calling
1551                     # _check_compute_supports_volume_type more than once in
1552                     # this for loop.
1553                     volume_type_is_supported = True
1554 
1555                 if not volume_types:
1556                     # In order to reduce the number of hit cinder APIs,
1557                     # initialize our cache of volume types.
1558                     volume_types = self.volume_api.get_all_volume_types(
1559                         context)
1560                 # NOTE(brinzhang): Ensure the validity of volume_type.
1561                 self._check_requested_volume_type(bdm, volume_type,
1562                                                   volume_types)
1563 
1564             # NOTE(vish): For now, just make sure the volumes are accessible.
1565             # Additionally, check that the volume can be attached to this
1566             # instance.
1567             snapshot_id = bdm.snapshot_id
1568             volume_id = bdm.volume_id
1569             image_id = bdm.image_id
1570             if image_id is not None:
1571                 if image_id != instance.get('image_ref'):
1572                     try:
1573                         self._get_image(context, image_id)
1574                     except Exception:
1575                         raise exception.InvalidBDMImage(id=image_id)
1576                 if (bdm.source_type == 'image' and
1577                         bdm.destination_type == 'volume' and
1578                         not bdm.volume_size):
1579                     raise exception.InvalidBDM(message=_("Images with "
1580                         "destination_type 'volume' need to have a non-zero "
1581                         "size specified"))
1582             elif volume_id is not None:
1583                 try:
1584                     volume = self.volume_api.get(context, volume_id)
1585                     self._check_attach_and_reserve_volume(
1586                         context, volume, instance, bdm, supports_multiattach)
1587                     bdm.volume_size = volume.get('size')
1588 
1589                     # NOTE(mnaser): If we end up reserving the volume, it will
1590                     #               not have an attachment_id which is needed
1591                     #               for cleanups.  This can be removed once
1592                     #               all calls to reserve_volume are gone.
1593                     if 'attachment_id' not in bdm:
1594                         bdm.attachment_id = None
1595                 except (exception.CinderConnectionFailed,
1596                         exception.InvalidVolume,
1597                         exception.MultiattachNotSupportedOldMicroversion):
1598                     raise
1599                 except exception.InvalidInput as exc:
1600                     raise exception.InvalidVolume(reason=exc.format_message())
1601                 except Exception as e:
1602                     LOG.info('Failed validating volume %s. Error: %s',
1603                              volume_id, e)
1604                     raise exception.InvalidBDMVolume(id=volume_id)
1605             elif snapshot_id is not None:
1606                 try:
1607                     snap = self.volume_api.get_snapshot(context, snapshot_id)
1608                     bdm.volume_size = bdm.volume_size or snap.get('size')
1609                 except exception.CinderConnectionFailed:
1610                     raise
1611                 except Exception:
1612                     raise exception.InvalidBDMSnapshot(id=snapshot_id)
1613             elif (bdm.source_type == 'blank' and
1614                     bdm.destination_type == 'volume' and
1615                     not bdm.volume_size):
1616                 raise exception.InvalidBDM(message=_("Blank volumes "
1617                     "(source: 'blank', dest: 'volume') need to have non-zero "
1618                     "size"))
1619 
1620         ephemeral_size = sum(bdm.volume_size or instance_type['ephemeral_gb']
1621                 for bdm in block_device_mappings
1622                 if block_device.new_format_is_ephemeral(bdm))
1623         if ephemeral_size > instance_type['ephemeral_gb']:
1624             raise exception.InvalidBDMEphemeralSize()
1625 
1626         # There should be only one swap
1627         swap_list = block_device.get_bdm_swap_list(block_device_mappings)
1628         if len(swap_list) > 1:
1629             msg = _("More than one swap drive requested.")
1630             raise exception.InvalidBDMFormat(details=msg)
1631 
1632         if swap_list:
1633             swap_size = swap_list[0].volume_size or 0
1634             if swap_size > instance_type['swap']:
1635                 raise exception.InvalidBDMSwapSize()
1636 
1637         max_local = CONF.max_local_block_devices
1638         if max_local >= 0:
1639             num_local = len([bdm for bdm in block_device_mappings
1640                              if bdm.destination_type == 'local'])
1641             if num_local > max_local:
1642                 raise exception.InvalidBDMLocalsLimit()
1643 
1644     def _populate_instance_names(self, instance, num_instances, index):
1645         """Populate instance display_name and hostname.
1646 
1647         :param instance: The instance to set the display_name, hostname for
1648         :type instance: nova.objects.Instance
1649         :param num_instances: Total number of instances being created in this
1650             request
1651         :param index: The 0-based index of this particular instance
1652         """
1653         # NOTE(mriedem): This is only here for test simplicity since a server
1654         # name is required in the REST API.
1655         if 'display_name' not in instance or instance.display_name is None:
1656             instance.display_name = 'Server %s' % instance.uuid
1657 
1658         # if we're booting multiple instances, we need to add an indexing
1659         # suffix to both instance.hostname and instance.display_name. This is
1660         # not necessary for a single instance.
1661         if num_instances == 1:
1662             default_hostname = 'Server-%s' % instance.uuid
1663             instance.hostname = utils.sanitize_hostname(
1664                 instance.display_name, default_hostname)
1665         elif num_instances > 1:
1666             old_display_name = instance.display_name
1667             new_display_name = '%s-%d' % (old_display_name, index + 1)
1668 
1669             if utils.sanitize_hostname(old_display_name) == "":
1670                 instance.hostname = 'Server-%s' % instance.uuid
1671             else:
1672                 instance.hostname = utils.sanitize_hostname(
1673                     new_display_name)
1674 
1675             instance.display_name = new_display_name
1676 
1677     def _populate_instance_for_create(self, context, instance, image,
1678                                       index, security_groups, instance_type,
1679                                       num_instances, shutdown_terminate):
1680         """Build the beginning of a new instance."""
1681 
1682         instance.launch_index = index
1683         instance.vm_state = vm_states.BUILDING
1684         instance.task_state = task_states.SCHEDULING
1685         info_cache = objects.InstanceInfoCache()
1686         info_cache.instance_uuid = instance.uuid
1687         info_cache.network_info = network_model.NetworkInfo()
1688         instance.info_cache = info_cache
1689         instance.flavor = instance_type
1690         instance.old_flavor = None
1691         instance.new_flavor = None
1692         if CONF.ephemeral_storage_encryption.enabled:
1693             # NOTE(kfarr): dm-crypt expects the cipher in a
1694             # hyphenated format: cipher-chainmode-ivmode
1695             # (ex: aes-xts-plain64). The algorithm needs
1696             # to be parsed out to pass to the key manager (ex: aes).
1697             cipher = CONF.ephemeral_storage_encryption.cipher
1698             algorithm = cipher.split('-')[0] if cipher else None
1699             instance.ephemeral_key_uuid = self.key_manager.create_key(
1700                 context,
1701                 algorithm=algorithm,
1702                 length=CONF.ephemeral_storage_encryption.key_size)
1703         else:
1704             instance.ephemeral_key_uuid = None
1705 
1706         # Store image properties so we can use them later
1707         # (for notifications, etc).  Only store what we can.
1708         if not instance.obj_attr_is_set('system_metadata'):
1709             instance.system_metadata = {}
1710         # Make sure we have the dict form that we need for instance_update.
1711         instance.system_metadata = utils.instance_sys_meta(instance)
1712 
1713         system_meta = utils.get_system_metadata_from_image(
1714             image, instance_type)
1715 
1716         # In case we couldn't find any suitable base_image
1717         system_meta.setdefault('image_base_image_ref', instance.image_ref)
1718 
1719         system_meta['owner_user_name'] = context.user_name
1720         system_meta['owner_project_name'] = context.project_name
1721 
1722         instance.system_metadata.update(system_meta)
1723 
1724         if CONF.use_neutron:
1725             # For Neutron we don't actually store anything in the database, we
1726             # proxy the security groups on the instance from the ports
1727             # attached to the instance.
1728             instance.security_groups = objects.SecurityGroupList()
1729         else:
1730             instance.security_groups = security_groups
1731 
1732         self._populate_instance_names(instance, num_instances, index)
1733         instance.shutdown_terminate = shutdown_terminate
1734 
1735         return instance
1736 
1737     def _create_tag_list_obj(self, context, tags):
1738         """Create TagList objects from simple string tags.
1739 
1740         :param context: security context.
1741         :param tags: simple string tags from API request.
1742         :returns: TagList object.
1743         """
1744         tag_list = [objects.Tag(context=context, tag=t) for t in tags]
1745         tag_list_obj = objects.TagList(objects=tag_list)
1746         return tag_list_obj
1747 
1748     def _transform_tags(self, tags, resource_id):
1749         """Change the resource_id of the tags according to the input param.
1750 
1751         Because this method can be called multiple times when more than one
1752         instance is booted in a single request it makes a copy of the tags
1753         list.
1754 
1755         :param tags: TagList object.
1756         :param resource_id: string.
1757         :returns: TagList object.
1758         """
1759         instance_tags = tags.obj_clone()
1760         for tag in instance_tags:
1761             tag.resource_id = resource_id
1762         return instance_tags
1763 
1764     # This method remains because cellsv1 uses it in the scheduler
1765     def create_db_entry_for_new_instance(self, context, instance_type, image,
1766             instance, security_group, block_device_mapping, num_instances,
1767             index, shutdown_terminate=False, create_instance=True):
1768         """Create an entry in the DB for this new instance,
1769         including any related table updates (such as security group,
1770         etc).
1771 
1772         This is called by the scheduler after a location for the
1773         instance has been determined.
1774 
1775         :param create_instance: Determines if the instance is created here or
1776             just populated for later creation. This is done so that this code
1777             can be shared with cellsv1 which needs the instance creation to
1778             happen here. It should be removed and this method cleaned up when
1779             cellsv1 is a distant memory.
1780         """
1781         self._populate_instance_for_create(context, instance, image, index,
1782                                            security_group, instance_type,
1783                                            num_instances, shutdown_terminate)
1784 
1785         if create_instance:
1786             instance.create()
1787 
1788         return instance
1789 
1790     def _check_multiple_instances_with_neutron_ports(self,
1791                                                      requested_networks):
1792         """Check whether multiple instances are created from port id(s)."""
1793         for requested_net in requested_networks:
1794             if requested_net.port_id:
1795                 msg = _("Unable to launch multiple instances with"
1796                         " a single configured port ID. Please launch your"
1797                         " instance one by one with different ports.")
1798                 raise exception.MultiplePortsNotApplicable(reason=msg)
1799 
1800     def _check_multiple_instances_with_specified_ip(self, requested_networks):
1801         """Check whether multiple instances are created with specified ip."""
1802 
1803         for requested_net in requested_networks:
1804             if requested_net.network_id and requested_net.address:
1805                 msg = _("max_count cannot be greater than 1 if an fixed_ip "
1806                         "is specified.")
1807                 raise exception.InvalidFixedIpAndMaxCountRequest(reason=msg)
1808 
1809     @hooks.add_hook("create_instance")
1810     def create(self, context, instance_type,
1811                image_href, kernel_id=None, ramdisk_id=None,
1812                min_count=None, max_count=None,
1813                display_name=None, display_description=None,
1814                key_name=None, key_data=None, security_groups=None,
1815                availability_zone=None, forced_host=None, forced_node=None,
1816                user_data=None, metadata=None, injected_files=None,
1817                admin_password=None, block_device_mapping=None,
1818                access_ip_v4=None, access_ip_v6=None, requested_networks=None,
1819                config_drive=None, auto_disk_config=None, scheduler_hints=None,
1820                legacy_bdm=True, shutdown_terminate=False,
1821                check_server_group_quota=False, tags=None,
1822                supports_multiattach=False, trusted_certs=None,
1823                supports_port_resource_request=False,
1824                requested_host=None, requested_hypervisor_hostname=None):
1825         """Provision instances, sending instance information to the
1826         scheduler.  The scheduler will determine where the instance(s)
1827         go and will handle creating the DB entries.
1828 
1829         Returns a tuple of (instances, reservation_id)
1830         """
1831         if requested_networks and max_count is not None and max_count > 1:
1832             self._check_multiple_instances_with_specified_ip(
1833                 requested_networks)
1834             if utils.is_neutron():
1835                 self._check_multiple_instances_with_neutron_ports(
1836                     requested_networks)
1837 
1838         if availability_zone:
1839             available_zones = availability_zones.\
1840                 get_availability_zones(context.elevated(), self.host_api,
1841                                        get_only_available=True)
1842             if forced_host is None and availability_zone not in \
1843                     available_zones:
1844                 msg = _('The requested availability zone is not available')
1845                 raise exception.InvalidRequest(msg)
1846 
1847         filter_properties = scheduler_utils.build_filter_properties(
1848                 scheduler_hints, forced_host, forced_node, instance_type)
1849 
1850         return self._create_instance(
1851             context, instance_type,
1852             image_href, kernel_id, ramdisk_id,
1853             min_count, max_count,
1854             display_name, display_description,
1855             key_name, key_data, security_groups,
1856             availability_zone, user_data, metadata,
1857             injected_files, admin_password,
1858             access_ip_v4, access_ip_v6,
1859             requested_networks, config_drive,
1860             block_device_mapping, auto_disk_config,
1861             filter_properties=filter_properties,
1862             legacy_bdm=legacy_bdm,
1863             shutdown_terminate=shutdown_terminate,
1864             check_server_group_quota=check_server_group_quota,
1865             tags=tags, supports_multiattach=supports_multiattach,
1866             trusted_certs=trusted_certs,
1867             supports_port_resource_request=supports_port_resource_request,
1868             requested_host=requested_host,
1869             requested_hypervisor_hostname=requested_hypervisor_hostname)
1870 
1871     def _check_auto_disk_config(self, instance=None, image=None,
1872                                 **extra_instance_updates):
1873         auto_disk_config = extra_instance_updates.get("auto_disk_config")
1874         if auto_disk_config is None:
1875             return
1876         if not image and not instance:
1877             return
1878 
1879         if image:
1880             image_props = image.get("properties", {})
1881             auto_disk_config_img = \
1882                 utils.get_auto_disk_config_from_image_props(image_props)
1883             image_ref = image.get("id")
1884         else:
1885             sys_meta = utils.instance_sys_meta(instance)
1886             image_ref = sys_meta.get('image_base_image_ref')
1887             auto_disk_config_img = \
1888                 utils.get_auto_disk_config_from_instance(sys_meta=sys_meta)
1889 
1890         self._ensure_auto_disk_config_is_valid(auto_disk_config_img,
1891                                                auto_disk_config,
1892                                                image_ref)
1893 
1894     def _lookup_instance(self, context, uuid):
1895         '''Helper method for pulling an instance object from a database.
1896 
1897         During the transition to cellsv2 there is some complexity around
1898         retrieving an instance from the database which this method hides. If
1899         there is an instance mapping then query the cell for the instance, if
1900         no mapping exists then query the configured nova database.
1901 
1902         Once we are past the point that all deployments can be assumed to be
1903         migrated to cellsv2 this method can go away.
1904         '''
1905         inst_map = None
1906         try:
1907             inst_map = objects.InstanceMapping.get_by_instance_uuid(
1908                 context, uuid)
1909         except exception.InstanceMappingNotFound:
1910             # TODO(alaski): This exception block can be removed once we're
1911             # guaranteed everyone is using cellsv2.
1912             pass
1913 
1914         if inst_map is None or inst_map.cell_mapping is None:
1915             # If inst_map is None then the deployment has not migrated to
1916             # cellsv2 yet.
1917             # If inst_map.cell_mapping is None then the instance is not in a
1918             # cell yet. Until instance creation moves to the conductor the
1919             # instance can be found in the configured database, so attempt
1920             # to look it up.
1921             cell = None
1922             try:
1923                 instance = objects.Instance.get_by_uuid(context, uuid)
1924             except exception.InstanceNotFound:
1925                 # If we get here then the conductor is in charge of writing the
1926                 # instance to the database and hasn't done that yet. It's up to
1927                 # the caller of this method to determine what to do with that
1928                 # information.
1929                 return None, None
1930         else:
1931             cell = inst_map.cell_mapping
1932             with nova_context.target_cell(context, cell) as cctxt:
1933                 try:
1934                     instance = objects.Instance.get_by_uuid(cctxt, uuid)
1935                 except exception.InstanceNotFound:
1936                     # Since the cell_mapping exists we know the instance is in
1937                     # the cell, however InstanceNotFound means it's already
1938                     # deleted.
1939                     return None, None
1940         return cell, instance
1941 
1942     def _delete_while_booting(self, context, instance):
1943         """Handle deletion if the instance has not reached a cell yet
1944 
1945         Deletion before an instance reaches a cell needs to be handled
1946         differently. What we're attempting to do is delete the BuildRequest
1947         before the api level conductor does.  If we succeed here then the boot
1948         request stops before reaching a cell.  If not then the instance will
1949         need to be looked up in a cell db and the normal delete path taken.
1950         """
1951         deleted = self._attempt_delete_of_buildrequest(context, instance)
1952         if deleted:
1953             # If we've reached this block the successful deletion of the
1954             # buildrequest indicates that the build process should be halted by
1955             # the conductor.
1956 
1957             # NOTE(alaski): Though the conductor halts the build process it
1958             # does not currently delete the instance record. This is
1959             # because in the near future the instance record will not be
1960             # created if the buildrequest has been deleted here. For now we
1961             # ensure the instance has been set to deleted at this point.
1962             # Yes this directly contradicts the comment earlier in this
1963             # method, but this is a temporary measure.
1964             # Look up the instance because the current instance object was
1965             # stashed on the buildrequest and therefore not complete enough
1966             # to run .destroy().
1967             try:
1968                 instance_uuid = instance.uuid
1969                 cell, instance = self._lookup_instance(context, instance_uuid)
1970                 if instance is not None:
1971                     # If instance is None it has already been deleted.
1972                     if cell:
1973                         with nova_context.target_cell(context, cell) as cctxt:
1974                             # FIXME: When the instance context is targeted,
1975                             # we can remove this
1976                             with compute_utils.notify_about_instance_delete(
1977                                     self.notifier, cctxt, instance):
1978                                 instance.destroy()
1979                     else:
1980                         instance.destroy()
1981             except exception.InstanceNotFound:
1982                 pass
1983 
1984             return True
1985         return False
1986 
1987     def _attempt_delete_of_buildrequest(self, context, instance):
1988         # If there is a BuildRequest then the instance may not have been
1989         # written to a cell db yet. Delete the BuildRequest here, which
1990         # will indicate that the Instance build should not proceed.
1991         try:
1992             build_req = objects.BuildRequest.get_by_instance_uuid(
1993                 context, instance.uuid)
1994             build_req.destroy()
1995         except exception.BuildRequestNotFound:
1996             # This means that conductor has deleted the BuildRequest so the
1997             # instance is now in a cell and the delete needs to proceed
1998             # normally.
1999             return False
2000 
2001         # We need to detach from any volumes so they aren't orphaned.
2002         self._local_cleanup_bdm_volumes(
2003             build_req.block_device_mappings, instance, context)
2004 
2005         return True
2006 
2007     def _delete(self, context, instance, delete_type, cb, **instance_attrs):
2008         if instance.disable_terminate:
2009             LOG.info('instance termination disabled', instance=instance)
2010             return
2011 
2012         cell = None
2013         # If there is an instance.host (or the instance is shelved-offloaded or
2014         # in error state), the instance has been scheduled and sent to a
2015         # cell/compute which means it was pulled from the cell db.
2016         # Normal delete should be attempted.
2017         may_have_ports_or_volumes = compute_utils.may_have_ports_or_volumes(
2018             instance)
2019         if not instance.host and not may_have_ports_or_volumes:
2020             try:
2021                 if self._delete_while_booting(context, instance):
2022                     return
2023                 # If instance.host was not set it's possible that the Instance
2024                 # object here was pulled from a BuildRequest object and is not
2025                 # fully populated. Notably it will be missing an 'id' field
2026                 # which will prevent instance.destroy from functioning
2027                 # properly. A lookup is attempted which will either return a
2028                 # full Instance or None if not found. If not found then it's
2029                 # acceptable to skip the rest of the delete processing.
2030                 cell, instance = self._lookup_instance(context, instance.uuid)
2031                 if cell and instance:
2032                     try:
2033                         # Now destroy the instance from the cell it lives in.
2034                         with compute_utils.notify_about_instance_delete(
2035                                 self.notifier, context, instance):
2036                             instance.destroy()
2037                     except exception.InstanceNotFound:
2038                         pass
2039                     # The instance was deleted or is already gone.
2040                     return
2041                 if not instance:
2042                     # Instance is already deleted.
2043                     return
2044             except exception.ObjectActionError:
2045                 # NOTE(melwitt): This means the instance.host changed
2046                 # under us indicating the instance became scheduled
2047                 # during the destroy(). Refresh the instance from the DB and
2048                 # continue on with the delete logic for a scheduled instance.
2049                 # NOTE(danms): If instance.host is set, we should be able to
2050                 # do the following lookup. If not, there's not much we can
2051                 # do to recover.
2052                 cell, instance = self._lookup_instance(context, instance.uuid)
2053                 if not instance:
2054                     # Instance is already deleted
2055                     return
2056 
2057         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
2058                 context, instance.uuid)
2059 
2060         # At these states an instance has a snapshot associate.
2061         if instance.vm_state in (vm_states.SHELVED,
2062                                  vm_states.SHELVED_OFFLOADED):
2063             snapshot_id = instance.system_metadata.get('shelved_image_id')
2064             LOG.info("Working on deleting snapshot %s "
2065                      "from shelved instance...",
2066                      snapshot_id, instance=instance)
2067             try:
2068                 self.image_api.delete(context, snapshot_id)
2069             except (exception.ImageNotFound,
2070                     exception.ImageNotAuthorized) as exc:
2071                 LOG.warning("Failed to delete snapshot "
2072                             "from shelved instance (%s).",
2073                             exc.format_message(), instance=instance)
2074             except Exception:
2075                 LOG.exception("Something wrong happened when trying to "
2076                               "delete snapshot from shelved instance.",
2077                               instance=instance)
2078 
2079         original_task_state = instance.task_state
2080         try:
2081             # NOTE(maoy): no expected_task_state needs to be set
2082             instance.update(instance_attrs)
2083             instance.progress = 0
2084             instance.save()
2085 
2086             if not instance.host and not may_have_ports_or_volumes:
2087                 try:
2088                     with compute_utils.notify_about_instance_delete(
2089                             self.notifier, context, instance,
2090                             delete_type
2091                             if delete_type != 'soft_delete'
2092                             else 'delete'):
2093                         instance.destroy()
2094                     LOG.info('Instance deleted and does not have host '
2095                              'field, its vm_state is %(state)s.',
2096                              {'state': instance.vm_state},
2097                               instance=instance)
2098                     return
2099                 except exception.ObjectActionError as ex:
2100                     # The instance's host likely changed under us as
2101                     # this instance could be building and has since been
2102                     # scheduled. Continue with attempts to delete it.
2103                     LOG.debug('Refreshing instance because: %s', ex,
2104                               instance=instance)
2105                     instance.refresh()
2106 
2107             if instance.vm_state == vm_states.RESIZED:
2108                 self._confirm_resize_on_deleting(context, instance)
2109                 # NOTE(neha_alhat): After confirm resize vm_state will become
2110                 # 'active' and task_state will be set to 'None'. But for soft
2111                 # deleting a vm, the _do_soft_delete callback requires
2112                 # task_state in 'SOFT_DELETING' status. So, we need to set
2113                 # task_state as 'SOFT_DELETING' again for soft_delete case.
2114                 # After confirm resize and before saving the task_state to
2115                 # "SOFT_DELETING", during the short window, user can submit
2116                 # soft delete vm request again and system will accept and
2117                 # process it without any errors.
2118                 if delete_type == 'soft_delete':
2119                     instance.task_state = instance_attrs['task_state']
2120                     instance.save()
2121 
2122             is_local_delete = True
2123             try:
2124                 # instance.host must be set in order to look up the service.
2125                 if instance.host is not None:
2126                     service = objects.Service.get_by_compute_host(
2127                         context.elevated(), instance.host)
2128                     is_local_delete = not self.servicegroup_api.service_is_up(
2129                         service)
2130                 if not is_local_delete:
2131                     if original_task_state in (task_states.DELETING,
2132                                                   task_states.SOFT_DELETING):
2133                         LOG.info('Instance is already in deleting state, '
2134                                  'ignoring this request',
2135                                  instance=instance)
2136                         return
2137                     self._record_action_start(context, instance,
2138                                               instance_actions.DELETE)
2139 
2140                     cb(context, instance, bdms)
2141             except exception.ComputeHostNotFound:
2142                 LOG.debug('Compute host %s not found during service up check, '
2143                           'going to local delete instance', instance.host,
2144                           instance=instance)
2145 
2146             if is_local_delete:
2147                 # If instance is in shelved_offloaded state or compute node
2148                 # isn't up, delete instance from db and clean bdms info and
2149                 # network info
2150                 if cell is None:
2151                     # NOTE(danms): If we didn't get our cell from one of the
2152                     # paths above, look it up now.
2153                     try:
2154                         im = objects.InstanceMapping.get_by_instance_uuid(
2155                             context, instance.uuid)
2156                         cell = im.cell_mapping
2157                     except exception.InstanceMappingNotFound:
2158                         LOG.warning('During local delete, failed to find '
2159                                     'instance mapping', instance=instance)
2160                         return
2161 
2162                 LOG.debug('Doing local delete in cell %s', cell.identity,
2163                           instance=instance)
2164                 with nova_context.target_cell(context, cell) as cctxt:
2165                     self._local_delete(cctxt, instance, bdms, delete_type, cb)
2166 
2167         except exception.InstanceNotFound:
2168             # NOTE(comstud): Race condition. Instance already gone.
2169             pass
2170 
2171     def _confirm_resize_on_deleting(self, context, instance):
2172         # If in the middle of a resize, use confirm_resize to
2173         # ensure the original instance is cleaned up too along
2174         # with its allocations (and migration-based allocations)
2175         # in placement.
2176         migration = None
2177         for status in ('finished', 'confirming'):
2178             try:
2179                 migration = objects.Migration.get_by_instance_and_status(
2180                         context.elevated(), instance.uuid, status)
2181                 LOG.info('Found an unconfirmed migration during delete, '
2182                          'id: %(id)s, status: %(status)s',
2183                          {'id': migration.id,
2184                           'status': migration.status},
2185                          instance=instance)
2186                 break
2187             except exception.MigrationNotFoundByStatus:
2188                 pass
2189 
2190         if not migration:
2191             LOG.info('Instance may have been confirmed during delete',
2192                      instance=instance)
2193             return
2194 
2195         src_host = migration.source_compute
2196 
2197         self._record_action_start(context, instance,
2198                                   instance_actions.CONFIRM_RESIZE)
2199 
2200         self.compute_rpcapi.confirm_resize(context,
2201                 instance, migration, src_host, cast=False)
2202 
2203     def _local_cleanup_bdm_volumes(self, bdms, instance, context):
2204         """The method deletes the bdm records and, if a bdm is a volume, call
2205         the terminate connection and the detach volume via the Volume API.
2206         """
2207         elevated = context.elevated()
2208         for bdm in bdms:
2209             if bdm.is_volume:
2210                 try:
2211                     if bdm.attachment_id:
2212                         self.volume_api.attachment_delete(context,
2213                                                           bdm.attachment_id)
2214                     else:
2215                         connector = compute_utils.get_stashed_volume_connector(
2216                             bdm, instance)
2217                         if connector:
2218                             self.volume_api.terminate_connection(context,
2219                                                                  bdm.volume_id,
2220                                                                  connector)
2221                         else:
2222                             LOG.debug('Unable to find connector for volume %s,'
2223                                       ' not attempting terminate_connection.',
2224                                       bdm.volume_id, instance=instance)
2225                         # Attempt to detach the volume. If there was no
2226                         # connection made in the first place this is just
2227                         # cleaning up the volume state in the Cinder DB.
2228                         self.volume_api.detach(elevated, bdm.volume_id,
2229                                                instance.uuid)
2230 
2231                     if bdm.delete_on_termination:
2232                         self.volume_api.delete(context, bdm.volume_id)
2233                 except Exception as exc:
2234                     LOG.warning("Ignoring volume cleanup failure due to %s",
2235                                 exc, instance=instance)
2236             # If we're cleaning up volumes from an instance that wasn't yet
2237             # created in a cell, i.e. the user deleted the server while
2238             # the BuildRequest still existed, then the BDM doesn't actually
2239             # exist in the DB to destroy it.
2240             if 'id' in bdm:
2241                 bdm.destroy()
2242 
2243     @property
2244     def placementclient(self):
2245         if self._placementclient is None:
2246             self._placementclient = report.SchedulerReportClient()
2247         return self._placementclient
2248 
2249     def _local_delete(self, context, instance, bdms, delete_type, cb):
2250         if instance.vm_state == vm_states.SHELVED_OFFLOADED:
2251             LOG.info("instance is in SHELVED_OFFLOADED state, cleanup"
2252                      " the instance's info from database.",
2253                      instance=instance)
2254         else:
2255             LOG.warning("instance's host %s is down, deleting from "
2256                         "database", instance.host, instance=instance)
2257         with compute_utils.notify_about_instance_delete(
2258                 self.notifier, context, instance,
2259                 delete_type if delete_type != 'soft_delete' else 'delete'):
2260 
2261             elevated = context.elevated()
2262             # NOTE(liusheng): In nova-network multi_host scenario,deleting
2263             # network info of the instance may need instance['host'] as
2264             # destination host of RPC call. If instance in
2265             # SHELVED_OFFLOADED state, instance['host'] is None, here, use
2266             # shelved_host as host to deallocate network info and reset
2267             # instance['host'] after that. Here we shouldn't use
2268             # instance.save(), because this will mislead user who may think
2269             # the instance's host has been changed, and actually, the
2270             # instance.host is always None.
2271             orig_host = instance.host
2272             try:
2273                 if instance.vm_state == vm_states.SHELVED_OFFLOADED:
2274                     sysmeta = getattr(instance,
2275                                       obj_base.get_attrname(
2276                                           'system_metadata'))
2277                     instance.host = sysmeta.get('shelved_host')
2278                 self.network_api.deallocate_for_instance(elevated,
2279                                                          instance)
2280             finally:
2281                 instance.host = orig_host
2282 
2283             # cleanup volumes
2284             self._local_cleanup_bdm_volumes(bdms, instance, context)
2285             # Cleanup allocations in Placement since we can't do it from the
2286             # compute service.
2287             self.placementclient.delete_allocation_for_instance(
2288                 context, instance.uuid)
2289             cb(context, instance, bdms, local=True)
2290             instance.destroy()
2291 
2292     @staticmethod
2293     def _update_queued_for_deletion(context, instance, qfd):
2294         # NOTE(tssurya): We query the instance_mapping record of this instance
2295         # and update the queued_for_delete flag to True (or False according to
2296         # the state of the instance). This just means that the instance is
2297         # queued for deletion (or is no longer queued for deletion). It does
2298         # not guarantee its successful deletion (or restoration). Hence the
2299         # value could be stale which is fine, considering its use is only
2300         # during down cell (desperate) situation.
2301         im = objects.InstanceMapping.get_by_instance_uuid(context,
2302                                                           instance.uuid)
2303         im.queued_for_delete = qfd
2304         im.save()
2305 
2306     def _do_delete(self, context, instance, bdms, local=False):
2307         if local:
2308             instance.vm_state = vm_states.DELETED
2309             instance.task_state = None
2310             instance.terminated_at = timeutils.utcnow()
2311             instance.save()
2312         else:
2313             self.compute_rpcapi.terminate_instance(context, instance, bdms)
2314         self._update_queued_for_deletion(context, instance, True)
2315 
2316     def _do_soft_delete(self, context, instance, bdms, local=False):
2317         if local:
2318             instance.vm_state = vm_states.SOFT_DELETED
2319             instance.task_state = None
2320             instance.terminated_at = timeutils.utcnow()
2321             instance.save()
2322         else:
2323             self.compute_rpcapi.soft_delete_instance(context, instance)
2324         self._update_queued_for_deletion(context, instance, True)
2325 
2326     # NOTE(maoy): we allow delete to be called no matter what vm_state says.
2327     @check_instance_lock
2328     @check_instance_state(vm_state=None, task_state=None,
2329                           must_have_launched=True)
2330     def soft_delete(self, context, instance):
2331         """Terminate an instance."""
2332         LOG.debug('Going to try to soft delete instance',
2333                   instance=instance)
2334 
2335         self._delete(context, instance, 'soft_delete', self._do_soft_delete,
2336                      task_state=task_states.SOFT_DELETING,
2337                      deleted_at=timeutils.utcnow())
2338 
2339     def _delete_instance(self, context, instance):
2340         self._delete(context, instance, 'delete', self._do_delete,
2341                      task_state=task_states.DELETING)
2342 
2343     @check_instance_lock
2344     @check_instance_state(vm_state=None, task_state=None,
2345                           must_have_launched=False)
2346     def delete(self, context, instance):
2347         """Terminate an instance."""
2348         LOG.debug("Going to try to terminate instance", instance=instance)
2349         self._delete_instance(context, instance)
2350 
2351     @check_instance_lock
2352     @check_instance_state(vm_state=[vm_states.SOFT_DELETED])
2353     def restore(self, context, instance):
2354         """Restore a previously deleted (but not reclaimed) instance."""
2355         # Check quotas
2356         flavor = instance.get_flavor()
2357         project_id, user_id = quotas_obj.ids_from_instance(context, instance)
2358         compute_utils.check_num_instances_quota(context, flavor, 1, 1,
2359                 project_id=project_id, user_id=user_id)
2360 
2361         self._record_action_start(context, instance, instance_actions.RESTORE)
2362 
2363         if instance.host:
2364             instance.task_state = task_states.RESTORING
2365             instance.deleted_at = None
2366             instance.save(expected_task_state=[None])
2367             # TODO(melwitt): We're not rechecking for strict quota here to
2368             # guard against going over quota during a race at this time because
2369             # the resource consumption for this operation is written to the
2370             # database by compute.
2371             self.compute_rpcapi.restore_instance(context, instance)
2372         else:
2373             instance.vm_state = vm_states.ACTIVE
2374             instance.task_state = None
2375             instance.deleted_at = None
2376             instance.save(expected_task_state=[None])
2377         self._update_queued_for_deletion(context, instance, False)
2378 
2379     @check_instance_lock
2380     @check_instance_state(task_state=None,
2381                           must_have_launched=False)
2382     def force_delete(self, context, instance):
2383         """Force delete an instance in any vm_state/task_state."""
2384         self._delete(context, instance, 'force_delete', self._do_delete,
2385                      task_state=task_states.DELETING)
2386 
2387     def force_stop(self, context, instance, do_cast=True, clean_shutdown=True):
2388         LOG.debug("Going to try to stop instance", instance=instance)
2389 
2390         instance.task_state = task_states.POWERING_OFF
2391         instance.progress = 0
2392         instance.save(expected_task_state=[None])
2393 
2394         self._record_action_start(context, instance, instance_actions.STOP)
2395 
2396         self.compute_rpcapi.stop_instance(context, instance, do_cast=do_cast,
2397                                           clean_shutdown=clean_shutdown)
2398 
2399     @check_instance_lock
2400     @check_instance_host
2401     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.ERROR])
2402     def stop(self, context, instance, do_cast=True, clean_shutdown=True):
2403         """Stop an instance."""
2404         self.force_stop(context, instance, do_cast, clean_shutdown)
2405 
2406     @check_instance_lock
2407     @check_instance_host
2408     @check_instance_state(vm_state=[vm_states.STOPPED])
2409     def start(self, context, instance):
2410         """Start an instance."""
2411         LOG.debug("Going to try to start instance", instance=instance)
2412 
2413         instance.task_state = task_states.POWERING_ON
2414         instance.save(expected_task_state=[None])
2415 
2416         self._record_action_start(context, instance, instance_actions.START)
2417         self.compute_rpcapi.start_instance(context, instance)
2418 
2419     @check_instance_lock
2420     @check_instance_host
2421     @check_instance_state(vm_state=vm_states.ALLOW_TRIGGER_CRASH_DUMP)
2422     def trigger_crash_dump(self, context, instance):
2423         """Trigger crash dump in an instance."""
2424         LOG.debug("Try to trigger crash dump", instance=instance)
2425 
2426         self._record_action_start(context, instance,
2427                                   instance_actions.TRIGGER_CRASH_DUMP)
2428 
2429         self.compute_rpcapi.trigger_crash_dump(context, instance)
2430 
2431     def _generate_minimal_construct_for_down_cells(self, context,
2432                                                    down_cell_uuids,
2433                                                    project, limit):
2434         """Generate a list of minimal instance constructs for a given list of
2435         cells that did not respond to a list operation. This will list
2436         every instance mapping in the affected cells and return a minimal
2437         objects.Instance for each (non-queued-for-delete) mapping.
2438 
2439         :param context: RequestContext
2440         :param down_cell_uuids: A list of cell UUIDs that did not respond
2441         :param project: A project ID to filter mappings, or None
2442         :param limit: A numeric limit on the number of results, or None
2443         :returns: An InstanceList() of partial Instance() objects
2444         """
2445         unavailable_servers = objects.InstanceList()
2446         for cell_uuid in down_cell_uuids:
2447             LOG.warning("Cell %s is not responding and hence only "
2448                         "partial results are available from this "
2449                         "cell if any.", cell_uuid)
2450             instance_mappings = (objects.InstanceMappingList.
2451                 get_not_deleted_by_cell_and_project(context, cell_uuid,
2452                                                     project, limit=limit))
2453             for im in instance_mappings:
2454                 unavailable_servers.objects.append(
2455                     objects.Instance(
2456                         context=context,
2457                         uuid=im.instance_uuid,
2458                         project_id=im.project_id,
2459                         created_at=im.created_at
2460                     )
2461                 )
2462             if limit is not None:
2463                 limit -= len(instance_mappings)
2464                 if limit <= 0:
2465                     break
2466         return unavailable_servers
2467 
2468     def _get_instance_map_or_none(self, context, instance_uuid):
2469         try:
2470             inst_map = objects.InstanceMapping.get_by_instance_uuid(
2471                     context, instance_uuid)
2472         except exception.InstanceMappingNotFound:
2473             # InstanceMapping should always be found generally. This exception
2474             # may be raised if a deployment has partially migrated the nova-api
2475             # services.
2476             inst_map = None
2477         return inst_map
2478 
2479     @staticmethod
2480     def _save_user_id_in_instance_mapping(mapping, instance):
2481         # TODO(melwitt): We take the opportunity to migrate user_id on the
2482         # instance mapping if it's not yet been migrated. This can be removed
2483         # in a future release, when all migrations are complete.
2484         # If the instance came from a RequestSpec because of a down cell, its
2485         # user_id could be None and the InstanceMapping.user_id field is
2486         # non-nullable. Avoid trying to set/save the user_id in that case.
2487         if 'user_id' not in mapping and instance.user_id is not None:
2488             mapping.user_id = instance.user_id
2489             mapping.save()
2490 
2491     def _get_instance_from_cell(self, context, im, expected_attrs,
2492                                 cell_down_support):
2493         # NOTE(danms): Even though we're going to scatter/gather to the
2494         # right cell, other code depends on this being force targeted when
2495         # the get call returns.
2496         nova_context.set_target_cell(context, im.cell_mapping)
2497 
2498         uuid = im.instance_uuid
2499         result = nova_context.scatter_gather_single_cell(context,
2500             im.cell_mapping, objects.Instance.get_by_uuid, uuid,
2501             expected_attrs=expected_attrs)
2502         cell_uuid = im.cell_mapping.uuid
2503         if not nova_context.is_cell_failure_sentinel(result[cell_uuid]):
2504             inst = result[cell_uuid]
2505             self._save_user_id_in_instance_mapping(im, inst)
2506             return inst
2507         elif isinstance(result[cell_uuid], exception.InstanceNotFound):
2508             raise exception.InstanceNotFound(instance_id=uuid)
2509         elif cell_down_support:
2510             if im.queued_for_delete:
2511                 # should be treated like deleted instance.
2512                 raise exception.InstanceNotFound(instance_id=uuid)
2513 
2514             # instance in down cell, return a minimal construct
2515             LOG.warning("Cell %s is not responding and hence only "
2516                         "partial results are available from this "
2517                         "cell.", cell_uuid)
2518             try:
2519                 rs = objects.RequestSpec.get_by_instance_uuid(context,
2520                                                               uuid)
2521                 # For BFV case, we could have rs.image but rs.image.id might
2522                 # still not be set. So we check the existence of both image
2523                 # and its id.
2524                 image_ref = (rs.image.id if rs.image and
2525                              'id' in rs.image else None)
2526                 inst = objects.Instance(context=context, power_state=0,
2527                                         uuid=uuid,
2528                                         project_id=im.project_id,
2529                                         created_at=im.created_at,
2530                                         user_id=rs.user_id,
2531                                         flavor=rs.flavor,
2532                                         image_ref=image_ref,
2533                                         availability_zone=rs.availability_zone)
2534                 self._save_user_id_in_instance_mapping(im, inst)
2535                 return inst
2536             except exception.RequestSpecNotFound:
2537                 # could be that a deleted instance whose request
2538                 # spec has been archived is being queried.
2539                 raise exception.InstanceNotFound(instance_id=uuid)
2540         else:
2541             raise exception.NovaException(
2542                 _("Cell %s is not responding and hence instance "
2543                   "info is not available.") % cell_uuid)
2544 
2545     def _get_instance(self, context, instance_uuid, expected_attrs,
2546                       cell_down_support=False):
2547         inst_map = self._get_instance_map_or_none(context, instance_uuid)
2548         if inst_map and (inst_map.cell_mapping is not None):
2549             instance = self._get_instance_from_cell(context, inst_map,
2550                 expected_attrs, cell_down_support)
2551         elif inst_map and (inst_map.cell_mapping is None):
2552             # This means the instance has not been scheduled and put in
2553             # a cell yet. For now it also may mean that the deployer
2554             # has not created their cell(s) yet.
2555             try:
2556                 build_req = objects.BuildRequest.get_by_instance_uuid(
2557                         context, instance_uuid)
2558                 instance = build_req.instance
2559             except exception.BuildRequestNotFound:
2560                 # Instance was mapped and the BuildRequest was deleted
2561                 # while fetching. Try again.
2562                 inst_map = self._get_instance_map_or_none(context,
2563                                                           instance_uuid)
2564                 if inst_map and (inst_map.cell_mapping is not None):
2565                     instance = self._get_instance_from_cell(context, inst_map,
2566                         expected_attrs, cell_down_support)
2567                 else:
2568                     raise exception.InstanceNotFound(instance_id=instance_uuid)
2569         else:
2570             # If we got here, we don't have an instance mapping, but we aren't
2571             # sure why. The instance mapping might be missing because the
2572             # upgrade is incomplete (map_instances wasn't run). Or because the
2573             # instance was deleted and the DB was archived at which point the
2574             # mapping is deleted. The former case is bad, but because of the
2575             # latter case we can't really log any kind of warning/error here
2576             # since it might be normal.
2577             raise exception.InstanceNotFound(instance_id=instance_uuid)
2578 
2579         return instance
2580 
2581     def get(self, context, instance_id, expected_attrs=None,
2582             cell_down_support=False):
2583         """Get a single instance with the given instance_id.
2584 
2585         :param cell_down_support: True if the API (and caller) support
2586                                   returning a minimal instance
2587                                   construct if the relevant cell is
2588                                   down. If False, an error is raised
2589                                   since the instance cannot be retrieved
2590                                   due to the cell being down.
2591         """
2592         if not expected_attrs:
2593             expected_attrs = []
2594         expected_attrs.extend(['metadata', 'system_metadata',
2595                                'security_groups', 'info_cache'])
2596         # NOTE(ameade): we still need to support integer ids for ec2
2597         try:
2598             if uuidutils.is_uuid_like(instance_id):
2599                 LOG.debug("Fetching instance by UUID",
2600                            instance_uuid=instance_id)
2601 
2602                 instance = self._get_instance(context, instance_id,
2603                     expected_attrs, cell_down_support=cell_down_support)
2604             else:
2605                 LOG.debug("Failed to fetch instance by id %s", instance_id)
2606                 raise exception.InstanceNotFound(instance_id=instance_id)
2607         except exception.InvalidID:
2608             LOG.debug("Invalid instance id %s", instance_id)
2609             raise exception.InstanceNotFound(instance_id=instance_id)
2610 
2611         return instance
2612 
2613     def get_all(self, context, search_opts=None, limit=None, marker=None,
2614                 expected_attrs=None, sort_keys=None, sort_dirs=None,
2615                 cell_down_support=False, all_tenants=False):
2616         """Get all instances filtered by one of the given parameters.
2617 
2618         If there is no filter and the context is an admin, it will retrieve
2619         all instances in the system.
2620 
2621         Deleted instances will be returned by default, unless there is a
2622         search option that says otherwise.
2623 
2624         The results will be sorted based on the list of sort keys in the
2625         'sort_keys' parameter (first value is primary sort key, second value is
2626         secondary sort ket, etc.). For each sort key, the associated sort
2627         direction is based on the list of sort directions in the 'sort_dirs'
2628         parameter.
2629 
2630         :param cell_down_support: True if the API (and caller) support
2631                                   returning a minimal instance
2632                                   construct if the relevant cell is
2633                                   down. If False, instances from
2634                                   unreachable cells will be omitted.
2635         :param all_tenants: True if the "all_tenants" filter was passed.
2636 
2637         """
2638         if search_opts is None:
2639             search_opts = {}
2640 
2641         LOG.debug("Searching by: %s", str(search_opts))
2642 
2643         # Fixups for the DB call
2644         filters = {}
2645 
2646         def _remap_flavor_filter(flavor_id):
2647             flavor = objects.Flavor.get_by_flavor_id(context, flavor_id)
2648             filters['instance_type_id'] = flavor.id
2649 
2650         def _remap_fixed_ip_filter(fixed_ip):
2651             # Turn fixed_ip into a regexp match. Since '.' matches
2652             # any character, we need to use regexp escaping for it.
2653             filters['ip'] = '^%s$' % fixed_ip.replace('.', '\\.')
2654 
2655         # search_option to filter_name mapping.
2656         filter_mapping = {
2657                 'image': 'image_ref',
2658                 'name': 'display_name',
2659                 'tenant_id': 'project_id',
2660                 'flavor': _remap_flavor_filter,
2661                 'fixed_ip': _remap_fixed_ip_filter}
2662 
2663         # copy from search_opts, doing various remappings as necessary
2664         for opt, value in search_opts.items():
2665             # Do remappings.
2666             # Values not in the filter_mapping table are copied as-is.
2667             # If remapping is None, option is not copied
2668             # If the remapping is a string, it is the filter_name to use
2669             try:
2670                 remap_object = filter_mapping[opt]
2671             except KeyError:
2672                 filters[opt] = value
2673             else:
2674                 # Remaps are strings to translate to, or functions to call
2675                 # to do the translating as defined by the table above.
2676                 if isinstance(remap_object, six.string_types):
2677                     filters[remap_object] = value
2678                 else:
2679                     try:
2680                         remap_object(value)
2681 
2682                     # We already know we can't match the filter, so
2683                     # return an empty list
2684                     except ValueError:
2685                         return objects.InstanceList()
2686 
2687         # IP address filtering cannot be applied at the DB layer, remove any DB
2688         # limit so that it can be applied after the IP filter.
2689         filter_ip = 'ip6' in filters or 'ip' in filters
2690         skip_build_request = False
2691         orig_limit = limit
2692         if filter_ip:
2693             # We cannot skip build requests if there is a marker since the
2694             # the marker could be a build request.
2695             skip_build_request = marker is None
2696             if self.network_api.has_substr_port_filtering_extension(context):
2697                 # We're going to filter by IP using Neutron so set filter_ip
2698                 # to False so we don't attempt post-DB query filtering in
2699                 # memory below.
2700                 filter_ip = False
2701                 instance_uuids = self._ip_filter_using_neutron(context,
2702                                                                filters)
2703                 if instance_uuids:
2704                     # Note that 'uuid' is not in the 2.1 GET /servers query
2705                     # parameter schema, however, we allow additionalProperties
2706                     # so someone could filter instances by uuid, which doesn't
2707                     # make a lot of sense but we have to account for it.
2708                     if 'uuid' in filters and filters['uuid']:
2709                         filter_uuids = filters['uuid']
2710                         if isinstance(filter_uuids, list):
2711                             instance_uuids.extend(filter_uuids)
2712                         else:
2713                             # Assume a string. If it's a dict or tuple or
2714                             # something, well...that's too bad. This is why
2715                             # we have query parameter schema definitions.
2716                             if filter_uuids not in instance_uuids:
2717                                 instance_uuids.append(filter_uuids)
2718                     filters['uuid'] = instance_uuids
2719                 else:
2720                     # No matches on the ip filter(s), return an empty list.
2721                     return objects.InstanceList()
2722             elif limit:
2723                 LOG.debug('Removing limit for DB query due to IP filter')
2724                 limit = None
2725 
2726         # Skip get BuildRequest if filtering by IP address, as building
2727         # instances will not have IP addresses.
2728         if skip_build_request:
2729             build_requests = objects.BuildRequestList()
2730         else:
2731             # The ordering of instances will be
2732             # [sorted instances with no host] + [sorted instances with host].
2733             # This means BuildRequest and cell0 instances first, then cell
2734             # instances
2735             try:
2736                 build_requests = objects.BuildRequestList.get_by_filters(
2737                     context, filters, limit=limit, marker=marker,
2738                     sort_keys=sort_keys, sort_dirs=sort_dirs)
2739                 # If we found the marker in we need to set it to None
2740                 # so we don't expect to find it in the cells below.
2741                 marker = None
2742             except exception.MarkerNotFound:
2743                 # If we didn't find the marker in the build requests then keep
2744                 # looking for it in the cells.
2745                 build_requests = objects.BuildRequestList()
2746 
2747         build_req_instances = objects.InstanceList(
2748             objects=[build_req.instance for build_req in build_requests])
2749         # Only subtract from limit if it is not None
2750         limit = (limit - len(build_req_instances)) if limit else limit
2751 
2752         # We could arguably avoid joining on security_groups if we're using
2753         # neutron (which is the default) but if you're using neutron then the
2754         # security_group_instance_association table should be empty anyway
2755         # and the DB should optimize out that join, making it insignificant.
2756         fields = ['metadata', 'info_cache', 'security_groups']
2757         if expected_attrs:
2758             fields.extend(expected_attrs)
2759 
2760         insts, down_cell_uuids = instance_list.get_instance_objects_sorted(
2761             context, filters, limit, marker, fields, sort_keys, sort_dirs,
2762             cell_down_support=cell_down_support)
2763 
2764         def _get_unique_filter_method():
2765             seen_uuids = set()
2766 
2767             def _filter(instance):
2768                 # During a cross-cell move operation we could have the instance
2769                 # in more than one cell database so we not only have to filter
2770                 # duplicates but we want to make sure we only return the
2771                 # "current" one which should also be the one that the instance
2772                 # mapping points to, but we don't want to do that expensive
2773                 # lookup here. The DB API will filter out hidden instances by
2774                 # default but there is a small window where two copies of an
2775                 # instance could be hidden=False in separate cell DBs.
2776                 # NOTE(mriedem): We could make this better in the case that we
2777                 # have duplicate instances that are both hidden=False by
2778                 # showing the one with the newer updated_at value, but that
2779                 # could be tricky if the user is filtering on
2780                 # changes-since/before or updated_at, or sorting on updated_at,
2781                 # but technically that was already potentially broken with this
2782                 # _filter method if we return an older BuildRequest.instance,
2783                 # and given the window should be very small where we have
2784                 # duplicates, it's probably not worth the complexity.
2785                 if instance.uuid in seen_uuids:
2786                     return False
2787                 seen_uuids.add(instance.uuid)
2788                 return True
2789 
2790             return _filter
2791 
2792         filter_method = _get_unique_filter_method()
2793         # Only subtract from limit if it is not None
2794         limit = (limit - len(insts)) if limit else limit
2795         # TODO(alaski): Clean up the objects concatenation when List objects
2796         # support it natively.
2797         instances = objects.InstanceList(
2798             objects=list(filter(filter_method,
2799                            build_req_instances.objects +
2800                            insts.objects)))
2801 
2802         if filter_ip:
2803             instances = self._ip_filter(instances, filters, orig_limit)
2804 
2805         if cell_down_support:
2806             # API and client want minimal construct instances for any cells
2807             # that didn't return, so generate and prefix those to the actual
2808             # results.
2809             project = search_opts.get('project_id', context.project_id)
2810             if all_tenants:
2811                 # NOTE(tssurya): The only scenario where project has to be None
2812                 # is when using "all_tenants" in which case we do not want
2813                 # the query to be restricted based on the project_id.
2814                 project = None
2815             limit = (orig_limit - len(instances)) if limit else limit
2816             return (self._generate_minimal_construct_for_down_cells(context,
2817                 down_cell_uuids, project, limit) + instances)
2818 
2819         return instances
2820 
2821     @staticmethod
2822     def _ip_filter(inst_models, filters, limit):
2823         ipv4_f = re.compile(str(filters.get('ip')))
2824         ipv6_f = re.compile(str(filters.get('ip6')))
2825 
2826         def _match_instance(instance):
2827             nw_info = instance.get_network_info()
2828             for vif in nw_info:
2829                 for fixed_ip in vif.fixed_ips():
2830                     address = fixed_ip.get('address')
2831                     if not address:
2832                         continue
2833                     version = fixed_ip.get('version')
2834                     if ((version == 4 and ipv4_f.match(address)) or
2835                         (version == 6 and ipv6_f.match(address))):
2836                         return True
2837             return False
2838 
2839         result_objs = []
2840         for instance in inst_models:
2841             if _match_instance(instance):
2842                 result_objs.append(instance)
2843                 if limit and len(result_objs) == limit:
2844                     break
2845         return objects.InstanceList(objects=result_objs)
2846 
2847     def _ip_filter_using_neutron(self, context, filters):
2848         ip4_address = filters.get('ip')
2849         ip6_address = filters.get('ip6')
2850         addresses = [ip4_address, ip6_address]
2851         uuids = []
2852         for address in addresses:
2853             if address:
2854                 try:
2855                     ports = self.network_api.list_ports(
2856                         context, fixed_ips='ip_address_substr=' + address,
2857                         fields=['device_id'])['ports']
2858                     for port in ports:
2859                         uuids.append(port['device_id'])
2860                 except Exception as e:
2861                     LOG.error('An error occurred while listing ports '
2862                               'with an ip_address filter value of "%s". '
2863                               'Error: %s',
2864                               address, six.text_type(e))
2865         return uuids
2866 
2867     def update_instance(self, context, instance, updates):
2868         """Updates a single Instance object with some updates dict.
2869 
2870         Returns the updated instance.
2871         """
2872 
2873         # NOTE(sbauza): Given we only persist the Instance object after we
2874         # create the BuildRequest, we are sure that if the Instance object
2875         # has an ID field set, then it was persisted in the right Cell DB.
2876         if instance.obj_attr_is_set('id'):
2877             instance.update(updates)
2878             instance.save()
2879         else:
2880             # Instance is not yet mapped to a cell, so we need to update
2881             # BuildRequest instead
2882             # TODO(sbauza): Fix the possible race conditions where BuildRequest
2883             # could be deleted because of either a concurrent instance delete
2884             # or because the scheduler just returned a destination right
2885             # after we called the instance in the API.
2886             try:
2887                 build_req = objects.BuildRequest.get_by_instance_uuid(
2888                     context, instance.uuid)
2889                 instance = build_req.instance
2890                 instance.update(updates)
2891                 # FIXME(sbauza): Here we are updating the current
2892                 # thread-related BuildRequest object. Given that another worker
2893                 # could have looking up at that BuildRequest in the API, it
2894                 # means that it could pass it down to the conductor without
2895                 # making sure that it's not updated, we could have some race
2896                 # condition where it would missing the updated fields, but
2897                 # that's something we could discuss once the instance record
2898                 # is persisted by the conductor.
2899                 build_req.save()
2900             except exception.BuildRequestNotFound:
2901                 # Instance was mapped and the BuildRequest was deleted
2902                 # while fetching (and possibly the instance could have been
2903                 # deleted as well). We need to lookup again the Instance object
2904                 # in order to correctly update it.
2905                 # TODO(sbauza): Figure out a good way to know the expected
2906                 # attributes by checking which fields are set or not.
2907                 expected_attrs = ['flavor', 'pci_devices', 'numa_topology',
2908                                   'tags', 'metadata', 'system_metadata',
2909                                   'security_groups', 'info_cache']
2910                 inst_map = self._get_instance_map_or_none(context,
2911                                                           instance.uuid)
2912                 if inst_map and (inst_map.cell_mapping is not None):
2913                     with nova_context.target_cell(
2914                             context,
2915                             inst_map.cell_mapping) as cctxt:
2916                         instance = objects.Instance.get_by_uuid(
2917                             cctxt, instance.uuid,
2918                             expected_attrs=expected_attrs)
2919                         instance.update(updates)
2920                         instance.save()
2921                 else:
2922                     # Conductor doesn't delete the BuildRequest until after the
2923                     # InstanceMapping record is created, so if we didn't get
2924                     # that and the BuildRequest doesn't exist, then the
2925                     # instance is already gone and we need to just error out.
2926                     raise exception.InstanceNotFound(instance_id=instance.uuid)
2927         return instance
2928 
2929     # NOTE(melwitt): We don't check instance lock for backup because lock is
2930     #                intended to prevent accidental change/delete of instances
2931     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
2932                                     vm_states.PAUSED, vm_states.SUSPENDED])
2933     def backup(self, context, instance, name, backup_type, rotation,
2934                extra_properties=None):
2935         """Backup the given instance
2936 
2937         :param instance: nova.objects.instance.Instance object
2938         :param name: name of the backup
2939         :param backup_type: 'daily' or 'weekly'
2940         :param rotation: int representing how many backups to keep around;
2941             None if rotation shouldn't be used (as in the case of snapshots)
2942         :param extra_properties: dict of extra image properties to include
2943                                  when creating the image.
2944         :returns: A dict containing image metadata
2945         """
2946         props_copy = dict(extra_properties, backup_type=backup_type)
2947 
2948         if compute_utils.is_volume_backed_instance(context, instance):
2949             LOG.info("It's not supported to backup volume backed "
2950                      "instance.", instance=instance)
2951             raise exception.InvalidRequest(
2952                 _('Backup is not supported for volume-backed instances.'))
2953         else:
2954             image_meta = compute_utils.create_image(
2955                 context, instance, name, 'backup', self.image_api,
2956                 extra_properties=props_copy)
2957 
2958         instance.task_state = task_states.IMAGE_BACKUP
2959         instance.save(expected_task_state=[None])
2960 
2961         self._record_action_start(context, instance,
2962                                   instance_actions.BACKUP)
2963 
2964         self.compute_rpcapi.backup_instance(context, instance,
2965                                             image_meta['id'],
2966                                             backup_type,
2967                                             rotation)
2968         return image_meta
2969 
2970     # NOTE(melwitt): We don't check instance lock for snapshot because lock is
2971     #                intended to prevent accidental change/delete of instances
2972     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
2973                                     vm_states.PAUSED, vm_states.SUSPENDED])
2974     def snapshot(self, context, instance, name, extra_properties=None):
2975         """Snapshot the given instance.
2976 
2977         :param instance: nova.objects.instance.Instance object
2978         :param name: name of the snapshot
2979         :param extra_properties: dict of extra image properties to include
2980                                  when creating the image.
2981         :returns: A dict containing image metadata
2982         """
2983         image_meta = compute_utils.create_image(
2984             context, instance, name, 'snapshot', self.image_api,
2985             extra_properties=extra_properties)
2986 
2987         instance.task_state = task_states.IMAGE_SNAPSHOT_PENDING
2988         try:
2989             instance.save(expected_task_state=[None])
2990         except (exception.InstanceNotFound,
2991                 exception.UnexpectedDeletingTaskStateError) as ex:
2992             # Changing the instance task state to use in raising the
2993             # InstanceInvalidException below
2994             LOG.debug('Instance disappeared during snapshot.',
2995                       instance=instance)
2996             try:
2997                 image_id = image_meta['id']
2998                 self.image_api.delete(context, image_id)
2999                 LOG.info('Image %s deleted because instance '
3000                          'deleted before snapshot started.',
3001                          image_id, instance=instance)
3002             except exception.ImageNotFound:
3003                 pass
3004             except Exception as exc:
3005                 LOG.warning("Error while trying to clean up image %(img_id)s: "
3006                             "%(error_msg)s",
3007                             {"img_id": image_meta['id'],
3008                              "error_msg": six.text_type(exc)})
3009             attr = 'task_state'
3010             state = task_states.DELETING
3011             if type(ex) == exception.InstanceNotFound:
3012                 attr = 'vm_state'
3013                 state = vm_states.DELETED
3014             raise exception.InstanceInvalidState(attr=attr,
3015                                            instance_uuid=instance.uuid,
3016                                            state=state,
3017                                            method='snapshot')
3018 
3019         self._record_action_start(context, instance,
3020                                   instance_actions.CREATE_IMAGE)
3021 
3022         self.compute_rpcapi.snapshot_instance(context, instance,
3023                                               image_meta['id'])
3024 
3025         return image_meta
3026 
3027     # NOTE(melwitt): We don't check instance lock for snapshot because lock is
3028     #                intended to prevent accidental change/delete of instances
3029     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
3030                                     vm_states.SUSPENDED])
3031     def snapshot_volume_backed(self, context, instance, name,
3032                                extra_properties=None):
3033         """Snapshot the given volume-backed instance.
3034 
3035         :param instance: nova.objects.instance.Instance object
3036         :param name: name of the backup or snapshot
3037         :param extra_properties: dict of extra image properties to include
3038 
3039         :returns: the new image metadata
3040         """
3041         image_meta = compute_utils.initialize_instance_snapshot_metadata(
3042             context, instance, name, extra_properties)
3043         # the new image is simply a bucket of properties (particularly the
3044         # block device mapping, kernel and ramdisk IDs) with no image data,
3045         # hence the zero size
3046         image_meta['size'] = 0
3047         for attr in ('container_format', 'disk_format'):
3048             image_meta.pop(attr, None)
3049         properties = image_meta['properties']
3050         # clean properties before filling
3051         for key in ('block_device_mapping', 'bdm_v2', 'root_device_name'):
3052             properties.pop(key, None)
3053         if instance.root_device_name:
3054             properties['root_device_name'] = instance.root_device_name
3055 
3056         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
3057                 context, instance.uuid)
3058 
3059         mapping = []  # list of BDM dicts that can go into the image properties
3060         # Do some up-front filtering of the list of BDMs from
3061         # which we are going to create snapshots.
3062         volume_bdms = []
3063         for bdm in bdms:
3064             if bdm.no_device:
3065                 continue
3066             if bdm.is_volume:
3067                 # These will be handled below.
3068                 volume_bdms.append(bdm)
3069             else:
3070                 mapping.append(bdm.get_image_mapping())
3071 
3072         # Check limits in Cinder before creating snapshots to avoid going over
3073         # quota in the middle of a list of volumes. This is a best-effort check
3074         # but concurrently running snapshot requests from the same project
3075         # could still fail to create volume snapshots if they go over limit.
3076         if volume_bdms:
3077             limits = self.volume_api.get_absolute_limits(context)
3078             total_snapshots_used = limits['totalSnapshotsUsed']
3079             max_snapshots = limits['maxTotalSnapshots']
3080             # -1 means there is unlimited quota for snapshots
3081             if (max_snapshots > -1 and
3082                     len(volume_bdms) + total_snapshots_used > max_snapshots):
3083                 LOG.debug('Unable to create volume snapshots for instance. '
3084                           'Currently has %s snapshots, requesting %s new '
3085                           'snapshots, with a limit of %s.',
3086                           total_snapshots_used, len(volume_bdms),
3087                           max_snapshots, instance=instance)
3088                 raise exception.OverQuota(overs='snapshots')
3089 
3090         quiesced = False
3091         if instance.vm_state == vm_states.ACTIVE:
3092             try:
3093                 LOG.info("Attempting to quiesce instance before volume "
3094                          "snapshot.", instance=instance)
3095                 self.compute_rpcapi.quiesce_instance(context, instance)
3096                 quiesced = True
3097             except (exception.InstanceQuiesceNotSupported,
3098                     exception.QemuGuestAgentNotEnabled,
3099                     exception.NovaException, NotImplementedError) as err:
3100                 if strutils.bool_from_string(instance.system_metadata.get(
3101                         'image_os_require_quiesce')):
3102                     raise
3103 
3104                 if isinstance(err, exception.NovaException):
3105                     LOG.info('Skipping quiescing instance: %(reason)s.',
3106                              {'reason': err.format_message()},
3107                              instance=instance)
3108                 else:
3109                     LOG.info('Skipping quiescing instance because the '
3110                              'operation is not supported by the underlying '
3111                              'compute driver.', instance=instance)
3112             # NOTE(tasker): discovered that an uncaught exception could occur
3113             #               after the instance has been frozen. catch and thaw.
3114             except Exception as ex:
3115                 with excutils.save_and_reraise_exception():
3116                     LOG.error("An error occurred during quiesce of instance. "
3117                               "Unquiescing to ensure instance is thawed. "
3118                               "Error: %s", six.text_type(ex),
3119                               instance=instance)
3120                     self.compute_rpcapi.unquiesce_instance(context, instance,
3121                                                            mapping=None)
3122 
3123         @wrap_instance_event(prefix='api')
3124         def snapshot_instance(self, context, instance, bdms):
3125             try:
3126                 for bdm in volume_bdms:
3127                     # create snapshot based on volume_id
3128                     volume = self.volume_api.get(context, bdm.volume_id)
3129                     # NOTE(yamahata): Should we wait for snapshot creation?
3130                     #                 Linux LVM snapshot creation completes in
3131                     #                 short time, it doesn't matter for now.
3132                     name = _('snapshot for %s') % image_meta['name']
3133                     LOG.debug('Creating snapshot from volume %s.',
3134                               volume['id'], instance=instance)
3135                     snapshot = self.volume_api.create_snapshot_force(
3136                         context, volume['id'],
3137                         name, volume['display_description'])
3138                     mapping_dict = block_device.snapshot_from_bdm(
3139                         snapshot['id'], bdm)
3140                     mapping_dict = mapping_dict.get_image_mapping()
3141                     mapping.append(mapping_dict)
3142                 return mapping
3143             # NOTE(tasker): No error handling is done in the above for loop.
3144             # This means that if the snapshot fails and throws an exception
3145             # the traceback will skip right over the unquiesce needed below.
3146             # Here, catch any exception, unquiesce the instance, and raise the
3147             # error so that the calling function can do what it needs to in
3148             # order to properly treat a failed snap.
3149             except Exception:
3150                 with excutils.save_and_reraise_exception():
3151                     if quiesced:
3152                         LOG.info("Unquiescing instance after volume snapshot "
3153                                  "failure.", instance=instance)
3154                         self.compute_rpcapi.unquiesce_instance(
3155                             context, instance, mapping)
3156 
3157         self._record_action_start(context, instance,
3158                                   instance_actions.CREATE_IMAGE)
3159         mapping = snapshot_instance(self, context, instance, bdms)
3160 
3161         if quiesced:
3162             self.compute_rpcapi.unquiesce_instance(context, instance, mapping)
3163 
3164         if mapping:
3165             properties['block_device_mapping'] = mapping
3166             properties['bdm_v2'] = True
3167 
3168         return self.image_api.create(context, image_meta)
3169 
3170     @check_instance_lock
3171     def reboot(self, context, instance, reboot_type):
3172         """Reboot the given instance."""
3173         if reboot_type == 'SOFT':
3174             self._soft_reboot(context, instance)
3175         else:
3176             self._hard_reboot(context, instance)
3177 
3178     @check_instance_state(vm_state=set(vm_states.ALLOW_SOFT_REBOOT),
3179                           task_state=[None])
3180     def _soft_reboot(self, context, instance):
3181         expected_task_state = [None]
3182         instance.task_state = task_states.REBOOTING
3183         instance.save(expected_task_state=expected_task_state)
3184 
3185         self._record_action_start(context, instance, instance_actions.REBOOT)
3186 
3187         self.compute_rpcapi.reboot_instance(context, instance=instance,
3188                                             block_device_info=None,
3189                                             reboot_type='SOFT')
3190 
3191     @check_instance_state(vm_state=set(vm_states.ALLOW_HARD_REBOOT),
3192                           task_state=task_states.ALLOW_REBOOT)
3193     def _hard_reboot(self, context, instance):
3194         instance.task_state = task_states.REBOOTING_HARD
3195         instance.save(expected_task_state=task_states.ALLOW_REBOOT)
3196 
3197         self._record_action_start(context, instance, instance_actions.REBOOT)
3198 
3199         self.compute_rpcapi.reboot_instance(context, instance=instance,
3200                                             block_device_info=None,
3201                                             reboot_type='HARD')
3202 
3203     # TODO(stephenfin): We should expand kwargs out to named args
3204     @check_instance_lock
3205     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
3206                                     vm_states.ERROR])
3207     def rebuild(self, context, instance, image_href, admin_password,
3208                 files_to_inject=None, **kwargs):
3209         """Rebuild the given instance with the provided attributes."""
3210         files_to_inject = files_to_inject or []
3211         metadata = kwargs.get('metadata', {})
3212         preserve_ephemeral = kwargs.get('preserve_ephemeral', False)
3213         auto_disk_config = kwargs.get('auto_disk_config')
3214 
3215         if 'key_name' in kwargs:
3216             key_name = kwargs.pop('key_name')
3217             if key_name:
3218                 # NOTE(liuyulong): we are intentionally using the user_id from
3219                 # the request context rather than the instance.user_id because
3220                 # users own keys but instances are owned by projects, and
3221                 # another user in the same project can rebuild an instance
3222                 # even if they didn't create it.
3223                 key_pair = objects.KeyPair.get_by_name(context,
3224                                                        context.user_id,
3225                                                        key_name)
3226                 instance.key_name = key_pair.name
3227                 instance.key_data = key_pair.public_key
3228                 instance.keypairs = objects.KeyPairList(objects=[key_pair])
3229             else:
3230                 instance.key_name = None
3231                 instance.key_data = None
3232                 instance.keypairs = objects.KeyPairList(objects=[])
3233 
3234         # Use trusted_certs value from kwargs to create TrustedCerts object
3235         trusted_certs = None
3236         if 'trusted_certs' in kwargs:
3237             # Note that the user can set, change, or unset / reset trusted
3238             # certs. If they are explicitly specifying
3239             # trusted_image_certificates=None, that means we'll either unset
3240             # them on the instance *or* reset to use the defaults (if defaults
3241             # are configured).
3242             trusted_certs = kwargs.pop('trusted_certs')
3243             instance.trusted_certs = self._retrieve_trusted_certs_object(
3244                 context, trusted_certs, rebuild=True)
3245 
3246         image_id, image = self._get_image(context, image_href)
3247         self._check_auto_disk_config(image=image, **kwargs)
3248 
3249         flavor = instance.get_flavor()
3250         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
3251             context, instance.uuid)
3252         root_bdm = compute_utils.get_root_bdm(context, instance, bdms)
3253 
3254         # Check to see if the image is changing and we have a volume-backed
3255         # server. The compute doesn't support changing the image in the
3256         # root disk of a volume-backed server, so we need to just fail fast.
3257         is_volume_backed = compute_utils.is_volume_backed_instance(
3258             context, instance, bdms)
3259         if is_volume_backed:
3260             if trusted_certs:
3261                 # The only way we can get here is if the user tried to set
3262                 # trusted certs or specified trusted_image_certificates=None
3263                 # and default_trusted_certificate_ids is configured.
3264                 msg = _("Image certificate validation is not supported "
3265                         "for volume-backed servers.")
3266                 raise exception.CertificateValidationFailed(message=msg)
3267 
3268             # For boot from volume, instance.image_ref is empty, so we need to
3269             # query the image from the volume.
3270             if root_bdm is None:
3271                 # This shouldn't happen and is an error, we need to fail. This
3272                 # is not the users fault, it's an internal error. Without a
3273                 # root BDM we have no way of knowing the backing volume (or
3274                 # image in that volume) for this instance.
3275                 raise exception.NovaException(
3276                     _('Unable to find root block device mapping for '
3277                       'volume-backed instance.'))
3278 
3279             volume = self.volume_api.get(context, root_bdm.volume_id)
3280             volume_image_metadata = volume.get('volume_image_metadata', {})
3281             orig_image_ref = volume_image_metadata.get('image_id')
3282 
3283             if orig_image_ref != image_href:
3284                 # Leave a breadcrumb.
3285                 LOG.debug('Requested to rebuild instance with a new image %s '
3286                           'for a volume-backed server with image %s in its '
3287                           'root volume which is not supported.', image_href,
3288                           orig_image_ref, instance=instance)
3289                 msg = _('Unable to rebuild with a different image for a '
3290                         'volume-backed server.')
3291                 raise exception.ImageUnacceptable(
3292                     image_id=image_href, reason=msg)
3293         else:
3294             orig_image_ref = instance.image_ref
3295 
3296         request_spec = objects.RequestSpec.get_by_instance_uuid(
3297             context, instance.uuid)
3298 
3299         self._checks_for_create_and_rebuild(context, image_id, image,
3300                 flavor, metadata, files_to_inject, root_bdm)
3301 
3302         kernel_id, ramdisk_id = self._handle_kernel_and_ramdisk(
3303                 context, None, None, image)
3304 
3305         def _reset_image_metadata():
3306             """Remove old image properties that we're storing as instance
3307             system metadata.  These properties start with 'image_'.
3308             Then add the properties for the new image.
3309             """
3310             # FIXME(comstud): There's a race condition here in that if
3311             # the system_metadata for this instance is updated after
3312             # we do the previous save() and before we update.. those
3313             # other updates will be lost. Since this problem exists in
3314             # a lot of other places, I think it should be addressed in
3315             # a DB layer overhaul.
3316 
3317             orig_sys_metadata = dict(instance.system_metadata)
3318             # Remove the old keys
3319             for key in list(instance.system_metadata.keys()):
3320                 if key.startswith(utils.SM_IMAGE_PROP_PREFIX):
3321                     del instance.system_metadata[key]
3322 
3323             # Add the new ones
3324             new_sys_metadata = utils.get_system_metadata_from_image(
3325                 image, flavor)
3326 
3327             instance.system_metadata.update(new_sys_metadata)
3328             instance.save()
3329             return orig_sys_metadata
3330 
3331         # Since image might have changed, we may have new values for
3332         # os_type, vm_mode, etc
3333         options_from_image = self._inherit_properties_from_image(
3334                 image, auto_disk_config)
3335         instance.update(options_from_image)
3336 
3337         instance.task_state = task_states.REBUILDING
3338         # An empty instance.image_ref is currently used as an indication
3339         # of BFV.  Preserve that over a rebuild to not break users.
3340         if not is_volume_backed:
3341             instance.image_ref = image_href
3342         instance.kernel_id = kernel_id or ""
3343         instance.ramdisk_id = ramdisk_id or ""
3344         instance.progress = 0
3345         instance.update(kwargs)
3346         instance.save(expected_task_state=[None])
3347 
3348         # On a rebuild, since we're potentially changing images, we need to
3349         # wipe out the old image properties that we're storing as instance
3350         # system metadata... and copy in the properties for the new image.
3351         orig_sys_metadata = _reset_image_metadata()
3352 
3353         self._record_action_start(context, instance, instance_actions.REBUILD)
3354 
3355         # NOTE(sbauza): The migration script we provided in Newton should make
3356         # sure that all our instances are currently migrated to have an
3357         # attached RequestSpec object but let's consider that the operator only
3358         # half migrated all their instances in the meantime.
3359         host = instance.host
3360         # If a new image is provided on rebuild, we will need to run
3361         # through the scheduler again, but we want the instance to be
3362         # rebuilt on the same host it's already on.
3363         if orig_image_ref != image_href:
3364             # We have to modify the request spec that goes to the scheduler
3365             # to contain the new image. We persist this since we've already
3366             # changed the instance.image_ref above so we're being
3367             # consistent.
3368             request_spec.image = objects.ImageMeta.from_dict(image)
3369             request_spec.save()
3370             if 'scheduler_hints' not in request_spec:
3371                 request_spec.scheduler_hints = {}
3372             # Nuke the id on this so we can't accidentally save
3373             # this hint hack later
3374             del request_spec.id
3375 
3376             # NOTE(danms): Passing host=None tells conductor to
3377             # call the scheduler. The _nova_check_type hint
3378             # requires that the scheduler returns only the same
3379             # host that we are currently on and only checks
3380             # rebuild-related filters.
3381             request_spec.scheduler_hints['_nova_check_type'] = ['rebuild']
3382             request_spec.force_hosts = [instance.host]
3383             request_spec.force_nodes = [instance.node]
3384             host = None
3385 
3386         self.compute_task_api.rebuild_instance(context, instance=instance,
3387                 new_pass=admin_password, injected_files=files_to_inject,
3388                 image_ref=image_href, orig_image_ref=orig_image_ref,
3389                 orig_sys_metadata=orig_sys_metadata, bdms=bdms,
3390                 preserve_ephemeral=preserve_ephemeral, host=host,
3391                 request_spec=request_spec)
3392 
3393     @staticmethod
3394     def _check_quota_for_upsize(context, instance, current_flavor, new_flavor):
3395         project_id, user_id = quotas_obj.ids_from_instance(context,
3396                                                            instance)
3397         # Deltas will be empty if the resize is not an upsize.
3398         deltas = compute_utils.upsize_quota_delta(new_flavor,
3399                                                   current_flavor)
3400         if deltas:
3401             try:
3402                 res_deltas = {'cores': deltas.get('cores', 0),
3403                               'ram': deltas.get('ram', 0)}
3404                 objects.Quotas.check_deltas(context, res_deltas,
3405                                             project_id, user_id=user_id,
3406                                             check_project_id=project_id,
3407                                             check_user_id=user_id)
3408             except exception.OverQuota as exc:
3409                 quotas = exc.kwargs['quotas']
3410                 overs = exc.kwargs['overs']
3411                 usages = exc.kwargs['usages']
3412                 headroom = compute_utils.get_headroom(quotas, usages,
3413                                                       deltas)
3414                 (overs, reqs, total_alloweds,
3415                  useds) = compute_utils.get_over_quota_detail(headroom,
3416                                                               overs,
3417                                                               quotas,
3418                                                               deltas)
3419                 LOG.info("%(overs)s quota exceeded for %(pid)s,"
3420                          " tried to resize instance.",
3421                          {'overs': overs, 'pid': context.project_id})
3422                 raise exception.TooManyInstances(overs=overs,
3423                                                  req=reqs,
3424                                                  used=useds,
3425                                                  allowed=total_alloweds)
3426 
3427     @check_instance_lock
3428     @check_instance_state(vm_state=[vm_states.RESIZED])
3429     def revert_resize(self, context, instance):
3430         """Reverts a resize or cold migration, deleting the 'new' instance in
3431         the process.
3432         """
3433         elevated = context.elevated()
3434         migration = objects.Migration.get_by_instance_and_status(
3435             elevated, instance.uuid, 'finished')
3436 
3437         # If this is a resize down, a revert might go over quota.
3438         self._check_quota_for_upsize(context, instance, instance.flavor,
3439                                      instance.old_flavor)
3440 
3441         # The AZ for the server may have changed when it was migrated so while
3442         # we are in the API and have access to the API DB, update the
3443         # instance.availability_zone before casting off to the compute service.
3444         # Note that we do this in the API to avoid an "up-call" from the
3445         # compute service to the API DB. This is not great in case something
3446         # fails during revert before the instance.host is updated to the
3447         # original source host, but it is good enough for now. Long-term we
3448         # could consider passing the AZ down to compute so it can set it when
3449         # the instance.host value is set in finish_revert_resize.
3450         instance.availability_zone = (
3451             availability_zones.get_host_availability_zone(
3452                 context, migration.source_compute))
3453 
3454         # Conductor updated the RequestSpec.flavor during the initial resize
3455         # operation to point at the new flavor, so we need to update the
3456         # RequestSpec to point back at the original flavor, otherwise
3457         # subsequent move operations through the scheduler will be using the
3458         # wrong flavor.
3459         reqspec = objects.RequestSpec.get_by_instance_uuid(
3460             context, instance.uuid)
3461         reqspec.flavor = instance.old_flavor
3462         reqspec.save()
3463 
3464         # NOTE(gibi): This is a performance optimization. If the network info
3465         # cache does not have ports with allocations in the binding profile
3466         # then we can skip reading port resource request from neutron below.
3467         # If a port has resource request then that would have already caused
3468         # that the finish_resize call put allocation in the binding profile
3469         # during the resize.
3470         if instance.get_network_info().has_port_with_allocation():
3471             # TODO(gibi): do not directly overwrite the
3472             # RequestSpec.requested_resources as others like cyborg might added
3473             # to things there already
3474             # NOTE(gibi): We need to collect the requested resource again as it
3475             # is intentionally not persisted in nova. Note that this needs to
3476             # be done here as the nova API code directly calls revert on the
3477             # dest compute service skipping the conductor.
3478             port_res_req = (
3479                 self.network_api.get_requested_resource_for_instance(
3480                     context, instance.uuid))
3481             reqspec.requested_resources = port_res_req
3482 
3483         instance.task_state = task_states.RESIZE_REVERTING
3484         instance.save(expected_task_state=[None])
3485 
3486         migration.status = 'reverting'
3487         migration.save()
3488 
3489         self._record_action_start(context, instance,
3490                                   instance_actions.REVERT_RESIZE)
3491 
3492         # TODO(melwitt): We're not rechecking for strict quota here to guard
3493         # against going over quota during a race at this time because the
3494         # resource consumption for this operation is written to the database
3495         # by compute.
3496         self.compute_rpcapi.revert_resize(context, instance,
3497                                           migration,
3498                                           migration.dest_compute,
3499                                           reqspec)
3500 
3501     @check_instance_lock
3502     @check_instance_state(vm_state=[vm_states.RESIZED])
3503     def confirm_resize(self, context, instance, migration=None):
3504         """Confirms a migration/resize and deletes the 'old' instance."""
3505         elevated = context.elevated()
3506         # NOTE(melwitt): We're not checking quota here because there isn't a
3507         # change in resource usage when confirming a resize. Resource
3508         # consumption for resizes are written to the database by compute, so
3509         # a confirm resize is just a clean up of the migration objects and a
3510         # state change in compute.
3511         if migration is None:
3512             migration = objects.Migration.get_by_instance_and_status(
3513                 elevated, instance.uuid, 'finished')
3514 
3515         migration.status = 'confirming'
3516         migration.save()
3517 
3518         self._record_action_start(context, instance,
3519                                   instance_actions.CONFIRM_RESIZE)
3520 
3521         self.compute_rpcapi.confirm_resize(context,
3522                                            instance,
3523                                            migration,
3524                                            migration.source_compute)
3525 
3526     # TODO(mriedem): It looks like for resize (not cold migrate) the only
3527     # possible kwarg here is auto_disk_config. Drop this dumb **kwargs and make
3528     # it explicitly an auto_disk_config param
3529     @check_instance_lock
3530     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED])
3531     def resize(self, context, instance, flavor_id=None, clean_shutdown=True,
3532                host_name=None, **extra_instance_updates):
3533         """Resize (ie, migrate) a running instance.
3534 
3535         If flavor_id is None, the process is considered a migration, keeping
3536         the original flavor_id. If flavor_id is not None, the instance should
3537         be migrated to a new host and resized to the new flavor_id.
3538         host_name is always None in the resize case.
3539         host_name can be set in the cold migration case only.
3540         """
3541         if host_name is not None:
3542             # Cannot migrate to the host where the instance exists
3543             # because it is useless.
3544             if host_name == instance.host:
3545                 raise exception.CannotMigrateToSameHost()
3546 
3547             # Check whether host exists or not.
3548             node = objects.ComputeNode.get_first_node_by_host_for_old_compat(
3549                 context, host_name, use_slave=True)
3550 
3551         self._check_auto_disk_config(instance, **extra_instance_updates)
3552 
3553         current_instance_type = instance.get_flavor()
3554 
3555         # If flavor_id is not provided, only migrate the instance.
3556         volume_backed = None
3557         if not flavor_id:
3558             LOG.debug("flavor_id is None. Assuming migration.",
3559                       instance=instance)
3560             new_instance_type = current_instance_type
3561         else:
3562             new_instance_type = flavors.get_flavor_by_flavor_id(
3563                     flavor_id, read_deleted="no")
3564             # Check to see if we're resizing to a zero-disk flavor which is
3565             # only supported with volume-backed servers.
3566             if (new_instance_type.get('root_gb') == 0 and
3567                     current_instance_type.get('root_gb') != 0):
3568                 volume_backed = compute_utils.is_volume_backed_instance(
3569                         context, instance)
3570                 if not volume_backed:
3571                     reason = _('Resize to zero disk flavor is not allowed.')
3572                     raise exception.CannotResizeDisk(reason=reason)
3573 
3574         current_instance_type_name = current_instance_type['name']
3575         new_instance_type_name = new_instance_type['name']
3576         LOG.debug("Old instance type %(current_instance_type_name)s, "
3577                   "new instance type %(new_instance_type_name)s",
3578                   {'current_instance_type_name': current_instance_type_name,
3579                    'new_instance_type_name': new_instance_type_name},
3580                   instance=instance)
3581 
3582         same_instance_type = (current_instance_type['id'] ==
3583                               new_instance_type['id'])
3584 
3585         # NOTE(sirp): We don't want to force a customer to change their flavor
3586         # when Ops is migrating off of a failed host.
3587         if not same_instance_type and new_instance_type.get('disabled'):
3588             raise exception.FlavorNotFound(flavor_id=flavor_id)
3589 
3590         if same_instance_type and flavor_id:
3591             raise exception.CannotResizeToSameFlavor()
3592 
3593         # ensure there is sufficient headroom for upsizes
3594         if flavor_id:
3595             self._check_quota_for_upsize(context, instance,
3596                                          current_instance_type,
3597                                          new_instance_type)
3598 
3599         if not same_instance_type:
3600             image = utils.get_image_from_system_metadata(
3601                 instance.system_metadata)
3602             # Figure out if the instance is volume-backed but only if we didn't
3603             # already figure that out above (avoid the extra db hit).
3604             if volume_backed is None:
3605                 volume_backed = compute_utils.is_volume_backed_instance(
3606                     context, instance)
3607             # If the server is volume-backed, we still want to validate numa
3608             # and pci information in the new flavor, but we don't call
3609             # _validate_flavor_image_nostatus because how it handles checking
3610             # disk size validation was not intended for a volume-backed
3611             # resize case.
3612             if volume_backed:
3613                 self._validate_flavor_image_numa_pci(
3614                     image, new_instance_type, validate_pci=True)
3615             else:
3616                 self._validate_flavor_image_nostatus(
3617                     context, image, new_instance_type, root_bdm=None,
3618                     validate_pci=True)
3619 
3620         filter_properties = {'ignore_hosts': []}
3621 
3622         if not CONF.allow_resize_to_same_host:
3623             filter_properties['ignore_hosts'].append(instance.host)
3624 
3625         request_spec = objects.RequestSpec.get_by_instance_uuid(
3626             context, instance.uuid)
3627         request_spec.ignore_hosts = filter_properties['ignore_hosts']
3628 
3629         instance.task_state = task_states.RESIZE_PREP
3630         instance.progress = 0
3631         instance.update(extra_instance_updates)
3632         instance.save(expected_task_state=[None])
3633 
3634         if not flavor_id:
3635             self._record_action_start(context, instance,
3636                                       instance_actions.MIGRATE)
3637         else:
3638             self._record_action_start(context, instance,
3639                                       instance_actions.RESIZE)
3640 
3641         # TODO(melwitt): We're not rechecking for strict quota here to guard
3642         # against going over quota during a race at this time because the
3643         # resource consumption for this operation is written to the database
3644         # by compute.
3645         scheduler_hint = {'filter_properties': filter_properties}
3646 
3647         if host_name is None:
3648             # If 'host_name' is not specified,
3649             # clear the 'requested_destination' field of the RequestSpec.
3650             request_spec.requested_destination = None
3651         else:
3652             # Set the host and the node so that the scheduler will
3653             # validate them.
3654             request_spec.requested_destination = objects.Destination(
3655                 host=node.host, node=node.hypervisor_hostname)
3656 
3657         self.compute_task_api.resize_instance(context, instance,
3658             scheduler_hint=scheduler_hint,
3659             flavor=new_instance_type,
3660             clean_shutdown=clean_shutdown,
3661             request_spec=request_spec)
3662 
3663     @check_instance_lock
3664     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
3665                                     vm_states.PAUSED, vm_states.SUSPENDED])
3666     def shelve(self, context, instance, clean_shutdown=True):
3667         """Shelve an instance.
3668 
3669         Shuts down an instance and frees it up to be removed from the
3670         hypervisor.
3671         """
3672         instance.task_state = task_states.SHELVING
3673         instance.save(expected_task_state=[None])
3674 
3675         self._record_action_start(context, instance, instance_actions.SHELVE)
3676 
3677         if not compute_utils.is_volume_backed_instance(context, instance):
3678             name = '%s-shelved' % instance.display_name
3679             image_meta = compute_utils.create_image(
3680                 context, instance, name, 'snapshot', self.image_api)
3681             image_id = image_meta['id']
3682             self.compute_rpcapi.shelve_instance(context, instance=instance,
3683                     image_id=image_id, clean_shutdown=clean_shutdown)
3684         else:
3685             self.compute_rpcapi.shelve_offload_instance(context,
3686                     instance=instance, clean_shutdown=clean_shutdown)
3687 
3688     @check_instance_lock
3689     @check_instance_state(vm_state=[vm_states.SHELVED])
3690     def shelve_offload(self, context, instance, clean_shutdown=True):
3691         """Remove a shelved instance from the hypervisor."""
3692         instance.task_state = task_states.SHELVING_OFFLOADING
3693         instance.save(expected_task_state=[None])
3694 
3695         self._record_action_start(context, instance,
3696                                   instance_actions.SHELVE_OFFLOAD)
3697 
3698         self.compute_rpcapi.shelve_offload_instance(context, instance=instance,
3699             clean_shutdown=clean_shutdown)
3700 
3701     def _validate_unshelve_az(self, context, instance, availability_zone):
3702         """Verify the specified availability_zone during unshelve.
3703 
3704         Verifies that the server is shelved offloaded, the AZ exists and
3705         if [cinder]/cross_az_attach=False, that any attached volumes are in
3706         the same AZ.
3707 
3708         :param context: nova auth RequestContext for the unshelve action
3709         :param instance: Instance object for the server being unshelved
3710         :param availability_zone: The user-requested availability zone in
3711             which to unshelve the server.
3712         :raises: UnshelveInstanceInvalidState if the server is not shelved
3713             offloaded
3714         :raises: InvalidRequest if the requested AZ does not exist
3715         :raises: MismatchVolumeAZException if [cinder]/cross_az_attach=False
3716             and any attached volumes are not in the requested AZ
3717         """
3718         if instance.vm_state != vm_states.SHELVED_OFFLOADED:
3719             # NOTE(brinzhang): If the server status is 'SHELVED', it still
3720             # belongs to a host, the availability_zone has not changed.
3721             # Unshelving a shelved offloaded server will go through the
3722             # scheduler to find a new host.
3723             raise exception.UnshelveInstanceInvalidState(
3724                 state=instance.vm_state, instance_uuid=instance.uuid)
3725 
3726         available_zones = availability_zones.get_availability_zones(
3727             context, self.host_api, get_only_available=True)
3728         if availability_zone not in available_zones:
3729             msg = _('The requested availability zone is not available')
3730             raise exception.InvalidRequest(msg)
3731 
3732         # NOTE(brinzhang): When specifying a availability zone to unshelve
3733         # a shelved offloaded server, and conf cross_az_attach=False, need
3734         # to determine if attached volume AZ matches the user-specified AZ.
3735         if not CONF.cinder.cross_az_attach:
3736             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
3737                 context, instance.uuid)
3738             for bdm in bdms:
3739                 if bdm.is_volume and bdm.volume_id:
3740                     volume = self.volume_api.get(context, bdm.volume_id)
3741                     if availability_zone != volume['availability_zone']:
3742                         msg = _("The specified availability zone does not "
3743                                 "match the volume %(vol_id)s attached to the "
3744                                 "server. Specified availability zone is "
3745                                 "%(az)s. Volume is in %(vol_zone)s.") % {
3746                             "vol_id": volume['id'],
3747                             "az": availability_zone,
3748                             "vol_zone": volume['availability_zone']}
3749                         raise exception.MismatchVolumeAZException(reason=msg)
3750 
3751     @check_instance_lock
3752     @check_instance_state(vm_state=[vm_states.SHELVED,
3753         vm_states.SHELVED_OFFLOADED])
3754     def unshelve(self, context, instance, new_az=None):
3755         """Restore a shelved instance."""
3756         request_spec = objects.RequestSpec.get_by_instance_uuid(
3757             context, instance.uuid)
3758 
3759         if new_az:
3760             self._validate_unshelve_az(context, instance, new_az)
3761             LOG.debug("Replace the old AZ %(old_az)s in RequestSpec "
3762                       "with a new AZ %(new_az)s of the instance.",
3763                       {"old_az": request_spec.availability_zone,
3764                        "new_az": new_az}, instance=instance)
3765             # Unshelving a shelved offloaded server will go through the
3766             # scheduler to pick a new host, so we update the
3767             # RequestSpec.availability_zone here. Note that if scheduling
3768             # fails the RequestSpec will remain updated, which is not great,
3769             # but if we want to change that we need to defer updating the
3770             # RequestSpec until conductor which probably means RPC changes to
3771             # pass the new_az variable to conductor. This is likely low
3772             # priority since the RequestSpec.availability_zone on a shelved
3773             # offloaded server does not mean much anyway and clearly the user
3774             # is trying to put the server in the target AZ.
3775             request_spec.availability_zone = new_az
3776             request_spec.save()
3777 
3778         instance.task_state = task_states.UNSHELVING
3779         instance.save(expected_task_state=[None])
3780 
3781         self._record_action_start(context, instance, instance_actions.UNSHELVE)
3782 
3783         self.compute_task_api.unshelve_instance(context, instance,
3784                                                 request_spec)
3785 
3786     @check_instance_lock
3787     def add_fixed_ip(self, context, instance, network_id):
3788         """Add fixed_ip from specified network to given instance."""
3789         self.compute_rpcapi.add_fixed_ip_to_instance(context,
3790                 instance=instance, network_id=network_id)
3791 
3792     @check_instance_lock
3793     def remove_fixed_ip(self, context, instance, address):
3794         """Remove fixed_ip from specified network to given instance."""
3795         self.compute_rpcapi.remove_fixed_ip_from_instance(context,
3796                 instance=instance, address=address)
3797 
3798     @check_instance_lock
3799     @check_instance_state(vm_state=[vm_states.ACTIVE])
3800     def pause(self, context, instance):
3801         """Pause the given instance."""
3802         instance.task_state = task_states.PAUSING
3803         instance.save(expected_task_state=[None])
3804         self._record_action_start(context, instance, instance_actions.PAUSE)
3805         self.compute_rpcapi.pause_instance(context, instance)
3806 
3807     @check_instance_lock
3808     @check_instance_state(vm_state=[vm_states.PAUSED])
3809     def unpause(self, context, instance):
3810         """Unpause the given instance."""
3811         instance.task_state = task_states.UNPAUSING
3812         instance.save(expected_task_state=[None])
3813         self._record_action_start(context, instance, instance_actions.UNPAUSE)
3814         self.compute_rpcapi.unpause_instance(context, instance)
3815 
3816     @check_instance_host
3817     def get_diagnostics(self, context, instance):
3818         """Retrieve diagnostics for the given instance."""
3819         return self.compute_rpcapi.get_diagnostics(context, instance=instance)
3820 
3821     @check_instance_host
3822     def get_instance_diagnostics(self, context, instance):
3823         """Retrieve diagnostics for the given instance."""
3824         return self.compute_rpcapi.get_instance_diagnostics(context,
3825                                                             instance=instance)
3826 
3827     @reject_sev_instances(instance_actions.SUSPEND)
3828     @check_instance_lock
3829     @check_instance_state(vm_state=[vm_states.ACTIVE])
3830     def suspend(self, context, instance):
3831         """Suspend the given instance."""
3832         instance.task_state = task_states.SUSPENDING
3833         instance.save(expected_task_state=[None])
3834         self._record_action_start(context, instance, instance_actions.SUSPEND)
3835         self.compute_rpcapi.suspend_instance(context, instance)
3836 
3837     @check_instance_lock
3838     @check_instance_state(vm_state=[vm_states.SUSPENDED])
3839     def resume(self, context, instance):
3840         """Resume the given instance."""
3841         instance.task_state = task_states.RESUMING
3842         instance.save(expected_task_state=[None])
3843         self._record_action_start(context, instance, instance_actions.RESUME)
3844         self.compute_rpcapi.resume_instance(context, instance)
3845 
3846     @check_instance_lock
3847     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
3848                                     vm_states.ERROR])
3849     def rescue(self, context, instance, rescue_password=None,
3850                rescue_image_ref=None, clean_shutdown=True):
3851         """Rescue the given instance."""
3852 
3853         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
3854                     context, instance.uuid)
3855         for bdm in bdms:
3856             if bdm.volume_id:
3857                 vol = self.volume_api.get(context, bdm.volume_id)
3858                 self.volume_api.check_attached(context, vol)
3859         if compute_utils.is_volume_backed_instance(context, instance, bdms):
3860             reason = _("Cannot rescue a volume-backed instance")
3861             raise exception.InstanceNotRescuable(instance_id=instance.uuid,
3862                                                  reason=reason)
3863 
3864         instance.task_state = task_states.RESCUING
3865         instance.save(expected_task_state=[None])
3866 
3867         self._record_action_start(context, instance, instance_actions.RESCUE)
3868 
3869         self.compute_rpcapi.rescue_instance(context, instance=instance,
3870             rescue_password=rescue_password, rescue_image_ref=rescue_image_ref,
3871             clean_shutdown=clean_shutdown)
3872 
3873     @check_instance_lock
3874     @check_instance_state(vm_state=[vm_states.RESCUED])
3875     def unrescue(self, context, instance):
3876         """Unrescue the given instance."""
3877         instance.task_state = task_states.UNRESCUING
3878         instance.save(expected_task_state=[None])
3879 
3880         self._record_action_start(context, instance, instance_actions.UNRESCUE)
3881 
3882         self.compute_rpcapi.unrescue_instance(context, instance=instance)
3883 
3884     @check_instance_lock
3885     @check_instance_state(vm_state=[vm_states.ACTIVE])
3886     def set_admin_password(self, context, instance, password=None):
3887         """Set the root/admin password for the given instance.
3888 
3889         @param context: Nova auth context.
3890         @param instance: Nova instance object.
3891         @param password: The admin password for the instance.
3892         """
3893         instance.task_state = task_states.UPDATING_PASSWORD
3894         instance.save(expected_task_state=[None])
3895 
3896         self._record_action_start(context, instance,
3897                                   instance_actions.CHANGE_PASSWORD)
3898 
3899         self.compute_rpcapi.set_admin_password(context,
3900                                                instance=instance,
3901                                                new_pass=password)
3902 
3903     @check_instance_host
3904     @reject_instance_state(
3905         task_state=[task_states.DELETING, task_states.MIGRATING])
3906     def get_vnc_console(self, context, instance, console_type):
3907         """Get a url to an instance Console."""
3908         connect_info = self.compute_rpcapi.get_vnc_console(context,
3909                 instance=instance, console_type=console_type)
3910         return {'url': connect_info['access_url']}
3911 
3912     @check_instance_host
3913     @reject_instance_state(
3914         task_state=[task_states.DELETING, task_states.MIGRATING])
3915     def get_spice_console(self, context, instance, console_type):
3916         """Get a url to an instance Console."""
3917         connect_info = self.compute_rpcapi.get_spice_console(context,
3918                 instance=instance, console_type=console_type)
3919         return {'url': connect_info['access_url']}
3920 
3921     @check_instance_host
3922     @reject_instance_state(
3923         task_state=[task_states.DELETING, task_states.MIGRATING])
3924     def get_rdp_console(self, context, instance, console_type):
3925         """Get a url to an instance Console."""
3926         connect_info = self.compute_rpcapi.get_rdp_console(context,
3927                 instance=instance, console_type=console_type)
3928         return {'url': connect_info['access_url']}
3929 
3930     @check_instance_host
3931     @reject_instance_state(
3932         task_state=[task_states.DELETING, task_states.MIGRATING])
3933     def get_serial_console(self, context, instance, console_type):
3934         """Get a url to a serial console."""
3935         connect_info = self.compute_rpcapi.get_serial_console(context,
3936                 instance=instance, console_type=console_type)
3937         return {'url': connect_info['access_url']}
3938 
3939     @check_instance_host
3940     @reject_instance_state(
3941         task_state=[task_states.DELETING, task_states.MIGRATING])
3942     def get_mks_console(self, context, instance, console_type):
3943         """Get a url to a MKS console."""
3944         connect_info = self.compute_rpcapi.get_mks_console(context,
3945                 instance=instance, console_type=console_type)
3946         return {'url': connect_info['access_url']}
3947 
3948     @check_instance_host
3949     def get_console_output(self, context, instance, tail_length=None):
3950         """Get console output for an instance."""
3951         return self.compute_rpcapi.get_console_output(context,
3952                 instance=instance, tail_length=tail_length)
3953 
3954     def lock(self, context, instance, reason=None):
3955         """Lock the given instance."""
3956         # Only update the lock if we are an admin (non-owner)
3957         is_owner = instance.project_id == context.project_id
3958         if instance.locked and is_owner:
3959             return
3960 
3961         context = context.elevated()
3962         self._record_action_start(context, instance,
3963                                   instance_actions.LOCK)
3964 
3965         @wrap_instance_event(prefix='api')
3966         def lock(self, context, instance, reason=None):
3967             LOG.debug('Locking', instance=instance)
3968             instance.locked = True
3969             instance.locked_by = 'owner' if is_owner else 'admin'
3970             if reason:
3971                 instance.system_metadata['locked_reason'] = reason
3972             instance.save()
3973 
3974         lock(self, context, instance, reason=reason)
3975         compute_utils.notify_about_instance_action(
3976             context, instance, CONF.host,
3977             action=fields_obj.NotificationAction.LOCK,
3978             source=fields_obj.NotificationSource.API)
3979 
3980     def is_expected_locked_by(self, context, instance):
3981         is_owner = instance.project_id == context.project_id
3982         expect_locked_by = 'owner' if is_owner else 'admin'
3983         locked_by = instance.locked_by
3984         if locked_by and locked_by != expect_locked_by:
3985             return False
3986         return True
3987 
3988     def unlock(self, context, instance):
3989         """Unlock the given instance."""
3990         context = context.elevated()
3991         self._record_action_start(context, instance,
3992                                   instance_actions.UNLOCK)
3993 
3994         @wrap_instance_event(prefix='api')
3995         def unlock(self, context, instance):
3996             LOG.debug('Unlocking', instance=instance)
3997             instance.locked = False
3998             instance.locked_by = None
3999             instance.system_metadata.pop('locked_reason', None)
4000             instance.save()
4001 
4002         unlock(self, context, instance)
4003         compute_utils.notify_about_instance_action(
4004             context, instance, CONF.host,
4005             action=fields_obj.NotificationAction.UNLOCK,
4006             source=fields_obj.NotificationSource.API)
4007 
4008     @check_instance_lock
4009     def reset_network(self, context, instance):
4010         """Reset networking on the instance."""
4011         self.compute_rpcapi.reset_network(context, instance=instance)
4012 
4013     @check_instance_lock
4014     def inject_network_info(self, context, instance):
4015         """Inject network info for the instance."""
4016         self.compute_rpcapi.inject_network_info(context, instance=instance)
4017 
4018     def _create_volume_bdm(self, context, instance, device, volume,
4019                            disk_bus, device_type, is_local_creation=False,
4020                            tag=None, delete_on_termination=False):
4021         volume_id = volume['id']
4022         if is_local_creation:
4023             # when the creation is done locally we can't specify the device
4024             # name as we do not have a way to check that the name specified is
4025             # a valid one.
4026             # We leave the setting of that value when the actual attach
4027             # happens on the compute manager
4028             # NOTE(artom) Local attach (to a shelved-offload instance) cannot
4029             # support device tagging because we have no way to call the compute
4030             # manager to check that it supports device tagging. In fact, we
4031             # don't even know which computer manager the instance will
4032             # eventually end up on when it's unshelved.
4033             volume_bdm = objects.BlockDeviceMapping(
4034                 context=context,
4035                 source_type='volume', destination_type='volume',
4036                 instance_uuid=instance.uuid, boot_index=None,
4037                 volume_id=volume_id,
4038                 device_name=None, guest_format=None,
4039                 disk_bus=disk_bus, device_type=device_type,
4040                 delete_on_termination=delete_on_termination)
4041             volume_bdm.create()
4042         else:
4043             try:
4044                 # NOTE(vish): This is done on the compute host because we want
4045                 # to avoid a race where two devices are requested at the same
4046                 # time. When db access is removed from compute, the bdm will be
4047                 # created here and we will have to make sure that they are
4048                 # assigned atomically.
4049                 volume_bdm = self.compute_rpcapi.reserve_block_device_name(
4050                     context, instance, device, volume_id, disk_bus=disk_bus,
4051                     device_type=device_type, tag=tag,
4052                     multiattach=volume['multiattach'])
4053             # NOTE(lyarwood): Try to lookup and destroy any BDMs that were
4054             # created on the remote compute when exceptions are encountered.
4055             except Exception:
4056                 with excutils.save_and_reraise_exception():
4057                     try:
4058                         LOG.debug("reserve_block_device_name has failed, "
4059                                     "attempting to lookup and remove any "
4060                                     "BDMs created by the compute.")
4061                         bdm_obj = block_device_obj.BlockDeviceMapping
4062                         bdm = bdm_obj.get_by_volume_and_instance(context,
4063                             volume_id, instance.uuid)
4064                         LOG.debug("BDM %s found and will now be destroyed.",
4065                                   bdm.uuid)
4066                         bdm.destroy()
4067                     except exception.VolumeBDMNotFound:
4068                         LOG.debug("No BDM found while attempting to rollback.")
4069 
4070             volume_bdm.delete_on_termination = delete_on_termination
4071             volume_bdm.save()
4072         return volume_bdm
4073 
4074     def _check_volume_already_attached_to_instance(self, context, instance,
4075                                                    volume_id):
4076         """Avoid attaching the same volume to the same instance twice.
4077 
4078            As the new Cinder flow (microversion 3.44) is handling the checks
4079            differently and allows to attach the same volume to the same
4080            instance twice to enable live_migrate we are checking whether the
4081            BDM already exists for this combination for the new flow and fail
4082            if it does.
4083         """
4084 
4085         try:
4086             objects.BlockDeviceMapping.get_by_volume_and_instance(
4087                 context, volume_id, instance.uuid)
4088 
4089             msg = _("volume %s already attached") % volume_id
4090             raise exception.InvalidVolume(reason=msg)
4091         except exception.VolumeBDMNotFound:
4092             pass
4093 
4094     def _check_attach_and_reserve_volume(self, context, volume, instance,
4095                                          bdm, supports_multiattach=False):
4096         volume_id = volume['id']
4097         self.volume_api.check_availability_zone(context, volume,
4098                                                 instance=instance)
4099         # If volume.multiattach=True and the microversion to
4100         # support multiattach is not used, fail the request.
4101         if volume['multiattach'] and not supports_multiattach:
4102             raise exception.MultiattachNotSupportedOldMicroversion()
4103 
4104         attachment_id = self.volume_api.attachment_create(
4105             context, volume_id, instance.uuid)['id']
4106         bdm.attachment_id = attachment_id
4107         # NOTE(ildikov): In case of boot from volume the BDM at this
4108         # point is not yet created in a cell database, so we can't
4109         # call save().  When attaching a volume to an existing
4110         # instance, the instance is already in a cell and the BDM has
4111         # been created in that same cell so updating here in that case
4112         # is "ok".
4113         if bdm.obj_attr_is_set('id'):
4114             bdm.save()
4115 
4116     # TODO(stephenfin): Fold this back in now that cells v1 no longer needs to
4117     # override it.
4118     def _attach_volume(self, context, instance, volume, device,
4119                        disk_bus, device_type, tag=None,
4120                        supports_multiattach=False,
4121                        delete_on_termination=False):
4122         """Attach an existing volume to an existing instance.
4123 
4124         This method is separated to make it possible for cells version
4125         to override it.
4126         """
4127         volume_bdm = self._create_volume_bdm(
4128             context, instance, device, volume, disk_bus=disk_bus,
4129             device_type=device_type, tag=tag,
4130             delete_on_termination=delete_on_termination)
4131         try:
4132             self._check_attach_and_reserve_volume(context, volume, instance,
4133                                                   volume_bdm,
4134                                                   supports_multiattach)
4135             self._record_action_start(
4136                 context, instance, instance_actions.ATTACH_VOLUME)
4137             self.compute_rpcapi.attach_volume(context, instance, volume_bdm)
4138         except Exception:
4139             with excutils.save_and_reraise_exception():
4140                 volume_bdm.destroy()
4141 
4142         return volume_bdm.device_name
4143 
4144     def _attach_volume_shelved_offloaded(self, context, instance, volume,
4145                                          device, disk_bus, device_type,
4146                                          delete_on_termination):
4147         """Attach an existing volume to an instance in shelved offloaded state.
4148 
4149         Attaching a volume for an instance in shelved offloaded state requires
4150         to perform the regular check to see if we can attach and reserve the
4151         volume then we need to call the attach method on the volume API
4152         to mark the volume as 'in-use'.
4153         The instance at this stage is not managed by a compute manager
4154         therefore the actual attachment will be performed once the
4155         instance will be unshelved.
4156         """
4157         volume_id = volume['id']
4158 
4159         @wrap_instance_event(prefix='api')
4160         def attach_volume(self, context, v_id, instance, dev, attachment_id):
4161             if attachment_id:
4162                 # Normally we wouldn't complete an attachment without a host
4163                 # connector, but we do this to make the volume status change
4164                 # to "in-use" to maintain the API semantics with the old flow.
4165                 # When unshelving the instance, the compute service will deal
4166                 # with this disconnected attachment.
4167                 self.volume_api.attachment_complete(context, attachment_id)
4168             else:
4169                 self.volume_api.attach(context,
4170                                        v_id,
4171                                        instance.uuid,
4172                                        dev)
4173 
4174         volume_bdm = self._create_volume_bdm(
4175             context, instance, device, volume, disk_bus=disk_bus,
4176             device_type=device_type, is_local_creation=True,
4177             delete_on_termination=delete_on_termination)
4178         try:
4179             self._check_attach_and_reserve_volume(context, volume, instance,
4180                                                   volume_bdm)
4181             self._record_action_start(
4182                 context, instance,
4183                 instance_actions.ATTACH_VOLUME)
4184             attach_volume(self, context, volume_id, instance, device,
4185                           volume_bdm.attachment_id)
4186         except Exception:
4187             with excutils.save_and_reraise_exception():
4188                 volume_bdm.destroy()
4189 
4190         return volume_bdm.device_name
4191 
4192     @check_instance_lock
4193     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
4194                                     vm_states.STOPPED, vm_states.RESIZED,
4195                                     vm_states.SOFT_DELETED, vm_states.SHELVED,
4196                                     vm_states.SHELVED_OFFLOADED])
4197     def attach_volume(self, context, instance, volume_id, device=None,
4198                       disk_bus=None, device_type=None, tag=None,
4199                       supports_multiattach=False,
4200                       delete_on_termination=False):
4201         """Attach an existing volume to an existing instance."""
4202         # NOTE(vish): Fail fast if the device is not going to pass. This
4203         #             will need to be removed along with the test if we
4204         #             change the logic in the manager for what constitutes
4205         #             a valid device.
4206         if device and not block_device.match_device(device):
4207             raise exception.InvalidDevicePath(path=device)
4208 
4209         # Make sure the volume isn't already attached to this instance
4210         # because we'll use the v3.44 attachment flow in
4211         # _check_attach_and_reserve_volume and Cinder will allow multiple
4212         # attachments between the same volume and instance but the old flow
4213         # API semantics don't allow that so we enforce it here.
4214         self._check_volume_already_attached_to_instance(context,
4215                                                         instance,
4216                                                         volume_id)
4217 
4218         volume = self.volume_api.get(context, volume_id)
4219         is_shelved_offloaded = instance.vm_state == vm_states.SHELVED_OFFLOADED
4220         if is_shelved_offloaded:
4221             if tag:
4222                 # NOTE(artom) Local attach (to a shelved-offload instance)
4223                 # cannot support device tagging because we have no way to call
4224                 # the compute manager to check that it supports device tagging.
4225                 # In fact, we don't even know which computer manager the
4226                 # instance will eventually end up on when it's unshelved.
4227                 raise exception.VolumeTaggedAttachToShelvedNotSupported()
4228             if volume['multiattach']:
4229                 # NOTE(mriedem): Similar to tagged attach, we don't support
4230                 # attaching a multiattach volume to shelved offloaded instances
4231                 # because we can't tell if the compute host (since there isn't
4232                 # one) supports it. This could possibly be supported in the
4233                 # future if the scheduler was made aware of which computes
4234                 # support multiattach volumes.
4235                 raise exception.MultiattachToShelvedNotSupported()
4236             return self._attach_volume_shelved_offloaded(context,
4237                                                          instance,
4238                                                          volume,
4239                                                          device,
4240                                                          disk_bus,
4241                                                          device_type,
4242                                                          delete_on_termination)
4243 
4244         return self._attach_volume(context, instance, volume, device,
4245                                    disk_bus, device_type, tag=tag,
4246                                    supports_multiattach=supports_multiattach,
4247                                    delete_on_termination=delete_on_termination)
4248 
4249     # TODO(stephenfin): Fold this back in now that cells v1 no longer needs to
4250     # override it.
4251     def _detach_volume(self, context, instance, volume):
4252         """Detach volume from instance.
4253 
4254         This method is separated to make it easier for cells version
4255         to override.
4256         """
4257         try:
4258             self.volume_api.begin_detaching(context, volume['id'])
4259         except exception.InvalidInput as exc:
4260             raise exception.InvalidVolume(reason=exc.format_message())
4261         attachments = volume.get('attachments', {})
4262         attachment_id = None
4263         if attachments and instance.uuid in attachments:
4264             attachment_id = attachments[instance.uuid]['attachment_id']
4265         self._record_action_start(
4266             context, instance, instance_actions.DETACH_VOLUME)
4267         self.compute_rpcapi.detach_volume(context, instance=instance,
4268                 volume_id=volume['id'], attachment_id=attachment_id)
4269 
4270     def _detach_volume_shelved_offloaded(self, context, instance, volume):
4271         """Detach a volume from an instance in shelved offloaded state.
4272 
4273         If the instance is shelved offloaded we just need to cleanup volume
4274         calling the volume api detach, the volume api terminate_connection
4275         and delete the bdm record.
4276         If the volume has delete_on_termination option set then we call the
4277         volume api delete as well.
4278         """
4279         @wrap_instance_event(prefix='api')
4280         def detach_volume(self, context, instance, bdms):
4281             self._local_cleanup_bdm_volumes(bdms, instance, context)
4282 
4283         bdms = [objects.BlockDeviceMapping.get_by_volume_id(
4284                 context, volume['id'], instance.uuid)]
4285         # The begin_detaching() call only works with in-use volumes,
4286         # which will not be the case for volumes attached to a shelved
4287         # offloaded server via the attachments API since those volumes
4288         # will have `reserved` status.
4289         if not bdms[0].attachment_id:
4290             try:
4291                 self.volume_api.begin_detaching(context, volume['id'])
4292             except exception.InvalidInput as exc:
4293                 raise exception.InvalidVolume(reason=exc.format_message())
4294         self._record_action_start(
4295             context, instance,
4296             instance_actions.DETACH_VOLUME)
4297         detach_volume(self, context, instance, bdms)
4298 
4299     @check_instance_lock
4300     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
4301                                     vm_states.STOPPED, vm_states.RESIZED,
4302                                     vm_states.SOFT_DELETED, vm_states.SHELVED,
4303                                     vm_states.SHELVED_OFFLOADED])
4304     def detach_volume(self, context, instance, volume):
4305         """Detach a volume from an instance."""
4306         if instance.vm_state == vm_states.SHELVED_OFFLOADED:
4307             self._detach_volume_shelved_offloaded(context, instance, volume)
4308         else:
4309             self._detach_volume(context, instance, volume)
4310 
4311     def _count_attachments_for_swap(self, ctxt, volume):
4312         """Counts the number of attachments for a swap-related volume.
4313 
4314         Attempts to only count read/write attachments if the volume attachment
4315         records exist, otherwise simply just counts the number of attachments
4316         regardless of attach mode.
4317 
4318         :param ctxt: nova.context.RequestContext - user request context
4319         :param volume: nova-translated volume dict from nova.volume.cinder.
4320         :returns: count of attachments for the volume
4321         """
4322         # This is a dict, keyed by server ID, to a dict of attachment_id and
4323         # mountpoint.
4324         attachments = volume.get('attachments', {})
4325         # Multiattach volumes can have more than one attachment, so if there
4326         # is more than one attachment, attempt to count the read/write
4327         # attachments.
4328         if len(attachments) > 1:
4329             count = 0
4330             for attachment in attachments.values():
4331                 attachment_id = attachment['attachment_id']
4332                 # Get the attachment record for this attachment so we can
4333                 # get the attach_mode.
4334                 # TODO(mriedem): This could be optimized if we had
4335                 # GET /attachments/detail?volume_id=volume['id'] in Cinder.
4336                 try:
4337                     attachment_record = self.volume_api.attachment_get(
4338                         ctxt, attachment_id)
4339                     # Note that the attachment record from Cinder has
4340                     # attach_mode in the top-level of the resource but the
4341                     # nova.volume.cinder code translates it and puts the
4342                     # attach_mode in the connection_info for some legacy
4343                     # reason...
4344                     if attachment_record['attach_mode'] == 'rw':
4345                         count += 1
4346                 except exception.VolumeAttachmentNotFound:
4347                     # attachments are read/write by default so count it
4348                     count += 1
4349         else:
4350             count = len(attachments)
4351 
4352         return count
4353 
4354     @check_instance_lock
4355     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
4356                                     vm_states.RESIZED])
4357     def swap_volume(self, context, instance, old_volume, new_volume):
4358         """Swap volume attached to an instance."""
4359         # The caller likely got the instance from volume['attachments']
4360         # in the first place, but let's sanity check.
4361         if not old_volume.get('attachments', {}).get(instance.uuid):
4362             msg = _("Old volume is attached to a different instance.")
4363             raise exception.InvalidVolume(reason=msg)
4364         if new_volume['attach_status'] == 'attached':
4365             msg = _("New volume must be detached in order to swap.")
4366             raise exception.InvalidVolume(reason=msg)
4367         if int(new_volume['size']) < int(old_volume['size']):
4368             msg = _("New volume must be the same size or larger.")
4369             raise exception.InvalidVolume(reason=msg)
4370         self.volume_api.check_availability_zone(context, new_volume,
4371                                                 instance=instance)
4372         try:
4373             self.volume_api.begin_detaching(context, old_volume['id'])
4374         except exception.InvalidInput as exc:
4375             raise exception.InvalidVolume(reason=exc.format_message())
4376 
4377         # Disallow swapping from multiattach volumes that have more than one
4378         # read/write attachment. We know the old_volume has at least one
4379         # attachment since it's attached to this server. The new_volume
4380         # can't have any attachments because of the attach_status check above.
4381         # We do this count after calling "begin_detaching" to lock against
4382         # concurrent attachments being made while we're counting.
4383         try:
4384             if self._count_attachments_for_swap(context, old_volume) > 1:
4385                 raise exception.MultiattachSwapVolumeNotSupported()
4386         except Exception:  # This is generic to handle failures while counting
4387             # We need to reset the detaching status before raising.
4388             with excutils.save_and_reraise_exception():
4389                 self.volume_api.roll_detaching(context, old_volume['id'])
4390 
4391         # Get the BDM for the attached (old) volume so we can tell if it was
4392         # attached with the new-style Cinder 3.44 API.
4393         bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
4394             context, old_volume['id'], instance.uuid)
4395         new_attachment_id = None
4396         if bdm.attachment_id is None:
4397             # This is an old-style attachment so reserve the new volume before
4398             # we cast to the compute host.
4399             self.volume_api.reserve_volume(context, new_volume['id'])
4400         else:
4401             try:
4402                 self._check_volume_already_attached_to_instance(
4403                     context, instance, new_volume['id'])
4404             except exception.InvalidVolume:
4405                 with excutils.save_and_reraise_exception():
4406                     self.volume_api.roll_detaching(context, old_volume['id'])
4407 
4408             # This is a new-style attachment so for the volume that we are
4409             # going to swap to, create a new volume attachment.
4410             new_attachment_id = self.volume_api.attachment_create(
4411                 context, new_volume['id'], instance.uuid)['id']
4412 
4413         self._record_action_start(
4414             context, instance, instance_actions.SWAP_VOLUME)
4415 
4416         try:
4417             self.compute_rpcapi.swap_volume(
4418                     context, instance=instance,
4419                     old_volume_id=old_volume['id'],
4420                     new_volume_id=new_volume['id'],
4421                     new_attachment_id=new_attachment_id)
4422         except Exception:
4423             with excutils.save_and_reraise_exception():
4424                 self.volume_api.roll_detaching(context, old_volume['id'])
4425                 if new_attachment_id is None:
4426                     self.volume_api.unreserve_volume(context, new_volume['id'])
4427                 else:
4428                     self.volume_api.attachment_delete(
4429                         context, new_attachment_id)
4430 
4431     @check_instance_lock
4432     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
4433                                     vm_states.STOPPED],
4434                           task_state=[None])
4435     def attach_interface(self, context, instance, network_id, port_id,
4436                          requested_ip, tag=None):
4437         """Use hotplug to add an network adapter to an instance."""
4438         self._record_action_start(
4439             context, instance, instance_actions.ATTACH_INTERFACE)
4440 
4441         # NOTE(gibi): Checking if the requested port has resource request as
4442         # such ports are currently not supported as they would at least
4443         # need resource allocation manipulation in placement but might also
4444         # need a new scheduling if resource on this host is not available.
4445         if port_id:
4446             port = self.network_api.show_port(context, port_id)
4447             if port['port'].get(constants.RESOURCE_REQUEST):
4448                 raise exception.AttachInterfaceWithQoSPolicyNotSupported(
4449                     instance_uuid=instance.uuid)
4450 
4451         return self.compute_rpcapi.attach_interface(context,
4452             instance=instance, network_id=network_id, port_id=port_id,
4453             requested_ip=requested_ip, tag=tag)
4454 
4455     @check_instance_lock
4456     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
4457                                     vm_states.STOPPED],
4458                           task_state=[None])
4459     def detach_interface(self, context, instance, port_id):
4460         """Detach an network adapter from an instance."""
4461         self._record_action_start(
4462             context, instance, instance_actions.DETACH_INTERFACE)
4463         self.compute_rpcapi.detach_interface(context, instance=instance,
4464             port_id=port_id)
4465 
4466     def get_instance_metadata(self, context, instance):
4467         """Get all metadata associated with an instance."""
4468         return self.db.instance_metadata_get(context, instance.uuid)
4469 
4470     @check_instance_lock
4471     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
4472                                     vm_states.SUSPENDED, vm_states.STOPPED],
4473                           task_state=None)
4474     def delete_instance_metadata(self, context, instance, key):
4475         """Delete the given metadata item from an instance."""
4476         instance.delete_metadata_key(key)
4477         self.compute_rpcapi.change_instance_metadata(context,
4478                                                      instance=instance,
4479                                                      diff={key: ['-']})
4480 
4481     @check_instance_lock
4482     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
4483                                     vm_states.SUSPENDED, vm_states.STOPPED],
4484                           task_state=None)
4485     def update_instance_metadata(self, context, instance,
4486                                  metadata, delete=False):
4487         """Updates or creates instance metadata.
4488 
4489         If delete is True, metadata items that are not specified in the
4490         `metadata` argument will be deleted.
4491 
4492         """
4493         orig = dict(instance.metadata)
4494         if delete:
4495             _metadata = metadata
4496         else:
4497             _metadata = dict(instance.metadata)
4498             _metadata.update(metadata)
4499 
4500         self._check_metadata_properties_quota(context, _metadata)
4501         instance.metadata = _metadata
4502         instance.save()
4503         diff = _diff_dict(orig, instance.metadata)
4504         self.compute_rpcapi.change_instance_metadata(context,
4505                                                      instance=instance,
4506                                                      diff=diff)
4507         return _metadata
4508 
4509     @reject_sev_instances(instance_actions.LIVE_MIGRATION)
4510     @check_instance_lock
4511     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED])
4512     def live_migrate(self, context, instance, block_migration,
4513                      disk_over_commit, host_name, force=None, async_=False):
4514         """Migrate a server lively to a new host."""
4515         LOG.debug("Going to try to live migrate instance to %s",
4516                   host_name or "another host", instance=instance)
4517 
4518         if host_name:
4519             # Validate the specified host before changing the instance task
4520             # state.
4521             nodes = objects.ComputeNodeList.get_all_by_host(context, host_name)
4522 
4523         request_spec = objects.RequestSpec.get_by_instance_uuid(
4524             context, instance.uuid)
4525 
4526         instance.task_state = task_states.MIGRATING
4527         instance.save(expected_task_state=[None])
4528 
4529         self._record_action_start(context, instance,
4530                                   instance_actions.LIVE_MIGRATION)
4531 
4532         # NOTE(sbauza): Force is a boolean by the new related API version
4533         if force is False and host_name:
4534             # Unset the host to make sure we call the scheduler
4535             # from the conductor LiveMigrationTask. Yes this is tightly-coupled
4536             # to behavior in conductor and not great.
4537             host_name = None
4538             # FIXME(sbauza): Since only Ironic driver uses more than one
4539             # compute per service but doesn't support live migrations,
4540             # let's provide the first one.
4541             target = nodes[0]
4542             destination = objects.Destination(
4543                 host=target.host,
4544                 node=target.hypervisor_hostname
4545             )
4546             # This is essentially a hint to the scheduler to only consider
4547             # the specified host but still run it through the filters.
4548             request_spec.requested_destination = destination
4549 
4550         try:
4551             self.compute_task_api.live_migrate_instance(context, instance,
4552                 host_name, block_migration=block_migration,
4553                 disk_over_commit=disk_over_commit,
4554                 request_spec=request_spec, async_=async_)
4555         except oslo_exceptions.MessagingTimeout as messaging_timeout:
4556             with excutils.save_and_reraise_exception():
4557                 # NOTE(pkoniszewski): It is possible that MessagingTimeout
4558                 # occurs, but LM will still be in progress, so write
4559                 # instance fault to database
4560                 compute_utils.add_instance_fault_from_exc(context,
4561                                                           instance,
4562                                                           messaging_timeout)
4563 
4564     @check_instance_lock
4565     @check_instance_state(vm_state=[vm_states.ACTIVE],
4566                           task_state=[task_states.MIGRATING])
4567     def live_migrate_force_complete(self, context, instance, migration_id):
4568         """Force live migration to complete.
4569 
4570         :param context: Security context
4571         :param instance: The instance that is being migrated
4572         :param migration_id: ID of ongoing migration
4573 
4574         """
4575         LOG.debug("Going to try to force live migration to complete",
4576                   instance=instance)
4577 
4578         # NOTE(pkoniszewski): Get migration object to check if there is ongoing
4579         # live migration for particular instance. Also pass migration id to
4580         # compute to double check and avoid possible race condition.
4581         migration = objects.Migration.get_by_id_and_instance(
4582             context, migration_id, instance.uuid)
4583         if migration.status != 'running':
4584             raise exception.InvalidMigrationState(migration_id=migration_id,
4585                                                   instance_uuid=instance.uuid,
4586                                                   state=migration.status,
4587                                                   method='force complete')
4588 
4589         self._record_action_start(
4590             context, instance, instance_actions.LIVE_MIGRATION_FORCE_COMPLETE)
4591 
4592         self.compute_rpcapi.live_migration_force_complete(
4593             context, instance, migration)
4594 
4595     @check_instance_lock
4596     @check_instance_state(task_state=[task_states.MIGRATING])
4597     def live_migrate_abort(self, context, instance, migration_id,
4598                            support_abort_in_queue=False):
4599         """Abort an in-progress live migration.
4600 
4601         :param context: Security context
4602         :param instance: The instance that is being migrated
4603         :param migration_id: ID of in-progress live migration
4604         :param support_abort_in_queue: Flag indicating whether we can support
4605             abort migrations in "queued" or "preparing" status.
4606 
4607         """
4608         migration = objects.Migration.get_by_id_and_instance(context,
4609                     migration_id, instance.uuid)
4610         LOG.debug("Going to cancel live migration %s",
4611                   migration.id, instance=instance)
4612 
4613         # If the microversion does not support abort migration in queue,
4614         # we are only be able to abort migrations with `running` status;
4615         # if it is supported, we are able to also abort migrations in
4616         # `queued` and `preparing` status.
4617         allowed_states = ['running']
4618         queued_states = ['queued', 'preparing']
4619         if support_abort_in_queue:
4620             # The user requested a microversion that supports aborting a queued
4621             # or preparing live migration. But we need to check that the
4622             # compute service hosting the instance is new enough to support
4623             # aborting a queued/preparing live migration, so we check the
4624             # service version here.
4625             # TODO(Kevin_Zheng): This service version check can be removed in
4626             # Stein (at the earliest) when the API only supports Rocky or
4627             # newer computes.
4628             if migration.status in queued_states:
4629                 service = objects.Service.get_by_compute_host(
4630                     context, instance.host)
4631                 if service.version < MIN_COMPUTE_ABORT_QUEUED_LIVE_MIGRATION:
4632                     raise exception.AbortQueuedLiveMigrationNotYetSupported(
4633                         migration_id=migration_id, status=migration.status)
4634             allowed_states.extend(queued_states)
4635 
4636         if migration.status not in allowed_states:
4637             raise exception.InvalidMigrationState(migration_id=migration_id,
4638                     instance_uuid=instance.uuid,
4639                     state=migration.status,
4640                     method='abort live migration')
4641         self._record_action_start(context, instance,
4642                                   instance_actions.LIVE_MIGRATION_CANCEL)
4643 
4644         self.compute_rpcapi.live_migration_abort(context,
4645                 instance, migration.id)
4646 
4647     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
4648                                     vm_states.ERROR])
4649     def evacuate(self, context, instance, host, on_shared_storage,
4650                  admin_password=None, force=None):
4651         """Running evacuate to target host.
4652 
4653         Checking vm compute host state, if the host not in expected_state,
4654         raising an exception.
4655 
4656         :param instance: The instance to evacuate
4657         :param host: Target host. if not set, the scheduler will pick up one
4658         :param on_shared_storage: True if instance files on shared storage
4659         :param admin_password: password to set on rebuilt instance
4660         :param force: Force the evacuation to the specific host target
4661 
4662         """
4663         LOG.debug('vm evacuation scheduled', instance=instance)
4664         inst_host = instance.host
4665         service = objects.Service.get_by_compute_host(context, inst_host)
4666         if self.servicegroup_api.service_is_up(service):
4667             LOG.error('Instance compute service state on %s '
4668                       'expected to be down, but it was up.', inst_host)
4669             raise exception.ComputeServiceInUse(host=inst_host)
4670 
4671         request_spec = objects.RequestSpec.get_by_instance_uuid(
4672             context, instance.uuid)
4673 
4674         instance.task_state = task_states.REBUILDING
4675         instance.save(expected_task_state=[None])
4676         self._record_action_start(context, instance, instance_actions.EVACUATE)
4677 
4678         # NOTE(danms): Create this as a tombstone for the source compute
4679         # to find and cleanup. No need to pass it anywhere else.
4680         migration = objects.Migration(context,
4681                                       source_compute=instance.host,
4682                                       source_node=instance.node,
4683                                       instance_uuid=instance.uuid,
4684                                       status='accepted',
4685                                       migration_type='evacuation')
4686         if host:
4687             migration.dest_compute = host
4688         migration.create()
4689 
4690         compute_utils.notify_about_instance_usage(
4691             self.notifier, context, instance, "evacuate")
4692         compute_utils.notify_about_instance_action(
4693             context, instance, CONF.host,
4694             action=fields_obj.NotificationAction.EVACUATE,
4695             source=fields_obj.NotificationSource.API)
4696 
4697         # NOTE(sbauza): Force is a boolean by the new related API version
4698         # TODO(stephenfin): Any reason we can't use 'not force' here to handle
4699         # the pre-v2.29 API microversion, which wouldn't set force
4700         if force is False and host:
4701             nodes = objects.ComputeNodeList.get_all_by_host(context, host)
4702             # NOTE(sbauza): Unset the host to make sure we call the scheduler
4703             host = None
4704             # FIXME(sbauza): Since only Ironic driver uses more than one
4705             # compute per service but doesn't support evacuations,
4706             # let's provide the first one.
4707             target = nodes[0]
4708             destination = objects.Destination(
4709                 host=target.host,
4710                 node=target.hypervisor_hostname
4711             )
4712             request_spec.requested_destination = destination
4713 
4714         return self.compute_task_api.rebuild_instance(context,
4715                        instance=instance,
4716                        new_pass=admin_password,
4717                        injected_files=None,
4718                        image_ref=None,
4719                        orig_image_ref=None,
4720                        orig_sys_metadata=None,
4721                        bdms=None,
4722                        recreate=True,
4723                        on_shared_storage=on_shared_storage,
4724                        host=host,
4725                        request_spec=request_spec,
4726                        )
4727 
4728     def get_migrations(self, context, filters):
4729         """Get all migrations for the given filters."""
4730         load_cells()
4731 
4732         migrations = []
4733         for cell in CELLS:
4734             if cell.uuid == objects.CellMapping.CELL0_UUID:
4735                 continue
4736             with nova_context.target_cell(context, cell) as cctxt:
4737                 migrations.extend(objects.MigrationList.get_by_filters(
4738                     cctxt, filters).objects)
4739         return objects.MigrationList(objects=migrations)
4740 
4741     def get_migrations_sorted(self, context, filters, sort_dirs=None,
4742                               sort_keys=None, limit=None, marker=None):
4743         """Get all migrations for the given parameters."""
4744         mig_objs = migration_list.get_migration_objects_sorted(
4745             context, filters, limit, marker, sort_keys, sort_dirs)
4746         return mig_objs
4747 
4748     def get_migrations_in_progress_by_instance(self, context, instance_uuid,
4749                                                migration_type=None):
4750         """Get all migrations of an instance in progress."""
4751         return objects.MigrationList.get_in_progress_by_instance(
4752                 context, instance_uuid, migration_type)
4753 
4754     def get_migration_by_id_and_instance(self, context,
4755                                          migration_id, instance_uuid):
4756         """Get the migration of an instance by id."""
4757         return objects.Migration.get_by_id_and_instance(
4758                 context, migration_id, instance_uuid)
4759 
4760     def _get_bdm_by_volume_id(self, context, volume_id, expected_attrs=None):
4761         """Retrieve a BDM without knowing its cell.
4762 
4763         .. note:: The context will be targeted to the cell in which the
4764             BDM is found, if any.
4765 
4766         :param context: The API request context.
4767         :param volume_id: The ID of the volume.
4768         :param expected_attrs: list of any additional attributes that should
4769             be joined when the BDM is loaded from the database.
4770         :raises: nova.exception.VolumeBDMNotFound if not found in any cell
4771         """
4772         load_cells()
4773         for cell in CELLS:
4774             nova_context.set_target_cell(context, cell)
4775             try:
4776                 return objects.BlockDeviceMapping.get_by_volume(
4777                     context, volume_id, expected_attrs=expected_attrs)
4778             except exception.NotFound:
4779                 continue
4780         raise exception.VolumeBDMNotFound(volume_id=volume_id)
4781 
4782     def volume_snapshot_create(self, context, volume_id, create_info):
4783         bdm = self._get_bdm_by_volume_id(
4784             context, volume_id, expected_attrs=['instance'])
4785 
4786         # We allow creating the snapshot in any vm_state as long as there is
4787         # no task being performed on the instance and it has a host.
4788         @check_instance_host
4789         @check_instance_state(vm_state=None)
4790         def do_volume_snapshot_create(self, context, instance):
4791             self.compute_rpcapi.volume_snapshot_create(context, instance,
4792                     volume_id, create_info)
4793             snapshot = {
4794                 'snapshot': {
4795                     'id': create_info.get('id'),
4796                     'volumeId': volume_id
4797                 }
4798             }
4799             return snapshot
4800 
4801         return do_volume_snapshot_create(self, context, bdm.instance)
4802 
4803     def volume_snapshot_delete(self, context, volume_id, snapshot_id,
4804                                delete_info):
4805         bdm = self._get_bdm_by_volume_id(
4806             context, volume_id, expected_attrs=['instance'])
4807 
4808         # We allow deleting the snapshot in any vm_state as long as there is
4809         # no task being performed on the instance and it has a host.
4810         @check_instance_host
4811         @check_instance_state(vm_state=None)
4812         def do_volume_snapshot_delete(self, context, instance):
4813             self.compute_rpcapi.volume_snapshot_delete(context, instance,
4814                     volume_id, snapshot_id, delete_info)
4815 
4816         do_volume_snapshot_delete(self, context, bdm.instance)
4817 
4818     def external_instance_event(self, api_context, instances, events):
4819         # NOTE(danms): The external API consumer just provides events,
4820         # but doesn't know where they go. We need to collate lists
4821         # by the host the affected instance is on and dispatch them
4822         # according to host
4823         instances_by_host = collections.defaultdict(list)
4824         events_by_host = collections.defaultdict(list)
4825         hosts_by_instance = collections.defaultdict(list)
4826         cell_contexts_by_host = {}
4827         for instance in instances:
4828             # instance._context is used here since it's already targeted to
4829             # the cell that the instance lives in, and we need to use that
4830             # cell context to lookup any migrations associated to the instance.
4831             for host in self._get_relevant_hosts(instance._context, instance):
4832                 # NOTE(danms): All instances on a host must have the same
4833                 # mapping, so just use that
4834                 # NOTE(mdbooth): We don't currently support migrations between
4835                 # cells, and given that the Migration record is hosted in the
4836                 # cell _get_relevant_hosts will likely have to change before we
4837                 # do. Consequently we can currently assume that the context for
4838                 # both the source and destination hosts of a migration is the
4839                 # same.
4840                 if host not in cell_contexts_by_host:
4841                     cell_contexts_by_host[host] = instance._context
4842 
4843                 instances_by_host[host].append(instance)
4844                 hosts_by_instance[instance.uuid].append(host)
4845 
4846         for event in events:
4847             if event.name == 'volume-extended':
4848                 # Volume extend is a user-initiated operation starting in the
4849                 # Block Storage service API. We record an instance action so
4850                 # the user can monitor the operation to completion.
4851                 host = hosts_by_instance[event.instance_uuid][0]
4852                 cell_context = cell_contexts_by_host[host]
4853                 objects.InstanceAction.action_start(
4854                     cell_context, event.instance_uuid,
4855                     instance_actions.EXTEND_VOLUME, want_result=False)
4856             elif event.name == 'power-update':
4857                 host = hosts_by_instance[event.instance_uuid][0]
4858                 cell_context = cell_contexts_by_host[host]
4859                 if event.tag == external_event_obj.POWER_ON:
4860                     inst_action = instance_actions.START
4861                 elif event.tag == external_event_obj.POWER_OFF:
4862                     inst_action = instance_actions.STOP
4863                 else:
4864                     LOG.warning("Invalid power state %s. Cannot process "
4865                                 "the event %s. Skipping it.", event.tag,
4866                                 event)
4867                     continue
4868                 objects.InstanceAction.action_start(
4869                     cell_context, event.instance_uuid, inst_action,
4870                     want_result=False)
4871 
4872             for host in hosts_by_instance[event.instance_uuid]:
4873                 events_by_host[host].append(event)
4874 
4875         for host in instances_by_host:
4876             cell_context = cell_contexts_by_host[host]
4877 
4878             # TODO(salv-orlando): Handle exceptions raised by the rpc api layer
4879             # in order to ensure that a failure in processing events on a host
4880             # will not prevent processing events on other hosts
4881             self.compute_rpcapi.external_instance_event(
4882                 cell_context, instances_by_host[host], events_by_host[host],
4883                 host=host)
4884 
4885     def _get_relevant_hosts(self, context, instance):
4886         hosts = set()
4887         hosts.add(instance.host)
4888         if instance.migration_context is not None:
4889             migration_id = instance.migration_context.migration_id
4890             migration = objects.Migration.get_by_id(context, migration_id)
4891             hosts.add(migration.dest_compute)
4892             hosts.add(migration.source_compute)
4893             LOG.debug('Instance %(instance)s is migrating, '
4894                       'copying events to all relevant hosts: '
4895                       '%(hosts)s', {'instance': instance.uuid,
4896                                     'hosts': hosts})
4897         return hosts
4898 
4899     def get_instance_host_status(self, instance):
4900         if instance.host:
4901             try:
4902                 service = [service for service in instance.services if
4903                            service.binary == 'nova-compute'][0]
4904                 if service.forced_down:
4905                     host_status = fields_obj.HostStatus.DOWN
4906                 elif service.disabled:
4907                     host_status = fields_obj.HostStatus.MAINTENANCE
4908                 else:
4909                     alive = self.servicegroup_api.service_is_up(service)
4910                     host_status = ((alive and fields_obj.HostStatus.UP) or
4911                                    fields_obj.HostStatus.UNKNOWN)
4912             except IndexError:
4913                 host_status = fields_obj.HostStatus.NONE
4914         else:
4915             host_status = fields_obj.HostStatus.NONE
4916         return host_status
4917 
4918     def get_instances_host_statuses(self, instance_list):
4919         host_status_dict = dict()
4920         host_statuses = dict()
4921         for instance in instance_list:
4922             if instance.host:
4923                 if instance.host not in host_status_dict:
4924                     host_status = self.get_instance_host_status(instance)
4925                     host_status_dict[instance.host] = host_status
4926                 else:
4927                     host_status = host_status_dict[instance.host]
4928             else:
4929                 host_status = fields_obj.HostStatus.NONE
4930             host_statuses[instance.uuid] = host_status
4931         return host_statuses
4932 
4933 
4934 def target_host_cell(fn):
4935     """Target a host-based function to a cell.
4936 
4937     Expects to wrap a function of signature:
4938 
4939        func(self, context, host, ...)
4940     """
4941 
4942     @functools.wraps(fn)
4943     def targeted(self, context, host, *args, **kwargs):
4944         mapping = objects.HostMapping.get_by_host(context, host)
4945         nova_context.set_target_cell(context, mapping.cell_mapping)
4946         return fn(self, context, host, *args, **kwargs)
4947     return targeted
4948 
4949 
4950 def _find_service_in_cell(context, service_id=None, service_host=None):
4951     """Find a service by id or hostname by searching all cells.
4952 
4953     If one matching service is found, return it. If none or multiple
4954     are found, raise an exception.
4955 
4956     :param context: A context.RequestContext
4957     :param service_id: If not none, the DB ID of the service to find
4958     :param service_host: If not None, the hostname of the service to find
4959     :returns: An objects.Service
4960     :raises: ServiceNotUnique if multiple matching IDs are found
4961     :raises: NotFound if no matches are found
4962     :raises: NovaException if called with neither search option
4963     """
4964 
4965     load_cells()
4966     service = None
4967     found_in_cell = None
4968 
4969     is_uuid = False
4970     if service_id is not None:
4971         is_uuid = uuidutils.is_uuid_like(service_id)
4972         if is_uuid:
4973             lookup_fn = lambda c: objects.Service.get_by_uuid(c, service_id)
4974         else:
4975             lookup_fn = lambda c: objects.Service.get_by_id(c, service_id)
4976     elif service_host is not None:
4977         lookup_fn = lambda c: (
4978             objects.Service.get_by_compute_host(c, service_host))
4979     else:
4980         LOG.exception('_find_service_in_cell called with no search parameters')
4981         # This is intentionally cryptic so we don't leak implementation details
4982         # out of the API.
4983         raise exception.NovaException()
4984 
4985     for cell in CELLS:
4986         # NOTE(danms): Services can be in cell0, so don't skip it here
4987         try:
4988             with nova_context.target_cell(context, cell) as cctxt:
4989                 cell_service = lookup_fn(cctxt)
4990         except exception.NotFound:
4991             # NOTE(danms): Keep looking in other cells
4992             continue
4993         if service and cell_service:
4994             raise exception.ServiceNotUnique()
4995         service = cell_service
4996         found_in_cell = cell
4997         if service and is_uuid:
4998             break
4999 
5000     if service:
5001         # NOTE(danms): Set the cell on the context so it remains
5002         # when we return to our caller
5003         nova_context.set_target_cell(context, found_in_cell)
5004         return service
5005     else:
5006         raise exception.NotFound()
5007 
5008 
5009 class HostAPI(base.Base):
5010     """Sub-set of the Compute Manager API for managing host operations."""
5011 
5012     def __init__(self, rpcapi=None, servicegroup_api=None):
5013         self.rpcapi = rpcapi or compute_rpcapi.ComputeAPI()
5014         self.servicegroup_api = servicegroup_api or servicegroup.API()
5015         super(HostAPI, self).__init__()
5016 
5017     def _assert_host_exists(self, context, host_name, must_be_up=False):
5018         """Raise HostNotFound if compute host doesn't exist."""
5019         service = objects.Service.get_by_compute_host(context, host_name)
5020         if not service:
5021             raise exception.HostNotFound(host=host_name)
5022         if must_be_up and not self.servicegroup_api.service_is_up(service):
5023             raise exception.ComputeServiceUnavailable(host=host_name)
5024         return service['host']
5025 
5026     @wrap_exception()
5027     @target_host_cell
5028     def set_host_enabled(self, context, host_name, enabled):
5029         """Sets the specified host's ability to accept new instances."""
5030         host_name = self._assert_host_exists(context, host_name)
5031         payload = {'host_name': host_name, 'enabled': enabled}
5032         compute_utils.notify_about_host_update(context,
5033                                                'set_enabled.start',
5034                                                payload)
5035         result = self.rpcapi.set_host_enabled(context, enabled=enabled,
5036                 host=host_name)
5037         compute_utils.notify_about_host_update(context,
5038                                                'set_enabled.end',
5039                                                payload)
5040         return result
5041 
5042     @target_host_cell
5043     def get_host_uptime(self, context, host_name):
5044         """Returns the result of calling "uptime" on the target host."""
5045         host_name = self._assert_host_exists(context, host_name,
5046                          must_be_up=True)
5047         return self.rpcapi.get_host_uptime(context, host=host_name)
5048 
5049     @wrap_exception()
5050     @target_host_cell
5051     def host_power_action(self, context, host_name, action):
5052         """Reboots, shuts down or powers up the host."""
5053         host_name = self._assert_host_exists(context, host_name)
5054         payload = {'host_name': host_name, 'action': action}
5055         compute_utils.notify_about_host_update(context,
5056                                                'power_action.start',
5057                                                payload)
5058         result = self.rpcapi.host_power_action(context, action=action,
5059                 host=host_name)
5060         compute_utils.notify_about_host_update(context,
5061                                                'power_action.end',
5062                                                payload)
5063         return result
5064 
5065     @wrap_exception()
5066     @target_host_cell
5067     def set_host_maintenance(self, context, host_name, mode):
5068         """Start/Stop host maintenance window. On start, it triggers
5069         guest VMs evacuation.
5070         """
5071         host_name = self._assert_host_exists(context, host_name)
5072         payload = {'host_name': host_name, 'mode': mode}
5073         compute_utils.notify_about_host_update(context,
5074                                                'set_maintenance.start',
5075                                                payload)
5076         result = self.rpcapi.host_maintenance_mode(context,
5077                 host_param=host_name, mode=mode, host=host_name)
5078         compute_utils.notify_about_host_update(context,
5079                                                'set_maintenance.end',
5080                                                payload)
5081         return result
5082 
5083     def service_get_all(self, context, filters=None, set_zones=False,
5084                         all_cells=False, cell_down_support=False):
5085         """Returns a list of services, optionally filtering the results.
5086 
5087         If specified, 'filters' should be a dictionary containing services
5088         attributes and matching values.  Ie, to get a list of services for
5089         the 'compute' topic, use filters={'topic': 'compute'}.
5090 
5091         If all_cells=True, then scan all cells and merge the results.
5092 
5093         If cell_down_support=True then return minimal service records
5094         for cells that do not respond based on what we have in the
5095         host mappings. These will have only 'binary' and 'host' set.
5096         """
5097         if filters is None:
5098             filters = {}
5099         disabled = filters.pop('disabled', None)
5100         if 'availability_zone' in filters:
5101             set_zones = True
5102 
5103         # NOTE(danms): Eventually this all_cells nonsense should go away
5104         # and we should always iterate over the cells. However, certain
5105         # callers need the legacy behavior for now.
5106         if all_cells:
5107             services = []
5108             service_dict = nova_context.scatter_gather_all_cells(context,
5109                 objects.ServiceList.get_all, disabled, set_zones=set_zones)
5110             for cell_uuid, service in service_dict.items():
5111                 if not nova_context.is_cell_failure_sentinel(service):
5112                     services.extend(service)
5113                 elif cell_down_support:
5114                     unavailable_services = objects.ServiceList()
5115                     cid = [cm.id for cm in nova_context.CELLS
5116                            if cm.uuid == cell_uuid]
5117                     # We know cid[0] is in the list because we are using the
5118                     # same list that scatter_gather_all_cells used
5119                     hms = objects.HostMappingList.get_by_cell_id(context,
5120                                                                  cid[0])
5121                     for hm in hms:
5122                         unavailable_services.objects.append(objects.Service(
5123                             binary='nova-compute', host=hm.host))
5124                     LOG.warning("Cell %s is not responding and hence only "
5125                                 "partial results are available from this "
5126                                 "cell.", cell_uuid)
5127                     services.extend(unavailable_services)
5128                 else:
5129                     LOG.warning("Cell %s is not responding and hence skipped "
5130                                 "from the results.", cell_uuid)
5131         else:
5132             services = objects.ServiceList.get_all(context, disabled,
5133                                                    set_zones=set_zones)
5134         ret_services = []
5135         for service in services:
5136             for key, val in filters.items():
5137                 if service[key] != val:
5138                     break
5139             else:
5140                 # All filters matched.
5141                 ret_services.append(service)
5142         return ret_services
5143 
5144     def service_get_by_id(self, context, service_id):
5145         """Get service entry for the given service id or uuid."""
5146         try:
5147             return _find_service_in_cell(context, service_id=service_id)
5148         except exception.NotFound:
5149             raise exception.ServiceNotFound(service_id=service_id)
5150 
5151     @target_host_cell
5152     def service_get_by_compute_host(self, context, host_name):
5153         """Get service entry for the given compute hostname."""
5154         return objects.Service.get_by_compute_host(context, host_name)
5155 
5156     def _update_compute_provider_status(self, context, service):
5157         """Calls the compute service to sync the COMPUTE_STATUS_DISABLED trait.
5158 
5159         There are two cases where the API will not call the compute service:
5160 
5161         * The compute service is down. In this case the trait is synchronized
5162           when the compute service is restarted.
5163         * The compute service is old. In this case the trait is synchronized
5164           when the compute service is upgraded and restarted.
5165 
5166         :param context: nova auth RequestContext
5167         :param service: nova.objects.Service object which has been enabled
5168             or disabled (see ``service_update``).
5169         """
5170         # Make sure the service is up so we can make the RPC call.
5171         if not self.servicegroup_api.service_is_up(service):
5172             LOG.info('Compute service on host %s is down. The '
5173                      'COMPUTE_STATUS_DISABLED trait will be synchronized '
5174                      'when the service is restarted.', service.host)
5175             return
5176 
5177         # Make sure the compute service is new enough for the trait sync
5178         # behavior.
5179         # TODO(mriedem): Remove this compat check in the U release.
5180         if service.version < MIN_COMPUTE_SYNC_COMPUTE_STATUS_DISABLED:
5181             LOG.info('Compute service on host %s is too old to sync the '
5182                      'COMPUTE_STATUS_DISABLED trait in Placement. The '
5183                      'trait will be synchronized when the service is '
5184                      'upgraded and restarted.', service.host)
5185             return
5186 
5187         enabled = not service.disabled
5188         # Avoid leaking errors out of the API.
5189         try:
5190             LOG.debug('Calling the compute service on host %s to sync the '
5191                       'COMPUTE_STATUS_DISABLED trait.', service.host)
5192             self.rpcapi.set_host_enabled(context, service.host, enabled)
5193         except Exception:
5194             LOG.exception('An error occurred while updating the '
5195                           'COMPUTE_STATUS_DISABLED trait on compute node '
5196                           'resource providers managed by host %s. The trait '
5197                           'will be synchronized automatically by the compute '
5198                           'service when the update_available_resource '
5199                           'periodic task runs.', service.host)
5200 
5201     def service_update(self, context, service):
5202         """Performs the actual service update operation.
5203 
5204         If the "disabled" field is changed, potentially calls the compute
5205         service to sync the COMPUTE_STATUS_DISABLED trait on the compute node
5206         resource providers managed by this compute service.
5207 
5208         :param context: nova auth RequestContext
5209         :param service: nova.objects.Service object with changes already
5210             set on the object
5211         """
5212         # Before persisting changes and resetting the changed fields on the
5213         # Service object, determine if the disabled field changed.
5214         update_placement = 'disabled' in service.obj_what_changed()
5215         # Persist the Service object changes to the database.
5216         service.save()
5217         # If the disabled field changed, potentially call the compute service
5218         # to sync the COMPUTE_STATUS_DISABLED trait.
5219         if update_placement:
5220             self._update_compute_provider_status(context, service)
5221         return service
5222 
5223     @target_host_cell
5224     def service_update_by_host_and_binary(self, context, host_name, binary,
5225                                           params_to_update):
5226         """Enable / Disable a service.
5227 
5228         Determines the cell that the service is in using the HostMapping.
5229 
5230         For compute services, this stops new builds and migrations going to
5231         the host.
5232 
5233         See also ``service_update``.
5234 
5235         :param context: nova auth RequestContext
5236         :param host_name: hostname of the service
5237         :param binary: service binary (really only supports "nova-compute")
5238         :param params_to_update: dict of changes to make to the Service object
5239         :raises: HostMappingNotFound if the host is not mapped to a cell
5240         :raises: HostBinaryNotFound if a services table record is not found
5241             with the given host_name and binary
5242         """
5243         # TODO(mriedem): Service.get_by_args is deprecated; we should use
5244         # get_by_compute_host here (remember to update the "raises" docstring).
5245         service = objects.Service.get_by_args(context, host_name, binary)
5246         service.update(params_to_update)
5247         return self.service_update(context, service)
5248 
5249     def _service_delete(self, context, service_id):
5250         """Performs the actual Service deletion operation."""
5251         try:
5252             service = _find_service_in_cell(context, service_id=service_id)
5253         except exception.NotFound:
5254             raise exception.ServiceNotFound(service_id=service_id)
5255         service.destroy()
5256 
5257     # TODO(mriedem): Nothing outside of tests is using this now so we should
5258     # be able to remove it.
5259     def service_delete(self, context, service_id):
5260         """Deletes the specified service found via id or uuid."""
5261         self._service_delete(context, service_id)
5262 
5263     @target_host_cell
5264     def instance_get_all_by_host(self, context, host_name):
5265         """Return all instances on the given host."""
5266         return objects.InstanceList.get_by_host(context, host_name)
5267 
5268     def task_log_get_all(self, context, task_name, period_beginning,
5269                          period_ending, host=None, state=None):
5270         """Return the task logs within a given range, optionally
5271         filtering by host and/or state.
5272         """
5273         return self.db.task_log_get_all(context, task_name,
5274                                         period_beginning,
5275                                         period_ending,
5276                                         host=host,
5277                                         state=state)
5278 
5279     def compute_node_get(self, context, compute_id):
5280         """Return compute node entry for particular integer ID or UUID."""
5281         load_cells()
5282 
5283         # NOTE(danms): Unfortunately this API exposes database identifiers
5284         # which means we really can't do something efficient here
5285         is_uuid = uuidutils.is_uuid_like(compute_id)
5286         for cell in CELLS:
5287             if cell.uuid == objects.CellMapping.CELL0_UUID:
5288                 continue
5289             with nova_context.target_cell(context, cell) as cctxt:
5290                 try:
5291                     if is_uuid:
5292                         return objects.ComputeNode.get_by_uuid(cctxt,
5293                                                                compute_id)
5294                     return objects.ComputeNode.get_by_id(cctxt,
5295                                                          int(compute_id))
5296                 except exception.ComputeHostNotFound:
5297                     # NOTE(danms): Keep looking in other cells
5298                     continue
5299 
5300         raise exception.ComputeHostNotFound(host=compute_id)
5301 
5302     def compute_node_get_all(self, context, limit=None, marker=None):
5303         load_cells()
5304 
5305         computes = []
5306         uuid_marker = marker and uuidutils.is_uuid_like(marker)
5307         for cell in CELLS:
5308             if cell.uuid == objects.CellMapping.CELL0_UUID:
5309                 continue
5310             with nova_context.target_cell(context, cell) as cctxt:
5311 
5312                 # If we have a marker and it's a uuid, see if the compute node
5313                 # is in this cell.
5314                 if marker and uuid_marker:
5315                     try:
5316                         compute_marker = objects.ComputeNode.get_by_uuid(
5317                             cctxt, marker)
5318                         # we found the marker compute node, so use it's id
5319                         # for the actual marker for paging in this cell's db
5320                         marker = compute_marker.id
5321                     except exception.ComputeHostNotFound:
5322                         # The marker node isn't in this cell so keep looking.
5323                         continue
5324 
5325                 try:
5326                     cell_computes = objects.ComputeNodeList.get_by_pagination(
5327                         cctxt, limit=limit, marker=marker)
5328                 except exception.MarkerNotFound:
5329                     # NOTE(danms): Keep looking through cells
5330                     continue
5331                 computes.extend(cell_computes)
5332                 # NOTE(danms): We must have found the marker, so continue on
5333                 # without one
5334                 marker = None
5335                 if limit:
5336                     limit -= len(cell_computes)
5337                     if limit <= 0:
5338                         break
5339 
5340         if marker is not None and len(computes) == 0:
5341             # NOTE(danms): If we did not find the marker in any cell,
5342             # mimic the db_api behavior here.
5343             raise exception.MarkerNotFound(marker=marker)
5344 
5345         return objects.ComputeNodeList(objects=computes)
5346 
5347     def compute_node_search_by_hypervisor(self, context, hypervisor_match):
5348         load_cells()
5349 
5350         computes = []
5351         for cell in CELLS:
5352             if cell.uuid == objects.CellMapping.CELL0_UUID:
5353                 continue
5354             with nova_context.target_cell(context, cell) as cctxt:
5355                 cell_computes = objects.ComputeNodeList.get_by_hypervisor(
5356                     cctxt, hypervisor_match)
5357             computes.extend(cell_computes)
5358         return objects.ComputeNodeList(objects=computes)
5359 
5360     def compute_node_statistics(self, context):
5361         load_cells()
5362 
5363         cell_stats = []
5364         for cell in CELLS:
5365             if cell.uuid == objects.CellMapping.CELL0_UUID:
5366                 continue
5367             with nova_context.target_cell(context, cell) as cctxt:
5368                 cell_stats.append(self.db.compute_node_statistics(cctxt))
5369 
5370         if cell_stats:
5371             keys = cell_stats[0].keys()
5372             return {k: sum(stats[k] for stats in cell_stats)
5373                     for k in keys}
5374         else:
5375             return {}
5376 
5377 
5378 class InstanceActionAPI(base.Base):
5379     """Sub-set of the Compute Manager API for managing instance actions."""
5380 
5381     def actions_get(self, context, instance, limit=None, marker=None,
5382                     filters=None):
5383         return objects.InstanceActionList.get_by_instance_uuid(
5384             context, instance.uuid, limit, marker, filters)
5385 
5386     def action_get_by_request_id(self, context, instance, request_id):
5387         return objects.InstanceAction.get_by_request_id(
5388             context, instance.uuid, request_id)
5389 
5390     def action_events_get(self, context, instance, action_id):
5391         return objects.InstanceActionEventList.get_by_action(
5392             context, action_id)
5393 
5394 
5395 class AggregateAPI(base.Base):
5396     """Sub-set of the Compute Manager API for managing host aggregates."""
5397     def __init__(self, **kwargs):
5398         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
5399         self.query_client = query.SchedulerQueryClient()
5400         self._placement_client = None  # Lazy-load on first access.
5401         super(AggregateAPI, self).__init__(**kwargs)
5402 
5403     @property
5404     def placement_client(self):
5405         if self._placement_client is None:
5406             self._placement_client = report.SchedulerReportClient()
5407         return self._placement_client
5408 
5409     @wrap_exception()
5410     def create_aggregate(self, context, aggregate_name, availability_zone):
5411         """Creates the model for the aggregate."""
5412 
5413         aggregate = objects.Aggregate(context=context)
5414         aggregate.name = aggregate_name
5415         if availability_zone:
5416             aggregate.metadata = {'availability_zone': availability_zone}
5417         aggregate.create()
5418         self.query_client.update_aggregates(context, [aggregate])
5419         return aggregate
5420 
5421     def get_aggregate(self, context, aggregate_id):
5422         """Get an aggregate by id."""
5423         return objects.Aggregate.get_by_id(context, aggregate_id)
5424 
5425     def get_aggregate_list(self, context):
5426         """Get all the aggregates."""
5427         return objects.AggregateList.get_all(context)
5428 
5429     def get_aggregates_by_host(self, context, compute_host):
5430         """Get all the aggregates where the given host is presented."""
5431         return objects.AggregateList.get_by_host(context, compute_host)
5432 
5433     @wrap_exception()
5434     def update_aggregate(self, context, aggregate_id, values):
5435         """Update the properties of an aggregate."""
5436         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
5437         if 'name' in values:
5438             aggregate.name = values.pop('name')
5439             aggregate.save()
5440         self.is_safe_to_update_az(context, values, aggregate=aggregate,
5441                                   action_name=AGGREGATE_ACTION_UPDATE,
5442                                   check_no_instances_in_az=True)
5443         if values:
5444             aggregate.update_metadata(values)
5445             aggregate.updated_at = timeutils.utcnow()
5446         self.query_client.update_aggregates(context, [aggregate])
5447         # If updated values include availability_zones, then the cache
5448         # which stored availability_zones and host need to be reset
5449         if values.get('availability_zone'):
5450             availability_zones.reset_cache()
5451         return aggregate
5452 
5453     @wrap_exception()
5454     def update_aggregate_metadata(self, context, aggregate_id, metadata):
5455         """Updates the aggregate metadata."""
5456         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
5457         self.is_safe_to_update_az(context, metadata, aggregate=aggregate,
5458                                   action_name=AGGREGATE_ACTION_UPDATE_META,
5459                                   check_no_instances_in_az=True)
5460         aggregate.update_metadata(metadata)
5461         self.query_client.update_aggregates(context, [aggregate])
5462         # If updated metadata include availability_zones, then the cache
5463         # which stored availability_zones and host need to be reset
5464         if metadata and metadata.get('availability_zone'):
5465             availability_zones.reset_cache()
5466         aggregate.updated_at = timeutils.utcnow()
5467         return aggregate
5468 
5469     @wrap_exception()
5470     def delete_aggregate(self, context, aggregate_id):
5471         """Deletes the aggregate."""
5472         aggregate_payload = {'aggregate_id': aggregate_id}
5473         compute_utils.notify_about_aggregate_update(context,
5474                                                     "delete.start",
5475                                                     aggregate_payload)
5476         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
5477 
5478         compute_utils.notify_about_aggregate_action(
5479             context=context,
5480             aggregate=aggregate,
5481             action=fields_obj.NotificationAction.DELETE,
5482             phase=fields_obj.NotificationPhase.START)
5483 
5484         if len(aggregate.hosts) > 0:
5485             msg = _("Host aggregate is not empty")
5486             raise exception.InvalidAggregateActionDelete(
5487                 aggregate_id=aggregate_id, reason=msg)
5488         aggregate.destroy()
5489         self.query_client.delete_aggregate(context, aggregate)
5490         compute_utils.notify_about_aggregate_update(context,
5491                                                     "delete.end",
5492                                                     aggregate_payload)
5493         compute_utils.notify_about_aggregate_action(
5494             context=context,
5495             aggregate=aggregate,
5496             action=fields_obj.NotificationAction.DELETE,
5497             phase=fields_obj.NotificationPhase.END)
5498 
5499     def is_safe_to_update_az(self, context, metadata, aggregate,
5500                              hosts=None,
5501                              action_name=AGGREGATE_ACTION_ADD,
5502                              check_no_instances_in_az=False):
5503         """Determine if updates alter an aggregate's availability zone.
5504 
5505             :param context: local context
5506             :param metadata: Target metadata for updating aggregate
5507             :param aggregate: Aggregate to update
5508             :param hosts: Hosts to check. If None, aggregate.hosts is used
5509             :type hosts: list
5510             :param action_name: Calling method for logging purposes
5511             :param check_no_instances_in_az: if True, it checks
5512                 there is no instances on any hosts of the aggregate
5513 
5514         """
5515         if 'availability_zone' in metadata:
5516             if not metadata['availability_zone']:
5517                 msg = _("Aggregate %s does not support empty named "
5518                         "availability zone") % aggregate.name
5519                 self._raise_invalid_aggregate_exc(action_name, aggregate.id,
5520                                                   msg)
5521             _hosts = hosts or aggregate.hosts
5522             host_aggregates = objects.AggregateList.get_by_metadata_key(
5523                 context, 'availability_zone', hosts=_hosts)
5524             conflicting_azs = [
5525                 agg.availability_zone for agg in host_aggregates
5526                 if agg.availability_zone != metadata['availability_zone'] and
5527                 agg.id != aggregate.id]
5528             if conflicting_azs:
5529                 msg = _("One or more hosts already in availability zone(s) "
5530                         "%s") % conflicting_azs
5531                 self._raise_invalid_aggregate_exc(action_name, aggregate.id,
5532                                                   msg)
5533             same_az_name = (aggregate.availability_zone ==
5534                             metadata['availability_zone'])
5535             if check_no_instances_in_az and not same_az_name:
5536                 instance_count_by_cell = (
5537                     nova_context.scatter_gather_skip_cell0(
5538                         context,
5539                         objects.InstanceList.get_count_by_hosts,
5540                         _hosts))
5541                 if any(cnt for cnt in instance_count_by_cell.values()):
5542                     msg = _("One or more hosts contain instances in this zone")
5543                     self._raise_invalid_aggregate_exc(
5544                         action_name, aggregate.id, msg)
5545 
5546     def _raise_invalid_aggregate_exc(self, action_name, aggregate_id, reason):
5547         if action_name == AGGREGATE_ACTION_ADD:
5548             raise exception.InvalidAggregateActionAdd(
5549                 aggregate_id=aggregate_id, reason=reason)
5550         elif action_name == AGGREGATE_ACTION_UPDATE:
5551             raise exception.InvalidAggregateActionUpdate(
5552                 aggregate_id=aggregate_id, reason=reason)
5553         elif action_name == AGGREGATE_ACTION_UPDATE_META:
5554             raise exception.InvalidAggregateActionUpdateMeta(
5555                 aggregate_id=aggregate_id, reason=reason)
5556         elif action_name == AGGREGATE_ACTION_DELETE:
5557             raise exception.InvalidAggregateActionDelete(
5558                 aggregate_id=aggregate_id, reason=reason)
5559 
5560         raise exception.NovaException(
5561             _("Unexpected aggregate action %s") % action_name)
5562 
5563     def _update_az_cache_for_host(self, context, host_name, aggregate_meta):
5564         # Update the availability_zone cache to avoid getting wrong
5565         # availability_zone in cache retention time when add/remove
5566         # host to/from aggregate.
5567         if aggregate_meta and aggregate_meta.get('availability_zone'):
5568             availability_zones.update_host_availability_zone_cache(context,
5569                                                                    host_name)
5570 
5571     @wrap_exception()
5572     def add_host_to_aggregate(self, context, aggregate_id, host_name):
5573         """Adds the host to an aggregate."""
5574         aggregate_payload = {'aggregate_id': aggregate_id,
5575                              'host_name': host_name}
5576         compute_utils.notify_about_aggregate_update(context,
5577                                                     "addhost.start",
5578                                                     aggregate_payload)
5579         # validates the host; HostMappingNotFound or ComputeHostNotFound
5580         # is raised if invalid
5581         try:
5582             mapping = objects.HostMapping.get_by_host(context, host_name)
5583             nova_context.set_target_cell(context, mapping.cell_mapping)
5584             service = objects.Service.get_by_compute_host(context, host_name)
5585         except exception.HostMappingNotFound:
5586             try:
5587                 # NOTE(danms): This targets our cell
5588                 service = _find_service_in_cell(context,
5589                                                 service_host=host_name)
5590             except exception.NotFound:
5591                 raise exception.ComputeHostNotFound(host=host_name)
5592 
5593         if service.host != host_name:
5594             # NOTE(danms): If we found a service but it is not an
5595             # exact match, we may have a case-insensitive backend
5596             # database (like mysql) which will end up with us
5597             # adding the host-aggregate mapping with a
5598             # non-matching hostname.
5599             raise exception.ComputeHostNotFound(host=host_name)
5600 
5601         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
5602 
5603         compute_utils.notify_about_aggregate_action(
5604             context=context,
5605             aggregate=aggregate,
5606             action=fields_obj.NotificationAction.ADD_HOST,
5607             phase=fields_obj.NotificationPhase.START)
5608 
5609         self.is_safe_to_update_az(context, aggregate.metadata,
5610                                   hosts=[host_name], aggregate=aggregate)
5611 
5612         aggregate.add_host(host_name)
5613         self.query_client.update_aggregates(context, [aggregate])
5614         try:
5615             self.placement_client.aggregate_add_host(
5616                 context, aggregate.uuid, host_name=host_name)
5617         except exception.PlacementAPIConnectFailure:
5618             # NOTE(jaypipes): Rocky should be able to tolerate the nova-api
5619             # service not communicating with the Placement API, so just log a
5620             # warning here.
5621             # TODO(jaypipes): Remove this in Stein, when placement must be able
5622             # to be contacted from the nova-api service.
5623             LOG.warning("Failed to associate %s with a placement "
5624                         "aggregate: %s. There was a failure to communicate "
5625                         "with the placement service.",
5626                         host_name, aggregate.uuid)
5627         except (exception.ResourceProviderNotFound,
5628                 exception.ResourceProviderAggregateRetrievalFailed,
5629                 exception.ResourceProviderUpdateFailed,
5630                 exception.ResourceProviderUpdateConflict) as err:
5631             # NOTE(jaypipes): We don't want a failure perform the mirroring
5632             # action in the placement service to be returned to the user (they
5633             # probably don't know anything about the placement service and
5634             # would just be confused). So, we just log a warning here, noting
5635             # that on the next run of nova-manage placement sync_aggregates
5636             # things will go back to normal
5637             LOG.warning("Failed to associate %s with a placement "
5638                         "aggregate: %s. This may be corrected after running "
5639                         "nova-manage placement sync_aggregates.",
5640                         host_name, err)
5641         self._update_az_cache_for_host(context, host_name, aggregate.metadata)
5642         # NOTE(jogo): Send message to host to support resource pools
5643         self.compute_rpcapi.add_aggregate_host(context,
5644                 aggregate=aggregate, host_param=host_name, host=host_name)
5645         aggregate_payload.update({'name': aggregate.name})
5646         compute_utils.notify_about_aggregate_update(context,
5647                                                     "addhost.end",
5648                                                     aggregate_payload)
5649         compute_utils.notify_about_aggregate_action(
5650             context=context,
5651             aggregate=aggregate,
5652             action=fields_obj.NotificationAction.ADD_HOST,
5653             phase=fields_obj.NotificationPhase.END)
5654 
5655         return aggregate
5656 
5657     @wrap_exception()
5658     def remove_host_from_aggregate(self, context, aggregate_id, host_name):
5659         """Removes host from the aggregate."""
5660         aggregate_payload = {'aggregate_id': aggregate_id,
5661                              'host_name': host_name}
5662         compute_utils.notify_about_aggregate_update(context,
5663                                                     "removehost.start",
5664                                                     aggregate_payload)
5665         # validates the host; HostMappingNotFound or ComputeHostNotFound
5666         # is raised if invalid
5667         mapping = objects.HostMapping.get_by_host(context, host_name)
5668         nova_context.set_target_cell(context, mapping.cell_mapping)
5669         objects.Service.get_by_compute_host(context, host_name)
5670         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
5671 
5672         compute_utils.notify_about_aggregate_action(
5673             context=context,
5674             aggregate=aggregate,
5675             action=fields_obj.NotificationAction.REMOVE_HOST,
5676             phase=fields_obj.NotificationPhase.START)
5677 
5678         aggregate.delete_host(host_name)
5679         self.query_client.update_aggregates(context, [aggregate])
5680         try:
5681             self.placement_client.aggregate_remove_host(
5682                 context, aggregate.uuid, host_name)
5683         except exception.PlacementAPIConnectFailure:
5684             # NOTE(jaypipes): Rocky should be able to tolerate the nova-api
5685             # service not communicating with the Placement API, so just log a
5686             # warning here.
5687             # TODO(jaypipes): Remove this in Stein, when placement must be able
5688             # to be contacted from the nova-api service.
5689             LOG.warning("Failed to remove association of %s with a placement "
5690                         "aggregate: %s. There was a failure to communicate "
5691                         "with the placement service.",
5692                         host_name, aggregate.uuid)
5693         except (exception.ResourceProviderNotFound,
5694                 exception.ResourceProviderAggregateRetrievalFailed,
5695                 exception.ResourceProviderUpdateFailed,
5696                 exception.ResourceProviderUpdateConflict) as err:
5697             # NOTE(jaypipes): We don't want a failure perform the mirroring
5698             # action in the placement service to be returned to the user (they
5699             # probably don't know anything about the placement service and
5700             # would just be confused). So, we just log a warning here, noting
5701             # that on the next run of nova-manage placement sync_aggregates
5702             # things will go back to normal
5703             LOG.warning("Failed to remove association of %s with a placement "
5704                         "aggregate: %s. This may be corrected after running "
5705                         "nova-manage placement sync_aggregates.",
5706                         host_name, err)
5707         self._update_az_cache_for_host(context, host_name, aggregate.metadata)
5708         self.compute_rpcapi.remove_aggregate_host(context,
5709                 aggregate=aggregate, host_param=host_name, host=host_name)
5710         compute_utils.notify_about_aggregate_update(context,
5711                                                     "removehost.end",
5712                                                     aggregate_payload)
5713         compute_utils.notify_about_aggregate_action(
5714             context=context,
5715             aggregate=aggregate,
5716             action=fields_obj.NotificationAction.REMOVE_HOST,
5717             phase=fields_obj.NotificationPhase.END)
5718         return aggregate
5719 
5720 
5721 class KeypairAPI(base.Base):
5722     """Subset of the Compute Manager API for managing key pairs."""
5723 
5724     get_notifier = functools.partial(rpc.get_notifier, service='api')
5725     wrap_exception = functools.partial(exception_wrapper.wrap_exception,
5726                                        get_notifier=get_notifier,
5727                                        binary='nova-api')
5728 
5729     def _notify(self, context, event_suffix, keypair_name):
5730         payload = {
5731             'tenant_id': context.project_id,
5732             'user_id': context.user_id,
5733             'key_name': keypair_name,
5734         }
5735         notify = self.get_notifier()
5736         notify.info(context, 'keypair.%s' % event_suffix, payload)
5737 
5738     def _validate_new_key_pair(self, context, user_id, key_name, key_type):
5739         safe_chars = "_- " + string.digits + string.ascii_letters
5740         clean_value = "".join(x for x in key_name if x in safe_chars)
5741         if clean_value != key_name:
5742             raise exception.InvalidKeypair(
5743                 reason=_("Keypair name contains unsafe characters"))
5744 
5745         try:
5746             utils.check_string_length(key_name, min_length=1, max_length=255)
5747         except exception.InvalidInput:
5748             raise exception.InvalidKeypair(
5749                 reason=_('Keypair name must be string and between '
5750                          '1 and 255 characters long'))
5751         try:
5752             objects.Quotas.check_deltas(context, {'key_pairs': 1}, user_id)
5753         except exception.OverQuota:
5754             raise exception.KeypairLimitExceeded()
5755 
5756     @wrap_exception()
5757     def import_key_pair(self, context, user_id, key_name, public_key,
5758                         key_type=keypair_obj.KEYPAIR_TYPE_SSH):
5759         """Import a key pair using an existing public key."""
5760         self._validate_new_key_pair(context, user_id, key_name, key_type)
5761 
5762         self._notify(context, 'import.start', key_name)
5763 
5764         keypair = objects.KeyPair(context)
5765         keypair.user_id = user_id
5766         keypair.name = key_name
5767         keypair.type = key_type
5768         keypair.fingerprint = None
5769         keypair.public_key = public_key
5770 
5771         compute_utils.notify_about_keypair_action(
5772             context=context,
5773             keypair=keypair,
5774             action=fields_obj.NotificationAction.IMPORT,
5775             phase=fields_obj.NotificationPhase.START)
5776 
5777         fingerprint = self._generate_fingerprint(public_key, key_type)
5778 
5779         keypair.fingerprint = fingerprint
5780         keypair.create()
5781 
5782         compute_utils.notify_about_keypair_action(
5783             context=context,
5784             keypair=keypair,
5785             action=fields_obj.NotificationAction.IMPORT,
5786             phase=fields_obj.NotificationPhase.END)
5787         self._notify(context, 'import.end', key_name)
5788 
5789         return keypair
5790 
5791     @wrap_exception()
5792     def create_key_pair(self, context, user_id, key_name,
5793                         key_type=keypair_obj.KEYPAIR_TYPE_SSH):
5794         """Create a new key pair."""
5795         self._validate_new_key_pair(context, user_id, key_name, key_type)
5796 
5797         keypair = objects.KeyPair(context)
5798         keypair.user_id = user_id
5799         keypair.name = key_name
5800         keypair.type = key_type
5801         keypair.fingerprint = None
5802         keypair.public_key = None
5803 
5804         self._notify(context, 'create.start', key_name)
5805         compute_utils.notify_about_keypair_action(
5806             context=context,
5807             keypair=keypair,
5808             action=fields_obj.NotificationAction.CREATE,
5809             phase=fields_obj.NotificationPhase.START)
5810 
5811         private_key, public_key, fingerprint = self._generate_key_pair(
5812             user_id, key_type)
5813 
5814         keypair.fingerprint = fingerprint
5815         keypair.public_key = public_key
5816         keypair.create()
5817 
5818         # NOTE(melwitt): We recheck the quota after creating the object to
5819         # prevent users from allocating more resources than their allowed quota
5820         # in the event of a race. This is configurable because it can be
5821         # expensive if strict quota limits are not required in a deployment.
5822         if CONF.quota.recheck_quota:
5823             try:
5824                 objects.Quotas.check_deltas(context, {'key_pairs': 0}, user_id)
5825             except exception.OverQuota:
5826                 keypair.destroy()
5827                 raise exception.KeypairLimitExceeded()
5828 
5829         compute_utils.notify_about_keypair_action(
5830             context=context,
5831             keypair=keypair,
5832             action=fields_obj.NotificationAction.CREATE,
5833             phase=fields_obj.NotificationPhase.END)
5834 
5835         self._notify(context, 'create.end', key_name)
5836 
5837         return keypair, private_key
5838 
5839     def _generate_fingerprint(self, public_key, key_type):
5840         if key_type == keypair_obj.KEYPAIR_TYPE_SSH:
5841             return crypto.generate_fingerprint(public_key)
5842         elif key_type == keypair_obj.KEYPAIR_TYPE_X509:
5843             return crypto.generate_x509_fingerprint(public_key)
5844 
5845     def _generate_key_pair(self, user_id, key_type):
5846         if key_type == keypair_obj.KEYPAIR_TYPE_SSH:
5847             return crypto.generate_key_pair()
5848         elif key_type == keypair_obj.KEYPAIR_TYPE_X509:
5849             return crypto.generate_winrm_x509_cert(user_id)
5850 
5851     @wrap_exception()
5852     def delete_key_pair(self, context, user_id, key_name):
5853         """Delete a keypair by name."""
5854         self._notify(context, 'delete.start', key_name)
5855         keypair = self.get_key_pair(context, user_id, key_name)
5856         compute_utils.notify_about_keypair_action(
5857             context=context,
5858             keypair=keypair,
5859             action=fields_obj.NotificationAction.DELETE,
5860             phase=fields_obj.NotificationPhase.START)
5861         objects.KeyPair.destroy_by_name(context, user_id, key_name)
5862         compute_utils.notify_about_keypair_action(
5863             context=context,
5864             keypair=keypair,
5865             action=fields_obj.NotificationAction.DELETE,
5866             phase=fields_obj.NotificationPhase.END)
5867         self._notify(context, 'delete.end', key_name)
5868 
5869     def get_key_pairs(self, context, user_id, limit=None, marker=None):
5870         """List key pairs."""
5871         return objects.KeyPairList.get_by_user(
5872             context, user_id, limit=limit, marker=marker)
5873 
5874     def get_key_pair(self, context, user_id, key_name):
5875         """Get a keypair by name."""
5876         return objects.KeyPair.get_by_name(context, user_id, key_name)
5877 
5878 
5879 class SecurityGroupAPI(base.Base, security_group_base.SecurityGroupBase):
5880     """Sub-set of the Compute API related to managing security groups
5881     and security group rules
5882     """
5883 
5884     # The nova security group api does not use a uuid for the id.
5885     id_is_uuid = False
5886 
5887     def __init__(self, **kwargs):
5888         super(SecurityGroupAPI, self).__init__(**kwargs)
5889         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
5890 
5891     def validate_property(self, value, property, allowed):
5892         """Validate given security group property.
5893 
5894         :param value:          the value to validate, as a string or unicode
5895         :param property:       the property, either 'name' or 'description'
5896         :param allowed:        the range of characters allowed
5897         """
5898 
5899         try:
5900             val = value.strip()
5901         except AttributeError:
5902             msg = _("Security group %s is not a string or unicode") % property
5903             self.raise_invalid_property(msg)
5904         utils.check_string_length(val, name=property, min_length=1,
5905                                   max_length=255)
5906 
5907         if allowed and not re.match(allowed, val):
5908             # Some validation to ensure that values match API spec.
5909             # - Alphanumeric characters, spaces, dashes, and underscores.
5910             # TODO(Daviey): LP: #813685 extend beyond group_name checking, and
5911             #  probably create a param validator that can be used elsewhere.
5912             msg = (_("Value (%(value)s) for parameter Group%(property)s is "
5913                      "invalid. Content limited to '%(allowed)s'.") %
5914                    {'value': value, 'allowed': allowed,
5915                     'property': property.capitalize()})
5916             self.raise_invalid_property(msg)
5917 
5918     def ensure_default(self, context):
5919         """Ensure that a context has a security group.
5920 
5921         Creates a security group for the security context if it does not
5922         already exist.
5923 
5924         :param context: the security context
5925         """
5926         self.db.security_group_ensure_default(context)
5927 
5928     def create_security_group(self, context, name, description):
5929         try:
5930             objects.Quotas.check_deltas(context, {'security_groups': 1},
5931                                         context.project_id,
5932                                         user_id=context.user_id)
5933         except exception.OverQuota:
5934             msg = _("Quota exceeded, too many security groups.")
5935             self.raise_over_quota(msg)
5936 
5937         LOG.info("Create Security Group %s", name)
5938 
5939         self.ensure_default(context)
5940 
5941         group = {'user_id': context.user_id,
5942                  'project_id': context.project_id,
5943                  'name': name,
5944                  'description': description}
5945         try:
5946             group_ref = self.db.security_group_create(context, group)
5947         except exception.SecurityGroupExists:
5948             msg = _('Security group %s already exists') % name
5949             self.raise_group_already_exists(msg)
5950 
5951         # NOTE(melwitt): We recheck the quota after creating the object to
5952         # prevent users from allocating more resources than their allowed quota
5953         # in the event of a race. This is configurable because it can be
5954         # expensive if strict quota limits are not required in a deployment.
5955         if CONF.quota.recheck_quota:
5956             try:
5957                 objects.Quotas.check_deltas(context, {'security_groups': 0},
5958                                             context.project_id,
5959                                             user_id=context.user_id)
5960             except exception.OverQuota:
5961                 self.db.security_group_destroy(context, group_ref['id'])
5962                 msg = _("Quota exceeded, too many security groups.")
5963                 self.raise_over_quota(msg)
5964 
5965         return group_ref
5966 
5967     def update_security_group(self, context, security_group,
5968                                 name, description):
5969         if security_group['name'] in RO_SECURITY_GROUPS:
5970             msg = (_("Unable to update system group '%s'") %
5971                     security_group['name'])
5972             self.raise_invalid_group(msg)
5973 
5974         group = {'name': name,
5975                  'description': description}
5976 
5977         columns_to_join = ['rules.grantee_group']
5978         group_ref = self.db.security_group_update(context,
5979                 security_group['id'],
5980                 group,
5981                 columns_to_join=columns_to_join)
5982         return group_ref
5983 
5984     def get(self, context, name=None, id=None, map_exception=False):
5985         self.ensure_default(context)
5986         cols = ['rules']
5987         try:
5988             if name:
5989                 return self.db.security_group_get_by_name(context,
5990                                                           context.project_id,
5991                                                           name,
5992                                                           columns_to_join=cols)
5993             elif id:
5994                 return self.db.security_group_get(context, id,
5995                                                   columns_to_join=cols)
5996         except exception.NotFound as exp:
5997             if map_exception:
5998                 msg = exp.format_message()
5999                 self.raise_not_found(msg)
6000             else:
6001                 raise
6002 
6003     def list(self, context, names=None, ids=None, project=None,
6004              search_opts=None):
6005         self.ensure_default(context)
6006 
6007         groups = []
6008         if names or ids:
6009             if names:
6010                 for name in names:
6011                     groups.append(self.db.security_group_get_by_name(context,
6012                                                                      project,
6013                                                                      name))
6014             if ids:
6015                 for id in ids:
6016                     groups.append(self.db.security_group_get(context, id))
6017 
6018         elif context.is_admin:
6019             # TODO(eglynn): support a wider set of search options than just
6020             # all_tenants, at least include the standard filters defined for
6021             # the EC2 DescribeSecurityGroups API for the non-admin case also
6022             if (search_opts and 'all_tenants' in search_opts):
6023                 groups = self.db.security_group_get_all(context)
6024             else:
6025                 groups = self.db.security_group_get_by_project(context,
6026                                                                project)
6027 
6028         elif project:
6029             groups = self.db.security_group_get_by_project(context, project)
6030 
6031         return groups
6032 
6033     def destroy(self, context, security_group):
6034         if security_group['name'] in RO_SECURITY_GROUPS:
6035             msg = _("Unable to delete system group '%s'") % \
6036                     security_group['name']
6037             self.raise_invalid_group(msg)
6038 
6039         if self.db.security_group_in_use(context, security_group['id']):
6040             msg = _("Security group is still in use")
6041             self.raise_invalid_group(msg)
6042 
6043         LOG.info("Delete security group %s", security_group['name'])
6044         self.db.security_group_destroy(context, security_group['id'])
6045 
6046     def is_associated_with_server(self, security_group, instance_uuid):
6047         """Check if the security group is already associated
6048            with the instance. If Yes, return True.
6049         """
6050 
6051         if not security_group:
6052             return False
6053 
6054         instances = security_group.get('instances')
6055         if not instances:
6056             return False
6057 
6058         for inst in instances:
6059             if (instance_uuid == inst['uuid']):
6060                 return True
6061 
6062         return False
6063 
6064     def add_to_instance(self, context, instance, security_group_name):
6065         """Add security group to the instance."""
6066         security_group = self.db.security_group_get_by_name(context,
6067                 context.project_id,
6068                 security_group_name)
6069 
6070         instance_uuid = instance.uuid
6071 
6072         # check if the security group is associated with the server
6073         if self.is_associated_with_server(security_group, instance_uuid):
6074             raise exception.SecurityGroupExistsForInstance(
6075                                         security_group_id=security_group['id'],
6076                                         instance_id=instance_uuid)
6077 
6078         self.db.instance_add_security_group(context.elevated(),
6079                                             instance_uuid,
6080                                             security_group['id'])
6081         if instance.host:
6082             self.compute_rpcapi.refresh_instance_security_rules(
6083                     context, instance, instance.host)
6084 
6085     def remove_from_instance(self, context, instance, security_group_name):
6086         """Remove the security group associated with the instance."""
6087         security_group = self.db.security_group_get_by_name(context,
6088                 context.project_id,
6089                 security_group_name)
6090 
6091         instance_uuid = instance.uuid
6092 
6093         # check if the security group is associated with the server
6094         if not self.is_associated_with_server(security_group, instance_uuid):
6095             raise exception.SecurityGroupNotExistsForInstance(
6096                                     security_group_id=security_group['id'],
6097                                     instance_id=instance_uuid)
6098 
6099         self.db.instance_remove_security_group(context.elevated(),
6100                                                instance_uuid,
6101                                                security_group['id'])
6102         if instance.host:
6103             self.compute_rpcapi.refresh_instance_security_rules(
6104                     context, instance, instance.host)
6105 
6106     def get_rule(self, context, id):
6107         self.ensure_default(context)
6108         try:
6109             return self.db.security_group_rule_get(context, id)
6110         except exception.NotFound:
6111             msg = _("Rule (%s) not found") % id
6112             self.raise_not_found(msg)
6113 
6114     def add_rules(self, context, id, name, vals):
6115         """Add security group rule(s) to security group.
6116 
6117         Note: the Nova security group API doesn't support adding multiple
6118         security group rules at once but the EC2 one does. Therefore,
6119         this function is written to support both.
6120         """
6121 
6122         try:
6123             objects.Quotas.check_deltas(context,
6124                                         {'security_group_rules': len(vals)},
6125                                         id)
6126         except exception.OverQuota:
6127             msg = _("Quota exceeded, too many security group rules.")
6128             self.raise_over_quota(msg)
6129 
6130         msg = ("Security group %(name)s added %(protocol)s ingress "
6131                "(%(from_port)s:%(to_port)s)")
6132         rules = []
6133         for v in vals:
6134             rule = self.db.security_group_rule_create(context, v)
6135 
6136             # NOTE(melwitt): We recheck the quota after creating the object to
6137             # prevent users from allocating more resources than their allowed
6138             # quota in the event of a race. This is configurable because it can
6139             # be expensive if strict quota limits are not required in a
6140             # deployment.
6141             if CONF.quota.recheck_quota:
6142                 try:
6143                     objects.Quotas.check_deltas(context,
6144                                                 {'security_group_rules': 0},
6145                                                 id)
6146                 except exception.OverQuota:
6147                     self.db.security_group_rule_destroy(context, rule['id'])
6148                     msg = _("Quota exceeded, too many security group rules.")
6149                     self.raise_over_quota(msg)
6150 
6151             rules.append(rule)
6152             LOG.info(msg, {'name': name,
6153                            'protocol': rule.protocol,
6154                            'from_port': rule.from_port,
6155                            'to_port': rule.to_port})
6156 
6157         self.trigger_rules_refresh(context, id=id)
6158         return rules
6159 
6160     def remove_rules(self, context, security_group, rule_ids):
6161         msg = ("Security group %(name)s removed %(protocol)s ingress "
6162                "(%(from_port)s:%(to_port)s)")
6163         for rule_id in rule_ids:
6164             rule = self.get_rule(context, rule_id)
6165             LOG.info(msg, {'name': security_group['name'],
6166                            'protocol': rule.protocol,
6167                            'from_port': rule.from_port,
6168                            'to_port': rule.to_port})
6169 
6170             self.db.security_group_rule_destroy(context, rule_id)
6171 
6172         # NOTE(vish): we removed some rules, so refresh
6173         self.trigger_rules_refresh(context, id=security_group['id'])
6174 
6175     def remove_default_rules(self, context, rule_ids):
6176         for rule_id in rule_ids:
6177             self.db.security_group_default_rule_destroy(context, rule_id)
6178 
6179     def add_default_rules(self, context, vals):
6180         rules = [self.db.security_group_default_rule_create(context, v)
6181                  for v in vals]
6182         return rules
6183 
6184     def default_rule_exists(self, context, values):
6185         """Indicates whether the specified rule values are already
6186            defined in the default security group rules.
6187         """
6188         for rule in self.db.security_group_default_rule_list(context):
6189             keys = ('cidr', 'from_port', 'to_port', 'protocol')
6190             for key in keys:
6191                 if rule.get(key) != values.get(key):
6192                     break
6193             else:
6194                 return rule.get('id') or True
6195         return False
6196 
6197     def get_all_default_rules(self, context):
6198         try:
6199             rules = self.db.security_group_default_rule_list(context)
6200         except Exception:
6201             msg = 'cannot get default security group rules'
6202             raise exception.SecurityGroupDefaultRuleNotFound(msg)
6203 
6204         return rules
6205 
6206     def get_default_rule(self, context, id):
6207         return self.db.security_group_default_rule_get(context, id)
6208 
6209     def validate_id(self, id):
6210         try:
6211             return int(id)
6212         except ValueError:
6213             msg = _("Security group id should be integer")
6214             self.raise_invalid_property(msg)
6215 
6216     def _refresh_instance_security_rules(self, context, instances):
6217         for instance in instances:
6218             if instance.host is not None:
6219                 self.compute_rpcapi.refresh_instance_security_rules(
6220                         context, instance, instance.host)
6221 
6222     def trigger_rules_refresh(self, context, id):
6223         """Called when a rule is added to or removed from a security_group."""
6224         instances = objects.InstanceList.get_by_security_group_id(context, id)
6225         self._refresh_instance_security_rules(context, instances)
6226 
6227     def trigger_members_refresh(self, context, group_ids):
6228         """Called when a security group gains a new or loses a member.
6229 
6230         Sends an update request to each compute node for each instance for
6231         which this is relevant.
6232         """
6233         instances = objects.InstanceList.get_by_grantee_security_group_ids(
6234             context, group_ids)
6235         self._refresh_instance_security_rules(context, instances)
6236 
6237     def get_instance_security_groups(self, context, instance, detailed=False):
6238         if detailed:
6239             return self.db.security_group_get_by_instance(context,
6240                                                           instance.uuid)
6241         return [{'name': group.name} for group in instance.security_groups]
