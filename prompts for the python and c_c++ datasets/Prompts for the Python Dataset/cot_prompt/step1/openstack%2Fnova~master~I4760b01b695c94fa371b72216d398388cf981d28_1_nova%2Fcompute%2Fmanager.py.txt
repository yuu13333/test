Based on the given code from a commit, please generate supplementary code files according to the commit message.
####commit message
Don't unset Instance.old_flavor, new_flavor until necessary

Since change Ia6d8a7909081b0b856bd7e290e234af7e42a2b38, the resource
tracker's 'drop_move_claim' method has been capable of freeing up
resources usage. However, this relies on accurate resource reporting.
It transpires that there's a race whereby the resource trackers
'update_available_resource' periodic task can end up not accounting for
usage from migrations that in the process of being completed. The root
cause is the resource tracker's reliance on the stashed flavor in a
given migration record [1]. Previously, this information was deleted by
the compute manager at the start of the confirm migration operation [2].
The compute manager would then call the virt driver [3], which could
take a variable amount of time to return, before finally dropping the
move claim. If the periodic task ran between the clearing of the stashed
flavor and the return of the virt driver, it would find a migration
record with no stashed flavor and would therefore ignore this record for
accounting purposes [4]. This results in an incorrect record for the
compute node, and an exception when the 'drop_move_claim' attempts to
free up the resources that aren't being tracked.

The solution is pretty simple. Instead of unsetting the old flavor
record from the migration at the start of the various move operations,
do it afterwards.

[1] https://github.com/openstack/nova/blob/6557d67/nova/compute/resource_tracker.py#L1288
[2] https://github.com/openstack/nova/blob/6557d67/nova/compute/manager.py#L4310-L4315
[3] https://github.com/openstack/nova/blob/6557d67/nova/compute/manager.py#L4330-L4331
[4] https://github.com/openstack/nova/blob/6557d67/nova/compute/resource_tracker.py#L1300

Change-Id: I4760b01b695c94fa371b72216d398388cf981d28
Signed-off-by: Stephen Finucane <stephenfin@redhat.com>
Closes-Bug: #1879878
Related-Bug: #1834349
Related-Bug: #1818914

####code 
1 # Copyright 2010 United States Government as represented by the
2 # Administrator of the National Aeronautics and Space Administration.
3 # Copyright 2011 Justin Santa Barbara
4 # All Rights Reserved.
5 #
6 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
7 #    not use this file except in compliance with the License. You may obtain
8 #    a copy of the License at
9 #
10 #         http://www.apache.org/licenses/LICENSE-2.0
11 #
12 #    Unless required by applicable law or agreed to in writing, software
13 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
14 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
15 #    License for the specific language governing permissions and limitations
16 #    under the License.
17 
18 """Handles all processes relating to instances (guest vms).
19 
20 The :py:class:`ComputeManager` class is a :py:class:`nova.manager.Manager` that
21 handles RPC calls relating to creating instances.  It is responsible for
22 building a disk image, launching it via the underlying virtualization driver,
23 responding to calls to check its state, attaching persistent storage, and
24 terminating it.
25 
26 """
27 
28 import base64
29 import binascii
30 import contextlib
31 import copy
32 import functools
33 import inspect
34 import sys
35 import time
36 import traceback
37 import typing as ty
38 
39 from cinderclient import exceptions as cinder_exception
40 from cursive import exception as cursive_exception
41 import eventlet.event
42 from eventlet import greenthread
43 import eventlet.semaphore
44 import eventlet.timeout
45 import futurist
46 from keystoneauth1 import exceptions as keystone_exception
47 import os_traits
48 from oslo_log import log as logging
49 import oslo_messaging as messaging
50 from oslo_serialization import jsonutils
51 from oslo_service import loopingcall
52 from oslo_service import periodic_task
53 from oslo_utils import excutils
54 from oslo_utils import strutils
55 from oslo_utils import timeutils
56 from oslo_utils import units
57 import six
58 from six.moves import range
59 
60 from nova.accelerator import cyborg
61 from nova import block_device
62 from nova.compute import api as compute
63 from nova.compute import build_results
64 from nova.compute import claims
65 from nova.compute import power_state
66 from nova.compute import resource_tracker
67 from nova.compute import rpcapi as compute_rpcapi
68 from nova.compute import task_states
69 from nova.compute import utils as compute_utils
70 from nova.compute.utils import wrap_instance_event
71 from nova.compute import vm_states
72 from nova import conductor
73 import nova.conf
74 import nova.context
75 from nova import exception
76 from nova import exception_wrapper
77 from nova.i18n import _
78 from nova.image import glance
79 from nova import manager
80 from nova.network import model as network_model
81 from nova.network import neutron
82 from nova import objects
83 from nova.objects import base as obj_base
84 from nova.objects import external_event as external_event_obj
85 from nova.objects import fields
86 from nova.objects import instance as obj_instance
87 from nova.objects import migrate_data as migrate_data_obj
88 from nova.pci import request as pci_req_module
89 from nova.pci import whitelist
90 from nova import rpc
91 from nova import safe_utils
92 from nova.scheduler.client import query
93 from nova.scheduler.client import report
94 from nova.scheduler import utils as scheduler_utils
95 from nova import utils
96 from nova.virt import block_device as driver_block_device
97 from nova.virt import configdrive
98 from nova.virt import driver
99 from nova.virt import event as virtevent
100 from nova.virt import hardware
101 from nova.virt import storage_users
102 from nova.virt import virtapi
103 from nova.volume import cinder
104 
105 CONF = nova.conf.CONF
106 
107 LOG = logging.getLogger(__name__)
108 
109 get_notifier = functools.partial(rpc.get_notifier, service='compute')
110 wrap_exception = functools.partial(exception_wrapper.wrap_exception,
111                                    get_notifier=get_notifier,
112                                    binary='nova-compute')
113 
114 
115 @contextlib.contextmanager
116 def errors_out_migration_ctxt(migration):
117     """Context manager to error out migration on failure."""
118 
119     try:
120         yield
121     except Exception:
122         with excutils.save_and_reraise_exception():
123             if migration:
124                 # We may have been passed None for our migration if we're
125                 # receiving from an older client. The migration will be
126                 # errored via the legacy path.
127                 migration.status = 'error'
128                 try:
129                     migration.save()
130                 except Exception:
131                     LOG.debug(
132                         'Error setting migration status for instance %s.',
133                         migration.instance_uuid, exc_info=True)
134 
135 
136 @utils.expects_func_args('migration')
137 def errors_out_migration(function):
138     """Decorator to error out migration on failure."""
139 
140     @functools.wraps(function)
141     def decorated_function(self, context, *args, **kwargs):
142         wrapped_func = safe_utils.get_wrapped_function(function)
143         keyed_args = inspect.getcallargs(wrapped_func, self, context,
144                                          *args, **kwargs)
145         migration = keyed_args['migration']
146         with errors_out_migration_ctxt(migration):
147             return function(self, context, *args, **kwargs)
148 
149     return decorated_function
150 
151 
152 @utils.expects_func_args('instance')
153 def reverts_task_state(function):
154     """Decorator to revert task_state on failure."""
155 
156     @functools.wraps(function)
157     def decorated_function(self, context, *args, **kwargs):
158         try:
159             return function(self, context, *args, **kwargs)
160         except exception.UnexpectedTaskStateError as e:
161             # Note(maoy): unexpected task state means the current
162             # task is preempted. Do not clear task state in this
163             # case.
164             with excutils.save_and_reraise_exception():
165                 LOG.info("Task possibly preempted: %s",
166                          e.format_message())
167         except Exception:
168             with excutils.save_and_reraise_exception():
169                 wrapped_func = safe_utils.get_wrapped_function(function)
170                 keyed_args = inspect.getcallargs(wrapped_func, self, context,
171                                                  *args, **kwargs)
172                 # NOTE(mriedem): 'instance' must be in keyed_args because we
173                 # have utils.expects_func_args('instance') decorating this
174                 # method.
175                 instance = keyed_args['instance']
176                 original_task_state = instance.task_state
177                 try:
178                     self._instance_update(context, instance, task_state=None)
179                     LOG.info("Successfully reverted task state from %s on "
180                              "failure for instance.",
181                              original_task_state, instance=instance)
182                 except exception.InstanceNotFound:
183                     # We might delete an instance that failed to build shortly
184                     # after it errored out this is an expected case and we
185                     # should not trace on it.
186                     pass
187                 except Exception as e:
188                     LOG.warning("Failed to revert task state for instance. "
189                                 "Error: %s", e, instance=instance)
190 
191     return decorated_function
192 
193 
194 @utils.expects_func_args('instance')
195 def wrap_instance_fault(function):
196     """Wraps a method to catch exceptions related to instances.
197 
198     This decorator wraps a method to catch any exceptions having to do with
199     an instance that may get thrown. It then logs an instance fault in the db.
200     """
201 
202     @functools.wraps(function)
203     def decorated_function(self, context, *args, **kwargs):
204         try:
205             return function(self, context, *args, **kwargs)
206         except exception.InstanceNotFound:
207             raise
208         except Exception as e:
209             # NOTE(gtt): If argument 'instance' is in args rather than kwargs,
210             # we will get a KeyError exception which will cover up the real
211             # exception. So, we update kwargs with the values from args first.
212             # then, we can get 'instance' from kwargs easily.
213             kwargs.update(dict(zip(function.__code__.co_varnames[2:], args)))
214 
215             with excutils.save_and_reraise_exception():
216                 compute_utils.add_instance_fault_from_exc(context,
217                         kwargs['instance'], e, sys.exc_info())
218 
219     return decorated_function
220 
221 
222 @utils.expects_func_args('image_id', 'instance')
223 def delete_image_on_error(function):
224     """Used for snapshot related method to ensure the image created in
225     compute.api is deleted when an error occurs.
226     """
227 
228     @functools.wraps(function)
229     def decorated_function(self, context, image_id, instance,
230                            *args, **kwargs):
231         try:
232             return function(self, context, image_id, instance,
233                             *args, **kwargs)
234         except Exception:
235             with excutils.save_and_reraise_exception():
236                 compute_utils.delete_image(
237                     context, instance, self.image_api, image_id,
238                     log_exc_info=True)
239 
240     return decorated_function
241 
242 
243 class InstanceEvents(object):
244     def __init__(self):
245         self._events = {}
246 
247     @staticmethod
248     def _lock_name(instance):
249         return '%s-%s' % (instance.uuid, 'events')
250 
251     def prepare_for_instance_event(self, instance, name, tag):
252         """Prepare to receive an event for an instance.
253 
254         This will register an event for the given instance that we will
255         wait on later. This should be called before initiating whatever
256         action will trigger the event. The resulting eventlet.event.Event
257         object should be wait()'d on to ensure completion.
258 
259         :param instance: the instance for which the event will be generated
260         :param name: the name of the event we're expecting
261         :param tag: the tag associated with the event we're expecting
262         :returns: an event object that should be wait()'d on
263         """
264         if self._events is None:
265             # NOTE(danms): We really should have a more specific error
266             # here, but this is what we use for our default error case
267             raise exception.NovaException('In shutdown, no new events '
268                                           'can be scheduled')
269 
270         @utils.synchronized(self._lock_name(instance))
271         def _create_or_get_event():
272             instance_events = self._events.setdefault(instance.uuid, {})
273             return instance_events.setdefault((name, tag),
274                                               eventlet.event.Event())
275         LOG.debug('Preparing to wait for external event %(name)s-%(tag)s',
276                   {'name': name, 'tag': tag}, instance=instance)
277         return _create_or_get_event()
278 
279     def pop_instance_event(self, instance, event):
280         """Remove a pending event from the wait list.
281 
282         This will remove a pending event from the wait list so that it
283         can be used to signal the waiters to wake up.
284 
285         :param instance: the instance for which the event was generated
286         :param event: the nova.objects.external_event.InstanceExternalEvent
287                       that describes the event
288         :returns: the eventlet.event.Event object on which the waiters
289                   are blocked
290         """
291         no_events_sentinel = object()
292         no_matching_event_sentinel = object()
293 
294         @utils.synchronized(self._lock_name(instance))
295         def _pop_event():
296             if self._events is None:
297                 LOG.debug('Unexpected attempt to pop events during shutdown',
298                           instance=instance)
299                 return no_events_sentinel
300             events = self._events.get(instance.uuid)
301             if not events:
302                 return no_events_sentinel
303             _event = events.pop((event.name, event.tag), None)
304             if not events:
305                 del self._events[instance.uuid]
306             if _event is None:
307                 return no_matching_event_sentinel
308             return _event
309 
310         result = _pop_event()
311         if result is no_events_sentinel:
312             LOG.debug('No waiting events found dispatching %(event)s',
313                       {'event': event.key},
314                       instance=instance)
315             return None
316         elif result is no_matching_event_sentinel:
317             LOG.debug('No event matching %(event)s in %(events)s',
318                       {'event': event.key,
319                        'events': self._events.get(instance.uuid, {}).keys()},
320                       instance=instance)
321             return None
322         else:
323             return result
324 
325     def clear_events_for_instance(self, instance):
326         """Remove all pending events for an instance.
327 
328         This will remove all events currently pending for an instance
329         and return them (indexed by event name).
330 
331         :param instance: the instance for which events should be purged
332         :returns: a dictionary of {event_name: eventlet.event.Event}
333         """
334         @utils.synchronized(self._lock_name(instance))
335         def _clear_events():
336             if self._events is None:
337                 LOG.debug('Unexpected attempt to clear events during shutdown',
338                           instance=instance)
339                 return dict()
340             # NOTE(danms): We have historically returned the raw internal
341             # format here, which is {event.key: [events, ...])} so just
342             # trivially convert it here.
343             return {'%s-%s' % k: e
344                     for k, e in self._events.pop(instance.uuid, {}).items()}
345         return _clear_events()
346 
347     def cancel_all_events(self):
348         if self._events is None:
349             LOG.debug('Unexpected attempt to cancel events during shutdown.')
350             return
351         our_events = self._events
352         # NOTE(danms): Block new events
353         self._events = None
354 
355         for instance_uuid, events in our_events.items():
356             for (name, tag), eventlet_event in events.items():
357                 LOG.debug('Canceling in-flight event %(name)s-%(tag)s for '
358                           'instance %(instance_uuid)s',
359                           {'name': name,
360                            'tag': tag,
361                            'instance_uuid': instance_uuid})
362                 event = objects.InstanceExternalEvent(
363                     instance_uuid=instance_uuid,
364                     name=name, status='failed',
365                     tag=tag, data={})
366                 eventlet_event.send(event)
367 
368 
369 class ComputeVirtAPI(virtapi.VirtAPI):
370     def __init__(self, compute):
371         super(ComputeVirtAPI, self).__init__()
372         self._compute = compute
373         self.reportclient = compute.reportclient
374 
375         class ExitEarly(Exception):
376             def __init__(self, events):
377                 super(Exception, self).__init__()
378                 self.events = events
379 
380         self._exit_early_exc = ExitEarly
381 
382     def exit_wait_early(self, events):
383         """Exit a wait_for_instance_event() immediately and avoid
384         waiting for some events.
385 
386         :param: events: A list of (name, tag) tuples for events that we should
387                         skip waiting for during a wait_for_instance_event().
388         """
389         raise self._exit_early_exc(events=events)
390 
391     def _default_error_callback(self, event_name, instance):
392         raise exception.NovaException(_('Instance event failed'))
393 
394     @contextlib.contextmanager
395     def wait_for_instance_event(self, instance, event_names, deadline=300,
396                                 error_callback=None):
397         """Plan to wait for some events, run some code, then wait.
398 
399         This context manager will first create plans to wait for the
400         provided event_names, yield, and then wait for all the scheduled
401         events to complete.
402 
403         Note that this uses an eventlet.timeout.Timeout to bound the
404         operation, so callers should be prepared to catch that
405         failure and handle that situation appropriately.
406 
407         If the event is not received by the specified timeout deadline,
408         eventlet.timeout.Timeout is raised.
409 
410         If the event is received but did not have a 'completed'
411         status, a NovaException is raised.  If an error_callback is
412         provided, instead of raising an exception as detailed above
413         for the failure case, the callback will be called with the
414         event_name and instance, and can return True to continue
415         waiting for the rest of the events, False to stop processing,
416         or raise an exception which will bubble up to the waiter.
417 
418         If the inner code wishes to abort waiting for one or more
419         events because it knows some state to be finished or condition
420         to be satisfied, it can use VirtAPI.exit_wait_early() with a
421         list of event (name,tag) items to avoid waiting for those
422         events upon context exit. Note that exit_wait_early() exits
423         the context immediately and should be used to signal that all
424         work has been completed and provide the unified list of events
425         that need not be waited for. Waiting for the remaining events
426         will begin immediately upon early exit as if the context was
427         exited normally.
428 
429         :param instance: The instance for which an event is expected
430         :param event_names: A list of event names. Each element is a
431                             tuple of strings to indicate (name, tag),
432                             where name is required, but tag may be None.
433         :param deadline: Maximum number of seconds we should wait for all
434                          of the specified events to arrive.
435         :param error_callback: A function to be called if an event arrives
436 
437         """
438 
439         if error_callback is None:
440             error_callback = self._default_error_callback
441         events = {}
442         for event_name in event_names:
443             name, tag = event_name
444             event_name = objects.InstanceExternalEvent.make_key(name, tag)
445             try:
446                 events[event_name] = (
447                     self._compute.instance_events.prepare_for_instance_event(
448                         instance, name, tag))
449             except exception.NovaException:
450                 error_callback(event_name, instance)
451                 # NOTE(danms): Don't wait for any of the events. They
452                 # should all be canceled and fired immediately below,
453                 # but don't stick around if not.
454                 deadline = 0
455         try:
456             yield
457         except self._exit_early_exc as e:
458             early_events = set([objects.InstanceExternalEvent.make_key(n, t)
459                                 for n, t in e.events])
460         else:
461             early_events = []
462 
463         with eventlet.timeout.Timeout(deadline):
464             for event_name, event in events.items():
465                 if event_name in early_events:
466                     continue
467                 else:
468                     actual_event = event.wait()
469                     if actual_event.status == 'completed':
470                         continue
471                 # If we get here, we have an event that was not completed,
472                 # nor skipped via exit_wait_early(). Decide whether to
473                 # keep waiting by calling the error_callback() hook.
474                 decision = error_callback(event_name, instance)
475                 if decision is False:
476                     break
477 
478     def update_compute_provider_status(self, context, rp_uuid, enabled):
479         """Used to add/remove the COMPUTE_STATUS_DISABLED trait on the provider
480 
481         :param context: nova auth RequestContext
482         :param rp_uuid: UUID of a compute node resource provider in Placement
483         :param enabled: True if the node is enabled in which case the trait
484             would be removed, False if the node is disabled in which case
485             the trait would be added.
486         :raises: ResourceProviderTraitRetrievalFailed
487         :raises: ResourceProviderUpdateConflict
488         :raises: ResourceProviderUpdateFailed
489         :raises: TraitRetrievalFailed
490         :raises: keystoneauth1.exceptions.ClientException
491         """
492         trait_name = os_traits.COMPUTE_STATUS_DISABLED
493         # Get the current traits (and generation) for the provider.
494         # TODO(mriedem): Leverage the ProviderTree cache in get_provider_traits
495         trait_info = self.reportclient.get_provider_traits(context, rp_uuid)
496         # If the host is enabled, remove the trait (if set), else add
497         # the trait if it doesn't already exist.
498         original_traits = trait_info.traits
499         new_traits = None
500         if enabled and trait_name in original_traits:
501             new_traits = original_traits - {trait_name}
502             LOG.debug('Removing trait %s from compute node resource '
503                       'provider %s in placement.', trait_name, rp_uuid)
504         elif not enabled and trait_name not in original_traits:
505             new_traits = original_traits | {trait_name}
506             LOG.debug('Adding trait %s to compute node resource '
507                       'provider %s in placement.', trait_name, rp_uuid)
508 
509         if new_traits is not None:
510             self.reportclient.set_traits_for_provider(
511                 context, rp_uuid, new_traits, generation=trait_info.generation)
512 
513 
514 class ComputeManager(manager.Manager):
515     """Manages the running instances from creation to destruction."""
516 
517     target = messaging.Target(version='5.11')
518 
519     def __init__(self, compute_driver=None, *args, **kwargs):
520         """Load configuration options and connect to the hypervisor."""
521         # We want the ComputeManager, ResourceTracker and ComputeVirtAPI all
522         # using the same instance of SchedulerReportClient which has the
523         # ProviderTree cache for this compute service.
524         self.reportclient = report.SchedulerReportClient()
525         self.virtapi = ComputeVirtAPI(self)
526         self.network_api = neutron.API()
527         self.volume_api = cinder.API()
528         self.image_api = glance.API()
529         self._last_bw_usage_poll = 0
530         self._bw_usage_supported = True
531         self.compute_api = compute.API()
532         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
533         self.compute_task_api = conductor.ComputeTaskAPI()
534         self.query_client = query.SchedulerQueryClient()
535         self.instance_events = InstanceEvents()
536         self._sync_power_pool = eventlet.GreenPool(
537             size=CONF.sync_power_state_pool_size)
538         self._syncs_in_progress = {}
539         self.send_instance_updates = (
540             CONF.filter_scheduler.track_instance_changes)
541         if CONF.max_concurrent_builds != 0:
542             self._build_semaphore = eventlet.semaphore.Semaphore(
543                 CONF.max_concurrent_builds)
544         else:
545             self._build_semaphore = compute_utils.UnlimitedSemaphore()
546         if CONF.max_concurrent_snapshots > 0:
547             self._snapshot_semaphore = eventlet.semaphore.Semaphore(
548                 CONF.max_concurrent_snapshots)
549         else:
550             self._snapshot_semaphore = compute_utils.UnlimitedSemaphore()
551         if CONF.max_concurrent_live_migrations > 0:
552             self._live_migration_executor = futurist.GreenThreadPoolExecutor(
553                 max_workers=CONF.max_concurrent_live_migrations)
554         else:
555             # CONF.max_concurrent_live_migrations is 0 (unlimited)
556             self._live_migration_executor = futurist.GreenThreadPoolExecutor()
557         # This is a dict, keyed by instance uuid, to a two-item tuple of
558         # migration object and Future for the queued live migration.
559         self._waiting_live_migrations = {}
560 
561         super(ComputeManager, self).__init__(service_name="compute",
562                                              *args, **kwargs)
563 
564         # NOTE(russellb) Load the driver last.  It may call back into the
565         # compute manager via the virtapi, so we want it to be fully
566         # initialized before that happens.
567         self.driver = driver.load_compute_driver(self.virtapi, compute_driver)
568         self.use_legacy_block_device_info = \
569                             self.driver.need_legacy_block_device_info
570         self.rt = resource_tracker.ResourceTracker(
571             self.host, self.driver, reportclient=self.reportclient)
572 
573     def reset(self):
574         LOG.info('Reloading compute RPC API')
575         compute_rpcapi.reset_globals()
576         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
577         self.reportclient.clear_provider_cache()
578 
579     def _update_resource_tracker(self, context, instance):
580         """Let the resource tracker know that an instance has changed state."""
581 
582         if instance.host == self.host:
583             self.rt.update_usage(context, instance, instance.node)
584 
585     def _instance_update(self, context, instance, **kwargs):
586         """Update an instance in the database using kwargs as value."""
587 
588         for k, v in kwargs.items():
589             setattr(instance, k, v)
590         instance.save()
591         self._update_resource_tracker(context, instance)
592 
593     def _nil_out_instance_obj_host_and_node(self, instance):
594         # NOTE(jwcroppe): We don't do instance.save() here for performance
595         # reasons; a call to this is expected to be immediately followed by
596         # another call that does instance.save(), thus avoiding two writes
597         # to the database layer.
598         instance.host = None
599         instance.node = None
600         # ResourceTracker._set_instance_host_and_node also sets launched_on
601         # to the same value as host and is really only ever used by legacy
602         # nova-network code, but we should also null it out to avoid confusion
603         # if there is an instance in the database with no host set but
604         # launched_on is set. Note that we do not care about using launched_on
605         # as some kind of debug helper if diagnosing a build failure, that is
606         # what instance action events are for.
607         instance.launched_on = None
608         # If the instance is not on a host, it's not in an aggregate and
609         # therefore is not in an availability zone.
610         instance.availability_zone = None
611 
612     def _set_instance_obj_error_state(self, instance, clean_task_state=False):
613         try:
614             instance.vm_state = vm_states.ERROR
615             if clean_task_state:
616                 instance.task_state = None
617             instance.save()
618         except exception.InstanceNotFound:
619             LOG.debug('Instance has been destroyed from under us while '
620                       'trying to set it to ERROR', instance=instance)
621 
622     def _get_instances_on_driver(self, context, filters=None):
623         """Return a list of instance records for the instances found
624         on the hypervisor which satisfy the specified filters. If filters=None
625         return a list of instance records for all the instances found on the
626         hypervisor.
627         """
628         if not filters:
629             filters = {}
630         try:
631             driver_uuids = self.driver.list_instance_uuids()
632             if len(driver_uuids) == 0:
633                 # Short circuit, don't waste a DB call
634                 return objects.InstanceList()
635             filters['uuid'] = driver_uuids
636             local_instances = objects.InstanceList.get_by_filters(
637                 context, filters, use_slave=True)
638             return local_instances
639         except NotImplementedError:
640             pass
641 
642         # The driver doesn't support uuids listing, so we'll have
643         # to brute force.
644         driver_instances = self.driver.list_instances()
645         # NOTE(mjozefcz): In this case we need to apply host filter.
646         # Without this all instance data would be fetched from db.
647         filters['host'] = self.host
648         instances = objects.InstanceList.get_by_filters(context, filters,
649                                                         use_slave=True)
650         name_map = {instance.name: instance for instance in instances}
651         local_instances = []
652         for driver_instance in driver_instances:
653             instance = name_map.get(driver_instance)
654             if not instance:
655                 continue
656             local_instances.append(instance)
657         return local_instances
658 
659     def _destroy_evacuated_instances(self, context, node_cache):
660         """Destroys evacuated instances.
661 
662         While nova-compute was down, the instances running on it could be
663         evacuated to another host. This method looks for evacuation migration
664         records where this is the source host and which were either started
665         (accepted), in-progress (pre-migrating) or migrated (done). From those
666         migration records, local instances reported by the hypervisor are
667         compared to the instances for the migration records and those local
668         guests are destroyed, along with instance allocation records in
669         Placement for this node.
670         Then allocations are removed from Placement for every instance that is
671         evacuated from this host regardless if the instance is reported by the
672         hypervisor or not.
673 
674         :param context: The request context
675         :param node_cache: A dict of ComputeNode objects keyed by the UUID of
676             the compute node
677         :return: A dict keyed by instance uuid mapped to Migration objects
678             for instances that were migrated away from this host
679         """
680         filters = {
681             'source_compute': self.host,
682             # NOTE(mriedem): Migration records that have been accepted are
683             # included in case the source node comes back up while instances
684             # are being evacuated to another host. We don't want the same
685             # instance being reported from multiple hosts.
686             # NOTE(lyarwood): pre-migrating is also included here as the
687             # source compute can come back online shortly after the RT
688             # claims on the destination that in-turn moves the migration to
689             # pre-migrating. If the evacuate fails on the destination host,
690             # the user can rebuild the instance (in ERROR state) on the source
691             # host.
692             'status': ['accepted', 'pre-migrating', 'done'],
693             'migration_type': fields.MigrationType.EVACUATION,
694         }
695         with utils.temporary_mutation(context, read_deleted='yes'):
696             evacuations = objects.MigrationList.get_by_filters(context,
697                                                                filters)
698         if not evacuations:
699             return {}
700         evacuations = {mig.instance_uuid: mig for mig in evacuations}
701 
702         # TODO(mriedem): We could optimize by pre-loading the joined fields
703         # we know we'll use, like info_cache and flavor.
704         local_instances = self._get_instances_on_driver(context)
705         evacuated_local_instances = {inst.uuid: inst
706                                      for inst in local_instances
707                                      if inst.uuid in evacuations}
708 
709         for instance in evacuated_local_instances.values():
710             LOG.info('Destroying instance as it has been evacuated from '
711                      'this host but still exists in the hypervisor',
712                      instance=instance)
713             try:
714                 network_info = self.network_api.get_instance_nw_info(
715                     context, instance)
716                 bdi = self._get_instance_block_device_info(context,
717                                                            instance)
718                 destroy_disks = not (self._is_instance_storage_shared(
719                     context, instance))
720             except exception.InstanceNotFound:
721                 network_info = network_model.NetworkInfo()
722                 bdi = {}
723                 LOG.info('Instance has been marked deleted already, '
724                          'removing it from the hypervisor.',
725                          instance=instance)
726                 # always destroy disks if the instance was deleted
727                 destroy_disks = True
728             self.driver.destroy(context, instance,
729                                 network_info,
730                                 bdi, destroy_disks)
731 
732         hostname_to_cn_uuid = {
733             cn.hypervisor_hostname: cn.uuid
734             for cn in node_cache.values()}
735 
736         for instance_uuid, migration in evacuations.items():
737             try:
738                 if instance_uuid in evacuated_local_instances:
739                     # Avoid the db call if we already have the instance loaded
740                     # above
741                     instance = evacuated_local_instances[instance_uuid]
742                 else:
743                     instance = objects.Instance.get_by_uuid(
744                         context, instance_uuid)
745             except exception.InstanceNotFound:
746                 # The instance already deleted so we expect that every
747                 # allocation of that instance has already been cleaned up
748                 continue
749 
750             LOG.info('Cleaning up allocations of the instance as it has been '
751                      'evacuated from this host',
752                      instance=instance)
753             if migration.source_node not in hostname_to_cn_uuid:
754                 LOG.error("Failed to clean allocation of evacuated "
755                           "instance as the source node %s is not found",
756                           migration.source_node, instance=instance)
757                 continue
758             cn_uuid = hostname_to_cn_uuid[migration.source_node]
759 
760             # If the instance was deleted in the interim, assume its
761             # allocations were properly cleaned up (either by its hosting
762             # compute service or the API).
763             if (not instance.deleted and
764                     not self.reportclient.
765                         remove_provider_tree_from_instance_allocation(
766                             context, instance.uuid, cn_uuid)):
767                 LOG.error("Failed to clean allocation of evacuated instance "
768                           "on the source node %s",
769                           cn_uuid, instance=instance)
770 
771             migration.status = 'completed'
772             migration.save()
773         return evacuations
774 
775     def _is_instance_storage_shared(self, context, instance, host=None):
776         shared_storage = True
777         data = None
778         try:
779             data = self.driver.check_instance_shared_storage_local(context,
780                                                        instance)
781             if data:
782                 shared_storage = (self.compute_rpcapi.
783                                   check_instance_shared_storage(context,
784                                   instance, data, host=host))
785         except NotImplementedError:
786             LOG.debug('Hypervisor driver does not support '
787                       'instance shared storage check, '
788                       'assuming it\'s not on shared storage',
789                       instance=instance)
790             shared_storage = False
791         except Exception:
792             LOG.exception('Failed to check if instance shared',
793                           instance=instance)
794         finally:
795             if data:
796                 self.driver.check_instance_shared_storage_cleanup(context,
797                                                                   data)
798         return shared_storage
799 
800     def _complete_partial_deletion(self, context, instance):
801         """Complete deletion for instances in DELETED status but not marked as
802         deleted in the DB
803         """
804         instance.destroy()
805         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
806                 context, instance.uuid)
807         self._complete_deletion(context,
808                                 instance)
809         self._notify_about_instance_usage(context, instance, "delete.end")
810         compute_utils.notify_about_instance_action(context, instance,
811                 self.host, action=fields.NotificationAction.DELETE,
812                 phase=fields.NotificationPhase.END, bdms=bdms)
813 
814     def _complete_deletion(self, context, instance):
815         self._update_resource_tracker(context, instance)
816 
817         self.reportclient.delete_allocation_for_instance(context,
818                                                          instance.uuid)
819 
820         self._clean_instance_console_tokens(context, instance)
821         self._delete_scheduler_instance_info(context, instance.uuid)
822 
823     def _validate_pinning_configuration(self, instances):
824         if not self.driver.capabilities.get('supports_pcpus', False):
825             return
826 
827         for instance in instances:
828             # ignore deleted instances
829             if instance.deleted:
830                 continue
831 
832             # if this is an unpinned instance and the host only has
833             # 'cpu_dedicated_set' configured, we need to tell the operator to
834             # correct their configuration
835             if not instance.numa_topology or (
836                 instance.numa_topology.cpu_policy in (
837                     None, fields.CPUAllocationPolicy.SHARED
838                 )
839             ):
840                 # we don't need to check 'vcpu_pin_set' since it can't coexist
841                 # alongside 'cpu_dedicated_set'
842                 if (CONF.compute.cpu_dedicated_set and
843                         not CONF.compute.cpu_shared_set):
844                     msg = _("This host has unpinned instances but has no CPUs "
845                             "set aside for this purpose; configure '[compute] "
846                             "cpu_shared_set' instead of, or in addition to, "
847                             "'[compute] cpu_dedicated_set'")
848                     raise exception.InvalidConfiguration(msg)
849 
850                 continue
851 
852             # ditto for pinned instances if only 'cpu_shared_set' is configured
853             if (CONF.compute.cpu_shared_set and
854                     not CONF.compute.cpu_dedicated_set and
855                     not CONF.vcpu_pin_set):
856                 msg = _("This host has pinned instances but has no CPUs "
857                         "set aside for this purpose; configure '[compute] "
858                         "cpu_dedicated_set' instead of, or in addition to, "
859                         "'[compute] cpu_shared_set'.")
860                 raise exception.InvalidConfiguration(msg)
861 
862             # if this is a mixed instance with both pinned and unpinned CPUs,
863             # the host must have both 'cpu_dedicated_set' and 'cpu_shared_set'
864             # configured. check if 'cpu_shared_set' is set.
865             if (instance.numa_topology.cpu_policy ==
866                     fields.CPUAllocationPolicy.MIXED and
867                     not CONF.compute.cpu_shared_set):
868                 msg = _("This host has mixed instance requesting both pinned "
869                         "and unpinned CPUs but hasn't set aside unpinned CPUs "
870                         "for this purpose; Configure "
871                         "'[compute] cpu_shared_set'.")
872                 raise exception.InvalidConfiguration(msg)
873 
874             # for mixed instance check if 'cpu_dedicated_set' is set.
875             if (instance.numa_topology.cpu_policy ==
876                     fields.CPUAllocationPolicy.MIXED and
877                     not CONF.compute.cpu_dedicated_set):
878                 msg = _("This host has mixed instance requesting both pinned "
879                         "and unpinned CPUs but hasn't set aside pinned CPUs "
880                         "for this purpose; Configure "
881                         "'[compute] cpu_dedicated_set'")
882                 raise exception.InvalidConfiguration(msg)
883 
884             # also check to make sure the operator hasn't accidentally
885             # dropped some cores that instances are currently using
886             available_dedicated_cpus = (hardware.get_vcpu_pin_set() or
887                                         hardware.get_cpu_dedicated_set())
888             pinned_cpus = instance.numa_topology.cpu_pinning
889             if available_dedicated_cpus and (
890                     pinned_cpus - available_dedicated_cpus):
891                 # we can't raise an exception because of bug #1289064,
892                 # which meant we didn't recalculate CPU pinning information
893                 # when we live migrated a pinned instance
894                 LOG.warning(
895                     "Instance is pinned to host CPUs %(cpus)s "
896                     "but one or more of these CPUs are not included in "
897                     "either '[compute] cpu_dedicated_set' or "
898                     "'vcpu_pin_set'; you should update these "
899                     "configuration options to include the missing CPUs "
900                     "or rebuild or cold migrate this instance.",
901                     {'cpus': list(pinned_cpus)},
902                     instance=instance)
903 
904     def _validate_vtpm_configuration(self, instances):
905         if self.driver.capabilities.get('supports_vtpm', False):
906             return
907 
908         for instance in instances:
909             if instance.deleted:
910                 continue
911 
912             # NOTE(stephenfin): We don't have an attribute on the instance to
913             # check for this, so we need to inspect the flavor/image metadata
914             if hardware.get_vtpm_constraint(
915                 instance.flavor, instance.image_meta,
916             ):
917                 msg = _(
918                     'This host has instances with the vTPM feature enabled, '
919                     'but the host is not correctly configured; enable '
920                     'vTPM support.'
921                 )
922                 raise exception.InvalidConfiguration(msg)
923 
924     def _reset_live_migration(self, context, instance):
925         migration = None
926         try:
927             migration = objects.Migration.get_by_instance_and_status(
928                                       context, instance.uuid, 'running')
929             if migration:
930                 self.live_migration_abort(context, instance, migration.id)
931         except Exception:
932             LOG.exception('Failed to abort live-migration',
933                           instance=instance)
934         finally:
935             if migration:
936                 self._set_migration_status(migration, 'error')
937             LOG.info('Instance found in migrating state during '
938                      'startup. Resetting task_state',
939                      instance=instance)
940             instance.task_state = None
941             instance.save(expected_task_state=[task_states.MIGRATING])
942 
943     def _init_instance(self, context, instance):
944         """Initialize this instance during service init."""
945 
946         # NOTE(danms): If the instance appears to not be owned by this
947         # host, it may have been evacuated away, but skipped by the
948         # evacuation cleanup code due to configuration. Thus, if that
949         # is a possibility, don't touch the instance in any way, but
950         # log the concern. This will help avoid potential issues on
951         # startup due to misconfiguration.
952         if instance.host != self.host:
953             LOG.warning('Instance %(uuid)s appears to not be owned '
954                         'by this host, but by %(host)s. Startup '
955                         'processing is being skipped.',
956                         {'uuid': instance.uuid,
957                          'host': instance.host})
958             return
959 
960         # Instances that are shut down, or in an error state can not be
961         # initialized and are not attempted to be recovered. The exception
962         # to this are instances that are in RESIZE_MIGRATING or DELETING,
963         # which are dealt with further down.
964         if (instance.vm_state == vm_states.SOFT_DELETED or
965             (instance.vm_state == vm_states.ERROR and
966             instance.task_state not in
967             (task_states.RESIZE_MIGRATING, task_states.DELETING))):
968             LOG.debug("Instance is in %s state.",
969                       instance.vm_state, instance=instance)
970             return
971 
972         if instance.vm_state == vm_states.DELETED:
973             try:
974                 self._complete_partial_deletion(context, instance)
975             except Exception:
976                 # we don't want that an exception blocks the init_host
977                 LOG.exception('Failed to complete a deletion',
978                               instance=instance)
979             return
980 
981         if (instance.vm_state == vm_states.BUILDING or
982             instance.task_state in [task_states.SCHEDULING,
983                                     task_states.BLOCK_DEVICE_MAPPING,
984                                     task_states.NETWORKING,
985                                     task_states.SPAWNING]):
986             # NOTE(dave-mcnally) compute stopped before instance was fully
987             # spawned so set to ERROR state. This is safe to do as the state
988             # may be set by the api but the host is not so if we get here the
989             # instance has already been scheduled to this particular host.
990             LOG.debug("Instance failed to spawn correctly, "
991                       "setting to ERROR state", instance=instance)
992             self._set_instance_obj_error_state(instance, clean_task_state=True)
993             return
994 
995         if (instance.vm_state in [vm_states.ACTIVE, vm_states.STOPPED] and
996             instance.task_state in [task_states.REBUILDING,
997                                     task_states.REBUILD_BLOCK_DEVICE_MAPPING,
998                                     task_states.REBUILD_SPAWNING]):
999             # NOTE(jichenjc) compute stopped before instance was fully
1000             # spawned so set to ERROR state. This is consistent to BUILD
1001             LOG.debug("Instance failed to rebuild correctly, "
1002                       "setting to ERROR state", instance=instance)
1003             self._set_instance_obj_error_state(instance, clean_task_state=True)
1004             return
1005 
1006         if (instance.vm_state != vm_states.ERROR and
1007             instance.task_state in [task_states.IMAGE_SNAPSHOT_PENDING,
1008                                     task_states.IMAGE_PENDING_UPLOAD,
1009                                     task_states.IMAGE_UPLOADING,
1010                                     task_states.IMAGE_SNAPSHOT]):
1011             LOG.debug("Instance in transitional state %s at start-up "
1012                       "clearing task state",
1013                       instance.task_state, instance=instance)
1014             try:
1015                 self._post_interrupted_snapshot_cleanup(context, instance)
1016             except Exception:
1017                 # we don't want that an exception blocks the init_host
1018                 LOG.exception('Failed to cleanup snapshot.', instance=instance)
1019             instance.task_state = None
1020             instance.save()
1021 
1022         if (instance.vm_state != vm_states.ERROR and
1023             instance.task_state in [task_states.RESIZE_PREP]):
1024             LOG.debug("Instance in transitional state %s at start-up "
1025                       "clearing task state",
1026                       instance['task_state'], instance=instance)
1027             instance.task_state = None
1028             instance.save()
1029 
1030         if instance.task_state == task_states.DELETING:
1031             try:
1032                 LOG.info('Service started deleting the instance during '
1033                          'the previous run, but did not finish. Restarting'
1034                          ' the deletion now.', instance=instance)
1035                 instance.obj_load_attr('metadata')
1036                 instance.obj_load_attr('system_metadata')
1037                 bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
1038                         context, instance.uuid)
1039                 self._delete_instance(context, instance, bdms)
1040             except Exception:
1041                 # we don't want that an exception blocks the init_host
1042                 LOG.exception('Failed to complete a deletion',
1043                               instance=instance)
1044                 self._set_instance_obj_error_state(instance)
1045             return
1046 
1047         current_power_state = self._get_power_state(instance)
1048         try_reboot, reboot_type = self._retry_reboot(
1049             instance, current_power_state)
1050 
1051         if try_reboot:
1052             LOG.debug("Instance in transitional state (%(task_state)s) at "
1053                       "start-up and power state is (%(power_state)s), "
1054                       "triggering reboot",
1055                       {'task_state': instance.task_state,
1056                        'power_state': current_power_state},
1057                       instance=instance)
1058 
1059             # NOTE(mikal): if the instance was doing a soft reboot that got as
1060             # far as shutting down the instance but not as far as starting it
1061             # again, then we've just become a hard reboot. That means the
1062             # task state for the instance needs to change so that we're in one
1063             # of the expected task states for a hard reboot.
1064             if (instance.task_state in task_states.soft_reboot_states and
1065                 reboot_type == 'HARD'):
1066                 instance.task_state = task_states.REBOOT_PENDING_HARD
1067                 instance.save()
1068 
1069             self.reboot_instance(context, instance, block_device_info=None,
1070                                  reboot_type=reboot_type)
1071             return
1072 
1073         elif (current_power_state == power_state.RUNNING and
1074               instance.task_state in [task_states.REBOOT_STARTED,
1075                                       task_states.REBOOT_STARTED_HARD,
1076                                       task_states.PAUSING,
1077                                       task_states.UNPAUSING]):
1078             LOG.warning("Instance in transitional state "
1079                         "(%(task_state)s) at start-up and power state "
1080                         "is (%(power_state)s), clearing task state",
1081                         {'task_state': instance.task_state,
1082                          'power_state': current_power_state},
1083                         instance=instance)
1084             instance.task_state = None
1085             instance.vm_state = vm_states.ACTIVE
1086             instance.save()
1087         elif (current_power_state == power_state.PAUSED and
1088               instance.task_state == task_states.UNPAUSING):
1089             LOG.warning("Instance in transitional state "
1090                         "(%(task_state)s) at start-up and power state "
1091                         "is (%(power_state)s), clearing task state "
1092                         "and unpausing the instance",
1093                         {'task_state': instance.task_state,
1094                          'power_state': current_power_state},
1095                         instance=instance)
1096             try:
1097                 self.unpause_instance(context, instance)
1098             except NotImplementedError:
1099                 # Some virt driver didn't support pause and unpause
1100                 pass
1101             except Exception:
1102                 LOG.exception('Failed to unpause instance', instance=instance)
1103             return
1104 
1105         if instance.task_state == task_states.POWERING_OFF:
1106             try:
1107                 LOG.debug("Instance in transitional state %s at start-up "
1108                           "retrying stop request",
1109                           instance.task_state, instance=instance)
1110                 self.stop_instance(context, instance, True)
1111             except Exception:
1112                 # we don't want that an exception blocks the init_host
1113                 LOG.exception('Failed to stop instance', instance=instance)
1114             return
1115 
1116         if instance.task_state == task_states.POWERING_ON:
1117             try:
1118                 LOG.debug("Instance in transitional state %s at start-up "
1119                           "retrying start request",
1120                           instance.task_state, instance=instance)
1121                 self.start_instance(context, instance)
1122             except Exception:
1123                 # we don't want that an exception blocks the init_host
1124                 LOG.exception('Failed to start instance', instance=instance)
1125             return
1126 
1127         net_info = instance.get_network_info()
1128         try:
1129             self.driver.plug_vifs(instance, net_info)
1130         except NotImplementedError as e:
1131             LOG.debug(e, instance=instance)
1132         except exception.VirtualInterfacePlugException:
1133             # NOTE(mriedem): If we get here, it could be because the vif_type
1134             # in the cache is "binding_failed" or "unbound".
1135             # The periodic task _heal_instance_info_cache checks for this
1136             # condition. It should fix this by binding the ports again when
1137             # it gets to this instance.
1138             LOG.exception('Virtual interface plugging failed for instance. '
1139                           'The port binding:host_id may need to be manually '
1140                           'updated.', instance=instance)
1141             self._set_instance_obj_error_state(instance)
1142             return
1143 
1144         if instance.task_state == task_states.RESIZE_MIGRATING:
1145             # We crashed during resize/migration, so roll back for safety
1146             try:
1147                 # NOTE(mriedem): check old_vm_state for STOPPED here, if it's
1148                 # not in system_metadata we default to True for backwards
1149                 # compatibility
1150                 power_on = (instance.system_metadata.get('old_vm_state') !=
1151                             vm_states.STOPPED)
1152 
1153                 block_dev_info = self._get_instance_block_device_info(context,
1154                                                                       instance)
1155 
1156                 migration = objects.Migration.get_by_id_and_instance(
1157                     context, instance.migration_context.migration_id,
1158                     instance.uuid)
1159                 self.driver.finish_revert_migration(context, instance,
1160                     net_info, migration, block_dev_info, power_on)
1161 
1162             except Exception:
1163                 LOG.exception('Failed to revert crashed migration',
1164                               instance=instance)
1165             finally:
1166                 LOG.info('Instance found in migrating state during '
1167                          'startup. Resetting task_state',
1168                          instance=instance)
1169                 instance.task_state = None
1170                 instance.save()
1171         if instance.task_state == task_states.MIGRATING:
1172             # Live migration did not complete, but instance is on this
1173             # host. Abort ongoing migration if still running and reset state.
1174             self._reset_live_migration(context, instance)
1175 
1176         db_state = instance.power_state
1177         drv_state = self._get_power_state(instance)
1178         expect_running = (db_state == power_state.RUNNING and
1179                           drv_state != db_state)
1180 
1181         LOG.debug('Current state is %(drv_state)s, state in DB is '
1182                   '%(db_state)s.',
1183                   {'drv_state': drv_state, 'db_state': db_state},
1184                   instance=instance)
1185 
1186         if expect_running and CONF.resume_guests_state_on_host_boot:
1187             self._resume_guests_state(context, instance, net_info)
1188 
1189     def _resume_guests_state(self, context, instance, net_info):
1190         LOG.info('Rebooting instance after nova-compute restart.',
1191                  instance=instance)
1192         block_device_info = \
1193             self._get_instance_block_device_info(context, instance)
1194 
1195         try:
1196             self.driver.resume_state_on_host_boot(
1197                 context, instance, net_info, block_device_info)
1198         except NotImplementedError:
1199             LOG.warning('Hypervisor driver does not support '
1200                         'resume guests', instance=instance)
1201         except Exception:
1202             # NOTE(vish): The instance failed to resume, so we set the
1203             #             instance to error and attempt to continue.
1204             LOG.warning('Failed to resume instance',
1205                         instance=instance)
1206             self._set_instance_obj_error_state(instance)
1207 
1208     def _retry_reboot(self, instance, current_power_state):
1209         current_task_state = instance.task_state
1210         retry_reboot = False
1211         reboot_type = compute_utils.get_reboot_type(current_task_state,
1212                                                     current_power_state)
1213 
1214         pending_soft = (
1215             current_task_state == task_states.REBOOT_PENDING and
1216             instance.vm_state in vm_states.ALLOW_SOFT_REBOOT)
1217         pending_hard = (
1218             current_task_state == task_states.REBOOT_PENDING_HARD and
1219             instance.vm_state in vm_states.ALLOW_HARD_REBOOT)
1220         started_not_running = (current_task_state in
1221                                [task_states.REBOOT_STARTED,
1222                                 task_states.REBOOT_STARTED_HARD] and
1223                                current_power_state != power_state.RUNNING)
1224 
1225         if pending_soft or pending_hard or started_not_running:
1226             retry_reboot = True
1227 
1228         return retry_reboot, reboot_type
1229 
1230     def handle_lifecycle_event(self, event):
1231         LOG.info("VM %(state)s (Lifecycle Event)",
1232                  {'state': event.get_name()},
1233                  instance_uuid=event.get_instance_uuid())
1234         context = nova.context.get_admin_context(read_deleted='yes')
1235         vm_power_state = None
1236         event_transition = event.get_transition()
1237         if event_transition == virtevent.EVENT_LIFECYCLE_STOPPED:
1238             vm_power_state = power_state.SHUTDOWN
1239         elif event_transition == virtevent.EVENT_LIFECYCLE_STARTED:
1240             vm_power_state = power_state.RUNNING
1241         elif event_transition in (
1242                 virtevent.EVENT_LIFECYCLE_PAUSED,
1243                 virtevent.EVENT_LIFECYCLE_POSTCOPY_STARTED,
1244                 virtevent.EVENT_LIFECYCLE_MIGRATION_COMPLETED):
1245             vm_power_state = power_state.PAUSED
1246         elif event_transition == virtevent.EVENT_LIFECYCLE_RESUMED:
1247             vm_power_state = power_state.RUNNING
1248         elif event_transition == virtevent.EVENT_LIFECYCLE_SUSPENDED:
1249             vm_power_state = power_state.SUSPENDED
1250         else:
1251             LOG.warning("Unexpected lifecycle event: %d", event_transition)
1252 
1253         migrate_finish_statuses = {
1254             # This happens on the source node and indicates live migration
1255             # entered post-copy mode.
1256             virtevent.EVENT_LIFECYCLE_POSTCOPY_STARTED: 'running (post-copy)',
1257             # Suspended for offline migration.
1258             virtevent.EVENT_LIFECYCLE_MIGRATION_COMPLETED: 'running'
1259         }
1260 
1261         expected_attrs = []
1262         if event_transition in migrate_finish_statuses:
1263             # Join on info_cache since that's needed in migrate_instance_start.
1264             expected_attrs.append('info_cache')
1265         instance = objects.Instance.get_by_uuid(context,
1266                                                 event.get_instance_uuid(),
1267                                                 expected_attrs=expected_attrs)
1268 
1269         # Note(lpetrut): The event may be delayed, thus not reflecting
1270         # the current instance power state. In that case, ignore the event.
1271         current_power_state = self._get_power_state(instance)
1272         if current_power_state == vm_power_state:
1273             LOG.debug('Synchronizing instance power state after lifecycle '
1274                       'event "%(event)s"; current vm_state: %(vm_state)s, '
1275                       'current task_state: %(task_state)s, current DB '
1276                       'power_state: %(db_power_state)s, VM power_state: '
1277                       '%(vm_power_state)s',
1278                       {'event': event.get_name(),
1279                        'vm_state': instance.vm_state,
1280                        'task_state': instance.task_state,
1281                        'db_power_state': instance.power_state,
1282                        'vm_power_state': vm_power_state},
1283                       instance_uuid=instance.uuid)
1284             self._sync_instance_power_state(context,
1285                                             instance,
1286                                             vm_power_state)
1287 
1288         # The following checks are for live migration. We want to activate
1289         # the port binding for the destination host before the live migration
1290         # is resumed on the destination host in order to reduce network
1291         # downtime. Otherwise the ports are bound to the destination host
1292         # in post_live_migration_at_destination.
1293         # TODO(danms): Explore options for using a different live migration
1294         # specific callback for this instead of piggy-backing on the
1295         # handle_lifecycle_event callback.
1296         if (instance.task_state == task_states.MIGRATING and
1297                 event_transition in migrate_finish_statuses):
1298             status = migrate_finish_statuses[event_transition]
1299             try:
1300                 migration = objects.Migration.get_by_instance_and_status(
1301                             context, instance.uuid, status)
1302                 LOG.debug('Binding ports to destination host: %s',
1303                           migration.dest_compute, instance=instance)
1304                 # For neutron, migrate_instance_start will activate the
1305                 # destination host port bindings, if there are any created by
1306                 # conductor before live migration started.
1307                 self.network_api.migrate_instance_start(
1308                     context, instance, migration)
1309             except exception.MigrationNotFoundByStatus:
1310                 LOG.warning("Unable to find migration record with status "
1311                             "'%s' for instance. Port binding will happen in "
1312                             "post live migration.", status, instance=instance)
1313 
1314     def handle_events(self, event):
1315         if isinstance(event, virtevent.LifecycleEvent):
1316             try:
1317                 self.handle_lifecycle_event(event)
1318             except exception.InstanceNotFound:
1319                 LOG.debug("Event %s arrived for non-existent instance. The "
1320                           "instance was probably deleted.", event)
1321         else:
1322             LOG.debug("Ignoring event %s", event)
1323 
1324     def init_virt_events(self):
1325         if CONF.workarounds.handle_virt_lifecycle_events:
1326             self.driver.register_event_listener(self.handle_events)
1327         else:
1328             # NOTE(mriedem): If the _sync_power_states periodic task is
1329             # disabled we should emit a warning in the logs.
1330             if CONF.sync_power_state_interval < 0:
1331                 LOG.warning('Instance lifecycle events from the compute '
1332                             'driver have been disabled. Note that lifecycle '
1333                             'changes to an instance outside of the compute '
1334                             'service will not be synchronized '
1335                             'automatically since the _sync_power_states '
1336                             'periodic task is also disabled.')
1337             else:
1338                 LOG.info('Instance lifecycle events from the compute '
1339                          'driver have been disabled. Note that lifecycle '
1340                          'changes to an instance outside of the compute '
1341                          'service will only be synchronized by the '
1342                          '_sync_power_states periodic task.')
1343 
1344     def _get_nodes(self, context):
1345         """Queried the ComputeNode objects from the DB that are reported by the
1346         hypervisor.
1347 
1348         :param context: the request context
1349         :return: a dict of ComputeNode objects keyed by the UUID of the given
1350             node.
1351         """
1352         nodes_by_uuid = {}
1353         try:
1354             node_names = self.driver.get_available_nodes()
1355         except exception.VirtDriverNotReady:
1356             LOG.warning(
1357                 "Virt driver is not ready. If this is the first time this "
1358                 "service is starting on this host, then you can ignore this "
1359                 "warning.")
1360             return {}
1361 
1362         for node_name in node_names:
1363             try:
1364                 node = objects.ComputeNode.get_by_host_and_nodename(
1365                     context, self.host, node_name)
1366                 nodes_by_uuid[node.uuid] = node
1367             except exception.ComputeHostNotFound:
1368                 LOG.warning(
1369                     "Compute node %s not found in the database. If this is "
1370                     "the first time this service is starting on this host, "
1371                     "then you can ignore this warning.", node_name)
1372         return nodes_by_uuid
1373 
1374     def init_host(self):
1375         """Initialization for a standalone compute service."""
1376 
1377         if CONF.pci.passthrough_whitelist:
1378             # Simply loading the PCI passthrough whitelist will do a bunch of
1379             # validation that would otherwise wait until the PciDevTracker is
1380             # constructed when updating available resources for the compute
1381             # node(s) in the resource tracker, effectively killing that task.
1382             # So load up the whitelist when starting the compute service to
1383             # flush any invalid configuration early so we can kill the service
1384             # if the configuration is wrong.
1385             whitelist.Whitelist(CONF.pci.passthrough_whitelist)
1386 
1387         nova.conf.neutron.register_dynamic_opts(CONF)
1388         # Even if only libvirt uses them, make it available for all drivers
1389         nova.conf.devices.register_dynamic_opts(CONF)
1390 
1391         # Override the number of concurrent disk operations allowed if the
1392         # user has specified a limit.
1393         if CONF.compute.max_concurrent_disk_ops != 0:
1394             compute_utils.disk_ops_semaphore = \
1395                 eventlet.semaphore.BoundedSemaphore(
1396                     CONF.compute.max_concurrent_disk_ops)
1397 
1398         self.driver.init_host(host=self.host)
1399         context = nova.context.get_admin_context()
1400         instances = objects.InstanceList.get_by_host(
1401             context, self.host,
1402             expected_attrs=['info_cache', 'metadata', 'numa_topology'])
1403 
1404         self.init_virt_events()
1405 
1406         self._validate_pinning_configuration(instances)
1407         self._validate_vtpm_configuration(instances)
1408 
1409         # NOTE(gibi): At this point the compute_nodes of the resource tracker
1410         # has not been populated yet so we cannot rely on the resource tracker
1411         # here.
1412         # NOTE(gibi): If ironic and vcenter virt driver slow start time
1413         # becomes problematic here then we should consider adding a config
1414         # option or a driver flag to tell us if we should thread
1415         # _destroy_evacuated_instances and
1416         # _error_out_instances_whose_build_was_interrupted out in the
1417         # background on startup
1418         nodes_by_uuid = self._get_nodes(context)
1419 
1420         try:
1421             # checking that instance was not already evacuated to other host
1422             evacuated_instances = self._destroy_evacuated_instances(
1423                 context, nodes_by_uuid)
1424 
1425             # Initialise instances on the host that are not evacuating
1426             for instance in instances:
1427                 if instance.uuid not in evacuated_instances:
1428                     self._init_instance(context, instance)
1429 
1430             # NOTE(gibi): collect all the instance uuids that is in some way
1431             # was already handled above. Either by init_instance or by
1432             # _destroy_evacuated_instances. This way we can limit the scope of
1433             # the _error_out_instances_whose_build_was_interrupted call to look
1434             # only for instances that have allocations on this node and not
1435             # handled by the above calls.
1436             already_handled = {instance.uuid for instance in instances}.union(
1437                 evacuated_instances)
1438             self._error_out_instances_whose_build_was_interrupted(
1439                 context, already_handled, nodes_by_uuid.keys())
1440 
1441         finally:
1442             if instances:
1443                 # We only send the instance info to the scheduler on startup
1444                 # if there is anything to send, otherwise this host might
1445                 # not be mapped yet in a cell and the scheduler may have
1446                 # issues dealing with the information. Later changes to
1447                 # instances on this host will update the scheduler, or the
1448                 # _sync_scheduler_instance_info periodic task will.
1449                 self._update_scheduler_instance_info(context, instances)
1450 
1451     def _error_out_instances_whose_build_was_interrupted(
1452             self, context, already_handled_instances, node_uuids):
1453         """If there are instances in BUILDING state that are not
1454         assigned to this host but have allocations in placement towards
1455         this compute that means the nova-compute service was
1456         restarted while those instances waited for the resource claim
1457         to finish and the _set_instance_host_and_node() to update the
1458         instance.host field. We need to push them to ERROR state here to
1459         prevent keeping them in BUILDING state forever.
1460 
1461         :param context: The request context
1462         :param already_handled_instances: The set of instance UUIDs that the
1463             host initialization process already handled in some way.
1464         :param node_uuids: The list of compute node uuids handled by this
1465             service
1466         """
1467 
1468         # Strategy:
1469         # 1) Get the allocations from placement for our compute node(s)
1470         # 2) Remove the already handled instances from the consumer list;
1471         #    they are either already initialized or need to be skipped.
1472         # 3) Check which remaining consumer is an instance in BUILDING state
1473         #    and push it to ERROR state.
1474 
1475         LOG.info(
1476             "Looking for unclaimed instances stuck in BUILDING status for "
1477             "nodes managed by this host")
1478         for cn_uuid in node_uuids:
1479             try:
1480                 f = self.reportclient.get_allocations_for_resource_provider
1481                 allocations = f(context, cn_uuid).allocations
1482             except (exception.ResourceProviderAllocationRetrievalFailed,
1483                     keystone_exception.ClientException) as e:
1484                 LOG.error(
1485                     "Could not retrieve compute node resource provider %s and "
1486                     "therefore unable to error out any instances stuck in "
1487                     "BUILDING state. Error: %s", cn_uuid, six.text_type(e))
1488                 continue
1489 
1490             not_handled_consumers = (set(allocations) -
1491                                      already_handled_instances)
1492 
1493             if not not_handled_consumers:
1494                 continue
1495 
1496             filters = {
1497                 'vm_state': vm_states.BUILDING,
1498                 'uuid': not_handled_consumers
1499             }
1500 
1501             instances = objects.InstanceList.get_by_filters(
1502                 context, filters, expected_attrs=[])
1503 
1504             for instance in instances:
1505                 LOG.debug(
1506                     "Instance spawn was interrupted before instance_claim, "
1507                     "setting instance to ERROR state", instance=instance)
1508                 self._set_instance_obj_error_state(
1509                     instance, clean_task_state=True)
1510 
1511     def cleanup_host(self):
1512         self.driver.register_event_listener(None)
1513         self.instance_events.cancel_all_events()
1514         self.driver.cleanup_host(host=self.host)
1515         self._cleanup_live_migrations_in_pool()
1516 
1517     def _cleanup_live_migrations_in_pool(self):
1518         # Shutdown the pool so we don't get new requests.
1519         self._live_migration_executor.shutdown(wait=False)
1520         # For any queued migrations, cancel the migration and update
1521         # its status.
1522         for migration, future in self._waiting_live_migrations.values():
1523             # If we got here before the Future was submitted then we need
1524             # to move on since there isn't anything we can do.
1525             if future is None:
1526                 continue
1527             if future.cancel():
1528                 self._set_migration_status(migration, 'cancelled')
1529                 LOG.info('Successfully cancelled queued live migration.',
1530                          instance_uuid=migration.instance_uuid)
1531             else:
1532                 LOG.warning('Unable to cancel live migration.',
1533                             instance_uuid=migration.instance_uuid)
1534         self._waiting_live_migrations.clear()
1535 
1536     def pre_start_hook(self):
1537         """After the service is initialized, but before we fully bring
1538         the service up by listening on RPC queues, make sure to update
1539         our available resources (and indirectly our available nodes).
1540         """
1541         self.update_available_resource(nova.context.get_admin_context(),
1542                                        startup=True)
1543 
1544     def _get_power_state(self, instance):
1545         """Retrieve the power state for the given instance."""
1546         LOG.debug('Checking state', instance=instance)
1547         try:
1548             return self.driver.get_info(instance, use_cache=False).state
1549         except exception.InstanceNotFound:
1550             return power_state.NOSTATE
1551 
1552     # TODO(stephenfin): Remove this once we bump the compute API to v6.0
1553     def get_console_topic(self, context):
1554         """Retrieves the console host for a project on this host.
1555 
1556         Currently this is just set in the flags for each compute host.
1557 
1558         """
1559         # TODO(mdragon): perhaps make this variable by console_type?
1560         return 'console.%s' % CONF.console_host
1561 
1562     # TODO(stephenfin): Remove this once we bump the compute API to v6.0
1563     @wrap_exception()
1564     def get_console_pool_info(self, context, console_type):
1565         return self.driver.get_console_pool_info(console_type)
1566 
1567     # TODO(stephenfin): Remove this as it's nova-network only
1568     @wrap_exception()
1569     def refresh_instance_security_rules(self, context, instance):
1570         """Tell the virtualization driver to refresh security rules for
1571         an instance.
1572 
1573         Passes straight through to the virtualization driver.
1574 
1575         Synchronize the call because we may still be in the middle of
1576         creating the instance.
1577         """
1578         pass
1579 
1580     def _await_block_device_map_created(self, context, vol_id):
1581         # TODO(yamahata): creating volume simultaneously
1582         #                 reduces creation time?
1583         # TODO(yamahata): eliminate dumb polling
1584         start = time.time()
1585         retries = CONF.block_device_allocate_retries
1586         # (1) if the configured value is 0, one attempt should be made
1587         # (2) if the configured value is > 0, then the total number attempts
1588         #      is (retries + 1)
1589         attempts = 1
1590         if retries >= 1:
1591             attempts = retries + 1
1592         for attempt in range(1, attempts + 1):
1593             volume = self.volume_api.get(context, vol_id)
1594             volume_status = volume['status']
1595             if volume_status not in ['creating', 'downloading']:
1596                 if volume_status == 'available':
1597                     return attempt
1598                 LOG.warning("Volume id: %(vol_id)s finished being "
1599                             "created but its status is %(vol_status)s.",
1600                             {'vol_id': vol_id,
1601                              'vol_status': volume_status})
1602                 break
1603             greenthread.sleep(CONF.block_device_allocate_retries_interval)
1604         raise exception.VolumeNotCreated(volume_id=vol_id,
1605                                          seconds=int(time.time() - start),
1606                                          attempts=attempt,
1607                                          volume_status=volume_status)
1608 
1609     def _decode_files(self, injected_files):
1610         """Base64 decode the list of files to inject."""
1611         if not injected_files:
1612             return []
1613 
1614         def _decode(f):
1615             path, contents = f
1616             # Py3 raises binascii.Error instead of TypeError as in Py27
1617             try:
1618                 decoded = base64.b64decode(contents)
1619                 return path, decoded
1620             except (TypeError, binascii.Error):
1621                 raise exception.Base64Exception(path=path)
1622 
1623         return [_decode(f) for f in injected_files]
1624 
1625     def _validate_instance_group_policy(self, context, instance,
1626                                         scheduler_hints):
1627         # NOTE(russellb) Instance group policy is enforced by the scheduler.
1628         # However, there is a race condition with the enforcement of
1629         # the policy.  Since more than one instance may be scheduled at the
1630         # same time, it's possible that more than one instance with an
1631         # anti-affinity policy may end up here.  It's also possible that
1632         # multiple instances with an affinity policy could end up on different
1633         # hosts.  This is a validation step to make sure that starting the
1634         # instance here doesn't violate the policy.
1635         group_hint = scheduler_hints.get('group')
1636         if not group_hint:
1637             return
1638 
1639         # The RequestSpec stores scheduler_hints as key=list pairs so we need
1640         # to check the type on the value and pull the single entry out. The
1641         # API request schema validates that the 'group' hint is a single value.
1642         if isinstance(group_hint, list):
1643             group_hint = group_hint[0]
1644 
1645         @utils.synchronized(group_hint)
1646         def _do_validation(context, instance, group_hint):
1647             group = objects.InstanceGroup.get_by_hint(context, group_hint)
1648             if group.policy and 'anti-affinity' == group.policy:
1649                 instances_uuids = objects.InstanceList.get_uuids_by_host(
1650                     context, self.host)
1651                 ins_on_host = set(instances_uuids)
1652                 members = set(group.members)
1653                 # Determine the set of instance group members on this host
1654                 # which are not the instance in question. This is used to
1655                 # determine how many other members from the same anti-affinity
1656                 # group can be on this host.
1657                 members_on_host = ins_on_host & members - set([instance.uuid])
1658                 rules = group.rules
1659                 if rules and 'max_server_per_host' in rules:
1660                     max_server = rules['max_server_per_host']
1661                 else:
1662                     max_server = 1
1663                 if len(members_on_host) >= max_server:
1664                     msg = _("Anti-affinity instance group policy "
1665                             "was violated.")
1666                     raise exception.RescheduledException(
1667                             instance_uuid=instance.uuid,
1668                             reason=msg)
1669             elif group.policy and 'affinity' == group.policy:
1670                 group_hosts = group.get_hosts(exclude=[instance.uuid])
1671                 if group_hosts and self.host not in group_hosts:
1672                     msg = _("Affinity instance group policy was violated.")
1673                     raise exception.RescheduledException(
1674                             instance_uuid=instance.uuid,
1675                             reason=msg)
1676 
1677         if not CONF.workarounds.disable_group_policy_check_upcall:
1678             _do_validation(context, instance, group_hint)
1679 
1680     def _log_original_error(self, exc_info, instance_uuid):
1681         LOG.error('Error: %s', exc_info[1], instance_uuid=instance_uuid,
1682                   exc_info=exc_info)
1683 
1684     @periodic_task.periodic_task
1685     def _check_instance_build_time(self, context):
1686         """Ensure that instances are not stuck in build."""
1687         timeout = CONF.instance_build_timeout
1688         if timeout == 0:
1689             return
1690 
1691         filters = {'vm_state': vm_states.BUILDING,
1692                    'host': self.host}
1693 
1694         building_insts = objects.InstanceList.get_by_filters(context,
1695                            filters, expected_attrs=[], use_slave=True)
1696 
1697         for instance in building_insts:
1698             if timeutils.is_older_than(instance.created_at, timeout):
1699                 self._set_instance_obj_error_state(instance)
1700                 LOG.warning("Instance build timed out. Set to error "
1701                             "state.", instance=instance)
1702 
1703     def _check_instance_exists(self, instance):
1704         """Ensure an instance with the same name is not already present."""
1705         if self.driver.instance_exists(instance):
1706             raise exception.InstanceExists(name=instance.name)
1707 
1708     def _allocate_network_async(self, context, instance, requested_networks,
1709                                 security_groups, is_vpn,
1710                                 resource_provider_mapping):
1711         """Method used to allocate networks in the background.
1712 
1713         Broken out for testing.
1714         """
1715         # First check to see if we're specifically not supposed to allocate
1716         # networks because if so, we can exit early.
1717         if requested_networks and requested_networks.no_allocate:
1718             LOG.debug("Not allocating networking since 'none' was specified.",
1719                       instance=instance)
1720             return network_model.NetworkInfo([])
1721 
1722         LOG.debug("Allocating IP information in the background.",
1723                   instance=instance)
1724         retries = CONF.network_allocate_retries
1725         attempts = retries + 1
1726         retry_time = 1
1727         bind_host_id = self.driver.network_binding_host_id(context, instance)
1728         for attempt in range(1, attempts + 1):
1729             try:
1730                 nwinfo = self.network_api.allocate_for_instance(
1731                         context, instance, vpn=is_vpn,
1732                         requested_networks=requested_networks,
1733                         security_groups=security_groups,
1734                         bind_host_id=bind_host_id,
1735                         resource_provider_mapping=resource_provider_mapping)
1736                 LOG.debug('Instance network_info: |%s|', nwinfo,
1737                           instance=instance)
1738                 instance.system_metadata['network_allocated'] = 'True'
1739                 # NOTE(JoshNang) do not save the instance here, as it can cause
1740                 # races. The caller shares a reference to instance and waits
1741                 # for this async greenthread to finish before calling
1742                 # instance.save().
1743                 return nwinfo
1744             except Exception:
1745                 exc_info = sys.exc_info()
1746                 log_info = {'attempt': attempt,
1747                             'attempts': attempts}
1748                 if attempt == attempts:
1749                     LOG.exception('Instance failed network setup '
1750                                   'after %(attempts)d attempt(s)',
1751                                   log_info)
1752                     six.reraise(*exc_info)
1753                 LOG.warning('Instance failed network setup '
1754                             '(attempt %(attempt)d of %(attempts)d)',
1755                             log_info, instance=instance)
1756                 time.sleep(retry_time)
1757                 retry_time *= 2
1758                 if retry_time > 30:
1759                     retry_time = 30
1760         # Not reached.
1761 
1762     def _build_networks_for_instance(self, context, instance,
1763             requested_networks, security_groups, resource_provider_mapping):
1764 
1765         # If we're here from a reschedule the network may already be allocated.
1766         if strutils.bool_from_string(
1767                 instance.system_metadata.get('network_allocated', 'False')):
1768             # NOTE(alex_xu): The network_allocated is True means the network
1769             # resource already allocated at previous scheduling, and the
1770             # network setup is cleanup at previous. After rescheduling, the
1771             # network resource need setup on the new host.
1772             self.network_api.setup_instance_network_on_host(
1773                 context, instance, instance.host)
1774             return self.network_api.get_instance_nw_info(context, instance)
1775 
1776         network_info = self._allocate_network(context, instance,
1777                 requested_networks, security_groups,
1778                 resource_provider_mapping)
1779 
1780         return network_info
1781 
1782     def _allocate_network(self, context, instance, requested_networks,
1783                           security_groups, resource_provider_mapping):
1784         """Start network allocation asynchronously.  Return an instance
1785         of NetworkInfoAsyncWrapper that can be used to retrieve the
1786         allocated networks when the operation has finished.
1787         """
1788         # NOTE(comstud): Since we're allocating networks asynchronously,
1789         # this task state has little meaning, as we won't be in this
1790         # state for very long.
1791         instance.vm_state = vm_states.BUILDING
1792         instance.task_state = task_states.NETWORKING
1793         instance.save(expected_task_state=[None])
1794 
1795         is_vpn = False
1796         return network_model.NetworkInfoAsyncWrapper(
1797                 self._allocate_network_async, context, instance,
1798                 requested_networks, security_groups, is_vpn,
1799                 resource_provider_mapping)
1800 
1801     def _default_root_device_name(self, instance, image_meta, root_bdm):
1802         """Gets a default root device name from the driver.
1803 
1804         :param nova.objects.Instance instance:
1805             The instance for which to get the root device name.
1806         :param nova.objects.ImageMeta image_meta:
1807             The metadata of the image of the instance.
1808         :param nova.objects.BlockDeviceMapping root_bdm:
1809             The description of the root device.
1810         :returns: str -- The default root device name.
1811         :raises: InternalError, TooManyDiskDevices
1812         """
1813         try:
1814             return self.driver.default_root_device_name(instance,
1815                                                         image_meta,
1816                                                         root_bdm)
1817         except NotImplementedError:
1818             return compute_utils.get_next_device_name(instance, [])
1819 
1820     def _default_device_names_for_instance(self, instance,
1821                                            root_device_name,
1822                                            *block_device_lists):
1823         """Default the missing device names in the BDM from the driver.
1824 
1825         :param nova.objects.Instance instance:
1826             The instance for which to get default device names.
1827         :param str root_device_name: The root device name.
1828         :param list block_device_lists: List of block device mappings.
1829         :returns: None
1830         :raises: InternalError, TooManyDiskDevices
1831         """
1832         try:
1833             self.driver.default_device_names_for_instance(instance,
1834                                                           root_device_name,
1835                                                           *block_device_lists)
1836         except NotImplementedError:
1837             compute_utils.default_device_names_for_instance(
1838                 instance, root_device_name, *block_device_lists)
1839 
1840     def _get_device_name_for_instance(self, instance, bdms, block_device_obj):
1841         """Get the next device name from the driver, based on the BDM.
1842 
1843         :param nova.objects.Instance instance:
1844             The instance whose volume is requesting a device name.
1845         :param nova.objects.BlockDeviceMappingList bdms:
1846             The block device mappings for the instance.
1847         :param nova.objects.BlockDeviceMapping block_device_obj:
1848             A block device mapping containing info about the requested block
1849             device.
1850         :returns: The next device name.
1851         :raises: InternalError, TooManyDiskDevices
1852         """
1853         # NOTE(ndipanov): Copy obj to avoid changing the original
1854         block_device_obj = block_device_obj.obj_clone()
1855         try:
1856             return self.driver.get_device_name_for_instance(
1857                 instance, bdms, block_device_obj)
1858         except NotImplementedError:
1859             return compute_utils.get_device_name_for_instance(
1860                 instance, bdms, block_device_obj.get("device_name"))
1861 
1862     def _default_block_device_names(self, instance, image_meta, block_devices):
1863         """Verify that all the devices have the device_name set. If not,
1864         provide a default name.
1865 
1866         It also ensures that there is a root_device_name and is set to the
1867         first block device in the boot sequence (boot_index=0).
1868         """
1869         root_bdm = block_device.get_root_bdm(block_devices)
1870         if not root_bdm:
1871             return
1872 
1873         # Get the root_device_name from the root BDM or the instance
1874         root_device_name = None
1875         update_root_bdm = False
1876 
1877         if root_bdm.device_name:
1878             root_device_name = root_bdm.device_name
1879             instance.root_device_name = root_device_name
1880         elif instance.root_device_name:
1881             root_device_name = instance.root_device_name
1882             root_bdm.device_name = root_device_name
1883             update_root_bdm = True
1884         else:
1885             root_device_name = self._default_root_device_name(instance,
1886                                                               image_meta,
1887                                                               root_bdm)
1888 
1889             instance.root_device_name = root_device_name
1890             root_bdm.device_name = root_device_name
1891             update_root_bdm = True
1892 
1893         if update_root_bdm:
1894             root_bdm.save()
1895 
1896         ephemerals = list(filter(block_device.new_format_is_ephemeral,
1897                             block_devices))
1898         swap = list(filter(block_device.new_format_is_swap,
1899                       block_devices))
1900         block_device_mapping = list(filter(
1901               driver_block_device.is_block_device_mapping, block_devices))
1902 
1903         self._default_device_names_for_instance(instance,
1904                                                 root_device_name,
1905                                                 ephemerals,
1906                                                 swap,
1907                                                 block_device_mapping)
1908 
1909     def _block_device_info_to_legacy(self, block_device_info):
1910         """Convert BDI to the old format for drivers that need it."""
1911 
1912         if self.use_legacy_block_device_info:
1913             ephemerals = driver_block_device.legacy_block_devices(
1914                 driver.block_device_info_get_ephemerals(block_device_info))
1915             mapping = driver_block_device.legacy_block_devices(
1916                 driver.block_device_info_get_mapping(block_device_info))
1917             swap = block_device_info['swap']
1918             if swap:
1919                 swap = swap.legacy()
1920 
1921             block_device_info.update({
1922                 'ephemerals': ephemerals,
1923                 'swap': swap,
1924                 'block_device_mapping': mapping})
1925 
1926     def _add_missing_dev_names(self, bdms, instance):
1927         for bdm in bdms:
1928             if bdm.device_name is not None:
1929                 continue
1930 
1931             device_name = self._get_device_name_for_instance(instance,
1932                                                              bdms, bdm)
1933             values = {'device_name': device_name}
1934             bdm.update(values)
1935             bdm.save()
1936 
1937     def _prep_block_device(self, context, instance, bdms):
1938         """Set up the block device for an instance with error logging."""
1939         try:
1940             self._add_missing_dev_names(bdms, instance)
1941             block_device_info = driver.get_block_device_info(instance, bdms)
1942             mapping = driver.block_device_info_get_mapping(block_device_info)
1943             driver_block_device.attach_block_devices(
1944                 mapping, context, instance, self.volume_api, self.driver,
1945                 wait_func=self._await_block_device_map_created)
1946 
1947             self._block_device_info_to_legacy(block_device_info)
1948             return block_device_info
1949 
1950         except exception.OverQuota as e:
1951             LOG.warning('Failed to create block device for instance due'
1952                         ' to exceeding volume related resource quota.'
1953                         ' Error: %s', e.message, instance=instance)
1954             raise
1955 
1956         except Exception as ex:
1957             LOG.exception('Instance failed block device setup',
1958                           instance=instance)
1959             # InvalidBDM will eventually result in a BuildAbortException when
1960             # booting from volume, and will be recorded as an instance fault.
1961             # Maintain the original exception message which most likely has
1962             # useful details which the standard InvalidBDM error message lacks.
1963             raise exception.InvalidBDM(six.text_type(ex))
1964 
1965     def _update_instance_after_spawn(self, instance,
1966                                      vm_state=vm_states.ACTIVE):
1967         instance.power_state = self._get_power_state(instance)
1968         instance.vm_state = vm_state
1969         instance.task_state = None
1970         # NOTE(sean-k-mooney): configdrive.update_instance checks
1971         # instance.launched_at to determine if it is the first or
1972         # subsequent spawn of an instance. We need to call update_instance
1973         # first before setting instance.launched_at or instance.config_drive
1974         # will never be set to true based on the value of force_config_drive.
1975         # As a result the config drive will be lost on a hard reboot of the
1976         # instance even when force_config_drive=true. see bug #1835822.
1977         configdrive.update_instance(instance)
1978         instance.launched_at = timeutils.utcnow()
1979 
1980     def _update_scheduler_instance_info(self, context, instance):
1981         """Sends an InstanceList with created or updated Instance objects to
1982         the Scheduler client.
1983 
1984         In the case of init_host, the value passed will already be an
1985         InstanceList. Other calls will send individual Instance objects that
1986         have been created or resized. In this case, we create an InstanceList
1987         object containing that Instance.
1988         """
1989         if not self.send_instance_updates:
1990             return
1991         if isinstance(instance, obj_instance.Instance):
1992             instance = objects.InstanceList(objects=[instance])
1993         context = context.elevated()
1994         self.query_client.update_instance_info(context, self.host,
1995                                                instance)
1996 
1997     def _delete_scheduler_instance_info(self, context, instance_uuid):
1998         """Sends the uuid of the deleted Instance to the Scheduler client."""
1999         if not self.send_instance_updates:
2000             return
2001         context = context.elevated()
2002         self.query_client.delete_instance_info(context, self.host,
2003                                                instance_uuid)
2004 
2005     @periodic_task.periodic_task(spacing=CONF.scheduler_instance_sync_interval)
2006     def _sync_scheduler_instance_info(self, context):
2007         if not self.send_instance_updates:
2008             return
2009         context = context.elevated()
2010         instances = objects.InstanceList.get_by_host(context, self.host,
2011                                                      expected_attrs=[],
2012                                                      use_slave=True)
2013         uuids = [instance.uuid for instance in instances]
2014         self.query_client.sync_instance_info(context, self.host, uuids)
2015 
2016     def _notify_about_instance_usage(self, context, instance, event_suffix,
2017                                      network_info=None, extra_usage_info=None,
2018                                      fault=None):
2019         compute_utils.notify_about_instance_usage(
2020             self.notifier, context, instance, event_suffix,
2021             network_info=network_info,
2022             extra_usage_info=extra_usage_info, fault=fault)
2023 
2024     def _deallocate_network(self, context, instance,
2025                             requested_networks=None):
2026         # If we were told not to allocate networks let's save ourselves
2027         # the trouble of calling the network API.
2028         if requested_networks and requested_networks.no_allocate:
2029             LOG.debug("Skipping network deallocation for instance since "
2030                       "networking was not requested.", instance=instance)
2031             return
2032 
2033         LOG.debug('Deallocating network for instance', instance=instance)
2034         with timeutils.StopWatch() as timer:
2035             self.network_api.deallocate_for_instance(
2036                 context, instance, requested_networks=requested_networks)
2037         # nova-network does an rpc call so we're OK tracking time spent here
2038         LOG.info('Took %0.2f seconds to deallocate network for instance.',
2039                  timer.elapsed(), instance=instance)
2040 
2041     def _get_instance_block_device_info(self, context, instance,
2042                                         refresh_conn_info=False,
2043                                         bdms=None):
2044         """Transform block devices to the driver block_device format."""
2045 
2046         if bdms is None:
2047             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
2048                     context, instance.uuid)
2049         block_device_info = driver.get_block_device_info(instance, bdms)
2050 
2051         if not refresh_conn_info:
2052             # if the block_device_mapping has no value in connection_info
2053             # (returned as None), don't include in the mapping
2054             block_device_info['block_device_mapping'] = [
2055                 bdm for bdm in driver.block_device_info_get_mapping(
2056                                     block_device_info)
2057                 if bdm.get('connection_info')]
2058         else:
2059             driver_block_device.refresh_conn_infos(
2060                 driver.block_device_info_get_mapping(block_device_info),
2061                 context, instance, self.volume_api, self.driver)
2062 
2063         self._block_device_info_to_legacy(block_device_info)
2064 
2065         return block_device_info
2066 
2067     def _build_failed(self, node):
2068         if CONF.compute.consecutive_build_service_disable_threshold:
2069             # NOTE(danms): Update our counter, but wait for the next
2070             # update_available_resource() periodic to flush it to the DB
2071             self.rt.build_failed(node)
2072 
2073     def _build_succeeded(self, node):
2074         self.rt.build_succeeded(node)
2075 
2076     @wrap_exception()
2077     @reverts_task_state
2078     @wrap_instance_fault
2079     def build_and_run_instance(self, context, instance, image, request_spec,
2080                      filter_properties, admin_password=None,
2081                      injected_files=None, requested_networks=None,
2082                      security_groups=None, block_device_mapping=None,
2083                      node=None, limits=None, host_list=None, accel_uuids=None):
2084 
2085         @utils.synchronized(instance.uuid)
2086         def _locked_do_build_and_run_instance(*args, **kwargs):
2087             # NOTE(danms): We grab the semaphore with the instance uuid
2088             # locked because we could wait in line to build this instance
2089             # for a while and we want to make sure that nothing else tries
2090             # to do anything with this instance while we wait.
2091             with self._build_semaphore:
2092                 try:
2093                     result = self._do_build_and_run_instance(*args, **kwargs)
2094                 except Exception:
2095                     # NOTE(mriedem): This should really only happen if
2096                     # _decode_files in _do_build_and_run_instance fails, and
2097                     # that's before a guest is spawned so it's OK to remove
2098                     # allocations for the instance for this node from Placement
2099                     # below as there is no guest consuming resources anyway.
2100                     # The _decode_files case could be handled more specifically
2101                     # but that's left for another day.
2102                     result = build_results.FAILED
2103                     raise
2104                 finally:
2105                     if result == build_results.FAILED:
2106                         # Remove the allocation records from Placement for the
2107                         # instance if the build failed. The instance.host is
2108                         # likely set to None in _do_build_and_run_instance
2109                         # which means if the user deletes the instance, it
2110                         # will be deleted in the API, not the compute service.
2111                         # Setting the instance.host to None in
2112                         # _do_build_and_run_instance means that the
2113                         # ResourceTracker will no longer consider this instance
2114                         # to be claiming resources against it, so we want to
2115                         # reflect that same thing in Placement.  No need to
2116                         # call this for a reschedule, as the allocations will
2117                         # have already been removed in
2118                         # self._do_build_and_run_instance().
2119                         self.reportclient.delete_allocation_for_instance(
2120                             context, instance.uuid)
2121 
2122                     if result in (build_results.FAILED,
2123                                   build_results.RESCHEDULED):
2124                         self._build_failed(node)
2125                     else:
2126                         self._build_succeeded(node)
2127 
2128         # NOTE(danms): We spawn here to return the RPC worker thread back to
2129         # the pool. Since what follows could take a really long time, we don't
2130         # want to tie up RPC workers.
2131         utils.spawn_n(_locked_do_build_and_run_instance,
2132                       context, instance, image, request_spec,
2133                       filter_properties, admin_password, injected_files,
2134                       requested_networks, security_groups,
2135                       block_device_mapping, node, limits, host_list,
2136                       accel_uuids)
2137 
2138     def _check_device_tagging(self, requested_networks, block_device_mapping):
2139         tagging_requested = False
2140         if requested_networks:
2141             for net in requested_networks:
2142                 if 'tag' in net and net.tag is not None:
2143                     tagging_requested = True
2144                     break
2145         if block_device_mapping and not tagging_requested:
2146             for bdm in block_device_mapping:
2147                 if 'tag' in bdm and bdm.tag is not None:
2148                     tagging_requested = True
2149                     break
2150         if (tagging_requested and
2151                 not self.driver.capabilities.get('supports_device_tagging',
2152                                                  False)):
2153             raise exception.BuildAbortException('Attempt to boot guest with '
2154                                                 'tagged devices on host that '
2155                                                 'does not support tagging.')
2156 
2157     def _check_trusted_certs(self, instance):
2158         if (instance.trusted_certs and
2159                 not self.driver.capabilities.get('supports_trusted_certs',
2160                                                  False)):
2161             raise exception.BuildAbortException(
2162                 'Trusted image certificates provided on host that does not '
2163                 'support certificate validation.')
2164 
2165     @wrap_exception()
2166     @reverts_task_state
2167     @wrap_instance_event(prefix='compute')
2168     @wrap_instance_fault
2169     def _do_build_and_run_instance(self, context, instance, image,
2170             request_spec, filter_properties, admin_password, injected_files,
2171             requested_networks, security_groups, block_device_mapping,
2172             node=None, limits=None, host_list=None, accel_uuids=None):
2173 
2174         try:
2175             LOG.debug('Starting instance...', instance=instance)
2176             instance.vm_state = vm_states.BUILDING
2177             instance.task_state = None
2178             instance.save(expected_task_state=
2179                     (task_states.SCHEDULING, None))
2180         except exception.InstanceNotFound:
2181             msg = 'Instance disappeared before build.'
2182             LOG.debug(msg, instance=instance)
2183             return build_results.FAILED
2184         except exception.UnexpectedTaskStateError as e:
2185             LOG.debug(e.format_message(), instance=instance)
2186             return build_results.FAILED
2187 
2188         # b64 decode the files to inject:
2189         decoded_files = self._decode_files(injected_files)
2190 
2191         if limits is None:
2192             limits = {}
2193 
2194         if node is None:
2195             node = self._get_nodename(instance, refresh=True)
2196 
2197         try:
2198             with timeutils.StopWatch() as timer:
2199                 self._build_and_run_instance(context, instance, image,
2200                         decoded_files, admin_password, requested_networks,
2201                         security_groups, block_device_mapping, node, limits,
2202                         filter_properties, request_spec, accel_uuids)
2203             LOG.info('Took %0.2f seconds to build instance.',
2204                      timer.elapsed(), instance=instance)
2205             return build_results.ACTIVE
2206         except exception.RescheduledException as e:
2207             retry = filter_properties.get('retry')
2208             if not retry:
2209                 # no retry information, do not reschedule.
2210                 LOG.debug("Retry info not present, will not reschedule",
2211                     instance=instance)
2212                 self._cleanup_allocated_networks(context, instance,
2213                     requested_networks)
2214                 self._cleanup_volumes(context, instance,
2215                     block_device_mapping, raise_exc=False)
2216                 compute_utils.add_instance_fault_from_exc(context,
2217                         instance, e, sys.exc_info(),
2218                         fault_message=e.kwargs['reason'])
2219                 self._nil_out_instance_obj_host_and_node(instance)
2220                 self._set_instance_obj_error_state(instance,
2221                                                    clean_task_state=True)
2222                 return build_results.FAILED
2223             LOG.debug(e.format_message(), instance=instance)
2224             # This will be used for logging the exception
2225             retry['exc'] = traceback.format_exception(*sys.exc_info())
2226             # This will be used for setting the instance fault message
2227             retry['exc_reason'] = e.kwargs['reason']
2228 
2229             self._cleanup_allocated_networks(context, instance,
2230                                              requested_networks)
2231 
2232             self._nil_out_instance_obj_host_and_node(instance)
2233             instance.task_state = task_states.SCHEDULING
2234             instance.save()
2235             # The instance will have already claimed resources from this host
2236             # before this build was attempted. Now that it has failed, we need
2237             # to unclaim those resources before casting to the conductor, so
2238             # that if there are alternate hosts available for a retry, it can
2239             # claim resources on that new host for the instance.
2240             self.reportclient.delete_allocation_for_instance(context,
2241                                                              instance.uuid)
2242 
2243             self.compute_task_api.build_instances(context, [instance],
2244                     image, filter_properties, admin_password,
2245                     injected_files, requested_networks, security_groups,
2246                     block_device_mapping, request_spec=request_spec,
2247                     host_lists=[host_list])
2248             return build_results.RESCHEDULED
2249         except (exception.InstanceNotFound,
2250                 exception.UnexpectedDeletingTaskStateError):
2251             msg = 'Instance disappeared during build.'
2252             LOG.debug(msg, instance=instance)
2253             self._cleanup_allocated_networks(context, instance,
2254                     requested_networks)
2255             return build_results.FAILED
2256         except Exception as e:
2257             if isinstance(e, exception.BuildAbortException):
2258                 LOG.error(e.format_message(), instance=instance)
2259             else:
2260                 # Should not reach here.
2261                 LOG.exception('Unexpected build failure, not rescheduling '
2262                               'build.', instance=instance)
2263             self._cleanup_allocated_networks(context, instance,
2264                     requested_networks)
2265             self._cleanup_volumes(context, instance,
2266                     block_device_mapping, raise_exc=False)
2267             compute_utils.add_instance_fault_from_exc(context, instance,
2268                     e, sys.exc_info())
2269             self._nil_out_instance_obj_host_and_node(instance)
2270             self._set_instance_obj_error_state(instance, clean_task_state=True)
2271             return build_results.FAILED
2272 
2273     @staticmethod
2274     def _get_scheduler_hints(filter_properties, request_spec=None):
2275         """Helper method to get scheduler hints.
2276 
2277         This method prefers to get the hints out of the request spec, but that
2278         might not be provided. Conductor will pass request_spec down to the
2279         first compute chosen for a build but older computes will not pass
2280         the request_spec to conductor's build_instances method for a
2281         a reschedule, so if we're on a host via a retry, request_spec may not
2282         be provided so we need to fallback to use the filter_properties
2283         to get scheduler hints.
2284         """
2285         hints = {}
2286         if request_spec is not None and 'scheduler_hints' in request_spec:
2287             hints = request_spec.scheduler_hints
2288         if not hints:
2289             hints = filter_properties.get('scheduler_hints') or {}
2290         return hints
2291 
2292     @staticmethod
2293     def _get_request_group_mapping(request_spec):
2294         """Return request group resource - provider mapping. This is currently
2295         used for Neutron ports that have resource request due to the port
2296         having QoS minimum bandwidth policy rule attached.
2297 
2298         :param request_spec: A RequestSpec object or None
2299         :returns: A dict keyed by RequestGroup requester_id, currently Neutron
2300         port_id, to resource provider UUID that provides resource for that
2301         RequestGroup. Or None if the request_spec was None.
2302         """
2303         if request_spec:
2304             return request_spec.get_request_group_mapping()
2305         else:
2306             return None
2307 
2308     def _build_and_run_instance(self, context, instance, image, injected_files,
2309             admin_password, requested_networks, security_groups,
2310             block_device_mapping, node, limits, filter_properties,
2311             request_spec=None, accel_uuids=None):
2312 
2313         image_name = image.get('name')
2314         self._notify_about_instance_usage(context, instance, 'create.start',
2315                 extra_usage_info={'image_name': image_name})
2316         compute_utils.notify_about_instance_create(
2317             context, instance, self.host,
2318             phase=fields.NotificationPhase.START,
2319             bdms=block_device_mapping)
2320 
2321         # NOTE(mikal): cache the keystone roles associated with the instance
2322         # at boot time for later reference
2323         instance.system_metadata.update(
2324             {'boot_roles': ','.join(context.roles)})
2325 
2326         self._check_device_tagging(requested_networks, block_device_mapping)
2327         self._check_trusted_certs(instance)
2328 
2329         provider_mapping = self._get_request_group_mapping(request_spec)
2330 
2331         if provider_mapping:
2332             try:
2333                 compute_utils\
2334                     .update_pci_request_spec_with_allocated_interface_name(
2335                         context, self.reportclient, instance, provider_mapping)
2336             except (exception.AmbiguousResourceProviderForPCIRequest,
2337                     exception.UnexpectedResourceProviderNameForPCIRequest
2338                     ) as e:
2339                 raise exception.BuildAbortException(
2340                     reason=six.text_type(e), instance_uuid=instance.uuid)
2341 
2342         # TODO(Luyao) cut over to get_allocs_for_consumer
2343         allocs = self.reportclient.get_allocations_for_consumer(
2344                 context, instance.uuid)
2345 
2346         try:
2347             scheduler_hints = self._get_scheduler_hints(filter_properties,
2348                                                         request_spec)
2349             with self.rt.instance_claim(context, instance, node, allocs,
2350                                         limits):
2351                 # NOTE(russellb) It's important that this validation be done
2352                 # *after* the resource tracker instance claim, as that is where
2353                 # the host is set on the instance.
2354                 self._validate_instance_group_policy(context, instance,
2355                                                      scheduler_hints)
2356                 image_meta = objects.ImageMeta.from_dict(image)
2357 
2358                 with self._build_resources(context, instance,
2359                         requested_networks, security_groups, image_meta,
2360                         block_device_mapping, provider_mapping,
2361                         accel_uuids) as resources:
2362                     instance.vm_state = vm_states.BUILDING
2363                     instance.task_state = task_states.SPAWNING
2364                     # NOTE(JoshNang) This also saves the changes to the
2365                     # instance from _allocate_network_async, as they aren't
2366                     # saved in that function to prevent races.
2367                     instance.save(expected_task_state=
2368                             task_states.BLOCK_DEVICE_MAPPING)
2369                     block_device_info = resources['block_device_info']
2370                     network_info = resources['network_info']
2371                     accel_info = resources['accel_info']
2372                     LOG.debug('Start spawning the instance on the hypervisor.',
2373                               instance=instance)
2374                     with timeutils.StopWatch() as timer:
2375                         self.driver.spawn(context, instance, image_meta,
2376                                           injected_files, admin_password,
2377                                           allocs, network_info=network_info,
2378                                           block_device_info=block_device_info,
2379                                           accel_info=accel_info)
2380                     LOG.info('Took %0.2f seconds to spawn the instance on '
2381                              'the hypervisor.', timer.elapsed(),
2382                              instance=instance)
2383         except (exception.InstanceNotFound,
2384                 exception.UnexpectedDeletingTaskStateError) as e:
2385             with excutils.save_and_reraise_exception():
2386                 self._notify_about_instance_usage(context, instance,
2387                     'create.error', fault=e)
2388                 compute_utils.notify_about_instance_create(
2389                     context, instance, self.host,
2390                     phase=fields.NotificationPhase.ERROR, exception=e,
2391                     bdms=block_device_mapping)
2392         except exception.ComputeResourcesUnavailable as e:
2393             LOG.debug(e.format_message(), instance=instance)
2394             self._notify_about_instance_usage(context, instance,
2395                     'create.error', fault=e)
2396             compute_utils.notify_about_instance_create(
2397                     context, instance, self.host,
2398                     phase=fields.NotificationPhase.ERROR, exception=e,
2399                     bdms=block_device_mapping)
2400             raise exception.RescheduledException(
2401                     instance_uuid=instance.uuid, reason=e.format_message())
2402         except exception.BuildAbortException as e:
2403             with excutils.save_and_reraise_exception():
2404                 LOG.debug(e.format_message(), instance=instance)
2405                 self._notify_about_instance_usage(context, instance,
2406                     'create.error', fault=e)
2407                 compute_utils.notify_about_instance_create(
2408                     context, instance, self.host,
2409                     phase=fields.NotificationPhase.ERROR, exception=e,
2410                     bdms=block_device_mapping)
2411         except exception.NoMoreFixedIps as e:
2412             LOG.warning('No more fixed IP to be allocated',
2413                         instance=instance)
2414             self._notify_about_instance_usage(context, instance,
2415                     'create.error', fault=e)
2416             compute_utils.notify_about_instance_create(
2417                     context, instance, self.host,
2418                     phase=fields.NotificationPhase.ERROR, exception=e,
2419                     bdms=block_device_mapping)
2420             msg = _('Failed to allocate the network(s) with error %s, '
2421                     'not rescheduling.') % e.format_message()
2422             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2423                     reason=msg)
2424         except (exception.ExternalNetworkAttachForbidden,
2425                 exception.VirtualInterfaceCreateException,
2426                 exception.VirtualInterfaceMacAddressException,
2427                 exception.FixedIpInvalidOnHost,
2428                 exception.UnableToAutoAllocateNetwork,
2429                 exception.NetworksWithQoSPolicyNotSupported) as e:
2430             LOG.exception('Failed to allocate network(s)',
2431                           instance=instance)
2432             self._notify_about_instance_usage(context, instance,
2433                     'create.error', fault=e)
2434             compute_utils.notify_about_instance_create(
2435                     context, instance, self.host,
2436                     phase=fields.NotificationPhase.ERROR, exception=e,
2437                     bdms=block_device_mapping)
2438             msg = _('Failed to allocate the network(s), not rescheduling.')
2439             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2440                     reason=msg)
2441         except (exception.FlavorDiskTooSmall,
2442                 exception.FlavorMemoryTooSmall,
2443                 exception.ImageNotActive,
2444                 exception.ImageUnacceptable,
2445                 exception.InvalidDiskInfo,
2446                 exception.InvalidDiskFormat,
2447                 cursive_exception.SignatureVerificationError,
2448                 exception.CertificateValidationFailed,
2449                 exception.VolumeEncryptionNotSupported,
2450                 exception.InvalidInput,
2451                 # TODO(mriedem): We should be validating RequestedVRamTooHigh
2452                 # in the API during server create and rebuild.
2453                 exception.RequestedVRamTooHigh) as e:
2454             self._notify_about_instance_usage(context, instance,
2455                     'create.error', fault=e)
2456             compute_utils.notify_about_instance_create(
2457                     context, instance, self.host,
2458                     phase=fields.NotificationPhase.ERROR, exception=e,
2459                     bdms=block_device_mapping)
2460             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2461                     reason=e.format_message())
2462         except Exception as e:
2463             LOG.exception('Failed to build and run instance',
2464                           instance=instance)
2465             self._notify_about_instance_usage(context, instance,
2466                     'create.error', fault=e)
2467             compute_utils.notify_about_instance_create(
2468                     context, instance, self.host,
2469                     phase=fields.NotificationPhase.ERROR, exception=e,
2470                     bdms=block_device_mapping)
2471             raise exception.RescheduledException(
2472                     instance_uuid=instance.uuid, reason=six.text_type(e))
2473 
2474         # NOTE(alaski): This is only useful during reschedules, remove it now.
2475         instance.system_metadata.pop('network_allocated', None)
2476 
2477         # If CONF.default_access_ip_network_name is set, grab the
2478         # corresponding network and set the access ip values accordingly.
2479         network_name = CONF.default_access_ip_network_name
2480         if (network_name and not instance.access_ip_v4 and
2481                 not instance.access_ip_v6):
2482             # Note that when there are multiple ips to choose from, an
2483             # arbitrary one will be chosen.
2484             for vif in network_info:
2485                 if vif['network']['label'] == network_name:
2486                     for ip in vif.fixed_ips():
2487                         if not instance.access_ip_v4 and ip['version'] == 4:
2488                             instance.access_ip_v4 = ip['address']
2489                         if not instance.access_ip_v6 and ip['version'] == 6:
2490                             instance.access_ip_v6 = ip['address']
2491                     break
2492 
2493         self._update_instance_after_spawn(instance)
2494 
2495         try:
2496             instance.save(expected_task_state=task_states.SPAWNING)
2497         except (exception.InstanceNotFound,
2498                 exception.UnexpectedDeletingTaskStateError) as e:
2499             with excutils.save_and_reraise_exception():
2500                 self._notify_about_instance_usage(context, instance,
2501                     'create.error', fault=e)
2502                 compute_utils.notify_about_instance_create(
2503                     context, instance, self.host,
2504                     phase=fields.NotificationPhase.ERROR, exception=e,
2505                     bdms=block_device_mapping)
2506 
2507         self._update_scheduler_instance_info(context, instance)
2508         self._notify_about_instance_usage(context, instance, 'create.end',
2509                 extra_usage_info={'message': _('Success')},
2510                 network_info=network_info)
2511         compute_utils.notify_about_instance_create(context, instance,
2512                 self.host, phase=fields.NotificationPhase.END,
2513                 bdms=block_device_mapping)
2514 
2515     def _build_resources_cleanup(self, instance, network_info):
2516         # Make sure the async call finishes
2517         if network_info is not None:
2518             network_info.wait(do_raise=False)
2519             self.driver.clean_networks_preparation(instance,
2520                                                    network_info)
2521         self.driver.failed_spawn_cleanup(instance)
2522 
2523     @contextlib.contextmanager
2524     def _build_resources(self, context, instance, requested_networks,
2525                          security_groups, image_meta, block_device_mapping,
2526                          resource_provider_mapping, accel_uuids):
2527         resources = {}
2528         network_info = None
2529         try:
2530             LOG.debug('Start building networks asynchronously for instance.',
2531                       instance=instance)
2532             network_info = self._build_networks_for_instance(context, instance,
2533                     requested_networks, security_groups,
2534                     resource_provider_mapping)
2535             resources['network_info'] = network_info
2536         except (exception.InstanceNotFound,
2537                 exception.UnexpectedDeletingTaskStateError):
2538             raise
2539         except exception.UnexpectedTaskStateError as e:
2540             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2541                     reason=e.format_message())
2542         except Exception:
2543             # Because this allocation is async any failures are likely to occur
2544             # when the driver accesses network_info during spawn().
2545             LOG.exception('Failed to allocate network(s)',
2546                           instance=instance)
2547             msg = _('Failed to allocate the network(s), not rescheduling.')
2548             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2549                     reason=msg)
2550 
2551         try:
2552             # Perform any driver preparation work for the driver.
2553             self.driver.prepare_for_spawn(instance)
2554 
2555             # Depending on a virt driver, some network configuration is
2556             # necessary before preparing block devices.
2557             self.driver.prepare_networks_before_block_device_mapping(
2558                 instance, network_info)
2559 
2560             # Verify that all the BDMs have a device_name set and assign a
2561             # default to the ones missing it with the help of the driver.
2562             self._default_block_device_names(instance, image_meta,
2563                                              block_device_mapping)
2564 
2565             LOG.debug('Start building block device mappings for instance.',
2566                       instance=instance)
2567             instance.vm_state = vm_states.BUILDING
2568             instance.task_state = task_states.BLOCK_DEVICE_MAPPING
2569             instance.save()
2570 
2571             block_device_info = self._prep_block_device(context, instance,
2572                     block_device_mapping)
2573             resources['block_device_info'] = block_device_info
2574         except (exception.InstanceNotFound,
2575                 exception.UnexpectedDeletingTaskStateError):
2576             with excutils.save_and_reraise_exception():
2577                 self._build_resources_cleanup(instance, network_info)
2578         except (exception.UnexpectedTaskStateError,
2579                 exception.OverQuota, exception.InvalidBDM) as e:
2580             self._build_resources_cleanup(instance, network_info)
2581             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2582                     reason=e.format_message())
2583         except Exception:
2584             LOG.exception('Failure prepping block device',
2585                           instance=instance)
2586             self._build_resources_cleanup(instance, network_info)
2587             msg = _('Failure prepping block device.')
2588             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2589                     reason=msg)
2590 
2591         arqs = []
2592         if instance.flavor.extra_specs.get('accel:device_profile'):
2593             try:
2594                 arqs = self._get_bound_arq_resources(
2595                     context, instance, accel_uuids)
2596             except (Exception, eventlet.timeout.Timeout) as exc:
2597                 LOG.exception(exc.format_message())
2598                 self._build_resources_cleanup(instance, network_info)
2599                 compute_utils.delete_arqs_if_needed(context, instance)
2600                 msg = _('Failure getting accelerator requests.')
2601                 raise exception.BuildAbortException(
2602                     reason=msg, instance_uuid=instance.uuid)
2603 
2604         resources['accel_info'] = arqs
2605         try:
2606             yield resources
2607         except Exception as exc:
2608             with excutils.save_and_reraise_exception() as ctxt:
2609                 if not isinstance(exc, (
2610                         exception.InstanceNotFound,
2611                         exception.UnexpectedDeletingTaskStateError)):
2612                     LOG.exception('Instance failed to spawn',
2613                                   instance=instance)
2614                 # Make sure the async call finishes
2615                 if network_info is not None:
2616                     network_info.wait(do_raise=False)
2617                 # if network_info is empty we're likely here because of
2618                 # network allocation failure. Since nothing can be reused on
2619                 # rescheduling it's better to deallocate network to eliminate
2620                 # the chance of orphaned ports in neutron
2621                 deallocate_networks = False if network_info else True
2622                 try:
2623                     self._shutdown_instance(context, instance,
2624                             block_device_mapping, requested_networks,
2625                             try_deallocate_networks=deallocate_networks)
2626                 except Exception as exc2:
2627                     ctxt.reraise = False
2628                     LOG.warning('Could not clean up failed build,'
2629                                 ' not rescheduling. Error: %s',
2630                                 six.text_type(exc2))
2631                     raise exception.BuildAbortException(
2632                             instance_uuid=instance.uuid,
2633                             reason=six.text_type(exc))
2634                 finally:
2635                     # Call Cyborg to delete accelerator requests
2636                     compute_utils.delete_arqs_if_needed(context, instance)
2637 
2638     def _get_bound_arq_resources(self, context, instance, arq_uuids):
2639         """Get bound accelerator requests.
2640 
2641         The ARQ binding was kicked off in the conductor as an async
2642         operation. Here we wait for the notification from Cyborg.
2643 
2644         If the notification arrived before this point, which can happen
2645         in many/most cases (see [1]), it will be lost. To handle that,
2646         we use exit_wait_early.
2647         [1] https://review.opendev.org/#/c/631244/46/nova/compute/
2648             manager.py@2627
2649 
2650         :param instance: instance object
2651         :param arq_uuids: List of accelerator request (ARQ) UUIDs.
2652         :returns: List of ARQs for which bindings have completed,
2653                   successfully or otherwise
2654         """
2655 
2656         cyclient = cyborg.get_client(context)
2657         if arq_uuids is None:
2658             arqs = cyclient.get_arqs_for_instance(instance.uuid)
2659             arq_uuids = [arq['uuid'] for arq in arqs]
2660         events = [('accelerator-request-bound', arq_uuid)
2661                   for arq_uuid in arq_uuids]
2662 
2663         timeout = CONF.arq_binding_timeout
2664         with self.virtapi.wait_for_instance_event(
2665                 instance, events, deadline=timeout):
2666             resolved_arqs = cyclient.get_arqs_for_instance(
2667                     instance.uuid, only_resolved=True)
2668             # Events for these resolved ARQs may have already arrived.
2669             # Such 'early' events need to be ignored.
2670             early_events = [('accelerator-request-bound', arq['uuid'])
2671                              for arq in resolved_arqs]
2672             if early_events:
2673                 self.virtapi.exit_wait_early(early_events)
2674 
2675         # Since a timeout in wait_for_instance_event will raise, we get
2676         # here only if all binding events have been received.
2677         resolved_uuids = [arq['uuid'] for arq in resolved_arqs]
2678         if sorted(resolved_uuids) != sorted(arq_uuids):
2679             # Query Cyborg to get all.
2680             arqs = cyclient.get_arqs_for_instance(instance.uuid)
2681         else:
2682             arqs = resolved_arqs
2683         return arqs
2684 
2685     def _cleanup_allocated_networks(self, context, instance,
2686             requested_networks):
2687         """Cleanup networks allocated for instance.
2688 
2689         :param context: nova request context
2690         :param instance: nova.objects.instance.Instance object
2691         :param requested_networks: nova.objects.NetworkRequestList
2692         """
2693         LOG.debug('Unplugging VIFs for instance', instance=instance)
2694 
2695         network_info = instance.get_network_info()
2696 
2697         # NOTE(stephenfin) to avoid nova destroying the instance without
2698         # unplugging the interface, refresh network_info if it is empty.
2699         if not network_info:
2700             try:
2701                 network_info = self.network_api.get_instance_nw_info(
2702                     context, instance,
2703                 )
2704             except Exception as exc:
2705                 LOG.warning(
2706                     'Failed to update network info cache when cleaning up '
2707                     'allocated networks. Stale VIFs may be left on this host.'
2708                     'Error: %s', six.text_type(exc)
2709                 )
2710                 return
2711 
2712         try:
2713             self.driver.unplug_vifs(instance, network_info)
2714         except NotImplementedError:
2715             # This is an optional method so ignore things if it doesn't exist
2716             LOG.debug(
2717                 'Virt driver does not provide unplug_vifs method, so it '
2718                 'is not possible determine if VIFs should be unplugged.'
2719             )
2720         except exception.NovaException as exc:
2721             # It's possible that the instance never got as far as plugging
2722             # VIFs, in which case we would see an exception which can be
2723             # mostly ignored
2724             LOG.warning(
2725                 'Cleaning up VIFs failed for instance. Error: %s',
2726                 six.text_type(exc), instance=instance,
2727             )
2728         else:
2729             LOG.debug('Unplugged VIFs for instance', instance=instance)
2730 
2731         try:
2732             self._deallocate_network(context, instance, requested_networks)
2733         except Exception:
2734             LOG.exception('Failed to deallocate networks', instance=instance)
2735             return
2736 
2737         instance.system_metadata['network_allocated'] = 'False'
2738         try:
2739             instance.save()
2740         except exception.InstanceNotFound:
2741             # NOTE(alaski): It's possible that we're cleaning up the networks
2742             # because the instance was deleted.  If that's the case then this
2743             # exception will be raised by instance.save()
2744             pass
2745 
2746     def _try_deallocate_network(self, context, instance,
2747                                 requested_networks=None):
2748 
2749         # During auto-scale cleanup, we could be deleting a large number
2750         # of servers at the same time and overloading parts of the system,
2751         # so we retry a few times in case of connection failures to the
2752         # networking service.
2753         @loopingcall.RetryDecorator(
2754             max_retry_count=3, inc_sleep_time=2, max_sleep_time=12,
2755             exceptions=(keystone_exception.connection.ConnectFailure,))
2756         def _deallocate_network_with_retries():
2757             try:
2758                 self._deallocate_network(
2759                     context, instance, requested_networks)
2760             except keystone_exception.connection.ConnectFailure as e:
2761                 # Provide a warning that something is amiss.
2762                 with excutils.save_and_reraise_exception():
2763                     LOG.warning('Failed to deallocate network for instance; '
2764                                 'retrying. Error: %s', six.text_type(e),
2765                                 instance=instance)
2766 
2767         try:
2768             # tear down allocated network structure
2769             _deallocate_network_with_retries()
2770         except Exception as ex:
2771             with excutils.save_and_reraise_exception():
2772                 LOG.error('Failed to deallocate network for instance. '
2773                           'Error: %s', ex, instance=instance)
2774                 self._set_instance_obj_error_state(instance)
2775 
2776     def _get_power_off_values(self, instance, clean_shutdown):
2777         """Get the timing configuration for powering down this instance."""
2778         if clean_shutdown:
2779             timeout = compute_utils.get_value_from_system_metadata(instance,
2780                           key='image_os_shutdown_timeout', type=int,
2781                           default=CONF.shutdown_timeout)
2782             retry_interval = CONF.compute.shutdown_retry_interval
2783         else:
2784             timeout = 0
2785             retry_interval = 0
2786 
2787         return timeout, retry_interval
2788 
2789     def _power_off_instance(self, instance, clean_shutdown=True):
2790         """Power off an instance on this host."""
2791         timeout, retry_interval = self._get_power_off_values(
2792             instance, clean_shutdown)
2793         self.driver.power_off(instance, timeout, retry_interval)
2794 
2795     def _shutdown_instance(self, context, instance,
2796                            bdms, requested_networks=None, notify=True,
2797                            try_deallocate_networks=True):
2798         """Shutdown an instance on this host.
2799 
2800         :param:context: security context
2801         :param:instance: a nova.objects.Instance object
2802         :param:bdms: the block devices for the instance to be torn
2803                      down
2804         :param:requested_networks: the networks on which the instance
2805                                    has ports
2806         :param:notify: true if a final usage notification should be
2807                        emitted
2808         :param:try_deallocate_networks: false if we should avoid
2809                                         trying to teardown networking
2810         """
2811         context = context.elevated()
2812         LOG.info('Terminating instance', instance=instance)
2813 
2814         if notify:
2815             self._notify_about_instance_usage(context, instance,
2816                                               "shutdown.start")
2817             compute_utils.notify_about_instance_action(context, instance,
2818                     self.host, action=fields.NotificationAction.SHUTDOWN,
2819                     phase=fields.NotificationPhase.START, bdms=bdms)
2820 
2821         network_info = instance.get_network_info()
2822 
2823         # NOTE(arnaudmorin) to avoid nova destroying the instance without
2824         # unplugging the interface, refresh network_info if it is empty.
2825         if not network_info:
2826             network_info = self.network_api.get_instance_nw_info(
2827                 context, instance)
2828 
2829         # NOTE(vish) get bdms before destroying the instance
2830         vol_bdms = [bdm for bdm in bdms if bdm.is_volume]
2831         block_device_info = self._get_instance_block_device_info(
2832             context, instance, bdms=bdms)
2833 
2834         # NOTE(melwitt): attempt driver destroy before releasing ip, may
2835         #                want to keep ip allocated for certain failures
2836         try:
2837             LOG.debug('Start destroying the instance on the hypervisor.',
2838                       instance=instance)
2839             with timeutils.StopWatch() as timer:
2840                 self.driver.destroy(context, instance, network_info,
2841                                     block_device_info)
2842             LOG.info('Took %0.2f seconds to destroy the instance on the '
2843                      'hypervisor.', timer.elapsed(), instance=instance)
2844         except exception.InstancePowerOffFailure:
2845             # if the instance can't power off, don't release the ip
2846             with excutils.save_and_reraise_exception():
2847                 pass
2848         except Exception:
2849             with excutils.save_and_reraise_exception():
2850                 # deallocate ip and fail without proceeding to
2851                 # volume api calls, preserving current behavior
2852                 if try_deallocate_networks:
2853                     self._try_deallocate_network(context, instance,
2854                                                  requested_networks)
2855 
2856         if try_deallocate_networks:
2857             self._try_deallocate_network(context, instance, requested_networks)
2858 
2859         timer.restart()
2860         for bdm in vol_bdms:
2861             try:
2862                 if bdm.attachment_id:
2863                     self.volume_api.attachment_delete(context,
2864                                                       bdm.attachment_id)
2865                 else:
2866                     # NOTE(vish): actual driver detach done in driver.destroy,
2867                     #             so just tell cinder that we are done with it.
2868                     connector = self.driver.get_volume_connector(instance)
2869                     self.volume_api.terminate_connection(context,
2870                                                          bdm.volume_id,
2871                                                          connector)
2872                     self.volume_api.detach(context, bdm.volume_id,
2873                                            instance.uuid)
2874 
2875             except exception.VolumeAttachmentNotFound as exc:
2876                 LOG.debug('Ignoring VolumeAttachmentNotFound: %s', exc,
2877                           instance=instance)
2878             except exception.DiskNotFound as exc:
2879                 LOG.debug('Ignoring DiskNotFound: %s', exc,
2880                           instance=instance)
2881             except exception.VolumeNotFound as exc:
2882                 LOG.debug('Ignoring VolumeNotFound: %s', exc,
2883                           instance=instance)
2884             except (cinder_exception.EndpointNotFound,
2885                     keystone_exception.EndpointNotFound) as exc:
2886                 LOG.warning('Ignoring EndpointNotFound for '
2887                             'volume %(volume_id)s: %(exc)s',
2888                             {'exc': exc, 'volume_id': bdm.volume_id},
2889                             instance=instance)
2890             except cinder_exception.ClientException as exc:
2891                 LOG.warning('Ignoring unknown cinder exception for '
2892                             'volume %(volume_id)s: %(exc)s',
2893                             {'exc': exc, 'volume_id': bdm.volume_id},
2894                             instance=instance)
2895             except Exception as exc:
2896                 LOG.warning('Ignoring unknown exception for '
2897                             'volume %(volume_id)s: %(exc)s',
2898                             {'exc': exc, 'volume_id': bdm.volume_id},
2899                             instance=instance)
2900         if vol_bdms:
2901             LOG.info('Took %(time).2f seconds to detach %(num)s volumes '
2902                      'for instance.',
2903                      {'time': timer.elapsed(), 'num': len(vol_bdms)},
2904                      instance=instance)
2905 
2906         if notify:
2907             self._notify_about_instance_usage(context, instance,
2908                                               "shutdown.end")
2909             compute_utils.notify_about_instance_action(context, instance,
2910                     self.host, action=fields.NotificationAction.SHUTDOWN,
2911                     phase=fields.NotificationPhase.END, bdms=bdms)
2912 
2913     def _cleanup_volumes(self, context, instance, bdms, raise_exc=True,
2914                          detach=True):
2915         exc_info = None
2916         for bdm in bdms:
2917             if detach and bdm.volume_id:
2918                 try:
2919                     LOG.debug("Detaching volume: %s", bdm.volume_id,
2920                               instance_uuid=instance.uuid)
2921                     destroy = bdm.delete_on_termination
2922                     self._detach_volume(context, bdm, instance,
2923                                         destroy_bdm=destroy)
2924                 except Exception as exc:
2925                     exc_info = sys.exc_info()
2926                     LOG.warning('Failed to detach volume: %(volume_id)s '
2927                                 'due to %(exc)s',
2928                                 {'volume_id': bdm.volume_id, 'exc': exc})
2929 
2930             if bdm.volume_id and bdm.delete_on_termination:
2931                 try:
2932                     LOG.debug("Deleting volume: %s", bdm.volume_id,
2933                               instance_uuid=instance.uuid)
2934                     self.volume_api.delete(context, bdm.volume_id)
2935                 except Exception as exc:
2936                     exc_info = sys.exc_info()
2937                     LOG.warning('Failed to delete volume: %(volume_id)s '
2938                                 'due to %(exc)s',
2939                                 {'volume_id': bdm.volume_id, 'exc': exc})
2940         if exc_info is not None and raise_exc:
2941             six.reraise(exc_info[0], exc_info[1], exc_info[2])
2942 
2943     def _delete_instance(self, context, instance, bdms):
2944         """Delete an instance on this host.
2945 
2946         :param context: nova request context
2947         :param instance: nova.objects.instance.Instance object
2948         :param bdms: nova.objects.block_device.BlockDeviceMappingList object
2949         """
2950         events = self.instance_events.clear_events_for_instance(instance)
2951         if events:
2952             LOG.debug('Events pending at deletion: %(events)s',
2953                       {'events': ','.join(events.keys())},
2954                       instance=instance)
2955         self._notify_about_instance_usage(context, instance,
2956                                           "delete.start")
2957         compute_utils.notify_about_instance_action(context, instance,
2958                 self.host, action=fields.NotificationAction.DELETE,
2959                 phase=fields.NotificationPhase.START, bdms=bdms)
2960 
2961         self._shutdown_instance(context, instance, bdms)
2962 
2963         # NOTE(vish): We have already deleted the instance, so we have
2964         #             to ignore problems cleaning up the volumes. It
2965         #             would be nice to let the user know somehow that
2966         #             the volume deletion failed, but it is not
2967         #             acceptable to have an instance that can not be
2968         #             deleted. Perhaps this could be reworked in the
2969         #             future to set an instance fault the first time
2970         #             and to only ignore the failure if the instance
2971         #             is already in ERROR.
2972 
2973         # NOTE(ameeda): The volumes already detached during the above
2974         #               _shutdown_instance() call and this is why
2975         #               detach is not requested from _cleanup_volumes()
2976         #               in this case
2977 
2978         self._cleanup_volumes(context, instance, bdms,
2979                 raise_exc=False, detach=False)
2980         # Delete Cyborg ARQs if the instance has a device profile.
2981         compute_utils.delete_arqs_if_needed(context, instance)
2982         # if a delete task succeeded, always update vm state and task
2983         # state without expecting task state to be DELETING
2984         instance.vm_state = vm_states.DELETED
2985         instance.task_state = None
2986         instance.power_state = power_state.NOSTATE
2987         instance.terminated_at = timeutils.utcnow()
2988         instance.save()
2989 
2990         self._complete_deletion(context, instance)
2991         # only destroy the instance in the db if the _complete_deletion
2992         # doesn't raise and therefore allocation is successfully
2993         # deleted in placement
2994         instance.destroy()
2995 
2996         self._notify_about_instance_usage(context, instance, "delete.end")
2997         compute_utils.notify_about_instance_action(context, instance,
2998                 self.host, action=fields.NotificationAction.DELETE,
2999                 phase=fields.NotificationPhase.END, bdms=bdms)
3000 
3001     @wrap_exception()
3002     @reverts_task_state
3003     @wrap_instance_event(prefix='compute')
3004     @wrap_instance_fault
3005     def terminate_instance(self, context, instance, bdms):
3006         """Terminate an instance on this host."""
3007         @utils.synchronized(instance.uuid)
3008         def do_terminate_instance(instance, bdms):
3009             # NOTE(mriedem): If we are deleting the instance while it was
3010             # booting from volume, we could be racing with a database update of
3011             # the BDM volume_id. Since the compute API passes the BDMs over RPC
3012             # to compute here, the BDMs may be stale at this point. So check
3013             # for any volume BDMs that don't have volume_id set and if we
3014             # detect that, we need to refresh the BDM list before proceeding.
3015             # TODO(mriedem): Move this into _delete_instance and make the bdms
3016             # parameter optional.
3017             for bdm in list(bdms):
3018                 if bdm.is_volume and not bdm.volume_id:
3019                     LOG.debug('There are potentially stale BDMs during '
3020                               'delete, refreshing the BlockDeviceMappingList.',
3021                               instance=instance)
3022                     bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
3023                         context, instance.uuid)
3024                     break
3025             try:
3026                 self._delete_instance(context, instance, bdms)
3027             except exception.InstanceNotFound:
3028                 LOG.info("Instance disappeared during terminate",
3029                          instance=instance)
3030             except Exception:
3031                 # As we're trying to delete always go to Error if something
3032                 # goes wrong that _delete_instance can't handle.
3033                 with excutils.save_and_reraise_exception():
3034                     LOG.exception('Setting instance vm_state to ERROR',
3035                                   instance=instance)
3036                     self._set_instance_obj_error_state(instance)
3037 
3038         do_terminate_instance(instance, bdms)
3039 
3040     # NOTE(johannes): This is probably better named power_off_instance
3041     # so it matches the driver method, but because of other issues, we
3042     # can't use that name in grizzly.
3043     @wrap_exception()
3044     @reverts_task_state
3045     @wrap_instance_event(prefix='compute')
3046     @wrap_instance_fault
3047     def stop_instance(self, context, instance, clean_shutdown):
3048         """Stopping an instance on this host."""
3049 
3050         @utils.synchronized(instance.uuid)
3051         def do_stop_instance():
3052             current_power_state = self._get_power_state(instance)
3053             LOG.debug('Stopping instance; current vm_state: %(vm_state)s, '
3054                       'current task_state: %(task_state)s, current DB '
3055                       'power_state: %(db_power_state)s, current VM '
3056                       'power_state: %(current_power_state)s',
3057                       {'vm_state': instance.vm_state,
3058                        'task_state': instance.task_state,
3059                        'db_power_state': instance.power_state,
3060                        'current_power_state': current_power_state},
3061                       instance_uuid=instance.uuid)
3062 
3063             # NOTE(mriedem): If the instance is already powered off, we are
3064             # possibly tearing down and racing with other operations, so we can
3065             # expect the task_state to be None if something else updates the
3066             # instance and we're not locking it.
3067             expected_task_state = [task_states.POWERING_OFF]
3068             # The list of power states is from _sync_instance_power_state.
3069             if current_power_state in (power_state.NOSTATE,
3070                                        power_state.SHUTDOWN,
3071                                        power_state.CRASHED):
3072                 LOG.info('Instance is already powered off in the '
3073                          'hypervisor when stop is called.',
3074                          instance=instance)
3075                 expected_task_state.append(None)
3076 
3077             self._notify_about_instance_usage(context, instance,
3078                                               "power_off.start")
3079 
3080             compute_utils.notify_about_instance_action(context, instance,
3081                         self.host, action=fields.NotificationAction.POWER_OFF,
3082                         phase=fields.NotificationPhase.START)
3083 
3084             self._power_off_instance(instance, clean_shutdown)
3085             instance.power_state = self._get_power_state(instance)
3086             instance.vm_state = vm_states.STOPPED
3087             instance.task_state = None
3088             instance.save(expected_task_state=expected_task_state)
3089             self._notify_about_instance_usage(context, instance,
3090                                               "power_off.end")
3091 
3092             compute_utils.notify_about_instance_action(context, instance,
3093                         self.host, action=fields.NotificationAction.POWER_OFF,
3094                         phase=fields.NotificationPhase.END)
3095 
3096         do_stop_instance()
3097 
3098     def _power_on(self, context, instance):
3099         network_info = self.network_api.get_instance_nw_info(context, instance)
3100         block_device_info = self._get_instance_block_device_info(context,
3101                                                                  instance)
3102         accel_info = self._get_accel_info(context, instance)
3103         self.driver.power_on(context, instance,
3104                              network_info,
3105                              block_device_info, accel_info)
3106 
3107     def _delete_snapshot_of_shelved_instance(self, context, instance,
3108                                              snapshot_id):
3109         """Delete snapshot of shelved instance."""
3110         try:
3111             self.image_api.delete(context, snapshot_id)
3112         except (exception.ImageNotFound,
3113                 exception.ImageNotAuthorized) as exc:
3114             LOG.warning("Failed to delete snapshot "
3115                         "from shelved instance (%s).",
3116                         exc.format_message(), instance=instance)
3117         except Exception:
3118             LOG.exception("Something wrong happened when trying to "
3119                           "delete snapshot from shelved instance.",
3120                           instance=instance)
3121 
3122     # NOTE(johannes): This is probably better named power_on_instance
3123     # so it matches the driver method, but because of other issues, we
3124     # can't use that name in grizzly.
3125     @wrap_exception()
3126     @reverts_task_state
3127     @wrap_instance_event(prefix='compute')
3128     @wrap_instance_fault
3129     def start_instance(self, context, instance):
3130         """Starting an instance on this host."""
3131         self._notify_about_instance_usage(context, instance, "power_on.start")
3132         compute_utils.notify_about_instance_action(context, instance,
3133             self.host, action=fields.NotificationAction.POWER_ON,
3134             phase=fields.NotificationPhase.START)
3135         self._power_on(context, instance)
3136         instance.power_state = self._get_power_state(instance)
3137         instance.vm_state = vm_states.ACTIVE
3138         instance.task_state = None
3139 
3140         # Delete an image(VM snapshot) for a shelved instance
3141         snapshot_id = instance.system_metadata.get('shelved_image_id')
3142         if snapshot_id:
3143             self._delete_snapshot_of_shelved_instance(context, instance,
3144                                                       snapshot_id)
3145 
3146         # Delete system_metadata for a shelved instance
3147         compute_utils.remove_shelved_keys_from_system_metadata(instance)
3148 
3149         instance.save(expected_task_state=task_states.POWERING_ON)
3150         self._notify_about_instance_usage(context, instance, "power_on.end")
3151         compute_utils.notify_about_instance_action(context, instance,
3152             self.host, action=fields.NotificationAction.POWER_ON,
3153             phase=fields.NotificationPhase.END)
3154 
3155     @messaging.expected_exceptions(NotImplementedError,
3156                                    exception.TriggerCrashDumpNotSupported,
3157                                    exception.InstanceNotRunning)
3158     @wrap_exception()
3159     @wrap_instance_event(prefix='compute')
3160     @wrap_instance_fault
3161     def trigger_crash_dump(self, context, instance):
3162         """Trigger crash dump in an instance."""
3163 
3164         self._notify_about_instance_usage(context, instance,
3165                                           "trigger_crash_dump.start")
3166         compute_utils.notify_about_instance_action(context, instance,
3167                 self.host, action=fields.NotificationAction.TRIGGER_CRASH_DUMP,
3168                 phase=fields.NotificationPhase.START)
3169 
3170         # This method does not change task_state and power_state because the
3171         # effect of a trigger depends on user's configuration.
3172         self.driver.trigger_crash_dump(instance)
3173 
3174         self._notify_about_instance_usage(context, instance,
3175                                           "trigger_crash_dump.end")
3176         compute_utils.notify_about_instance_action(context, instance,
3177                 self.host, action=fields.NotificationAction.TRIGGER_CRASH_DUMP,
3178                 phase=fields.NotificationPhase.END)
3179 
3180     @wrap_exception()
3181     @reverts_task_state
3182     @wrap_instance_event(prefix='compute')
3183     @wrap_instance_fault
3184     def soft_delete_instance(self, context, instance):
3185         """Soft delete an instance on this host."""
3186         with compute_utils.notify_about_instance_delete(
3187                 self.notifier, context, instance, 'soft_delete',
3188                 source=fields.NotificationSource.COMPUTE):
3189             try:
3190                 self.driver.soft_delete(instance)
3191             except NotImplementedError:
3192                 # Fallback to just powering off the instance if the
3193                 # hypervisor doesn't implement the soft_delete method
3194                 self.driver.power_off(instance)
3195             instance.power_state = self._get_power_state(instance)
3196             instance.vm_state = vm_states.SOFT_DELETED
3197             instance.task_state = None
3198             instance.save(expected_task_state=[task_states.SOFT_DELETING])
3199 
3200     @wrap_exception()
3201     @reverts_task_state
3202     @wrap_instance_event(prefix='compute')
3203     @wrap_instance_fault
3204     def restore_instance(self, context, instance):
3205         """Restore a soft-deleted instance on this host."""
3206         self._notify_about_instance_usage(context, instance, "restore.start")
3207         compute_utils.notify_about_instance_action(context, instance,
3208             self.host, action=fields.NotificationAction.RESTORE,
3209             phase=fields.NotificationPhase.START)
3210         try:
3211             self.driver.restore(instance)
3212         except NotImplementedError:
3213             # Fallback to just powering on the instance if the hypervisor
3214             # doesn't implement the restore method
3215             self._power_on(context, instance)
3216         instance.power_state = self._get_power_state(instance)
3217         instance.vm_state = vm_states.ACTIVE
3218         instance.task_state = None
3219         instance.save(expected_task_state=task_states.RESTORING)
3220         self._notify_about_instance_usage(context, instance, "restore.end")
3221         compute_utils.notify_about_instance_action(context, instance,
3222             self.host, action=fields.NotificationAction.RESTORE,
3223             phase=fields.NotificationPhase.END)
3224 
3225     @staticmethod
3226     def _set_migration_status(migration, status):
3227         """Set the status, and guard against a None being passed in.
3228 
3229         This is useful as some of the compute RPC calls will not pass
3230         a migration object in older versions. The check can be removed when
3231         we move past 4.x major version of the RPC API.
3232         """
3233         if migration:
3234             migration.status = status
3235             migration.save()
3236 
3237     def _rebuild_default_impl(self, context, instance, image_meta,
3238                               injected_files, admin_password, allocations,
3239                               bdms, detach_block_devices, attach_block_devices,
3240                               network_info=None,
3241                               evacuate=False, block_device_info=None,
3242                               preserve_ephemeral=False):
3243         if preserve_ephemeral:
3244             # The default code path does not support preserving ephemeral
3245             # partitions.
3246             raise exception.PreserveEphemeralNotSupported()
3247 
3248         if evacuate:
3249             detach_block_devices(context, bdms)
3250         else:
3251             self._power_off_instance(instance, clean_shutdown=True)
3252             detach_block_devices(context, bdms)
3253             self.driver.destroy(context, instance,
3254                                 network_info=network_info,
3255                                 block_device_info=block_device_info)
3256 
3257         instance.task_state = task_states.REBUILD_BLOCK_DEVICE_MAPPING
3258         instance.save(expected_task_state=[task_states.REBUILDING])
3259 
3260         new_block_device_info = attach_block_devices(context, instance, bdms)
3261 
3262         instance.task_state = task_states.REBUILD_SPAWNING
3263         instance.save(
3264             expected_task_state=[task_states.REBUILD_BLOCK_DEVICE_MAPPING])
3265 
3266         with instance.mutated_migration_context():
3267             self.driver.spawn(context, instance, image_meta, injected_files,
3268                               admin_password, allocations,
3269                               network_info=network_info,
3270                               block_device_info=new_block_device_info)
3271 
3272     def _notify_instance_rebuild_error(self, context, instance, error, bdms):
3273         self._notify_about_instance_usage(context, instance,
3274                                           'rebuild.error', fault=error)
3275         compute_utils.notify_about_instance_rebuild(
3276             context, instance, self.host,
3277             phase=fields.NotificationPhase.ERROR, exception=error, bdms=bdms)
3278 
3279     @messaging.expected_exceptions(exception.PreserveEphemeralNotSupported)
3280     @wrap_exception()
3281     @reverts_task_state
3282     @wrap_instance_event(prefix='compute')
3283     @wrap_instance_fault
3284     def rebuild_instance(self, context, instance, orig_image_ref, image_ref,
3285                          injected_files, new_pass, orig_sys_metadata,
3286                          bdms, recreate, on_shared_storage,
3287                          preserve_ephemeral, migration,
3288                          scheduled_node, limits, request_spec):
3289         """Destroy and re-make this instance.
3290 
3291         A 'rebuild' effectively purges all existing data from the system and
3292         remakes the VM with given 'metadata' and 'personalities'.
3293 
3294         :param context: `nova.RequestContext` object
3295         :param instance: Instance object
3296         :param orig_image_ref: Original image_ref before rebuild
3297         :param image_ref: New image_ref for rebuild
3298         :param injected_files: Files to inject
3299         :param new_pass: password to set on rebuilt instance
3300         :param orig_sys_metadata: instance system metadata from pre-rebuild
3301         :param bdms: block-device-mappings to use for rebuild
3302         :param recreate: True if the instance is being evacuated (e.g. the
3303             hypervisor it was on failed) - cleanup of old state will be
3304             skipped.
3305         :param on_shared_storage: True if instance files on shared storage.
3306                                   If not provided then information from the
3307                                   driver will be used to decide if the instance
3308                                   files are available or not on the target host
3309         :param preserve_ephemeral: True if the default ephemeral storage
3310                                    partition must be preserved on rebuild
3311         :param migration: a Migration object if one was created for this
3312                           rebuild operation (if it's a part of evacuate)
3313         :param scheduled_node: A node of the host chosen by the scheduler. If a
3314                                host was specified by the user, this will be
3315                                None
3316         :param limits: Overcommit limits set by the scheduler. If a host was
3317                        specified by the user, this will be None
3318         :param request_spec: a RequestSpec object used to schedule the instance
3319 
3320         """
3321         # recreate=True means the instance is being evacuated from a failed
3322         # host to a new destination host (this host). The 'recreate' variable
3323         # name is confusing, so rename it to evacuate here at the top, which
3324         # is simpler than renaming a parameter in an RPC versioned method.
3325         evacuate = recreate
3326         context = context.elevated()
3327 
3328         if evacuate:
3329             LOG.info("Evacuating instance", instance=instance)
3330         else:
3331             LOG.info("Rebuilding instance", instance=instance)
3332 
3333         if evacuate:
3334             # This is an evacuation to a new host, so we need to perform a
3335             # resource claim.
3336             rebuild_claim = self.rt.rebuild_claim
3337         else:
3338             # This is a rebuild to the same host, so we don't need to make
3339             # a claim since the instance is already on this host.
3340             rebuild_claim = claims.NopClaim
3341 
3342         if image_ref:
3343             image_meta = objects.ImageMeta.from_image_ref(
3344                 context, self.image_api, image_ref)
3345         elif evacuate:
3346             # For evacuate the API does not send down the image_ref since the
3347             # image does not change so just get it from what was stashed in
3348             # the instance system_metadata when the instance was created (or
3349             # last rebuilt). This also works for volume-backed instances.
3350             image_meta = instance.image_meta
3351         else:
3352             image_meta = objects.ImageMeta()
3353 
3354         # NOTE(mriedem): On an evacuate, we need to update
3355         # the instance's host and node properties to reflect it's
3356         # destination node for the evacuate.
3357         if not scheduled_node:
3358             if evacuate:
3359                 try:
3360                     compute_node = self._get_compute_info(context, self.host)
3361                     scheduled_node = compute_node.hypervisor_hostname
3362                 except exception.ComputeHostNotFound:
3363                     LOG.exception('Failed to get compute_info for %s',
3364                                   self.host)
3365             else:
3366                 scheduled_node = instance.node
3367 
3368         allocs = self.reportclient.get_allocations_for_consumer(
3369                     context, instance.uuid)
3370 
3371         # If the resource claim or group policy validation fails before we
3372         # do anything to the guest or its networking/volumes we want to keep
3373         # the current status rather than put the instance into ERROR status.
3374         instance_state = instance.vm_state
3375         with self._error_out_instance_on_exception(
3376                 context, instance, instance_state=instance_state):
3377             try:
3378                 self._do_rebuild_instance_with_claim(
3379                     context, instance, orig_image_ref,
3380                     image_meta, injected_files, new_pass, orig_sys_metadata,
3381                     bdms, evacuate, on_shared_storage, preserve_ephemeral,
3382                     migration, request_spec, allocs, rebuild_claim,
3383                     scheduled_node, limits)
3384             except (exception.ComputeResourcesUnavailable,
3385                     exception.RescheduledException) as e:
3386                 if isinstance(e, exception.ComputeResourcesUnavailable):
3387                     LOG.debug("Could not rebuild instance on this host, not "
3388                               "enough resources available.", instance=instance)
3389                 else:
3390                     # RescheduledException is raised by the late server group
3391                     # policy check during evacuation if a parallel scheduling
3392                     # violated the policy.
3393                     # We catch the RescheduledException here but we don't have
3394                     # the plumbing to do an actual reschedule so we abort the
3395                     # operation.
3396                     LOG.debug("Could not rebuild instance on this host, "
3397                               "late server group check failed.",
3398                               instance=instance)
3399                 # NOTE(ndipanov): We just abort the build for now and leave a
3400                 # migration record for potential cleanup later
3401                 self._set_migration_status(migration, 'failed')
3402                 # Since the claim failed, we need to remove the allocation
3403                 # created against the destination node. Note that we can only
3404                 # get here when evacuating to a destination node. Rebuilding
3405                 # on the same host (not evacuate) uses the NopClaim which will
3406                 # not raise ComputeResourcesUnavailable.
3407                 self.rt.delete_allocation_for_evacuated_instance(
3408                     context, instance, scheduled_node, node_type='destination')
3409                 self._notify_instance_rebuild_error(context, instance, e, bdms)
3410                 # Wrap this in InstanceFaultRollback so that the
3411                 # _error_out_instance_on_exception context manager keeps the
3412                 # vm_state unchanged.
3413                 raise exception.InstanceFaultRollback(
3414                     inner_exception=exception.BuildAbortException(
3415                         instance_uuid=instance.uuid,
3416                         reason=e.format_message()))
3417             except (exception.InstanceNotFound,
3418                     exception.UnexpectedDeletingTaskStateError) as e:
3419                 LOG.debug('Instance was deleted while rebuilding',
3420                           instance=instance)
3421                 self._set_migration_status(migration, 'failed')
3422                 self._notify_instance_rebuild_error(context, instance, e, bdms)
3423             except Exception as e:
3424                 self._set_migration_status(migration, 'failed')
3425                 if evacuate or scheduled_node is not None:
3426                     self.rt.delete_allocation_for_evacuated_instance(
3427                         context, instance, scheduled_node,
3428                         node_type='destination')
3429                 self._notify_instance_rebuild_error(context, instance, e, bdms)
3430                 raise
3431             else:
3432                 instance.apply_migration_context()
3433                 # NOTE (ndipanov): This save will now update the host and node
3434                 # attributes making sure that next RT pass is consistent since
3435                 # it will be based on the instance and not the migration DB
3436                 # entry.
3437                 instance.host = self.host
3438                 instance.node = scheduled_node
3439                 instance.save()
3440                 instance.drop_migration_context()
3441 
3442                 # NOTE (ndipanov): Mark the migration as done only after we
3443                 # mark the instance as belonging to this host.
3444                 self._set_migration_status(migration, 'done')
3445 
3446     def _do_rebuild_instance_with_claim(
3447             self, context, instance, orig_image_ref, image_meta,
3448             injected_files, new_pass, orig_sys_metadata, bdms, evacuate,
3449             on_shared_storage, preserve_ephemeral, migration, request_spec,
3450             allocations, rebuild_claim, scheduled_node, limits):
3451         """Helper to avoid deep nesting in the top-level method."""
3452 
3453         provider_mapping = None
3454         if evacuate:
3455             provider_mapping = self._get_request_group_mapping(request_spec)
3456 
3457             if provider_mapping:
3458                 compute_utils.\
3459                     update_pci_request_spec_with_allocated_interface_name(
3460                         context, self.reportclient, instance, provider_mapping)
3461 
3462         claim_context = rebuild_claim(
3463             context, instance, scheduled_node, allocations,
3464             limits=limits, image_meta=image_meta, migration=migration)
3465 
3466         with claim_context:
3467             self._do_rebuild_instance(
3468                 context, instance, orig_image_ref, image_meta, injected_files,
3469                 new_pass, orig_sys_metadata, bdms, evacuate, on_shared_storage,
3470                 preserve_ephemeral, migration, request_spec, allocations,
3471                 provider_mapping)
3472 
3473     @staticmethod
3474     def _get_image_name(image_meta):
3475         if image_meta.obj_attr_is_set("name"):
3476             return image_meta.name
3477         else:
3478             return ''
3479 
3480     def _do_rebuild_instance(self, context, instance, orig_image_ref,
3481                              image_meta, injected_files, new_pass,
3482                              orig_sys_metadata, bdms, evacuate,
3483                              on_shared_storage, preserve_ephemeral,
3484                              migration, request_spec, allocations,
3485                              request_group_resource_providers_mapping):
3486         orig_vm_state = instance.vm_state
3487 
3488         if evacuate:
3489             if request_spec:
3490                 # NOTE(gibi): Do a late check of server group policy as
3491                 # parallel scheduling could violate such policy. This will
3492                 # cause the evacuate to fail as rebuild does not implement
3493                 # reschedule.
3494                 hints = self._get_scheduler_hints({}, request_spec)
3495                 self._validate_instance_group_policy(context, instance, hints)
3496 
3497             if not self.driver.capabilities.get("supports_evacuate", False):
3498                 raise exception.InstanceEvacuateNotSupported
3499 
3500             self._check_instance_exists(instance)
3501 
3502             if on_shared_storage is None:
3503                 LOG.debug('on_shared_storage is not provided, using driver '
3504                           'information to decide if the instance needs to '
3505                           'be evacuated')
3506                 on_shared_storage = self.driver.instance_on_disk(instance)
3507 
3508             elif (on_shared_storage !=
3509                     self.driver.instance_on_disk(instance)):
3510                 # To cover case when admin expects that instance files are
3511                 # on shared storage, but not accessible and vice versa
3512                 raise exception.InvalidSharedStorage(
3513                         _("Invalid state of instance files on shared"
3514                             " storage"))
3515 
3516             if on_shared_storage:
3517                 LOG.info('disk on shared storage, evacuating using'
3518                          ' existing disk')
3519             elif instance.image_ref:
3520                 orig_image_ref = instance.image_ref
3521                 LOG.info("disk not on shared storage, evacuating from "
3522                          "image: '%s'", str(orig_image_ref))
3523             else:
3524                 LOG.info('disk on volume, evacuating using existing '
3525                          'volume')
3526 
3527         # We check trusted certs capabilities for both evacuate (rebuild on
3528         # another host) and rebuild (rebuild on the same host) because for
3529         # evacuate we need to make sure an instance with trusted certs can
3530         # have the image verified with those certs during rebuild, and for
3531         # rebuild we could be rebuilding a server that started out with no
3532         # trusted certs on this host, and then was rebuilt with trusted certs
3533         # for a new image, in which case we need to validate that new image
3534         # with the trusted certs during the rebuild.
3535         self._check_trusted_certs(instance)
3536 
3537         # This instance.exists message should contain the original
3538         # image_ref, not the new one.  Since the DB has been updated
3539         # to point to the new one... we have to override it.
3540         orig_image_ref_url = self.image_api.generate_image_url(orig_image_ref,
3541                                                                context)
3542         extra_usage_info = {'image_ref_url': orig_image_ref_url}
3543         compute_utils.notify_usage_exists(
3544                 self.notifier, context, instance, self.host,
3545                 current_period=True, system_metadata=orig_sys_metadata,
3546                 extra_usage_info=extra_usage_info)
3547 
3548         # This message should contain the new image_ref
3549         extra_usage_info = {'image_name': self._get_image_name(image_meta)}
3550         self._notify_about_instance_usage(context, instance,
3551                 "rebuild.start", extra_usage_info=extra_usage_info)
3552         # NOTE: image_name is not included in the versioned notification
3553         # because we already provide the image_uuid in the notification
3554         # payload and the image details can be looked up via the uuid.
3555         compute_utils.notify_about_instance_rebuild(
3556             context, instance, self.host,
3557             phase=fields.NotificationPhase.START,
3558             bdms=bdms)
3559 
3560         instance.power_state = self._get_power_state(instance)
3561         instance.task_state = task_states.REBUILDING
3562         instance.save(expected_task_state=[task_states.REBUILDING])
3563 
3564         if evacuate:
3565             self.network_api.setup_networks_on_host(
3566                     context, instance, self.host)
3567             # For nova-network this is needed to move floating IPs
3568             # For neutron this updates the host in the port binding
3569             # TODO(cfriesen): this network_api call and the one above
3570             # are so similar, we should really try to unify them.
3571             self.network_api.setup_instance_network_on_host(
3572                 context, instance, self.host, migration,
3573                 provider_mappings=request_group_resource_providers_mapping)
3574             # TODO(mriedem): Consider decorating setup_instance_network_on_host
3575             # with @api.refresh_cache and then we wouldn't need this explicit
3576             # call to get_instance_nw_info.
3577             network_info = self.network_api.get_instance_nw_info(context,
3578                                                                  instance)
3579         else:
3580             network_info = instance.get_network_info()
3581 
3582         if bdms is None:
3583             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
3584                     context, instance.uuid)
3585 
3586         block_device_info = \
3587             self._get_instance_block_device_info(
3588                     context, instance, bdms=bdms)
3589 
3590         def detach_block_devices(context, bdms):
3591             for bdm in bdms:
3592                 if bdm.is_volume:
3593                     # NOTE (ildikov): Having the attachment_id set in the BDM
3594                     # means that it's the new Cinder attach/detach flow
3595                     # (available from v3.44). In that case we explicitly
3596                     # attach and detach the volumes through attachment level
3597                     # operations. In this scenario _detach_volume will delete
3598                     # the existing attachment which would make the volume
3599                     # status change to 'available' if we don't pre-create
3600                     # another empty attachment before deleting the old one.
3601                     attachment_id = None
3602                     if bdm.attachment_id:
3603                         attachment_id = self.volume_api.attachment_create(
3604                             context, bdm['volume_id'], instance.uuid)['id']
3605                     self._detach_volume(context, bdm, instance,
3606                                         destroy_bdm=False)
3607                     if attachment_id:
3608                         bdm.attachment_id = attachment_id
3609                         bdm.save()
3610 
3611         files = self._decode_files(injected_files)
3612 
3613         kwargs = dict(
3614             context=context,
3615             instance=instance,
3616             image_meta=image_meta,
3617             injected_files=files,
3618             admin_password=new_pass,
3619             allocations=allocations,
3620             bdms=bdms,
3621             detach_block_devices=detach_block_devices,
3622             attach_block_devices=self._prep_block_device,
3623             block_device_info=block_device_info,
3624             network_info=network_info,
3625             preserve_ephemeral=preserve_ephemeral,
3626             evacuate=evacuate)
3627         try:
3628             with instance.mutated_migration_context():
3629                 self.driver.rebuild(**kwargs)
3630         except NotImplementedError:
3631             # NOTE(rpodolyaka): driver doesn't provide specialized version
3632             # of rebuild, fall back to the default implementation
3633             self._rebuild_default_impl(**kwargs)
3634         self._update_instance_after_spawn(instance)
3635         instance.save(expected_task_state=[task_states.REBUILD_SPAWNING])
3636 
3637         if orig_vm_state == vm_states.STOPPED:
3638             LOG.info("bringing vm to original state: '%s'",
3639                      orig_vm_state, instance=instance)
3640             instance.vm_state = vm_states.ACTIVE
3641             instance.task_state = task_states.POWERING_OFF
3642             instance.progress = 0
3643             instance.save()
3644             self.stop_instance(context, instance, False)
3645         # TODO(melwitt): We should clean up instance console tokens here in the
3646         # case of evacuate. The instance is on a new host and will need to
3647         # establish a new console connection.
3648         self._update_scheduler_instance_info(context, instance)
3649         self._notify_about_instance_usage(
3650                 context, instance, "rebuild.end",
3651                 network_info=network_info,
3652                 extra_usage_info=extra_usage_info)
3653         compute_utils.notify_about_instance_rebuild(
3654             context, instance, self.host,
3655             phase=fields.NotificationPhase.END,
3656             bdms=bdms)
3657 
3658     def _handle_bad_volumes_detached(self, context, instance, bad_devices,
3659                                      block_device_info):
3660         """Handle cases where the virt-layer had to detach non-working volumes
3661         in order to complete an operation.
3662         """
3663         for bdm in block_device_info['block_device_mapping']:
3664             if bdm.get('mount_device') in bad_devices:
3665                 try:
3666                     volume_id = bdm['connection_info']['data']['volume_id']
3667                 except KeyError:
3668                     continue
3669 
3670                 # NOTE(sirp): ideally we'd just call
3671                 # `compute_api.detach_volume` here but since that hits the
3672                 # DB directly, that's off limits from within the
3673                 # compute-manager.
3674                 #
3675                 # API-detach
3676                 LOG.info("Detaching from volume api: %s", volume_id)
3677                 self.volume_api.begin_detaching(context, volume_id)
3678 
3679                 # Manager-detach
3680                 self.detach_volume(context, volume_id, instance)
3681 
3682     def _get_accel_info(self, context, instance):
3683         dp_name = instance.flavor.extra_specs.get('accel:device_profile')
3684         if dp_name:
3685             cyclient = cyborg.get_client(context)
3686             accel_info = cyclient.get_arqs_for_instance(instance.uuid)
3687         else:
3688             accel_info = []
3689         return accel_info
3690 
3691     @wrap_exception()
3692     @reverts_task_state
3693     @wrap_instance_event(prefix='compute')
3694     @wrap_instance_fault
3695     def reboot_instance(self, context, instance, block_device_info,
3696                         reboot_type):
3697         @utils.synchronized(instance.uuid)
3698         def do_reboot_instance(context, instance, block_device_info,
3699                                reboot_type):
3700             self._reboot_instance(context, instance, block_device_info,
3701                                   reboot_type)
3702         do_reboot_instance(context, instance, block_device_info, reboot_type)
3703 
3704     def _reboot_instance(self, context, instance, block_device_info,
3705                          reboot_type):
3706         """Reboot an instance on this host."""
3707         # acknowledge the request made it to the manager
3708         if reboot_type == "SOFT":
3709             instance.task_state = task_states.REBOOT_PENDING
3710             expected_states = task_states.soft_reboot_states
3711         else:
3712             instance.task_state = task_states.REBOOT_PENDING_HARD
3713             expected_states = task_states.hard_reboot_states
3714 
3715         context = context.elevated()
3716         LOG.info("Rebooting instance", instance=instance)
3717 
3718         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
3719             context, instance.uuid)
3720         block_device_info = self._get_instance_block_device_info(
3721             context, instance, bdms=bdms)
3722 
3723         network_info = self.network_api.get_instance_nw_info(context, instance)
3724 
3725         accel_info = self._get_accel_info(context, instance)
3726 
3727         self._notify_about_instance_usage(context, instance, "reboot.start")
3728         compute_utils.notify_about_instance_action(
3729             context, instance, self.host,
3730             action=fields.NotificationAction.REBOOT,
3731             phase=fields.NotificationPhase.START,
3732             bdms=bdms
3733         )
3734 
3735         instance.power_state = self._get_power_state(instance)
3736         instance.save(expected_task_state=expected_states)
3737 
3738         if instance.power_state != power_state.RUNNING:
3739             state = instance.power_state
3740             running = power_state.RUNNING
3741             LOG.warning('trying to reboot a non-running instance:'
3742                         ' (state: %(state)s expected: %(running)s)',
3743                         {'state': state, 'running': running},
3744                         instance=instance)
3745 
3746         def bad_volumes_callback(bad_devices):
3747             self._handle_bad_volumes_detached(
3748                     context, instance, bad_devices, block_device_info)
3749 
3750         try:
3751             # Don't change it out of rescue mode
3752             if instance.vm_state == vm_states.RESCUED:
3753                 new_vm_state = vm_states.RESCUED
3754             else:
3755                 new_vm_state = vm_states.ACTIVE
3756             new_power_state = None
3757             if reboot_type == "SOFT":
3758                 instance.task_state = task_states.REBOOT_STARTED
3759                 expected_state = task_states.REBOOT_PENDING
3760             else:
3761                 instance.task_state = task_states.REBOOT_STARTED_HARD
3762                 expected_state = task_states.REBOOT_PENDING_HARD
3763             instance.save(expected_task_state=expected_state)
3764             self.driver.reboot(context, instance,
3765                                network_info,
3766                                reboot_type,
3767                                block_device_info=block_device_info,
3768                                accel_info=accel_info,
3769                                bad_volumes_callback=bad_volumes_callback)
3770 
3771         except Exception as error:
3772             with excutils.save_and_reraise_exception() as ctxt:
3773                 exc_info = sys.exc_info()
3774                 # if the reboot failed but the VM is running don't
3775                 # put it into an error state
3776                 new_power_state = self._get_power_state(instance)
3777                 if new_power_state == power_state.RUNNING:
3778                     LOG.warning('Reboot failed but instance is running',
3779                                 instance=instance)
3780                     compute_utils.add_instance_fault_from_exc(context,
3781                             instance, error, exc_info)
3782                     self._notify_about_instance_usage(context, instance,
3783                             'reboot.error', fault=error)
3784                     compute_utils.notify_about_instance_action(
3785                         context, instance, self.host,
3786                         action=fields.NotificationAction.REBOOT,
3787                         phase=fields.NotificationPhase.ERROR,
3788                         exception=error, bdms=bdms
3789                     )
3790                     ctxt.reraise = False
3791                 else:
3792                     LOG.error('Cannot reboot instance: %s', error,
3793                               instance=instance)
3794                     self._set_instance_obj_error_state(instance)
3795 
3796         if not new_power_state:
3797             new_power_state = self._get_power_state(instance)
3798         try:
3799             instance.power_state = new_power_state
3800             instance.vm_state = new_vm_state
3801             instance.task_state = None
3802             instance.save()
3803         except exception.InstanceNotFound:
3804             LOG.warning("Instance disappeared during reboot",
3805                         instance=instance)
3806 
3807         self._notify_about_instance_usage(context, instance, "reboot.end")
3808         compute_utils.notify_about_instance_action(
3809             context, instance, self.host,
3810             action=fields.NotificationAction.REBOOT,
3811             phase=fields.NotificationPhase.END,
3812             bdms=bdms
3813         )
3814 
3815     @delete_image_on_error
3816     def _do_snapshot_instance(self, context, image_id, instance):
3817         self._snapshot_instance(context, image_id, instance,
3818                                 task_states.IMAGE_BACKUP)
3819 
3820     @wrap_exception()
3821     @reverts_task_state
3822     @wrap_instance_event(prefix='compute')
3823     @wrap_instance_fault
3824     def backup_instance(self, context, image_id, instance, backup_type,
3825                         rotation):
3826         """Backup an instance on this host.
3827 
3828         :param backup_type: daily | weekly
3829         :param rotation: int representing how many backups to keep around
3830         """
3831         self._do_snapshot_instance(context, image_id, instance)
3832         self._rotate_backups(context, instance, backup_type, rotation)
3833 
3834     @wrap_exception()
3835     @reverts_task_state
3836     @wrap_instance_event(prefix='compute')
3837     @wrap_instance_fault
3838     @delete_image_on_error
3839     def snapshot_instance(self, context, image_id, instance):
3840         """Snapshot an instance on this host.
3841 
3842         :param context: security context
3843         :param image_id: glance.db.sqlalchemy.models.Image.Id
3844         :param instance: a nova.objects.instance.Instance object
3845         """
3846         # NOTE(dave-mcnally) the task state will already be set by the api
3847         # but if the compute manager has crashed/been restarted prior to the
3848         # request getting here the task state may have been cleared so we set
3849         # it again and things continue normally
3850         try:
3851             instance.task_state = task_states.IMAGE_SNAPSHOT
3852             instance.save(
3853                         expected_task_state=task_states.IMAGE_SNAPSHOT_PENDING)
3854         except exception.InstanceNotFound:
3855             # possibility instance no longer exists, no point in continuing
3856             LOG.debug("Instance not found, could not set state %s "
3857                       "for instance.",
3858                       task_states.IMAGE_SNAPSHOT, instance=instance)
3859             return
3860 
3861         except exception.UnexpectedDeletingTaskStateError:
3862             LOG.debug("Instance being deleted, snapshot cannot continue",
3863                       instance=instance)
3864             return
3865 
3866         with self._snapshot_semaphore:
3867             self._snapshot_instance(context, image_id, instance,
3868                                     task_states.IMAGE_SNAPSHOT)
3869 
3870     def _snapshot_instance(self, context, image_id, instance,
3871                            expected_task_state):
3872         context = context.elevated()
3873 
3874         instance.power_state = self._get_power_state(instance)
3875         try:
3876             instance.save()
3877 
3878             LOG.info('instance snapshotting', instance=instance)
3879 
3880             if instance.power_state != power_state.RUNNING:
3881                 state = instance.power_state
3882                 running = power_state.RUNNING
3883                 LOG.warning('trying to snapshot a non-running instance: '
3884                             '(state: %(state)s expected: %(running)s)',
3885                             {'state': state, 'running': running},
3886                             instance=instance)
3887 
3888             self._notify_about_instance_usage(
3889                 context, instance, "snapshot.start")
3890             compute_utils.notify_about_instance_snapshot(context, instance,
3891                 self.host, phase=fields.NotificationPhase.START,
3892                 snapshot_image_id=image_id)
3893 
3894             def update_task_state(task_state,
3895                                   expected_state=expected_task_state):
3896                 instance.task_state = task_state
3897                 instance.save(expected_task_state=expected_state)
3898 
3899             with timeutils.StopWatch() as timer:
3900                 self.driver.snapshot(context, instance, image_id,
3901                                      update_task_state)
3902             LOG.info('Took %0.2f seconds to snapshot the instance on '
3903                      'the hypervisor.', timer.elapsed(), instance=instance)
3904 
3905             instance.task_state = None
3906             instance.save(expected_task_state=task_states.IMAGE_UPLOADING)
3907 
3908             self._notify_about_instance_usage(context, instance,
3909                                               "snapshot.end")
3910             compute_utils.notify_about_instance_snapshot(context, instance,
3911                 self.host, phase=fields.NotificationPhase.END,
3912                 snapshot_image_id=image_id)
3913         except (exception.InstanceNotFound,
3914                 exception.InstanceNotRunning,
3915                 exception.UnexpectedDeletingTaskStateError):
3916             # the instance got deleted during the snapshot
3917             # Quickly bail out of here
3918             msg = 'Instance disappeared during snapshot'
3919             LOG.debug(msg, instance=instance)
3920             try:
3921                 image = self.image_api.get(context, image_id)
3922                 if image['status'] != 'active':
3923                     self.image_api.delete(context, image_id)
3924             except exception.ImageNotFound:
3925                 LOG.debug('Image not found during clean up %s', image_id)
3926             except Exception:
3927                 LOG.warning("Error while trying to clean up image %s",
3928                             image_id, instance=instance)
3929         except exception.ImageNotFound:
3930             instance.task_state = None
3931             instance.save()
3932             LOG.warning("Image not found during snapshot", instance=instance)
3933 
3934     def _post_interrupted_snapshot_cleanup(self, context, instance):
3935         self.driver.post_interrupted_snapshot_cleanup(context, instance)
3936 
3937     @messaging.expected_exceptions(NotImplementedError)
3938     @wrap_exception()
3939     def volume_snapshot_create(self, context, instance, volume_id,
3940                                create_info):
3941         try:
3942             self.driver.volume_snapshot_create(context, instance, volume_id,
3943                                                create_info)
3944         except exception.InstanceNotRunning:
3945             # Libvirt driver can raise this exception
3946             LOG.debug('Instance disappeared during volume snapshot create',
3947                       instance=instance)
3948 
3949     @messaging.expected_exceptions(NotImplementedError)
3950     @wrap_exception()
3951     def volume_snapshot_delete(self, context, instance, volume_id,
3952                                snapshot_id, delete_info):
3953         try:
3954             self.driver.volume_snapshot_delete(context, instance, volume_id,
3955                                                snapshot_id, delete_info)
3956         except exception.InstanceNotRunning:
3957             # Libvirt driver can raise this exception
3958             LOG.debug('Instance disappeared during volume snapshot delete',
3959                       instance=instance)
3960 
3961     @wrap_instance_fault
3962     def _rotate_backups(self, context, instance, backup_type, rotation):
3963         """Delete excess backups associated to an instance.
3964 
3965         Instances are allowed a fixed number of backups (the rotation number);
3966         this method deletes the oldest backups that exceed the rotation
3967         threshold.
3968 
3969         :param context: security context
3970         :param instance: Instance dict
3971         :param backup_type: a user-defined type, like "daily" or "weekly" etc.
3972         :param rotation: int representing how many backups to keep around;
3973             None if rotation shouldn't be used (as in the case of snapshots)
3974         """
3975         filters = {'property-image_type': 'backup',
3976                    'property-backup_type': backup_type,
3977                    'property-instance_uuid': instance.uuid}
3978 
3979         images = self.image_api.get_all(context, filters=filters,
3980                                         sort_key='created_at', sort_dir='desc')
3981         num_images = len(images)
3982         LOG.debug("Found %(num_images)d images (rotation: %(rotation)d)",
3983                   {'num_images': num_images, 'rotation': rotation},
3984                   instance=instance)
3985 
3986         if num_images > rotation:
3987             # NOTE(sirp): this deletes all backups that exceed the rotation
3988             # limit
3989             excess = len(images) - rotation
3990             LOG.debug("Rotating out %d backups", excess,
3991                       instance=instance)
3992             for i in range(excess):
3993                 image = images.pop()
3994                 image_id = image['id']
3995                 LOG.debug("Deleting image %s", image_id,
3996                           instance=instance)
3997                 try:
3998                     self.image_api.delete(context, image_id)
3999                 except exception.ImageNotFound:
4000                     LOG.info("Failed to find image %(image_id)s to "
4001                              "delete", {'image_id': image_id},
4002                              instance=instance)
4003                 except (exception.ImageDeleteConflict, Exception) as exc:
4004                     LOG.info("Failed to delete image %(image_id)s during "
4005                              "deleting excess backups. "
4006                              "Continuing for next image.. %(exc)s",
4007                              {'image_id': image_id, 'exc': exc},
4008                              instance=instance)
4009 
4010     @wrap_exception()
4011     @reverts_task_state
4012     @wrap_instance_event(prefix='compute')
4013     @wrap_instance_fault
4014     def set_admin_password(self, context, instance, new_pass):
4015         """Set the root/admin password for an instance on this host.
4016 
4017         This is generally only called by API password resets after an
4018         image has been built.
4019 
4020         @param context: Nova auth context.
4021         @param instance: Nova instance object.
4022         @param new_pass: The admin password for the instance.
4023         """
4024 
4025         context = context.elevated()
4026         current_power_state = self._get_power_state(instance)
4027         expected_state = power_state.RUNNING
4028 
4029         if current_power_state != expected_state:
4030             instance.task_state = None
4031             instance.save(expected_task_state=task_states.UPDATING_PASSWORD)
4032             _msg = _('instance %s is not running') % instance.uuid
4033             raise exception.InstancePasswordSetFailed(
4034                 instance=instance.uuid, reason=_msg)
4035 
4036         try:
4037             self.driver.set_admin_password(instance, new_pass)
4038             LOG.info("Admin password set", instance=instance)
4039             instance.task_state = None
4040             instance.save(
4041                 expected_task_state=task_states.UPDATING_PASSWORD)
4042         except exception.InstanceAgentNotEnabled:
4043             with excutils.save_and_reraise_exception():
4044                 LOG.debug('Guest agent is not enabled for the instance.',
4045                           instance=instance)
4046                 instance.task_state = None
4047                 instance.save(
4048                     expected_task_state=task_states.UPDATING_PASSWORD)
4049         except exception.SetAdminPasswdNotSupported:
4050             with excutils.save_and_reraise_exception():
4051                 LOG.info('set_admin_password is not supported '
4052                          'by this driver or guest instance.',
4053                          instance=instance)
4054                 instance.task_state = None
4055                 instance.save(
4056                     expected_task_state=task_states.UPDATING_PASSWORD)
4057         except NotImplementedError:
4058             LOG.warning('set_admin_password is not implemented '
4059                         'by this driver or guest instance.',
4060                         instance=instance)
4061             instance.task_state = None
4062             instance.save(
4063                 expected_task_state=task_states.UPDATING_PASSWORD)
4064             raise NotImplementedError(_('set_admin_password is not '
4065                                         'implemented by this driver or guest '
4066                                         'instance.'))
4067         except exception.UnexpectedTaskStateError:
4068             # interrupted by another (most likely delete) task
4069             # do not retry
4070             raise
4071         except Exception:
4072             # Catch all here because this could be anything.
4073             LOG.exception('set_admin_password failed', instance=instance)
4074             # We create a new exception here so that we won't
4075             # potentially reveal password information to the
4076             # API caller.  The real exception is logged above
4077             _msg = _('error setting admin password')
4078             raise exception.InstancePasswordSetFailed(
4079                 instance=instance.uuid, reason=_msg)
4080 
4081     def _get_rescue_image(self, context, instance, rescue_image_ref=None):
4082         """Determine what image should be used to boot the rescue VM."""
4083         # 1. If rescue_image_ref is passed in, use that for rescue.
4084         # 2. Else, use the base image associated with instance's current image.
4085         #       The idea here is to provide the customer with a rescue
4086         #       environment which they are familiar with.
4087         #       So, if they built their instance off of a Debian image,
4088         #       their rescue VM will also be Debian.
4089         # 3. As a last resort, use instance's current image.
4090         if not rescue_image_ref:
4091             system_meta = utils.instance_sys_meta(instance)
4092             rescue_image_ref = system_meta.get('image_base_image_ref')
4093 
4094         if not rescue_image_ref:
4095             LOG.warning('Unable to find a different image to use for '
4096                         'rescue VM, using instance\'s current image',
4097                         instance=instance)
4098             rescue_image_ref = instance.image_ref
4099 
4100         return objects.ImageMeta.from_image_ref(
4101             context, self.image_api, rescue_image_ref)
4102 
4103     @wrap_exception()
4104     @reverts_task_state
4105     @wrap_instance_event(prefix='compute')
4106     @wrap_instance_fault
4107     def rescue_instance(self, context, instance, rescue_password,
4108                         rescue_image_ref, clean_shutdown):
4109         context = context.elevated()
4110         LOG.info('Rescuing', instance=instance)
4111 
4112         admin_password = (rescue_password if rescue_password else
4113                       utils.generate_password())
4114 
4115         network_info = self.network_api.get_instance_nw_info(context, instance)
4116 
4117         rescue_image_meta = self._get_rescue_image(context, instance,
4118                                                    rescue_image_ref)
4119 
4120         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
4121                                               context, instance.uuid)
4122         block_device_info = self._get_instance_block_device_info(
4123                                 context, instance, bdms=bdms)
4124 
4125         extra_usage_info = {'rescue_image_name':
4126                             self._get_image_name(rescue_image_meta)}
4127         self._notify_about_instance_usage(context, instance,
4128                 "rescue.start", extra_usage_info=extra_usage_info,
4129                 network_info=network_info)
4130         compute_utils.notify_about_instance_rescue_action(
4131             context, instance, self.host, rescue_image_ref,
4132             phase=fields.NotificationPhase.START)
4133 
4134         try:
4135             self._power_off_instance(instance, clean_shutdown)
4136 
4137             self.driver.rescue(context, instance, network_info,
4138                                rescue_image_meta, admin_password,
4139                                block_device_info)
4140         except Exception as e:
4141             LOG.exception("Error trying to Rescue Instance",
4142                           instance=instance)
4143             self._set_instance_obj_error_state(instance)
4144             raise exception.InstanceNotRescuable(
4145                 instance_id=instance.uuid,
4146                 reason=_("Driver Error: %s") % e)
4147 
4148         compute_utils.notify_usage_exists(self.notifier, context, instance,
4149                                           self.host, current_period=True)
4150 
4151         instance.vm_state = vm_states.RESCUED
4152         instance.task_state = None
4153         instance.power_state = self._get_power_state(instance)
4154         instance.launched_at = timeutils.utcnow()
4155         instance.save(expected_task_state=task_states.RESCUING)
4156 
4157         self._notify_about_instance_usage(context, instance,
4158                 "rescue.end", extra_usage_info=extra_usage_info,
4159                 network_info=network_info)
4160         compute_utils.notify_about_instance_rescue_action(
4161             context, instance, self.host, rescue_image_ref,
4162             phase=fields.NotificationPhase.END)
4163 
4164     @wrap_exception()
4165     @reverts_task_state
4166     @wrap_instance_event(prefix='compute')
4167     @wrap_instance_fault
4168     def unrescue_instance(self, context, instance):
4169         orig_context = context
4170         context = context.elevated()
4171         LOG.info('Unrescuing', instance=instance)
4172 
4173         network_info = self.network_api.get_instance_nw_info(context, instance)
4174         self._notify_about_instance_usage(context, instance,
4175                 "unrescue.start", network_info=network_info)
4176         compute_utils.notify_about_instance_action(context, instance,
4177             self.host, action=fields.NotificationAction.UNRESCUE,
4178             phase=fields.NotificationPhase.START)
4179 
4180         with self._error_out_instance_on_exception(context, instance):
4181             self.driver.unrescue(orig_context, instance)
4182 
4183         instance.vm_state = vm_states.ACTIVE
4184         instance.task_state = None
4185         instance.power_state = self._get_power_state(instance)
4186         instance.save(expected_task_state=task_states.UNRESCUING)
4187 
4188         self._notify_about_instance_usage(context,
4189                                           instance,
4190                                           "unrescue.end",
4191                                           network_info=network_info)
4192         compute_utils.notify_about_instance_action(context, instance,
4193             self.host, action=fields.NotificationAction.UNRESCUE,
4194             phase=fields.NotificationPhase.END)
4195 
4196     @wrap_exception()
4197     @wrap_instance_fault
4198     def change_instance_metadata(self, context, diff, instance):
4199         """Update the metadata published to the instance."""
4200         LOG.debug("Changing instance metadata according to %r",
4201                   diff, instance=instance)
4202         self.driver.change_instance_metadata(context, instance, diff)
4203 
4204     @wrap_exception()
4205     @wrap_instance_event(prefix='compute')
4206     @errors_out_migration
4207     @wrap_instance_fault
4208     def confirm_resize(self, context, instance, migration):
4209         """Confirms a migration/resize and deletes the 'old' instance.
4210 
4211         This is called from the API and runs on the source host.
4212 
4213         Nothing needs to happen on the destination host at this point since
4214         the instance is already running there. This routine just cleans up the
4215         source host.
4216         """
4217         @utils.synchronized(instance.uuid)
4218         def do_confirm_resize(context, instance, migration):
4219             LOG.debug("Going to confirm migration %s", migration.id,
4220                       instance=instance)
4221 
4222             if migration.status == 'confirmed':
4223                 LOG.info("Migration %s is already confirmed",
4224                          migration.id, instance=instance)
4225                 return
4226 
4227             if migration.status not in ('finished', 'confirming'):
4228                 LOG.warning("Unexpected confirmation status '%(status)s' "
4229                             "of migration %(id)s, exit confirmation process",
4230                             {"status": migration.status, "id": migration.id},
4231                             instance=instance)
4232                 return
4233 
4234             # NOTE(wangpan): Get the instance from db, if it has been
4235             #                deleted, we do nothing and return here
4236             expected_attrs = ['metadata', 'system_metadata', 'flavor']
4237             try:
4238                 instance = objects.Instance.get_by_uuid(
4239                         context, instance.uuid,
4240                         expected_attrs=expected_attrs)
4241             except exception.InstanceNotFound:
4242                 LOG.info("Instance is not found during confirmation",
4243                          instance=instance)
4244                 return
4245 
4246             with self._error_out_instance_on_exception(context, instance):
4247                 try:
4248                     self._confirm_resize(
4249                         context, instance, migration=migration)
4250                 except Exception:
4251                     # Something failed when cleaning up the source host so
4252                     # log a traceback and leave a hint about hard rebooting
4253                     # the server to correct its state in the DB.
4254                     with excutils.save_and_reraise_exception(logger=LOG):
4255                         LOG.exception(
4256                             'Confirm resize failed on source host %s. '
4257                             'Resource allocations in the placement service '
4258                             'will be removed regardless because the instance '
4259                             'is now on the destination host %s. You can try '
4260                             'hard rebooting the instance to correct its '
4261                             'state.', self.host, migration.dest_compute,
4262                             instance=instance)
4263                 finally:
4264                     # Whether an error occurred or not, at this point the
4265                     # instance is on the dest host so to avoid leaking
4266                     # allocations in placement, delete them here.
4267                     self._delete_allocation_after_move(
4268                         context, instance, migration)
4269                     # As the instance is not any more on this host, update the
4270                     # scheduler about the move
4271                     self._delete_scheduler_instance_info(
4272                         context, instance.uuid)
4273                     # The cached flavor information is confusing and useless so
4274                     # unset it
4275                     self._delete_stashed_migration_information(instance)
4276 
4277         do_confirm_resize(context, instance, migration)
4278 
4279     def _get_updated_nw_info_with_pci_mapping(self, nw_info, pci_mapping):
4280         # NOTE(adrianc): This method returns a copy of nw_info if modifications
4281         # are made else it returns the original nw_info.
4282         updated_nw_info = nw_info
4283         if nw_info and pci_mapping:
4284             updated_nw_info = copy.deepcopy(nw_info)
4285             for vif in updated_nw_info:
4286                 if vif['vnic_type'] in network_model.VNIC_TYPES_SRIOV:
4287                     try:
4288                         vif_pci_addr = vif['profile']['pci_slot']
4289                         new_addr = pci_mapping[vif_pci_addr].address
4290                         vif['profile']['pci_slot'] = new_addr
4291                         LOG.debug("Updating VIF's PCI address for VIF %(id)s. "
4292                                   "Original value %(orig_val)s, "
4293                                   "new value %(new_val)s",
4294                                   {'id': vif['id'],
4295                                    'orig_val': vif_pci_addr,
4296                                    'new_val': new_addr})
4297                     except (KeyError, AttributeError):
4298                         with excutils.save_and_reraise_exception():
4299                             # NOTE(adrianc): This should never happen. If we
4300                             # get here it means there is some inconsistency
4301                             # with either 'nw_info' or 'pci_mapping'.
4302                             LOG.error("Unexpected error when updating network "
4303                                       "information with PCI mapping.")
4304         return updated_nw_info
4305 
4306     def _confirm_resize(self, context, instance, migration=None):
4307         """Destroys the source instance."""
4308         self._notify_about_instance_usage(context, instance,
4309                                           "resize.confirm.start")
4310         compute_utils.notify_about_instance_action(context, instance,
4311             self.host, action=fields.NotificationAction.RESIZE_CONFIRM,
4312             phase=fields.NotificationPhase.START)
4313 
4314         # NOTE(tr3buchet): tear down networks on source host
4315         self.network_api.setup_networks_on_host(context, instance,
4316                            migration.source_compute, teardown=True)
4317         network_info = self.network_api.get_instance_nw_info(context,
4318                                                              instance)
4319 
4320         # NOTE(adrianc): Populate old PCI device in VIF profile
4321         # to allow virt driver to properly unplug it from Hypervisor.
4322         pci_mapping = (instance.migration_context.
4323                        get_pci_mapping_for_migration(True))
4324         network_info = self._get_updated_nw_info_with_pci_mapping(
4325             network_info, pci_mapping)
4326 
4327         self.driver.confirm_migration(context, migration, instance,
4328                                       network_info)
4329 
4330         migration.status = 'confirmed'
4331         migration.save()
4332 
4333         # NOTE(mriedem): drop_move_claim relies on
4334         # instance.migration_context so make sure to not call
4335         # instance.drop_migration_context() until after drop_move_claim
4336         # is called.
4337         self.rt.drop_move_claim(
4338             context, instance, migration.source_node, instance.old_flavor,
4339             prefix='old_')
4340         instance.drop_migration_context()
4341 
4342         # NOTE(mriedem): The old_vm_state could be STOPPED but the user
4343         # might have manually powered up the instance to confirm the
4344         # resize/migrate, so we need to check the current power state
4345         # on the instance and set the vm_state appropriately. We default
4346         # to ACTIVE because if the power state is not SHUTDOWN, we
4347         # assume _sync_instance_power_state will clean it up.
4348         p_state = instance.power_state
4349         vm_state = None
4350         if p_state == power_state.SHUTDOWN:
4351             vm_state = vm_states.STOPPED
4352             LOG.debug("Resized/migrated instance is powered off. "
4353                       "Setting vm_state to '%s'.", vm_state,
4354                       instance=instance)
4355         else:
4356             vm_state = vm_states.ACTIVE
4357 
4358         instance.vm_state = vm_state
4359         instance.task_state = None
4360         instance.save(expected_task_state=[None, task_states.DELETING,
4361                                            task_states.SOFT_DELETING])
4362 
4363         self._notify_about_instance_usage(
4364             context, instance, "resize.confirm.end",
4365             network_info=network_info)
4366         compute_utils.notify_about_instance_action(context, instance,
4367                self.host, action=fields.NotificationAction.RESIZE_CONFIRM,
4368                phase=fields.NotificationPhase.END)
4369 
4370     def _delete_allocation_after_move(self, context, instance, migration):
4371         """Deletes resource allocations held by the migration record against
4372         the source compute node resource provider after a confirmed cold /
4373         successful live migration.
4374         """
4375         try:
4376             # NOTE(danms): We're finishing on the source node, so try
4377             # to delete the allocation based on the migration uuid
4378             self.reportclient.delete_allocation_for_instance(
4379                 context, migration.uuid, consumer_type='migration')
4380         except exception.AllocationDeleteFailed:
4381             LOG.error('Deleting allocation in placement for migration '
4382                       '%(migration_uuid)s failed. The instance '
4383                       '%(instance_uuid)s will be put to ERROR state '
4384                       'but the allocation held by the migration is '
4385                       'leaked.',
4386                       {'instance_uuid': instance.uuid,
4387                        'migration_uuid': migration.uuid})
4388             raise
4389 
4390     def _delete_stashed_migration_information(
4391         self, instance: 'objects.Instance',
4392     ) -> None:
4393         """Remove information about the flavor change after a resize."""
4394         instance.old_flavor = None
4395         instance.new_flavor = None
4396         instance.system_metadata.pop('old_vm_state', None)
4397         instance.save()
4398 
4399     @wrap_exception()
4400     @wrap_instance_event(prefix='compute')
4401     @errors_out_migration
4402     @wrap_instance_fault
4403     def confirm_snapshot_based_resize_at_source(
4404             self, ctxt, instance, migration):
4405         """Confirms a snapshot-based resize on the source host.
4406 
4407         Cleans the guest from the source hypervisor including disks and drops
4408         the MoveClaim which will free up "old_flavor" usage from the
4409         ResourceTracker.
4410 
4411         Deletes the allocations held by the migration consumer against the
4412         source compute node resource provider.
4413 
4414         :param ctxt: nova auth request context targeted at the source cell
4415         :param instance: Instance object being resized which should have the
4416             "old_flavor" attribute set
4417         :param migration: Migration object for the resize operation
4418         """
4419 
4420         @utils.synchronized(instance.uuid)
4421         def do_confirm():
4422             LOG.info('Confirming resize on source host.', instance=instance)
4423             with self._error_out_instance_on_exception(ctxt, instance):
4424                 # TODO(mriedem): Could probably make this try/except/finally
4425                 # a context manager to share with confirm_resize().
4426                 try:
4427                     self._confirm_snapshot_based_resize_at_source(
4428                         ctxt, instance, migration)
4429                 except Exception:
4430                     # Something failed when cleaning up the source host so
4431                     # log a traceback and leave a hint about hard rebooting
4432                     # the server to correct its state in the DB.
4433                     with excutils.save_and_reraise_exception(logger=LOG):
4434                         LOG.exception(
4435                             'Confirm resize failed on source host %s. '
4436                             'Resource allocations in the placement service '
4437                             'will be removed regardless because the instance '
4438                             'is now on the destination host %s. You can try '
4439                             'hard rebooting the instance to correct its '
4440                             'state.', self.host, migration.dest_compute,
4441                             instance=instance)
4442                 finally:
4443                     # Whether an error occurred or not, at this point the
4444                     # instance is on the dest host so to avoid leaking
4445                     # allocations in placement, delete them here.
4446                     # TODO(mriedem): Should we catch and just log
4447                     # AllocationDeleteFailed? What is the user's recourse if
4448                     # we got this far but this fails? At this point the
4449                     # instance is on the target host and the allocations
4450                     # could just be manually cleaned up by the operator.
4451                     self._delete_allocation_after_move(ctxt, instance,
4452                                                        migration)
4453         do_confirm()
4454 
4455     def _confirm_snapshot_based_resize_at_source(
4456             self, ctxt, instance, migration):
4457         """Private version of confirm_snapshot_based_resize_at_source
4458 
4459         This allows the main method to be decorated with error handlers.
4460 
4461         :param ctxt: nova auth request context targeted at the source cell
4462         :param instance: Instance object being resized which should have the
4463             "old_flavor" attribute set
4464         :param migration: Migration object for the resize operation
4465         """
4466         # Cleanup the guest from the hypervisor including local disks.
4467         network_info = self.network_api.get_instance_nw_info(ctxt, instance)
4468         LOG.debug('Cleaning up guest from source hypervisor including disks.',
4469                   instance=instance)
4470 
4471         # FIXME(mriedem): Per bug 1809095, _confirm_resize calls
4472         # _get_updated_nw_info_with_pci_mapping here prior to unplugging
4473         # VIFs on the source, but in our case we have already unplugged
4474         # VIFs during prep_snapshot_based_resize_at_source, so what do we
4475         # need to do about those kinds of ports? Do we need to wait to unplug
4476         # VIFs until confirm like normal resize?
4477 
4478         # Note that prep_snapshot_based_resize_at_source already destroyed the
4479         # guest which disconnected volumes and unplugged VIFs but did not
4480         # destroy disks in case something failed during the resize and the
4481         # instance needed to be rebooted or rebuilt on the source host. Now
4482         # that we are confirming the resize we want to cleanup the disks left
4483         # on the source host. We call cleanup() instead of destroy() to avoid
4484         # any InstanceNotFound confusion from the driver since the guest was
4485         # already destroyed on this host. block_device_info=None and
4486         # destroy_vifs=False means cleanup() will not try to disconnect volumes
4487         # or unplug VIFs.
4488         self.driver.cleanup(
4489             ctxt, instance, network_info, block_device_info=None,
4490             destroy_disks=True, destroy_vifs=False)
4491 
4492         # Delete port bindings for the source host.
4493         self._confirm_snapshot_based_resize_delete_port_bindings(
4494             ctxt, instance)
4495 
4496         # Delete volume attachments for the source host.
4497         self._delete_volume_attachments(ctxt, instance.get_bdms())
4498 
4499         # Free up the old_flavor usage from the resource tracker for this host.
4500         self.rt.drop_move_claim(
4501             ctxt, instance, migration.source_node, instance.old_flavor,
4502             prefix='old_')
4503         instance.drop_migration_context()
4504 
4505         migration.status = 'confirmed'
4506         migration.save()
4507 
4508     def _confirm_snapshot_based_resize_delete_port_bindings(
4509             self, ctxt, instance):
4510         """Delete port bindings for the source host when confirming
4511         snapshot-based resize on the source host."
4512 
4513         :param ctxt: nova auth RequestContext
4514         :param instance: Instance object that was resized/cold migrated
4515         """
4516         LOG.debug('Deleting port bindings for source host.',
4517                   instance=instance)
4518         try:
4519             self.network_api.cleanup_instance_network_on_host(
4520                 ctxt, instance, self.host)
4521         except exception.PortBindingDeletionFailed as e:
4522             # Do not let this stop us from cleaning up since the guest
4523             # is already gone.
4524             LOG.error('Failed to delete port bindings from source host. '
4525                       'Error: %s', six.text_type(e), instance=instance)
4526 
4527     def _delete_volume_attachments(self, ctxt, bdms):
4528         """Deletes volume attachment records for the given bdms.
4529 
4530         This method will log but not re-raise any exceptions if the volume
4531         attachment delete fails.
4532 
4533         :param ctxt: nova auth request context used to make
4534             DELETE /attachments/{attachment_id} requests to cinder.
4535         :param bdms: objects.BlockDeviceMappingList representing volume
4536             attachments to delete based on BlockDeviceMapping.attachment_id.
4537         """
4538         for bdm in bdms:
4539             if bdm.attachment_id:
4540                 try:
4541                     self.volume_api.attachment_delete(ctxt, bdm.attachment_id)
4542                 except Exception as e:
4543                     LOG.error('Failed to delete volume attachment with ID %s. '
4544                               'Error: %s', bdm.attachment_id, six.text_type(e),
4545                               instance_uuid=bdm.instance_uuid)
4546 
4547     @wrap_exception()
4548     @reverts_task_state
4549     @wrap_instance_event(prefix='compute')
4550     @errors_out_migration
4551     @wrap_instance_fault
4552     def revert_snapshot_based_resize_at_dest(self, ctxt, instance, migration):
4553         """Reverts a snapshot-based resize at the destination host.
4554 
4555         Cleans the guest from the destination compute service host hypervisor
4556         and related resources (ports, volumes) and frees resource usage from
4557         the compute service on that host.
4558 
4559         :param ctxt: nova auth request context targeted at the target cell
4560         :param instance: Instance object whose vm_state is "resized" and
4561             task_state is "resize_reverting".
4562         :param migration: Migration object whose status is "reverting".
4563         """
4564         # A resize revert is essentially a resize back to the old size, so we
4565         # need to send a usage event here.
4566         compute_utils.notify_usage_exists(
4567             self.notifier, ctxt, instance, self.host, current_period=True)
4568 
4569         @utils.synchronized(instance.uuid)
4570         def do_revert():
4571             LOG.info('Reverting resize on destination host.',
4572                      instance=instance)
4573             with self._error_out_instance_on_exception(ctxt, instance):
4574                 self._revert_snapshot_based_resize_at_dest(
4575                     ctxt, instance, migration)
4576         do_revert()
4577 
4578         # Broadcast to all schedulers that the instance is no longer on
4579         # this host and clear any waiting callback events. This is best effort
4580         # so if anything fails just log it.
4581         try:
4582             self._delete_scheduler_instance_info(ctxt, instance.uuid)
4583             self.instance_events.clear_events_for_instance(instance)
4584         except Exception as e:
4585             LOG.warning('revert_snapshot_based_resize_at_dest failed during '
4586                         'post-processing. Error: %s', e, instance=instance)
4587 
4588     def _revert_snapshot_based_resize_at_dest(
4589             self, ctxt, instance, migration):
4590         """Private version of revert_snapshot_based_resize_at_dest.
4591 
4592         This allows the main method to be decorated with error handlers.
4593 
4594         :param ctxt: nova auth request context targeted at the target cell
4595         :param instance: Instance object whose vm_state is "resized" and
4596             task_state is "resize_reverting".
4597         :param migration: Migration object whose status is "reverting".
4598         """
4599         # Cleanup the guest from the hypervisor including local disks.
4600         network_info = self.network_api.get_instance_nw_info(ctxt, instance)
4601         bdms = instance.get_bdms()
4602         block_device_info = self._get_instance_block_device_info(
4603             ctxt, instance, bdms=bdms)
4604         LOG.debug('Destroying guest from destination hypervisor including '
4605                   'disks.', instance=instance)
4606         self.driver.destroy(
4607             ctxt, instance, network_info, block_device_info=block_device_info)
4608 
4609         # Activate source host port bindings. We need to do this before
4610         # deleting the (active) dest host port bindings in
4611         # setup_networks_on_host otherwise the ports will be unbound and
4612         # finish on the source will fail.
4613         # migrate_instance_start uses migration.dest_compute for the port
4614         # binding host and since we want to activate the source host port
4615         # bindings, we need to temporarily mutate the migration object.
4616         with utils.temporary_mutation(
4617                 migration, dest_compute=migration.source_compute):
4618             LOG.debug('Activating port bindings for source host %s.',
4619                       migration.source_compute, instance=instance)
4620             # TODO(mriedem): https://review.opendev.org/#/c/594139/ would allow
4621             # us to remove this and make setup_networks_on_host do it.
4622             # TODO(mriedem): Should we try/except/log any errors but continue?
4623             self.network_api.migrate_instance_start(
4624                 ctxt, instance, migration)
4625 
4626         # Delete port bindings for the target host.
4627         LOG.debug('Deleting port bindings for target host %s.',
4628                   self.host, instance=instance)
4629         try:
4630             # Note that deleting the destination host port bindings does
4631             # not automatically activate the source host port bindings.
4632             self.network_api.cleanup_instance_network_on_host(
4633                 ctxt, instance, self.host)
4634         except exception.PortBindingDeletionFailed as e:
4635             # Do not let this stop us from cleaning up since the guest
4636             # is already gone.
4637             LOG.error('Failed to delete port bindings from target host. '
4638                       'Error: %s', six.text_type(e), instance=instance)
4639 
4640         # Delete any volume attachments remaining for this target host.
4641         LOG.debug('Deleting volume attachments for target host.',
4642                   instance=instance)
4643         self._delete_volume_attachments(ctxt, bdms)
4644 
4645         # Free up the new_flavor usage from the resource tracker for this host.
4646         instance.revert_migration_context()
4647         instance.save(expected_task_state=task_states.RESIZE_REVERTING)
4648         self.rt.drop_move_claim(ctxt, instance, instance.node,
4649                                 instance_type=instance.new_flavor)
4650 
4651     @wrap_exception()
4652     @reverts_task_state
4653     @wrap_instance_event(prefix='compute')
4654     @errors_out_migration
4655     @wrap_instance_fault
4656     def finish_revert_snapshot_based_resize_at_source(
4657         self,
4658         ctxt: nova.context.RequestContext,
4659         instance: 'objects.Instance',
4660         migration: 'objects.Migration',
4661     ) -> None:
4662         """Reverts a snapshot-based resize at the source host.
4663 
4664         Spawn the guest and re-connect volumes/VIFs on the source host and
4665         revert the instance to use the old_flavor for resource usage reporting.
4666 
4667         Updates allocations in the placement service to move the source node
4668         allocations, held by the migration record, to the instance and drop
4669         the allocations held by the instance on the destination node.
4670 
4671         :param ctxt: nova auth request context targeted at the target cell
4672         :param instance: Instance object whose vm_state is "resized" and
4673             task_state is "resize_reverting".
4674         :param migration: Migration object whose status is "reverting".
4675         """
4676 
4677         @utils.synchronized(instance.uuid)
4678         def do_revert():
4679             LOG.info('Reverting resize on source host.', instance=instance)
4680             with self._error_out_instance_on_exception(ctxt, instance):
4681                 self._finish_revert_snapshot_based_resize_at_source(
4682                     ctxt, instance, migration)
4683 
4684         try:
4685             do_revert()
4686         finally:
4687             self._delete_stashed_migration_information(instance)
4688 
4689         # Broadcast to all schedulers that the instance is on this host.
4690         # This is best effort so if anything fails just log it.
4691         try:
4692             self._update_scheduler_instance_info(ctxt, instance)
4693         except Exception as e:
4694             LOG.warning('finish_revert_snapshot_based_resize_at_source failed '
4695                         'during post-processing. Error: %s', e,
4696                         instance=instance)
4697 
4698     def _finish_revert_snapshot_based_resize_at_source(
4699         self,
4700         ctxt: nova.context.RequestContext,
4701         instance: 'objects.Instance',
4702         migration: 'objects.Migration',
4703     ) -> None:
4704         """Private version of finish_revert_snapshot_based_resize_at_source.
4705 
4706         This allows the main method to be decorated with error handlers.
4707 
4708         :param ctxt: nova auth request context targeted at the source cell
4709         :param instance: Instance object whose vm_state is "resized" and
4710             task_state is "resize_reverting".
4711         :param migration: Migration object whose status is "reverting".
4712         """
4713         # Get stashed old_vm_state information to determine if guest should
4714         # be powered on after spawn; we default to ACTIVE for backwards
4715         # compatibility if old_vm_state is not set
4716         old_vm_state = instance.system_metadata.pop(
4717             'old_vm_state', vm_states.ACTIVE)
4718 
4719         # Revert the flavor and host/node fields to their previous values
4720         self._set_instance_info(instance, instance.old_flavor)
4721         instance.host = migration.source_compute
4722         instance.node = migration.source_node
4723         instance.save(expected_task_state=[task_states.RESIZE_REVERTING])
4724 
4725         # Move the allocations against the source compute node resource
4726         # provider, held by the migration, to the instance which will drop
4727         # the destination compute node resource provider allocations held by
4728         # the instance. This puts the allocations against the source node
4729         # back to the old_flavor and owned by the instance.
4730         try:
4731             self._revert_allocation(ctxt, instance, migration)
4732         except exception.AllocationMoveFailed:
4733             # Log the error but do not re-raise because we want to continue to
4734             # process ports and volumes below.
4735             LOG.error('Reverting allocation in placement for migration '
4736                       '%(migration_uuid)s failed. You may need to manually '
4737                       'remove the allocations for the migration consumer '
4738                       'against the source node resource provider '
4739                       '%(source_provider)s and the allocations for the '
4740                       'instance consumer against the destination node '
4741                       'resource provider %(dest_provider)s and then run the '
4742                       '"nova-manage placement heal_allocations" command.',
4743                       {'instance_uuid': instance.uuid,
4744                        'migration_uuid': migration.uuid,
4745                        'source_provider': migration.source_node,
4746                        'dest_provider': migration.dest_node},
4747                       instance=instance)
4748 
4749         bdms = instance.get_bdms()
4750         # prep_snapshot_based_resize_at_source created empty volume attachments
4751         # that we need to update here to get the connection_info before calling
4752         # driver.finish_revert_migration which will connect the volumes to this
4753         # host.
4754         LOG.debug('Updating volume attachments for target host %s.',
4755                   self.host, instance=instance)
4756         # TODO(mriedem): We should probably make _update_volume_attachments
4757         # (optionally) graceful to errors so we (1) try to process all
4758         # attachments and (2) continue to process networking below.
4759         self._update_volume_attachments(ctxt, instance, bdms)
4760 
4761         LOG.debug('Updating port bindings for source host %s.',
4762                   self.host, instance=instance)
4763         # TODO(mriedem): Calculate provider mappings when we support
4764         # cross-cell resize/migrate with ports having resource requests.
4765         self._finish_revert_resize_network_migrate_finish(
4766             ctxt, instance, migration, provider_mappings=None)
4767         network_info = self.network_api.get_instance_nw_info(ctxt, instance)
4768 
4769         # Remember that prep_snapshot_based_resize_at_source destroyed the
4770         # guest but left the disks intact so we cannot call spawn() here but
4771         # finish_revert_migration should do the job.
4772         block_device_info = self._get_instance_block_device_info(
4773             ctxt, instance, bdms=bdms)
4774         power_on = old_vm_state == vm_states.ACTIVE
4775         driver_error = None
4776         try:
4777             self.driver.finish_revert_migration(
4778                 ctxt, instance, network_info, migration,
4779                 block_device_info=block_device_info, power_on=power_on)
4780         except Exception as e:
4781             driver_error = e
4782             # Leave a hint about hard rebooting the guest and reraise so the
4783             # instance is put into ERROR state.
4784             with excutils.save_and_reraise_exception(logger=LOG):
4785                 LOG.error('An error occurred during finish_revert_migration. '
4786                           'The instance may need to be hard rebooted. Error: '
4787                           '%s', driver_error, instance=instance)
4788         else:
4789             # Perform final cleanup of the instance in the database.
4790             instance.drop_migration_context()
4791             # If the original vm_state was STOPPED, set it back to STOPPED.
4792             vm_state = vm_states.ACTIVE if power_on else vm_states.STOPPED
4793             self._update_instance_after_spawn(instance, vm_state=vm_state)
4794             instance.save(expected_task_state=[task_states.RESIZE_REVERTING])
4795         finally:
4796             # Complete any volume attachments so the volumes are in-use. We
4797             # do this regardless of finish_revert_migration failing because
4798             # the instance is back on this host now and we do not want to leave
4799             # the volumes in a pending state in case the instance is hard
4800             # rebooted.
4801             LOG.debug('Completing volume attachments for instance on source '
4802                       'host.', instance=instance)
4803             with excutils.save_and_reraise_exception(
4804                     reraise=driver_error is not None, logger=LOG):
4805                 self._complete_volume_attachments(ctxt, bdms)
4806 
4807         migration.status = 'reverted'
4808         migration.save()
4809 
4810     @wrap_exception()
4811     @reverts_task_state
4812     @wrap_instance_event(prefix='compute')
4813     @errors_out_migration
4814     @wrap_instance_fault
4815     def revert_resize(self, context, instance, migration, request_spec=None):
4816         """Destroys the new instance on the destination machine.
4817 
4818         Reverts the model changes, and powers on the old instance on the
4819         source machine.
4820 
4821         """
4822         # NOTE(comstud): A revert_resize is essentially a resize back to
4823         # the old size, so we need to send a usage event here.
4824         compute_utils.notify_usage_exists(self.notifier, context, instance,
4825                                           self.host, current_period=True)
4826 
4827         with self._error_out_instance_on_exception(context, instance):
4828             # NOTE(tr3buchet): tear down networks on destination host
4829             self.network_api.setup_networks_on_host(context, instance,
4830                                                     teardown=True)
4831 
4832             self.network_api.migrate_instance_start(context,
4833                                                     instance,
4834                                                     migration)
4835 
4836             network_info = self.network_api.get_instance_nw_info(context,
4837                                                                  instance)
4838             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
4839                     context, instance.uuid)
4840             block_device_info = self._get_instance_block_device_info(
4841                                 context, instance, bdms=bdms)
4842 
4843             destroy_disks = not self._is_instance_storage_shared(
4844                 context, instance, host=migration.source_compute)
4845             self.driver.destroy(context, instance, network_info,
4846                                 block_device_info, destroy_disks)
4847 
4848             self._terminate_volume_connections(context, instance, bdms)
4849 
4850             migration.status = 'reverted'
4851             migration.save()
4852 
4853             # NOTE(ndipanov): We need to do this here because dropping the
4854             # claim means we lose the migration_context data. We really should
4855             # fix this by moving the drop_move_claim call to the
4856             # finish_revert_resize method as this is racy (revert is dropped,
4857             # but instance resources will be tracked with the new flavor until
4858             # it gets rolled back in finish_revert_resize, which is
4859             # potentially wrong for a period of time).
4860             instance.revert_migration_context()
4861             instance.save()
4862 
4863             self.rt.drop_move_claim(context, instance, instance.node)
4864 
4865             # RPC cast back to the source host to finish the revert there.
4866             self.compute_rpcapi.finish_revert_resize(context, instance,
4867                     migration, migration.source_compute, request_spec)
4868 
4869     def _finish_revert_resize_network_migrate_finish(
4870             self, context, instance, migration, provider_mappings):
4871         """Causes port binding to be updated. In some Neutron or port
4872         configurations - see NetworkModel.get_bind_time_events() - we
4873         expect the vif-plugged event from Neutron immediately and wait for it.
4874         The rest of the time, the event is expected further along in the
4875         virt driver, so we don't wait here.
4876 
4877         :param context: The request context.
4878         :param instance: The instance undergoing the revert resize.
4879         :param migration: The Migration object of the resize being reverted.
4880         :param provider_mappings: a dict of list of resource provider uuids
4881             keyed by port uuid
4882         :raises: eventlet.timeout.Timeout or
4883                  exception.VirtualInterfacePlugException.
4884         """
4885         network_info = instance.get_network_info()
4886         events = []
4887         deadline = CONF.vif_plugging_timeout
4888         if deadline and network_info:
4889             events = network_info.get_bind_time_events(migration)
4890             if events:
4891                 LOG.debug('Will wait for bind-time events: %s', events)
4892         error_cb = self._neutron_failed_migration_callback
4893         try:
4894             with self.virtapi.wait_for_instance_event(instance, events,
4895                                                       deadline=deadline,
4896                                                       error_callback=error_cb):
4897                 # NOTE(hanrong): we need to change migration.dest_compute to
4898                 # source host temporarily.
4899                 # "network_api.migrate_instance_finish" will setup the network
4900                 # for the instance on the destination host. For revert resize,
4901                 # the instance will back to the source host, the setup of the
4902                 # network for instance should be on the source host. So set
4903                 # the migration.dest_compute to source host at here.
4904                 with utils.temporary_mutation(
4905                         migration, dest_compute=migration.source_compute):
4906                     self.network_api.migrate_instance_finish(
4907                         context, instance, migration, provider_mappings)
4908         except eventlet.timeout.Timeout:
4909             with excutils.save_and_reraise_exception():
4910                 LOG.error('Timeout waiting for Neutron events: %s', events,
4911                           instance=instance)
4912 
4913     @wrap_exception()
4914     @reverts_task_state
4915     @wrap_instance_event(prefix='compute')
4916     @errors_out_migration
4917     @wrap_instance_fault
4918     def finish_revert_resize(
4919         self,
4920         context: nova.context.RequestContext,
4921         instance: 'objects.Instance',
4922         migration: 'objects.Migration',
4923         request_spec: ty.Optional['objects.RequestSpec'] = None,
4924     ) -> None:
4925         """Finishes the second half of reverting a resize on the source host.
4926 
4927         Bring the original source instance state back (active/shutoff) and
4928         revert the resized attributes in the database.
4929         """
4930         try:
4931             with self._error_out_instance_on_exception(context, instance):
4932                 self._finish_revert_resize(
4933                     context, instance, migration, request_spec)
4934         finally:
4935             self._delete_stashed_migration_information(instance)
4936 
4937     def _finish_revert_resize(
4938         self,
4939         context: nova.context.RequestContext,
4940         instance: 'objects.Instance',
4941         migration: 'objects.Migration',
4942         request_spec: ty.Optional['objects.RequestSpec'] = None,
4943     ) -> None:
4944         """Inner version of finish_revert_resize."""
4945         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
4946             context, instance.uuid)
4947         self._notify_about_instance_usage(
4948             context, instance, "resize.revert.start")
4949         compute_utils.notify_about_instance_action(context, instance,
4950             self.host, action=fields.NotificationAction.RESIZE_REVERT,
4951             phase=fields.NotificationPhase.START, bdms=bdms)
4952 
4953         # Get stashed old_vm_state information to determine if guest should
4954         # be powered on after spawn; we default to ACTIVE for backwards
4955         # compatibility if old_vm_state is not set
4956         old_vm_state = instance.system_metadata.get(
4957             'old_vm_state', vm_states.ACTIVE)
4958 
4959         # Revert the flavor and host/node fields to their previous values
4960         self._set_instance_info(instance, instance.old_flavor)
4961         instance.host = migration.source_compute
4962         instance.node = migration.source_node
4963         instance.save(expected_task_state=[task_states.RESIZE_REVERTING])
4964 
4965         try:
4966             source_allocations = self._revert_allocation(
4967                 context, instance, migration)
4968         except exception.AllocationMoveFailed:
4969             LOG.error('Reverting allocation in placement for migration '
4970                       '%(migration_uuid)s failed. The instance '
4971                       '%(instance_uuid)s will be put into ERROR state but '
4972                       'the allocation held by the migration is leaked.',
4973                       {'instance_uuid': instance.uuid,
4974                        'migration_uuid': migration.uuid})
4975             raise
4976 
4977         provider_mappings = self._fill_provider_mapping_based_on_allocs(
4978             context, source_allocations, request_spec)
4979 
4980         self.network_api.setup_networks_on_host(
4981             context, instance, migration.source_compute)
4982         self._finish_revert_resize_network_migrate_finish(
4983             context, instance, migration, provider_mappings)
4984         network_info = self.network_api.get_instance_nw_info(
4985             context, instance)
4986 
4987         # revert_resize deleted any volume attachments for the instance
4988         # and created new ones to be used on this host, but we
4989         # have to update those attachments with the host connector so the
4990         # BDM.connection_info will get set in the call to
4991         # _get_instance_block_device_info below with refresh_conn_info=True
4992         # and then the volumes can be re-connected via the driver on this
4993         # host.
4994         self._update_volume_attachments(context, instance, bdms)
4995 
4996         block_device_info = self._get_instance_block_device_info(
4997             context, instance, refresh_conn_info=True, bdms=bdms)
4998 
4999         power_on = old_vm_state != vm_states.STOPPED
5000         self.driver.finish_revert_migration(
5001             context, instance, network_info, migration, block_device_info,
5002             power_on)
5003 
5004         instance.drop_migration_context()
5005         instance.launched_at = timeutils.utcnow()
5006         instance.save(expected_task_state=task_states.RESIZE_REVERTING)
5007 
5008         # Complete any volume attachments so the volumes are in-use.
5009         self._complete_volume_attachments(context, bdms)
5010 
5011         # if the original vm state was STOPPED, set it back to STOPPED
5012         LOG.info("Updating instance to original state: '%s'",
5013                  old_vm_state, instance=instance)
5014         if power_on:
5015             instance.vm_state = vm_states.ACTIVE
5016             instance.task_state = None
5017             instance.save()
5018         else:
5019             instance.task_state = task_states.POWERING_OFF
5020             instance.save()
5021             self.stop_instance(context, instance=instance,
5022                                clean_shutdown=True)
5023 
5024         self._notify_about_instance_usage(
5025             context, instance, "resize.revert.end")
5026         compute_utils.notify_about_instance_action(context, instance,
5027             self.host, action=fields.NotificationAction.RESIZE_REVERT,
5028             phase=fields.NotificationPhase.END, bdms=bdms)
5029 
5030     def _fill_provider_mapping_based_on_allocs(
5031             self, context, allocations, request_spec):
5032         """Fills and returns the request group - resource provider mapping
5033         based on the allocation passed in.
5034 
5035         :param context: The security context
5036         :param allocation: allocation dict keyed by RP UUID.
5037         :param request_spec: The RequestSpec object associated with the
5038             operation
5039         :returns: None if the request_spec is None. Otherwise a mapping
5040             between RequestGroup requester_id, currently Neutron port_id,
5041             and a list of resource provider UUIDs providing resource for
5042             that RequestGroup.
5043         """
5044         if request_spec:
5045             # NOTE(gibi): We need to re-calculate the resource provider -
5046             # port mapping as we have to have the neutron ports allocate
5047             # from the source compute after revert.
5048             scheduler_utils.fill_provider_mapping_based_on_allocation(
5049                 context, self.reportclient, request_spec, allocations)
5050             provider_mappings = self._get_request_group_mapping(
5051                 request_spec)
5052         else:
5053             # NOTE(gibi): The compute RPC is pinned to be older than 5.2
5054             # and therefore request_spec is not sent. We cannot calculate
5055             # the provider mappings. If the instance has ports with
5056             # resource request then the port update will fail in
5057             # _update_port_binding_for_instance() called via
5058             # _finish_revert_resize_network_migrate_finish() in
5059             # finish_revert_resize.
5060             provider_mappings = None
5061         return provider_mappings
5062 
5063     def _revert_allocation(self, context, instance, migration):
5064         """Revert an allocation that is held by migration to our instance."""
5065 
5066         # Fetch the original allocation that the instance had on the source
5067         # node, which are now held by the migration
5068         orig_alloc = self.reportclient.get_allocations_for_consumer(
5069             context, migration.uuid)
5070         if not orig_alloc:
5071             LOG.error('Did not find resource allocations for migration '
5072                       '%s on source node %s. Unable to revert source node '
5073                       'allocations back to the instance.',
5074                       migration.uuid, migration.source_node, instance=instance)
5075             return False
5076 
5077         LOG.info('Swapping old allocation on %(rp_uuids)s held by migration '
5078                  '%(mig)s for instance',
5079                  {'rp_uuids': orig_alloc.keys(), 'mig': migration.uuid},
5080                  instance=instance)
5081         # FIXME(gibi): This method is flawed in that it does not handle
5082         # allocations against sharing providers in any special way. This leads
5083         # to duplicate allocations against the sharing provider during
5084         # migration.
5085         # TODO(cdent): Should we be doing anything with return values here?
5086         self.reportclient.move_allocations(context, migration.uuid,
5087                                            instance.uuid)
5088         return orig_alloc
5089 
5090     def _prep_resize(self, context, image, instance, instance_type,
5091                      filter_properties, node, migration, request_spec,
5092                      clean_shutdown=True):
5093 
5094         if not filter_properties:
5095             filter_properties = {}
5096 
5097         if not instance.host:
5098             self._set_instance_obj_error_state(instance)
5099             msg = _('Instance has no source host')
5100             raise exception.MigrationError(reason=msg)
5101 
5102         same_host = instance.host == self.host
5103         # if the flavor IDs match, it's migrate; otherwise resize
5104         if same_host and instance_type.id == instance['instance_type_id']:
5105             # check driver whether support migrate to same host
5106             if not self.driver.capabilities.get(
5107                     'supports_migrate_to_same_host', False):
5108                 # Raise InstanceFaultRollback so that the
5109                 # _error_out_instance_on_exception context manager in
5110                 # prep_resize will set the instance.vm_state properly.
5111                 raise exception.InstanceFaultRollback(
5112                     inner_exception=exception.UnableToMigrateToSelf(
5113                         instance_id=instance.uuid, host=self.host))
5114 
5115         # NOTE(danms): Stash the new instance_type to avoid having to
5116         # look it up in the database later
5117         instance.new_flavor = instance_type
5118         # NOTE(mriedem): Stash the old vm_state so we can set the
5119         # resized/reverted instance back to the same state later.
5120         vm_state = instance.vm_state
5121         LOG.debug('Stashing vm_state: %s', vm_state, instance=instance)
5122         instance.system_metadata['old_vm_state'] = vm_state
5123         instance.save()
5124 
5125         if not isinstance(request_spec, objects.RequestSpec):
5126             # Prior to compute RPC API 5.1 conductor would pass a legacy dict
5127             # version of the request spec to compute and since Stein compute
5128             # could be sending that back to conductor on reschedule, so if we
5129             # got a dict convert it to an object.
5130             # TODO(mriedem): We can drop this compat code when we only support
5131             # compute RPC API >=6.0.
5132             request_spec = objects.RequestSpec.from_primitives(
5133                 context, request_spec, filter_properties)
5134             # We don't have to set the new flavor on the request spec because
5135             # if we got here it was due to a reschedule from the compute and
5136             # the request spec would already have the new flavor in it from the
5137             # else block below.
5138 
5139         provider_mapping = self._get_request_group_mapping(request_spec)
5140 
5141         if provider_mapping:
5142             try:
5143                 compute_utils.\
5144                     update_pci_request_spec_with_allocated_interface_name(
5145                         context, self.reportclient, instance, provider_mapping)
5146             except (exception.AmbiguousResourceProviderForPCIRequest,
5147                     exception.UnexpectedResourceProviderNameForPCIRequest
5148                     ) as e:
5149                 raise exception.BuildAbortException(
5150                     reason=six.text_type(e), instance_uuid=instance.uuid)
5151 
5152         limits = filter_properties.get('limits', {})
5153         allocs = self.reportclient.get_allocations_for_consumer(
5154             context, instance.uuid)
5155         with self.rt.resize_claim(context, instance, instance_type, node,
5156                                   migration, allocs, image_meta=image,
5157                                   limits=limits) as claim:
5158             LOG.info('Migrating', instance=instance)
5159             # RPC cast to the source host to start the actual resize/migration.
5160             self.compute_rpcapi.resize_instance(
5161                     context, instance, claim.migration, image,
5162                     instance_type, request_spec, clean_shutdown)
5163 
5164     def _send_prep_resize_notifications(
5165             self, context, instance, phase, flavor):
5166         """Send "resize.prep.*" notifications.
5167 
5168         :param context: nova auth request context
5169         :param instance: The instance being resized
5170         :param phase: The phase of the action (NotificationPhase enum)
5171         :param flavor: The (new) flavor for the resize (same as existing
5172             instance.flavor for a cold migration)
5173         """
5174         # Only send notify_usage_exists if it's the "start" phase.
5175         if phase == fields.NotificationPhase.START:
5176             compute_utils.notify_usage_exists(
5177                 self.notifier, context, instance, self.host,
5178                 current_period=True)
5179 
5180         # Send extra usage info about the flavor if it's the "end" phase for
5181         # the legacy unversioned notification.
5182         extra_usage_info = None
5183         if phase == fields.NotificationPhase.END:
5184             extra_usage_info = dict(
5185                 new_instance_type=flavor.name,
5186                 new_instance_type_id=flavor.id)
5187         self._notify_about_instance_usage(
5188             context, instance, "resize.prep.%s" % phase,
5189             extra_usage_info=extra_usage_info)
5190 
5191         # Send the versioned notification.
5192         compute_utils.notify_about_resize_prep_instance(
5193             context, instance, self.host, phase, flavor)
5194 
5195     @wrap_exception()
5196     @reverts_task_state
5197     @wrap_instance_event(prefix='compute')
5198     @wrap_instance_fault
5199     def prep_resize(self, context, image, instance, instance_type,
5200                     request_spec, filter_properties, node,
5201                     clean_shutdown, migration, host_list):
5202         """Initiates the process of moving a running instance to another host.
5203 
5204         Possibly changes the VCPU, RAM and disk size in the process.
5205 
5206         This is initiated from conductor and runs on the destination host.
5207 
5208         The main purpose of this method is performing some checks on the
5209         destination host and making a claim for resources. If the claim fails
5210         then a reschedule to another host may be attempted which involves
5211         calling back to conductor to start the process over again.
5212         """
5213         if node is None:
5214             node = self._get_nodename(instance, refresh=True)
5215 
5216         # Pass instance_state=instance.vm_state because we can resize
5217         # a STOPPED server and we don't want to set it back to ACTIVE
5218         # in case _prep_resize fails.
5219         instance_state = instance.vm_state
5220         with self._error_out_instance_on_exception(
5221                 context, instance, instance_state=instance_state),\
5222                 errors_out_migration_ctxt(migration):
5223             self._send_prep_resize_notifications(
5224                 context, instance, fields.NotificationPhase.START,
5225                 instance_type)
5226             try:
5227                 self._prep_resize(context, image, instance,
5228                                   instance_type, filter_properties,
5229                                   node, migration, request_spec,
5230                                   clean_shutdown)
5231             except exception.BuildAbortException:
5232                 # NOTE(gibi): We failed
5233                 # update_pci_request_spec_with_allocated_interface_name so
5234                 # there is no reason to re-schedule. Just revert the allocation
5235                 # and fail the migration.
5236                 with excutils.save_and_reraise_exception():
5237                     self._revert_allocation(context, instance, migration)
5238             except Exception:
5239                 # Since we hit a failure, we're either rescheduling or dead
5240                 # and either way we need to cleanup any allocations created
5241                 # by the scheduler for the destination node.
5242                 self._revert_allocation(context, instance, migration)
5243                 # try to re-schedule the resize elsewhere:
5244                 exc_info = sys.exc_info()
5245                 self._reschedule_resize_or_reraise(context, instance,
5246                         exc_info, instance_type, request_spec,
5247                         filter_properties, host_list)
5248             finally:
5249                 self._send_prep_resize_notifications(
5250                     context, instance, fields.NotificationPhase.END,
5251                     instance_type)
5252 
5253     def _reschedule_resize_or_reraise(self, context, instance, exc_info,
5254             instance_type, request_spec, filter_properties, host_list):
5255         """Try to re-schedule the resize or re-raise the original error to
5256         error out the instance.
5257         """
5258         if not filter_properties:
5259             filter_properties = {}
5260 
5261         rescheduled = False
5262         instance_uuid = instance.uuid
5263 
5264         try:
5265             retry = filter_properties.get('retry')
5266             if retry:
5267                 LOG.debug('Rescheduling, attempt %d', retry['num_attempts'],
5268                           instance_uuid=instance_uuid)
5269 
5270                 # reset the task state
5271                 task_state = task_states.RESIZE_PREP
5272                 self._instance_update(context, instance, task_state=task_state)
5273 
5274                 if exc_info:
5275                     # stringify to avoid circular ref problem in json
5276                     # serialization
5277                     retry['exc'] = traceback.format_exception_only(
5278                         exc_info[0], exc_info[1])
5279 
5280                 scheduler_hint = {'filter_properties': filter_properties}
5281 
5282                 self.compute_task_api.resize_instance(
5283                     context, instance, scheduler_hint, instance_type,
5284                     request_spec=request_spec, host_list=host_list)
5285 
5286                 rescheduled = True
5287             else:
5288                 # no retry information, do not reschedule.
5289                 LOG.debug('Retry info not present, will not reschedule',
5290                           instance_uuid=instance_uuid)
5291                 rescheduled = False
5292         except Exception as error:
5293             rescheduled = False
5294             LOG.exception("Error trying to reschedule",
5295                           instance_uuid=instance_uuid)
5296             compute_utils.add_instance_fault_from_exc(context,
5297                     instance, error,
5298                     exc_info=sys.exc_info())
5299             self._notify_about_instance_usage(context, instance,
5300                     'resize.error', fault=error)
5301             compute_utils.notify_about_instance_action(
5302                 context, instance, self.host,
5303                 action=fields.NotificationAction.RESIZE,
5304                 phase=fields.NotificationPhase.ERROR,
5305                 exception=error,
5306             )
5307 
5308         if rescheduled:
5309             self._log_original_error(exc_info, instance_uuid)
5310             compute_utils.add_instance_fault_from_exc(context,
5311                     instance, exc_info[1], exc_info=exc_info)
5312             self._notify_about_instance_usage(context, instance,
5313                     'resize.error', fault=exc_info[1])
5314             compute_utils.notify_about_instance_action(
5315                 context, instance, self.host,
5316                 action=fields.NotificationAction.RESIZE,
5317                 phase=fields.NotificationPhase.ERROR,
5318                 exception=exc_info[1],
5319             )
5320         else:
5321             # not re-scheduling
5322             six.reraise(*exc_info)
5323 
5324     # TODO(stephenfin): Remove unused request_spec parameter in API v6.0
5325     @messaging.expected_exceptions(exception.MigrationPreCheckError)
5326     @wrap_exception()
5327     @wrap_instance_event(prefix='compute')
5328     @wrap_instance_fault
5329     def prep_snapshot_based_resize_at_dest(
5330             self, ctxt, instance, flavor, nodename, migration, limits,
5331             request_spec):
5332         """Performs pre-cross-cell resize resource claim on the dest host.
5333 
5334         This runs on the destination host in a cross-cell resize operation
5335         before the resize is actually started.
5336 
5337         Performs a resize_claim for resources that are not claimed in placement
5338         like PCI devices and NUMA topology.
5339 
5340         Note that this is different from same-cell prep_resize in that this:
5341 
5342         * Does not RPC cast to the source compute, that is orchestrated from
5343           conductor.
5344         * This does not reschedule on failure, conductor handles that since
5345           conductor is synchronously RPC calling this method. As such, the
5346           reverts_task_state decorator is not used on this method.
5347 
5348         :param ctxt: user auth request context
5349         :param instance: the instance being resized
5350         :param flavor: the flavor being resized to (unchanged for cold migrate)
5351         :param nodename: Name of the target compute node
5352         :param migration: nova.objects.Migration object for the operation
5353         :param limits: nova.objects.SchedulerLimits object of resource limits
5354         :param request_spec: nova.objects.RequestSpec object for the operation
5355         :returns: nova.objects.MigrationContext; the migration context created
5356             on the destination host during the resize_claim.
5357         :raises: nova.exception.MigrationPreCheckError if the pre-check
5358             validation fails for the given host selection
5359         """
5360         LOG.debug('Checking if we can cross-cell migrate instance to this '
5361                   'host (%s).', self.host, instance=instance)
5362         self._send_prep_resize_notifications(
5363             ctxt, instance, fields.NotificationPhase.START, flavor)
5364         # TODO(mriedem): update_pci_request_spec_with_allocated_interface_name
5365         # should be called here if the request spec has request group mappings,
5366         # e.g. for things like QoS ports with resource requests. Do it outside
5367         # the try/except so if it raises BuildAbortException we do not attempt
5368         # to reschedule.
5369         try:
5370             # Get the allocations within the try/except block in case we get
5371             # an error so MigrationPreCheckError is raised up.
5372             allocations = self.reportclient.get_allocs_for_consumer(
5373                 ctxt, instance.uuid)['allocations']
5374             # Claim resources on this target host using the new flavor which
5375             # will create the MigrationContext object. Note that in the future
5376             # if we want to do other validation here we should do it within
5377             # the MoveClaim context so we can drop the claim if anything fails.
5378             self.rt.resize_claim(
5379                 ctxt, instance, flavor, nodename, migration, allocations,
5380                 image_meta=instance.image_meta, limits=limits)
5381         except Exception as ex:
5382             err = six.text_type(ex)
5383             LOG.warning(
5384                 'Cross-cell resize pre-checks failed for this host (%s). '
5385                 'Cleaning up. Failure: %s', self.host, err,
5386                 instance=instance, exc_info=True)
5387             raise exception.MigrationPreCheckError(
5388                 reason=(_("Pre-checks failed on host '%(host)s'. "
5389                           "Error: %(error)s") %
5390                         {'host': self.host, 'error': err}))
5391         finally:
5392             self._send_prep_resize_notifications(
5393                 ctxt, instance, fields.NotificationPhase.END, flavor)
5394 
5395         # ResourceTracker.resize_claim() sets instance.migration_context.
5396         return instance.migration_context
5397 
5398     @messaging.expected_exceptions(exception.InstancePowerOffFailure)
5399     @wrap_exception()
5400     @reverts_task_state
5401     @wrap_instance_event(prefix='compute')
5402     @errors_out_migration
5403     @wrap_instance_fault
5404     def prep_snapshot_based_resize_at_source(
5405             self, ctxt, instance, migration, snapshot_id=None):
5406         """Prepares the instance at the source host for cross-cell resize
5407 
5408         Performs actions like powering off the guest, upload snapshot data if
5409         the instance is not volume-backed, disconnecting volumes, unplugging
5410         VIFs and activating the destination host port bindings.
5411 
5412         :param ctxt: user auth request context targeted at source cell
5413         :param instance: nova.objects.Instance; the instance being resized.
5414             The expected instance.task_state is "resize_migrating" when calling
5415             this method, and the expected task_state upon successful completion
5416             is "resize_migrated".
5417         :param migration: nova.objects.Migration object for the operation.
5418             The expected migration.status is "pre-migrating" when calling this
5419             method and the expected status upon successful completion is
5420             "post-migrating".
5421         :param snapshot_id: ID of the image snapshot to upload if not a
5422             volume-backed instance
5423         :raises: nova.exception.InstancePowerOffFailure if stopping the
5424             instance fails
5425         """
5426         LOG.info('Preparing for snapshot based resize on source host %s.',
5427                  self.host, instance=instance)
5428         # Note that if anything fails here, the migration-based allocations
5429         # created in conductor should be reverted by conductor as well,
5430         # see MigrationTask.rollback.
5431         self._prep_snapshot_based_resize_at_source(
5432             ctxt, instance, migration, snapshot_id=snapshot_id)
5433 
5434     @delete_image_on_error
5435     def _snapshot_for_resize(self, ctxt, image_id, instance):
5436         """Uploads snapshot for the instance during a snapshot-based resize
5437 
5438         If the snapshot operation fails the image will be deleted.
5439 
5440         :param ctxt: the nova auth request context for the resize operation
5441         :param image_id: the snapshot image ID
5442         :param instance: the instance to snapshot/resize
5443         """
5444         LOG.debug('Uploading snapshot data for image %s', image_id,
5445                   instance=instance)
5446         # Note that we do not track the snapshot phase task states
5447         # during resize since we do not want to reflect those into the
5448         # actual instance.task_state.
5449         update_task_state = lambda *args, **kwargs: None
5450         with timeutils.StopWatch() as timer:
5451             self.driver.snapshot(ctxt, instance, image_id, update_task_state)
5452             LOG.debug('Took %0.2f seconds to snapshot the instance on '
5453                       'the hypervisor.', timer.elapsed(), instance=instance)
5454 
5455     def _prep_snapshot_based_resize_at_source(
5456             self, ctxt, instance, migration, snapshot_id=None):
5457         """Private method for prep_snapshot_based_resize_at_source so calling
5458         code can handle errors and perform rollbacks as necessary.
5459         """
5460         # Fetch and update the instance.info_cache.
5461         network_info = self.network_api.get_instance_nw_info(ctxt, instance)
5462         # Get the BDMs attached to this instance on this source host.
5463         bdms = instance.get_bdms()
5464         # Send the resize.start notification.
5465         self._send_resize_instance_notifications(
5466             ctxt, instance, bdms, network_info, fields.NotificationPhase.START)
5467         # Update the migration status from "pre-migrating" to "migrating".
5468         migration.status = 'migrating'
5469         migration.save()
5470 
5471         # Since the instance is going to be left on the source host during the
5472         # resize, we need to power it off so we do not have the instance
5473         # potentially running in two places.
5474         LOG.debug('Stopping instance', instance=instance)
5475         try:
5476             self._power_off_instance(instance)
5477         except Exception as e:
5478             LOG.exception('Failed to power off instance.', instance=instance)
5479             raise exception.InstancePowerOffFailure(reason=six.text_type(e))
5480         instance.power_state = self._get_power_state(instance)
5481 
5482         # If a snapshot image ID was provided, we need to snapshot the guest
5483         # disk image and upload it to the image service.
5484         if snapshot_id:
5485             self._snapshot_for_resize(ctxt, snapshot_id, instance)
5486 
5487         block_device_info = self._get_instance_block_device_info(
5488             ctxt, instance, bdms=bdms)
5489 
5490         # If something fails at this point the instance must go to ERROR
5491         # status for operator intervention or to reboot/rebuild the instance.
5492         with self._error_out_instance_on_exception(
5493                 ctxt, instance, instance_state=vm_states.ERROR):
5494 
5495             # Destroy the guest on the source host which will disconnect
5496             # volumes and unplug VIFs. Note that we DO NOT destroy disks since
5497             # we want to leave those on the source host in case of a later
5498             # failure and disks are needed to recover the guest or in case the
5499             # resize is reverted.
5500             LOG.debug('Destroying guest on source host but retaining disks.',
5501                       instance=instance)
5502             self.driver.destroy(
5503                 ctxt, instance, network_info,
5504                 block_device_info=block_device_info, destroy_disks=False)
5505 
5506             # At this point the volumes are disconnected from this source host.
5507             # Delete the old volume attachment records and create new empty
5508             # ones which will be used later if the resize is reverted.
5509             LOG.debug('Deleting volume attachments for the source host.',
5510                       instance=instance)
5511             self._terminate_volume_connections(ctxt, instance, bdms)
5512 
5513             # At this point the VIFs are unplugged from this source host.
5514             # Activate the dest host port bindings created by conductor.
5515             self.network_api.migrate_instance_start(ctxt, instance, migration)
5516 
5517             # Update the migration status from "migrating" to "post-migrating".
5518             migration.status = 'post-migrating'
5519             migration.save()
5520 
5521             # At this point, the traditional resize_instance would update the
5522             # instance host/node values to point at the dest host/node because
5523             # that is where the disk is transferred during resize_instance, but
5524             # with cross-cell resize the instance is not yet at the dest host
5525             # so we do not make that update here.
5526             instance.task_state = task_states.RESIZE_MIGRATED
5527             instance.save(expected_task_state=task_states.RESIZE_MIGRATING)
5528 
5529         self._send_resize_instance_notifications(
5530             ctxt, instance, bdms, network_info,
5531             fields.NotificationPhase.END)
5532         self.instance_events.clear_events_for_instance(instance)
5533 
5534     @wrap_exception()
5535     @reverts_task_state
5536     @wrap_instance_event(prefix='compute')
5537     @wrap_instance_fault
5538     def resize_instance(self, context, instance, image,
5539                         migration, instance_type, clean_shutdown,
5540                         request_spec=None):
5541         """Starts the migration of a running instance to another host.
5542 
5543         This is initiated from the destination host's ``prep_resize`` routine
5544         and runs on the source host.
5545         """
5546         try:
5547             self._resize_instance(context, instance, image, migration,
5548                                   instance_type, clean_shutdown, request_spec)
5549         except Exception:
5550             with excutils.save_and_reraise_exception():
5551                 self._revert_allocation(context, instance, migration)
5552 
5553     def _resize_instance(self, context, instance, image,
5554                          migration, instance_type, clean_shutdown,
5555                          request_spec):
5556         # Pass instance_state=instance.vm_state because we can resize
5557         # a STOPPED server and we don't want to set it back to ACTIVE
5558         # in case migrate_disk_and_power_off raises InstanceFaultRollback.
5559         instance_state = instance.vm_state
5560         with self._error_out_instance_on_exception(
5561                 context, instance, instance_state=instance_state), \
5562              errors_out_migration_ctxt(migration):
5563             network_info = self.network_api.get_instance_nw_info(context,
5564                                                                  instance)
5565 
5566             migration.status = 'migrating'
5567             migration.save()
5568 
5569             instance.task_state = task_states.RESIZE_MIGRATING
5570             instance.save(expected_task_state=task_states.RESIZE_PREP)
5571 
5572             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
5573                     context, instance.uuid)
5574             self._send_resize_instance_notifications(
5575                 context, instance, bdms, network_info,
5576                 fields.NotificationPhase.START)
5577 
5578             block_device_info = self._get_instance_block_device_info(
5579                                 context, instance, bdms=bdms)
5580 
5581             timeout, retry_interval = self._get_power_off_values(
5582                 instance, clean_shutdown)
5583             disk_info = self.driver.migrate_disk_and_power_off(
5584                     context, instance, migration.dest_host,
5585                     instance_type, network_info,
5586                     block_device_info,
5587                     timeout, retry_interval)
5588 
5589             self._terminate_volume_connections(context, instance, bdms)
5590 
5591             self.network_api.migrate_instance_start(context,
5592                                                     instance,
5593                                                     migration)
5594 
5595             migration.status = 'post-migrating'
5596             migration.save()
5597 
5598             instance.host = migration.dest_compute
5599             instance.node = migration.dest_node
5600             instance.task_state = task_states.RESIZE_MIGRATED
5601             instance.save(expected_task_state=task_states.RESIZE_MIGRATING)
5602 
5603             # RPC cast to the destination host to finish the resize/migration.
5604             self.compute_rpcapi.finish_resize(context, instance,
5605                 migration, image, disk_info, migration.dest_compute,
5606                 request_spec)
5607 
5608         self._send_resize_instance_notifications(
5609             context, instance, bdms, network_info,
5610             fields.NotificationPhase.END)
5611         self.instance_events.clear_events_for_instance(instance)
5612 
5613     def _send_resize_instance_notifications(
5614             self, context, instance, bdms, network_info, phase):
5615         """Send "resize.(start|end)" notifications.
5616 
5617         :param context: nova auth request context
5618         :param instance: The instance being resized
5619         :param bdms: BlockDeviceMappingList for the BDMs associated with the
5620             instance
5621         :param network_info: NetworkInfo for the instance info cache of ports
5622         :param phase: The phase of the action (NotificationPhase enum, either
5623             ``start`` or ``end``)
5624         """
5625         action = fields.NotificationAction.RESIZE
5626         # Send the legacy unversioned notification.
5627         self._notify_about_instance_usage(
5628             context, instance, "%s.%s" % (action, phase),
5629             network_info=network_info)
5630         # Send the versioned notification.
5631         compute_utils.notify_about_instance_action(
5632             context, instance, self.host, action=action, phase=phase,
5633             bdms=bdms)
5634 
5635     def _terminate_volume_connections(self, context, instance, bdms):
5636         connector = None
5637         for bdm in bdms:
5638             if bdm.is_volume:
5639                 if bdm.attachment_id:
5640                     # NOTE(jdg): So here's the thing, the idea behind the new
5641                     # attach API's was to have a new code fork/path that we
5642                     # followed, we're not going to do that so we have to do
5643                     # some extra work in here to make it *behave* just like the
5644                     # old code. Cinder doesn't allow disconnect/reconnect (you
5645                     # just delete the attachment and get a new one)
5646                     # attachments in the new attach code so we have to do
5647                     # a delete and create without a connector (reserve),
5648                     # in other words, beware
5649                     attachment_id = self.volume_api.attachment_create(
5650                         context, bdm.volume_id, instance.uuid)['id']
5651                     self.volume_api.attachment_delete(context,
5652                                                       bdm.attachment_id)
5653                     bdm.attachment_id = attachment_id
5654                     bdm.save()
5655 
5656                 else:
5657                     if connector is None:
5658                         connector = self.driver.get_volume_connector(instance)
5659                     self.volume_api.terminate_connection(context,
5660                                                          bdm.volume_id,
5661                                                          connector)
5662 
5663     @staticmethod
5664     def _set_instance_info(instance, instance_type):
5665         instance.instance_type_id = instance_type.id
5666         instance.memory_mb = instance_type.memory_mb
5667         instance.vcpus = instance_type.vcpus
5668         instance.root_gb = instance_type.root_gb
5669         instance.ephemeral_gb = instance_type.ephemeral_gb
5670         instance.flavor = instance_type
5671 
5672     def _update_volume_attachments(self, context, instance, bdms):
5673         """Updates volume attachments using the virt driver host connector.
5674 
5675         :param context: nova.context.RequestContext - user request context
5676         :param instance: nova.objects.Instance
5677         :param bdms: nova.objects.BlockDeviceMappingList - the list of block
5678                      device mappings for the given instance
5679         """
5680         if bdms:
5681             connector = None
5682             for bdm in bdms:
5683                 if bdm.is_volume and bdm.attachment_id:
5684                     if connector is None:
5685                         connector = self.driver.get_volume_connector(instance)
5686                     self.volume_api.attachment_update(
5687                         context, bdm.attachment_id, connector, bdm.device_name)
5688 
5689     def _complete_volume_attachments(self, context, bdms):
5690         """Completes volume attachments for the instance
5691 
5692         :param context: nova.context.RequestContext - user request context
5693         :param bdms: nova.objects.BlockDeviceMappingList - the list of block
5694                      device mappings for the given instance
5695         """
5696         if bdms:
5697             for bdm in bdms:
5698                 if bdm.is_volume and bdm.attachment_id:
5699                     self.volume_api.attachment_complete(
5700                         context, bdm.attachment_id)
5701 
5702     def _finish_resize(self, context, instance, migration, disk_info,
5703                        image_meta, bdms, request_spec):
5704         resize_instance = False  # indicates disks have been resized
5705         old_instance_type_id = migration['old_instance_type_id']
5706         new_instance_type_id = migration['new_instance_type_id']
5707         old_flavor = instance.flavor  # the current flavor is now old
5708         # NOTE(mriedem): Get the old_vm_state so we know if we should
5709         # power on the instance. If old_vm_state is not set we need to default
5710         # to ACTIVE for backwards compatibility
5711         old_vm_state = instance.system_metadata.get('old_vm_state',
5712                                                     vm_states.ACTIVE)
5713         instance.old_flavor = old_flavor
5714 
5715         if old_instance_type_id != new_instance_type_id:
5716             new_flavor = instance.new_flavor  # this is set in _prep_resize
5717             # Set the flavor-related fields on the instance object including
5718             # making instance.flavor = new_flavor.
5719             self._set_instance_info(instance, new_flavor)
5720             for key in ('root_gb', 'swap', 'ephemeral_gb'):
5721                 if old_flavor[key] != new_flavor[key]:
5722                     resize_instance = True
5723                     break
5724         instance.apply_migration_context()
5725 
5726         # NOTE(tr3buchet): setup networks on destination host
5727         self.network_api.setup_networks_on_host(context, instance,
5728                                                 migration.dest_compute)
5729         provider_mappings = self._get_request_group_mapping(request_spec)
5730 
5731         # For neutron, migrate_instance_finish updates port bindings for this
5732         # host including any PCI devices claimed for SR-IOV ports.
5733         self.network_api.migrate_instance_finish(
5734             context, instance, migration, provider_mappings)
5735 
5736         network_info = self.network_api.get_instance_nw_info(context, instance)
5737 
5738         instance.task_state = task_states.RESIZE_FINISH
5739         instance.save(expected_task_state=task_states.RESIZE_MIGRATED)
5740 
5741         self._send_finish_resize_notifications(
5742             context, instance, bdms, network_info,
5743             fields.NotificationPhase.START)
5744 
5745         # We need to update any volume attachments using the destination
5746         # host connector so that we can update the BDM.connection_info
5747         # before calling driver.finish_migration otherwise the driver
5748         # won't know how to connect the volumes to this host.
5749         # Note that _get_instance_block_device_info with
5750         # refresh_conn_info=True will update the BDM.connection_info value
5751         # in the database so we must do this before calling that method.
5752         self._update_volume_attachments(context, instance, bdms)
5753 
5754         block_device_info = self._get_instance_block_device_info(
5755             context, instance, refresh_conn_info=True, bdms=bdms)
5756 
5757         # NOTE(mriedem): If the original vm_state was STOPPED, we don't
5758         # automatically power on the instance after it's migrated
5759         power_on = old_vm_state != vm_states.STOPPED
5760 
5761         # NOTE(sbauza): During a migration, the original allocation is against
5762         # the migration UUID while the target allocation (for the destination
5763         # node) is related to the instance UUID, so here we need to pass the
5764         # new ones.
5765         allocations = self.reportclient.get_allocs_for_consumer(
5766             context, instance.uuid)['allocations']
5767 
5768         try:
5769             self.driver.finish_migration(context, migration, instance,
5770                                          disk_info,
5771                                          network_info,
5772                                          image_meta, resize_instance,
5773                                          allocations,
5774                                          block_device_info, power_on)
5775         except Exception:
5776             # Note that we do not rollback port bindings to the source host
5777             # because resize_instance (on the source host) updated the
5778             # instance.host to point to *this* host (the destination host)
5779             # so the port bindings pointing at this host are correct even
5780             # though we failed to create the guest.
5781             with excutils.save_and_reraise_exception():
5782                 # If we failed to create the guest on this host, reset the
5783                 # instance flavor-related fields to the old flavor. An
5784                 # error handler like reverts_task_state will save the changes.
5785                 if old_instance_type_id != new_instance_type_id:
5786                     self._set_instance_info(instance, old_flavor)
5787 
5788         # Now complete any volume attachments that were previously updated.
5789         self._complete_volume_attachments(context, bdms)
5790 
5791         migration.status = 'finished'
5792         migration.save()
5793 
5794         instance.vm_state = vm_states.RESIZED
5795         instance.task_state = None
5796         instance.launched_at = timeutils.utcnow()
5797         instance.save(expected_task_state=task_states.RESIZE_FINISH)
5798 
5799         return network_info
5800 
5801     @wrap_exception()
5802     @reverts_task_state
5803     @wrap_instance_event(prefix='compute')
5804     @errors_out_migration
5805     @wrap_instance_fault
5806     def finish_resize(self, context, disk_info, image, instance,
5807                       migration, request_spec=None):
5808         """Completes the migration process.
5809 
5810         Sets up the newly transferred disk and turns on the instance at its
5811         new host machine.
5812 
5813         """
5814         try:
5815             self._finish_resize_helper(context, disk_info, image, instance,
5816                                        migration, request_spec)
5817         except Exception:
5818             with excutils.save_and_reraise_exception():
5819                 # At this point, resize_instance (which runs on the source) has
5820                 # already updated the instance host/node values to point to
5821                 # this (the dest) compute, so we need to leave the allocations
5822                 # against the dest node resource provider intact and drop the
5823                 # allocations against the source node resource provider. If the
5824                 # user tries to recover the server by hard rebooting it, it
5825                 # will happen on this host so that's where the allocations
5826                 # should go. Note that this is the same method called from
5827                 # confirm_resize to cleanup the source node allocations held
5828                 # by the migration record.
5829                 LOG.info('Deleting allocations for old flavor on source node '
5830                          '%s after finish_resize failure. You may be able to '
5831                          'recover the instance by hard rebooting it.',
5832                          migration.source_compute, instance=instance)
5833                 self._delete_allocation_after_move(
5834                     context, instance, migration)
5835 
5836     def _finish_resize_helper(self, context, disk_info, image, instance,
5837                               migration, request_spec):
5838         """Completes the migration process.
5839 
5840         The caller must revert the instance's allocations if the migration
5841         process failed.
5842         """
5843         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
5844             context, instance.uuid)
5845 
5846         with self._error_out_instance_on_exception(context, instance):
5847             image_meta = objects.ImageMeta.from_dict(image)
5848             network_info = self._finish_resize(context, instance, migration,
5849                                                disk_info, image_meta, bdms,
5850                                                request_spec)
5851 
5852         # TODO(melwitt): We should clean up instance console tokens here. The
5853         # instance is on a new host and will need to establish a new console
5854         # connection.
5855         self._update_scheduler_instance_info(context, instance)
5856         self._send_finish_resize_notifications(
5857             context, instance, bdms, network_info,
5858             fields.NotificationPhase.END)
5859 
5860     def _send_finish_resize_notifications(
5861             self, context, instance, bdms, network_info, phase):
5862         """Send notifications for the finish_resize flow.
5863 
5864         :param context: nova auth request context
5865         :param instance: The instance being resized
5866         :param bdms: BlockDeviceMappingList for the BDMs associated with the
5867             instance
5868         :param network_info: NetworkInfo for the instance info cache of ports
5869         :param phase: The phase of the action (NotificationPhase enum, either
5870             ``start`` or ``end``)
5871         """
5872         # Send the legacy unversioned notification.
5873         self._notify_about_instance_usage(
5874             context, instance, "finish_resize.%s" % phase,
5875             network_info=network_info)
5876         # Send the versioned notification.
5877         compute_utils.notify_about_instance_action(
5878             context, instance, self.host,
5879             action=fields.NotificationAction.RESIZE_FINISH, phase=phase,
5880             bdms=bdms)
5881 
5882     # TODO(stephenfin): Remove unused request_spec parameter in API v6.0
5883     @wrap_exception()
5884     @reverts_task_state
5885     @wrap_instance_event(prefix='compute')
5886     @errors_out_migration
5887     @wrap_instance_fault
5888     def finish_snapshot_based_resize_at_dest(
5889             self, ctxt, instance, migration, snapshot_id, request_spec):
5890         """Finishes the snapshot-based resize at the destination compute.
5891 
5892         Sets up block devices and networking on the destination compute and
5893         spawns the guest.
5894 
5895         :param ctxt: nova auth request context targeted at the target cell DB
5896         :param instance: The Instance object being resized with the
5897             ``migration_context`` field set. Upon successful completion of this
5898             method the vm_state should be "resized", the task_state should be
5899             None, and migration context, host/node and flavor-related fields
5900             should be set on the instance.
5901         :param migration: The Migration object for this resize operation. Upon
5902             successful completion of this method the migration status should
5903             be "finished".
5904         :param snapshot_id: ID of the image snapshot created for a
5905             non-volume-backed instance, else None.
5906         :param request_spec: nova.objects.RequestSpec object for the operation
5907         """
5908         LOG.info('Finishing snapshot based resize on destination host %s.',
5909                  self.host, instance=instance)
5910         with self._error_out_instance_on_exception(ctxt, instance):
5911             # Note that if anything fails here, the migration-based allocations
5912             # created in conductor should be reverted by conductor as well,
5913             # see MigrationTask.rollback.
5914             self._finish_snapshot_based_resize_at_dest(
5915                 ctxt, instance, migration, snapshot_id)
5916 
5917     def _finish_snapshot_based_resize_at_dest(
5918             self, ctxt, instance, migration, snapshot_id):
5919         """Private variant of finish_snapshot_based_resize_at_dest so the
5920         caller can handle reverting resource allocations on failure and perform
5921         other generic error handling.
5922         """
5923         # Figure out the image metadata to use when spawning the guest.
5924         if snapshot_id:
5925             image_meta = objects.ImageMeta.from_image_ref(
5926                 ctxt, self.image_api, snapshot_id)
5927         else:
5928             # Just use what is already on the volume-backed instance.
5929             image_meta = instance.image_meta
5930 
5931         resize = migration.migration_type == 'resize'
5932         instance.old_flavor = instance.flavor
5933         if resize:
5934             flavor = instance.new_flavor
5935             # If we are resizing to a new flavor we need to set the
5936             # flavor-related fields on the instance.
5937             # NOTE(mriedem): This is likely where storing old/new_flavor on
5938             # the MigrationContext would make this cleaner.
5939             self._set_instance_info(instance, flavor)
5940 
5941         instance.apply_migration_context()
5942         instance.task_state = task_states.RESIZE_FINISH
5943         instance.save(expected_task_state=task_states.RESIZE_MIGRATED)
5944 
5945         # This seems a bit late to be sending the start notification but
5946         # it is what traditional resize has always done as well and it does
5947         # contain the changes to the instance with the new_flavor and
5948         # task_state.
5949         bdms = instance.get_bdms()
5950         network_info = instance.get_network_info()
5951         self._send_finish_resize_notifications(
5952             ctxt, instance, bdms, network_info,
5953             fields.NotificationPhase.START)
5954 
5955         # Setup volumes and networking and spawn the guest in the hypervisor.
5956         self._finish_snapshot_based_resize_at_dest_spawn(
5957             ctxt, instance, migration, image_meta, bdms)
5958 
5959         # If we spawned from a temporary snapshot image we can delete that now,
5960         # similar to how unshelve works.
5961         if snapshot_id:
5962             compute_utils.delete_image(
5963                 ctxt, instance, self.image_api, snapshot_id)
5964 
5965         migration.status = 'finished'
5966         migration.save()
5967 
5968         self._update_instance_after_spawn(instance, vm_state=vm_states.RESIZED)
5969         # Setting the host/node values will make the ResourceTracker continue
5970         # to track usage for this instance on this host.
5971         instance.host = migration.dest_compute
5972         instance.node = migration.dest_node
5973         instance.save(expected_task_state=task_states.RESIZE_FINISH)
5974 
5975         # Broadcast to all schedulers that the instance is on this host.
5976         self._update_scheduler_instance_info(ctxt, instance)
5977         self._send_finish_resize_notifications(
5978             ctxt, instance, bdms, network_info,
5979             fields.NotificationPhase.END)
5980 
5981     def _finish_snapshot_based_resize_at_dest_spawn(
5982             self, ctxt, instance, migration, image_meta, bdms):
5983         """Sets up volumes and networking and spawns the guest on the dest host
5984 
5985         If the instance was stopped when the resize was initiated the guest
5986         will be created but remain in a shutdown power state.
5987 
5988         If the spawn fails, port bindings are rolled back to the source host
5989         and volume connections are terminated for this dest host.
5990 
5991         :param ctxt: nova auth request context
5992         :param instance: Instance object being migrated
5993         :param migration: Migration object for the operation
5994         :param image_meta: ImageMeta object used during driver.spawn
5995         :param bdms: BlockDeviceMappingList of BDMs for the instance
5996         """
5997         # Update the volume attachments using this host's connector.
5998         # That will update the BlockDeviceMapping.connection_info which
5999         # will be used to connect the volumes on this host during spawn().
6000         block_device_info = self._prep_block_device(ctxt, instance, bdms)
6001 
6002         allocations = self.reportclient.get_allocations_for_consumer(
6003             ctxt, instance.uuid)
6004 
6005         # We do not call self.network_api.setup_networks_on_host here because
6006         # for neutron that sets up the port migration profile which is only
6007         # used during live migration with DVR. Yes it is gross knowing what
6008         # that method does internally. We could change this when bug 1814837
6009         # is fixed if setup_networks_on_host is made smarter by passing the
6010         # migration record and the method checks the migration_type.
6011 
6012         # Activate the port bindings for this host.
6013         # FIXME(mriedem): We're going to have the same issue as bug 1813789
6014         # here because this will update the port bindings and send the
6015         # network-vif-plugged event and that means when driver.spawn waits for
6016         # it we might have already gotten the event and neutron won't send
6017         # another one so we could timeout.
6018         # TODO(mriedem): Calculate provider mappings when we support cross-cell
6019         # resize/migrate with ports having resource requests.
6020         self.network_api.migrate_instance_finish(
6021             ctxt, instance, migration, provider_mappings=None)
6022         network_info = self.network_api.get_instance_nw_info(ctxt, instance)
6023 
6024         # If the original vm_state was STOPPED, we do not automatically
6025         # power on the instance after it is migrated.
6026         power_on = instance.system_metadata['old_vm_state'] == vm_states.ACTIVE
6027         try:
6028             # NOTE(mriedem): If this instance uses a config drive, it will get
6029             # rebuilt here which means any personality files will be lost,
6030             # similar to unshelve. If the instance is not using a config drive
6031             # and getting metadata from the metadata API service, personality
6032             # files would be lost regardless of the move operation.
6033             self.driver.spawn(
6034                 ctxt, instance, image_meta, injected_files=[],
6035                 admin_password=None, allocations=allocations,
6036                 network_info=network_info, block_device_info=block_device_info,
6037                 power_on=power_on)
6038         except Exception:
6039             with excutils.save_and_reraise_exception(logger=LOG):
6040                 # Rollback port bindings to the source host.
6041                 try:
6042                     # This is gross but migrate_instance_start looks at the
6043                     # migration.dest_compute to determine where to activate the
6044                     # port bindings and we want the source compute port
6045                     # bindings to be re-activated. Remember at this point the
6046                     # instance.host is still pointing at the source compute.
6047                     # TODO(mriedem): Maybe we should be calling
6048                     # setup_instance_network_on_host here to deal with pci
6049                     # devices?
6050                     with utils.temporary_mutation(
6051                             migration, dest_compute=migration.source_compute):
6052                         self.network_api.migrate_instance_start(
6053                             ctxt, instance, migration)
6054                 except Exception:
6055                     LOG.exception(
6056                         'Failed to activate port bindings on the source '
6057                         'host: %s', migration.source_compute,
6058                         instance=instance)
6059 
6060                 # Rollback volume connections on this host.
6061                 for bdm in bdms:
6062                     if bdm.is_volume:
6063                         try:
6064                             self._remove_volume_connection(
6065                                 ctxt, bdm, instance, delete_attachment=True)
6066                         except Exception:
6067                             LOG.exception('Failed to remove volume connection '
6068                                           'on this host %s for volume %s.',
6069                                           self.host, bdm.volume_id,
6070                                           instance=instance)
6071 
6072     @wrap_exception()
6073     @wrap_instance_fault
6074     def add_fixed_ip_to_instance(self, context, network_id, instance):
6075         """Calls network_api to add new fixed_ip to instance
6076         then injects the new network info and resets instance networking.
6077 
6078         """
6079         self._notify_about_instance_usage(
6080                 context, instance, "create_ip.start")
6081 
6082         network_info = self.network_api.add_fixed_ip_to_instance(context,
6083                                                                  instance,
6084                                                                  network_id)
6085         self._inject_network_info(instance, network_info)
6086         self.reset_network(context, instance)
6087 
6088         # NOTE(russellb) We just want to bump updated_at.  See bug 1143466.
6089         instance.updated_at = timeutils.utcnow()
6090         instance.save()
6091 
6092         self._notify_about_instance_usage(
6093             context, instance, "create_ip.end", network_info=network_info)
6094 
6095     @wrap_exception()
6096     @wrap_instance_fault
6097     def remove_fixed_ip_from_instance(self, context, address, instance):
6098         """Calls network_api to remove existing fixed_ip from instance
6099         by injecting the altered network info and resetting
6100         instance networking.
6101         """
6102         self._notify_about_instance_usage(
6103                 context, instance, "delete_ip.start")
6104 
6105         network_info = self.network_api.remove_fixed_ip_from_instance(context,
6106                                                                       instance,
6107                                                                       address)
6108         self._inject_network_info(instance, network_info)
6109         self.reset_network(context, instance)
6110 
6111         # NOTE(russellb) We just want to bump updated_at.  See bug 1143466.
6112         instance.updated_at = timeutils.utcnow()
6113         instance.save()
6114 
6115         self._notify_about_instance_usage(
6116             context, instance, "delete_ip.end", network_info=network_info)
6117 
6118     @wrap_exception()
6119     @reverts_task_state
6120     @wrap_instance_event(prefix='compute')
6121     @wrap_instance_fault
6122     def pause_instance(self, context, instance):
6123         """Pause an instance on this host."""
6124         context = context.elevated()
6125         LOG.info('Pausing', instance=instance)
6126         self._notify_about_instance_usage(context, instance, 'pause.start')
6127         compute_utils.notify_about_instance_action(context, instance,
6128                self.host, action=fields.NotificationAction.PAUSE,
6129                phase=fields.NotificationPhase.START)
6130         self.driver.pause(instance)
6131         instance.power_state = self._get_power_state(instance)
6132         instance.vm_state = vm_states.PAUSED
6133         instance.task_state = None
6134         instance.save(expected_task_state=task_states.PAUSING)
6135         self._notify_about_instance_usage(context, instance, 'pause.end')
6136         compute_utils.notify_about_instance_action(context, instance,
6137                self.host, action=fields.NotificationAction.PAUSE,
6138                phase=fields.NotificationPhase.END)
6139 
6140     @wrap_exception()
6141     @reverts_task_state
6142     @wrap_instance_event(prefix='compute')
6143     @wrap_instance_fault
6144     def unpause_instance(self, context, instance):
6145         """Unpause a paused instance on this host."""
6146         context = context.elevated()
6147         LOG.info('Unpausing', instance=instance)
6148         self._notify_about_instance_usage(context, instance, 'unpause.start')
6149         compute_utils.notify_about_instance_action(context, instance,
6150             self.host, action=fields.NotificationAction.UNPAUSE,
6151             phase=fields.NotificationPhase.START)
6152         self.driver.unpause(instance)
6153         instance.power_state = self._get_power_state(instance)
6154         instance.vm_state = vm_states.ACTIVE
6155         instance.task_state = None
6156         instance.save(expected_task_state=task_states.UNPAUSING)
6157         self._notify_about_instance_usage(context, instance, 'unpause.end')
6158         compute_utils.notify_about_instance_action(context, instance,
6159             self.host, action=fields.NotificationAction.UNPAUSE,
6160             phase=fields.NotificationPhase.END)
6161 
6162     @wrap_exception()
6163     def host_power_action(self, context, action):
6164         """Reboots, shuts down or powers up the host."""
6165         return self.driver.host_power_action(action)
6166 
6167     @wrap_exception()
6168     def host_maintenance_mode(self, context, host, mode):
6169         """Start/Stop host maintenance window. On start, it triggers
6170         guest VMs evacuation.
6171         """
6172         return self.driver.host_maintenance_mode(host, mode)
6173 
6174     def _update_compute_provider_status(self, context, enabled):
6175         """Adds or removes the COMPUTE_STATUS_DISABLED trait for this host.
6176 
6177         For each ComputeNode managed by this service, adds or removes the
6178         COMPUTE_STATUS_DISABLED traits to/from the associated resource provider
6179         in Placement.
6180 
6181         :param context: nova auth RequestContext
6182         :param enabled: True if the node is enabled in which case the trait
6183             would be removed, False if the node is disabled in which case
6184             the trait would be added.
6185         :raises: ComputeHostNotFound if there are no compute nodes found in
6186             the ResourceTracker for this service.
6187         """
6188         # Get the compute node(s) on this host. Remember that ironic can be
6189         # managing more than one compute node.
6190         nodes = self.rt.compute_nodes.values()
6191         if not nodes:
6192             raise exception.ComputeHostNotFound(host=self.host)
6193         # For each node, we want to add (or remove) the COMPUTE_STATUS_DISABLED
6194         # trait on the related resource provider in placement so the scheduler
6195         # (pre-)filters the provider based on its status.
6196         for node in nodes:
6197             try:
6198                 self.virtapi.update_compute_provider_status(
6199                     context, node.uuid, enabled)
6200             except (exception.ResourceProviderTraitRetrievalFailed,
6201                     exception.ResourceProviderUpdateConflict,
6202                     exception.ResourceProviderUpdateFailed,
6203                     exception.TraitRetrievalFailed) as e:
6204                 # This is best effort so just log a warning and continue.
6205                 LOG.warning('An error occurred while updating '
6206                             'COMPUTE_STATUS_DISABLED trait on compute node '
6207                             'resource provider %s. The trait will be '
6208                             'synchronized when the update_available_resource '
6209                             'periodic task runs. Error: %s',
6210                             node.uuid, e.format_message())
6211             except Exception:
6212                 LOG.exception('An error occurred while updating '
6213                               'COMPUTE_STATUS_DISABLED trait on compute node '
6214                               'resource provider %s. The trait will be '
6215                               'synchronized when the '
6216                               'update_available_resource periodic task runs.',
6217                               node.uuid)
6218 
6219     @wrap_exception()
6220     def set_host_enabled(self, context, enabled):
6221         """Sets the specified host's ability to accept new instances.
6222 
6223         This method will add or remove the COMPUTE_STATUS_DISABLED trait
6224         to/from the associated compute node resource provider(s) for this
6225         compute service.
6226         """
6227         try:
6228             self._update_compute_provider_status(context, enabled)
6229         except exception.ComputeHostNotFound:
6230             LOG.warning('Unable to add/remove trait COMPUTE_STATUS_DISABLED. '
6231                         'No ComputeNode(s) found for host: %s', self.host)
6232 
6233         try:
6234             return self.driver.set_host_enabled(enabled)
6235         except NotImplementedError:
6236             # Only the xenapi driver implements set_host_enabled but we don't
6237             # want NotImplementedError to get raised back to the API. We still
6238             # need to honor the compute RPC API contract and return 'enabled'
6239             # or 'disabled' though.
6240             return 'enabled' if enabled else 'disabled'
6241 
6242     @wrap_exception()
6243     def get_host_uptime(self, context):
6244         """Returns the result of calling "uptime" on the target host."""
6245         return self.driver.get_host_uptime()
6246 
6247     @wrap_exception()
6248     @wrap_instance_fault
6249     def get_diagnostics(self, context, instance):
6250         """Retrieve diagnostics for an instance on this host."""
6251         current_power_state = self._get_power_state(instance)
6252         if current_power_state == power_state.RUNNING:
6253             LOG.info("Retrieving diagnostics", instance=instance)
6254             return self.driver.get_diagnostics(instance)
6255         else:
6256             raise exception.InstanceInvalidState(
6257                 attr='power state',
6258                 instance_uuid=instance.uuid,
6259                 state=power_state.STATE_MAP[instance.power_state],
6260                 method='get_diagnostics')
6261 
6262     @wrap_exception()
6263     @wrap_instance_fault
6264     def get_instance_diagnostics(self, context, instance):
6265         """Retrieve diagnostics for an instance on this host."""
6266         current_power_state = self._get_power_state(instance)
6267         if current_power_state == power_state.RUNNING:
6268             LOG.info("Retrieving diagnostics", instance=instance)
6269             return self.driver.get_instance_diagnostics(instance)
6270         else:
6271             raise exception.InstanceInvalidState(
6272                 attr='power state',
6273                 instance_uuid=instance.uuid,
6274                 state=power_state.STATE_MAP[instance.power_state],
6275                 method='get_diagnostics')
6276 
6277     @wrap_exception()
6278     @reverts_task_state
6279     @wrap_instance_event(prefix='compute')
6280     @wrap_instance_fault
6281     def suspend_instance(self, context, instance):
6282         """Suspend the given instance."""
6283         context = context.elevated()
6284 
6285         # Store the old state
6286         instance.system_metadata['old_vm_state'] = instance.vm_state
6287         self._notify_about_instance_usage(context, instance, 'suspend.start')
6288         compute_utils.notify_about_instance_action(context, instance,
6289                 self.host, action=fields.NotificationAction.SUSPEND,
6290                 phase=fields.NotificationPhase.START)
6291         with self._error_out_instance_on_exception(context, instance,
6292              instance_state=instance.vm_state):
6293             self.driver.suspend(context, instance)
6294         instance.power_state = self._get_power_state(instance)
6295         instance.vm_state = vm_states.SUSPENDED
6296         instance.task_state = None
6297         instance.save(expected_task_state=task_states.SUSPENDING)
6298         self._notify_about_instance_usage(context, instance, 'suspend.end')
6299         compute_utils.notify_about_instance_action(context, instance,
6300                 self.host, action=fields.NotificationAction.SUSPEND,
6301                 phase=fields.NotificationPhase.END)
6302 
6303     @wrap_exception()
6304     @reverts_task_state
6305     @wrap_instance_event(prefix='compute')
6306     @wrap_instance_fault
6307     def resume_instance(self, context, instance):
6308         """Resume the given suspended instance."""
6309         context = context.elevated()
6310         LOG.info('Resuming', instance=instance)
6311 
6312         self._notify_about_instance_usage(context, instance, 'resume.start')
6313 
6314         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
6315             context, instance.uuid)
6316         block_device_info = self._get_instance_block_device_info(
6317             context, instance, bdms=bdms)
6318 
6319         compute_utils.notify_about_instance_action(context, instance,
6320             self.host, action=fields.NotificationAction.RESUME,
6321             phase=fields.NotificationPhase.START, bdms=bdms)
6322 
6323         network_info = self.network_api.get_instance_nw_info(context, instance)
6324 
6325         with self._error_out_instance_on_exception(context, instance,
6326              instance_state=instance.vm_state):
6327             self.driver.resume(context, instance, network_info,
6328                                block_device_info)
6329 
6330         instance.power_state = self._get_power_state(instance)
6331 
6332         # We default to the ACTIVE state for backwards compatibility
6333         instance.vm_state = instance.system_metadata.pop('old_vm_state',
6334                                                          vm_states.ACTIVE)
6335 
6336         instance.task_state = None
6337         instance.save(expected_task_state=task_states.RESUMING)
6338         self._notify_about_instance_usage(context, instance, 'resume.end')
6339         compute_utils.notify_about_instance_action(context, instance,
6340             self.host, action=fields.NotificationAction.RESUME,
6341             phase=fields.NotificationPhase.END, bdms=bdms)
6342 
6343     @wrap_exception()
6344     @reverts_task_state
6345     @wrap_instance_event(prefix='compute')
6346     @wrap_instance_fault
6347     def shelve_instance(self, context, instance, image_id,
6348                         clean_shutdown):
6349         """Shelve an instance.
6350 
6351         This should be used when you want to take a snapshot of the instance.
6352         It also adds system_metadata that can be used by a periodic task to
6353         offload the shelved instance after a period of time.
6354 
6355         :param context: request context
6356         :param instance: an Instance object
6357         :param image_id: an image id to snapshot to.
6358         :param clean_shutdown: give the GuestOS a chance to stop
6359         """
6360 
6361         @utils.synchronized(instance.uuid)
6362         def do_shelve_instance():
6363             self._shelve_instance(context, instance, image_id, clean_shutdown)
6364         do_shelve_instance()
6365 
6366     def _shelve_instance(self, context, instance, image_id,
6367                          clean_shutdown):
6368         LOG.info('Shelving', instance=instance)
6369         offload = CONF.shelved_offload_time == 0
6370         if offload:
6371             # Get the BDMs early so we can pass them into versioned
6372             # notifications since _shelve_offload_instance needs the
6373             # BDMs anyway.
6374             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
6375                 context, instance.uuid)
6376         else:
6377             bdms = None
6378         compute_utils.notify_usage_exists(self.notifier, context, instance,
6379                                           self.host, current_period=True)
6380         self._notify_about_instance_usage(context, instance, 'shelve.start')
6381         compute_utils.notify_about_instance_action(context, instance,
6382                 self.host, action=fields.NotificationAction.SHELVE,
6383                 phase=fields.NotificationPhase.START, bdms=bdms)
6384 
6385         def update_task_state(task_state, expected_state=task_states.SHELVING):
6386             shelving_state_map = {
6387                     task_states.IMAGE_PENDING_UPLOAD:
6388                         task_states.SHELVING_IMAGE_PENDING_UPLOAD,
6389                     task_states.IMAGE_UPLOADING:
6390                         task_states.SHELVING_IMAGE_UPLOADING,
6391                     task_states.SHELVING: task_states.SHELVING}
6392             task_state = shelving_state_map[task_state]
6393             expected_state = shelving_state_map[expected_state]
6394             instance.task_state = task_state
6395             instance.save(expected_task_state=expected_state)
6396         # Do not attempt a clean shutdown of a paused guest since some
6397         # hypervisors will fail the clean shutdown if the guest is not
6398         # running.
6399         if instance.power_state == power_state.PAUSED:
6400             clean_shutdown = False
6401         self._power_off_instance(instance, clean_shutdown)
6402         self.driver.snapshot(context, instance, image_id, update_task_state)
6403 
6404         instance.system_metadata['shelved_at'] = timeutils.utcnow().isoformat()
6405         instance.system_metadata['shelved_image_id'] = image_id
6406         instance.system_metadata['shelved_host'] = self.host
6407         instance.vm_state = vm_states.SHELVED
6408         instance.task_state = None
6409         if CONF.shelved_offload_time == 0:
6410             instance.task_state = task_states.SHELVING_OFFLOADING
6411         instance.power_state = self._get_power_state(instance)
6412         instance.save(expected_task_state=[
6413                 task_states.SHELVING,
6414                 task_states.SHELVING_IMAGE_UPLOADING])
6415 
6416         self._notify_about_instance_usage(context, instance, 'shelve.end')
6417         compute_utils.notify_about_instance_action(context, instance,
6418                 self.host, action=fields.NotificationAction.SHELVE,
6419                 phase=fields.NotificationPhase.END, bdms=bdms)
6420 
6421         if offload:
6422             self._shelve_offload_instance(context, instance,
6423                                           clean_shutdown=False, bdms=bdms)
6424 
6425     @wrap_exception()
6426     @reverts_task_state
6427     @wrap_instance_event(prefix='compute')
6428     @wrap_instance_fault
6429     def shelve_offload_instance(self, context, instance, clean_shutdown):
6430         """Remove a shelved instance from the hypervisor.
6431 
6432         This frees up those resources for use by other instances, but may lead
6433         to slower unshelve times for this instance.  This method is used by
6434         volume backed instances since restoring them doesn't involve the
6435         potentially large download of an image.
6436 
6437         :param context: request context
6438         :param instance: nova.objects.instance.Instance
6439         :param clean_shutdown: give the GuestOS a chance to stop
6440         """
6441 
6442         @utils.synchronized(instance.uuid)
6443         def do_shelve_offload_instance():
6444             self._shelve_offload_instance(context, instance, clean_shutdown)
6445         do_shelve_offload_instance()
6446 
6447     def _shelve_offload_instance(self, context, instance, clean_shutdown,
6448                                  bdms=None):
6449         LOG.info('Shelve offloading', instance=instance)
6450         if bdms is None:
6451             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
6452                 context, instance.uuid)
6453         self._notify_about_instance_usage(context, instance,
6454                 'shelve_offload.start')
6455         compute_utils.notify_about_instance_action(context, instance,
6456                 self.host, action=fields.NotificationAction.SHELVE_OFFLOAD,
6457                 phase=fields.NotificationPhase.START, bdms=bdms)
6458 
6459         self._power_off_instance(instance, clean_shutdown)
6460         current_power_state = self._get_power_state(instance)
6461         network_info = self.network_api.get_instance_nw_info(context, instance)
6462 
6463         block_device_info = self._get_instance_block_device_info(context,
6464                                                                  instance,
6465                                                                  bdms=bdms)
6466         self.driver.destroy(context, instance, network_info,
6467                 block_device_info)
6468 
6469         # the instance is going to be removed from the host so we want to
6470         # terminate all the connections with the volume server and the host
6471         self._terminate_volume_connections(context, instance, bdms)
6472 
6473         # Free up the resource allocations in the placement service.
6474         # This should happen *before* the vm_state is changed to
6475         # SHELVED_OFFLOADED in case client-side code is polling the API to
6476         # schedule more instances (or unshelve) once this server is offloaded.
6477         self.rt.delete_allocation_for_shelve_offloaded_instance(context,
6478                                                                 instance)
6479 
6480         instance.power_state = current_power_state
6481         # NOTE(mriedem): The vm_state has to be set before updating the
6482         # resource tracker, see vm_states.ALLOW_RESOURCE_REMOVAL. The host/node
6483         # values cannot be nulled out until after updating the resource tracker
6484         # though.
6485         instance.vm_state = vm_states.SHELVED_OFFLOADED
6486         instance.task_state = None
6487         instance.save(expected_task_state=[task_states.SHELVING,
6488                                            task_states.SHELVING_OFFLOADING])
6489 
6490         # NOTE(ndipanov): Free resources from the resource tracker
6491         self._update_resource_tracker(context, instance)
6492 
6493         # NOTE(sfinucan): RPC calls should no longer be attempted against this
6494         # instance, so ensure any calls result in errors
6495         self._nil_out_instance_obj_host_and_node(instance)
6496         instance.save(expected_task_state=None)
6497 
6498         # TODO(melwitt): We should clean up instance console tokens here. The
6499         # instance has no host at this point and will need to establish a new
6500         # console connection in the future after it is unshelved.
6501         self._delete_scheduler_instance_info(context, instance.uuid)
6502         self._notify_about_instance_usage(context, instance,
6503                 'shelve_offload.end')
6504         compute_utils.notify_about_instance_action(context, instance,
6505                 self.host, action=fields.NotificationAction.SHELVE_OFFLOAD,
6506                 phase=fields.NotificationPhase.END, bdms=bdms)
6507 
6508     @wrap_exception()
6509     @reverts_task_state
6510     @wrap_instance_event(prefix='compute')
6511     @wrap_instance_fault
6512     def unshelve_instance(self, context, instance, image,
6513                           filter_properties, node, request_spec=None):
6514         """Unshelve the instance.
6515 
6516         :param context: request context
6517         :param instance: a nova.objects.instance.Instance object
6518         :param image: an image to build from.  If None we assume a
6519             volume backed instance.
6520         :param filter_properties: dict containing limits, retry info etc.
6521         :param node: target compute node
6522         :param request_spec: the RequestSpec object used to schedule the
6523             instance
6524         """
6525         if filter_properties is None:
6526             filter_properties = {}
6527 
6528         @utils.synchronized(instance.uuid)
6529         def do_unshelve_instance():
6530             self._unshelve_instance(
6531                 context, instance, image, filter_properties, node,
6532                 request_spec)
6533         do_unshelve_instance()
6534 
6535     def _unshelve_instance_key_scrub(self, instance):
6536         """Remove data from the instance that may cause side effects."""
6537         cleaned_keys = dict(
6538                 key_data=instance.key_data,
6539                 auto_disk_config=instance.auto_disk_config)
6540         instance.key_data = None
6541         instance.auto_disk_config = False
6542         return cleaned_keys
6543 
6544     def _unshelve_instance_key_restore(self, instance, keys):
6545         """Restore previously scrubbed keys before saving the instance."""
6546         instance.update(keys)
6547 
6548     def _unshelve_instance(self, context, instance, image, filter_properties,
6549                            node, request_spec):
6550         LOG.info('Unshelving', instance=instance)
6551         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
6552                 context, instance.uuid)
6553 
6554         self._notify_about_instance_usage(context, instance, 'unshelve.start')
6555         compute_utils.notify_about_instance_action(context, instance,
6556                 self.host, action=fields.NotificationAction.UNSHELVE,
6557                 phase=fields.NotificationPhase.START, bdms=bdms)
6558 
6559         instance.task_state = task_states.SPAWNING
6560         instance.save()
6561 
6562         block_device_info = self._prep_block_device(context, instance, bdms)
6563         scrubbed_keys = self._unshelve_instance_key_scrub(instance)
6564 
6565         if node is None:
6566             node = self._get_nodename(instance)
6567 
6568         limits = filter_properties.get('limits', {})
6569 
6570         allocations = self.reportclient.get_allocations_for_consumer(
6571             context, instance.uuid)
6572 
6573         shelved_image_ref = instance.image_ref
6574         if image:
6575             instance.image_ref = image['id']
6576             image_meta = objects.ImageMeta.from_dict(image)
6577         else:
6578             image_meta = objects.ImageMeta.from_dict(
6579                 utils.get_image_from_system_metadata(
6580                     instance.system_metadata))
6581 
6582         provider_mappings = self._get_request_group_mapping(request_spec)
6583 
6584         try:
6585             if provider_mappings:
6586                 update = (
6587                     compute_utils.
6588                         update_pci_request_spec_with_allocated_interface_name)
6589                 update(context, self.reportclient, instance, provider_mappings)
6590 
6591             self.network_api.setup_instance_network_on_host(
6592                 context, instance, self.host,
6593                 provider_mappings=provider_mappings)
6594             network_info = self.network_api.get_instance_nw_info(
6595                 context, instance)
6596             with self.rt.instance_claim(context, instance, node, allocations,
6597                                         limits):
6598                 self.driver.spawn(context, instance, image_meta,
6599                                   injected_files=[],
6600                                   admin_password=None,
6601                                   allocations=allocations,
6602                                   network_info=network_info,
6603                                   block_device_info=block_device_info)
6604         except Exception:
6605             with excutils.save_and_reraise_exception(logger=LOG):
6606                 LOG.exception('Instance failed to spawn',
6607                               instance=instance)
6608                 # Cleanup allocations created by the scheduler on this host
6609                 # since we failed to spawn the instance. We do this both if
6610                 # the instance claim failed with ComputeResourcesUnavailable
6611                 # or if we did claim but the spawn failed, because aborting the
6612                 # instance claim will not remove the allocations.
6613                 self.reportclient.delete_allocation_for_instance(context,
6614                                                                  instance.uuid)
6615                 # FIXME: Umm, shouldn't we be rolling back port bindings too?
6616                 self._terminate_volume_connections(context, instance, bdms)
6617                 # The reverts_task_state decorator on unshelve_instance will
6618                 # eventually save these updates.
6619                 self._nil_out_instance_obj_host_and_node(instance)
6620 
6621         if image:
6622             instance.image_ref = shelved_image_ref
6623             self._delete_snapshot_of_shelved_instance(context, instance,
6624                                                       image['id'])
6625 
6626         self._unshelve_instance_key_restore(instance, scrubbed_keys)
6627         self._update_instance_after_spawn(instance)
6628         # Delete system_metadata for a shelved instance
6629         compute_utils.remove_shelved_keys_from_system_metadata(instance)
6630 
6631         instance.save(expected_task_state=task_states.SPAWNING)
6632         self._update_scheduler_instance_info(context, instance)
6633         self._notify_about_instance_usage(context, instance, 'unshelve.end')
6634         compute_utils.notify_about_instance_action(context, instance,
6635                 self.host, action=fields.NotificationAction.UNSHELVE,
6636                 phase=fields.NotificationPhase.END, bdms=bdms)
6637 
6638     # TODO(stephenfin): Remove this in RPC 6.0 since it's nova-network only
6639     @messaging.expected_exceptions(NotImplementedError)
6640     @wrap_instance_fault
6641     def reset_network(self, context, instance):
6642         """Reset networking on the given instance."""
6643         LOG.debug('Reset network', instance=instance)
6644         self.driver.reset_network(instance)
6645 
6646     def _inject_network_info(self, instance, network_info):
6647         """Inject network info for the given instance."""
6648         LOG.debug('Inject network info', instance=instance)
6649         LOG.debug('network_info to inject: |%s|', network_info,
6650                   instance=instance)
6651 
6652         self.driver.inject_network_info(instance, network_info)
6653 
6654     @wrap_instance_fault
6655     def inject_network_info(self, context, instance):
6656         """Inject network info, but don't return the info."""
6657         network_info = self.network_api.get_instance_nw_info(context, instance)
6658         self._inject_network_info(instance, network_info)
6659 
6660     @messaging.expected_exceptions(NotImplementedError,
6661                                    exception.ConsoleNotAvailable,
6662                                    exception.InstanceNotFound)
6663     @wrap_exception()
6664     @wrap_instance_fault
6665     def get_console_output(self, context, instance, tail_length):
6666         """Send the console output for the given instance."""
6667         context = context.elevated()
6668         LOG.info("Get console output", instance=instance)
6669         output = self.driver.get_console_output(context, instance)
6670 
6671         if type(output) is six.text_type:
6672             output = six.b(output)
6673 
6674         if tail_length is not None:
6675             output = self._tail_log(output, tail_length)
6676 
6677         return output.decode('ascii', 'replace')
6678 
6679     def _tail_log(self, log, length):
6680         try:
6681             length = int(length)
6682         except ValueError:
6683             length = 0
6684 
6685         if length == 0:
6686             return b''
6687         else:
6688             return b'\n'.join(log.split(b'\n')[-int(length):])
6689 
6690     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
6691                                    exception.InstanceNotReady,
6692                                    exception.InstanceNotFound,
6693                                    exception.ConsoleTypeUnavailable,
6694                                    NotImplementedError)
6695     @wrap_exception()
6696     @wrap_instance_fault
6697     def get_vnc_console(self, context, console_type, instance):
6698         """Return connection information for a vnc console."""
6699         context = context.elevated()
6700         LOG.debug("Getting vnc console", instance=instance)
6701 
6702         if not CONF.vnc.enabled:
6703             raise exception.ConsoleTypeUnavailable(console_type=console_type)
6704 
6705         if console_type == 'novnc':
6706             # For essex, novncproxy_base_url must include the full path
6707             # including the html file (like http://myhost/vnc_auto.html)
6708             access_url_base = CONF.vnc.novncproxy_base_url
6709         else:
6710             raise exception.ConsoleTypeInvalid(console_type=console_type)
6711 
6712         try:
6713             # Retrieve connect info from driver, and then decorate with our
6714             # access info token
6715             console = self.driver.get_vnc_console(context, instance)
6716             console_auth = objects.ConsoleAuthToken(
6717                 context=context,
6718                 console_type=console_type,
6719                 host=console.host,
6720                 port=console.port,
6721                 internal_access_path=console.internal_access_path,
6722                 instance_uuid=instance.uuid,
6723                 access_url_base=access_url_base,
6724             )
6725             console_auth.authorize(CONF.consoleauth.token_ttl)
6726             connect_info = console.get_connection_info(
6727                 console_auth.token, console_auth.access_url)
6728 
6729         except exception.InstanceNotFound:
6730             if instance.vm_state != vm_states.BUILDING:
6731                 raise
6732             raise exception.InstanceNotReady(instance_id=instance.uuid)
6733 
6734         return connect_info
6735 
6736     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
6737                                    exception.InstanceNotReady,
6738                                    exception.InstanceNotFound,
6739                                    exception.ConsoleTypeUnavailable,
6740                                    NotImplementedError)
6741     @wrap_exception()
6742     @wrap_instance_fault
6743     def get_spice_console(self, context, console_type, instance):
6744         """Return connection information for a spice console."""
6745         context = context.elevated()
6746         LOG.debug("Getting spice console", instance=instance)
6747 
6748         if not CONF.spice.enabled:
6749             raise exception.ConsoleTypeUnavailable(console_type=console_type)
6750 
6751         if console_type != 'spice-html5':
6752             raise exception.ConsoleTypeInvalid(console_type=console_type)
6753 
6754         try:
6755             # Retrieve connect info from driver, and then decorate with our
6756             # access info token
6757             console = self.driver.get_spice_console(context, instance)
6758             console_auth = objects.ConsoleAuthToken(
6759                 context=context,
6760                 console_type=console_type,
6761                 host=console.host,
6762                 port=console.port,
6763                 internal_access_path=console.internal_access_path,
6764                 instance_uuid=instance.uuid,
6765                 access_url_base=CONF.spice.html5proxy_base_url,
6766             )
6767             console_auth.authorize(CONF.consoleauth.token_ttl)
6768             connect_info = console.get_connection_info(
6769                 console_auth.token, console_auth.access_url)
6770 
6771         except exception.InstanceNotFound:
6772             if instance.vm_state != vm_states.BUILDING:
6773                 raise
6774             raise exception.InstanceNotReady(instance_id=instance.uuid)
6775 
6776         return connect_info
6777 
6778     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
6779                                    exception.InstanceNotReady,
6780                                    exception.InstanceNotFound,
6781                                    exception.ConsoleTypeUnavailable,
6782                                    NotImplementedError)
6783     @wrap_exception()
6784     @wrap_instance_fault
6785     def get_rdp_console(self, context, console_type, instance):
6786         """Return connection information for a RDP console."""
6787         context = context.elevated()
6788         LOG.debug("Getting RDP console", instance=instance)
6789 
6790         if not CONF.rdp.enabled:
6791             raise exception.ConsoleTypeUnavailable(console_type=console_type)
6792 
6793         if console_type != 'rdp-html5':
6794             raise exception.ConsoleTypeInvalid(console_type=console_type)
6795 
6796         try:
6797             # Retrieve connect info from driver, and then decorate with our
6798             # access info token
6799             console = self.driver.get_rdp_console(context, instance)
6800             console_auth = objects.ConsoleAuthToken(
6801                 context=context,
6802                 console_type=console_type,
6803                 host=console.host,
6804                 port=console.port,
6805                 internal_access_path=console.internal_access_path,
6806                 instance_uuid=instance.uuid,
6807                 access_url_base=CONF.rdp.html5_proxy_base_url,
6808             )
6809             console_auth.authorize(CONF.consoleauth.token_ttl)
6810             connect_info = console.get_connection_info(
6811                 console_auth.token, console_auth.access_url)
6812 
6813         except exception.InstanceNotFound:
6814             if instance.vm_state != vm_states.BUILDING:
6815                 raise
6816             raise exception.InstanceNotReady(instance_id=instance.uuid)
6817 
6818         return connect_info
6819 
6820     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
6821                                    exception.InstanceNotReady,
6822                                    exception.InstanceNotFound,
6823                                    exception.ConsoleTypeUnavailable,
6824                                    NotImplementedError)
6825     @wrap_exception()
6826     @wrap_instance_fault
6827     def get_mks_console(self, context, console_type, instance):
6828         """Return connection information for a MKS console."""
6829         context = context.elevated()
6830         LOG.debug("Getting MKS console", instance=instance)
6831 
6832         if not CONF.mks.enabled:
6833             raise exception.ConsoleTypeUnavailable(console_type=console_type)
6834 
6835         if console_type != 'webmks':
6836             raise exception.ConsoleTypeInvalid(console_type=console_type)
6837 
6838         try:
6839             # Retrieve connect info from driver, and then decorate with our
6840             # access info token
6841             console = self.driver.get_mks_console(context, instance)
6842             console_auth = objects.ConsoleAuthToken(
6843                 context=context,
6844                 console_type=console_type,
6845                 host=console.host,
6846                 port=console.port,
6847                 internal_access_path=console.internal_access_path,
6848                 instance_uuid=instance.uuid,
6849                 access_url_base=CONF.mks.mksproxy_base_url,
6850             )
6851             console_auth.authorize(CONF.consoleauth.token_ttl)
6852             connect_info = console.get_connection_info(
6853                 console_auth.token, console_auth.access_url)
6854 
6855         except exception.InstanceNotFound:
6856             if instance.vm_state != vm_states.BUILDING:
6857                 raise
6858             raise exception.InstanceNotReady(instance_id=instance.uuid)
6859 
6860         return connect_info
6861 
6862     @messaging.expected_exceptions(
6863         exception.ConsoleTypeInvalid,
6864         exception.InstanceNotReady,
6865         exception.InstanceNotFound,
6866         exception.ConsoleTypeUnavailable,
6867         exception.SocketPortRangeExhaustedException,
6868         exception.ImageSerialPortNumberInvalid,
6869         exception.ImageSerialPortNumberExceedFlavorValue,
6870         NotImplementedError)
6871     @wrap_exception()
6872     @wrap_instance_fault
6873     def get_serial_console(self, context, console_type, instance):
6874         """Returns connection information for a serial console."""
6875 
6876         LOG.debug("Getting serial console", instance=instance)
6877 
6878         if not CONF.serial_console.enabled:
6879             raise exception.ConsoleTypeUnavailable(console_type=console_type)
6880 
6881         context = context.elevated()
6882 
6883         try:
6884             # Retrieve connect info from driver, and then decorate with our
6885             # access info token
6886             console = self.driver.get_serial_console(context, instance)
6887             console_auth = objects.ConsoleAuthToken(
6888                 context=context,
6889                 console_type=console_type,
6890                 host=console.host,
6891                 port=console.port,
6892                 internal_access_path=console.internal_access_path,
6893                 instance_uuid=instance.uuid,
6894                 access_url_base=CONF.serial_console.base_url,
6895             )
6896             console_auth.authorize(CONF.consoleauth.token_ttl)
6897             connect_info = console.get_connection_info(
6898                 console_auth.token, console_auth.access_url)
6899 
6900         except exception.InstanceNotFound:
6901             if instance.vm_state != vm_states.BUILDING:
6902                 raise
6903             raise exception.InstanceNotReady(instance_id=instance.uuid)
6904 
6905         return connect_info
6906 
6907     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
6908                                    exception.InstanceNotReady,
6909                                    exception.InstanceNotFound)
6910     @wrap_exception()
6911     @wrap_instance_fault
6912     def validate_console_port(self, ctxt, instance, port, console_type):
6913         if console_type == "spice-html5":
6914             console_info = self.driver.get_spice_console(ctxt, instance)
6915         elif console_type == "rdp-html5":
6916             console_info = self.driver.get_rdp_console(ctxt, instance)
6917         elif console_type == "serial":
6918             console_info = self.driver.get_serial_console(ctxt, instance)
6919         elif console_type == "webmks":
6920             console_info = self.driver.get_mks_console(ctxt, instance)
6921         else:
6922             console_info = self.driver.get_vnc_console(ctxt, instance)
6923 
6924         # Some drivers may return an int on console_info.port but the port
6925         # variable in this method is a string, so cast to be sure we are
6926         # comparing the correct types.
6927         return str(console_info.port) == port
6928 
6929     @wrap_exception()
6930     @reverts_task_state
6931     @wrap_instance_fault
6932     def reserve_block_device_name(self, context, instance, device,
6933                                   volume_id, disk_bus, device_type, tag,
6934                                   multiattach):
6935         if (tag and not
6936                 self.driver.capabilities.get('supports_tagged_attach_volume',
6937                                              False)):
6938             raise exception.VolumeTaggedAttachNotSupported()
6939 
6940         if (multiattach and not
6941                 self.driver.capabilities.get('supports_multiattach', False)):
6942             raise exception.MultiattachNotSupportedByVirtDriver(
6943                 volume_id=volume_id)
6944 
6945         @utils.synchronized(instance.uuid)
6946         def do_reserve():
6947             bdms = (
6948                 objects.BlockDeviceMappingList.get_by_instance_uuid(
6949                     context, instance.uuid))
6950 
6951             # NOTE(ndipanov): We need to explicitly set all the fields on the
6952             #                 object so that obj_load_attr does not fail
6953             new_bdm = objects.BlockDeviceMapping(
6954                     context=context,
6955                     source_type='volume', destination_type='volume',
6956                     instance_uuid=instance.uuid, boot_index=None,
6957                     volume_id=volume_id,
6958                     device_name=device, guest_format=None,
6959                     disk_bus=disk_bus, device_type=device_type, tag=tag)
6960 
6961             new_bdm.device_name = self._get_device_name_for_instance(
6962                     instance, bdms, new_bdm)
6963 
6964             # NOTE(vish): create bdm here to avoid race condition
6965             new_bdm.create()
6966             return new_bdm
6967 
6968         return do_reserve()
6969 
6970     @wrap_exception()
6971     @wrap_instance_event(prefix='compute')
6972     @wrap_instance_fault
6973     def attach_volume(self, context, instance, bdm):
6974         """Attach a volume to an instance."""
6975         driver_bdm = driver_block_device.convert_volume(bdm)
6976 
6977         @utils.synchronized(instance.uuid)
6978         def do_attach_volume(context, instance, driver_bdm):
6979             try:
6980                 return self._attach_volume(context, instance, driver_bdm)
6981             except Exception:
6982                 with excutils.save_and_reraise_exception():
6983                     bdm.destroy()
6984 
6985         do_attach_volume(context, instance, driver_bdm)
6986 
6987     def _attach_volume(self, context, instance, bdm):
6988         context = context.elevated()
6989         LOG.info('Attaching volume %(volume_id)s to %(mountpoint)s',
6990                  {'volume_id': bdm.volume_id,
6991                   'mountpoint': bdm['mount_device']},
6992                  instance=instance)
6993         compute_utils.notify_about_volume_attach_detach(
6994             context, instance, self.host,
6995             action=fields.NotificationAction.VOLUME_ATTACH,
6996             phase=fields.NotificationPhase.START,
6997             volume_id=bdm.volume_id)
6998         try:
6999             bdm.attach(context, instance, self.volume_api, self.driver,
7000                        do_driver_attach=True)
7001         except Exception as e:
7002             with excutils.save_and_reraise_exception():
7003                 LOG.exception("Failed to attach %(volume_id)s "
7004                               "at %(mountpoint)s",
7005                               {'volume_id': bdm.volume_id,
7006                                'mountpoint': bdm['mount_device']},
7007                               instance=instance)
7008                 if bdm['attachment_id']:
7009                     # Try to delete the attachment to make the volume
7010                     # available again. Note that DriverVolumeBlockDevice
7011                     # may have already deleted the attachment so ignore
7012                     # VolumeAttachmentNotFound.
7013                     try:
7014                         self.volume_api.attachment_delete(
7015                             context, bdm['attachment_id'])
7016                     except exception.VolumeAttachmentNotFound as exc:
7017                         LOG.debug('Ignoring VolumeAttachmentNotFound: %s',
7018                                   exc, instance=instance)
7019                 else:
7020                     self.volume_api.unreserve_volume(context, bdm.volume_id)
7021                 compute_utils.notify_about_volume_attach_detach(
7022                     context, instance, self.host,
7023                     action=fields.NotificationAction.VOLUME_ATTACH,
7024                     phase=fields.NotificationPhase.ERROR,
7025                     exception=e,
7026                     volume_id=bdm.volume_id)
7027 
7028         info = {'volume_id': bdm.volume_id}
7029         self._notify_about_instance_usage(
7030             context, instance, "volume.attach", extra_usage_info=info)
7031         compute_utils.notify_about_volume_attach_detach(
7032             context, instance, self.host,
7033             action=fields.NotificationAction.VOLUME_ATTACH,
7034             phase=fields.NotificationPhase.END,
7035             volume_id=bdm.volume_id)
7036 
7037     def _notify_volume_usage_detach(self, context, instance, bdm):
7038         if CONF.volume_usage_poll_interval <= 0:
7039             return
7040 
7041         mp = bdm.device_name
7042         # Handle bootable volumes which will not contain /dev/
7043         if '/dev/' in mp:
7044             mp = mp[5:]
7045         try:
7046             vol_stats = self.driver.block_stats(instance, mp)
7047             if vol_stats is None:
7048                 return
7049         except NotImplementedError:
7050             return
7051 
7052         LOG.debug("Updating volume usage cache with totals", instance=instance)
7053         rd_req, rd_bytes, wr_req, wr_bytes, flush_ops = vol_stats
7054         vol_usage = objects.VolumeUsage(context)
7055         vol_usage.volume_id = bdm.volume_id
7056         vol_usage.instance_uuid = instance.uuid
7057         vol_usage.project_id = instance.project_id
7058         vol_usage.user_id = instance.user_id
7059         vol_usage.availability_zone = instance.availability_zone
7060         vol_usage.curr_reads = rd_req
7061         vol_usage.curr_read_bytes = rd_bytes
7062         vol_usage.curr_writes = wr_req
7063         vol_usage.curr_write_bytes = wr_bytes
7064         vol_usage.save(update_totals=True)
7065         self.notifier.info(context, 'volume.usage', vol_usage.to_dict())
7066         compute_utils.notify_about_volume_usage(context, vol_usage, self.host)
7067 
7068     def _detach_volume(self, context, bdm, instance, destroy_bdm=True,
7069                        attachment_id=None):
7070         """Detach a volume from an instance.
7071 
7072         :param context: security context
7073         :param bdm: nova.objects.BlockDeviceMapping volume bdm to detach
7074         :param instance: the Instance object to detach the volume from
7075         :param destroy_bdm: if True, the corresponding BDM entry will be marked
7076                             as deleted. Disabling this is useful for operations
7077                             like rebuild, when we don't want to destroy BDM
7078         :param attachment_id: The volume attachment_id for the given instance
7079                               and volume.
7080         """
7081         volume_id = bdm.volume_id
7082         compute_utils.notify_about_volume_attach_detach(
7083             context, instance, self.host,
7084             action=fields.NotificationAction.VOLUME_DETACH,
7085             phase=fields.NotificationPhase.START,
7086             volume_id=volume_id)
7087 
7088         self._notify_volume_usage_detach(context, instance, bdm)
7089 
7090         LOG.info('Detaching volume %(volume_id)s',
7091                  {'volume_id': volume_id}, instance=instance)
7092 
7093         driver_bdm = driver_block_device.convert_volume(bdm)
7094         driver_bdm.detach(context, instance, self.volume_api, self.driver,
7095                           attachment_id=attachment_id, destroy_bdm=destroy_bdm)
7096 
7097         info = dict(volume_id=volume_id)
7098         self._notify_about_instance_usage(
7099             context, instance, "volume.detach", extra_usage_info=info)
7100         compute_utils.notify_about_volume_attach_detach(
7101             context, instance, self.host,
7102             action=fields.NotificationAction.VOLUME_DETACH,
7103             phase=fields.NotificationPhase.END,
7104             volume_id=volume_id)
7105 
7106         if 'tag' in bdm and bdm.tag:
7107             self._delete_disk_metadata(instance, bdm)
7108         if destroy_bdm:
7109             bdm.destroy()
7110 
7111     def _delete_disk_metadata(self, instance, bdm):
7112         for device in instance.device_metadata.devices:
7113             if isinstance(device, objects.DiskMetadata):
7114                 if 'serial' in device:
7115                     if device.serial == bdm.volume_id:
7116                         instance.device_metadata.devices.remove(device)
7117                         instance.save()
7118                         break
7119                 else:
7120                     # NOTE(artom) We log the entire device object because all
7121                     # fields are nullable and may not be set
7122                     LOG.warning('Unable to determine whether to clean up '
7123                                 'device metadata for disk %s', device,
7124                                 instance=instance)
7125 
7126     @wrap_exception()
7127     @wrap_instance_event(prefix='compute')
7128     @wrap_instance_fault
7129     def detach_volume(self, context, volume_id, instance, attachment_id):
7130         """Detach a volume from an instance.
7131 
7132         :param context: security context
7133         :param volume_id: the volume id
7134         :param instance: the Instance object to detach the volume from
7135         :param attachment_id: The volume attachment_id for the given instance
7136                               and volume.
7137 
7138         """
7139         @utils.synchronized(instance.uuid)
7140         def do_detach_volume(context, volume_id, instance, attachment_id):
7141             bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
7142                     context, volume_id, instance.uuid)
7143             self._detach_volume(context, bdm, instance,
7144                                 attachment_id=attachment_id)
7145 
7146         do_detach_volume(context, volume_id, instance, attachment_id)
7147 
7148     def _init_volume_connection(self, context, new_volume,
7149                                 old_volume_id, connector, bdm,
7150                                 new_attachment_id, mountpoint):
7151         new_volume_id = new_volume['id']
7152         if new_attachment_id is None:
7153             # We're dealing with an old-style attachment so initialize the
7154             # connection so we can get the connection_info.
7155             new_cinfo = self.volume_api.initialize_connection(context,
7156                                                               new_volume_id,
7157                                                               connector)
7158         else:
7159             # Check for multiattach on the new volume and if True, check to
7160             # see if the virt driver supports multiattach.
7161             # TODO(mriedem): This is copied from DriverVolumeBlockDevice
7162             # and should be consolidated into some common code at some point.
7163             vol_multiattach = new_volume.get('multiattach', False)
7164             virt_multiattach = self.driver.capabilities.get(
7165                 'supports_multiattach', False)
7166             if vol_multiattach and not virt_multiattach:
7167                 raise exception.MultiattachNotSupportedByVirtDriver(
7168                     volume_id=new_volume_id)
7169 
7170             # This is a new style attachment and the API created the new
7171             # volume attachment and passed the id to the compute over RPC.
7172             # At this point we need to update the new volume attachment with
7173             # the host connector, which will give us back the new attachment
7174             # connection_info.
7175             new_cinfo = self.volume_api.attachment_update(
7176                 context, new_attachment_id, connector,
7177                 mountpoint)['connection_info']
7178 
7179             if vol_multiattach:
7180                 # This will be used by the volume driver to determine the
7181                 # proper disk configuration.
7182                 new_cinfo['multiattach'] = True
7183 
7184         old_cinfo = jsonutils.loads(bdm['connection_info'])
7185         if old_cinfo and 'serial' not in old_cinfo:
7186             old_cinfo['serial'] = old_volume_id
7187         # NOTE(lyarwood): serial is not always present in the returned
7188         # connection_info so set it if it is missing as we do in
7189         # DriverVolumeBlockDevice.attach().
7190         if 'serial' not in new_cinfo:
7191             new_cinfo['serial'] = new_volume_id
7192         return (old_cinfo, new_cinfo)
7193 
7194     def _swap_volume(self, context, instance, bdm, connector,
7195                      old_volume_id, new_volume, resize_to,
7196                      new_attachment_id, is_cinder_migration):
7197         new_volume_id = new_volume['id']
7198         mountpoint = bdm['device_name']
7199         failed = False
7200         new_cinfo = None
7201         try:
7202             old_cinfo, new_cinfo = self._init_volume_connection(
7203                 context, new_volume, old_volume_id, connector,
7204                 bdm, new_attachment_id, mountpoint)
7205             # NOTE(lyarwood): The Libvirt driver, the only virt driver
7206             # currently implementing swap_volume, will modify the contents of
7207             # new_cinfo when connect_volume is called. This is then saved to
7208             # the BDM in swap_volume for future use outside of this flow.
7209             msg = ("swap_volume: Calling driver volume swap with "
7210                    "connection infos: new: %(new_cinfo)s; "
7211                    "old: %(old_cinfo)s" %
7212                    {'new_cinfo': new_cinfo, 'old_cinfo': old_cinfo})
7213             # Both new and old info might contain password
7214             LOG.debug(strutils.mask_password(msg), instance=instance)
7215 
7216             self.driver.swap_volume(context, old_cinfo, new_cinfo, instance,
7217                                     mountpoint, resize_to)
7218             if new_attachment_id:
7219                 self.volume_api.attachment_complete(context, new_attachment_id)
7220             msg = ("swap_volume: Driver volume swap returned, new "
7221                    "connection_info is now : %(new_cinfo)s" %
7222                    {'new_cinfo': new_cinfo})
7223             LOG.debug(strutils.mask_password(msg))
7224         except Exception as ex:
7225             failed = True
7226             with excutils.save_and_reraise_exception():
7227                 compute_utils.notify_about_volume_swap(
7228                     context, instance, self.host,
7229                     fields.NotificationPhase.ERROR,
7230                     old_volume_id, new_volume_id, ex)
7231                 if new_cinfo:
7232                     msg = ("Failed to swap volume %(old_volume_id)s "
7233                            "for %(new_volume_id)s")
7234                     LOG.exception(msg, {'old_volume_id': old_volume_id,
7235                                         'new_volume_id': new_volume_id},
7236                                   instance=instance)
7237                 else:
7238                     msg = ("Failed to connect to volume %(volume_id)s "
7239                            "with volume at %(mountpoint)s")
7240                     LOG.exception(msg, {'volume_id': new_volume_id,
7241                                         'mountpoint': bdm['device_name']},
7242                                   instance=instance)
7243 
7244                 # The API marked the volume as 'detaching' for the old volume
7245                 # so we need to roll that back so the volume goes back to
7246                 # 'in-use' state.
7247                 self.volume_api.roll_detaching(context, old_volume_id)
7248 
7249                 if new_attachment_id is None:
7250                     # The API reserved the new volume so it would be in
7251                     # 'attaching' status, so we need to unreserve it so it
7252                     # goes back to 'available' status.
7253                     self.volume_api.unreserve_volume(context, new_volume_id)
7254                 else:
7255                     # This is a new style attachment for the new volume, which
7256                     # was created in the API. We just need to delete it here
7257                     # to put the new volume back into 'available' status.
7258                     self.volume_api.attachment_delete(
7259                         context, new_attachment_id)
7260         finally:
7261             # TODO(mriedem): This finally block is terribly confusing and is
7262             # trying to do too much. We should consider removing the finally
7263             # block and move whatever needs to happen on success and failure
7264             # into the blocks above for clarity, even if it means a bit of
7265             # redundant code.
7266             conn_volume = new_volume_id if failed else old_volume_id
7267             if new_cinfo:
7268                 LOG.debug("swap_volume: removing Cinder connection "
7269                           "for volume %(volume)s", {'volume': conn_volume},
7270                           instance=instance)
7271                 if bdm.attachment_id is None:
7272                     # This is the pre-3.44 flow for new-style volume
7273                     # attachments so just terminate the connection.
7274                     self.volume_api.terminate_connection(context,
7275                                                          conn_volume,
7276                                                          connector)
7277                 else:
7278                     # This is a new style volume attachment. If we failed, then
7279                     # the new attachment was already deleted above in the
7280                     # exception block and we have nothing more to do here. If
7281                     # swap_volume was successful in the driver, then we need to
7282                     # "detach" the original attachment by deleting it.
7283                     if not failed:
7284                         self.volume_api.attachment_delete(
7285                             context, bdm.attachment_id)
7286 
7287             # Need to make some decisions based on whether this was
7288             # a Cinder initiated migration or not. The callback to
7289             # migration completion isn't needed in the case of a
7290             # nova initiated simple swap of two volume
7291             # "volume-update" call so skip that. The new attachment
7292             # scenarios will give us a new attachment record and
7293             # that's what we want.
7294             if bdm.attachment_id and not is_cinder_migration:
7295                 # we don't callback to cinder
7296                 comp_ret = {'save_volume_id': new_volume_id}
7297             else:
7298                 # NOTE(lyarwood): The following call to
7299                 # os-migrate-volume-completion returns a dict containing
7300                 # save_volume_id, this volume id has two possible values :
7301                 # 1. old_volume_id if we are migrating (retyping) volumes
7302                 # 2. new_volume_id if we are swapping between two existing
7303                 #    volumes
7304                 # This volume id is later used to update the volume_id and
7305                 # connection_info['serial'] of the BDM.
7306                 comp_ret = self.volume_api.migrate_volume_completion(
7307                                                           context,
7308                                                           old_volume_id,
7309                                                           new_volume_id,
7310                                                           error=failed)
7311                 LOG.debug("swap_volume: Cinder migrate_volume_completion "
7312                           "returned: %(comp_ret)s", {'comp_ret': comp_ret},
7313                           instance=instance)
7314 
7315         return (comp_ret, new_cinfo)
7316 
7317     @wrap_exception()
7318     @wrap_instance_event(prefix='compute')
7319     @wrap_instance_fault
7320     def swap_volume(self, context, old_volume_id, new_volume_id, instance,
7321                     new_attachment_id):
7322         """Swap volume for an instance."""
7323         context = context.elevated()
7324 
7325         compute_utils.notify_about_volume_swap(
7326             context, instance, self.host,
7327             fields.NotificationPhase.START,
7328             old_volume_id, new_volume_id)
7329 
7330         bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
7331                 context, old_volume_id, instance.uuid)
7332         connector = self.driver.get_volume_connector(instance)
7333 
7334         resize_to = 0
7335         old_volume = self.volume_api.get(context, old_volume_id)
7336         # Yes this is a tightly-coupled state check of what's going on inside
7337         # cinder, but we need this while we still support old (v1/v2) and
7338         # new style attachments (v3.44). Once we drop support for old style
7339         # attachments we could think about cleaning up the cinder-initiated
7340         # swap volume API flows.
7341         is_cinder_migration = False
7342         if 'migration_status' in old_volume:
7343             is_cinder_migration = old_volume['migration_status'] == 'migrating'
7344         old_vol_size = old_volume['size']
7345         new_volume = self.volume_api.get(context, new_volume_id)
7346         new_vol_size = new_volume['size']
7347         if new_vol_size > old_vol_size:
7348             resize_to = new_vol_size
7349 
7350         LOG.info('Swapping volume %(old_volume)s for %(new_volume)s',
7351                  {'old_volume': old_volume_id, 'new_volume': new_volume_id},
7352                  instance=instance)
7353         comp_ret, new_cinfo = self._swap_volume(context,
7354                                                 instance,
7355                                                 bdm,
7356                                                 connector,
7357                                                 old_volume_id,
7358                                                 new_volume,
7359                                                 resize_to,
7360                                                 new_attachment_id,
7361                                                 is_cinder_migration)
7362 
7363         # NOTE(lyarwood): Update the BDM with the modified new_cinfo and
7364         # correct volume_id returned by Cinder.
7365         save_volume_id = comp_ret['save_volume_id']
7366         new_cinfo['serial'] = save_volume_id
7367         values = {
7368             'connection_info': jsonutils.dumps(new_cinfo),
7369             'source_type': 'volume',
7370             'destination_type': 'volume',
7371             'snapshot_id': None,
7372             'volume_id': save_volume_id,
7373             'no_device': None}
7374 
7375         if resize_to:
7376             values['volume_size'] = resize_to
7377 
7378         if new_attachment_id is not None:
7379             # This was a volume swap for a new-style attachment so we
7380             # need to update the BDM attachment_id for the new attachment.
7381             values['attachment_id'] = new_attachment_id
7382 
7383         LOG.debug("swap_volume: Updating volume %(volume_id)s BDM record with "
7384                   "%(updates)s", {'volume_id': bdm.volume_id,
7385                                   'updates': values},
7386                   instance=instance)
7387         bdm.update(values)
7388         bdm.save()
7389 
7390         compute_utils.notify_about_volume_swap(
7391             context, instance, self.host,
7392             fields.NotificationPhase.END,
7393             old_volume_id, new_volume_id)
7394 
7395     @wrap_exception()
7396     def remove_volume_connection(self, context, volume_id, instance):
7397         """Remove the volume connection on this host
7398 
7399         Detach the volume from this instance on this host, and if this is
7400         the cinder v2 flow, call cinder to terminate the connection.
7401         """
7402         try:
7403             # NOTE(mriedem): If the BDM was just passed directly we would not
7404             # need to do this DB query, but this is an RPC interface so
7405             # changing that requires some care.
7406             bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
7407                     context, volume_id, instance.uuid)
7408             # NOTE(mriedem): Normally we would pass delete_attachment=True to
7409             # _remove_volume_connection to delete a v3 style volume attachment,
7410             # but this method is RPC called from _rollback_live_migration which
7411             # already deletes the attachment, so because of that tight coupling
7412             # we cannot simply delete a v3 style attachment here without
7413             # needing to do some behavior modification of that
7414             # _rollback_live_migration flow which gets messy.
7415             self._remove_volume_connection(context, bdm, instance)
7416         except exception.NotFound:
7417             pass
7418 
7419     def _remove_volume_connection(self, context, bdm, instance,
7420                                   delete_attachment=False):
7421         """Remove the volume connection on this host
7422 
7423         Detach the volume from this instance on this host.
7424 
7425         :param context: nova auth request context
7426         :param bdm: BlockDeviceMapping object for a volume attached to the
7427             instance
7428         :param instance: Instance object with a volume attached represented
7429             by ``bdm``
7430         :param delete_attachment: If ``bdm.attachment_id`` is not None the
7431             attachment was made as a cinder v3 style attachment and if True,
7432             then deletes the volume attachment, otherwise just terminates
7433             the connection for a cinder legacy style connection.
7434         """
7435         driver_bdm = driver_block_device.convert_volume(bdm)
7436         driver_bdm.driver_detach(context, instance,
7437                                  self.volume_api, self.driver)
7438         if bdm.attachment_id is None:
7439             # cinder v2 api flow
7440             connector = self.driver.get_volume_connector(instance)
7441             self.volume_api.terminate_connection(context, bdm.volume_id,
7442                                                  connector)
7443         elif delete_attachment:
7444             # cinder v3 api flow
7445             self.volume_api.attachment_delete(context, bdm.attachment_id)
7446 
7447     def _deallocate_port_for_instance(self, context, instance, port_id,
7448                                       raise_on_failure=False):
7449         try:
7450             result = self.network_api.deallocate_port_for_instance(
7451                 context, instance, port_id)
7452             __, port_allocation = result
7453         except Exception as ex:
7454             with excutils.save_and_reraise_exception(
7455                     reraise=raise_on_failure):
7456                 LOG.warning('Failed to deallocate port %(port_id)s '
7457                             'for instance. Error: %(error)s',
7458                             {'port_id': port_id, 'error': ex},
7459                             instance=instance)
7460         else:
7461             if port_allocation:
7462                 # Deallocate the resources in placement that were used by the
7463                 # detached port.
7464                 try:
7465                     client = self.reportclient
7466                     client.remove_resources_from_instance_allocation(
7467                         context, instance.uuid, port_allocation)
7468                 except Exception as ex:
7469                     # We always raise here as it is not a race condition where
7470                     # somebody has already deleted the port we want to cleanup.
7471                     # Here we see that the port exists, the allocation exists,
7472                     # but we cannot clean it up so we will actually leak
7473                     # allocations.
7474                     with excutils.save_and_reraise_exception():
7475                         LOG.warning('Failed to remove resource allocation '
7476                                     'of port %(port_id)s for instance. Error: '
7477                                     '%(error)s',
7478                                     {'port_id': port_id, 'error': ex},
7479                                     instance=instance)
7480 
7481     # TODO(mriedem): There are likely race failures which can result in
7482     # NotFound and QuotaError exceptions getting traced as well.
7483     @messaging.expected_exceptions(
7484         # Do not log a traceback for user errors. We use Invalid generically
7485         # since this method can raise lots of different exceptions:
7486         # AttachInterfaceNotSupported
7487         # NetworkInterfaceTaggedAttachNotSupported
7488         # NetworkAmbiguous
7489         # PortNotUsable
7490         # PortInUse
7491         # PortNotUsableDNS
7492         # AttachSRIOVPortNotSupported
7493         # NetworksWithQoSPolicyNotSupported
7494         exception.Invalid)
7495     @wrap_exception()
7496     @wrap_instance_event(prefix='compute')
7497     @wrap_instance_fault
7498     def attach_interface(self, context, instance, network_id, port_id,
7499                          requested_ip, tag):
7500         """Use hotplug to add an network adapter to an instance."""
7501         if not self.driver.capabilities.get('supports_attach_interface',
7502                                             False):
7503             raise exception.AttachInterfaceNotSupported(
7504                 instance_uuid=instance.uuid)
7505         if (tag and not
7506             self.driver.capabilities.get('supports_tagged_attach_interface',
7507                                          False)):
7508             raise exception.NetworkInterfaceTaggedAttachNotSupported()
7509 
7510         compute_utils.notify_about_instance_action(
7511             context, instance, self.host,
7512             action=fields.NotificationAction.INTERFACE_ATTACH,
7513             phase=fields.NotificationPhase.START)
7514 
7515         bind_host_id = self.driver.network_binding_host_id(context, instance)
7516         network_info = self.network_api.allocate_port_for_instance(
7517             context, instance, port_id, network_id, requested_ip,
7518             bind_host_id=bind_host_id, tag=tag)
7519         if len(network_info) != 1:
7520             LOG.error('allocate_port_for_instance returned %(ports)s '
7521                       'ports', {'ports': len(network_info)})
7522             # TODO(elod.illes): an instance.interface_attach.error notification
7523             # should be sent here
7524             raise exception.InterfaceAttachFailed(
7525                     instance_uuid=instance.uuid)
7526         image_meta = objects.ImageMeta.from_instance(instance)
7527 
7528         try:
7529             self.driver.attach_interface(context, instance, image_meta,
7530                                          network_info[0])
7531         except exception.NovaException as ex:
7532             port_id = network_info[0].get('id')
7533             LOG.warning("attach interface failed , try to deallocate "
7534                         "port %(port_id)s, reason: %(msg)s",
7535                         {'port_id': port_id, 'msg': ex},
7536                         instance=instance)
7537             self._deallocate_port_for_instance(context, instance, port_id)
7538 
7539             compute_utils.notify_about_instance_action(
7540                 context, instance, self.host,
7541                 action=fields.NotificationAction.INTERFACE_ATTACH,
7542                 phase=fields.NotificationPhase.ERROR,
7543                 exception=ex)
7544 
7545             raise exception.InterfaceAttachFailed(
7546                 instance_uuid=instance.uuid)
7547 
7548         compute_utils.notify_about_instance_action(
7549             context, instance, self.host,
7550             action=fields.NotificationAction.INTERFACE_ATTACH,
7551             phase=fields.NotificationPhase.END)
7552 
7553         return network_info[0]
7554 
7555     @wrap_exception()
7556     @wrap_instance_event(prefix='compute')
7557     @wrap_instance_fault
7558     def detach_interface(self, context, instance, port_id):
7559         """Detach a network adapter from an instance."""
7560         network_info = instance.info_cache.network_info
7561         condemned = None
7562         for vif in network_info:
7563             if vif['id'] == port_id:
7564                 condemned = vif
7565                 break
7566         if condemned is None:
7567             raise exception.PortNotFound(_("Port %s is not "
7568                                            "attached") % port_id)
7569 
7570         compute_utils.notify_about_instance_action(
7571             context, instance, self.host,
7572             action=fields.NotificationAction.INTERFACE_DETACH,
7573             phase=fields.NotificationPhase.START)
7574 
7575         try:
7576             self.driver.detach_interface(context, instance, condemned)
7577         except exception.NovaException as ex:
7578             # If the instance was deleted before the interface was detached,
7579             # just log it at debug.
7580             log_level = (logging.DEBUG
7581                          if isinstance(ex, exception.InstanceNotFound)
7582                          else logging.WARNING)
7583             LOG.log(log_level,
7584                     "Detach interface failed, port_id=%(port_id)s, reason: "
7585                     "%(msg)s", {'port_id': port_id, 'msg': ex},
7586                     instance=instance)
7587             raise exception.InterfaceDetachFailed(instance_uuid=instance.uuid)
7588         else:
7589             self._deallocate_port_for_instance(
7590                 context, instance, port_id, raise_on_failure=True)
7591 
7592         compute_utils.notify_about_instance_action(
7593             context, instance, self.host,
7594             action=fields.NotificationAction.INTERFACE_DETACH,
7595             phase=fields.NotificationPhase.END)
7596 
7597     def _get_compute_info(self, context, host):
7598         return objects.ComputeNode.get_first_node_by_host_for_old_compat(
7599             context, host)
7600 
7601     # TODO(stephenfin): Remove the unused instance argument in RPC version 6.0
7602     @wrap_exception()
7603     def check_instance_shared_storage(self, ctxt, instance, data):
7604         """Check if the instance files are shared
7605 
7606         :param ctxt: security context
7607         :param instance: dict of instance data
7608         :param data: result of driver.check_instance_shared_storage_local
7609 
7610         Returns True if instance disks located on shared storage and
7611         False otherwise.
7612         """
7613         return self.driver.check_instance_shared_storage_remote(ctxt, data)
7614 
7615     def _dest_can_numa_live_migrate(self, dest_check_data, migration):
7616         # TODO(artom) If we have a libvirt driver we expect it to set
7617         # dst_supports_numa_live_migration, but we have to remove it if we
7618         # did not get a migration from the conductor, indicating that it
7619         # cannot send RPC 5.3. This check can be removed in RPC 6.0.
7620         if ('dst_supports_numa_live_migration' in dest_check_data and
7621                 dest_check_data.dst_supports_numa_live_migration and
7622                 not migration):
7623             delattr(dest_check_data, 'dst_supports_numa_live_migration')
7624         return dest_check_data
7625 
7626     @wrap_exception()
7627     @wrap_instance_event(prefix='compute')
7628     @wrap_instance_fault
7629     def check_can_live_migrate_destination(self, ctxt, instance,
7630                                            block_migration, disk_over_commit,
7631                                            migration=None, limits=None):
7632         """Check if it is possible to execute live migration.
7633 
7634         This runs checks on the destination host, and then calls
7635         back to the source host to check the results.
7636 
7637         :param context: security context
7638         :param instance: dict of instance data
7639         :param block_migration: if true, prepare for block migration
7640                                 if None, calculate it in driver
7641         :param disk_over_commit: if true, allow disk over commit
7642                                  if None, ignore disk usage checking
7643         :param migration: objects.Migration object for this live migration.
7644         :param limits: objects.SchedulerLimits object for this live migration.
7645         :returns: a LiveMigrateData object (hypervisor-dependent)
7646         """
7647         src_compute_info = obj_base.obj_to_primitive(
7648             self._get_compute_info(ctxt, instance.host))
7649         dst_compute_info = obj_base.obj_to_primitive(
7650             self._get_compute_info(ctxt, self.host))
7651         dest_check_data = self.driver.check_can_live_migrate_destination(ctxt,
7652             instance, src_compute_info, dst_compute_info,
7653             block_migration, disk_over_commit)
7654         dest_check_data = self._dest_can_numa_live_migrate(dest_check_data,
7655                                                            migration)
7656         LOG.debug('destination check data is %s', dest_check_data)
7657         try:
7658             allocs = self.reportclient.get_allocations_for_consumer(
7659                 ctxt, instance.uuid)
7660             migrate_data = self.compute_rpcapi.check_can_live_migrate_source(
7661                 ctxt, instance, dest_check_data)
7662             if ('src_supports_numa_live_migration' in migrate_data and
7663                     migrate_data.src_supports_numa_live_migration):
7664                 migrate_data = self._live_migration_claim(
7665                     ctxt, instance, migrate_data, migration, limits, allocs)
7666             elif 'dst_supports_numa_live_migration' in dest_check_data:
7667                 LOG.info('Destination was ready for NUMA live migration, '
7668                          'but source is either too old, or is set to an '
7669                          'older upgrade level.', instance=instance)
7670             # Create migrate_data vifs
7671             migrate_data.vifs = \
7672                 migrate_data_obj.VIFMigrateData.create_skeleton_migrate_vifs(
7673                     instance.get_network_info())
7674             # Claim PCI devices for VIFs on destination (if needed)
7675             port_id_to_pci = self._claim_pci_for_instance_vifs(ctxt, instance)
7676             # Update migrate VIFs with the newly claimed PCI devices
7677             self._update_migrate_vifs_profile_with_pci(migrate_data.vifs,
7678                                                        port_id_to_pci)
7679         finally:
7680             self.driver.cleanup_live_migration_destination_check(ctxt,
7681                     dest_check_data)
7682         return migrate_data
7683 
7684     def _live_migration_claim(self, ctxt, instance, migrate_data,
7685                               migration, limits, allocs):
7686         """Runs on the destination and does a resources claim, if necessary.
7687         Currently, only NUMA live migrations require it.
7688 
7689         :param ctxt: Request context
7690         :param instance: The Instance being live migrated
7691         :param migrate_data: The MigrateData object for this live migration
7692         :param migration: The Migration object for this live migration
7693         :param limits: The SchedulerLimits object for this live migration
7694         :returns: migrate_data with dst_numa_info set if necessary
7695         """
7696         try:
7697             # NOTE(artom) We might have gotten here from _find_destination() in
7698             # the conductor live migrate task. At that point,
7699             # migration.dest_node is not set yet (nor should it be, we're still
7700             # looking for a destination, after all). Therefore, we cannot use
7701             # migration.dest_node here and must use self._get_nodename().
7702             claim = self.rt.live_migration_claim(
7703                 ctxt, instance, self._get_nodename(instance), migration,
7704                 limits, allocs)
7705             LOG.debug('Created live migration claim.', instance=instance)
7706         except exception.ComputeResourcesUnavailable as e:
7707             raise exception.MigrationPreCheckError(
7708                 reason=e.format_message())
7709         return self.driver.post_claim_migrate_data(ctxt, instance,
7710                                                    migrate_data, claim)
7711 
7712     def _source_can_numa_live_migrate(self, ctxt, dest_check_data,
7713                                       source_check_data):
7714         # TODO(artom) Our virt driver may have told us that it supports NUMA
7715         # live migration. However, the following other conditions must be met
7716         # for a NUMA live migration to happen:
7717         # 1. We got a True dst_supports_numa_live_migration in
7718         #    dest_check_data, indicating that the dest virt driver supports
7719         #    NUMA live migration and that the conductor can send RPC 5.3 and
7720         #    that the destination compute manager can receive it.
7721         # 2. Ourselves, the source, can send RPC 5.3. There's no
7722         #    sentinel/parameter for this, so we just ask our rpcapi directly.
7723         # If any of these are not met, we need to remove the
7724         # src_supports_numa_live_migration flag from source_check_data to avoid
7725         # incorrectly initiating a NUMA live migration.
7726         # All of this can be removed in RPC 6.0/objects 2.0.
7727         can_numa_live_migrate = (
7728             'dst_supports_numa_live_migration' in dest_check_data and
7729             dest_check_data.dst_supports_numa_live_migration and
7730             self.compute_rpcapi.supports_numa_live_migration(ctxt))
7731         if ('src_supports_numa_live_migration' in source_check_data and
7732                 source_check_data.src_supports_numa_live_migration and
7733                 not can_numa_live_migrate):
7734             delattr(source_check_data, 'src_supports_numa_live_migration')
7735         return source_check_data
7736 
7737     @wrap_exception()
7738     @wrap_instance_event(prefix='compute')
7739     @wrap_instance_fault
7740     def check_can_live_migrate_source(self, ctxt, instance, dest_check_data):
7741         """Check if it is possible to execute live migration.
7742 
7743         This checks if the live migration can succeed, based on the
7744         results from check_can_live_migrate_destination.
7745 
7746         :param ctxt: security context
7747         :param instance: dict of instance data
7748         :param dest_check_data: result of check_can_live_migrate_destination
7749         :returns: a LiveMigrateData object
7750         """
7751         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
7752             ctxt, instance.uuid)
7753         is_volume_backed = compute_utils.is_volume_backed_instance(
7754             ctxt, instance, bdms)
7755         dest_check_data.is_volume_backed = is_volume_backed
7756         block_device_info = self._get_instance_block_device_info(
7757                             ctxt, instance, refresh_conn_info=False, bdms=bdms)
7758         result = self.driver.check_can_live_migrate_source(ctxt, instance,
7759                                                            dest_check_data,
7760                                                            block_device_info)
7761         result = self._source_can_numa_live_migrate(ctxt, dest_check_data,
7762                                                     result)
7763         LOG.debug('source check data is %s', result)
7764         return result
7765 
7766     # TODO(mriedem): Remove the block_migration argument in v6.0 of the compute
7767     # RPC API.
7768     @wrap_exception()
7769     @wrap_instance_event(prefix='compute')
7770     @wrap_instance_fault
7771     def pre_live_migration(self, context, instance, block_migration, disk,
7772                            migrate_data):
7773         """Preparations for live migration at dest host.
7774 
7775         :param context: security context
7776         :param instance: dict of instance data
7777         :param block_migration: if true, prepare for block migration
7778         :param disk: disk info of instance
7779         :param migrate_data: A dict or LiveMigrateData object holding data
7780                              required for live migration without shared
7781                              storage.
7782         :returns: migrate_data containing additional migration info
7783         """
7784         LOG.debug('pre_live_migration data is %s', migrate_data)
7785 
7786         migrate_data.old_vol_attachment_ids = {}
7787         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
7788             context, instance.uuid)
7789         network_info = self.network_api.get_instance_nw_info(context, instance)
7790         self._notify_about_instance_usage(
7791             context, instance, "live_migration.pre.start",
7792             network_info=network_info)
7793         compute_utils.notify_about_instance_action(
7794             context, instance, self.host,
7795             action=fields.NotificationAction.LIVE_MIGRATION_PRE,
7796             phase=fields.NotificationPhase.START, bdms=bdms)
7797 
7798         connector = self.driver.get_volume_connector(instance)
7799         try:
7800             for bdm in bdms:
7801                 if bdm.is_volume and bdm.attachment_id is not None:
7802                     # This bdm uses the new cinder v3.44 API.
7803                     # We will create a new attachment for this
7804                     # volume on this migration destination host. The old
7805                     # attachment will be deleted on the source host
7806                     # when the migration succeeds. The old attachment_id
7807                     # is stored in dict with the key being the bdm.volume_id
7808                     # so it can be restored on rollback.
7809                     #
7810                     # Also note that attachment_update is not needed as we
7811                     # are providing the connector in the create call.
7812                     attach_ref = self.volume_api.attachment_create(
7813                         context, bdm.volume_id, bdm.instance_uuid,
7814                         connector=connector, mountpoint=bdm.device_name)
7815 
7816                     # save current attachment so we can detach it on success,
7817                     # or restore it on a rollback.
7818                     # NOTE(mdbooth): This data is no longer used by the source
7819                     # host since change Ibe9215c0. We can't remove it until we
7820                     # are sure the source host has been upgraded.
7821                     migrate_data.old_vol_attachment_ids[bdm.volume_id] = \
7822                         bdm.attachment_id
7823 
7824                     # update the bdm with the new attachment_id.
7825                     bdm.attachment_id = attach_ref['id']
7826                     bdm.save()
7827 
7828             block_device_info = self._get_instance_block_device_info(
7829                                 context, instance, refresh_conn_info=True,
7830                                 bdms=bdms)
7831 
7832             # The driver pre_live_migration will plug vifs on the host
7833             migrate_data = self.driver.pre_live_migration(context,
7834                                            instance,
7835                                            block_device_info,
7836                                            network_info,
7837                                            disk,
7838                                            migrate_data)
7839             LOG.debug('driver pre_live_migration data is %s', migrate_data)
7840             # driver.pre_live_migration is what plugs vifs on the destination
7841             # host so now we can set the wait_for_vif_plugged flag in the
7842             # migrate_data object which the source compute will use to
7843             # determine if it should wait for a 'network-vif-plugged' event
7844             # from neutron before starting the actual guest transfer in the
7845             # hypervisor
7846             migrate_data.wait_for_vif_plugged = (
7847                 CONF.compute.live_migration_wait_for_vif_plug)
7848 
7849             # NOTE(tr3buchet): setup networks on destination host
7850             self.network_api.setup_networks_on_host(context, instance,
7851                                                              self.host)
7852 
7853         except Exception:
7854             # If we raise, migrate_data with the updated attachment ids
7855             # will not be returned to the source host for rollback.
7856             # So we need to rollback new attachments here.
7857             with excutils.save_and_reraise_exception():
7858                 old_attachments = migrate_data.old_vol_attachment_ids
7859                 for bdm in bdms:
7860                     if (bdm.is_volume and bdm.attachment_id is not None and
7861                             bdm.volume_id in old_attachments):
7862                         self.volume_api.attachment_delete(context,
7863                                                           bdm.attachment_id)
7864                         bdm.attachment_id = old_attachments[bdm.volume_id]
7865                         bdm.save()
7866 
7867         # Volume connections are complete, tell cinder that all the
7868         # attachments have completed.
7869         for bdm in bdms:
7870             if bdm.is_volume and bdm.attachment_id is not None:
7871                 self.volume_api.attachment_complete(context,
7872                                                     bdm.attachment_id)
7873 
7874         self._notify_about_instance_usage(
7875                      context, instance, "live_migration.pre.end",
7876                      network_info=network_info)
7877         compute_utils.notify_about_instance_action(
7878             context, instance, self.host,
7879             action=fields.NotificationAction.LIVE_MIGRATION_PRE,
7880             phase=fields.NotificationPhase.END, bdms=bdms)
7881 
7882         LOG.debug('pre_live_migration result data is %s', migrate_data)
7883         return migrate_data
7884 
7885     @staticmethod
7886     def _neutron_failed_migration_callback(event_name, instance):
7887         msg = ('Neutron reported failure during migration '
7888                'with %(event)s for instance %(uuid)s')
7889         msg_args = {'event': event_name, 'uuid': instance.uuid}
7890         if CONF.vif_plugging_is_fatal:
7891             raise exception.VirtualInterfacePlugException(msg % msg_args)
7892         LOG.error(msg, msg_args)
7893 
7894     @staticmethod
7895     def _get_neutron_events_for_live_migration(instance):
7896         # We don't generate events if CONF.vif_plugging_timeout=0
7897         # meaning that the operator disabled using them.
7898         if CONF.vif_plugging_timeout:
7899             return [('network-vif-plugged', vif['id'])
7900                     for vif in instance.get_network_info()]
7901         else:
7902             return []
7903 
7904     def _cleanup_pre_live_migration(self, context, dest, instance,
7905                                     migration, migrate_data, source_bdms):
7906         """Helper method for when pre_live_migration fails
7907 
7908         Sets the migration status to "error" and rolls back the live migration
7909         setup on the destination host.
7910 
7911         :param context: The user request context.
7912         :type context: nova.context.RequestContext
7913         :param dest: The live migration destination hostname.
7914         :type dest: str
7915         :param instance: The instance being live migrated.
7916         :type instance: nova.objects.Instance
7917         :param migration: The migration record tracking this live migration.
7918         :type migration: nova.objects.Migration
7919         :param migrate_data: Data about the live migration, populated from
7920                              the destination host.
7921         :type migrate_data: Subclass of nova.objects.LiveMigrateData
7922         :param source_bdms: BDMs prior to modification by the destination
7923                             compute host. Set by _do_live_migration and not
7924                             part of the callback interface, so this is never
7925                             None
7926         """
7927         self._set_migration_status(migration, 'error')
7928         # Make sure we set this for _rollback_live_migration()
7929         # so it can find it, as expected if it was called later
7930         migrate_data.migration = migration
7931         self._rollback_live_migration(context, instance, dest,
7932                                       migrate_data=migrate_data,
7933                                       source_bdms=source_bdms)
7934 
7935     def _do_pre_live_migration_from_source(self, context, dest, instance,
7936                                            block_migration, migration,
7937                                            migrate_data, source_bdms):
7938         """Prepares for pre-live-migration on the source host and calls dest
7939 
7940         Will setup a callback networking event handler (if configured) and
7941         then call the dest host's pre_live_migration method to prepare the
7942         dest host for live migration (plugs vifs, connect volumes, etc).
7943 
7944         _rollback_live_migration (on the source) will be called if
7945         pre_live_migration (on the dest) fails.
7946 
7947         :param context: nova auth request context for this operation
7948         :param dest: name of the destination compute service host
7949         :param instance: Instance object being live migrated
7950         :param block_migration: If true, prepare for block migration.
7951         :param migration: Migration object tracking this operation
7952         :param migrate_data: MigrateData object for this operation populated
7953             by the destination host compute driver as part of the
7954             check_can_live_migrate_destination call.
7955         :param source_bdms: BlockDeviceMappingList of BDMs currently attached
7956             to the instance from the source host.
7957         :returns: MigrateData object which is a modified version of the
7958             ``migrate_data`` argument from the compute driver on the dest
7959             host during the ``pre_live_migration`` call.
7960         :raises: MigrationError if waiting for the network-vif-plugged event
7961             timed out and is fatal.
7962         """
7963         class _BreakWaitForInstanceEvent(Exception):
7964             """Used as a signal to stop waiting for the network-vif-plugged
7965             event when we discover that
7966             [compute]/live_migration_wait_for_vif_plug is not set on the
7967             destination.
7968             """
7969             pass
7970 
7971         events = self._get_neutron_events_for_live_migration(instance)
7972         try:
7973             if ('block_migration' in migrate_data and
7974                     migrate_data.block_migration):
7975                 block_device_info = self._get_instance_block_device_info(
7976                     context, instance, bdms=source_bdms)
7977                 disk = self.driver.get_instance_disk_info(
7978                     instance, block_device_info=block_device_info)
7979             else:
7980                 disk = None
7981 
7982             deadline = CONF.vif_plugging_timeout
7983             error_cb = self._neutron_failed_migration_callback
7984             # In order to avoid a race with the vif plugging that the virt
7985             # driver does on the destination host, we register our events
7986             # to wait for before calling pre_live_migration. Then if the
7987             # dest host reports back that we shouldn't wait, we can break
7988             # out of the context manager using _BreakWaitForInstanceEvent.
7989             with self.virtapi.wait_for_instance_event(
7990                     instance, events, deadline=deadline,
7991                     error_callback=error_cb):
7992                 with timeutils.StopWatch() as timer:
7993                     # TODO(mriedem): The "block_migration" parameter passed
7994                     # here is not actually used in pre_live_migration but it
7995                     # is not optional in the RPC interface either.
7996                     migrate_data = self.compute_rpcapi.pre_live_migration(
7997                         context, instance,
7998                         block_migration, disk, dest, migrate_data)
7999                 LOG.info('Took %0.2f seconds for pre_live_migration on '
8000                          'destination host %s.',
8001                          timer.elapsed(), dest, instance=instance)
8002                 wait_for_vif_plugged = (
8003                     'wait_for_vif_plugged' in migrate_data and
8004                     migrate_data.wait_for_vif_plugged)
8005                 if events and not wait_for_vif_plugged:
8006                     raise _BreakWaitForInstanceEvent
8007         except _BreakWaitForInstanceEvent:
8008             if events:
8009                 LOG.debug('Not waiting for events after pre_live_migration: '
8010                           '%s. ', events, instance=instance)
8011             # This is a bit weird, but we need to clear sys.exc_info() so that
8012             # oslo.log formatting does not inadvertently use it later if an
8013             # error message is logged without an explicit exc_info. This is
8014             # only a problem with python 2.
8015             if six.PY2:
8016                 sys.exc_clear()
8017         except exception.VirtualInterfacePlugException:
8018             with excutils.save_and_reraise_exception():
8019                 LOG.exception('Failed waiting for network virtual interfaces '
8020                               'to be plugged on the destination host %s.',
8021                               dest, instance=instance)
8022                 self._cleanup_pre_live_migration(
8023                     context, dest, instance, migration, migrate_data,
8024                     source_bdms)
8025         except eventlet.timeout.Timeout:
8026             # We only get here if wait_for_vif_plugged is True which means
8027             # live_migration_wait_for_vif_plug=True on the destination host.
8028             msg = (
8029                 'Timed out waiting for events: %(events)s. If these timeouts '
8030                 'are a persistent issue it could mean the networking backend '
8031                 'on host %(dest)s does not support sending these events '
8032                 'unless there are port binding host changes which does not '
8033                 'happen at this point in the live migration process. You may '
8034                 'need to disable the live_migration_wait_for_vif_plug option '
8035                 'on host %(dest)s.')
8036             subs = {'events': events, 'dest': dest}
8037             LOG.warning(msg, subs, instance=instance)
8038             if CONF.vif_plugging_is_fatal:
8039                 self._cleanup_pre_live_migration(
8040                     context, dest, instance, migration, migrate_data,
8041                     source_bdms)
8042                 raise exception.MigrationError(reason=msg % subs)
8043         except Exception:
8044             with excutils.save_and_reraise_exception():
8045                 LOG.exception('Pre live migration failed at %s',
8046                               dest, instance=instance)
8047                 self._cleanup_pre_live_migration(
8048                     context, dest, instance, migration, migrate_data,
8049                     source_bdms)
8050         return migrate_data
8051 
8052     def _do_live_migration(self, context, dest, instance, block_migration,
8053                            migration, migrate_data):
8054         # NOTE(danms): We should enhance the RT to account for migrations
8055         # and use the status field to denote when the accounting has been
8056         # done on source/destination. For now, this is just here for status
8057         # reporting
8058         self._set_migration_status(migration, 'preparing')
8059         source_bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
8060                 context, instance.uuid)
8061 
8062         migrate_data = self._do_pre_live_migration_from_source(
8063             context, dest, instance, block_migration, migration, migrate_data,
8064             source_bdms)
8065 
8066         # Set migrate_data.migration because that is how _post_live_migration
8067         # and _rollback_live_migration get the migration object for cleanup.
8068         # Yes this is gross but changing the _post_live_migration and
8069         # _rollback_live_migration interfaces would also mean changing how the
8070         # virt drivers call them from the driver.live_migration method, i.e.
8071         # we would have to pass the migration object through the driver (or
8072         # consider using a partial but some do not like that pattern).
8073         migrate_data.migration = migration
8074 
8075         # NOTE(Kevin_Zheng): Pop the migration from the waiting queue
8076         # if it exist in the queue, then we are good to moving on, if
8077         # not, some other process must have aborted it, then we should
8078         # rollback.
8079         try:
8080             self._waiting_live_migrations.pop(instance.uuid)
8081         except KeyError:
8082             LOG.debug('Migration %s aborted by another process, rollback.',
8083                       migration.uuid, instance=instance)
8084             self._rollback_live_migration(context, instance, dest,
8085                                           migrate_data, 'cancelled',
8086                                           source_bdms=source_bdms)
8087             self._notify_live_migrate_abort_end(context, instance)
8088             return
8089 
8090         self._set_migration_status(migration, 'running')
8091 
8092         # NOTE(mdbooth): pre_live_migration will update connection_info and
8093         # attachment_id on all volume BDMS to reflect the new destination
8094         # host attachment. We fetch BDMs before that to retain connection_info
8095         # and attachment_id relating to the source host for post migration
8096         # cleanup.
8097         post_live_migration = functools.partial(self._post_live_migration,
8098                                                 source_bdms=source_bdms)
8099         rollback_live_migration = functools.partial(
8100             self._rollback_live_migration, source_bdms=source_bdms)
8101 
8102         LOG.debug('live_migration data is %s', migrate_data)
8103         try:
8104             self.driver.live_migration(context, instance, dest,
8105                                        post_live_migration,
8106                                        rollback_live_migration,
8107                                        block_migration, migrate_data)
8108         except Exception:
8109             LOG.exception('Live migration failed.', instance=instance)
8110             with excutils.save_and_reraise_exception():
8111                 # Put instance and migration into error state,
8112                 # as its almost certainly too late to rollback
8113                 self._set_migration_status(migration, 'error')
8114                 # first refresh instance as it may have got updated by
8115                 # post_live_migration_at_destination
8116                 instance.refresh()
8117                 self._set_instance_obj_error_state(instance,
8118                                                    clean_task_state=True)
8119 
8120     @wrap_exception()
8121     @wrap_instance_event(prefix='compute')
8122     @errors_out_migration
8123     @wrap_instance_fault
8124     def live_migration(self, context, dest, instance, block_migration,
8125                        migration, migrate_data):
8126         """Executing live migration.
8127 
8128         :param context: security context
8129         :param dest: destination host
8130         :param instance: a nova.objects.instance.Instance object
8131         :param block_migration: if true, prepare for block migration
8132         :param migration: an nova.objects.Migration object
8133         :param migrate_data: implementation specific params
8134 
8135         """
8136         self._set_migration_status(migration, 'queued')
8137         # NOTE(Kevin_Zheng): Submit the live_migration job to the pool and
8138         # put the returned Future object into dict mapped with migration.uuid
8139         # in order to be able to track and abort it in the future.
8140         self._waiting_live_migrations[instance.uuid] = (None, None)
8141         try:
8142             future = self._live_migration_executor.submit(
8143                 self._do_live_migration, context, dest, instance,
8144                 block_migration, migration, migrate_data)
8145             self._waiting_live_migrations[instance.uuid] = (migration, future)
8146         except RuntimeError:
8147             # GreenThreadPoolExecutor.submit will raise RuntimeError if the
8148             # pool is shutdown, which happens in
8149             # _cleanup_live_migrations_in_pool.
8150             LOG.info('Migration %s failed to submit as the compute service '
8151                      'is shutting down.', migration.uuid, instance=instance)
8152             raise exception.LiveMigrationNotSubmitted(
8153                 migration_uuid=migration.uuid, instance_uuid=instance.uuid)
8154 
8155     @wrap_exception()
8156     @wrap_instance_event(prefix='compute')
8157     @wrap_instance_fault
8158     def live_migration_force_complete(self, context, instance):
8159         """Force live migration to complete.
8160 
8161         :param context: Security context
8162         :param instance: The instance that is being migrated
8163         """
8164 
8165         self._notify_about_instance_usage(
8166             context, instance, 'live.migration.force.complete.start')
8167         compute_utils.notify_about_instance_action(
8168             context, instance, self.host,
8169             action=fields.NotificationAction.LIVE_MIGRATION_FORCE_COMPLETE,
8170             phase=fields.NotificationPhase.START)
8171         self.driver.live_migration_force_complete(instance)
8172         self._notify_about_instance_usage(
8173             context, instance, 'live.migration.force.complete.end')
8174         compute_utils.notify_about_instance_action(
8175             context, instance, self.host,
8176             action=fields.NotificationAction.LIVE_MIGRATION_FORCE_COMPLETE,
8177             phase=fields.NotificationPhase.END)
8178 
8179     def _notify_live_migrate_abort_end(self, context, instance):
8180         self._notify_about_instance_usage(
8181             context, instance, 'live.migration.abort.end')
8182         compute_utils.notify_about_instance_action(
8183             context, instance, self.host,
8184             action=fields.NotificationAction.LIVE_MIGRATION_ABORT,
8185             phase=fields.NotificationPhase.END)
8186 
8187     @wrap_exception()
8188     @wrap_instance_event(prefix='compute')
8189     @wrap_instance_fault
8190     def live_migration_abort(self, context, instance, migration_id):
8191         """Abort an in-progress live migration.
8192 
8193         :param context: Security context
8194         :param instance: The instance that is being migrated
8195         :param migration_id: ID of in-progress live migration
8196 
8197         """
8198         self._notify_about_instance_usage(
8199             context, instance, 'live.migration.abort.start')
8200         compute_utils.notify_about_instance_action(
8201             context, instance, self.host,
8202             action=fields.NotificationAction.LIVE_MIGRATION_ABORT,
8203             phase=fields.NotificationPhase.START)
8204         # NOTE(Kevin_Zheng): Pop the migration out from the queue, this might
8205         # lead to 3 scenarios:
8206         # 1. The selected migration is still in queue, and the future.cancel()
8207         #    succeed, then the abort action is succeed, mark the migration
8208         #    status to 'cancelled'.
8209         # 2. The selected migration is still in queue, but the future.cancel()
8210         #    failed, then the _do_live_migration() has started executing, and
8211         #    the migration status is 'preparing', then we just pop it from the
8212         #    queue, and the migration process will handle it later. And the
8213         #    migration status couldn't be 'running' in this scenario because
8214         #    if _do_live_migration has started executing and we've already
8215         #    popped it from the queue and set the migration status to
8216         #    'running' at this point, popping it here will raise KeyError at
8217         #    which point we check if it's running and if so, we abort the old
8218         #    way.
8219         # 3. The selected migration is not in the queue, then the migration
8220         #    status is 'running', let the driver handle it.
8221         try:
8222             migration, future = (
8223                 self._waiting_live_migrations.pop(instance.uuid))
8224             if future and future.cancel():
8225                 # If we got here, we've successfully aborted the queued
8226                 # migration and _do_live_migration won't run so we need
8227                 # to set the migration status to cancelled and send the
8228                 # notification. If Future.cancel() fails, it means
8229                 # _do_live_migration is running and the migration status
8230                 # is preparing, and _do_live_migration() itself will attempt
8231                 # to pop the queued migration, hit a KeyError, and rollback,
8232                 # set the migration to cancelled and send the
8233                 # live.migration.abort.end notification.
8234                 self._set_migration_status(migration, 'cancelled')
8235         except KeyError:
8236             migration = objects.Migration.get_by_id(context, migration_id)
8237             if migration.status != 'running':
8238                 raise exception.InvalidMigrationState(
8239                     migration_id=migration_id, instance_uuid=instance.uuid,
8240                     state=migration.status, method='abort live migration')
8241             self.driver.live_migration_abort(instance)
8242         self._notify_live_migrate_abort_end(context, instance)
8243 
8244     def _live_migration_cleanup_flags(self, migrate_data, migr_ctxt=None):
8245         """Determine whether disks, instance path or other resources
8246         need to be cleaned up after live migration (at source on success,
8247         at destination on rollback)
8248 
8249         Block migration needs empty image at destination host before migration
8250         starts, so if any failure occurs, any empty images has to be deleted.
8251 
8252         Also Volume backed live migration w/o shared storage needs to delete
8253         newly created instance-xxx dir on the destination as a part of its
8254         rollback process
8255 
8256         There may be other resources which need cleanup; currently this is
8257         limited to vPMEM devices with the libvirt driver.
8258 
8259         :param migrate_data: implementation specific data
8260         :param migr_ctxt: specific resources stored in migration_context
8261         :returns: (bool, bool) -- do_cleanup, destroy_disks
8262         """
8263         # NOTE(pkoniszewski): block migration specific params are set inside
8264         # migrate_data objects for drivers that expose block live migration
8265         # information (i.e. Libvirt, Xenapi and HyperV). For other drivers
8266         # cleanup is not needed.
8267         do_cleanup = False
8268         destroy_disks = False
8269         if isinstance(migrate_data, migrate_data_obj.LibvirtLiveMigrateData):
8270             has_vpmem = False
8271             if migr_ctxt and migr_ctxt.old_resources:
8272                 for resource in migr_ctxt.old_resources:
8273                     if ('metadata' in resource and
8274                         isinstance(resource.metadata,
8275                                    objects.LibvirtVPMEMDevice)):
8276                         has_vpmem = True
8277                         break
8278             # No instance booting at source host, but instance dir
8279             # must be deleted for preparing next block migration
8280             # must be deleted for preparing next live migration w/o shared
8281             # storage
8282             # vpmem must be cleanped
8283             do_cleanup = not migrate_data.is_shared_instance_path or has_vpmem
8284             destroy_disks = not migrate_data.is_shared_block_storage
8285         elif isinstance(migrate_data, migrate_data_obj.XenapiLiveMigrateData):
8286             do_cleanup = migrate_data.block_migration
8287             destroy_disks = migrate_data.block_migration
8288         elif isinstance(migrate_data, migrate_data_obj.HyperVLiveMigrateData):
8289             # NOTE(claudiub): We need to cleanup any zombie Planned VM.
8290             do_cleanup = True
8291             destroy_disks = not migrate_data.is_shared_instance_path
8292 
8293         return (do_cleanup, destroy_disks)
8294 
8295     def _post_live_migration_remove_source_vol_connections(
8296             self, context, instance, source_bdms):
8297         """Disconnect volume connections from the source host during
8298         _post_live_migration.
8299 
8300         :param context: nova auth RequestContext
8301         :param instance: Instance object being live migrated
8302         :param source_bdms: BlockDeviceMappingList representing the attached
8303             volumes with connection_info set for the source host
8304         """
8305         # Detaching volumes.
8306         connector = self.driver.get_volume_connector(instance)
8307         for bdm in source_bdms:
8308             if bdm.is_volume:
8309                 # Detaching volumes is a call to an external API that can fail.
8310                 # If it does, we need to handle it gracefully so that the call
8311                 # to post_live_migration_at_destination - where we set instance
8312                 # host and task state - still happens. We need to rethink the
8313                 # current approach of setting instance host and task state
8314                 # AFTER a whole bunch of things that could fail in unhandled
8315                 # ways, but that is left as a TODO(artom).
8316                 try:
8317                     if bdm.attachment_id is None:
8318                         # Prior to cinder v3.44:
8319                         # We don't want to actually mark the volume detached,
8320                         # or delete the bdm, just remove the connection from
8321                         # this host.
8322                         #
8323                         # remove the volume connection without detaching from
8324                         # hypervisor because the instance is not running
8325                         # anymore on the current host
8326                         self.volume_api.terminate_connection(context,
8327                                                              bdm.volume_id,
8328                                                              connector)
8329                     else:
8330                         # cinder v3.44 api flow - delete the old attachment
8331                         # for the source host
8332                         self.volume_api.attachment_delete(context,
8333                                                           bdm.attachment_id)
8334 
8335                 except Exception as e:
8336                     if bdm.attachment_id is None:
8337                         LOG.error('Connection for volume %s not terminated on '
8338                                   'source host %s during post_live_migration: '
8339                                   '%s', bdm.volume_id, self.host,
8340                                   six.text_type(e), instance=instance)
8341                     else:
8342                         LOG.error('Volume attachment %s not deleted on source '
8343                                   'host %s during post_live_migration: %s',
8344                                   bdm.attachment_id, self.host,
8345                                   six.text_type(e), instance=instance)
8346 
8347     @wrap_exception()
8348     @wrap_instance_fault
8349     def _post_live_migration(self, ctxt, instance, dest,
8350                              block_migration=False, migrate_data=None,
8351                              source_bdms=None):
8352         """Post operations for live migration.
8353 
8354         This method is called from live_migration
8355         and mainly updating database record.
8356 
8357         :param ctxt: security context
8358         :param instance: instance dict
8359         :param dest: destination host
8360         :param block_migration: if true, prepare for block migration
8361         :param migrate_data: if not None, it is a dict which has data
8362         :param source_bdms: BDMs prior to modification by the destination
8363                             compute host. Set by _do_live_migration and not
8364                             part of the callback interface, so this is never
8365                             None
8366         required for live migration without shared storage
8367 
8368         """
8369         LOG.info('_post_live_migration() is started..',
8370                  instance=instance)
8371 
8372         # Cleanup source host post live-migration
8373         block_device_info = self._get_instance_block_device_info(
8374                             ctxt, instance, bdms=source_bdms)
8375         self.driver.post_live_migration(ctxt, instance, block_device_info,
8376                                         migrate_data)
8377 
8378         # Disconnect volumes from this (the source) host.
8379         self._post_live_migration_remove_source_vol_connections(
8380             ctxt, instance, source_bdms)
8381 
8382         # Releasing vlan.
8383         # (not necessary in current implementation?)
8384 
8385         network_info = self.network_api.get_instance_nw_info(ctxt, instance)
8386 
8387         self._notify_about_instance_usage(ctxt, instance,
8388                                           "live_migration._post.start",
8389                                           network_info=network_info)
8390         compute_utils.notify_about_instance_action(
8391             ctxt, instance, self.host,
8392             action=fields.NotificationAction.LIVE_MIGRATION_POST,
8393             phase=fields.NotificationPhase.START)
8394 
8395         migration = {'source_compute': self.host,
8396                      'dest_compute': dest, }
8397         # For neutron, migrate_instance_start will activate the destination
8398         # host port bindings, if there are any created by conductor before live
8399         # migration started.
8400         self.network_api.migrate_instance_start(ctxt,
8401                                                 instance,
8402                                                 migration)
8403 
8404         destroy_vifs = False
8405         try:
8406             # It's possible that the vif type changed on the destination
8407             # host and is already bound and active, so we need to use the
8408             # stashed source vifs in migrate_data.vifs (if present) to unplug
8409             # on the source host.
8410             unplug_nw_info = network_info
8411             if migrate_data and 'vifs' in migrate_data:
8412                 nw_info = []
8413                 for migrate_vif in migrate_data.vifs:
8414                     nw_info.append(migrate_vif.source_vif)
8415                 unplug_nw_info = network_model.NetworkInfo.hydrate(nw_info)
8416                 LOG.debug('Calling driver.post_live_migration_at_source '
8417                           'with original source VIFs from migrate_data: %s',
8418                           unplug_nw_info, instance=instance)
8419             self.driver.post_live_migration_at_source(ctxt, instance,
8420                                                       unplug_nw_info)
8421         except NotImplementedError as ex:
8422             LOG.debug(ex, instance=instance)
8423             # For all hypervisors other than libvirt, there is a possibility
8424             # they are unplugging networks from source node in the cleanup
8425             # method
8426             destroy_vifs = True
8427 
8428         # Free instance allocations on source before claims are allocated on
8429         # destination node
8430         self.rt.free_pci_device_allocations_for_instance(ctxt, instance)
8431         # NOTE(danms): Save source node before calling post method on
8432         # destination, which will update it
8433         source_node = instance.node
8434 
8435         do_cleanup, destroy_disks = self._live_migration_cleanup_flags(
8436             migrate_data, migr_ctxt=instance.migration_context)
8437 
8438         if do_cleanup:
8439             LOG.debug('Calling driver.cleanup from _post_live_migration',
8440                       instance=instance)
8441             self.driver.cleanup(ctxt, instance, unplug_nw_info,
8442                                 destroy_disks=destroy_disks,
8443                                 migrate_data=migrate_data,
8444                                 destroy_vifs=destroy_vifs)
8445 
8446         # Define domain at destination host, without doing it,
8447         # pause/suspend/terminate do not work.
8448         post_at_dest_success = True
8449         try:
8450             self.compute_rpcapi.post_live_migration_at_destination(ctxt,
8451                     instance, block_migration, dest)
8452         except Exception as error:
8453             post_at_dest_success = False
8454             # We don't want to break _post_live_migration() if
8455             # post_live_migration_at_destination() fails as it should never
8456             # affect cleaning up source node.
8457             LOG.exception("Post live migration at destination %s failed",
8458                           dest, instance=instance, error=error)
8459 
8460         self.instance_events.clear_events_for_instance(instance)
8461 
8462         # NOTE(timello): make sure we update available resources on source
8463         # host even before next periodic task.
8464         self.update_available_resource(ctxt)
8465 
8466         self._update_scheduler_instance_info(ctxt, instance)
8467         self._notify_about_instance_usage(ctxt, instance,
8468                                           "live_migration._post.end",
8469                                           network_info=network_info)
8470         compute_utils.notify_about_instance_action(
8471             ctxt, instance, self.host,
8472             action=fields.NotificationAction.LIVE_MIGRATION_POST,
8473             phase=fields.NotificationPhase.END)
8474         if post_at_dest_success:
8475             LOG.info('Migrating instance to %s finished successfully.',
8476                      dest, instance=instance)
8477 
8478         self._clean_instance_console_tokens(ctxt, instance)
8479         if migrate_data and migrate_data.obj_attr_is_set('migration'):
8480             migrate_data.migration.status = 'completed'
8481             migrate_data.migration.save()
8482             self._delete_allocation_after_move(ctxt,
8483                                                instance,
8484                                                migrate_data.migration)
8485         else:
8486             # We didn't have data on a migration, which means we can't
8487             # look up to see if we had new-style migration-based
8488             # allocations. This should really only happen in cases of
8489             # a buggy virt driver. Log a warning so we know it happened.
8490             LOG.warning('Live migration ended with no migrate_data '
8491                         'record. Unable to clean up migration-based '
8492                         'allocations for node %s which is almost certainly '
8493                         'not an expected situation.', source_node,
8494                         instance=instance)
8495 
8496     def _consoles_enabled(self):
8497         """Returns whether a console is enable."""
8498         return (CONF.vnc.enabled or CONF.spice.enabled or
8499                 CONF.rdp.enabled or CONF.serial_console.enabled or
8500                 CONF.mks.enabled)
8501 
8502     def _clean_instance_console_tokens(self, ctxt, instance):
8503         """Clean console tokens stored for an instance."""
8504         # If the database backend isn't in use, don't bother trying to clean
8505         # tokens.
8506         if self._consoles_enabled():
8507             objects.ConsoleAuthToken.\
8508                 clean_console_auths_for_instance(ctxt, instance.uuid)
8509 
8510     @wrap_exception()
8511     @wrap_instance_event(prefix='compute')
8512     @wrap_instance_fault
8513     def post_live_migration_at_destination(self, context, instance,
8514                                            block_migration):
8515         """Post operations for live migration .
8516 
8517         :param context: security context
8518         :param instance: Instance dict
8519         :param block_migration: if true, prepare for block migration
8520 
8521         """
8522         LOG.info('Post operation of migration started',
8523                  instance=instance)
8524 
8525         # NOTE(tr3buchet): setup networks on destination host
8526         #                  this is called a second time because
8527         #                  multi_host does not create the bridge in
8528         #                  plug_vifs
8529         # NOTE(mriedem): This is a no-op for neutron.
8530         self.network_api.setup_networks_on_host(context, instance,
8531                                                          self.host)
8532         migration = objects.Migration(
8533             source_compute=instance.host,
8534             dest_compute=self.host,
8535             migration_type=fields.MigrationType.LIVE_MIGRATION)
8536         self.network_api.migrate_instance_finish(
8537             context, instance, migration, provider_mappings=None)
8538 
8539         network_info = self.network_api.get_instance_nw_info(context, instance)
8540         self._notify_about_instance_usage(
8541                      context, instance, "live_migration.post.dest.start",
8542                      network_info=network_info)
8543         compute_utils.notify_about_instance_action(context, instance,
8544                 self.host,
8545                 action=fields.NotificationAction.LIVE_MIGRATION_POST_DEST,
8546                 phase=fields.NotificationPhase.START)
8547         block_device_info = self._get_instance_block_device_info(context,
8548                                                                  instance)
8549         # Allocate the claimed PCI resources at destination.
8550         self.rt.allocate_pci_devices_for_instance(context, instance)
8551 
8552         try:
8553             self.driver.post_live_migration_at_destination(
8554                 context, instance, network_info, block_migration,
8555                 block_device_info)
8556         except Exception:
8557             with excutils.save_and_reraise_exception():
8558                 instance.vm_state = vm_states.ERROR
8559                 LOG.error('Unexpected error during post live migration at '
8560                           'destination host.', instance=instance)
8561         finally:
8562             # Restore instance state and update host
8563             current_power_state = self._get_power_state(instance)
8564             node_name = None
8565             prev_host = instance.host
8566             try:
8567                 compute_node = self._get_compute_info(context, self.host)
8568                 node_name = compute_node.hypervisor_hostname
8569             except exception.ComputeHostNotFound:
8570                 LOG.exception('Failed to get compute_info for %s', self.host)
8571             finally:
8572                 # NOTE(artom) We need to apply the migration context here
8573                 # regardless of whether the driver's
8574                 # post_live_migration_at_destination succeeded or not: the
8575                 # instance is on the destination, potentially with a new NUMA
8576                 # topology and resource usage. We need to persist that.
8577                 # NOTE(artom) Apply followed by drop looks weird, but apply
8578                 # just saves the new fields while drop actually removes the
8579                 # migration context from the instance.
8580                 instance.apply_migration_context()
8581                 instance.drop_migration_context()
8582                 instance.host = self.host
8583                 instance.power_state = current_power_state
8584                 instance.task_state = None
8585                 instance.node = node_name
8586                 instance.progress = 0
8587                 instance.save(expected_task_state=task_states.MIGRATING)
8588 
8589         # NOTE(tr3buchet): tear down networks on source host (nova-net)
8590         # NOTE(mriedem): For neutron, this will delete any inactive source
8591         # host port bindings.
8592         try:
8593             self.network_api.setup_networks_on_host(context, instance,
8594                                                     prev_host, teardown=True)
8595         except exception.PortBindingDeletionFailed as e:
8596             # Removing the inactive port bindings from the source host is not
8597             # critical so just log an error but don't fail.
8598             LOG.error('Network cleanup failed for source host %s during post '
8599                       'live migration. You may need to manually clean up '
8600                       'resources in the network service. Error: %s',
8601                       prev_host, six.text_type(e))
8602         # NOTE(vish): this is necessary to update dhcp for nova-network
8603         # NOTE(mriedem): This is a no-op for neutron.
8604         self.network_api.setup_networks_on_host(context, instance, self.host)
8605         self._notify_about_instance_usage(
8606                      context, instance, "live_migration.post.dest.end",
8607                      network_info=network_info)
8608         compute_utils.notify_about_instance_action(context, instance,
8609                 self.host,
8610                 action=fields.NotificationAction.LIVE_MIGRATION_POST_DEST,
8611                 phase=fields.NotificationPhase.END)
8612 
8613     def _remove_remote_volume_connections(self, context, dest, bdms, instance):
8614         """Rollback remote volume connections on the dest"""
8615         for bdm in bdms:
8616             try:
8617                 # remove the connection on the destination host
8618                 # NOTE(lyarwood): This actually calls the cinderv2
8619                 # os-terminate_connection API if required.
8620                 self.compute_rpcapi.remove_volume_connection(
8621                         context, instance, bdm.volume_id, dest)
8622             except Exception:
8623                 LOG.warning("Ignoring exception while attempting "
8624                             "to rollback volume connections for "
8625                             "volume %s on host %s.", bdm.volume_id,
8626                             dest, instance=instance)
8627 
8628     def _rollback_volume_bdms(self, context, bdms, original_bdms, instance):
8629         """Rollback the connection_info and attachment_id for each bdm"""
8630         original_bdms_by_volid = {bdm.volume_id: bdm for bdm in original_bdms
8631                                   if bdm.is_volume}
8632         for bdm in bdms:
8633             try:
8634                 original_bdm = original_bdms_by_volid[bdm.volume_id]
8635                 if bdm.attachment_id and original_bdm.attachment_id:
8636                     # NOTE(lyarwood): 3.44 cinder api flow. Delete the
8637                     # attachment used by the bdm and reset it to that of
8638                     # the original bdm.
8639                     self.volume_api.attachment_delete(context,
8640                                                       bdm.attachment_id)
8641                     bdm.attachment_id = original_bdm.attachment_id
8642                 # NOTE(lyarwood): Reset the connection_info to the original
8643                 bdm.connection_info = original_bdm.connection_info
8644                 bdm.save()
8645             except cinder_exception.ClientException:
8646                 LOG.warning("Ignoring cinderclient exception when "
8647                             "attempting to delete attachment %s for volume "
8648                             "%s while rolling back volume bdms.",
8649                             bdm.attachment_id, bdm.volume_id,
8650                             instance=instance)
8651             except Exception:
8652                 with excutils.save_and_reraise_exception():
8653                     LOG.exception("Exception while attempting to rollback "
8654                                   "BDM for volume %s.", bdm.volume_id,
8655                                   instance=instance)
8656 
8657     @wrap_exception()
8658     @wrap_instance_fault
8659     def _rollback_live_migration(self, context, instance,
8660                                  dest, migrate_data=None,
8661                                  migration_status='error',
8662                                  source_bdms=None):
8663         """Recovers Instance/volume state from migrating -> running.
8664 
8665         :param context: security context
8666         :param instance: nova.objects.instance.Instance object
8667         :param dest:
8668             This method is called from live migration src host.
8669             This param specifies destination host.
8670         :param migrate_data:
8671             if not none, contains implementation specific data.
8672         :param migration_status:
8673             Contains the status we want to set for the migration object
8674         :param source_bdms: BDMs prior to modification by the destination
8675                             compute host. Set by _do_live_migration and not
8676                             part of the callback interface, so this is never
8677                             None
8678 
8679         """
8680         # NOTE(gibi): We need to refresh pci_requests of the instance as it
8681         # might be changed by the conductor during scheduling based on the
8682         # selected destination host. If the instance has SRIOV ports with
8683         # resource request then the LiveMigrationTask._find_destination call
8684         # updated the instance.pci_requests.requests[].spec with the SRIOV PF
8685         # device name to be used on the destination host. As the migration is
8686         # rolling back to the source host now we don't want to persist the
8687         # destination host related changes in the DB.
8688         instance.pci_requests = \
8689             objects.InstancePCIRequests.get_by_instance_uuid(
8690                 context, instance.uuid)
8691 
8692         if (isinstance(migrate_data, migrate_data_obj.LiveMigrateData) and
8693               migrate_data.obj_attr_is_set('migration')):
8694             migration = migrate_data.migration
8695         else:
8696             migration = None
8697 
8698         if migration:
8699             # Remove allocations created in Placement for the dest node.
8700             # If migration is None, the virt driver didn't pass it which is
8701             # a bug.
8702             self._revert_allocation(context, instance, migration)
8703         else:
8704             LOG.error('Unable to revert allocations during live migration '
8705                       'rollback; compute driver did not provide migrate_data',
8706                       instance=instance)
8707 
8708         # NOTE(tr3buchet): setup networks on source host (really it's re-setup
8709         #                  for nova-network)
8710         # NOTE(mriedem): This is a no-op for neutron.
8711         self.network_api.setup_networks_on_host(context, instance, self.host)
8712         self.driver.rollback_live_migration_at_source(context, instance,
8713                                                       migrate_data)
8714 
8715         # NOTE(lyarwood): Fetch the current list of BDMs, disconnect any
8716         # connected volumes from the dest and delete any volume attachments
8717         # used by the destination host before rolling back to the original
8718         # still valid source host volume attachments.
8719         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
8720                 context, instance.uuid)
8721         # TODO(lyarwood): Turn the following into a lookup method within
8722         # BlockDeviceMappingList.
8723         vol_bdms = [bdm for bdm in bdms if bdm.is_volume]
8724         self._remove_remote_volume_connections(context, dest, vol_bdms,
8725                                                instance)
8726         self._rollback_volume_bdms(context, vol_bdms, source_bdms, instance)
8727 
8728         self._notify_about_instance_usage(context, instance,
8729                                           "live_migration._rollback.start")
8730         compute_utils.notify_about_instance_action(context, instance,
8731                 self.host,
8732                 action=fields.NotificationAction.LIVE_MIGRATION_ROLLBACK,
8733                 phase=fields.NotificationPhase.START,
8734                 bdms=bdms)
8735 
8736         do_cleanup, destroy_disks = self._live_migration_cleanup_flags(
8737                 migrate_data, migr_ctxt=instance.migration_context)
8738 
8739         if do_cleanup:
8740             self.compute_rpcapi.rollback_live_migration_at_destination(
8741                     context, instance, dest, destroy_disks=destroy_disks,
8742                     migrate_data=migrate_data)
8743         else:
8744             # The port binding profiles need to be cleaned up.
8745             with errors_out_migration_ctxt(migration):
8746                 try:
8747                     # This call will delete any inactive destination host
8748                     # port bindings.
8749                     self.network_api.setup_networks_on_host(
8750                         context, instance, host=dest, teardown=True)
8751                 except exception.PortBindingDeletionFailed as e:
8752                     # Removing the inactive port bindings from the destination
8753                     # host is not critical so just log an error but don't fail.
8754                     LOG.error(
8755                         'Network cleanup failed for destination host %s '
8756                         'during live migration rollback. You may need to '
8757                         'manually clean up resources in the network service. '
8758                         'Error: %s', dest, six.text_type(e))
8759                 except Exception:
8760                     with excutils.save_and_reraise_exception():
8761                         LOG.exception(
8762                             'An error occurred while cleaning up networking '
8763                             'during live migration rollback.',
8764                             instance=instance)
8765 
8766         # NOTE(luyao): We drop move_claim and migration_context after cleanup
8767         # is complete, to ensure the specific resources claimed on destination
8768         # are released safely.
8769         # TODO(artom) drop_move_claim_at_destination() is new in RPC 5.3, only
8770         # call it if we performed a NUMA-aware live migration (which implies us
8771         # being able to send RPC 5.3). To check this, we can use the
8772         # src_supports_numa_live_migration flag, as it will be set if and only
8773         # if:
8774         # - dst_supports_numa_live_migration made its way to the source
8775         #   (meaning both dest and source are new and conductor can speak
8776         #   RPC 5.3)
8777         # - src_supports_numa_live_migration was set by the source driver and
8778         #   passed the send-RPC-5.3 check.
8779         # This check can be removed in RPC 6.0.
8780         if ('src_supports_numa_live_migration' in migrate_data and
8781                 migrate_data.src_supports_numa_live_migration):
8782             LOG.debug('Calling destination to drop move claim.',
8783                       instance=instance)
8784             self.compute_rpcapi.drop_move_claim_at_destination(context,
8785                                                                instance, dest)
8786 
8787         # NOTE(luyao): We only update instance info after rollback operations
8788         # are complete
8789         instance.task_state = None
8790         instance.progress = 0
8791         instance.drop_migration_context()
8792         instance.save(expected_task_state=[task_states.MIGRATING])
8793 
8794         self._notify_about_instance_usage(context, instance,
8795                                           "live_migration._rollback.end")
8796         compute_utils.notify_about_instance_action(context, instance,
8797                 self.host,
8798                 action=fields.NotificationAction.LIVE_MIGRATION_ROLLBACK,
8799                 phase=fields.NotificationPhase.END,
8800                 bdms=bdms)
8801 
8802         # TODO(luyao): set migration status to 'failed' but not 'error'
8803         # which means rollback_live_migration is done, we have successfully
8804         # cleaned up and returned instance back to normal status.
8805         self._set_migration_status(migration, migration_status)
8806 
8807     @wrap_exception()
8808     @wrap_instance_fault
8809     def drop_move_claim_at_destination(self, context, instance):
8810         """Called by the source of a live migration during rollback to ask the
8811         destination to drop the MoveClaim object that was created for the live
8812         migration on the destination.
8813         """
8814         nodename = self._get_nodename(instance)
8815         LOG.debug('Dropping live migration resource claim on destination '
8816                   'node %s', nodename, instance=instance)
8817         self.rt.drop_move_claim(
8818             context, instance, nodename, instance_type=instance.flavor)
8819 
8820     @wrap_exception()
8821     @wrap_instance_event(prefix='compute')
8822     @wrap_instance_fault
8823     def rollback_live_migration_at_destination(self, context, instance,
8824                                                destroy_disks,
8825                                                migrate_data):
8826         """Cleaning up image directory that is created pre_live_migration.
8827 
8828         :param context: security context
8829         :param instance: a nova.objects.instance.Instance object sent over rpc
8830         :param destroy_disks: whether to destroy volumes or not
8831         :param migrate_data: contains migration info
8832         """
8833         network_info = self.network_api.get_instance_nw_info(context, instance)
8834         self._notify_about_instance_usage(
8835                       context, instance, "live_migration.rollback.dest.start",
8836                       network_info=network_info)
8837         compute_utils.notify_about_instance_action(
8838             context, instance, self.host,
8839             action=fields.NotificationAction.LIVE_MIGRATION_ROLLBACK_DEST,
8840             phase=fields.NotificationPhase.START)
8841         try:
8842             # NOTE(tr3buchet): tear down networks on dest host (nova-net)
8843             # NOTE(mriedem): For neutron, this call will delete any
8844             # destination host port bindings.
8845             # TODO(mriedem): We should eventually remove this call from
8846             # this method (rollback_live_migration_at_destination) since this
8847             # method is only called conditionally based on whether or not the
8848             # instance is running on shared storage. _rollback_live_migration
8849             # already calls this method for neutron if we are running on
8850             # shared storage.
8851             self.network_api.setup_networks_on_host(context, instance,
8852                                                     self.host, teardown=True)
8853         except exception.PortBindingDeletionFailed as e:
8854             # Removing the inactive port bindings from the destination
8855             # host is not critical so just log an error but don't fail.
8856             LOG.error(
8857                 'Network cleanup failed for destination host %s '
8858                 'during live migration rollback. You may need to '
8859                 'manually clean up resources in the network service. '
8860                 'Error: %s', self.host, six.text_type(e))
8861         except Exception:
8862             with excutils.save_and_reraise_exception():
8863                 # NOTE(tdurakov): even if teardown networks fails driver
8864                 # should try to rollback live migration on destination.
8865                 LOG.exception('An error occurred while deallocating network.',
8866                               instance=instance)
8867         finally:
8868             # always run this even if setup_networks_on_host fails
8869             # NOTE(vish): The mapping is passed in so the driver can disconnect
8870             #             from remote volumes if necessary
8871             block_device_info = self._get_instance_block_device_info(context,
8872                                                                      instance)
8873             # free any instance PCI claims done on destination during
8874             # check_can_live_migrate_destination()
8875             self.rt.free_pci_device_claims_for_instance(context, instance)
8876 
8877             # NOTE(luyao): Apply migration_context temporarily since it's
8878             # on destination host, we rely on instance object to cleanup
8879             # specific resources like vpmem
8880             with instance.mutated_migration_context():
8881                 self.driver.rollback_live_migration_at_destination(
8882                     context, instance, network_info, block_device_info,
8883                     destroy_disks=destroy_disks, migrate_data=migrate_data)
8884 
8885         self._notify_about_instance_usage(
8886                         context, instance, "live_migration.rollback.dest.end",
8887                         network_info=network_info)
8888         compute_utils.notify_about_instance_action(
8889             context, instance, self.host,
8890             action=fields.NotificationAction.LIVE_MIGRATION_ROLLBACK_DEST,
8891             phase=fields.NotificationPhase.END)
8892 
8893     def _require_nw_info_update(self, context, instance):
8894         """Detect whether there is a mismatch in binding:host_id, or
8895         binding_failed or unbound binding:vif_type for any of the instances
8896         ports.
8897         """
8898         # Only update port bindings if compute manager does manage port
8899         # bindings instead of the compute driver. For example IronicDriver
8900         # manages the port binding for baremetal instance ports, hence,
8901         # external intervention with the binding is not desired.
8902         if self.driver.manages_network_binding_host_id():
8903             return False
8904 
8905         search_opts = {'device_id': instance.uuid,
8906                        'fields': ['binding:host_id', 'binding:vif_type']}
8907         ports = self.network_api.list_ports(context, **search_opts)
8908         for p in ports['ports']:
8909             if p.get('binding:host_id') != self.host:
8910                 return True
8911             vif_type = p.get('binding:vif_type')
8912             if (vif_type == network_model.VIF_TYPE_UNBOUND or
8913                     vif_type == network_model.VIF_TYPE_BINDING_FAILED):
8914                 return True
8915         return False
8916 
8917     @periodic_task.periodic_task(
8918         spacing=CONF.heal_instance_info_cache_interval)
8919     def _heal_instance_info_cache(self, context):
8920         """Called periodically.  On every call, try to update the
8921         info_cache's network information for another instance by
8922         calling to the network manager.
8923 
8924         This is implemented by keeping a cache of uuids of instances
8925         that live on this host.  On each call, we pop one off of a
8926         list, pull the DB record, and try the call to the network API.
8927         If anything errors don't fail, as it's possible the instance
8928         has been deleted, etc.
8929         """
8930         heal_interval = CONF.heal_instance_info_cache_interval
8931         if not heal_interval:
8932             return
8933 
8934         instance_uuids = getattr(self, '_instance_uuids_to_heal', [])
8935         instance = None
8936 
8937         LOG.debug('Starting heal instance info cache')
8938 
8939         if not instance_uuids:
8940             # The list of instances to heal is empty so rebuild it
8941             LOG.debug('Rebuilding the list of instances to heal')
8942             db_instances = objects.InstanceList.get_by_host(
8943                 context, self.host, expected_attrs=[], use_slave=True)
8944             for inst in db_instances:
8945                 # We don't want to refresh the cache for instances
8946                 # which are building or deleting so don't put them
8947                 # in the list. If they are building they will get
8948                 # added to the list next time we build it.
8949                 if (inst.vm_state == vm_states.BUILDING):
8950                     LOG.debug('Skipping network cache update for instance '
8951                               'because it is Building.', instance=inst)
8952                     continue
8953                 if (inst.task_state == task_states.DELETING):
8954                     LOG.debug('Skipping network cache update for instance '
8955                               'because it is being deleted.', instance=inst)
8956                     continue
8957 
8958                 if not instance:
8959                     # Save the first one we find so we don't
8960                     # have to get it again
8961                     instance = inst
8962                 else:
8963                     instance_uuids.append(inst['uuid'])
8964 
8965             self._instance_uuids_to_heal = instance_uuids
8966         else:
8967             # Find the next valid instance on the list
8968             while instance_uuids:
8969                 try:
8970                     inst = objects.Instance.get_by_uuid(
8971                             context, instance_uuids.pop(0),
8972                             expected_attrs=['system_metadata', 'info_cache',
8973                                             'flavor'],
8974                             use_slave=True)
8975                 except exception.InstanceNotFound:
8976                     # Instance is gone.  Try to grab another.
8977                     continue
8978 
8979                 # Check the instance hasn't been migrated
8980                 if inst.host != self.host:
8981                     LOG.debug('Skipping network cache update for instance '
8982                               'because it has been migrated to another '
8983                               'host.', instance=inst)
8984                 # Check the instance isn't being deleting
8985                 elif inst.task_state == task_states.DELETING:
8986                     LOG.debug('Skipping network cache update for instance '
8987                               'because it is being deleted.', instance=inst)
8988                 else:
8989                     instance = inst
8990                     break
8991 
8992         if instance:
8993             # We have an instance now to refresh
8994             try:
8995                 # Fix potential mismatch in port binding if evacuation failed
8996                 # after reassigning the port binding to the dest host but
8997                 # before the instance host is changed.
8998                 # Do this only when instance has no pending task.
8999                 if instance.task_state is None and \
9000                         self._require_nw_info_update(context, instance):
9001                     LOG.info("Updating ports in neutron", instance=instance)
9002                     self.network_api.setup_instance_network_on_host(
9003                         context, instance, self.host)
9004                 # Call to network API to get instance info.. this will
9005                 # force an update to the instance's info_cache
9006                 self.network_api.get_instance_nw_info(
9007                     context, instance, force_refresh=True)
9008                 LOG.debug('Updated the network info_cache for instance',
9009                           instance=instance)
9010             except exception.InstanceNotFound:
9011                 # Instance is gone.
9012                 LOG.debug('Instance no longer exists. Unable to refresh',
9013                           instance=instance)
9014                 return
9015             except exception.InstanceInfoCacheNotFound:
9016                 # InstanceInfoCache is gone.
9017                 LOG.debug('InstanceInfoCache no longer exists. '
9018                           'Unable to refresh', instance=instance)
9019             except Exception:
9020                 LOG.error('An error occurred while refreshing the network '
9021                           'cache.', instance=instance, exc_info=True)
9022         else:
9023             LOG.debug("Didn't find any instances for network info cache "
9024                       "update.")
9025 
9026     @periodic_task.periodic_task
9027     def _poll_rebooting_instances(self, context):
9028         if CONF.reboot_timeout > 0:
9029             filters = {'task_state':
9030                        [task_states.REBOOTING,
9031                         task_states.REBOOT_STARTED,
9032                         task_states.REBOOT_PENDING],
9033                        'host': self.host}
9034             rebooting = objects.InstanceList.get_by_filters(
9035                 context, filters, expected_attrs=[], use_slave=True)
9036 
9037             to_poll = []
9038             for instance in rebooting:
9039                 if timeutils.is_older_than(instance.updated_at,
9040                                            CONF.reboot_timeout):
9041                     to_poll.append(instance)
9042 
9043             self.driver.poll_rebooting_instances(CONF.reboot_timeout, to_poll)
9044 
9045     @periodic_task.periodic_task
9046     def _poll_rescued_instances(self, context):
9047         if CONF.rescue_timeout > 0:
9048             filters = {'vm_state': vm_states.RESCUED,
9049                        'host': self.host}
9050             rescued_instances = objects.InstanceList.get_by_filters(
9051                 context, filters, expected_attrs=["system_metadata"],
9052                 use_slave=True)
9053 
9054             to_unrescue = []
9055             for instance in rescued_instances:
9056                 if timeutils.is_older_than(instance.launched_at,
9057                                            CONF.rescue_timeout):
9058                     to_unrescue.append(instance)
9059 
9060             for instance in to_unrescue:
9061                 self.compute_api.unrescue(context, instance)
9062 
9063     @periodic_task.periodic_task
9064     def _poll_unconfirmed_resizes(self, context):
9065         if CONF.resize_confirm_window == 0:
9066             return
9067 
9068         migrations = objects.MigrationList.get_unconfirmed_by_dest_compute(
9069                 context, CONF.resize_confirm_window, self.host,
9070                 use_slave=True)
9071 
9072         migrations_info = dict(migration_count=len(migrations),
9073                 confirm_window=CONF.resize_confirm_window)
9074 
9075         if migrations_info["migration_count"] > 0:
9076             LOG.info("Found %(migration_count)d unconfirmed migrations "
9077                      "older than %(confirm_window)d seconds",
9078                      migrations_info)
9079 
9080         def _set_migration_to_error(migration, reason, **kwargs):
9081             LOG.warning("Setting migration %(migration_id)s to error: "
9082                         "%(reason)s",
9083                         {'migration_id': migration['id'], 'reason': reason},
9084                         **kwargs)
9085             migration.status = 'error'
9086             migration.save()
9087 
9088         for migration in migrations:
9089             instance_uuid = migration.instance_uuid
9090             LOG.info("Automatically confirming migration "
9091                      "%(migration_id)s for instance %(instance_uuid)s",
9092                      {'migration_id': migration.id,
9093                       'instance_uuid': instance_uuid})
9094             expected_attrs = ['metadata', 'system_metadata']
9095             try:
9096                 instance = objects.Instance.get_by_uuid(context,
9097                             instance_uuid, expected_attrs=expected_attrs,
9098                             use_slave=True)
9099             except exception.InstanceNotFound:
9100                 reason = (_("Instance %s not found") %
9101                           instance_uuid)
9102                 _set_migration_to_error(migration, reason)
9103                 continue
9104             if instance.vm_state == vm_states.ERROR:
9105                 reason = _("In ERROR state")
9106                 _set_migration_to_error(migration, reason,
9107                                         instance=instance)
9108                 continue
9109             # race condition: The instance in DELETING state should not be
9110             # set the migration state to error, otherwise the instance in
9111             # to be deleted which is in RESIZED state
9112             # will not be able to confirm resize
9113             if instance.task_state in [task_states.DELETING,
9114                                        task_states.SOFT_DELETING]:
9115                 msg = ("Instance being deleted or soft deleted during resize "
9116                        "confirmation. Skipping.")
9117                 LOG.debug(msg, instance=instance)
9118                 continue
9119 
9120             # race condition: This condition is hit when this method is
9121             # called between the save of the migration record with a status of
9122             # finished and the save of the instance object with a state of
9123             # RESIZED. The migration record should not be set to error.
9124             if instance.task_state == task_states.RESIZE_FINISH:
9125                 msg = ("Instance still resizing during resize "
9126                        "confirmation. Skipping.")
9127                 LOG.debug(msg, instance=instance)
9128                 continue
9129 
9130             vm_state = instance.vm_state
9131             task_state = instance.task_state
9132             if vm_state != vm_states.RESIZED or task_state is not None:
9133                 reason = (_("In states %(vm_state)s/%(task_state)s, not "
9134                            "RESIZED/None") %
9135                           {'vm_state': vm_state,
9136                            'task_state': task_state})
9137                 _set_migration_to_error(migration, reason,
9138                                         instance=instance)
9139                 continue
9140             try:
9141                 self.compute_api.confirm_resize(context, instance,
9142                                                 migration=migration)
9143             except Exception as e:
9144                 LOG.info("Error auto-confirming resize: %s. "
9145                          "Will retry later.", e, instance=instance)
9146 
9147     @periodic_task.periodic_task(spacing=CONF.shelved_poll_interval)
9148     def _poll_shelved_instances(self, context):
9149 
9150         if CONF.shelved_offload_time <= 0:
9151             return
9152 
9153         filters = {'vm_state': vm_states.SHELVED,
9154                    'task_state': None,
9155                    'host': self.host}
9156         shelved_instances = objects.InstanceList.get_by_filters(
9157             context, filters=filters, expected_attrs=['system_metadata'],
9158             use_slave=True)
9159 
9160         to_gc = []
9161         for instance in shelved_instances:
9162             sys_meta = instance.system_metadata
9163             shelved_at = timeutils.parse_strtime(sys_meta['shelved_at'])
9164             if timeutils.is_older_than(shelved_at, CONF.shelved_offload_time):
9165                 to_gc.append(instance)
9166 
9167         for instance in to_gc:
9168             try:
9169                 instance.task_state = task_states.SHELVING_OFFLOADING
9170                 instance.save(expected_task_state=(None,))
9171                 self.shelve_offload_instance(context, instance,
9172                                              clean_shutdown=False)
9173             except Exception:
9174                 LOG.exception('Periodic task failed to offload instance.',
9175                               instance=instance)
9176 
9177     @periodic_task.periodic_task
9178     def _instance_usage_audit(self, context):
9179         if not CONF.instance_usage_audit:
9180             return
9181 
9182         begin, end = utils.last_completed_audit_period()
9183         if objects.TaskLog.get(context, 'instance_usage_audit', begin, end,
9184                                self.host):
9185             return
9186 
9187         instances = objects.InstanceList.get_active_by_window_joined(
9188             context, begin, end, host=self.host,
9189             expected_attrs=['system_metadata', 'info_cache', 'metadata',
9190                             'flavor'],
9191             use_slave=True)
9192         num_instances = len(instances)
9193         errors = 0
9194         successes = 0
9195         LOG.info("Running instance usage audit for host %(host)s "
9196                  "from %(begin_time)s to %(end_time)s. "
9197                  "%(number_instances)s instances.",
9198                  {'host': self.host,
9199                   'begin_time': begin,
9200                   'end_time': end,
9201                   'number_instances': num_instances})
9202         start_time = time.time()
9203         task_log = objects.TaskLog(context)
9204         task_log.task_name = 'instance_usage_audit'
9205         task_log.period_beginning = begin
9206         task_log.period_ending = end
9207         task_log.host = self.host
9208         task_log.task_items = num_instances
9209         task_log.message = 'Instance usage audit started...'
9210         task_log.begin_task()
9211         for instance in instances:
9212             try:
9213                 compute_utils.notify_usage_exists(
9214                     self.notifier, context, instance, self.host,
9215                     ignore_missing_network_data=False)
9216                 successes += 1
9217             except Exception:
9218                 LOG.exception('Failed to generate usage '
9219                               'audit for instance '
9220                               'on host %s', self.host,
9221                               instance=instance)
9222                 errors += 1
9223         task_log.errors = errors
9224         task_log.message = (
9225             'Instance usage audit ran for host %s, %s instances in %s seconds.'
9226             % (self.host, num_instances, time.time() - start_time))
9227         task_log.end_task()
9228 
9229     @periodic_task.periodic_task(spacing=CONF.bandwidth_poll_interval)
9230     def _poll_bandwidth_usage(self, context):
9231 
9232         if not self._bw_usage_supported:
9233             return
9234 
9235         prev_time, start_time = utils.last_completed_audit_period()
9236 
9237         curr_time = time.time()
9238         if (curr_time - self._last_bw_usage_poll >
9239                 CONF.bandwidth_poll_interval):
9240             self._last_bw_usage_poll = curr_time
9241             LOG.info("Updating bandwidth usage cache")
9242 
9243             instances = objects.InstanceList.get_by_host(context,
9244                                                               self.host,
9245                                                               use_slave=True)
9246             try:
9247                 bw_counters = self.driver.get_all_bw_counters(instances)
9248             except NotImplementedError:
9249                 # NOTE(mdragon): Not all hypervisors have bandwidth polling
9250                 # implemented yet.  If they don't it doesn't break anything,
9251                 # they just don't get the info in the usage events.
9252                 # NOTE(PhilDay): Record that its not supported so we can
9253                 # skip fast on future calls rather than waste effort getting
9254                 # the list of instances.
9255                 LOG.info("Bandwidth usage not supported by %(driver)s.",
9256                          {'driver': CONF.compute_driver})
9257                 self._bw_usage_supported = False
9258                 return
9259 
9260             refreshed = timeutils.utcnow()
9261             for bw_ctr in bw_counters:
9262                 # Allow switching of greenthreads between queries.
9263                 greenthread.sleep(0)
9264                 bw_in = 0
9265                 bw_out = 0
9266                 last_ctr_in = None
9267                 last_ctr_out = None
9268                 usage = objects.BandwidthUsage.get_by_instance_uuid_and_mac(
9269                     context, bw_ctr['uuid'], bw_ctr['mac_address'],
9270                     start_period=start_time, use_slave=True)
9271                 if usage:
9272                     bw_in = usage.bw_in
9273                     bw_out = usage.bw_out
9274                     last_ctr_in = usage.last_ctr_in
9275                     last_ctr_out = usage.last_ctr_out
9276                 else:
9277                     usage = (objects.BandwidthUsage.
9278                              get_by_instance_uuid_and_mac(
9279                         context, bw_ctr['uuid'], bw_ctr['mac_address'],
9280                         start_period=prev_time, use_slave=True))
9281                     if usage:
9282                         last_ctr_in = usage.last_ctr_in
9283                         last_ctr_out = usage.last_ctr_out
9284 
9285                 if last_ctr_in is not None:
9286                     if bw_ctr['bw_in'] < last_ctr_in:
9287                         # counter rollover
9288                         bw_in += bw_ctr['bw_in']
9289                     else:
9290                         bw_in += (bw_ctr['bw_in'] - last_ctr_in)
9291 
9292                 if last_ctr_out is not None:
9293                     if bw_ctr['bw_out'] < last_ctr_out:
9294                         # counter rollover
9295                         bw_out += bw_ctr['bw_out']
9296                     else:
9297                         bw_out += (bw_ctr['bw_out'] - last_ctr_out)
9298 
9299                 objects.BandwidthUsage(context=context).create(
9300                                               bw_ctr['uuid'],
9301                                               bw_ctr['mac_address'],
9302                                               bw_in,
9303                                               bw_out,
9304                                               bw_ctr['bw_in'],
9305                                               bw_ctr['bw_out'],
9306                                               start_period=start_time,
9307                                               last_refreshed=refreshed)
9308 
9309     def _get_host_volume_bdms(self, context, use_slave=False):
9310         """Return all block device mappings on a compute host."""
9311         compute_host_bdms = []
9312         instances = objects.InstanceList.get_by_host(context, self.host,
9313             use_slave=use_slave)
9314         for instance in instances:
9315             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
9316                     context, instance.uuid, use_slave=use_slave)
9317             instance_bdms = [bdm for bdm in bdms if bdm.is_volume]
9318             compute_host_bdms.append(dict(instance=instance,
9319                                           instance_bdms=instance_bdms))
9320 
9321         return compute_host_bdms
9322 
9323     def _update_volume_usage_cache(self, context, vol_usages):
9324         """Updates the volume usage cache table with a list of stats."""
9325         for usage in vol_usages:
9326             # Allow switching of greenthreads between queries.
9327             greenthread.sleep(0)
9328             vol_usage = objects.VolumeUsage(context)
9329             vol_usage.volume_id = usage['volume']
9330             vol_usage.instance_uuid = usage['instance'].uuid
9331             vol_usage.project_id = usage['instance'].project_id
9332             vol_usage.user_id = usage['instance'].user_id
9333             vol_usage.availability_zone = usage['instance'].availability_zone
9334             vol_usage.curr_reads = usage['rd_req']
9335             vol_usage.curr_read_bytes = usage['rd_bytes']
9336             vol_usage.curr_writes = usage['wr_req']
9337             vol_usage.curr_write_bytes = usage['wr_bytes']
9338             vol_usage.save()
9339             self.notifier.info(context, 'volume.usage', vol_usage.to_dict())
9340             compute_utils.notify_about_volume_usage(context, vol_usage,
9341                                                     self.host)
9342 
9343     @periodic_task.periodic_task(spacing=CONF.volume_usage_poll_interval)
9344     def _poll_volume_usage(self, context):
9345         if CONF.volume_usage_poll_interval == 0:
9346             return
9347 
9348         compute_host_bdms = self._get_host_volume_bdms(context,
9349                                                        use_slave=True)
9350         if not compute_host_bdms:
9351             return
9352 
9353         LOG.debug("Updating volume usage cache")
9354         try:
9355             vol_usages = self.driver.get_all_volume_usage(context,
9356                                                           compute_host_bdms)
9357         except NotImplementedError:
9358             return
9359 
9360         self._update_volume_usage_cache(context, vol_usages)
9361 
9362     @periodic_task.periodic_task(spacing=CONF.sync_power_state_interval,
9363                                  run_immediately=True)
9364     def _sync_power_states(self, context):
9365         """Align power states between the database and the hypervisor.
9366 
9367         To sync power state data we make a DB call to get the number of
9368         virtual machines known by the hypervisor and if the number matches the
9369         number of virtual machines known by the database, we proceed in a lazy
9370         loop, one database record at a time, checking if the hypervisor has the
9371         same power state as is in the database.
9372         """
9373         db_instances = objects.InstanceList.get_by_host(context, self.host,
9374                                                         expected_attrs=[],
9375                                                         use_slave=True)
9376 
9377         try:
9378             num_vm_instances = self.driver.get_num_instances()
9379         except exception.VirtDriverNotReady as e:
9380             # If the virt driver is not ready, like ironic-api not being up
9381             # yet in the case of ironic, just log it and exit.
9382             LOG.info('Skipping _sync_power_states periodic task due to: %s', e)
9383             return
9384 
9385         num_db_instances = len(db_instances)
9386 
9387         if num_vm_instances != num_db_instances:
9388             LOG.warning("While synchronizing instance power states, found "
9389                         "%(num_db_instances)s instances in the database "
9390                         "and %(num_vm_instances)s instances on the "
9391                         "hypervisor.",
9392                         {'num_db_instances': num_db_instances,
9393                          'num_vm_instances': num_vm_instances})
9394 
9395         def _sync(db_instance):
9396             # NOTE(melwitt): This must be synchronized as we query state from
9397             #                two separate sources, the driver and the database.
9398             #                They are set (in stop_instance) and read, in sync.
9399             @utils.synchronized(db_instance.uuid)
9400             def query_driver_power_state_and_sync():
9401                 self._query_driver_power_state_and_sync(context, db_instance)
9402 
9403             try:
9404                 query_driver_power_state_and_sync()
9405             except Exception:
9406                 LOG.exception("Periodic sync_power_state task had an "
9407                               "error while processing an instance.",
9408                               instance=db_instance)
9409 
9410             self._syncs_in_progress.pop(db_instance.uuid)
9411 
9412         for db_instance in db_instances:
9413             # process syncs asynchronously - don't want instance locking to
9414             # block entire periodic task thread
9415             uuid = db_instance.uuid
9416             if uuid in self._syncs_in_progress:
9417                 LOG.debug('Sync already in progress for %s', uuid)
9418             else:
9419                 LOG.debug('Triggering sync for uuid %s', uuid)
9420                 self._syncs_in_progress[uuid] = True
9421                 self._sync_power_pool.spawn_n(_sync, db_instance)
9422 
9423     def _query_driver_power_state_and_sync(self, context, db_instance):
9424         if db_instance.task_state is not None:
9425             LOG.info("During sync_power_state the instance has a "
9426                      "pending task (%(task)s). Skip.",
9427                      {'task': db_instance.task_state}, instance=db_instance)
9428             return
9429         # No pending tasks. Now try to figure out the real vm_power_state.
9430         try:
9431             vm_instance = self.driver.get_info(db_instance)
9432             vm_power_state = vm_instance.state
9433         except exception.InstanceNotFound:
9434             vm_power_state = power_state.NOSTATE
9435         # Note(maoy): the above get_info call might take a long time,
9436         # for example, because of a broken libvirt driver.
9437         try:
9438             self._sync_instance_power_state(context,
9439                                             db_instance,
9440                                             vm_power_state,
9441                                             use_slave=True)
9442         except exception.InstanceNotFound:
9443             # NOTE(hanlind): If the instance gets deleted during sync,
9444             # silently ignore.
9445             pass
9446 
9447     def _stop_unexpected_shutdown_instance(self, context, vm_state,
9448                                            db_instance, orig_db_power_state):
9449         # this is an exceptional case; make sure our data is up
9450         # to date before slamming through a power off
9451         vm_instance = self.driver.get_info(db_instance,
9452                                            use_cache=False)
9453         vm_power_state = vm_instance.state
9454 
9455         # if it still looks off, go ahead and call stop()
9456         if vm_power_state in (power_state.SHUTDOWN,
9457                               power_state.CRASHED):
9458 
9459             LOG.warning("Instance shutdown by itself. Calling the "
9460                         "stop API. Current vm_state: %(vm_state)s, "
9461                         "current task_state: %(task_state)s, "
9462                         "original DB power_state: %(db_power_state)s, "
9463                         "current VM power_state: %(vm_power_state)s",
9464                         {'vm_state': vm_state,
9465                          'task_state': db_instance.task_state,
9466                          'db_power_state': orig_db_power_state,
9467                          'vm_power_state': vm_power_state},
9468                         instance=db_instance)
9469             try:
9470                 # Note(maoy): here we call the API instead of
9471                 # brutally updating the vm_state in the database
9472                 # to allow all the hooks and checks to be performed.
9473                 if db_instance.shutdown_terminate:
9474                     self.compute_api.delete(context, db_instance)
9475                 else:
9476                     self.compute_api.stop(context, db_instance)
9477             except Exception:
9478                 # Note(maoy): there is no need to propagate the error
9479                 # because the same power_state will be retrieved next
9480                 # time and retried.
9481                 # For example, there might be another task scheduled.
9482                 LOG.exception("error during stop() in sync_power_state.",
9483                               instance=db_instance)
9484 
9485     def _sync_instance_power_state(self, context, db_instance, vm_power_state,
9486                                    use_slave=False):
9487         """Align instance power state between the database and hypervisor.
9488 
9489         If the instance is not found on the hypervisor, but is in the database,
9490         then a stop() API will be called on the instance.
9491         """
9492 
9493         # We re-query the DB to get the latest instance info to minimize
9494         # (not eliminate) race condition.
9495         db_instance.refresh(use_slave=use_slave)
9496         db_power_state = db_instance.power_state
9497         vm_state = db_instance.vm_state
9498 
9499         if self.host != db_instance.host:
9500             # on the sending end of nova-compute _sync_power_state
9501             # may have yielded to the greenthread performing a live
9502             # migration; this in turn has changed the resident-host
9503             # for the VM; However, the instance is still active, it
9504             # is just in the process of migrating to another host.
9505             # This implies that the compute source must relinquish
9506             # control to the compute destination.
9507             LOG.info("During the sync_power process the "
9508                      "instance has moved from "
9509                      "host %(src)s to host %(dst)s",
9510                      {'src': db_instance.host,
9511                       'dst': self.host},
9512                      instance=db_instance)
9513             return
9514         elif db_instance.task_state is not None:
9515             # on the receiving end of nova-compute, it could happen
9516             # that the DB instance already report the new resident
9517             # but the actual VM has not showed up on the hypervisor
9518             # yet. In this case, let's allow the loop to continue
9519             # and run the state sync in a later round
9520             LOG.info("During sync_power_state the instance has a "
9521                      "pending task (%(task)s). Skip.",
9522                      {'task': db_instance.task_state},
9523                      instance=db_instance)
9524             return
9525 
9526         orig_db_power_state = db_power_state
9527         if vm_power_state != db_power_state:
9528             LOG.info('During _sync_instance_power_state the DB '
9529                      'power_state (%(db_power_state)s) does not match '
9530                      'the vm_power_state from the hypervisor '
9531                      '(%(vm_power_state)s). Updating power_state in the '
9532                      'DB to match the hypervisor.',
9533                      {'db_power_state': db_power_state,
9534                       'vm_power_state': vm_power_state},
9535                      instance=db_instance)
9536             # power_state is always updated from hypervisor to db
9537             db_instance.power_state = vm_power_state
9538             db_instance.save()
9539             db_power_state = vm_power_state
9540 
9541         # Note(maoy): Now resolve the discrepancy between vm_state and
9542         # vm_power_state. We go through all possible vm_states.
9543         if vm_state in (vm_states.BUILDING,
9544                         vm_states.RESCUED,
9545                         vm_states.RESIZED,
9546                         vm_states.SUSPENDED,
9547                         vm_states.ERROR):
9548             # TODO(maoy): we ignore these vm_state for now.
9549             pass
9550         elif vm_state == vm_states.ACTIVE:
9551             # The only rational power state should be RUNNING
9552             if vm_power_state in (power_state.SHUTDOWN,
9553                                   power_state.CRASHED):
9554                 self._stop_unexpected_shutdown_instance(
9555                     context, vm_state, db_instance, orig_db_power_state)
9556             elif vm_power_state == power_state.SUSPENDED:
9557                 LOG.warning("Instance is suspended unexpectedly. Calling "
9558                             "the stop API.", instance=db_instance)
9559                 try:
9560                     self.compute_api.stop(context, db_instance)
9561                 except Exception:
9562                     LOG.exception("error during stop() in sync_power_state.",
9563                                   instance=db_instance)
9564             elif vm_power_state == power_state.PAUSED:
9565                 # Note(maoy): a VM may get into the paused state not only
9566                 # because the user request via API calls, but also
9567                 # due to (temporary) external instrumentations.
9568                 # Before the virt layer can reliably report the reason,
9569                 # we simply ignore the state discrepancy. In many cases,
9570                 # the VM state will go back to running after the external
9571                 # instrumentation is done. See bug 1097806 for details.
9572                 LOG.warning("Instance is paused unexpectedly. Ignore.",
9573                             instance=db_instance)
9574             elif vm_power_state == power_state.NOSTATE:
9575                 # Occasionally, depending on the status of the hypervisor,
9576                 # which could be restarting for example, an instance may
9577                 # not be found.  Therefore just log the condition.
9578                 LOG.warning("Instance is unexpectedly not found. Ignore.",
9579                             instance=db_instance)
9580         elif vm_state == vm_states.STOPPED:
9581             if vm_power_state not in (power_state.NOSTATE,
9582                                       power_state.SHUTDOWN,
9583                                       power_state.CRASHED):
9584                 LOG.warning("Instance is not stopped. Calling "
9585                             "the stop API. Current vm_state: %(vm_state)s,"
9586                             " current task_state: %(task_state)s, "
9587                             "original DB power_state: %(db_power_state)s, "
9588                             "current VM power_state: %(vm_power_state)s",
9589                             {'vm_state': vm_state,
9590                              'task_state': db_instance.task_state,
9591                              'db_power_state': orig_db_power_state,
9592                              'vm_power_state': vm_power_state},
9593                             instance=db_instance)
9594                 try:
9595                     # NOTE(russellb) Force the stop, because normally the
9596                     # compute API would not allow an attempt to stop a stopped
9597                     # instance.
9598                     self.compute_api.force_stop(context, db_instance)
9599                 except Exception:
9600                     LOG.exception("error during stop() in sync_power_state.",
9601                                   instance=db_instance)
9602         elif vm_state == vm_states.PAUSED:
9603             if vm_power_state in (power_state.SHUTDOWN,
9604                                   power_state.CRASHED):
9605                 LOG.warning("Paused instance shutdown by itself. Calling "
9606                             "the stop API.", instance=db_instance)
9607                 try:
9608                     self.compute_api.force_stop(context, db_instance)
9609                 except Exception:
9610                     LOG.exception("error during stop() in sync_power_state.",
9611                                   instance=db_instance)
9612         elif vm_state in (vm_states.SOFT_DELETED,
9613                           vm_states.DELETED):
9614             if vm_power_state not in (power_state.NOSTATE,
9615                                       power_state.SHUTDOWN):
9616                 # Note(maoy): this should be taken care of periodically in
9617                 # _cleanup_running_deleted_instances().
9618                 LOG.warning("Instance is not (soft-)deleted.",
9619                             instance=db_instance)
9620 
9621     @periodic_task.periodic_task
9622     def _reclaim_queued_deletes(self, context):
9623         """Reclaim instances that are queued for deletion."""
9624         interval = CONF.reclaim_instance_interval
9625         if interval <= 0:
9626             LOG.debug("CONF.reclaim_instance_interval <= 0, skipping...")
9627             return
9628 
9629         filters = {'vm_state': vm_states.SOFT_DELETED,
9630                    'task_state': None,
9631                    'host': self.host}
9632         instances = objects.InstanceList.get_by_filters(
9633             context, filters,
9634             expected_attrs=objects.instance.INSTANCE_DEFAULT_FIELDS,
9635             use_slave=True)
9636         for instance in instances:
9637             if self._deleted_old_enough(instance, interval):
9638                 bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
9639                         context, instance.uuid)
9640                 LOG.info('Reclaiming deleted instance', instance=instance)
9641                 try:
9642                     self._delete_instance(context, instance, bdms)
9643                 except Exception as e:
9644                     LOG.warning("Periodic reclaim failed to delete "
9645                                 "instance: %s",
9646                                 e, instance=instance)
9647 
9648     def _get_nodename(self, instance, refresh=False):
9649         """Helper method to get the name of the first available node
9650         on this host. This method should not be used with any operations
9651         on ironic instances since it does not handle multiple nodes.
9652         """
9653         node = self.driver.get_available_nodes(refresh=refresh)[0]
9654         LOG.debug("No node specified, defaulting to %s", node,
9655                   instance=instance)
9656         return node
9657 
9658     def _update_available_resource_for_node(self, context, nodename,
9659                                             startup=False):
9660 
9661         try:
9662             self.rt.update_available_resource(context, nodename,
9663                                               startup=startup)
9664         except exception.ComputeHostNotFound:
9665             LOG.warning("Compute node '%s' not found in "
9666                         "update_available_resource.", nodename)
9667         except exception.ReshapeFailed:
9668             # We're only supposed to get here on startup, if a reshape was
9669             # needed, was attempted, and failed. We want to kill the service.
9670             with excutils.save_and_reraise_exception():
9671                 LOG.critical("Resource provider data migration failed "
9672                              "fatally during startup for node %s.", nodename)
9673         except exception.ReshapeNeeded:
9674             # This exception should only find its way here if the virt driver's
9675             # update_provider_tree raised it incorrectly: either
9676             # a) After the resource tracker already caught it once and
9677             # reinvoked update_provider_tree with allocations. At this point
9678             # the driver is just supposed to *do* the reshape, so if it raises
9679             # ReshapeNeeded, it's a bug, and we want to kill the compute
9680             # service.
9681             # b) On periodic rather than startup (we only allow reshapes to
9682             # happen on startup). In this case we'll just make the logs red and
9683             # go again at the next periodic interval, where the same thing may
9684             # or may not happen again. Depending on the previous and intended
9685             # shape of the providers/inventories, this may not actually cause
9686             # any immediately visible symptoms (in terms of scheduling, etc.)
9687             # If this becomes a problem, we may wish to make it pop immediately
9688             # (e.g. disable the service).
9689             with excutils.save_and_reraise_exception():
9690                 LOG.exception("ReshapeNeeded exception is unexpected here!")
9691         except Exception:
9692             LOG.exception("Error updating resources for node %(node)s.",
9693                           {'node': nodename})
9694 
9695     @periodic_task.periodic_task(spacing=CONF.update_resources_interval)
9696     def update_available_resource(self, context, startup=False):
9697         """See driver.get_available_resource()
9698 
9699         Periodic process that keeps that the compute host's understanding of
9700         resource availability and usage in sync with the underlying hypervisor.
9701 
9702         :param context: security context
9703         :param startup: True if this is being called when the nova-compute
9704             service is starting, False otherwise.
9705         """
9706         try:
9707             nodenames = set(self.driver.get_available_nodes())
9708         except exception.VirtDriverNotReady:
9709             LOG.warning("Virt driver is not ready.")
9710             return
9711 
9712         compute_nodes_in_db = self._get_compute_nodes_in_db(context,
9713                                                             nodenames,
9714                                                             use_slave=True,
9715                                                             startup=startup)
9716 
9717         # Delete orphan compute node not reported by driver but still in db
9718         for cn in compute_nodes_in_db:
9719             if cn.hypervisor_hostname not in nodenames:
9720                 LOG.info("Deleting orphan compute node %(id)s "
9721                          "hypervisor host is %(hh)s, "
9722                          "nodes are %(nodes)s",
9723                          {'id': cn.id, 'hh': cn.hypervisor_hostname,
9724                           'nodes': nodenames})
9725                 cn.destroy()
9726                 self.rt.remove_node(cn.hypervisor_hostname)
9727                 # Delete the corresponding resource provider in placement,
9728                 # along with any associated allocations.
9729                 try:
9730                     self.reportclient.delete_resource_provider(context, cn,
9731                                                                cascade=True)
9732                 except keystone_exception.ClientException as e:
9733                     LOG.error(
9734                         "Failed to delete compute node resource provider "
9735                         "for compute node %s: %s", cn.uuid, six.text_type(e))
9736 
9737         for nodename in nodenames:
9738             self._update_available_resource_for_node(context, nodename,
9739                                                      startup=startup)
9740 
9741     def _get_compute_nodes_in_db(self, context, nodenames, use_slave=False,
9742                                  startup=False):
9743         try:
9744             return objects.ComputeNodeList.get_all_by_host(context, self.host,
9745                                                            use_slave=use_slave)
9746         except exception.NotFound:
9747             # If the driver is not reporting any nodenames we should not
9748             # expect there to be compute nodes so we just return in that case.
9749             # For example, this could be an ironic compute and it is not
9750             # managing any nodes yet.
9751             if nodenames:
9752                 if startup:
9753                     LOG.warning(
9754                         "No compute node record found for host %s. If this is "
9755                         "the first time this service is starting on this "
9756                         "host, then you can ignore this warning.", self.host)
9757                 else:
9758                     LOG.error("No compute node record for host %s", self.host)
9759             return []
9760 
9761     @periodic_task.periodic_task(
9762         spacing=CONF.running_deleted_instance_poll_interval,
9763         run_immediately=True)
9764     def _cleanup_running_deleted_instances(self, context):
9765         """Cleanup any instances which are erroneously still running after
9766         having been deleted.
9767 
9768         Valid actions to take are:
9769 
9770             1. noop - do nothing
9771             2. log - log which instances are erroneously running
9772             3. reap - shutdown and cleanup any erroneously running instances
9773             4. shutdown - power off *and disable* any erroneously running
9774                           instances
9775 
9776         The use-case for this cleanup task is: for various reasons, it may be
9777         possible for the database to show an instance as deleted but for that
9778         instance to still be running on a host machine (see bug
9779         https://bugs.launchpad.net/nova/+bug/911366).
9780 
9781         This cleanup task is a cross-hypervisor utility for finding these
9782         zombied instances and either logging the discrepancy (likely what you
9783         should do in production), or automatically reaping the instances (more
9784         appropriate for dev environments).
9785         """
9786         action = CONF.running_deleted_instance_action
9787 
9788         if action == "noop":
9789             return
9790 
9791         # NOTE(sirp): admin contexts don't ordinarily return deleted records
9792         with utils.temporary_mutation(context, read_deleted="yes"):
9793 
9794             try:
9795                 instances = self._running_deleted_instances(context)
9796             except exception.VirtDriverNotReady:
9797                 # Since this task runs immediately on startup, if the
9798                 # hypervisor is not yet ready handle it gracefully.
9799                 LOG.debug('Unable to check for running deleted instances '
9800                           'at this time since the hypervisor is not ready.')
9801                 return
9802 
9803             for instance in instances:
9804                 if action == "log":
9805                     LOG.warning("Detected instance with name label "
9806                                 "'%s' which is marked as "
9807                                 "DELETED but still present on host.",
9808                                 instance.name, instance=instance)
9809 
9810                 elif action == 'shutdown':
9811                     LOG.info("Powering off instance with name label "
9812                              "'%s' which is marked as "
9813                              "DELETED but still present on host.",
9814                              instance.name, instance=instance)
9815                     try:
9816                         try:
9817                             # disable starting the instance
9818                             self.driver.set_bootable(instance, False)
9819                         except NotImplementedError:
9820                             LOG.debug("set_bootable is not implemented "
9821                                       "for the current driver")
9822                         # and power it off
9823                         self.driver.power_off(instance)
9824                     except Exception:
9825                         LOG.warning("Failed to power off instance",
9826                                     instance=instance, exc_info=True)
9827 
9828                 elif action == 'reap':
9829                     LOG.info("Destroying instance with name label "
9830                              "'%s' which is marked as "
9831                              "DELETED but still present on host.",
9832                              instance.name, instance=instance)
9833                     bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
9834                         context, instance.uuid, use_slave=True)
9835                     self.instance_events.clear_events_for_instance(instance)
9836                     try:
9837                         self._shutdown_instance(context, instance, bdms,
9838                                                 notify=False)
9839                         self._cleanup_volumes(context, instance, bdms,
9840                                               detach=False)
9841                     except Exception as e:
9842                         LOG.warning("Periodic cleanup failed to delete "
9843                                     "instance: %s",
9844                                     e, instance=instance)
9845                 else:
9846                     raise Exception(_("Unrecognized value '%s'"
9847                                       " for CONF.running_deleted_"
9848                                       "instance_action") % action)
9849 
9850     def _running_deleted_instances(self, context):
9851         """Returns a list of instances nova thinks is deleted,
9852         but the hypervisor thinks is still running.
9853         """
9854         timeout = CONF.running_deleted_instance_timeout
9855         filters = {'deleted': True,
9856                    'soft_deleted': False}
9857         instances = self._get_instances_on_driver(context, filters)
9858         return [i for i in instances if self._deleted_old_enough(i, timeout)]
9859 
9860     def _deleted_old_enough(self, instance, timeout):
9861         deleted_at = instance.deleted_at
9862         if deleted_at:
9863             deleted_at = deleted_at.replace(tzinfo=None)
9864         return (not deleted_at or timeutils.is_older_than(deleted_at, timeout))
9865 
9866     @contextlib.contextmanager
9867     def _error_out_instance_on_exception(self, context, instance,
9868                                          instance_state=vm_states.ACTIVE):
9869         """Context manager to set instance.vm_state after some operation raises
9870 
9871         Used to handle NotImplementedError and InstanceFaultRollback errors
9872         and reset the instance vm_state and task_state. The vm_state is set
9873         to the $instance_state parameter and task_state is set to None.
9874         For all other types of exceptions, the vm_state is set to ERROR and
9875         the task_state is left unchanged (although most callers will have the
9876         @reverts_task_state decorator which will set the task_state to None).
9877 
9878         Re-raises the original exception *except* in the case of
9879         InstanceFaultRollback in which case the wrapped `inner_exception` is
9880         re-raised.
9881 
9882         :param context: The nova auth request context for the operation.
9883         :param instance: The instance to update. The vm_state will be set by
9884             this context manager when an exception is raised.
9885         :param instance_state: For NotImplementedError and
9886             InstanceFaultRollback this is the vm_state to set the instance to
9887             when handling one of those types of exceptions. By default the
9888             instance will be set to ACTIVE, but the caller should control this
9889             in case there have been no changes to the running state of the
9890             instance. For example, resizing a stopped server where prep_resize
9891             fails early and does not change the power state of the guest should
9892             not set the instance status to ACTIVE but remain STOPPED.
9893             This parameter is ignored for all other types of exceptions and the
9894             instance vm_state is set to ERROR.
9895         """
9896         # NOTE(mriedem): Why doesn't this method just save off the
9897         # original instance.vm_state here rather than use a parameter? Or use
9898         # instance_state=None as an override but default to the current
9899         # vm_state when rolling back.
9900         instance_uuid = instance.uuid
9901         try:
9902             yield
9903         except (NotImplementedError, exception.InstanceFaultRollback) as error:
9904             # Use reraise=False to determine if we want to raise the original
9905             # exception or something else.
9906             with excutils.save_and_reraise_exception(reraise=False) as ctxt:
9907                 LOG.info("Setting instance back to %(state)s after: %(error)s",
9908                          {'state': instance_state, 'error': error},
9909                          instance_uuid=instance_uuid)
9910                 self._instance_update(context, instance,
9911                                       vm_state=instance_state,
9912                                       task_state=None)
9913                 if isinstance(error, exception.InstanceFaultRollback):
9914                     # Raise the wrapped exception.
9915                     raise error.inner_exception
9916                 # Else re-raise the NotImplementedError.
9917                 ctxt.reraise = True
9918         except Exception:
9919             LOG.exception('Setting instance vm_state to ERROR',
9920                           instance_uuid=instance_uuid)
9921             with excutils.save_and_reraise_exception():
9922                 # NOTE(mriedem): Why don't we pass clean_task_state=True here?
9923                 self._set_instance_obj_error_state(instance)
9924 
9925     @wrap_exception()
9926     def add_aggregate_host(self, context, aggregate, host, slave_info):
9927         """Notify hypervisor of change (for hypervisor pools)."""
9928         try:
9929             self.driver.add_to_aggregate(context, aggregate, host,
9930                                          slave_info=slave_info)
9931         except NotImplementedError:
9932             LOG.debug('Hypervisor driver does not support '
9933                       'add_aggregate_host')
9934         except exception.AggregateError:
9935             with excutils.save_and_reraise_exception():
9936                 self.driver.undo_aggregate_operation(
9937                                     context,
9938                                     aggregate.delete_host,
9939                                     aggregate, host)
9940 
9941     @wrap_exception()
9942     def remove_aggregate_host(self, context, host, slave_info, aggregate):
9943         """Removes a host from a physical hypervisor pool."""
9944         try:
9945             self.driver.remove_from_aggregate(context, aggregate, host,
9946                                               slave_info=slave_info)
9947         except NotImplementedError:
9948             LOG.debug('Hypervisor driver does not support '
9949                       'remove_aggregate_host')
9950         except (exception.AggregateError,
9951                 exception.InvalidAggregateAction) as e:
9952             with excutils.save_and_reraise_exception():
9953                 self.driver.undo_aggregate_operation(
9954                                     context,
9955                                     aggregate.add_host,
9956                                     aggregate, host,
9957                                     isinstance(e, exception.AggregateError))
9958 
9959     def _process_instance_event(self, instance, event):
9960         _event = self.instance_events.pop_instance_event(instance, event)
9961         if _event:
9962             LOG.debug('Processing event %(event)s',
9963                       {'event': event.key}, instance=instance)
9964             _event.send(event)
9965         else:
9966             # If it's a network-vif-unplugged event and the instance is being
9967             # deleted or live migrated then we don't need to make this a
9968             # warning as it's expected. There are other expected things which
9969             # could trigger this event like detaching an interface, but we
9970             # don't have a task state for that.
9971             # TODO(mriedem): We have other move operations and things like
9972             # hard reboot (probably rebuild as well) which trigger this event
9973             # but nothing listens for network-vif-unplugged. We should either
9974             # handle those other known cases or consider just not logging a
9975             # warning if we get this event and the instance is undergoing some
9976             # task state transition.
9977             if (event.name == 'network-vif-unplugged' and
9978                     instance.task_state in (
9979                         task_states.DELETING, task_states.MIGRATING)):
9980                 LOG.debug('Received event %s for instance with task_state %s.',
9981                           event.key, instance.task_state, instance=instance)
9982             else:
9983                 LOG.warning('Received unexpected event %(event)s for '
9984                             'instance with vm_state %(vm_state)s and '
9985                             'task_state %(task_state)s.',
9986                             {'event': event.key,
9987                              'vm_state': instance.vm_state,
9988                              'task_state': instance.task_state},
9989                             instance=instance)
9990 
9991     def _process_instance_vif_deleted_event(self, context, instance,
9992                                             deleted_vif_id):
9993         # If an attached port is deleted by neutron, it needs to
9994         # be detached from the instance.
9995         # And info cache needs to be updated.
9996         network_info = instance.info_cache.network_info
9997         for index, vif in enumerate(network_info):
9998             if vif['id'] == deleted_vif_id:
9999                 LOG.info('Neutron deleted interface %(intf)s; '
10000                          'detaching it from the instance and '
10001                          'deleting it from the info cache',
10002                          {'intf': vif['id']},
10003                          instance=instance)
10004                 profile = vif.get('profile', {}) or {}  # profile can be None
10005                 if profile.get('allocation'):
10006                     LOG.error(
10007                         'The bound port %(port_id)s is deleted in Neutron but '
10008                         'the resource allocation on the resource provider '
10009                         '%(rp_uuid)s is leaked until the server '
10010                         '%(server_uuid)s is deleted.',
10011                         {'port_id': vif['id'],
10012                          'rp_uuid': vif['profile']['allocation'],
10013                          'server_uuid': instance.uuid})
10014 
10015                 del network_info[index]
10016                 neutron.update_instance_cache_with_nw_info(
10017                     self.network_api, context, instance, nw_info=network_info)
10018                 try:
10019                     self.driver.detach_interface(context, instance, vif)
10020                 except NotImplementedError:
10021                     # Not all virt drivers support attach/detach of interfaces
10022                     # yet (like Ironic), so just ignore this.
10023                     pass
10024                 except exception.NovaException as ex:
10025                     # If the instance was deleted before the interface was
10026                     # detached, just log it at debug.
10027                     log_level = (logging.DEBUG
10028                                  if isinstance(ex, exception.InstanceNotFound)
10029                                  else logging.WARNING)
10030                     LOG.log(log_level,
10031                             "Detach interface failed, "
10032                             "port_id=%(port_id)s, reason: %(msg)s",
10033                             {'port_id': deleted_vif_id, 'msg': ex},
10034                             instance=instance)
10035                 break
10036 
10037     @wrap_instance_event(prefix='compute')
10038     @wrap_instance_fault
10039     def extend_volume(self, context, instance, extended_volume_id):
10040 
10041         # If an attached volume is extended by cinder, it needs to
10042         # be extended by virt driver so host can detect its new size.
10043         # And bdm needs to be updated.
10044         LOG.debug('Handling volume-extended event for volume %(vol)s',
10045                   {'vol': extended_volume_id}, instance=instance)
10046 
10047         try:
10048             bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
10049                    context, extended_volume_id, instance.uuid)
10050         except exception.NotFound:
10051             LOG.warning('Extend volume failed, '
10052                         'volume %(vol)s is not attached to instance.',
10053                         {'vol': extended_volume_id},
10054                         instance=instance)
10055             return
10056 
10057         LOG.info('Cinder extended volume %(vol)s; '
10058                  'extending it to detect new size',
10059                  {'vol': extended_volume_id},
10060                  instance=instance)
10061         volume = self.volume_api.get(context, bdm.volume_id)
10062 
10063         if bdm.connection_info is None:
10064             LOG.warning('Extend volume failed, '
10065                         'attached volume %(vol)s has no connection_info',
10066                         {'vol': extended_volume_id},
10067                         instance=instance)
10068             return
10069 
10070         connection_info = jsonutils.loads(bdm.connection_info)
10071         bdm.volume_size = volume['size']
10072         bdm.save()
10073 
10074         if not self.driver.capabilities.get('supports_extend_volume', False):
10075             raise exception.ExtendVolumeNotSupported()
10076 
10077         try:
10078             self.driver.extend_volume(context, connection_info, instance,
10079                                       bdm.volume_size * units.Gi)
10080         except Exception as ex:
10081             LOG.warning('Extend volume failed, '
10082                         'volume_id=%(volume_id)s, reason: %(msg)s',
10083                         {'volume_id': extended_volume_id, 'msg': ex},
10084                         instance=instance)
10085             raise
10086 
10087     @staticmethod
10088     def _is_state_valid_for_power_update_event(instance, target_power_state):
10089         """Check if the current state of the instance allows it to be
10090         a candidate for the power-update event.
10091 
10092         :param instance: The nova instance object.
10093         :param target_power_state: The desired target power state; this should
10094                                    either be "POWER_ON" or "POWER_OFF".
10095         :returns Boolean: True if the instance can be subjected to the
10096                           power-update event.
10097         """
10098         if ((target_power_state == external_event_obj.POWER_ON and
10099                 instance.task_state is None and
10100                 instance.vm_state == vm_states.STOPPED and
10101                 instance.power_state == power_state.SHUTDOWN) or
10102             (target_power_state == external_event_obj.POWER_OFF and
10103                 instance.task_state is None and
10104                 instance.vm_state == vm_states.ACTIVE and
10105                 instance.power_state == power_state.RUNNING)):
10106             return True
10107         return False
10108 
10109     @wrap_exception()
10110     @reverts_task_state
10111     @wrap_instance_event(prefix='compute')
10112     @wrap_instance_fault
10113     def power_update(self, context, instance, target_power_state):
10114         """Power update of an instance prompted by an external event.
10115         :param context: The API request context.
10116         :param instance: The nova instance object.
10117         :param target_power_state: The desired target power state;
10118                                    this should either be "POWER_ON" or
10119                                    "POWER_OFF".
10120         """
10121 
10122         @utils.synchronized(instance.uuid)
10123         def do_power_update():
10124             LOG.debug('Handling power-update event with target_power_state %s '
10125                       'for instance', target_power_state, instance=instance)
10126             if not self._is_state_valid_for_power_update_event(
10127                     instance, target_power_state):
10128                 pow_state = fields.InstancePowerState.from_index(
10129                     instance.power_state)
10130                 LOG.info('The power-update %(tag)s event for instance '
10131                          '%(uuid)s is a no-op since the instance is in '
10132                          'vm_state %(vm_state)s, task_state '
10133                          '%(task_state)s and power_state '
10134                          '%(power_state)s.',
10135                          {'tag': target_power_state, 'uuid': instance.uuid,
10136                          'vm_state': instance.vm_state,
10137                          'task_state': instance.task_state,
10138                          'power_state': pow_state})
10139                 return
10140             LOG.debug("Trying to %s instance",
10141                       target_power_state, instance=instance)
10142             if target_power_state == external_event_obj.POWER_ON:
10143                 action = fields.NotificationAction.POWER_ON
10144                 notification_name = "power_on."
10145                 instance.task_state = task_states.POWERING_ON
10146             else:
10147                 # It's POWER_OFF
10148                 action = fields.NotificationAction.POWER_OFF
10149                 notification_name = "power_off."
10150                 instance.task_state = task_states.POWERING_OFF
10151                 instance.progress = 0
10152 
10153             try:
10154                 # Note that the task_state is set here rather than the API
10155                 # because this is a best effort operation and deferring
10156                 # updating the task_state until we get to the compute service
10157                 # avoids error handling in the API and needing to account for
10158                 # older compute services during rolling upgrades from Stein.
10159                 # If we lose a race, UnexpectedTaskStateError is handled
10160                 # below.
10161                 instance.save(expected_task_state=[None])
10162                 self._notify_about_instance_usage(context, instance,
10163                                                   notification_name + "start")
10164                 compute_utils.notify_about_instance_action(context, instance,
10165                     self.host, action=action,
10166                     phase=fields.NotificationPhase.START)
10167                 # UnexpectedTaskStateError raised from the driver will be
10168                 # handled below and not result in a fault, error notification
10169                 # or failure of the instance action. Other driver errors like
10170                 # NotImplementedError will be record a fault, send an error
10171                 # notification and mark the instance action as failed.
10172                 self.driver.power_update_event(instance, target_power_state)
10173                 self._notify_about_instance_usage(context, instance,
10174                                                   notification_name + "end")
10175                 compute_utils.notify_about_instance_action(context, instance,
10176                     self.host, action=action,
10177                     phase=fields.NotificationPhase.END)
10178             except exception.UnexpectedTaskStateError as e:
10179                 # Handling the power-update event is best effort and if we lost
10180                 # a race with some other action happening to the instance we
10181                 # just log it and return rather than fail the action.
10182                 LOG.info("The power-update event was possibly preempted: %s ",
10183                          e.format_message(), instance=instance)
10184                 return
10185         do_power_update()
10186 
10187     @wrap_exception()
10188     def external_instance_event(self, context, instances, events):
10189         # NOTE(danms): Some event types are handled by the manager, such
10190         # as when we're asked to update the instance's info_cache. If it's
10191         # not one of those, look for some thread(s) waiting for the event and
10192         # unblock them if so.
10193         for event in events:
10194             instance = [inst for inst in instances
10195                         if inst.uuid == event.instance_uuid][0]
10196             LOG.debug('Received event %(event)s',
10197                       {'event': event.key},
10198                       instance=instance)
10199             if event.name == 'network-changed':
10200                 try:
10201                     LOG.debug('Refreshing instance network info cache due to '
10202                               'event %s.', event.key, instance=instance)
10203                     self.network_api.get_instance_nw_info(
10204                         context, instance, refresh_vif_id=event.tag)
10205                 except exception.NotFound as e:
10206                     LOG.info('Failed to process external instance event '
10207                              '%(event)s due to: %(error)s',
10208                              {'event': event.key, 'error': six.text_type(e)},
10209                              instance=instance)
10210             elif event.name == 'network-vif-deleted':
10211                 try:
10212                     self._process_instance_vif_deleted_event(context,
10213                                                              instance,
10214                                                              event.tag)
10215                 except exception.NotFound as e:
10216                     LOG.info('Failed to process external instance event '
10217                              '%(event)s due to: %(error)s',
10218                              {'event': event.key, 'error': six.text_type(e)},
10219                              instance=instance)
10220             elif event.name == 'volume-extended':
10221                 self.extend_volume(context, instance, event.tag)
10222             elif event.name == 'power-update':
10223                 self.power_update(context, instance, event.tag)
10224             else:
10225                 self._process_instance_event(instance, event)
10226 
10227     @periodic_task.periodic_task(spacing=CONF.image_cache.manager_interval,
10228                                  external_process_ok=True)
10229     def _run_image_cache_manager_pass(self, context):
10230         """Run a single pass of the image cache manager."""
10231 
10232         if not self.driver.capabilities.get("has_imagecache", False):
10233             return
10234 
10235         # Determine what other nodes use this storage
10236         storage_users.register_storage_use(CONF.instances_path, CONF.host)
10237         nodes = storage_users.get_storage_users(CONF.instances_path)
10238 
10239         # Filter all_instances to only include those nodes which share this
10240         # storage path.
10241         # TODO(mikal): this should be further refactored so that the cache
10242         # cleanup code doesn't know what those instances are, just a remote
10243         # count, and then this logic should be pushed up the stack.
10244         filters = {'deleted': False,
10245                    'soft_deleted': True,
10246                    'host': nodes}
10247         filtered_instances = objects.InstanceList.get_by_filters(context,
10248                                  filters, expected_attrs=[], use_slave=True)
10249 
10250         self.driver.manage_image_cache(context, filtered_instances)
10251 
10252     def cache_images(self, context, image_ids):
10253         """Ask the virt driver to pre-cache a set of base images.
10254 
10255         :param context: The RequestContext
10256         :param image_ids: The image IDs to be cached
10257         :return: A dict, keyed by image-id where the values are one of:
10258                  'cached' if the image was downloaded,
10259                  'existing' if the image was already in the cache,
10260                  'unsupported' if the virt driver does not support caching,
10261                  'error' if the virt driver raised an exception.
10262         """
10263 
10264         results = {}
10265 
10266         LOG.info('Caching %i image(s) by request', len(image_ids))
10267         for image_id in image_ids:
10268             try:
10269                 cached = self.driver.cache_image(context, image_id)
10270                 if cached:
10271                     results[image_id] = 'cached'
10272                 else:
10273                     results[image_id] = 'existing'
10274             except NotImplementedError:
10275                 LOG.warning('Virt driver does not support image pre-caching;'
10276                             ' ignoring request')
10277                 # NOTE(danms): Yes, technically we could short-circuit here to
10278                 # avoid trying the rest of the images, but it's very cheap to
10279                 # just keep hitting the NotImplementedError to keep the logic
10280                 # clean.
10281                 results[image_id] = 'unsupported'
10282             except Exception as e:
10283                 results[image_id] = 'error'
10284                 LOG.error('Failed to cache image %(image_id)s: %(err)s',
10285                           {'image_id': image_id,
10286                            'err': e})
10287 
10288         return results
10289 
10290     @periodic_task.periodic_task(spacing=CONF.instance_delete_interval)
10291     def _run_pending_deletes(self, context):
10292         """Retry any pending instance file deletes."""
10293         LOG.debug('Cleaning up deleted instances')
10294         filters = {'deleted': True,
10295                    'soft_deleted': False,
10296                    'host': CONF.host,
10297                    'cleaned': False}
10298         attrs = ['system_metadata']
10299         with utils.temporary_mutation(context, read_deleted='yes'):
10300             instances = objects.InstanceList.get_by_filters(
10301                 context, filters, expected_attrs=attrs, use_slave=True)
10302         LOG.debug('There are %d instances to clean', len(instances))
10303 
10304         for instance in instances:
10305             attempts = int(instance.system_metadata.get('clean_attempts', '0'))
10306             LOG.debug('Instance has had %(attempts)s of %(max)s '
10307                       'cleanup attempts',
10308                       {'attempts': attempts,
10309                        'max': CONF.maximum_instance_delete_attempts},
10310                       instance=instance)
10311             if attempts < CONF.maximum_instance_delete_attempts:
10312                 success = self.driver.delete_instance_files(instance)
10313 
10314                 instance.system_metadata['clean_attempts'] = str(attempts + 1)
10315                 if success:
10316                     instance.cleaned = True
10317                 with utils.temporary_mutation(context, read_deleted='yes'):
10318                     instance.save()
10319 
10320     @periodic_task.periodic_task(spacing=CONF.instance_delete_interval)
10321     def _cleanup_incomplete_migrations(self, context):
10322         """Delete instance files on failed resize/revert-resize operation
10323 
10324         During resize/revert-resize operation, if that instance gets deleted
10325         in-between then instance files might remain either on source or
10326         destination compute node because of race condition.
10327         """
10328         LOG.debug('Cleaning up deleted instances with incomplete migration ')
10329         migration_filters = {'host': CONF.host,
10330                              'status': 'error'}
10331         migrations = objects.MigrationList.get_by_filters(context,
10332                                                           migration_filters)
10333 
10334         if not migrations:
10335             return
10336 
10337         inst_uuid_from_migrations = set([migration.instance_uuid for migration
10338                                          in migrations])
10339 
10340         inst_filters = {'deleted': True, 'soft_deleted': False,
10341                         'uuid': inst_uuid_from_migrations}
10342         attrs = ['info_cache', 'security_groups', 'system_metadata']
10343         with utils.temporary_mutation(context, read_deleted='yes'):
10344             instances = objects.InstanceList.get_by_filters(
10345                 context, inst_filters, expected_attrs=attrs, use_slave=True)
10346 
10347         for instance in instances:
10348             if instance.host != CONF.host:
10349                 for migration in migrations:
10350                     if instance.uuid == migration.instance_uuid:
10351                         # Delete instance files if not cleanup properly either
10352                         # from the source or destination compute nodes when
10353                         # the instance is deleted during resizing.
10354                         self.driver.delete_instance_files(instance)
10355                         try:
10356                             migration.status = 'failed'
10357                             migration.save()
10358                         except exception.MigrationNotFound:
10359                             LOG.warning("Migration %s is not found.",
10360                                         migration.id,
10361                                         instance=instance)
10362                         break
10363 
10364     @messaging.expected_exceptions(exception.InstanceQuiesceNotSupported,
10365                                    exception.QemuGuestAgentNotEnabled,
10366                                    exception.NovaException,
10367                                    NotImplementedError)
10368     @wrap_exception()
10369     def quiesce_instance(self, context, instance):
10370         """Quiesce an instance on this host."""
10371         context = context.elevated()
10372         image_meta = objects.ImageMeta.from_instance(instance)
10373         self.driver.quiesce(context, instance, image_meta)
10374 
10375     def _wait_for_snapshots_completion(self, context, mapping):
10376         for mapping_dict in mapping:
10377             if mapping_dict.get('source_type') == 'snapshot':
10378 
10379                 def _wait_snapshot():
10380                     snapshot = self.volume_api.get_snapshot(
10381                         context, mapping_dict['snapshot_id'])
10382                     if snapshot.get('status') != 'creating':
10383                         raise loopingcall.LoopingCallDone()
10384 
10385                 timer = loopingcall.FixedIntervalLoopingCall(_wait_snapshot)
10386                 timer.start(interval=0.5).wait()
10387 
10388     @messaging.expected_exceptions(exception.InstanceQuiesceNotSupported,
10389                                    exception.QemuGuestAgentNotEnabled,
10390                                    exception.NovaException,
10391                                    NotImplementedError)
10392     @wrap_exception()
10393     def unquiesce_instance(self, context, instance, mapping=None):
10394         """Unquiesce an instance on this host.
10395 
10396         If snapshots' image mapping is provided, it waits until snapshots are
10397         completed before unqueiscing.
10398         """
10399         context = context.elevated()
10400         if mapping:
10401             try:
10402                 self._wait_for_snapshots_completion(context, mapping)
10403             except Exception as error:
10404                 LOG.exception("Exception while waiting completion of "
10405                               "volume snapshots: %s",
10406                               error, instance=instance)
10407         image_meta = objects.ImageMeta.from_instance(instance)
10408         self.driver.unquiesce(context, instance, image_meta)
10409 
10410     @periodic_task.periodic_task(spacing=CONF.instance_delete_interval)
10411     def _cleanup_expired_console_auth_tokens(self, context):
10412         """Remove all expired console auth tokens.
10413 
10414         Console authorization tokens and their connection data are stored
10415         in the database when a user asks for a console connection to an
10416         instance. After a time they expire. We periodically remove any expired
10417         tokens from the database.
10418         """
10419         objects.ConsoleAuthToken.clean_expired_console_auths(context)
10420 
10421     def _claim_pci_for_instance_vifs(self, ctxt, instance):
10422         """Claim PCI devices for the instance's VIFs on the compute node
10423 
10424         :param ctxt: Context
10425         :param instance: Instance object
10426         :return: <port ID: PciDevice> mapping for the VIFs that yielded a
10427                 PCI claim on the compute node
10428         """
10429         pci_req_id_to_port_id = {}
10430         pci_reqs = []
10431         port_id_to_pci_dev = {}
10432 
10433         for vif in instance.get_network_info():
10434             pci_req = pci_req_module.get_instance_pci_request_from_vif(
10435                 ctxt,
10436                 instance,
10437                 vif)
10438             if pci_req:
10439                 pci_req_id_to_port_id[pci_req.request_id] = vif['id']
10440                 pci_reqs.append(pci_req)
10441 
10442         if pci_reqs:
10443             # Create PCI requests and claim against PCI resource tracker
10444             # NOTE(adrianc): We claim against the same requests as on the
10445             # source node.
10446             vif_pci_requests = objects.InstancePCIRequests(
10447                 requests=pci_reqs,
10448                 instance_uuid=instance.uuid)
10449 
10450             claimed_pci_devices_objs = self.rt.claim_pci_devices(
10451                 ctxt,
10452                 vif_pci_requests)
10453 
10454             # Update VIFMigrateData profile with the newly claimed PCI
10455             # device
10456             for pci_dev in claimed_pci_devices_objs:
10457                 LOG.debug("PCI device: %s Claimed on destination node",
10458                           pci_dev.address)
10459                 port_id = pci_req_id_to_port_id[pci_dev.request_id]
10460                 port_id_to_pci_dev[port_id] = pci_dev
10461 
10462         return port_id_to_pci_dev
10463 
10464     def _update_migrate_vifs_profile_with_pci(self,
10465                                               migrate_vifs,
10466                                               port_id_to_pci_dev):
10467         """Update migrate vifs profile with the claimed PCI devices
10468 
10469         :param migrate_vifs: list of VIFMigrateData objects
10470         :param port_id_to_pci_dev: a <port_id: PciDevice> mapping
10471         :return: None.
10472         """
10473         for mig_vif in migrate_vifs:
10474             port_id = mig_vif.port_id
10475             if port_id not in port_id_to_pci_dev:
10476                 continue
10477 
10478             pci_dev = port_id_to_pci_dev[port_id]
10479             profile = copy.deepcopy(mig_vif.source_vif['profile'])
10480             profile['pci_slot'] = pci_dev.address
10481             profile['pci_vendor_info'] = ':'.join([pci_dev.vendor_id,
10482                                                    pci_dev.product_id])
10483             mig_vif.profile = profile
10484             LOG.debug("Updating migrate VIF profile for port %(port_id)s:"
10485                       "%(profile)s", {'port_id': port_id,
10486                                       'profile': profile})
