Based on the given code from a commit, please generate supplementary code files according to the commit message.
####commit message
Automatically revert resize which fails on destination

This change causes a resize/cold migration operation which fails on
the destination node to automatically start a revert operation.

Closes-Bug: #1686703
Change-Id: I26706a93ab938cebf2ffa3141bbe1b85a30938b5

####code 
1 # Copyright 2010 United States Government as represented by the
2 # Administrator of the National Aeronautics and Space Administration.
3 # Copyright 2011 Justin Santa Barbara
4 # All Rights Reserved.
5 #
6 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
7 #    not use this file except in compliance with the License. You may obtain
8 #    a copy of the License at
9 #
10 #         http://www.apache.org/licenses/LICENSE-2.0
11 #
12 #    Unless required by applicable law or agreed to in writing, software
13 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
14 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
15 #    License for the specific language governing permissions and limitations
16 #    under the License.
17 
18 """Handles all processes relating to instances (guest vms).
19 
20 The :py:class:`ComputeManager` class is a :py:class:`nova.manager.Manager` that
21 handles RPC calls relating to creating instances.  It is responsible for
22 building a disk image, launching it via the underlying virtualization driver,
23 responding to calls to check its state, attaching persistent storage, and
24 terminating it.
25 
26 """
27 
28 import base64
29 import binascii
30 import contextlib
31 import functools
32 import inspect
33 import sys
34 import time
35 import traceback
36 
37 from cinderclient import exceptions as cinder_exception
38 from cursive import exception as cursive_exception
39 import eventlet.event
40 from eventlet import greenthread
41 import eventlet.semaphore
42 import eventlet.timeout
43 from keystoneauth1 import exceptions as keystone_exception
44 from oslo_log import log as logging
45 import oslo_messaging as messaging
46 from oslo_serialization import jsonutils
47 from oslo_service import loopingcall
48 from oslo_service import periodic_task
49 from oslo_utils import excutils
50 from oslo_utils import strutils
51 from oslo_utils import timeutils
52 from oslo_utils import uuidutils
53 import six
54 from six.moves import range
55 
56 from nova import block_device
57 from nova.cells import rpcapi as cells_rpcapi
58 from nova import compute
59 from nova.compute import build_results
60 from nova.compute import claims
61 from nova.compute import power_state
62 from nova.compute import resource_tracker
63 from nova.compute import rpcapi as compute_rpcapi
64 from nova.compute import task_states
65 from nova.compute import utils as compute_utils
66 from nova.compute.utils import wrap_instance_event
67 from nova.compute import vm_states
68 from nova import conductor
69 import nova.conf
70 from nova.console import rpcapi as console_rpcapi
71 import nova.context
72 from nova import exception
73 from nova import exception_wrapper
74 from nova import hooks
75 from nova.i18n import _
76 from nova import image
77 from nova.image import glance
78 from nova import manager
79 from nova import network
80 from nova.network import base_api as base_net_api
81 from nova.network import model as network_model
82 from nova.network.security_group import openstack_driver
83 from nova import objects
84 from nova.objects import base as obj_base
85 from nova.objects import fields
86 from nova.objects import instance as obj_instance
87 from nova.objects import migrate_data as migrate_data_obj
88 from nova.pci import whitelist
89 from nova import rpc
90 from nova import safe_utils
91 from nova.scheduler import client as scheduler_client
92 from nova import utils
93 from nova.virt import block_device as driver_block_device
94 from nova.virt import configdrive
95 from nova.virt import driver
96 from nova.virt import event as virtevent
97 from nova.virt import storage_users
98 from nova.virt import virtapi
99 from nova.volume import cinder
100 
101 CONF = nova.conf.CONF
102 
103 LOG = logging.getLogger(__name__)
104 
105 get_notifier = functools.partial(rpc.get_notifier, service='compute')
106 wrap_exception = functools.partial(exception_wrapper.wrap_exception,
107                                    get_notifier=get_notifier,
108                                    binary='nova-compute')
109 
110 
111 @contextlib.contextmanager
112 def errors_out_migration_ctxt(migration):
113     """Context manager to error out migration on failure."""
114 
115     try:
116         yield
117     except Exception:
118         with excutils.save_and_reraise_exception():
119             migration.status = 'error'
120             try:
121                 with migration.obj_as_admin():
122                     migration.save()
123             except Exception:
124                 LOG.debug('Error setting migration status for instance %s.',
125                           migration.instance_uuid, exc_info=True)
126 
127 
128 @utils.expects_func_args('migration')
129 def errors_out_migration(function):
130     """Decorator to error out migration on failure."""
131 
132     @functools.wraps(function)
133     def decorated_function(self, context, *args, **kwargs):
134         wrapped_func = safe_utils.get_wrapped_function(function)
135         keyed_args = inspect.getcallargs(wrapped_func, self, context,
136                                          *args, **kwargs)
137         migration = keyed_args['migration']
138         with errors_out_migration_ctxt(migration):
139             return function(self, context, *args, **kwargs)
140 
141     return decorated_function
142 
143 
144 @utils.expects_func_args('instance')
145 def reverts_task_state(function):
146     """Decorator to revert task_state on failure."""
147 
148     @functools.wraps(function)
149     def decorated_function(self, context, *args, **kwargs):
150         try:
151             return function(self, context, *args, **kwargs)
152         except exception.UnexpectedTaskStateError as e:
153             # Note(maoy): unexpected task state means the current
154             # task is preempted. Do not clear task state in this
155             # case.
156             with excutils.save_and_reraise_exception():
157                 LOG.info("Task possibly preempted: %s",
158                          e.format_message())
159         except Exception:
160             with excutils.save_and_reraise_exception():
161                 wrapped_func = safe_utils.get_wrapped_function(function)
162                 keyed_args = inspect.getcallargs(wrapped_func, self, context,
163                                                  *args, **kwargs)
164                 # NOTE(mriedem): 'instance' must be in keyed_args because we
165                 # have utils.expects_func_args('instance') decorating this
166                 # method.
167                 instance = keyed_args['instance']
168                 original_task_state = instance.task_state
169                 try:
170                     self._instance_update(context, instance, task_state=None)
171                     LOG.info("Successfully reverted task state from %s on "
172                              "failure for instance.",
173                              original_task_state, instance=instance)
174                 except exception.InstanceNotFound:
175                     # We might delete an instance that failed to build shortly
176                     # after it errored out this is an expected case and we
177                     # should not trace on it.
178                     pass
179                 except Exception as e:
180                     LOG.warning("Failed to revert task state for instance. "
181                                 "Error: %s", e, instance=instance)
182 
183     return decorated_function
184 
185 
186 @utils.expects_func_args('instance')
187 def wrap_instance_fault(function):
188     """Wraps a method to catch exceptions related to instances.
189 
190     This decorator wraps a method to catch any exceptions having to do with
191     an instance that may get thrown. It then logs an instance fault in the db.
192     """
193 
194     @functools.wraps(function)
195     def decorated_function(self, context, *args, **kwargs):
196         try:
197             return function(self, context, *args, **kwargs)
198         except exception.InstanceNotFound:
199             raise
200         except Exception as e:
201             # NOTE(gtt): If argument 'instance' is in args rather than kwargs,
202             # we will get a KeyError exception which will cover up the real
203             # exception. So, we update kwargs with the values from args first.
204             # then, we can get 'instance' from kwargs easily.
205             kwargs.update(dict(zip(function.__code__.co_varnames[2:], args)))
206 
207             with excutils.save_and_reraise_exception():
208                 compute_utils.add_instance_fault_from_exc(context,
209                         kwargs['instance'], e, sys.exc_info())
210 
211     return decorated_function
212 
213 
214 @utils.expects_func_args('image_id', 'instance')
215 def delete_image_on_error(function):
216     """Used for snapshot related method to ensure the image created in
217     compute.api is deleted when an error occurs.
218     """
219 
220     @functools.wraps(function)
221     def decorated_function(self, context, image_id, instance,
222                            *args, **kwargs):
223         try:
224             return function(self, context, image_id, instance,
225                             *args, **kwargs)
226         except Exception:
227             with excutils.save_and_reraise_exception():
228                 LOG.debug("Cleaning up image %s", image_id,
229                           exc_info=True, instance=instance)
230                 try:
231                     self.image_api.delete(context, image_id)
232                 except exception.ImageNotFound:
233                     # Since we're trying to cleanup an image, we don't care if
234                     # if it's already gone.
235                     pass
236                 except Exception:
237                     LOG.exception("Error while trying to clean up image %s",
238                                   image_id, instance=instance)
239 
240     return decorated_function
241 
242 
243 # TODO(danms): Remove me after Icehouse
244 # TODO(alaski): Actually remove this after Newton, assuming a major RPC bump
245 # NOTE(mikal): if the method being decorated has more than one decorator, then
246 # put this one first. Otherwise the various exception handling decorators do
247 # not function correctly.
248 def object_compat(function):
249     """Wraps a method that expects a new-world instance
250 
251     This provides compatibility for callers passing old-style dict
252     instances.
253     """
254 
255     @functools.wraps(function)
256     def decorated_function(self, context, *args, **kwargs):
257         def _load_instance(instance_or_dict):
258             if isinstance(instance_or_dict, dict):
259                 # try to get metadata and system_metadata for most cases but
260                 # only attempt to load those if the db instance already has
261                 # those fields joined
262                 metas = [meta for meta in ('metadata', 'system_metadata')
263                          if meta in instance_or_dict]
264                 instance = objects.Instance._from_db_object(
265                     context, objects.Instance(), instance_or_dict,
266                     expected_attrs=metas)
267                 instance._context = context
268                 return instance
269             return instance_or_dict
270 
271         try:
272             kwargs['instance'] = _load_instance(kwargs['instance'])
273         except KeyError:
274             args = (_load_instance(args[0]),) + args[1:]
275 
276         migration = kwargs.get('migration')
277         if isinstance(migration, dict):
278             migration = objects.Migration._from_db_object(
279                     context.elevated(), objects.Migration(),
280                     migration)
281             kwargs['migration'] = migration
282 
283         return function(self, context, *args, **kwargs)
284 
285     return decorated_function
286 
287 
288 class InstanceEvents(object):
289     def __init__(self):
290         self._events = {}
291 
292     @staticmethod
293     def _lock_name(instance):
294         return '%s-%s' % (instance.uuid, 'events')
295 
296     def prepare_for_instance_event(self, instance, event_name):
297         """Prepare to receive an event for an instance.
298 
299         This will register an event for the given instance that we will
300         wait on later. This should be called before initiating whatever
301         action will trigger the event. The resulting eventlet.event.Event
302         object should be wait()'d on to ensure completion.
303 
304         :param instance: the instance for which the event will be generated
305         :param event_name: the name of the event we're expecting
306         :returns: an event object that should be wait()'d on
307         """
308         if self._events is None:
309             # NOTE(danms): We really should have a more specific error
310             # here, but this is what we use for our default error case
311             raise exception.NovaException('In shutdown, no new events '
312                                           'can be scheduled')
313 
314         @utils.synchronized(self._lock_name(instance))
315         def _create_or_get_event():
316             instance_events = self._events.setdefault(instance.uuid, {})
317             return instance_events.setdefault(event_name,
318                                               eventlet.event.Event())
319         LOG.debug('Preparing to wait for external event %(event)s',
320                   {'event': event_name}, instance=instance)
321         return _create_or_get_event()
322 
323     def pop_instance_event(self, instance, event):
324         """Remove a pending event from the wait list.
325 
326         This will remove a pending event from the wait list so that it
327         can be used to signal the waiters to wake up.
328 
329         :param instance: the instance for which the event was generated
330         :param event: the nova.objects.external_event.InstanceExternalEvent
331                       that describes the event
332         :returns: the eventlet.event.Event object on which the waiters
333                   are blocked
334         """
335         no_events_sentinel = object()
336         no_matching_event_sentinel = object()
337 
338         @utils.synchronized(self._lock_name(instance))
339         def _pop_event():
340             if not self._events:
341                 LOG.debug('Unexpected attempt to pop events during shutdown',
342                           instance=instance)
343                 return no_events_sentinel
344             events = self._events.get(instance.uuid)
345             if not events:
346                 return no_events_sentinel
347             _event = events.pop(event.key, None)
348             if not events:
349                 del self._events[instance.uuid]
350             if _event is None:
351                 return no_matching_event_sentinel
352             return _event
353 
354         result = _pop_event()
355         if result is no_events_sentinel:
356             LOG.debug('No waiting events found dispatching %(event)s',
357                       {'event': event.key},
358                       instance=instance)
359             return None
360         elif result is no_matching_event_sentinel:
361             LOG.debug('No event matching %(event)s in %(events)s',
362                       {'event': event.key,
363                        'events': self._events.get(instance.uuid, {}).keys()},
364                       instance=instance)
365             return None
366         else:
367             return result
368 
369     def clear_events_for_instance(self, instance):
370         """Remove all pending events for an instance.
371 
372         This will remove all events currently pending for an instance
373         and return them (indexed by event name).
374 
375         :param instance: the instance for which events should be purged
376         :returns: a dictionary of {event_name: eventlet.event.Event}
377         """
378         @utils.synchronized(self._lock_name(instance))
379         def _clear_events():
380             if self._events is None:
381                 LOG.debug('Unexpected attempt to clear events during shutdown',
382                           instance=instance)
383                 return dict()
384             return self._events.pop(instance.uuid, {})
385         return _clear_events()
386 
387     def cancel_all_events(self):
388         if self._events is None:
389             LOG.debug('Unexpected attempt to cancel events during shutdown.')
390             return
391         our_events = self._events
392         # NOTE(danms): Block new events
393         self._events = None
394 
395         for instance_uuid, events in our_events.items():
396             for event_name, eventlet_event in events.items():
397                 LOG.debug('Canceling in-flight event %(event)s for '
398                           'instance %(instance_uuid)s',
399                           {'event': event_name,
400                            'instance_uuid': instance_uuid})
401                 name, tag = event_name.rsplit('-', 1)
402                 event = objects.InstanceExternalEvent(
403                     instance_uuid=instance_uuid,
404                     name=name, status='failed',
405                     tag=tag, data={})
406                 eventlet_event.send(event)
407 
408 
409 class ComputeVirtAPI(virtapi.VirtAPI):
410     def __init__(self, compute):
411         super(ComputeVirtAPI, self).__init__()
412         self._compute = compute
413 
414     def _default_error_callback(self, event_name, instance):
415         raise exception.NovaException(_('Instance event failed'))
416 
417     @contextlib.contextmanager
418     def wait_for_instance_event(self, instance, event_names, deadline=300,
419                                 error_callback=None):
420         """Plan to wait for some events, run some code, then wait.
421 
422         This context manager will first create plans to wait for the
423         provided event_names, yield, and then wait for all the scheduled
424         events to complete.
425 
426         Note that this uses an eventlet.timeout.Timeout to bound the
427         operation, so callers should be prepared to catch that
428         failure and handle that situation appropriately.
429 
430         If the event is not received by the specified timeout deadline,
431         eventlet.timeout.Timeout is raised.
432 
433         If the event is received but did not have a 'completed'
434         status, a NovaException is raised.  If an error_callback is
435         provided, instead of raising an exception as detailed above
436         for the failure case, the callback will be called with the
437         event_name and instance, and can return True to continue
438         waiting for the rest of the events, False to stop processing,
439         or raise an exception which will bubble up to the waiter.
440 
441         :param instance: The instance for which an event is expected
442         :param event_names: A list of event names. Each element can be a
443                             string event name or tuple of strings to
444                             indicate (name, tag).
445         :param deadline: Maximum number of seconds we should wait for all
446                          of the specified events to arrive.
447         :param error_callback: A function to be called if an event arrives
448 
449         """
450 
451         if error_callback is None:
452             error_callback = self._default_error_callback
453         events = {}
454         for event_name in event_names:
455             if isinstance(event_name, tuple):
456                 name, tag = event_name
457                 event_name = objects.InstanceExternalEvent.make_key(
458                     name, tag)
459             try:
460                 events[event_name] = (
461                     self._compute.instance_events.prepare_for_instance_event(
462                         instance, event_name))
463             except exception.NovaException:
464                 error_callback(event_name, instance)
465                 # NOTE(danms): Don't wait for any of the events. They
466                 # should all be canceled and fired immediately below,
467                 # but don't stick around if not.
468                 deadline = 0
469         yield
470         with eventlet.timeout.Timeout(deadline):
471             for event_name, event in events.items():
472                 actual_event = event.wait()
473                 if actual_event.status == 'completed':
474                     continue
475                 decision = error_callback(event_name, instance)
476                 if decision is False:
477                     break
478 
479 
480 class ComputeManager(manager.Manager):
481     """Manages the running instances from creation to destruction."""
482 
483     target = messaging.Target(version='4.17')
484 
485     # How long to wait in seconds before re-issuing a shutdown
486     # signal to an instance during power off.  The overall
487     # time to wait is set by CONF.shutdown_timeout.
488     SHUTDOWN_RETRY_INTERVAL = 10
489 
490     def __init__(self, compute_driver=None, *args, **kwargs):
491         """Load configuration options and connect to the hypervisor."""
492         self.virtapi = ComputeVirtAPI(self)
493         self.network_api = network.API()
494         self.volume_api = cinder.API()
495         self.image_api = image.API()
496         self._last_host_check = 0
497         self._last_bw_usage_poll = 0
498         self._bw_usage_supported = True
499         self._last_bw_usage_cell_update = 0
500         self.compute_api = compute.API()
501         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
502         self.conductor_api = conductor.API()
503         self.compute_task_api = conductor.ComputeTaskAPI()
504         self.is_neutron_security_groups = (
505             openstack_driver.is_neutron_security_groups())
506         self.cells_rpcapi = cells_rpcapi.CellsAPI()
507         self.scheduler_client = scheduler_client.SchedulerClient()
508         self._resource_tracker = None
509         self.instance_events = InstanceEvents()
510         self._sync_power_pool = eventlet.GreenPool(
511             size=CONF.sync_power_state_pool_size)
512         self._syncs_in_progress = {}
513         self.send_instance_updates = (
514             CONF.filter_scheduler.track_instance_changes)
515         if CONF.max_concurrent_builds != 0:
516             self._build_semaphore = eventlet.semaphore.Semaphore(
517                 CONF.max_concurrent_builds)
518         else:
519             self._build_semaphore = compute_utils.UnlimitedSemaphore()
520         if max(CONF.max_concurrent_live_migrations, 0) != 0:
521             self._live_migration_semaphore = eventlet.semaphore.Semaphore(
522                 CONF.max_concurrent_live_migrations)
523         else:
524             self._live_migration_semaphore = compute_utils.UnlimitedSemaphore()
525         self._failed_builds = 0
526 
527         super(ComputeManager, self).__init__(service_name="compute",
528                                              *args, **kwargs)
529 
530         # NOTE(russellb) Load the driver last.  It may call back into the
531         # compute manager via the virtapi, so we want it to be fully
532         # initialized before that happens.
533         self.driver = driver.load_compute_driver(self.virtapi, compute_driver)
534         self.use_legacy_block_device_info = \
535                             self.driver.need_legacy_block_device_info
536 
537     def reset(self):
538         LOG.info('Reloading compute RPC API')
539         compute_rpcapi.LAST_VERSION = None
540         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
541 
542     def _get_resource_tracker(self):
543         if not self._resource_tracker:
544             rt = resource_tracker.ResourceTracker(self.host, self.driver)
545             self._resource_tracker = rt
546         return self._resource_tracker
547 
548     def _update_resource_tracker(self, context, instance):
549         """Let the resource tracker know that an instance has changed state."""
550 
551         if instance.host == self.host:
552             rt = self._get_resource_tracker()
553             rt.update_usage(context, instance, instance.node)
554 
555     def _instance_update(self, context, instance, **kwargs):
556         """Update an instance in the database using kwargs as value."""
557 
558         for k, v in kwargs.items():
559             setattr(instance, k, v)
560         instance.save()
561         self._update_resource_tracker(context, instance)
562 
563     def _nil_out_instance_obj_host_and_node(self, instance):
564         # NOTE(jwcroppe): We don't do instance.save() here for performance
565         # reasons; a call to this is expected to be immediately followed by
566         # another call that does instance.save(), thus avoiding two writes
567         # to the database layer.
568         instance.host = None
569         instance.node = None
570 
571     def _set_instance_obj_error_state(self, context, instance,
572                                       clean_task_state=False):
573         try:
574             instance.vm_state = vm_states.ERROR
575             if clean_task_state:
576                 instance.task_state = None
577             instance.save()
578         except exception.InstanceNotFound:
579             LOG.debug('Instance has been destroyed from under us while '
580                       'trying to set it to ERROR', instance=instance)
581 
582     def _get_instances_on_driver(self, context, filters=None):
583         """Return a list of instance records for the instances found
584         on the hypervisor which satisfy the specified filters. If filters=None
585         return a list of instance records for all the instances found on the
586         hypervisor.
587         """
588         if not filters:
589             filters = {}
590         try:
591             driver_uuids = self.driver.list_instance_uuids()
592             if len(driver_uuids) == 0:
593                 # Short circuit, don't waste a DB call
594                 return objects.InstanceList()
595             filters['uuid'] = driver_uuids
596             local_instances = objects.InstanceList.get_by_filters(
597                 context, filters, use_slave=True)
598             return local_instances
599         except NotImplementedError:
600             pass
601 
602         # The driver doesn't support uuids listing, so we'll have
603         # to brute force.
604         driver_instances = self.driver.list_instances()
605         instances = objects.InstanceList.get_by_filters(context, filters,
606                                                         use_slave=True)
607         name_map = {instance.name: instance for instance in instances}
608         local_instances = []
609         for driver_instance in driver_instances:
610             instance = name_map.get(driver_instance)
611             if not instance:
612                 continue
613             local_instances.append(instance)
614         return local_instances
615 
616     def _destroy_evacuated_instances(self, context):
617         """Destroys evacuated instances.
618 
619         While nova-compute was down, the instances running on it could be
620         evacuated to another host. Check that the instances reported
621         by the driver are still associated with this host.  If they are
622         not, destroy them, with the exception of instances which are in
623         the MIGRATING, RESIZE_MIGRATING, RESIZE_MIGRATED, RESIZE_FINISH
624         task state or RESIZED vm state.
625         """
626         filters = {
627             'source_compute': self.host,
628             'status': ['accepted', 'done'],
629             'migration_type': 'evacuation',
630         }
631         with utils.temporary_mutation(context, read_deleted='yes'):
632             evacuations = objects.MigrationList.get_by_filters(context,
633                                                                filters)
634         if not evacuations:
635             return
636         evacuations = {mig.instance_uuid: mig for mig in evacuations}
637 
638         local_instances = self._get_instances_on_driver(context)
639         evacuated = [inst for inst in local_instances
640                      if inst.uuid in evacuations]
641         for instance in evacuated:
642             migration = evacuations[instance.uuid]
643             LOG.info('Deleting instance as it has been evacuated from '
644                      'this host', instance=instance)
645             try:
646                 network_info = self.network_api.get_instance_nw_info(
647                     context, instance)
648                 bdi = self._get_instance_block_device_info(context,
649                                                            instance)
650                 destroy_disks = not (self._is_instance_storage_shared(
651                     context, instance))
652             except exception.InstanceNotFound:
653                 network_info = network_model.NetworkInfo()
654                 bdi = {}
655                 LOG.info('Instance has been marked deleted already, '
656                          'removing it from the hypervisor.',
657                          instance=instance)
658                 # always destroy disks if the instance was deleted
659                 destroy_disks = True
660             self.driver.destroy(context, instance,
661                                 network_info,
662                                 bdi, destroy_disks)
663             migration.status = 'completed'
664             migration.save()
665 
666     def _is_instance_storage_shared(self, context, instance, host=None):
667         shared_storage = True
668         data = None
669         try:
670             data = self.driver.check_instance_shared_storage_local(context,
671                                                        instance)
672             if data:
673                 shared_storage = (self.compute_rpcapi.
674                                   check_instance_shared_storage(context,
675                                   instance, data, host=host))
676         except NotImplementedError:
677             LOG.debug('Hypervisor driver does not support '
678                       'instance shared storage check, '
679                       'assuming it\'s not on shared storage',
680                       instance=instance)
681             shared_storage = False
682         except Exception:
683             LOG.exception('Failed to check if instance shared',
684                           instance=instance)
685         finally:
686             if data:
687                 self.driver.check_instance_shared_storage_cleanup(context,
688                                                                   data)
689         return shared_storage
690 
691     def _complete_partial_deletion(self, context, instance):
692         """Complete deletion for instances in DELETED status but not marked as
693         deleted in the DB
694         """
695         system_meta = instance.system_metadata
696         instance.destroy()
697         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
698                 context, instance.uuid)
699         self._complete_deletion(context,
700                                 instance,
701                                 bdms,
702                                 system_meta)
703 
704     def _complete_deletion(self, context, instance, bdms,
705                            system_meta):
706         # ensure block device mappings are not leaked
707         for bdm in bdms:
708             bdm.destroy()
709 
710         self._update_resource_tracker(context, instance)
711         self._notify_about_instance_usage(context, instance, "delete.end",
712                 system_metadata=system_meta)
713         compute_utils.notify_about_instance_action(context, instance,
714                 self.host, action=fields.NotificationAction.DELETE,
715                 phase=fields.NotificationPhase.END)
716         self._delete_scheduler_instance_info(context, instance.uuid)
717 
718     def _init_instance(self, context, instance):
719         '''Initialize this instance during service init.'''
720 
721         # NOTE(danms): If the instance appears to not be owned by this
722         # host, it may have been evacuated away, but skipped by the
723         # evacuation cleanup code due to configuration. Thus, if that
724         # is a possibility, don't touch the instance in any way, but
725         # log the concern. This will help avoid potential issues on
726         # startup due to misconfiguration.
727         if instance.host != self.host:
728             LOG.warning('Instance %(uuid)s appears to not be owned '
729                         'by this host, but by %(host)s. Startup '
730                         'processing is being skipped.',
731                         {'uuid': instance.uuid,
732                          'host': instance.host})
733             return
734 
735         # Instances that are shut down, or in an error state can not be
736         # initialized and are not attempted to be recovered. The exception
737         # to this are instances that are in RESIZE_MIGRATING or DELETING,
738         # which are dealt with further down.
739         if (instance.vm_state == vm_states.SOFT_DELETED or
740             (instance.vm_state == vm_states.ERROR and
741             instance.task_state not in
742             (task_states.RESIZE_MIGRATING, task_states.DELETING))):
743             LOG.debug("Instance is in %s state.",
744                       instance.vm_state, instance=instance)
745             return
746 
747         if instance.vm_state == vm_states.DELETED:
748             try:
749                 self._complete_partial_deletion(context, instance)
750             except Exception:
751                 # we don't want that an exception blocks the init_host
752                 LOG.exception('Failed to complete a deletion',
753                               instance=instance)
754             return
755 
756         if (instance.vm_state == vm_states.BUILDING or
757             instance.task_state in [task_states.SCHEDULING,
758                                     task_states.BLOCK_DEVICE_MAPPING,
759                                     task_states.NETWORKING,
760                                     task_states.SPAWNING]):
761             # NOTE(dave-mcnally) compute stopped before instance was fully
762             # spawned so set to ERROR state. This is safe to do as the state
763             # may be set by the api but the host is not so if we get here the
764             # instance has already been scheduled to this particular host.
765             LOG.debug("Instance failed to spawn correctly, "
766                       "setting to ERROR state", instance=instance)
767             instance.task_state = None
768             instance.vm_state = vm_states.ERROR
769             instance.save()
770             return
771 
772         if (instance.vm_state in [vm_states.ACTIVE, vm_states.STOPPED] and
773             instance.task_state in [task_states.REBUILDING,
774                                     task_states.REBUILD_BLOCK_DEVICE_MAPPING,
775                                     task_states.REBUILD_SPAWNING]):
776             # NOTE(jichenjc) compute stopped before instance was fully
777             # spawned so set to ERROR state. This is consistent to BUILD
778             LOG.debug("Instance failed to rebuild correctly, "
779                       "setting to ERROR state", instance=instance)
780             instance.task_state = None
781             instance.vm_state = vm_states.ERROR
782             instance.save()
783             return
784 
785         if (instance.vm_state != vm_states.ERROR and
786             instance.task_state in [task_states.IMAGE_SNAPSHOT_PENDING,
787                                     task_states.IMAGE_PENDING_UPLOAD,
788                                     task_states.IMAGE_UPLOADING,
789                                     task_states.IMAGE_SNAPSHOT]):
790             LOG.debug("Instance in transitional state %s at start-up "
791                       "clearing task state",
792                       instance.task_state, instance=instance)
793             try:
794                 self._post_interrupted_snapshot_cleanup(context, instance)
795             except Exception:
796                 # we don't want that an exception blocks the init_host
797                 LOG.exception('Failed to cleanup snapshot.', instance=instance)
798             instance.task_state = None
799             instance.save()
800 
801         if (instance.vm_state != vm_states.ERROR and
802             instance.task_state in [task_states.RESIZE_PREP]):
803             LOG.debug("Instance in transitional state %s at start-up "
804                       "clearing task state",
805                       instance['task_state'], instance=instance)
806             instance.task_state = None
807             instance.save()
808 
809         if instance.task_state == task_states.DELETING:
810             try:
811                 LOG.info('Service started deleting the instance during '
812                          'the previous run, but did not finish. Restarting'
813                          ' the deletion now.', instance=instance)
814                 instance.obj_load_attr('metadata')
815                 instance.obj_load_attr('system_metadata')
816                 bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
817                         context, instance.uuid)
818                 self._delete_instance(context, instance, bdms)
819             except Exception:
820                 # we don't want that an exception blocks the init_host
821                 LOG.exception('Failed to complete a deletion',
822                               instance=instance)
823                 self._set_instance_obj_error_state(context, instance)
824             return
825 
826         current_power_state = self._get_power_state(context, instance)
827         try_reboot, reboot_type = self._retry_reboot(context, instance,
828                                                      current_power_state)
829 
830         if try_reboot:
831             LOG.debug("Instance in transitional state (%(task_state)s) at "
832                       "start-up and power state is (%(power_state)s), "
833                       "triggering reboot",
834                       {'task_state': instance.task_state,
835                        'power_state': current_power_state},
836                       instance=instance)
837 
838             # NOTE(mikal): if the instance was doing a soft reboot that got as
839             # far as shutting down the instance but not as far as starting it
840             # again, then we've just become a hard reboot. That means the
841             # task state for the instance needs to change so that we're in one
842             # of the expected task states for a hard reboot.
843             soft_types = [task_states.REBOOT_STARTED,
844                           task_states.REBOOT_PENDING,
845                           task_states.REBOOTING]
846             if instance.task_state in soft_types and reboot_type == 'HARD':
847                 instance.task_state = task_states.REBOOT_PENDING_HARD
848                 instance.save()
849 
850             self.reboot_instance(context, instance, block_device_info=None,
851                                  reboot_type=reboot_type)
852             return
853 
854         elif (current_power_state == power_state.RUNNING and
855               instance.task_state in [task_states.REBOOT_STARTED,
856                                       task_states.REBOOT_STARTED_HARD,
857                                       task_states.PAUSING,
858                                       task_states.UNPAUSING]):
859             LOG.warning("Instance in transitional state "
860                         "(%(task_state)s) at start-up and power state "
861                         "is (%(power_state)s), clearing task state",
862                         {'task_state': instance.task_state,
863                          'power_state': current_power_state},
864                         instance=instance)
865             instance.task_state = None
866             instance.vm_state = vm_states.ACTIVE
867             instance.save()
868         elif (current_power_state == power_state.PAUSED and
869               instance.task_state == task_states.UNPAUSING):
870             LOG.warning("Instance in transitional state "
871                         "(%(task_state)s) at start-up and power state "
872                         "is (%(power_state)s), clearing task state "
873                         "and unpausing the instance",
874                         {'task_state': instance.task_state,
875                          'power_state': current_power_state},
876                         instance=instance)
877             try:
878                 self.unpause_instance(context, instance)
879             except NotImplementedError:
880                 # Some virt driver didn't support pause and unpause
881                 pass
882             except Exception:
883                 LOG.exception('Failed to unpause instance', instance=instance)
884             return
885 
886         if instance.task_state == task_states.POWERING_OFF:
887             try:
888                 LOG.debug("Instance in transitional state %s at start-up "
889                           "retrying stop request",
890                           instance.task_state, instance=instance)
891                 self.stop_instance(context, instance, True)
892             except Exception:
893                 # we don't want that an exception blocks the init_host
894                 LOG.exception('Failed to stop instance', instance=instance)
895             return
896 
897         if instance.task_state == task_states.POWERING_ON:
898             try:
899                 LOG.debug("Instance in transitional state %s at start-up "
900                           "retrying start request",
901                           instance.task_state, instance=instance)
902                 self.start_instance(context, instance)
903             except Exception:
904                 # we don't want that an exception blocks the init_host
905                 LOG.exception('Failed to start instance', instance=instance)
906             return
907 
908         net_info = instance.get_network_info()
909         try:
910             self.driver.plug_vifs(instance, net_info)
911         except NotImplementedError as e:
912             LOG.debug(e, instance=instance)
913         except exception.VirtualInterfacePlugException:
914             # we don't want an exception to block the init_host
915             LOG.exception("Vifs plug failed", instance=instance)
916             self._set_instance_obj_error_state(context, instance)
917             return
918 
919         if instance.task_state == task_states.RESIZE_MIGRATING:
920             # We crashed during resize/migration, so roll back for safety
921             try:
922                 # NOTE(mriedem): check old_vm_state for STOPPED here, if it's
923                 # not in system_metadata we default to True for backwards
924                 # compatibility
925                 power_on = (instance.system_metadata.get('old_vm_state') !=
926                             vm_states.STOPPED)
927 
928                 block_dev_info = self._get_instance_block_device_info(context,
929                                                                       instance)
930 
931                 self.driver.finish_revert_migration(context,
932                     instance, net_info, block_dev_info, power_on)
933 
934             except Exception:
935                 LOG.exception('Failed to revert crashed migration',
936                               instance=instance)
937             finally:
938                 LOG.info('Instance found in migrating state during '
939                          'startup. Resetting task_state',
940                          instance=instance)
941                 instance.task_state = None
942                 instance.save()
943         if instance.task_state == task_states.MIGRATING:
944             # Live migration did not complete, but instance is on this
945             # host, so reset the state.
946             instance.task_state = None
947             instance.save(expected_task_state=[task_states.MIGRATING])
948 
949         db_state = instance.power_state
950         drv_state = self._get_power_state(context, instance)
951         expect_running = (db_state == power_state.RUNNING and
952                           drv_state != db_state)
953 
954         LOG.debug('Current state is %(drv_state)s, state in DB is '
955                   '%(db_state)s.',
956                   {'drv_state': drv_state, 'db_state': db_state},
957                   instance=instance)
958 
959         if expect_running and CONF.resume_guests_state_on_host_boot:
960             LOG.info('Rebooting instance after nova-compute restart.',
961                      instance=instance)
962 
963             block_device_info = \
964                 self._get_instance_block_device_info(context, instance)
965 
966             try:
967                 self.driver.resume_state_on_host_boot(
968                     context, instance, net_info, block_device_info)
969             except NotImplementedError:
970                 LOG.warning('Hypervisor driver does not support '
971                             'resume guests', instance=instance)
972             except Exception:
973                 # NOTE(vish): The instance failed to resume, so we set the
974                 #             instance to error and attempt to continue.
975                 LOG.warning('Failed to resume instance',
976                             instance=instance)
977                 self._set_instance_obj_error_state(context, instance)
978 
979         elif drv_state == power_state.RUNNING:
980             # VMwareAPI drivers will raise an exception
981             try:
982                 self.driver.ensure_filtering_rules_for_instance(
983                                        instance, net_info)
984             except NotImplementedError:
985                 LOG.debug('Hypervisor driver does not support '
986                           'firewall rules', instance=instance)
987 
988     def _retry_reboot(self, context, instance, current_power_state):
989         current_task_state = instance.task_state
990         retry_reboot = False
991         reboot_type = compute_utils.get_reboot_type(current_task_state,
992                                                     current_power_state)
993 
994         pending_soft = (current_task_state == task_states.REBOOT_PENDING and
995                         instance.vm_state in vm_states.ALLOW_SOFT_REBOOT)
996         pending_hard = (current_task_state == task_states.REBOOT_PENDING_HARD
997                         and instance.vm_state in vm_states.ALLOW_HARD_REBOOT)
998         started_not_running = (current_task_state in
999                                [task_states.REBOOT_STARTED,
1000                                 task_states.REBOOT_STARTED_HARD] and
1001                                current_power_state != power_state.RUNNING)
1002 
1003         if pending_soft or pending_hard or started_not_running:
1004             retry_reboot = True
1005 
1006         return retry_reboot, reboot_type
1007 
1008     def handle_lifecycle_event(self, event):
1009         LOG.info("VM %(state)s (Lifecycle Event)",
1010                  {'state': event.get_name()},
1011                  instance_uuid=event.get_instance_uuid())
1012         context = nova.context.get_admin_context(read_deleted='yes')
1013         instance = objects.Instance.get_by_uuid(context,
1014                                                 event.get_instance_uuid(),
1015                                                 expected_attrs=[])
1016         vm_power_state = None
1017         if event.get_transition() == virtevent.EVENT_LIFECYCLE_STOPPED:
1018             vm_power_state = power_state.SHUTDOWN
1019         elif event.get_transition() == virtevent.EVENT_LIFECYCLE_STARTED:
1020             vm_power_state = power_state.RUNNING
1021         elif event.get_transition() == virtevent.EVENT_LIFECYCLE_PAUSED:
1022             vm_power_state = power_state.PAUSED
1023         elif event.get_transition() == virtevent.EVENT_LIFECYCLE_RESUMED:
1024             vm_power_state = power_state.RUNNING
1025         elif event.get_transition() == virtevent.EVENT_LIFECYCLE_SUSPENDED:
1026             vm_power_state = power_state.SUSPENDED
1027         else:
1028             LOG.warning("Unexpected power state %d", event.get_transition())
1029 
1030         # Note(lpetrut): The event may be delayed, thus not reflecting
1031         # the current instance power state. In that case, ignore the event.
1032         current_power_state = self._get_power_state(context, instance)
1033         if current_power_state == vm_power_state:
1034             LOG.debug('Synchronizing instance power state after lifecycle '
1035                       'event "%(event)s"; current vm_state: %(vm_state)s, '
1036                       'current task_state: %(task_state)s, current DB '
1037                       'power_state: %(db_power_state)s, VM power_state: '
1038                       '%(vm_power_state)s',
1039                       {'event': event.get_name(),
1040                        'vm_state': instance.vm_state,
1041                        'task_state': instance.task_state,
1042                        'db_power_state': instance.power_state,
1043                        'vm_power_state': vm_power_state},
1044                       instance_uuid=instance.uuid)
1045             self._sync_instance_power_state(context,
1046                                             instance,
1047                                             vm_power_state)
1048 
1049     def handle_events(self, event):
1050         if isinstance(event, virtevent.LifecycleEvent):
1051             try:
1052                 self.handle_lifecycle_event(event)
1053             except exception.InstanceNotFound:
1054                 LOG.debug("Event %s arrived for non-existent instance. The "
1055                           "instance was probably deleted.", event)
1056         else:
1057             LOG.debug("Ignoring event %s", event)
1058 
1059     def init_virt_events(self):
1060         if CONF.workarounds.handle_virt_lifecycle_events:
1061             self.driver.register_event_listener(self.handle_events)
1062         else:
1063             # NOTE(mriedem): If the _sync_power_states periodic task is
1064             # disabled we should emit a warning in the logs.
1065             if CONF.sync_power_state_interval < 0:
1066                 LOG.warning('Instance lifecycle events from the compute '
1067                             'driver have been disabled. Note that lifecycle '
1068                             'changes to an instance outside of the compute '
1069                             'service will not be synchronized '
1070                             'automatically since the _sync_power_states '
1071                             'periodic task is also disabled.')
1072             else:
1073                 LOG.info('Instance lifecycle events from the compute '
1074                          'driver have been disabled. Note that lifecycle '
1075                          'changes to an instance outside of the compute '
1076                          'service will only be synchronized by the '
1077                          '_sync_power_states periodic task.')
1078 
1079     def init_host(self):
1080         """Initialization for a standalone compute service."""
1081 
1082         if CONF.pci.passthrough_whitelist:
1083             # Simply loading the PCI passthrough whitelist will do a bunch of
1084             # validation that would otherwise wait until the PciDevTracker is
1085             # constructed when updating available resources for the compute
1086             # node(s) in the resource tracker, effectively killing that task.
1087             # So load up the whitelist when starting the compute service to
1088             # flush any invalid configuration early so we can kill the service
1089             # if the configuration is wrong.
1090             whitelist.Whitelist(CONF.pci.passthrough_whitelist)
1091 
1092         # NOTE(sbauza): We want the compute node to hard fail if it can't be
1093         # able to provide its resources to the placement API, or it would not
1094         # be able to be eligible as a destination.
1095         if CONF.placement.os_region_name is None:
1096             raise exception.PlacementNotConfigured()
1097 
1098         self.driver.init_host(host=self.host)
1099         context = nova.context.get_admin_context()
1100         instances = objects.InstanceList.get_by_host(
1101             context, self.host, expected_attrs=['info_cache', 'metadata'])
1102 
1103         if CONF.defer_iptables_apply:
1104             self.driver.filter_defer_apply_on()
1105 
1106         self.init_virt_events()
1107 
1108         try:
1109             # checking that instance was not already evacuated to other host
1110             self._destroy_evacuated_instances(context)
1111             for instance in instances:
1112                 self._init_instance(context, instance)
1113         finally:
1114             if CONF.defer_iptables_apply:
1115                 self.driver.filter_defer_apply_off()
1116             if instances:
1117                 # We only send the instance info to the scheduler on startup
1118                 # if there is anything to send, otherwise this host might
1119                 # not be mapped yet in a cell and the scheduler may have
1120                 # issues dealing with the information. Later changes to
1121                 # instances on this host will update the scheduler, or the
1122                 # _sync_scheduler_instance_info periodic task will.
1123                 self._update_scheduler_instance_info(context, instances)
1124 
1125     def cleanup_host(self):
1126         self.driver.register_event_listener(None)
1127         self.instance_events.cancel_all_events()
1128         self.driver.cleanup_host(host=self.host)
1129 
1130     def pre_start_hook(self):
1131         """After the service is initialized, but before we fully bring
1132         the service up by listening on RPC queues, make sure to update
1133         our available resources (and indirectly our available nodes).
1134         """
1135         self.update_available_resource(nova.context.get_admin_context(),
1136                                        startup=True)
1137 
1138     def _get_power_state(self, context, instance):
1139         """Retrieve the power state for the given instance."""
1140         LOG.debug('Checking state', instance=instance)
1141         try:
1142             return self.driver.get_info(instance).state
1143         except exception.InstanceNotFound:
1144             return power_state.NOSTATE
1145 
1146     def get_console_topic(self, context):
1147         """Retrieves the console host for a project on this host.
1148 
1149         Currently this is just set in the flags for each compute host.
1150 
1151         """
1152         # TODO(mdragon): perhaps make this variable by console_type?
1153         return '%s.%s' % (console_rpcapi.RPC_TOPIC, CONF.console_host)
1154 
1155     @wrap_exception()
1156     def get_console_pool_info(self, context, console_type):
1157         return self.driver.get_console_pool_info(console_type)
1158 
1159     # NOTE(hanlind): This and the virt method it calls can be removed in
1160     # version 5.0 of the RPC API
1161     @wrap_exception()
1162     def refresh_security_group_rules(self, context, security_group_id):
1163         """Tell the virtualization driver to refresh security group rules.
1164 
1165         Passes straight through to the virtualization driver.
1166 
1167         """
1168         return self.driver.refresh_security_group_rules(security_group_id)
1169 
1170     # TODO(alaski): Remove object_compat for RPC version 5.0
1171     @object_compat
1172     @wrap_exception()
1173     def refresh_instance_security_rules(self, context, instance):
1174         """Tell the virtualization driver to refresh security rules for
1175         an instance.
1176 
1177         Passes straight through to the virtualization driver.
1178 
1179         Synchronize the call because we may still be in the middle of
1180         creating the instance.
1181         """
1182         @utils.synchronized(instance.uuid)
1183         def _sync_refresh():
1184             try:
1185                 return self.driver.refresh_instance_security_rules(instance)
1186             except NotImplementedError:
1187                 LOG.debug('Hypervisor driver does not support '
1188                           'security groups.', instance=instance)
1189 
1190         return _sync_refresh()
1191 
1192     def _await_block_device_map_created(self, context, vol_id):
1193         # TODO(yamahata): creating volume simultaneously
1194         #                 reduces creation time?
1195         # TODO(yamahata): eliminate dumb polling
1196         start = time.time()
1197         retries = CONF.block_device_allocate_retries
1198         if retries < 0:
1199             LOG.warning("Treating negative config value (%(retries)s) for "
1200                         "'block_device_retries' as 0.",
1201                         {'retries': retries})
1202         # (1) treat  negative config value as 0
1203         # (2) the configured value is 0, one attempt should be made
1204         # (3) the configured value is > 0, then the total number attempts
1205         #      is (retries + 1)
1206         attempts = 1
1207         if retries >= 1:
1208             attempts = retries + 1
1209         for attempt in range(1, attempts + 1):
1210             volume = self.volume_api.get(context, vol_id)
1211             volume_status = volume['status']
1212             if volume_status not in ['creating', 'downloading']:
1213                 if volume_status == 'available':
1214                     return attempt
1215                 LOG.warning("Volume id: %(vol_id)s finished being "
1216                             "created but its status is %(vol_status)s.",
1217                             {'vol_id': vol_id,
1218                              'vol_status': volume_status})
1219                 break
1220             greenthread.sleep(CONF.block_device_allocate_retries_interval)
1221         raise exception.VolumeNotCreated(volume_id=vol_id,
1222                                          seconds=int(time.time() - start),
1223                                          attempts=attempt,
1224                                          volume_status=volume_status)
1225 
1226     def _decode_files(self, injected_files):
1227         """Base64 decode the list of files to inject."""
1228         if not injected_files:
1229             return []
1230 
1231         def _decode(f):
1232             path, contents = f
1233             # Py3 raises binascii.Error instead of TypeError as in Py27
1234             try:
1235                 decoded = base64.b64decode(contents)
1236                 return path, decoded
1237             except (TypeError, binascii.Error):
1238                 raise exception.Base64Exception(path=path)
1239 
1240         return [_decode(f) for f in injected_files]
1241 
1242     def _validate_instance_group_policy(self, context, instance,
1243             filter_properties):
1244         # NOTE(russellb) Instance group policy is enforced by the scheduler.
1245         # However, there is a race condition with the enforcement of
1246         # the policy.  Since more than one instance may be scheduled at the
1247         # same time, it's possible that more than one instance with an
1248         # anti-affinity policy may end up here.  It's also possible that
1249         # multiple instances with an affinity policy could end up on different
1250         # hosts.  This is a validation step to make sure that starting the
1251         # instance here doesn't violate the policy.
1252 
1253         scheduler_hints = filter_properties.get('scheduler_hints') or {}
1254         group_hint = scheduler_hints.get('group')
1255         if not group_hint:
1256             return
1257 
1258         @utils.synchronized(group_hint)
1259         def _do_validation(context, instance, group_hint):
1260             group = objects.InstanceGroup.get_by_hint(context, group_hint)
1261             if 'anti-affinity' in group.policies:
1262                 group_hosts = group.get_hosts(exclude=[instance.uuid])
1263                 if self.host in group_hosts:
1264                     msg = _("Anti-affinity instance group policy "
1265                             "was violated.")
1266                     raise exception.RescheduledException(
1267                             instance_uuid=instance.uuid,
1268                             reason=msg)
1269             elif 'affinity' in group.policies:
1270                 group_hosts = group.get_hosts(exclude=[instance.uuid])
1271                 if group_hosts and self.host not in group_hosts:
1272                     msg = _("Affinity instance group policy was violated.")
1273                     raise exception.RescheduledException(
1274                             instance_uuid=instance.uuid,
1275                             reason=msg)
1276 
1277         if not CONF.workarounds.disable_group_policy_check_upcall:
1278             _do_validation(context, instance, group_hint)
1279 
1280     def _log_original_error(self, exc_info, instance_uuid):
1281         LOG.error('Error: %s', exc_info[1], instance_uuid=instance_uuid,
1282                   exc_info=exc_info)
1283 
1284     def _reschedule(self, context, request_spec, filter_properties,
1285             instance, reschedule_method, method_args, task_state,
1286             exc_info=None):
1287         """Attempt to re-schedule a compute operation."""
1288 
1289         instance_uuid = instance.uuid
1290         retry = filter_properties.get('retry')
1291         if not retry:
1292             # no retry information, do not reschedule.
1293             LOG.debug("Retry info not present, will not reschedule",
1294                       instance_uuid=instance_uuid)
1295             return
1296 
1297         if not request_spec:
1298             LOG.debug("No request spec, will not reschedule",
1299                       instance_uuid=instance_uuid)
1300             return
1301 
1302         LOG.debug("Re-scheduling %(method)s: attempt %(num)d",
1303                   {'method': reschedule_method.__name__,
1304                    'num': retry['num_attempts']}, instance_uuid=instance_uuid)
1305 
1306         # reset the task state:
1307         self._instance_update(context, instance, task_state=task_state)
1308 
1309         if exc_info:
1310             # stringify to avoid circular ref problem in json serialization:
1311             retry['exc'] = traceback.format_exception_only(exc_info[0],
1312                                     exc_info[1])
1313 
1314         reschedule_method(context, *method_args)
1315         return True
1316 
1317     @periodic_task.periodic_task
1318     def _check_instance_build_time(self, context):
1319         """Ensure that instances are not stuck in build."""
1320         timeout = CONF.instance_build_timeout
1321         if timeout == 0:
1322             return
1323 
1324         filters = {'vm_state': vm_states.BUILDING,
1325                    'host': self.host}
1326 
1327         building_insts = objects.InstanceList.get_by_filters(context,
1328                            filters, expected_attrs=[], use_slave=True)
1329 
1330         for instance in building_insts:
1331             if timeutils.is_older_than(instance.created_at, timeout):
1332                 self._set_instance_obj_error_state(context, instance)
1333                 LOG.warning("Instance build timed out. Set to error "
1334                             "state.", instance=instance)
1335 
1336     def _check_instance_exists(self, context, instance):
1337         """Ensure an instance with the same name is not already present."""
1338         if self.driver.instance_exists(instance):
1339             raise exception.InstanceExists(name=instance.name)
1340 
1341     def _allocate_network_async(self, context, instance, requested_networks,
1342                                 macs, security_groups, is_vpn, dhcp_options):
1343         """Method used to allocate networks in the background.
1344 
1345         Broken out for testing.
1346         """
1347         # First check to see if we're specifically not supposed to allocate
1348         # networks because if so, we can exit early.
1349         if requested_networks and requested_networks.no_allocate:
1350             LOG.debug("Not allocating networking since 'none' was specified.",
1351                       instance=instance)
1352             return network_model.NetworkInfo([])
1353 
1354         LOG.debug("Allocating IP information in the background.",
1355                   instance=instance)
1356         retries = CONF.network_allocate_retries
1357         attempts = retries + 1
1358         retry_time = 1
1359         bind_host_id = self.driver.network_binding_host_id(context, instance)
1360         for attempt in range(1, attempts + 1):
1361             try:
1362                 nwinfo = self.network_api.allocate_for_instance(
1363                         context, instance, vpn=is_vpn,
1364                         requested_networks=requested_networks,
1365                         macs=macs,
1366                         security_groups=security_groups,
1367                         dhcp_options=dhcp_options,
1368                         bind_host_id=bind_host_id)
1369                 LOG.debug('Instance network_info: |%s|', nwinfo,
1370                           instance=instance)
1371                 instance.system_metadata['network_allocated'] = 'True'
1372                 # NOTE(JoshNang) do not save the instance here, as it can cause
1373                 # races. The caller shares a reference to instance and waits
1374                 # for this async greenthread to finish before calling
1375                 # instance.save().
1376                 return nwinfo
1377             except Exception:
1378                 exc_info = sys.exc_info()
1379                 log_info = {'attempt': attempt,
1380                             'attempts': attempts}
1381                 if attempt == attempts:
1382                     LOG.exception('Instance failed network setup '
1383                                   'after %(attempts)d attempt(s)',
1384                                   log_info)
1385                     six.reraise(*exc_info)
1386                 LOG.warning('Instance failed network setup '
1387                             '(attempt %(attempt)d of %(attempts)d)',
1388                             log_info, instance=instance)
1389                 time.sleep(retry_time)
1390                 retry_time *= 2
1391                 if retry_time > 30:
1392                     retry_time = 30
1393         # Not reached.
1394 
1395     def _build_networks_for_instance(self, context, instance,
1396             requested_networks, security_groups):
1397 
1398         # If we're here from a reschedule the network may already be allocated.
1399         if strutils.bool_from_string(
1400                 instance.system_metadata.get('network_allocated', 'False')):
1401             # NOTE(alex_xu): The network_allocated is True means the network
1402             # resource already allocated at previous scheduling, and the
1403             # network setup is cleanup at previous. After rescheduling, the
1404             # network resource need setup on the new host.
1405             self.network_api.setup_instance_network_on_host(
1406                 context, instance, instance.host)
1407             return self.network_api.get_instance_nw_info(context, instance)
1408 
1409         if not self.is_neutron_security_groups:
1410             security_groups = []
1411 
1412         macs = self.driver.macs_for_instance(instance)
1413         dhcp_options = self.driver.dhcp_options_for_instance(instance)
1414         network_info = self._allocate_network(context, instance,
1415                 requested_networks, macs, security_groups, dhcp_options)
1416 
1417         return network_info
1418 
1419     def _allocate_network(self, context, instance, requested_networks, macs,
1420                           security_groups, dhcp_options):
1421         """Start network allocation asynchronously.  Return an instance
1422         of NetworkInfoAsyncWrapper that can be used to retrieve the
1423         allocated networks when the operation has finished.
1424         """
1425         # NOTE(comstud): Since we're allocating networks asynchronously,
1426         # this task state has little meaning, as we won't be in this
1427         # state for very long.
1428         instance.vm_state = vm_states.BUILDING
1429         instance.task_state = task_states.NETWORKING
1430         instance.save(expected_task_state=[None])
1431         self._update_resource_tracker(context, instance)
1432 
1433         is_vpn = False
1434         return network_model.NetworkInfoAsyncWrapper(
1435                 self._allocate_network_async, context, instance,
1436                 requested_networks, macs, security_groups, is_vpn,
1437                 dhcp_options)
1438 
1439     def _default_root_device_name(self, instance, image_meta, root_bdm):
1440         try:
1441             return self.driver.default_root_device_name(instance,
1442                                                         image_meta,
1443                                                         root_bdm)
1444         except NotImplementedError:
1445             return compute_utils.get_next_device_name(instance, [])
1446 
1447     def _default_device_names_for_instance(self, instance,
1448                                            root_device_name,
1449                                            *block_device_lists):
1450         try:
1451             self.driver.default_device_names_for_instance(instance,
1452                                                           root_device_name,
1453                                                           *block_device_lists)
1454         except NotImplementedError:
1455             compute_utils.default_device_names_for_instance(
1456                 instance, root_device_name, *block_device_lists)
1457 
1458     def _get_device_name_for_instance(self, instance, bdms, block_device_obj):
1459         # NOTE(ndipanov): Copy obj to avoid changing the original
1460         block_device_obj = block_device_obj.obj_clone()
1461         try:
1462             return self.driver.get_device_name_for_instance(
1463                 instance, bdms, block_device_obj)
1464         except NotImplementedError:
1465             return compute_utils.get_device_name_for_instance(
1466                 instance, bdms, block_device_obj.get("device_name"))
1467 
1468     def _default_block_device_names(self, instance, image_meta, block_devices):
1469         """Verify that all the devices have the device_name set. If not,
1470         provide a default name.
1471 
1472         It also ensures that there is a root_device_name and is set to the
1473         first block device in the boot sequence (boot_index=0).
1474         """
1475         root_bdm = block_device.get_root_bdm(block_devices)
1476         if not root_bdm:
1477             return
1478 
1479         # Get the root_device_name from the root BDM or the instance
1480         root_device_name = None
1481         update_root_bdm = False
1482 
1483         if root_bdm.device_name:
1484             root_device_name = root_bdm.device_name
1485             instance.root_device_name = root_device_name
1486         elif instance.root_device_name:
1487             root_device_name = instance.root_device_name
1488             root_bdm.device_name = root_device_name
1489             update_root_bdm = True
1490         else:
1491             root_device_name = self._default_root_device_name(instance,
1492                                                               image_meta,
1493                                                               root_bdm)
1494 
1495             instance.root_device_name = root_device_name
1496             root_bdm.device_name = root_device_name
1497             update_root_bdm = True
1498 
1499         if update_root_bdm:
1500             root_bdm.save()
1501 
1502         ephemerals = list(filter(block_device.new_format_is_ephemeral,
1503                             block_devices))
1504         swap = list(filter(block_device.new_format_is_swap,
1505                       block_devices))
1506         block_device_mapping = list(filter(
1507               driver_block_device.is_block_device_mapping, block_devices))
1508 
1509         self._default_device_names_for_instance(instance,
1510                                                 root_device_name,
1511                                                 ephemerals,
1512                                                 swap,
1513                                                 block_device_mapping)
1514 
1515     def _block_device_info_to_legacy(self, block_device_info):
1516         """Convert BDI to the old format for drivers that need it."""
1517 
1518         if self.use_legacy_block_device_info:
1519             ephemerals = driver_block_device.legacy_block_devices(
1520                 driver.block_device_info_get_ephemerals(block_device_info))
1521             mapping = driver_block_device.legacy_block_devices(
1522                 driver.block_device_info_get_mapping(block_device_info))
1523             swap = block_device_info['swap']
1524             if swap:
1525                 swap = swap.legacy()
1526 
1527             block_device_info.update({
1528                 'ephemerals': ephemerals,
1529                 'swap': swap,
1530                 'block_device_mapping': mapping})
1531 
1532     def _add_missing_dev_names(self, bdms, instance):
1533         for bdm in bdms:
1534             if bdm.device_name is not None:
1535                 continue
1536 
1537             device_name = self._get_device_name_for_instance(instance,
1538                                                              bdms, bdm)
1539             values = {'device_name': device_name}
1540             bdm.update(values)
1541             bdm.save()
1542 
1543     def _prep_block_device(self, context, instance, bdms):
1544         """Set up the block device for an instance with error logging."""
1545         try:
1546             self._add_missing_dev_names(bdms, instance)
1547             block_device_info = driver.get_block_device_info(instance, bdms)
1548             mapping = driver.block_device_info_get_mapping(block_device_info)
1549             driver_block_device.attach_block_devices(
1550                 mapping, context, instance, self.volume_api, self.driver,
1551                 wait_func=self._await_block_device_map_created)
1552 
1553             self._block_device_info_to_legacy(block_device_info)
1554             return block_device_info
1555 
1556         except exception.OverQuota as e:
1557             LOG.warning('Failed to create block device for instance due'
1558                         ' to exceeding volume related resource quota.'
1559                         ' Error: %s', e.message, instance=instance)
1560             raise
1561 
1562         except Exception as ex:
1563             LOG.exception('Instance failed block device setup',
1564                           instance=instance)
1565             # InvalidBDM will eventually result in a BuildAbortException when
1566             # booting from volume, and will be recorded as an instance fault.
1567             # Maintain the original exception message which most likely has
1568             # useful details which the standard InvalidBDM error message lacks.
1569             raise exception.InvalidBDM(six.text_type(ex))
1570 
1571     def _update_instance_after_spawn(self, context, instance):
1572         instance.power_state = self._get_power_state(context, instance)
1573         instance.vm_state = vm_states.ACTIVE
1574         instance.task_state = None
1575         instance.launched_at = timeutils.utcnow()
1576         configdrive.update_instance(instance)
1577 
1578     def _update_scheduler_instance_info(self, context, instance):
1579         """Sends an InstanceList with created or updated Instance objects to
1580         the Scheduler client.
1581 
1582         In the case of init_host, the value passed will already be an
1583         InstanceList. Other calls will send individual Instance objects that
1584         have been created or resized. In this case, we create an InstanceList
1585         object containing that Instance.
1586         """
1587         if not self.send_instance_updates:
1588             return
1589         if isinstance(instance, obj_instance.Instance):
1590             instance = objects.InstanceList(objects=[instance])
1591         context = context.elevated()
1592         self.scheduler_client.update_instance_info(context, self.host,
1593                                                    instance)
1594 
1595     def _delete_scheduler_instance_info(self, context, instance_uuid):
1596         """Sends the uuid of the deleted Instance to the Scheduler client."""
1597         if not self.send_instance_updates:
1598             return
1599         context = context.elevated()
1600         self.scheduler_client.delete_instance_info(context, self.host,
1601                                                    instance_uuid)
1602 
1603     @periodic_task.periodic_task(spacing=CONF.scheduler_instance_sync_interval)
1604     def _sync_scheduler_instance_info(self, context):
1605         if not self.send_instance_updates:
1606             return
1607         context = context.elevated()
1608         instances = objects.InstanceList.get_by_host(context, self.host,
1609                                                      expected_attrs=[],
1610                                                      use_slave=True)
1611         uuids = [instance.uuid for instance in instances]
1612         self.scheduler_client.sync_instance_info(context, self.host, uuids)
1613 
1614     def _notify_about_instance_usage(self, context, instance, event_suffix,
1615                                      network_info=None, system_metadata=None,
1616                                      extra_usage_info=None, fault=None):
1617         compute_utils.notify_about_instance_usage(
1618             self.notifier, context, instance, event_suffix,
1619             network_info=network_info,
1620             system_metadata=system_metadata,
1621             extra_usage_info=extra_usage_info, fault=fault)
1622 
1623     def _deallocate_network(self, context, instance,
1624                             requested_networks=None):
1625         # If we were told not to allocate networks let's save ourselves
1626         # the trouble of calling the network API.
1627         if requested_networks and requested_networks.no_allocate:
1628             LOG.debug("Skipping network deallocation for instance since "
1629                       "networking was not requested.", instance=instance)
1630             return
1631 
1632         LOG.debug('Deallocating network for instance', instance=instance)
1633         with timeutils.StopWatch() as timer:
1634             self.network_api.deallocate_for_instance(
1635                 context, instance, requested_networks=requested_networks)
1636         # nova-network does an rpc call so we're OK tracking time spent here
1637         LOG.info('Took %0.2f seconds to deallocate network for instance.',
1638                  timer.elapsed(), instance=instance)
1639 
1640     def _get_instance_block_device_info(self, context, instance,
1641                                         refresh_conn_info=False,
1642                                         bdms=None):
1643         """Transform block devices to the driver block_device format."""
1644 
1645         if not bdms:
1646             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
1647                     context, instance.uuid)
1648         block_device_info = driver.get_block_device_info(instance, bdms)
1649 
1650         if not refresh_conn_info:
1651             # if the block_device_mapping has no value in connection_info
1652             # (returned as None), don't include in the mapping
1653             block_device_info['block_device_mapping'] = [
1654                 bdm for bdm in driver.block_device_info_get_mapping(
1655                                     block_device_info)
1656                 if bdm.get('connection_info')]
1657         else:
1658             driver_block_device.refresh_conn_infos(
1659                 driver.block_device_info_get_mapping(block_device_info),
1660                 context, instance, self.volume_api, self.driver)
1661 
1662         self._block_device_info_to_legacy(block_device_info)
1663 
1664         return block_device_info
1665 
1666     def _build_failed(self):
1667         self._failed_builds += 1
1668         limit = CONF.compute.consecutive_build_service_disable_threshold
1669         if limit and self._failed_builds >= limit:
1670             # NOTE(danms): If we're doing a bunch of parallel builds,
1671             # it is possible (although not likely) that we have already
1672             # failed N-1 builds before this and we race with a successful
1673             # build and disable ourselves here when we might've otherwise
1674             # not.
1675             LOG.error('Disabling service due to %(fails)i '
1676                       'consecutive build failures',
1677                       {'fails': self._failed_builds})
1678             ctx = nova.context.get_admin_context()
1679             service = objects.Service.get_by_compute_host(ctx, CONF.host)
1680             service.disabled = True
1681             service.disabled_reason = (
1682                 'Auto-disabled due to %i build failures' % self._failed_builds)
1683             service.save()
1684             # NOTE(danms): Reset our counter now so that when the admin
1685             # re-enables us we can start fresh
1686             self._failed_builds = 0
1687         elif self._failed_builds > 1:
1688             LOG.warning('%(fails)i consecutive build failures',
1689                         {'fails': self._failed_builds})
1690 
1691     @wrap_exception()
1692     @reverts_task_state
1693     @wrap_instance_fault
1694     def build_and_run_instance(self, context, instance, image, request_spec,
1695                      filter_properties, admin_password=None,
1696                      injected_files=None, requested_networks=None,
1697                      security_groups=None, block_device_mapping=None,
1698                      node=None, limits=None):
1699 
1700         @utils.synchronized(instance.uuid)
1701         def _locked_do_build_and_run_instance(*args, **kwargs):
1702             # NOTE(danms): We grab the semaphore with the instance uuid
1703             # locked because we could wait in line to build this instance
1704             # for a while and we want to make sure that nothing else tries
1705             # to do anything with this instance while we wait.
1706             with self._build_semaphore:
1707                 try:
1708                     result = self._do_build_and_run_instance(*args, **kwargs)
1709                 except Exception:
1710                     result = build_results.FAILED
1711                     raise
1712                 finally:
1713                     fails = (build_results.FAILED,
1714                              build_results.RESCHEDULED)
1715                     if result in fails:
1716                         self._build_failed()
1717                     else:
1718                         self._failed_builds = 0
1719 
1720         # NOTE(danms): We spawn here to return the RPC worker thread back to
1721         # the pool. Since what follows could take a really long time, we don't
1722         # want to tie up RPC workers.
1723         utils.spawn_n(_locked_do_build_and_run_instance,
1724                       context, instance, image, request_spec,
1725                       filter_properties, admin_password, injected_files,
1726                       requested_networks, security_groups,
1727                       block_device_mapping, node, limits)
1728 
1729     def _check_device_tagging(self, requested_networks, block_device_mapping):
1730         tagging_requested = False
1731         if requested_networks:
1732             for net in requested_networks:
1733                 if 'tag' in net and net.tag is not None:
1734                     tagging_requested = True
1735                     break
1736         if block_device_mapping and not tagging_requested:
1737             for bdm in block_device_mapping:
1738                 if 'tag' in bdm and bdm.tag is not None:
1739                     tagging_requested = True
1740                     break
1741         if (tagging_requested and
1742                 not self.driver.capabilities.get('supports_device_tagging')):
1743             raise exception.BuildAbortException('Attempt to boot guest with '
1744                                                 'tagged devices on host that '
1745                                                 'does not support tagging.')
1746 
1747     @hooks.add_hook('build_instance')
1748     @wrap_exception()
1749     @reverts_task_state
1750     @wrap_instance_event(prefix='compute')
1751     @wrap_instance_fault
1752     def _do_build_and_run_instance(self, context, instance, image,
1753             request_spec, filter_properties, admin_password, injected_files,
1754             requested_networks, security_groups, block_device_mapping,
1755             node=None, limits=None):
1756 
1757         try:
1758             LOG.debug('Starting instance...', instance=instance)
1759             instance.vm_state = vm_states.BUILDING
1760             instance.task_state = None
1761             instance.save(expected_task_state=
1762                     (task_states.SCHEDULING, None))
1763         except exception.InstanceNotFound:
1764             msg = 'Instance disappeared before build.'
1765             LOG.debug(msg, instance=instance)
1766             return build_results.FAILED
1767         except exception.UnexpectedTaskStateError as e:
1768             LOG.debug(e.format_message(), instance=instance)
1769             return build_results.FAILED
1770 
1771         # b64 decode the files to inject:
1772         decoded_files = self._decode_files(injected_files)
1773 
1774         if limits is None:
1775             limits = {}
1776 
1777         if node is None:
1778             node = self.driver.get_available_nodes(refresh=True)[0]
1779             LOG.debug('No node specified, defaulting to %s', node,
1780                       instance=instance)
1781 
1782         try:
1783             with timeutils.StopWatch() as timer:
1784                 self._build_and_run_instance(context, instance, image,
1785                         decoded_files, admin_password, requested_networks,
1786                         security_groups, block_device_mapping, node, limits,
1787                         filter_properties)
1788             LOG.info('Took %0.2f seconds to build instance.',
1789                      timer.elapsed(), instance=instance)
1790             return build_results.ACTIVE
1791         except exception.RescheduledException as e:
1792             retry = filter_properties.get('retry')
1793             if not retry:
1794                 # no retry information, do not reschedule.
1795                 LOG.debug("Retry info not present, will not reschedule",
1796                     instance=instance)
1797                 self._cleanup_allocated_networks(context, instance,
1798                     requested_networks)
1799                 self._cleanup_volumes(context, instance.uuid,
1800                     block_device_mapping, raise_exc=False)
1801                 compute_utils.add_instance_fault_from_exc(context,
1802                         instance, e, sys.exc_info(),
1803                         fault_message=e.kwargs['reason'])
1804                 self._nil_out_instance_obj_host_and_node(instance)
1805                 self._set_instance_obj_error_state(context, instance,
1806                                                    clean_task_state=True)
1807                 return build_results.FAILED
1808             LOG.debug(e.format_message(), instance=instance)
1809             # This will be used for logging the exception
1810             retry['exc'] = traceback.format_exception(*sys.exc_info())
1811             # This will be used for setting the instance fault message
1812             retry['exc_reason'] = e.kwargs['reason']
1813             # NOTE(comstud): Deallocate networks if the driver wants
1814             # us to do so.
1815             # NOTE(vladikr): SR-IOV ports should be deallocated to
1816             # allow new sriov pci devices to be allocated on a new host.
1817             # Otherwise, if devices with pci addresses are already allocated
1818             # on the destination host, the instance will fail to spawn.
1819             # info_cache.network_info should be present at this stage.
1820             if (self.driver.deallocate_networks_on_reschedule(instance) or
1821                 self.deallocate_sriov_ports_on_reschedule(instance)):
1822                 self._cleanup_allocated_networks(context, instance,
1823                         requested_networks)
1824             else:
1825                 # NOTE(alex_xu): Network already allocated and we don't
1826                 # want to deallocate them before rescheduling. But we need
1827                 # to cleanup those network resources setup on this host before
1828                 # rescheduling.
1829                 self.network_api.cleanup_instance_network_on_host(
1830                     context, instance, self.host)
1831 
1832             self._nil_out_instance_obj_host_and_node(instance)
1833             instance.task_state = task_states.SCHEDULING
1834             instance.save()
1835 
1836             self.compute_task_api.build_instances(context, [instance],
1837                     image, filter_properties, admin_password,
1838                     injected_files, requested_networks, security_groups,
1839                     block_device_mapping)
1840             return build_results.RESCHEDULED
1841         except (exception.InstanceNotFound,
1842                 exception.UnexpectedDeletingTaskStateError):
1843             msg = 'Instance disappeared during build.'
1844             LOG.debug(msg, instance=instance)
1845             self._cleanup_allocated_networks(context, instance,
1846                     requested_networks)
1847             return build_results.FAILED
1848         except exception.BuildAbortException as e:
1849             LOG.exception(e.format_message(), instance=instance)
1850             self._cleanup_allocated_networks(context, instance,
1851                     requested_networks)
1852             self._cleanup_volumes(context, instance.uuid,
1853                     block_device_mapping, raise_exc=False)
1854             compute_utils.add_instance_fault_from_exc(context, instance,
1855                     e, sys.exc_info())
1856             self._nil_out_instance_obj_host_and_node(instance)
1857             self._set_instance_obj_error_state(context, instance,
1858                                                clean_task_state=True)
1859             return build_results.FAILED
1860         except Exception as e:
1861             # Should not reach here.
1862             LOG.exception('Unexpected build failure, not rescheduling build.',
1863                           instance=instance)
1864             self._cleanup_allocated_networks(context, instance,
1865                     requested_networks)
1866             self._cleanup_volumes(context, instance.uuid,
1867                     block_device_mapping, raise_exc=False)
1868             compute_utils.add_instance_fault_from_exc(context, instance,
1869                     e, sys.exc_info())
1870             self._nil_out_instance_obj_host_and_node(instance)
1871             self._set_instance_obj_error_state(context, instance,
1872                                                clean_task_state=True)
1873             return build_results.FAILED
1874 
1875     def deallocate_sriov_ports_on_reschedule(self, instance):
1876         """Determine if networks are needed to be deallocated before reschedule
1877 
1878         Check the cached network info for any assigned SR-IOV ports.
1879         SR-IOV ports should be deallocated prior to rescheduling
1880         in order to allow new sriov pci devices to be allocated on a new host.
1881         """
1882         info_cache = instance.info_cache
1883 
1884         def _has_sriov_port(vif):
1885             return vif['vnic_type'] in network_model.VNIC_TYPES_SRIOV
1886 
1887         if (info_cache and info_cache.network_info):
1888             for vif in info_cache.network_info:
1889                 if _has_sriov_port(vif):
1890                     return True
1891         return False
1892 
1893     def _build_and_run_instance(self, context, instance, image, injected_files,
1894             admin_password, requested_networks, security_groups,
1895             block_device_mapping, node, limits, filter_properties):
1896 
1897         image_name = image.get('name')
1898         self._notify_about_instance_usage(context, instance, 'create.start',
1899                 extra_usage_info={'image_name': image_name})
1900         compute_utils.notify_about_instance_create(
1901             context, instance, self.host,
1902             phase=fields.NotificationPhase.START)
1903 
1904         # NOTE(mikal): cache the keystone roles associated with the instance
1905         # at boot time for later reference
1906         instance.system_metadata.update(
1907             {'boot_roles': ','.join(context.roles)})
1908 
1909         self._check_device_tagging(requested_networks, block_device_mapping)
1910 
1911         try:
1912             rt = self._get_resource_tracker()
1913             with rt.instance_claim(context, instance, node, limits):
1914                 # NOTE(russellb) It's important that this validation be done
1915                 # *after* the resource tracker instance claim, as that is where
1916                 # the host is set on the instance.
1917                 self._validate_instance_group_policy(context, instance,
1918                         filter_properties)
1919                 image_meta = objects.ImageMeta.from_dict(image)
1920                 with self._build_resources(context, instance,
1921                         requested_networks, security_groups, image_meta,
1922                         block_device_mapping) as resources:
1923                     instance.vm_state = vm_states.BUILDING
1924                     instance.task_state = task_states.SPAWNING
1925                     # NOTE(JoshNang) This also saves the changes to the
1926                     # instance from _allocate_network_async, as they aren't
1927                     # saved in that function to prevent races.
1928                     instance.save(expected_task_state=
1929                             task_states.BLOCK_DEVICE_MAPPING)
1930                     block_device_info = resources['block_device_info']
1931                     network_info = resources['network_info']
1932                     LOG.debug('Start spawning the instance on the hypervisor.',
1933                               instance=instance)
1934                     with timeutils.StopWatch() as timer:
1935                         self.driver.spawn(context, instance, image_meta,
1936                                           injected_files, admin_password,
1937                                           network_info=network_info,
1938                                           block_device_info=block_device_info)
1939                     LOG.info('Took %0.2f seconds to spawn the instance on '
1940                              'the hypervisor.', timer.elapsed(),
1941                              instance=instance)
1942         except (exception.InstanceNotFound,
1943                 exception.UnexpectedDeletingTaskStateError) as e:
1944             with excutils.save_and_reraise_exception():
1945                 self._notify_about_instance_usage(context, instance,
1946                     'create.error', fault=e)
1947                 compute_utils.notify_about_instance_create(
1948                     context, instance, self.host,
1949                     phase=fields.NotificationPhase.ERROR, exception=e)
1950         except exception.ComputeResourcesUnavailable as e:
1951             LOG.debug(e.format_message(), instance=instance)
1952             self._notify_about_instance_usage(context, instance,
1953                     'create.error', fault=e)
1954             compute_utils.notify_about_instance_create(
1955                     context, instance, self.host,
1956                     phase=fields.NotificationPhase.ERROR, exception=e)
1957             raise exception.RescheduledException(
1958                     instance_uuid=instance.uuid, reason=e.format_message())
1959         except exception.BuildAbortException as e:
1960             with excutils.save_and_reraise_exception():
1961                 LOG.debug(e.format_message(), instance=instance)
1962                 self._notify_about_instance_usage(context, instance,
1963                     'create.error', fault=e)
1964                 compute_utils.notify_about_instance_create(
1965                     context, instance, self.host,
1966                     phase=fields.NotificationPhase.ERROR, exception=e)
1967         except (exception.FixedIpLimitExceeded,
1968                 exception.NoMoreNetworks, exception.NoMoreFixedIps) as e:
1969             LOG.warning('No more network or fixed IP to be allocated',
1970                         instance=instance)
1971             self._notify_about_instance_usage(context, instance,
1972                     'create.error', fault=e)
1973             compute_utils.notify_about_instance_create(
1974                     context, instance, self.host,
1975                     phase=fields.NotificationPhase.ERROR, exception=e)
1976             msg = _('Failed to allocate the network(s) with error %s, '
1977                     'not rescheduling.') % e.format_message()
1978             raise exception.BuildAbortException(instance_uuid=instance.uuid,
1979                     reason=msg)
1980         except (exception.VirtualInterfaceCreateException,
1981                 exception.VirtualInterfaceMacAddressException,
1982                 exception.FixedIpInvalidOnHost,
1983                 exception.UnableToAutoAllocateNetwork) as e:
1984             LOG.exception('Failed to allocate network(s)',
1985                           instance=instance)
1986             self._notify_about_instance_usage(context, instance,
1987                     'create.error', fault=e)
1988             compute_utils.notify_about_instance_create(
1989                     context, instance, self.host,
1990                     phase=fields.NotificationPhase.ERROR, exception=e)
1991             msg = _('Failed to allocate the network(s), not rescheduling.')
1992             raise exception.BuildAbortException(instance_uuid=instance.uuid,
1993                     reason=msg)
1994         except (exception.FlavorDiskTooSmall,
1995                 exception.FlavorMemoryTooSmall,
1996                 exception.ImageNotActive,
1997                 exception.ImageUnacceptable,
1998                 exception.InvalidDiskInfo,
1999                 exception.InvalidDiskFormat,
2000                 cursive_exception.SignatureVerificationError,
2001                 exception.VolumeEncryptionNotSupported,
2002                 exception.InvalidInput) as e:
2003             self._notify_about_instance_usage(context, instance,
2004                     'create.error', fault=e)
2005             compute_utils.notify_about_instance_create(
2006                     context, instance, self.host,
2007                     phase=fields.NotificationPhase.ERROR, exception=e)
2008             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2009                     reason=e.format_message())
2010         except Exception as e:
2011             self._notify_about_instance_usage(context, instance,
2012                     'create.error', fault=e)
2013             compute_utils.notify_about_instance_create(
2014                     context, instance, self.host,
2015                     phase=fields.NotificationPhase.ERROR, exception=e)
2016             raise exception.RescheduledException(
2017                     instance_uuid=instance.uuid, reason=six.text_type(e))
2018 
2019         # NOTE(alaski): This is only useful during reschedules, remove it now.
2020         instance.system_metadata.pop('network_allocated', None)
2021 
2022         # If CONF.default_access_ip_network_name is set, grab the
2023         # corresponding network and set the access ip values accordingly.
2024         network_name = CONF.default_access_ip_network_name
2025         if (network_name and not instance.access_ip_v4 and
2026                 not instance.access_ip_v6):
2027             # Note that when there are multiple ips to choose from, an
2028             # arbitrary one will be chosen.
2029             for vif in network_info:
2030                 if vif['network']['label'] == network_name:
2031                     for ip in vif.fixed_ips():
2032                         if not instance.access_ip_v4 and ip['version'] == 4:
2033                             instance.access_ip_v4 = ip['address']
2034                         if not instance.access_ip_v6 and ip['version'] == 6:
2035                             instance.access_ip_v6 = ip['address']
2036                     break
2037 
2038         self._update_instance_after_spawn(context, instance)
2039 
2040         try:
2041             instance.save(expected_task_state=task_states.SPAWNING)
2042         except (exception.InstanceNotFound,
2043                 exception.UnexpectedDeletingTaskStateError) as e:
2044             with excutils.save_and_reraise_exception():
2045                 self._notify_about_instance_usage(context, instance,
2046                     'create.error', fault=e)
2047                 compute_utils.notify_about_instance_create(
2048                     context, instance, self.host,
2049                     phase=fields.NotificationPhase.ERROR, exception=e)
2050 
2051         self._update_scheduler_instance_info(context, instance)
2052         self._notify_about_instance_usage(context, instance, 'create.end',
2053                 extra_usage_info={'message': _('Success')},
2054                 network_info=network_info)
2055         compute_utils.notify_about_instance_create(context, instance,
2056                 self.host, phase=fields.NotificationPhase.END)
2057 
2058     @contextlib.contextmanager
2059     def _build_resources(self, context, instance, requested_networks,
2060                          security_groups, image_meta, block_device_mapping):
2061         resources = {}
2062         network_info = None
2063         try:
2064             LOG.debug('Start building networks asynchronously for instance.',
2065                       instance=instance)
2066             network_info = self._build_networks_for_instance(context, instance,
2067                     requested_networks, security_groups)
2068             resources['network_info'] = network_info
2069         except (exception.InstanceNotFound,
2070                 exception.UnexpectedDeletingTaskStateError):
2071             raise
2072         except exception.UnexpectedTaskStateError as e:
2073             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2074                     reason=e.format_message())
2075         except Exception:
2076             # Because this allocation is async any failures are likely to occur
2077             # when the driver accesses network_info during spawn().
2078             LOG.exception('Failed to allocate network(s)',
2079                           instance=instance)
2080             msg = _('Failed to allocate the network(s), not rescheduling.')
2081             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2082                     reason=msg)
2083 
2084         try:
2085             # Verify that all the BDMs have a device_name set and assign a
2086             # default to the ones missing it with the help of the driver.
2087             self._default_block_device_names(instance, image_meta,
2088                                              block_device_mapping)
2089 
2090             LOG.debug('Start building block device mappings for instance.',
2091                       instance=instance)
2092             instance.vm_state = vm_states.BUILDING
2093             instance.task_state = task_states.BLOCK_DEVICE_MAPPING
2094             instance.save()
2095 
2096             block_device_info = self._prep_block_device(context, instance,
2097                     block_device_mapping)
2098             resources['block_device_info'] = block_device_info
2099         except (exception.InstanceNotFound,
2100                 exception.UnexpectedDeletingTaskStateError):
2101             with excutils.save_and_reraise_exception():
2102                 # Make sure the async call finishes
2103                 if network_info is not None:
2104                     network_info.wait(do_raise=False)
2105         except (exception.UnexpectedTaskStateError,
2106                 exception.OverQuota, exception.InvalidBDM) as e:
2107             # Make sure the async call finishes
2108             if network_info is not None:
2109                 network_info.wait(do_raise=False)
2110             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2111                     reason=e.format_message())
2112         except Exception:
2113             LOG.exception('Failure prepping block device',
2114                           instance=instance)
2115             # Make sure the async call finishes
2116             if network_info is not None:
2117                 network_info.wait(do_raise=False)
2118             msg = _('Failure prepping block device.')
2119             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2120                     reason=msg)
2121 
2122         try:
2123             yield resources
2124         except Exception as exc:
2125             with excutils.save_and_reraise_exception() as ctxt:
2126                 if not isinstance(exc, (
2127                         exception.InstanceNotFound,
2128                         exception.UnexpectedDeletingTaskStateError)):
2129                     LOG.exception('Instance failed to spawn',
2130                                   instance=instance)
2131                 # Make sure the async call finishes
2132                 if network_info is not None:
2133                     network_info.wait(do_raise=False)
2134                 # if network_info is empty we're likely here because of
2135                 # network allocation failure. Since nothing can be reused on
2136                 # rescheduling it's better to deallocate network to eliminate
2137                 # the chance of orphaned ports in neutron
2138                 deallocate_networks = False if network_info else True
2139                 try:
2140                     self._shutdown_instance(context, instance,
2141                             block_device_mapping, requested_networks,
2142                             try_deallocate_networks=deallocate_networks)
2143                 except Exception as exc2:
2144                     ctxt.reraise = False
2145                     LOG.warning('Could not clean up failed build,'
2146                                 ' not rescheduling. Error: %s',
2147                                 six.text_type(exc2))
2148                     raise exception.BuildAbortException(
2149                             instance_uuid=instance.uuid,
2150                             reason=six.text_type(exc))
2151 
2152     def _cleanup_allocated_networks(self, context, instance,
2153             requested_networks):
2154         try:
2155             self._deallocate_network(context, instance, requested_networks)
2156         except Exception:
2157             LOG.exception('Failed to deallocate networks', instance=instance)
2158             return
2159 
2160         instance.system_metadata['network_allocated'] = 'False'
2161         try:
2162             instance.save()
2163         except exception.InstanceNotFound:
2164             # NOTE(alaski): It's possible that we're cleaning up the networks
2165             # because the instance was deleted.  If that's the case then this
2166             # exception will be raised by instance.save()
2167             pass
2168 
2169     def _try_deallocate_network(self, context, instance,
2170                                 requested_networks=None):
2171         try:
2172             # tear down allocated network structure
2173             self._deallocate_network(context, instance, requested_networks)
2174         except Exception as ex:
2175             with excutils.save_and_reraise_exception():
2176                 LOG.error('Failed to deallocate network for instance. '
2177                           'Error: %s', ex, instance=instance)
2178                 self._set_instance_obj_error_state(context, instance)
2179 
2180     def _get_power_off_values(self, context, instance, clean_shutdown):
2181         """Get the timing configuration for powering down this instance."""
2182         if clean_shutdown:
2183             timeout = compute_utils.get_value_from_system_metadata(instance,
2184                           key='image_os_shutdown_timeout', type=int,
2185                           default=CONF.shutdown_timeout)
2186             retry_interval = self.SHUTDOWN_RETRY_INTERVAL
2187         else:
2188             timeout = 0
2189             retry_interval = 0
2190 
2191         return timeout, retry_interval
2192 
2193     def _power_off_instance(self, context, instance, clean_shutdown=True):
2194         """Power off an instance on this host."""
2195         timeout, retry_interval = self._get_power_off_values(context,
2196                                         instance, clean_shutdown)
2197         self.driver.power_off(instance, timeout, retry_interval)
2198 
2199     def _shutdown_instance(self, context, instance,
2200                            bdms, requested_networks=None, notify=True,
2201                            try_deallocate_networks=True):
2202         """Shutdown an instance on this host.
2203 
2204         :param:context: security context
2205         :param:instance: a nova.objects.Instance object
2206         :param:bdms: the block devices for the instance to be torn
2207                      down
2208         :param:requested_networks: the networks on which the instance
2209                                    has ports
2210         :param:notify: true if a final usage notification should be
2211                        emitted
2212         :param:try_deallocate_networks: false if we should avoid
2213                                         trying to teardown networking
2214         """
2215         context = context.elevated()
2216         LOG.info('Terminating instance', instance=instance)
2217 
2218         if notify:
2219             self._notify_about_instance_usage(context, instance,
2220                                               "shutdown.start")
2221             compute_utils.notify_about_instance_action(context, instance,
2222                     self.host, action=fields.NotificationAction.SHUTDOWN,
2223                     phase=fields.NotificationPhase.START)
2224 
2225         network_info = instance.get_network_info()
2226 
2227         # NOTE(vish) get bdms before destroying the instance
2228         vol_bdms = [bdm for bdm in bdms if bdm.is_volume]
2229         block_device_info = self._get_instance_block_device_info(
2230             context, instance, bdms=bdms)
2231 
2232         # NOTE(melwitt): attempt driver destroy before releasing ip, may
2233         #                want to keep ip allocated for certain failures
2234         timer = timeutils.StopWatch()
2235         try:
2236             LOG.debug('Start destroying the instance on the hypervisor.',
2237                       instance=instance)
2238             timer.start()
2239             self.driver.destroy(context, instance, network_info,
2240                     block_device_info)
2241             LOG.info('Took %0.2f seconds to destroy the instance on the '
2242                      'hypervisor.', timer.elapsed(), instance=instance)
2243         except exception.InstancePowerOffFailure:
2244             # if the instance can't power off, don't release the ip
2245             with excutils.save_and_reraise_exception():
2246                 pass
2247         except Exception:
2248             with excutils.save_and_reraise_exception():
2249                 # deallocate ip and fail without proceeding to
2250                 # volume api calls, preserving current behavior
2251                 if try_deallocate_networks:
2252                     self._try_deallocate_network(context, instance,
2253                                                  requested_networks)
2254 
2255         if try_deallocate_networks:
2256             self._try_deallocate_network(context, instance, requested_networks)
2257 
2258         timer.restart()
2259         for bdm in vol_bdms:
2260             try:
2261                 if bdm.attachment_id:
2262                     self.volume_api.attachment_delete(context,
2263                                                       bdm.attachment_id)
2264                 else:
2265                     # NOTE(vish): actual driver detach done in driver.destroy,
2266                     #             so just tell cinder that we are done with it.
2267                     connector = self.driver.get_volume_connector(instance)
2268                     self.volume_api.terminate_connection(context,
2269                                                          bdm.volume_id,
2270                                                          connector)
2271                     self.volume_api.detach(context, bdm.volume_id,
2272                                            instance.uuid)
2273 
2274             except exception.VolumeAttachmentNotFound as exc:
2275                 LOG.debug('Ignoring VolumeAttachmentNotFound: %s', exc,
2276                           instance=instance)
2277             except exception.DiskNotFound as exc:
2278                 LOG.debug('Ignoring DiskNotFound: %s', exc,
2279                           instance=instance)
2280             except exception.VolumeNotFound as exc:
2281                 LOG.debug('Ignoring VolumeNotFound: %s', exc,
2282                           instance=instance)
2283             except (cinder_exception.EndpointNotFound,
2284                     keystone_exception.EndpointNotFound) as exc:
2285                 LOG.warning('Ignoring EndpointNotFound for '
2286                             'volume %(volume_id)s: %(exc)s',
2287                             {'exc': exc, 'volume_id': bdm.volume_id},
2288                             instance=instance)
2289             except cinder_exception.ClientException as exc:
2290                 LOG.warning('Ignoring unknown cinder exception for '
2291                             'volume %(volume_id)s: %(exc)s',
2292                             {'exc': exc, 'volume_id': bdm.volume_id},
2293                             instance=instance)
2294             except Exception as exc:
2295                 LOG.warning('Ignoring unknown exception for '
2296                             'volume %(volume_id)s: %(exc)s',
2297                             {'exc': exc, 'volume_id': bdm.volume_id},
2298                             instance=instance)
2299         if vol_bdms:
2300             LOG.info('Took %(time).2f seconds to detach %(num)s volumes '
2301                      'for instance.',
2302                      {'time': timer.elapsed(), 'num': len(vol_bdms)},
2303                      instance=instance)
2304 
2305         if notify:
2306             self._notify_about_instance_usage(context, instance,
2307                                               "shutdown.end")
2308             compute_utils.notify_about_instance_action(context, instance,
2309                     self.host, action=fields.NotificationAction.SHUTDOWN,
2310                     phase=fields.NotificationPhase.END)
2311 
2312     def _cleanup_volumes(self, context, instance_uuid, bdms, raise_exc=True):
2313         exc_info = None
2314 
2315         for bdm in bdms:
2316             LOG.debug("terminating bdm %s", bdm,
2317                       instance_uuid=instance_uuid)
2318             if bdm.volume_id and bdm.delete_on_termination:
2319                 try:
2320                     self.volume_api.delete(context, bdm.volume_id)
2321                 except Exception as exc:
2322                     exc_info = sys.exc_info()
2323                     LOG.warning('Failed to delete volume: %(volume_id)s '
2324                                 'due to %(exc)s',
2325                                 {'volume_id': bdm.volume_id, 'exc': exc})
2326         if exc_info is not None and raise_exc:
2327             six.reraise(exc_info[0], exc_info[1], exc_info[2])
2328 
2329     @hooks.add_hook("delete_instance")
2330     def _delete_instance(self, context, instance, bdms):
2331         """Delete an instance on this host.
2332 
2333         :param context: nova request context
2334         :param instance: nova.objects.instance.Instance object
2335         :param bdms: nova.objects.block_device.BlockDeviceMappingList object
2336         """
2337         events = self.instance_events.clear_events_for_instance(instance)
2338         if events:
2339             LOG.debug('Events pending at deletion: %(events)s',
2340                       {'events': ','.join(events.keys())},
2341                       instance=instance)
2342         self._notify_about_instance_usage(context, instance,
2343                                           "delete.start")
2344         compute_utils.notify_about_instance_action(context, instance,
2345                 self.host, action=fields.NotificationAction.DELETE,
2346                 phase=fields.NotificationPhase.START)
2347 
2348         self._shutdown_instance(context, instance, bdms)
2349         # NOTE(dims): instance.info_cache.delete() should be called after
2350         # _shutdown_instance in the compute manager as shutdown calls
2351         # deallocate_for_instance so the info_cache is still needed
2352         # at this point.
2353         if instance.info_cache is not None:
2354             instance.info_cache.delete()
2355         else:
2356             # NOTE(yoshimatsu): Avoid AttributeError if instance.info_cache
2357             # is None. When the root cause that instance.info_cache becomes
2358             # None is fixed, the log level should be reconsidered.
2359             LOG.warning("Info cache for instance could not be found. "
2360                         "Ignore.", instance=instance)
2361 
2362         # NOTE(vish): We have already deleted the instance, so we have
2363         #             to ignore problems cleaning up the volumes. It
2364         #             would be nice to let the user know somehow that
2365         #             the volume deletion failed, but it is not
2366         #             acceptable to have an instance that can not be
2367         #             deleted. Perhaps this could be reworked in the
2368         #             future to set an instance fault the first time
2369         #             and to only ignore the failure if the instance
2370         #             is already in ERROR.
2371         self._cleanup_volumes(context, instance.uuid, bdms,
2372                 raise_exc=False)
2373         # if a delete task succeeded, always update vm state and task
2374         # state without expecting task state to be DELETING
2375         instance.vm_state = vm_states.DELETED
2376         instance.task_state = None
2377         instance.power_state = power_state.NOSTATE
2378         instance.terminated_at = timeutils.utcnow()
2379         instance.save()
2380         system_meta = instance.system_metadata
2381         instance.destroy()
2382 
2383         self._complete_deletion(context,
2384                                 instance,
2385                                 bdms,
2386                                 system_meta)
2387 
2388     @wrap_exception()
2389     @reverts_task_state
2390     @wrap_instance_event(prefix='compute')
2391     @wrap_instance_fault
2392     def terminate_instance(self, context, instance, bdms, reservations):
2393         """Terminate an instance on this host."""
2394         @utils.synchronized(instance.uuid)
2395         def do_terminate_instance(instance, bdms):
2396             # NOTE(mriedem): If we are deleting the instance while it was
2397             # booting from volume, we could be racing with a database update of
2398             # the BDM volume_id. Since the compute API passes the BDMs over RPC
2399             # to compute here, the BDMs may be stale at this point. So check
2400             # for any volume BDMs that don't have volume_id set and if we
2401             # detect that, we need to refresh the BDM list before proceeding.
2402             # TODO(mriedem): Move this into _delete_instance and make the bdms
2403             # parameter optional.
2404             for bdm in list(bdms):
2405                 if bdm.is_volume and not bdm.volume_id:
2406                     LOG.debug('There are potentially stale BDMs during '
2407                               'delete, refreshing the BlockDeviceMappingList.',
2408                               instance=instance)
2409                     bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
2410                         context, instance.uuid)
2411                     break
2412             try:
2413                 self._delete_instance(context, instance, bdms)
2414             except exception.InstanceNotFound:
2415                 LOG.info("Instance disappeared during terminate",
2416                          instance=instance)
2417             except Exception:
2418                 # As we're trying to delete always go to Error if something
2419                 # goes wrong that _delete_instance can't handle.
2420                 with excutils.save_and_reraise_exception():
2421                     LOG.exception('Setting instance vm_state to ERROR',
2422                                   instance=instance)
2423                     self._set_instance_obj_error_state(context, instance)
2424 
2425         do_terminate_instance(instance, bdms)
2426 
2427     # NOTE(johannes): This is probably better named power_off_instance
2428     # so it matches the driver method, but because of other issues, we
2429     # can't use that name in grizzly.
2430     @wrap_exception()
2431     @reverts_task_state
2432     @wrap_instance_event(prefix='compute')
2433     @wrap_instance_fault
2434     def stop_instance(self, context, instance, clean_shutdown):
2435         """Stopping an instance on this host."""
2436 
2437         @utils.synchronized(instance.uuid)
2438         def do_stop_instance():
2439             current_power_state = self._get_power_state(context, instance)
2440             LOG.debug('Stopping instance; current vm_state: %(vm_state)s, '
2441                       'current task_state: %(task_state)s, current DB '
2442                       'power_state: %(db_power_state)s, current VM '
2443                       'power_state: %(current_power_state)s',
2444                       {'vm_state': instance.vm_state,
2445                        'task_state': instance.task_state,
2446                        'db_power_state': instance.power_state,
2447                        'current_power_state': current_power_state},
2448                       instance_uuid=instance.uuid)
2449 
2450             # NOTE(mriedem): If the instance is already powered off, we are
2451             # possibly tearing down and racing with other operations, so we can
2452             # expect the task_state to be None if something else updates the
2453             # instance and we're not locking it.
2454             expected_task_state = [task_states.POWERING_OFF]
2455             # The list of power states is from _sync_instance_power_state.
2456             if current_power_state in (power_state.NOSTATE,
2457                                        power_state.SHUTDOWN,
2458                                        power_state.CRASHED):
2459                 LOG.info('Instance is already powered off in the '
2460                          'hypervisor when stop is called.',
2461                          instance=instance)
2462                 expected_task_state.append(None)
2463 
2464             self._notify_about_instance_usage(context, instance,
2465                                               "power_off.start")
2466 
2467             compute_utils.notify_about_instance_action(context, instance,
2468                         self.host, action=fields.NotificationAction.POWER_OFF,
2469                         phase=fields.NotificationPhase.START)
2470 
2471             self._power_off_instance(context, instance, clean_shutdown)
2472             instance.power_state = self._get_power_state(context, instance)
2473             instance.vm_state = vm_states.STOPPED
2474             instance.task_state = None
2475             instance.save(expected_task_state=expected_task_state)
2476             self._notify_about_instance_usage(context, instance,
2477                                               "power_off.end")
2478 
2479             compute_utils.notify_about_instance_action(context, instance,
2480                         self.host, action=fields.NotificationAction.POWER_OFF,
2481                         phase=fields.NotificationPhase.END)
2482 
2483         do_stop_instance()
2484 
2485     def _power_on(self, context, instance):
2486         network_info = self.network_api.get_instance_nw_info(context, instance)
2487         block_device_info = self._get_instance_block_device_info(context,
2488                                                                  instance)
2489         self.driver.power_on(context, instance,
2490                              network_info,
2491                              block_device_info)
2492 
2493     def _delete_snapshot_of_shelved_instance(self, context, instance,
2494                                              snapshot_id):
2495         """Delete snapshot of shelved instance."""
2496         try:
2497             self.image_api.delete(context, snapshot_id)
2498         except (exception.ImageNotFound,
2499                 exception.ImageNotAuthorized) as exc:
2500             LOG.warning("Failed to delete snapshot "
2501                         "from shelved instance (%s).",
2502                         exc.format_message(), instance=instance)
2503         except Exception:
2504             LOG.exception("Something wrong happened when trying to "
2505                           "delete snapshot from shelved instance.",
2506                           instance=instance)
2507 
2508     # NOTE(johannes): This is probably better named power_on_instance
2509     # so it matches the driver method, but because of other issues, we
2510     # can't use that name in grizzly.
2511     @wrap_exception()
2512     @reverts_task_state
2513     @wrap_instance_event(prefix='compute')
2514     @wrap_instance_fault
2515     def start_instance(self, context, instance):
2516         """Starting an instance on this host."""
2517         self._notify_about_instance_usage(context, instance, "power_on.start")
2518         compute_utils.notify_about_instance_action(context, instance,
2519             self.host, action=fields.NotificationAction.POWER_ON,
2520             phase=fields.NotificationPhase.START)
2521         self._power_on(context, instance)
2522         instance.power_state = self._get_power_state(context, instance)
2523         instance.vm_state = vm_states.ACTIVE
2524         instance.task_state = None
2525 
2526         # Delete an image(VM snapshot) for a shelved instance
2527         snapshot_id = instance.system_metadata.get('shelved_image_id')
2528         if snapshot_id:
2529             self._delete_snapshot_of_shelved_instance(context, instance,
2530                                                       snapshot_id)
2531 
2532         # Delete system_metadata for a shelved instance
2533         compute_utils.remove_shelved_keys_from_system_metadata(instance)
2534 
2535         instance.save(expected_task_state=task_states.POWERING_ON)
2536         self._notify_about_instance_usage(context, instance, "power_on.end")
2537         compute_utils.notify_about_instance_action(context, instance,
2538             self.host, action=fields.NotificationAction.POWER_ON,
2539             phase=fields.NotificationPhase.END)
2540 
2541     @messaging.expected_exceptions(NotImplementedError,
2542                                    exception.TriggerCrashDumpNotSupported,
2543                                    exception.InstanceNotRunning)
2544     @wrap_exception()
2545     @wrap_instance_event(prefix='compute')
2546     @wrap_instance_fault
2547     def trigger_crash_dump(self, context, instance):
2548         """Trigger crash dump in an instance."""
2549 
2550         self._notify_about_instance_usage(context, instance,
2551                                           "trigger_crash_dump.start")
2552 
2553         # This method does not change task_state and power_state because the
2554         # effect of a trigger depends on user's configuration.
2555         self.driver.trigger_crash_dump(instance)
2556 
2557         self._notify_about_instance_usage(context, instance,
2558                                           "trigger_crash_dump.end")
2559 
2560     @wrap_exception()
2561     @reverts_task_state
2562     @wrap_instance_event(prefix='compute')
2563     @wrap_instance_fault
2564     def soft_delete_instance(self, context, instance, reservations):
2565         """Soft delete an instance on this host."""
2566         self._notify_about_instance_usage(context, instance,
2567                                           "soft_delete.start")
2568         compute_utils.notify_about_instance_action(context, instance,
2569             self.host, action=fields.NotificationAction.SOFT_DELETE,
2570             phase=fields.NotificationPhase.START)
2571         try:
2572             self.driver.soft_delete(instance)
2573         except NotImplementedError:
2574             # Fallback to just powering off the instance if the
2575             # hypervisor doesn't implement the soft_delete method
2576             self.driver.power_off(instance)
2577         instance.power_state = self._get_power_state(context, instance)
2578         instance.vm_state = vm_states.SOFT_DELETED
2579         instance.task_state = None
2580         instance.save(expected_task_state=[task_states.SOFT_DELETING])
2581         self._notify_about_instance_usage(context, instance, "soft_delete.end")
2582         compute_utils.notify_about_instance_action(context, instance,
2583             self.host, action=fields.NotificationAction.SOFT_DELETE,
2584             phase=fields.NotificationPhase.END)
2585 
2586     @wrap_exception()
2587     @reverts_task_state
2588     @wrap_instance_event(prefix='compute')
2589     @wrap_instance_fault
2590     def restore_instance(self, context, instance):
2591         """Restore a soft-deleted instance on this host."""
2592         self._notify_about_instance_usage(context, instance, "restore.start")
2593         compute_utils.notify_about_instance_action(context, instance,
2594             self.host, action=fields.NotificationAction.RESTORE,
2595             phase=fields.NotificationPhase.START)
2596         try:
2597             self.driver.restore(instance)
2598         except NotImplementedError:
2599             # Fallback to just powering on the instance if the hypervisor
2600             # doesn't implement the restore method
2601             self._power_on(context, instance)
2602         instance.power_state = self._get_power_state(context, instance)
2603         instance.vm_state = vm_states.ACTIVE
2604         instance.task_state = None
2605         instance.save(expected_task_state=task_states.RESTORING)
2606         self._notify_about_instance_usage(context, instance, "restore.end")
2607         compute_utils.notify_about_instance_action(context, instance,
2608             self.host, action=fields.NotificationAction.RESTORE,
2609             phase=fields.NotificationPhase.END)
2610 
2611     @staticmethod
2612     def _set_migration_status(migration, status):
2613         """Set the status, and guard against a None being passed in.
2614 
2615         This is useful as some of the compute RPC calls will not pass
2616         a migration object in older versions. The check can be removed when
2617         we move past 4.x major version of the RPC API.
2618         """
2619         if migration:
2620             migration.status = status
2621             migration.save()
2622 
2623     def _rebuild_default_impl(self, context, instance, image_meta,
2624                               injected_files, admin_password, bdms,
2625                               detach_block_devices, attach_block_devices,
2626                               network_info=None,
2627                               recreate=False, block_device_info=None,
2628                               preserve_ephemeral=False):
2629         if preserve_ephemeral:
2630             # The default code path does not support preserving ephemeral
2631             # partitions.
2632             raise exception.PreserveEphemeralNotSupported()
2633 
2634         if recreate:
2635             detach_block_devices(context, bdms)
2636         else:
2637             self._power_off_instance(context, instance, clean_shutdown=True)
2638             detach_block_devices(context, bdms)
2639             self.driver.destroy(context, instance,
2640                                 network_info=network_info,
2641                                 block_device_info=block_device_info)
2642 
2643         instance.task_state = task_states.REBUILD_BLOCK_DEVICE_MAPPING
2644         instance.save(expected_task_state=[task_states.REBUILDING])
2645 
2646         new_block_device_info = attach_block_devices(context, instance, bdms)
2647 
2648         instance.task_state = task_states.REBUILD_SPAWNING
2649         instance.save(
2650             expected_task_state=[task_states.REBUILD_BLOCK_DEVICE_MAPPING])
2651 
2652         with instance.mutated_migration_context():
2653             self.driver.spawn(context, instance, image_meta, injected_files,
2654                               admin_password, network_info=network_info,
2655                               block_device_info=new_block_device_info)
2656 
2657     def _notify_instance_rebuild_error(self, context, instance, error):
2658         self._notify_about_instance_usage(context, instance,
2659                                           'rebuild.error', fault=error)
2660         compute_utils.notify_about_instance_action(
2661             context, instance, self.host,
2662             action=fields.NotificationAction.REBUILD,
2663             phase=fields.NotificationPhase.ERROR, exception=error)
2664 
2665     @messaging.expected_exceptions(exception.PreserveEphemeralNotSupported)
2666     @wrap_exception()
2667     @reverts_task_state
2668     @wrap_instance_event(prefix='compute')
2669     @wrap_instance_fault
2670     def rebuild_instance(self, context, instance, orig_image_ref, image_ref,
2671                          injected_files, new_pass, orig_sys_metadata,
2672                          bdms, recreate, on_shared_storage=None,
2673                          preserve_ephemeral=False, migration=None,
2674                          scheduled_node=None, limits=None):
2675         """Destroy and re-make this instance.
2676 
2677         A 'rebuild' effectively purges all existing data from the system and
2678         remakes the VM with given 'metadata' and 'personalities'.
2679 
2680         :param context: `nova.RequestContext` object
2681         :param instance: Instance object
2682         :param orig_image_ref: Original image_ref before rebuild
2683         :param image_ref: New image_ref for rebuild
2684         :param injected_files: Files to inject
2685         :param new_pass: password to set on rebuilt instance
2686         :param orig_sys_metadata: instance system metadata from pre-rebuild
2687         :param bdms: block-device-mappings to use for rebuild
2688         :param recreate: True if the instance is being recreated (e.g. the
2689             hypervisor it was on failed) - cleanup of old state will be
2690             skipped.
2691         :param on_shared_storage: True if instance files on shared storage.
2692                                   If not provided then information from the
2693                                   driver will be used to decide if the instance
2694                                   files are available or not on the target host
2695         :param preserve_ephemeral: True if the default ephemeral storage
2696                                    partition must be preserved on rebuild
2697         :param migration: a Migration object if one was created for this
2698                           rebuild operation (if it's a part of evacuate)
2699         :param scheduled_node: A node of the host chosen by the scheduler. If a
2700                                host was specified by the user, this will be
2701                                None
2702         :param limits: Overcommit limits set by the scheduler. If a host was
2703                        specified by the user, this will be None
2704         """
2705         context = context.elevated()
2706 
2707         LOG.info("Rebuilding instance", instance=instance)
2708 
2709         # NOTE(gyee): there are three possible scenarios.
2710         #
2711         #   1. instance is being rebuilt on the same node. In this case,
2712         #      recreate should be False and scheduled_node should be None.
2713         #   2. instance is being rebuilt on a node chosen by the
2714         #      scheduler (i.e. evacuate). In this case, scheduled_node should
2715         #      be specified and recreate should be True.
2716         #   3. instance is being rebuilt on a node chosen by the user. (i.e.
2717         #      force evacuate). In this case, scheduled_node is not specified
2718         #      and recreate is set to True.
2719         #
2720         # For scenarios #2 and #3, we must do rebuild claim as server is
2721         # being evacuated to a different node.
2722         if recreate or scheduled_node is not None:
2723             rt = self._get_resource_tracker()
2724             rebuild_claim = rt.rebuild_claim
2725         else:
2726             rebuild_claim = claims.NopClaim
2727 
2728         image_meta = {}
2729         if image_ref:
2730             image_meta = self.image_api.get(context, image_ref)
2731 
2732         # NOTE(mriedem): On a recreate (evacuate), we need to update
2733         # the instance's host and node properties to reflect it's
2734         # destination node for the recreate.
2735         if not scheduled_node:
2736             if recreate:
2737                 try:
2738                     compute_node = self._get_compute_info(context, self.host)
2739                     scheduled_node = compute_node.hypervisor_hostname
2740                 except exception.ComputeHostNotFound:
2741                     LOG.exception('Failed to get compute_info for %s',
2742                                   self.host)
2743             else:
2744                 scheduled_node = instance.node
2745 
2746         with self._error_out_instance_on_exception(context, instance):
2747             try:
2748                 claim_ctxt = rebuild_claim(
2749                     context, instance, scheduled_node,
2750                     limits=limits, image_meta=image_meta,
2751                     migration=migration)
2752                 self._do_rebuild_instance_with_claim(
2753                     claim_ctxt, context, instance, orig_image_ref,
2754                     image_ref, injected_files, new_pass, orig_sys_metadata,
2755                     bdms, recreate, on_shared_storage, preserve_ephemeral)
2756             except exception.ComputeResourcesUnavailable as e:
2757                 LOG.debug("Could not rebuild instance on this host, not "
2758                           "enough resources available.", instance=instance)
2759 
2760                 # NOTE(ndipanov): We just abort the build for now and leave a
2761                 # migration record for potential cleanup later
2762                 self._set_migration_status(migration, 'failed')
2763                 self._notify_instance_rebuild_error(context, instance, e)
2764 
2765                 raise exception.BuildAbortException(
2766                     instance_uuid=instance.uuid, reason=e.format_message())
2767             except (exception.InstanceNotFound,
2768                     exception.UnexpectedDeletingTaskStateError) as e:
2769                 LOG.debug('Instance was deleted while rebuilding',
2770                           instance=instance)
2771                 self._set_migration_status(migration, 'failed')
2772                 self._notify_instance_rebuild_error(context, instance, e)
2773             except Exception as e:
2774                 self._set_migration_status(migration, 'failed')
2775                 self._notify_instance_rebuild_error(context, instance, e)
2776                 raise
2777             else:
2778                 instance.apply_migration_context()
2779                 # NOTE (ndipanov): This save will now update the host and node
2780                 # attributes making sure that next RT pass is consistent since
2781                 # it will be based on the instance and not the migration DB
2782                 # entry.
2783                 instance.host = self.host
2784                 instance.node = scheduled_node
2785                 instance.save()
2786                 instance.drop_migration_context()
2787 
2788                 # NOTE (ndipanov): Mark the migration as done only after we
2789                 # mark the instance as belonging to this host.
2790                 self._set_migration_status(migration, 'done')
2791 
2792     def _do_rebuild_instance_with_claim(self, claim_context, *args, **kwargs):
2793         """Helper to avoid deep nesting in the top-level method."""
2794 
2795         with claim_context:
2796             self._do_rebuild_instance(*args, **kwargs)
2797 
2798     @staticmethod
2799     def _get_image_name(image_meta):
2800         if image_meta.obj_attr_is_set("name"):
2801             return image_meta.name
2802         else:
2803             return ''
2804 
2805     def _do_rebuild_instance(self, context, instance, orig_image_ref,
2806                              image_ref, injected_files, new_pass,
2807                              orig_sys_metadata, bdms, recreate,
2808                              on_shared_storage, preserve_ephemeral):
2809         orig_vm_state = instance.vm_state
2810 
2811         if recreate:
2812             if not self.driver.capabilities["supports_recreate"]:
2813                 raise exception.InstanceRecreateNotSupported
2814 
2815             self._check_instance_exists(context, instance)
2816 
2817             if on_shared_storage is None:
2818                 LOG.debug('on_shared_storage is not provided, using driver'
2819                             'information to decide if the instance needs to'
2820                             'be recreated')
2821                 on_shared_storage = self.driver.instance_on_disk(instance)
2822 
2823             elif (on_shared_storage !=
2824                     self.driver.instance_on_disk(instance)):
2825                 # To cover case when admin expects that instance files are
2826                 # on shared storage, but not accessible and vice versa
2827                 raise exception.InvalidSharedStorage(
2828                         _("Invalid state of instance files on shared"
2829                             " storage"))
2830 
2831             if on_shared_storage:
2832                 LOG.info('disk on shared storage, recreating using'
2833                          ' existing disk')
2834             else:
2835                 image_ref = orig_image_ref = instance.image_ref
2836                 LOG.info("disk not on shared storage, rebuilding from:"
2837                          " '%s'", str(image_ref))
2838 
2839         if image_ref:
2840             image_meta = objects.ImageMeta.from_image_ref(
2841                 context, self.image_api, image_ref)
2842         else:
2843             image_meta = instance.image_meta
2844 
2845         # This instance.exists message should contain the original
2846         # image_ref, not the new one.  Since the DB has been updated
2847         # to point to the new one... we have to override it.
2848         # TODO(jaypipes): Move generate_image_url() into the nova.image.api
2849         orig_image_ref_url = glance.generate_image_url(orig_image_ref)
2850         extra_usage_info = {'image_ref_url': orig_image_ref_url}
2851         compute_utils.notify_usage_exists(
2852                 self.notifier, context, instance,
2853                 current_period=True, system_metadata=orig_sys_metadata,
2854                 extra_usage_info=extra_usage_info)
2855 
2856         # This message should contain the new image_ref
2857         extra_usage_info = {'image_name': self._get_image_name(image_meta)}
2858         self._notify_about_instance_usage(context, instance,
2859                 "rebuild.start", extra_usage_info=extra_usage_info)
2860         # NOTE: image_name is not included in the versioned notification
2861         # because we already provide the image_uuid in the notification
2862         # payload and the image details can be looked up via the uuid.
2863         compute_utils.notify_about_instance_action(
2864             context, instance, self.host,
2865             action=fields.NotificationAction.REBUILD,
2866             phase=fields.NotificationPhase.START)
2867 
2868         instance.power_state = self._get_power_state(context, instance)
2869         instance.task_state = task_states.REBUILDING
2870         instance.save(expected_task_state=[task_states.REBUILDING])
2871 
2872         if recreate:
2873             self.network_api.setup_networks_on_host(
2874                     context, instance, self.host)
2875             # For nova-network this is needed to move floating IPs
2876             # For neutron this updates the host in the port binding
2877             # TODO(cfriesen): this network_api call and the one above
2878             # are so similar, we should really try to unify them.
2879             self.network_api.setup_instance_network_on_host(
2880                     context, instance, self.host)
2881 
2882         network_info = instance.get_network_info()
2883         if bdms is None:
2884             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
2885                     context, instance.uuid)
2886 
2887         block_device_info = \
2888             self._get_instance_block_device_info(
2889                     context, instance, bdms=bdms)
2890 
2891         def detach_block_devices(context, bdms):
2892             for bdm in bdms:
2893                 if bdm.is_volume:
2894                     self._detach_volume(context, bdm, instance,
2895                                         destroy_bdm=False)
2896 
2897         files = self._decode_files(injected_files)
2898 
2899         kwargs = dict(
2900             context=context,
2901             instance=instance,
2902             image_meta=image_meta,
2903             injected_files=files,
2904             admin_password=new_pass,
2905             bdms=bdms,
2906             detach_block_devices=detach_block_devices,
2907             attach_block_devices=self._prep_block_device,
2908             block_device_info=block_device_info,
2909             network_info=network_info,
2910             preserve_ephemeral=preserve_ephemeral,
2911             recreate=recreate)
2912         try:
2913             with instance.mutated_migration_context():
2914                 self.driver.rebuild(**kwargs)
2915         except NotImplementedError:
2916             # NOTE(rpodolyaka): driver doesn't provide specialized version
2917             # of rebuild, fall back to the default implementation
2918             self._rebuild_default_impl(**kwargs)
2919         self._update_instance_after_spawn(context, instance)
2920         instance.save(expected_task_state=[task_states.REBUILD_SPAWNING])
2921 
2922         if orig_vm_state == vm_states.STOPPED:
2923             LOG.info("bringing vm to original state: '%s'",
2924                      orig_vm_state, instance=instance)
2925             instance.vm_state = vm_states.ACTIVE
2926             instance.task_state = task_states.POWERING_OFF
2927             instance.progress = 0
2928             instance.save()
2929             self.stop_instance(context, instance, False)
2930         self._update_scheduler_instance_info(context, instance)
2931         self._notify_about_instance_usage(
2932                 context, instance, "rebuild.end",
2933                 network_info=network_info,
2934                 extra_usage_info=extra_usage_info)
2935         compute_utils.notify_about_instance_action(
2936             context, instance, self.host,
2937             action=fields.NotificationAction.REBUILD,
2938             phase=fields.NotificationPhase.END)
2939 
2940     def _handle_bad_volumes_detached(self, context, instance, bad_devices,
2941                                      block_device_info):
2942         """Handle cases where the virt-layer had to detach non-working volumes
2943         in order to complete an operation.
2944         """
2945         for bdm in block_device_info['block_device_mapping']:
2946             if bdm.get('mount_device') in bad_devices:
2947                 try:
2948                     volume_id = bdm['connection_info']['data']['volume_id']
2949                 except KeyError:
2950                     continue
2951 
2952                 # NOTE(sirp): ideally we'd just call
2953                 # `compute_api.detach_volume` here but since that hits the
2954                 # DB directly, that's off limits from within the
2955                 # compute-manager.
2956                 #
2957                 # API-detach
2958                 LOG.info("Detaching from volume api: %s", volume_id)
2959                 self.volume_api.begin_detaching(context, volume_id)
2960 
2961                 # Manager-detach
2962                 self.detach_volume(context, volume_id, instance)
2963 
2964     @wrap_exception()
2965     @reverts_task_state
2966     @wrap_instance_event(prefix='compute')
2967     @wrap_instance_fault
2968     def reboot_instance(self, context, instance, block_device_info,
2969                         reboot_type):
2970         """Reboot an instance on this host."""
2971         # acknowledge the request made it to the manager
2972         if reboot_type == "SOFT":
2973             instance.task_state = task_states.REBOOT_PENDING
2974             expected_states = (task_states.REBOOTING,
2975                                task_states.REBOOT_PENDING,
2976                                task_states.REBOOT_STARTED)
2977         else:
2978             instance.task_state = task_states.REBOOT_PENDING_HARD
2979             expected_states = (task_states.REBOOTING_HARD,
2980                                task_states.REBOOT_PENDING_HARD,
2981                                task_states.REBOOT_STARTED_HARD)
2982         context = context.elevated()
2983         LOG.info("Rebooting instance", instance=instance)
2984 
2985         block_device_info = self._get_instance_block_device_info(context,
2986                                                                  instance)
2987 
2988         network_info = self.network_api.get_instance_nw_info(context, instance)
2989 
2990         self._notify_about_instance_usage(context, instance, "reboot.start")
2991         compute_utils.notify_about_instance_action(
2992             context, instance, self.host,
2993             action=fields.NotificationAction.REBOOT,
2994             phase=fields.NotificationPhase.START
2995         )
2996 
2997         instance.power_state = self._get_power_state(context, instance)
2998         instance.save(expected_task_state=expected_states)
2999 
3000         if instance.power_state != power_state.RUNNING:
3001             state = instance.power_state
3002             running = power_state.RUNNING
3003             LOG.warning('trying to reboot a non-running instance:'
3004                         ' (state: %(state)s expected: %(running)s)',
3005                         {'state': state, 'running': running},
3006                         instance=instance)
3007 
3008         def bad_volumes_callback(bad_devices):
3009             self._handle_bad_volumes_detached(
3010                     context, instance, bad_devices, block_device_info)
3011 
3012         try:
3013             # Don't change it out of rescue mode
3014             if instance.vm_state == vm_states.RESCUED:
3015                 new_vm_state = vm_states.RESCUED
3016             else:
3017                 new_vm_state = vm_states.ACTIVE
3018             new_power_state = None
3019             if reboot_type == "SOFT":
3020                 instance.task_state = task_states.REBOOT_STARTED
3021                 expected_state = task_states.REBOOT_PENDING
3022             else:
3023                 instance.task_state = task_states.REBOOT_STARTED_HARD
3024                 expected_state = task_states.REBOOT_PENDING_HARD
3025             instance.save(expected_task_state=expected_state)
3026             self.driver.reboot(context, instance,
3027                                network_info,
3028                                reboot_type,
3029                                block_device_info=block_device_info,
3030                                bad_volumes_callback=bad_volumes_callback)
3031 
3032         except Exception as error:
3033             with excutils.save_and_reraise_exception() as ctxt:
3034                 exc_info = sys.exc_info()
3035                 # if the reboot failed but the VM is running don't
3036                 # put it into an error state
3037                 new_power_state = self._get_power_state(context, instance)
3038                 if new_power_state == power_state.RUNNING:
3039                     LOG.warning('Reboot failed but instance is running',
3040                                 instance=instance)
3041                     compute_utils.add_instance_fault_from_exc(context,
3042                             instance, error, exc_info)
3043                     self._notify_about_instance_usage(context, instance,
3044                             'reboot.error', fault=error)
3045                     compute_utils.notify_about_instance_action(
3046                         context, instance, self.host,
3047                         action=fields.NotificationAction.REBOOT,
3048                         phase=fields.NotificationPhase.ERROR,
3049                         exception=error
3050                     )
3051                     ctxt.reraise = False
3052                 else:
3053                     LOG.error('Cannot reboot instance: %s', error,
3054                               instance=instance)
3055                     self._set_instance_obj_error_state(context, instance)
3056 
3057         if not new_power_state:
3058             new_power_state = self._get_power_state(context, instance)
3059         try:
3060             instance.power_state = new_power_state
3061             instance.vm_state = new_vm_state
3062             instance.task_state = None
3063             instance.save()
3064         except exception.InstanceNotFound:
3065             LOG.warning("Instance disappeared during reboot",
3066                         instance=instance)
3067 
3068         self._notify_about_instance_usage(context, instance, "reboot.end")
3069         compute_utils.notify_about_instance_action(
3070             context, instance, self.host,
3071             action=fields.NotificationAction.REBOOT,
3072             phase=fields.NotificationPhase.END
3073         )
3074 
3075     @delete_image_on_error
3076     def _do_snapshot_instance(self, context, image_id, instance):
3077         self._snapshot_instance(context, image_id, instance,
3078                                 task_states.IMAGE_BACKUP)
3079 
3080     @wrap_exception()
3081     @reverts_task_state
3082     @wrap_instance_fault
3083     def backup_instance(self, context, image_id, instance, backup_type,
3084                         rotation):
3085         """Backup an instance on this host.
3086 
3087         :param backup_type: daily | weekly
3088         :param rotation: int representing how many backups to keep around
3089         """
3090         self._do_snapshot_instance(context, image_id, instance)
3091         self._rotate_backups(context, instance, backup_type, rotation)
3092 
3093     @wrap_exception()
3094     @reverts_task_state
3095     @wrap_instance_fault
3096     @delete_image_on_error
3097     def snapshot_instance(self, context, image_id, instance):
3098         """Snapshot an instance on this host.
3099 
3100         :param context: security context
3101         :param image_id: glance.db.sqlalchemy.models.Image.Id
3102         :param instance: a nova.objects.instance.Instance object
3103         """
3104         # NOTE(dave-mcnally) the task state will already be set by the api
3105         # but if the compute manager has crashed/been restarted prior to the
3106         # request getting here the task state may have been cleared so we set
3107         # it again and things continue normally
3108         try:
3109             instance.task_state = task_states.IMAGE_SNAPSHOT
3110             instance.save(
3111                         expected_task_state=task_states.IMAGE_SNAPSHOT_PENDING)
3112         except exception.InstanceNotFound:
3113             # possibility instance no longer exists, no point in continuing
3114             LOG.debug("Instance not found, could not set state %s "
3115                       "for instance.",
3116                       task_states.IMAGE_SNAPSHOT, instance=instance)
3117             return
3118 
3119         except exception.UnexpectedDeletingTaskStateError:
3120             LOG.debug("Instance being deleted, snapshot cannot continue",
3121                       instance=instance)
3122             return
3123 
3124         self._snapshot_instance(context, image_id, instance,
3125                                 task_states.IMAGE_SNAPSHOT)
3126 
3127     def _snapshot_instance(self, context, image_id, instance,
3128                            expected_task_state):
3129         context = context.elevated()
3130 
3131         instance.power_state = self._get_power_state(context, instance)
3132         try:
3133             instance.save()
3134 
3135             LOG.info('instance snapshotting', instance=instance)
3136 
3137             if instance.power_state != power_state.RUNNING:
3138                 state = instance.power_state
3139                 running = power_state.RUNNING
3140                 LOG.warning('trying to snapshot a non-running instance: '
3141                             '(state: %(state)s expected: %(running)s)',
3142                             {'state': state, 'running': running},
3143                             instance=instance)
3144 
3145             self._notify_about_instance_usage(
3146                 context, instance, "snapshot.start")
3147             compute_utils.notify_about_instance_action(context, instance,
3148                 self.host, action=fields.NotificationAction.SNAPSHOT,
3149                 phase=fields.NotificationPhase.START)
3150 
3151             def update_task_state(task_state,
3152                                   expected_state=expected_task_state):
3153                 instance.task_state = task_state
3154                 instance.save(expected_task_state=expected_state)
3155 
3156             self.driver.snapshot(context, instance, image_id,
3157                                  update_task_state)
3158 
3159             instance.task_state = None
3160             instance.save(expected_task_state=task_states.IMAGE_UPLOADING)
3161 
3162             self._notify_about_instance_usage(context, instance,
3163                                               "snapshot.end")
3164             compute_utils.notify_about_instance_action(context, instance,
3165                 self.host, action=fields.NotificationAction.SNAPSHOT,
3166                 phase=fields.NotificationPhase.END)
3167         except (exception.InstanceNotFound,
3168                 exception.UnexpectedDeletingTaskStateError):
3169             # the instance got deleted during the snapshot
3170             # Quickly bail out of here
3171             msg = 'Instance disappeared during snapshot'
3172             LOG.debug(msg, instance=instance)
3173             try:
3174                 image_service = glance.get_default_image_service()
3175                 image = image_service.show(context, image_id)
3176                 if image['status'] != 'active':
3177                     image_service.delete(context, image_id)
3178             except Exception:
3179                 LOG.warning("Error while trying to clean up image %s",
3180                             image_id, instance=instance)
3181         except exception.ImageNotFound:
3182             instance.task_state = None
3183             instance.save()
3184             LOG.warning("Image not found during snapshot", instance=instance)
3185 
3186     def _post_interrupted_snapshot_cleanup(self, context, instance):
3187         self.driver.post_interrupted_snapshot_cleanup(context, instance)
3188 
3189     @messaging.expected_exceptions(NotImplementedError)
3190     @wrap_exception()
3191     def volume_snapshot_create(self, context, instance, volume_id,
3192                                create_info):
3193         self.driver.volume_snapshot_create(context, instance, volume_id,
3194                                            create_info)
3195 
3196     @messaging.expected_exceptions(NotImplementedError)
3197     @wrap_exception()
3198     def volume_snapshot_delete(self, context, instance, volume_id,
3199                                snapshot_id, delete_info):
3200         self.driver.volume_snapshot_delete(context, instance, volume_id,
3201                                            snapshot_id, delete_info)
3202 
3203     @wrap_instance_fault
3204     def _rotate_backups(self, context, instance, backup_type, rotation):
3205         """Delete excess backups associated to an instance.
3206 
3207         Instances are allowed a fixed number of backups (the rotation number);
3208         this method deletes the oldest backups that exceed the rotation
3209         threshold.
3210 
3211         :param context: security context
3212         :param instance: Instance dict
3213         :param backup_type: a user-defined type, like "daily" or "weekly" etc.
3214         :param rotation: int representing how many backups to keep around;
3215             None if rotation shouldn't be used (as in the case of snapshots)
3216         """
3217         filters = {'property-image_type': 'backup',
3218                    'property-backup_type': backup_type,
3219                    'property-instance_uuid': instance.uuid}
3220 
3221         images = self.image_api.get_all(context, filters=filters,
3222                                         sort_key='created_at', sort_dir='desc')
3223         num_images = len(images)
3224         LOG.debug("Found %(num_images)d images (rotation: %(rotation)d)",
3225                   {'num_images': num_images, 'rotation': rotation},
3226                   instance=instance)
3227 
3228         if num_images > rotation:
3229             # NOTE(sirp): this deletes all backups that exceed the rotation
3230             # limit
3231             excess = len(images) - rotation
3232             LOG.debug("Rotating out %d backups", excess,
3233                       instance=instance)
3234             for i in range(excess):
3235                 image = images.pop()
3236                 image_id = image['id']
3237                 LOG.debug("Deleting image %s", image_id,
3238                           instance=instance)
3239                 try:
3240                     self.image_api.delete(context, image_id)
3241                 except exception.ImageNotFound:
3242                     LOG.info("Failed to find image %(image_id)s to "
3243                              "delete", {'image_id': image_id},
3244                              instance=instance)
3245 
3246     @wrap_exception()
3247     @reverts_task_state
3248     @wrap_instance_event(prefix='compute')
3249     @wrap_instance_fault
3250     def set_admin_password(self, context, instance, new_pass):
3251         """Set the root/admin password for an instance on this host.
3252 
3253         This is generally only called by API password resets after an
3254         image has been built.
3255 
3256         @param context: Nova auth context.
3257         @param instance: Nova instance object.
3258         @param new_pass: The admin password for the instance.
3259         """
3260 
3261         context = context.elevated()
3262         if new_pass is None:
3263             # Generate a random password
3264             new_pass = utils.generate_password()
3265 
3266         current_power_state = self._get_power_state(context, instance)
3267         expected_state = power_state.RUNNING
3268 
3269         if current_power_state != expected_state:
3270             instance.task_state = None
3271             instance.save(expected_task_state=task_states.UPDATING_PASSWORD)
3272             _msg = _('instance %s is not running') % instance.uuid
3273             raise exception.InstancePasswordSetFailed(
3274                 instance=instance.uuid, reason=_msg)
3275 
3276         try:
3277             self.driver.set_admin_password(instance, new_pass)
3278             LOG.info("Root password set", instance=instance)
3279             instance.task_state = None
3280             instance.save(
3281                 expected_task_state=task_states.UPDATING_PASSWORD)
3282         except exception.InstanceAgentNotEnabled:
3283             with excutils.save_and_reraise_exception():
3284                 LOG.debug('Guest agent is not enabled for the instance.',
3285                           instance=instance)
3286                 instance.task_state = None
3287                 instance.save(
3288                     expected_task_state=task_states.UPDATING_PASSWORD)
3289         except exception.SetAdminPasswdNotSupported:
3290             with excutils.save_and_reraise_exception():
3291                 LOG.info('set_admin_password is not supported '
3292                          'by this driver or guest instance.',
3293                          instance=instance)
3294                 instance.task_state = None
3295                 instance.save(
3296                     expected_task_state=task_states.UPDATING_PASSWORD)
3297         except NotImplementedError:
3298             LOG.warning('set_admin_password is not implemented '
3299                         'by this driver or guest instance.',
3300                         instance=instance)
3301             instance.task_state = None
3302             instance.save(
3303                 expected_task_state=task_states.UPDATING_PASSWORD)
3304             raise NotImplementedError(_('set_admin_password is not '
3305                                         'implemented by this driver or guest '
3306                                         'instance.'))
3307         except exception.UnexpectedTaskStateError:
3308             # interrupted by another (most likely delete) task
3309             # do not retry
3310             raise
3311         except Exception:
3312             # Catch all here because this could be anything.
3313             LOG.exception('set_admin_password failed', instance=instance)
3314             self._set_instance_obj_error_state(context, instance)
3315             # We create a new exception here so that we won't
3316             # potentially reveal password information to the
3317             # API caller.  The real exception is logged above
3318             _msg = _('error setting admin password')
3319             raise exception.InstancePasswordSetFailed(
3320                 instance=instance.uuid, reason=_msg)
3321 
3322     @wrap_exception()
3323     @reverts_task_state
3324     @wrap_instance_fault
3325     def inject_file(self, context, path, file_contents, instance):
3326         """Write a file to the specified path in an instance on this host."""
3327         # NOTE(russellb) Remove this method, as well as the underlying virt
3328         # driver methods, when the compute rpc interface is bumped to 4.x
3329         # as it is no longer used.
3330         context = context.elevated()
3331         current_power_state = self._get_power_state(context, instance)
3332         expected_state = power_state.RUNNING
3333         if current_power_state != expected_state:
3334             LOG.warning('trying to inject a file into a non-running '
3335                         '(state: %(current_state)s expected: '
3336                         '%(expected_state)s)',
3337                         {'current_state': current_power_state,
3338                          'expected_state': expected_state},
3339                         instance=instance)
3340         LOG.info('injecting file to %s', path, instance=instance)
3341         self.driver.inject_file(instance, path, file_contents)
3342 
3343     def _get_rescue_image(self, context, instance, rescue_image_ref=None):
3344         """Determine what image should be used to boot the rescue VM."""
3345         # 1. If rescue_image_ref is passed in, use that for rescue.
3346         # 2. Else, use the base image associated with instance's current image.
3347         #       The idea here is to provide the customer with a rescue
3348         #       environment which they are familiar with.
3349         #       So, if they built their instance off of a Debian image,
3350         #       their rescue VM will also be Debian.
3351         # 3. As a last resort, use instance's current image.
3352         if not rescue_image_ref:
3353             system_meta = utils.instance_sys_meta(instance)
3354             rescue_image_ref = system_meta.get('image_base_image_ref')
3355 
3356         if not rescue_image_ref:
3357             LOG.warning('Unable to find a different image to use for '
3358                         'rescue VM, using instance\'s current image',
3359                         instance=instance)
3360             rescue_image_ref = instance.image_ref
3361 
3362         return objects.ImageMeta.from_image_ref(
3363             context, self.image_api, rescue_image_ref)
3364 
3365     @wrap_exception()
3366     @reverts_task_state
3367     @wrap_instance_event(prefix='compute')
3368     @wrap_instance_fault
3369     def rescue_instance(self, context, instance, rescue_password,
3370                         rescue_image_ref, clean_shutdown):
3371         context = context.elevated()
3372         LOG.info('Rescuing', instance=instance)
3373 
3374         admin_password = (rescue_password if rescue_password else
3375                       utils.generate_password())
3376 
3377         network_info = self.network_api.get_instance_nw_info(context, instance)
3378 
3379         rescue_image_meta = self._get_rescue_image(context, instance,
3380                                                    rescue_image_ref)
3381 
3382         extra_usage_info = {'rescue_image_name':
3383                             self._get_image_name(rescue_image_meta)}
3384         self._notify_about_instance_usage(context, instance,
3385                 "rescue.start", extra_usage_info=extra_usage_info,
3386                 network_info=network_info)
3387 
3388         try:
3389             self._power_off_instance(context, instance, clean_shutdown)
3390 
3391             self.driver.rescue(context, instance,
3392                                network_info,
3393                                rescue_image_meta, admin_password)
3394         except Exception as e:
3395             LOG.exception("Error trying to Rescue Instance",
3396                           instance=instance)
3397             self._set_instance_obj_error_state(context, instance)
3398             raise exception.InstanceNotRescuable(
3399                 instance_id=instance.uuid,
3400                 reason=_("Driver Error: %s") % e)
3401 
3402         compute_utils.notify_usage_exists(self.notifier, context, instance,
3403                                           current_period=True)
3404 
3405         instance.vm_state = vm_states.RESCUED
3406         instance.task_state = None
3407         instance.power_state = self._get_power_state(context, instance)
3408         instance.launched_at = timeutils.utcnow()
3409         instance.save(expected_task_state=task_states.RESCUING)
3410 
3411         self._notify_about_instance_usage(context, instance,
3412                 "rescue.end", extra_usage_info=extra_usage_info,
3413                 network_info=network_info)
3414 
3415     @wrap_exception()
3416     @reverts_task_state
3417     @wrap_instance_event(prefix='compute')
3418     @wrap_instance_fault
3419     def unrescue_instance(self, context, instance):
3420         context = context.elevated()
3421         LOG.info('Unrescuing', instance=instance)
3422 
3423         network_info = self.network_api.get_instance_nw_info(context, instance)
3424         self._notify_about_instance_usage(context, instance,
3425                 "unrescue.start", network_info=network_info)
3426         with self._error_out_instance_on_exception(context, instance):
3427             self.driver.unrescue(instance,
3428                                  network_info)
3429 
3430         instance.vm_state = vm_states.ACTIVE
3431         instance.task_state = None
3432         instance.power_state = self._get_power_state(context, instance)
3433         instance.save(expected_task_state=task_states.UNRESCUING)
3434 
3435         self._notify_about_instance_usage(context,
3436                                           instance,
3437                                           "unrescue.end",
3438                                           network_info=network_info)
3439 
3440     @wrap_exception()
3441     @wrap_instance_fault
3442     def change_instance_metadata(self, context, diff, instance):
3443         """Update the metadata published to the instance."""
3444         LOG.debug("Changing instance metadata according to %r",
3445                   diff, instance=instance)
3446         self.driver.change_instance_metadata(context, instance, diff)
3447 
3448     @wrap_exception()
3449     @wrap_instance_event(prefix='compute')
3450     @wrap_instance_fault
3451     def confirm_resize(self, context, instance, reservations, migration):
3452         @utils.synchronized(instance.uuid)
3453         def do_confirm_resize(context, instance, migration_id):
3454             # NOTE(wangpan): Get the migration status from db, if it has been
3455             #                confirmed, we do nothing and return here
3456             LOG.debug("Going to confirm migration %s", migration_id,
3457                       instance=instance)
3458             try:
3459                 # TODO(russellb) Why are we sending the migration object just
3460                 # to turn around and look it up from the db again?
3461                 migration = objects.Migration.get_by_id(
3462                                     context.elevated(), migration_id)
3463             except exception.MigrationNotFound:
3464                 LOG.error("Migration %s is not found during confirmation",
3465                           migration_id, instance=instance)
3466                 return
3467 
3468             if migration.status == 'confirmed':
3469                 LOG.info("Migration %s is already confirmed",
3470                          migration_id, instance=instance)
3471                 return
3472             elif migration.status not in ('finished', 'confirming'):
3473                 LOG.warning("Unexpected confirmation status '%(status)s' "
3474                             "of migration %(id)s, exit confirmation process",
3475                             {"status": migration.status, "id": migration_id},
3476                             instance=instance)
3477                 return
3478 
3479             # NOTE(wangpan): Get the instance from db, if it has been
3480             #                deleted, we do nothing and return here
3481             expected_attrs = ['metadata', 'system_metadata', 'flavor']
3482             try:
3483                 instance = objects.Instance.get_by_uuid(
3484                         context, instance.uuid,
3485                         expected_attrs=expected_attrs)
3486             except exception.InstanceNotFound:
3487                 LOG.info("Instance is not found during confirmation",
3488                          instance=instance)
3489                 return
3490 
3491             self._confirm_resize(context, instance, migration=migration)
3492 
3493         do_confirm_resize(context, instance, migration.id)
3494 
3495     def _confirm_resize(self, context, instance, migration=None):
3496         """Destroys the source instance."""
3497         self._notify_about_instance_usage(context, instance,
3498                                           "resize.confirm.start")
3499 
3500         with self._error_out_instance_on_exception(context, instance):
3501             # NOTE(danms): delete stashed migration information
3502             old_instance_type = instance.old_flavor
3503             instance.old_flavor = None
3504             instance.new_flavor = None
3505             instance.system_metadata.pop('old_vm_state', None)
3506             instance.save()
3507 
3508             # NOTE(tr3buchet): tear down networks on source host
3509             self.network_api.setup_networks_on_host(context, instance,
3510                                migration.source_compute, teardown=True)
3511 
3512             network_info = self.network_api.get_instance_nw_info(context,
3513                                                                  instance)
3514             self.driver.confirm_migration(context, migration, instance,
3515                                           network_info)
3516 
3517             migration.status = 'confirmed'
3518             with migration.obj_as_admin():
3519                 migration.save()
3520 
3521             rt = self._get_resource_tracker()
3522             rt.drop_move_claim(context, instance, migration.source_node,
3523                                old_instance_type, prefix='old_')
3524             instance.drop_migration_context()
3525 
3526             # NOTE(mriedem): The old_vm_state could be STOPPED but the user
3527             # might have manually powered up the instance to confirm the
3528             # resize/migrate, so we need to check the current power state
3529             # on the instance and set the vm_state appropriately. We default
3530             # to ACTIVE because if the power state is not SHUTDOWN, we
3531             # assume _sync_instance_power_state will clean it up.
3532             p_state = instance.power_state
3533             vm_state = None
3534             if p_state == power_state.SHUTDOWN:
3535                 vm_state = vm_states.STOPPED
3536                 LOG.debug("Resized/migrated instance is powered off. "
3537                           "Setting vm_state to '%s'.", vm_state,
3538                           instance=instance)
3539             else:
3540                 vm_state = vm_states.ACTIVE
3541 
3542             instance.vm_state = vm_state
3543             instance.task_state = None
3544             instance.save(expected_task_state=[None, task_states.DELETING])
3545 
3546             self._notify_about_instance_usage(
3547                 context, instance, "resize.confirm.end",
3548                 network_info=network_info)
3549 
3550     @wrap_exception()
3551     @reverts_task_state
3552     @wrap_instance_event(prefix='compute')
3553     @errors_out_migration
3554     @wrap_instance_fault
3555     def revert_resize(self, context, instance, migration, reservations):
3556         """Destroys the new instance on the destination machine.
3557 
3558         Reverts the model changes, and powers on the old instance on the
3559         source machine.
3560 
3561         """
3562         # NOTE(comstud): A revert_resize is essentially a resize back to
3563         # the old size, so we need to send a usage event here.
3564         compute_utils.notify_usage_exists(self.notifier, context, instance,
3565                                           current_period=True)
3566 
3567         with self._error_out_instance_on_exception(context, instance):
3568             # NOTE(tr3buchet): tear down networks on destination host
3569             self.network_api.setup_networks_on_host(context, instance,
3570                                                     teardown=True)
3571 
3572             migration_p = obj_base.obj_to_primitive(migration)
3573             self.network_api.migrate_instance_start(context,
3574                                                     instance,
3575                                                     migration_p)
3576 
3577             network_info = self.network_api.get_instance_nw_info(context,
3578                                                                  instance)
3579             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
3580                     context, instance.uuid)
3581             block_device_info = self._get_instance_block_device_info(
3582                                 context, instance, bdms=bdms)
3583 
3584             destroy_disks = not self._is_instance_storage_shared(
3585                 context, instance, host=migration.source_compute)
3586             self.driver.destroy(context, instance, network_info,
3587                                 block_device_info, destroy_disks)
3588 
3589             self._terminate_volume_connections(context, instance, bdms)
3590 
3591             migration.status = 'reverted'
3592             with migration.obj_as_admin():
3593                 migration.save()
3594 
3595             # NOTE(ndipanov): We need to do this here because dropping the
3596             # claim means we lose the migration_context data. We really should
3597             # fix this by moving the drop_move_claim call to the
3598             # finish_revert_resize method as this is racy (revert is dropped,
3599             # but instance resources will be tracked with the new flavor until
3600             # it gets rolled back in finish_revert_resize, which is
3601             # potentially wrong for a period of time).
3602             instance.revert_migration_context()
3603             instance.save()
3604 
3605             rt = self._get_resource_tracker()
3606             rt.drop_move_claim(context, instance, instance.node)
3607 
3608             self.compute_rpcapi.finish_revert_resize(context, instance,
3609                     migration, migration.source_compute)
3610 
3611     @wrap_exception()
3612     @reverts_task_state
3613     @wrap_instance_event(prefix='compute')
3614     @errors_out_migration
3615     @wrap_instance_fault
3616     def finish_revert_resize(self, context, instance, reservations, migration):
3617         """Finishes the second half of reverting a resize.
3618 
3619         Bring the original source instance state back (active/shutoff) and
3620         revert the resized attributes in the database.
3621 
3622         """
3623         with self._error_out_instance_on_exception(context, instance):
3624             self._notify_about_instance_usage(
3625                     context, instance, "resize.revert.start")
3626 
3627             # NOTE(mriedem): delete stashed old_vm_state information; we
3628             # default to ACTIVE for backwards compatibility if old_vm_state
3629             # is not set
3630             old_vm_state = instance.system_metadata.pop('old_vm_state',
3631                                                         vm_states.ACTIVE)
3632 
3633             self._set_instance_info(instance, instance.old_flavor)
3634             instance.old_flavor = None
3635             instance.new_flavor = None
3636             instance.host = migration.source_compute
3637             instance.node = migration.source_node
3638             instance.save()
3639 
3640             self.network_api.setup_networks_on_host(context, instance,
3641                                                     migration.source_compute)
3642             migration_p = obj_base.obj_to_primitive(migration)
3643             # NOTE(hanrong): we need to change migration_p['dest_compute'] to
3644             # source host temporarily. "network_api.migrate_instance_finish"
3645             # will setup the network for the instance on the destination host.
3646             # For revert resize, the instance will back to the source host, the
3647             # setup of the network for instance should be on the source host.
3648             # So set the migration_p['dest_compute'] to source host at here.
3649             migration_p['dest_compute'] = migration.source_compute
3650             self.network_api.migrate_instance_finish(context,
3651                                                      instance,
3652                                                      migration_p)
3653             network_info = self.network_api.get_instance_nw_info(context,
3654                                                                  instance)
3655 
3656             block_device_info = self._get_instance_block_device_info(
3657                     context, instance, refresh_conn_info=True)
3658 
3659             power_on = old_vm_state != vm_states.STOPPED
3660             self.driver.finish_revert_migration(context, instance,
3661                                        network_info,
3662                                        block_device_info, power_on)
3663 
3664             instance.drop_migration_context()
3665             instance.launched_at = timeutils.utcnow()
3666             instance.save(expected_task_state=task_states.RESIZE_REVERTING)
3667 
3668             # if the original vm state was STOPPED, set it back to STOPPED
3669             LOG.info("Updating instance to original state: '%s'",
3670                      old_vm_state, instance=instance)
3671             if power_on:
3672                 instance.vm_state = vm_states.ACTIVE
3673                 instance.task_state = None
3674                 instance.save()
3675             else:
3676                 instance.task_state = task_states.POWERING_OFF
3677                 instance.save()
3678                 self.stop_instance(context, instance=instance,
3679                                    clean_shutdown=True)
3680 
3681             self._notify_about_instance_usage(
3682                     context, instance, "resize.revert.end")
3683 
3684     def _prep_resize(self, context, image, instance, instance_type,
3685                      filter_properties, node, clean_shutdown=True):
3686 
3687         if not filter_properties:
3688             filter_properties = {}
3689 
3690         if not instance.host:
3691             self._set_instance_obj_error_state(context, instance)
3692             msg = _('Instance has no source host')
3693             raise exception.MigrationError(reason=msg)
3694 
3695         same_host = instance.host == self.host
3696         # if the flavor IDs match, it's migrate; otherwise resize
3697         if same_host and instance_type.id == instance['instance_type_id']:
3698             # check driver whether support migrate to same host
3699             if not self.driver.capabilities['supports_migrate_to_same_host']:
3700                 raise exception.UnableToMigrateToSelf(
3701                     instance_id=instance.uuid, host=self.host)
3702 
3703         # NOTE(danms): Stash the new instance_type to avoid having to
3704         # look it up in the database later
3705         instance.new_flavor = instance_type
3706         # NOTE(mriedem): Stash the old vm_state so we can set the
3707         # resized/reverted instance back to the same state later.
3708         vm_state = instance.vm_state
3709         LOG.debug('Stashing vm_state: %s', vm_state, instance=instance)
3710         instance.system_metadata['old_vm_state'] = vm_state
3711         instance.save()
3712 
3713         limits = filter_properties.get('limits', {})
3714         rt = self._get_resource_tracker()
3715         with rt.resize_claim(context, instance, instance_type, node,
3716                              image_meta=image, limits=limits) as claim:
3717             LOG.info('Migrating', instance=instance)
3718             self.compute_rpcapi.resize_instance(
3719                     context, instance, claim.migration, image,
3720                     instance_type, clean_shutdown)
3721 
3722     @wrap_exception()
3723     @reverts_task_state
3724     @wrap_instance_event(prefix='compute')
3725     @wrap_instance_fault
3726     def prep_resize(self, context, image, instance, instance_type,
3727                     reservations, request_spec, filter_properties, node,
3728                     clean_shutdown):
3729         """Initiates the process of moving a running instance to another host.
3730 
3731         Possibly changes the RAM and disk size in the process.
3732 
3733         """
3734         if node is None:
3735             node = self.driver.get_available_nodes(refresh=True)[0]
3736             LOG.debug("No node specified, defaulting to %s", node,
3737                       instance=instance)
3738 
3739         # NOTE(melwitt): Remove this in version 5.0 of the RPC API
3740         # Code downstream may expect extra_specs to be populated since it
3741         # is receiving an object, so lookup the flavor to ensure this.
3742         if not isinstance(instance_type, objects.Flavor):
3743             instance_type = objects.Flavor.get_by_id(context,
3744                                                      instance_type['id'])
3745         with self._error_out_instance_on_exception(context, instance):
3746             compute_utils.notify_usage_exists(self.notifier, context, instance,
3747                                               current_period=True)
3748             self._notify_about_instance_usage(
3749                     context, instance, "resize.prep.start")
3750             try:
3751                 self._prep_resize(context, image, instance,
3752                                   instance_type, filter_properties,
3753                                   node, clean_shutdown)
3754             # NOTE(dgenin): This is thrown in LibvirtDriver when the
3755             #               instance to be migrated is backed by LVM.
3756             #               Remove when LVM migration is implemented.
3757             except exception.MigrationPreCheckError:
3758                 raise
3759             except Exception:
3760                 # try to re-schedule the resize elsewhere:
3761                 exc_info = sys.exc_info()
3762                 self._reschedule_resize_or_reraise(context, image, instance,
3763                         exc_info, instance_type, request_spec,
3764                         filter_properties)
3765             finally:
3766                 extra_usage_info = dict(
3767                         new_instance_type=instance_type.name,
3768                         new_instance_type_id=instance_type.id)
3769 
3770                 self._notify_about_instance_usage(
3771                     context, instance, "resize.prep.end",
3772                     extra_usage_info=extra_usage_info)
3773 
3774     def _reschedule_resize_or_reraise(self, context, image, instance, exc_info,
3775             instance_type, request_spec, filter_properties):
3776         """Try to re-schedule the resize or re-raise the original error to
3777         error out the instance.
3778         """
3779         if not request_spec:
3780             request_spec = {}
3781         if not filter_properties:
3782             filter_properties = {}
3783 
3784         rescheduled = False
3785         instance_uuid = instance.uuid
3786 
3787         try:
3788             reschedule_method = self.compute_task_api.resize_instance
3789             scheduler_hint = dict(filter_properties=filter_properties)
3790             method_args = (instance, None, scheduler_hint, instance_type)
3791             task_state = task_states.RESIZE_PREP
3792 
3793             rescheduled = self._reschedule(context, request_spec,
3794                     filter_properties, instance, reschedule_method,
3795                     method_args, task_state, exc_info)
3796         except Exception as error:
3797             rescheduled = False
3798             LOG.exception("Error trying to reschedule",
3799                           instance_uuid=instance_uuid)
3800             compute_utils.add_instance_fault_from_exc(context,
3801                     instance, error,
3802                     exc_info=sys.exc_info())
3803             self._notify_about_instance_usage(context, instance,
3804                     'resize.error', fault=error)
3805 
3806         if rescheduled:
3807             self._log_original_error(exc_info, instance_uuid)
3808             compute_utils.add_instance_fault_from_exc(context,
3809                     instance, exc_info[1], exc_info=exc_info)
3810             self._notify_about_instance_usage(context, instance,
3811                     'resize.error', fault=exc_info[1])
3812         else:
3813             # not re-scheduling
3814             six.reraise(*exc_info)
3815 
3816     @wrap_exception()
3817     @reverts_task_state
3818     @wrap_instance_event(prefix='compute')
3819     @wrap_instance_fault
3820     def resize_instance(self, context, instance, image,
3821                         reservations, migration, instance_type,
3822                         clean_shutdown):
3823         """Starts the migration of a running instance to another host."""
3824         with self._error_out_instance_on_exception(context, instance), \
3825              errors_out_migration_ctxt(migration):
3826             # TODO(chaochin) Remove this until v5 RPC API
3827             # Code downstream may expect extra_specs to be populated since it
3828             # is receiving an object, so lookup the flavor to ensure this.
3829             if (not instance_type or
3830                 not isinstance(instance_type, objects.Flavor)):
3831                 instance_type = objects.Flavor.get_by_id(
3832                     context, migration['new_instance_type_id'])
3833 
3834             network_info = self.network_api.get_instance_nw_info(context,
3835                                                                  instance)
3836 
3837             migration.status = 'migrating'
3838             with migration.obj_as_admin():
3839                 migration.save()
3840 
3841             instance.task_state = task_states.RESIZE_MIGRATING
3842             instance.save(expected_task_state=task_states.RESIZE_PREP)
3843 
3844             self._notify_about_instance_usage(
3845                 context, instance, "resize.start", network_info=network_info)
3846 
3847             compute_utils.notify_about_instance_action(context, instance,
3848                    self.host, action=fields.NotificationAction.RESIZE,
3849                    phase=fields.NotificationPhase.START)
3850 
3851             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
3852                     context, instance.uuid)
3853             block_device_info = self._get_instance_block_device_info(
3854                                 context, instance, bdms=bdms)
3855 
3856             timeout, retry_interval = self._get_power_off_values(context,
3857                                             instance, clean_shutdown)
3858             disk_info = self.driver.migrate_disk_and_power_off(
3859                     context, instance, migration.dest_host,
3860                     instance_type, network_info,
3861                     block_device_info,
3862                     timeout, retry_interval)
3863 
3864             self._terminate_volume_connections(context, instance, bdms)
3865 
3866             migration_p = obj_base.obj_to_primitive(migration)
3867             self.network_api.migrate_instance_start(context,
3868                                                     instance,
3869                                                     migration_p)
3870 
3871             migration.status = 'post-migrating'
3872             with migration.obj_as_admin():
3873                 migration.save()
3874 
3875             instance.host = migration.dest_compute
3876             instance.node = migration.dest_node
3877             instance.task_state = task_states.RESIZE_MIGRATED
3878             instance.save(expected_task_state=task_states.RESIZE_MIGRATING)
3879 
3880             self.compute_rpcapi.finish_resize(context, instance,
3881                     migration, image, disk_info, migration.dest_compute)
3882 
3883         self._notify_about_instance_usage(context, instance, "resize.end",
3884                                           network_info=network_info)
3885 
3886         compute_utils.notify_about_instance_action(context, instance,
3887                self.host, action=fields.NotificationAction.RESIZE,
3888                phase=fields.NotificationPhase.END)
3889         self.instance_events.clear_events_for_instance(instance)
3890 
3891     def _terminate_volume_connections(self, context, instance, bdms):
3892         connector = None
3893         for bdm in bdms:
3894             if bdm.is_volume:
3895                 if bdm.attachment_id:
3896                     self.volume_api.attachment_delete(context,
3897                                                       bdm.attachment_id)
3898                 else:
3899                     if connector is None:
3900                         connector = self.driver.get_volume_connector(instance)
3901                     self.volume_api.terminate_connection(context,
3902                                                          bdm.volume_id,
3903                                                          connector)
3904 
3905     @staticmethod
3906     def _set_instance_info(instance, instance_type):
3907         instance.instance_type_id = instance_type.id
3908         instance.memory_mb = instance_type.memory_mb
3909         instance.vcpus = instance_type.vcpus
3910         instance.root_gb = instance_type.root_gb
3911         instance.ephemeral_gb = instance_type.ephemeral_gb
3912         instance.flavor = instance_type
3913 
3914     def _finish_resize(self, context, instance, migration, disk_info,
3915                        image_meta):
3916         resize_instance = False
3917         old_instance_type_id = migration['old_instance_type_id']
3918         new_instance_type_id = migration['new_instance_type_id']
3919         old_instance_type = instance.get_flavor()
3920         # NOTE(mriedem): Get the old_vm_state so we know if we should
3921         # power on the instance. If old_vm_state is not set we need to default
3922         # to ACTIVE for backwards compatibility
3923         old_vm_state = instance.system_metadata.get('old_vm_state',
3924                                                     vm_states.ACTIVE)
3925         instance.old_flavor = old_instance_type
3926 
3927         if old_instance_type_id != new_instance_type_id:
3928             instance_type = instance.get_flavor('new')
3929             self._set_instance_info(instance, instance_type)
3930             for key in ('root_gb', 'swap', 'ephemeral_gb'):
3931                 if old_instance_type[key] != instance_type[key]:
3932                     resize_instance = True
3933                     break
3934         instance.apply_migration_context()
3935 
3936         # NOTE(tr3buchet): setup networks on destination host
3937         self.network_api.setup_networks_on_host(context, instance,
3938                                                 migration['dest_compute'])
3939 
3940         migration_p = obj_base.obj_to_primitive(migration)
3941         self.network_api.migrate_instance_finish(context,
3942                                                  instance,
3943                                                  migration_p)
3944 
3945         network_info = self.network_api.get_instance_nw_info(context, instance)
3946 
3947         instance.task_state = task_states.RESIZE_FINISH
3948         instance.save(expected_task_state=task_states.RESIZE_MIGRATED)
3949 
3950         self._notify_about_instance_usage(
3951             context, instance, "finish_resize.start",
3952             network_info=network_info)
3953         compute_utils.notify_about_instance_action(context, instance,
3954                self.host, action=fields.NotificationAction.RESIZE_FINISH,
3955                phase=fields.NotificationPhase.START)
3956 
3957         block_device_info = self._get_instance_block_device_info(
3958                             context, instance, refresh_conn_info=True)
3959 
3960         # NOTE(mriedem): If the original vm_state was STOPPED, we don't
3961         # automatically power on the instance after it's migrated
3962         power_on = old_vm_state != vm_states.STOPPED
3963 
3964         try:
3965             self.driver.finish_migration(context, migration, instance,
3966                                          disk_info,
3967                                          network_info,
3968                                          image_meta, resize_instance,
3969                                          block_device_info, power_on)
3970         except Exception:
3971             with excutils.save_and_reraise_exception():
3972                 if old_instance_type_id != new_instance_type_id:
3973                     self._set_instance_info(instance,
3974                                             old_instance_type)
3975 
3976         migration.status = 'finished'
3977         with migration.obj_as_admin():
3978             migration.save()
3979 
3980         instance.vm_state = vm_states.RESIZED
3981         instance.task_state = None
3982         instance.launched_at = timeutils.utcnow()
3983         instance.save(expected_task_state=task_states.RESIZE_FINISH)
3984 
3985         return network_info
3986 
3987     @wrap_instance_event(prefix='compute')
3988     def finish_resize(self, context, disk_info, image, instance,
3989                       reservations, migration):
3990         """Completes the migration process.
3991 
3992         Sets up the newly transferred disk and turns on the instance at its
3993         new host machine.
3994 
3995         """
3996 
3997         @wrap_exception()
3998         @wrap_instance_fault
3999         def do_finish_resize(self, context, instance):
4000             # In the event of an exception, we want to log any exception from
4001             # _finish_resize before attempting an auto revert.
4002             with self._error_out_instance_on_exception(context, instance), \
4003                  errors_out_migration_ctxt(migration):
4004                 image_meta = objects.ImageMeta.from_dict(image)
4005                 return self._finish_resize(context, instance, migration,
4006                                            disk_info, image_meta)
4007 
4008         try:
4009             network_info = do_finish_resize(self, context, instance)
4010         except Exception:
4011             LOG.exception('Migration failed on the destination host. '
4012                           'Starting automatic revert.', instance=instance)
4013             with excutils.save_and_reraise_exception():
4014                 # NOTE(mdbooth): We set instance.task_state and
4015                 # migration.status here to the same values they are set to in
4016                 # API.revert_resize().
4017                 instance.task_state = task_states.RESIZE_REVERTING
4018                 instance.save()
4019                 migration.status = 'reverting'
4020                 migration.save()
4021 
4022                 self.revert_resize(context, instance, migration, [])
4023 
4024         self._update_scheduler_instance_info(context, instance)
4025         self._notify_about_instance_usage(
4026             context, instance, "finish_resize.end",
4027             network_info=network_info)
4028         compute_utils.notify_about_instance_action(context, instance,
4029                self.host, action=fields.NotificationAction.RESIZE_FINISH,
4030                phase=fields.NotificationPhase.END)
4031 
4032     @wrap_exception()
4033     @wrap_instance_fault
4034     def add_fixed_ip_to_instance(self, context, network_id, instance):
4035         """Calls network_api to add new fixed_ip to instance
4036         then injects the new network info and resets instance networking.
4037 
4038         """
4039         self._notify_about_instance_usage(
4040                 context, instance, "create_ip.start")
4041 
4042         network_info = self.network_api.add_fixed_ip_to_instance(context,
4043                                                                  instance,
4044                                                                  network_id)
4045         self._inject_network_info(context, instance, network_info)
4046         self.reset_network(context, instance)
4047 
4048         # NOTE(russellb) We just want to bump updated_at.  See bug 1143466.
4049         instance.updated_at = timeutils.utcnow()
4050         instance.save()
4051 
4052         self._notify_about_instance_usage(
4053             context, instance, "create_ip.end", network_info=network_info)
4054 
4055     @wrap_exception()
4056     @wrap_instance_fault
4057     def remove_fixed_ip_from_instance(self, context, address, instance):
4058         """Calls network_api to remove existing fixed_ip from instance
4059         by injecting the altered network info and resetting
4060         instance networking.
4061         """
4062         self._notify_about_instance_usage(
4063                 context, instance, "delete_ip.start")
4064 
4065         network_info = self.network_api.remove_fixed_ip_from_instance(context,
4066                                                                       instance,
4067                                                                       address)
4068         self._inject_network_info(context, instance, network_info)
4069         self.reset_network(context, instance)
4070 
4071         # NOTE(russellb) We just want to bump updated_at.  See bug 1143466.
4072         instance.updated_at = timeutils.utcnow()
4073         instance.save()
4074 
4075         self._notify_about_instance_usage(
4076             context, instance, "delete_ip.end", network_info=network_info)
4077 
4078     @wrap_exception()
4079     @reverts_task_state
4080     @wrap_instance_event(prefix='compute')
4081     @wrap_instance_fault
4082     def pause_instance(self, context, instance):
4083         """Pause an instance on this host."""
4084         context = context.elevated()
4085         LOG.info('Pausing', instance=instance)
4086         self._notify_about_instance_usage(context, instance, 'pause.start')
4087         compute_utils.notify_about_instance_action(context, instance,
4088                self.host, action=fields.NotificationAction.PAUSE,
4089                phase=fields.NotificationPhase.START)
4090         self.driver.pause(instance)
4091         instance.power_state = self._get_power_state(context, instance)
4092         instance.vm_state = vm_states.PAUSED
4093         instance.task_state = None
4094         instance.save(expected_task_state=task_states.PAUSING)
4095         self._notify_about_instance_usage(context, instance, 'pause.end')
4096         compute_utils.notify_about_instance_action(context, instance,
4097                self.host, action=fields.NotificationAction.PAUSE,
4098                phase=fields.NotificationPhase.END)
4099 
4100     @wrap_exception()
4101     @reverts_task_state
4102     @wrap_instance_event(prefix='compute')
4103     @wrap_instance_fault
4104     def unpause_instance(self, context, instance):
4105         """Unpause a paused instance on this host."""
4106         context = context.elevated()
4107         LOG.info('Unpausing', instance=instance)
4108         self._notify_about_instance_usage(context, instance, 'unpause.start')
4109         compute_utils.notify_about_instance_action(context, instance,
4110             self.host, action=fields.NotificationAction.UNPAUSE,
4111             phase=fields.NotificationPhase.START)
4112         self.driver.unpause(instance)
4113         instance.power_state = self._get_power_state(context, instance)
4114         instance.vm_state = vm_states.ACTIVE
4115         instance.task_state = None
4116         instance.save(expected_task_state=task_states.UNPAUSING)
4117         self._notify_about_instance_usage(context, instance, 'unpause.end')
4118         compute_utils.notify_about_instance_action(context, instance,
4119             self.host, action=fields.NotificationAction.UNPAUSE,
4120             phase=fields.NotificationPhase.END)
4121 
4122     @wrap_exception()
4123     def host_power_action(self, context, action):
4124         """Reboots, shuts down or powers up the host."""
4125         return self.driver.host_power_action(action)
4126 
4127     @wrap_exception()
4128     def host_maintenance_mode(self, context, host, mode):
4129         """Start/Stop host maintenance window. On start, it triggers
4130         guest VMs evacuation.
4131         """
4132         return self.driver.host_maintenance_mode(host, mode)
4133 
4134     @wrap_exception()
4135     def set_host_enabled(self, context, enabled):
4136         """Sets the specified host's ability to accept new instances."""
4137         return self.driver.set_host_enabled(enabled)
4138 
4139     @wrap_exception()
4140     def get_host_uptime(self, context):
4141         """Returns the result of calling "uptime" on the target host."""
4142         return self.driver.get_host_uptime()
4143 
4144     @wrap_exception()
4145     @wrap_instance_fault
4146     def get_diagnostics(self, context, instance):
4147         """Retrieve diagnostics for an instance on this host."""
4148         current_power_state = self._get_power_state(context, instance)
4149         if current_power_state == power_state.RUNNING:
4150             LOG.info("Retrieving diagnostics", instance=instance)
4151             return self.driver.get_diagnostics(instance)
4152         else:
4153             raise exception.InstanceInvalidState(
4154                 attr='power state',
4155                 instance_uuid=instance.uuid,
4156                 state=power_state.STATE_MAP[instance.power_state],
4157                 method='get_diagnostics')
4158 
4159     # TODO(alaski): Remove object_compat for RPC version 5.0
4160     @object_compat
4161     @wrap_exception()
4162     @wrap_instance_fault
4163     def get_instance_diagnostics(self, context, instance):
4164         """Retrieve diagnostics for an instance on this host."""
4165         current_power_state = self._get_power_state(context, instance)
4166         if current_power_state == power_state.RUNNING:
4167             LOG.info("Retrieving diagnostics", instance=instance)
4168             return self.driver.get_instance_diagnostics(instance)
4169         else:
4170             raise exception.InstanceInvalidState(
4171                 attr='power state',
4172                 instance_uuid=instance.uuid,
4173                 state=power_state.STATE_MAP[instance.power_state],
4174                 method='get_diagnostics')
4175 
4176     @wrap_exception()
4177     @reverts_task_state
4178     @wrap_instance_event(prefix='compute')
4179     @wrap_instance_fault
4180     def suspend_instance(self, context, instance):
4181         """Suspend the given instance."""
4182         context = context.elevated()
4183 
4184         # Store the old state
4185         instance.system_metadata['old_vm_state'] = instance.vm_state
4186         self._notify_about_instance_usage(context, instance, 'suspend.start')
4187         compute_utils.notify_about_instance_action(context, instance,
4188                 self.host, action=fields.NotificationAction.SUSPEND,
4189                 phase=fields.NotificationPhase.START)
4190         with self._error_out_instance_on_exception(context, instance,
4191              instance_state=instance.vm_state):
4192             self.driver.suspend(context, instance)
4193         instance.power_state = self._get_power_state(context, instance)
4194         instance.vm_state = vm_states.SUSPENDED
4195         instance.task_state = None
4196         instance.save(expected_task_state=task_states.SUSPENDING)
4197         self._notify_about_instance_usage(context, instance, 'suspend.end')
4198         compute_utils.notify_about_instance_action(context, instance,
4199                 self.host, action=fields.NotificationAction.SUSPEND,
4200                 phase=fields.NotificationPhase.END)
4201 
4202     @wrap_exception()
4203     @reverts_task_state
4204     @wrap_instance_event(prefix='compute')
4205     @wrap_instance_fault
4206     def resume_instance(self, context, instance):
4207         """Resume the given suspended instance."""
4208         context = context.elevated()
4209         LOG.info('Resuming', instance=instance)
4210 
4211         self._notify_about_instance_usage(context, instance, 'resume.start')
4212         compute_utils.notify_about_instance_action(context, instance,
4213             self.host, action=fields.NotificationAction.RESUME,
4214             phase=fields.NotificationPhase.START)
4215 
4216         network_info = self.network_api.get_instance_nw_info(context, instance)
4217         block_device_info = self._get_instance_block_device_info(
4218                             context, instance)
4219 
4220         with self._error_out_instance_on_exception(context, instance,
4221              instance_state=instance.vm_state):
4222             self.driver.resume(context, instance, network_info,
4223                                block_device_info)
4224 
4225         instance.power_state = self._get_power_state(context, instance)
4226 
4227         # We default to the ACTIVE state for backwards compatibility
4228         instance.vm_state = instance.system_metadata.pop('old_vm_state',
4229                                                          vm_states.ACTIVE)
4230 
4231         instance.task_state = None
4232         instance.save(expected_task_state=task_states.RESUMING)
4233         self._notify_about_instance_usage(context, instance, 'resume.end')
4234         compute_utils.notify_about_instance_action(context, instance,
4235             self.host, action=fields.NotificationAction.RESUME,
4236             phase=fields.NotificationPhase.END)
4237 
4238     @wrap_exception()
4239     @reverts_task_state
4240     @wrap_instance_event(prefix='compute')
4241     @wrap_instance_fault
4242     def shelve_instance(self, context, instance, image_id,
4243                         clean_shutdown):
4244         """Shelve an instance.
4245 
4246         This should be used when you want to take a snapshot of the instance.
4247         It also adds system_metadata that can be used by a periodic task to
4248         offload the shelved instance after a period of time.
4249 
4250         :param context: request context
4251         :param instance: an Instance object
4252         :param image_id: an image id to snapshot to.
4253         :param clean_shutdown: give the GuestOS a chance to stop
4254         """
4255 
4256         @utils.synchronized(instance.uuid)
4257         def do_shelve_instance():
4258             self._shelve_instance(context, instance, image_id, clean_shutdown)
4259         do_shelve_instance()
4260 
4261     def _shelve_instance(self, context, instance, image_id,
4262                          clean_shutdown):
4263         LOG.info('Shelving', instance=instance)
4264         compute_utils.notify_usage_exists(self.notifier, context, instance,
4265                                           current_period=True)
4266         self._notify_about_instance_usage(context, instance, 'shelve.start')
4267         compute_utils.notify_about_instance_action(context, instance,
4268                 self.host, action=fields.NotificationAction.SHELVE,
4269                 phase=fields.NotificationPhase.START)
4270 
4271         def update_task_state(task_state, expected_state=task_states.SHELVING):
4272             shelving_state_map = {
4273                     task_states.IMAGE_PENDING_UPLOAD:
4274                         task_states.SHELVING_IMAGE_PENDING_UPLOAD,
4275                     task_states.IMAGE_UPLOADING:
4276                         task_states.SHELVING_IMAGE_UPLOADING,
4277                     task_states.SHELVING: task_states.SHELVING}
4278             task_state = shelving_state_map[task_state]
4279             expected_state = shelving_state_map[expected_state]
4280             instance.task_state = task_state
4281             instance.save(expected_task_state=expected_state)
4282 
4283         self._power_off_instance(context, instance, clean_shutdown)
4284         self.driver.snapshot(context, instance, image_id, update_task_state)
4285 
4286         instance.system_metadata['shelved_at'] = timeutils.utcnow().isoformat()
4287         instance.system_metadata['shelved_image_id'] = image_id
4288         instance.system_metadata['shelved_host'] = self.host
4289         instance.vm_state = vm_states.SHELVED
4290         instance.task_state = None
4291         if CONF.shelved_offload_time == 0:
4292             instance.task_state = task_states.SHELVING_OFFLOADING
4293         instance.power_state = self._get_power_state(context, instance)
4294         instance.save(expected_task_state=[
4295                 task_states.SHELVING,
4296                 task_states.SHELVING_IMAGE_UPLOADING])
4297 
4298         self._notify_about_instance_usage(context, instance, 'shelve.end')
4299         compute_utils.notify_about_instance_action(context, instance,
4300                 self.host, action=fields.NotificationAction.SHELVE,
4301                 phase=fields.NotificationPhase.END)
4302 
4303         if CONF.shelved_offload_time == 0:
4304             self._shelve_offload_instance(context, instance,
4305                                           clean_shutdown=False)
4306 
4307     @wrap_exception()
4308     @reverts_task_state
4309     @wrap_instance_fault
4310     def shelve_offload_instance(self, context, instance, clean_shutdown):
4311         """Remove a shelved instance from the hypervisor.
4312 
4313         This frees up those resources for use by other instances, but may lead
4314         to slower unshelve times for this instance.  This method is used by
4315         volume backed instances since restoring them doesn't involve the
4316         potentially large download of an image.
4317 
4318         :param context: request context
4319         :param instance: nova.objects.instance.Instance
4320         :param clean_shutdown: give the GuestOS a chance to stop
4321         """
4322 
4323         @utils.synchronized(instance.uuid)
4324         def do_shelve_offload_instance():
4325             self._shelve_offload_instance(context, instance, clean_shutdown)
4326         do_shelve_offload_instance()
4327 
4328     def _shelve_offload_instance(self, context, instance, clean_shutdown):
4329         LOG.info('Shelve offloading', instance=instance)
4330         self._notify_about_instance_usage(context, instance,
4331                 'shelve_offload.start')
4332         compute_utils.notify_about_instance_action(context, instance,
4333                 self.host, action=fields.NotificationAction.SHELVE_OFFLOAD,
4334                 phase=fields.NotificationPhase.START)
4335 
4336         self._power_off_instance(context, instance, clean_shutdown)
4337         current_power_state = self._get_power_state(context, instance)
4338 
4339         self.network_api.cleanup_instance_network_on_host(context, instance,
4340                                                           instance.host)
4341         network_info = self.network_api.get_instance_nw_info(context, instance)
4342         block_device_info = self._get_instance_block_device_info(context,
4343                                                                  instance)
4344         self.driver.destroy(context, instance, network_info,
4345                 block_device_info)
4346 
4347         instance.power_state = current_power_state
4348         # NOTE(mriedem): The vm_state has to be set before updating the
4349         # resource tracker, see vm_states.ALLOW_RESOURCE_REMOVAL. The host/node
4350         # values cannot be nulled out until after updating the resource tracker
4351         # though.
4352         instance.vm_state = vm_states.SHELVED_OFFLOADED
4353         instance.task_state = None
4354         instance.save(expected_task_state=[task_states.SHELVING,
4355                                            task_states.SHELVING_OFFLOADING])
4356 
4357         # NOTE(ndipanov): Free resources from the resource tracker
4358         self._update_resource_tracker(context, instance)
4359 
4360         # NOTE(sfinucan): RPC calls should no longer be attempted against this
4361         # instance, so ensure any calls result in errors
4362         self._nil_out_instance_obj_host_and_node(instance)
4363         instance.save(expected_task_state=None)
4364 
4365         self._delete_scheduler_instance_info(context, instance.uuid)
4366         self._notify_about_instance_usage(context, instance,
4367                 'shelve_offload.end')
4368         compute_utils.notify_about_instance_action(context, instance,
4369                 self.host, action=fields.NotificationAction.SHELVE_OFFLOAD,
4370                 phase=fields.NotificationPhase.END)
4371 
4372     @wrap_exception()
4373     @reverts_task_state
4374     @wrap_instance_event(prefix='compute')
4375     @wrap_instance_fault
4376     def unshelve_instance(self, context, instance, image,
4377                           filter_properties, node):
4378         """Unshelve the instance.
4379 
4380         :param context: request context
4381         :param instance: a nova.objects.instance.Instance object
4382         :param image: an image to build from.  If None we assume a
4383             volume backed instance.
4384         :param filter_properties: dict containing limits, retry info etc.
4385         :param node: target compute node
4386         """
4387         if filter_properties is None:
4388             filter_properties = {}
4389 
4390         @utils.synchronized(instance.uuid)
4391         def do_unshelve_instance():
4392             self._unshelve_instance(context, instance, image,
4393                                     filter_properties, node)
4394         do_unshelve_instance()
4395 
4396     def _unshelve_instance_key_scrub(self, instance):
4397         """Remove data from the instance that may cause side effects."""
4398         cleaned_keys = dict(
4399                 key_data=instance.key_data,
4400                 auto_disk_config=instance.auto_disk_config)
4401         instance.key_data = None
4402         instance.auto_disk_config = False
4403         return cleaned_keys
4404 
4405     def _unshelve_instance_key_restore(self, instance, keys):
4406         """Restore previously scrubbed keys before saving the instance."""
4407         instance.update(keys)
4408 
4409     def _unshelve_instance(self, context, instance, image, filter_properties,
4410                            node):
4411         LOG.info('Unshelving', instance=instance)
4412         self._notify_about_instance_usage(context, instance, 'unshelve.start')
4413         compute_utils.notify_about_instance_action(context, instance,
4414                 self.host, action=fields.NotificationAction.UNSHELVE,
4415                 phase=fields.NotificationPhase.START)
4416 
4417         instance.task_state = task_states.SPAWNING
4418         instance.save()
4419 
4420         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
4421                 context, instance.uuid)
4422         block_device_info = self._prep_block_device(context, instance, bdms)
4423         scrubbed_keys = self._unshelve_instance_key_scrub(instance)
4424 
4425         if node is None:
4426             node = self.driver.get_available_nodes()[0]
4427             LOG.debug('No node specified, defaulting to %s', node,
4428                       instance=instance)
4429 
4430         rt = self._get_resource_tracker()
4431         limits = filter_properties.get('limits', {})
4432 
4433         shelved_image_ref = instance.image_ref
4434         if image:
4435             instance.image_ref = image['id']
4436             image_meta = objects.ImageMeta.from_dict(image)
4437         else:
4438             image_meta = objects.ImageMeta.from_dict(
4439                 utils.get_image_from_system_metadata(
4440                     instance.system_metadata))
4441 
4442         self.network_api.setup_instance_network_on_host(context, instance,
4443                                                         self.host)
4444         network_info = self.network_api.get_instance_nw_info(context, instance)
4445         try:
4446             with rt.instance_claim(context, instance, node, limits):
4447                 self.driver.spawn(context, instance, image_meta,
4448                                   injected_files=[],
4449                                   admin_password=None,
4450                                   network_info=network_info,
4451                                   block_device_info=block_device_info)
4452         except Exception:
4453             with excutils.save_and_reraise_exception():
4454                 LOG.exception('Instance failed to spawn',
4455                               instance=instance)
4456 
4457         if image:
4458             instance.image_ref = shelved_image_ref
4459             self._delete_snapshot_of_shelved_instance(context, instance,
4460                                                       image['id'])
4461 
4462         self._unshelve_instance_key_restore(instance, scrubbed_keys)
4463         self._update_instance_after_spawn(context, instance)
4464         # Delete system_metadata for a shelved instance
4465         compute_utils.remove_shelved_keys_from_system_metadata(instance)
4466 
4467         instance.save(expected_task_state=task_states.SPAWNING)
4468         self._update_scheduler_instance_info(context, instance)
4469         self._notify_about_instance_usage(context, instance, 'unshelve.end')
4470         compute_utils.notify_about_instance_action(context, instance,
4471                 self.host, action=fields.NotificationAction.UNSHELVE,
4472                 phase=fields.NotificationPhase.END)
4473 
4474     @messaging.expected_exceptions(NotImplementedError)
4475     @wrap_instance_fault
4476     def reset_network(self, context, instance):
4477         """Reset networking on the given instance."""
4478         LOG.debug('Reset network', instance=instance)
4479         self.driver.reset_network(instance)
4480 
4481     def _inject_network_info(self, context, instance, network_info):
4482         """Inject network info for the given instance."""
4483         LOG.debug('Inject network info', instance=instance)
4484         LOG.debug('network_info to inject: |%s|', network_info,
4485                   instance=instance)
4486 
4487         self.driver.inject_network_info(instance,
4488                                         network_info)
4489 
4490     @wrap_instance_fault
4491     def inject_network_info(self, context, instance):
4492         """Inject network info, but don't return the info."""
4493         network_info = self.network_api.get_instance_nw_info(context, instance)
4494         self._inject_network_info(context, instance, network_info)
4495 
4496     @messaging.expected_exceptions(NotImplementedError,
4497                                    exception.ConsoleNotAvailable,
4498                                    exception.InstanceNotFound)
4499     @wrap_exception()
4500     @wrap_instance_fault
4501     def get_console_output(self, context, instance, tail_length):
4502         """Send the console output for the given instance."""
4503         context = context.elevated()
4504         LOG.info("Get console output", instance=instance)
4505         output = self.driver.get_console_output(context, instance)
4506 
4507         if type(output) is six.text_type:
4508             output = six.b(output)
4509 
4510         if tail_length is not None:
4511             output = self._tail_log(output, tail_length)
4512 
4513         return output.decode('ascii', 'replace')
4514 
4515     def _tail_log(self, log, length):
4516         try:
4517             length = int(length)
4518         except ValueError:
4519             length = 0
4520 
4521         if length == 0:
4522             return b''
4523         else:
4524             return b'\n'.join(log.split(b'\n')[-int(length):])
4525 
4526     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
4527                                    exception.InstanceNotReady,
4528                                    exception.InstanceNotFound,
4529                                    exception.ConsoleTypeUnavailable,
4530                                    NotImplementedError)
4531     @wrap_exception()
4532     @wrap_instance_fault
4533     def get_vnc_console(self, context, console_type, instance):
4534         """Return connection information for a vnc console."""
4535         context = context.elevated()
4536         LOG.debug("Getting vnc console", instance=instance)
4537         token = uuidutils.generate_uuid()
4538 
4539         if not CONF.vnc.enabled:
4540             raise exception.ConsoleTypeUnavailable(console_type=console_type)
4541 
4542         if console_type == 'novnc':
4543             # For essex, novncproxy_base_url must include the full path
4544             # including the html file (like http://myhost/vnc_auto.html)
4545             access_url = '%s?token=%s' % (CONF.vnc.novncproxy_base_url, token)
4546         elif console_type == 'xvpvnc':
4547             access_url = '%s?token=%s' % (CONF.vnc.xvpvncproxy_base_url, token)
4548         else:
4549             raise exception.ConsoleTypeInvalid(console_type=console_type)
4550 
4551         try:
4552             # Retrieve connect info from driver, and then decorate with our
4553             # access info token
4554             console = self.driver.get_vnc_console(context, instance)
4555             connect_info = console.get_connection_info(token, access_url)
4556         except exception.InstanceNotFound:
4557             if instance.vm_state != vm_states.BUILDING:
4558                 raise
4559             raise exception.InstanceNotReady(instance_id=instance.uuid)
4560 
4561         return connect_info
4562 
4563     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
4564                                    exception.InstanceNotReady,
4565                                    exception.InstanceNotFound,
4566                                    exception.ConsoleTypeUnavailable,
4567                                    NotImplementedError)
4568     @wrap_exception()
4569     @wrap_instance_fault
4570     def get_spice_console(self, context, console_type, instance):
4571         """Return connection information for a spice console."""
4572         context = context.elevated()
4573         LOG.debug("Getting spice console", instance=instance)
4574         token = uuidutils.generate_uuid()
4575 
4576         if not CONF.spice.enabled:
4577             raise exception.ConsoleTypeUnavailable(console_type=console_type)
4578 
4579         if console_type == 'spice-html5':
4580             # For essex, spicehtml5proxy_base_url must include the full path
4581             # including the html file (like http://myhost/spice_auto.html)
4582             access_url = '%s?token=%s' % (CONF.spice.html5proxy_base_url,
4583                                           token)
4584         else:
4585             raise exception.ConsoleTypeInvalid(console_type=console_type)
4586 
4587         try:
4588             # Retrieve connect info from driver, and then decorate with our
4589             # access info token
4590             console = self.driver.get_spice_console(context, instance)
4591             connect_info = console.get_connection_info(token, access_url)
4592         except exception.InstanceNotFound:
4593             if instance.vm_state != vm_states.BUILDING:
4594                 raise
4595             raise exception.InstanceNotReady(instance_id=instance.uuid)
4596 
4597         return connect_info
4598 
4599     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
4600                                    exception.InstanceNotReady,
4601                                    exception.InstanceNotFound,
4602                                    exception.ConsoleTypeUnavailable,
4603                                    NotImplementedError)
4604     @wrap_exception()
4605     @wrap_instance_fault
4606     def get_rdp_console(self, context, console_type, instance):
4607         """Return connection information for a RDP console."""
4608         context = context.elevated()
4609         LOG.debug("Getting RDP console", instance=instance)
4610         token = uuidutils.generate_uuid()
4611 
4612         if not CONF.rdp.enabled:
4613             raise exception.ConsoleTypeUnavailable(console_type=console_type)
4614 
4615         if console_type == 'rdp-html5':
4616             access_url = '%s?token=%s' % (CONF.rdp.html5_proxy_base_url,
4617                                           token)
4618         else:
4619             raise exception.ConsoleTypeInvalid(console_type=console_type)
4620 
4621         try:
4622             # Retrieve connect info from driver, and then decorate with our
4623             # access info token
4624             console = self.driver.get_rdp_console(context, instance)
4625             connect_info = console.get_connection_info(token, access_url)
4626         except exception.InstanceNotFound:
4627             if instance.vm_state != vm_states.BUILDING:
4628                 raise
4629             raise exception.InstanceNotReady(instance_id=instance.uuid)
4630 
4631         return connect_info
4632 
4633     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
4634                                    exception.InstanceNotReady,
4635                                    exception.InstanceNotFound,
4636                                    exception.ConsoleTypeUnavailable,
4637                                    NotImplementedError)
4638     @wrap_exception()
4639     @wrap_instance_fault
4640     def get_mks_console(self, context, console_type, instance):
4641         """Return connection information for a MKS console."""
4642         context = context.elevated()
4643         LOG.debug("Getting MKS console", instance=instance)
4644         token = uuidutils.generate_uuid()
4645 
4646         if not CONF.mks.enabled:
4647             raise exception.ConsoleTypeUnavailable(console_type=console_type)
4648 
4649         if console_type == 'webmks':
4650             access_url = '%s?token=%s' % (CONF.mks.mksproxy_base_url,
4651                                           token)
4652         else:
4653             raise exception.ConsoleTypeInvalid(console_type=console_type)
4654 
4655         try:
4656             # Retrieve connect info from driver, and then decorate with our
4657             # access info token
4658             console = self.driver.get_mks_console(context, instance)
4659             connect_info = console.get_connection_info(token, access_url)
4660         except exception.InstanceNotFound:
4661             if instance.vm_state != vm_states.BUILDING:
4662                 raise
4663             raise exception.InstanceNotReady(instance_id=instance.uuid)
4664 
4665         return connect_info
4666 
4667     @messaging.expected_exceptions(
4668         exception.ConsoleTypeInvalid,
4669         exception.InstanceNotReady,
4670         exception.InstanceNotFound,
4671         exception.ConsoleTypeUnavailable,
4672         exception.SocketPortRangeExhaustedException,
4673         exception.ImageSerialPortNumberInvalid,
4674         exception.ImageSerialPortNumberExceedFlavorValue,
4675         NotImplementedError)
4676     @wrap_exception()
4677     @wrap_instance_fault
4678     def get_serial_console(self, context, console_type, instance):
4679         """Returns connection information for a serial console."""
4680 
4681         LOG.debug("Getting serial console", instance=instance)
4682 
4683         if not CONF.serial_console.enabled:
4684             raise exception.ConsoleTypeUnavailable(console_type=console_type)
4685 
4686         context = context.elevated()
4687 
4688         token = uuidutils.generate_uuid()
4689         access_url = '%s?token=%s' % (CONF.serial_console.base_url, token)
4690 
4691         try:
4692             # Retrieve connect info from driver, and then decorate with our
4693             # access info token
4694             console = self.driver.get_serial_console(context, instance)
4695             connect_info = console.get_connection_info(token, access_url)
4696         except exception.InstanceNotFound:
4697             if instance.vm_state != vm_states.BUILDING:
4698                 raise
4699             raise exception.InstanceNotReady(instance_id=instance.uuid)
4700 
4701         return connect_info
4702 
4703     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
4704                                    exception.InstanceNotReady,
4705                                    exception.InstanceNotFound)
4706     @wrap_exception()
4707     @wrap_instance_fault
4708     def validate_console_port(self, ctxt, instance, port, console_type):
4709         if console_type == "spice-html5":
4710             console_info = self.driver.get_spice_console(ctxt, instance)
4711         elif console_type == "rdp-html5":
4712             console_info = self.driver.get_rdp_console(ctxt, instance)
4713         elif console_type == "serial":
4714             console_info = self.driver.get_serial_console(ctxt, instance)
4715         elif console_type == "webmks":
4716             console_info = self.driver.get_mks_console(ctxt, instance)
4717         else:
4718             console_info = self.driver.get_vnc_console(ctxt, instance)
4719 
4720         return console_info.port == port
4721 
4722     @wrap_exception()
4723     @reverts_task_state
4724     @wrap_instance_fault
4725     def reserve_block_device_name(self, context, instance, device,
4726                                   volume_id, disk_bus, device_type, tag=None):
4727         if (tag and not
4728                 self.driver.capabilities.get('supports_tagged_attach_volume',
4729                                              False)):
4730             raise exception.VolumeTaggedAttachNotSupported()
4731 
4732         @utils.synchronized(instance.uuid)
4733         def do_reserve():
4734             bdms = (
4735                 objects.BlockDeviceMappingList.get_by_instance_uuid(
4736                     context, instance.uuid))
4737 
4738             # NOTE(ndipanov): We need to explicitly set all the fields on the
4739             #                 object so that obj_load_attr does not fail
4740             new_bdm = objects.BlockDeviceMapping(
4741                     context=context,
4742                     source_type='volume', destination_type='volume',
4743                     instance_uuid=instance.uuid, boot_index=None,
4744                     volume_id=volume_id,
4745                     device_name=device, guest_format=None,
4746                     disk_bus=disk_bus, device_type=device_type, tag=tag)
4747 
4748             new_bdm.device_name = self._get_device_name_for_instance(
4749                     instance, bdms, new_bdm)
4750 
4751             # NOTE(vish): create bdm here to avoid race condition
4752             new_bdm.create()
4753             return new_bdm
4754 
4755         return do_reserve()
4756 
4757     @wrap_exception()
4758     @wrap_instance_fault
4759     def attach_volume(self, context, instance, bdm):
4760         """Attach a volume to an instance."""
4761         driver_bdm = driver_block_device.convert_volume(bdm)
4762 
4763         @utils.synchronized(instance.uuid)
4764         def do_attach_volume(context, instance, driver_bdm):
4765             try:
4766                 return self._attach_volume(context, instance, driver_bdm)
4767             except Exception:
4768                 with excutils.save_and_reraise_exception():
4769                     bdm.destroy()
4770 
4771         do_attach_volume(context, instance, driver_bdm)
4772 
4773     def _attach_volume(self, context, instance, bdm):
4774         context = context.elevated()
4775         LOG.info('Attaching volume %(volume_id)s to %(mountpoint)s',
4776                  {'volume_id': bdm.volume_id,
4777                   'mountpoint': bdm['mount_device']},
4778                  instance=instance)
4779         compute_utils.notify_about_volume_attach_detach(
4780             context, instance, self.host,
4781             action=fields.NotificationAction.VOLUME_ATTACH,
4782             phase=fields.NotificationPhase.START,
4783             volume_id=bdm.volume_id)
4784         try:
4785             bdm.attach(context, instance, self.volume_api, self.driver,
4786                        do_driver_attach=True)
4787         except Exception as e:
4788             with excutils.save_and_reraise_exception():
4789                 LOG.exception("Failed to attach %(volume_id)s "
4790                               "at %(mountpoint)s",
4791                               {'volume_id': bdm.volume_id,
4792                                'mountpoint': bdm['mount_device']},
4793                               instance=instance)
4794                 self.volume_api.unreserve_volume(context, bdm.volume_id)
4795                 compute_utils.notify_about_volume_attach_detach(
4796                     context, instance, self.host,
4797                     action=fields.NotificationAction.VOLUME_ATTACH,
4798                     phase=fields.NotificationPhase.ERROR,
4799                     exception=e,
4800                     volume_id=bdm.volume_id)
4801 
4802         info = {'volume_id': bdm.volume_id}
4803         self._notify_about_instance_usage(
4804             context, instance, "volume.attach", extra_usage_info=info)
4805         compute_utils.notify_about_volume_attach_detach(
4806             context, instance, self.host,
4807             action=fields.NotificationAction.VOLUME_ATTACH,
4808             phase=fields.NotificationPhase.END,
4809             volume_id=bdm.volume_id)
4810 
4811     def _notify_volume_usage_detach(self, context, instance, bdm):
4812         if CONF.volume_usage_poll_interval <= 0:
4813             return
4814 
4815         vol_stats = []
4816         mp = bdm.device_name
4817         # Handle bootable volumes which will not contain /dev/
4818         if '/dev/' in mp:
4819             mp = mp[5:]
4820         try:
4821             vol_stats = self.driver.block_stats(instance, mp)
4822         except NotImplementedError:
4823             return
4824 
4825         LOG.debug("Updating volume usage cache with totals", instance=instance)
4826         rd_req, rd_bytes, wr_req, wr_bytes, flush_ops = vol_stats
4827         vol_usage = objects.VolumeUsage(context)
4828         vol_usage.volume_id = bdm.volume_id
4829         vol_usage.instance_uuid = instance.uuid
4830         vol_usage.project_id = instance.project_id
4831         vol_usage.user_id = instance.user_id
4832         vol_usage.availability_zone = instance.availability_zone
4833         vol_usage.curr_reads = rd_req
4834         vol_usage.curr_read_bytes = rd_bytes
4835         vol_usage.curr_writes = wr_req
4836         vol_usage.curr_write_bytes = wr_bytes
4837         vol_usage.save(update_totals=True)
4838         self.notifier.info(context, 'volume.usage',
4839                            compute_utils.usage_volume_info(vol_usage))
4840 
4841     def _detach_volume(self, context, bdm, instance, destroy_bdm=True,
4842                        attachment_id=None):
4843         """Detach a volume from an instance.
4844 
4845         :param context: security context
4846         :param bdm: nova.objects.BlockDeviceMapping volume bdm to detach
4847         :param instance: the Instance object to detach the volume from
4848         :param destroy_bdm: if True, the corresponding BDM entry will be marked
4849                             as deleted. Disabling this is useful for operations
4850                             like rebuild, when we don't want to destroy BDM
4851         :param attachment_id: The volume attachment_id for the given instance
4852                               and volume.
4853         """
4854         volume_id = bdm.volume_id
4855         compute_utils.notify_about_volume_attach_detach(
4856             context, instance, self.host,
4857             action=fields.NotificationAction.VOLUME_DETACH,
4858             phase=fields.NotificationPhase.START,
4859             volume_id=volume_id)
4860 
4861         self._notify_volume_usage_detach(context, instance, bdm)
4862 
4863         LOG.info('Detaching volume %(volume_id)s',
4864                  {'volume_id': volume_id}, instance=instance)
4865 
4866         driver_bdm = driver_block_device.convert_volume(bdm)
4867         driver_bdm.detach(context, instance, self.volume_api, self.driver,
4868                           attachment_id=attachment_id, destroy_bdm=destroy_bdm)
4869 
4870         info = dict(volume_id=volume_id)
4871         self._notify_about_instance_usage(
4872             context, instance, "volume.detach", extra_usage_info=info)
4873         compute_utils.notify_about_volume_attach_detach(
4874             context, instance, self.host,
4875             action=fields.NotificationAction.VOLUME_DETACH,
4876             phase=fields.NotificationPhase.END,
4877             volume_id=volume_id)
4878 
4879         if 'tag' in bdm and bdm.tag:
4880             self._delete_disk_metadata(instance, bdm)
4881         if destroy_bdm:
4882             bdm.destroy()
4883 
4884     def _delete_disk_metadata(self, instance, bdm):
4885         for device in instance.device_metadata.devices:
4886             if isinstance(device, objects.DiskMetadata):
4887                 if 'serial' in device:
4888                     if device.serial == bdm.volume_id:
4889                         instance.device_metadata.devices.remove(device)
4890                         instance.save()
4891                         break
4892                 else:
4893                     # NOTE(artom) We log the entire device object because all
4894                     # fields are nullable and may not be set
4895                     LOG.warning('Unable to determine whether to clean up '
4896                                 'device metadata for disk %s', device,
4897                                 instance=instance)
4898 
4899     @wrap_exception()
4900     @wrap_instance_fault
4901     def detach_volume(self, context, volume_id, instance, attachment_id=None):
4902         """Detach a volume from an instance.
4903 
4904         :param context: security context
4905         :param volume_id: the volume id
4906         :param instance: the Instance object to detach the volume from
4907         :param attachment_id: The volume attachment_id for the given instance
4908                               and volume.
4909 
4910         """
4911         bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
4912                 context, volume_id, instance.uuid)
4913         self._detach_volume(context, bdm, instance,
4914                             attachment_id=attachment_id)
4915 
4916     def _init_volume_connection(self, context, new_volume_id,
4917                                 old_volume_id, connector, bdm,
4918                                 new_attachment_id):
4919 
4920         if new_attachment_id is None:
4921             # We're dealing with an old-style attachment so initialize the
4922             # connection so we can get the connection_info.
4923             new_cinfo = self.volume_api.initialize_connection(context,
4924                                                               new_volume_id,
4925                                                               connector)
4926         else:
4927             # This is a new style attachment and the API created the new
4928             # volume attachment and passed the id to the compute over RPC.
4929             # At this point we need to update the new volume attachment with
4930             # the host connector, which will give us back the new attachment
4931             # connection_info.
4932             new_cinfo = self.volume_api.attachment_update(
4933                 context, new_attachment_id, connector)['connection_info']
4934 
4935         old_cinfo = jsonutils.loads(bdm['connection_info'])
4936         if old_cinfo and 'serial' not in old_cinfo:
4937             old_cinfo['serial'] = old_volume_id
4938         # NOTE(lyarwood): serial is not always present in the returned
4939         # connection_info so set it if it is missing as we do in
4940         # DriverVolumeBlockDevice.attach().
4941         if 'serial' not in new_cinfo:
4942             new_cinfo['serial'] = new_volume_id
4943         return (old_cinfo, new_cinfo)
4944 
4945     def _swap_volume(self, context, instance, bdm, connector,
4946                      old_volume_id, new_volume_id, resize_to,
4947                      new_attachment_id, is_cinder_migration):
4948         mountpoint = bdm['device_name']
4949         failed = False
4950         new_cinfo = None
4951         try:
4952             old_cinfo, new_cinfo = self._init_volume_connection(
4953                 context, new_volume_id, old_volume_id, connector,
4954                 bdm, new_attachment_id)
4955             # NOTE(lyarwood): The Libvirt driver, the only virt driver
4956             # currently implementing swap_volume, will modify the contents of
4957             # new_cinfo when connect_volume is called. This is then saved to
4958             # the BDM in swap_volume for future use outside of this flow.
4959             LOG.debug("swap_volume: Calling driver volume swap with "
4960                       "connection infos: new: %(new_cinfo)s; "
4961                       "old: %(old_cinfo)s",
4962                       {'new_cinfo': new_cinfo, 'old_cinfo': old_cinfo},
4963                       instance=instance)
4964             self.driver.swap_volume(old_cinfo, new_cinfo, instance, mountpoint,
4965                                     resize_to)
4966             LOG.debug("swap_volume: Driver volume swap returned, new "
4967                       "connection_info is now : %(new_cinfo)s",
4968                       {'new_cinfo': new_cinfo})
4969         except Exception as ex:
4970             failed = True
4971             with excutils.save_and_reraise_exception():
4972                 compute_utils.notify_about_volume_swap(
4973                     context, instance, self.host,
4974                     fields.NotificationAction.VOLUME_SWAP,
4975                     fields.NotificationPhase.ERROR,
4976                     old_volume_id, new_volume_id, ex)
4977                 if new_cinfo:
4978                     msg = ("Failed to swap volume %(old_volume_id)s "
4979                            "for %(new_volume_id)s")
4980                     LOG.exception(msg, {'old_volume_id': old_volume_id,
4981                                         'new_volume_id': new_volume_id},
4982                                   instance=instance)
4983                 else:
4984                     msg = ("Failed to connect to volume %(volume_id)s "
4985                            "with volume at %(mountpoint)s")
4986                     LOG.exception(msg, {'volume_id': new_volume_id,
4987                                         'mountpoint': bdm['device_name']},
4988                                   instance=instance)
4989 
4990                 # The API marked the volume as 'detaching' for the old volume
4991                 # so we need to roll that back so the volume goes back to
4992                 # 'in-use' state.
4993                 self.volume_api.roll_detaching(context, old_volume_id)
4994 
4995                 if new_attachment_id is None:
4996                     # The API reserved the new volume so it would be in
4997                     # 'attaching' status, so we need to unreserve it so it
4998                     # goes back to 'available' status.
4999                     self.volume_api.unreserve_volume(context, new_volume_id)
5000                 else:
5001                     # This is a new style attachment for the new volume, which
5002                     # was created in the API. We just need to delete it here
5003                     # to put the new volume back into 'available' status.
5004                     self.volume_api.attachment_delete(
5005                         context, new_attachment_id)
5006         finally:
5007             # TODO(mriedem): This finally block is terribly confusing and is
5008             # trying to do too much. We should consider removing the finally
5009             # block and move whatever needs to happen on success and failure
5010             # into the blocks above for clarity, even if it means a bit of
5011             # redundant code.
5012             conn_volume = new_volume_id if failed else old_volume_id
5013             if new_cinfo:
5014                 LOG.debug("swap_volume: removing Cinder connection "
5015                           "for volume %(volume)s", {'volume': conn_volume},
5016                           instance=instance)
5017                 if bdm.attachment_id is None:
5018                     # This is the pre-3.27 flow for new-style volume
5019                     # attachments so just terminate the connection.
5020                     self.volume_api.terminate_connection(context,
5021                                                          conn_volume,
5022                                                          connector)
5023                 else:
5024                     # This is a new style volume attachment. If we failed, then
5025                     # the new attachment was already deleted above in the
5026                     # exception block and we have nothing more to do here. If
5027                     # swap_volume was successful in the driver, then we need to
5028                     # "detach" the original attachment by deleting it.
5029                     if not failed:
5030                         self.volume_api.attachment_delete(
5031                             context, bdm.attachment_id)
5032 
5033             # Need to make some decisions based on whether this was
5034             # a Cinder initiated migration or not. The callback to
5035             # migration completion isn't needed in the case of a
5036             # nova initiated simple swap of two volume
5037             # "volume-update" call so skip that. The new attachment
5038             # scenarios will give us a new attachment record and
5039             # that's what we want.
5040             if bdm.attachment_id and not is_cinder_migration:
5041                 # we don't callback to cinder
5042                 comp_ret = {'save_volume_id': new_volume_id}
5043             else:
5044                 # NOTE(lyarwood): The following call to
5045                 # os-migrate-volume-completion returns a dict containing
5046                 # save_volume_id, this volume id has two possible values :
5047                 # 1. old_volume_id if we are migrating (retyping) volumes
5048                 # 2. new_volume_id if we are swapping between two existing
5049                 #    volumes
5050                 # This volume id is later used to update the volume_id and
5051                 # connection_info['serial'] of the BDM.
5052                 comp_ret = self.volume_api.migrate_volume_completion(
5053                                                           context,
5054                                                           old_volume_id,
5055                                                           new_volume_id,
5056                                                           error=failed)
5057                 LOG.debug("swap_volume: Cinder migrate_volume_completion "
5058                           "returned: %(comp_ret)s", {'comp_ret': comp_ret},
5059                           instance=instance)
5060 
5061         return (comp_ret, new_cinfo)
5062 
5063     @wrap_exception()
5064     @wrap_instance_fault
5065     def swap_volume(self, context, old_volume_id, new_volume_id, instance,
5066                     new_attachment_id=None):
5067         """Swap volume for an instance."""
5068         context = context.elevated()
5069 
5070         compute_utils.notify_about_volume_swap(
5071             context, instance, self.host,
5072             fields.NotificationAction.VOLUME_SWAP,
5073             fields.NotificationPhase.START,
5074             old_volume_id, new_volume_id)
5075 
5076         bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
5077                 context, old_volume_id, instance.uuid)
5078         connector = self.driver.get_volume_connector(instance)
5079 
5080         resize_to = 0
5081         old_volume = self.volume_api.get(context, old_volume_id)
5082         # Yes this is a tightly-coupled state check of what's going on inside
5083         # cinder, but we need this while we still support old (v1/v2) and
5084         # new style attachments (v3.27). Once we drop support for old style
5085         # attachments we could think about cleaning up the cinder-initiated
5086         # swap volume API flows.
5087         is_cinder_migration = (
5088             True if old_volume['status'] in ('retyping',
5089                                              'migrating') else False)
5090         old_vol_size = old_volume['size']
5091         new_vol_size = self.volume_api.get(context, new_volume_id)['size']
5092         if new_vol_size > old_vol_size:
5093             resize_to = new_vol_size
5094 
5095         LOG.info('Swapping volume %(old_volume)s for %(new_volume)s',
5096                  {'old_volume': old_volume_id, 'new_volume': new_volume_id},
5097                  instance=instance)
5098         comp_ret, new_cinfo = self._swap_volume(context,
5099                                                 instance,
5100                                                 bdm,
5101                                                 connector,
5102                                                 old_volume_id,
5103                                                 new_volume_id,
5104                                                 resize_to,
5105                                                 new_attachment_id,
5106                                                 is_cinder_migration)
5107 
5108         # NOTE(lyarwood): Update the BDM with the modified new_cinfo and
5109         # correct volume_id returned by Cinder.
5110         save_volume_id = comp_ret['save_volume_id']
5111         new_cinfo['serial'] = save_volume_id
5112         values = {
5113             'connection_info': jsonutils.dumps(new_cinfo),
5114             'source_type': 'volume',
5115             'destination_type': 'volume',
5116             'snapshot_id': None,
5117             'volume_id': save_volume_id,
5118             'no_device': None}
5119 
5120         if resize_to:
5121             values['volume_size'] = resize_to
5122 
5123         if new_attachment_id is not None:
5124             # This was a volume swap for a new-style attachment so we
5125             # need to update the BDM attachment_id for the new attachment.
5126             values['attachment_id'] = new_attachment_id
5127 
5128         LOG.debug("swap_volume: Updating volume %(volume_id)s BDM record with "
5129                   "%(updates)s", {'volume_id': bdm.volume_id,
5130                                   'updates': values},
5131                   instance=instance)
5132         bdm.update(values)
5133         bdm.save()
5134 
5135         compute_utils.notify_about_volume_swap(
5136             context, instance, self.host,
5137             fields.NotificationAction.VOLUME_SWAP,
5138             fields.NotificationPhase.END,
5139             old_volume_id, new_volume_id)
5140 
5141     @wrap_exception()
5142     def remove_volume_connection(self, context, volume_id, instance):
5143         """Remove a volume connection using the volume api."""
5144         # NOTE(vish): We don't want to actually mark the volume
5145         #             detached, or delete the bdm, just remove the
5146         #             connection from this host.
5147 
5148         try:
5149             bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
5150                     context, volume_id, instance.uuid)
5151             driver_bdm = driver_block_device.convert_volume(bdm)
5152             driver_bdm.driver_detach(context, instance,
5153                                      self.volume_api, self.driver)
5154             connector = self.driver.get_volume_connector(instance)
5155             self.volume_api.terminate_connection(context, volume_id, connector)
5156         except exception.NotFound:
5157             pass
5158 
5159     @wrap_exception()
5160     @wrap_instance_fault
5161     def attach_interface(self, context, instance, network_id, port_id,
5162                          requested_ip, tag=None):
5163         """Use hotplug to add an network adapter to an instance."""
5164         if not self.driver.capabilities['supports_attach_interface']:
5165             raise exception.AttachInterfaceNotSupported(
5166                 instance_uuid=instance.uuid)
5167         if (tag and not
5168             self.driver.capabilities.get('supports_tagged_attach_interface',
5169                                          False)):
5170             raise exception.NetworkInterfaceTaggedAttachNotSupported()
5171         bind_host_id = self.driver.network_binding_host_id(context, instance)
5172         network_info = self.network_api.allocate_port_for_instance(
5173             context, instance, port_id, network_id, requested_ip,
5174             bind_host_id=bind_host_id, tag=tag)
5175         if len(network_info) != 1:
5176             LOG.error('allocate_port_for_instance returned %(ports)s '
5177                       'ports', {'ports': len(network_info)})
5178             raise exception.InterfaceAttachFailed(
5179                     instance_uuid=instance.uuid)
5180         image_meta = objects.ImageMeta.from_instance(instance)
5181 
5182         try:
5183             self.driver.attach_interface(context, instance, image_meta,
5184                                          network_info[0])
5185         except exception.NovaException as ex:
5186             port_id = network_info[0].get('id')
5187             LOG.warning("attach interface failed , try to deallocate "
5188                         "port %(port_id)s, reason: %(msg)s",
5189                         {'port_id': port_id, 'msg': ex},
5190                         instance=instance)
5191             try:
5192                 self.network_api.deallocate_port_for_instance(
5193                     context, instance, port_id)
5194             except Exception:
5195                 LOG.warning("deallocate port %(port_id)s failed",
5196                             {'port_id': port_id}, instance=instance)
5197             raise exception.InterfaceAttachFailed(
5198                 instance_uuid=instance.uuid)
5199 
5200         return network_info[0]
5201 
5202     @wrap_exception()
5203     @wrap_instance_fault
5204     def detach_interface(self, context, instance, port_id):
5205         """Detach a network adapter from an instance."""
5206         network_info = instance.info_cache.network_info
5207         condemned = None
5208         for vif in network_info:
5209             if vif['id'] == port_id:
5210                 condemned = vif
5211                 break
5212         if condemned is None:
5213             raise exception.PortNotFound(_("Port %s is not "
5214                                            "attached") % port_id)
5215         try:
5216             self.driver.detach_interface(context, instance, condemned)
5217         except exception.NovaException as ex:
5218             LOG.warning("Detach interface failed, port_id=%(port_id)s,"
5219                         " reason: %(msg)s",
5220                         {'port_id': port_id, 'msg': ex}, instance=instance)
5221             raise exception.InterfaceDetachFailed(instance_uuid=instance.uuid)
5222         else:
5223             try:
5224                 self.network_api.deallocate_port_for_instance(
5225                     context, instance, port_id)
5226             except Exception as ex:
5227                 with excutils.save_and_reraise_exception():
5228                     # Since this is a cast operation, log the failure for
5229                     # triage.
5230                     LOG.warning('Failed to deallocate port %(port_id)s '
5231                                 'for instance. Error: %(error)s',
5232                                 {'port_id': port_id, 'error': ex},
5233                                 instance=instance)
5234 
5235     def _get_compute_info(self, context, host):
5236         return objects.ComputeNode.get_first_node_by_host_for_old_compat(
5237             context, host)
5238 
5239     @wrap_exception()
5240     def check_instance_shared_storage(self, ctxt, instance, data):
5241         """Check if the instance files are shared
5242 
5243         :param ctxt: security context
5244         :param instance: dict of instance data
5245         :param data: result of driver.check_instance_shared_storage_local
5246 
5247         Returns True if instance disks located on shared storage and
5248         False otherwise.
5249         """
5250         return self.driver.check_instance_shared_storage_remote(ctxt, data)
5251 
5252     @wrap_exception()
5253     @wrap_instance_event(prefix='compute')
5254     @wrap_instance_fault
5255     def check_can_live_migrate_destination(self, ctxt, instance,
5256                                            block_migration, disk_over_commit):
5257         """Check if it is possible to execute live migration.
5258 
5259         This runs checks on the destination host, and then calls
5260         back to the source host to check the results.
5261 
5262         :param context: security context
5263         :param instance: dict of instance data
5264         :param block_migration: if true, prepare for block migration
5265                                 if None, calculate it in driver
5266         :param disk_over_commit: if true, allow disk over commit
5267                                  if None, ignore disk usage checking
5268         :returns: a dict containing migration info
5269         """
5270         return self._do_check_can_live_migrate_destination(ctxt, instance,
5271                                                             block_migration,
5272                                                             disk_over_commit)
5273 
5274     def _do_check_can_live_migrate_destination(self, ctxt, instance,
5275                                                block_migration,
5276                                                disk_over_commit):
5277         src_compute_info = obj_base.obj_to_primitive(
5278             self._get_compute_info(ctxt, instance.host))
5279         dst_compute_info = obj_base.obj_to_primitive(
5280             self._get_compute_info(ctxt, CONF.host))
5281         dest_check_data = self.driver.check_can_live_migrate_destination(ctxt,
5282             instance, src_compute_info, dst_compute_info,
5283             block_migration, disk_over_commit)
5284         LOG.debug('destination check data is %s', dest_check_data)
5285         try:
5286             migrate_data = self.compute_rpcapi.\
5287                                 check_can_live_migrate_source(ctxt, instance,
5288                                                               dest_check_data)
5289         finally:
5290             self.driver.cleanup_live_migration_destination_check(ctxt,
5291                     dest_check_data)
5292         return migrate_data
5293 
5294     @wrap_exception()
5295     @wrap_instance_event(prefix='compute')
5296     @wrap_instance_fault
5297     def check_can_live_migrate_source(self, ctxt, instance, dest_check_data):
5298         """Check if it is possible to execute live migration.
5299 
5300         This checks if the live migration can succeed, based on the
5301         results from check_can_live_migrate_destination.
5302 
5303         :param ctxt: security context
5304         :param instance: dict of instance data
5305         :param dest_check_data: result of check_can_live_migrate_destination
5306         :returns: a dict containing migration info
5307         """
5308         is_volume_backed = compute_utils.is_volume_backed_instance(ctxt,
5309                                                                       instance)
5310         # TODO(tdurakov): remove dict to object conversion once RPC API version
5311         # is bumped to 5.x
5312         got_migrate_data_object = isinstance(dest_check_data,
5313                                              migrate_data_obj.LiveMigrateData)
5314         if not got_migrate_data_object:
5315             dest_check_data = \
5316                 migrate_data_obj.LiveMigrateData.detect_implementation(
5317                     dest_check_data)
5318         dest_check_data.is_volume_backed = is_volume_backed
5319         block_device_info = self._get_instance_block_device_info(
5320                             ctxt, instance, refresh_conn_info=False)
5321         result = self.driver.check_can_live_migrate_source(ctxt, instance,
5322                                                            dest_check_data,
5323                                                            block_device_info)
5324         if not got_migrate_data_object:
5325             result = result.to_legacy_dict()
5326         LOG.debug('source check data is %s', result)
5327         return result
5328 
5329     @wrap_exception()
5330     @wrap_instance_event(prefix='compute')
5331     @wrap_instance_fault
5332     def pre_live_migration(self, context, instance, block_migration, disk,
5333                            migrate_data):
5334         """Preparations for live migration at dest host.
5335 
5336         :param context: security context
5337         :param instance: dict of instance data
5338         :param block_migration: if true, prepare for block migration
5339         :param migrate_data: A dict or LiveMigrateData object holding data
5340                              required for live migration without shared
5341                              storage.
5342 
5343         """
5344         LOG.debug('pre_live_migration data is %s', migrate_data)
5345         # TODO(tdurakov): remove dict to object conversion once RPC API version
5346         # is bumped to 5.x
5347         got_migrate_data_object = isinstance(migrate_data,
5348                                              migrate_data_obj.LiveMigrateData)
5349         if not got_migrate_data_object:
5350             migrate_data = \
5351                 migrate_data_obj.LiveMigrateData.detect_implementation(
5352                     migrate_data)
5353         block_device_info = self._get_instance_block_device_info(
5354                             context, instance, refresh_conn_info=True)
5355 
5356         network_info = self.network_api.get_instance_nw_info(context, instance)
5357         self._notify_about_instance_usage(
5358                      context, instance, "live_migration.pre.start",
5359                      network_info=network_info)
5360 
5361         migrate_data = self.driver.pre_live_migration(context,
5362                                        instance,
5363                                        block_device_info,
5364                                        network_info,
5365                                        disk,
5366                                        migrate_data)
5367         LOG.debug('driver pre_live_migration data is %s', migrate_data)
5368 
5369         # NOTE(tr3buchet): setup networks on destination host
5370         self.network_api.setup_networks_on_host(context, instance,
5371                                                          self.host)
5372 
5373         # Creating filters to hypervisors and firewalls.
5374         # An example is that nova-instance-instance-xxx,
5375         # which is written to libvirt.xml(Check "virsh nwfilter-list")
5376         # This nwfilter is necessary on the destination host.
5377         # In addition, this method is creating filtering rule
5378         # onto destination host.
5379         self.driver.ensure_filtering_rules_for_instance(instance,
5380                                             network_info)
5381 
5382         self._notify_about_instance_usage(
5383                      context, instance, "live_migration.pre.end",
5384                      network_info=network_info)
5385         # TODO(tdurakov): remove dict to object conversion once RPC API version
5386         # is bumped to 5.x
5387         if not got_migrate_data_object and migrate_data:
5388             migrate_data = migrate_data.to_legacy_dict(
5389                 pre_migration_result=True)
5390             migrate_data = migrate_data['pre_live_migration_result']
5391         LOG.debug('pre_live_migration result data is %s', migrate_data)
5392         return migrate_data
5393 
5394     def _do_live_migration(self, context, dest, instance, block_migration,
5395                            migration, migrate_data):
5396         # NOTE(danms): We should enhance the RT to account for migrations
5397         # and use the status field to denote when the accounting has been
5398         # done on source/destination. For now, this is just here for status
5399         # reporting
5400         self._set_migration_status(migration, 'preparing')
5401 
5402         got_migrate_data_object = isinstance(migrate_data,
5403                                              migrate_data_obj.LiveMigrateData)
5404         if not got_migrate_data_object:
5405             migrate_data = \
5406                 migrate_data_obj.LiveMigrateData.detect_implementation(
5407                     migrate_data)
5408 
5409         try:
5410             if ('block_migration' in migrate_data and
5411                     migrate_data.block_migration):
5412                 block_device_info = self._get_instance_block_device_info(
5413                     context, instance)
5414                 disk = self.driver.get_instance_disk_info(
5415                     instance, block_device_info=block_device_info)
5416             else:
5417                 disk = None
5418 
5419             migrate_data = self.compute_rpcapi.pre_live_migration(
5420                 context, instance,
5421                 block_migration, disk, dest, migrate_data)
5422         except Exception:
5423             with excutils.save_and_reraise_exception():
5424                 LOG.exception('Pre live migration failed at %s',
5425                               dest, instance=instance)
5426                 self._set_migration_status(migration, 'error')
5427                 self._rollback_live_migration(context, instance, dest,
5428                                               migrate_data)
5429 
5430         self._set_migration_status(migration, 'running')
5431 
5432         if migrate_data:
5433             migrate_data.migration = migration
5434         LOG.debug('live_migration data is %s', migrate_data)
5435         try:
5436             self.driver.live_migration(context, instance, dest,
5437                                        self._post_live_migration,
5438                                        self._rollback_live_migration,
5439                                        block_migration, migrate_data)
5440         except Exception:
5441             LOG.exception('Live migration failed.', instance=instance)
5442             with excutils.save_and_reraise_exception():
5443                 # Put instance and migration into error state,
5444                 # as its almost certainly too late to rollback
5445                 self._set_migration_status(migration, 'error')
5446                 # first refresh instance as it may have got updated by
5447                 # post_live_migration_at_destination
5448                 instance.refresh()
5449                 self._set_instance_obj_error_state(context, instance,
5450                                                    clean_task_state=True)
5451 
5452     @wrap_exception()
5453     @wrap_instance_event(prefix='compute')
5454     @wrap_instance_fault
5455     def live_migration(self, context, dest, instance, block_migration,
5456                        migration, migrate_data):
5457         """Executing live migration.
5458 
5459         :param context: security context
5460         :param dest: destination host
5461         :param instance: a nova.objects.instance.Instance object
5462         :param block_migration: if true, prepare for block migration
5463         :param migration: an nova.objects.Migration object
5464         :param migrate_data: implementation specific params
5465 
5466         """
5467         self._set_migration_status(migration, 'queued')
5468 
5469         def dispatch_live_migration(*args, **kwargs):
5470             with self._live_migration_semaphore:
5471                 self._do_live_migration(*args, **kwargs)
5472 
5473         # NOTE(danms): We spawn here to return the RPC worker thread back to
5474         # the pool. Since what follows could take a really long time, we don't
5475         # want to tie up RPC workers.
5476         utils.spawn_n(dispatch_live_migration,
5477                       context, dest, instance,
5478                       block_migration, migration,
5479                       migrate_data)
5480 
5481     # TODO(tdurakov): migration_id is used since 4.12 rpc api version
5482     # remove migration_id parameter when the compute RPC version
5483     # is bumped to 5.x.
5484     @wrap_exception()
5485     @wrap_instance_event(prefix='compute')
5486     @wrap_instance_fault
5487     def live_migration_force_complete(self, context, instance,
5488                                       migration_id=None):
5489         """Force live migration to complete.
5490 
5491         :param context: Security context
5492         :param instance: The instance that is being migrated
5493         :param migration_id: ID of ongoing migration; is currently not used,
5494         and isn't removed for backward compatibility
5495         """
5496 
5497         self._notify_about_instance_usage(
5498             context, instance, 'live.migration.force.complete.start')
5499         self.driver.live_migration_force_complete(instance)
5500         self._notify_about_instance_usage(
5501             context, instance, 'live.migration.force.complete.end')
5502 
5503     @wrap_exception()
5504     @wrap_instance_event(prefix='compute')
5505     @wrap_instance_fault
5506     def live_migration_abort(self, context, instance, migration_id):
5507         """Abort an in-progress live migration.
5508 
5509         :param context: Security context
5510         :param instance: The instance that is being migrated
5511         :param migration_id: ID of in-progress live migration
5512 
5513         """
5514         migration = objects.Migration.get_by_id(context, migration_id)
5515         if migration.status != 'running':
5516             raise exception.InvalidMigrationState(migration_id=migration_id,
5517                     instance_uuid=instance.uuid,
5518                     state=migration.status,
5519                     method='abort live migration')
5520 
5521         self._notify_about_instance_usage(
5522             context, instance, 'live.migration.abort.start')
5523         self.driver.live_migration_abort(instance)
5524         self._notify_about_instance_usage(
5525             context, instance, 'live.migration.abort.end')
5526 
5527     def _live_migration_cleanup_flags(self, migrate_data):
5528         """Determine whether disks or instance path need to be cleaned up after
5529         live migration (at source on success, at destination on rollback)
5530 
5531         Block migration needs empty image at destination host before migration
5532         starts, so if any failure occurs, any empty images has to be deleted.
5533 
5534         Also Volume backed live migration w/o shared storage needs to delete
5535         newly created instance-xxx dir on the destination as a part of its
5536         rollback process
5537 
5538         :param migrate_data: implementation specific data
5539         :returns: (bool, bool) -- do_cleanup, destroy_disks
5540         """
5541         # NOTE(pkoniszewski): block migration specific params are set inside
5542         # migrate_data objects for drivers that expose block live migration
5543         # information (i.e. Libvirt, Xenapi and HyperV). For other drivers
5544         # cleanup is not needed.
5545         is_shared_block_storage = True
5546         is_shared_instance_path = True
5547         if isinstance(migrate_data, migrate_data_obj.LibvirtLiveMigrateData):
5548             is_shared_block_storage = migrate_data.is_shared_block_storage
5549             is_shared_instance_path = migrate_data.is_shared_instance_path
5550         elif isinstance(migrate_data, migrate_data_obj.XenapiLiveMigrateData):
5551             is_shared_block_storage = not migrate_data.block_migration
5552             is_shared_instance_path = not migrate_data.block_migration
5553         elif isinstance(migrate_data, migrate_data_obj.HyperVLiveMigrateData):
5554             is_shared_instance_path = migrate_data.is_shared_instance_path
5555             is_shared_block_storage = migrate_data.is_shared_instance_path
5556 
5557         # No instance booting at source host, but instance dir
5558         # must be deleted for preparing next block migration
5559         # must be deleted for preparing next live migration w/o shared storage
5560         do_cleanup = not is_shared_instance_path
5561         destroy_disks = not is_shared_block_storage
5562 
5563         return (do_cleanup, destroy_disks)
5564 
5565     @wrap_exception()
5566     @wrap_instance_fault
5567     def _post_live_migration(self, ctxt, instance,
5568                             dest, block_migration=False, migrate_data=None):
5569         """Post operations for live migration.
5570 
5571         This method is called from live_migration
5572         and mainly updating database record.
5573 
5574         :param ctxt: security context
5575         :param instance: instance dict
5576         :param dest: destination host
5577         :param block_migration: if true, prepare for block migration
5578         :param migrate_data: if not None, it is a dict which has data
5579         required for live migration without shared storage
5580 
5581         """
5582         LOG.info('_post_live_migration() is started..',
5583                  instance=instance)
5584 
5585         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
5586                 ctxt, instance.uuid)
5587 
5588         # Cleanup source host post live-migration
5589         block_device_info = self._get_instance_block_device_info(
5590                             ctxt, instance, bdms=bdms)
5591         self.driver.post_live_migration(ctxt, instance, block_device_info,
5592                                         migrate_data)
5593 
5594         # Detaching volumes.
5595         connector = self.driver.get_volume_connector(instance)
5596         for bdm in bdms:
5597             # NOTE(vish): We don't want to actually mark the volume
5598             #             detached, or delete the bdm, just remove the
5599             #             connection from this host.
5600 
5601             # remove the volume connection without detaching from hypervisor
5602             # because the instance is not running anymore on the current host
5603             if bdm.is_volume:
5604                 self.volume_api.terminate_connection(ctxt, bdm.volume_id,
5605                                                      connector)
5606 
5607         # Releasing vlan.
5608         # (not necessary in current implementation?)
5609 
5610         network_info = self.network_api.get_instance_nw_info(ctxt, instance)
5611 
5612         self._notify_about_instance_usage(ctxt, instance,
5613                                           "live_migration._post.start",
5614                                           network_info=network_info)
5615         # Releasing security group ingress rule.
5616         LOG.debug('Calling driver.unfilter_instance from _post_live_migration',
5617                   instance=instance)
5618         self.driver.unfilter_instance(instance,
5619                                       network_info)
5620 
5621         migration = {'source_compute': self.host,
5622                      'dest_compute': dest, }
5623         self.network_api.migrate_instance_start(ctxt,
5624                                                 instance,
5625                                                 migration)
5626 
5627         destroy_vifs = False
5628         try:
5629             self.driver.post_live_migration_at_source(ctxt, instance,
5630                                                       network_info)
5631         except NotImplementedError as ex:
5632             LOG.debug(ex, instance=instance)
5633             # For all hypervisors other than libvirt, there is a possibility
5634             # they are unplugging networks from source node in the cleanup
5635             # method
5636             destroy_vifs = True
5637 
5638         # Define domain at destination host, without doing it,
5639         # pause/suspend/terminate do not work.
5640         post_at_dest_success = True
5641         try:
5642             self.compute_rpcapi.post_live_migration_at_destination(ctxt,
5643                     instance, block_migration, dest)
5644         except Exception as error:
5645             post_at_dest_success = False
5646             # We don't want to break _post_live_migration() if
5647             # post_live_migration_at_destination() fails as it should never
5648             # affect cleaning up source node.
5649             LOG.exception("Post live migration at destination %s failed",
5650                           dest, instance=instance, error=error)
5651 
5652         do_cleanup, destroy_disks = self._live_migration_cleanup_flags(
5653                 migrate_data)
5654 
5655         if do_cleanup:
5656             LOG.debug('Calling driver.cleanup from _post_live_migration',
5657                       instance=instance)
5658             self.driver.cleanup(ctxt, instance, network_info,
5659                                 destroy_disks=destroy_disks,
5660                                 migrate_data=migrate_data,
5661                                 destroy_vifs=destroy_vifs)
5662 
5663         self.instance_events.clear_events_for_instance(instance)
5664 
5665         # NOTE(timello): make sure we update available resources on source
5666         # host even before next periodic task.
5667         self.update_available_resource(ctxt)
5668 
5669         self._update_scheduler_instance_info(ctxt, instance)
5670         self._notify_about_instance_usage(ctxt, instance,
5671                                           "live_migration._post.end",
5672                                           network_info=network_info)
5673         if post_at_dest_success:
5674             LOG.info('Migrating instance to %s finished successfully.',
5675                      dest, instance=instance)
5676 
5677         if migrate_data and migrate_data.obj_attr_is_set('migration'):
5678             migrate_data.migration.status = 'completed'
5679             migrate_data.migration.save()
5680 
5681     def _consoles_enabled(self):
5682         """Returns whether a console is enable."""
5683         return (CONF.vnc.enabled or CONF.spice.enabled or
5684                 CONF.rdp.enabled or CONF.serial_console.enabled or
5685                 CONF.mks.enabled)
5686 
5687     @wrap_exception()
5688     @wrap_instance_event(prefix='compute')
5689     @wrap_instance_fault
5690     def post_live_migration_at_destination(self, context, instance,
5691                                            block_migration):
5692         """Post operations for live migration .
5693 
5694         :param context: security context
5695         :param instance: Instance dict
5696         :param block_migration: if true, prepare for block migration
5697 
5698         """
5699         LOG.info('Post operation of migration started',
5700                  instance=instance)
5701 
5702         # NOTE(tr3buchet): setup networks on destination host
5703         #                  this is called a second time because
5704         #                  multi_host does not create the bridge in
5705         #                  plug_vifs
5706         self.network_api.setup_networks_on_host(context, instance,
5707                                                          self.host)
5708         migration = {'source_compute': instance.host,
5709                      'dest_compute': self.host, }
5710         self.network_api.migrate_instance_finish(context,
5711                                                  instance,
5712                                                  migration)
5713 
5714         network_info = self.network_api.get_instance_nw_info(context, instance)
5715         self._notify_about_instance_usage(
5716                      context, instance, "live_migration.post.dest.start",
5717                      network_info=network_info)
5718         block_device_info = self._get_instance_block_device_info(context,
5719                                                                  instance)
5720 
5721         try:
5722             self.driver.post_live_migration_at_destination(
5723                 context, instance, network_info, block_migration,
5724                 block_device_info)
5725         except Exception:
5726             with excutils.save_and_reraise_exception():
5727                 instance.vm_state = vm_states.ERROR
5728                 LOG.error('Unexpected error during post live migration at '
5729                           'destination host.', instance=instance)
5730         finally:
5731             # Restore instance state and update host
5732             current_power_state = self._get_power_state(context, instance)
5733             node_name = None
5734             prev_host = instance.host
5735             try:
5736                 compute_node = self._get_compute_info(context, self.host)
5737                 node_name = compute_node.hypervisor_hostname
5738             except exception.ComputeHostNotFound:
5739                 LOG.exception('Failed to get compute_info for %s', self.host)
5740             finally:
5741                 instance.host = self.host
5742                 instance.power_state = current_power_state
5743                 instance.task_state = None
5744                 instance.node = node_name
5745                 instance.progress = 0
5746                 instance.save(expected_task_state=task_states.MIGRATING)
5747 
5748         # NOTE(tr3buchet): tear down networks on source host
5749         self.network_api.setup_networks_on_host(context, instance,
5750                                                 prev_host, teardown=True)
5751         # NOTE(vish): this is necessary to update dhcp
5752         self.network_api.setup_networks_on_host(context, instance, self.host)
5753         self._notify_about_instance_usage(
5754                      context, instance, "live_migration.post.dest.end",
5755                      network_info=network_info)
5756 
5757     @wrap_exception()
5758     @wrap_instance_fault
5759     def _rollback_live_migration(self, context, instance,
5760                                  dest, migrate_data=None,
5761                                  migration_status='error'):
5762         """Recovers Instance/volume state from migrating -> running.
5763 
5764         :param context: security context
5765         :param instance: nova.objects.instance.Instance object
5766         :param dest:
5767             This method is called from live migration src host.
5768             This param specifies destination host.
5769         :param migrate_data:
5770             if not none, contains implementation specific data.
5771         :param migration_status:
5772             Contains the status we want to set for the migration object
5773 
5774         """
5775         instance.task_state = None
5776         instance.progress = 0
5777         instance.save(expected_task_state=[task_states.MIGRATING])
5778 
5779         # TODO(tdurakov): remove dict to object conversion once RPC API version
5780         # is bumped to 5.x
5781         if isinstance(migrate_data, dict):
5782             migration = migrate_data.pop('migration', None)
5783             migrate_data = \
5784                 migrate_data_obj.LiveMigrateData.detect_implementation(
5785                     migrate_data)
5786         elif (isinstance(migrate_data, migrate_data_obj.LiveMigrateData) and
5787               migrate_data.obj_attr_is_set('migration')):
5788             migration = migrate_data.migration
5789         else:
5790             migration = None
5791 
5792         # NOTE(tr3buchet): setup networks on source host (really it's re-setup)
5793         self.network_api.setup_networks_on_host(context, instance, self.host)
5794 
5795         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
5796                 context, instance.uuid)
5797         for bdm in bdms:
5798             if bdm.is_volume:
5799                 self.compute_rpcapi.remove_volume_connection(
5800                         context, instance, bdm.volume_id, dest)
5801 
5802         self._notify_about_instance_usage(context, instance,
5803                                           "live_migration._rollback.start")
5804         compute_utils.notify_about_instance_action(context, instance,
5805                 self.host,
5806                 action=fields.NotificationAction.LIVE_MIGRATION_ROLLBACK,
5807                 phase=fields.NotificationPhase.START)
5808 
5809         do_cleanup, destroy_disks = self._live_migration_cleanup_flags(
5810                 migrate_data)
5811 
5812         if do_cleanup:
5813             self.compute_rpcapi.rollback_live_migration_at_destination(
5814                     context, instance, dest, destroy_disks=destroy_disks,
5815                     migrate_data=migrate_data)
5816 
5817         self._notify_about_instance_usage(context, instance,
5818                                           "live_migration._rollback.end")
5819         compute_utils.notify_about_instance_action(context, instance,
5820                 self.host,
5821                 action=fields.NotificationAction.LIVE_MIGRATION_ROLLBACK,
5822                 phase=fields.NotificationPhase.END)
5823 
5824         self._set_migration_status(migration, migration_status)
5825 
5826     @wrap_exception()
5827     @wrap_instance_event(prefix='compute')
5828     @wrap_instance_fault
5829     def rollback_live_migration_at_destination(self, context, instance,
5830                                                destroy_disks,
5831                                                migrate_data):
5832         """Cleaning up image directory that is created pre_live_migration.
5833 
5834         :param context: security context
5835         :param instance: a nova.objects.instance.Instance object sent over rpc
5836         """
5837         network_info = self.network_api.get_instance_nw_info(context, instance)
5838         self._notify_about_instance_usage(
5839                       context, instance, "live_migration.rollback.dest.start",
5840                       network_info=network_info)
5841         try:
5842             # NOTE(tr3buchet): tear down networks on destination host
5843             self.network_api.setup_networks_on_host(context, instance,
5844                                                     self.host, teardown=True)
5845         except Exception:
5846             with excutils.save_and_reraise_exception():
5847                 # NOTE(tdurakov): even if teardown networks fails driver
5848                 # should try to rollback live migration on destination.
5849                 LOG.exception('An error occurred while deallocating network.',
5850                               instance=instance)
5851         finally:
5852             # always run this even if setup_networks_on_host fails
5853             # NOTE(vish): The mapping is passed in so the driver can disconnect
5854             #             from remote volumes if necessary
5855             block_device_info = self._get_instance_block_device_info(context,
5856                                                                      instance)
5857             # TODO(tdurakov): remove dict to object conversion once RPC API
5858             # version is bumped to 5.x
5859             if isinstance(migrate_data, dict):
5860                 migrate_data = \
5861                     migrate_data_obj.LiveMigrateData.detect_implementation(
5862                         migrate_data)
5863             self.driver.rollback_live_migration_at_destination(
5864                 context, instance, network_info, block_device_info,
5865                 destroy_disks=destroy_disks, migrate_data=migrate_data)
5866 
5867         self._notify_about_instance_usage(
5868                         context, instance, "live_migration.rollback.dest.end",
5869                         network_info=network_info)
5870 
5871     @periodic_task.periodic_task(
5872         spacing=CONF.heal_instance_info_cache_interval)
5873     def _heal_instance_info_cache(self, context):
5874         """Called periodically.  On every call, try to update the
5875         info_cache's network information for another instance by
5876         calling to the network manager.
5877 
5878         This is implemented by keeping a cache of uuids of instances
5879         that live on this host.  On each call, we pop one off of a
5880         list, pull the DB record, and try the call to the network API.
5881         If anything errors don't fail, as it's possible the instance
5882         has been deleted, etc.
5883         """
5884         heal_interval = CONF.heal_instance_info_cache_interval
5885         if not heal_interval:
5886             return
5887 
5888         instance_uuids = getattr(self, '_instance_uuids_to_heal', [])
5889         instance = None
5890 
5891         LOG.debug('Starting heal instance info cache')
5892 
5893         if not instance_uuids:
5894             # The list of instances to heal is empty so rebuild it
5895             LOG.debug('Rebuilding the list of instances to heal')
5896             db_instances = objects.InstanceList.get_by_host(
5897                 context, self.host, expected_attrs=[], use_slave=True)
5898             for inst in db_instances:
5899                 # We don't want to refresh the cache for instances
5900                 # which are building or deleting so don't put them
5901                 # in the list. If they are building they will get
5902                 # added to the list next time we build it.
5903                 if (inst.vm_state == vm_states.BUILDING):
5904                     LOG.debug('Skipping network cache update for instance '
5905                               'because it is Building.', instance=inst)
5906                     continue
5907                 if (inst.task_state == task_states.DELETING):
5908                     LOG.debug('Skipping network cache update for instance '
5909                               'because it is being deleted.', instance=inst)
5910                     continue
5911 
5912                 if not instance:
5913                     # Save the first one we find so we don't
5914                     # have to get it again
5915                     instance = inst
5916                 else:
5917                     instance_uuids.append(inst['uuid'])
5918 
5919             self._instance_uuids_to_heal = instance_uuids
5920         else:
5921             # Find the next valid instance on the list
5922             while instance_uuids:
5923                 try:
5924                     inst = objects.Instance.get_by_uuid(
5925                             context, instance_uuids.pop(0),
5926                             expected_attrs=['system_metadata', 'info_cache',
5927                                             'flavor'],
5928                             use_slave=True)
5929                 except exception.InstanceNotFound:
5930                     # Instance is gone.  Try to grab another.
5931                     continue
5932 
5933                 # Check the instance hasn't been migrated
5934                 if inst.host != self.host:
5935                     LOG.debug('Skipping network cache update for instance '
5936                               'because it has been migrated to another '
5937                               'host.', instance=inst)
5938                 # Check the instance isn't being deleting
5939                 elif inst.task_state == task_states.DELETING:
5940                     LOG.debug('Skipping network cache update for instance '
5941                               'because it is being deleted.', instance=inst)
5942                 else:
5943                     instance = inst
5944                     break
5945 
5946         if instance:
5947             # We have an instance now to refresh
5948             try:
5949                 # Call to network API to get instance info.. this will
5950                 # force an update to the instance's info_cache
5951                 self.network_api.get_instance_nw_info(context, instance)
5952                 LOG.debug('Updated the network info_cache for instance',
5953                           instance=instance)
5954             except exception.InstanceNotFound:
5955                 # Instance is gone.
5956                 LOG.debug('Instance no longer exists. Unable to refresh',
5957                           instance=instance)
5958                 return
5959             except exception.InstanceInfoCacheNotFound:
5960                 # InstanceInfoCache is gone.
5961                 LOG.debug('InstanceInfoCache no longer exists. '
5962                           'Unable to refresh', instance=instance)
5963             except Exception:
5964                 LOG.error('An error occurred while refreshing the network '
5965                           'cache.', instance=instance, exc_info=True)
5966         else:
5967             LOG.debug("Didn't find any instances for network info cache "
5968                       "update.")
5969 
5970     @periodic_task.periodic_task
5971     def _poll_rebooting_instances(self, context):
5972         if CONF.reboot_timeout > 0:
5973             filters = {'task_state':
5974                        [task_states.REBOOTING,
5975                         task_states.REBOOT_STARTED,
5976                         task_states.REBOOT_PENDING],
5977                        'host': self.host}
5978             rebooting = objects.InstanceList.get_by_filters(
5979                 context, filters, expected_attrs=[], use_slave=True)
5980 
5981             to_poll = []
5982             for instance in rebooting:
5983                 if timeutils.is_older_than(instance.updated_at,
5984                                            CONF.reboot_timeout):
5985                     to_poll.append(instance)
5986 
5987             self.driver.poll_rebooting_instances(CONF.reboot_timeout, to_poll)
5988 
5989     @periodic_task.periodic_task
5990     def _poll_rescued_instances(self, context):
5991         if CONF.rescue_timeout > 0:
5992             filters = {'vm_state': vm_states.RESCUED,
5993                        'host': self.host}
5994             rescued_instances = objects.InstanceList.get_by_filters(
5995                 context, filters, expected_attrs=["system_metadata"],
5996                 use_slave=True)
5997 
5998             to_unrescue = []
5999             for instance in rescued_instances:
6000                 if timeutils.is_older_than(instance.launched_at,
6001                                            CONF.rescue_timeout):
6002                     to_unrescue.append(instance)
6003 
6004             for instance in to_unrescue:
6005                 self.compute_api.unrescue(context, instance)
6006 
6007     @periodic_task.periodic_task
6008     def _poll_unconfirmed_resizes(self, context):
6009         if CONF.resize_confirm_window == 0:
6010             return
6011 
6012         migrations = objects.MigrationList.get_unconfirmed_by_dest_compute(
6013                 context, CONF.resize_confirm_window, self.host,
6014                 use_slave=True)
6015 
6016         migrations_info = dict(migration_count=len(migrations),
6017                 confirm_window=CONF.resize_confirm_window)
6018 
6019         if migrations_info["migration_count"] > 0:
6020             LOG.info("Found %(migration_count)d unconfirmed migrations "
6021                      "older than %(confirm_window)d seconds",
6022                      migrations_info)
6023 
6024         def _set_migration_to_error(migration, reason, **kwargs):
6025             LOG.warning("Setting migration %(migration_id)s to error: "
6026                         "%(reason)s",
6027                         {'migration_id': migration['id'], 'reason': reason},
6028                         **kwargs)
6029             migration.status = 'error'
6030             with migration.obj_as_admin():
6031                 migration.save()
6032 
6033         for migration in migrations:
6034             instance_uuid = migration.instance_uuid
6035             LOG.info("Automatically confirming migration "
6036                      "%(migration_id)s for instance %(instance_uuid)s",
6037                      {'migration_id': migration.id,
6038                       'instance_uuid': instance_uuid})
6039             expected_attrs = ['metadata', 'system_metadata']
6040             try:
6041                 instance = objects.Instance.get_by_uuid(context,
6042                             instance_uuid, expected_attrs=expected_attrs,
6043                             use_slave=True)
6044             except exception.InstanceNotFound:
6045                 reason = (_("Instance %s not found") %
6046                           instance_uuid)
6047                 _set_migration_to_error(migration, reason)
6048                 continue
6049             if instance.vm_state == vm_states.ERROR:
6050                 reason = _("In ERROR state")
6051                 _set_migration_to_error(migration, reason,
6052                                         instance=instance)
6053                 continue
6054             # race condition: The instance in DELETING state should not be
6055             # set the migration state to error, otherwise the instance in
6056             # to be deleted which is in RESIZED state
6057             # will not be able to confirm resize
6058             if instance.task_state in [task_states.DELETING,
6059                                        task_states.SOFT_DELETING]:
6060                 msg = ("Instance being deleted or soft deleted during resize "
6061                        "confirmation. Skipping.")
6062                 LOG.debug(msg, instance=instance)
6063                 continue
6064 
6065             # race condition: This condition is hit when this method is
6066             # called between the save of the migration record with a status of
6067             # finished and the save of the instance object with a state of
6068             # RESIZED. The migration record should not be set to error.
6069             if instance.task_state == task_states.RESIZE_FINISH:
6070                 msg = ("Instance still resizing during resize "
6071                        "confirmation. Skipping.")
6072                 LOG.debug(msg, instance=instance)
6073                 continue
6074 
6075             vm_state = instance.vm_state
6076             task_state = instance.task_state
6077             if vm_state != vm_states.RESIZED or task_state is not None:
6078                 reason = (_("In states %(vm_state)s/%(task_state)s, not "
6079                            "RESIZED/None") %
6080                           {'vm_state': vm_state,
6081                            'task_state': task_state})
6082                 _set_migration_to_error(migration, reason,
6083                                         instance=instance)
6084                 continue
6085             try:
6086                 self.compute_api.confirm_resize(context, instance,
6087                                                 migration=migration)
6088             except Exception as e:
6089                 LOG.info("Error auto-confirming resize: %s. "
6090                          "Will retry later.", e, instance=instance)
6091 
6092     @periodic_task.periodic_task(spacing=CONF.shelved_poll_interval)
6093     def _poll_shelved_instances(self, context):
6094 
6095         if CONF.shelved_offload_time <= 0:
6096             return
6097 
6098         filters = {'vm_state': vm_states.SHELVED,
6099                    'task_state': None,
6100                    'host': self.host}
6101         shelved_instances = objects.InstanceList.get_by_filters(
6102             context, filters=filters, expected_attrs=['system_metadata'],
6103             use_slave=True)
6104 
6105         to_gc = []
6106         for instance in shelved_instances:
6107             sys_meta = instance.system_metadata
6108             shelved_at = timeutils.parse_strtime(sys_meta['shelved_at'])
6109             if timeutils.is_older_than(shelved_at, CONF.shelved_offload_time):
6110                 to_gc.append(instance)
6111 
6112         for instance in to_gc:
6113             try:
6114                 instance.task_state = task_states.SHELVING_OFFLOADING
6115                 instance.save(expected_task_state=(None,))
6116                 self.shelve_offload_instance(context, instance,
6117                                              clean_shutdown=False)
6118             except Exception:
6119                 LOG.exception('Periodic task failed to offload instance.',
6120                               instance=instance)
6121 
6122     @periodic_task.periodic_task
6123     def _instance_usage_audit(self, context):
6124         if not CONF.instance_usage_audit:
6125             return
6126 
6127         begin, end = utils.last_completed_audit_period()
6128         if objects.TaskLog.get(context, 'instance_usage_audit', begin, end,
6129                                self.host):
6130             return
6131 
6132         instances = objects.InstanceList.get_active_by_window_joined(
6133             context, begin, end, host=self.host,
6134             expected_attrs=['system_metadata', 'info_cache', 'metadata',
6135                             'flavor'],
6136             use_slave=True)
6137         num_instances = len(instances)
6138         errors = 0
6139         successes = 0
6140         LOG.info("Running instance usage audit for host %(host)s "
6141                  "from %(begin_time)s to %(end_time)s. "
6142                  "%(number_instances)s instances.",
6143                  {'host': self.host,
6144                   'begin_time': begin,
6145                   'end_time': end,
6146                   'number_instances': num_instances})
6147         start_time = time.time()
6148         task_log = objects.TaskLog(context)
6149         task_log.task_name = 'instance_usage_audit'
6150         task_log.period_beginning = begin
6151         task_log.period_ending = end
6152         task_log.host = self.host
6153         task_log.task_items = num_instances
6154         task_log.message = 'Instance usage audit started...'
6155         task_log.begin_task()
6156         for instance in instances:
6157             try:
6158                 compute_utils.notify_usage_exists(
6159                     self.notifier, context, instance,
6160                     ignore_missing_network_data=False)
6161                 successes += 1
6162             except Exception:
6163                 LOG.exception('Failed to generate usage '
6164                               'audit for instance '
6165                               'on host %s', self.host,
6166                               instance=instance)
6167                 errors += 1
6168         task_log.errors = errors
6169         task_log.message = (
6170             'Instance usage audit ran for host %s, %s instances in %s seconds.'
6171             % (self.host, num_instances, time.time() - start_time))
6172         task_log.end_task()
6173 
6174     @periodic_task.periodic_task(spacing=CONF.bandwidth_poll_interval)
6175     def _poll_bandwidth_usage(self, context):
6176 
6177         if not self._bw_usage_supported:
6178             return
6179 
6180         prev_time, start_time = utils.last_completed_audit_period()
6181 
6182         curr_time = time.time()
6183         if (curr_time - self._last_bw_usage_poll >
6184                 CONF.bandwidth_poll_interval):
6185             self._last_bw_usage_poll = curr_time
6186             LOG.info("Updating bandwidth usage cache")
6187             cells_update_interval = CONF.cells.bandwidth_update_interval
6188             if (cells_update_interval > 0 and
6189                    curr_time - self._last_bw_usage_cell_update >
6190                            cells_update_interval):
6191                 self._last_bw_usage_cell_update = curr_time
6192                 update_cells = True
6193             else:
6194                 update_cells = False
6195 
6196             instances = objects.InstanceList.get_by_host(context,
6197                                                               self.host,
6198                                                               use_slave=True)
6199             try:
6200                 bw_counters = self.driver.get_all_bw_counters(instances)
6201             except NotImplementedError:
6202                 # NOTE(mdragon): Not all hypervisors have bandwidth polling
6203                 # implemented yet.  If they don't it doesn't break anything,
6204                 # they just don't get the info in the usage events.
6205                 # NOTE(PhilDay): Record that its not supported so we can
6206                 # skip fast on future calls rather than waste effort getting
6207                 # the list of instances.
6208                 LOG.info("Bandwidth usage not supported by hypervisor.")
6209                 self._bw_usage_supported = False
6210                 return
6211 
6212             refreshed = timeutils.utcnow()
6213             for bw_ctr in bw_counters:
6214                 # Allow switching of greenthreads between queries.
6215                 greenthread.sleep(0)
6216                 bw_in = 0
6217                 bw_out = 0
6218                 last_ctr_in = None
6219                 last_ctr_out = None
6220                 usage = objects.BandwidthUsage.get_by_instance_uuid_and_mac(
6221                     context, bw_ctr['uuid'], bw_ctr['mac_address'],
6222                     start_period=start_time, use_slave=True)
6223                 if usage:
6224                     bw_in = usage.bw_in
6225                     bw_out = usage.bw_out
6226                     last_ctr_in = usage.last_ctr_in
6227                     last_ctr_out = usage.last_ctr_out
6228                 else:
6229                     usage = (objects.BandwidthUsage.
6230                              get_by_instance_uuid_and_mac(
6231                         context, bw_ctr['uuid'], bw_ctr['mac_address'],
6232                         start_period=prev_time, use_slave=True))
6233                     if usage:
6234                         last_ctr_in = usage.last_ctr_in
6235                         last_ctr_out = usage.last_ctr_out
6236 
6237                 if last_ctr_in is not None:
6238                     if bw_ctr['bw_in'] < last_ctr_in:
6239                         # counter rollover
6240                         bw_in += bw_ctr['bw_in']
6241                     else:
6242                         bw_in += (bw_ctr['bw_in'] - last_ctr_in)
6243 
6244                 if last_ctr_out is not None:
6245                     if bw_ctr['bw_out'] < last_ctr_out:
6246                         # counter rollover
6247                         bw_out += bw_ctr['bw_out']
6248                     else:
6249                         bw_out += (bw_ctr['bw_out'] - last_ctr_out)
6250 
6251                 objects.BandwidthUsage(context=context).create(
6252                                               bw_ctr['uuid'],
6253                                               bw_ctr['mac_address'],
6254                                               bw_in,
6255                                               bw_out,
6256                                               bw_ctr['bw_in'],
6257                                               bw_ctr['bw_out'],
6258                                               start_period=start_time,
6259                                               last_refreshed=refreshed,
6260                                               update_cells=update_cells)
6261 
6262     def _get_host_volume_bdms(self, context, use_slave=False):
6263         """Return all block device mappings on a compute host."""
6264         compute_host_bdms = []
6265         instances = objects.InstanceList.get_by_host(context, self.host,
6266             use_slave=use_slave)
6267         for instance in instances:
6268             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
6269                     context, instance.uuid, use_slave=use_slave)
6270             instance_bdms = [bdm for bdm in bdms if bdm.is_volume]
6271             compute_host_bdms.append(dict(instance=instance,
6272                                           instance_bdms=instance_bdms))
6273 
6274         return compute_host_bdms
6275 
6276     def _update_volume_usage_cache(self, context, vol_usages):
6277         """Updates the volume usage cache table with a list of stats."""
6278         for usage in vol_usages:
6279             # Allow switching of greenthreads between queries.
6280             greenthread.sleep(0)
6281             vol_usage = objects.VolumeUsage(context)
6282             vol_usage.volume_id = usage['volume']
6283             vol_usage.instance_uuid = usage['instance'].uuid
6284             vol_usage.project_id = usage['instance'].project_id
6285             vol_usage.user_id = usage['instance'].user_id
6286             vol_usage.availability_zone = usage['instance'].availability_zone
6287             vol_usage.curr_reads = usage['rd_req']
6288             vol_usage.curr_read_bytes = usage['rd_bytes']
6289             vol_usage.curr_writes = usage['wr_req']
6290             vol_usage.curr_write_bytes = usage['wr_bytes']
6291             vol_usage.save()
6292             self.notifier.info(context, 'volume.usage',
6293                                compute_utils.usage_volume_info(vol_usage))
6294 
6295     @periodic_task.periodic_task(spacing=CONF.volume_usage_poll_interval)
6296     def _poll_volume_usage(self, context):
6297         if CONF.volume_usage_poll_interval == 0:
6298             return
6299 
6300         compute_host_bdms = self._get_host_volume_bdms(context,
6301                                                        use_slave=True)
6302         if not compute_host_bdms:
6303             return
6304 
6305         LOG.debug("Updating volume usage cache")
6306         try:
6307             vol_usages = self.driver.get_all_volume_usage(context,
6308                                                           compute_host_bdms)
6309         except NotImplementedError:
6310             return
6311 
6312         self._update_volume_usage_cache(context, vol_usages)
6313 
6314     @periodic_task.periodic_task(spacing=CONF.sync_power_state_interval,
6315                                  run_immediately=True)
6316     def _sync_power_states(self, context):
6317         """Align power states between the database and the hypervisor.
6318 
6319         To sync power state data we make a DB call to get the number of
6320         virtual machines known by the hypervisor and if the number matches the
6321         number of virtual machines known by the database, we proceed in a lazy
6322         loop, one database record at a time, checking if the hypervisor has the
6323         same power state as is in the database.
6324         """
6325         db_instances = objects.InstanceList.get_by_host(context, self.host,
6326                                                         expected_attrs=[],
6327                                                         use_slave=True)
6328 
6329         num_vm_instances = self.driver.get_num_instances()
6330         num_db_instances = len(db_instances)
6331 
6332         if num_vm_instances != num_db_instances:
6333             LOG.warning("While synchronizing instance power states, found "
6334                         "%(num_db_instances)s instances in the database "
6335                         "and %(num_vm_instances)s instances on the "
6336                         "hypervisor.",
6337                         {'num_db_instances': num_db_instances,
6338                          'num_vm_instances': num_vm_instances})
6339 
6340         def _sync(db_instance):
6341             # NOTE(melwitt): This must be synchronized as we query state from
6342             #                two separate sources, the driver and the database.
6343             #                They are set (in stop_instance) and read, in sync.
6344             @utils.synchronized(db_instance.uuid)
6345             def query_driver_power_state_and_sync():
6346                 self._query_driver_power_state_and_sync(context, db_instance)
6347 
6348             try:
6349                 query_driver_power_state_and_sync()
6350             except Exception:
6351                 LOG.exception("Periodic sync_power_state task had an "
6352                               "error while processing an instance.",
6353                               instance=db_instance)
6354 
6355             self._syncs_in_progress.pop(db_instance.uuid)
6356 
6357         for db_instance in db_instances:
6358             # process syncs asynchronously - don't want instance locking to
6359             # block entire periodic task thread
6360             uuid = db_instance.uuid
6361             if uuid in self._syncs_in_progress:
6362                 LOG.debug('Sync already in progress for %s', uuid)
6363             else:
6364                 LOG.debug('Triggering sync for uuid %s', uuid)
6365                 self._syncs_in_progress[uuid] = True
6366                 self._sync_power_pool.spawn_n(_sync, db_instance)
6367 
6368     def _query_driver_power_state_and_sync(self, context, db_instance):
6369         if db_instance.task_state is not None:
6370             LOG.info("During sync_power_state the instance has a "
6371                      "pending task (%(task)s). Skip.",
6372                      {'task': db_instance.task_state}, instance=db_instance)
6373             return
6374         # No pending tasks. Now try to figure out the real vm_power_state.
6375         try:
6376             vm_instance = self.driver.get_info(db_instance)
6377             vm_power_state = vm_instance.state
6378         except exception.InstanceNotFound:
6379             vm_power_state = power_state.NOSTATE
6380         # Note(maoy): the above get_info call might take a long time,
6381         # for example, because of a broken libvirt driver.
6382         try:
6383             self._sync_instance_power_state(context,
6384                                             db_instance,
6385                                             vm_power_state,
6386                                             use_slave=True)
6387         except exception.InstanceNotFound:
6388             # NOTE(hanlind): If the instance gets deleted during sync,
6389             # silently ignore.
6390             pass
6391 
6392     def _sync_instance_power_state(self, context, db_instance, vm_power_state,
6393                                    use_slave=False):
6394         """Align instance power state between the database and hypervisor.
6395 
6396         If the instance is not found on the hypervisor, but is in the database,
6397         then a stop() API will be called on the instance.
6398         """
6399 
6400         # We re-query the DB to get the latest instance info to minimize
6401         # (not eliminate) race condition.
6402         db_instance.refresh(use_slave=use_slave)
6403         db_power_state = db_instance.power_state
6404         vm_state = db_instance.vm_state
6405 
6406         if self.host != db_instance.host:
6407             # on the sending end of nova-compute _sync_power_state
6408             # may have yielded to the greenthread performing a live
6409             # migration; this in turn has changed the resident-host
6410             # for the VM; However, the instance is still active, it
6411             # is just in the process of migrating to another host.
6412             # This implies that the compute source must relinquish
6413             # control to the compute destination.
6414             LOG.info("During the sync_power process the "
6415                      "instance has moved from "
6416                      "host %(src)s to host %(dst)s",
6417                      {'src': db_instance.host,
6418                       'dst': self.host},
6419                      instance=db_instance)
6420             return
6421         elif db_instance.task_state is not None:
6422             # on the receiving end of nova-compute, it could happen
6423             # that the DB instance already report the new resident
6424             # but the actual VM has not showed up on the hypervisor
6425             # yet. In this case, let's allow the loop to continue
6426             # and run the state sync in a later round
6427             LOG.info("During sync_power_state the instance has a "
6428                      "pending task (%(task)s). Skip.",
6429                      {'task': db_instance.task_state},
6430                      instance=db_instance)
6431             return
6432 
6433         orig_db_power_state = db_power_state
6434         if vm_power_state != db_power_state:
6435             LOG.info('During _sync_instance_power_state the DB '
6436                      'power_state (%(db_power_state)s) does not match '
6437                      'the vm_power_state from the hypervisor '
6438                      '(%(vm_power_state)s). Updating power_state in the '
6439                      'DB to match the hypervisor.',
6440                      {'db_power_state': db_power_state,
6441                       'vm_power_state': vm_power_state},
6442                      instance=db_instance)
6443             # power_state is always updated from hypervisor to db
6444             db_instance.power_state = vm_power_state
6445             db_instance.save()
6446             db_power_state = vm_power_state
6447 
6448         # Note(maoy): Now resolve the discrepancy between vm_state and
6449         # vm_power_state. We go through all possible vm_states.
6450         if vm_state in (vm_states.BUILDING,
6451                         vm_states.RESCUED,
6452                         vm_states.RESIZED,
6453                         vm_states.SUSPENDED,
6454                         vm_states.ERROR):
6455             # TODO(maoy): we ignore these vm_state for now.
6456             pass
6457         elif vm_state == vm_states.ACTIVE:
6458             # The only rational power state should be RUNNING
6459             if vm_power_state in (power_state.SHUTDOWN,
6460                                   power_state.CRASHED):
6461                 LOG.warning("Instance shutdown by itself. Calling the "
6462                             "stop API. Current vm_state: %(vm_state)s, "
6463                             "current task_state: %(task_state)s, "
6464                             "original DB power_state: %(db_power_state)s, "
6465                             "current VM power_state: %(vm_power_state)s",
6466                             {'vm_state': vm_state,
6467                              'task_state': db_instance.task_state,
6468                              'db_power_state': orig_db_power_state,
6469                              'vm_power_state': vm_power_state},
6470                             instance=db_instance)
6471                 try:
6472                     # Note(maoy): here we call the API instead of
6473                     # brutally updating the vm_state in the database
6474                     # to allow all the hooks and checks to be performed.
6475                     if db_instance.shutdown_terminate:
6476                         self.compute_api.delete(context, db_instance)
6477                     else:
6478                         self.compute_api.stop(context, db_instance)
6479                 except Exception:
6480                     # Note(maoy): there is no need to propagate the error
6481                     # because the same power_state will be retrieved next
6482                     # time and retried.
6483                     # For example, there might be another task scheduled.
6484                     LOG.exception("error during stop() in sync_power_state.",
6485                                   instance=db_instance)
6486             elif vm_power_state == power_state.SUSPENDED:
6487                 LOG.warning("Instance is suspended unexpectedly. Calling "
6488                             "the stop API.", instance=db_instance)
6489                 try:
6490                     self.compute_api.stop(context, db_instance)
6491                 except Exception:
6492                     LOG.exception("error during stop() in sync_power_state.",
6493                                   instance=db_instance)
6494             elif vm_power_state == power_state.PAUSED:
6495                 # Note(maoy): a VM may get into the paused state not only
6496                 # because the user request via API calls, but also
6497                 # due to (temporary) external instrumentations.
6498                 # Before the virt layer can reliably report the reason,
6499                 # we simply ignore the state discrepancy. In many cases,
6500                 # the VM state will go back to running after the external
6501                 # instrumentation is done. See bug 1097806 for details.
6502                 LOG.warning("Instance is paused unexpectedly. Ignore.",
6503                             instance=db_instance)
6504             elif vm_power_state == power_state.NOSTATE:
6505                 # Occasionally, depending on the status of the hypervisor,
6506                 # which could be restarting for example, an instance may
6507                 # not be found.  Therefore just log the condition.
6508                 LOG.warning("Instance is unexpectedly not found. Ignore.",
6509                             instance=db_instance)
6510         elif vm_state == vm_states.STOPPED:
6511             if vm_power_state not in (power_state.NOSTATE,
6512                                       power_state.SHUTDOWN,
6513                                       power_state.CRASHED):
6514                 LOG.warning("Instance is not stopped. Calling "
6515                             "the stop API. Current vm_state: %(vm_state)s,"
6516                             " current task_state: %(task_state)s, "
6517                             "original DB power_state: %(db_power_state)s, "
6518                             "current VM power_state: %(vm_power_state)s",
6519                             {'vm_state': vm_state,
6520                              'task_state': db_instance.task_state,
6521                              'db_power_state': orig_db_power_state,
6522                              'vm_power_state': vm_power_state},
6523                             instance=db_instance)
6524                 try:
6525                     # NOTE(russellb) Force the stop, because normally the
6526                     # compute API would not allow an attempt to stop a stopped
6527                     # instance.
6528                     self.compute_api.force_stop(context, db_instance)
6529                 except Exception:
6530                     LOG.exception("error during stop() in sync_power_state.",
6531                                   instance=db_instance)
6532         elif vm_state == vm_states.PAUSED:
6533             if vm_power_state in (power_state.SHUTDOWN,
6534                                   power_state.CRASHED):
6535                 LOG.warning("Paused instance shutdown by itself. Calling "
6536                             "the stop API.", instance=db_instance)
6537                 try:
6538                     self.compute_api.force_stop(context, db_instance)
6539                 except Exception:
6540                     LOG.exception("error during stop() in sync_power_state.",
6541                                   instance=db_instance)
6542         elif vm_state in (vm_states.SOFT_DELETED,
6543                           vm_states.DELETED):
6544             if vm_power_state not in (power_state.NOSTATE,
6545                                       power_state.SHUTDOWN):
6546                 # Note(maoy): this should be taken care of periodically in
6547                 # _cleanup_running_deleted_instances().
6548                 LOG.warning("Instance is not (soft-)deleted.",
6549                             instance=db_instance)
6550 
6551     @periodic_task.periodic_task
6552     def _reclaim_queued_deletes(self, context):
6553         """Reclaim instances that are queued for deletion."""
6554         interval = CONF.reclaim_instance_interval
6555         if interval <= 0:
6556             LOG.debug("CONF.reclaim_instance_interval <= 0, skipping...")
6557             return
6558 
6559         filters = {'vm_state': vm_states.SOFT_DELETED,
6560                    'task_state': None,
6561                    'host': self.host}
6562         instances = objects.InstanceList.get_by_filters(
6563             context, filters,
6564             expected_attrs=objects.instance.INSTANCE_DEFAULT_FIELDS,
6565             use_slave=True)
6566         for instance in instances:
6567             if self._deleted_old_enough(instance, interval):
6568                 bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
6569                         context, instance.uuid)
6570                 LOG.info('Reclaiming deleted instance', instance=instance)
6571                 try:
6572                     self._delete_instance(context, instance, bdms)
6573                 except Exception as e:
6574                     LOG.warning("Periodic reclaim failed to delete "
6575                                 "instance: %s",
6576                                 e, instance=instance)
6577 
6578     def update_available_resource_for_node(self, context, nodename):
6579 
6580         rt = self._get_resource_tracker()
6581         try:
6582             rt.update_available_resource(context, nodename)
6583         except exception.ComputeHostNotFound:
6584             # NOTE(comstud): We can get to this case if a node was
6585             # marked 'deleted' in the DB and then re-added with a
6586             # different auto-increment id. The cached resource
6587             # tracker tried to update a deleted record and failed.
6588             # Don't add this resource tracker to the new dict, so
6589             # that this will resolve itself on the next run.
6590             LOG.info("Compute node '%s' not found in "
6591                      "update_available_resource.", nodename)
6592             # TODO(jaypipes): Yes, this is inefficient to throw away all of the
6593             # compute nodes to force a rebuild, but this is only temporary
6594             # until Ironic baremetal node resource providers are tracked
6595             # properly in the report client and this is a tiny edge case
6596             # anyway.
6597             self._resource_tracker = None
6598             return
6599         except Exception:
6600             LOG.exception("Error updating resources for node %(node)s.",
6601                           {'node': nodename})
6602 
6603     @periodic_task.periodic_task(spacing=CONF.update_resources_interval)
6604     def update_available_resource(self, context, startup=False):
6605         """See driver.get_available_resource()
6606 
6607         Periodic process that keeps that the compute host's understanding of
6608         resource availability and usage in sync with the underlying hypervisor.
6609 
6610         :param context: security context
6611         :param startup: True if this is being called when the nova-compute
6612             service is starting, False otherwise.
6613         """
6614 
6615         compute_nodes_in_db = self._get_compute_nodes_in_db(context,
6616                                                             use_slave=True,
6617                                                             startup=startup)
6618         nodenames = set(self.driver.get_available_nodes())
6619         for nodename in nodenames:
6620             self.update_available_resource_for_node(context, nodename)
6621 
6622         # Delete orphan compute node not reported by driver but still in db
6623         for cn in compute_nodes_in_db:
6624             if cn.hypervisor_hostname not in nodenames:
6625                 LOG.info("Deleting orphan compute node %(id)s "
6626                          "hypervisor host is %(hh)s, "
6627                          "nodes are %(nodes)s",
6628                          {'id': cn.id, 'hh': cn.hypervisor_hostname,
6629                           'nodes': nodenames})
6630                 cn.destroy()
6631                 # Delete the corresponding resource provider in placement,
6632                 # along with any associated allocations and inventory.
6633                 # TODO(cdent): Move use of reportclient into resource tracker.
6634                 self.scheduler_client.reportclient.delete_resource_provider(
6635                     context, cn, cascade=True)
6636 
6637     def _get_compute_nodes_in_db(self, context, use_slave=False,
6638                                  startup=False):
6639         try:
6640             return objects.ComputeNodeList.get_all_by_host(context, self.host,
6641                                                            use_slave=use_slave)
6642         except exception.NotFound:
6643             if startup:
6644                 LOG.warning(
6645                     "No compute node record found for host %s. If this is "
6646                     "the first time this service is starting on this "
6647                     "host, then you can ignore this warning.", self.host)
6648             else:
6649                 LOG.error("No compute node record for host %s", self.host)
6650             return []
6651 
6652     @periodic_task.periodic_task(
6653         spacing=CONF.running_deleted_instance_poll_interval)
6654     def _cleanup_running_deleted_instances(self, context):
6655         """Cleanup any instances which are erroneously still running after
6656         having been deleted.
6657 
6658         Valid actions to take are:
6659 
6660             1. noop - do nothing
6661             2. log - log which instances are erroneously running
6662             3. reap - shutdown and cleanup any erroneously running instances
6663             4. shutdown - power off *and disable* any erroneously running
6664                           instances
6665 
6666         The use-case for this cleanup task is: for various reasons, it may be
6667         possible for the database to show an instance as deleted but for that
6668         instance to still be running on a host machine (see bug
6669         https://bugs.launchpad.net/nova/+bug/911366).
6670 
6671         This cleanup task is a cross-hypervisor utility for finding these
6672         zombied instances and either logging the discrepancy (likely what you
6673         should do in production), or automatically reaping the instances (more
6674         appropriate for dev environments).
6675         """
6676         action = CONF.running_deleted_instance_action
6677 
6678         if action == "noop":
6679             return
6680 
6681         # NOTE(sirp): admin contexts don't ordinarily return deleted records
6682         with utils.temporary_mutation(context, read_deleted="yes"):
6683             for instance in self._running_deleted_instances(context):
6684                 if action == "log":
6685                     LOG.warning("Detected instance with name label "
6686                                 "'%s' which is marked as "
6687                                 "DELETED but still present on host.",
6688                                 instance.name, instance=instance)
6689 
6690                 elif action == 'shutdown':
6691                     LOG.info("Powering off instance with name label "
6692                              "'%s' which is marked as "
6693                              "DELETED but still present on host.",
6694                              instance.name, instance=instance)
6695                     try:
6696                         try:
6697                             # disable starting the instance
6698                             self.driver.set_bootable(instance, False)
6699                         except NotImplementedError:
6700                             LOG.debug("set_bootable is not implemented "
6701                                       "for the current driver")
6702                         # and power it off
6703                         self.driver.power_off(instance)
6704                     except Exception:
6705                         LOG.warning("Failed to power off instance",
6706                                     instance=instance, exc_info=True)
6707 
6708                 elif action == 'reap':
6709                     LOG.info("Destroying instance with name label "
6710                              "'%s' which is marked as "
6711                              "DELETED but still present on host.",
6712                              instance.name, instance=instance)
6713                     bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
6714                         context, instance.uuid, use_slave=True)
6715                     self.instance_events.clear_events_for_instance(instance)
6716                     try:
6717                         self._shutdown_instance(context, instance, bdms,
6718                                                 notify=False)
6719                         self._cleanup_volumes(context, instance.uuid, bdms)
6720                     except Exception as e:
6721                         LOG.warning("Periodic cleanup failed to delete "
6722                                     "instance: %s",
6723                                     e, instance=instance)
6724                 else:
6725                     raise Exception(_("Unrecognized value '%s'"
6726                                       " for CONF.running_deleted_"
6727                                       "instance_action") % action)
6728 
6729     def _running_deleted_instances(self, context):
6730         """Returns a list of instances nova thinks is deleted,
6731         but the hypervisor thinks is still running.
6732         """
6733         timeout = CONF.running_deleted_instance_timeout
6734         filters = {'deleted': True,
6735                    'soft_deleted': False,
6736                    'host': self.host}
6737         instances = self._get_instances_on_driver(context, filters)
6738         return [i for i in instances if self._deleted_old_enough(i, timeout)]
6739 
6740     def _deleted_old_enough(self, instance, timeout):
6741         deleted_at = instance.deleted_at
6742         if deleted_at:
6743             deleted_at = deleted_at.replace(tzinfo=None)
6744         return (not deleted_at or timeutils.is_older_than(deleted_at, timeout))
6745 
6746     @contextlib.contextmanager
6747     def _error_out_instance_on_exception(self, context, instance,
6748                                          instance_state=vm_states.ACTIVE):
6749         instance_uuid = instance.uuid
6750         try:
6751             yield
6752         except NotImplementedError as error:
6753             with excutils.save_and_reraise_exception():
6754                 LOG.info("Setting instance back to %(state)s after: "
6755                          "%(error)s",
6756                          {'state': instance_state, 'error': error},
6757                          instance_uuid=instance_uuid)
6758                 self._instance_update(context, instance,
6759                                       vm_state=instance_state,
6760                                       task_state=None)
6761         except exception.InstanceFaultRollback as error:
6762             LOG.info("Setting instance back to ACTIVE after: %s",
6763                      error, instance_uuid=instance_uuid)
6764             self._instance_update(context, instance,
6765                                   vm_state=vm_states.ACTIVE,
6766                                   task_state=None)
6767             raise error.inner_exception
6768         except Exception:
6769             LOG.exception('Setting instance vm_state to ERROR',
6770                           instance_uuid=instance_uuid)
6771             with excutils.save_and_reraise_exception():
6772                 self._set_instance_obj_error_state(context, instance)
6773 
6774     @wrap_exception()
6775     def add_aggregate_host(self, context, aggregate, host, slave_info):
6776         """Notify hypervisor of change (for hypervisor pools)."""
6777         try:
6778             self.driver.add_to_aggregate(context, aggregate, host,
6779                                          slave_info=slave_info)
6780         except NotImplementedError:
6781             LOG.debug('Hypervisor driver does not support '
6782                       'add_aggregate_host')
6783         except exception.AggregateError:
6784             with excutils.save_and_reraise_exception():
6785                 self.driver.undo_aggregate_operation(
6786                                     context,
6787                                     aggregate.delete_host,
6788                                     aggregate, host)
6789 
6790     @wrap_exception()
6791     def remove_aggregate_host(self, context, host, slave_info, aggregate):
6792         """Removes a host from a physical hypervisor pool."""
6793         try:
6794             self.driver.remove_from_aggregate(context, aggregate, host,
6795                                               slave_info=slave_info)
6796         except NotImplementedError:
6797             LOG.debug('Hypervisor driver does not support '
6798                       'remove_aggregate_host')
6799         except (exception.AggregateError,
6800                 exception.InvalidAggregateAction) as e:
6801             with excutils.save_and_reraise_exception():
6802                 self.driver.undo_aggregate_operation(
6803                                     context,
6804                                     aggregate.add_host,
6805                                     aggregate, host,
6806                                     isinstance(e, exception.AggregateError))
6807 
6808     def _process_instance_event(self, instance, event):
6809         _event = self.instance_events.pop_instance_event(instance, event)
6810         if _event:
6811             LOG.debug('Processing event %(event)s',
6812                       {'event': event.key}, instance=instance)
6813             _event.send(event)
6814         else:
6815             LOG.warning('Received unexpected event %(event)s for instance',
6816                         {'event': event.key}, instance=instance)
6817 
6818     def _process_instance_vif_deleted_event(self, context, instance,
6819                                             deleted_vif_id):
6820         # If an attached port is deleted by neutron, it needs to
6821         # be detached from the instance.
6822         # And info cache needs to be updated.
6823         network_info = instance.info_cache.network_info
6824         for index, vif in enumerate(network_info):
6825             if vif['id'] == deleted_vif_id:
6826                 LOG.info('Neutron deleted interface %(intf)s; '
6827                          'detaching it from the instance and '
6828                          'deleting it from the info cache',
6829                          {'intf': vif['id']},
6830                          instance=instance)
6831                 del network_info[index]
6832                 base_net_api.update_instance_cache_with_nw_info(
6833                                  self.network_api, context,
6834                                  instance,
6835                                  nw_info=network_info)
6836                 try:
6837                     self.driver.detach_interface(context, instance, vif)
6838                 except NotImplementedError:
6839                     # Not all virt drivers support attach/detach of interfaces
6840                     # yet (like Ironic), so just ignore this.
6841                     pass
6842                 except exception.NovaException as ex:
6843                     LOG.warning("Detach interface failed, "
6844                                 "port_id=%(port_id)s, reason: %(msg)s",
6845                                 {'port_id': deleted_vif_id, 'msg': ex},
6846                                 instance=instance)
6847                 break
6848 
6849     @wrap_instance_event(prefix='compute')
6850     @wrap_instance_fault
6851     def extend_volume(self, context, instance, extended_volume_id):
6852 
6853         # If an attached volume is extended by cinder, it needs to
6854         # be extended by virt driver so host can detect its new size.
6855         # And bdm needs to be updated.
6856         LOG.debug('Handling volume-extended event for volume %(vol)s',
6857                   {'vol': extended_volume_id}, instance=instance)
6858 
6859         try:
6860             bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
6861                    context, extended_volume_id, instance.uuid)
6862         except exception.NotFound:
6863             LOG.warning('Extend volume failed, '
6864                         'volume %(vol)s is not attached to instance.',
6865                         {'vol': extended_volume_id},
6866                         instance=instance)
6867             return
6868 
6869         LOG.info('Cinder extended volume %(vol)s; '
6870                  'extending it to detect new size',
6871                  {'vol': extended_volume_id},
6872                  instance=instance)
6873         volume = self.volume_api.get(context, bdm.volume_id)
6874 
6875         if bdm.connection_info is None:
6876             LOG.warning('Extend volume failed, '
6877                         'attached volume %(vol)s has no connection_info',
6878                         {'vol': extended_volume_id},
6879                         instance=instance)
6880             return
6881 
6882         connection_info = jsonutils.loads(bdm.connection_info)
6883         bdm.volume_size = volume['size']
6884         bdm.save()
6885 
6886         if not self.driver.capabilities.get('supports_extend_volume', False):
6887             raise exception.ExtendVolumeNotSupported()
6888 
6889         try:
6890             self.driver.extend_volume(connection_info,
6891                                       instance)
6892         except Exception as ex:
6893             LOG.warning('Extend volume failed, '
6894                         'volume_id=%(volume_id)s, reason: %(msg)s',
6895                         {'volume_id': extended_volume_id, 'msg': ex},
6896                         instance=instance)
6897             raise
6898 
6899     @wrap_exception()
6900     def external_instance_event(self, context, instances, events):
6901         # NOTE(danms): Some event types are handled by the manager, such
6902         # as when we're asked to update the instance's info_cache. If it's
6903         # not one of those, look for some thread(s) waiting for the event and
6904         # unblock them if so.
6905         for event in events:
6906             instance = [inst for inst in instances
6907                         if inst.uuid == event.instance_uuid][0]
6908             LOG.debug('Received event %(event)s',
6909                       {'event': event.key},
6910                       instance=instance)
6911             if event.name == 'network-changed':
6912                 try:
6913                     self.network_api.get_instance_nw_info(context, instance)
6914                 except exception.NotFound as e:
6915                     LOG.info('Failed to process external instance event '
6916                              '%(event)s due to: %(error)s',
6917                              {'event': event.key, 'error': six.text_type(e)},
6918                              instance=instance)
6919             elif event.name == 'network-vif-deleted':
6920                 try:
6921                     self._process_instance_vif_deleted_event(context,
6922                                                              instance,
6923                                                              event.tag)
6924                 except exception.NotFound as e:
6925                     LOG.info('Failed to process external instance event '
6926                              '%(event)s due to: %(error)s',
6927                              {'event': event.key, 'error': six.text_type(e)},
6928                              instance=instance)
6929             elif event.name == 'volume-extended':
6930                 self.extend_volume(context, instance, event.tag)
6931             else:
6932                 self._process_instance_event(instance, event)
6933 
6934     @periodic_task.periodic_task(spacing=CONF.image_cache_manager_interval,
6935                                  external_process_ok=True)
6936     def _run_image_cache_manager_pass(self, context):
6937         """Run a single pass of the image cache manager."""
6938 
6939         if not self.driver.capabilities["has_imagecache"]:
6940             return
6941 
6942         # Determine what other nodes use this storage
6943         storage_users.register_storage_use(CONF.instances_path, CONF.host)
6944         nodes = storage_users.get_storage_users(CONF.instances_path)
6945 
6946         # Filter all_instances to only include those nodes which share this
6947         # storage path.
6948         # TODO(mikal): this should be further refactored so that the cache
6949         # cleanup code doesn't know what those instances are, just a remote
6950         # count, and then this logic should be pushed up the stack.
6951         filters = {'deleted': False,
6952                    'soft_deleted': True,
6953                    'host': nodes}
6954         filtered_instances = objects.InstanceList.get_by_filters(context,
6955                                  filters, expected_attrs=[], use_slave=True)
6956 
6957         self.driver.manage_image_cache(context, filtered_instances)
6958 
6959     @periodic_task.periodic_task(spacing=CONF.instance_delete_interval)
6960     def _run_pending_deletes(self, context):
6961         """Retry any pending instance file deletes."""
6962         LOG.debug('Cleaning up deleted instances')
6963         filters = {'deleted': True,
6964                    'soft_deleted': False,
6965                    'host': CONF.host,
6966                    'cleaned': False}
6967         attrs = ['system_metadata']
6968         with utils.temporary_mutation(context, read_deleted='yes'):
6969             instances = objects.InstanceList.get_by_filters(
6970                 context, filters, expected_attrs=attrs, use_slave=True)
6971         LOG.debug('There are %d instances to clean', len(instances))
6972 
6973         # TODO(raj_singh): Remove this if condition when min value is
6974         # introduced to "maximum_instance_delete_attempts" cfg option.
6975         if CONF.maximum_instance_delete_attempts < 1:
6976             LOG.warning('Future versions of Nova will restrict the '
6977                         '"maximum_instance_delete_attempts" config option '
6978                         'to values >=1. Update your configuration file to '
6979                         'mitigate future upgrade issues.')
6980 
6981         for instance in instances:
6982             attempts = int(instance.system_metadata.get('clean_attempts', '0'))
6983             LOG.debug('Instance has had %(attempts)s of %(max)s '
6984                       'cleanup attempts',
6985                       {'attempts': attempts,
6986                        'max': CONF.maximum_instance_delete_attempts},
6987                       instance=instance)
6988             if attempts < CONF.maximum_instance_delete_attempts:
6989                 success = self.driver.delete_instance_files(instance)
6990 
6991                 instance.system_metadata['clean_attempts'] = str(attempts + 1)
6992                 if success:
6993                     instance.cleaned = True
6994                 with utils.temporary_mutation(context, read_deleted='yes'):
6995                     instance.save()
6996 
6997     @periodic_task.periodic_task(spacing=CONF.instance_delete_interval)
6998     def _cleanup_incomplete_migrations(self, context):
6999         """Delete instance files on failed resize/revert-resize operation
7000 
7001         During resize/revert-resize operation, if that instance gets deleted
7002         in-between then instance files might remain either on source or
7003         destination compute node because of race condition.
7004         """
7005         LOG.debug('Cleaning up deleted instances with incomplete migration ')
7006         migration_filters = {'host': CONF.host,
7007                              'status': 'error'}
7008         migrations = objects.MigrationList.get_by_filters(context,
7009                                                           migration_filters)
7010 
7011         if not migrations:
7012             return
7013 
7014         inst_uuid_from_migrations = set([migration.instance_uuid for migration
7015                                          in migrations])
7016 
7017         inst_filters = {'deleted': True, 'soft_deleted': False,
7018                         'uuid': inst_uuid_from_migrations}
7019         attrs = ['info_cache', 'security_groups', 'system_metadata']
7020         with utils.temporary_mutation(context, read_deleted='yes'):
7021             instances = objects.InstanceList.get_by_filters(
7022                 context, inst_filters, expected_attrs=attrs, use_slave=True)
7023 
7024         for instance in instances:
7025             if instance.host != CONF.host:
7026                 for migration in migrations:
7027                     if instance.uuid == migration.instance_uuid:
7028                         # Delete instance files if not cleanup properly either
7029                         # from the source or destination compute nodes when
7030                         # the instance is deleted during resizing.
7031                         self.driver.delete_instance_files(instance)
7032                         try:
7033                             migration.status = 'failed'
7034                             with migration.obj_as_admin():
7035                                 migration.save()
7036                         except exception.MigrationNotFound:
7037                             LOG.warning("Migration %s is not found.",
7038                                         migration.id,
7039                                         instance=instance)
7040                         break
7041 
7042     @messaging.expected_exceptions(exception.InstanceQuiesceNotSupported,
7043                                    exception.QemuGuestAgentNotEnabled,
7044                                    exception.NovaException,
7045                                    NotImplementedError)
7046     @wrap_exception()
7047     def quiesce_instance(self, context, instance):
7048         """Quiesce an instance on this host."""
7049         context = context.elevated()
7050         image_meta = objects.ImageMeta.from_instance(instance)
7051         self.driver.quiesce(context, instance, image_meta)
7052 
7053     def _wait_for_snapshots_completion(self, context, mapping):
7054         for mapping_dict in mapping:
7055             if mapping_dict.get('source_type') == 'snapshot':
7056 
7057                 def _wait_snapshot():
7058                     snapshot = self.volume_api.get_snapshot(
7059                         context, mapping_dict['snapshot_id'])
7060                     if snapshot.get('status') != 'creating':
7061                         raise loopingcall.LoopingCallDone()
7062 
7063                 timer = loopingcall.FixedIntervalLoopingCall(_wait_snapshot)
7064                 timer.start(interval=0.5).wait()
7065 
7066     @messaging.expected_exceptions(exception.InstanceQuiesceNotSupported,
7067                                    exception.QemuGuestAgentNotEnabled,
7068                                    exception.NovaException,
7069                                    NotImplementedError)
7070     @wrap_exception()
7071     def unquiesce_instance(self, context, instance, mapping=None):
7072         """Unquiesce an instance on this host.
7073 
7074         If snapshots' image mapping is provided, it waits until snapshots are
7075         completed before unqueiscing.
7076         """
7077         context = context.elevated()
7078         if mapping:
7079             try:
7080                 self._wait_for_snapshots_completion(context, mapping)
7081             except Exception as error:
7082                 LOG.exception("Exception while waiting completion of "
7083                               "volume snapshots: %s",
7084                               error, instance=instance)
7085         image_meta = objects.ImageMeta.from_instance(instance)
7086         self.driver.unquiesce(context, instance, image_meta)
