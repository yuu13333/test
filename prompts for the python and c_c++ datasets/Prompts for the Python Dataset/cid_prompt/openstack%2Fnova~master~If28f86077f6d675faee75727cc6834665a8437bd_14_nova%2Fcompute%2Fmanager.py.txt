Please review the code below for security defects. You can consider defect types in terms of:
1.CWE-284 (Improper Access Control)
2.CWE-435 (Improper Interaction Between Multiple Entities)
3.CWE-664 (Improper Control of a Resource Through its Lifetime)
4.CWE-682 (Incorrect Calculation)
5.CWE-691 (Insufficient Control Flow Management)
6.CWE-693 (Protection Mechanism Failure)
7.CWE-697 (Incorrect Comparison)
8.CWE-703 (Improper Check or Handling of Exceptional Conditions)
9.CWE-707 (Improper Neutralization)
10.CWE-710 (Improper Adherence to Coding Standards)
If any are found, please describe the security defect in detail and indicate the corresponding line number of code and solution. If none are detected, states: 'No security defects are detected in the code'.

1 # Copyright 2010 United States Government as represented by the
2 # Administrator of the National Aeronautics and Space Administration.
3 # Copyright 2011 Justin Santa Barbara
4 # All Rights Reserved.
5 #
6 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
7 #    not use this file except in compliance with the License. You may obtain
8 #    a copy of the License at
9 #
10 #         http://www.apache.org/licenses/LICENSE-2.0
11 #
12 #    Unless required by applicable law or agreed to in writing, software
13 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
14 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
15 #    License for the specific language governing permissions and limitations
16 #    under the License.
17 
18 """Handles all processes relating to instances (guest vms).
19 
20 The :py:class:`ComputeManager` class is a :py:class:`nova.manager.Manager` that
21 handles RPC calls relating to creating instances.  It is responsible for
22 building a disk image, launching it via the underlying virtualization driver,
23 responding to calls to check its state, attaching persistent storage, and
24 terminating it.
25 
26 """
27 
28 import base64
29 import binascii
30 import contextlib
31 import functools
32 import inspect
33 import sys
34 import time
35 import traceback
36 
37 from cinderclient import exceptions as cinder_exception
38 import eventlet.event
39 from eventlet import greenthread
40 import eventlet.semaphore
41 import eventlet.timeout
42 from keystoneauth1 import exceptions as keystone_exception
43 from oslo_log import log as logging
44 import oslo_messaging as messaging
45 from oslo_serialization import jsonutils
46 from oslo_service import loopingcall
47 from oslo_service import periodic_task
48 from oslo_utils import excutils
49 from oslo_utils import strutils
50 from oslo_utils import timeutils
51 from oslo_utils import uuidutils
52 import six
53 from six.moves import range
54 
55 from nova import block_device
56 from nova.cells import rpcapi as cells_rpcapi
57 from nova.cloudpipe import pipelib
58 from nova import compute
59 from nova.compute import build_results
60 from nova.compute import claims
61 from nova.compute import power_state
62 from nova.compute import resource_tracker
63 from nova.compute import rpcapi as compute_rpcapi
64 from nova.compute import task_states
65 from nova.compute import utils as compute_utils
66 from nova.compute.utils import wrap_instance_event
67 from nova.compute import vm_states
68 from nova import conductor
69 import nova.conf
70 from nova import consoleauth
71 import nova.context
72 from nova import exception
73 from nova import exception_wrapper
74 from nova import hooks
75 from nova.i18n import _
76 from nova.i18n import _LE
77 from nova.i18n import _LI
78 from nova.i18n import _LW
79 from nova import image
80 from nova.image import glance
81 from nova import manager
82 from nova import network
83 from nova.network import base_api as base_net_api
84 from nova.network import model as network_model
85 from nova.network.security_group import openstack_driver
86 from nova import objects
87 from nova.objects import base as obj_base
88 from nova.objects import fields
89 from nova.objects import instance as obj_instance
90 from nova.objects import migrate_data as migrate_data_obj
91 from nova.pci import whitelist
92 from nova import rpc
93 from nova import safe_utils
94 from nova.scheduler import client as scheduler_client
95 from nova import utils
96 from nova.virt import block_device as driver_block_device
97 from nova.virt import configdrive
98 from nova.virt import driver
99 from nova.virt import event as virtevent
100 from nova.virt import storage_users
101 from nova.virt import virtapi
102 from nova.volume import cinder
103 from nova.volume import encryptors
104 
105 CONF = nova.conf.CONF
106 
107 LOG = logging.getLogger(__name__)
108 
109 get_notifier = functools.partial(rpc.get_notifier, service='compute')
110 wrap_exception = functools.partial(exception_wrapper.wrap_exception,
111                                    get_notifier=get_notifier,
112                                    binary='nova-compute')
113 
114 
115 @utils.expects_func_args('migration')
116 def errors_out_migration(function):
117     """Decorator to error out migration on failure."""
118 
119     @functools.wraps(function)
120     def decorated_function(self, context, *args, **kwargs):
121         try:
122             return function(self, context, *args, **kwargs)
123         except Exception as ex:
124             with excutils.save_and_reraise_exception():
125                 wrapped_func = safe_utils.get_wrapped_function(function)
126                 keyed_args = inspect.getcallargs(wrapped_func, self, context,
127                                                  *args, **kwargs)
128                 migration = keyed_args['migration']
129 
130                 # NOTE(rajesht): If InstanceNotFound error is thrown from
131                 # decorated function, migration status should be set to
132                 # 'error', without checking current migration status.
133                 if not isinstance(ex, exception.InstanceNotFound):
134                     status = migration.status
135                     if status not in ['migrating', 'post-migrating']:
136                         return
137 
138                 migration.status = 'error'
139                 try:
140                     with migration.obj_as_admin():
141                         migration.save()
142                 except Exception:
143                     LOG.debug('Error setting migration status '
144                               'for instance %s.',
145                               migration.instance_uuid, exc_info=True)
146 
147     return decorated_function
148 
149 
150 @utils.expects_func_args('instance')
151 def reverts_task_state(function):
152     """Decorator to revert task_state on failure."""
153 
154     @functools.wraps(function)
155     def decorated_function(self, context, *args, **kwargs):
156         try:
157             return function(self, context, *args, **kwargs)
158         except exception.UnexpectedTaskStateError as e:
159             # Note(maoy): unexpected task state means the current
160             # task is preempted. Do not clear task state in this
161             # case.
162             with excutils.save_and_reraise_exception():
163                 LOG.info(_LI("Task possibly preempted: %s"),
164                          e.format_message())
165         except Exception:
166             with excutils.save_and_reraise_exception():
167                 wrapped_func = safe_utils.get_wrapped_function(function)
168                 keyed_args = inspect.getcallargs(wrapped_func, self, context,
169                                                  *args, **kwargs)
170                 # NOTE(mriedem): 'instance' must be in keyed_args because we
171                 # have utils.expects_func_args('instance') decorating this
172                 # method.
173                 instance = keyed_args['instance']
174                 original_task_state = instance.task_state
175                 try:
176                     self._instance_update(context, instance, task_state=None)
177                     LOG.info(_LI("Successfully reverted task state from %s on "
178                                  "failure for instance."), original_task_state,
179                                                            instance=instance)
180                 except exception.InstanceNotFound:
181                     # We might delete an instance that failed to build shortly
182                     # after it errored out this is an expected case and we
183                     # should not trace on it.
184                     pass
185                 except Exception as e:
186                     msg = _LW("Failed to revert task state for instance. "
187                               "Error: %s")
188                     LOG.warning(msg, e, instance=instance)
189 
190     return decorated_function
191 
192 
193 @utils.expects_func_args('instance')
194 def wrap_instance_fault(function):
195     """Wraps a method to catch exceptions related to instances.
196 
197     This decorator wraps a method to catch any exceptions having to do with
198     an instance that may get thrown. It then logs an instance fault in the db.
199     """
200 
201     @functools.wraps(function)
202     def decorated_function(self, context, *args, **kwargs):
203         try:
204             return function(self, context, *args, **kwargs)
205         except exception.InstanceNotFound:
206             raise
207         except Exception as e:
208             # NOTE(gtt): If argument 'instance' is in args rather than kwargs,
209             # we will get a KeyError exception which will cover up the real
210             # exception. So, we update kwargs with the values from args first.
211             # then, we can get 'instance' from kwargs easily.
212             kwargs.update(dict(zip(function.__code__.co_varnames[2:], args)))
213 
214             with excutils.save_and_reraise_exception():
215                 compute_utils.add_instance_fault_from_exc(context,
216                         kwargs['instance'], e, sys.exc_info())
217 
218     return decorated_function
219 
220 
221 @utils.expects_func_args('image_id', 'instance')
222 def delete_image_on_error(function):
223     """Used for snapshot related method to ensure the image created in
224     compute.api is deleted when an error occurs.
225     """
226 
227     @functools.wraps(function)
228     def decorated_function(self, context, image_id, instance,
229                            *args, **kwargs):
230         try:
231             return function(self, context, image_id, instance,
232                             *args, **kwargs)
233         except Exception:
234             with excutils.save_and_reraise_exception():
235                 LOG.debug("Cleaning up image %s", image_id,
236                           exc_info=True, instance=instance)
237                 try:
238                     self.image_api.delete(context, image_id)
239                 except Exception:
240                     LOG.exception(_LE("Error while trying to clean up "
241                                       "image %s"), image_id,
242                                   instance=instance)
243 
244     return decorated_function
245 
246 
247 # TODO(danms): Remove me after Icehouse
248 # TODO(alaski): Actually remove this after Newton, assuming a major RPC bump
249 # NOTE(mikal): if the method being decorated has more than one decorator, then
250 # put this one first. Otherwise the various exception handling decorators do
251 # not function correctly.
252 def object_compat(function):
253     """Wraps a method that expects a new-world instance
254 
255     This provides compatibility for callers passing old-style dict
256     instances.
257     """
258 
259     @functools.wraps(function)
260     def decorated_function(self, context, *args, **kwargs):
261         def _load_instance(instance_or_dict):
262             if isinstance(instance_or_dict, dict):
263                 # try to get metadata and system_metadata for most cases but
264                 # only attempt to load those if the db instance already has
265                 # those fields joined
266                 metas = [meta for meta in ('metadata', 'system_metadata')
267                          if meta in instance_or_dict]
268                 instance = objects.Instance._from_db_object(
269                     context, objects.Instance(), instance_or_dict,
270                     expected_attrs=metas)
271                 instance._context = context
272                 return instance
273             return instance_or_dict
274 
275         try:
276             kwargs['instance'] = _load_instance(kwargs['instance'])
277         except KeyError:
278             args = (_load_instance(args[0]),) + args[1:]
279 
280         migration = kwargs.get('migration')
281         if isinstance(migration, dict):
282             migration = objects.Migration._from_db_object(
283                     context.elevated(), objects.Migration(),
284                     migration)
285             kwargs['migration'] = migration
286 
287         return function(self, context, *args, **kwargs)
288 
289     return decorated_function
290 
291 
292 class InstanceEvents(object):
293     def __init__(self):
294         self._events = {}
295 
296     @staticmethod
297     def _lock_name(instance):
298         return '%s-%s' % (instance.uuid, 'events')
299 
300     def prepare_for_instance_event(self, instance, event_name):
301         """Prepare to receive an event for an instance.
302 
303         This will register an event for the given instance that we will
304         wait on later. This should be called before initiating whatever
305         action will trigger the event. The resulting eventlet.event.Event
306         object should be wait()'d on to ensure completion.
307 
308         :param instance: the instance for which the event will be generated
309         :param event_name: the name of the event we're expecting
310         :returns: an event object that should be wait()'d on
311         """
312         if self._events is None:
313             # NOTE(danms): We really should have a more specific error
314             # here, but this is what we use for our default error case
315             raise exception.NovaException('In shutdown, no new events '
316                                           'can be scheduled')
317 
318         @utils.synchronized(self._lock_name(instance))
319         def _create_or_get_event():
320             instance_events = self._events.setdefault(instance.uuid, {})
321             return instance_events.setdefault(event_name,
322                                               eventlet.event.Event())
323         LOG.debug('Preparing to wait for external event %(event)s',
324                   {'event': event_name}, instance=instance)
325         return _create_or_get_event()
326 
327     def pop_instance_event(self, instance, event):
328         """Remove a pending event from the wait list.
329 
330         This will remove a pending event from the wait list so that it
331         can be used to signal the waiters to wake up.
332 
333         :param instance: the instance for which the event was generated
334         :param event: the nova.objects.external_event.InstanceExternalEvent
335                       that describes the event
336         :returns: the eventlet.event.Event object on which the waiters
337                   are blocked
338         """
339         no_events_sentinel = object()
340         no_matching_event_sentinel = object()
341 
342         @utils.synchronized(self._lock_name(instance))
343         def _pop_event():
344             if not self._events:
345                 LOG.debug('Unexpected attempt to pop events during shutdown',
346                           instance=instance)
347                 return no_events_sentinel
348             events = self._events.get(instance.uuid)
349             if not events:
350                 return no_events_sentinel
351             _event = events.pop(event.key, None)
352             if not events:
353                 del self._events[instance.uuid]
354             if _event is None:
355                 return no_matching_event_sentinel
356             return _event
357 
358         result = _pop_event()
359         if result is no_events_sentinel:
360             LOG.debug('No waiting events found dispatching %(event)s',
361                       {'event': event.key},
362                       instance=instance)
363             return None
364         elif result is no_matching_event_sentinel:
365             LOG.debug('No event matching %(event)s in %(events)s',
366                       {'event': event.key,
367                        'events': self._events.get(instance.uuid, {}).keys()},
368                       instance=instance)
369             return None
370         else:
371             return result
372 
373     def clear_events_for_instance(self, instance):
374         """Remove all pending events for an instance.
375 
376         This will remove all events currently pending for an instance
377         and return them (indexed by event name).
378 
379         :param instance: the instance for which events should be purged
380         :returns: a dictionary of {event_name: eventlet.event.Event}
381         """
382         @utils.synchronized(self._lock_name(instance))
383         def _clear_events():
384             if self._events is None:
385                 LOG.debug('Unexpected attempt to clear events during shutdown',
386                           instance=instance)
387                 return dict()
388             return self._events.pop(instance.uuid, {})
389         return _clear_events()
390 
391     def cancel_all_events(self):
392         if self._events is None:
393             LOG.debug('Unexpected attempt to cancel events during shutdown.')
394             return
395         our_events = self._events
396         # NOTE(danms): Block new events
397         self._events = None
398 
399         for instance_uuid, events in our_events.items():
400             for event_name, eventlet_event in events.items():
401                 LOG.debug('Canceling in-flight event %(event)s for '
402                           'instance %(instance_uuid)s',
403                           {'event': event_name,
404                            'instance_uuid': instance_uuid})
405                 name, tag = event_name.rsplit('-', 1)
406                 event = objects.InstanceExternalEvent(
407                     instance_uuid=instance_uuid,
408                     name=name, status='failed',
409                     tag=tag, data={})
410                 eventlet_event.send(event)
411 
412 
413 class ComputeVirtAPI(virtapi.VirtAPI):
414     def __init__(self, compute):
415         super(ComputeVirtAPI, self).__init__()
416         self._compute = compute
417 
418     def _default_error_callback(self, event_name, instance):
419         raise exception.NovaException(_('Instance event failed'))
420 
421     @contextlib.contextmanager
422     def wait_for_instance_event(self, instance, event_names, deadline=300,
423                                 error_callback=None):
424         """Plan to wait for some events, run some code, then wait.
425 
426         This context manager will first create plans to wait for the
427         provided event_names, yield, and then wait for all the scheduled
428         events to complete.
429 
430         Note that this uses an eventlet.timeout.Timeout to bound the
431         operation, so callers should be prepared to catch that
432         failure and handle that situation appropriately.
433 
434         If the event is not received by the specified timeout deadline,
435         eventlet.timeout.Timeout is raised.
436 
437         If the event is received but did not have a 'completed'
438         status, a NovaException is raised.  If an error_callback is
439         provided, instead of raising an exception as detailed above
440         for the failure case, the callback will be called with the
441         event_name and instance, and can return True to continue
442         waiting for the rest of the events, False to stop processing,
443         or raise an exception which will bubble up to the waiter.
444 
445         :param instance: The instance for which an event is expected
446         :param event_names: A list of event names. Each element can be a
447                             string event name or tuple of strings to
448                             indicate (name, tag).
449         :param deadline: Maximum number of seconds we should wait for all
450                          of the specified events to arrive.
451         :param error_callback: A function to be called if an event arrives
452 
453         """
454 
455         if error_callback is None:
456             error_callback = self._default_error_callback
457         events = {}
458         for event_name in event_names:
459             if isinstance(event_name, tuple):
460                 name, tag = event_name
461                 event_name = objects.InstanceExternalEvent.make_key(
462                     name, tag)
463             try:
464                 events[event_name] = (
465                     self._compute.instance_events.prepare_for_instance_event(
466                         instance, event_name))
467             except exception.NovaException:
468                 error_callback(event_name, instance)
469                 # NOTE(danms): Don't wait for any of the events. They
470                 # should all be canceled and fired immediately below,
471                 # but don't stick around if not.
472                 deadline = 0
473         yield
474         with eventlet.timeout.Timeout(deadline):
475             for event_name, event in events.items():
476                 actual_event = event.wait()
477                 if actual_event.status == 'completed':
478                     continue
479                 decision = error_callback(event_name, instance)
480                 if decision is False:
481                     break
482 
483 
484 class ComputeManager(manager.Manager):
485     """Manages the running instances from creation to destruction."""
486 
487     target = messaging.Target(version='4.14')
488 
489     # How long to wait in seconds before re-issuing a shutdown
490     # signal to an instance during power off.  The overall
491     # time to wait is set by CONF.shutdown_timeout.
492     SHUTDOWN_RETRY_INTERVAL = 10
493 
494     def __init__(self, compute_driver=None, *args, **kwargs):
495         """Load configuration options and connect to the hypervisor."""
496         self.virtapi = ComputeVirtAPI(self)
497         self.network_api = network.API()
498         self.volume_api = cinder.API()
499         self.image_api = image.API()
500         self._last_host_check = 0
501         self._last_bw_usage_poll = 0
502         self._bw_usage_supported = True
503         self._last_bw_usage_cell_update = 0
504         self.compute_api = compute.API()
505         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
506         self.conductor_api = conductor.API()
507         self.compute_task_api = conductor.ComputeTaskAPI()
508         self.is_neutron_security_groups = (
509             openstack_driver.is_neutron_security_groups())
510         self.consoleauth_rpcapi = consoleauth.rpcapi.ConsoleAuthAPI()
511         self.cells_rpcapi = cells_rpcapi.CellsAPI()
512         self.scheduler_client = scheduler_client.SchedulerClient()
513         self._resource_tracker_dict = {}
514         self.instance_events = InstanceEvents()
515         self._sync_power_pool = eventlet.GreenPool(
516             size=CONF.sync_power_state_pool_size)
517         self._syncs_in_progress = {}
518         self.send_instance_updates = (
519             CONF.filter_scheduler.track_instance_changes)
520         if CONF.max_concurrent_builds != 0:
521             self._build_semaphore = eventlet.semaphore.Semaphore(
522                 CONF.max_concurrent_builds)
523         else:
524             self._build_semaphore = compute_utils.UnlimitedSemaphore()
525         if max(CONF.max_concurrent_live_migrations, 0) != 0:
526             self._live_migration_semaphore = eventlet.semaphore.Semaphore(
527                 CONF.max_concurrent_live_migrations)
528         else:
529             self._live_migration_semaphore = compute_utils.UnlimitedSemaphore()
530 
531         super(ComputeManager, self).__init__(service_name="compute",
532                                              *args, **kwargs)
533 
534         # NOTE(russellb) Load the driver last.  It may call back into the
535         # compute manager via the virtapi, so we want it to be fully
536         # initialized before that happens.
537         self.driver = driver.load_compute_driver(self.virtapi, compute_driver)
538         self.use_legacy_block_device_info = \
539                             self.driver.need_legacy_block_device_info
540 
541     def reset(self):
542         LOG.info(_LI('Reloading compute RPC API'))
543         compute_rpcapi.LAST_VERSION = None
544         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
545 
546     def _get_resource_tracker(self, nodename):
547         rt = self._resource_tracker_dict.get(nodename)
548         if not rt:
549             if not self.driver.node_is_available(nodename):
550                 raise exception.NovaException(
551                         _("%s is not a valid node managed by this "
552                           "compute host.") % nodename)
553 
554             rt = resource_tracker.ResourceTracker(self.host,
555                                                   self.driver,
556                                                   nodename)
557             self._resource_tracker_dict[nodename] = rt
558         return rt
559 
560     def _update_resource_tracker(self, context, instance):
561         """Let the resource tracker know that an instance has changed state."""
562 
563         if (instance.host == self.host and
564                 self.driver.node_is_available(instance.node)):
565             rt = self._get_resource_tracker(instance.node)
566             rt.update_usage(context, instance)
567 
568     def _instance_update(self, context, instance, **kwargs):
569         """Update an instance in the database using kwargs as value."""
570 
571         for k, v in kwargs.items():
572             setattr(instance, k, v)
573         instance.save()
574         self._update_resource_tracker(context, instance)
575 
576     def _nil_out_instance_obj_host_and_node(self, instance):
577         # NOTE(jwcroppe): We don't do instance.save() here for performance
578         # reasons; a call to this is expected to be immediately followed by
579         # another call that does instance.save(), thus avoiding two writes
580         # to the database layer.
581         instance.host = None
582         instance.node = None
583 
584     def _set_instance_obj_error_state(self, context, instance,
585                                       clean_task_state=False):
586         try:
587             instance.vm_state = vm_states.ERROR
588             if clean_task_state:
589                 instance.task_state = None
590             instance.save()
591         except exception.InstanceNotFound:
592             LOG.debug('Instance has been destroyed from under us while '
593                       'trying to set it to ERROR', instance=instance)
594 
595     def _get_instances_on_driver(self, context, filters=None):
596         """Return a list of instance records for the instances found
597         on the hypervisor which satisfy the specified filters. If filters=None
598         return a list of instance records for all the instances found on the
599         hypervisor.
600         """
601         if not filters:
602             filters = {}
603         try:
604             driver_uuids = self.driver.list_instance_uuids()
605             if len(driver_uuids) == 0:
606                 # Short circuit, don't waste a DB call
607                 return objects.InstanceList()
608             filters['uuid'] = driver_uuids
609             local_instances = objects.InstanceList.get_by_filters(
610                 context, filters, use_slave=True)
611             return local_instances
612         except NotImplementedError:
613             pass
614 
615         # The driver doesn't support uuids listing, so we'll have
616         # to brute force.
617         driver_instances = self.driver.list_instances()
618         instances = objects.InstanceList.get_by_filters(context, filters,
619                                                         use_slave=True)
620         name_map = {instance.name: instance for instance in instances}
621         local_instances = []
622         for driver_instance in driver_instances:
623             instance = name_map.get(driver_instance)
624             if not instance:
625                 continue
626             local_instances.append(instance)
627         return local_instances
628 
629     def _destroy_evacuated_instances(self, context):
630         """Destroys evacuated instances.
631 
632         While nova-compute was down, the instances running on it could be
633         evacuated to another host. Check that the instances reported
634         by the driver are still associated with this host.  If they are
635         not, destroy them, with the exception of instances which are in
636         the MIGRATING, RESIZE_MIGRATING, RESIZE_MIGRATED, RESIZE_FINISH
637         task state or RESIZED vm state.
638         """
639         filters = {
640             'source_compute': self.host,
641             'status': ['accepted', 'done'],
642             'migration_type': 'evacuation',
643         }
644         evacuations = objects.MigrationList.get_by_filters(context, filters)
645         if not evacuations:
646             return
647         evacuations = {mig.instance_uuid: mig for mig in evacuations}
648 
649         filters = {'deleted': False}
650         local_instances = self._get_instances_on_driver(context, filters)
651         evacuated = [inst for inst in local_instances
652                      if inst.uuid in evacuations]
653         for instance in evacuated:
654             migration = evacuations[instance.uuid]
655             LOG.info(_LI('Deleting instance as it has been evacuated from '
656                          'this host'), instance=instance)
657             try:
658                 network_info = self.network_api.get_instance_nw_info(
659                     context, instance)
660                 bdi = self._get_instance_block_device_info(context,
661                                                            instance)
662                 destroy_disks = not (self._is_instance_storage_shared(
663                     context, instance))
664             except exception.InstanceNotFound:
665                 network_info = network_model.NetworkInfo()
666                 bdi = {}
667                 LOG.info(_LI('Instance has been marked deleted already, '
668                              'removing it from the hypervisor.'),
669                          instance=instance)
670                 # always destroy disks if the instance was deleted
671                 destroy_disks = True
672             self.driver.destroy(context, instance,
673                                 network_info,
674                                 bdi, destroy_disks)
675             migration.status = 'completed'
676             migration.save()
677 
678     def _is_instance_storage_shared(self, context, instance, host=None):
679         shared_storage = True
680         data = None
681         try:
682             data = self.driver.check_instance_shared_storage_local(context,
683                                                        instance)
684             if data:
685                 shared_storage = (self.compute_rpcapi.
686                                   check_instance_shared_storage(context,
687                                   instance, data, host=host))
688         except NotImplementedError:
689             LOG.debug('Hypervisor driver does not support '
690                       'instance shared storage check, '
691                       'assuming it\'s not on shared storage',
692                       instance=instance)
693             shared_storage = False
694         except Exception:
695             LOG.exception(_LE('Failed to check if instance shared'),
696                       instance=instance)
697         finally:
698             if data:
699                 self.driver.check_instance_shared_storage_cleanup(context,
700                                                                   data)
701         return shared_storage
702 
703     def _complete_partial_deletion(self, context, instance):
704         """Complete deletion for instances in DELETED status but not marked as
705         deleted in the DB
706         """
707         system_meta = instance.system_metadata
708         instance.destroy()
709         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
710                 context, instance.uuid)
711         quotas = objects.Quotas(context=context)
712         project_id, user_id = objects.quotas.ids_from_instance(context,
713                                                                instance)
714         quotas.reserve(project_id=project_id, user_id=user_id, instances=-1,
715                        cores=-instance.flavor.vcpus,
716                        ram=-instance.flavor.memory_mb)
717         self._complete_deletion(context,
718                                 instance,
719                                 bdms,
720                                 quotas,
721                                 system_meta)
722 
723     def _complete_deletion(self, context, instance, bdms,
724                            quotas, system_meta):
725         if quotas:
726             quotas.commit()
727 
728         # ensure block device mappings are not leaked
729         for bdm in bdms:
730             bdm.destroy()
731 
732         self._update_resource_tracker(context, instance)
733         self._notify_about_instance_usage(context, instance, "delete.end",
734                 system_metadata=system_meta)
735         compute_utils.notify_about_instance_action(context, instance,
736                 self.host, action=fields.NotificationAction.DELETE,
737                 phase=fields.NotificationPhase.END)
738         self._clean_instance_console_tokens(context, instance)
739         self._delete_scheduler_instance_info(context, instance.uuid)
740 
741     def _create_reservations(self, context, instance, project_id, user_id):
742         vcpus = instance.flavor.vcpus
743         mem_mb = instance.flavor.memory_mb
744 
745         quotas = objects.Quotas(context=context)
746         quotas.reserve(project_id=project_id,
747                        user_id=user_id,
748                        instances=-1,
749                        cores=-vcpus,
750                        ram=-mem_mb)
751         return quotas
752 
753     def _init_instance(self, context, instance):
754         '''Initialize this instance during service init.'''
755 
756         # NOTE(danms): If the instance appears to not be owned by this
757         # host, it may have been evacuated away, but skipped by the
758         # evacuation cleanup code due to configuration. Thus, if that
759         # is a possibility, don't touch the instance in any way, but
760         # log the concern. This will help avoid potential issues on
761         # startup due to misconfiguration.
762         if instance.host != self.host:
763             LOG.warning(_LW('Instance %(uuid)s appears to not be owned '
764                             'by this host, but by %(host)s. Startup '
765                             'processing is being skipped.'),
766                         {'uuid': instance.uuid,
767                          'host': instance.host})
768             return
769 
770         # Instances that are shut down, or in an error state can not be
771         # initialized and are not attempted to be recovered. The exception
772         # to this are instances that are in RESIZE_MIGRATING or DELETING,
773         # which are dealt with further down.
774         if (instance.vm_state == vm_states.SOFT_DELETED or
775             (instance.vm_state == vm_states.ERROR and
776             instance.task_state not in
777             (task_states.RESIZE_MIGRATING, task_states.DELETING))):
778             LOG.debug("Instance is in %s state.",
779                       instance.vm_state, instance=instance)
780             return
781 
782         if instance.vm_state == vm_states.DELETED:
783             try:
784                 self._complete_partial_deletion(context, instance)
785             except Exception:
786                 # we don't want that an exception blocks the init_host
787                 msg = _LE('Failed to complete a deletion')
788                 LOG.exception(msg, instance=instance)
789             return
790 
791         if (instance.vm_state == vm_states.BUILDING or
792             instance.task_state in [task_states.SCHEDULING,
793                                     task_states.BLOCK_DEVICE_MAPPING,
794                                     task_states.NETWORKING,
795                                     task_states.SPAWNING]):
796             # NOTE(dave-mcnally) compute stopped before instance was fully
797             # spawned so set to ERROR state. This is safe to do as the state
798             # may be set by the api but the host is not so if we get here the
799             # instance has already been scheduled to this particular host.
800             LOG.debug("Instance failed to spawn correctly, "
801                       "setting to ERROR state", instance=instance)
802             instance.task_state = None
803             instance.vm_state = vm_states.ERROR
804             instance.save()
805             return
806 
807         if (instance.vm_state in [vm_states.ACTIVE, vm_states.STOPPED] and
808             instance.task_state in [task_states.REBUILDING,
809                                     task_states.REBUILD_BLOCK_DEVICE_MAPPING,
810                                     task_states.REBUILD_SPAWNING]):
811             # NOTE(jichenjc) compute stopped before instance was fully
812             # spawned so set to ERROR state. This is consistent to BUILD
813             LOG.debug("Instance failed to rebuild correctly, "
814                       "setting to ERROR state", instance=instance)
815             instance.task_state = None
816             instance.vm_state = vm_states.ERROR
817             instance.save()
818             return
819 
820         if (instance.vm_state != vm_states.ERROR and
821             instance.task_state in [task_states.IMAGE_SNAPSHOT_PENDING,
822                                     task_states.IMAGE_PENDING_UPLOAD,
823                                     task_states.IMAGE_UPLOADING,
824                                     task_states.IMAGE_SNAPSHOT]):
825             LOG.debug("Instance in transitional state %s at start-up "
826                       "clearing task state",
827                       instance.task_state, instance=instance)
828             try:
829                 self._post_interrupted_snapshot_cleanup(context, instance)
830             except Exception:
831                 # we don't want that an exception blocks the init_host
832                 msg = _LE('Failed to cleanup snapshot.')
833                 LOG.exception(msg, instance=instance)
834             instance.task_state = None
835             instance.save()
836 
837         if (instance.vm_state != vm_states.ERROR and
838             instance.task_state in [task_states.RESIZE_PREP]):
839             LOG.debug("Instance in transitional state %s at start-up "
840                       "clearing task state",
841                       instance['task_state'], instance=instance)
842             instance.task_state = None
843             instance.save()
844 
845         if instance.task_state == task_states.DELETING:
846             try:
847                 LOG.info(_LI('Service started deleting the instance during '
848                              'the previous run, but did not finish. Restarting'
849                              ' the deletion now.'), instance=instance)
850                 instance.obj_load_attr('metadata')
851                 instance.obj_load_attr('system_metadata')
852                 bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
853                         context, instance.uuid)
854                 project_id, user_id = objects.quotas.ids_from_instance(
855                     context, instance)
856                 quotas = self._create_reservations(context, instance,
857                                                    project_id, user_id)
858 
859                 self._delete_instance(context, instance, bdms, quotas)
860             except Exception:
861                 # we don't want that an exception blocks the init_host
862                 msg = _LE('Failed to complete a deletion')
863                 LOG.exception(msg, instance=instance)
864                 self._set_instance_obj_error_state(context, instance)
865             return
866 
867         current_power_state = self._get_power_state(context, instance)
868         try_reboot, reboot_type = self._retry_reboot(context, instance,
869                                                      current_power_state)
870 
871         if try_reboot:
872             LOG.debug("Instance in transitional state (%(task_state)s) at "
873                       "start-up and power state is (%(power_state)s), "
874                       "triggering reboot",
875                       {'task_state': instance.task_state,
876                        'power_state': current_power_state},
877                       instance=instance)
878 
879             # NOTE(mikal): if the instance was doing a soft reboot that got as
880             # far as shutting down the instance but not as far as starting it
881             # again, then we've just become a hard reboot. That means the
882             # task state for the instance needs to change so that we're in one
883             # of the expected task states for a hard reboot.
884             soft_types = [task_states.REBOOT_STARTED,
885                           task_states.REBOOT_PENDING,
886                           task_states.REBOOTING]
887             if instance.task_state in soft_types and reboot_type == 'HARD':
888                 instance.task_state = task_states.REBOOT_PENDING_HARD
889                 instance.save()
890 
891             self.reboot_instance(context, instance, block_device_info=None,
892                                  reboot_type=reboot_type)
893             return
894 
895         elif (current_power_state == power_state.RUNNING and
896               instance.task_state in [task_states.REBOOT_STARTED,
897                                       task_states.REBOOT_STARTED_HARD,
898                                       task_states.PAUSING,
899                                       task_states.UNPAUSING]):
900             LOG.warning(_LW("Instance in transitional state "
901                             "(%(task_state)s) at start-up and power state "
902                             "is (%(power_state)s), clearing task state"),
903                         {'task_state': instance.task_state,
904                          'power_state': current_power_state},
905                         instance=instance)
906             instance.task_state = None
907             instance.vm_state = vm_states.ACTIVE
908             instance.save()
909         elif (current_power_state == power_state.PAUSED and
910               instance.task_state == task_states.UNPAUSING):
911             LOG.warning(_LW("Instance in transitional state "
912                             "(%(task_state)s) at start-up and power state "
913                             "is (%(power_state)s), clearing task state "
914                             "and unpausing the instance"),
915                         {'task_state': instance.task_state,
916                          'power_state': current_power_state},
917                         instance=instance)
918             try:
919                 self.unpause_instance(context, instance)
920             except NotImplementedError:
921                 # Some virt driver didn't support pause and unpause
922                 pass
923             except Exception:
924                 LOG.exception(_LE('Failed to unpause instance'),
925                               instance=instance)
926             return
927 
928         if instance.task_state == task_states.POWERING_OFF:
929             try:
930                 LOG.debug("Instance in transitional state %s at start-up "
931                           "retrying stop request",
932                           instance.task_state, instance=instance)
933                 self.stop_instance(context, instance, True)
934             except Exception:
935                 # we don't want that an exception blocks the init_host
936                 msg = _LE('Failed to stop instance')
937                 LOG.exception(msg, instance=instance)
938             return
939 
940         if instance.task_state == task_states.POWERING_ON:
941             try:
942                 LOG.debug("Instance in transitional state %s at start-up "
943                           "retrying start request",
944                           instance.task_state, instance=instance)
945                 self.start_instance(context, instance)
946             except Exception:
947                 # we don't want that an exception blocks the init_host
948                 msg = _LE('Failed to start instance')
949                 LOG.exception(msg, instance=instance)
950             return
951 
952         net_info = compute_utils.get_nw_info_for_instance(instance)
953         try:
954             self.driver.plug_vifs(instance, net_info)
955         except NotImplementedError as e:
956             LOG.debug(e, instance=instance)
957         except exception.VirtualInterfacePlugException:
958             # we don't want an exception to block the init_host
959             LOG.exception(_LE("Vifs plug failed"), instance=instance)
960             self._set_instance_obj_error_state(context, instance)
961             return
962 
963         if instance.task_state == task_states.RESIZE_MIGRATING:
964             # We crashed during resize/migration, so roll back for safety
965             try:
966                 # NOTE(mriedem): check old_vm_state for STOPPED here, if it's
967                 # not in system_metadata we default to True for backwards
968                 # compatibility
969                 power_on = (instance.system_metadata.get('old_vm_state') !=
970                             vm_states.STOPPED)
971 
972                 block_dev_info = self._get_instance_block_device_info(context,
973                                                                       instance)
974 
975                 self.driver.finish_revert_migration(context,
976                     instance, net_info, block_dev_info, power_on)
977 
978             except Exception:
979                 LOG.exception(_LE('Failed to revert crashed migration'),
980                               instance=instance)
981             finally:
982                 LOG.info(_LI('Instance found in migrating state during '
983                              'startup. Resetting task_state'),
984                          instance=instance)
985                 instance.task_state = None
986                 instance.save()
987         if instance.task_state == task_states.MIGRATING:
988             # Live migration did not complete, but instance is on this
989             # host, so reset the state.
990             instance.task_state = None
991             instance.save(expected_task_state=[task_states.MIGRATING])
992 
993         db_state = instance.power_state
994         drv_state = self._get_power_state(context, instance)
995         expect_running = (db_state == power_state.RUNNING and
996                           drv_state != db_state)
997 
998         LOG.debug('Current state is %(drv_state)s, state in DB is '
999                   '%(db_state)s.',
1000                   {'drv_state': drv_state, 'db_state': db_state},
1001                   instance=instance)
1002 
1003         if expect_running and CONF.resume_guests_state_on_host_boot:
1004             LOG.info(_LI('Rebooting instance after nova-compute restart.'),
1005                      instance=instance)
1006 
1007             block_device_info = \
1008                 self._get_instance_block_device_info(context, instance)
1009 
1010             try:
1011                 self.driver.resume_state_on_host_boot(
1012                     context, instance, net_info, block_device_info)
1013             except NotImplementedError:
1014                 LOG.warning(_LW('Hypervisor driver does not support '
1015                                 'resume guests'), instance=instance)
1016             except Exception:
1017                 # NOTE(vish): The instance failed to resume, so we set the
1018                 #             instance to error and attempt to continue.
1019                 LOG.warning(_LW('Failed to resume instance'),
1020                             instance=instance)
1021                 self._set_instance_obj_error_state(context, instance)
1022 
1023         elif drv_state == power_state.RUNNING:
1024             # VMwareAPI drivers will raise an exception
1025             try:
1026                 self.driver.ensure_filtering_rules_for_instance(
1027                                        instance, net_info)
1028             except NotImplementedError:
1029                 LOG.debug('Hypervisor driver does not support '
1030                           'firewall rules', instance=instance)
1031 
1032     def _retry_reboot(self, context, instance, current_power_state):
1033         current_task_state = instance.task_state
1034         retry_reboot = False
1035         reboot_type = compute_utils.get_reboot_type(current_task_state,
1036                                                     current_power_state)
1037 
1038         pending_soft = (current_task_state == task_states.REBOOT_PENDING and
1039                         instance.vm_state in vm_states.ALLOW_SOFT_REBOOT)
1040         pending_hard = (current_task_state == task_states.REBOOT_PENDING_HARD
1041                         and instance.vm_state in vm_states.ALLOW_HARD_REBOOT)
1042         started_not_running = (current_task_state in
1043                                [task_states.REBOOT_STARTED,
1044                                 task_states.REBOOT_STARTED_HARD] and
1045                                current_power_state != power_state.RUNNING)
1046 
1047         if pending_soft or pending_hard or started_not_running:
1048             retry_reboot = True
1049 
1050         return retry_reboot, reboot_type
1051 
1052     def handle_lifecycle_event(self, event):
1053         LOG.info(_LI("VM %(state)s (Lifecycle Event)"),
1054                  {'state': event.get_name()},
1055                  instance_uuid=event.get_instance_uuid())
1056         context = nova.context.get_admin_context(read_deleted='yes')
1057         instance = objects.Instance.get_by_uuid(context,
1058                                                 event.get_instance_uuid(),
1059                                                 expected_attrs=[])
1060         vm_power_state = None
1061         if event.get_transition() == virtevent.EVENT_LIFECYCLE_STOPPED:
1062             vm_power_state = power_state.SHUTDOWN
1063         elif event.get_transition() == virtevent.EVENT_LIFECYCLE_STARTED:
1064             vm_power_state = power_state.RUNNING
1065         elif event.get_transition() == virtevent.EVENT_LIFECYCLE_PAUSED:
1066             vm_power_state = power_state.PAUSED
1067         elif event.get_transition() == virtevent.EVENT_LIFECYCLE_RESUMED:
1068             vm_power_state = power_state.RUNNING
1069         elif event.get_transition() == virtevent.EVENT_LIFECYCLE_SUSPENDED:
1070             vm_power_state = power_state.SUSPENDED
1071         else:
1072             LOG.warning(_LW("Unexpected power state %d"),
1073                         event.get_transition())
1074 
1075         # Note(lpetrut): The event may be delayed, thus not reflecting
1076         # the current instance power state. In that case, ignore the event.
1077         current_power_state = self._get_power_state(context, instance)
1078         if current_power_state == vm_power_state:
1079             LOG.debug('Synchronizing instance power state after lifecycle '
1080                       'event "%(event)s"; current vm_state: %(vm_state)s, '
1081                       'current task_state: %(task_state)s, current DB '
1082                       'power_state: %(db_power_state)s, VM power_state: '
1083                       '%(vm_power_state)s',
1084                       {'event': event.get_name(),
1085                        'vm_state': instance.vm_state,
1086                        'task_state': instance.task_state,
1087                        'db_power_state': instance.power_state,
1088                        'vm_power_state': vm_power_state},
1089                       instance_uuid=instance.uuid)
1090             self._sync_instance_power_state(context,
1091                                             instance,
1092                                             vm_power_state)
1093 
1094     def handle_events(self, event):
1095         if isinstance(event, virtevent.LifecycleEvent):
1096             try:
1097                 self.handle_lifecycle_event(event)
1098             except exception.InstanceNotFound:
1099                 LOG.debug("Event %s arrived for non-existent instance. The "
1100                           "instance was probably deleted.", event)
1101         else:
1102             LOG.debug("Ignoring event %s", event)
1103 
1104     def init_virt_events(self):
1105         if CONF.workarounds.handle_virt_lifecycle_events:
1106             self.driver.register_event_listener(self.handle_events)
1107         else:
1108             # NOTE(mriedem): If the _sync_power_states periodic task is
1109             # disabled we should emit a warning in the logs.
1110             if CONF.sync_power_state_interval < 0:
1111                 LOG.warning(_LW('Instance lifecycle events from the compute '
1112                              'driver have been disabled. Note that lifecycle '
1113                              'changes to an instance outside of the compute '
1114                              'service will not be synchronized '
1115                              'automatically since the _sync_power_states '
1116                              'periodic task is also disabled.'))
1117             else:
1118                 LOG.info(_LI('Instance lifecycle events from the compute '
1119                              'driver have been disabled. Note that lifecycle '
1120                              'changes to an instance outside of the compute '
1121                              'service will only be synchronized by the '
1122                              '_sync_power_states periodic task.'))
1123 
1124     def init_host(self):
1125         """Initialization for a standalone compute service."""
1126 
1127         if CONF.pci.passthrough_whitelist:
1128             # Simply loading the PCI passthrough whitelist will do a bunch of
1129             # validation that would otherwise wait until the PciDevTracker is
1130             # constructed when updating available resources for the compute
1131             # node(s) in the resource tracker, effectively killing that task.
1132             # So load up the whitelist when starting the compute service to
1133             # flush any invalid configuration early so we can kill the service
1134             # if the configuration is wrong.
1135             whitelist.Whitelist(CONF.pci.passthrough_whitelist)
1136 
1137         self.driver.init_host(host=self.host)
1138         context = nova.context.get_admin_context()
1139         instances = objects.InstanceList.get_by_host(
1140             context, self.host, expected_attrs=['info_cache', 'metadata'])
1141 
1142         if CONF.defer_iptables_apply:
1143             self.driver.filter_defer_apply_on()
1144 
1145         self.init_virt_events()
1146 
1147         try:
1148             # checking that instance was not already evacuated to other host
1149             self._destroy_evacuated_instances(context)
1150             for instance in instances:
1151                 self._init_instance(context, instance)
1152         finally:
1153             if CONF.defer_iptables_apply:
1154                 self.driver.filter_defer_apply_off()
1155             self._update_scheduler_instance_info(context, instances)
1156 
1157     def cleanup_host(self):
1158         self.driver.register_event_listener(None)
1159         self.instance_events.cancel_all_events()
1160         self.driver.cleanup_host(host=self.host)
1161 
1162     def pre_start_hook(self):
1163         """After the service is initialized, but before we fully bring
1164         the service up by listening on RPC queues, make sure to update
1165         our available resources (and indirectly our available nodes).
1166         """
1167         self.update_available_resource(nova.context.get_admin_context())
1168 
1169     def _get_power_state(self, context, instance):
1170         """Retrieve the power state for the given instance."""
1171         LOG.debug('Checking state', instance=instance)
1172         try:
1173             return self.driver.get_info(instance).state
1174         except exception.InstanceNotFound:
1175             return power_state.NOSTATE
1176 
1177     def get_console_topic(self, context):
1178         """Retrieves the console host for a project on this host.
1179 
1180         Currently this is just set in the flags for each compute host.
1181 
1182         """
1183         # TODO(mdragon): perhaps make this variable by console_type?
1184         return '%s.%s' % (CONF.console_topic, CONF.console_host)
1185 
1186     @wrap_exception()
1187     def get_console_pool_info(self, context, console_type):
1188         return self.driver.get_console_pool_info(console_type)
1189 
1190     # NOTE(hanlind): This and the virt method it calls can be removed in
1191     # version 5.0 of the RPC API
1192     @wrap_exception()
1193     def refresh_security_group_rules(self, context, security_group_id):
1194         """Tell the virtualization driver to refresh security group rules.
1195 
1196         Passes straight through to the virtualization driver.
1197 
1198         """
1199         return self.driver.refresh_security_group_rules(security_group_id)
1200 
1201     # TODO(alaski): Remove object_compat for RPC version 5.0
1202     @object_compat
1203     @wrap_exception()
1204     def refresh_instance_security_rules(self, context, instance):
1205         """Tell the virtualization driver to refresh security rules for
1206         an instance.
1207 
1208         Passes straight through to the virtualization driver.
1209 
1210         Synchronize the call because we may still be in the middle of
1211         creating the instance.
1212         """
1213         @utils.synchronized(instance.uuid)
1214         def _sync_refresh():
1215             try:
1216                 return self.driver.refresh_instance_security_rules(instance)
1217             except NotImplementedError:
1218                 LOG.debug('Hypervisor driver does not support '
1219                           'security groups.', instance=instance)
1220 
1221         return _sync_refresh()
1222 
1223     def _await_block_device_map_created(self, context, vol_id):
1224         # TODO(yamahata): creating volume simultaneously
1225         #                 reduces creation time?
1226         # TODO(yamahata): eliminate dumb polling
1227         start = time.time()
1228         retries = CONF.block_device_allocate_retries
1229         if retries < 0:
1230             LOG.warning(_LW("Treating negative config value (%(retries)s) for "
1231                             "'block_device_retries' as 0."),
1232                         {'retries': retries})
1233         # (1) treat  negative config value as 0
1234         # (2) the configured value is 0, one attempt should be made
1235         # (3) the configured value is > 0, then the total number attempts
1236         #      is (retries + 1)
1237         attempts = 1
1238         if retries >= 1:
1239             attempts = retries + 1
1240         for attempt in range(1, attempts + 1):
1241             volume = self.volume_api.get(context, vol_id)
1242             volume_status = volume['status']
1243             if volume_status not in ['creating', 'downloading']:
1244                 if volume_status == 'available':
1245                     return attempt
1246                 LOG.warning(_LW("Volume id: %(vol_id)s finished being "
1247                                 "created but its status is %(vol_status)s."),
1248                             {'vol_id': vol_id,
1249                              'vol_status': volume_status})
1250                 break
1251             greenthread.sleep(CONF.block_device_allocate_retries_interval)
1252         raise exception.VolumeNotCreated(volume_id=vol_id,
1253                                          seconds=int(time.time() - start),
1254                                          attempts=attempt,
1255                                          volume_status=volume_status)
1256 
1257     def _decode_files(self, injected_files):
1258         """Base64 decode the list of files to inject."""
1259         if not injected_files:
1260             return []
1261 
1262         def _decode(f):
1263             path, contents = f
1264             # Py3 raises binascii.Error instead of TypeError as in Py27
1265             try:
1266                 decoded = base64.b64decode(contents)
1267                 return path, decoded
1268             except (TypeError, binascii.Error):
1269                 raise exception.Base64Exception(path=path)
1270 
1271         return [_decode(f) for f in injected_files]
1272 
1273     def _validate_instance_group_policy(self, context, instance,
1274             filter_properties):
1275         # NOTE(russellb) Instance group policy is enforced by the scheduler.
1276         # However, there is a race condition with the enforcement of
1277         # the policy.  Since more than one instance may be scheduled at the
1278         # same time, it's possible that more than one instance with an
1279         # anti-affinity policy may end up here.  It's also possible that
1280         # multiple instances with an affinity policy could end up on different
1281         # hosts.  This is a validation step to make sure that starting the
1282         # instance here doesn't violate the policy.
1283 
1284         scheduler_hints = filter_properties.get('scheduler_hints') or {}
1285         group_hint = scheduler_hints.get('group')
1286         if not group_hint:
1287             return
1288 
1289         @utils.synchronized(group_hint)
1290         def _do_validation(context, instance, group_hint):
1291             group = objects.InstanceGroup.get_by_hint(context, group_hint)
1292             if 'anti-affinity' in group.policies:
1293                 group_hosts = group.get_hosts(exclude=[instance.uuid])
1294                 if self.host in group_hosts:
1295                     msg = _("Anti-affinity instance group policy "
1296                             "was violated.")
1297                     raise exception.RescheduledException(
1298                             instance_uuid=instance.uuid,
1299                             reason=msg)
1300             elif 'affinity' in group.policies:
1301                 group_hosts = group.get_hosts(exclude=[instance.uuid])
1302                 if group_hosts and self.host not in group_hosts:
1303                     msg = _("Affinity instance group policy was violated.")
1304                     raise exception.RescheduledException(
1305                             instance_uuid=instance.uuid,
1306                             reason=msg)
1307 
1308         _do_validation(context, instance, group_hint)
1309 
1310     def _log_original_error(self, exc_info, instance_uuid):
1311         LOG.error(_LE('Error: %s'), exc_info[1], instance_uuid=instance_uuid,
1312                   exc_info=exc_info)
1313 
1314     def _reschedule(self, context, request_spec, filter_properties,
1315             instance, reschedule_method, method_args, task_state,
1316             exc_info=None):
1317         """Attempt to re-schedule a compute operation."""
1318 
1319         instance_uuid = instance.uuid
1320         retry = filter_properties.get('retry')
1321         if not retry:
1322             # no retry information, do not reschedule.
1323             LOG.debug("Retry info not present, will not reschedule",
1324                       instance_uuid=instance_uuid)
1325             return
1326 
1327         if not request_spec:
1328             LOG.debug("No request spec, will not reschedule",
1329                       instance_uuid=instance_uuid)
1330             return
1331 
1332         LOG.debug("Re-scheduling %(method)s: attempt %(num)d",
1333                   {'method': reschedule_method.__name__,
1334                    'num': retry['num_attempts']}, instance_uuid=instance_uuid)
1335 
1336         # reset the task state:
1337         self._instance_update(context, instance, task_state=task_state)
1338 
1339         if exc_info:
1340             # stringify to avoid circular ref problem in json serialization:
1341             retry['exc'] = traceback.format_exception_only(exc_info[0],
1342                                     exc_info[1])
1343 
1344         reschedule_method(context, *method_args)
1345         return True
1346 
1347     @periodic_task.periodic_task
1348     def _check_instance_build_time(self, context):
1349         """Ensure that instances are not stuck in build."""
1350         timeout = CONF.instance_build_timeout
1351         if timeout == 0:
1352             return
1353 
1354         filters = {'vm_state': vm_states.BUILDING,
1355                    'host': self.host}
1356 
1357         building_insts = objects.InstanceList.get_by_filters(context,
1358                            filters, expected_attrs=[], use_slave=True)
1359 
1360         for instance in building_insts:
1361             if timeutils.is_older_than(instance.created_at, timeout):
1362                 self._set_instance_obj_error_state(context, instance)
1363                 LOG.warning(_LW("Instance build timed out. Set to error "
1364                                 "state."), instance=instance)
1365 
1366     def _check_instance_exists(self, context, instance):
1367         """Ensure an instance with the same name is not already present."""
1368         if self.driver.instance_exists(instance):
1369             raise exception.InstanceExists(name=instance.name)
1370 
1371     def _allocate_network_async(self, context, instance, requested_networks,
1372                                 macs, security_groups, is_vpn, dhcp_options):
1373         """Method used to allocate networks in the background.
1374 
1375         Broken out for testing.
1376         """
1377         # First check to see if we're specifically not supposed to allocate
1378         # networks because if so, we can exit early.
1379         if requested_networks and requested_networks.no_allocate:
1380             LOG.debug("Not allocating networking since 'none' was specified.",
1381                       instance=instance)
1382             return network_model.NetworkInfo([])
1383 
1384         LOG.debug("Allocating IP information in the background.",
1385                   instance=instance)
1386         retries = CONF.network_allocate_retries
1387         attempts = retries + 1
1388         retry_time = 1
1389         bind_host_id = self.driver.network_binding_host_id(context, instance)
1390         for attempt in range(1, attempts + 1):
1391             try:
1392                 nwinfo = self.network_api.allocate_for_instance(
1393                         context, instance, vpn=is_vpn,
1394                         requested_networks=requested_networks,
1395                         macs=macs,
1396                         security_groups=security_groups,
1397                         dhcp_options=dhcp_options,
1398                         bind_host_id=bind_host_id)
1399                 LOG.debug('Instance network_info: |%s|', nwinfo,
1400                           instance=instance)
1401                 instance.system_metadata['network_allocated'] = 'True'
1402                 # NOTE(JoshNang) do not save the instance here, as it can cause
1403                 # races. The caller shares a reference to instance and waits
1404                 # for this async greenthread to finish before calling
1405                 # instance.save().
1406                 return nwinfo
1407             except Exception:
1408                 exc_info = sys.exc_info()
1409                 log_info = {'attempt': attempt,
1410                             'attempts': attempts}
1411                 if attempt == attempts:
1412                     LOG.exception(_LE('Instance failed network setup '
1413                                       'after %(attempts)d attempt(s)'),
1414                                   log_info)
1415                     six.reraise(*exc_info)
1416                 LOG.warning(_LW('Instance failed network setup '
1417                                 '(attempt %(attempt)d of %(attempts)d)'),
1418                             log_info, instance=instance)
1419                 time.sleep(retry_time)
1420                 retry_time *= 2
1421                 if retry_time > 30:
1422                     retry_time = 30
1423         # Not reached.
1424 
1425     def _build_networks_for_instance(self, context, instance,
1426             requested_networks, security_groups):
1427 
1428         # If we're here from a reschedule the network may already be allocated.
1429         if strutils.bool_from_string(
1430                 instance.system_metadata.get('network_allocated', 'False')):
1431             # NOTE(alex_xu): The network_allocated is True means the network
1432             # resource already allocated at previous scheduling, and the
1433             # network setup is cleanup at previous. After rescheduling, the
1434             # network resource need setup on the new host.
1435             self.network_api.setup_instance_network_on_host(
1436                 context, instance, instance.host)
1437             return self.network_api.get_instance_nw_info(context, instance)
1438 
1439         if not self.is_neutron_security_groups:
1440             security_groups = []
1441 
1442         macs = self.driver.macs_for_instance(instance)
1443         dhcp_options = self.driver.dhcp_options_for_instance(instance)
1444         network_info = self._allocate_network(context, instance,
1445                 requested_networks, macs, security_groups, dhcp_options)
1446 
1447         return network_info
1448 
1449     def _allocate_network(self, context, instance, requested_networks, macs,
1450                           security_groups, dhcp_options):
1451         """Start network allocation asynchronously.  Return an instance
1452         of NetworkInfoAsyncWrapper that can be used to retrieve the
1453         allocated networks when the operation has finished.
1454         """
1455         # NOTE(comstud): Since we're allocating networks asynchronously,
1456         # this task state has little meaning, as we won't be in this
1457         # state for very long.
1458         instance.vm_state = vm_states.BUILDING
1459         instance.task_state = task_states.NETWORKING
1460         instance.save(expected_task_state=[None])
1461         self._update_resource_tracker(context, instance)
1462 
1463         is_vpn = pipelib.is_vpn_image(instance.image_ref)
1464         return network_model.NetworkInfoAsyncWrapper(
1465                 self._allocate_network_async, context, instance,
1466                 requested_networks, macs, security_groups, is_vpn,
1467                 dhcp_options)
1468 
1469     def _default_root_device_name(self, instance, image_meta, root_bdm):
1470         try:
1471             return self.driver.default_root_device_name(instance,
1472                                                         image_meta,
1473                                                         root_bdm)
1474         except NotImplementedError:
1475             return compute_utils.get_next_device_name(instance, [])
1476 
1477     def _default_device_names_for_instance(self, instance,
1478                                            root_device_name,
1479                                            *block_device_lists):
1480         try:
1481             self.driver.default_device_names_for_instance(instance,
1482                                                           root_device_name,
1483                                                           *block_device_lists)
1484         except NotImplementedError:
1485             compute_utils.default_device_names_for_instance(
1486                 instance, root_device_name, *block_device_lists)
1487 
1488     def _get_device_name_for_instance(self, instance, bdms, block_device_obj):
1489         # NOTE(ndipanov): Copy obj to avoid changing the original
1490         block_device_obj = block_device_obj.obj_clone()
1491         try:
1492             return self.driver.get_device_name_for_instance(
1493                 instance, bdms, block_device_obj)
1494         except NotImplementedError:
1495             return compute_utils.get_device_name_for_instance(
1496                 instance, bdms, block_device_obj.get("device_name"))
1497 
1498     def _default_block_device_names(self, instance, image_meta, block_devices):
1499         """Verify that all the devices have the device_name set. If not,
1500         provide a default name.
1501 
1502         It also ensures that there is a root_device_name and is set to the
1503         first block device in the boot sequence (boot_index=0).
1504         """
1505         root_bdm = block_device.get_root_bdm(block_devices)
1506         if not root_bdm:
1507             return
1508 
1509         # Get the root_device_name from the root BDM or the instance
1510         root_device_name = None
1511         update_root_bdm = False
1512 
1513         if root_bdm.device_name:
1514             root_device_name = root_bdm.device_name
1515             instance.root_device_name = root_device_name
1516         elif instance.root_device_name:
1517             root_device_name = instance.root_device_name
1518             root_bdm.device_name = root_device_name
1519             update_root_bdm = True
1520         else:
1521             root_device_name = self._default_root_device_name(instance,
1522                                                               image_meta,
1523                                                               root_bdm)
1524 
1525             instance.root_device_name = root_device_name
1526             root_bdm.device_name = root_device_name
1527             update_root_bdm = True
1528 
1529         if update_root_bdm:
1530             root_bdm.save()
1531 
1532         ephemerals = list(filter(block_device.new_format_is_ephemeral,
1533                             block_devices))
1534         swap = list(filter(block_device.new_format_is_swap,
1535                       block_devices))
1536         block_device_mapping = list(filter(
1537               driver_block_device.is_block_device_mapping, block_devices))
1538 
1539         self._default_device_names_for_instance(instance,
1540                                                 root_device_name,
1541                                                 ephemerals,
1542                                                 swap,
1543                                                 block_device_mapping)
1544 
1545     def _block_device_info_to_legacy(self, block_device_info):
1546         """Convert BDI to the old format for drivers that need it."""
1547 
1548         if self.use_legacy_block_device_info:
1549             ephemerals = driver_block_device.legacy_block_devices(
1550                 driver.block_device_info_get_ephemerals(block_device_info))
1551             mapping = driver_block_device.legacy_block_devices(
1552                 driver.block_device_info_get_mapping(block_device_info))
1553             swap = block_device_info['swap']
1554             if swap:
1555                 swap = swap.legacy()
1556 
1557             block_device_info.update({
1558                 'ephemerals': ephemerals,
1559                 'swap': swap,
1560                 'block_device_mapping': mapping})
1561 
1562     def _add_missing_dev_names(self, bdms, instance):
1563         for bdm in bdms:
1564             if bdm.device_name is not None:
1565                 continue
1566 
1567             device_name = self._get_device_name_for_instance(instance,
1568                                                              bdms, bdm)
1569             values = {'device_name': device_name}
1570             bdm.update(values)
1571             bdm.save()
1572 
1573     def _prep_block_device(self, context, instance, bdms,
1574                            do_check_attach=True):
1575         """Set up the block device for an instance with error logging."""
1576         try:
1577             self._add_missing_dev_names(bdms, instance)
1578             block_device_info = driver.get_block_device_info(instance, bdms)
1579             mapping = driver.block_device_info_get_mapping(block_device_info)
1580             driver_block_device.attach_block_devices(
1581                 mapping, context, instance, self.volume_api, self.driver,
1582                 do_check_attach=do_check_attach,
1583                 wait_func=self._await_block_device_map_created)
1584 
1585             self._block_device_info_to_legacy(block_device_info)
1586             return block_device_info
1587 
1588         except exception.OverQuota:
1589             msg = _LW('Failed to create block device for instance due to '
1590                       'being over volume resource quota')
1591             LOG.warning(msg, instance=instance)
1592             raise exception.VolumeLimitExceeded()
1593 
1594         except Exception:
1595             LOG.exception(_LE('Instance failed block device setup'),
1596                           instance=instance)
1597             raise exception.InvalidBDM()
1598 
1599     def _update_instance_after_spawn(self, context, instance):
1600         instance.power_state = self._get_power_state(context, instance)
1601         instance.vm_state = vm_states.ACTIVE
1602         instance.task_state = None
1603         instance.launched_at = timeutils.utcnow()
1604         configdrive.update_instance(instance)
1605 
1606     def _update_scheduler_instance_info(self, context, instance):
1607         """Sends an InstanceList with created or updated Instance objects to
1608         the Scheduler client.
1609 
1610         In the case of init_host, the value passed will already be an
1611         InstanceList. Other calls will send individual Instance objects that
1612         have been created or resized. In this case, we create an InstanceList
1613         object containing that Instance.
1614         """
1615         if not self.send_instance_updates:
1616             return
1617         if isinstance(instance, obj_instance.Instance):
1618             instance = objects.InstanceList(objects=[instance])
1619         context = context.elevated()
1620         self.scheduler_client.update_instance_info(context, self.host,
1621                                                    instance)
1622 
1623     def _delete_scheduler_instance_info(self, context, instance_uuid):
1624         """Sends the uuid of the deleted Instance to the Scheduler client."""
1625         if not self.send_instance_updates:
1626             return
1627         context = context.elevated()
1628         self.scheduler_client.delete_instance_info(context, self.host,
1629                                                    instance_uuid)
1630 
1631     @periodic_task.periodic_task(spacing=CONF.scheduler_instance_sync_interval)
1632     def _sync_scheduler_instance_info(self, context):
1633         if not self.send_instance_updates:
1634             return
1635         context = context.elevated()
1636         instances = objects.InstanceList.get_by_host(context, self.host,
1637                                                      expected_attrs=[],
1638                                                      use_slave=True)
1639         uuids = [instance.uuid for instance in instances]
1640         self.scheduler_client.sync_instance_info(context, self.host, uuids)
1641 
1642     def _notify_about_instance_usage(self, context, instance, event_suffix,
1643                                      network_info=None, system_metadata=None,
1644                                      extra_usage_info=None, fault=None):
1645         compute_utils.notify_about_instance_usage(
1646             self.notifier, context, instance, event_suffix,
1647             network_info=network_info,
1648             system_metadata=system_metadata,
1649             extra_usage_info=extra_usage_info, fault=fault)
1650 
1651     def _deallocate_network(self, context, instance,
1652                             requested_networks=None):
1653         # If we were told not to allocate networks let's save ourselves
1654         # the trouble of calling the network API.
1655         if requested_networks and requested_networks.no_allocate:
1656             LOG.debug("Skipping network deallocation for instance since "
1657                       "networking was not requested.", instance=instance)
1658             return
1659 
1660         LOG.debug('Deallocating network for instance', instance=instance)
1661         with timeutils.StopWatch() as timer:
1662             self.network_api.deallocate_for_instance(
1663                 context, instance, requested_networks=requested_networks)
1664         # nova-network does an rpc call so we're OK tracking time spent here
1665         LOG.info(_LI('Took %0.2f seconds to deallocate network for instance.'),
1666                  timer.elapsed(), instance=instance)
1667 
1668     def _get_instance_block_device_info(self, context, instance,
1669                                         refresh_conn_info=False,
1670                                         bdms=None):
1671         """Transform block devices to the driver block_device format."""
1672 
1673         if not bdms:
1674             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
1675                     context, instance.uuid)
1676         block_device_info = driver.get_block_device_info(instance, bdms)
1677 
1678         if not refresh_conn_info:
1679             # if the block_device_mapping has no value in connection_info
1680             # (returned as None), don't include in the mapping
1681             block_device_info['block_device_mapping'] = [
1682                 bdm for bdm in driver.block_device_info_get_mapping(
1683                                     block_device_info)
1684                 if bdm.get('connection_info')]
1685         else:
1686             driver_block_device.refresh_conn_infos(
1687                 driver.block_device_info_get_mapping(block_device_info),
1688                 context, instance, self.volume_api, self.driver)
1689 
1690         self._block_device_info_to_legacy(block_device_info)
1691 
1692         return block_device_info
1693 
1694     @wrap_exception()
1695     @reverts_task_state
1696     @wrap_instance_fault
1697     def build_and_run_instance(self, context, instance, image, request_spec,
1698                      filter_properties, admin_password=None,
1699                      injected_files=None, requested_networks=None,
1700                      security_groups=None, block_device_mapping=None,
1701                      node=None, limits=None):
1702 
1703         @utils.synchronized(instance.uuid)
1704         def _locked_do_build_and_run_instance(*args, **kwargs):
1705             # NOTE(danms): We grab the semaphore with the instance uuid
1706             # locked because we could wait in line to build this instance
1707             # for a while and we want to make sure that nothing else tries
1708             # to do anything with this instance while we wait.
1709             with self._build_semaphore:
1710                 self._do_build_and_run_instance(*args, **kwargs)
1711 
1712         # NOTE(danms): We spawn here to return the RPC worker thread back to
1713         # the pool. Since what follows could take a really long time, we don't
1714         # want to tie up RPC workers.
1715         utils.spawn_n(_locked_do_build_and_run_instance,
1716                       context, instance, image, request_spec,
1717                       filter_properties, admin_password, injected_files,
1718                       requested_networks, security_groups,
1719                       block_device_mapping, node, limits)
1720 
1721     def _check_device_tagging(self, requested_networks, block_device_mapping):
1722         tagging_requested = False
1723         if requested_networks:
1724             for net in requested_networks:
1725                 if 'tag' in net and net.tag is not None:
1726                     tagging_requested = True
1727                     break
1728         if block_device_mapping and not tagging_requested:
1729             for bdm in block_device_mapping:
1730                 if 'tag' in bdm and bdm.tag is not None:
1731                     tagging_requested = True
1732                     break
1733         if (tagging_requested and
1734                 not self.driver.capabilities.get('supports_device_tagging')):
1735             raise exception.BuildAbortException('Attempt to boot guest with '
1736                                                 'tagged devices on host that '
1737                                                 'does not support tagging.')
1738 
1739     @hooks.add_hook('build_instance')
1740     @wrap_exception()
1741     @reverts_task_state
1742     @wrap_instance_event(prefix='compute')
1743     @wrap_instance_fault
1744     def _do_build_and_run_instance(self, context, instance, image,
1745             request_spec, filter_properties, admin_password, injected_files,
1746             requested_networks, security_groups, block_device_mapping,
1747             node=None, limits=None):
1748 
1749         try:
1750             LOG.debug('Starting instance...', instance=instance)
1751             instance.vm_state = vm_states.BUILDING
1752             instance.task_state = None
1753             instance.save(expected_task_state=
1754                     (task_states.SCHEDULING, None))
1755         except exception.InstanceNotFound:
1756             msg = 'Instance disappeared before build.'
1757             LOG.debug(msg, instance=instance)
1758             return build_results.FAILED
1759         except exception.UnexpectedTaskStateError as e:
1760             LOG.debug(e.format_message(), instance=instance)
1761             return build_results.FAILED
1762 
1763         # b64 decode the files to inject:
1764         decoded_files = self._decode_files(injected_files)
1765 
1766         if limits is None:
1767             limits = {}
1768 
1769         if node is None:
1770             node = self.driver.get_available_nodes(refresh=True)[0]
1771             LOG.debug('No node specified, defaulting to %s', node,
1772                       instance=instance)
1773 
1774         try:
1775             with timeutils.StopWatch() as timer:
1776                 self._build_and_run_instance(context, instance, image,
1777                         decoded_files, admin_password, requested_networks,
1778                         security_groups, block_device_mapping, node, limits,
1779                         filter_properties)
1780             LOG.info(_LI('Took %0.2f seconds to build instance.'),
1781                      timer.elapsed(), instance=instance)
1782             return build_results.ACTIVE
1783         except exception.RescheduledException as e:
1784             retry = filter_properties.get('retry')
1785             if not retry:
1786                 # no retry information, do not reschedule.
1787                 LOG.debug("Retry info not present, will not reschedule",
1788                     instance=instance)
1789                 self._cleanup_allocated_networks(context, instance,
1790                     requested_networks)
1791                 compute_utils.add_instance_fault_from_exc(context,
1792                         instance, e, sys.exc_info(),
1793                         fault_message=e.kwargs['reason'])
1794                 self._nil_out_instance_obj_host_and_node(instance)
1795                 self._set_instance_obj_error_state(context, instance,
1796                                                    clean_task_state=True)
1797                 return build_results.FAILED
1798             LOG.debug(e.format_message(), instance=instance)
1799             # This will be used for logging the exception
1800             retry['exc'] = traceback.format_exception(*sys.exc_info())
1801             # This will be used for setting the instance fault message
1802             retry['exc_reason'] = e.kwargs['reason']
1803             # NOTE(comstud): Deallocate networks if the driver wants
1804             # us to do so.
1805             # NOTE(vladikr): SR-IOV ports should be deallocated to
1806             # allow new sriov pci devices to be allocated on a new host.
1807             # Otherwise, if devices with pci addresses are already allocated
1808             # on the destination host, the instance will fail to spawn.
1809             # info_cache.network_info should be present at this stage.
1810             if (self.driver.deallocate_networks_on_reschedule(instance) or
1811                 self.deallocate_sriov_ports_on_reschedule(instance)):
1812                 self._cleanup_allocated_networks(context, instance,
1813                         requested_networks)
1814             else:
1815                 # NOTE(alex_xu): Network already allocated and we don't
1816                 # want to deallocate them before rescheduling. But we need
1817                 # to cleanup those network resources setup on this host before
1818                 # rescheduling.
1819                 self.network_api.cleanup_instance_network_on_host(
1820                     context, instance, self.host)
1821 
1822             self._nil_out_instance_obj_host_and_node(instance)
1823             instance.task_state = task_states.SCHEDULING
1824             instance.save()
1825 
1826             self.compute_task_api.build_instances(context, [instance],
1827                     image, filter_properties, admin_password,
1828                     injected_files, requested_networks, security_groups,
1829                     block_device_mapping)
1830             return build_results.RESCHEDULED
1831         except (exception.InstanceNotFound,
1832                 exception.UnexpectedDeletingTaskStateError):
1833             msg = 'Instance disappeared during build.'
1834             LOG.debug(msg, instance=instance)
1835             self._cleanup_allocated_networks(context, instance,
1836                     requested_networks)
1837             return build_results.FAILED
1838         except exception.BuildAbortException as e:
1839             LOG.exception(e.format_message(), instance=instance)
1840             self._cleanup_allocated_networks(context, instance,
1841                     requested_networks)
1842             self._cleanup_volumes(context, instance.uuid,
1843                     block_device_mapping, raise_exc=False)
1844             compute_utils.add_instance_fault_from_exc(context, instance,
1845                     e, sys.exc_info())
1846             self._nil_out_instance_obj_host_and_node(instance)
1847             self._set_instance_obj_error_state(context, instance,
1848                                                clean_task_state=True)
1849             return build_results.FAILED
1850         except Exception as e:
1851             # Should not reach here.
1852             msg = _LE('Unexpected build failure, not rescheduling build.')
1853             LOG.exception(msg, instance=instance)
1854             self._cleanup_allocated_networks(context, instance,
1855                     requested_networks)
1856             self._cleanup_volumes(context, instance.uuid,
1857                     block_device_mapping, raise_exc=False)
1858             compute_utils.add_instance_fault_from_exc(context, instance,
1859                     e, sys.exc_info())
1860             self._nil_out_instance_obj_host_and_node(instance)
1861             self._set_instance_obj_error_state(context, instance,
1862                                                clean_task_state=True)
1863             return build_results.FAILED
1864 
1865     def deallocate_sriov_ports_on_reschedule(self, instance):
1866         """Determine if networks are needed to be deallocated before reschedule
1867 
1868         Check the cached network info for any assigned SR-IOV ports.
1869         SR-IOV ports should be deallocated prior to rescheduling
1870         in order to allow new sriov pci devices to be allocated on a new host.
1871         """
1872         info_cache = instance.info_cache
1873 
1874         def _has_sriov_port(vif):
1875             return vif['vnic_type'] in network_model.VNIC_TYPES_SRIOV
1876 
1877         if (info_cache and info_cache.network_info):
1878             for vif in info_cache.network_info:
1879                 if _has_sriov_port(vif):
1880                     return True
1881         return False
1882 
1883     def _build_and_run_instance(self, context, instance, image, injected_files,
1884             admin_password, requested_networks, security_groups,
1885             block_device_mapping, node, limits, filter_properties):
1886 
1887         image_name = image.get('name')
1888         self._notify_about_instance_usage(context, instance, 'create.start',
1889                 extra_usage_info={'image_name': image_name})
1890 
1891         self._check_device_tagging(requested_networks, block_device_mapping)
1892 
1893         try:
1894             rt = self._get_resource_tracker(node)
1895             with rt.instance_claim(context, instance, limits):
1896                 # NOTE(russellb) It's important that this validation be done
1897                 # *after* the resource tracker instance claim, as that is where
1898                 # the host is set on the instance.
1899                 self._validate_instance_group_policy(context, instance,
1900                         filter_properties)
1901                 image_meta = objects.ImageMeta.from_dict(image)
1902                 with self._build_resources(context, instance,
1903                         requested_networks, security_groups, image_meta,
1904                         block_device_mapping) as resources:
1905                     instance.vm_state = vm_states.BUILDING
1906                     instance.task_state = task_states.SPAWNING
1907                     # NOTE(JoshNang) This also saves the changes to the
1908                     # instance from _allocate_network_async, as they aren't
1909                     # saved in that function to prevent races.
1910                     instance.save(expected_task_state=
1911                             task_states.BLOCK_DEVICE_MAPPING)
1912                     block_device_info = resources['block_device_info']
1913                     network_info = resources['network_info']
1914                     LOG.debug('Start spawning the instance on the hypervisor.',
1915                               instance=instance)
1916                     with timeutils.StopWatch() as timer:
1917                         self.driver.spawn(context, instance, image_meta,
1918                                           injected_files, admin_password,
1919                                           network_info=network_info,
1920                                           block_device_info=block_device_info)
1921                     LOG.info(_LI('Took %0.2f seconds to spawn the instance on '
1922                                  'the hypervisor.'), timer.elapsed(),
1923                              instance=instance)
1924         except (exception.InstanceNotFound,
1925                 exception.UnexpectedDeletingTaskStateError) as e:
1926             with excutils.save_and_reraise_exception():
1927                 self._notify_about_instance_usage(context, instance,
1928                     'create.error', fault=e)
1929         except exception.ComputeResourcesUnavailable as e:
1930             LOG.debug(e.format_message(), instance=instance)
1931             self._notify_about_instance_usage(context, instance,
1932                     'create.error', fault=e)
1933             raise exception.RescheduledException(
1934                     instance_uuid=instance.uuid, reason=e.format_message())
1935         except exception.BuildAbortException as e:
1936             with excutils.save_and_reraise_exception():
1937                 LOG.debug(e.format_message(), instance=instance)
1938                 self._notify_about_instance_usage(context, instance,
1939                     'create.error', fault=e)
1940         except (exception.FixedIpLimitExceeded,
1941                 exception.NoMoreNetworks, exception.NoMoreFixedIps) as e:
1942             LOG.warning(_LW('No more network or fixed IP to be allocated'),
1943                         instance=instance)
1944             self._notify_about_instance_usage(context, instance,
1945                     'create.error', fault=e)
1946             msg = _('Failed to allocate the network(s) with error %s, '
1947                     'not rescheduling.') % e.format_message()
1948             raise exception.BuildAbortException(instance_uuid=instance.uuid,
1949                     reason=msg)
1950         except (exception.VirtualInterfaceCreateException,
1951                 exception.VirtualInterfaceMacAddressException,
1952                 exception.FixedIpInvalidOnHost,
1953                 exception.UnableToAutoAllocateNetwork) as e:
1954             LOG.exception(_LE('Failed to allocate network(s)'),
1955                           instance=instance)
1956             self._notify_about_instance_usage(context, instance,
1957                     'create.error', fault=e)
1958             msg = _('Failed to allocate the network(s), not rescheduling.')
1959             raise exception.BuildAbortException(instance_uuid=instance.uuid,
1960                     reason=msg)
1961         except (exception.FlavorDiskTooSmall,
1962                 exception.FlavorMemoryTooSmall,
1963                 exception.ImageNotActive,
1964                 exception.ImageUnacceptable,
1965                 exception.InvalidDiskInfo,
1966                 exception.InvalidDiskFormat,
1967                 exception.SignatureVerificationError) as e:
1968             self._notify_about_instance_usage(context, instance,
1969                     'create.error', fault=e)
1970             raise exception.BuildAbortException(instance_uuid=instance.uuid,
1971                     reason=e.format_message())
1972         except Exception as e:
1973             self._notify_about_instance_usage(context, instance,
1974                     'create.error', fault=e)
1975             raise exception.RescheduledException(
1976                     instance_uuid=instance.uuid, reason=six.text_type(e))
1977 
1978         # NOTE(alaski): This is only useful during reschedules, remove it now.
1979         instance.system_metadata.pop('network_allocated', None)
1980 
1981         # If CONF.default_access_ip_network_name is set, grab the
1982         # corresponding network and set the access ip values accordingly.
1983         network_name = CONF.default_access_ip_network_name
1984         if (network_name and not instance.access_ip_v4 and
1985                 not instance.access_ip_v6):
1986             # Note that when there are multiple ips to choose from, an
1987             # arbitrary one will be chosen.
1988             for vif in network_info:
1989                 if vif['network']['label'] == network_name:
1990                     for ip in vif.fixed_ips():
1991                         if not instance.access_ip_v4 and ip['version'] == 4:
1992                             instance.access_ip_v4 = ip['address']
1993                         if not instance.access_ip_v6 and ip['version'] == 6:
1994                             instance.access_ip_v6 = ip['address']
1995                     break
1996 
1997         self._update_instance_after_spawn(context, instance)
1998 
1999         try:
2000             instance.save(expected_task_state=task_states.SPAWNING)
2001         except (exception.InstanceNotFound,
2002                 exception.UnexpectedDeletingTaskStateError) as e:
2003             with excutils.save_and_reraise_exception():
2004                 self._notify_about_instance_usage(context, instance,
2005                     'create.error', fault=e)
2006 
2007         self._update_scheduler_instance_info(context, instance)
2008         self._notify_about_instance_usage(context, instance, 'create.end',
2009                 extra_usage_info={'message': _('Success')},
2010                 network_info=network_info)
2011 
2012     @contextlib.contextmanager
2013     def _build_resources(self, context, instance, requested_networks,
2014                          security_groups, image_meta, block_device_mapping):
2015         resources = {}
2016         network_info = None
2017         try:
2018             LOG.debug('Start building networks asynchronously for instance.',
2019                       instance=instance)
2020             network_info = self._build_networks_for_instance(context, instance,
2021                     requested_networks, security_groups)
2022             resources['network_info'] = network_info
2023         except (exception.InstanceNotFound,
2024                 exception.UnexpectedDeletingTaskStateError):
2025             raise
2026         except exception.UnexpectedTaskStateError as e:
2027             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2028                     reason=e.format_message())
2029         except Exception:
2030             # Because this allocation is async any failures are likely to occur
2031             # when the driver accesses network_info during spawn().
2032             LOG.exception(_LE('Failed to allocate network(s)'),
2033                           instance=instance)
2034             msg = _('Failed to allocate the network(s), not rescheduling.')
2035             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2036                     reason=msg)
2037 
2038         try:
2039             # Verify that all the BDMs have a device_name set and assign a
2040             # default to the ones missing it with the help of the driver.
2041             self._default_block_device_names(instance, image_meta,
2042                                              block_device_mapping)
2043 
2044             LOG.debug('Start building block device mappings for instance.',
2045                       instance=instance)
2046             instance.vm_state = vm_states.BUILDING
2047             instance.task_state = task_states.BLOCK_DEVICE_MAPPING
2048             instance.save()
2049 
2050             block_device_info = self._prep_block_device(context, instance,
2051                     block_device_mapping)
2052             resources['block_device_info'] = block_device_info
2053         except (exception.InstanceNotFound,
2054                 exception.UnexpectedDeletingTaskStateError):
2055             with excutils.save_and_reraise_exception():
2056                 # Make sure the async call finishes
2057                 if network_info is not None:
2058                     network_info.wait(do_raise=False)
2059         except (exception.UnexpectedTaskStateError,
2060                 exception.VolumeLimitExceeded,
2061                 exception.InvalidBDM) as e:
2062             # Make sure the async call finishes
2063             if network_info is not None:
2064                 network_info.wait(do_raise=False)
2065             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2066                     reason=e.format_message())
2067         except Exception:
2068             LOG.exception(_LE('Failure prepping block device'),
2069                     instance=instance)
2070             # Make sure the async call finishes
2071             if network_info is not None:
2072                 network_info.wait(do_raise=False)
2073             msg = _('Failure prepping block device.')
2074             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2075                     reason=msg)
2076 
2077         try:
2078             yield resources
2079         except Exception as exc:
2080             with excutils.save_and_reraise_exception() as ctxt:
2081                 if not isinstance(exc, (
2082                         exception.InstanceNotFound,
2083                         exception.UnexpectedDeletingTaskStateError)):
2084                     LOG.exception(_LE('Instance failed to spawn'),
2085                                   instance=instance)
2086                 # Make sure the async call finishes
2087                 if network_info is not None:
2088                     network_info.wait(do_raise=False)
2089                 # if network_info is empty we're likely here because of
2090                 # network allocation failure. Since nothing can be reused on
2091                 # rescheduling it's better to deallocate network to eliminate
2092                 # the chance of orphaned ports in neutron
2093                 deallocate_networks = False if network_info else True
2094                 try:
2095                     self._shutdown_instance(context, instance,
2096                             block_device_mapping, requested_networks,
2097                             try_deallocate_networks=deallocate_networks)
2098                 except Exception as exc2:
2099                     ctxt.reraise = False
2100                     LOG.warning(_LW('Could not clean up failed build,'
2101                                     ' not rescheduling. Error: %s'),
2102                                 six.text_type(exc2))
2103                     raise exception.BuildAbortException(
2104                             instance_uuid=instance.uuid,
2105                             reason=six.text_type(exc))
2106 
2107     def _cleanup_allocated_networks(self, context, instance,
2108             requested_networks):
2109         try:
2110             self._deallocate_network(context, instance, requested_networks)
2111         except Exception:
2112             msg = _LE('Failed to deallocate networks')
2113             LOG.exception(msg, instance=instance)
2114             return
2115 
2116         instance.system_metadata['network_allocated'] = 'False'
2117         try:
2118             instance.save()
2119         except exception.InstanceNotFound:
2120             # NOTE(alaski): It's possible that we're cleaning up the networks
2121             # because the instance was deleted.  If that's the case then this
2122             # exception will be raised by instance.save()
2123             pass
2124 
2125     def _try_deallocate_network(self, context, instance,
2126                                 requested_networks=None):
2127         try:
2128             # tear down allocated network structure
2129             self._deallocate_network(context, instance, requested_networks)
2130         except Exception as ex:
2131             with excutils.save_and_reraise_exception():
2132                 LOG.error(_LE('Failed to deallocate network for instance. '
2133                               'Error: %s'), ex,
2134                           instance=instance)
2135                 self._set_instance_obj_error_state(context, instance)
2136 
2137     def _get_power_off_values(self, context, instance, clean_shutdown):
2138         """Get the timing configuration for powering down this instance."""
2139         if clean_shutdown:
2140             timeout = compute_utils.get_value_from_system_metadata(instance,
2141                           key='image_os_shutdown_timeout', type=int,
2142                           default=CONF.shutdown_timeout)
2143             retry_interval = self.SHUTDOWN_RETRY_INTERVAL
2144         else:
2145             timeout = 0
2146             retry_interval = 0
2147 
2148         return timeout, retry_interval
2149 
2150     def _power_off_instance(self, context, instance, clean_shutdown=True):
2151         """Power off an instance on this host."""
2152         timeout, retry_interval = self._get_power_off_values(context,
2153                                         instance, clean_shutdown)
2154         self.driver.power_off(instance, timeout, retry_interval)
2155 
2156     def _shutdown_instance(self, context, instance,
2157                            bdms, requested_networks=None, notify=True,
2158                            try_deallocate_networks=True):
2159         """Shutdown an instance on this host.
2160 
2161         :param:context: security context
2162         :param:instance: a nova.objects.Instance object
2163         :param:bdms: the block devices for the instance to be torn
2164                      down
2165         :param:requested_networks: the networks on which the instance
2166                                    has ports
2167         :param:notify: true if a final usage notification should be
2168                        emitted
2169         :param:try_deallocate_networks: false if we should avoid
2170                                         trying to teardown networking
2171         """
2172         context = context.elevated()
2173         LOG.info(_LI('Terminating instance'), instance=instance)
2174 
2175         if notify:
2176             self._notify_about_instance_usage(context, instance,
2177                                               "shutdown.start")
2178 
2179         network_info = compute_utils.get_nw_info_for_instance(instance)
2180 
2181         # NOTE(vish) get bdms before destroying the instance
2182         vol_bdms = [bdm for bdm in bdms if bdm.is_volume]
2183         block_device_info = self._get_instance_block_device_info(
2184             context, instance, bdms=bdms)
2185 
2186         # NOTE(melwitt): attempt driver destroy before releasing ip, may
2187         #                want to keep ip allocated for certain failures
2188         timer = timeutils.StopWatch()
2189         try:
2190             LOG.debug('Start destroying the instance on the hypervisor.',
2191                       instance=instance)
2192             timer.start()
2193             self.driver.destroy(context, instance, network_info,
2194                     block_device_info)
2195             LOG.info(_LI('Took %0.2f seconds to destroy the instance on the '
2196                          'hypervisor.'), timer.elapsed(), instance=instance)
2197         except exception.InstancePowerOffFailure:
2198             # if the instance can't power off, don't release the ip
2199             with excutils.save_and_reraise_exception():
2200                 pass
2201         except Exception:
2202             with excutils.save_and_reraise_exception():
2203                 # deallocate ip and fail without proceeding to
2204                 # volume api calls, preserving current behavior
2205                 if try_deallocate_networks:
2206                     self._try_deallocate_network(context, instance,
2207                                                  requested_networks)
2208 
2209         if try_deallocate_networks:
2210             self._try_deallocate_network(context, instance, requested_networks)
2211 
2212         timer.restart()
2213         for bdm in vol_bdms:
2214             try:
2215                 # NOTE(vish): actual driver detach done in driver.destroy, so
2216                 #             just tell cinder that we are done with it.
2217                 connector = self.driver.get_volume_connector(instance)
2218                 self.volume_api.terminate_connection(context,
2219                                                      bdm.volume_id,
2220                                                      connector)
2221                 self.volume_api.detach(context, bdm.volume_id, instance.uuid)
2222             except exception.DiskNotFound as exc:
2223                 LOG.debug('Ignoring DiskNotFound: %s', exc,
2224                           instance=instance)
2225             except exception.VolumeNotFound as exc:
2226                 LOG.debug('Ignoring VolumeNotFound: %s', exc,
2227                           instance=instance)
2228             except (cinder_exception.EndpointNotFound,
2229                     keystone_exception.EndpointNotFound) as exc:
2230                 LOG.warning(_LW('Ignoring EndpointNotFound for '
2231                                 'volume %(volume_id)s: %(exc)s'),
2232                             {'exc': exc, 'volume_id': bdm.volume_id},
2233                             instance=instance)
2234             except cinder_exception.ClientException as exc:
2235                 LOG.warning(_LW('Ignoring unknown cinder exception for '
2236                                 'volume %(volume_id)s: %(exc)s'),
2237                             {'exc': exc, 'volume_id': bdm.volume_id},
2238                             instance=instance)
2239             except Exception as exc:
2240                 LOG.warning(_LW('Ignoring unknown exception for '
2241                                 'volume %(volume_id)s: %(exc)s'),
2242                             {'exc': exc, 'volume_id': bdm.volume_id},
2243                             instance=instance)
2244         if vol_bdms:
2245             LOG.info(_LI('Took %(time).2f seconds to detach %(num)s volumes '
2246                          'for instance.'),
2247                      {'time': timer.elapsed(), 'num': len(vol_bdms)},
2248                      instance=instance)
2249 
2250         if notify:
2251             self._notify_about_instance_usage(context, instance,
2252                                               "shutdown.end")
2253 
2254     def _cleanup_volumes(self, context, instance_uuid, bdms, raise_exc=True):
2255         exc_info = None
2256 
2257         for bdm in bdms:
2258             LOG.debug("terminating bdm %s", bdm,
2259                       instance_uuid=instance_uuid)
2260             if bdm.volume_id and bdm.delete_on_termination:
2261                 try:
2262                     self.volume_api.delete(context, bdm.volume_id)
2263                 except Exception as exc:
2264                     exc_info = sys.exc_info()
2265                     LOG.warning(_LW('Failed to delete volume: %(volume_id)s '
2266                                     'due to %(exc)s'),
2267                                 {'volume_id': bdm.volume_id, 'exc': exc})
2268         if exc_info is not None and raise_exc:
2269             six.reraise(exc_info[0], exc_info[1], exc_info[2])
2270 
2271     @hooks.add_hook("delete_instance")
2272     def _delete_instance(self, context, instance, bdms, quotas):
2273         """Delete an instance on this host.  Commit or rollback quotas
2274         as necessary.
2275 
2276         :param context: nova request context
2277         :param instance: nova.objects.instance.Instance object
2278         :param bdms: nova.objects.block_device.BlockDeviceMappingList object
2279         :param quotas: nova.objects.quotas.Quotas object
2280         """
2281         was_soft_deleted = instance.vm_state == vm_states.SOFT_DELETED
2282         if was_soft_deleted:
2283             # Instances in SOFT_DELETED vm_state have already had quotas
2284             # decremented.
2285             try:
2286                 quotas.rollback()
2287             except Exception:
2288                 pass
2289 
2290         try:
2291             events = self.instance_events.clear_events_for_instance(instance)
2292             if events:
2293                 LOG.debug('Events pending at deletion: %(events)s',
2294                           {'events': ','.join(events.keys())},
2295                           instance=instance)
2296             self._notify_about_instance_usage(context, instance,
2297                                               "delete.start")
2298             compute_utils.notify_about_instance_action(context, instance,
2299                     self.host, action=fields.NotificationAction.DELETE,
2300                     phase=fields.NotificationPhase.START)
2301 
2302             self._shutdown_instance(context, instance, bdms)
2303             # NOTE(dims): instance.info_cache.delete() should be called after
2304             # _shutdown_instance in the compute manager as shutdown calls
2305             # deallocate_for_instance so the info_cache is still needed
2306             # at this point.
2307             if instance.info_cache is not None:
2308                 instance.info_cache.delete()
2309             else:
2310                 # NOTE(yoshimatsu): Avoid AttributeError if instance.info_cache
2311                 # is None. When the root cause that instance.info_cache becomes
2312                 # None is fixed, the log level should be reconsidered.
2313                 LOG.warning(_LW("Info cache for instance could not be found. "
2314                                 "Ignore."), instance=instance)
2315 
2316             # NOTE(vish): We have already deleted the instance, so we have
2317             #             to ignore problems cleaning up the volumes. It
2318             #             would be nice to let the user know somehow that
2319             #             the volume deletion failed, but it is not
2320             #             acceptable to have an instance that can not be
2321             #             deleted. Perhaps this could be reworked in the
2322             #             future to set an instance fault the first time
2323             #             and to only ignore the failure if the instance
2324             #             is already in ERROR.
2325             self._cleanup_volumes(context, instance.uuid, bdms,
2326                     raise_exc=False)
2327             # if a delete task succeeded, always update vm state and task
2328             # state without expecting task state to be DELETING
2329             instance.vm_state = vm_states.DELETED
2330             instance.task_state = None
2331             instance.power_state = power_state.NOSTATE
2332             instance.terminated_at = timeutils.utcnow()
2333             instance.save()
2334             system_meta = instance.system_metadata
2335             instance.destroy()
2336         except Exception:
2337             with excutils.save_and_reraise_exception():
2338                 quotas.rollback()
2339 
2340         self._complete_deletion(context,
2341                                 instance,
2342                                 bdms,
2343                                 quotas,
2344                                 system_meta)
2345 
2346     @wrap_exception()
2347     @reverts_task_state
2348     @wrap_instance_event(prefix='compute')
2349     @wrap_instance_fault
2350     def terminate_instance(self, context, instance, bdms, reservations):
2351         """Terminate an instance on this host."""
2352         quotas = objects.Quotas.from_reservations(context,
2353                                                   reservations,
2354                                                   instance=instance)
2355 
2356         @utils.synchronized(instance.uuid)
2357         def do_terminate_instance(instance, bdms):
2358             # NOTE(mriedem): If we are deleting the instance while it was
2359             # booting from volume, we could be racing with a database update of
2360             # the BDM volume_id. Since the compute API passes the BDMs over RPC
2361             # to compute here, the BDMs may be stale at this point. So check
2362             # for any volume BDMs that don't have volume_id set and if we
2363             # detect that, we need to refresh the BDM list before proceeding.
2364             # TODO(mriedem): Move this into _delete_instance and make the bdms
2365             # parameter optional.
2366             for bdm in list(bdms):
2367                 if bdm.is_volume and not bdm.volume_id:
2368                     LOG.debug('There are potentially stale BDMs during '
2369                               'delete, refreshing the BlockDeviceMappingList.',
2370                               instance=instance)
2371                     bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
2372                         context, instance.uuid)
2373                     break
2374             try:
2375                 self._delete_instance(context, instance, bdms, quotas)
2376             except exception.InstanceNotFound:
2377                 LOG.info(_LI("Instance disappeared during terminate"),
2378                          instance=instance)
2379             except Exception:
2380                 # As we're trying to delete always go to Error if something
2381                 # goes wrong that _delete_instance can't handle.
2382                 with excutils.save_and_reraise_exception():
2383                     LOG.exception(_LE('Setting instance vm_state to ERROR'),
2384                                   instance=instance)
2385                     self._set_instance_obj_error_state(context, instance)
2386 
2387         do_terminate_instance(instance, bdms)
2388 
2389     # NOTE(johannes): This is probably better named power_off_instance
2390     # so it matches the driver method, but because of other issues, we
2391     # can't use that name in grizzly.
2392     @wrap_exception()
2393     @reverts_task_state
2394     @wrap_instance_event(prefix='compute')
2395     @wrap_instance_fault
2396     def stop_instance(self, context, instance, clean_shutdown):
2397         """Stopping an instance on this host."""
2398 
2399         @utils.synchronized(instance.uuid)
2400         def do_stop_instance():
2401             current_power_state = self._get_power_state(context, instance)
2402             LOG.debug('Stopping instance; current vm_state: %(vm_state)s, '
2403                       'current task_state: %(task_state)s, current DB '
2404                       'power_state: %(db_power_state)s, current VM '
2405                       'power_state: %(current_power_state)s',
2406                       {'vm_state': instance.vm_state,
2407                        'task_state': instance.task_state,
2408                        'db_power_state': instance.power_state,
2409                        'current_power_state': current_power_state},
2410                       instance_uuid=instance.uuid)
2411 
2412             # NOTE(mriedem): If the instance is already powered off, we are
2413             # possibly tearing down and racing with other operations, so we can
2414             # expect the task_state to be None if something else updates the
2415             # instance and we're not locking it.
2416             expected_task_state = [task_states.POWERING_OFF]
2417             # The list of power states is from _sync_instance_power_state.
2418             if current_power_state in (power_state.NOSTATE,
2419                                        power_state.SHUTDOWN,
2420                                        power_state.CRASHED):
2421                 LOG.info(_LI('Instance is already powered off in the '
2422                              'hypervisor when stop is called.'),
2423                          instance=instance)
2424                 expected_task_state.append(None)
2425 
2426             self._notify_about_instance_usage(context, instance,
2427                                               "power_off.start")
2428             self._power_off_instance(context, instance, clean_shutdown)
2429             instance.power_state = self._get_power_state(context, instance)
2430             instance.vm_state = vm_states.STOPPED
2431             instance.task_state = None
2432             instance.save(expected_task_state=expected_task_state)
2433             self._notify_about_instance_usage(context, instance,
2434                                               "power_off.end")
2435 
2436         do_stop_instance()
2437 
2438     def _power_on(self, context, instance):
2439         network_info = self.network_api.get_instance_nw_info(context, instance)
2440         block_device_info = self._get_instance_block_device_info(context,
2441                                                                  instance)
2442         self.driver.power_on(context, instance,
2443                              network_info,
2444                              block_device_info)
2445 
2446     def _delete_snapshot_of_shelved_instance(self, context, instance,
2447                                              snapshot_id):
2448         """Delete snapshot of shelved instance."""
2449         try:
2450             self.image_api.delete(context, snapshot_id)
2451         except (exception.ImageNotFound,
2452                 exception.ImageNotAuthorized) as exc:
2453             LOG.warning(_LW("Failed to delete snapshot "
2454                             "from shelved instance (%s)."),
2455                         exc.format_message(), instance=instance)
2456         except Exception:
2457             LOG.exception(_LE("Something wrong happened when trying to "
2458                               "delete snapshot from shelved instance."),
2459                           instance=instance)
2460 
2461     # NOTE(johannes): This is probably better named power_on_instance
2462     # so it matches the driver method, but because of other issues, we
2463     # can't use that name in grizzly.
2464     @wrap_exception()
2465     @reverts_task_state
2466     @wrap_instance_event(prefix='compute')
2467     @wrap_instance_fault
2468     def start_instance(self, context, instance):
2469         """Starting an instance on this host."""
2470         self._notify_about_instance_usage(context, instance, "power_on.start")
2471         compute_utils.notify_about_instance_action(context, instance,
2472             self.host, action=fields.NotificationAction.POWER_ON,
2473             phase=fields.NotificationPhase.START)
2474         self._power_on(context, instance)
2475         instance.power_state = self._get_power_state(context, instance)
2476         instance.vm_state = vm_states.ACTIVE
2477         instance.task_state = None
2478 
2479         # Delete an image(VM snapshot) for a shelved instance
2480         snapshot_id = instance.system_metadata.get('shelved_image_id')
2481         if snapshot_id:
2482             self._delete_snapshot_of_shelved_instance(context, instance,
2483                                                       snapshot_id)
2484 
2485         # Delete system_metadata for a shelved instance
2486         compute_utils.remove_shelved_keys_from_system_metadata(instance)
2487 
2488         instance.save(expected_task_state=task_states.POWERING_ON)
2489         self._notify_about_instance_usage(context, instance, "power_on.end")
2490         compute_utils.notify_about_instance_action(context, instance,
2491             self.host, action=fields.NotificationAction.POWER_ON,
2492             phase=fields.NotificationPhase.END)
2493 
2494     @messaging.expected_exceptions(NotImplementedError,
2495                                    exception.TriggerCrashDumpNotSupported,
2496                                    exception.InstanceNotRunning)
2497     @wrap_exception()
2498     @wrap_instance_event(prefix='compute')
2499     @wrap_instance_fault
2500     def trigger_crash_dump(self, context, instance):
2501         """Trigger crash dump in an instance."""
2502 
2503         self._notify_about_instance_usage(context, instance,
2504                                           "trigger_crash_dump.start")
2505 
2506         # This method does not change task_state and power_state because the
2507         # effect of a trigger depends on user's configuration.
2508         self.driver.trigger_crash_dump(instance)
2509 
2510         self._notify_about_instance_usage(context, instance,
2511                                           "trigger_crash_dump.end")
2512 
2513     @wrap_exception()
2514     @reverts_task_state
2515     @wrap_instance_event(prefix='compute')
2516     @wrap_instance_fault
2517     def soft_delete_instance(self, context, instance, reservations):
2518         """Soft delete an instance on this host."""
2519 
2520         quotas = objects.Quotas.from_reservations(context,
2521                                                   reservations,
2522                                                   instance=instance)
2523         try:
2524             self._notify_about_instance_usage(context, instance,
2525                                               "soft_delete.start")
2526             try:
2527                 self.driver.soft_delete(instance)
2528             except NotImplementedError:
2529                 # Fallback to just powering off the instance if the
2530                 # hypervisor doesn't implement the soft_delete method
2531                 self.driver.power_off(instance)
2532             instance.power_state = self._get_power_state(context, instance)
2533             instance.vm_state = vm_states.SOFT_DELETED
2534             instance.task_state = None
2535             instance.save(expected_task_state=[task_states.SOFT_DELETING])
2536         except Exception:
2537             with excutils.save_and_reraise_exception():
2538                 quotas.rollback()
2539         quotas.commit()
2540         self._notify_about_instance_usage(context, instance, "soft_delete.end")
2541 
2542     @wrap_exception()
2543     @reverts_task_state
2544     @wrap_instance_event(prefix='compute')
2545     @wrap_instance_fault
2546     def restore_instance(self, context, instance):
2547         """Restore a soft-deleted instance on this host."""
2548         self._notify_about_instance_usage(context, instance, "restore.start")
2549         compute_utils.notify_about_instance_action(context, instance,
2550             self.host, action=fields.NotificationAction.RESTORE,
2551             phase=fields.NotificationPhase.START)
2552         try:
2553             self.driver.restore(instance)
2554         except NotImplementedError:
2555             # Fallback to just powering on the instance if the hypervisor
2556             # doesn't implement the restore method
2557             self._power_on(context, instance)
2558         instance.power_state = self._get_power_state(context, instance)
2559         instance.vm_state = vm_states.ACTIVE
2560         instance.task_state = None
2561         instance.save(expected_task_state=task_states.RESTORING)
2562         self._notify_about_instance_usage(context, instance, "restore.end")
2563         compute_utils.notify_about_instance_action(context, instance,
2564             self.host, action=fields.NotificationAction.RESTORE,
2565             phase=fields.NotificationPhase.END)
2566 
2567     @staticmethod
2568     def _set_migration_status(migration, status):
2569         """Set the status, and guard against a None being passed in.
2570 
2571         This is useful as some of the compute RPC calls will not pass
2572         a migration object in older versions. The check can be removed when
2573         we move past 4.x major version of the RPC API.
2574         """
2575         if migration:
2576             migration.status = status
2577             migration.save()
2578 
2579     def _rebuild_default_impl(self, context, instance, image_meta,
2580                               injected_files, admin_password, bdms,
2581                               detach_block_devices, attach_block_devices,
2582                               network_info=None,
2583                               recreate=False, block_device_info=None,
2584                               preserve_ephemeral=False):
2585         if preserve_ephemeral:
2586             # The default code path does not support preserving ephemeral
2587             # partitions.
2588             raise exception.PreserveEphemeralNotSupported()
2589 
2590         if recreate:
2591             detach_block_devices(context, bdms)
2592         else:
2593             self._power_off_instance(context, instance, clean_shutdown=True)
2594             detach_block_devices(context, bdms)
2595             self.driver.destroy(context, instance,
2596                                 network_info=network_info,
2597                                 block_device_info=block_device_info)
2598 
2599         instance.task_state = task_states.REBUILD_BLOCK_DEVICE_MAPPING
2600         instance.save(expected_task_state=[task_states.REBUILDING])
2601 
2602         new_block_device_info = attach_block_devices(context, instance, bdms)
2603 
2604         instance.task_state = task_states.REBUILD_SPAWNING
2605         instance.save(
2606             expected_task_state=[task_states.REBUILD_BLOCK_DEVICE_MAPPING])
2607 
2608         with instance.mutated_migration_context():
2609             self.driver.spawn(context, instance, image_meta, injected_files,
2610                               admin_password, network_info=network_info,
2611                               block_device_info=new_block_device_info)
2612 
2613     @messaging.expected_exceptions(exception.PreserveEphemeralNotSupported)
2614     @wrap_exception()
2615     @reverts_task_state
2616     @wrap_instance_event(prefix='compute')
2617     @wrap_instance_fault
2618     def rebuild_instance(self, context, instance, orig_image_ref, image_ref,
2619                          injected_files, new_pass, orig_sys_metadata,
2620                          bdms, recreate, on_shared_storage=None,
2621                          preserve_ephemeral=False, migration=None,
2622                          scheduled_node=None, limits=None):
2623         """Destroy and re-make this instance.
2624 
2625         A 'rebuild' effectively purges all existing data from the system and
2626         remakes the VM with given 'metadata' and 'personalities'.
2627 
2628         :param context: `nova.RequestContext` object
2629         :param instance: Instance object
2630         :param orig_image_ref: Original image_ref before rebuild
2631         :param image_ref: New image_ref for rebuild
2632         :param injected_files: Files to inject
2633         :param new_pass: password to set on rebuilt instance
2634         :param orig_sys_metadata: instance system metadata from pre-rebuild
2635         :param bdms: block-device-mappings to use for rebuild
2636         :param recreate: True if the instance is being recreated (e.g. the
2637             hypervisor it was on failed) - cleanup of old state will be
2638             skipped.
2639         :param on_shared_storage: True if instance files on shared storage.
2640                                   If not provided then information from the
2641                                   driver will be used to decide if the instance
2642                                   files are available or not on the target host
2643         :param preserve_ephemeral: True if the default ephemeral storage
2644                                    partition must be preserved on rebuild
2645         :param migration: a Migration object if one was created for this
2646                           rebuild operation (if it's a part of evacuate)
2647         :param scheduled_node: A node of the host chosen by the scheduler. If a
2648                                host was specified by the user, this will be
2649                                None
2650         :param limits: Overcommit limits set by the scheduler. If a host was
2651                        specified by the user, this will be None
2652         """
2653         context = context.elevated()
2654 
2655         LOG.info(_LI("Rebuilding instance"), instance=instance)
2656         if scheduled_node is not None:
2657             rt = self._get_resource_tracker(scheduled_node)
2658             rebuild_claim = rt.rebuild_claim
2659         else:
2660             rebuild_claim = claims.NopClaim
2661 
2662         image_meta = {}
2663         if image_ref:
2664             image_meta = self.image_api.get(context, image_ref)
2665 
2666         # NOTE(mriedem): On a recreate (evacuate), we need to update
2667         # the instance's host and node properties to reflect it's
2668         # destination node for the recreate.
2669         if not scheduled_node:
2670             if recreate:
2671                 try:
2672                     compute_node = self._get_compute_info(context, self.host)
2673                     scheduled_node = compute_node.hypervisor_hostname
2674                 except exception.ComputeHostNotFound:
2675                     LOG.exception(_LE('Failed to get compute_info for %s'),
2676                                   self.host)
2677             else:
2678                 scheduled_node = instance.node
2679 
2680         with self._error_out_instance_on_exception(context, instance):
2681             try:
2682                 claim_ctxt = rebuild_claim(
2683                     context, instance, limits=limits, image_meta=image_meta,
2684                     migration=migration)
2685                 self._do_rebuild_instance_with_claim(
2686                     claim_ctxt, context, instance, orig_image_ref,
2687                     image_ref, injected_files, new_pass, orig_sys_metadata,
2688                     bdms, recreate, on_shared_storage, preserve_ephemeral)
2689             except exception.ComputeResourcesUnavailable as e:
2690                 LOG.debug("Could not rebuild instance on this host, not "
2691                           "enough resources available.", instance=instance)
2692 
2693                 # NOTE(ndipanov): We just abort the build for now and leave a
2694                 # migration record for potential cleanup later
2695                 self._set_migration_status(migration, 'failed')
2696 
2697                 self._notify_about_instance_usage(context, instance,
2698                         'rebuild.error', fault=e)
2699                 raise exception.BuildAbortException(
2700                     instance_uuid=instance.uuid, reason=e.format_message())
2701             except (exception.InstanceNotFound,
2702                     exception.UnexpectedDeletingTaskStateError) as e:
2703                 LOG.debug('Instance was deleted while rebuilding',
2704                           instance=instance)
2705                 self._set_migration_status(migration, 'failed')
2706                 self._notify_about_instance_usage(context, instance,
2707                         'rebuild.error', fault=e)
2708             except Exception as e:
2709                 self._set_migration_status(migration, 'failed')
2710                 self._notify_about_instance_usage(context, instance,
2711                         'rebuild.error', fault=e)
2712                 raise
2713             else:
2714                 instance.apply_migration_context()
2715                 # NOTE (ndipanov): This save will now update the host and node
2716                 # attributes making sure that next RT pass is consistent since
2717                 # it will be based on the instance and not the migration DB
2718                 # entry.
2719                 instance.host = self.host
2720                 instance.node = scheduled_node
2721                 instance.save()
2722                 instance.drop_migration_context()
2723 
2724                 # NOTE (ndipanov): Mark the migration as done only after we
2725                 # mark the instance as belonging to this host.
2726                 self._set_migration_status(migration, 'done')
2727 
2728     def _do_rebuild_instance_with_claim(self, claim_context, *args, **kwargs):
2729         """Helper to avoid deep nesting in the top-level method."""
2730 
2731         with claim_context:
2732             self._do_rebuild_instance(*args, **kwargs)
2733 
2734     @staticmethod
2735     def _get_image_name(image_meta):
2736         if image_meta.obj_attr_is_set("name"):
2737             return image_meta.name
2738         else:
2739             return ''
2740 
2741     def _do_rebuild_instance(self, context, instance, orig_image_ref,
2742                              image_ref, injected_files, new_pass,
2743                              orig_sys_metadata, bdms, recreate,
2744                              on_shared_storage, preserve_ephemeral):
2745         orig_vm_state = instance.vm_state
2746 
2747         if recreate:
2748             if not self.driver.capabilities["supports_recreate"]:
2749                 raise exception.InstanceRecreateNotSupported
2750 
2751             self._check_instance_exists(context, instance)
2752 
2753             if on_shared_storage is None:
2754                 LOG.debug('on_shared_storage is not provided, using driver'
2755                             'information to decide if the instance needs to'
2756                             'be recreated')
2757                 on_shared_storage = self.driver.instance_on_disk(instance)
2758 
2759             elif (on_shared_storage !=
2760                     self.driver.instance_on_disk(instance)):
2761                 # To cover case when admin expects that instance files are
2762                 # on shared storage, but not accessible and vice versa
2763                 raise exception.InvalidSharedStorage(
2764                         _("Invalid state of instance files on shared"
2765                             " storage"))
2766 
2767             if on_shared_storage:
2768                 LOG.info(_LI('disk on shared storage, recreating using'
2769                                 ' existing disk'))
2770             else:
2771                 image_ref = orig_image_ref = instance.image_ref
2772                 LOG.info(_LI("disk not on shared storage, rebuilding from:"
2773                                 " '%s'"), str(image_ref))
2774 
2775         if image_ref:
2776             image_meta = objects.ImageMeta.from_image_ref(
2777                 context, self.image_api, image_ref)
2778         else:
2779             image_meta = instance.image_meta
2780 
2781         # This instance.exists message should contain the original
2782         # image_ref, not the new one.  Since the DB has been updated
2783         # to point to the new one... we have to override it.
2784         # TODO(jaypipes): Move generate_image_url() into the nova.image.api
2785         orig_image_ref_url = glance.generate_image_url(orig_image_ref)
2786         extra_usage_info = {'image_ref_url': orig_image_ref_url}
2787         compute_utils.notify_usage_exists(
2788                 self.notifier, context, instance,
2789                 current_period=True, system_metadata=orig_sys_metadata,
2790                 extra_usage_info=extra_usage_info)
2791 
2792         # This message should contain the new image_ref
2793         extra_usage_info = {'image_name': self._get_image_name(image_meta)}
2794         self._notify_about_instance_usage(context, instance,
2795                 "rebuild.start", extra_usage_info=extra_usage_info)
2796 
2797         instance.power_state = self._get_power_state(context, instance)
2798         instance.task_state = task_states.REBUILDING
2799         instance.save(expected_task_state=[task_states.REBUILDING])
2800 
2801         if recreate:
2802             self.network_api.setup_networks_on_host(
2803                     context, instance, self.host)
2804             # For nova-network this is needed to move floating IPs
2805             # For neutron this updates the host in the port binding
2806             # TODO(cfriesen): this network_api call and the one above
2807             # are so similar, we should really try to unify them.
2808             self.network_api.setup_instance_network_on_host(
2809                     context, instance, self.host)
2810 
2811         network_info = compute_utils.get_nw_info_for_instance(instance)
2812         if bdms is None:
2813             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
2814                     context, instance.uuid)
2815 
2816         block_device_info = \
2817             self._get_instance_block_device_info(
2818                     context, instance, bdms=bdms)
2819 
2820         def detach_block_devices(context, bdms):
2821             for bdm in bdms:
2822                 if bdm.is_volume:
2823                     self._detach_volume(context, bdm.volume_id, instance,
2824                                         destroy_bdm=False)
2825 
2826         files = self._decode_files(injected_files)
2827 
2828         kwargs = dict(
2829             context=context,
2830             instance=instance,
2831             image_meta=image_meta,
2832             injected_files=files,
2833             admin_password=new_pass,
2834             bdms=bdms,
2835             detach_block_devices=detach_block_devices,
2836             attach_block_devices=self._prep_block_device,
2837             block_device_info=block_device_info,
2838             network_info=network_info,
2839             preserve_ephemeral=preserve_ephemeral,
2840             recreate=recreate)
2841         try:
2842             with instance.mutated_migration_context():
2843                 self.driver.rebuild(**kwargs)
2844         except NotImplementedError:
2845             # NOTE(rpodolyaka): driver doesn't provide specialized version
2846             # of rebuild, fall back to the default implementation
2847             self._rebuild_default_impl(**kwargs)
2848         self._update_instance_after_spawn(context, instance)
2849         instance.save(expected_task_state=[task_states.REBUILD_SPAWNING])
2850 
2851         if orig_vm_state == vm_states.STOPPED:
2852             LOG.info(_LI("bringing vm to original state: '%s'"),
2853                         orig_vm_state, instance=instance)
2854             instance.vm_state = vm_states.ACTIVE
2855             instance.task_state = task_states.POWERING_OFF
2856             instance.progress = 0
2857             instance.save()
2858             self.stop_instance(context, instance, False)
2859         self._update_scheduler_instance_info(context, instance)
2860         self._notify_about_instance_usage(
2861                 context, instance, "rebuild.end",
2862                 network_info=network_info,
2863                 extra_usage_info=extra_usage_info)
2864 
2865     def _handle_bad_volumes_detached(self, context, instance, bad_devices,
2866                                      block_device_info):
2867         """Handle cases where the virt-layer had to detach non-working volumes
2868         in order to complete an operation.
2869         """
2870         for bdm in block_device_info['block_device_mapping']:
2871             if bdm.get('mount_device') in bad_devices:
2872                 try:
2873                     volume_id = bdm['connection_info']['data']['volume_id']
2874                 except KeyError:
2875                     continue
2876 
2877                 # NOTE(sirp): ideally we'd just call
2878                 # `compute_api.detach_volume` here but since that hits the
2879                 # DB directly, that's off limits from within the
2880                 # compute-manager.
2881                 #
2882                 # API-detach
2883                 LOG.info(_LI("Detaching from volume api: %s"), volume_id)
2884                 volume = self.volume_api.get(context, volume_id)
2885                 self.volume_api.check_detach(context, volume)
2886                 self.volume_api.begin_detaching(context, volume_id)
2887 
2888                 # Manager-detach
2889                 self.detach_volume(context, volume_id, instance)
2890 
2891     @wrap_exception()
2892     @reverts_task_state
2893     @wrap_instance_event(prefix='compute')
2894     @wrap_instance_fault
2895     def reboot_instance(self, context, instance, block_device_info,
2896                         reboot_type):
2897         """Reboot an instance on this host."""
2898         # acknowledge the request made it to the manager
2899         if reboot_type == "SOFT":
2900             instance.task_state = task_states.REBOOT_PENDING
2901             expected_states = (task_states.REBOOTING,
2902                                task_states.REBOOT_PENDING,
2903                                task_states.REBOOT_STARTED)
2904         else:
2905             instance.task_state = task_states.REBOOT_PENDING_HARD
2906             expected_states = (task_states.REBOOTING_HARD,
2907                                task_states.REBOOT_PENDING_HARD,
2908                                task_states.REBOOT_STARTED_HARD)
2909         context = context.elevated()
2910         LOG.info(_LI("Rebooting instance"), instance=instance)
2911 
2912         block_device_info = self._get_instance_block_device_info(context,
2913                                                                  instance)
2914 
2915         network_info = self.network_api.get_instance_nw_info(context, instance)
2916 
2917         self._notify_about_instance_usage(context, instance, "reboot.start")
2918 
2919         instance.power_state = self._get_power_state(context, instance)
2920         instance.save(expected_task_state=expected_states)
2921 
2922         if instance.power_state != power_state.RUNNING:
2923             state = instance.power_state
2924             running = power_state.RUNNING
2925             LOG.warning(_LW('trying to reboot a non-running instance:'
2926                             ' (state: %(state)s expected: %(running)s)'),
2927                         {'state': state, 'running': running},
2928                         instance=instance)
2929 
2930         def bad_volumes_callback(bad_devices):
2931             self._handle_bad_volumes_detached(
2932                     context, instance, bad_devices, block_device_info)
2933 
2934         try:
2935             # Don't change it out of rescue mode
2936             if instance.vm_state == vm_states.RESCUED:
2937                 new_vm_state = vm_states.RESCUED
2938             else:
2939                 new_vm_state = vm_states.ACTIVE
2940             new_power_state = None
2941             if reboot_type == "SOFT":
2942                 instance.task_state = task_states.REBOOT_STARTED
2943                 expected_state = task_states.REBOOT_PENDING
2944             else:
2945                 instance.task_state = task_states.REBOOT_STARTED_HARD
2946                 expected_state = task_states.REBOOT_PENDING_HARD
2947             instance.save(expected_task_state=expected_state)
2948             self.driver.reboot(context, instance,
2949                                network_info,
2950                                reboot_type,
2951                                block_device_info=block_device_info,
2952                                bad_volumes_callback=bad_volumes_callback)
2953 
2954         except Exception as error:
2955             with excutils.save_and_reraise_exception() as ctxt:
2956                 exc_info = sys.exc_info()
2957                 # if the reboot failed but the VM is running don't
2958                 # put it into an error state
2959                 new_power_state = self._get_power_state(context, instance)
2960                 if new_power_state == power_state.RUNNING:
2961                     LOG.warning(_LW('Reboot failed but instance is running'),
2962                                 instance=instance)
2963                     compute_utils.add_instance_fault_from_exc(context,
2964                             instance, error, exc_info)
2965                     self._notify_about_instance_usage(context, instance,
2966                             'reboot.error', fault=error)
2967                     ctxt.reraise = False
2968                 else:
2969                     LOG.error(_LE('Cannot reboot instance: %s'), error,
2970                               instance=instance)
2971                     self._set_instance_obj_error_state(context, instance)
2972 
2973         if not new_power_state:
2974             new_power_state = self._get_power_state(context, instance)
2975         try:
2976             instance.power_state = new_power_state
2977             instance.vm_state = new_vm_state
2978             instance.task_state = None
2979             instance.save()
2980         except exception.InstanceNotFound:
2981             LOG.warning(_LW("Instance disappeared during reboot"),
2982                         instance=instance)
2983 
2984         self._notify_about_instance_usage(context, instance, "reboot.end")
2985 
2986     @delete_image_on_error
2987     def _do_snapshot_instance(self, context, image_id, instance):
2988         self._snapshot_instance(context, image_id, instance,
2989                                 task_states.IMAGE_BACKUP)
2990 
2991     @wrap_exception()
2992     @reverts_task_state
2993     @wrap_instance_fault
2994     def backup_instance(self, context, image_id, instance, backup_type,
2995                         rotation):
2996         """Backup an instance on this host.
2997 
2998         :param backup_type: daily | weekly
2999         :param rotation: int representing how many backups to keep around
3000         """
3001         self._do_snapshot_instance(context, image_id, instance)
3002         self._rotate_backups(context, instance, backup_type, rotation)
3003 
3004     @wrap_exception()
3005     @reverts_task_state
3006     @wrap_instance_fault
3007     @delete_image_on_error
3008     def snapshot_instance(self, context, image_id, instance):
3009         """Snapshot an instance on this host.
3010 
3011         :param context: security context
3012         :param image_id: glance.db.sqlalchemy.models.Image.Id
3013         :param instance: a nova.objects.instance.Instance object
3014         """
3015         # NOTE(dave-mcnally) the task state will already be set by the api
3016         # but if the compute manager has crashed/been restarted prior to the
3017         # request getting here the task state may have been cleared so we set
3018         # it again and things continue normally
3019         try:
3020             instance.task_state = task_states.IMAGE_SNAPSHOT
3021             instance.save(
3022                         expected_task_state=task_states.IMAGE_SNAPSHOT_PENDING)
3023         except exception.InstanceNotFound:
3024             # possibility instance no longer exists, no point in continuing
3025             LOG.debug("Instance not found, could not set state %s "
3026                       "for instance.",
3027                       task_states.IMAGE_SNAPSHOT, instance=instance)
3028             return
3029 
3030         except exception.UnexpectedDeletingTaskStateError:
3031             LOG.debug("Instance being deleted, snapshot cannot continue",
3032                       instance=instance)
3033             return
3034 
3035         self._snapshot_instance(context, image_id, instance,
3036                                 task_states.IMAGE_SNAPSHOT)
3037 
3038     def _snapshot_instance(self, context, image_id, instance,
3039                            expected_task_state):
3040         context = context.elevated()
3041 
3042         instance.power_state = self._get_power_state(context, instance)
3043         try:
3044             instance.save()
3045 
3046             LOG.info(_LI('instance snapshotting'), instance=instance)
3047 
3048             if instance.power_state != power_state.RUNNING:
3049                 state = instance.power_state
3050                 running = power_state.RUNNING
3051                 LOG.warning(_LW('trying to snapshot a non-running instance: '
3052                                 '(state: %(state)s expected: %(running)s)'),
3053                             {'state': state, 'running': running},
3054                             instance=instance)
3055 
3056             self._notify_about_instance_usage(
3057                 context, instance, "snapshot.start")
3058 
3059             def update_task_state(task_state,
3060                                   expected_state=expected_task_state):
3061                 instance.task_state = task_state
3062                 instance.save(expected_task_state=expected_state)
3063 
3064             self.driver.snapshot(context, instance, image_id,
3065                                  update_task_state)
3066 
3067             instance.task_state = None
3068             instance.save(expected_task_state=task_states.IMAGE_UPLOADING)
3069 
3070             self._notify_about_instance_usage(context, instance,
3071                                               "snapshot.end")
3072         except (exception.InstanceNotFound,
3073                 exception.UnexpectedDeletingTaskStateError):
3074             # the instance got deleted during the snapshot
3075             # Quickly bail out of here
3076             msg = 'Instance disappeared during snapshot'
3077             LOG.debug(msg, instance=instance)
3078             try:
3079                 image_service = glance.get_default_image_service()
3080                 image = image_service.show(context, image_id)
3081                 if image['status'] != 'active':
3082                     image_service.delete(context, image_id)
3083             except Exception:
3084                 LOG.warning(_LW("Error while trying to clean up image %s"),
3085                             image_id, instance=instance)
3086         except exception.ImageNotFound:
3087             instance.task_state = None
3088             instance.save()
3089             msg = _LW("Image not found during snapshot")
3090             LOG.warning(msg, instance=instance)
3091 
3092     def _post_interrupted_snapshot_cleanup(self, context, instance):
3093         self.driver.post_interrupted_snapshot_cleanup(context, instance)
3094 
3095     @messaging.expected_exceptions(NotImplementedError)
3096     @wrap_exception()
3097     def volume_snapshot_create(self, context, instance, volume_id,
3098                                create_info):
3099         self.driver.volume_snapshot_create(context, instance, volume_id,
3100                                            create_info)
3101 
3102     @messaging.expected_exceptions(NotImplementedError)
3103     @wrap_exception()
3104     def volume_snapshot_delete(self, context, instance, volume_id,
3105                                snapshot_id, delete_info):
3106         self.driver.volume_snapshot_delete(context, instance, volume_id,
3107                                            snapshot_id, delete_info)
3108 
3109     @wrap_instance_fault
3110     def _rotate_backups(self, context, instance, backup_type, rotation):
3111         """Delete excess backups associated to an instance.
3112 
3113         Instances are allowed a fixed number of backups (the rotation number);
3114         this method deletes the oldest backups that exceed the rotation
3115         threshold.
3116 
3117         :param context: security context
3118         :param instance: Instance dict
3119         :param backup_type: a user-defined type, like "daily" or "weekly" etc.
3120         :param rotation: int representing how many backups to keep around;
3121             None if rotation shouldn't be used (as in the case of snapshots)
3122         """
3123         filters = {'property-image_type': 'backup',
3124                    'property-backup_type': backup_type,
3125                    'property-instance_uuid': instance.uuid}
3126 
3127         images = self.image_api.get_all(context, filters=filters,
3128                                         sort_key='created_at', sort_dir='desc')
3129         num_images = len(images)
3130         LOG.debug("Found %(num_images)d images (rotation: %(rotation)d)",
3131                   {'num_images': num_images, 'rotation': rotation},
3132                   instance=instance)
3133 
3134         if num_images > rotation:
3135             # NOTE(sirp): this deletes all backups that exceed the rotation
3136             # limit
3137             excess = len(images) - rotation
3138             LOG.debug("Rotating out %d backups", excess,
3139                       instance=instance)
3140             for i in range(excess):
3141                 image = images.pop()
3142                 image_id = image['id']
3143                 LOG.debug("Deleting image %s", image_id,
3144                           instance=instance)
3145                 self.image_api.delete(context, image_id)
3146 
3147     @wrap_exception()
3148     @reverts_task_state
3149     @wrap_instance_event(prefix='compute')
3150     @wrap_instance_fault
3151     def set_admin_password(self, context, instance, new_pass):
3152         """Set the root/admin password for an instance on this host.
3153 
3154         This is generally only called by API password resets after an
3155         image has been built.
3156 
3157         @param context: Nova auth context.
3158         @param instance: Nova instance object.
3159         @param new_pass: The admin password for the instance.
3160         """
3161 
3162         context = context.elevated()
3163         if new_pass is None:
3164             # Generate a random password
3165             new_pass = utils.generate_password()
3166 
3167         current_power_state = self._get_power_state(context, instance)
3168         expected_state = power_state.RUNNING
3169 
3170         if current_power_state != expected_state:
3171             instance.task_state = None
3172             instance.save(expected_task_state=task_states.UPDATING_PASSWORD)
3173             _msg = _('instance %s is not running') % instance.uuid
3174             raise exception.InstancePasswordSetFailed(
3175                 instance=instance.uuid, reason=_msg)
3176 
3177         try:
3178             self.driver.set_admin_password(instance, new_pass)
3179             LOG.info(_LI("Root password set"), instance=instance)
3180             instance.task_state = None
3181             instance.save(
3182                 expected_task_state=task_states.UPDATING_PASSWORD)
3183         except exception.InstanceAgentNotEnabled:
3184             with excutils.save_and_reraise_exception():
3185                 LOG.debug('Guest agent is not enabled for the instance.',
3186                           instance=instance)
3187                 instance.task_state = None
3188                 instance.save(
3189                     expected_task_state=task_states.UPDATING_PASSWORD)
3190         except exception.SetAdminPasswdNotSupported:
3191             with excutils.save_and_reraise_exception():
3192                 LOG.info(_LI('set_admin_password is not supported '
3193                                 'by this driver or guest instance.'),
3194                             instance=instance)
3195                 instance.task_state = None
3196                 instance.save(
3197                     expected_task_state=task_states.UPDATING_PASSWORD)
3198         except NotImplementedError:
3199             LOG.warning(_LW('set_admin_password is not implemented '
3200                             'by this driver or guest instance.'),
3201                         instance=instance)
3202             instance.task_state = None
3203             instance.save(
3204                 expected_task_state=task_states.UPDATING_PASSWORD)
3205             raise NotImplementedError(_('set_admin_password is not '
3206                                         'implemented by this driver or guest '
3207                                         'instance.'))
3208         except exception.UnexpectedTaskStateError:
3209             # interrupted by another (most likely delete) task
3210             # do not retry
3211             raise
3212         except Exception:
3213             # Catch all here because this could be anything.
3214             LOG.exception(_LE('set_admin_password failed'),
3215                           instance=instance)
3216             self._set_instance_obj_error_state(context, instance)
3217             # We create a new exception here so that we won't
3218             # potentially reveal password information to the
3219             # API caller.  The real exception is logged above
3220             _msg = _('error setting admin password')
3221             raise exception.InstancePasswordSetFailed(
3222                 instance=instance.uuid, reason=_msg)
3223 
3224     @wrap_exception()
3225     @reverts_task_state
3226     @wrap_instance_fault
3227     def inject_file(self, context, path, file_contents, instance):
3228         """Write a file to the specified path in an instance on this host."""
3229         # NOTE(russellb) Remove this method, as well as the underlying virt
3230         # driver methods, when the compute rpc interface is bumped to 4.x
3231         # as it is no longer used.
3232         context = context.elevated()
3233         current_power_state = self._get_power_state(context, instance)
3234         expected_state = power_state.RUNNING
3235         if current_power_state != expected_state:
3236             LOG.warning(_LW('trying to inject a file into a non-running '
3237                             '(state: %(current_state)s expected: '
3238                             '%(expected_state)s)'),
3239                         {'current_state': current_power_state,
3240                          'expected_state': expected_state},
3241                         instance=instance)
3242         LOG.info(_LI('injecting file to %s'), path,
3243                     instance=instance)
3244         self.driver.inject_file(instance, path, file_contents)
3245 
3246     def _get_rescue_image(self, context, instance, rescue_image_ref=None):
3247         """Determine what image should be used to boot the rescue VM."""
3248         # 1. If rescue_image_ref is passed in, use that for rescue.
3249         # 2. Else, use the base image associated with instance's current image.
3250         #       The idea here is to provide the customer with a rescue
3251         #       environment which they are familiar with.
3252         #       So, if they built their instance off of a Debian image,
3253         #       their rescue VM will also be Debian.
3254         # 3. As a last resort, use instance's current image.
3255         if not rescue_image_ref:
3256             system_meta = utils.instance_sys_meta(instance)
3257             rescue_image_ref = system_meta.get('image_base_image_ref')
3258 
3259         if not rescue_image_ref:
3260             LOG.warning(_LW('Unable to find a different image to use for '
3261                             'rescue VM, using instance\'s current image'),
3262                         instance=instance)
3263             rescue_image_ref = instance.image_ref
3264 
3265         return objects.ImageMeta.from_image_ref(
3266             context, self.image_api, rescue_image_ref)
3267 
3268     @wrap_exception()
3269     @reverts_task_state
3270     @wrap_instance_event(prefix='compute')
3271     @wrap_instance_fault
3272     def rescue_instance(self, context, instance, rescue_password,
3273                         rescue_image_ref, clean_shutdown):
3274         context = context.elevated()
3275         LOG.info(_LI('Rescuing'), instance=instance)
3276 
3277         admin_password = (rescue_password if rescue_password else
3278                       utils.generate_password())
3279 
3280         network_info = self.network_api.get_instance_nw_info(context, instance)
3281 
3282         rescue_image_meta = self._get_rescue_image(context, instance,
3283                                                    rescue_image_ref)
3284 
3285         extra_usage_info = {'rescue_image_name':
3286                             self._get_image_name(rescue_image_meta)}
3287         self._notify_about_instance_usage(context, instance,
3288                 "rescue.start", extra_usage_info=extra_usage_info,
3289                 network_info=network_info)
3290 
3291         try:
3292             self._power_off_instance(context, instance, clean_shutdown)
3293 
3294             self.driver.rescue(context, instance,
3295                                network_info,
3296                                rescue_image_meta, admin_password)
3297         except Exception as e:
3298             LOG.exception(_LE("Error trying to Rescue Instance"),
3299                           instance=instance)
3300             self._set_instance_obj_error_state(context, instance)
3301             raise exception.InstanceNotRescuable(
3302                 instance_id=instance.uuid,
3303                 reason=_("Driver Error: %s") % e)
3304 
3305         compute_utils.notify_usage_exists(self.notifier, context, instance,
3306                                           current_period=True)
3307 
3308         instance.vm_state = vm_states.RESCUED
3309         instance.task_state = None
3310         instance.power_state = self._get_power_state(context, instance)
3311         instance.launched_at = timeutils.utcnow()
3312         instance.save(expected_task_state=task_states.RESCUING)
3313 
3314         self._notify_about_instance_usage(context, instance,
3315                 "rescue.end", extra_usage_info=extra_usage_info,
3316                 network_info=network_info)
3317 
3318     @wrap_exception()
3319     @reverts_task_state
3320     @wrap_instance_event(prefix='compute')
3321     @wrap_instance_fault
3322     def unrescue_instance(self, context, instance):
3323         context = context.elevated()
3324         LOG.info(_LI('Unrescuing'), instance=instance)
3325 
3326         network_info = self.network_api.get_instance_nw_info(context, instance)
3327         self._notify_about_instance_usage(context, instance,
3328                 "unrescue.start", network_info=network_info)
3329         with self._error_out_instance_on_exception(context, instance):
3330             self.driver.unrescue(instance,
3331                                  network_info)
3332 
3333         instance.vm_state = vm_states.ACTIVE
3334         instance.task_state = None
3335         instance.power_state = self._get_power_state(context, instance)
3336         instance.save(expected_task_state=task_states.UNRESCUING)
3337 
3338         self._notify_about_instance_usage(context,
3339                                           instance,
3340                                           "unrescue.end",
3341                                           network_info=network_info)
3342 
3343     @wrap_exception()
3344     @wrap_instance_fault
3345     def change_instance_metadata(self, context, diff, instance):
3346         """Update the metadata published to the instance."""
3347         LOG.debug("Changing instance metadata according to %r",
3348                   diff, instance=instance)
3349         self.driver.change_instance_metadata(context, instance, diff)
3350 
3351     @wrap_exception()
3352     @wrap_instance_event(prefix='compute')
3353     @wrap_instance_fault
3354     def confirm_resize(self, context, instance, reservations, migration):
3355 
3356         quotas = objects.Quotas.from_reservations(context,
3357                                                   reservations,
3358                                                   instance=instance)
3359 
3360         @utils.synchronized(instance.uuid)
3361         def do_confirm_resize(context, instance, migration_id):
3362             # NOTE(wangpan): Get the migration status from db, if it has been
3363             #                confirmed, we do nothing and return here
3364             LOG.debug("Going to confirm migration %s", migration_id,
3365                       instance=instance)
3366             try:
3367                 # TODO(russellb) Why are we sending the migration object just
3368                 # to turn around and look it up from the db again?
3369                 migration = objects.Migration.get_by_id(
3370                                     context.elevated(), migration_id)
3371             except exception.MigrationNotFound:
3372                 LOG.error(_LE("Migration %s is not found during confirmation"),
3373                           migration_id, instance=instance)
3374                 quotas.rollback()
3375                 return
3376 
3377             if migration.status == 'confirmed':
3378                 LOG.info(_LI("Migration %s is already confirmed"),
3379                          migration_id, instance=instance)
3380                 quotas.rollback()
3381                 return
3382             elif migration.status not in ('finished', 'confirming'):
3383                 LOG.warning(_LW("Unexpected confirmation status '%(status)s' "
3384                                 "of migration %(id)s, exit confirmation "
3385                                 "process"),
3386                             {"status": migration.status, "id": migration_id},
3387                             instance=instance)
3388                 quotas.rollback()
3389                 return
3390 
3391             # NOTE(wangpan): Get the instance from db, if it has been
3392             #                deleted, we do nothing and return here
3393             expected_attrs = ['metadata', 'system_metadata', 'flavor']
3394             try:
3395                 instance = objects.Instance.get_by_uuid(
3396                         context, instance.uuid,
3397                         expected_attrs=expected_attrs)
3398             except exception.InstanceNotFound:
3399                 LOG.info(_LI("Instance is not found during confirmation"),
3400                          instance=instance)
3401                 quotas.rollback()
3402                 return
3403 
3404             self._confirm_resize(context, instance, quotas,
3405                                  migration=migration)
3406 
3407         do_confirm_resize(context, instance, migration.id)
3408 
3409     def _confirm_resize(self, context, instance, quotas,
3410                         migration=None):
3411         """Destroys the source instance."""
3412         self._notify_about_instance_usage(context, instance,
3413                                           "resize.confirm.start")
3414 
3415         with self._error_out_instance_on_exception(context, instance,
3416                                                    quotas=quotas):
3417             # NOTE(danms): delete stashed migration information
3418             old_instance_type = instance.old_flavor
3419             instance.old_flavor = None
3420             instance.new_flavor = None
3421             instance.system_metadata.pop('old_vm_state', None)
3422             instance.save()
3423 
3424             # NOTE(tr3buchet): tear down networks on source host
3425             self.network_api.setup_networks_on_host(context, instance,
3426                                migration.source_compute, teardown=True)
3427 
3428             network_info = self.network_api.get_instance_nw_info(context,
3429                                                                  instance)
3430             self.driver.confirm_migration(migration, instance,
3431                                           network_info)
3432 
3433             migration.status = 'confirmed'
3434             with migration.obj_as_admin():
3435                 migration.save()
3436 
3437             rt = self._get_resource_tracker(migration.source_node)
3438             rt.drop_move_claim(context, instance, old_instance_type,
3439                                prefix='old_')
3440             instance.drop_migration_context()
3441 
3442             # NOTE(mriedem): The old_vm_state could be STOPPED but the user
3443             # might have manually powered up the instance to confirm the
3444             # resize/migrate, so we need to check the current power state
3445             # on the instance and set the vm_state appropriately. We default
3446             # to ACTIVE because if the power state is not SHUTDOWN, we
3447             # assume _sync_instance_power_state will clean it up.
3448             p_state = instance.power_state
3449             vm_state = None
3450             if p_state == power_state.SHUTDOWN:
3451                 vm_state = vm_states.STOPPED
3452                 LOG.debug("Resized/migrated instance is powered off. "
3453                           "Setting vm_state to '%s'.", vm_state,
3454                           instance=instance)
3455             else:
3456                 vm_state = vm_states.ACTIVE
3457 
3458             instance.vm_state = vm_state
3459             instance.task_state = None
3460             instance.save(expected_task_state=[None, task_states.DELETING])
3461 
3462             self._notify_about_instance_usage(
3463                 context, instance, "resize.confirm.end",
3464                 network_info=network_info)
3465 
3466             quotas.commit()
3467 
3468     @wrap_exception()
3469     @reverts_task_state
3470     @wrap_instance_event(prefix='compute')
3471     @errors_out_migration
3472     @wrap_instance_fault
3473     def revert_resize(self, context, instance, migration, reservations):
3474         """Destroys the new instance on the destination machine.
3475 
3476         Reverts the model changes, and powers on the old instance on the
3477         source machine.
3478 
3479         """
3480 
3481         quotas = objects.Quotas.from_reservations(context,
3482                                                   reservations,
3483                                                   instance=instance)
3484 
3485         # NOTE(comstud): A revert_resize is essentially a resize back to
3486         # the old size, so we need to send a usage event here.
3487         compute_utils.notify_usage_exists(self.notifier, context, instance,
3488                                           current_period=True)
3489 
3490         with self._error_out_instance_on_exception(context, instance,
3491                                                    quotas=quotas):
3492             # NOTE(tr3buchet): tear down networks on destination host
3493             self.network_api.setup_networks_on_host(context, instance,
3494                                                     teardown=True)
3495 
3496             migration_p = obj_base.obj_to_primitive(migration)
3497             self.network_api.migrate_instance_start(context,
3498                                                     instance,
3499                                                     migration_p)
3500 
3501             network_info = self.network_api.get_instance_nw_info(context,
3502                                                                  instance)
3503             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
3504                     context, instance.uuid)
3505             block_device_info = self._get_instance_block_device_info(
3506                                 context, instance, bdms=bdms)
3507 
3508             destroy_disks = not self._is_instance_storage_shared(
3509                 context, instance, host=migration.source_compute)
3510             self.driver.destroy(context, instance, network_info,
3511                                 block_device_info, destroy_disks)
3512 
3513             self._terminate_volume_connections(context, instance, bdms)
3514 
3515             migration.status = 'reverted'
3516             with migration.obj_as_admin():
3517                 migration.save()
3518 
3519             # NOTE(ndipanov): We need to do this here because dropping the
3520             # claim means we lose the migration_context data. We really should
3521             # fix this by moving the drop_move_claim call to the
3522             # finish_revert_resize method as this is racy (revert is dropped,
3523             # but instance resources will be tracked with the new flavor until
3524             # it gets rolled back in finish_revert_resize, which is
3525             # potentially wrong for a period of time).
3526             instance.revert_migration_context()
3527             instance.save()
3528 
3529             rt = self._get_resource_tracker(instance.node)
3530             rt.drop_move_claim(context, instance)
3531 
3532             self.compute_rpcapi.finish_revert_resize(context, instance,
3533                     migration, migration.source_compute,
3534                     quotas.reservations)
3535 
3536     @wrap_exception()
3537     @reverts_task_state
3538     @wrap_instance_event(prefix='compute')
3539     @errors_out_migration
3540     @wrap_instance_fault
3541     def finish_revert_resize(self, context, instance, reservations, migration):
3542         """Finishes the second half of reverting a resize.
3543 
3544         Bring the original source instance state back (active/shutoff) and
3545         revert the resized attributes in the database.
3546 
3547         """
3548 
3549         quotas = objects.Quotas.from_reservations(context,
3550                                                   reservations,
3551                                                   instance=instance)
3552 
3553         with self._error_out_instance_on_exception(context, instance,
3554                                                    quotas=quotas):
3555             self._notify_about_instance_usage(
3556                     context, instance, "resize.revert.start")
3557 
3558             # NOTE(mriedem): delete stashed old_vm_state information; we
3559             # default to ACTIVE for backwards compatibility if old_vm_state
3560             # is not set
3561             old_vm_state = instance.system_metadata.pop('old_vm_state',
3562                                                         vm_states.ACTIVE)
3563 
3564             self._set_instance_info(instance, instance.old_flavor)
3565             instance.old_flavor = None
3566             instance.new_flavor = None
3567             instance.host = migration.source_compute
3568             instance.node = migration.source_node
3569             instance.save()
3570 
3571             migration.dest_compute = migration.source_compute
3572             with migration.obj_as_admin():
3573                 migration.save()
3574 
3575             self.network_api.setup_networks_on_host(context, instance,
3576                                                     migration.source_compute)
3577             migration_p = obj_base.obj_to_primitive(migration)
3578             self.network_api.migrate_instance_finish(context,
3579                                                      instance,
3580                                                      migration_p)
3581             network_info = self.network_api.get_instance_nw_info(context,
3582                                                                  instance)
3583 
3584             block_device_info = self._get_instance_block_device_info(
3585                     context, instance, refresh_conn_info=True)
3586 
3587             power_on = old_vm_state != vm_states.STOPPED
3588             self.driver.finish_revert_migration(context, instance,
3589                                        network_info,
3590                                        block_device_info, power_on)
3591 
3592             instance.drop_migration_context()
3593             instance.launched_at = timeutils.utcnow()
3594             instance.save(expected_task_state=task_states.RESIZE_REVERTING)
3595 
3596             # if the original vm state was STOPPED, set it back to STOPPED
3597             LOG.info(_LI("Updating instance to original state: '%s'"),
3598                      old_vm_state, instance=instance)
3599             if power_on:
3600                 instance.vm_state = vm_states.ACTIVE
3601                 instance.task_state = None
3602                 instance.save()
3603             else:
3604                 instance.task_state = task_states.POWERING_OFF
3605                 instance.save()
3606                 self.stop_instance(context, instance=instance,
3607                                    clean_shutdown=True)
3608 
3609             self._notify_about_instance_usage(
3610                     context, instance, "resize.revert.end")
3611             quotas.commit()
3612 
3613     def _prep_resize(self, context, image, instance, instance_type,
3614             quotas, request_spec, filter_properties, node,
3615             clean_shutdown=True):
3616 
3617         if not filter_properties:
3618             filter_properties = {}
3619 
3620         if not instance.host:
3621             self._set_instance_obj_error_state(context, instance)
3622             msg = _('Instance has no source host')
3623             raise exception.MigrationError(reason=msg)
3624 
3625         same_host = instance.host == self.host
3626         # if the flavor IDs match, it's migrate; otherwise resize
3627         if same_host and instance_type.id == instance['instance_type_id']:
3628             # check driver whether support migrate to same host
3629             if not self.driver.capabilities['supports_migrate_to_same_host']:
3630                 raise exception.UnableToMigrateToSelf(
3631                     instance_id=instance.uuid, host=self.host)
3632 
3633         # NOTE(danms): Stash the new instance_type to avoid having to
3634         # look it up in the database later
3635         instance.new_flavor = instance_type
3636         # NOTE(mriedem): Stash the old vm_state so we can set the
3637         # resized/reverted instance back to the same state later.
3638         vm_state = instance.vm_state
3639         LOG.debug('Stashing vm_state: %s', vm_state, instance=instance)
3640         instance.system_metadata['old_vm_state'] = vm_state
3641         instance.save()
3642 
3643         limits = filter_properties.get('limits', {})
3644         rt = self._get_resource_tracker(node)
3645         with rt.resize_claim(context, instance, instance_type,
3646                              image_meta=image, limits=limits) as claim:
3647             LOG.info(_LI('Migrating'), instance=instance)
3648             self.compute_rpcapi.resize_instance(
3649                     context, instance, claim.migration, image,
3650                     instance_type, quotas.reservations,
3651                     clean_shutdown)
3652 
3653     @wrap_exception()
3654     @reverts_task_state
3655     @wrap_instance_event(prefix='compute')
3656     @wrap_instance_fault
3657     def prep_resize(self, context, image, instance, instance_type,
3658                     reservations, request_spec, filter_properties, node,
3659                     clean_shutdown):
3660         """Initiates the process of moving a running instance to another host.
3661 
3662         Possibly changes the RAM and disk size in the process.
3663 
3664         """
3665         if node is None:
3666             node = self.driver.get_available_nodes(refresh=True)[0]
3667             LOG.debug("No node specified, defaulting to %s", node,
3668                       instance=instance)
3669 
3670         # NOTE(melwitt): Remove this in version 5.0 of the RPC API
3671         # Code downstream may expect extra_specs to be populated since it
3672         # is receiving an object, so lookup the flavor to ensure this.
3673         if not isinstance(instance_type, objects.Flavor):
3674             instance_type = objects.Flavor.get_by_id(context,
3675                                                      instance_type['id'])
3676 
3677         quotas = objects.Quotas.from_reservations(context,
3678                                                   reservations,
3679                                                   instance=instance)
3680         with self._error_out_instance_on_exception(context, instance,
3681                                                    quotas=quotas):
3682             compute_utils.notify_usage_exists(self.notifier, context, instance,
3683                                               current_period=True)
3684             self._notify_about_instance_usage(
3685                     context, instance, "resize.prep.start")
3686             try:
3687                 self._prep_resize(context, image, instance,
3688                                   instance_type, quotas,
3689                                   request_spec, filter_properties,
3690                                   node, clean_shutdown)
3691             # NOTE(dgenin): This is thrown in LibvirtDriver when the
3692             #               instance to be migrated is backed by LVM.
3693             #               Remove when LVM migration is implemented.
3694             except exception.MigrationPreCheckError:
3695                 raise
3696             except Exception:
3697                 # try to re-schedule the resize elsewhere:
3698                 exc_info = sys.exc_info()
3699                 self._reschedule_resize_or_reraise(context, image, instance,
3700                         exc_info, instance_type, quotas, request_spec,
3701                         filter_properties)
3702             finally:
3703                 extra_usage_info = dict(
3704                         new_instance_type=instance_type.name,
3705                         new_instance_type_id=instance_type.id)
3706 
3707                 self._notify_about_instance_usage(
3708                     context, instance, "resize.prep.end",
3709                     extra_usage_info=extra_usage_info)
3710 
3711     def _reschedule_resize_or_reraise(self, context, image, instance, exc_info,
3712             instance_type, quotas, request_spec, filter_properties):
3713         """Try to re-schedule the resize or re-raise the original error to
3714         error out the instance.
3715         """
3716         if not request_spec:
3717             request_spec = {}
3718         if not filter_properties:
3719             filter_properties = {}
3720 
3721         rescheduled = False
3722         instance_uuid = instance.uuid
3723 
3724         try:
3725             reschedule_method = self.compute_task_api.resize_instance
3726             scheduler_hint = dict(filter_properties=filter_properties)
3727             method_args = (instance, None, scheduler_hint, instance_type,
3728                            quotas.reservations)
3729             task_state = task_states.RESIZE_PREP
3730 
3731             rescheduled = self._reschedule(context, request_spec,
3732                     filter_properties, instance, reschedule_method,
3733                     method_args, task_state, exc_info)
3734         except Exception as error:
3735             rescheduled = False
3736             LOG.exception(_LE("Error trying to reschedule"),
3737                           instance_uuid=instance_uuid)
3738             compute_utils.add_instance_fault_from_exc(context,
3739                     instance, error,
3740                     exc_info=sys.exc_info())
3741             self._notify_about_instance_usage(context, instance,
3742                     'resize.error', fault=error)
3743 
3744         if rescheduled:
3745             self._log_original_error(exc_info, instance_uuid)
3746             compute_utils.add_instance_fault_from_exc(context,
3747                     instance, exc_info[1], exc_info=exc_info)
3748             self._notify_about_instance_usage(context, instance,
3749                     'resize.error', fault=exc_info[1])
3750         else:
3751             # not re-scheduling
3752             six.reraise(*exc_info)
3753 
3754     @wrap_exception()
3755     @reverts_task_state
3756     @wrap_instance_event(prefix='compute')
3757     @errors_out_migration
3758     @wrap_instance_fault
3759     def resize_instance(self, context, instance, image,
3760                         reservations, migration, instance_type,
3761                         clean_shutdown):
3762         """Starts the migration of a running instance to another host."""
3763 
3764         quotas = objects.Quotas.from_reservations(context,
3765                                                   reservations,
3766                                                   instance=instance)
3767         with self._error_out_instance_on_exception(context, instance,
3768                                                    quotas=quotas):
3769             # TODO(chaochin) Remove this until v5 RPC API
3770             # Code downstream may expect extra_specs to be populated since it
3771             # is receiving an object, so lookup the flavor to ensure this.
3772             if (not instance_type or
3773                 not isinstance(instance_type, objects.Flavor)):
3774                 instance_type = objects.Flavor.get_by_id(
3775                     context, migration['new_instance_type_id'])
3776 
3777             network_info = self.network_api.get_instance_nw_info(context,
3778                                                                  instance)
3779 
3780             migration.status = 'migrating'
3781             with migration.obj_as_admin():
3782                 migration.save()
3783 
3784             instance.task_state = task_states.RESIZE_MIGRATING
3785             instance.save(expected_task_state=task_states.RESIZE_PREP)
3786 
3787             self._notify_about_instance_usage(
3788                 context, instance, "resize.start", network_info=network_info)
3789 
3790             compute_utils.notify_about_instance_action(context, instance,
3791                    self.host, action=fields.NotificationAction.RESIZE,
3792                    phase=fields.NotificationPhase.START)
3793 
3794             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
3795                     context, instance.uuid)
3796             block_device_info = self._get_instance_block_device_info(
3797                                 context, instance, bdms=bdms)
3798 
3799             timeout, retry_interval = self._get_power_off_values(context,
3800                                             instance, clean_shutdown)
3801             disk_info = self.driver.migrate_disk_and_power_off(
3802                     context, instance, migration.dest_host,
3803                     instance_type, network_info,
3804                     block_device_info,
3805                     timeout, retry_interval)
3806 
3807             self._terminate_volume_connections(context, instance, bdms)
3808 
3809             migration_p = obj_base.obj_to_primitive(migration)
3810             self.network_api.migrate_instance_start(context,
3811                                                     instance,
3812                                                     migration_p)
3813 
3814             migration.status = 'post-migrating'
3815             with migration.obj_as_admin():
3816                 migration.save()
3817 
3818             instance.host = migration.dest_compute
3819             instance.node = migration.dest_node
3820             instance.task_state = task_states.RESIZE_MIGRATED
3821             instance.save(expected_task_state=task_states.RESIZE_MIGRATING)
3822 
3823             self.compute_rpcapi.finish_resize(context, instance,
3824                     migration, image, disk_info,
3825                     migration.dest_compute, reservations=quotas.reservations)
3826 
3827             self._notify_about_instance_usage(context, instance, "resize.end",
3828                                               network_info=network_info)
3829 
3830             compute_utils.notify_about_instance_action(context, instance,
3831                    self.host, action=fields.NotificationAction.RESIZE,
3832                    phase=fields.NotificationPhase.END)
3833             self.instance_events.clear_events_for_instance(instance)
3834 
3835     def _terminate_volume_connections(self, context, instance, bdms):
3836         connector = self.driver.get_volume_connector(instance)
3837         for bdm in bdms:
3838             if bdm.is_volume:
3839                 self.volume_api.terminate_connection(context, bdm.volume_id,
3840                                                      connector)
3841 
3842     @staticmethod
3843     def _set_instance_info(instance, instance_type):
3844         instance.instance_type_id = instance_type.id
3845         # NOTE(danms): These are purely for any legacy code that still
3846         # looks at them.
3847         instance.memory_mb = instance_type.memory_mb
3848         instance.vcpus = instance_type.vcpus
3849         instance.root_gb = instance_type.root_gb
3850         instance.ephemeral_gb = instance_type.ephemeral_gb
3851         instance.flavor = instance_type
3852 
3853     def _finish_resize(self, context, instance, migration, disk_info,
3854                        image_meta):
3855         resize_instance = False
3856         old_instance_type_id = migration['old_instance_type_id']
3857         new_instance_type_id = migration['new_instance_type_id']
3858         old_instance_type = instance.get_flavor()
3859         # NOTE(mriedem): Get the old_vm_state so we know if we should
3860         # power on the instance. If old_vm_state is not set we need to default
3861         # to ACTIVE for backwards compatibility
3862         old_vm_state = instance.system_metadata.get('old_vm_state',
3863                                                     vm_states.ACTIVE)
3864         instance.old_flavor = old_instance_type
3865 
3866         if old_instance_type_id != new_instance_type_id:
3867             instance_type = instance.get_flavor('new')
3868             self._set_instance_info(instance, instance_type)
3869             for key in ('root_gb', 'swap', 'ephemeral_gb'):
3870                 if old_instance_type[key] != instance_type[key]:
3871                     resize_instance = True
3872                     break
3873         instance.apply_migration_context()
3874 
3875         # NOTE(tr3buchet): setup networks on destination host
3876         self.network_api.setup_networks_on_host(context, instance,
3877                                                 migration['dest_compute'])
3878 
3879         migration_p = obj_base.obj_to_primitive(migration)
3880         self.network_api.migrate_instance_finish(context,
3881                                                  instance,
3882                                                  migration_p)
3883 
3884         network_info = self.network_api.get_instance_nw_info(context, instance)
3885 
3886         instance.task_state = task_states.RESIZE_FINISH
3887         instance.save(expected_task_state=task_states.RESIZE_MIGRATED)
3888 
3889         self._notify_about_instance_usage(
3890             context, instance, "finish_resize.start",
3891             network_info=network_info)
3892 
3893         block_device_info = self._get_instance_block_device_info(
3894                             context, instance, refresh_conn_info=True)
3895 
3896         # NOTE(mriedem): If the original vm_state was STOPPED, we don't
3897         # automatically power on the instance after it's migrated
3898         power_on = old_vm_state != vm_states.STOPPED
3899 
3900         try:
3901             self.driver.finish_migration(context, migration, instance,
3902                                          disk_info,
3903                                          network_info,
3904                                          image_meta, resize_instance,
3905                                          block_device_info, power_on)
3906         except Exception:
3907             with excutils.save_and_reraise_exception():
3908                 if old_instance_type_id != new_instance_type_id:
3909                     self._set_instance_info(instance,
3910                                             old_instance_type)
3911 
3912         migration.status = 'finished'
3913         with migration.obj_as_admin():
3914             migration.save()
3915 
3916         instance.vm_state = vm_states.RESIZED
3917         instance.task_state = None
3918         instance.launched_at = timeutils.utcnow()
3919         instance.save(expected_task_state=task_states.RESIZE_FINISH)
3920 
3921         self._update_scheduler_instance_info(context, instance)
3922         self._notify_about_instance_usage(
3923             context, instance, "finish_resize.end",
3924             network_info=network_info)
3925 
3926     @wrap_exception()
3927     @reverts_task_state
3928     @wrap_instance_event(prefix='compute')
3929     @errors_out_migration
3930     @wrap_instance_fault
3931     def finish_resize(self, context, disk_info, image, instance,
3932                       reservations, migration):
3933         """Completes the migration process.
3934 
3935         Sets up the newly transferred disk and turns on the instance at its
3936         new host machine.
3937 
3938         """
3939         quotas = objects.Quotas.from_reservations(context,
3940                                                   reservations,
3941                                                   instance=instance)
3942         try:
3943             image_meta = objects.ImageMeta.from_dict(image)
3944             self._finish_resize(context, instance, migration,
3945                                 disk_info, image_meta)
3946             quotas.commit()
3947         except Exception:
3948             LOG.exception(_LE('Setting instance vm_state to ERROR'),
3949                           instance=instance)
3950             with excutils.save_and_reraise_exception():
3951                 try:
3952                     quotas.rollback()
3953                 except Exception:
3954                     LOG.exception(_LE("Failed to rollback quota for failed "
3955                                       "finish_resize"),
3956                                   instance=instance)
3957                 self._set_instance_obj_error_state(context, instance)
3958 
3959     @wrap_exception()
3960     @wrap_instance_fault
3961     def add_fixed_ip_to_instance(self, context, network_id, instance):
3962         """Calls network_api to add new fixed_ip to instance
3963         then injects the new network info and resets instance networking.
3964 
3965         """
3966         self._notify_about_instance_usage(
3967                 context, instance, "create_ip.start")
3968 
3969         network_info = self.network_api.add_fixed_ip_to_instance(context,
3970                                                                  instance,
3971                                                                  network_id)
3972         self._inject_network_info(context, instance, network_info)
3973         self.reset_network(context, instance)
3974 
3975         # NOTE(russellb) We just want to bump updated_at.  See bug 1143466.
3976         instance.updated_at = timeutils.utcnow()
3977         instance.save()
3978 
3979         self._notify_about_instance_usage(
3980             context, instance, "create_ip.end", network_info=network_info)
3981 
3982     @wrap_exception()
3983     @wrap_instance_fault
3984     def remove_fixed_ip_from_instance(self, context, address, instance):
3985         """Calls network_api to remove existing fixed_ip from instance
3986         by injecting the altered network info and resetting
3987         instance networking.
3988         """
3989         self._notify_about_instance_usage(
3990                 context, instance, "delete_ip.start")
3991 
3992         network_info = self.network_api.remove_fixed_ip_from_instance(context,
3993                                                                       instance,
3994                                                                       address)
3995         self._inject_network_info(context, instance, network_info)
3996         self.reset_network(context, instance)
3997 
3998         # NOTE(russellb) We just want to bump updated_at.  See bug 1143466.
3999         instance.updated_at = timeutils.utcnow()
4000         instance.save()
4001 
4002         self._notify_about_instance_usage(
4003             context, instance, "delete_ip.end", network_info=network_info)
4004 
4005     @wrap_exception()
4006     @reverts_task_state
4007     @wrap_instance_event(prefix='compute')
4008     @wrap_instance_fault
4009     def pause_instance(self, context, instance):
4010         """Pause an instance on this host."""
4011         context = context.elevated()
4012         LOG.info(_LI('Pausing'), instance=instance)
4013         self._notify_about_instance_usage(context, instance, 'pause.start')
4014         compute_utils.notify_about_instance_action(context, instance,
4015                self.host, action=fields.NotificationAction.PAUSE,
4016                phase=fields.NotificationPhase.START)
4017         self.driver.pause(instance)
4018         instance.power_state = self._get_power_state(context, instance)
4019         instance.vm_state = vm_states.PAUSED
4020         instance.task_state = None
4021         instance.save(expected_task_state=task_states.PAUSING)
4022         self._notify_about_instance_usage(context, instance, 'pause.end')
4023         compute_utils.notify_about_instance_action(context, instance,
4024                self.host, action=fields.NotificationAction.PAUSE,
4025                phase=fields.NotificationPhase.END)
4026 
4027     @wrap_exception()
4028     @reverts_task_state
4029     @wrap_instance_event(prefix='compute')
4030     @wrap_instance_fault
4031     def unpause_instance(self, context, instance):
4032         """Unpause a paused instance on this host."""
4033         context = context.elevated()
4034         LOG.info(_LI('Unpausing'), instance=instance)
4035         self._notify_about_instance_usage(context, instance, 'unpause.start')
4036         self.driver.unpause(instance)
4037         instance.power_state = self._get_power_state(context, instance)
4038         instance.vm_state = vm_states.ACTIVE
4039         instance.task_state = None
4040         instance.save(expected_task_state=task_states.UNPAUSING)
4041         self._notify_about_instance_usage(context, instance, 'unpause.end')
4042 
4043     @wrap_exception()
4044     def host_power_action(self, context, action):
4045         """Reboots, shuts down or powers up the host."""
4046         return self.driver.host_power_action(action)
4047 
4048     @wrap_exception()
4049     def host_maintenance_mode(self, context, host, mode):
4050         """Start/Stop host maintenance window. On start, it triggers
4051         guest VMs evacuation.
4052         """
4053         return self.driver.host_maintenance_mode(host, mode)
4054 
4055     @wrap_exception()
4056     def set_host_enabled(self, context, enabled):
4057         """Sets the specified host's ability to accept new instances."""
4058         return self.driver.set_host_enabled(enabled)
4059 
4060     @wrap_exception()
4061     def get_host_uptime(self, context):
4062         """Returns the result of calling "uptime" on the target host."""
4063         return self.driver.get_host_uptime()
4064 
4065     @wrap_exception()
4066     @wrap_instance_fault
4067     def get_diagnostics(self, context, instance):
4068         """Retrieve diagnostics for an instance on this host."""
4069         current_power_state = self._get_power_state(context, instance)
4070         if current_power_state == power_state.RUNNING:
4071             LOG.info(_LI("Retrieving diagnostics"), instance=instance)
4072             return self.driver.get_diagnostics(instance)
4073         else:
4074             raise exception.InstanceInvalidState(
4075                 attr='power state',
4076                 instance_uuid=instance.uuid,
4077                 state=power_state.STATE_MAP[instance.power_state],
4078                 method='get_diagnostics')
4079 
4080     # TODO(alaski): Remove object_compat for RPC version 5.0
4081     @object_compat
4082     @wrap_exception()
4083     @wrap_instance_fault
4084     def get_instance_diagnostics(self, context, instance):
4085         """Retrieve diagnostics for an instance on this host."""
4086         current_power_state = self._get_power_state(context, instance)
4087         if current_power_state == power_state.RUNNING:
4088             LOG.info(_LI("Retrieving diagnostics"), instance=instance)
4089             diags = self.driver.get_instance_diagnostics(instance)
4090             return diags.serialize()
4091         else:
4092             raise exception.InstanceInvalidState(
4093                 attr='power state',
4094                 instance_uuid=instance.uuid,
4095                 state=power_state.STATE_MAP[instance.power_state],
4096                 method='get_diagnostics')
4097 
4098     @wrap_exception()
4099     @reverts_task_state
4100     @wrap_instance_event(prefix='compute')
4101     @wrap_instance_fault
4102     def suspend_instance(self, context, instance):
4103         """Suspend the given instance."""
4104         context = context.elevated()
4105 
4106         # Store the old state
4107         instance.system_metadata['old_vm_state'] = instance.vm_state
4108         self._notify_about_instance_usage(context, instance, 'suspend.start')
4109         compute_utils.notify_about_instance_action(context, instance,
4110                 self.host, action=fields.NotificationAction.SUSPEND,
4111                 phase=fields.NotificationPhase.START)
4112         with self._error_out_instance_on_exception(context, instance,
4113              instance_state=instance.vm_state):
4114             self.driver.suspend(context, instance)
4115         instance.power_state = self._get_power_state(context, instance)
4116         instance.vm_state = vm_states.SUSPENDED
4117         instance.task_state = None
4118         instance.save(expected_task_state=task_states.SUSPENDING)
4119         self._notify_about_instance_usage(context, instance, 'suspend.end')
4120         compute_utils.notify_about_instance_action(context, instance,
4121                 self.host, action=fields.NotificationAction.SUSPEND,
4122                 phase=fields.NotificationPhase.END)
4123 
4124     @wrap_exception()
4125     @reverts_task_state
4126     @wrap_instance_event(prefix='compute')
4127     @wrap_instance_fault
4128     def resume_instance(self, context, instance):
4129         """Resume the given suspended instance."""
4130         context = context.elevated()
4131         LOG.info(_LI('Resuming'), instance=instance)
4132 
4133         self._notify_about_instance_usage(context, instance, 'resume.start')
4134         network_info = self.network_api.get_instance_nw_info(context, instance)
4135         block_device_info = self._get_instance_block_device_info(
4136                             context, instance)
4137 
4138         with self._error_out_instance_on_exception(context, instance,
4139              instance_state=instance.vm_state):
4140             self.driver.resume(context, instance, network_info,
4141                                block_device_info)
4142 
4143         instance.power_state = self._get_power_state(context, instance)
4144 
4145         # We default to the ACTIVE state for backwards compatibility
4146         instance.vm_state = instance.system_metadata.pop('old_vm_state',
4147                                                          vm_states.ACTIVE)
4148 
4149         instance.task_state = None
4150         instance.save(expected_task_state=task_states.RESUMING)
4151         self._notify_about_instance_usage(context, instance, 'resume.end')
4152 
4153     @wrap_exception()
4154     @reverts_task_state
4155     @wrap_instance_event(prefix='compute')
4156     @wrap_instance_fault
4157     def shelve_instance(self, context, instance, image_id,
4158                         clean_shutdown):
4159         """Shelve an instance.
4160 
4161         This should be used when you want to take a snapshot of the instance.
4162         It also adds system_metadata that can be used by a periodic task to
4163         offload the shelved instance after a period of time.
4164 
4165         :param context: request context
4166         :param instance: an Instance object
4167         :param image_id: an image id to snapshot to.
4168         :param clean_shutdown: give the GuestOS a chance to stop
4169         """
4170 
4171         @utils.synchronized(instance.uuid)
4172         def do_shelve_instance():
4173             self._shelve_instance(context, instance, image_id, clean_shutdown)
4174         do_shelve_instance()
4175 
4176     def _shelve_instance(self, context, instance, image_id,
4177                          clean_shutdown):
4178         LOG.info(_LI('Shelving'), instance=instance)
4179         compute_utils.notify_usage_exists(self.notifier, context, instance,
4180                                           current_period=True)
4181         self._notify_about_instance_usage(context, instance, 'shelve.start')
4182         compute_utils.notify_about_instance_action(context, instance,
4183                 self.host, action=fields.NotificationAction.SHELVE,
4184                 phase=fields.NotificationPhase.START)
4185 
4186         def update_task_state(task_state, expected_state=task_states.SHELVING):
4187             shelving_state_map = {
4188                     task_states.IMAGE_PENDING_UPLOAD:
4189                         task_states.SHELVING_IMAGE_PENDING_UPLOAD,
4190                     task_states.IMAGE_UPLOADING:
4191                         task_states.SHELVING_IMAGE_UPLOADING,
4192                     task_states.SHELVING: task_states.SHELVING}
4193             task_state = shelving_state_map[task_state]
4194             expected_state = shelving_state_map[expected_state]
4195             instance.task_state = task_state
4196             instance.save(expected_task_state=expected_state)
4197 
4198         self._power_off_instance(context, instance, clean_shutdown)
4199         self.driver.snapshot(context, instance, image_id, update_task_state)
4200 
4201         instance.system_metadata['shelved_at'] = timeutils.utcnow().isoformat()
4202         instance.system_metadata['shelved_image_id'] = image_id
4203         instance.system_metadata['shelved_host'] = self.host
4204         instance.vm_state = vm_states.SHELVED
4205         instance.task_state = None
4206         if CONF.shelved_offload_time == 0:
4207             instance.task_state = task_states.SHELVING_OFFLOADING
4208         instance.power_state = self._get_power_state(context, instance)
4209         instance.save(expected_task_state=[
4210                 task_states.SHELVING,
4211                 task_states.SHELVING_IMAGE_UPLOADING])
4212 
4213         self._notify_about_instance_usage(context, instance, 'shelve.end')
4214         compute_utils.notify_about_instance_action(context, instance,
4215                 self.host, action=fields.NotificationAction.SHELVE,
4216                 phase=fields.NotificationPhase.END)
4217 
4218         if CONF.shelved_offload_time == 0:
4219             self._shelve_offload_instance(context, instance,
4220                                           clean_shutdown=False)
4221 
4222     @wrap_exception()
4223     @reverts_task_state
4224     @wrap_instance_fault
4225     def shelve_offload_instance(self, context, instance, clean_shutdown):
4226         """Remove a shelved instance from the hypervisor.
4227 
4228         This frees up those resources for use by other instances, but may lead
4229         to slower unshelve times for this instance.  This method is used by
4230         volume backed instances since restoring them doesn't involve the
4231         potentially large download of an image.
4232 
4233         :param context: request context
4234         :param instance: nova.objects.instance.Instance
4235         :param clean_shutdown: give the GuestOS a chance to stop
4236         """
4237 
4238         @utils.synchronized(instance.uuid)
4239         def do_shelve_offload_instance():
4240             self._shelve_offload_instance(context, instance, clean_shutdown)
4241         do_shelve_offload_instance()
4242 
4243     def _shelve_offload_instance(self, context, instance, clean_shutdown):
4244         LOG.info(_LI('Shelve offloading'), instance=instance)
4245         self._notify_about_instance_usage(context, instance,
4246                 'shelve_offload.start')
4247 
4248         self._power_off_instance(context, instance, clean_shutdown)
4249         current_power_state = self._get_power_state(context, instance)
4250 
4251         self.network_api.cleanup_instance_network_on_host(context, instance,
4252                                                           instance.host)
4253         network_info = self.network_api.get_instance_nw_info(context, instance)
4254         block_device_info = self._get_instance_block_device_info(context,
4255                                                                  instance)
4256         self.driver.destroy(context, instance, network_info,
4257                 block_device_info)
4258 
4259         instance.power_state = current_power_state
4260         # NOTE(mriedem): The vm_state has to be set before updating the
4261         # resource tracker, see vm_states.ALLOW_RESOURCE_REMOVAL. The host/node
4262         # values cannot be nulled out until after updating the resource tracker
4263         # though.
4264         instance.vm_state = vm_states.SHELVED_OFFLOADED
4265         instance.task_state = None
4266         instance.save(expected_task_state=[task_states.SHELVING,
4267                                            task_states.SHELVING_OFFLOADING])
4268 
4269         # NOTE(ndipanov): Free resources from the resource tracker
4270         self._update_resource_tracker(context, instance)
4271 
4272         # NOTE(sfinucan): RPC calls should no longer be attempted against this
4273         # instance, so ensure any calls result in errors
4274         self._nil_out_instance_obj_host_and_node(instance)
4275         instance.save(expected_task_state=None)
4276 
4277         self._delete_scheduler_instance_info(context, instance.uuid)
4278         self._notify_about_instance_usage(context, instance,
4279                 'shelve_offload.end')
4280 
4281     @wrap_exception()
4282     @reverts_task_state
4283     @wrap_instance_event(prefix='compute')
4284     @wrap_instance_fault
4285     def unshelve_instance(self, context, instance, image,
4286                           filter_properties, node):
4287         """Unshelve the instance.
4288 
4289         :param context: request context
4290         :param instance: a nova.objects.instance.Instance object
4291         :param image: an image to build from.  If None we assume a
4292             volume backed instance.
4293         :param filter_properties: dict containing limits, retry info etc.
4294         :param node: target compute node
4295         """
4296         if filter_properties is None:
4297             filter_properties = {}
4298 
4299         @utils.synchronized(instance.uuid)
4300         def do_unshelve_instance():
4301             self._unshelve_instance(context, instance, image,
4302                                     filter_properties, node)
4303         do_unshelve_instance()
4304 
4305     def _unshelve_instance_key_scrub(self, instance):
4306         """Remove data from the instance that may cause side effects."""
4307         cleaned_keys = dict(
4308                 key_data=instance.key_data,
4309                 auto_disk_config=instance.auto_disk_config)
4310         instance.key_data = None
4311         instance.auto_disk_config = False
4312         return cleaned_keys
4313 
4314     def _unshelve_instance_key_restore(self, instance, keys):
4315         """Restore previously scrubbed keys before saving the instance."""
4316         instance.update(keys)
4317 
4318     def _unshelve_instance(self, context, instance, image, filter_properties,
4319                            node):
4320         LOG.info(_LI('Unshelving'), instance=instance)
4321         self._notify_about_instance_usage(context, instance, 'unshelve.start')
4322         instance.task_state = task_states.SPAWNING
4323         instance.save()
4324 
4325         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
4326                 context, instance.uuid)
4327         block_device_info = self._prep_block_device(context, instance, bdms,
4328                                                     do_check_attach=False)
4329         scrubbed_keys = self._unshelve_instance_key_scrub(instance)
4330 
4331         if node is None:
4332             node = self.driver.get_available_nodes()[0]
4333             LOG.debug('No node specified, defaulting to %s', node,
4334                       instance=instance)
4335 
4336         rt = self._get_resource_tracker(node)
4337         limits = filter_properties.get('limits', {})
4338 
4339         shelved_image_ref = instance.image_ref
4340         if image:
4341             instance.image_ref = image['id']
4342             image_meta = objects.ImageMeta.from_dict(image)
4343         else:
4344             image_meta = objects.ImageMeta.from_dict(
4345                 utils.get_image_from_system_metadata(
4346                     instance.system_metadata))
4347 
4348         self.network_api.setup_instance_network_on_host(context, instance,
4349                                                         self.host)
4350         network_info = self.network_api.get_instance_nw_info(context, instance)
4351         try:
4352             with rt.instance_claim(context, instance, limits):
4353                 self.driver.spawn(context, instance, image_meta,
4354                                   injected_files=[],
4355                                   admin_password=None,
4356                                   network_info=network_info,
4357                                   block_device_info=block_device_info)
4358         except Exception:
4359             with excutils.save_and_reraise_exception():
4360                 LOG.exception(_LE('Instance failed to spawn'),
4361                               instance=instance)
4362 
4363         if image:
4364             instance.image_ref = shelved_image_ref
4365             self._delete_snapshot_of_shelved_instance(context, instance,
4366                                                       image['id'])
4367 
4368         self._unshelve_instance_key_restore(instance, scrubbed_keys)
4369         self._update_instance_after_spawn(context, instance)
4370         # Delete system_metadata for a shelved instance
4371         compute_utils.remove_shelved_keys_from_system_metadata(instance)
4372 
4373         instance.save(expected_task_state=task_states.SPAWNING)
4374         self._update_scheduler_instance_info(context, instance)
4375         self._notify_about_instance_usage(context, instance, 'unshelve.end')
4376 
4377     @messaging.expected_exceptions(NotImplementedError)
4378     @wrap_instance_fault
4379     def reset_network(self, context, instance):
4380         """Reset networking on the given instance."""
4381         LOG.debug('Reset network', instance=instance)
4382         self.driver.reset_network(instance)
4383 
4384     def _inject_network_info(self, context, instance, network_info):
4385         """Inject network info for the given instance."""
4386         LOG.debug('Inject network info', instance=instance)
4387         LOG.debug('network_info to inject: |%s|', network_info,
4388                   instance=instance)
4389 
4390         self.driver.inject_network_info(instance,
4391                                         network_info)
4392 
4393     @wrap_instance_fault
4394     def inject_network_info(self, context, instance):
4395         """Inject network info, but don't return the info."""
4396         network_info = self.network_api.get_instance_nw_info(context, instance)
4397         self._inject_network_info(context, instance, network_info)
4398 
4399     @messaging.expected_exceptions(NotImplementedError,
4400                                    exception.ConsoleNotAvailable,
4401                                    exception.InstanceNotFound)
4402     @wrap_exception()
4403     @wrap_instance_fault
4404     def get_console_output(self, context, instance, tail_length):
4405         """Send the console output for the given instance."""
4406         context = context.elevated()
4407         LOG.info(_LI("Get console output"), instance=instance)
4408         output = self.driver.get_console_output(context, instance)
4409 
4410         if type(output) is six.text_type:
4411             output = six.b(output)
4412 
4413         if tail_length is not None:
4414             output = self._tail_log(output, tail_length)
4415 
4416         return output.decode('ascii', 'replace')
4417 
4418     def _tail_log(self, log, length):
4419         try:
4420             length = int(length)
4421         except ValueError:
4422             length = 0
4423 
4424         if length == 0:
4425             return b''
4426         else:
4427             return b'\n'.join(log.split(b'\n')[-int(length):])
4428 
4429     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
4430                                    exception.InstanceNotReady,
4431                                    exception.InstanceNotFound,
4432                                    exception.ConsoleTypeUnavailable,
4433                                    NotImplementedError)
4434     @wrap_exception()
4435     @wrap_instance_fault
4436     def get_vnc_console(self, context, console_type, instance):
4437         """Return connection information for a vnc console."""
4438         context = context.elevated()
4439         LOG.debug("Getting vnc console", instance=instance)
4440         token = uuidutils.generate_uuid()
4441 
4442         if not CONF.vnc.enabled:
4443             raise exception.ConsoleTypeUnavailable(console_type=console_type)
4444 
4445         if console_type == 'novnc':
4446             # For essex, novncproxy_base_url must include the full path
4447             # including the html file (like http://myhost/vnc_auto.html)
4448             access_url = '%s?token=%s' % (CONF.vnc.novncproxy_base_url, token)
4449         elif console_type == 'xvpvnc':
4450             access_url = '%s?token=%s' % (CONF.vnc.xvpvncproxy_base_url, token)
4451         else:
4452             raise exception.ConsoleTypeInvalid(console_type=console_type)
4453 
4454         try:
4455             # Retrieve connect info from driver, and then decorate with our
4456             # access info token
4457             console = self.driver.get_vnc_console(context, instance)
4458             connect_info = console.get_connection_info(token, access_url)
4459         except exception.InstanceNotFound:
4460             if instance.vm_state != vm_states.BUILDING:
4461                 raise
4462             raise exception.InstanceNotReady(instance_id=instance.uuid)
4463 
4464         return connect_info
4465 
4466     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
4467                                    exception.InstanceNotReady,
4468                                    exception.InstanceNotFound,
4469                                    exception.ConsoleTypeUnavailable,
4470                                    NotImplementedError)
4471     @wrap_exception()
4472     @wrap_instance_fault
4473     def get_spice_console(self, context, console_type, instance):
4474         """Return connection information for a spice console."""
4475         context = context.elevated()
4476         LOG.debug("Getting spice console", instance=instance)
4477         token = uuidutils.generate_uuid()
4478 
4479         if not CONF.spice.enabled:
4480             raise exception.ConsoleTypeUnavailable(console_type=console_type)
4481 
4482         if console_type == 'spice-html5':
4483             # For essex, spicehtml5proxy_base_url must include the full path
4484             # including the html file (like http://myhost/spice_auto.html)
4485             access_url = '%s?token=%s' % (CONF.spice.html5proxy_base_url,
4486                                           token)
4487         else:
4488             raise exception.ConsoleTypeInvalid(console_type=console_type)
4489 
4490         try:
4491             # Retrieve connect info from driver, and then decorate with our
4492             # access info token
4493             console = self.driver.get_spice_console(context, instance)
4494             connect_info = console.get_connection_info(token, access_url)
4495         except exception.InstanceNotFound:
4496             if instance.vm_state != vm_states.BUILDING:
4497                 raise
4498             raise exception.InstanceNotReady(instance_id=instance.uuid)
4499 
4500         return connect_info
4501 
4502     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
4503                                    exception.InstanceNotReady,
4504                                    exception.InstanceNotFound,
4505                                    exception.ConsoleTypeUnavailable,
4506                                    NotImplementedError)
4507     @wrap_exception()
4508     @wrap_instance_fault
4509     def get_rdp_console(self, context, console_type, instance):
4510         """Return connection information for a RDP console."""
4511         context = context.elevated()
4512         LOG.debug("Getting RDP console", instance=instance)
4513         token = uuidutils.generate_uuid()
4514 
4515         if not CONF.rdp.enabled:
4516             raise exception.ConsoleTypeUnavailable(console_type=console_type)
4517 
4518         if console_type == 'rdp-html5':
4519             access_url = '%s?token=%s' % (CONF.rdp.html5_proxy_base_url,
4520                                           token)
4521         else:
4522             raise exception.ConsoleTypeInvalid(console_type=console_type)
4523 
4524         try:
4525             # Retrieve connect info from driver, and then decorate with our
4526             # access info token
4527             console = self.driver.get_rdp_console(context, instance)
4528             connect_info = console.get_connection_info(token, access_url)
4529         except exception.InstanceNotFound:
4530             if instance.vm_state != vm_states.BUILDING:
4531                 raise
4532             raise exception.InstanceNotReady(instance_id=instance.uuid)
4533 
4534         return connect_info
4535 
4536     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
4537                                    exception.InstanceNotReady,
4538                                    exception.InstanceNotFound,
4539                                    exception.ConsoleTypeUnavailable,
4540                                    NotImplementedError)
4541     @wrap_exception()
4542     @wrap_instance_fault
4543     def get_mks_console(self, context, console_type, instance):
4544         """Return connection information for a MKS console."""
4545         context = context.elevated()
4546         LOG.debug("Getting MKS console", instance=instance)
4547         token = uuidutils.generate_uuid()
4548 
4549         if not CONF.mks.enabled:
4550             raise exception.ConsoleTypeUnavailable(console_type=console_type)
4551 
4552         if console_type == 'webmks':
4553             access_url = '%s?token=%s' % (CONF.mks.mksproxy_base_url,
4554                                           token)
4555         else:
4556             raise exception.ConsoleTypeInvalid(console_type=console_type)
4557 
4558         try:
4559             # Retrieve connect info from driver, and then decorate with our
4560             # access info token
4561             console = self.driver.get_mks_console(context, instance)
4562             connect_info = console.get_connection_info(token, access_url)
4563         except exception.InstanceNotFound:
4564             if instance.vm_state != vm_states.BUILDING:
4565                 raise
4566             raise exception.InstanceNotReady(instance_id=instance.uuid)
4567 
4568         return connect_info
4569 
4570     @messaging.expected_exceptions(
4571         exception.ConsoleTypeInvalid,
4572         exception.InstanceNotReady,
4573         exception.InstanceNotFound,
4574         exception.ConsoleTypeUnavailable,
4575         exception.SocketPortRangeExhaustedException,
4576         exception.ImageSerialPortNumberInvalid,
4577         exception.ImageSerialPortNumberExceedFlavorValue,
4578         NotImplementedError)
4579     @wrap_exception()
4580     @wrap_instance_fault
4581     def get_serial_console(self, context, console_type, instance):
4582         """Returns connection information for a serial console."""
4583 
4584         LOG.debug("Getting serial console", instance=instance)
4585 
4586         if not CONF.serial_console.enabled:
4587             raise exception.ConsoleTypeUnavailable(console_type=console_type)
4588 
4589         context = context.elevated()
4590 
4591         token = uuidutils.generate_uuid()
4592         access_url = '%s?token=%s' % (CONF.serial_console.base_url, token)
4593 
4594         try:
4595             # Retrieve connect info from driver, and then decorate with our
4596             # access info token
4597             console = self.driver.get_serial_console(context, instance)
4598             connect_info = console.get_connection_info(token, access_url)
4599         except exception.InstanceNotFound:
4600             if instance.vm_state != vm_states.BUILDING:
4601                 raise
4602             raise exception.InstanceNotReady(instance_id=instance.uuid)
4603 
4604         return connect_info
4605 
4606     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
4607                                    exception.InstanceNotReady,
4608                                    exception.InstanceNotFound)
4609     @wrap_exception()
4610     @wrap_instance_fault
4611     def validate_console_port(self, ctxt, instance, port, console_type):
4612         if console_type == "spice-html5":
4613             console_info = self.driver.get_spice_console(ctxt, instance)
4614         elif console_type == "rdp-html5":
4615             console_info = self.driver.get_rdp_console(ctxt, instance)
4616         elif console_type == "serial":
4617             console_info = self.driver.get_serial_console(ctxt, instance)
4618         elif console_type == "webmks":
4619             console_info = self.driver.get_mks_console(ctxt, instance)
4620         else:
4621             console_info = self.driver.get_vnc_console(ctxt, instance)
4622 
4623         return console_info.port == port
4624 
4625     @wrap_exception()
4626     @reverts_task_state
4627     @wrap_instance_fault
4628     def reserve_block_device_name(self, context, instance, device,
4629                                   volume_id, disk_bus, device_type):
4630         @utils.synchronized(instance.uuid)
4631         def do_reserve():
4632             bdms = (
4633                 objects.BlockDeviceMappingList.get_by_instance_uuid(
4634                     context, instance.uuid))
4635 
4636             # NOTE(ndipanov): We need to explicitly set all the fields on the
4637             #                 object so that obj_load_attr does not fail
4638             new_bdm = objects.BlockDeviceMapping(
4639                     context=context,
4640                     source_type='volume', destination_type='volume',
4641                     instance_uuid=instance.uuid, boot_index=None,
4642                     volume_id=volume_id,
4643                     device_name=device, guest_format=None,
4644                     disk_bus=disk_bus, device_type=device_type)
4645 
4646             new_bdm.device_name = self._get_device_name_for_instance(
4647                     instance, bdms, new_bdm)
4648 
4649             # NOTE(vish): create bdm here to avoid race condition
4650             new_bdm.create()
4651             return new_bdm
4652 
4653         return do_reserve()
4654 
4655     @wrap_exception()
4656     @wrap_instance_fault
4657     def attach_volume(self, context, instance, bdm):
4658         """Attach a volume to an instance."""
4659         driver_bdm = driver_block_device.convert_volume(bdm)
4660 
4661         @utils.synchronized(instance.uuid)
4662         def do_attach_volume(context, instance, driver_bdm):
4663             try:
4664                 return self._attach_volume(context, instance, driver_bdm)
4665             except Exception:
4666                 with excutils.save_and_reraise_exception():
4667                     bdm.destroy()
4668 
4669         do_attach_volume(context, instance, driver_bdm)
4670 
4671     def _attach_volume(self, context, instance, bdm):
4672         context = context.elevated()
4673         LOG.info(_LI('Attaching volume %(volume_id)s to %(mountpoint)s'),
4674                   {'volume_id': bdm.volume_id,
4675                   'mountpoint': bdm['mount_device']},
4676                  instance=instance)
4677         try:
4678             bdm.attach(context, instance, self.volume_api, self.driver,
4679                        do_check_attach=False, do_driver_attach=True)
4680         except Exception:
4681             with excutils.save_and_reraise_exception():
4682                 LOG.exception(_LE("Failed to attach %(volume_id)s "
4683                                   "at %(mountpoint)s"),
4684                               {'volume_id': bdm.volume_id,
4685                                'mountpoint': bdm['mount_device']},
4686                               instance=instance)
4687                 self.volume_api.unreserve_volume(context, bdm.volume_id)
4688 
4689         info = {'volume_id': bdm.volume_id}
4690         self._notify_about_instance_usage(
4691             context, instance, "volume.attach", extra_usage_info=info)
4692 
4693     def _driver_detach_volume(self, context, instance, bdm, connection_info):
4694         """Do the actual driver detach using block device mapping."""
4695         mp = bdm.device_name
4696         volume_id = bdm.volume_id
4697 
4698         LOG.info(_LI('Detach volume %(volume_id)s from mountpoint %(mp)s'),
4699                   {'volume_id': volume_id, 'mp': mp},
4700                   instance=instance)
4701 
4702         try:
4703             if not self.driver.instance_exists(instance):
4704                 LOG.warning(_LW('Detaching volume from unknown instance'),
4705                             instance=instance)
4706 
4707             encryption = encryptors.get_encryption_metadata(
4708                 context, self.volume_api, volume_id, connection_info)
4709 
4710             self.driver.detach_volume(connection_info,
4711                                       instance,
4712                                       mp,
4713                                       encryption=encryption)
4714         except exception.DiskNotFound as err:
4715             LOG.warning(_LW('Ignoring DiskNotFound exception while detaching '
4716                             'volume %(volume_id)s from %(mp)s: %(err)s'),
4717                         {'volume_id': volume_id, 'mp': mp, 'err': err},
4718                         instance=instance)
4719         except Exception:
4720             with excutils.save_and_reraise_exception():
4721                 LOG.exception(_LE('Failed to detach volume %(volume_id)s '
4722                                   'from %(mp)s'),
4723                               {'volume_id': volume_id, 'mp': mp},
4724                               instance=instance)
4725                 self.volume_api.roll_detaching(context, volume_id)
4726 
4727     def _detach_volume(self, context, volume_id, instance, destroy_bdm=True,
4728                        attachment_id=None):
4729         """Detach a volume from an instance.
4730 
4731         :param context: security context
4732         :param volume_id: the volume id
4733         :param instance: the Instance object to detach the volume from
4734         :param destroy_bdm: if True, the corresponding BDM entry will be marked
4735                             as deleted. Disabling this is useful for operations
4736                             like rebuild, when we don't want to destroy BDM
4737 
4738         """
4739 
4740         bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
4741                 context, volume_id, instance.uuid)
4742         if CONF.volume_usage_poll_interval > 0:
4743             vol_stats = []
4744             mp = bdm.device_name
4745             # Handle bootable volumes which will not contain /dev/
4746             if '/dev/' in mp:
4747                 mp = mp[5:]
4748             try:
4749                 vol_stats = self.driver.block_stats(instance, mp)
4750             except NotImplementedError:
4751                 pass
4752 
4753             if vol_stats:
4754                 LOG.debug("Updating volume usage cache with totals",
4755                           instance=instance)
4756                 rd_req, rd_bytes, wr_req, wr_bytes, flush_ops = vol_stats
4757                 vol_usage = objects.VolumeUsage(context)
4758                 vol_usage.volume_id = volume_id
4759                 vol_usage.instance_uuid = instance.uuid
4760                 vol_usage.project_id = instance.project_id
4761                 vol_usage.user_id = instance.user_id
4762                 vol_usage.availability_zone = instance.availability_zone
4763                 vol_usage.curr_reads = rd_req
4764                 vol_usage.curr_read_bytes = rd_bytes
4765                 vol_usage.curr_writes = wr_req
4766                 vol_usage.curr_write_bytes = wr_bytes
4767                 vol_usage.save(update_totals=True)
4768                 self.notifier.info(context, 'volume.usage',
4769                                    compute_utils.usage_volume_info(vol_usage))
4770 
4771         connection_info = jsonutils.loads(bdm.connection_info)
4772         connector = self.driver.get_volume_connector(instance)
4773         if CONF.host == instance.host:
4774             # Only attempt to detach and disconnect from the volume if the
4775             # instance is currently associated with the local compute host.
4776             self._driver_detach_volume(context, instance, bdm, connection_info)
4777         elif not destroy_bdm:
4778             LOG.debug("Skipping _driver_detach_volume during remote rebuild.",
4779                       instance=instance)
4780         elif destroy_bdm:
4781             LOG.error(_LE("Unable to call for a driver detach of volume "
4782                           "%(vol_id)s due to the instance being registered to "
4783                           "the remote host %(inst_host)s."),
4784                       {'vol_id': volume_id, 'inst_host': instance.host},
4785                       instance=instance)
4786 
4787         if connection_info and not destroy_bdm and (
4788            connector.get('host') != instance.host):
4789             # If the volume is attached to another host (evacuate) then
4790             # this connector is for the wrong host. Use the connector that
4791             # was stored in connection_info instead (if we have one, and it
4792             # is for the expected host).
4793             stashed_connector = connection_info.get('connector')
4794             if not stashed_connector:
4795                 # Volume was attached before we began stashing connectors
4796                 LOG.warning(_LW("Host mismatch detected, but stashed "
4797                                 "volume connector not found. Instance host is "
4798                                 "%(ihost)s, but volume connector host is "
4799                                 "%(chost)s."),
4800                             {'ihost': instance.host,
4801                              'chost': connector.get('host')})
4802             elif stashed_connector.get('host') != instance.host:
4803                 # Unexpected error. The stashed connector is also not matching
4804                 # the needed instance host.
4805                 LOG.error(_LE("Host mismatch detected in stashed volume "
4806                               "connector. Will use local volume connector. "
4807                               "Instance host is %(ihost)s. Local volume "
4808                               "connector host is %(chost)s. Stashed volume "
4809                               "connector host is %(schost)s."),
4810                           {'ihost': instance.host,
4811                            'chost': connector.get('host'),
4812                            'schost': stashed_connector.get('host')})
4813             else:
4814                 # Fix found. Use stashed connector.
4815                 LOG.debug("Host mismatch detected. Found usable stashed "
4816                           "volume connector. Instance host is %(ihost)s. "
4817                           "Local volume connector host was %(chost)s. "
4818                           "Stashed volume connector host is %(schost)s.",
4819                           {'ihost': instance.host,
4820                            'chost': connector.get('host'),
4821                            'schost': stashed_connector.get('host')})
4822                 connector = stashed_connector
4823 
4824         self.volume_api.terminate_connection(context, volume_id, connector)
4825 
4826         if destroy_bdm:
4827             bdm.destroy()
4828 
4829         info = dict(volume_id=volume_id)
4830         self._notify_about_instance_usage(
4831             context, instance, "volume.detach", extra_usage_info=info)
4832         self.volume_api.detach(context.elevated(), volume_id, instance.uuid,
4833                                attachment_id)
4834 
4835     @wrap_exception()
4836     @wrap_instance_fault
4837     def detach_volume(self, context, volume_id, instance, attachment_id=None):
4838         """Detach a volume from an instance."""
4839 
4840         self._detach_volume(context, volume_id, instance,
4841                             attachment_id=attachment_id)
4842 
4843     def _init_volume_connection(self, context, new_volume_id,
4844                                 old_volume_id, connector, instance, bdm):
4845 
4846         new_cinfo = self.volume_api.initialize_connection(context,
4847                                                           new_volume_id,
4848                                                           connector)
4849         old_cinfo = jsonutils.loads(bdm['connection_info'])
4850         if old_cinfo and 'serial' not in old_cinfo:
4851             old_cinfo['serial'] = old_volume_id
4852         new_cinfo['serial'] = old_cinfo['serial']
4853         return (old_cinfo, new_cinfo)
4854 
4855     def _swap_volume(self, context, instance, bdm, connector,
4856                      old_volume_id, new_volume_id, resize_to):
4857         mountpoint = bdm['device_name']
4858         failed = False
4859         new_cinfo = None
4860         try:
4861             old_cinfo, new_cinfo = self._init_volume_connection(context,
4862                                                                 new_volume_id,
4863                                                                 old_volume_id,
4864                                                                 connector,
4865                                                                 instance,
4866                                                                 bdm)
4867             LOG.debug("swap_volume: Calling driver volume swap with "
4868                       "connection infos: new: %(new_cinfo)s; "
4869                       "old: %(old_cinfo)s",
4870                       {'new_cinfo': new_cinfo, 'old_cinfo': old_cinfo},
4871                       instance=instance)
4872             self.driver.swap_volume(old_cinfo, new_cinfo, instance, mountpoint,
4873                                     resize_to)
4874         except Exception:
4875             failed = True
4876             with excutils.save_and_reraise_exception():
4877                 if new_cinfo:
4878                     msg = _LE("Failed to swap volume %(old_volume_id)s "
4879                               "for %(new_volume_id)s")
4880                     LOG.exception(msg, {'old_volume_id': old_volume_id,
4881                                         'new_volume_id': new_volume_id},
4882                                   instance=instance)
4883                 else:
4884                     msg = _LE("Failed to connect to volume %(volume_id)s "
4885                               "with volume at %(mountpoint)s")
4886                     LOG.exception(msg, {'volume_id': new_volume_id,
4887                                         'mountpoint': bdm['device_name']},
4888                                   instance=instance)
4889                 self.volume_api.roll_detaching(context, old_volume_id)
4890                 self.volume_api.unreserve_volume(context, new_volume_id)
4891         finally:
4892             conn_volume = new_volume_id if failed else old_volume_id
4893             if new_cinfo:
4894                 LOG.debug("swap_volume: calling Cinder terminate_connection "
4895                           "for %(volume)s", {'volume': conn_volume},
4896                           instance=instance)
4897                 self.volume_api.terminate_connection(context,
4898                                                      conn_volume,
4899                                                      connector)
4900             # If Cinder initiated the swap, it will keep
4901             # the original ID
4902             comp_ret = self.volume_api.migrate_volume_completion(
4903                                                       context,
4904                                                       old_volume_id,
4905                                                       new_volume_id,
4906                                                       error=failed)
4907             LOG.debug("swap_volume: Cinder migrate_volume_completion "
4908                       "returned: %(comp_ret)s", {'comp_ret': comp_ret},
4909                       instance=instance)
4910 
4911         return (comp_ret, new_cinfo)
4912 
4913     @wrap_exception()
4914     @reverts_task_state
4915     @wrap_instance_fault
4916     def swap_volume(self, context, old_volume_id, new_volume_id, instance):
4917         """Swap volume for an instance."""
4918         context = context.elevated()
4919 
4920         compute_utils.notify_about_volume_swap(
4921             context, instance, self.host,
4922             fields.NotificationAction.VOLUME_SWAP,
4923             fields.NotificationPhase.START,
4924             old_volume_id, new_volume_id)
4925 
4926         bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
4927                 context, old_volume_id, instance.uuid)
4928         connector = self.driver.get_volume_connector(instance)
4929 
4930         resize_to = 0
4931         old_vol_size = self.volume_api.get(context, old_volume_id)['size']
4932         new_vol_size = self.volume_api.get(context, new_volume_id)['size']
4933         if new_vol_size > old_vol_size:
4934             resize_to = new_vol_size
4935 
4936         LOG.info(_LI('Swapping volume %(old_volume)s for %(new_volume)s'),
4937                   {'old_volume': old_volume_id, 'new_volume': new_volume_id},
4938                  instance=instance)
4939         comp_ret, new_cinfo = self._swap_volume(context, instance,
4940                                                          bdm,
4941                                                          connector,
4942                                                          old_volume_id,
4943                                                          new_volume_id,
4944                                                          resize_to)
4945 
4946         save_volume_id = comp_ret['save_volume_id']
4947 
4948         # Update bdm
4949         values = {
4950             'connection_info': jsonutils.dumps(new_cinfo),
4951             'source_type': 'volume',
4952             'destination_type': 'volume',
4953             'snapshot_id': None,
4954             'volume_id': save_volume_id,
4955             'no_device': None}
4956 
4957         if resize_to:
4958             values['volume_size'] = resize_to
4959 
4960         LOG.debug("swap_volume: Updating volume %(volume_id)s BDM record with "
4961                   "%(updates)s", {'volume_id': bdm.volume_id,
4962                                   'updates': values},
4963                   instance=instance)
4964         bdm.update(values)
4965         bdm.save()
4966 
4967         compute_utils.notify_about_volume_swap(
4968             context, instance, self.host,
4969             fields.NotificationAction.VOLUME_SWAP,
4970             fields.NotificationPhase.END,
4971             old_volume_id, new_volume_id)
4972 
4973     @wrap_exception()
4974     def remove_volume_connection(self, context, volume_id, instance):
4975         """Remove a volume connection using the volume api."""
4976         # NOTE(vish): We don't want to actually mark the volume
4977         #             detached, or delete the bdm, just remove the
4978         #             connection from this host.
4979 
4980         try:
4981             bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
4982                     context, volume_id, instance.uuid)
4983             connection_info = jsonutils.loads(bdm.connection_info)
4984             self._driver_detach_volume(context, instance, bdm, connection_info)
4985             connector = self.driver.get_volume_connector(instance)
4986             self.volume_api.terminate_connection(context, volume_id, connector)
4987         except exception.NotFound:
4988             pass
4989 
4990     @wrap_exception()
4991     @wrap_instance_fault
4992     def attach_interface(self, context, instance, network_id, port_id,
4993                          requested_ip):
4994         """Use hotplug to add an network adapter to an instance."""
4995         if not self.driver.capabilities['supports_attach_interface']:
4996             raise exception.AttachInterfaceNotSupported(
4997                 instance_id=instance.uuid)
4998         bind_host_id = self.driver.network_binding_host_id(context, instance)
4999         network_info = self.network_api.allocate_port_for_instance(
5000             context, instance, port_id, network_id, requested_ip,
5001             bind_host_id=bind_host_id)
5002         if len(network_info) != 1:
5003             LOG.error(_LE('allocate_port_for_instance returned %(ports)s '
5004                           'ports'), {'ports': len(network_info)})
5005             raise exception.InterfaceAttachFailed(
5006                     instance_uuid=instance.uuid)
5007         image_meta = objects.ImageMeta.from_instance(instance)
5008 
5009         try:
5010             self.driver.attach_interface(instance, image_meta, network_info[0])
5011         except exception.NovaException as ex:
5012             port_id = network_info[0].get('id')
5013             LOG.warning(_LW("attach interface failed , try to deallocate "
5014                          "port %(port_id)s, reason: %(msg)s"),
5015                      {'port_id': port_id, 'msg': ex},
5016                      instance=instance)
5017             try:
5018                 self.network_api.deallocate_port_for_instance(
5019                     context, instance, port_id)
5020             except Exception:
5021                 LOG.warning(_LW("deallocate port %(port_id)s failed"),
5022                              {'port_id': port_id}, instance=instance)
5023             raise exception.InterfaceAttachFailed(
5024                 instance_uuid=instance.uuid)
5025 
5026         return network_info[0]
5027 
5028     @wrap_exception()
5029     @wrap_instance_fault
5030     def detach_interface(self, context, instance, port_id):
5031         """Detach a network adapter from an instance."""
5032         network_info = instance.info_cache.network_info
5033         condemned = None
5034         for vif in network_info:
5035             if vif['id'] == port_id:
5036                 condemned = vif
5037                 break
5038         if condemned is None:
5039             raise exception.PortNotFound(_("Port %s is not "
5040                                            "attached") % port_id)
5041         try:
5042             self.driver.detach_interface(instance, condemned)
5043         except exception.NovaException as ex:
5044             LOG.warning(_LW("Detach interface failed, port_id=%(port_id)s,"
5045                             " reason: %(msg)s"),
5046                         {'port_id': port_id, 'msg': ex}, instance=instance)
5047             raise exception.InterfaceDetachFailed(instance_uuid=instance.uuid)
5048         else:
5049             try:
5050                 self.network_api.deallocate_port_for_instance(
5051                     context, instance, port_id)
5052             except Exception as ex:
5053                 with excutils.save_and_reraise_exception():
5054                     # Since this is a cast operation, log the failure for
5055                     # triage.
5056                     LOG.warning(_LW('Failed to deallocate port %(port_id)s '
5057                                     'for instance. Error: %(error)s'),
5058                                 {'port_id': port_id, 'error': ex},
5059                                 instance=instance)
5060 
5061     def _get_compute_info(self, context, host):
5062         return objects.ComputeNode.get_first_node_by_host_for_old_compat(
5063             context, host)
5064 
5065     @wrap_exception()
5066     def check_instance_shared_storage(self, ctxt, instance, data):
5067         """Check if the instance files are shared
5068 
5069         :param ctxt: security context
5070         :param instance: dict of instance data
5071         :param data: result of driver.check_instance_shared_storage_local
5072 
5073         Returns True if instance disks located on shared storage and
5074         False otherwise.
5075         """
5076         return self.driver.check_instance_shared_storage_remote(ctxt, data)
5077 
5078     @wrap_exception()
5079     @wrap_instance_event(prefix='compute')
5080     @wrap_instance_fault
5081     def check_can_live_migrate_destination(self, ctxt, instance,
5082                                            block_migration, disk_over_commit):
5083         """Check if it is possible to execute live migration.
5084 
5085         This runs checks on the destination host, and then calls
5086         back to the source host to check the results.
5087 
5088         :param context: security context
5089         :param instance: dict of instance data
5090         :param block_migration: if true, prepare for block migration
5091                                 if None, calculate it in driver
5092         :param disk_over_commit: if true, allow disk over commit
5093                                  if None, ignore disk usage checking
5094         :returns: a dict containing migration info
5095         """
5096         return self._do_check_can_live_migrate_destination(ctxt, instance,
5097                                                             block_migration,
5098                                                             disk_over_commit)
5099 
5100     def _do_check_can_live_migrate_destination(self, ctxt, instance,
5101                                                block_migration,
5102                                                disk_over_commit):
5103         src_compute_info = obj_base.obj_to_primitive(
5104             self._get_compute_info(ctxt, instance.host))
5105         dst_compute_info = obj_base.obj_to_primitive(
5106             self._get_compute_info(ctxt, CONF.host))
5107         dest_check_data = self.driver.check_can_live_migrate_destination(ctxt,
5108             instance, src_compute_info, dst_compute_info,
5109             block_migration, disk_over_commit)
5110         LOG.debug('destination check data is %s', dest_check_data)
5111         try:
5112             migrate_data = self.compute_rpcapi.\
5113                                 check_can_live_migrate_source(ctxt, instance,
5114                                                               dest_check_data)
5115         finally:
5116             self.driver.cleanup_live_migration_destination_check(ctxt,
5117                     dest_check_data)
5118         return migrate_data
5119 
5120     @wrap_exception()
5121     @wrap_instance_event(prefix='compute')
5122     @wrap_instance_fault
5123     def check_can_live_migrate_source(self, ctxt, instance, dest_check_data):
5124         """Check if it is possible to execute live migration.
5125 
5126         This checks if the live migration can succeed, based on the
5127         results from check_can_live_migrate_destination.
5128 
5129         :param ctxt: security context
5130         :param instance: dict of instance data
5131         :param dest_check_data: result of check_can_live_migrate_destination
5132         :returns: a dict containing migration info
5133         """
5134         is_volume_backed = compute_utils.is_volume_backed_instance(ctxt,
5135                                                                       instance)
5136         # TODO(tdurakov): remove dict to object conversion once RPC API version
5137         # is bumped to 5.x
5138         got_migrate_data_object = isinstance(dest_check_data,
5139                                              migrate_data_obj.LiveMigrateData)
5140         if not got_migrate_data_object:
5141             dest_check_data = \
5142                 migrate_data_obj.LiveMigrateData.detect_implementation(
5143                     dest_check_data)
5144         dest_check_data.is_volume_backed = is_volume_backed
5145         block_device_info = self._get_instance_block_device_info(
5146                             ctxt, instance, refresh_conn_info=False)
5147         result = self.driver.check_can_live_migrate_source(ctxt, instance,
5148                                                            dest_check_data,
5149                                                            block_device_info)
5150         if not got_migrate_data_object:
5151             result = result.to_legacy_dict()
5152         LOG.debug('source check data is %s', result)
5153         return result
5154 
5155     @wrap_exception()
5156     @wrap_instance_event(prefix='compute')
5157     @wrap_instance_fault
5158     def pre_live_migration(self, context, instance, block_migration, disk,
5159                            migrate_data):
5160         """Preparations for live migration at dest host.
5161 
5162         :param context: security context
5163         :param instance: dict of instance data
5164         :param block_migration: if true, prepare for block migration
5165         :param migrate_data: A dict or LiveMigrateData object holding data
5166                              required for live migration without shared
5167                              storage.
5168 
5169         """
5170         LOG.debug('pre_live_migration data is %s', migrate_data)
5171         # TODO(tdurakov): remove dict to object conversion once RPC API version
5172         # is bumped to 5.x
5173         got_migrate_data_object = isinstance(migrate_data,
5174                                              migrate_data_obj.LiveMigrateData)
5175         if not got_migrate_data_object:
5176             migrate_data = \
5177                 migrate_data_obj.LiveMigrateData.detect_implementation(
5178                     migrate_data)
5179         block_device_info = self._get_instance_block_device_info(
5180                             context, instance, refresh_conn_info=True)
5181 
5182         network_info = self.network_api.get_instance_nw_info(context, instance)
5183         self._notify_about_instance_usage(
5184                      context, instance, "live_migration.pre.start",
5185                      network_info=network_info)
5186 
5187         migrate_data = self.driver.pre_live_migration(context,
5188                                        instance,
5189                                        block_device_info,
5190                                        network_info,
5191                                        disk,
5192                                        migrate_data)
5193         LOG.debug('driver pre_live_migration data is %s', migrate_data)
5194 
5195         # NOTE(tr3buchet): setup networks on destination host
5196         self.network_api.setup_networks_on_host(context, instance,
5197                                                          self.host)
5198 
5199         # Creating filters to hypervisors and firewalls.
5200         # An example is that nova-instance-instance-xxx,
5201         # which is written to libvirt.xml(Check "virsh nwfilter-list")
5202         # This nwfilter is necessary on the destination host.
5203         # In addition, this method is creating filtering rule
5204         # onto destination host.
5205         self.driver.ensure_filtering_rules_for_instance(instance,
5206                                             network_info)
5207 
5208         self._notify_about_instance_usage(
5209                      context, instance, "live_migration.pre.end",
5210                      network_info=network_info)
5211         # TODO(tdurakov): remove dict to object conversion once RPC API version
5212         # is bumped to 5.x
5213         if not got_migrate_data_object and migrate_data:
5214             migrate_data = migrate_data.to_legacy_dict(
5215                 pre_migration_result=True)
5216             migrate_data = migrate_data['pre_live_migration_result']
5217         LOG.debug('pre_live_migration result data is %s', migrate_data)
5218         return migrate_data
5219 
5220     def _do_live_migration(self, context, dest, instance, block_migration,
5221                            migration, migrate_data):
5222         # NOTE(danms): We should enhance the RT to account for migrations
5223         # and use the status field to denote when the accounting has been
5224         # done on source/destination. For now, this is just here for status
5225         # reporting
5226         self._set_migration_status(migration, 'preparing')
5227 
5228         got_migrate_data_object = isinstance(migrate_data,
5229                                              migrate_data_obj.LiveMigrateData)
5230         if not got_migrate_data_object:
5231             migrate_data = \
5232                 migrate_data_obj.LiveMigrateData.detect_implementation(
5233                     migrate_data)
5234 
5235         try:
5236             if ('block_migration' in migrate_data and
5237                     migrate_data.block_migration):
5238                 block_device_info = self._get_instance_block_device_info(
5239                     context, instance)
5240                 disk = self.driver.get_instance_disk_info(
5241                     instance, block_device_info=block_device_info)
5242             else:
5243                 disk = None
5244 
5245             migrate_data = self.compute_rpcapi.pre_live_migration(
5246                 context, instance,
5247                 block_migration, disk, dest, migrate_data)
5248         except Exception:
5249             with excutils.save_and_reraise_exception():
5250                 LOG.exception(_LE('Pre live migration failed at %s'),
5251                               dest, instance=instance)
5252                 self._set_migration_status(migration, 'error')
5253                 self._rollback_live_migration(context, instance, dest,
5254                                               block_migration, migrate_data)
5255 
5256         self._set_migration_status(migration, 'running')
5257 
5258         if migrate_data:
5259             migrate_data.migration = migration
5260         LOG.debug('live_migration data is %s', migrate_data)
5261         try:
5262             self.driver.live_migration(context, instance, dest,
5263                                        self._post_live_migration,
5264                                        self._rollback_live_migration,
5265                                        block_migration, migrate_data)
5266         except Exception:
5267             # Executing live migration
5268             # live_migration might raises exceptions, but
5269             # nothing must be recovered in this version.
5270             LOG.exception(_LE('Live migration failed.'), instance=instance)
5271             with excutils.save_and_reraise_exception():
5272                 self._set_migration_status(migration, 'error')
5273 
5274     @wrap_exception()
5275     @wrap_instance_event(prefix='compute')
5276     @wrap_instance_fault
5277     def live_migration(self, context, dest, instance, block_migration,
5278                        migration, migrate_data):
5279         """Executing live migration.
5280 
5281         :param context: security context
5282         :param dest: destination host
5283         :param instance: a nova.objects.instance.Instance object
5284         :param block_migration: if true, prepare for block migration
5285         :param migration: an nova.objects.Migration object
5286         :param migrate_data: implementation specific params
5287 
5288         """
5289         self._set_migration_status(migration, 'queued')
5290 
5291         def dispatch_live_migration(*args, **kwargs):
5292             with self._live_migration_semaphore:
5293                 self._do_live_migration(*args, **kwargs)
5294 
5295         # NOTE(danms): We spawn here to return the RPC worker thread back to
5296         # the pool. Since what follows could take a really long time, we don't
5297         # want to tie up RPC workers.
5298         utils.spawn_n(dispatch_live_migration,
5299                       context, dest, instance,
5300                       block_migration, migration,
5301                       migrate_data)
5302 
5303     # TODO(tdurakov): migration_id is used since 4.12 rpc api version
5304     # remove migration_id parameter when the compute RPC version
5305     # is bumped to 5.x.
5306     @wrap_exception()
5307     @wrap_instance_event(prefix='compute')
5308     @wrap_instance_fault
5309     def live_migration_force_complete(self, context, instance,
5310                                       migration_id=None):
5311         """Force live migration to complete.
5312 
5313         :param context: Security context
5314         :param instance: The instance that is being migrated
5315         :param migration_id: ID of ongoing migration; is currently not used,
5316         and isn't removed for backward compatibility
5317         """
5318 
5319         self._notify_about_instance_usage(
5320             context, instance, 'live.migration.force.complete.start')
5321         self.driver.live_migration_force_complete(instance)
5322         self._notify_about_instance_usage(
5323             context, instance, 'live.migration.force.complete.end')
5324 
5325     @wrap_exception()
5326     @wrap_instance_event(prefix='compute')
5327     @wrap_instance_fault
5328     def live_migration_abort(self, context, instance, migration_id):
5329         """Abort an in-progress live migration.
5330 
5331         :param context: Security context
5332         :param instance: The instance that is being migrated
5333         :param migration_id: ID of in-progress live migration
5334 
5335         """
5336         migration = objects.Migration.get_by_id(context, migration_id)
5337         if migration.status != 'running':
5338             raise exception.InvalidMigrationState(migration_id=migration_id,
5339                     instance_uuid=instance.uuid,
5340                     state=migration.status,
5341                     method='abort live migration')
5342 
5343         self._notify_about_instance_usage(
5344             context, instance, 'live.migration.abort.start')
5345         self.driver.live_migration_abort(instance)
5346         self._notify_about_instance_usage(
5347             context, instance, 'live.migration.abort.end')
5348 
5349     def _live_migration_cleanup_flags(self, migrate_data):
5350         """Determine whether disks or instance path need to be cleaned up after
5351         live migration (at source on success, at destination on rollback)
5352 
5353         Block migration needs empty image at destination host before migration
5354         starts, so if any failure occurs, any empty images has to be deleted.
5355 
5356         Also Volume backed live migration w/o shared storage needs to delete
5357         newly created instance-xxx dir on the destination as a part of its
5358         rollback process
5359 
5360         :param migrate_data: implementation specific data
5361         :returns: (bool, bool) -- do_cleanup, destroy_disks
5362         """
5363         # NOTE(pkoniszewski): block migration specific params are set inside
5364         # migrate_data objects for drivers that expose block live migration
5365         # information (i.e. Libvirt, Xenapi and HyperV). For other drivers
5366         # cleanup is not needed.
5367         is_shared_block_storage = True
5368         is_shared_instance_path = True
5369         if isinstance(migrate_data, migrate_data_obj.LibvirtLiveMigrateData):
5370             is_shared_block_storage = migrate_data.is_shared_block_storage
5371             is_shared_instance_path = migrate_data.is_shared_instance_path
5372         elif isinstance(migrate_data, migrate_data_obj.XenapiLiveMigrateData):
5373             is_shared_block_storage = not migrate_data.block_migration
5374             is_shared_instance_path = not migrate_data.block_migration
5375         elif isinstance(migrate_data, migrate_data_obj.HyperVLiveMigrateData):
5376             is_shared_instance_path = migrate_data.is_shared_instance_path
5377             is_shared_block_storage = migrate_data.is_shared_instance_path
5378 
5379         # No instance booting at source host, but instance dir
5380         # must be deleted for preparing next block migration
5381         # must be deleted for preparing next live migration w/o shared storage
5382         do_cleanup = not is_shared_instance_path
5383         destroy_disks = not is_shared_block_storage
5384 
5385         return (do_cleanup, destroy_disks)
5386 
5387     @wrap_exception()
5388     @wrap_instance_fault
5389     def _post_live_migration(self, ctxt, instance,
5390                             dest, block_migration=False, migrate_data=None):
5391         """Post operations for live migration.
5392 
5393         This method is called from live_migration
5394         and mainly updating database record.
5395 
5396         :param ctxt: security context
5397         :param instance: instance dict
5398         :param dest: destination host
5399         :param block_migration: if true, prepare for block migration
5400         :param migrate_data: if not None, it is a dict which has data
5401         required for live migration without shared storage
5402 
5403         """
5404         LOG.info(_LI('_post_live_migration() is started..'),
5405                  instance=instance)
5406 
5407         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
5408                 ctxt, instance.uuid)
5409 
5410         # Cleanup source host post live-migration
5411         block_device_info = self._get_instance_block_device_info(
5412                             ctxt, instance, bdms=bdms)
5413         self.driver.post_live_migration(ctxt, instance, block_device_info,
5414                                         migrate_data)
5415 
5416         # Detaching volumes.
5417         connector = self.driver.get_volume_connector(instance)
5418         for bdm in bdms:
5419             # NOTE(vish): We don't want to actually mark the volume
5420             #             detached, or delete the bdm, just remove the
5421             #             connection from this host.
5422 
5423             # remove the volume connection without detaching from hypervisor
5424             # because the instance is not running anymore on the current host
5425             if bdm.is_volume:
5426                 self.volume_api.terminate_connection(ctxt, bdm.volume_id,
5427                                                      connector)
5428 
5429         # Releasing vlan.
5430         # (not necessary in current implementation?)
5431 
5432         network_info = self.network_api.get_instance_nw_info(ctxt, instance)
5433 
5434         self._notify_about_instance_usage(ctxt, instance,
5435                                           "live_migration._post.start",
5436                                           network_info=network_info)
5437         # Releasing security group ingress rule.
5438         LOG.debug('Calling driver.unfilter_instance from _post_live_migration',
5439                   instance=instance)
5440         self.driver.unfilter_instance(instance,
5441                                       network_info)
5442 
5443         migration = {'source_compute': self.host,
5444                      'dest_compute': dest, }
5445         self.network_api.migrate_instance_start(ctxt,
5446                                                 instance,
5447                                                 migration)
5448 
5449         destroy_vifs = False
5450         try:
5451             self.driver.post_live_migration_at_source(ctxt, instance,
5452                                                       network_info)
5453         except NotImplementedError as ex:
5454             LOG.debug(ex, instance=instance)
5455             # For all hypervisors other than libvirt, there is a possibility
5456             # they are unplugging networks from source node in the cleanup
5457             # method
5458             destroy_vifs = True
5459 
5460         # Define domain at destination host, without doing it,
5461         # pause/suspend/terminate do not work.
5462         self.compute_rpcapi.post_live_migration_at_destination(ctxt,
5463                 instance, block_migration, dest)
5464 
5465         do_cleanup, destroy_disks = self._live_migration_cleanup_flags(
5466                 migrate_data)
5467 
5468         if do_cleanup:
5469             LOG.debug('Calling driver.cleanup from _post_live_migration',
5470                       instance=instance)
5471             self.driver.cleanup(ctxt, instance, network_info,
5472                                 destroy_disks=destroy_disks,
5473                                 migrate_data=migrate_data,
5474                                 destroy_vifs=destroy_vifs)
5475 
5476         self.instance_events.clear_events_for_instance(instance)
5477 
5478         # NOTE(timello): make sure we update available resources on source
5479         # host even before next periodic task.
5480         self.update_available_resource(ctxt)
5481 
5482         self._update_scheduler_instance_info(ctxt, instance)
5483         self._notify_about_instance_usage(ctxt, instance,
5484                                           "live_migration._post.end",
5485                                           network_info=network_info)
5486         LOG.info(_LI('Migrating instance to %s finished successfully.'),
5487                  dest, instance=instance)
5488         LOG.info(_LI("You may see the error \"libvirt: QEMU error: "
5489                      "Domain not found: no domain with matching name.\" "
5490                      "This error can be safely ignored."),
5491                  instance=instance)
5492 
5493         self._clean_instance_console_tokens(ctxt, instance)
5494         if migrate_data and migrate_data.obj_attr_is_set('migration'):
5495             migrate_data.migration.status = 'completed'
5496             migrate_data.migration.save()
5497 
5498     def _consoles_enabled(self):
5499         """Returns whether a console is enable."""
5500         return (CONF.vnc.enabled or CONF.spice.enabled or
5501                 CONF.rdp.enabled or CONF.serial_console.enabled or
5502                 CONF.mks.enabled)
5503 
5504     def _clean_instance_console_tokens(self, ctxt, instance):
5505         """Clean console tokens stored for an instance."""
5506         if self._consoles_enabled():
5507             if CONF.cells.enable:
5508                 self.cells_rpcapi.consoleauth_delete_tokens(
5509                     ctxt, instance.uuid)
5510             else:
5511                 self.consoleauth_rpcapi.delete_tokens_for_instance(
5512                     ctxt, instance.uuid)
5513 
5514     @wrap_exception()
5515     @wrap_instance_event(prefix='compute')
5516     @wrap_instance_fault
5517     def post_live_migration_at_destination(self, context, instance,
5518                                            block_migration):
5519         """Post operations for live migration .
5520 
5521         :param context: security context
5522         :param instance: Instance dict
5523         :param block_migration: if true, prepare for block migration
5524 
5525         """
5526         LOG.info(_LI('Post operation of migration started'),
5527                  instance=instance)
5528 
5529         # NOTE(tr3buchet): setup networks on destination host
5530         #                  this is called a second time because
5531         #                  multi_host does not create the bridge in
5532         #                  plug_vifs
5533         self.network_api.setup_networks_on_host(context, instance,
5534                                                          self.host)
5535         migration = {'source_compute': instance.host,
5536                      'dest_compute': self.host, }
5537         self.network_api.migrate_instance_finish(context,
5538                                                  instance,
5539                                                  migration)
5540 
5541         network_info = self.network_api.get_instance_nw_info(context, instance)
5542         self._notify_about_instance_usage(
5543                      context, instance, "live_migration.post.dest.start",
5544                      network_info=network_info)
5545         block_device_info = self._get_instance_block_device_info(context,
5546                                                                  instance)
5547 
5548         try:
5549             self.driver.post_live_migration_at_destination(
5550                 context, instance, network_info, block_migration,
5551                 block_device_info)
5552         except Exception:
5553             with excutils.save_and_reraise_exception():
5554                 instance.vm_state = vm_states.ERROR
5555                 LOG.error(_LE('Unexpected error during post live migration at '
5556                               'destination host.'), instance=instance)
5557         finally:
5558             # Restore instance state and update host
5559             current_power_state = self._get_power_state(context, instance)
5560             node_name = None
5561             prev_host = instance.host
5562             try:
5563                 compute_node = self._get_compute_info(context, self.host)
5564                 node_name = compute_node.hypervisor_hostname
5565             except exception.ComputeHostNotFound:
5566                 LOG.exception(_LE('Failed to get compute_info for %s'),
5567                               self.host)
5568             finally:
5569                 instance.host = self.host
5570                 instance.power_state = current_power_state
5571                 instance.task_state = None
5572                 instance.node = node_name
5573                 instance.progress = 0
5574                 instance.save(expected_task_state=task_states.MIGRATING)
5575 
5576         # NOTE(tr3buchet): tear down networks on source host
5577         self.network_api.setup_networks_on_host(context, instance,
5578                                                 prev_host, teardown=True)
5579         # NOTE(vish): this is necessary to update dhcp
5580         self.network_api.setup_networks_on_host(context, instance, self.host)
5581         self._notify_about_instance_usage(
5582                      context, instance, "live_migration.post.dest.end",
5583                      network_info=network_info)
5584 
5585     @wrap_exception()
5586     @wrap_instance_fault
5587     def _rollback_live_migration(self, context, instance,
5588                                  dest, block_migration, migrate_data=None,
5589                                  migration_status='error'):
5590         """Recovers Instance/volume state from migrating -> running.
5591 
5592         :param context: security context
5593         :param instance: nova.objects.instance.Instance object
5594         :param dest:
5595             This method is called from live migration src host.
5596             This param specifies destination host.
5597         :param block_migration: if true, prepare for block migration
5598         :param migrate_data:
5599             if not none, contains implementation specific data.
5600         :param migration_status:
5601             Contains the status we want to set for the migration object
5602 
5603         """
5604         instance.task_state = None
5605         instance.progress = 0
5606         instance.save(expected_task_state=[task_states.MIGRATING])
5607 
5608         # TODO(tdurakov): remove dict to object conversion once RPC API version
5609         # is bumped to 5.x
5610         if isinstance(migrate_data, dict):
5611             migration = migrate_data.pop('migration', None)
5612             migrate_data = \
5613                 migrate_data_obj.LiveMigrateData.detect_implementation(
5614                     migrate_data)
5615         elif (isinstance(migrate_data, migrate_data_obj.LiveMigrateData) and
5616               migrate_data.obj_attr_is_set('migration')):
5617             migration = migrate_data.migration
5618         else:
5619             migration = None
5620 
5621         # NOTE(tr3buchet): setup networks on source host (really it's re-setup)
5622         self.network_api.setup_networks_on_host(context, instance, self.host)
5623 
5624         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
5625                 context, instance.uuid)
5626         for bdm in bdms:
5627             if bdm.is_volume:
5628                 self.compute_rpcapi.remove_volume_connection(
5629                         context, instance, bdm.volume_id, dest)
5630 
5631         self._notify_about_instance_usage(context, instance,
5632                                           "live_migration._rollback.start")
5633 
5634         do_cleanup, destroy_disks = self._live_migration_cleanup_flags(
5635                 migrate_data)
5636 
5637         block_device_info = self._get_instance_block_device_info(context,
5638                                                                  instance,
5639                                                                  bdms=bdms)
5640         if do_cleanup:
5641             self.compute_rpcapi.rollback_live_migration_at_destination(
5642                     context, instance, dest, destroy_disks=destroy_disks,
5643                     migrate_data=migrate_data,
5644                     block_device_info=block_device_info)
5645         try:
5646             # NOTE(lyarwood):Now refresh connection_info on the source host to
5647             # revert any changes made during pre_live_migration by the dest.
5648             self._get_instance_block_device_info(context, instance, bdms=bdms,
5649                                                  refresh_conn_info=True)
5650         except Exception:
5651             with excutils.save_and_reraise_exception():
5652                 LOG.exception(_LE('An error occurred refreshing the '
5653                     'connection_info of attached volumes while rolling back a '
5654                     'live migration attempt'), instance=instance)
5655         finally:
5656             # NOTE(lyarwood): Always notify that the rollback has ended and
5657             # update the migration status to reflect this.
5658             self._notify_about_instance_usage(context, instance,
5659                                               "live_migration._rollback.end")
5660             self._set_migration_status(migration, migration_status)
5661 
5662     @wrap_exception()
5663     @wrap_instance_event(prefix='compute')
5664     @wrap_instance_fault
5665     def rollback_live_migration_at_destination(self, context, instance,
5666                                                destroy_disks,
5667                                                migrate_data,
5668                                                block_device_info=None):
5669         """Cleaning up image directory that is created pre_live_migration.
5670 
5671         :param context: security context
5672         :param instance: a nova.objects.instance.Instance object sent over rpc
5673         """
5674         network_info = self.network_api.get_instance_nw_info(context, instance)
5675         self._notify_about_instance_usage(
5676                       context, instance, "live_migration.rollback.dest.start",
5677                       network_info=network_info)
5678         try:
5679             # NOTE(tr3buchet): tear down networks on destination host
5680             self.network_api.setup_networks_on_host(context, instance,
5681                                                     self.host, teardown=True)
5682         except Exception:
5683             with excutils.save_and_reraise_exception():
5684                 # NOTE(tdurakov): even if teardown networks fails driver
5685                 # should try to rollback live migration on destination.
5686                 LOG.exception(
5687                     _LE('An error occurred while deallocating network.'),
5688                     instance=instance)
5689         finally:
5690             # always run this even if setup_networks_on_host fails
5691             if block_device_info is None:
5692                 block_device_info = self._get_instance_block_device_info(
5693                     context, instance)
5694             # TODO(tdurakov): remove dict to object conversion once RPC API
5695             # version is bumped to 5.x
5696             if isinstance(migrate_data, dict):
5697                 migrate_data = \
5698                     migrate_data_obj.LiveMigrateData.detect_implementation(
5699                         migrate_data)
5700             self.driver.rollback_live_migration_at_destination(
5701                 context, instance, network_info, block_device_info,
5702                 destroy_disks=destroy_disks, migrate_data=migrate_data)
5703 
5704         self._notify_about_instance_usage(
5705                         context, instance, "live_migration.rollback.dest.end",
5706                         network_info=network_info)
5707 
5708     @periodic_task.periodic_task(
5709         spacing=CONF.heal_instance_info_cache_interval)
5710     def _heal_instance_info_cache(self, context):
5711         """Called periodically.  On every call, try to update the
5712         info_cache's network information for another instance by
5713         calling to the network manager.
5714 
5715         This is implemented by keeping a cache of uuids of instances
5716         that live on this host.  On each call, we pop one off of a
5717         list, pull the DB record, and try the call to the network API.
5718         If anything errors don't fail, as it's possible the instance
5719         has been deleted, etc.
5720         """
5721         heal_interval = CONF.heal_instance_info_cache_interval
5722         if not heal_interval:
5723             return
5724 
5725         instance_uuids = getattr(self, '_instance_uuids_to_heal', [])
5726         instance = None
5727 
5728         LOG.debug('Starting heal instance info cache')
5729 
5730         if not instance_uuids:
5731             # The list of instances to heal is empty so rebuild it
5732             LOG.debug('Rebuilding the list of instances to heal')
5733             db_instances = objects.InstanceList.get_by_host(
5734                 context, self.host, expected_attrs=[], use_slave=True)
5735             for inst in db_instances:
5736                 # We don't want to refresh the cache for instances
5737                 # which are building or deleting so don't put them
5738                 # in the list. If they are building they will get
5739                 # added to the list next time we build it.
5740                 if (inst.vm_state == vm_states.BUILDING):
5741                     LOG.debug('Skipping network cache update for instance '
5742                               'because it is Building.', instance=inst)
5743                     continue
5744                 if (inst.task_state == task_states.DELETING):
5745                     LOG.debug('Skipping network cache update for instance '
5746                               'because it is being deleted.', instance=inst)
5747                     continue
5748 
5749                 if not instance:
5750                     # Save the first one we find so we don't
5751                     # have to get it again
5752                     instance = inst
5753                 else:
5754                     instance_uuids.append(inst['uuid'])
5755 
5756             self._instance_uuids_to_heal = instance_uuids
5757         else:
5758             # Find the next valid instance on the list
5759             while instance_uuids:
5760                 try:
5761                     inst = objects.Instance.get_by_uuid(
5762                             context, instance_uuids.pop(0),
5763                             expected_attrs=['system_metadata', 'info_cache',
5764                                             'flavor'],
5765                             use_slave=True)
5766                 except exception.InstanceNotFound:
5767                     # Instance is gone.  Try to grab another.
5768                     continue
5769 
5770                 # Check the instance hasn't been migrated
5771                 if inst.host != self.host:
5772                     LOG.debug('Skipping network cache update for instance '
5773                               'because it has been migrated to another '
5774                               'host.', instance=inst)
5775                 # Check the instance isn't being deleting
5776                 elif inst.task_state == task_states.DELETING:
5777                     LOG.debug('Skipping network cache update for instance '
5778                               'because it is being deleted.', instance=inst)
5779                 else:
5780                     instance = inst
5781                     break
5782 
5783         if instance:
5784             # We have an instance now to refresh
5785             try:
5786                 # Call to network API to get instance info.. this will
5787                 # force an update to the instance's info_cache
5788                 self.network_api.get_instance_nw_info(context, instance)
5789                 LOG.debug('Updated the network info_cache for instance',
5790                           instance=instance)
5791             except exception.InstanceNotFound:
5792                 # Instance is gone.
5793                 LOG.debug('Instance no longer exists. Unable to refresh',
5794                           instance=instance)
5795                 return
5796             except exception.InstanceInfoCacheNotFound:
5797                 # InstanceInfoCache is gone.
5798                 LOG.debug('InstanceInfoCache no longer exists. '
5799                           'Unable to refresh', instance=instance)
5800             except Exception:
5801                 LOG.error(_LE('An error occurred while refreshing the network '
5802                               'cache.'), instance=instance, exc_info=True)
5803         else:
5804             LOG.debug("Didn't find any instances for network info cache "
5805                       "update.")
5806 
5807     @periodic_task.periodic_task
5808     def _poll_rebooting_instances(self, context):
5809         if CONF.reboot_timeout > 0:
5810             filters = {'task_state':
5811                        [task_states.REBOOTING,
5812                         task_states.REBOOT_STARTED,
5813                         task_states.REBOOT_PENDING],
5814                        'host': self.host}
5815             rebooting = objects.InstanceList.get_by_filters(
5816                 context, filters, expected_attrs=[], use_slave=True)
5817 
5818             to_poll = []
5819             for instance in rebooting:
5820                 if timeutils.is_older_than(instance.updated_at,
5821                                            CONF.reboot_timeout):
5822                     to_poll.append(instance)
5823 
5824             self.driver.poll_rebooting_instances(CONF.reboot_timeout, to_poll)
5825 
5826     @periodic_task.periodic_task
5827     def _poll_rescued_instances(self, context):
5828         if CONF.rescue_timeout > 0:
5829             filters = {'vm_state': vm_states.RESCUED,
5830                        'host': self.host}
5831             rescued_instances = objects.InstanceList.get_by_filters(
5832                 context, filters, expected_attrs=["system_metadata"],
5833                 use_slave=True)
5834 
5835             to_unrescue = []
5836             for instance in rescued_instances:
5837                 if timeutils.is_older_than(instance.launched_at,
5838                                            CONF.rescue_timeout):
5839                     to_unrescue.append(instance)
5840 
5841             for instance in to_unrescue:
5842                 self.compute_api.unrescue(context, instance)
5843 
5844     @periodic_task.periodic_task
5845     def _poll_unconfirmed_resizes(self, context):
5846         if CONF.resize_confirm_window == 0:
5847             return
5848 
5849         migrations = objects.MigrationList.get_unconfirmed_by_dest_compute(
5850                 context, CONF.resize_confirm_window, self.host,
5851                 use_slave=True)
5852 
5853         migrations_info = dict(migration_count=len(migrations),
5854                 confirm_window=CONF.resize_confirm_window)
5855 
5856         if migrations_info["migration_count"] > 0:
5857             LOG.info(_LI("Found %(migration_count)d unconfirmed migrations "
5858                          "older than %(confirm_window)d seconds"),
5859                      migrations_info)
5860 
5861         def _set_migration_to_error(migration, reason, **kwargs):
5862             LOG.warning(_LW("Setting migration %(migration_id)s to error: "
5863                          "%(reason)s"),
5864                      {'migration_id': migration['id'], 'reason': reason},
5865                      **kwargs)
5866             migration.status = 'error'
5867             with migration.obj_as_admin():
5868                 migration.save()
5869 
5870         for migration in migrations:
5871             instance_uuid = migration.instance_uuid
5872             LOG.info(_LI("Automatically confirming migration "
5873                          "%(migration_id)s for instance %(instance_uuid)s"),
5874                      {'migration_id': migration.id,
5875                       'instance_uuid': instance_uuid})
5876             expected_attrs = ['metadata', 'system_metadata']
5877             try:
5878                 instance = objects.Instance.get_by_uuid(context,
5879                             instance_uuid, expected_attrs=expected_attrs,
5880                             use_slave=True)
5881             except exception.InstanceNotFound:
5882                 reason = (_("Instance %s not found") %
5883                           instance_uuid)
5884                 _set_migration_to_error(migration, reason)
5885                 continue
5886             if instance.vm_state == vm_states.ERROR:
5887                 reason = _("In ERROR state")
5888                 _set_migration_to_error(migration, reason,
5889                                         instance=instance)
5890                 continue
5891             # race condition: The instance in DELETING state should not be
5892             # set the migration state to error, otherwise the instance in
5893             # to be deleted which is in RESIZED state
5894             # will not be able to confirm resize
5895             if instance.task_state in [task_states.DELETING,
5896                                        task_states.SOFT_DELETING]:
5897                 msg = ("Instance being deleted or soft deleted during resize "
5898                        "confirmation. Skipping.")
5899                 LOG.debug(msg, instance=instance)
5900                 continue
5901 
5902             # race condition: This condition is hit when this method is
5903             # called between the save of the migration record with a status of
5904             # finished and the save of the instance object with a state of
5905             # RESIZED. The migration record should not be set to error.
5906             if instance.task_state == task_states.RESIZE_FINISH:
5907                 msg = ("Instance still resizing during resize "
5908                        "confirmation. Skipping.")
5909                 LOG.debug(msg, instance=instance)
5910                 continue
5911 
5912             vm_state = instance.vm_state
5913             task_state = instance.task_state
5914             if vm_state != vm_states.RESIZED or task_state is not None:
5915                 reason = (_("In states %(vm_state)s/%(task_state)s, not "
5916                            "RESIZED/None") %
5917                           {'vm_state': vm_state,
5918                            'task_state': task_state})
5919                 _set_migration_to_error(migration, reason,
5920                                         instance=instance)
5921                 continue
5922             try:
5923                 self.compute_api.confirm_resize(context, instance,
5924                                                 migration=migration)
5925             except Exception as e:
5926                 LOG.info(_LI("Error auto-confirming resize: %s. "
5927                              "Will retry later."),
5928                          e, instance=instance)
5929 
5930     @periodic_task.periodic_task(spacing=CONF.shelved_poll_interval)
5931     def _poll_shelved_instances(self, context):
5932 
5933         if CONF.shelved_offload_time <= 0:
5934             return
5935 
5936         filters = {'vm_state': vm_states.SHELVED,
5937                    'task_state': None,
5938                    'host': self.host}
5939         shelved_instances = objects.InstanceList.get_by_filters(
5940             context, filters=filters, expected_attrs=['system_metadata'],
5941             use_slave=True)
5942 
5943         to_gc = []
5944         for instance in shelved_instances:
5945             sys_meta = instance.system_metadata
5946             shelved_at = timeutils.parse_strtime(sys_meta['shelved_at'])
5947             if timeutils.is_older_than(shelved_at, CONF.shelved_offload_time):
5948                 to_gc.append(instance)
5949 
5950         for instance in to_gc:
5951             try:
5952                 instance.task_state = task_states.SHELVING_OFFLOADING
5953                 instance.save(expected_task_state=(None,))
5954                 self.shelve_offload_instance(context, instance,
5955                                              clean_shutdown=False)
5956             except Exception:
5957                 LOG.exception(_LE('Periodic task failed to offload instance.'),
5958                         instance=instance)
5959 
5960     @periodic_task.periodic_task
5961     def _instance_usage_audit(self, context):
5962         if not CONF.instance_usage_audit:
5963             return
5964 
5965         begin, end = utils.last_completed_audit_period()
5966         if objects.TaskLog.get(context, 'instance_usage_audit', begin, end,
5967                                self.host):
5968             return
5969 
5970         instances = objects.InstanceList.get_active_by_window_joined(
5971             context, begin, end, host=self.host,
5972             expected_attrs=['system_metadata', 'info_cache', 'metadata',
5973                             'flavor'],
5974             use_slave=True)
5975         num_instances = len(instances)
5976         errors = 0
5977         successes = 0
5978         LOG.info(_LI("Running instance usage audit for"
5979                      " host %(host)s from %(begin_time)s to "
5980                      "%(end_time)s. %(number_instances)s"
5981                      " instances."),
5982                  {'host': self.host,
5983                   'begin_time': begin,
5984                   'end_time': end,
5985                   'number_instances': num_instances})
5986         start_time = time.time()
5987         task_log = objects.TaskLog(context)
5988         task_log.task_name = 'instance_usage_audit'
5989         task_log.period_beginning = begin
5990         task_log.period_ending = end
5991         task_log.host = self.host
5992         task_log.task_items = num_instances
5993         task_log.message = 'Instance usage audit started...'
5994         task_log.begin_task()
5995         for instance in instances:
5996             try:
5997                 compute_utils.notify_usage_exists(
5998                     self.notifier, context, instance,
5999                     ignore_missing_network_data=False)
6000                 successes += 1
6001             except Exception:
6002                 LOG.exception(_LE('Failed to generate usage '
6003                                   'audit for instance '
6004                                   'on host %s'), self.host,
6005                               instance=instance)
6006                 errors += 1
6007         task_log.errors = errors
6008         task_log.message = (
6009             'Instance usage audit ran for host %s, %s instances in %s seconds.'
6010             % (self.host, num_instances, time.time() - start_time))
6011         task_log.end_task()
6012 
6013     @periodic_task.periodic_task(spacing=CONF.bandwidth_poll_interval)
6014     def _poll_bandwidth_usage(self, context):
6015 
6016         if not self._bw_usage_supported:
6017             return
6018 
6019         prev_time, start_time = utils.last_completed_audit_period()
6020 
6021         curr_time = time.time()
6022         if (curr_time - self._last_bw_usage_poll >
6023                 CONF.bandwidth_poll_interval):
6024             self._last_bw_usage_poll = curr_time
6025             LOG.info(_LI("Updating bandwidth usage cache"))
6026             cells_update_interval = CONF.cells.bandwidth_update_interval
6027             if (cells_update_interval > 0 and
6028                    curr_time - self._last_bw_usage_cell_update >
6029                            cells_update_interval):
6030                 self._last_bw_usage_cell_update = curr_time
6031                 update_cells = True
6032             else:
6033                 update_cells = False
6034 
6035             instances = objects.InstanceList.get_by_host(context,
6036                                                               self.host,
6037                                                               use_slave=True)
6038             try:
6039                 bw_counters = self.driver.get_all_bw_counters(instances)
6040             except NotImplementedError:
6041                 # NOTE(mdragon): Not all hypervisors have bandwidth polling
6042                 # implemented yet.  If they don't it doesn't break anything,
6043                 # they just don't get the info in the usage events.
6044                 # NOTE(PhilDay): Record that its not supported so we can
6045                 # skip fast on future calls rather than waste effort getting
6046                 # the list of instances.
6047                 LOG.info(_LI("Bandwidth usage not supported by "
6048                              "hypervisor."))
6049                 self._bw_usage_supported = False
6050                 return
6051 
6052             refreshed = timeutils.utcnow()
6053             for bw_ctr in bw_counters:
6054                 # Allow switching of greenthreads between queries.
6055                 greenthread.sleep(0)
6056                 bw_in = 0
6057                 bw_out = 0
6058                 last_ctr_in = None
6059                 last_ctr_out = None
6060                 usage = objects.BandwidthUsage.get_by_instance_uuid_and_mac(
6061                     context, bw_ctr['uuid'], bw_ctr['mac_address'],
6062                     start_period=start_time, use_slave=True)
6063                 if usage:
6064                     bw_in = usage.bw_in
6065                     bw_out = usage.bw_out
6066                     last_ctr_in = usage.last_ctr_in
6067                     last_ctr_out = usage.last_ctr_out
6068                 else:
6069                     usage = (objects.BandwidthUsage.
6070                              get_by_instance_uuid_and_mac(
6071                         context, bw_ctr['uuid'], bw_ctr['mac_address'],
6072                         start_period=prev_time, use_slave=True))
6073                     if usage:
6074                         last_ctr_in = usage.last_ctr_in
6075                         last_ctr_out = usage.last_ctr_out
6076 
6077                 if last_ctr_in is not None:
6078                     if bw_ctr['bw_in'] < last_ctr_in:
6079                         # counter rollover
6080                         bw_in += bw_ctr['bw_in']
6081                     else:
6082                         bw_in += (bw_ctr['bw_in'] - last_ctr_in)
6083 
6084                 if last_ctr_out is not None:
6085                     if bw_ctr['bw_out'] < last_ctr_out:
6086                         # counter rollover
6087                         bw_out += bw_ctr['bw_out']
6088                     else:
6089                         bw_out += (bw_ctr['bw_out'] - last_ctr_out)
6090 
6091                 objects.BandwidthUsage(context=context).create(
6092                                               bw_ctr['uuid'],
6093                                               bw_ctr['mac_address'],
6094                                               bw_in,
6095                                               bw_out,
6096                                               bw_ctr['bw_in'],
6097                                               bw_ctr['bw_out'],
6098                                               start_period=start_time,
6099                                               last_refreshed=refreshed,
6100                                               update_cells=update_cells)
6101 
6102     def _get_host_volume_bdms(self, context, use_slave=False):
6103         """Return all block device mappings on a compute host."""
6104         compute_host_bdms = []
6105         instances = objects.InstanceList.get_by_host(context, self.host,
6106             use_slave=use_slave)
6107         for instance in instances:
6108             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
6109                     context, instance.uuid, use_slave=use_slave)
6110             instance_bdms = [bdm for bdm in bdms if bdm.is_volume]
6111             compute_host_bdms.append(dict(instance=instance,
6112                                           instance_bdms=instance_bdms))
6113 
6114         return compute_host_bdms
6115 
6116     def _update_volume_usage_cache(self, context, vol_usages):
6117         """Updates the volume usage cache table with a list of stats."""
6118         for usage in vol_usages:
6119             # Allow switching of greenthreads between queries.
6120             greenthread.sleep(0)
6121             vol_usage = objects.VolumeUsage(context)
6122             vol_usage.volume_id = usage['volume']
6123             vol_usage.instance_uuid = usage['instance'].uuid
6124             vol_usage.project_id = usage['instance'].project_id
6125             vol_usage.user_id = usage['instance'].user_id
6126             vol_usage.availability_zone = usage['instance'].availability_zone
6127             vol_usage.curr_reads = usage['rd_req']
6128             vol_usage.curr_read_bytes = usage['rd_bytes']
6129             vol_usage.curr_writes = usage['wr_req']
6130             vol_usage.curr_write_bytes = usage['wr_bytes']
6131             vol_usage.save()
6132             self.notifier.info(context, 'volume.usage',
6133                                compute_utils.usage_volume_info(vol_usage))
6134 
6135     @periodic_task.periodic_task(spacing=CONF.volume_usage_poll_interval)
6136     def _poll_volume_usage(self, context):
6137         if CONF.volume_usage_poll_interval == 0:
6138             return
6139 
6140         compute_host_bdms = self._get_host_volume_bdms(context,
6141                                                        use_slave=True)
6142         if not compute_host_bdms:
6143             return
6144 
6145         LOG.debug("Updating volume usage cache")
6146         try:
6147             vol_usages = self.driver.get_all_volume_usage(context,
6148                                                           compute_host_bdms)
6149         except NotImplementedError:
6150             return
6151 
6152         self._update_volume_usage_cache(context, vol_usages)
6153 
6154     @periodic_task.periodic_task(spacing=CONF.sync_power_state_interval,
6155                                  run_immediately=True)
6156     def _sync_power_states(self, context):
6157         """Align power states between the database and the hypervisor.
6158 
6159         To sync power state data we make a DB call to get the number of
6160         virtual machines known by the hypervisor and if the number matches the
6161         number of virtual machines known by the database, we proceed in a lazy
6162         loop, one database record at a time, checking if the hypervisor has the
6163         same power state as is in the database.
6164         """
6165         db_instances = objects.InstanceList.get_by_host(context, self.host,
6166                                                         expected_attrs=[],
6167                                                         use_slave=True)
6168 
6169         num_vm_instances = self.driver.get_num_instances()
6170         num_db_instances = len(db_instances)
6171 
6172         if num_vm_instances != num_db_instances:
6173             LOG.warning(_LW("While synchronizing instance power states, found "
6174                             "%(num_db_instances)s instances in the database "
6175                             "and %(num_vm_instances)s instances on the "
6176                             "hypervisor."),
6177                         {'num_db_instances': num_db_instances,
6178                          'num_vm_instances': num_vm_instances})
6179 
6180         def _sync(db_instance):
6181             # NOTE(melwitt): This must be synchronized as we query state from
6182             #                two separate sources, the driver and the database.
6183             #                They are set (in stop_instance) and read, in sync.
6184             @utils.synchronized(db_instance.uuid)
6185             def query_driver_power_state_and_sync():
6186                 self._query_driver_power_state_and_sync(context, db_instance)
6187 
6188             try:
6189                 query_driver_power_state_and_sync()
6190             except Exception:
6191                 LOG.exception(_LE("Periodic sync_power_state task had an "
6192                                   "error while processing an instance."),
6193                               instance=db_instance)
6194 
6195             self._syncs_in_progress.pop(db_instance.uuid)
6196 
6197         for db_instance in db_instances:
6198             # process syncs asynchronously - don't want instance locking to
6199             # block entire periodic task thread
6200             uuid = db_instance.uuid
6201             if uuid in self._syncs_in_progress:
6202                 LOG.debug('Sync already in progress for %s', uuid)
6203             else:
6204                 LOG.debug('Triggering sync for uuid %s', uuid)
6205                 self._syncs_in_progress[uuid] = True
6206                 self._sync_power_pool.spawn_n(_sync, db_instance)
6207 
6208     def _query_driver_power_state_and_sync(self, context, db_instance):
6209         if db_instance.task_state is not None:
6210             LOG.info(_LI("During sync_power_state the instance has a "
6211                          "pending task (%(task)s). Skip."),
6212                      {'task': db_instance.task_state}, instance=db_instance)
6213             return
6214         # No pending tasks. Now try to figure out the real vm_power_state.
6215         try:
6216             vm_instance = self.driver.get_info(db_instance)
6217             vm_power_state = vm_instance.state
6218         except exception.InstanceNotFound:
6219             vm_power_state = power_state.NOSTATE
6220         # Note(maoy): the above get_info call might take a long time,
6221         # for example, because of a broken libvirt driver.
6222         try:
6223             self._sync_instance_power_state(context,
6224                                             db_instance,
6225                                             vm_power_state,
6226                                             use_slave=True)
6227         except exception.InstanceNotFound:
6228             # NOTE(hanlind): If the instance gets deleted during sync,
6229             # silently ignore.
6230             pass
6231 
6232     def _sync_instance_power_state(self, context, db_instance, vm_power_state,
6233                                    use_slave=False):
6234         """Align instance power state between the database and hypervisor.
6235 
6236         If the instance is not found on the hypervisor, but is in the database,
6237         then a stop() API will be called on the instance.
6238         """
6239 
6240         # We re-query the DB to get the latest instance info to minimize
6241         # (not eliminate) race condition.
6242         db_instance.refresh(use_slave=use_slave)
6243         db_power_state = db_instance.power_state
6244         vm_state = db_instance.vm_state
6245 
6246         if self.host != db_instance.host:
6247             # on the sending end of nova-compute _sync_power_state
6248             # may have yielded to the greenthread performing a live
6249             # migration; this in turn has changed the resident-host
6250             # for the VM; However, the instance is still active, it
6251             # is just in the process of migrating to another host.
6252             # This implies that the compute source must relinquish
6253             # control to the compute destination.
6254             LOG.info(_LI("During the sync_power process the "
6255                          "instance has moved from "
6256                          "host %(src)s to host %(dst)s"),
6257                      {'src': db_instance.host,
6258                       'dst': self.host},
6259                      instance=db_instance)
6260             return
6261         elif db_instance.task_state is not None:
6262             # on the receiving end of nova-compute, it could happen
6263             # that the DB instance already report the new resident
6264             # but the actual VM has not showed up on the hypervisor
6265             # yet. In this case, let's allow the loop to continue
6266             # and run the state sync in a later round
6267             LOG.info(_LI("During sync_power_state the instance has a "
6268                          "pending task (%(task)s). Skip."),
6269                      {'task': db_instance.task_state},
6270                      instance=db_instance)
6271             return
6272 
6273         orig_db_power_state = db_power_state
6274         if vm_power_state != db_power_state:
6275             LOG.info(_LI('During _sync_instance_power_state the DB '
6276                          'power_state (%(db_power_state)s) does not match '
6277                          'the vm_power_state from the hypervisor '
6278                          '(%(vm_power_state)s). Updating power_state in the '
6279                          'DB to match the hypervisor.'),
6280                      {'db_power_state': db_power_state,
6281                       'vm_power_state': vm_power_state},
6282                      instance=db_instance)
6283             # power_state is always updated from hypervisor to db
6284             db_instance.power_state = vm_power_state
6285             db_instance.save()
6286             db_power_state = vm_power_state
6287 
6288         # Note(maoy): Now resolve the discrepancy between vm_state and
6289         # vm_power_state. We go through all possible vm_states.
6290         if vm_state in (vm_states.BUILDING,
6291                         vm_states.RESCUED,
6292                         vm_states.RESIZED,
6293                         vm_states.SUSPENDED,
6294                         vm_states.ERROR):
6295             # TODO(maoy): we ignore these vm_state for now.
6296             pass
6297         elif vm_state == vm_states.ACTIVE:
6298             # The only rational power state should be RUNNING
6299             if vm_power_state in (power_state.SHUTDOWN,
6300                                   power_state.CRASHED):
6301                 LOG.warning(_LW("Instance shutdown by itself. Calling the "
6302                                 "stop API. Current vm_state: %(vm_state)s, "
6303                                 "current task_state: %(task_state)s, "
6304                                 "original DB power_state: %(db_power_state)s, "
6305                                 "current VM power_state: %(vm_power_state)s"),
6306                             {'vm_state': vm_state,
6307                              'task_state': db_instance.task_state,
6308                              'db_power_state': orig_db_power_state,
6309                              'vm_power_state': vm_power_state},
6310                             instance=db_instance)
6311                 try:
6312                     # Note(maoy): here we call the API instead of
6313                     # brutally updating the vm_state in the database
6314                     # to allow all the hooks and checks to be performed.
6315                     if db_instance.shutdown_terminate:
6316                         self.compute_api.delete(context, db_instance)
6317                     else:
6318                         self.compute_api.stop(context, db_instance)
6319                 except Exception:
6320                     # Note(maoy): there is no need to propagate the error
6321                     # because the same power_state will be retrieved next
6322                     # time and retried.
6323                     # For example, there might be another task scheduled.
6324                     LOG.exception(_LE("error during stop() in "
6325                                       "sync_power_state."),
6326                                   instance=db_instance)
6327             elif vm_power_state == power_state.SUSPENDED:
6328                 LOG.warning(_LW("Instance is suspended unexpectedly. Calling "
6329                                 "the stop API."), instance=db_instance)
6330                 try:
6331                     self.compute_api.stop(context, db_instance)
6332                 except Exception:
6333                     LOG.exception(_LE("error during stop() in "
6334                                       "sync_power_state."),
6335                                   instance=db_instance)
6336             elif vm_power_state == power_state.PAUSED:
6337                 # Note(maoy): a VM may get into the paused state not only
6338                 # because the user request via API calls, but also
6339                 # due to (temporary) external instrumentations.
6340                 # Before the virt layer can reliably report the reason,
6341                 # we simply ignore the state discrepancy. In many cases,
6342                 # the VM state will go back to running after the external
6343                 # instrumentation is done. See bug 1097806 for details.
6344                 LOG.warning(_LW("Instance is paused unexpectedly. Ignore."),
6345                             instance=db_instance)
6346             elif vm_power_state == power_state.NOSTATE:
6347                 # Occasionally, depending on the status of the hypervisor,
6348                 # which could be restarting for example, an instance may
6349                 # not be found.  Therefore just log the condition.
6350                 LOG.warning(_LW("Instance is unexpectedly not found. Ignore."),
6351                             instance=db_instance)
6352         elif vm_state == vm_states.STOPPED:
6353             if vm_power_state not in (power_state.NOSTATE,
6354                                       power_state.SHUTDOWN,
6355                                       power_state.CRASHED):
6356                 LOG.warning(_LW("Instance is not stopped. Calling "
6357                                 "the stop API. Current vm_state: %(vm_state)s,"
6358                                 " current task_state: %(task_state)s, "
6359                                 "original DB power_state: %(db_power_state)s, "
6360                                 "current VM power_state: %(vm_power_state)s"),
6361                             {'vm_state': vm_state,
6362                              'task_state': db_instance.task_state,
6363                              'db_power_state': orig_db_power_state,
6364                              'vm_power_state': vm_power_state},
6365                             instance=db_instance)
6366                 try:
6367                     # NOTE(russellb) Force the stop, because normally the
6368                     # compute API would not allow an attempt to stop a stopped
6369                     # instance.
6370                     self.compute_api.force_stop(context, db_instance)
6371                 except Exception:
6372                     LOG.exception(_LE("error during stop() in "
6373                                       "sync_power_state."),
6374                                   instance=db_instance)
6375         elif vm_state == vm_states.PAUSED:
6376             if vm_power_state in (power_state.SHUTDOWN,
6377                                   power_state.CRASHED):
6378                 LOG.warning(_LW("Paused instance shutdown by itself. Calling "
6379                                 "the stop API."), instance=db_instance)
6380                 try:
6381                     self.compute_api.force_stop(context, db_instance)
6382                 except Exception:
6383                     LOG.exception(_LE("error during stop() in "
6384                                       "sync_power_state."),
6385                                   instance=db_instance)
6386         elif vm_state in (vm_states.SOFT_DELETED,
6387                           vm_states.DELETED):
6388             if vm_power_state not in (power_state.NOSTATE,
6389                                       power_state.SHUTDOWN):
6390                 # Note(maoy): this should be taken care of periodically in
6391                 # _cleanup_running_deleted_instances().
6392                 LOG.warning(_LW("Instance is not (soft-)deleted."),
6393                             instance=db_instance)
6394 
6395     @periodic_task.periodic_task
6396     def _reclaim_queued_deletes(self, context):
6397         """Reclaim instances that are queued for deletion."""
6398         interval = CONF.reclaim_instance_interval
6399         if interval <= 0:
6400             LOG.debug("CONF.reclaim_instance_interval <= 0, skipping...")
6401             return
6402 
6403         # TODO(comstud, jichenjc): Dummy quota object for now See bug 1296414.
6404         # The only case that the quota might be inconsistent is
6405         # the compute node died between set instance state to SOFT_DELETED
6406         # and quota commit to DB. When compute node starts again
6407         # it will have no idea the reservation is committed or not or even
6408         # expired, since it's a rare case, so marked as todo.
6409         quotas = objects.Quotas.from_reservations(context, None)
6410 
6411         filters = {'vm_state': vm_states.SOFT_DELETED,
6412                    'task_state': None,
6413                    'host': self.host}
6414         instances = objects.InstanceList.get_by_filters(
6415             context, filters,
6416             expected_attrs=objects.instance.INSTANCE_DEFAULT_FIELDS,
6417             use_slave=True)
6418         for instance in instances:
6419             if self._deleted_old_enough(instance, interval):
6420                 bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
6421                         context, instance.uuid)
6422                 LOG.info(_LI('Reclaiming deleted instance'), instance=instance)
6423                 try:
6424                     self._delete_instance(context, instance, bdms, quotas)
6425                 except Exception as e:
6426                     LOG.warning(_LW("Periodic reclaim failed to delete "
6427                                     "instance: %s"),
6428                                 e, instance=instance)
6429 
6430     def update_available_resource_for_node(self, context, nodename):
6431 
6432         rt = self._get_resource_tracker(nodename)
6433         try:
6434             rt.update_available_resource(context)
6435         except exception.ComputeHostNotFound:
6436             # NOTE(comstud): We can get to this case if a node was
6437             # marked 'deleted' in the DB and then re-added with a
6438             # different auto-increment id. The cached resource
6439             # tracker tried to update a deleted record and failed.
6440             # Don't add this resource tracker to the new dict, so
6441             # that this will resolve itself on the next run.
6442             LOG.info(_LI("Compute node '%s' not found in "
6443                          "update_available_resource."), nodename)
6444             self._resource_tracker_dict.pop(nodename, None)
6445             return
6446         except Exception:
6447             LOG.exception(_LE("Error updating resources for node "
6448                           "%(node)s."), {'node': nodename})
6449 
6450         # NOTE(comstud): Replace the RT cache before looping through
6451         # compute nodes to delete below, as we can end up doing greenthread
6452         # switches there. Best to have everyone using the newest cache
6453         # ASAP.
6454         self._resource_tracker_dict[nodename] = rt
6455 
6456     @periodic_task.periodic_task(spacing=CONF.update_resources_interval)
6457     def update_available_resource(self, context):
6458         """See driver.get_available_resource()
6459 
6460         Periodic process that keeps that the compute host's understanding of
6461         resource availability and usage in sync with the underlying hypervisor.
6462 
6463         :param context: security context
6464         """
6465 
6466         compute_nodes_in_db = self._get_compute_nodes_in_db(context,
6467                                                             use_slave=True)
6468         nodenames = set(self.driver.get_available_nodes())
6469         for nodename in nodenames:
6470             self.update_available_resource_for_node(context, nodename)
6471 
6472         self._resource_tracker_dict = {
6473             k: v for k, v in self._resource_tracker_dict.items()
6474             if k in nodenames}
6475 
6476         # Delete orphan compute node not reported by driver but still in db
6477         for cn in compute_nodes_in_db:
6478             if cn.hypervisor_hostname not in nodenames:
6479                 LOG.info(_LI("Deleting orphan compute node %s"), cn.id)
6480                 cn.destroy()
6481 
6482     def _get_compute_nodes_in_db(self, context, use_slave=False):
6483         try:
6484             return objects.ComputeNodeList.get_all_by_host(context, self.host,
6485                                                            use_slave=use_slave)
6486         except exception.NotFound:
6487             LOG.error(_LE("No compute node record for host %s"), self.host)
6488             return []
6489 
6490     @periodic_task.periodic_task(
6491         spacing=CONF.running_deleted_instance_poll_interval)
6492     def _cleanup_running_deleted_instances(self, context):
6493         """Cleanup any instances which are erroneously still running after
6494         having been deleted.
6495 
6496         Valid actions to take are:
6497 
6498             1. noop - do nothing
6499             2. log - log which instances are erroneously running
6500             3. reap - shutdown and cleanup any erroneously running instances
6501             4. shutdown - power off *and disable* any erroneously running
6502                           instances
6503 
6504         The use-case for this cleanup task is: for various reasons, it may be
6505         possible for the database to show an instance as deleted but for that
6506         instance to still be running on a host machine (see bug
6507         https://bugs.launchpad.net/nova/+bug/911366).
6508 
6509         This cleanup task is a cross-hypervisor utility for finding these
6510         zombied instances and either logging the discrepancy (likely what you
6511         should do in production), or automatically reaping the instances (more
6512         appropriate for dev environments).
6513         """
6514         action = CONF.running_deleted_instance_action
6515 
6516         if action == "noop":
6517             return
6518 
6519         # NOTE(sirp): admin contexts don't ordinarily return deleted records
6520         with utils.temporary_mutation(context, read_deleted="yes"):
6521             for instance in self._running_deleted_instances(context):
6522                 if action == "log":
6523                     LOG.warning(_LW("Detected instance with name label "
6524                                     "'%s' which is marked as "
6525                                     "DELETED but still present on host."),
6526                                 instance.name, instance=instance)
6527 
6528                 elif action == 'shutdown':
6529                     LOG.info(_LI("Powering off instance with name label "
6530                                  "'%s' which is marked as "
6531                                  "DELETED but still present on host."),
6532                              instance.name, instance=instance)
6533                     try:
6534                         try:
6535                             # disable starting the instance
6536                             self.driver.set_bootable(instance, False)
6537                         except NotImplementedError:
6538                             LOG.debug("set_bootable is not implemented "
6539                                       "for the current driver")
6540                         # and power it off
6541                         self.driver.power_off(instance)
6542                     except Exception:
6543                         msg = _LW("Failed to power off instance")
6544                         LOG.warning(msg, instance=instance, exc_info=True)
6545 
6546                 elif action == 'reap':
6547                     LOG.info(_LI("Destroying instance with name label "
6548                                  "'%s' which is marked as "
6549                                  "DELETED but still present on host."),
6550                              instance.name, instance=instance)
6551                     bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
6552                         context, instance.uuid, use_slave=True)
6553                     self.instance_events.clear_events_for_instance(instance)
6554                     try:
6555                         self._shutdown_instance(context, instance, bdms,
6556                                                 notify=False)
6557                         self._cleanup_volumes(context, instance.uuid, bdms)
6558                     except Exception as e:
6559                         LOG.warning(_LW("Periodic cleanup failed to delete "
6560                                         "instance: %s"),
6561                                     e, instance=instance)
6562                 else:
6563                     raise Exception(_("Unrecognized value '%s'"
6564                                       " for CONF.running_deleted_"
6565                                       "instance_action") % action)
6566 
6567     def _running_deleted_instances(self, context):
6568         """Returns a list of instances nova thinks is deleted,
6569         but the hypervisor thinks is still running.
6570         """
6571         timeout = CONF.running_deleted_instance_timeout
6572         filters = {'deleted': True,
6573                    'soft_deleted': False,
6574                    'host': self.host}
6575         instances = self._get_instances_on_driver(context, filters)
6576         return [i for i in instances if self._deleted_old_enough(i, timeout)]
6577 
6578     def _deleted_old_enough(self, instance, timeout):
6579         deleted_at = instance.deleted_at
6580         if deleted_at:
6581             deleted_at = deleted_at.replace(tzinfo=None)
6582         return (not deleted_at or timeutils.is_older_than(deleted_at, timeout))
6583 
6584     @contextlib.contextmanager
6585     def _error_out_instance_on_exception(self, context, instance,
6586                                          quotas=None,
6587                                          instance_state=vm_states.ACTIVE):
6588         instance_uuid = instance.uuid
6589         try:
6590             yield
6591         except NotImplementedError as error:
6592             with excutils.save_and_reraise_exception():
6593                 if quotas:
6594                     quotas.rollback()
6595                 LOG.info(_LI("Setting instance back to %(state)s after: "
6596                              "%(error)s"),
6597                          {'state': instance_state, 'error': error},
6598                          instance_uuid=instance_uuid)
6599                 self._instance_update(context, instance,
6600                                       vm_state=instance_state,
6601                                       task_state=None)
6602         except exception.InstanceFaultRollback as error:
6603             if quotas:
6604                 quotas.rollback()
6605             LOG.info(_LI("Setting instance back to ACTIVE after: %s"),
6606                      error, instance_uuid=instance_uuid)
6607             self._instance_update(context, instance,
6608                                   vm_state=vm_states.ACTIVE,
6609                                   task_state=None)
6610             raise error.inner_exception
6611         except Exception:
6612             LOG.exception(_LE('Setting instance vm_state to ERROR'),
6613                           instance_uuid=instance_uuid)
6614             with excutils.save_and_reraise_exception():
6615                 if quotas:
6616                     quotas.rollback()
6617                 self._set_instance_obj_error_state(context, instance)
6618 
6619     @wrap_exception()
6620     def add_aggregate_host(self, context, aggregate, host, slave_info):
6621         """Notify hypervisor of change (for hypervisor pools)."""
6622         try:
6623             self.driver.add_to_aggregate(context, aggregate, host,
6624                                          slave_info=slave_info)
6625         except NotImplementedError:
6626             LOG.debug('Hypervisor driver does not support '
6627                       'add_aggregate_host')
6628         except exception.AggregateError:
6629             with excutils.save_and_reraise_exception():
6630                 self.driver.undo_aggregate_operation(
6631                                     context,
6632                                     aggregate.delete_host,
6633                                     aggregate, host)
6634 
6635     @wrap_exception()
6636     def remove_aggregate_host(self, context, host, slave_info, aggregate):
6637         """Removes a host from a physical hypervisor pool."""
6638         try:
6639             self.driver.remove_from_aggregate(context, aggregate, host,
6640                                               slave_info=slave_info)
6641         except NotImplementedError:
6642             LOG.debug('Hypervisor driver does not support '
6643                       'remove_aggregate_host')
6644         except (exception.AggregateError,
6645                 exception.InvalidAggregateAction) as e:
6646             with excutils.save_and_reraise_exception():
6647                 self.driver.undo_aggregate_operation(
6648                                     context,
6649                                     aggregate.add_host,
6650                                     aggregate, host,
6651                                     isinstance(e, exception.AggregateError))
6652 
6653     def _process_instance_event(self, instance, event):
6654         _event = self.instance_events.pop_instance_event(instance, event)
6655         if _event:
6656             LOG.debug('Processing event %(event)s',
6657                       {'event': event.key}, instance=instance)
6658             _event.send(event)
6659         else:
6660             LOG.warning(_LW('Received unexpected event %(event)s for '
6661                             'instance'),
6662                         {'event': event.key}, instance=instance)
6663 
6664     def _process_instance_vif_deleted_event(self, context, instance,
6665                                             deleted_vif_id):
6666         # If an attached port is deleted by neutron, it needs to
6667         # be detached from the instance.
6668         # And info cache needs to be updated.
6669         network_info = instance.info_cache.network_info
6670         for index, vif in enumerate(network_info):
6671             if vif['id'] == deleted_vif_id:
6672                 LOG.info(_LI('Neutron deleted interface %(intf)s; '
6673                              'detaching it from the instance and '
6674                              'deleting it from the info cache'),
6675                          {'intf': vif['id']},
6676                          instance=instance)
6677                 del network_info[index]
6678                 base_net_api.update_instance_cache_with_nw_info(
6679                                  self.network_api, context,
6680                                  instance,
6681                                  nw_info=network_info)
6682                 try:
6683                     self.driver.detach_interface(instance, vif)
6684                 except exception.NovaException as ex:
6685                     LOG.warning(_LW("Detach interface failed, "
6686                                     "port_id=%(port_id)s, reason: %(msg)s"),
6687                                 {'port_id': deleted_vif_id, 'msg': ex},
6688                                 instance=instance)
6689                 break
6690 
6691     @wrap_exception()
6692     def external_instance_event(self, context, instances, events):
6693         # NOTE(danms): Some event types are handled by the manager, such
6694         # as when we're asked to update the instance's info_cache. If it's
6695         # not one of those, look for some thread(s) waiting for the event and
6696         # unblock them if so.
6697         for event in events:
6698             instance = [inst for inst in instances
6699                         if inst.uuid == event.instance_uuid][0]
6700             LOG.debug('Received event %(event)s',
6701                       {'event': event.key},
6702                       instance=instance)
6703             if event.name == 'network-changed':
6704                 try:
6705                     self.network_api.get_instance_nw_info(context, instance)
6706                 except exception.NotFound as e:
6707                     LOG.info(_LI('Failed to process external instance event '
6708                                  '%(event)s due to: %(error)s'),
6709                              {'event': event.key, 'error': six.text_type(e)},
6710                              instance=instance)
6711             elif event.name == 'network-vif-deleted':
6712                 self._process_instance_vif_deleted_event(context,
6713                                                          instance,
6714                                                          event.tag)
6715             else:
6716                 self._process_instance_event(instance, event)
6717 
6718     @periodic_task.periodic_task(spacing=CONF.image_cache_manager_interval,
6719                                  external_process_ok=True)
6720     def _run_image_cache_manager_pass(self, context):
6721         """Run a single pass of the image cache manager."""
6722 
6723         if not self.driver.capabilities["has_imagecache"]:
6724             return
6725 
6726         # Determine what other nodes use this storage
6727         storage_users.register_storage_use(CONF.instances_path, CONF.host)
6728         nodes = storage_users.get_storage_users(CONF.instances_path)
6729 
6730         # Filter all_instances to only include those nodes which share this
6731         # storage path.
6732         # TODO(mikal): this should be further refactored so that the cache
6733         # cleanup code doesn't know what those instances are, just a remote
6734         # count, and then this logic should be pushed up the stack.
6735         filters = {'deleted': False,
6736                    'soft_deleted': True,
6737                    'host': nodes}
6738         filtered_instances = objects.InstanceList.get_by_filters(context,
6739                                  filters, expected_attrs=[], use_slave=True)
6740 
6741         self.driver.manage_image_cache(context, filtered_instances)
6742 
6743     @periodic_task.periodic_task(spacing=CONF.instance_delete_interval)
6744     def _run_pending_deletes(self, context):
6745         """Retry any pending instance file deletes."""
6746         LOG.debug('Cleaning up deleted instances')
6747         filters = {'deleted': True,
6748                    'soft_deleted': False,
6749                    'host': CONF.host,
6750                    'cleaned': False}
6751         attrs = ['info_cache', 'security_groups', 'system_metadata']
6752         with utils.temporary_mutation(context, read_deleted='yes'):
6753             instances = objects.InstanceList.get_by_filters(
6754                 context, filters, expected_attrs=attrs, use_slave=True)
6755         LOG.debug('There are %d instances to clean', len(instances))
6756 
6757         for instance in instances:
6758             attempts = int(instance.system_metadata.get('clean_attempts', '0'))
6759             LOG.debug('Instance has had %(attempts)s of %(max)s '
6760                       'cleanup attempts',
6761                       {'attempts': attempts,
6762                        'max': CONF.maximum_instance_delete_attempts},
6763                       instance=instance)
6764             if attempts < CONF.maximum_instance_delete_attempts:
6765                 success = self.driver.delete_instance_files(instance)
6766 
6767                 instance.system_metadata['clean_attempts'] = str(attempts + 1)
6768                 if success:
6769                     instance.cleaned = True
6770                 with utils.temporary_mutation(context, read_deleted='yes'):
6771                     instance.save()
6772 
6773     @periodic_task.periodic_task(spacing=CONF.instance_delete_interval)
6774     def _cleanup_incomplete_migrations(self, context):
6775         """Delete instance files on failed resize/revert-resize operation
6776 
6777         During resize/revert-resize operation, if that instance gets deleted
6778         in-between then instance files might remain either on source or
6779         destination compute node because of race condition.
6780         """
6781         LOG.debug('Cleaning up deleted instances with incomplete migration ')
6782         migration_filters = {'host': CONF.host,
6783                              'status': 'error'}
6784         migrations = objects.MigrationList.get_by_filters(context,
6785                                                           migration_filters)
6786 
6787         if not migrations:
6788             return
6789 
6790         inst_uuid_from_migrations = set([migration.instance_uuid for migration
6791                                          in migrations])
6792 
6793         inst_filters = {'deleted': True, 'soft_deleted': False,
6794                         'uuid': inst_uuid_from_migrations}
6795         attrs = ['info_cache', 'security_groups', 'system_metadata']
6796         with utils.temporary_mutation(context, read_deleted='yes'):
6797             instances = objects.InstanceList.get_by_filters(
6798                 context, inst_filters, expected_attrs=attrs, use_slave=True)
6799 
6800         for instance in instances:
6801             if instance.host != CONF.host:
6802                 for migration in migrations:
6803                     if instance.uuid == migration.instance_uuid:
6804                         # Delete instance files if not cleanup properly either
6805                         # from the source or destination compute nodes when
6806                         # the instance is deleted during resizing.
6807                         self.driver.delete_instance_files(instance)
6808                         try:
6809                             migration.status = 'failed'
6810                             with migration.obj_as_admin():
6811                                 migration.save()
6812                         except exception.MigrationNotFound:
6813                             LOG.warning(_LW("Migration %s is not found."),
6814                                         migration.id,
6815                                         instance=instance)
6816                         break
6817 
6818     @messaging.expected_exceptions(exception.InstanceQuiesceNotSupported,
6819                                    exception.QemuGuestAgentNotEnabled,
6820                                    exception.NovaException,
6821                                    NotImplementedError)
6822     @wrap_exception()
6823     def quiesce_instance(self, context, instance):
6824         """Quiesce an instance on this host."""
6825         context = context.elevated()
6826         image_meta = objects.ImageMeta.from_instance(instance)
6827         self.driver.quiesce(context, instance, image_meta)
6828 
6829     def _wait_for_snapshots_completion(self, context, mapping):
6830         for mapping_dict in mapping:
6831             if mapping_dict.get('source_type') == 'snapshot':
6832 
6833                 def _wait_snapshot():
6834                     snapshot = self.volume_api.get_snapshot(
6835                         context, mapping_dict['snapshot_id'])
6836                     if snapshot.get('status') != 'creating':
6837                         raise loopingcall.LoopingCallDone()
6838 
6839                 timer = loopingcall.FixedIntervalLoopingCall(_wait_snapshot)
6840                 timer.start(interval=0.5).wait()
6841 
6842     @messaging.expected_exceptions(exception.InstanceQuiesceNotSupported,
6843                                    exception.QemuGuestAgentNotEnabled,
6844                                    exception.NovaException,
6845                                    NotImplementedError)
6846     @wrap_exception()
6847     def unquiesce_instance(self, context, instance, mapping=None):
6848         """Unquiesce an instance on this host.
6849 
6850         If snapshots' image mapping is provided, it waits until snapshots are
6851         completed before unqueiscing.
6852         """
6853         context = context.elevated()
6854         if mapping:
6855             try:
6856                 self._wait_for_snapshots_completion(context, mapping)
6857             except Exception as error:
6858                 LOG.exception(_LE("Exception while waiting completion of "
6859                                   "volume snapshots: %s"),
6860                               error, instance=instance)
6861         image_meta = objects.ImageMeta.from_instance(instance)
6862         self.driver.unquiesce(context, instance, image_meta)
