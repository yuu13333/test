Please review the code below for security defects. You can consider defect types in terms of:
1.CWE-284 (Improper Access Control)
2.CWE-435 (Improper Interaction Between Multiple Entities)
3.CWE-664 (Improper Control of a Resource Through its Lifetime)
4.CWE-682 (Incorrect Calculation)
5.CWE-691 (Insufficient Control Flow Management)
6.CWE-693 (Protection Mechanism Failure)
7.CWE-697 (Incorrect Comparison)
8.CWE-703 (Improper Check or Handling of Exceptional Conditions)
9.CWE-707 (Improper Neutralization)
10.CWE-710 (Improper Adherence to Coding Standards)
If any are found, please describe the security defect in detail and indicate the corresponding line number of code and solution. If none are detected, states: 'No security defects are detected in the code'.

1 # Copyright 2010 United States Government as represented by the
2 # Administrator of the National Aeronautics and Space Administration.
3 # All Rights Reserved.
4 # Copyright (c) 2010 Citrix Systems, Inc.
5 # Copyright (c) 2011 Piston Cloud Computing, Inc
6 # Copyright (c) 2012 University Of Minho
7 # (c) Copyright 2013 Hewlett-Packard Development Company, L.P.
8 #
9 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
10 #    not use this file except in compliance with the License. You may obtain
11 #    a copy of the License at
12 #
13 #         http://www.apache.org/licenses/LICENSE-2.0
14 #
15 #    Unless required by applicable law or agreed to in writing, software
16 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
17 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
18 #    License for the specific language governing permissions and limitations
19 #    under the License.
20 
21 """
22 A connection to a hypervisor through libvirt.
23 
24 Supports KVM, LXC, QEMU, UML, XEN and Parallels.
25 
26 """
27 
28 import binascii
29 import collections
30 from collections import deque
31 import contextlib
32 import errno
33 import functools
34 import glob
35 import itertools
36 import operator
37 import os
38 import pwd
39 import random
40 import shutil
41 import tempfile
42 import time
43 import uuid
44 
45 from castellan import key_manager
46 from copy import deepcopy
47 import eventlet
48 from eventlet import greenthread
49 from eventlet import tpool
50 from lxml import etree
51 from os_brick import encryptors
52 from os_brick.encryptors import luks as luks_encryptor
53 from os_brick import exception as brick_exception
54 from os_brick.initiator import connector
55 from oslo_concurrency import processutils
56 from oslo_log import log as logging
57 from oslo_serialization import base64
58 from oslo_serialization import jsonutils
59 from oslo_service import loopingcall
60 from oslo_utils import encodeutils
61 from oslo_utils import excutils
62 from oslo_utils import fileutils
63 from oslo_utils import importutils
64 from oslo_utils import netutils as oslo_netutils
65 from oslo_utils import strutils
66 from oslo_utils import timeutils
67 from oslo_utils import units
68 from oslo_utils import uuidutils
69 import six
70 from six.moves import range
71 
72 from nova.api.metadata import base as instance_metadata
73 from nova.api.metadata import password
74 from nova import block_device
75 from nova.compute import power_state
76 from nova.compute import task_states
77 from nova.compute import utils as compute_utils
78 import nova.conf
79 from nova.console import serial as serial_console
80 from nova.console import type as ctype
81 from nova import context as nova_context
82 from nova import crypto
83 from nova import exception
84 from nova.i18n import _
85 from nova import image
86 from nova.network import model as network_model
87 from nova import objects
88 from nova.objects import diagnostics as diagnostics_obj
89 from nova.objects import fields
90 from nova.objects import migrate_data as migrate_data_obj
91 from nova.pci import manager as pci_manager
92 from nova.pci import utils as pci_utils
93 import nova.privsep.libvirt
94 import nova.privsep.path
95 import nova.privsep.utils
96 from nova import rc_fields
97 from nova import utils
98 from nova import version
99 from nova.virt import block_device as driver_block_device
100 from nova.virt import configdrive
101 from nova.virt.disk import api as disk_api
102 from nova.virt.disk.vfs import guestfs
103 from nova.virt import driver
104 from nova.virt import firewall
105 from nova.virt import hardware
106 from nova.virt.image import model as imgmodel
107 from nova.virt import images
108 from nova.virt.libvirt import blockinfo
109 from nova.virt.libvirt import config as vconfig
110 from nova.virt.libvirt import designer
111 from nova.virt.libvirt import firewall as libvirt_firewall
112 from nova.virt.libvirt import guest as libvirt_guest
113 from nova.virt.libvirt import host
114 from nova.virt.libvirt import imagebackend
115 from nova.virt.libvirt import imagecache
116 from nova.virt.libvirt import instancejobtracker
117 from nova.virt.libvirt import migration as libvirt_migrate
118 from nova.virt.libvirt.storage import dmcrypt
119 from nova.virt.libvirt.storage import lvm
120 from nova.virt.libvirt.storage import rbd_utils
121 from nova.virt.libvirt import utils as libvirt_utils
122 from nova.virt.libvirt import vif as libvirt_vif
123 from nova.virt.libvirt.volume import mount
124 from nova.virt.libvirt.volume import remotefs
125 from nova.virt import netutils
126 from nova.volume import cinder
127 
128 libvirt = None
129 
130 uefi_logged = False
131 
132 LOG = logging.getLogger(__name__)
133 
134 CONF = nova.conf.CONF
135 
136 DEFAULT_FIREWALL_DRIVER = "%s.%s" % (
137     libvirt_firewall.__name__,
138     libvirt_firewall.IptablesFirewallDriver.__name__)
139 
140 DEFAULT_UEFI_LOADER_PATH = {
141     "x86_64": "/usr/share/OVMF/OVMF_CODE.fd",
142     "aarch64": "/usr/share/AAVMF/AAVMF_CODE.fd"
143 }
144 
145 MAX_CONSOLE_BYTES = 100 * units.Ki
146 
147 # The libvirt driver will prefix any disable reason codes with this string.
148 DISABLE_PREFIX = 'AUTO: '
149 # Disable reason for the service which was enabled or disabled without reason
150 DISABLE_REASON_UNDEFINED = None
151 
152 # Guest config console string
153 CONSOLE = "console=tty0 console=ttyS0 console=hvc0"
154 
155 GuestNumaConfig = collections.namedtuple(
156     'GuestNumaConfig', ['cpuset', 'cputune', 'numaconfig', 'numatune'])
157 
158 
159 class InjectionInfo(collections.namedtuple(
160         'InjectionInfo', ['network_info', 'files', 'admin_pass'])):
161     __slots__ = ()
162 
163     def __repr__(self):
164         return ('InjectionInfo(network_info=%r, files=%r, '
165                 'admin_pass=<SANITIZED>)') % (self.network_info, self.files)
166 
167 libvirt_volume_drivers = [
168     'iscsi=nova.virt.libvirt.volume.iscsi.LibvirtISCSIVolumeDriver',
169     'iser=nova.virt.libvirt.volume.iser.LibvirtISERVolumeDriver',
170     'local=nova.virt.libvirt.volume.volume.LibvirtVolumeDriver',
171     'drbd=nova.virt.libvirt.volume.drbd.LibvirtDRBDVolumeDriver',
172     'fake=nova.virt.libvirt.volume.volume.LibvirtFakeVolumeDriver',
173     'rbd=nova.virt.libvirt.volume.net.LibvirtNetVolumeDriver',
174     'sheepdog=nova.virt.libvirt.volume.net.LibvirtNetVolumeDriver',
175     'nfs=nova.virt.libvirt.volume.nfs.LibvirtNFSVolumeDriver',
176     'smbfs=nova.virt.libvirt.volume.smbfs.LibvirtSMBFSVolumeDriver',
177     'aoe=nova.virt.libvirt.volume.aoe.LibvirtAOEVolumeDriver',
178     'fibre_channel='
179         'nova.virt.libvirt.volume.fibrechannel.'
180         'LibvirtFibreChannelVolumeDriver',
181     'gpfs=nova.virt.libvirt.volume.gpfs.LibvirtGPFSVolumeDriver',
182     'quobyte=nova.virt.libvirt.volume.quobyte.LibvirtQuobyteVolumeDriver',
183     'hgst=nova.virt.libvirt.volume.hgst.LibvirtHGSTVolumeDriver',
184     'scaleio=nova.virt.libvirt.volume.scaleio.LibvirtScaleIOVolumeDriver',
185     'disco=nova.virt.libvirt.volume.disco.LibvirtDISCOVolumeDriver',
186     'vzstorage='
187         'nova.virt.libvirt.volume.vzstorage.LibvirtVZStorageVolumeDriver',
188     'veritas_hyperscale='
189         'nova.virt.libvirt.volume.vrtshyperscale.'
190         'LibvirtHyperScaleVolumeDriver',
191     'storpool=nova.virt.libvirt.volume.storpool.LibvirtStorPoolVolumeDriver',
192     'nvmeof=nova.virt.libvirt.volume.nvme.LibvirtNVMEVolumeDriver',
193 ]
194 
195 
196 def patch_tpool_proxy():
197     """eventlet.tpool.Proxy doesn't work with old-style class in __str__()
198     or __repr__() calls. See bug #962840 for details.
199     We perform a monkey patch to replace those two instance methods.
200     """
201     def str_method(self):
202         return str(self._obj)
203 
204     def repr_method(self):
205         return repr(self._obj)
206 
207     tpool.Proxy.__str__ = str_method
208     tpool.Proxy.__repr__ = repr_method
209 
210 
211 patch_tpool_proxy()
212 
213 # For information about when MIN_LIBVIRT_VERSION and
214 # NEXT_MIN_LIBVIRT_VERSION can be changed, consult
215 #
216 #   https://wiki.openstack.org/wiki/LibvirtDistroSupportMatrix
217 #
218 # Currently this is effectively the min version for i686/x86_64
219 # + KVM/QEMU, as other architectures/hypervisors require newer
220 # versions. Over time, this will become a common min version
221 # for all architectures/hypervisors, as this value rises to
222 # meet them.
223 MIN_LIBVIRT_VERSION = (1, 3, 1)
224 MIN_QEMU_VERSION = (2, 5, 0)
225 # TODO(berrange): Re-evaluate this at start of each release cycle
226 # to decide if we want to plan a future min version bump.
227 # MIN_LIBVIRT_VERSION can be updated to match this after
228 # NEXT_MIN_LIBVIRT_VERSION  has been at a higher value for
229 # one cycle
230 NEXT_MIN_LIBVIRT_VERSION = (4, 0, 0)
231 NEXT_MIN_QEMU_VERSION = (2, 11, 0)
232 
233 
234 # Virtuozzo driver support
235 MIN_VIRTUOZZO_VERSION = (7, 0, 0)
236 
237 # Ability to set the user guest password with parallels
238 MIN_LIBVIRT_PARALLELS_SET_ADMIN_PASSWD = (2, 0, 0)
239 
240 # Use the "logd" backend for handling stdout/stderr from QEMU processes.
241 MIN_LIBVIRT_VIRTLOGD = (1, 3, 3)
242 MIN_QEMU_VIRTLOGD = (2, 7, 0)
243 
244 
245 # aarch64 architecture with KVM
246 # 'chardev' support got sorted out in 3.6.0
247 MIN_LIBVIRT_KVM_AARCH64_VERSION = (3, 6, 0)
248 
249 # Names of the types that do not get compressed during migration
250 NO_COMPRESSION_TYPES = ('qcow2',)
251 
252 
253 # number of serial console limit
254 QEMU_MAX_SERIAL_PORTS = 4
255 # Qemu supports 4 serial consoles, we remove 1 because of the PTY one defined
256 ALLOWED_QEMU_SERIAL_PORTS = QEMU_MAX_SERIAL_PORTS - 1
257 
258 # libvirt postcopy support
259 MIN_LIBVIRT_POSTCOPY_VERSION = (1, 3, 3)
260 
261 MIN_LIBVIRT_OTHER_ARCH = {
262     fields.Architecture.AARCH64: MIN_LIBVIRT_KVM_AARCH64_VERSION,
263 }
264 
265 # perf events support
266 MIN_LIBVIRT_PERF_VERSION = (2, 0, 0)
267 LIBVIRT_PERF_EVENT_PREFIX = 'VIR_PERF_PARAM_'
268 
269 PERF_EVENTS_CPU_FLAG_MAPPING = {'cmt': 'cmt',
270                                 'mbml': 'mbm_local',
271                                 'mbmt': 'mbm_total',
272                                }
273 
274 # Mediated devices support
275 MIN_LIBVIRT_MDEV_SUPPORT = (3, 4, 0)
276 
277 # libvirt>=3.10 is required for volume multiattach unless qemu<2.10.
278 # See https://bugzilla.redhat.com/show_bug.cgi?id=1378242
279 # for details.
280 MIN_LIBVIRT_MULTIATTACH = (3, 10, 0)
281 
282 MIN_LIBVIRT_LUKS_VERSION = (2, 2, 0)
283 MIN_QEMU_LUKS_VERSION = (2, 6, 0)
284 
285 MIN_LIBVIRT_FILE_BACKED_VERSION = (4, 0, 0)
286 MIN_QEMU_FILE_BACKED_VERSION = (2, 6, 0)
287 
288 MIN_LIBVIRT_FILE_BACKED_DISCARD_VERSION = (4, 4, 0)
289 MIN_QEMU_FILE_BACKED_DISCARD_VERSION = (2, 10, 0)
290 
291 MIN_LIBVIRT_NATIVE_TLS_VERSION = (4, 4, 0)
292 MIN_QEMU_NATIVE_TLS_VERSION = (2, 11, 0)
293 
294 VGPU_RESOURCE_SEMAPHORE = "vgpu_resources"
295 
296 
297 class LibvirtDriver(driver.ComputeDriver):
298     capabilities = {
299         "has_imagecache": True,
300         "supports_evacuate": True,
301         "supports_migrate_to_same_host": False,
302         "supports_attach_interface": True,
303         "supports_device_tagging": True,
304         "supports_tagged_attach_interface": True,
305         "supports_tagged_attach_volume": True,
306         "supports_extend_volume": True,
307         # Multiattach support is conditional on qemu and libvirt versions
308         # determined in init_host.
309         "supports_multiattach": False,
310         "supports_trusted_certs": True,
311     }
312 
313     def __init__(self, virtapi, read_only=False):
314         super(LibvirtDriver, self).__init__(virtapi)
315 
316         global libvirt
317         if libvirt is None:
318             libvirt = importutils.import_module('libvirt')
319             libvirt_migrate.libvirt = libvirt
320 
321         self._host = host.Host(self._uri(), read_only,
322                                lifecycle_event_handler=self.emit_event,
323                                conn_event_handler=self._handle_conn_event)
324         self._initiator = None
325         self._fc_wwnns = None
326         self._fc_wwpns = None
327         self._caps = None
328         self._supported_perf_events = []
329         self.firewall_driver = firewall.load_driver(
330             DEFAULT_FIREWALL_DRIVER,
331             host=self._host)
332 
333         self.vif_driver = libvirt_vif.LibvirtGenericVIFDriver()
334 
335         # TODO(mriedem): Long-term we should load up the volume drivers on
336         # demand as needed rather than doing this on startup, as there might
337         # be unsupported volume drivers in this list based on the underlying
338         # platform.
339         self.volume_drivers = self._get_volume_drivers()
340 
341         self._disk_cachemode = None
342         self.image_cache_manager = imagecache.ImageCacheManager()
343         self.image_backend = imagebackend.Backend(CONF.use_cow_images)
344 
345         self.disk_cachemodes = {}
346 
347         self.valid_cachemodes = ["default",
348                                  "none",
349                                  "writethrough",
350                                  "writeback",
351                                  "directsync",
352                                  "unsafe",
353                                 ]
354         self._conn_supports_start_paused = CONF.libvirt.virt_type in ('kvm',
355                                                                       'qemu')
356 
357         for mode_str in CONF.libvirt.disk_cachemodes:
358             disk_type, sep, cache_mode = mode_str.partition('=')
359             if cache_mode not in self.valid_cachemodes:
360                 LOG.warning('Invalid cachemode %(cache_mode)s specified '
361                             'for disk type %(disk_type)s.',
362                             {'cache_mode': cache_mode, 'disk_type': disk_type})
363                 continue
364             self.disk_cachemodes[disk_type] = cache_mode
365 
366         self._volume_api = cinder.API()
367         self._image_api = image.API()
368 
369         sysinfo_serial_funcs = {
370             'none': lambda: None,
371             'hardware': self._get_host_sysinfo_serial_hardware,
372             'os': self._get_host_sysinfo_serial_os,
373             'auto': self._get_host_sysinfo_serial_auto,
374         }
375 
376         self._sysinfo_serial_func = sysinfo_serial_funcs.get(
377             CONF.libvirt.sysinfo_serial)
378 
379         self.job_tracker = instancejobtracker.InstanceJobTracker()
380         self._remotefs = remotefs.RemoteFilesystem()
381 
382         self._live_migration_flags = self._block_migration_flags = 0
383         self.active_migrations = {}
384 
385         # Compute reserved hugepages from conf file at the very
386         # beginning to ensure any syntax error will be reported and
387         # avoid any re-calculation when computing resources.
388         self._reserved_hugepages = hardware.numa_get_reserved_huge_pages()
389 
390     def _get_volume_drivers(self):
391         driver_registry = dict()
392 
393         for driver_str in libvirt_volume_drivers:
394             driver_type, _sep, driver = driver_str.partition('=')
395             driver_class = importutils.import_class(driver)
396             try:
397                 driver_registry[driver_type] = driver_class(self._host)
398             except brick_exception.InvalidConnectorProtocol:
399                 LOG.debug('Unable to load volume driver %s. It is not '
400                           'supported on this host.', driver)
401 
402         return driver_registry
403 
404     @property
405     def disk_cachemode(self):
406         if self._disk_cachemode is None:
407             # We prefer 'none' for consistent performance, host crash
408             # safety & migration correctness by avoiding host page cache.
409             # Some filesystems don't support O_DIRECT though. For those we
410             # fallback to 'writethrough' which gives host crash safety, and
411             # is safe for migration provided the filesystem is cache coherent
412             # (cluster filesystems typically are, but things like NFS are not).
413             self._disk_cachemode = "none"
414             if not nova.privsep.utils.supports_direct_io(CONF.instances_path):
415                 self._disk_cachemode = "writethrough"
416         return self._disk_cachemode
417 
418     def _set_cache_mode(self, conf):
419         """Set cache mode on LibvirtConfigGuestDisk object."""
420         try:
421             source_type = conf.source_type
422             driver_cache = conf.driver_cache
423         except AttributeError:
424             return
425 
426         # Shareable disks like for a multi-attach volume need to have the
427         # driver cache disabled.
428         if getattr(conf, 'shareable', False):
429             conf.driver_cache = 'none'
430         else:
431             cache_mode = self.disk_cachemodes.get(source_type,
432                                                   driver_cache)
433             conf.driver_cache = cache_mode
434 
435     def _do_quality_warnings(self):
436         """Warn about potential configuration issues.
437 
438         This will log a warning message for things such as untested driver or
439         host arch configurations in order to indicate potential issues to
440         administrators.
441         """
442         caps = self._host.get_capabilities()
443         hostarch = caps.host.cpu.arch
444         if (CONF.libvirt.virt_type not in ('qemu', 'kvm') or
445             hostarch not in (fields.Architecture.I686,
446                              fields.Architecture.X86_64)):
447             LOG.warning('The libvirt driver is not tested on '
448                         '%(type)s/%(arch)s by the OpenStack project and '
449                         'thus its quality can not be ensured. For more '
450                         'information, see: https://docs.openstack.org/'
451                         'nova/latest/user/support-matrix.html',
452                         {'type': CONF.libvirt.virt_type, 'arch': hostarch})
453 
454         if CONF.vnc.keymap:
455             LOG.warning('The option "[vnc] keymap" has been deprecated '
456                         'in favor of configuration within the guest. '
457                         'Update nova.conf to address this change and '
458                         'refer to bug #1682020 for more information.')
459 
460         if CONF.spice.keymap:
461             LOG.warning('The option "[spice] keymap" has been deprecated '
462                         'in favor of configuration within the guest. '
463                         'Update nova.conf to address this change and '
464                         'refer to bug #1682020 for more information.')
465 
466     def _handle_conn_event(self, enabled, reason):
467         LOG.info("Connection event '%(enabled)d' reason '%(reason)s'",
468                  {'enabled': enabled, 'reason': reason})
469         self._set_host_enabled(enabled, reason)
470 
471     def init_host(self, host):
472         self._host.initialize()
473 
474         self._do_quality_warnings()
475 
476         self._parse_migration_flags()
477 
478         self._supported_perf_events = self._get_supported_perf_events()
479 
480         self._set_multiattach_support()
481 
482         self._check_file_backed_memory_support()
483 
484         if (CONF.libvirt.virt_type == 'lxc' and
485                 not (CONF.libvirt.uid_maps and CONF.libvirt.gid_maps)):
486             LOG.warning("Running libvirt-lxc without user namespaces is "
487                         "dangerous. Containers spawned by Nova will be run "
488                         "as the host's root user. It is highly suggested "
489                         "that user namespaces be used in a public or "
490                         "multi-tenant environment.")
491 
492         # Stop libguestfs using KVM unless we're also configured
493         # to use this. This solves problem where people need to
494         # stop Nova use of KVM because nested-virt is broken
495         if CONF.libvirt.virt_type != "kvm":
496             guestfs.force_tcg()
497 
498         if not self._host.has_min_version(MIN_LIBVIRT_VERSION):
499             raise exception.InternalError(
500                 _('Nova requires libvirt version %s or greater.') %
501                 libvirt_utils.version_to_string(MIN_LIBVIRT_VERSION))
502 
503         if CONF.libvirt.virt_type in ("qemu", "kvm"):
504             if self._host.has_min_version(hv_ver=MIN_QEMU_VERSION):
505                 # "qemu-img info" calls are version dependent, so we need to
506                 # store the version in the images module.
507                 images.QEMU_VERSION = self._host.get_connection().getVersion()
508             else:
509                 raise exception.InternalError(
510                     _('Nova requires QEMU version %s or greater.') %
511                     libvirt_utils.version_to_string(MIN_QEMU_VERSION))
512 
513         if CONF.libvirt.virt_type == 'parallels':
514             if not self._host.has_min_version(hv_ver=MIN_VIRTUOZZO_VERSION):
515                 raise exception.InternalError(
516                     _('Nova requires Virtuozzo version %s or greater.') %
517                     libvirt_utils.version_to_string(MIN_VIRTUOZZO_VERSION))
518 
519         # Give the cloud admin a heads up if we are intending to
520         # change the MIN_LIBVIRT_VERSION in the next release.
521         if not self._host.has_min_version(NEXT_MIN_LIBVIRT_VERSION):
522             LOG.warning('Running Nova with a libvirt version less than '
523                         '%(version)s is deprecated. The required minimum '
524                         'version of libvirt will be raised to %(version)s '
525                         'in the next release.',
526                         {'version': libvirt_utils.version_to_string(
527                             NEXT_MIN_LIBVIRT_VERSION)})
528         if (CONF.libvirt.virt_type in ("qemu", "kvm") and
529             not self._host.has_min_version(hv_ver=NEXT_MIN_QEMU_VERSION)):
530             LOG.warning('Running Nova with a QEMU version less than '
531                         '%(version)s is deprecated. The required minimum '
532                         'version of QEMU will be raised to %(version)s '
533                         'in the next release.',
534                         {'version': libvirt_utils.version_to_string(
535                             NEXT_MIN_QEMU_VERSION)})
536 
537         kvm_arch = fields.Architecture.from_host()
538         if (CONF.libvirt.virt_type in ('kvm', 'qemu') and
539             kvm_arch in MIN_LIBVIRT_OTHER_ARCH and
540                 not self._host.has_min_version(
541                     MIN_LIBVIRT_OTHER_ARCH.get(kvm_arch))):
542             raise exception.InternalError(
543                 _('Running Nova with qemu/kvm virt_type on %(arch)s '
544                   'requires libvirt version %(libvirt_ver)s or greater') %
545                 {'arch': kvm_arch,
546                  'libvirt_ver': libvirt_utils.version_to_string(
547                      MIN_LIBVIRT_OTHER_ARCH.get(kvm_arch))})
548 
549         # Allowing both "tunnelling via libvirtd" (which will be
550         # deprecated once the MIN_{LIBVIRT,QEMU}_VERSION is sufficiently
551         # new enough) and "native TLS" options at the same time is
552         # nonsensical.
553         if (CONF.libvirt.live_migration_tunnelled and
554             CONF.libvirt.live_migration_with_native_tls):
555                 msg = _("Setting both 'live_migration_tunnelled' and "
556                         "'live_migration_with_native_tls' at the same "
557                         "time is invalid. If you have the relevant "
558                         "libvirt and QEMU versions, and TLS configured "
559                         "in your environment, pick "
560                         "'live_migration_with_native_tls'.")
561                 raise exception.Invalid(msg)
562 
563         # TODO(sbauza): Remove this code once mediated devices are persisted
564         # across reboots.
565         if self._host.has_min_version(MIN_LIBVIRT_MDEV_SUPPORT):
566             self._recreate_assigned_mediated_devices()
567 
568     @staticmethod
569     def _is_existing_mdev(uuid):
570         # FIXME(sbauza): Some kernel can have a uevent race meaning that the
571         # libvirt daemon won't know when a mediated device is created unless
572         # you restart that daemon. Until all kernels we support are not having
573         # that possible race, check the sysfs directly instead of asking the
574         # libvirt API.
575         # See https://bugzilla.redhat.com/show_bug.cgi?id=1376907 for ref.
576         return os.path.exists('/sys/bus/mdev/devices/{0}'.format(uuid))
577 
578     def _recreate_assigned_mediated_devices(self):
579         """Recreate assigned mdevs that could have disappeared if we reboot
580         the host.
581         """
582         mdevs = self._get_all_assigned_mediated_devices()
583         requested_types = self._get_supported_vgpu_types()
584         for (mdev_uuid, instance_uuid) in six.iteritems(mdevs):
585             if not self._is_existing_mdev(mdev_uuid):
586                 self._create_new_mediated_device(requested_types, mdev_uuid)
587 
588     def _set_multiattach_support(self):
589         # Check to see if multiattach is supported. Based on bugzilla
590         # https://bugzilla.redhat.com/show_bug.cgi?id=1378242 and related
591         # clones, the shareable flag on a disk device will only work with
592         # qemu<2.10 or libvirt>=3.10. So check those versions here and set
593         # the capability appropriately.
594         if (self._host.has_min_version(lv_ver=MIN_LIBVIRT_MULTIATTACH) or
595                 not self._host.has_min_version(hv_ver=(2, 10, 0))):
596             self.capabilities['supports_multiattach'] = True
597         else:
598             LOG.debug('Volume multiattach is not supported based on current '
599                       'versions of QEMU and libvirt. QEMU must be less than '
600                       '2.10 or libvirt must be greater than or equal to 3.10.')
601 
602     def _check_file_backed_memory_support(self):
603         if CONF.libvirt.file_backed_memory:
604             # file_backed_memory is only compatible with qemu/kvm virts
605             if CONF.libvirt.virt_type not in ("qemu", "kvm"):
606                 raise exception.InternalError(
607                     _('Running Nova with file_backed_memory and virt_type '
608                       '%(type)s is not supported. file_backed_memory is only '
609                       'supported with qemu and kvm types.') %
610                     {'type': CONF.libvirt.virt_type})
611 
612             # Check needed versions for file_backed_memory
613             if not self._host.has_min_version(
614                     MIN_LIBVIRT_FILE_BACKED_VERSION,
615                     MIN_QEMU_FILE_BACKED_VERSION):
616                 raise exception.InternalError(
617                     _('Running Nova with file_backed_memory requires libvirt '
618                       'version %(libvirt)s and qemu version %(qemu)s') %
619                     {'libvirt': libvirt_utils.version_to_string(
620                         MIN_LIBVIRT_FILE_BACKED_VERSION),
621                     'qemu': libvirt_utils.version_to_string(
622                         MIN_QEMU_FILE_BACKED_VERSION)})
623 
624             # file-backed memory doesn't work with memory overcommit.
625             # Block service startup if file-backed memory is enabled and
626             # ram_allocation_ratio is not 1.0
627             if CONF.ram_allocation_ratio != 1.0:
628                 raise exception.InternalError(
629                     'Running Nova with file_backed_memory requires '
630                     'ram_allocation_ratio configured to 1.0')
631 
632     def _prepare_migration_flags(self):
633         migration_flags = 0
634 
635         migration_flags |= libvirt.VIR_MIGRATE_LIVE
636 
637         # Adding p2p flag only if xen is not in use, because xen does not
638         # support p2p migrations
639         if CONF.libvirt.virt_type != 'xen':
640             migration_flags |= libvirt.VIR_MIGRATE_PEER2PEER
641 
642         # Adding VIR_MIGRATE_UNDEFINE_SOURCE because, without it, migrated
643         # instance will remain defined on the source host
644         migration_flags |= libvirt.VIR_MIGRATE_UNDEFINE_SOURCE
645 
646         # Adding VIR_MIGRATE_PERSIST_DEST to persist the VM on the
647         # destination host
648         migration_flags |= libvirt.VIR_MIGRATE_PERSIST_DEST
649 
650         live_migration_flags = block_migration_flags = migration_flags
651 
652         # Adding VIR_MIGRATE_NON_SHARED_INC, otherwise all block-migrations
653         # will be live-migrations instead
654         block_migration_flags |= libvirt.VIR_MIGRATE_NON_SHARED_INC
655 
656         return (live_migration_flags, block_migration_flags)
657 
658     # TODO(kchamart) Once the MIN_LIBVIRT_VERSION and MIN_QEMU_VERSION
659     # reach 4.4.0 and 2.11.0, which provide "native TLS" support by
660     # default, deprecate and remove the support for "tunnelled live
661     # migration" (and related config attribute), because:
662     #
663     #  (a) it cannot handle live migration of disks in a non-shared
664     #      storage setup (a.k.a. "block migration");
665     #
666     #  (b) has a huge performance overhead and latency, because it burns
667     #      more CPU and memory bandwidth due to increased number of data
668     #      copies on both source and destination hosts.
669     #
670     # Both the above limitations are addressed by the QEMU-native TLS
671     # support (`live_migration_with_native_tls`).
672     def _handle_live_migration_tunnelled(self, migration_flags):
673         if CONF.libvirt.live_migration_tunnelled:
674             migration_flags |= libvirt.VIR_MIGRATE_TUNNELLED
675         return migration_flags
676 
677     def _is_native_tls_available(self):
678         return self._host.has_min_version(MIN_LIBVIRT_NATIVE_TLS_VERSION,
679                                           MIN_QEMU_NATIVE_TLS_VERSION)
680 
681     def _handle_native_tls(self, migration_flags):
682         if (CONF.libvirt.live_migration_with_native_tls and
683                 self._is_native_tls_available()):
684             migration_flags |= libvirt.VIR_MIGRATE_TLS
685         return migration_flags
686 
687     def _is_post_copy_available(self):
688         return self._host.has_min_version(lv_ver=MIN_LIBVIRT_POSTCOPY_VERSION)
689 
690     def _is_virtlogd_available(self):
691         return self._host.has_min_version(MIN_LIBVIRT_VIRTLOGD,
692                                           MIN_QEMU_VIRTLOGD)
693 
694     def _is_native_luks_available(self):
695         return self._host.has_min_version(MIN_LIBVIRT_LUKS_VERSION,
696                                           MIN_QEMU_LUKS_VERSION)
697 
698     def _handle_live_migration_post_copy(self, migration_flags):
699         if CONF.libvirt.live_migration_permit_post_copy:
700             if self._is_post_copy_available():
701                 migration_flags |= libvirt.VIR_MIGRATE_POSTCOPY
702             else:
703                 LOG.info('The live_migration_permit_post_copy is set '
704                          'to True, but it is not supported.')
705         return migration_flags
706 
707     def _handle_live_migration_auto_converge(self, migration_flags):
708         if (self._is_post_copy_available() and
709                 (migration_flags & libvirt.VIR_MIGRATE_POSTCOPY) != 0):
710             LOG.info('The live_migration_permit_post_copy is set to '
711                      'True and post copy live migration is available '
712                      'so auto-converge will not be in use.')
713         elif CONF.libvirt.live_migration_permit_auto_converge:
714             migration_flags |= libvirt.VIR_MIGRATE_AUTO_CONVERGE
715         return migration_flags
716 
717     def _parse_migration_flags(self):
718         (live_migration_flags,
719             block_migration_flags) = self._prepare_migration_flags()
720 
721         live_migration_flags = self._handle_live_migration_tunnelled(
722             live_migration_flags)
723         block_migration_flags = self._handle_live_migration_tunnelled(
724             block_migration_flags)
725 
726         live_migration_flags = self._handle_native_tls(
727             live_migration_flags)
728         block_migration_flags = self._handle_native_tls(
729             block_migration_flags)
730 
731         live_migration_flags = self._handle_live_migration_post_copy(
732             live_migration_flags)
733         block_migration_flags = self._handle_live_migration_post_copy(
734             block_migration_flags)
735 
736         live_migration_flags = self._handle_live_migration_auto_converge(
737             live_migration_flags)
738         block_migration_flags = self._handle_live_migration_auto_converge(
739             block_migration_flags)
740 
741         self._live_migration_flags = live_migration_flags
742         self._block_migration_flags = block_migration_flags
743 
744     # TODO(sahid): This method is targeted for removal when the tests
745     # have been updated to avoid its use
746     #
747     # All libvirt API calls on the libvirt.Connect object should be
748     # encapsulated by methods on the nova.virt.libvirt.host.Host
749     # object, rather than directly invoking the libvirt APIs. The goal
750     # is to avoid a direct dependency on the libvirt API from the
751     # driver.py file.
752     def _get_connection(self):
753         return self._host.get_connection()
754 
755     _conn = property(_get_connection)
756 
757     @staticmethod
758     def _uri():
759         if CONF.libvirt.virt_type == 'uml':
760             uri = CONF.libvirt.connection_uri or 'uml:///system'
761         elif CONF.libvirt.virt_type == 'xen':
762             uri = CONF.libvirt.connection_uri or 'xen:///'
763         elif CONF.libvirt.virt_type == 'lxc':
764             uri = CONF.libvirt.connection_uri or 'lxc:///'
765         elif CONF.libvirt.virt_type == 'parallels':
766             uri = CONF.libvirt.connection_uri or 'parallels:///system'
767         else:
768             uri = CONF.libvirt.connection_uri or 'qemu:///system'
769         return uri
770 
771     @staticmethod
772     def _live_migration_uri(dest):
773         uris = {
774             'kvm': 'qemu+%s://%s/system',
775             'qemu': 'qemu+%s://%s/system',
776             'xen': 'xenmigr://%s/system',
777             'parallels': 'parallels+tcp://%s/system',
778         }
779         dest = oslo_netutils.escape_ipv6(dest)
780 
781         virt_type = CONF.libvirt.virt_type
782         # TODO(pkoniszewski): Remove fetching live_migration_uri in Pike
783         uri = CONF.libvirt.live_migration_uri
784         if uri:
785             return uri % dest
786 
787         uri = uris.get(virt_type)
788         if uri is None:
789             raise exception.LiveMigrationURINotAvailable(virt_type=virt_type)
790 
791         str_format = (dest,)
792         if virt_type in ('kvm', 'qemu'):
793             scheme = CONF.libvirt.live_migration_scheme or 'tcp'
794             str_format = (scheme, dest)
795         return uris.get(virt_type) % str_format
796 
797     @staticmethod
798     def _migrate_uri(dest):
799         uri = None
800         dest = oslo_netutils.escape_ipv6(dest)
801 
802         # Only QEMU live migrations supports migrate-uri parameter
803         virt_type = CONF.libvirt.virt_type
804         if virt_type in ('qemu', 'kvm'):
805             # QEMU accept two schemes: tcp and rdma.  By default
806             # libvirt build the URI using the remote hostname and the
807             # tcp schema.
808             uri = 'tcp://%s' % dest
809         # Because dest might be of type unicode, here we might return value of
810         # type unicode as well which is not acceptable by libvirt python
811         # binding when Python 2.7 is in use, so let's convert it explicitly
812         # back to string. When Python 3.x is in use, libvirt python binding
813         # accepts unicode type so it is completely fine to do a no-op str(uri)
814         # conversion which will return value of type unicode.
815         return uri and str(uri)
816 
817     def instance_exists(self, instance):
818         """Efficient override of base instance_exists method."""
819         try:
820             self._host.get_guest(instance)
821             return True
822         except (exception.InternalError, exception.InstanceNotFound):
823             return False
824 
825     def estimate_instance_overhead(self, instance_info):
826         overhead = super(LibvirtDriver, self).estimate_instance_overhead(
827             instance_info)
828         if isinstance(instance_info, objects.Flavor):
829             # A flavor object is passed during case of migrate
830             emu_policy = hardware.get_emulator_thread_policy_constraint(
831                 instance_info)
832             if emu_policy == fields.CPUEmulatorThreadsPolicy.ISOLATE:
833                 overhead['vcpus'] += 1
834         else:
835             # An instance object is passed during case of spawing or a
836             # dict is passed when computing resource for an instance
837             numa_topology = hardware.instance_topology_from_instance(
838                 instance_info)
839             if numa_topology and numa_topology.emulator_threads_isolated:
840                 overhead['vcpus'] += 1
841         return overhead
842 
843     def list_instances(self):
844         names = []
845         for guest in self._host.list_guests(only_running=False):
846             names.append(guest.name)
847 
848         return names
849 
850     def list_instance_uuids(self):
851         uuids = []
852         for guest in self._host.list_guests(only_running=False):
853             uuids.append(guest.uuid)
854 
855         return uuids
856 
857     def plug_vifs(self, instance, network_info):
858         """Plug VIFs into networks."""
859         for vif in network_info:
860             self.vif_driver.plug(instance, vif)
861 
862     def _unplug_vifs(self, instance, network_info, ignore_errors):
863         """Unplug VIFs from networks."""
864         for vif in network_info:
865             try:
866                 self.vif_driver.unplug(instance, vif)
867             except exception.NovaException:
868                 if not ignore_errors:
869                     raise
870 
871     def unplug_vifs(self, instance, network_info):
872         self._unplug_vifs(instance, network_info, False)
873 
874     def _teardown_container(self, instance):
875         inst_path = libvirt_utils.get_instance_path(instance)
876         container_dir = os.path.join(inst_path, 'rootfs')
877         rootfs_dev = instance.system_metadata.get('rootfs_device_name')
878         LOG.debug('Attempting to teardown container at path %(dir)s with '
879                   'root device: %(rootfs_dev)s',
880                   {'dir': container_dir, 'rootfs_dev': rootfs_dev},
881                   instance=instance)
882         disk_api.teardown_container(container_dir, rootfs_dev)
883 
884     def _destroy(self, instance, attempt=1):
885         try:
886             guest = self._host.get_guest(instance)
887             if CONF.serial_console.enabled:
888                 # This method is called for several events: destroy,
889                 # rebuild, hard-reboot, power-off - For all of these
890                 # events we want to release the serial ports acquired
891                 # for the guest before destroying it.
892                 serials = self._get_serial_ports_from_guest(guest)
893                 for hostname, port in serials:
894                     serial_console.release_port(host=hostname, port=port)
895         except exception.InstanceNotFound:
896             guest = None
897 
898         # If the instance is already terminated, we're still happy
899         # Otherwise, destroy it
900         old_domid = -1
901         if guest is not None:
902             try:
903                 old_domid = guest.id
904                 guest.poweroff()
905 
906             except libvirt.libvirtError as e:
907                 is_okay = False
908                 errcode = e.get_error_code()
909                 if errcode == libvirt.VIR_ERR_NO_DOMAIN:
910                     # Domain already gone. This can safely be ignored.
911                     is_okay = True
912                 elif errcode == libvirt.VIR_ERR_OPERATION_INVALID:
913                     # If the instance is already shut off, we get this:
914                     # Code=55 Error=Requested operation is not valid:
915                     # domain is not running
916 
917                     state = guest.get_power_state(self._host)
918                     if state == power_state.SHUTDOWN:
919                         is_okay = True
920                 elif errcode == libvirt.VIR_ERR_INTERNAL_ERROR:
921                     errmsg = e.get_error_message()
922                     if (CONF.libvirt.virt_type == 'lxc' and
923                         errmsg == 'internal error: '
924                                   'Some processes refused to die'):
925                         # Some processes in the container didn't die
926                         # fast enough for libvirt. The container will
927                         # eventually die. For now, move on and let
928                         # the wait_for_destroy logic take over.
929                         is_okay = True
930                 elif errcode == libvirt.VIR_ERR_OPERATION_TIMEOUT:
931                     LOG.warning("Cannot destroy instance, operation time out",
932                                 instance=instance)
933                     reason = _("operation time out")
934                     raise exception.InstancePowerOffFailure(reason=reason)
935                 elif errcode == libvirt.VIR_ERR_SYSTEM_ERROR:
936                     if e.get_int1() == errno.EBUSY:
937                         # NOTE(danpb): When libvirt kills a process it sends it
938                         # SIGTERM first and waits 10 seconds. If it hasn't gone
939                         # it sends SIGKILL and waits another 5 seconds. If it
940                         # still hasn't gone then you get this EBUSY error.
941                         # Usually when a QEMU process fails to go away upon
942                         # SIGKILL it is because it is stuck in an
943                         # uninterruptible kernel sleep waiting on I/O from
944                         # some non-responsive server.
945                         # Given the CPU load of the gate tests though, it is
946                         # conceivable that the 15 second timeout is too short,
947                         # particularly if the VM running tempest has a high
948                         # steal time from the cloud host. ie 15 wallclock
949                         # seconds may have passed, but the VM might have only
950                         # have a few seconds of scheduled run time.
951                         LOG.warning('Error from libvirt during destroy. '
952                                     'Code=%(errcode)s Error=%(e)s; '
953                                     'attempt %(attempt)d of 3',
954                                     {'errcode': errcode, 'e': e,
955                                      'attempt': attempt},
956                                     instance=instance)
957                         with excutils.save_and_reraise_exception() as ctxt:
958                             # Try up to 3 times before giving up.
959                             if attempt < 3:
960                                 ctxt.reraise = False
961                                 self._destroy(instance, attempt + 1)
962                                 return
963 
964                 if not is_okay:
965                     with excutils.save_and_reraise_exception():
966                         LOG.error('Error from libvirt during destroy. '
967                                   'Code=%(errcode)s Error=%(e)s',
968                                   {'errcode': errcode, 'e': e},
969                                   instance=instance)
970 
971         def _wait_for_destroy(expected_domid):
972             """Called at an interval until the VM is gone."""
973             # NOTE(vish): If the instance disappears during the destroy
974             #             we ignore it so the cleanup can still be
975             #             attempted because we would prefer destroy to
976             #             never fail.
977             try:
978                 dom_info = self.get_info(instance)
979                 state = dom_info.state
980                 new_domid = dom_info.internal_id
981             except exception.InstanceNotFound:
982                 LOG.debug("During wait destroy, instance disappeared.",
983                           instance=instance)
984                 state = power_state.SHUTDOWN
985 
986             if state == power_state.SHUTDOWN:
987                 LOG.info("Instance destroyed successfully.", instance=instance)
988                 raise loopingcall.LoopingCallDone()
989 
990             # NOTE(wangpan): If the instance was booted again after destroy,
991             #                this may be an endless loop, so check the id of
992             #                domain here, if it changed and the instance is
993             #                still running, we should destroy it again.
994             # see https://bugs.launchpad.net/nova/+bug/1111213 for more details
995             if new_domid != expected_domid:
996                 LOG.info("Instance may be started again.", instance=instance)
997                 kwargs['is_running'] = True
998                 raise loopingcall.LoopingCallDone()
999 
1000         kwargs = {'is_running': False}
1001         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_destroy,
1002                                                      old_domid)
1003         timer.start(interval=0.5).wait()
1004         if kwargs['is_running']:
1005             LOG.info("Going to destroy instance again.", instance=instance)
1006             self._destroy(instance)
1007         else:
1008             # NOTE(GuanQiang): teardown container to avoid resource leak
1009             if CONF.libvirt.virt_type == 'lxc':
1010                 self._teardown_container(instance)
1011 
1012     def destroy(self, context, instance, network_info, block_device_info=None,
1013                 destroy_disks=True):
1014         self._destroy(instance)
1015         self.cleanup(context, instance, network_info, block_device_info,
1016                      destroy_disks)
1017 
1018     def _undefine_domain(self, instance):
1019         try:
1020             guest = self._host.get_guest(instance)
1021             try:
1022                 support_uefi = self._has_uefi_support()
1023                 guest.delete_configuration(support_uefi)
1024             except libvirt.libvirtError as e:
1025                 with excutils.save_and_reraise_exception() as ctxt:
1026                     errcode = e.get_error_code()
1027                     if errcode == libvirt.VIR_ERR_NO_DOMAIN:
1028                         LOG.debug("Called undefine, but domain already gone.",
1029                                   instance=instance)
1030                         ctxt.reraise = False
1031                     else:
1032                         LOG.error('Error from libvirt during undefine. '
1033                                   'Code=%(errcode)s Error=%(e)s',
1034                                   {'errcode': errcode,
1035                                    'e': encodeutils.exception_to_unicode(e)},
1036                                   instance=instance)
1037         except exception.InstanceNotFound:
1038             pass
1039 
1040     def cleanup(self, context, instance, network_info, block_device_info=None,
1041                 destroy_disks=True, migrate_data=None, destroy_vifs=True):
1042         if destroy_vifs:
1043             self._unplug_vifs(instance, network_info, True)
1044 
1045         # Continue attempting to remove firewall filters for the instance
1046         # until it's done or there is a failure to remove the filters. If
1047         # unfilter fails because the instance is not yet shutdown, try to
1048         # destroy the guest again and then retry the unfilter.
1049         while True:
1050             try:
1051                 self.unfilter_instance(instance, network_info)
1052                 break
1053             except libvirt.libvirtError as e:
1054                 try:
1055                     state = self.get_info(instance).state
1056                 except exception.InstanceNotFound:
1057                     state = power_state.SHUTDOWN
1058 
1059                 if state != power_state.SHUTDOWN:
1060                     LOG.warning("Instance may be still running, destroy "
1061                                 "it again.", instance=instance)
1062                     self._destroy(instance)
1063                 else:
1064                     errcode = e.get_error_code()
1065                     LOG.exception(_('Error from libvirt during unfilter. '
1066                                     'Code=%(errcode)s Error=%(e)s'),
1067                                   {'errcode': errcode, 'e': e},
1068                                   instance=instance)
1069                     reason = _("Error unfiltering instance.")
1070                     raise exception.InstanceTerminationFailure(reason=reason)
1071             except Exception:
1072                 raise
1073 
1074         # FIXME(wangpan): if the instance is booted again here, such as the
1075         #                 soft reboot operation boot it here, it will become
1076         #                 "running deleted", should we check and destroy it
1077         #                 at the end of this method?
1078 
1079         # NOTE(vish): we disconnect from volumes regardless
1080         block_device_mapping = driver.block_device_info_get_mapping(
1081             block_device_info)
1082         for vol in block_device_mapping:
1083             connection_info = vol['connection_info']
1084             disk_dev = vol['mount_device']
1085             if disk_dev is not None:
1086                 disk_dev = disk_dev.rpartition("/")[2]
1087             try:
1088                 self._disconnect_volume(context, connection_info, instance)
1089             except Exception as exc:
1090                 with excutils.save_and_reraise_exception() as ctxt:
1091                     if destroy_disks:
1092                         # Don't block on Volume errors if we're trying to
1093                         # delete the instance as we may be partially created
1094                         # or deleted
1095                         ctxt.reraise = False
1096                         LOG.warning(
1097                             "Ignoring Volume Error on vol %(vol_id)s "
1098                             "during delete %(exc)s",
1099                             {'vol_id': vol.get('volume_id'),
1100                              'exc': encodeutils.exception_to_unicode(exc)},
1101                             instance=instance)
1102 
1103         if destroy_disks:
1104             # NOTE(haomai): destroy volumes if needed
1105             if CONF.libvirt.images_type == 'lvm':
1106                 self._cleanup_lvm(instance, block_device_info)
1107             if CONF.libvirt.images_type == 'rbd':
1108                 self._cleanup_rbd(instance)
1109 
1110         is_shared_block_storage = False
1111         if migrate_data and 'is_shared_block_storage' in migrate_data:
1112             is_shared_block_storage = migrate_data.is_shared_block_storage
1113         # NOTE(lyarwood): The following workaround allows operators to ensure
1114         # that non-shared instance directories are removed after an evacuation
1115         # or revert resize when using the shared RBD imagebackend. This
1116         # workaround is not required when cleaning up migrations that provide
1117         # migrate_data to this method as the existing is_shared_block_storage
1118         # conditional will cause the instance directory to be removed.
1119         if ((destroy_disks or is_shared_block_storage) or
1120             (CONF.workarounds.ensure_libvirt_rbd_instance_dir_cleanup and
1121              CONF.libvirt.images_type == 'rbd')):
1122 
1123             attempts = int(instance.system_metadata.get('clean_attempts',
1124                                                         '0'))
1125             success = self.delete_instance_files(instance)
1126             # NOTE(mriedem): This is used in the _run_pending_deletes periodic
1127             # task in the compute manager. The tight coupling is not great...
1128             instance.system_metadata['clean_attempts'] = str(attempts + 1)
1129             if success:
1130                 instance.cleaned = True
1131             instance.save()
1132 
1133         self._undefine_domain(instance)
1134 
1135     def _detach_encrypted_volumes(self, instance, block_device_info):
1136         """Detaches encrypted volumes attached to instance."""
1137         disks = self._get_instance_disk_info(instance, block_device_info)
1138         encrypted_volumes = filter(dmcrypt.is_encrypted,
1139                                    [disk['path'] for disk in disks])
1140         for path in encrypted_volumes:
1141             dmcrypt.delete_volume(path)
1142 
1143     def _get_serial_ports_from_guest(self, guest, mode=None):
1144         """Returns an iterator over serial port(s) configured on guest.
1145 
1146         :param mode: Should be a value in (None, bind, connect)
1147         """
1148         xml = guest.get_xml_desc()
1149         tree = etree.fromstring(xml)
1150 
1151         # The 'serial' device is the base for x86 platforms. Other platforms
1152         # (e.g. kvm on system z = S390X) can only use 'console' devices.
1153         xpath_mode = "[@mode='%s']" % mode if mode else ""
1154         serial_tcp = "./devices/serial[@type='tcp']/source" + xpath_mode
1155         console_tcp = "./devices/console[@type='tcp']/source" + xpath_mode
1156 
1157         tcp_devices = tree.findall(serial_tcp)
1158         if len(tcp_devices) == 0:
1159             tcp_devices = tree.findall(console_tcp)
1160         for source in tcp_devices:
1161             yield (source.get("host"), int(source.get("service")))
1162 
1163     def _get_scsi_controller_max_unit(self, guest):
1164         """Returns the max disk unit used by scsi controller"""
1165         xml = guest.get_xml_desc()
1166         tree = etree.fromstring(xml)
1167         addrs = "./devices/disk[@device='disk']/address[@type='drive']"
1168 
1169         ret = []
1170         for obj in tree.findall(addrs):
1171             ret.append(int(obj.get('unit', 0)))
1172         return max(ret)
1173 
1174     @staticmethod
1175     def _get_rbd_driver():
1176         return rbd_utils.RBDDriver(
1177                 pool=CONF.libvirt.images_rbd_pool,
1178                 ceph_conf=CONF.libvirt.images_rbd_ceph_conf,
1179                 rbd_user=CONF.libvirt.rbd_user)
1180 
1181     def _cleanup_rbd(self, instance):
1182         # NOTE(nic): On revert_resize, the cleanup steps for the root
1183         # volume are handled with an "rbd snap rollback" command,
1184         # and none of this is needed (and is, in fact, harmful) so
1185         # filter out non-ephemerals from the list
1186         if instance.task_state == task_states.RESIZE_REVERTING:
1187             filter_fn = lambda disk: (disk.startswith(instance.uuid) and
1188                                       disk.endswith('disk.local'))
1189         else:
1190             filter_fn = lambda disk: disk.startswith(instance.uuid)
1191         LibvirtDriver._get_rbd_driver().cleanup_volumes(filter_fn)
1192 
1193     def _cleanup_lvm(self, instance, block_device_info):
1194         """Delete all LVM disks for given instance object."""
1195         if instance.get('ephemeral_key_uuid') is not None:
1196             self._detach_encrypted_volumes(instance, block_device_info)
1197 
1198         disks = self._lvm_disks(instance)
1199         if disks:
1200             lvm.remove_volumes(disks)
1201 
1202     def _lvm_disks(self, instance):
1203         """Returns all LVM disks for given instance object."""
1204         if CONF.libvirt.images_volume_group:
1205             vg = os.path.join('/dev', CONF.libvirt.images_volume_group)
1206             if not os.path.exists(vg):
1207                 return []
1208             pattern = '%s_' % instance.uuid
1209 
1210             def belongs_to_instance(disk):
1211                 return disk.startswith(pattern)
1212 
1213             def fullpath(name):
1214                 return os.path.join(vg, name)
1215 
1216             logical_volumes = lvm.list_volumes(vg)
1217 
1218             disks = [fullpath(disk) for disk in logical_volumes
1219                      if belongs_to_instance(disk)]
1220             return disks
1221         return []
1222 
1223     def get_volume_connector(self, instance):
1224         root_helper = utils.get_root_helper()
1225         return connector.get_connector_properties(
1226             root_helper, CONF.my_block_storage_ip,
1227             CONF.libvirt.volume_use_multipath,
1228             enforce_multipath=True,
1229             host=CONF.host)
1230 
1231     def _cleanup_resize(self, context, instance, network_info):
1232         inst_base = libvirt_utils.get_instance_path(instance)
1233         target = inst_base + '_resize'
1234 
1235         # Deletion can fail over NFS, so retry the deletion as required.
1236         # Set maximum attempt as 5, most test can remove the directory
1237         # for the second time.
1238         attempts = 0
1239         while(os.path.exists(target) and attempts < 5):
1240             shutil.rmtree(target, ignore_errors=True)
1241             if os.path.exists(target):
1242                 time.sleep(random.randint(20, 200) / 100.0)
1243             attempts += 1
1244 
1245         # NOTE(mriedem): Some image backends will recreate the instance path
1246         # and disk.info during init, and all we need the root disk for
1247         # here is removing cloned snapshots which is backend-specific, so
1248         # check that first before initializing the image backend object. If
1249         # there is ever an image type that supports clone *and* re-creates
1250         # the instance directory and disk.info on init, this condition will
1251         # need to be re-visited to make sure that backend doesn't re-create
1252         # the disk. Refer to bugs: 1666831 1728603 1769131
1253         if self.image_backend.backend(CONF.libvirt.images_type).SUPPORTS_CLONE:
1254             root_disk = self.image_backend.by_name(instance, 'disk')
1255             if root_disk.exists():
1256                 root_disk.remove_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME)
1257 
1258         if instance.host != CONF.host:
1259             self._undefine_domain(instance)
1260             self.unplug_vifs(instance, network_info)
1261             self.unfilter_instance(instance, network_info)
1262 
1263     def _get_volume_driver(self, connection_info):
1264         driver_type = connection_info.get('driver_volume_type')
1265         if driver_type not in self.volume_drivers:
1266             raise exception.VolumeDriverNotFound(driver_type=driver_type)
1267         return self.volume_drivers[driver_type]
1268 
1269     def _connect_volume(self, context, connection_info, instance,
1270                         encryption=None, allow_native_luks=True):
1271         vol_driver = self._get_volume_driver(connection_info)
1272         vol_driver.connect_volume(connection_info, instance)
1273         self._attach_encryptor(context, connection_info, encryption,
1274                                allow_native_luks)
1275 
1276     def _should_disconnect_target(self, context, connection_info, instance):
1277         connection_count = 0
1278 
1279         # NOTE(jdg): Multiattach is a special case (not to be confused
1280         # with shared_targets). With multiattach we may have a single volume
1281         # attached multiple times to *this* compute node (ie Server-1 and
1282         # Server-2).  So, if we receive a call to delete the attachment for
1283         # Server-1 we need to take special care to make sure that the Volume
1284         # isn't also attached to another Server on this Node.  Otherwise we
1285         # will indiscriminantly delete the connection for all Server and that's
1286         # no good.  So check if it's attached multiple times on this node
1287         # if it is we skip the call to brick to delete the connection.
1288         if connection_info.get('multiattach', False):
1289             volume = self._volume_api.get(
1290                 context,
1291                 driver_block_device.get_volume_id(connection_info))
1292             attachments = volume.get('attachments', {})
1293             if len(attachments) > 1:
1294                 # First we get a list of all Server UUID's associated with
1295                 # this Host (Compute Node).  We're going to use this to
1296                 # determine if the Volume being detached is also in-use by
1297                 # another Server on this Host, ie just check to see if more
1298                 # than one attachment.server_id for this volume is in our
1299                 # list of Server UUID's for this Host
1300                 servers_this_host = objects.InstanceList.get_uuids_by_host(
1301                     context, instance.host)
1302 
1303                 # NOTE(jdg): nova.volume.cinder translates the
1304                 # volume['attachments'] response into a dict which includes
1305                 # the Server UUID as the key, so we're using that
1306                 # here to check against our server_this_host list
1307                 for server_id, data in attachments.items():
1308                     if server_id in servers_this_host:
1309                         connection_count += 1
1310                 # NOTE(lyarwood): Handle calls to _disconnect_volume during
1311                 # post_live_migration from the source where the instance has
1312                 # already moved to the destination but another instance is
1313                 # still attached to the volume.
1314                 if (connection_count == 1 and
1315                     instance.uuid not in servers_this_host):
1316                     return False
1317         return (False if connection_count > 1 else True)
1318 
1319     def _disconnect_volume(self, context, connection_info, instance,
1320                            encryption=None):
1321         self._detach_encryptor(context, connection_info, encryption=encryption)
1322         if self._should_disconnect_target(context, connection_info, instance):
1323             vol_driver = self._get_volume_driver(connection_info)
1324             vol_driver.disconnect_volume(connection_info, instance)
1325         else:
1326             LOG.info("Detected multiple connections on this host for volume: "
1327                      "%s, skipping target disconnect.",
1328                      driver_block_device.get_volume_id(connection_info),
1329                      instance=instance)
1330 
1331     def _extend_volume(self, connection_info, instance):
1332         vol_driver = self._get_volume_driver(connection_info)
1333         return vol_driver.extend_volume(connection_info, instance)
1334 
1335     def _use_native_luks(self, encryption=None):
1336         """Is LUKS the required provider and native QEMU LUKS available
1337         """
1338         provider = None
1339         if encryption:
1340             provider = encryption.get('provider', None)
1341         if provider in encryptors.LEGACY_PROVIDER_CLASS_TO_FORMAT_MAP:
1342             provider = encryptors.LEGACY_PROVIDER_CLASS_TO_FORMAT_MAP[provider]
1343         return provider == encryptors.LUKS and self._is_native_luks_available()
1344 
1345     def _get_volume_config(self, connection_info, disk_info):
1346         vol_driver = self._get_volume_driver(connection_info)
1347         conf = vol_driver.get_config(connection_info, disk_info)
1348         self._set_cache_mode(conf)
1349         return conf
1350 
1351     def _get_volume_encryptor(self, connection_info, encryption):
1352         root_helper = utils.get_root_helper()
1353         return encryptors.get_volume_encryptor(root_helper=root_helper,
1354                                                keymgr=key_manager.API(CONF),
1355                                                connection_info=connection_info,
1356                                                **encryption)
1357 
1358     def _get_volume_encryption(self, context, connection_info):
1359         """Get the encryption metadata dict if it is not provided
1360         """
1361         encryption = {}
1362         volume_id = driver_block_device.get_volume_id(connection_info)
1363         if volume_id:
1364             encryption = encryptors.get_encryption_metadata(context,
1365                             self._volume_api, volume_id, connection_info)
1366         return encryption
1367 
1368     def _attach_encryptor(self, context, connection_info, encryption,
1369                           allow_native_luks):
1370         """Attach the frontend encryptor if one is required by the volume.
1371 
1372         The request context is only used when an encryption metadata dict is
1373         not provided. The encryption metadata dict being populated is then used
1374         to determine if an attempt to attach the encryptor should be made.
1375 
1376         If native LUKS decryption is enabled then create a Libvirt volume
1377         secret containing the LUKS passphrase for the volume.
1378         """
1379         if encryption is None:
1380             encryption = self._get_volume_encryption(context, connection_info)
1381 
1382         if (encryption and allow_native_luks and
1383             self._use_native_luks(encryption)):
1384             # NOTE(lyarwood): Fetch the associated key for the volume and
1385             # decode the passphrase from the key.
1386             # FIXME(lyarwood): c-vol currently creates symmetric keys for use
1387             # with volumes, leading to the binary to hex to string conversion
1388             # below.
1389             keymgr = key_manager.API(CONF)
1390             key = keymgr.get(context, encryption['encryption_key_id'])
1391             key_encoded = key.get_encoded()
1392             passphrase = binascii.hexlify(key_encoded).decode('utf-8')
1393 
1394             # NOTE(lyarwood): Retain the behaviour of the original os-brick
1395             # encryptors and format any volume that does not identify as
1396             # encrypted with LUKS.
1397             # FIXME(lyarwood): Remove this once c-vol correctly formats
1398             # encrypted volumes during their initial creation:
1399             # https://bugs.launchpad.net/cinder/+bug/1739442
1400             device_path = connection_info.get('data').get('device_path')
1401             if device_path:
1402                 root_helper = utils.get_root_helper()
1403                 if not luks_encryptor.is_luks(root_helper, device_path):
1404                     encryptor = self._get_volume_encryptor(connection_info,
1405                                                            encryption)
1406                     encryptor._format_volume(passphrase, **encryption)
1407 
1408             # NOTE(lyarwood): Store the passphrase as a libvirt secret locally
1409             # on the compute node. This secret is used later when generating
1410             # the volume config.
1411             volume_id = driver_block_device.get_volume_id(connection_info)
1412             self._host.create_secret('volume', volume_id, password=passphrase)
1413         elif encryption:
1414             encryptor = self._get_volume_encryptor(connection_info,
1415                                                    encryption)
1416             encryptor.attach_volume(context, **encryption)
1417 
1418     def _detach_encryptor(self, context, connection_info, encryption):
1419         """Detach the frontend encryptor if one is required by the volume.
1420 
1421         The request context is only used when an encryption metadata dict is
1422         not provided. The encryption metadata dict being populated is then used
1423         to determine if an attempt to detach the encryptor should be made.
1424 
1425         If native LUKS decryption is enabled then delete previously created
1426         Libvirt volume secret from the host.
1427         """
1428         volume_id = driver_block_device.get_volume_id(connection_info)
1429         if volume_id and self._host.find_secret('volume', volume_id):
1430             return self._host.delete_secret('volume', volume_id)
1431         if encryption is None:
1432             encryption = self._get_volume_encryption(context, connection_info)
1433         if encryption:
1434             encryptor = self._get_volume_encryptor(connection_info,
1435                                                    encryption)
1436             encryptor.detach_volume(**encryption)
1437 
1438     def _check_discard_for_attach_volume(self, conf, instance):
1439         """Perform some checks for volumes configured for discard support.
1440 
1441         If discard is configured for the volume, and the guest is using a
1442         configuration known to not work, we will log a message explaining
1443         the reason why.
1444         """
1445         if conf.driver_discard == 'unmap' and conf.target_bus == 'virtio':
1446             LOG.debug('Attempting to attach volume %(id)s with discard '
1447                       'support enabled to an instance using an '
1448                       'unsupported configuration. target_bus = '
1449                       '%(bus)s. Trim commands will not be issued to '
1450                       'the storage device.',
1451                       {'bus': conf.target_bus,
1452                        'id': conf.serial},
1453                       instance=instance)
1454 
1455     def attach_volume(self, context, connection_info, instance, mountpoint,
1456                       disk_bus=None, device_type=None, encryption=None):
1457         guest = self._host.get_guest(instance)
1458 
1459         disk_dev = mountpoint.rpartition("/")[2]
1460         bdm = {
1461             'device_name': disk_dev,
1462             'disk_bus': disk_bus,
1463             'device_type': device_type}
1464 
1465         # Note(cfb): If the volume has a custom block size, check that
1466         #            that we are using QEMU/KVM and libvirt >= 0.10.2. The
1467         #            presence of a block size is considered mandatory by
1468         #            cinder so we fail if we can't honor the request.
1469         data = {}
1470         if ('data' in connection_info):
1471             data = connection_info['data']
1472         if ('logical_block_size' in data or 'physical_block_size' in data):
1473             if ((CONF.libvirt.virt_type != "kvm" and
1474                  CONF.libvirt.virt_type != "qemu")):
1475                 msg = _("Volume sets block size, but the current "
1476                         "libvirt hypervisor '%s' does not support custom "
1477                         "block size") % CONF.libvirt.virt_type
1478                 raise exception.InvalidHypervisorType(msg)
1479 
1480         self._connect_volume(context, connection_info, instance,
1481                              encryption=encryption)
1482         disk_info = blockinfo.get_info_from_bdm(
1483             instance, CONF.libvirt.virt_type, instance.image_meta, bdm)
1484         if disk_info['bus'] == 'scsi':
1485             disk_info['unit'] = self._get_scsi_controller_max_unit(guest) + 1
1486 
1487         conf = self._get_volume_config(connection_info, disk_info)
1488 
1489         self._check_discard_for_attach_volume(conf, instance)
1490 
1491         try:
1492             state = guest.get_power_state(self._host)
1493             live = state in (power_state.RUNNING, power_state.PAUSED)
1494 
1495             guest.attach_device(conf, persistent=True, live=live)
1496             # NOTE(artom) If we're attaching with a device role tag, we need to
1497             # rebuild device_metadata. If we're attaching without a role
1498             # tag, we're rebuilding it here needlessly anyways. This isn't a
1499             # massive deal, and it helps reduce code complexity by not having
1500             # to indicate to the virt driver that the attach is tagged. The
1501             # really important optimization of not calling the database unless
1502             # device_metadata has actually changed is done for us by
1503             # instance.save().
1504             instance.device_metadata = self._build_device_metadata(
1505                 context, instance)
1506             instance.save()
1507 
1508         # TODO(lyarwood) Remove the following breadcrumb once all supported
1509         # distributions provide Libvirt 3.3.0 or earlier with
1510         # https://libvirt.org/git/?p=libvirt.git;a=commit;h=7189099 applied.
1511         except libvirt.libvirtError as ex:
1512             if 'Incorrect number of padding bytes' in six.text_type(ex):
1513                 LOG.warning(_('Failed to attach encrypted volume due to a '
1514                     'known Libvirt issue, see the following bug for details: '
1515                     'https://bugzilla.redhat.com/show_bug.cgi?id=1447297'))
1516                 raise
1517         except Exception:
1518             LOG.exception(_('Failed to attach volume at mountpoint: %s'),
1519                           mountpoint, instance=instance)
1520             with excutils.save_and_reraise_exception():
1521                 self._disconnect_volume(context, connection_info, instance,
1522                                         encryption=encryption)
1523 
1524     def _swap_volume(self, guest, disk_path, conf, resize_to):
1525         """Swap existing disk with a new block device."""
1526         dev = guest.get_block_device(disk_path)
1527 
1528         # Save a copy of the domain's persistent XML file. We'll use this
1529         # to redefine the domain if anything fails during the volume swap.
1530         xml = guest.get_xml_desc(dump_inactive=True, dump_sensitive=True)
1531 
1532         # Abort is an idempotent operation, so make sure any block
1533         # jobs which may have failed are ended.
1534         try:
1535             dev.abort_job()
1536         except Exception:
1537             pass
1538 
1539         try:
1540             # NOTE (rmk): blockRebase cannot be executed on persistent
1541             #             domains, so we need to temporarily undefine it.
1542             #             If any part of this block fails, the domain is
1543             #             re-defined regardless.
1544             if guest.has_persistent_configuration():
1545                 support_uefi = self._has_uefi_support()
1546                 guest.delete_configuration(support_uefi)
1547 
1548             try:
1549                 # Start copy with VIR_DOMAIN_BLOCK_REBASE_REUSE_EXT flag to
1550                 # allow writing to existing external volume file. Use
1551                 # VIR_DOMAIN_BLOCK_REBASE_COPY_DEV if it's a block device to
1552                 # make sure XML is generated correctly (bug 1691195)
1553                 copy_dev = conf.source_type == 'block'
1554                 dev.rebase(conf.source_path, copy=True, reuse_ext=True,
1555                            copy_dev=copy_dev)
1556                 while not dev.is_job_complete():
1557                     time.sleep(0.5)
1558 
1559                 dev.abort_job(pivot=True)
1560 
1561             except Exception as exc:
1562                 LOG.exception("Failure rebasing volume %(new_path)s on "
1563                     "%(old_path)s.", {'new_path': conf.source_path,
1564                                       'old_path': disk_path})
1565                 raise exception.VolumeRebaseFailed(reason=six.text_type(exc))
1566 
1567             if resize_to:
1568                 dev.resize(resize_to * units.Gi / units.Ki)
1569 
1570             # Make sure we will redefine the domain using the updated
1571             # configuration after the volume was swapped. The dump_inactive
1572             # keyword arg controls whether we pull the inactive (persistent)
1573             # or active (live) config from the domain. We want to pull the
1574             # live config after the volume was updated to use when we redefine
1575             # the domain.
1576             xml = guest.get_xml_desc(dump_inactive=False, dump_sensitive=True)
1577         finally:
1578             self._host.write_instance_config(xml)
1579 
1580     def swap_volume(self, context, old_connection_info,
1581                     new_connection_info, instance, mountpoint, resize_to):
1582 
1583         # NOTE(lyarwood): https://bugzilla.redhat.com/show_bug.cgi?id=760547
1584         old_encrypt = self._get_volume_encryption(context, old_connection_info)
1585         new_encrypt = self._get_volume_encryption(context, new_connection_info)
1586         if ((old_encrypt and self._use_native_luks(old_encrypt)) or
1587             (new_encrypt and self._use_native_luks(new_encrypt))):
1588             raise NotImplementedError(_("Swap volume is not supported for "
1589                 "encrypted volumes when native LUKS decryption is enabled."))
1590 
1591         guest = self._host.get_guest(instance)
1592 
1593         disk_dev = mountpoint.rpartition("/")[2]
1594         if not guest.get_disk(disk_dev):
1595             raise exception.DiskNotFound(location=disk_dev)
1596         disk_info = {
1597             'dev': disk_dev,
1598             'bus': blockinfo.get_disk_bus_for_disk_dev(
1599                 CONF.libvirt.virt_type, disk_dev),
1600             'type': 'disk',
1601             }
1602         # NOTE (lyarwood): new_connection_info will be modified by the
1603         # following _connect_volume call down into the volume drivers. The
1604         # majority of the volume drivers will add a device_path that is in turn
1605         # used by _get_volume_config to set the source_path of the
1606         # LibvirtConfigGuestDisk object it returns. We do not explicitly save
1607         # this to the BDM here as the upper compute swap_volume method will
1608         # eventually do this for us.
1609         self._connect_volume(context, new_connection_info, instance)
1610         conf = self._get_volume_config(new_connection_info, disk_info)
1611         if not conf.source_path:
1612             self._disconnect_volume(context, new_connection_info, instance)
1613             raise NotImplementedError(_("Swap only supports host devices"))
1614 
1615         try:
1616             self._swap_volume(guest, disk_dev, conf, resize_to)
1617         except exception.VolumeRebaseFailed:
1618             with excutils.save_and_reraise_exception():
1619                 self._disconnect_volume(context, new_connection_info, instance)
1620 
1621         self._disconnect_volume(context, old_connection_info, instance)
1622 
1623     def _get_existing_domain_xml(self, instance, network_info,
1624                                  block_device_info=None):
1625         try:
1626             guest = self._host.get_guest(instance)
1627             xml = guest.get_xml_desc()
1628         except exception.InstanceNotFound:
1629             disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
1630                                                 instance,
1631                                                 instance.image_meta,
1632                                                 block_device_info)
1633             xml = self._get_guest_xml(nova_context.get_admin_context(),
1634                                       instance, network_info, disk_info,
1635                                       instance.image_meta,
1636                                       block_device_info=block_device_info)
1637         return xml
1638 
1639     def detach_volume(self, context, connection_info, instance, mountpoint,
1640                       encryption=None):
1641         disk_dev = mountpoint.rpartition("/")[2]
1642         try:
1643             guest = self._host.get_guest(instance)
1644 
1645             state = guest.get_power_state(self._host)
1646             live = state in (power_state.RUNNING, power_state.PAUSED)
1647             # NOTE(lyarwood): The volume must be detached from the VM before
1648             # detaching any attached encryptors or disconnecting the underlying
1649             # volume in _disconnect_volume. Otherwise, the encryptor or volume
1650             # driver may report that the volume is still in use.
1651             wait_for_detach = guest.detach_device_with_retry(guest.get_disk,
1652                                                              disk_dev,
1653                                                              live=live)
1654             wait_for_detach()
1655 
1656         except exception.InstanceNotFound:
1657             # NOTE(zhaoqin): If the instance does not exist, _lookup_by_name()
1658             #                will throw InstanceNotFound exception. Need to
1659             #                disconnect volume under this circumstance.
1660             LOG.warning("During detach_volume, instance disappeared.",
1661                         instance=instance)
1662         except exception.DeviceNotFound:
1663             # We should still try to disconnect logical device from
1664             # host, an error might have happened during a previous
1665             # call.
1666             LOG.info("Device %s not found in instance.",
1667                      disk_dev, instance=instance)
1668         except libvirt.libvirtError as ex:
1669             # NOTE(vish): This is called to cleanup volumes after live
1670             #             migration, so we should still disconnect even if
1671             #             the instance doesn't exist here anymore.
1672             error_code = ex.get_error_code()
1673             if error_code == libvirt.VIR_ERR_NO_DOMAIN:
1674                 # NOTE(vish):
1675                 LOG.warning("During detach_volume, instance disappeared.",
1676                             instance=instance)
1677             else:
1678                 raise
1679 
1680         self._disconnect_volume(context, connection_info, instance,
1681                                 encryption=encryption)
1682 
1683     def extend_volume(self, connection_info, instance):
1684         try:
1685             new_size = self._extend_volume(connection_info, instance)
1686         except NotImplementedError:
1687             raise exception.ExtendVolumeNotSupported()
1688 
1689         # Resize the device in QEMU so its size is updated and
1690         # detected by the instance without rebooting.
1691         try:
1692             guest = self._host.get_guest(instance)
1693             state = guest.get_power_state(self._host)
1694             active_state = state in (power_state.RUNNING, power_state.PAUSED)
1695             if active_state:
1696                 disk_path = connection_info['data']['device_path']
1697                 LOG.debug('resizing block device %(dev)s to %(size)u kb',
1698                           {'dev': disk_path, 'size': new_size})
1699                 dev = guest.get_block_device(disk_path)
1700                 dev.resize(new_size // units.Ki)
1701             else:
1702                 LOG.debug('Skipping block device resize, guest is not running',
1703                           instance=instance)
1704         except exception.InstanceNotFound:
1705             with excutils.save_and_reraise_exception():
1706                 LOG.warning('During extend_volume, instance disappeared.',
1707                             instance=instance)
1708         except libvirt.libvirtError:
1709             with excutils.save_and_reraise_exception():
1710                 LOG.exception('resizing block device failed.',
1711                               instance=instance)
1712 
1713     def attach_interface(self, context, instance, image_meta, vif):
1714         guest = self._host.get_guest(instance)
1715 
1716         self.vif_driver.plug(instance, vif)
1717         self.firewall_driver.setup_basic_filtering(instance, [vif])
1718         cfg = self.vif_driver.get_config(instance, vif, image_meta,
1719                                          instance.flavor,
1720                                          CONF.libvirt.virt_type,
1721                                          self._host)
1722         try:
1723             state = guest.get_power_state(self._host)
1724             live = state in (power_state.RUNNING, power_state.PAUSED)
1725             guest.attach_device(cfg, persistent=True, live=live)
1726         except libvirt.libvirtError:
1727             LOG.error('attaching network adapter failed.',
1728                       instance=instance, exc_info=True)
1729             self.vif_driver.unplug(instance, vif)
1730             raise exception.InterfaceAttachFailed(
1731                     instance_uuid=instance.uuid)
1732         try:
1733             # NOTE(artom) If we're attaching with a device role tag, we need to
1734             # rebuild device_metadata. If we're attaching without a role
1735             # tag, we're rebuilding it here needlessly anyways. This isn't a
1736             # massive deal, and it helps reduce code complexity by not having
1737             # to indicate to the virt driver that the attach is tagged. The
1738             # really important optimization of not calling the database unless
1739             # device_metadata has actually changed is done for us by
1740             # instance.save().
1741             instance.device_metadata = self._build_device_metadata(
1742                 context, instance)
1743             instance.save()
1744         except Exception:
1745             # NOTE(artom) If we fail here it means the interface attached
1746             # successfully but building and/or saving the device metadata
1747             # failed. Just unplugging the vif is therefore not enough cleanup,
1748             # we need to detach the interface.
1749             with excutils.save_and_reraise_exception(reraise=False):
1750                 LOG.error('Interface attached successfully but building '
1751                           'and/or saving device metadata failed.',
1752                           instance=instance, exc_info=True)
1753                 self.detach_interface(context, instance, vif)
1754                 raise exception.InterfaceAttachFailed(
1755                     instance_uuid=instance.uuid)
1756 
1757     def detach_interface(self, context, instance, vif):
1758         guest = self._host.get_guest(instance)
1759         cfg = self.vif_driver.get_config(instance, vif,
1760                                          instance.image_meta,
1761                                          instance.flavor,
1762                                          CONF.libvirt.virt_type, self._host)
1763         interface = guest.get_interface_by_cfg(cfg)
1764         try:
1765             self.vif_driver.unplug(instance, vif)
1766             # NOTE(mriedem): When deleting an instance and using Neutron,
1767             # we can be racing against Neutron deleting the port and
1768             # sending the vif-deleted event which then triggers a call to
1769             # detach the interface, so if the interface is not found then
1770             # we can just log it as a warning.
1771             if not interface:
1772                 mac = vif.get('address')
1773                 # The interface is gone so just log it as a warning.
1774                 LOG.warning('Detaching interface %(mac)s failed because '
1775                             'the device is no longer found on the guest.',
1776                             {'mac': mac}, instance=instance)
1777                 return
1778 
1779             state = guest.get_power_state(self._host)
1780             live = state in (power_state.RUNNING, power_state.PAUSED)
1781             # Now we are going to loop until the interface is detached or we
1782             # timeout.
1783             wait_for_detach = guest.detach_device_with_retry(
1784                 guest.get_interface_by_cfg, cfg, live=live,
1785                 alternative_device_name=self.vif_driver.get_vif_devname(vif))
1786             wait_for_detach()
1787         except exception.DeviceDetachFailed:
1788             # We failed to detach the device even with the retry loop, so let's
1789             # dump some debug information to the logs before raising back up.
1790             with excutils.save_and_reraise_exception():
1791                 devname = self.vif_driver.get_vif_devname(vif)
1792                 interface = guest.get_interface_by_cfg(cfg)
1793                 if interface:
1794                     LOG.warning(
1795                         'Failed to detach interface %(devname)s after '
1796                         'repeated attempts. Final interface xml:\n'
1797                         '%(interface_xml)s\nFinal guest xml:\n%(guest_xml)s',
1798                         {'devname': devname,
1799                          'interface_xml': interface.to_xml(),
1800                          'guest_xml': guest.get_xml_desc()},
1801                         instance=instance)
1802         except exception.DeviceNotFound:
1803             # The interface is gone so just log it as a warning.
1804             LOG.warning('Detaching interface %(mac)s failed because '
1805                         'the device is no longer found on the guest.',
1806                         {'mac': vif.get('address')}, instance=instance)
1807         except libvirt.libvirtError as ex:
1808             error_code = ex.get_error_code()
1809             if error_code == libvirt.VIR_ERR_NO_DOMAIN:
1810                 LOG.warning("During detach_interface, instance disappeared.",
1811                             instance=instance)
1812             else:
1813                 # NOTE(mriedem): When deleting an instance and using Neutron,
1814                 # we can be racing against Neutron deleting the port and
1815                 # sending the vif-deleted event which then triggers a call to
1816                 # detach the interface, so we might have failed because the
1817                 # network device no longer exists. Libvirt will fail with
1818                 # "operation failed: no matching network device was found"
1819                 # which unfortunately does not have a unique error code so we
1820                 # need to look up the interface by config and if it's not found
1821                 # then we can just log it as a warning rather than tracing an
1822                 # error.
1823                 mac = vif.get('address')
1824                 interface = guest.get_interface_by_cfg(cfg)
1825                 if interface:
1826                     LOG.error('detaching network adapter failed.',
1827                               instance=instance, exc_info=True)
1828                     raise exception.InterfaceDetachFailed(
1829                             instance_uuid=instance.uuid)
1830 
1831                 # The interface is gone so just log it as a warning.
1832                 LOG.warning('Detaching interface %(mac)s failed because '
1833                             'the device is no longer found on the guest.',
1834                             {'mac': mac}, instance=instance)
1835 
1836     def _create_snapshot_metadata(self, image_meta, instance,
1837                                   img_fmt, snp_name):
1838         metadata = {'is_public': False,
1839                     'status': 'active',
1840                     'name': snp_name,
1841                     'properties': {
1842                                    'kernel_id': instance.kernel_id,
1843                                    'image_location': 'snapshot',
1844                                    'image_state': 'available',
1845                                    'owner_id': instance.project_id,
1846                                    'ramdisk_id': instance.ramdisk_id,
1847                                    }
1848                     }
1849         if instance.os_type:
1850             metadata['properties']['os_type'] = instance.os_type
1851 
1852         # NOTE(vish): glance forces ami disk format to be ami
1853         if image_meta.disk_format == 'ami':
1854             metadata['disk_format'] = 'ami'
1855         else:
1856             metadata['disk_format'] = img_fmt
1857 
1858         if image_meta.obj_attr_is_set("container_format"):
1859             metadata['container_format'] = image_meta.container_format
1860         else:
1861             metadata['container_format'] = "bare"
1862 
1863         return metadata
1864 
1865     def snapshot(self, context, instance, image_id, update_task_state):
1866         """Create snapshot from a running VM instance.
1867 
1868         This command only works with qemu 0.14+
1869         """
1870         try:
1871             guest = self._host.get_guest(instance)
1872 
1873             # TODO(sahid): We are converting all calls from a
1874             # virDomain object to use nova.virt.libvirt.Guest.
1875             # We should be able to remove virt_dom at the end.
1876             virt_dom = guest._domain
1877         except exception.InstanceNotFound:
1878             raise exception.InstanceNotRunning(instance_id=instance.uuid)
1879 
1880         snapshot = self._image_api.get(context, image_id)
1881 
1882         # source_format is an on-disk format
1883         # source_type is a backend type
1884         disk_path, source_format = libvirt_utils.find_disk(guest)
1885         source_type = libvirt_utils.get_disk_type_from_path(disk_path)
1886 
1887         # We won't have source_type for raw or qcow2 disks, because we can't
1888         # determine that from the path. We should have it from the libvirt
1889         # xml, though.
1890         if source_type is None:
1891             source_type = source_format
1892         # For lxc instances we won't have it either from libvirt xml
1893         # (because we just gave libvirt the mounted filesystem), or the path,
1894         # so source_type is still going to be None. In this case,
1895         # root_disk is going to default to CONF.libvirt.images_type
1896         # below, which is still safe.
1897 
1898         image_format = CONF.libvirt.snapshot_image_format or source_type
1899 
1900         # NOTE(bfilippov): save lvm and rbd as raw
1901         if image_format == 'lvm' or image_format == 'rbd':
1902             image_format = 'raw'
1903 
1904         metadata = self._create_snapshot_metadata(instance.image_meta,
1905                                                   instance,
1906                                                   image_format,
1907                                                   snapshot['name'])
1908 
1909         snapshot_name = uuidutils.generate_uuid(dashed=False)
1910 
1911         state = guest.get_power_state(self._host)
1912 
1913         # NOTE(dgenin): Instances with LVM encrypted ephemeral storage require
1914         #               cold snapshots. Currently, checking for encryption is
1915         #               redundant because LVM supports only cold snapshots.
1916         #               It is necessary in case this situation changes in the
1917         #               future.
1918         if (self._host.has_min_version(hv_type=host.HV_DRIVER_QEMU)
1919                 and source_type not in ('lvm')
1920                 and not CONF.ephemeral_storage_encryption.enabled
1921                 and not CONF.workarounds.disable_libvirt_livesnapshot
1922                 # NOTE(rmk): We cannot perform live snapshots when a
1923                 # managedSave file is present, so we will use the cold/legacy
1924                 # method for instances which are shutdown or paused.
1925                 # NOTE(mriedem): Live snapshot doesn't work with paused
1926                 # instances on older versions of libvirt/qemu. We can likely
1927                 # remove the restriction on PAUSED once we require
1928                 # libvirt>=3.6.0 and qemu>=2.10 since that works with the
1929                 # Pike Ubuntu Cloud Archive testing in Queens.
1930                 and state not in (power_state.SHUTDOWN, power_state.PAUSED)):
1931             live_snapshot = True
1932             # Abort is an idempotent operation, so make sure any block
1933             # jobs which may have failed are ended. This operation also
1934             # confirms the running instance, as opposed to the system as a
1935             # whole, has a new enough version of the hypervisor (bug 1193146).
1936             try:
1937                 guest.get_block_device(disk_path).abort_job()
1938             except libvirt.libvirtError as ex:
1939                 error_code = ex.get_error_code()
1940                 if error_code == libvirt.VIR_ERR_CONFIG_UNSUPPORTED:
1941                     live_snapshot = False
1942                 else:
1943                     pass
1944         else:
1945             live_snapshot = False
1946 
1947         self._prepare_domain_for_snapshot(context, live_snapshot, state,
1948                                           instance)
1949 
1950         root_disk = self.image_backend.by_libvirt_path(
1951             instance, disk_path, image_type=source_type)
1952 
1953         if live_snapshot:
1954             LOG.info("Beginning live snapshot process", instance=instance)
1955         else:
1956             LOG.info("Beginning cold snapshot process", instance=instance)
1957 
1958         update_task_state(task_state=task_states.IMAGE_PENDING_UPLOAD)
1959 
1960         update_task_state(task_state=task_states.IMAGE_UPLOADING,
1961                           expected_state=task_states.IMAGE_PENDING_UPLOAD)
1962 
1963         try:
1964             metadata['location'] = root_disk.direct_snapshot(
1965                 context, snapshot_name, image_format, image_id,
1966                 instance.image_ref)
1967             self._snapshot_domain(context, live_snapshot, virt_dom, state,
1968                                   instance)
1969             self._image_api.update(context, image_id, metadata,
1970                                    purge_props=False)
1971         except (NotImplementedError, exception.ImageUnacceptable,
1972                 exception.Forbidden) as e:
1973             if type(e) != NotImplementedError:
1974                 LOG.warning('Performing standard snapshot because direct '
1975                             'snapshot failed: %(error)s',
1976                             {'error': encodeutils.exception_to_unicode(e)})
1977             failed_snap = metadata.pop('location', None)
1978             if failed_snap:
1979                 failed_snap = {'url': str(failed_snap)}
1980             root_disk.cleanup_direct_snapshot(failed_snap,
1981                                                   also_destroy_volume=True,
1982                                                   ignore_errors=True)
1983             update_task_state(task_state=task_states.IMAGE_PENDING_UPLOAD,
1984                               expected_state=task_states.IMAGE_UPLOADING)
1985 
1986             # TODO(nic): possibly abstract this out to the root_disk
1987             if source_type == 'rbd' and live_snapshot:
1988                 # Standard snapshot uses qemu-img convert from RBD which is
1989                 # not safe to run with live_snapshot.
1990                 live_snapshot = False
1991                 # Suspend the guest, so this is no longer a live snapshot
1992                 self._prepare_domain_for_snapshot(context, live_snapshot,
1993                                                   state, instance)
1994 
1995             snapshot_directory = CONF.libvirt.snapshots_directory
1996             fileutils.ensure_tree(snapshot_directory)
1997             with utils.tempdir(dir=snapshot_directory) as tmpdir:
1998                 try:
1999                     out_path = os.path.join(tmpdir, snapshot_name)
2000                     if live_snapshot:
2001                         # NOTE(xqueralt): libvirt needs o+x in the tempdir
2002                         os.chmod(tmpdir, 0o701)
2003                         self._live_snapshot(context, instance, guest,
2004                                             disk_path, out_path, source_format,
2005                                             image_format, instance.image_meta)
2006                     else:
2007                         root_disk.snapshot_extract(out_path, image_format)
2008                     LOG.info("Snapshot extracted, beginning image upload",
2009                              instance=instance)
2010                 except libvirt.libvirtError as ex:
2011                     error_code = ex.get_error_code()
2012                     if error_code == libvirt.VIR_ERR_NO_DOMAIN:
2013                         LOG.info('Instance %(instance_name)s disappeared '
2014                                  'while taking snapshot of it: [Error Code '
2015                                  '%(error_code)s] %(ex)s',
2016                                  {'instance_name': instance.name,
2017                                   'error_code': error_code,
2018                                   'ex': ex},
2019                                  instance=instance)
2020                         raise exception.InstanceNotFound(
2021                             instance_id=instance.uuid)
2022                     else:
2023                         raise
2024                 finally:
2025                     self._snapshot_domain(context, live_snapshot, virt_dom,
2026                                           state, instance)
2027 
2028                 # Upload that image to the image service
2029                 update_task_state(task_state=task_states.IMAGE_UPLOADING,
2030                         expected_state=task_states.IMAGE_PENDING_UPLOAD)
2031                 with libvirt_utils.file_open(out_path, 'rb') as image_file:
2032                     # execute operation with disk concurrency semaphore
2033                     with compute_utils.disk_ops_semaphore:
2034                         self._image_api.update(context,
2035                                                image_id,
2036                                                metadata,
2037                                                image_file)
2038         except Exception:
2039             with excutils.save_and_reraise_exception():
2040                 LOG.exception(_("Failed to snapshot image"))
2041                 failed_snap = metadata.pop('location', None)
2042                 if failed_snap:
2043                     failed_snap = {'url': str(failed_snap)}
2044                 root_disk.cleanup_direct_snapshot(
2045                         failed_snap, also_destroy_volume=True,
2046                         ignore_errors=True)
2047 
2048         LOG.info("Snapshot image upload complete", instance=instance)
2049 
2050     def _prepare_domain_for_snapshot(self, context, live_snapshot, state,
2051                                      instance):
2052         # NOTE(dkang): managedSave does not work for LXC
2053         if CONF.libvirt.virt_type != 'lxc' and not live_snapshot:
2054             if state == power_state.RUNNING or state == power_state.PAUSED:
2055                 self.suspend(context, instance)
2056 
2057     def _snapshot_domain(self, context, live_snapshot, virt_dom, state,
2058                          instance):
2059         guest = None
2060         # NOTE(dkang): because previous managedSave is not called
2061         #              for LXC, _create_domain must not be called.
2062         if CONF.libvirt.virt_type != 'lxc' and not live_snapshot:
2063             if state == power_state.RUNNING:
2064                 guest = self._create_domain(domain=virt_dom)
2065             elif state == power_state.PAUSED:
2066                 guest = self._create_domain(domain=virt_dom, pause=True)
2067 
2068             if guest is not None:
2069                 self._attach_pci_devices(
2070                     guest, pci_manager.get_instance_pci_devs(instance))
2071                 self._attach_direct_passthrough_ports(
2072                     context, instance, guest)
2073 
2074     def _can_set_admin_password(self, image_meta):
2075 
2076         if CONF.libvirt.virt_type == 'parallels':
2077             if not self._host.has_min_version(
2078                    MIN_LIBVIRT_PARALLELS_SET_ADMIN_PASSWD):
2079                 raise exception.SetAdminPasswdNotSupported()
2080         elif CONF.libvirt.virt_type in ('kvm', 'qemu'):
2081             if not image_meta.properties.get('hw_qemu_guest_agent', False):
2082                 raise exception.QemuGuestAgentNotEnabled()
2083         else:
2084             raise exception.SetAdminPasswdNotSupported()
2085 
2086     # TODO(melwitt): Combine this with the similar xenapi code at some point.
2087     def _save_instance_password_if_sshkey_present(self, instance, new_pass):
2088         sshkey = instance.key_data if 'key_data' in instance else None
2089         if sshkey and sshkey.startswith("ssh-rsa"):
2090             enc = crypto.ssh_encrypt_text(sshkey, new_pass)
2091             # NOTE(melwitt): The convert_password method doesn't actually do
2092             # anything with the context argument, so we can pass None.
2093             instance.system_metadata.update(
2094                 password.convert_password(None, base64.encode_as_text(enc)))
2095             instance.save()
2096 
2097     def set_admin_password(self, instance, new_pass):
2098         self._can_set_admin_password(instance.image_meta)
2099 
2100         guest = self._host.get_guest(instance)
2101         user = instance.image_meta.properties.get("os_admin_user")
2102         if not user:
2103             if instance.os_type == "windows":
2104                 user = "Administrator"
2105             else:
2106                 user = "root"
2107         try:
2108             guest.set_user_password(user, new_pass)
2109         except libvirt.libvirtError as ex:
2110             error_code = ex.get_error_code()
2111             if error_code == libvirt.VIR_ERR_AGENT_UNRESPONSIVE:
2112                 LOG.debug('Failed to set password: QEMU agent unresponsive',
2113                           instance_uuid=instance.uuid)
2114                 raise NotImplementedError()
2115 
2116             err_msg = encodeutils.exception_to_unicode(ex)
2117             msg = (_('Error from libvirt while set password for username '
2118                      '"%(user)s": [Error Code %(error_code)s] %(ex)s')
2119                    % {'user': user, 'error_code': error_code, 'ex': err_msg})
2120             raise exception.InternalError(msg)
2121         else:
2122             # Save the password in sysmeta so it may be retrieved from the
2123             # metadata service.
2124             self._save_instance_password_if_sshkey_present(instance, new_pass)
2125 
2126     def _can_quiesce(self, instance, image_meta):
2127         if CONF.libvirt.virt_type not in ('kvm', 'qemu'):
2128             raise exception.InstanceQuiesceNotSupported(
2129                 instance_id=instance.uuid)
2130 
2131         if not image_meta.properties.get('hw_qemu_guest_agent', False):
2132             raise exception.QemuGuestAgentNotEnabled()
2133 
2134     def _requires_quiesce(self, image_meta):
2135         return image_meta.properties.get('os_require_quiesce', False)
2136 
2137     def _set_quiesced(self, context, instance, image_meta, quiesced):
2138         self._can_quiesce(instance, image_meta)
2139         try:
2140             guest = self._host.get_guest(instance)
2141             if quiesced:
2142                 guest.freeze_filesystems()
2143             else:
2144                 guest.thaw_filesystems()
2145         except libvirt.libvirtError as ex:
2146             error_code = ex.get_error_code()
2147             err_msg = encodeutils.exception_to_unicode(ex)
2148             msg = (_('Error from libvirt while quiescing %(instance_name)s: '
2149                      '[Error Code %(error_code)s] %(ex)s')
2150                    % {'instance_name': instance.name,
2151                       'error_code': error_code, 'ex': err_msg})
2152             raise exception.InternalError(msg)
2153 
2154     def quiesce(self, context, instance, image_meta):
2155         """Freeze the guest filesystems to prepare for snapshot.
2156 
2157         The qemu-guest-agent must be setup to execute fsfreeze.
2158         """
2159         self._set_quiesced(context, instance, image_meta, True)
2160 
2161     def unquiesce(self, context, instance, image_meta):
2162         """Thaw the guest filesystems after snapshot."""
2163         self._set_quiesced(context, instance, image_meta, False)
2164 
2165     def _live_snapshot(self, context, instance, guest, disk_path, out_path,
2166                        source_format, image_format, image_meta):
2167         """Snapshot an instance without downtime."""
2168         dev = guest.get_block_device(disk_path)
2169 
2170         # Save a copy of the domain's persistent XML file
2171         xml = guest.get_xml_desc(dump_inactive=True, dump_sensitive=True)
2172 
2173         # Abort is an idempotent operation, so make sure any block
2174         # jobs which may have failed are ended.
2175         try:
2176             dev.abort_job()
2177         except Exception:
2178             pass
2179 
2180         # NOTE (rmk): We are using shallow rebases as a workaround to a bug
2181         #             in QEMU 1.3. In order to do this, we need to create
2182         #             a destination image with the original backing file
2183         #             and matching size of the instance root disk.
2184         src_disk_size = libvirt_utils.get_disk_size(disk_path,
2185                                                     format=source_format)
2186         src_back_path = libvirt_utils.get_disk_backing_file(disk_path,
2187                                                         format=source_format,
2188                                                         basename=False)
2189         disk_delta = out_path + '.delta'
2190         libvirt_utils.create_cow_image(src_back_path, disk_delta,
2191                                        src_disk_size)
2192 
2193         quiesced = False
2194         try:
2195             self._set_quiesced(context, instance, image_meta, True)
2196             quiesced = True
2197         except exception.NovaException as err:
2198             if self._requires_quiesce(image_meta):
2199                 raise
2200             LOG.info('Skipping quiescing instance: %(reason)s.',
2201                      {'reason': err}, instance=instance)
2202 
2203         try:
2204             # NOTE (rmk): blockRebase cannot be executed on persistent
2205             #             domains, so we need to temporarily undefine it.
2206             #             If any part of this block fails, the domain is
2207             #             re-defined regardless.
2208             if guest.has_persistent_configuration():
2209                 support_uefi = self._has_uefi_support()
2210                 guest.delete_configuration(support_uefi)
2211 
2212             # NOTE (rmk): Establish a temporary mirror of our root disk and
2213             #             issue an abort once we have a complete copy.
2214             dev.rebase(disk_delta, copy=True, reuse_ext=True, shallow=True)
2215 
2216             while not dev.is_job_complete():
2217                 time.sleep(0.5)
2218 
2219             dev.abort_job()
2220             nova.privsep.path.chown(disk_delta, uid=os.getuid())
2221         finally:
2222             self._host.write_instance_config(xml)
2223             if quiesced:
2224                 self._set_quiesced(context, instance, image_meta, False)
2225 
2226         # Convert the delta (CoW) image with a backing file to a flat
2227         # image with no backing file.
2228         libvirt_utils.extract_snapshot(disk_delta, 'qcow2',
2229                                        out_path, image_format)
2230 
2231     def _volume_snapshot_update_status(self, context, snapshot_id, status):
2232         """Send a snapshot status update to Cinder.
2233 
2234         This method captures and logs exceptions that occur
2235         since callers cannot do anything useful with these exceptions.
2236 
2237         Operations on the Cinder side waiting for this will time out if
2238         a failure occurs sending the update.
2239 
2240         :param context: security context
2241         :param snapshot_id: id of snapshot being updated
2242         :param status: new status value
2243 
2244         """
2245 
2246         try:
2247             self._volume_api.update_snapshot_status(context,
2248                                                     snapshot_id,
2249                                                     status)
2250         except Exception:
2251             LOG.exception(_('Failed to send updated snapshot status '
2252                             'to volume service.'))
2253 
2254     def _volume_snapshot_create(self, context, instance, guest,
2255                                 volume_id, new_file):
2256         """Perform volume snapshot.
2257 
2258            :param guest: VM that volume is attached to
2259            :param volume_id: volume UUID to snapshot
2260            :param new_file: relative path to new qcow2 file present on share
2261 
2262         """
2263         xml = guest.get_xml_desc()
2264         xml_doc = etree.fromstring(xml)
2265 
2266         device_info = vconfig.LibvirtConfigGuest()
2267         device_info.parse_dom(xml_doc)
2268 
2269         disks_to_snap = []          # to be snapshotted by libvirt
2270         network_disks_to_snap = []  # network disks (netfs, etc.)
2271         disks_to_skip = []          # local disks not snapshotted
2272 
2273         for guest_disk in device_info.devices:
2274             if (guest_disk.root_name != 'disk'):
2275                 continue
2276 
2277             if (guest_disk.target_dev is None):
2278                 continue
2279 
2280             if (guest_disk.serial is None or guest_disk.serial != volume_id):
2281                 disks_to_skip.append(guest_disk.target_dev)
2282                 continue
2283 
2284             # disk is a Cinder volume with the correct volume_id
2285 
2286             disk_info = {
2287                 'dev': guest_disk.target_dev,
2288                 'serial': guest_disk.serial,
2289                 'current_file': guest_disk.source_path,
2290                 'source_protocol': guest_disk.source_protocol,
2291                 'source_name': guest_disk.source_name,
2292                 'source_hosts': guest_disk.source_hosts,
2293                 'source_ports': guest_disk.source_ports
2294             }
2295 
2296             # Determine path for new_file based on current path
2297             if disk_info['current_file'] is not None:
2298                 current_file = disk_info['current_file']
2299                 new_file_path = os.path.join(os.path.dirname(current_file),
2300                                              new_file)
2301                 disks_to_snap.append((current_file, new_file_path))
2302             # NOTE(mriedem): This used to include a check for gluster in
2303             # addition to netfs since they were added together. Support for
2304             # gluster was removed in the 16.0.0 Pike release. It is unclear,
2305             # however, if other volume drivers rely on the netfs disk source
2306             # protocol.
2307             elif disk_info['source_protocol'] == 'netfs':
2308                 network_disks_to_snap.append((disk_info, new_file))
2309 
2310         if not disks_to_snap and not network_disks_to_snap:
2311             msg = _('Found no disk to snapshot.')
2312             raise exception.InternalError(msg)
2313 
2314         snapshot = vconfig.LibvirtConfigGuestSnapshot()
2315 
2316         for current_name, new_filename in disks_to_snap:
2317             snap_disk = vconfig.LibvirtConfigGuestSnapshotDisk()
2318             snap_disk.name = current_name
2319             snap_disk.source_path = new_filename
2320             snap_disk.source_type = 'file'
2321             snap_disk.snapshot = 'external'
2322             snap_disk.driver_name = 'qcow2'
2323 
2324             snapshot.add_disk(snap_disk)
2325 
2326         for disk_info, new_filename in network_disks_to_snap:
2327             snap_disk = vconfig.LibvirtConfigGuestSnapshotDisk()
2328             snap_disk.name = disk_info['dev']
2329             snap_disk.source_type = 'network'
2330             snap_disk.source_protocol = disk_info['source_protocol']
2331             snap_disk.snapshot = 'external'
2332             snap_disk.source_path = new_filename
2333             old_dir = disk_info['source_name'].split('/')[0]
2334             snap_disk.source_name = '%s/%s' % (old_dir, new_filename)
2335             snap_disk.source_hosts = disk_info['source_hosts']
2336             snap_disk.source_ports = disk_info['source_ports']
2337 
2338             snapshot.add_disk(snap_disk)
2339 
2340         for dev in disks_to_skip:
2341             snap_disk = vconfig.LibvirtConfigGuestSnapshotDisk()
2342             snap_disk.name = dev
2343             snap_disk.snapshot = 'no'
2344 
2345             snapshot.add_disk(snap_disk)
2346 
2347         snapshot_xml = snapshot.to_xml()
2348         LOG.debug("snap xml: %s", snapshot_xml, instance=instance)
2349 
2350         image_meta = instance.image_meta
2351         try:
2352             # Check to see if we can quiesce the guest before taking the
2353             # snapshot.
2354             self._can_quiesce(instance, image_meta)
2355             try:
2356                 guest.snapshot(snapshot, no_metadata=True, disk_only=True,
2357                                reuse_ext=True, quiesce=True)
2358                 return
2359             except libvirt.libvirtError:
2360                 # If the image says that quiesce is required then we fail.
2361                 if self._requires_quiesce(image_meta):
2362                     raise
2363                 LOG.exception(_('Unable to create quiesced VM snapshot, '
2364                                 'attempting again with quiescing disabled.'),
2365                               instance=instance)
2366         except (exception.InstanceQuiesceNotSupported,
2367                 exception.QemuGuestAgentNotEnabled) as err:
2368             # If the image says that quiesce is required then we need to fail.
2369             if self._requires_quiesce(image_meta):
2370                 raise
2371             LOG.info('Skipping quiescing instance: %(reason)s.',
2372                      {'reason': err}, instance=instance)
2373 
2374         try:
2375             guest.snapshot(snapshot, no_metadata=True, disk_only=True,
2376                            reuse_ext=True, quiesce=False)
2377         except libvirt.libvirtError:
2378             LOG.exception(_('Unable to create VM snapshot, '
2379                             'failing volume_snapshot operation.'),
2380                           instance=instance)
2381 
2382             raise
2383 
2384     def _volume_refresh_connection_info(self, context, instance, volume_id):
2385         bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
2386                   context, volume_id, instance.uuid)
2387 
2388         driver_bdm = driver_block_device.convert_volume(bdm)
2389         if driver_bdm:
2390             driver_bdm.refresh_connection_info(context, instance,
2391                                                self._volume_api, self)
2392 
2393     def volume_snapshot_create(self, context, instance, volume_id,
2394                                create_info):
2395         """Create snapshots of a Cinder volume via libvirt.
2396 
2397         :param instance: VM instance object reference
2398         :param volume_id: id of volume being snapshotted
2399         :param create_info: dict of information used to create snapshots
2400                      - snapshot_id : ID of snapshot
2401                      - type : qcow2 / <other>
2402                      - new_file : qcow2 file created by Cinder which
2403                      becomes the VM's active image after
2404                      the snapshot is complete
2405         """
2406 
2407         LOG.debug("volume_snapshot_create: create_info: %(c_info)s",
2408                   {'c_info': create_info}, instance=instance)
2409 
2410         try:
2411             guest = self._host.get_guest(instance)
2412         except exception.InstanceNotFound:
2413             raise exception.InstanceNotRunning(instance_id=instance.uuid)
2414 
2415         if create_info['type'] != 'qcow2':
2416             msg = _('Unknown type: %s') % create_info['type']
2417             raise exception.InternalError(msg)
2418 
2419         snapshot_id = create_info.get('snapshot_id', None)
2420         if snapshot_id is None:
2421             msg = _('snapshot_id required in create_info')
2422             raise exception.InternalError(msg)
2423 
2424         try:
2425             self._volume_snapshot_create(context, instance, guest,
2426                                          volume_id, create_info['new_file'])
2427         except Exception:
2428             with excutils.save_and_reraise_exception():
2429                 LOG.exception(_('Error occurred during '
2430                                 'volume_snapshot_create, '
2431                                 'sending error status to Cinder.'),
2432                               instance=instance)
2433                 self._volume_snapshot_update_status(
2434                     context, snapshot_id, 'error')
2435 
2436         self._volume_snapshot_update_status(
2437             context, snapshot_id, 'creating')
2438 
2439         def _wait_for_snapshot():
2440             snapshot = self._volume_api.get_snapshot(context, snapshot_id)
2441 
2442             if snapshot.get('status') != 'creating':
2443                 self._volume_refresh_connection_info(context, instance,
2444                                                      volume_id)
2445                 raise loopingcall.LoopingCallDone()
2446 
2447         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_snapshot)
2448         timer.start(interval=0.5).wait()
2449 
2450     @staticmethod
2451     def _rebase_with_qemu_img(guest, device, active_disk_object,
2452                               rebase_base):
2453         """Rebase a device tied to a guest using qemu-img.
2454 
2455         :param guest:the Guest which owns the device being rebased
2456         :type guest: nova.virt.libvirt.guest.Guest
2457         :param device: the guest block device to rebase
2458         :type device: nova.virt.libvirt.guest.BlockDevice
2459         :param active_disk_object: the guest block device to rebase
2460         :type active_disk_object: nova.virt.libvirt.config.\
2461                                     LibvirtConfigGuestDisk
2462         :param rebase_base: the new parent in the backing chain
2463         :type rebase_base: None or string
2464         """
2465 
2466         # It's unsure how well qemu-img handles network disks for
2467         # every protocol. So let's be safe.
2468         active_protocol = active_disk_object.source_protocol
2469         if active_protocol is not None:
2470             msg = _("Something went wrong when deleting a volume snapshot: "
2471                     "rebasing a %(protocol)s network disk using qemu-img "
2472                     "has not been fully tested") % {'protocol':
2473                     active_protocol}
2474             LOG.error(msg)
2475             raise exception.InternalError(msg)
2476 
2477         if rebase_base is None:
2478             # If backing_file is specified as "" (the empty string), then
2479             # the image is rebased onto no backing file (i.e. it will exist
2480             # independently of any backing file).
2481             backing_file = ""
2482             qemu_img_extra_arg = []
2483         else:
2484             # If the rebased image is going to have a backing file then
2485             # explicitly set the backing file format to avoid any security
2486             # concerns related to file format auto detection.
2487             backing_file = rebase_base
2488             b_file_fmt = images.qemu_img_info(backing_file).file_format
2489             qemu_img_extra_arg = ['-F', b_file_fmt]
2490 
2491         qemu_img_extra_arg.append(active_disk_object.source_path)
2492         # execute operation with disk concurrency semaphore
2493         with compute_utils.disk_ops_semaphore:
2494             processutils.execute("qemu-img", "rebase", "-b", backing_file,
2495                                  *qemu_img_extra_arg)
2496 
2497     def _volume_snapshot_delete(self, context, instance, volume_id,
2498                                 snapshot_id, delete_info=None):
2499         """Note:
2500             if file being merged into == active image:
2501                 do a blockRebase (pull) operation
2502             else:
2503                 do a blockCommit operation
2504             Files must be adjacent in snap chain.
2505 
2506         :param instance: instance object reference
2507         :param volume_id: volume UUID
2508         :param snapshot_id: snapshot UUID (unused currently)
2509         :param delete_info: {
2510             'type':              'qcow2',
2511             'file_to_merge':     'a.img',
2512             'merge_target_file': 'b.img' or None (if merging file_to_merge into
2513                                                   active image)
2514           }
2515         """
2516 
2517         LOG.debug('volume_snapshot_delete: delete_info: %s', delete_info,
2518                   instance=instance)
2519 
2520         if delete_info['type'] != 'qcow2':
2521             msg = _('Unknown delete_info type %s') % delete_info['type']
2522             raise exception.InternalError(msg)
2523 
2524         try:
2525             guest = self._host.get_guest(instance)
2526         except exception.InstanceNotFound:
2527             raise exception.InstanceNotRunning(instance_id=instance.uuid)
2528 
2529         # Find dev name
2530         my_dev = None
2531         active_disk = None
2532 
2533         xml = guest.get_xml_desc()
2534         xml_doc = etree.fromstring(xml)
2535 
2536         device_info = vconfig.LibvirtConfigGuest()
2537         device_info.parse_dom(xml_doc)
2538 
2539         active_disk_object = None
2540 
2541         for guest_disk in device_info.devices:
2542             if (guest_disk.root_name != 'disk'):
2543                 continue
2544 
2545             if (guest_disk.target_dev is None or guest_disk.serial is None):
2546                 continue
2547 
2548             if guest_disk.serial == volume_id:
2549                 my_dev = guest_disk.target_dev
2550 
2551                 active_disk = guest_disk.source_path
2552                 active_protocol = guest_disk.source_protocol
2553                 active_disk_object = guest_disk
2554                 break
2555 
2556         if my_dev is None or (active_disk is None and active_protocol is None):
2557             LOG.debug('Domain XML: %s', xml, instance=instance)
2558             msg = (_('Disk with id: %s not found attached to instance.')
2559                    % volume_id)
2560             raise exception.InternalError(msg)
2561 
2562         LOG.debug("found device at %s", my_dev, instance=instance)
2563 
2564         def _get_snap_dev(filename, backing_store):
2565             if filename is None:
2566                 msg = _('filename cannot be None')
2567                 raise exception.InternalError(msg)
2568 
2569             # libgfapi delete
2570             LOG.debug("XML: %s", xml)
2571 
2572             LOG.debug("active disk object: %s", active_disk_object)
2573 
2574             # determine reference within backing store for desired image
2575             filename_to_merge = filename
2576             matched_name = None
2577             b = backing_store
2578             index = None
2579 
2580             current_filename = active_disk_object.source_name.split('/')[1]
2581             if current_filename == filename_to_merge:
2582                 return my_dev + '[0]'
2583 
2584             while b is not None:
2585                 source_filename = b.source_name.split('/')[1]
2586                 if source_filename == filename_to_merge:
2587                     LOG.debug('found match: %s', b.source_name)
2588                     matched_name = b.source_name
2589                     index = b.index
2590                     break
2591 
2592                 b = b.backing_store
2593 
2594             if matched_name is None:
2595                 msg = _('no match found for %s') % (filename_to_merge)
2596                 raise exception.InternalError(msg)
2597 
2598             LOG.debug('index of match (%s) is %s', b.source_name, index)
2599 
2600             my_snap_dev = '%s[%s]' % (my_dev, index)
2601             return my_snap_dev
2602 
2603         if delete_info['merge_target_file'] is None:
2604             # pull via blockRebase()
2605 
2606             # Merge the most recent snapshot into the active image
2607 
2608             rebase_disk = my_dev
2609             rebase_base = delete_info['file_to_merge']  # often None
2610             if (active_protocol is not None) and (rebase_base is not None):
2611                 rebase_base = _get_snap_dev(rebase_base,
2612                                             active_disk_object.backing_store)
2613 
2614             # NOTE(deepakcs): libvirt added support for _RELATIVE in v1.2.7,
2615             # and when available this flag _must_ be used to ensure backing
2616             # paths are maintained relative by qemu.
2617             #
2618             # If _RELATIVE flag not found, continue with old behaviour
2619             # (relative backing path seems to work for this case)
2620             try:
2621                 libvirt.VIR_DOMAIN_BLOCK_REBASE_RELATIVE
2622                 relative = rebase_base is not None
2623             except AttributeError:
2624                 LOG.warning(
2625                     "Relative blockrebase support was not detected. "
2626                     "Continuing with old behaviour.")
2627                 relative = False
2628 
2629             LOG.debug(
2630                 'disk: %(disk)s, base: %(base)s, '
2631                 'bw: %(bw)s, relative: %(relative)s',
2632                 {'disk': rebase_disk,
2633                  'base': rebase_base,
2634                  'bw': libvirt_guest.BlockDevice.REBASE_DEFAULT_BANDWIDTH,
2635                  'relative': str(relative)}, instance=instance)
2636 
2637             dev = guest.get_block_device(rebase_disk)
2638             if guest.is_active():
2639                 result = dev.rebase(rebase_base, relative=relative)
2640                 if result == 0:
2641                     LOG.debug('blockRebase started successfully',
2642                               instance=instance)
2643 
2644                 while not dev.is_job_complete():
2645                     LOG.debug('waiting for blockRebase job completion',
2646                               instance=instance)
2647                     time.sleep(0.5)
2648 
2649             # If the guest is not running libvirt won't do a blockRebase.
2650             # In that case, let's ask qemu-img to rebase the disk.
2651             else:
2652                 LOG.debug('Guest is not running so doing a block rebase '
2653                           'using "qemu-img rebase"', instance=instance)
2654                 self._rebase_with_qemu_img(guest, dev, active_disk_object,
2655                                            rebase_base)
2656 
2657         else:
2658             # commit with blockCommit()
2659             my_snap_base = None
2660             my_snap_top = None
2661             commit_disk = my_dev
2662 
2663             if active_protocol is not None:
2664                 my_snap_base = _get_snap_dev(delete_info['merge_target_file'],
2665                                              active_disk_object.backing_store)
2666                 my_snap_top = _get_snap_dev(delete_info['file_to_merge'],
2667                                             active_disk_object.backing_store)
2668 
2669             commit_base = my_snap_base or delete_info['merge_target_file']
2670             commit_top = my_snap_top or delete_info['file_to_merge']
2671 
2672             LOG.debug('will call blockCommit with commit_disk=%(commit_disk)s '
2673                       'commit_base=%(commit_base)s '
2674                       'commit_top=%(commit_top)s ',
2675                       {'commit_disk': commit_disk,
2676                        'commit_base': commit_base,
2677                        'commit_top': commit_top}, instance=instance)
2678 
2679             dev = guest.get_block_device(commit_disk)
2680             result = dev.commit(commit_base, commit_top, relative=True)
2681 
2682             if result == 0:
2683                 LOG.debug('blockCommit started successfully',
2684                           instance=instance)
2685 
2686             while not dev.is_job_complete():
2687                 LOG.debug('waiting for blockCommit job completion',
2688                           instance=instance)
2689                 time.sleep(0.5)
2690 
2691     def volume_snapshot_delete(self, context, instance, volume_id, snapshot_id,
2692                                delete_info):
2693         try:
2694             self._volume_snapshot_delete(context, instance, volume_id,
2695                                          snapshot_id, delete_info=delete_info)
2696         except Exception:
2697             with excutils.save_and_reraise_exception():
2698                 LOG.exception(_('Error occurred during '
2699                                 'volume_snapshot_delete, '
2700                                 'sending error status to Cinder.'),
2701                               instance=instance)
2702                 self._volume_snapshot_update_status(
2703                     context, snapshot_id, 'error_deleting')
2704 
2705         self._volume_snapshot_update_status(context, snapshot_id, 'deleting')
2706         self._volume_refresh_connection_info(context, instance, volume_id)
2707 
2708     def reboot(self, context, instance, network_info, reboot_type,
2709                block_device_info=None, bad_volumes_callback=None):
2710         """Reboot a virtual machine, given an instance reference."""
2711         if reboot_type == 'SOFT':
2712             # NOTE(vish): This will attempt to do a graceful shutdown/restart.
2713             try:
2714                 soft_reboot_success = self._soft_reboot(instance)
2715             except libvirt.libvirtError as e:
2716                 LOG.debug("Instance soft reboot failed: %s",
2717                           encodeutils.exception_to_unicode(e),
2718                           instance=instance)
2719                 soft_reboot_success = False
2720 
2721             if soft_reboot_success:
2722                 LOG.info("Instance soft rebooted successfully.",
2723                          instance=instance)
2724                 return
2725             else:
2726                 LOG.warning("Failed to soft reboot instance. "
2727                             "Trying hard reboot.",
2728                             instance=instance)
2729         return self._hard_reboot(context, instance, network_info,
2730                                  block_device_info)
2731 
2732     def _soft_reboot(self, instance):
2733         """Attempt to shutdown and restart the instance gracefully.
2734 
2735         We use shutdown and create here so we can return if the guest
2736         responded and actually rebooted. Note that this method only
2737         succeeds if the guest responds to acpi. Therefore we return
2738         success or failure so we can fall back to a hard reboot if
2739         necessary.
2740 
2741         :returns: True if the reboot succeeded
2742         """
2743         guest = self._host.get_guest(instance)
2744 
2745         state = guest.get_power_state(self._host)
2746         old_domid = guest.id
2747         # NOTE(vish): This check allows us to reboot an instance that
2748         #             is already shutdown.
2749         if state == power_state.RUNNING:
2750             guest.shutdown()
2751         # NOTE(vish): This actually could take slightly longer than the
2752         #             FLAG defines depending on how long the get_info
2753         #             call takes to return.
2754         self._prepare_pci_devices_for_use(
2755             pci_manager.get_instance_pci_devs(instance, 'all'))
2756         for x in range(CONF.libvirt.wait_soft_reboot_seconds):
2757             guest = self._host.get_guest(instance)
2758 
2759             state = guest.get_power_state(self._host)
2760             new_domid = guest.id
2761 
2762             # NOTE(ivoks): By checking domain IDs, we make sure we are
2763             #              not recreating domain that's already running.
2764             if old_domid != new_domid:
2765                 if state in [power_state.SHUTDOWN,
2766                              power_state.CRASHED]:
2767                     LOG.info("Instance shutdown successfully.",
2768                              instance=instance)
2769                     self._create_domain(domain=guest._domain)
2770                     timer = loopingcall.FixedIntervalLoopingCall(
2771                         self._wait_for_running, instance)
2772                     timer.start(interval=0.5).wait()
2773                     return True
2774                 else:
2775                     LOG.info("Instance may have been rebooted during soft "
2776                              "reboot, so return now.", instance=instance)
2777                     return True
2778             greenthread.sleep(1)
2779         return False
2780 
2781     def _hard_reboot(self, context, instance, network_info,
2782                      block_device_info=None):
2783         """Reboot a virtual machine, given an instance reference.
2784 
2785         Performs a Libvirt reset (if supported) on the domain.
2786 
2787         If Libvirt reset is unavailable this method actually destroys and
2788         re-creates the domain to ensure the reboot happens, as the guest
2789         OS cannot ignore this action.
2790         """
2791         # NOTE(sbauza): Since we undefine the guest XML when destroying, we
2792         # need to remember the existing mdevs for reusing them.
2793         mdevs = self._get_all_assigned_mediated_devices(instance)
2794         mdevs = list(mdevs.keys())
2795         # NOTE(mdbooth): In addition to performing a hard reboot of the domain,
2796         # the hard reboot operation is relied upon by operators to be an
2797         # automated attempt to fix as many things as possible about a
2798         # non-functioning instance before resorting to manual intervention.
2799         # With this goal in mind, we tear down all the aspects of an instance
2800         # we can here without losing data. This allows us to re-initialise from
2801         # scratch, and hopefully fix, most aspects of a non-functioning guest.
2802         self.destroy(context, instance, network_info, destroy_disks=False,
2803                      block_device_info=block_device_info)
2804 
2805         # Convert the system metadata to image metadata
2806         # NOTE(mdbooth): This is a workaround for stateless Nova compute
2807         #                https://bugs.launchpad.net/nova/+bug/1349978
2808         instance_dir = libvirt_utils.get_instance_path(instance)
2809         fileutils.ensure_tree(instance_dir)
2810 
2811         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
2812                                             instance,
2813                                             instance.image_meta,
2814                                             block_device_info)
2815         # NOTE(vish): This could generate the wrong device_format if we are
2816         #             using the raw backend and the images don't exist yet.
2817         #             The create_images_and_backing below doesn't properly
2818         #             regenerate raw backend images, however, so when it
2819         #             does we need to (re)generate the xml after the images
2820         #             are in place.
2821         xml = self._get_guest_xml(context, instance, network_info, disk_info,
2822                                   instance.image_meta,
2823                                   block_device_info=block_device_info,
2824                                   mdevs=mdevs)
2825 
2826         # NOTE(mdbooth): context.auth_token will not be set when we call
2827         #                _hard_reboot from resume_state_on_host_boot()
2828         if context.auth_token is not None:
2829             # NOTE (rmk): Re-populate any missing backing files.
2830             config = vconfig.LibvirtConfigGuest()
2831             config.parse_str(xml)
2832             backing_disk_info = self._get_instance_disk_info_from_config(
2833                 config, block_device_info)
2834             self._create_images_and_backing(context, instance, instance_dir,
2835                                             backing_disk_info)
2836 
2837         # Initialize all the necessary networking, block devices and
2838         # start the instance.
2839         # NOTE(melwitt): Pass vifs_already_plugged=True here even though we've
2840         # unplugged vifs earlier. The behavior of neutron plug events depends
2841         # on which vif type we're using and we are working with a stale network
2842         # info cache here, so won't rely on waiting for neutron plug events.
2843         # vifs_already_plugged=True means "do not wait for neutron plug events"
2844         self._create_domain_and_network(context, xml, instance, network_info,
2845                                         block_device_info=block_device_info,
2846                                         vifs_already_plugged=True)
2847         self._prepare_pci_devices_for_use(
2848             pci_manager.get_instance_pci_devs(instance, 'all'))
2849 
2850         def _wait_for_reboot():
2851             """Called at an interval until the VM is running again."""
2852             state = self.get_info(instance).state
2853 
2854             if state == power_state.RUNNING:
2855                 LOG.info("Instance rebooted successfully.",
2856                          instance=instance)
2857                 raise loopingcall.LoopingCallDone()
2858 
2859         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_reboot)
2860         timer.start(interval=0.5).wait()
2861 
2862     def pause(self, instance):
2863         """Pause VM instance."""
2864         self._host.get_guest(instance).pause()
2865 
2866     def unpause(self, instance):
2867         """Unpause paused VM instance."""
2868         guest = self._host.get_guest(instance)
2869         guest.resume()
2870         guest.sync_guest_time()
2871 
2872     def _clean_shutdown(self, instance, timeout, retry_interval):
2873         """Attempt to shutdown the instance gracefully.
2874 
2875         :param instance: The instance to be shutdown
2876         :param timeout: How long to wait in seconds for the instance to
2877                         shutdown
2878         :param retry_interval: How often in seconds to signal the instance
2879                                to shutdown while waiting
2880 
2881         :returns: True if the shutdown succeeded
2882         """
2883 
2884         # List of states that represent a shutdown instance
2885         SHUTDOWN_STATES = [power_state.SHUTDOWN,
2886                            power_state.CRASHED]
2887 
2888         try:
2889             guest = self._host.get_guest(instance)
2890         except exception.InstanceNotFound:
2891             # If the instance has gone then we don't need to
2892             # wait for it to shutdown
2893             return True
2894 
2895         state = guest.get_power_state(self._host)
2896         if state in SHUTDOWN_STATES:
2897             LOG.info("Instance already shutdown.", instance=instance)
2898             return True
2899 
2900         LOG.debug("Shutting down instance from state %s", state,
2901                   instance=instance)
2902         guest.shutdown()
2903         retry_countdown = retry_interval
2904 
2905         for sec in range(timeout):
2906 
2907             guest = self._host.get_guest(instance)
2908             state = guest.get_power_state(self._host)
2909 
2910             if state in SHUTDOWN_STATES:
2911                 LOG.info("Instance shutdown successfully after %d seconds.",
2912                          sec, instance=instance)
2913                 return True
2914 
2915             # Note(PhilD): We can't assume that the Guest was able to process
2916             #              any previous shutdown signal (for example it may
2917             #              have still been startingup, so within the overall
2918             #              timeout we re-trigger the shutdown every
2919             #              retry_interval
2920             if retry_countdown == 0:
2921                 retry_countdown = retry_interval
2922                 # Instance could shutdown at any time, in which case we
2923                 # will get an exception when we call shutdown
2924                 try:
2925                     LOG.debug("Instance in state %s after %d seconds - "
2926                               "resending shutdown", state, sec,
2927                               instance=instance)
2928                     guest.shutdown()
2929                 except libvirt.libvirtError:
2930                     # Assume this is because its now shutdown, so loop
2931                     # one more time to clean up.
2932                     LOG.debug("Ignoring libvirt exception from shutdown "
2933                               "request.", instance=instance)
2934                     continue
2935             else:
2936                 retry_countdown -= 1
2937 
2938             time.sleep(1)
2939 
2940         LOG.info("Instance failed to shutdown in %d seconds.",
2941                  timeout, instance=instance)
2942         return False
2943 
2944     def power_off(self, instance, timeout=0, retry_interval=0):
2945         """Power off the specified instance."""
2946         if timeout:
2947             self._clean_shutdown(instance, timeout, retry_interval)
2948         self._destroy(instance)
2949 
2950     def power_on(self, context, instance, network_info,
2951                  block_device_info=None):
2952         """Power on the specified instance."""
2953         # We use _hard_reboot here to ensure that all backing files,
2954         # network, and block device connections, etc. are established
2955         # and available before we attempt to start the instance.
2956         self._hard_reboot(context, instance, network_info, block_device_info)
2957 
2958     def trigger_crash_dump(self, instance):
2959 
2960         """Trigger crash dump by injecting an NMI to the specified instance."""
2961         try:
2962             self._host.get_guest(instance).inject_nmi()
2963         except libvirt.libvirtError as ex:
2964             error_code = ex.get_error_code()
2965 
2966             if error_code == libvirt.VIR_ERR_NO_SUPPORT:
2967                 raise exception.TriggerCrashDumpNotSupported()
2968             elif error_code == libvirt.VIR_ERR_OPERATION_INVALID:
2969                 raise exception.InstanceNotRunning(instance_id=instance.uuid)
2970 
2971             LOG.exception(_('Error from libvirt while injecting an NMI to '
2972                             '%(instance_uuid)s: '
2973                             '[Error Code %(error_code)s] %(ex)s'),
2974                           {'instance_uuid': instance.uuid,
2975                            'error_code': error_code, 'ex': ex})
2976             raise
2977 
2978     def suspend(self, context, instance):
2979         """Suspend the specified instance."""
2980         guest = self._host.get_guest(instance)
2981 
2982         self._detach_pci_devices(guest,
2983             pci_manager.get_instance_pci_devs(instance))
2984         self._detach_direct_passthrough_ports(context, instance, guest)
2985         self._detach_mediated_devices(guest)
2986         guest.save_memory_state()
2987 
2988     def resume(self, context, instance, network_info, block_device_info=None):
2989         """resume the specified instance."""
2990         xml = self._get_existing_domain_xml(instance, network_info,
2991                                             block_device_info)
2992         guest = self._create_domain_and_network(context, xml, instance,
2993                            network_info, block_device_info=block_device_info,
2994                            vifs_already_plugged=True)
2995         self._attach_pci_devices(guest,
2996             pci_manager.get_instance_pci_devs(instance))
2997         self._attach_direct_passthrough_ports(
2998             context, instance, guest, network_info)
2999         timer = loopingcall.FixedIntervalLoopingCall(self._wait_for_running,
3000                                                      instance)
3001         timer.start(interval=0.5).wait()
3002         guest.sync_guest_time()
3003 
3004     def resume_state_on_host_boot(self, context, instance, network_info,
3005                                   block_device_info=None):
3006         """resume guest state when a host is booted."""
3007         # Check if the instance is running already and avoid doing
3008         # anything if it is.
3009         try:
3010             guest = self._host.get_guest(instance)
3011             state = guest.get_power_state(self._host)
3012 
3013             ignored_states = (power_state.RUNNING,
3014                               power_state.SUSPENDED,
3015                               power_state.NOSTATE,
3016                               power_state.PAUSED)
3017 
3018             if state in ignored_states:
3019                 return
3020         except (exception.InternalError, exception.InstanceNotFound):
3021             pass
3022 
3023         # Instance is not up and could be in an unknown state.
3024         # Be as absolute as possible about getting it back into
3025         # a known and running state.
3026         self._hard_reboot(context, instance, network_info, block_device_info)
3027 
3028     def rescue(self, context, instance, network_info, image_meta,
3029                rescue_password):
3030         """Loads a VM using rescue images.
3031 
3032         A rescue is normally performed when something goes wrong with the
3033         primary images and data needs to be corrected/recovered. Rescuing
3034         should not edit or over-ride the original image, only allow for
3035         data recovery.
3036 
3037         """
3038         instance_dir = libvirt_utils.get_instance_path(instance)
3039         unrescue_xml = self._get_existing_domain_xml(instance, network_info)
3040         unrescue_xml_path = os.path.join(instance_dir, 'unrescue.xml')
3041         libvirt_utils.write_to_file(unrescue_xml_path, unrescue_xml)
3042 
3043         rescue_image_id = None
3044         if image_meta.obj_attr_is_set("id"):
3045             rescue_image_id = image_meta.id
3046 
3047         rescue_images = {
3048             'image_id': (rescue_image_id or
3049                         CONF.libvirt.rescue_image_id or instance.image_ref),
3050             'kernel_id': (CONF.libvirt.rescue_kernel_id or
3051                           instance.kernel_id),
3052             'ramdisk_id': (CONF.libvirt.rescue_ramdisk_id or
3053                            instance.ramdisk_id),
3054         }
3055         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
3056                                             instance,
3057                                             image_meta,
3058                                             rescue=True)
3059         injection_info = InjectionInfo(network_info=network_info,
3060                                        admin_pass=rescue_password,
3061                                        files=None)
3062         gen_confdrive = functools.partial(self._create_configdrive,
3063                                           context, instance, injection_info,
3064                                           rescue=True)
3065         # NOTE(sbauza): Since rescue recreates the guest XML, we need to
3066         # remember the existing mdevs for reusing them.
3067         mdevs = self._get_all_assigned_mediated_devices(instance)
3068         mdevs = list(mdevs.keys())
3069         self._create_image(context, instance, disk_info['mapping'],
3070                            injection_info=injection_info, suffix='.rescue',
3071                            disk_images=rescue_images)
3072         xml = self._get_guest_xml(context, instance, network_info, disk_info,
3073                                   image_meta, rescue=rescue_images,
3074                                   mdevs=mdevs)
3075         self._destroy(instance)
3076         self._create_domain(xml, post_xml_callback=gen_confdrive)
3077 
3078     def unrescue(self, instance, network_info):
3079         """Reboot the VM which is being rescued back into primary images.
3080         """
3081         instance_dir = libvirt_utils.get_instance_path(instance)
3082         unrescue_xml_path = os.path.join(instance_dir, 'unrescue.xml')
3083         xml = libvirt_utils.load_file(unrescue_xml_path)
3084         guest = self._host.get_guest(instance)
3085 
3086         # TODO(sahid): We are converting all calls from a
3087         # virDomain object to use nova.virt.libvirt.Guest.
3088         # We should be able to remove virt_dom at the end.
3089         virt_dom = guest._domain
3090         self._destroy(instance)
3091         self._create_domain(xml, virt_dom)
3092         os.unlink(unrescue_xml_path)
3093         rescue_files = os.path.join(instance_dir, "*.rescue")
3094         for rescue_file in glob.iglob(rescue_files):
3095             if os.path.isdir(rescue_file):
3096                 shutil.rmtree(rescue_file)
3097             else:
3098                 os.unlink(rescue_file)
3099         # cleanup rescue volume
3100         lvm.remove_volumes([lvmdisk for lvmdisk in self._lvm_disks(instance)
3101                                 if lvmdisk.endswith('.rescue')])
3102         if CONF.libvirt.images_type == 'rbd':
3103             filter_fn = lambda disk: (disk.startswith(instance.uuid) and
3104                                       disk.endswith('.rescue'))
3105             LibvirtDriver._get_rbd_driver().cleanup_volumes(filter_fn)
3106 
3107     def poll_rebooting_instances(self, timeout, instances):
3108         pass
3109 
3110     def spawn(self, context, instance, image_meta, injected_files,
3111               admin_password, allocations, network_info=None,
3112               block_device_info=None):
3113         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
3114                                             instance,
3115                                             image_meta,
3116                                             block_device_info)
3117         injection_info = InjectionInfo(network_info=network_info,
3118                                        files=injected_files,
3119                                        admin_pass=admin_password)
3120         gen_confdrive = functools.partial(self._create_configdrive,
3121                                           context, instance,
3122                                           injection_info)
3123         self._create_image(context, instance, disk_info['mapping'],
3124                            injection_info=injection_info,
3125                            block_device_info=block_device_info)
3126 
3127         # Required by Quobyte CI
3128         self._ensure_console_log_for_instance(instance)
3129 
3130         # Does the guest need to be assigned some vGPU mediated devices ?
3131         mdevs = self._allocate_mdevs(allocations)
3132 
3133         xml = self._get_guest_xml(context, instance, network_info,
3134                                   disk_info, image_meta,
3135                                   block_device_info=block_device_info,
3136                                   mdevs=mdevs)
3137         self._create_domain_and_network(
3138             context, xml, instance, network_info,
3139             block_device_info=block_device_info,
3140             post_xml_callback=gen_confdrive,
3141             destroy_disks_on_failure=True)
3142         LOG.debug("Guest created on hypervisor", instance=instance)
3143 
3144         def _wait_for_boot():
3145             """Called at an interval until the VM is running."""
3146             state = self.get_info(instance).state
3147 
3148             if state == power_state.RUNNING:
3149                 LOG.info("Instance spawned successfully.", instance=instance)
3150                 raise loopingcall.LoopingCallDone()
3151 
3152         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_boot)
3153         timer.start(interval=0.5).wait()
3154 
3155     def _get_console_output_file(self, instance, console_log):
3156         bytes_to_read = MAX_CONSOLE_BYTES
3157         log_data = b""  # The last N read bytes
3158         i = 0  # in case there is a log rotation (like "virtlogd")
3159         path = console_log
3160 
3161         while bytes_to_read > 0 and os.path.exists(path):
3162             read_log_data, remaining = nova.privsep.path.last_bytes(
3163                                         path, bytes_to_read)
3164             # We need the log file content in chronological order,
3165             # that's why we *prepend* the log data.
3166             log_data = read_log_data + log_data
3167 
3168             # Prep to read the next file in the chain
3169             bytes_to_read -= len(read_log_data)
3170             path = console_log + "." + str(i)
3171             i += 1
3172 
3173             if remaining > 0:
3174                 LOG.info('Truncated console log returned, '
3175                          '%d bytes ignored', remaining, instance=instance)
3176         return log_data
3177 
3178     def get_console_output(self, context, instance):
3179         guest = self._host.get_guest(instance)
3180 
3181         xml = guest.get_xml_desc()
3182         tree = etree.fromstring(xml)
3183 
3184         # check for different types of consoles
3185         path_sources = [
3186             ('file', "./devices/console[@type='file']/source[@path]", 'path'),
3187             ('tcp', "./devices/console[@type='tcp']/log[@file]", 'file'),
3188             ('pty', "./devices/console[@type='pty']/source[@path]", 'path')]
3189         console_type = ""
3190         console_path = ""
3191         for c_type, epath, attrib in path_sources:
3192             node = tree.find(epath)
3193             if (node is not None) and node.get(attrib):
3194                 console_type = c_type
3195                 console_path = node.get(attrib)
3196                 break
3197 
3198         # instance has no console at all
3199         if not console_path:
3200             raise exception.ConsoleNotAvailable()
3201 
3202         # instance has a console, but file doesn't exist (yet?)
3203         if not os.path.exists(console_path):
3204             LOG.info('console logfile for instance does not exist',
3205                       instance=instance)
3206             return ""
3207 
3208         # pty consoles need special handling
3209         if console_type == 'pty':
3210             console_log = self._get_console_log_path(instance)
3211             data = nova.privsep.libvirt.readpty(console_path)
3212 
3213             # NOTE(markus_z): The virt_types kvm and qemu are the only ones
3214             # which create a dedicated file device for the console logging.
3215             # Other virt_types like xen, lxc, uml, parallels depend on the
3216             # flush of that pty device into the "console.log" file to ensure
3217             # that a series of "get_console_output" calls return the complete
3218             # content even after rebooting a guest.
3219             nova.privsep.path.writefile(console_log, 'a+', data)
3220 
3221             # set console path to logfile, not to pty device
3222             console_path = console_log
3223 
3224         # return logfile content
3225         return self._get_console_output_file(instance, console_path)
3226 
3227     def get_host_ip_addr(self):
3228         ips = compute_utils.get_machine_ips()
3229         if CONF.my_ip not in ips:
3230             LOG.warning('my_ip address (%(my_ip)s) was not found on '
3231                         'any of the interfaces: %(ifaces)s',
3232                         {'my_ip': CONF.my_ip, 'ifaces': ", ".join(ips)})
3233         return CONF.my_ip
3234 
3235     def get_vnc_console(self, context, instance):
3236         def get_vnc_port_for_instance(instance_name):
3237             guest = self._host.get_guest(instance)
3238 
3239             xml = guest.get_xml_desc()
3240             xml_dom = etree.fromstring(xml)
3241 
3242             graphic = xml_dom.find("./devices/graphics[@type='vnc']")
3243             if graphic is not None:
3244                 return graphic.get('port')
3245             # NOTE(rmk): We had VNC consoles enabled but the instance in
3246             # question is not actually listening for connections.
3247             raise exception.ConsoleTypeUnavailable(console_type='vnc')
3248 
3249         port = get_vnc_port_for_instance(instance.name)
3250         host = CONF.vnc.server_proxyclient_address
3251 
3252         return ctype.ConsoleVNC(host=host, port=port)
3253 
3254     def get_spice_console(self, context, instance):
3255         def get_spice_ports_for_instance(instance_name):
3256             guest = self._host.get_guest(instance)
3257 
3258             xml = guest.get_xml_desc()
3259             xml_dom = etree.fromstring(xml)
3260 
3261             graphic = xml_dom.find("./devices/graphics[@type='spice']")
3262             if graphic is not None:
3263                 return (graphic.get('port'), graphic.get('tlsPort'))
3264             # NOTE(rmk): We had Spice consoles enabled but the instance in
3265             # question is not actually listening for connections.
3266             raise exception.ConsoleTypeUnavailable(console_type='spice')
3267 
3268         ports = get_spice_ports_for_instance(instance.name)
3269         host = CONF.spice.server_proxyclient_address
3270 
3271         return ctype.ConsoleSpice(host=host, port=ports[0], tlsPort=ports[1])
3272 
3273     def get_serial_console(self, context, instance):
3274         guest = self._host.get_guest(instance)
3275         for hostname, port in self._get_serial_ports_from_guest(
3276                 guest, mode='bind'):
3277             return ctype.ConsoleSerial(host=hostname, port=port)
3278         raise exception.ConsoleTypeUnavailable(console_type='serial')
3279 
3280     @staticmethod
3281     def _create_ephemeral(target, ephemeral_size,
3282                           fs_label, os_type, is_block_dev=False,
3283                           context=None, specified_fs=None,
3284                           vm_mode=None):
3285         if not is_block_dev:
3286             if (CONF.libvirt.virt_type == "parallels" and
3287                     vm_mode == fields.VMMode.EXE):
3288 
3289                 libvirt_utils.create_ploop_image('expanded', target,
3290                                                  '%dG' % ephemeral_size,
3291                                                  specified_fs)
3292                 return
3293             libvirt_utils.create_image('raw', target, '%dG' % ephemeral_size)
3294 
3295         # Run as root only for block devices.
3296         disk_api.mkfs(os_type, fs_label, target, run_as_root=is_block_dev,
3297                       specified_fs=specified_fs)
3298 
3299     @staticmethod
3300     def _create_swap(target, swap_mb, context=None):
3301         """Create a swap file of specified size."""
3302         libvirt_utils.create_image('raw', target, '%dM' % swap_mb)
3303         nova.privsep.fs.unprivileged_mkfs('swap', target)
3304 
3305     @staticmethod
3306     def _get_console_log_path(instance):
3307         return os.path.join(libvirt_utils.get_instance_path(instance),
3308                             'console.log')
3309 
3310     def _ensure_console_log_for_instance(self, instance):
3311         # NOTE(mdbooth): Although libvirt will create this file for us
3312         # automatically when it starts, it will initially create it with
3313         # root ownership and then chown it depending on the configuration of
3314         # the domain it is launching. Quobyte CI explicitly disables the
3315         # chown by setting dynamic_ownership=0 in libvirt's config.
3316         # Consequently when the domain starts it is unable to write to its
3317         # console.log. See bug https://bugs.launchpad.net/nova/+bug/1597644
3318         #
3319         # To work around this, we create the file manually before starting
3320         # the domain so it has the same ownership as Nova. This works
3321         # for Quobyte CI because it is also configured to run qemu as the same
3322         # user as the Nova service. Installations which don't set
3323         # dynamic_ownership=0 are not affected because libvirt will always
3324         # correctly configure permissions regardless of initial ownership.
3325         #
3326         # Setting dynamic_ownership=0 is dubious and potentially broken in
3327         # more ways than console.log (see comment #22 on the above bug), so
3328         # Future Maintainer who finds this code problematic should check to see
3329         # if we still support it.
3330         console_file = self._get_console_log_path(instance)
3331         LOG.debug('Ensure instance console log exists: %s', console_file,
3332                   instance=instance)
3333         try:
3334             libvirt_utils.file_open(console_file, 'a').close()
3335         # NOTE(sfinucan): We can safely ignore permission issues here and
3336         # assume that it is libvirt that has taken ownership of this file.
3337         except IOError as ex:
3338             if ex.errno != errno.EACCES:
3339                 raise
3340             LOG.debug('Console file already exists: %s.', console_file)
3341 
3342     @staticmethod
3343     def _get_disk_config_image_type():
3344         # TODO(mikal): there is a bug here if images_type has
3345         # changed since creation of the instance, but I am pretty
3346         # sure that this bug already exists.
3347         return 'rbd' if CONF.libvirt.images_type == 'rbd' else 'raw'
3348 
3349     @staticmethod
3350     def _is_booted_from_volume(block_device_info):
3351         """Determines whether the VM is booting from volume
3352 
3353         Determines whether the block device info indicates that the VM
3354         is booting from a volume.
3355         """
3356         block_device_mapping = driver.block_device_info_get_mapping(
3357             block_device_info)
3358         return bool(block_device.get_root_bdm(block_device_mapping))
3359 
3360     def _inject_data(self, disk, instance, injection_info):
3361         """Injects data in a disk image
3362 
3363         Helper used for injecting data in a disk image file system.
3364 
3365         :param disk: The disk we're injecting into (an Image object)
3366         :param instance: The instance we're injecting into
3367         :param injection_info: Injection info
3368         """
3369         # Handles the partition need to be used.
3370         LOG.debug('Checking root disk injection %s',
3371                   str(injection_info), instance=instance)
3372         target_partition = None
3373         if not instance.kernel_id:
3374             target_partition = CONF.libvirt.inject_partition
3375             if target_partition == 0:
3376                 target_partition = None
3377         if CONF.libvirt.virt_type == 'lxc':
3378             target_partition = None
3379 
3380         # Handles the key injection.
3381         if CONF.libvirt.inject_key and instance.get('key_data'):
3382             key = str(instance.key_data)
3383         else:
3384             key = None
3385 
3386         # Handles the admin password injection.
3387         if not CONF.libvirt.inject_password:
3388             admin_pass = None
3389         else:
3390             admin_pass = injection_info.admin_pass
3391 
3392         # Handles the network injection.
3393         net = netutils.get_injected_network_template(
3394             injection_info.network_info,
3395             libvirt_virt_type=CONF.libvirt.virt_type)
3396 
3397         # Handles the metadata injection
3398         metadata = instance.get('metadata')
3399 
3400         if any((key, net, metadata, admin_pass, injection_info.files)):
3401             LOG.debug('Injecting %s', str(injection_info),
3402                       instance=instance)
3403             img_id = instance.image_ref
3404             try:
3405                 disk_api.inject_data(disk.get_model(self._conn),
3406                                      key, net, metadata, admin_pass,
3407                                      injection_info.files,
3408                                      partition=target_partition,
3409                                      mandatory=('files',))
3410             except Exception as e:
3411                 with excutils.save_and_reraise_exception():
3412                     LOG.error('Error injecting data into image '
3413                               '%(img_id)s (%(e)s)',
3414                               {'img_id': img_id, 'e': e},
3415                               instance=instance)
3416 
3417     # NOTE(sileht): many callers of this method assume that this
3418     # method doesn't fail if an image already exists but instead
3419     # think that it will be reused (ie: (live)-migration/resize)
3420     def _create_image(self, context, instance,
3421                       disk_mapping, injection_info=None, suffix='',
3422                       disk_images=None, block_device_info=None,
3423                       fallback_from_host=None,
3424                       ignore_bdi_for_swap=False):
3425         booted_from_volume = self._is_booted_from_volume(block_device_info)
3426 
3427         def image(fname, image_type=CONF.libvirt.images_type):
3428             return self.image_backend.by_name(instance,
3429                                               fname + suffix, image_type)
3430 
3431         def raw(fname):
3432             return image(fname, image_type='raw')
3433 
3434         # ensure directories exist and are writable
3435         fileutils.ensure_tree(libvirt_utils.get_instance_path(instance))
3436 
3437         LOG.info('Creating image', instance=instance)
3438 
3439         inst_type = instance.get_flavor()
3440         swap_mb = 0
3441         if 'disk.swap' in disk_mapping:
3442             mapping = disk_mapping['disk.swap']
3443 
3444             if ignore_bdi_for_swap:
3445                 # This is a workaround to support legacy swap resizing,
3446                 # which does not touch swap size specified in bdm,
3447                 # but works with flavor specified size only.
3448                 # In this case we follow the legacy logic and ignore block
3449                 # device info completely.
3450                 # NOTE(ft): This workaround must be removed when a correct
3451                 # implementation of resize operation changing sizes in bdms is
3452                 # developed. Also at that stage we probably may get rid of
3453                 # the direct usage of flavor swap size here,
3454                 # leaving the work with bdm only.
3455                 swap_mb = inst_type['swap']
3456             else:
3457                 swap = driver.block_device_info_get_swap(block_device_info)
3458                 if driver.swap_is_usable(swap):
3459                     swap_mb = swap['swap_size']
3460                 elif (inst_type['swap'] > 0 and
3461                       not block_device.volume_in_mapping(
3462                         mapping['dev'], block_device_info)):
3463                     swap_mb = inst_type['swap']
3464 
3465             if swap_mb > 0:
3466                 if (CONF.libvirt.virt_type == "parallels" and
3467                         instance.vm_mode == fields.VMMode.EXE):
3468                     msg = _("Swap disk is not supported "
3469                             "for Virtuozzo container")
3470                     raise exception.Invalid(msg)
3471 
3472         if not disk_images:
3473             disk_images = {'image_id': instance.image_ref,
3474                            'kernel_id': instance.kernel_id,
3475                            'ramdisk_id': instance.ramdisk_id}
3476 
3477         if disk_images['kernel_id']:
3478             fname = imagecache.get_cache_fname(disk_images['kernel_id'])
3479             raw('kernel').cache(fetch_func=libvirt_utils.fetch_raw_image,
3480                                 context=context,
3481                                 filename=fname,
3482                                 image_id=disk_images['kernel_id'])
3483             if disk_images['ramdisk_id']:
3484                 fname = imagecache.get_cache_fname(disk_images['ramdisk_id'])
3485                 raw('ramdisk').cache(fetch_func=libvirt_utils.fetch_raw_image,
3486                                      context=context,
3487                                      filename=fname,
3488                                      image_id=disk_images['ramdisk_id'])
3489 
3490         if CONF.libvirt.virt_type == 'uml':
3491             # PONDERING(mikal): can I assume that root is UID zero in every
3492             # OS? Probably not.
3493             uid = pwd.getpwnam('root').pw_uid
3494             nova.privsep.path.chown(image('disk').path, uid=uid)
3495 
3496         self._create_and_inject_local_root(context, instance,
3497                                            booted_from_volume, suffix,
3498                                            disk_images, injection_info,
3499                                            fallback_from_host)
3500 
3501         # Lookup the filesystem type if required
3502         os_type_with_default = nova.privsep.fs.get_fs_type_for_os_type(
3503             instance.os_type)
3504         # Generate a file extension based on the file system
3505         # type and the mkfs commands configured if any
3506         file_extension = nova.privsep.fs.get_file_extension_for_os_type(
3507             os_type_with_default, CONF.default_ephemeral_format)
3508 
3509         vm_mode = fields.VMMode.get_from_instance(instance)
3510         ephemeral_gb = instance.flavor.ephemeral_gb
3511         if 'disk.local' in disk_mapping:
3512             disk_image = image('disk.local')
3513             fn = functools.partial(self._create_ephemeral,
3514                                    fs_label='ephemeral0',
3515                                    os_type=instance.os_type,
3516                                    is_block_dev=disk_image.is_block_dev,
3517                                    vm_mode=vm_mode)
3518             fname = "ephemeral_%s_%s" % (ephemeral_gb, file_extension)
3519             size = ephemeral_gb * units.Gi
3520             disk_image.cache(fetch_func=fn,
3521                              context=context,
3522                              filename=fname,
3523                              size=size,
3524                              ephemeral_size=ephemeral_gb)
3525 
3526         for idx, eph in enumerate(driver.block_device_info_get_ephemerals(
3527                 block_device_info)):
3528             disk_image = image(blockinfo.get_eph_disk(idx))
3529 
3530             specified_fs = eph.get('guest_format')
3531             if specified_fs and not self.is_supported_fs_format(specified_fs):
3532                 msg = _("%s format is not supported") % specified_fs
3533                 raise exception.InvalidBDMFormat(details=msg)
3534 
3535             fn = functools.partial(self._create_ephemeral,
3536                                    fs_label='ephemeral%d' % idx,
3537                                    os_type=instance.os_type,
3538                                    is_block_dev=disk_image.is_block_dev,
3539                                    vm_mode=vm_mode)
3540             size = eph['size'] * units.Gi
3541             fname = "ephemeral_%s_%s" % (eph['size'], file_extension)
3542             disk_image.cache(fetch_func=fn,
3543                              context=context,
3544                              filename=fname,
3545                              size=size,
3546                              ephemeral_size=eph['size'],
3547                              specified_fs=specified_fs)
3548 
3549         if swap_mb > 0:
3550             size = swap_mb * units.Mi
3551             image('disk.swap').cache(fetch_func=self._create_swap,
3552                                      context=context,
3553                                      filename="swap_%s" % swap_mb,
3554                                      size=size,
3555                                      swap_mb=swap_mb)
3556 
3557     def _create_and_inject_local_root(self, context, instance,
3558                                       booted_from_volume, suffix, disk_images,
3559                                       injection_info, fallback_from_host):
3560         # File injection only if needed
3561         need_inject = (not configdrive.required_by(instance) and
3562                        injection_info is not None and
3563                        CONF.libvirt.inject_partition != -2)
3564 
3565         # NOTE(ndipanov): Even if disk_mapping was passed in, which
3566         # currently happens only on rescue - we still don't want to
3567         # create a base image.
3568         if not booted_from_volume:
3569             root_fname = imagecache.get_cache_fname(disk_images['image_id'])
3570             size = instance.flavor.root_gb * units.Gi
3571 
3572             if size == 0 or suffix == '.rescue':
3573                 size = None
3574 
3575             backend = self.image_backend.by_name(instance, 'disk' + suffix,
3576                                                  CONF.libvirt.images_type)
3577             if instance.task_state == task_states.RESIZE_FINISH:
3578                 backend.create_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME)
3579             if backend.SUPPORTS_CLONE:
3580                 def clone_fallback_to_fetch(*args, **kwargs):
3581                     try:
3582                         backend.clone(context, disk_images['image_id'])
3583                     except exception.ImageUnacceptable:
3584                         libvirt_utils.fetch_image(*args, **kwargs)
3585                 fetch_func = clone_fallback_to_fetch
3586             else:
3587                 fetch_func = libvirt_utils.fetch_image
3588             self._try_fetch_image_cache(backend, fetch_func, context,
3589                                         root_fname, disk_images['image_id'],
3590                                         instance, size, fallback_from_host)
3591 
3592             if need_inject:
3593                 self._inject_data(backend, instance, injection_info)
3594 
3595         elif need_inject:
3596             LOG.warning('File injection into a boot from volume '
3597                         'instance is not supported', instance=instance)
3598 
3599     def _create_configdrive(self, context, instance, injection_info,
3600                             rescue=False):
3601         # As this method being called right after the definition of a
3602         # domain, but before its actual launch, device metadata will be built
3603         # and saved in the instance for it to be used by the config drive and
3604         # the metadata service.
3605         instance.device_metadata = self._build_device_metadata(context,
3606                                                                instance)
3607         if configdrive.required_by(instance):
3608             LOG.info('Using config drive', instance=instance)
3609 
3610             name = 'disk.config'
3611             if rescue:
3612                 name += '.rescue'
3613 
3614             config_disk = self.image_backend.by_name(
3615                 instance, name, self._get_disk_config_image_type())
3616 
3617             # Don't overwrite an existing config drive
3618             if not config_disk.exists():
3619                 extra_md = {}
3620                 if injection_info.admin_pass:
3621                     extra_md['admin_pass'] = injection_info.admin_pass
3622 
3623                 inst_md = instance_metadata.InstanceMetadata(
3624                     instance, content=injection_info.files, extra_md=extra_md,
3625                     network_info=injection_info.network_info,
3626                     request_context=context)
3627 
3628                 cdb = configdrive.ConfigDriveBuilder(instance_md=inst_md)
3629                 with cdb:
3630                     # NOTE(mdbooth): We're hardcoding here the path of the
3631                     # config disk when using the flat backend. This isn't
3632                     # good, but it's required because we need a local path we
3633                     # know we can write to in case we're subsequently
3634                     # importing into rbd. This will be cleaned up when we
3635                     # replace this with a call to create_from_func, but that
3636                     # can't happen until we've updated the backends and we
3637                     # teach them not to cache config disks. This isn't
3638                     # possible while we're still using cache() under the hood.
3639                     config_disk_local_path = os.path.join(
3640                         libvirt_utils.get_instance_path(instance), name)
3641                     LOG.info('Creating config drive at %(path)s',
3642                              {'path': config_disk_local_path},
3643                              instance=instance)
3644 
3645                     try:
3646                         cdb.make_drive(config_disk_local_path)
3647                     except processutils.ProcessExecutionError as e:
3648                         with excutils.save_and_reraise_exception():
3649                             LOG.error('Creating config drive failed with '
3650                                       'error: %s', e, instance=instance)
3651 
3652                 try:
3653                     config_disk.import_file(
3654                         instance, config_disk_local_path, name)
3655                 finally:
3656                     # NOTE(mikal): if the config drive was imported into RBD,
3657                     # then we no longer need the local copy
3658                     if CONF.libvirt.images_type == 'rbd':
3659                         LOG.info('Deleting local config drive %(path)s '
3660                                  'because it was imported into RBD.',
3661                                  {'path': config_disk_local_path},
3662                                  instance=instance)
3663                         os.unlink(config_disk_local_path)
3664 
3665     def _prepare_pci_devices_for_use(self, pci_devices):
3666         # kvm , qemu support managed mode
3667         # In managed mode, the configured device will be automatically
3668         # detached from the host OS drivers when the guest is started,
3669         # and then re-attached when the guest shuts down.
3670         if CONF.libvirt.virt_type != 'xen':
3671             # we do manual detach only for xen
3672             return
3673         try:
3674             for dev in pci_devices:
3675                 libvirt_dev_addr = dev['hypervisor_name']
3676                 libvirt_dev = \
3677                         self._host.device_lookup_by_name(libvirt_dev_addr)
3678                 # Note(yjiang5) Spelling for 'dettach' is correct, see
3679                 # http://libvirt.org/html/libvirt-libvirt.html.
3680                 libvirt_dev.dettach()
3681 
3682             # Note(yjiang5): A reset of one PCI device may impact other
3683             # devices on the same bus, thus we need two separated loops
3684             # to detach and then reset it.
3685             for dev in pci_devices:
3686                 libvirt_dev_addr = dev['hypervisor_name']
3687                 libvirt_dev = \
3688                         self._host.device_lookup_by_name(libvirt_dev_addr)
3689                 libvirt_dev.reset()
3690 
3691         except libvirt.libvirtError as exc:
3692             raise exception.PciDevicePrepareFailed(id=dev['id'],
3693                                                    instance_uuid=
3694                                                    dev['instance_uuid'],
3695                                                    reason=six.text_type(exc))
3696 
3697     def _detach_pci_devices(self, guest, pci_devs):
3698         try:
3699             for dev in pci_devs:
3700                 guest.detach_device(self._get_guest_pci_device(dev), live=True)
3701                 # after detachDeviceFlags returned, we should check the dom to
3702                 # ensure the detaching is finished
3703                 xml = guest.get_xml_desc()
3704                 xml_doc = etree.fromstring(xml)
3705                 guest_config = vconfig.LibvirtConfigGuest()
3706                 guest_config.parse_dom(xml_doc)
3707 
3708                 for hdev in [d for d in guest_config.devices
3709                     if isinstance(d, vconfig.LibvirtConfigGuestHostdevPCI)]:
3710                     hdbsf = [hdev.domain, hdev.bus, hdev.slot, hdev.function]
3711                     dbsf = pci_utils.parse_address(dev.address)
3712                     if [int(x, 16) for x in hdbsf] ==\
3713                             [int(x, 16) for x in dbsf]:
3714                         raise exception.PciDeviceDetachFailed(reason=
3715                                                               "timeout",
3716                                                               dev=dev)
3717 
3718         except libvirt.libvirtError as ex:
3719             error_code = ex.get_error_code()
3720             if error_code == libvirt.VIR_ERR_NO_DOMAIN:
3721                 LOG.warning("Instance disappeared while detaching "
3722                             "a PCI device from it.")
3723             else:
3724                 raise
3725 
3726     def _attach_pci_devices(self, guest, pci_devs):
3727         try:
3728             for dev in pci_devs:
3729                 guest.attach_device(self._get_guest_pci_device(dev))
3730 
3731         except libvirt.libvirtError:
3732             LOG.error('Attaching PCI devices %(dev)s to %(dom)s failed.',
3733                       {'dev': pci_devs, 'dom': guest.id})
3734             raise
3735 
3736     @staticmethod
3737     def _has_direct_passthrough_port(network_info):
3738         for vif in network_info:
3739             if (vif['vnic_type'] in
3740                 network_model.VNIC_TYPES_DIRECT_PASSTHROUGH):
3741                 return True
3742         return False
3743 
3744     def _attach_direct_passthrough_ports(
3745         self, context, instance, guest, network_info=None):
3746         if network_info is None:
3747             network_info = instance.info_cache.network_info
3748         if network_info is None:
3749             return
3750 
3751         if self._has_direct_passthrough_port(network_info):
3752             for vif in network_info:
3753                 if (vif['vnic_type'] in
3754                     network_model.VNIC_TYPES_DIRECT_PASSTHROUGH):
3755                     cfg = self.vif_driver.get_config(instance,
3756                                                      vif,
3757                                                      instance.image_meta,
3758                                                      instance.flavor,
3759                                                      CONF.libvirt.virt_type,
3760                                                      self._host)
3761                     LOG.debug('Attaching direct passthrough port %(port)s '
3762                               'to %(dom)s', {'port': vif, 'dom': guest.id},
3763                               instance=instance)
3764                     guest.attach_device(cfg)
3765 
3766     def _detach_direct_passthrough_ports(self, context, instance, guest):
3767         network_info = instance.info_cache.network_info
3768         if network_info is None:
3769             return
3770 
3771         if self._has_direct_passthrough_port(network_info):
3772             # In case of VNIC_TYPES_DIRECT_PASSTHROUGH ports we create
3773             # pci request per direct passthrough port. Therefore we can trust
3774             # that pci_slot value in the vif is correct.
3775             direct_passthrough_pci_addresses = [
3776                 vif['profile']['pci_slot']
3777                 for vif in network_info
3778                 if (vif['vnic_type'] in
3779                     network_model.VNIC_TYPES_DIRECT_PASSTHROUGH and
3780                     vif['profile'].get('pci_slot') is not None)
3781             ]
3782 
3783             # use detach_pci_devices to avoid failure in case of
3784             # multiple guest direct passthrough ports with the same MAC
3785             # (protection use-case, ports are on different physical
3786             # interfaces)
3787             pci_devs = pci_manager.get_instance_pci_devs(instance, 'all')
3788             direct_passthrough_pci_addresses = (
3789                 [pci_dev for pci_dev in pci_devs
3790                  if pci_dev.address in direct_passthrough_pci_addresses])
3791             self._detach_pci_devices(guest, direct_passthrough_pci_addresses)
3792 
3793     def _set_host_enabled(self, enabled,
3794                           disable_reason=DISABLE_REASON_UNDEFINED):
3795         """Enables / Disables the compute service on this host.
3796 
3797            This doesn't override non-automatic disablement with an automatic
3798            setting; thereby permitting operators to keep otherwise
3799            healthy hosts out of rotation.
3800         """
3801 
3802         status_name = {True: 'disabled',
3803                        False: 'enabled'}
3804 
3805         disable_service = not enabled
3806 
3807         ctx = nova_context.get_admin_context()
3808         try:
3809             service = objects.Service.get_by_compute_host(ctx, CONF.host)
3810 
3811             if service.disabled != disable_service:
3812                 # Note(jang): this is a quick fix to stop operator-
3813                 # disabled compute hosts from re-enabling themselves
3814                 # automatically. We prefix any automatic reason code
3815                 # with a fixed string. We only re-enable a host
3816                 # automatically if we find that string in place.
3817                 # This should probably be replaced with a separate flag.
3818                 if not service.disabled or (
3819                         service.disabled_reason and
3820                         service.disabled_reason.startswith(DISABLE_PREFIX)):
3821                     service.disabled = disable_service
3822                     service.disabled_reason = (
3823                        DISABLE_PREFIX + disable_reason
3824                        if disable_service and disable_reason else
3825                            DISABLE_REASON_UNDEFINED)
3826                     service.save()
3827                     LOG.debug('Updating compute service status to %s',
3828                               status_name[disable_service])
3829                 else:
3830                     LOG.debug('Not overriding manual compute service '
3831                               'status with: %s',
3832                               status_name[disable_service])
3833         except exception.ComputeHostNotFound:
3834             LOG.warning('Cannot update service status on host "%s" '
3835                         'since it is not registered.', CONF.host)
3836         except Exception:
3837             LOG.warning('Cannot update service status on host "%s" '
3838                         'due to an unexpected exception.', CONF.host,
3839                         exc_info=True)
3840 
3841         if enabled:
3842             mount.get_manager().host_up(self._host)
3843         else:
3844             mount.get_manager().host_down()
3845 
3846     def _get_guest_cpu_model_config(self):
3847         mode = CONF.libvirt.cpu_mode
3848         model = CONF.libvirt.cpu_model
3849         extra_flags = set([flag.lower() for flag in
3850             CONF.libvirt.cpu_model_extra_flags])
3851 
3852         if (CONF.libvirt.virt_type == "kvm" or
3853             CONF.libvirt.virt_type == "qemu"):
3854             if mode is None:
3855                 caps = self._host.get_capabilities()
3856                 # AArch64 lacks 'host-model' support because neither libvirt
3857                 # nor QEMU are able to tell what the host CPU model exactly is.
3858                 # And there is no CPU description code for ARM(64) at this
3859                 # point.
3860 
3861                 # Also worth noting: 'host-passthrough' mode will completely
3862                 # break live migration, *unless* all the Compute nodes (running
3863                 # libvirtd) have *identical* CPUs.
3864                 if caps.host.cpu.arch == fields.Architecture.AARCH64:
3865                     mode = "host-passthrough"
3866                     LOG.info('CPU mode "host-passthrough" was chosen. Live '
3867                              'migration can break unless all compute nodes '
3868                              'have identical cpus. AArch64 does not support '
3869                              'other modes.')
3870                 else:
3871                     mode = "host-model"
3872             if mode == "none":
3873                 return vconfig.LibvirtConfigGuestCPU()
3874         else:
3875             if mode is None or mode == "none":
3876                 return None
3877 
3878         if ((CONF.libvirt.virt_type != "kvm" and
3879              CONF.libvirt.virt_type != "qemu")):
3880             msg = _("Config requested an explicit CPU model, but "
3881                     "the current libvirt hypervisor '%s' does not "
3882                     "support selecting CPU models") % CONF.libvirt.virt_type
3883             raise exception.Invalid(msg)
3884 
3885         if mode == "custom" and model is None:
3886             msg = _("Config requested a custom CPU model, but no "
3887                     "model name was provided")
3888             raise exception.Invalid(msg)
3889         elif mode != "custom" and model is not None:
3890             msg = _("A CPU model name should not be set when a "
3891                     "host CPU model is requested")
3892             raise exception.Invalid(msg)
3893 
3894         LOG.debug("CPU mode '%(mode)s' model '%(model)s' was chosen, "
3895                   "with extra flags: '%(extra_flags)s'",
3896                   {'mode': mode,
3897                    'model': (model or ""),
3898                    'extra_flags': (extra_flags or "")})
3899 
3900         cpu = vconfig.LibvirtConfigGuestCPU()
3901         cpu.mode = mode
3902         cpu.model = model
3903 
3904         # NOTE (kchamart): Currently there's no existing way to ask if a
3905         # given CPU model + CPU flags combination is supported by KVM &
3906         # a specific QEMU binary.  However, libvirt runs the 'CPUID'
3907         # command upfront -- before even a Nova instance (a QEMU
3908         # process) is launched -- to construct CPU models and check
3909         # their validity; so we are good there.  In the long-term,
3910         # upstream libvirt intends to add an additional new API that can
3911         # do fine-grained validation of a certain CPU model + CPU flags
3912         # against a specific QEMU binary (the libvirt RFE bug for that:
3913         # https://bugzilla.redhat.com/show_bug.cgi?id=1559832).
3914         for flag in extra_flags:
3915             cpu.add_feature(vconfig.LibvirtConfigGuestCPUFeature(flag))
3916 
3917         return cpu
3918 
3919     def _get_guest_cpu_config(self, flavor, image_meta,
3920                               guest_cpu_numa_config, instance_numa_topology):
3921         cpu = self._get_guest_cpu_model_config()
3922 
3923         if cpu is None:
3924             return None
3925 
3926         topology = hardware.get_best_cpu_topology(
3927                 flavor, image_meta, numa_topology=instance_numa_topology)
3928 
3929         cpu.sockets = topology.sockets
3930         cpu.cores = topology.cores
3931         cpu.threads = topology.threads
3932         cpu.numa = guest_cpu_numa_config
3933 
3934         return cpu
3935 
3936     def _get_guest_disk_config(self, instance, name, disk_mapping, inst_type,
3937                                image_type=None):
3938         disk_unit = None
3939         disk = self.image_backend.by_name(instance, name, image_type)
3940         if (name == 'disk.config' and image_type == 'rbd' and
3941                 not disk.exists()):
3942             # This is likely an older config drive that has not been migrated
3943             # to rbd yet. Try to fall back on 'flat' image type.
3944             # TODO(melwitt): Add online migration of some sort so we can
3945             # remove this fall back once we know all config drives are in rbd.
3946             # NOTE(vladikr): make sure that the flat image exist, otherwise
3947             # the image will be created after the domain definition.
3948             flat_disk = self.image_backend.by_name(instance, name, 'flat')
3949             if flat_disk.exists():
3950                 disk = flat_disk
3951                 LOG.debug('Config drive not found in RBD, falling back to the '
3952                           'instance directory', instance=instance)
3953         disk_info = disk_mapping[name]
3954         if 'unit' in disk_mapping and disk_info['bus'] == 'scsi':
3955             disk_unit = disk_mapping['unit']
3956             disk_mapping['unit'] += 1  # Increments for the next disk added
3957         conf = disk.libvirt_info(disk_info, self.disk_cachemode,
3958                                  inst_type['extra_specs'],
3959                                  self._host.get_version(),
3960                                  disk_unit=disk_unit)
3961         return conf
3962 
3963     def _get_guest_fs_config(self, instance, name, image_type=None):
3964         disk = self.image_backend.by_name(instance, name, image_type)
3965         return disk.libvirt_fs_info("/", "ploop")
3966 
3967     def _get_guest_storage_config(self, context, instance, image_meta,
3968                                   disk_info,
3969                                   rescue, block_device_info,
3970                                   inst_type, os_type):
3971         devices = []
3972         disk_mapping = disk_info['mapping']
3973 
3974         block_device_mapping = driver.block_device_info_get_mapping(
3975             block_device_info)
3976         mount_rootfs = CONF.libvirt.virt_type == "lxc"
3977         scsi_controller = self._get_scsi_controller(image_meta)
3978 
3979         if scsi_controller and scsi_controller.model == 'virtio-scsi':
3980             # The virtio-scsi can handle up to 256 devices but the
3981             # optional element "address" must be defined to describe
3982             # where the device is placed on the controller (see:
3983             # LibvirtConfigGuestDeviceAddressDrive).
3984             #
3985             # Note about why it's added in disk_mapping: It's not
3986             # possible to pass an 'int' by reference in Python, so we
3987             # use disk_mapping as container to keep reference of the
3988             # unit added and be able to increment it for each disk
3989             # added.
3990             #
3991             # NOTE(jaypipes,melwitt): If this is a boot-from-volume instance,
3992             # we need to start the disk mapping unit at 1 since we set the
3993             # bootable volume's unit to 0 for the bootable volume.
3994             disk_mapping['unit'] = 0
3995             if self._is_booted_from_volume(block_device_info):
3996                 disk_mapping['unit'] = 1
3997 
3998         def _get_ephemeral_devices():
3999             eph_devices = []
4000             for idx, eph in enumerate(
4001                 driver.block_device_info_get_ephemerals(
4002                     block_device_info)):
4003                 diskeph = self._get_guest_disk_config(
4004                     instance,
4005                     blockinfo.get_eph_disk(idx),
4006                     disk_mapping, inst_type)
4007                 eph_devices.append(diskeph)
4008             return eph_devices
4009 
4010         if mount_rootfs:
4011             fs = vconfig.LibvirtConfigGuestFilesys()
4012             fs.source_type = "mount"
4013             fs.source_dir = os.path.join(
4014                 libvirt_utils.get_instance_path(instance), 'rootfs')
4015             devices.append(fs)
4016         elif (os_type == fields.VMMode.EXE and
4017               CONF.libvirt.virt_type == "parallels"):
4018             if rescue:
4019                 fsrescue = self._get_guest_fs_config(instance, "disk.rescue")
4020                 devices.append(fsrescue)
4021 
4022                 fsos = self._get_guest_fs_config(instance, "disk")
4023                 fsos.target_dir = "/mnt/rescue"
4024                 devices.append(fsos)
4025             else:
4026                 if 'disk' in disk_mapping:
4027                     fs = self._get_guest_fs_config(instance, "disk")
4028                     devices.append(fs)
4029                 devices = devices + _get_ephemeral_devices()
4030         else:
4031 
4032             if rescue:
4033                 diskrescue = self._get_guest_disk_config(instance,
4034                                                          'disk.rescue',
4035                                                          disk_mapping,
4036                                                          inst_type)
4037                 devices.append(diskrescue)
4038 
4039                 diskos = self._get_guest_disk_config(instance,
4040                                                      'disk',
4041                                                      disk_mapping,
4042                                                      inst_type)
4043                 devices.append(diskos)
4044             else:
4045                 if 'disk' in disk_mapping:
4046                     diskos = self._get_guest_disk_config(instance,
4047                                                          'disk',
4048                                                          disk_mapping,
4049                                                          inst_type)
4050                     devices.append(diskos)
4051 
4052                 if 'disk.local' in disk_mapping:
4053                     disklocal = self._get_guest_disk_config(instance,
4054                                                             'disk.local',
4055                                                             disk_mapping,
4056                                                             inst_type)
4057                     devices.append(disklocal)
4058                     instance.default_ephemeral_device = (
4059                         block_device.prepend_dev(disklocal.target_dev))
4060 
4061                 devices = devices + _get_ephemeral_devices()
4062 
4063                 if 'disk.swap' in disk_mapping:
4064                     diskswap = self._get_guest_disk_config(instance,
4065                                                            'disk.swap',
4066                                                            disk_mapping,
4067                                                            inst_type)
4068                     devices.append(diskswap)
4069                     instance.default_swap_device = (
4070                         block_device.prepend_dev(diskswap.target_dev))
4071 
4072             config_name = 'disk.config.rescue' if rescue else 'disk.config'
4073             if config_name in disk_mapping:
4074                 diskconfig = self._get_guest_disk_config(
4075                     instance, config_name, disk_mapping, inst_type,
4076                     self._get_disk_config_image_type())
4077                 devices.append(diskconfig)
4078 
4079         for vol in block_device.get_bdms_to_connect(block_device_mapping,
4080                                                    mount_rootfs):
4081             connection_info = vol['connection_info']
4082             vol_dev = block_device.prepend_dev(vol['mount_device'])
4083             info = disk_mapping[vol_dev]
4084             self._connect_volume(context, connection_info, instance)
4085             if scsi_controller and scsi_controller.model == 'virtio-scsi':
4086                 # Check if this is the bootable volume when in a
4087                 # boot-from-volume instance, and if so, ensure the unit
4088                 # attribute is 0.
4089                 if vol.get('boot_index') == 0:
4090                     info['unit'] = 0
4091                 else:
4092                     info['unit'] = disk_mapping['unit']
4093                     disk_mapping['unit'] += 1
4094             cfg = self._get_volume_config(connection_info, info)
4095             devices.append(cfg)
4096             vol['connection_info'] = connection_info
4097             vol.save()
4098 
4099         for d in devices:
4100             self._set_cache_mode(d)
4101 
4102         if scsi_controller:
4103             devices.append(scsi_controller)
4104 
4105         return devices
4106 
4107     @staticmethod
4108     def _get_scsi_controller(image_meta):
4109         """Return scsi controller or None based on image meta"""
4110         if image_meta.properties.get('hw_scsi_model'):
4111             hw_scsi_model = image_meta.properties.hw_scsi_model
4112             scsi_controller = vconfig.LibvirtConfigGuestController()
4113             scsi_controller.type = 'scsi'
4114             scsi_controller.model = hw_scsi_model
4115             scsi_controller.index = 0
4116             return scsi_controller
4117 
4118     def _get_host_sysinfo_serial_hardware(self):
4119         """Get a UUID from the host hardware
4120 
4121         Get a UUID for the host hardware reported by libvirt.
4122         This is typically from the SMBIOS data, unless it has
4123         been overridden in /etc/libvirt/libvirtd.conf
4124         """
4125         caps = self._host.get_capabilities()
4126         return caps.host.uuid
4127 
4128     def _get_host_sysinfo_serial_os(self):
4129         """Get a UUID from the host operating system
4130 
4131         Get a UUID for the host operating system. Modern Linux
4132         distros based on systemd provide a /etc/machine-id
4133         file containing a UUID. This is also provided inside
4134         systemd based containers and can be provided by other
4135         init systems too, since it is just a plain text file.
4136         """
4137         if not os.path.exists("/etc/machine-id"):
4138             msg = _("Unable to get host UUID: /etc/machine-id does not exist")
4139             raise exception.InternalError(msg)
4140 
4141         with open("/etc/machine-id") as f:
4142             # We want to have '-' in the right place
4143             # so we parse & reformat the value
4144             lines = f.read().split()
4145             if not lines:
4146                 msg = _("Unable to get host UUID: /etc/machine-id is empty")
4147                 raise exception.InternalError(msg)
4148 
4149             return str(uuid.UUID(lines[0]))
4150 
4151     def _get_host_sysinfo_serial_auto(self):
4152         if os.path.exists("/etc/machine-id"):
4153             return self._get_host_sysinfo_serial_os()
4154         else:
4155             return self._get_host_sysinfo_serial_hardware()
4156 
4157     def _get_guest_config_sysinfo(self, instance):
4158         sysinfo = vconfig.LibvirtConfigGuestSysinfo()
4159 
4160         sysinfo.system_manufacturer = version.vendor_string()
4161         sysinfo.system_product = version.product_string()
4162         sysinfo.system_version = version.version_string_with_package()
4163 
4164         sysinfo.system_serial = self._sysinfo_serial_func()
4165         sysinfo.system_uuid = instance.uuid
4166 
4167         sysinfo.system_family = "Virtual Machine"
4168 
4169         return sysinfo
4170 
4171     def _get_guest_pci_device(self, pci_device):
4172 
4173         dbsf = pci_utils.parse_address(pci_device.address)
4174         dev = vconfig.LibvirtConfigGuestHostdevPCI()
4175         dev.domain, dev.bus, dev.slot, dev.function = dbsf
4176 
4177         # only kvm support managed mode
4178         if CONF.libvirt.virt_type in ('xen', 'parallels',):
4179             dev.managed = 'no'
4180         if CONF.libvirt.virt_type in ('kvm', 'qemu'):
4181             dev.managed = 'yes'
4182 
4183         return dev
4184 
4185     def _get_guest_config_meta(self, instance):
4186         """Get metadata config for guest."""
4187 
4188         meta = vconfig.LibvirtConfigGuestMetaNovaInstance()
4189         meta.package = version.version_string_with_package()
4190         meta.name = instance.display_name
4191         meta.creationTime = time.time()
4192 
4193         if instance.image_ref not in ("", None):
4194             meta.roottype = "image"
4195             meta.rootid = instance.image_ref
4196 
4197         system_meta = instance.system_metadata
4198         ometa = vconfig.LibvirtConfigGuestMetaNovaOwner()
4199         ometa.userid = instance.user_id
4200         ometa.username = system_meta.get('owner_user_name', 'N/A')
4201         ometa.projectid = instance.project_id
4202         ometa.projectname = system_meta.get('owner_project_name', 'N/A')
4203         meta.owner = ometa
4204 
4205         fmeta = vconfig.LibvirtConfigGuestMetaNovaFlavor()
4206         flavor = instance.flavor
4207         fmeta.name = flavor.name
4208         fmeta.memory = flavor.memory_mb
4209         fmeta.vcpus = flavor.vcpus
4210         fmeta.ephemeral = flavor.ephemeral_gb
4211         fmeta.disk = flavor.root_gb
4212         fmeta.swap = flavor.swap
4213 
4214         meta.flavor = fmeta
4215 
4216         return meta
4217 
4218     def _machine_type_mappings(self):
4219         mappings = {}
4220         for mapping in CONF.libvirt.hw_machine_type:
4221             host_arch, _, machine_type = mapping.partition('=')
4222             mappings[host_arch] = machine_type
4223         return mappings
4224 
4225     def _get_machine_type(self, image_meta, caps):
4226         # The guest machine type can be set as an image metadata
4227         # property, or otherwise based on architecture-specific
4228         # defaults.
4229         mach_type = None
4230 
4231         if image_meta.properties.get('hw_machine_type') is not None:
4232             mach_type = image_meta.properties.hw_machine_type
4233         else:
4234             # NOTE(kchamart): For ARMv7 and AArch64, use the 'virt'
4235             # board as the default machine type.  It is the recommended
4236             # board, which is designed to be used with virtual machines.
4237             # The 'virt' board is more flexible, supports PCI, 'virtio',
4238             # has decent RAM limits, etc.
4239             if caps.host.cpu.arch in (fields.Architecture.ARMV7,
4240                                       fields.Architecture.AARCH64):
4241                 mach_type = "virt"
4242 
4243             if caps.host.cpu.arch in (fields.Architecture.S390,
4244                                       fields.Architecture.S390X):
4245                 mach_type = 's390-ccw-virtio'
4246 
4247             # If set in the config, use that as the default.
4248             if CONF.libvirt.hw_machine_type:
4249                 mappings = self._machine_type_mappings()
4250                 mach_type = mappings.get(caps.host.cpu.arch)
4251 
4252         return mach_type
4253 
4254     @staticmethod
4255     def _create_idmaps(klass, map_strings):
4256         idmaps = []
4257         if len(map_strings) > 5:
4258             map_strings = map_strings[0:5]
4259             LOG.warning("Too many id maps, only included first five.")
4260         for map_string in map_strings:
4261             try:
4262                 idmap = klass()
4263                 values = [int(i) for i in map_string.split(":")]
4264                 idmap.start = values[0]
4265                 idmap.target = values[1]
4266                 idmap.count = values[2]
4267                 idmaps.append(idmap)
4268             except (ValueError, IndexError):
4269                 LOG.warning("Invalid value for id mapping %s", map_string)
4270         return idmaps
4271 
4272     def _get_guest_idmaps(self):
4273         id_maps = []
4274         if CONF.libvirt.virt_type == 'lxc' and CONF.libvirt.uid_maps:
4275             uid_maps = self._create_idmaps(vconfig.LibvirtConfigGuestUIDMap,
4276                                            CONF.libvirt.uid_maps)
4277             id_maps.extend(uid_maps)
4278         if CONF.libvirt.virt_type == 'lxc' and CONF.libvirt.gid_maps:
4279             gid_maps = self._create_idmaps(vconfig.LibvirtConfigGuestGIDMap,
4280                                            CONF.libvirt.gid_maps)
4281             id_maps.extend(gid_maps)
4282         return id_maps
4283 
4284     def _update_guest_cputune(self, guest, flavor, virt_type):
4285         is_able = self._host.is_cpu_control_policy_capable()
4286 
4287         cputuning = ['shares', 'period', 'quota']
4288         wants_cputune = any([k for k in cputuning
4289             if "quota:cpu_" + k in flavor.extra_specs.keys()])
4290 
4291         if wants_cputune and not is_able:
4292             raise exception.UnsupportedHostCPUControlPolicy()
4293 
4294         if not is_able or virt_type not in ('lxc', 'kvm', 'qemu'):
4295             return
4296 
4297         if guest.cputune is None:
4298             guest.cputune = vconfig.LibvirtConfigGuestCPUTune()
4299             # Setting the default cpu.shares value to be a value
4300             # dependent on the number of vcpus
4301         guest.cputune.shares = 1024 * guest.vcpus
4302 
4303         for name in cputuning:
4304             key = "quota:cpu_" + name
4305             if key in flavor.extra_specs:
4306                 setattr(guest.cputune, name,
4307                         int(flavor.extra_specs[key]))
4308 
4309     def _get_cpu_numa_config_from_instance(self, instance_numa_topology,
4310                                            wants_hugepages):
4311         if instance_numa_topology:
4312             guest_cpu_numa = vconfig.LibvirtConfigGuestCPUNUMA()
4313             for instance_cell in instance_numa_topology.cells:
4314                 guest_cell = vconfig.LibvirtConfigGuestCPUNUMACell()
4315                 guest_cell.id = instance_cell.id
4316                 guest_cell.cpus = instance_cell.cpuset
4317                 guest_cell.memory = instance_cell.memory * units.Ki
4318 
4319                 # The vhost-user network backend requires file backed
4320                 # guest memory (ie huge pages) to be marked as shared
4321                 # access, not private, so an external process can read
4322                 # and write the pages.
4323                 #
4324                 # You can't change the shared vs private flag for an
4325                 # already running guest, and since we can't predict what
4326                 # types of NIC may be hotplugged, we have no choice but
4327                 # to unconditionally turn on the shared flag. This has
4328                 # no real negative functional effect on the guest, so
4329                 # is a reasonable approach to take
4330                 if wants_hugepages:
4331                     guest_cell.memAccess = "shared"
4332                 guest_cpu_numa.cells.append(guest_cell)
4333             return guest_cpu_numa
4334 
4335     def _wants_hugepages(self, host_topology, instance_topology):
4336         """Determine if the guest / host topology implies the
4337            use of huge pages for guest RAM backing
4338         """
4339 
4340         if host_topology is None or instance_topology is None:
4341             return False
4342 
4343         avail_pagesize = [page.size_kb
4344                           for page in host_topology.cells[0].mempages]
4345         avail_pagesize.sort()
4346         # Remove smallest page size as that's not classed as a largepage
4347         avail_pagesize = avail_pagesize[1:]
4348 
4349         # See if we have page size set
4350         for cell in instance_topology.cells:
4351             if (cell.pagesize is not None and
4352                 cell.pagesize in avail_pagesize):
4353                 return True
4354 
4355         return False
4356 
4357     def _get_cell_pairs(self, guest_cpu_numa_config, host_topology):
4358         """Returns the lists of pairs(tuple) of an instance cell and
4359         corresponding host cell:
4360             [(LibvirtConfigGuestCPUNUMACell, NUMACell), ...]
4361         """
4362         cell_pairs = []
4363         for guest_config_cell in guest_cpu_numa_config.cells:
4364             for host_cell in host_topology.cells:
4365                 if guest_config_cell.id == host_cell.id:
4366                     cell_pairs.append((guest_config_cell, host_cell))
4367         return cell_pairs
4368 
4369     def _get_pin_cpuset(self, vcpu, object_numa_cell, host_cell):
4370         """Returns the config object of LibvirtConfigGuestCPUTuneVCPUPin.
4371         Prepares vcpupin config for the guest with the following caveats:
4372 
4373             a) If there is pinning information in the cell, we pin vcpus to
4374                individual CPUs
4375             b) Otherwise we float over the whole host NUMA node
4376         """
4377         pin_cpuset = vconfig.LibvirtConfigGuestCPUTuneVCPUPin()
4378         pin_cpuset.id = vcpu
4379 
4380         if object_numa_cell.cpu_pinning:
4381             pin_cpuset.cpuset = set([object_numa_cell.cpu_pinning[vcpu]])
4382         else:
4383             pin_cpuset.cpuset = host_cell.cpuset
4384 
4385         return pin_cpuset
4386 
4387     def _get_emulatorpin_cpuset(self, vcpu, object_numa_cell, vcpus_rt,
4388                                 emulator_threads_policy, wants_realtime,
4389                                 pin_cpuset):
4390         """Returns a set of cpu_ids to add to the cpuset for emulator threads
4391            with the following caveats:
4392 
4393             a) If emulator threads policy is isolated, we pin emulator threads
4394                to one cpu we have reserved for it.
4395             b) If emulator threads policy is shared and CONF.cpu_shared_set is
4396                defined, we pin emulator threads on the set of pCPUs defined by
4397                CONF.cpu_shared_set
4398             c) Otherwise;
4399                 c1) If realtime IS NOT enabled, the emulator threads are
4400                     allowed to float cross all the pCPUs associated with
4401                     the guest vCPUs.
4402                 c2) If realtime IS enabled, at least 1 vCPU is required
4403                     to be set aside for non-realtime usage. The emulator
4404                     threads are allowed to float across the pCPUs that
4405                     are associated with the non-realtime VCPUs.
4406         """
4407         emulatorpin_cpuset = set([])
4408         shared_ids = hardware.get_cpu_shared_set()
4409 
4410         if emulator_threads_policy == fields.CPUEmulatorThreadsPolicy.ISOLATE:
4411             if object_numa_cell.cpuset_reserved:
4412                 emulatorpin_cpuset = object_numa_cell.cpuset_reserved
4413         elif ((emulator_threads_policy ==
4414               fields.CPUEmulatorThreadsPolicy.SHARE) and
4415               shared_ids):
4416             online_pcpus = self._host.get_online_cpus()
4417             cpuset = shared_ids & online_pcpus
4418             if not cpuset:
4419                 msg = (_("Invalid cpu_shared_set config, one or more of the "
4420                          "specified cpuset is not online. Online cpuset(s): "
4421                          "%(online)s, requested cpuset(s): %(req)s"),
4422                        {'online': sorted(online_pcpus),
4423                         'req': sorted(shared_ids)})
4424                 raise exception.Invalid(msg)
4425             emulatorpin_cpuset = cpuset
4426         elif not wants_realtime or vcpu not in vcpus_rt:
4427             emulatorpin_cpuset = pin_cpuset.cpuset
4428 
4429         return emulatorpin_cpuset
4430 
4431     def _get_guest_numa_config(self, instance_numa_topology, flavor,
4432                                allowed_cpus=None, image_meta=None):
4433         """Returns the config objects for the guest NUMA specs.
4434 
4435         Determines the CPUs that the guest can be pinned to if the guest
4436         specifies a cell topology and the host supports it. Constructs the
4437         libvirt XML config object representing the NUMA topology selected
4438         for the guest. Returns a tuple of:
4439 
4440             (cpu_set, guest_cpu_tune, guest_cpu_numa, guest_numa_tune)
4441 
4442         With the following caveats:
4443 
4444             a) If there is no specified guest NUMA topology, then
4445                all tuple elements except cpu_set shall be None. cpu_set
4446                will be populated with the chosen CPUs that the guest
4447                allowed CPUs fit within, which could be the supplied
4448                allowed_cpus value if the host doesn't support NUMA
4449                topologies.
4450 
4451             b) If there is a specified guest NUMA topology, then
4452                cpu_set will be None and guest_cpu_numa will be the
4453                LibvirtConfigGuestCPUNUMA object representing the guest's
4454                NUMA topology. If the host supports NUMA, then guest_cpu_tune
4455                will contain a LibvirtConfigGuestCPUTune object representing
4456                the optimized chosen cells that match the host capabilities
4457                with the instance's requested topology. If the host does
4458                not support NUMA, then guest_cpu_tune and guest_numa_tune
4459                will be None.
4460         """
4461 
4462         if (not self._has_numa_support() and
4463                 instance_numa_topology is not None):
4464             # We should not get here, since we should have avoided
4465             # reporting NUMA topology from _get_host_numa_topology
4466             # in the first place. Just in case of a scheduler
4467             # mess up though, raise an exception
4468             raise exception.NUMATopologyUnsupported()
4469 
4470         topology = self._get_host_numa_topology()
4471 
4472         # We have instance NUMA so translate it to the config class
4473         guest_cpu_numa_config = self._get_cpu_numa_config_from_instance(
4474                 instance_numa_topology,
4475                 self._wants_hugepages(topology, instance_numa_topology))
4476 
4477         if not guest_cpu_numa_config:
4478             # No NUMA topology defined for instance - let the host kernel deal
4479             # with the NUMA effects.
4480             # TODO(ndipanov): Attempt to spread the instance
4481             # across NUMA nodes and expose the topology to the
4482             # instance as an optimisation
4483             return GuestNumaConfig(allowed_cpus, None, None, None)
4484 
4485         if not topology:
4486             # No NUMA topology defined for host - This will only happen with
4487             # some libvirt versions and certain platforms.
4488             return GuestNumaConfig(allowed_cpus, None,
4489                                    guest_cpu_numa_config, None)
4490 
4491         # Now get configuration from the numa_topology
4492         # Init CPUTune configuration
4493         guest_cpu_tune = vconfig.LibvirtConfigGuestCPUTune()
4494         guest_cpu_tune.emulatorpin = (
4495             vconfig.LibvirtConfigGuestCPUTuneEmulatorPin())
4496         guest_cpu_tune.emulatorpin.cpuset = set([])
4497 
4498         # Init NUMATune configuration
4499         guest_numa_tune = vconfig.LibvirtConfigGuestNUMATune()
4500         guest_numa_tune.memory = vconfig.LibvirtConfigGuestNUMATuneMemory()
4501         guest_numa_tune.memnodes = []
4502 
4503         emulator_threads_policy = None
4504         if 'emulator_threads_policy' in instance_numa_topology:
4505             emulator_threads_policy = (
4506                 instance_numa_topology.emulator_threads_policy)
4507 
4508         # Set realtime scheduler for CPUTune
4509         vcpus_rt = set([])
4510         wants_realtime = hardware.is_realtime_enabled(flavor)
4511         if wants_realtime:
4512             vcpus_rt = hardware.vcpus_realtime_topology(flavor, image_meta)
4513             vcpusched = vconfig.LibvirtConfigGuestCPUTuneVCPUSched()
4514             designer.set_vcpu_realtime_scheduler(
4515                 vcpusched, vcpus_rt, CONF.libvirt.realtime_scheduler_priority)
4516             guest_cpu_tune.vcpusched.append(vcpusched)
4517 
4518         cell_pairs = self._get_cell_pairs(guest_cpu_numa_config, topology)
4519         for guest_node_id, (guest_config_cell, host_cell) in enumerate(
4520                 cell_pairs):
4521             # set NUMATune for the cell
4522             tnode = vconfig.LibvirtConfigGuestNUMATuneMemNode()
4523             designer.set_numa_memnode(tnode, guest_node_id, host_cell.id)
4524             guest_numa_tune.memnodes.append(tnode)
4525             guest_numa_tune.memory.nodeset.append(host_cell.id)
4526 
4527             # set CPUTune for the cell
4528             object_numa_cell = instance_numa_topology.cells[guest_node_id]
4529             for cpu in guest_config_cell.cpus:
4530                 pin_cpuset = self._get_pin_cpuset(cpu, object_numa_cell,
4531                                                   host_cell)
4532                 guest_cpu_tune.vcpupin.append(pin_cpuset)
4533 
4534                 emu_pin_cpuset = self._get_emulatorpin_cpuset(
4535                     cpu, object_numa_cell, vcpus_rt,
4536                     emulator_threads_policy, wants_realtime, pin_cpuset)
4537                 guest_cpu_tune.emulatorpin.cpuset.update(emu_pin_cpuset)
4538 
4539         # TODO(berrange) When the guest has >1 NUMA node, it will
4540         # span multiple host NUMA nodes. By pinning emulator threads
4541         # to the union of all nodes, we guarantee there will be
4542         # cross-node memory access by the emulator threads when
4543         # responding to guest I/O operations. The only way to avoid
4544         # this would be to pin emulator threads to a single node and
4545         # tell the guest OS to only do I/O from one of its virtual
4546         # NUMA nodes. This is not even remotely practical.
4547         #
4548         # The long term solution is to make use of a new QEMU feature
4549         # called "I/O Threads" which will let us configure an explicit
4550         # I/O thread for each guest vCPU or guest NUMA node. It is
4551         # still TBD how to make use of this feature though, especially
4552         # how to associate IO threads with guest devices to eliminate
4553         # cross NUMA node traffic. This is an area of investigation
4554         # for QEMU community devs.
4555 
4556         # Sort the vcpupin list per vCPU id for human-friendlier XML
4557         guest_cpu_tune.vcpupin.sort(key=operator.attrgetter("id"))
4558 
4559         # normalize cell.id
4560         for i, (cell, memnode) in enumerate(zip(guest_cpu_numa_config.cells,
4561                                                 guest_numa_tune.memnodes)):
4562             cell.id = i
4563             memnode.cellid = i
4564 
4565         return GuestNumaConfig(None, guest_cpu_tune, guest_cpu_numa_config,
4566                                guest_numa_tune)
4567 
4568     def _get_guest_os_type(self, virt_type):
4569         """Returns the guest OS type based on virt type."""
4570         if virt_type == "lxc":
4571             ret = fields.VMMode.EXE
4572         elif virt_type == "uml":
4573             ret = fields.VMMode.UML
4574         elif virt_type == "xen":
4575             ret = fields.VMMode.XEN
4576         else:
4577             ret = fields.VMMode.HVM
4578         return ret
4579 
4580     def _set_guest_for_rescue(self, rescue, guest, inst_path, virt_type,
4581                               root_device_name):
4582         if rescue.get('kernel_id'):
4583             guest.os_kernel = os.path.join(inst_path, "kernel.rescue")
4584             guest.os_cmdline = ("root=%s %s" % (root_device_name, CONSOLE))
4585             if virt_type == "qemu":
4586                 guest.os_cmdline += " no_timer_check"
4587         if rescue.get('ramdisk_id'):
4588             guest.os_initrd = os.path.join(inst_path, "ramdisk.rescue")
4589 
4590     def _set_guest_for_inst_kernel(self, instance, guest, inst_path, virt_type,
4591                                 root_device_name, image_meta):
4592         guest.os_kernel = os.path.join(inst_path, "kernel")
4593         guest.os_cmdline = ("root=%s %s" % (root_device_name, CONSOLE))
4594         if virt_type == "qemu":
4595             guest.os_cmdline += " no_timer_check"
4596         if instance.ramdisk_id:
4597             guest.os_initrd = os.path.join(inst_path, "ramdisk")
4598         # we only support os_command_line with images with an explicit
4599         # kernel set and don't want to break nova if there's an
4600         # os_command_line property without a specified kernel_id param
4601         if image_meta.properties.get("os_command_line"):
4602             guest.os_cmdline = image_meta.properties.os_command_line
4603 
4604     def _set_clock(self, guest, os_type, image_meta, virt_type):
4605         # NOTE(mikal): Microsoft Windows expects the clock to be in
4606         # "localtime". If the clock is set to UTC, then you can use a
4607         # registry key to let windows know, but Microsoft says this is
4608         # buggy in http://support.microsoft.com/kb/2687252
4609         clk = vconfig.LibvirtConfigGuestClock()
4610         if os_type == 'windows':
4611             LOG.info('Configuring timezone for windows instance to localtime')
4612             clk.offset = 'localtime'
4613         else:
4614             clk.offset = 'utc'
4615         guest.set_clock(clk)
4616 
4617         if virt_type == "kvm":
4618             self._set_kvm_timers(clk, os_type, image_meta)
4619 
4620     def _set_kvm_timers(self, clk, os_type, image_meta):
4621         # TODO(berrange) One day this should be per-guest
4622         # OS type configurable
4623         tmpit = vconfig.LibvirtConfigGuestTimer()
4624         tmpit.name = "pit"
4625         tmpit.tickpolicy = "delay"
4626 
4627         tmrtc = vconfig.LibvirtConfigGuestTimer()
4628         tmrtc.name = "rtc"
4629         tmrtc.tickpolicy = "catchup"
4630 
4631         clk.add_timer(tmpit)
4632         clk.add_timer(tmrtc)
4633 
4634         hpet = image_meta.properties.get('hw_time_hpet', False)
4635         guestarch = libvirt_utils.get_arch(image_meta)
4636         if guestarch in (fields.Architecture.I686,
4637                          fields.Architecture.X86_64):
4638             # NOTE(rfolco): HPET is a hardware timer for x86 arch.
4639             # qemu -no-hpet is not supported on non-x86 targets.
4640             tmhpet = vconfig.LibvirtConfigGuestTimer()
4641             tmhpet.name = "hpet"
4642             tmhpet.present = hpet
4643             clk.add_timer(tmhpet)
4644         else:
4645             if hpet:
4646                 LOG.warning('HPET is not turned on for non-x86 guests in image'
4647                             ' %s.', image_meta.id)
4648 
4649         # Provide Windows guests with the paravirtualized hyperv timer source.
4650         # This is the windows equiv of kvm-clock, allowing Windows
4651         # guests to accurately keep time.
4652         if os_type == 'windows':
4653             tmhyperv = vconfig.LibvirtConfigGuestTimer()
4654             tmhyperv.name = "hypervclock"
4655             tmhyperv.present = True
4656             clk.add_timer(tmhyperv)
4657 
4658     def _set_features(self, guest, os_type, caps, virt_type, image_meta,
4659             flavor):
4660         if virt_type == "xen":
4661             # PAE only makes sense in X86
4662             if caps.host.cpu.arch in (fields.Architecture.I686,
4663                                       fields.Architecture.X86_64):
4664                 guest.features.append(vconfig.LibvirtConfigGuestFeaturePAE())
4665 
4666         if (virt_type not in ("lxc", "uml", "parallels", "xen") or
4667                 (virt_type == "xen" and guest.os_type == fields.VMMode.HVM)):
4668             guest.features.append(vconfig.LibvirtConfigGuestFeatureACPI())
4669             guest.features.append(vconfig.LibvirtConfigGuestFeatureAPIC())
4670 
4671         if (virt_type in ("qemu", "kvm") and
4672                 os_type == 'windows'):
4673             hv = vconfig.LibvirtConfigGuestFeatureHyperV()
4674             hv.relaxed = True
4675 
4676             hv.spinlocks = True
4677             # Increase spinlock retries - value recommended by
4678             # KVM maintainers who certify Windows guests
4679             # with Microsoft
4680             hv.spinlock_retries = 8191
4681             hv.vapic = True
4682             guest.features.append(hv)
4683 
4684         flavor_hide_kvm = strutils.bool_from_string(
4685                 flavor.get('extra_specs', {}).get('hide_hypervisor_id'))
4686         if (virt_type in ("qemu", "kvm") and
4687                 (image_meta.properties.get('img_hide_hypervisor_id') or
4688                  flavor_hide_kvm)):
4689             guest.features.append(vconfig.LibvirtConfigGuestFeatureKvmHidden())
4690 
4691     def _check_number_of_serial_console(self, num_ports):
4692         virt_type = CONF.libvirt.virt_type
4693         if (virt_type in ("kvm", "qemu") and
4694             num_ports > ALLOWED_QEMU_SERIAL_PORTS):
4695             raise exception.SerialPortNumberLimitExceeded(
4696                 allowed=ALLOWED_QEMU_SERIAL_PORTS, virt_type=virt_type)
4697 
4698     def _add_video_driver(self, guest, image_meta, flavor):
4699         VALID_VIDEO_DEVICES = ("vga", "cirrus", "vmvga",
4700                                "xen", "qxl", "virtio")
4701         video = vconfig.LibvirtConfigGuestVideo()
4702         # NOTE(ldbragst): The following logic sets the video.type
4703         # depending on supported defaults given the architecture,
4704         # virtualization type, and features. The video.type attribute can
4705         # be overridden by the user with image_meta.properties, which
4706         # is carried out in the next if statement below this one.
4707         guestarch = libvirt_utils.get_arch(image_meta)
4708         if guest.os_type == fields.VMMode.XEN:
4709             video.type = 'xen'
4710         elif CONF.libvirt.virt_type == 'parallels':
4711             video.type = 'vga'
4712         elif guestarch in (fields.Architecture.PPC,
4713                            fields.Architecture.PPC64,
4714                            fields.Architecture.PPC64LE):
4715             # NOTE(ldbragst): PowerKVM doesn't support 'cirrus' be default
4716             # so use 'vga' instead when running on Power hardware.
4717             video.type = 'vga'
4718         elif guestarch in (fields.Architecture.AARCH64):
4719             # NOTE(kevinz): Only virtio device type is supported by AARCH64
4720             # so use 'virtio' instead when running on AArch64 hardware.
4721             video.type = 'virtio'
4722         elif CONF.spice.enabled:
4723             video.type = 'qxl'
4724         if image_meta.properties.get('hw_video_model'):
4725             video.type = image_meta.properties.hw_video_model
4726             if (video.type not in VALID_VIDEO_DEVICES):
4727                 raise exception.InvalidVideoMode(model=video.type)
4728 
4729         # Set video memory, only if the flavor's limit is set
4730         video_ram = image_meta.properties.get('hw_video_ram', 0)
4731         max_vram = int(flavor.extra_specs.get('hw_video:ram_max_mb', 0))
4732         if video_ram > max_vram:
4733             raise exception.RequestedVRamTooHigh(req_vram=video_ram,
4734                                                  max_vram=max_vram)
4735         if max_vram and video_ram:
4736             video.vram = video_ram * units.Mi / units.Ki
4737         guest.add_device(video)
4738 
4739     def _add_qga_device(self, guest, instance):
4740         qga = vconfig.LibvirtConfigGuestChannel()
4741         qga.type = "unix"
4742         qga.target_name = "org.qemu.guest_agent.0"
4743         qga.source_path = ("/var/lib/libvirt/qemu/%s.%s.sock" %
4744                           ("org.qemu.guest_agent.0", instance.name))
4745         guest.add_device(qga)
4746 
4747     def _add_rng_device(self, guest, flavor):
4748         rng_device = vconfig.LibvirtConfigGuestRng()
4749         rate_bytes = flavor.extra_specs.get('hw_rng:rate_bytes', 0)
4750         period = flavor.extra_specs.get('hw_rng:rate_period', 0)
4751         if rate_bytes:
4752             rng_device.rate_bytes = int(rate_bytes)
4753             rng_device.rate_period = int(period)
4754         rng_path = CONF.libvirt.rng_dev_path
4755         if (rng_path and not os.path.exists(rng_path)):
4756             raise exception.RngDeviceNotExist(path=rng_path)
4757         rng_device.backend = rng_path
4758         guest.add_device(rng_device)
4759 
4760     def _set_qemu_guest_agent(self, guest, flavor, instance, image_meta):
4761         # Enable qga only if the 'hw_qemu_guest_agent' is equal to yes
4762         if image_meta.properties.get('hw_qemu_guest_agent', False):
4763             LOG.debug("Qemu guest agent is enabled through image "
4764                       "metadata", instance=instance)
4765             self._add_qga_device(guest, instance)
4766         rng_is_virtio = image_meta.properties.get('hw_rng_model') == 'virtio'
4767         rng_allowed_str = flavor.extra_specs.get('hw_rng:allowed', '')
4768         rng_allowed = strutils.bool_from_string(rng_allowed_str)
4769         if rng_is_virtio and rng_allowed:
4770             self._add_rng_device(guest, flavor)
4771 
4772     def _get_guest_memory_backing_config(
4773             self, inst_topology, numatune, flavor):
4774         wantsmempages = False
4775         if inst_topology:
4776             for cell in inst_topology.cells:
4777                 if cell.pagesize:
4778                     wantsmempages = True
4779                     break
4780 
4781         wantsrealtime = hardware.is_realtime_enabled(flavor)
4782 
4783         wantsfilebacked = CONF.libvirt.file_backed_memory > 0
4784 
4785         if wantsmempages and wantsfilebacked:
4786             # Can't use file-backed memory with hugepages
4787             LOG.warning("Instance requested huge pages, but file-backed "
4788                     "memory is enabled, and incompatible with huge pages")
4789             raise exception.MemoryPagesUnsupported()
4790 
4791         membacking = None
4792         if wantsmempages:
4793             pages = self._get_memory_backing_hugepages_support(
4794                 inst_topology, numatune)
4795             if pages:
4796                 membacking = vconfig.LibvirtConfigGuestMemoryBacking()
4797                 membacking.hugepages = pages
4798         if wantsrealtime:
4799             if not membacking:
4800                 membacking = vconfig.LibvirtConfigGuestMemoryBacking()
4801             membacking.locked = True
4802             membacking.sharedpages = False
4803         if wantsfilebacked:
4804             if not membacking:
4805                 membacking = vconfig.LibvirtConfigGuestMemoryBacking()
4806             membacking.filesource = True
4807             membacking.sharedaccess = True
4808             membacking.allocateimmediate = True
4809             if self._host.has_min_version(
4810                     MIN_LIBVIRT_FILE_BACKED_DISCARD_VERSION,
4811                     MIN_QEMU_FILE_BACKED_DISCARD_VERSION):
4812                 membacking.discard = True
4813 
4814         return membacking
4815 
4816     def _get_memory_backing_hugepages_support(self, inst_topology, numatune):
4817         if not self._has_numa_support():
4818             # We should not get here, since we should have avoided
4819             # reporting NUMA topology from _get_host_numa_topology
4820             # in the first place. Just in case of a scheduler
4821             # mess up though, raise an exception
4822             raise exception.MemoryPagesUnsupported()
4823 
4824         host_topology = self._get_host_numa_topology()
4825 
4826         if host_topology is None:
4827             # As above, we should not get here but just in case...
4828             raise exception.MemoryPagesUnsupported()
4829 
4830         # Currently libvirt does not support the smallest
4831         # pagesize set as a backend memory.
4832         # https://bugzilla.redhat.com/show_bug.cgi?id=1173507
4833         avail_pagesize = [page.size_kb
4834                           for page in host_topology.cells[0].mempages]
4835         avail_pagesize.sort()
4836         smallest = avail_pagesize[0]
4837 
4838         pages = []
4839         for guest_cellid, inst_cell in enumerate(inst_topology.cells):
4840             if inst_cell.pagesize and inst_cell.pagesize > smallest:
4841                 for memnode in numatune.memnodes:
4842                     if guest_cellid == memnode.cellid:
4843                         page = (
4844                             vconfig.LibvirtConfigGuestMemoryBackingPage())
4845                         page.nodeset = [guest_cellid]
4846                         page.size_kb = inst_cell.pagesize
4847                         pages.append(page)
4848                         break  # Quit early...
4849         return pages
4850 
4851     def _get_flavor(self, ctxt, instance, flavor):
4852         if flavor is not None:
4853             return flavor
4854         return instance.flavor
4855 
4856     def _has_uefi_support(self):
4857         # This means that the host can support uefi booting for guests
4858         supported_archs = [fields.Architecture.X86_64,
4859                            fields.Architecture.AARCH64]
4860         caps = self._host.get_capabilities()
4861         return ((caps.host.cpu.arch in supported_archs) and
4862                 os.path.exists(DEFAULT_UEFI_LOADER_PATH[caps.host.cpu.arch]))
4863 
4864     def _get_supported_perf_events(self):
4865 
4866         if (len(CONF.libvirt.enabled_perf_events) == 0 or
4867              not self._host.has_min_version(MIN_LIBVIRT_PERF_VERSION)):
4868             return []
4869 
4870         supported_events = []
4871         host_cpu_info = self._get_cpu_info()
4872         for event in CONF.libvirt.enabled_perf_events:
4873             if self._supported_perf_event(event, host_cpu_info['features']):
4874                 supported_events.append(event)
4875         return supported_events
4876 
4877     def _supported_perf_event(self, event, cpu_features):
4878 
4879         libvirt_perf_event_name = LIBVIRT_PERF_EVENT_PREFIX + event.upper()
4880 
4881         if not hasattr(libvirt, libvirt_perf_event_name):
4882             LOG.warning("Libvirt doesn't support event type %s.", event)
4883             return False
4884 
4885         if event in PERF_EVENTS_CPU_FLAG_MAPPING:
4886             LOG.warning('Monitoring Intel CMT `perf` event(s) %s is '
4887                         'deprecated and will be removed in the "Stein" '
4888                         'release.  It was broken by design in the '
4889                         'Linux kernel, so support for Intel CMT was '
4890                         'removed from Linux 4.14 onwards. Therefore '
4891                         'it is recommended to not enable them.',
4892                         event)
4893             if PERF_EVENTS_CPU_FLAG_MAPPING[event] not in cpu_features:
4894                 LOG.warning("Host does not support event type %s.", event)
4895                 return False
4896         return True
4897 
4898     def _configure_guest_by_virt_type(self, guest, virt_type, caps, instance,
4899                                       image_meta, flavor, root_device_name):
4900         if virt_type == "xen":
4901             if guest.os_type == fields.VMMode.HVM:
4902                 guest.os_loader = CONF.libvirt.xen_hvmloader_path
4903             else:
4904                 guest.os_cmdline = CONSOLE
4905         elif virt_type in ("kvm", "qemu"):
4906             if caps.host.cpu.arch in (fields.Architecture.I686,
4907                                       fields.Architecture.X86_64):
4908                 guest.sysinfo = self._get_guest_config_sysinfo(instance)
4909                 guest.os_smbios = vconfig.LibvirtConfigGuestSMBIOS()
4910             hw_firmware_type = image_meta.properties.get('hw_firmware_type')
4911             if caps.host.cpu.arch == fields.Architecture.AARCH64:
4912                 if not hw_firmware_type:
4913                     hw_firmware_type = fields.FirmwareType.UEFI
4914             if hw_firmware_type == fields.FirmwareType.UEFI:
4915                 if self._has_uefi_support():
4916                     global uefi_logged
4917                     if not uefi_logged:
4918                         LOG.warning("uefi support is without some kind of "
4919                                     "functional testing and therefore "
4920                                     "considered experimental.")
4921                         uefi_logged = True
4922                     guest.os_loader = DEFAULT_UEFI_LOADER_PATH[
4923                         caps.host.cpu.arch]
4924                     guest.os_loader_type = "pflash"
4925                 else:
4926                     raise exception.UEFINotSupported()
4927             guest.os_mach_type = self._get_machine_type(image_meta, caps)
4928             if image_meta.properties.get('hw_boot_menu') is None:
4929                 guest.os_bootmenu = strutils.bool_from_string(
4930                     flavor.extra_specs.get('hw:boot_menu', 'no'))
4931             else:
4932                 guest.os_bootmenu = image_meta.properties.hw_boot_menu
4933 
4934         elif virt_type == "lxc":
4935             guest.os_init_path = "/sbin/init"
4936             guest.os_cmdline = CONSOLE
4937         elif virt_type == "uml":
4938             guest.os_kernel = "/usr/bin/linux"
4939             guest.os_root = root_device_name
4940         elif virt_type == "parallels":
4941             if guest.os_type == fields.VMMode.EXE:
4942                 guest.os_init_path = "/sbin/init"
4943 
4944     def _conf_non_lxc_uml(self, virt_type, guest, root_device_name, rescue,
4945                     instance, inst_path, image_meta, disk_info):
4946         if rescue:
4947             self._set_guest_for_rescue(rescue, guest, inst_path, virt_type,
4948                                        root_device_name)
4949         elif instance.kernel_id:
4950             self._set_guest_for_inst_kernel(instance, guest, inst_path,
4951                                             virt_type, root_device_name,
4952                                             image_meta)
4953         else:
4954             guest.os_boot_dev = blockinfo.get_boot_order(disk_info)
4955 
4956     def _create_consoles(self, virt_type, guest_cfg, instance, flavor,
4957                          image_meta):
4958         # NOTE(markus_z): Beware! Below are so many conditionals that it is
4959         # easy to lose track. Use this chart to figure out your case:
4960         #
4961         # case | is serial | has       | is qemu | resulting
4962         #      | enabled?  | virtlogd? | or kvm? | devices
4963         # --------------------------------------------------
4964         #    1 |        no |        no |     no  | pty*
4965         #    2 |        no |        no |     yes | file + pty
4966         #    3 |        no |       yes |      no | see case 1
4967         #    4 |        no |       yes |     yes | pty with logd
4968         #    5 |       yes |        no |      no | see case 1
4969         #    6 |       yes |        no |     yes | tcp + pty
4970         #    7 |       yes |       yes |      no | see case 1
4971         #    8 |       yes |       yes |     yes | tcp with logd
4972         #    * exception: virt_type "parallels" doesn't create a device
4973         if virt_type == 'parallels':
4974             pass
4975         elif virt_type not in ("qemu", "kvm"):
4976             log_path = self._get_console_log_path(instance)
4977             self._create_pty_device(guest_cfg,
4978                                     vconfig.LibvirtConfigGuestConsole,
4979                                     log_path=log_path)
4980         elif (virt_type in ("qemu", "kvm") and
4981                   self._is_s390x_guest(image_meta)):
4982             self._create_consoles_s390x(guest_cfg, instance,
4983                                         flavor, image_meta)
4984         elif virt_type in ("qemu", "kvm"):
4985             self._create_consoles_qemu_kvm(guest_cfg, instance,
4986                                         flavor, image_meta)
4987 
4988     def _is_s390x_guest(self, image_meta):
4989         s390x_archs = (fields.Architecture.S390, fields.Architecture.S390X)
4990         return libvirt_utils.get_arch(image_meta) in s390x_archs
4991 
4992     def _create_consoles_qemu_kvm(self, guest_cfg, instance, flavor,
4993                                   image_meta):
4994         char_dev_cls = vconfig.LibvirtConfigGuestSerial
4995         log_path = self._get_console_log_path(instance)
4996         if CONF.serial_console.enabled:
4997             if not self._serial_ports_already_defined(instance):
4998                 num_ports = hardware.get_number_of_serial_ports(flavor,
4999                                                                 image_meta)
5000                 self._check_number_of_serial_console(num_ports)
5001                 self._create_serial_consoles(guest_cfg, num_ports,
5002                                              char_dev_cls, log_path)
5003         else:
5004             self._create_file_device(guest_cfg, instance, char_dev_cls)
5005         self._create_pty_device(guest_cfg, char_dev_cls, log_path=log_path)
5006 
5007     def _create_consoles_s390x(self, guest_cfg, instance, flavor, image_meta):
5008         char_dev_cls = vconfig.LibvirtConfigGuestConsole
5009         log_path = self._get_console_log_path(instance)
5010         if CONF.serial_console.enabled:
5011             if not self._serial_ports_already_defined(instance):
5012                 num_ports = hardware.get_number_of_serial_ports(flavor,
5013                                                                 image_meta)
5014                 self._create_serial_consoles(guest_cfg, num_ports,
5015                                              char_dev_cls, log_path)
5016         else:
5017             self._create_file_device(guest_cfg, instance, char_dev_cls,
5018                                      "sclplm")
5019         self._create_pty_device(guest_cfg, char_dev_cls, "sclp", log_path)
5020 
5021     def _create_pty_device(self, guest_cfg, char_dev_cls, target_type=None,
5022                            log_path=None):
5023         def _create_base_dev():
5024             consolepty = char_dev_cls()
5025             consolepty.target_type = target_type
5026             consolepty.type = "pty"
5027             return consolepty
5028 
5029         def _create_logd_dev():
5030             consolepty = _create_base_dev()
5031             log = vconfig.LibvirtConfigGuestCharDeviceLog()
5032             log.file = log_path
5033             consolepty.log = log
5034             return consolepty
5035 
5036         if CONF.serial_console.enabled:
5037             if self._is_virtlogd_available():
5038                 return
5039             else:
5040                 # NOTE(markus_z): You may wonder why this is necessary and
5041                 # so do I. I'm certain that this is *not* needed in any
5042                 # real use case. It is, however, useful if you want to
5043                 # pypass the Nova API and use "virsh console <guest>" on
5044                 # an hypervisor, as this CLI command doesn't work with TCP
5045                 # devices (like the serial console is).
5046                 #     https://bugzilla.redhat.com/show_bug.cgi?id=781467
5047                 # Pypassing the Nova API however is a thing we don't want.
5048                 # Future changes should remove this and fix the unit tests
5049                 # which ask for the existence.
5050                 guest_cfg.add_device(_create_base_dev())
5051         else:
5052             if self._is_virtlogd_available():
5053                 guest_cfg.add_device(_create_logd_dev())
5054             else:
5055                 guest_cfg.add_device(_create_base_dev())
5056 
5057     def _create_file_device(self, guest_cfg, instance, char_dev_cls,
5058                             target_type=None):
5059         if self._is_virtlogd_available():
5060             return
5061 
5062         consolelog = char_dev_cls()
5063         consolelog.target_type = target_type
5064         consolelog.type = "file"
5065         consolelog.source_path = self._get_console_log_path(instance)
5066         guest_cfg.add_device(consolelog)
5067 
5068     def _serial_ports_already_defined(self, instance):
5069         try:
5070             guest = self._host.get_guest(instance)
5071             if list(self._get_serial_ports_from_guest(guest)):
5072                 # Serial port are already configured for instance that
5073                 # means we are in a context of migration.
5074                 return True
5075         except exception.InstanceNotFound:
5076             LOG.debug(
5077                 "Instance does not exist yet on libvirt, we can "
5078                 "safely pass on looking for already defined serial "
5079                 "ports in its domain XML", instance=instance)
5080         return False
5081 
5082     def _create_serial_consoles(self, guest_cfg, num_ports, char_dev_cls,
5083                                 log_path):
5084         for port in six.moves.range(num_ports):
5085             console = char_dev_cls()
5086             console.port = port
5087             console.type = "tcp"
5088             console.listen_host = CONF.serial_console.proxyclient_address
5089             listen_port = serial_console.acquire_port(console.listen_host)
5090             console.listen_port = listen_port
5091             # NOTE: only the first serial console gets the boot messages,
5092             # that's why we attach the logd subdevice only to that.
5093             if port == 0 and self._is_virtlogd_available():
5094                 log = vconfig.LibvirtConfigGuestCharDeviceLog()
5095                 log.file = log_path
5096                 console.log = log
5097             guest_cfg.add_device(console)
5098 
5099     def _cpu_config_to_vcpu_model(self, cpu_config, vcpu_model):
5100         """Update VirtCPUModel object according to libvirt CPU config.
5101 
5102         :param:cpu_config: vconfig.LibvirtConfigGuestCPU presenting the
5103                            instance's virtual cpu configuration.
5104         :param:vcpu_model: VirtCPUModel object. A new object will be created
5105                            if None.
5106 
5107         :return: Updated VirtCPUModel object, or None if cpu_config is None
5108 
5109         """
5110 
5111         if not cpu_config:
5112             return
5113         if not vcpu_model:
5114             vcpu_model = objects.VirtCPUModel()
5115 
5116         vcpu_model.arch = cpu_config.arch
5117         vcpu_model.vendor = cpu_config.vendor
5118         vcpu_model.model = cpu_config.model
5119         vcpu_model.mode = cpu_config.mode
5120         vcpu_model.match = cpu_config.match
5121 
5122         if cpu_config.sockets:
5123             vcpu_model.topology = objects.VirtCPUTopology(
5124                 sockets=cpu_config.sockets,
5125                 cores=cpu_config.cores,
5126                 threads=cpu_config.threads)
5127         else:
5128             vcpu_model.topology = None
5129 
5130         features = [objects.VirtCPUFeature(
5131             name=f.name,
5132             policy=f.policy) for f in cpu_config.features]
5133         vcpu_model.features = features
5134 
5135         return vcpu_model
5136 
5137     def _vcpu_model_to_cpu_config(self, vcpu_model):
5138         """Create libvirt CPU config according to VirtCPUModel object.
5139 
5140         :param:vcpu_model: VirtCPUModel object.
5141 
5142         :return: vconfig.LibvirtConfigGuestCPU.
5143 
5144         """
5145 
5146         cpu_config = vconfig.LibvirtConfigGuestCPU()
5147         cpu_config.arch = vcpu_model.arch
5148         cpu_config.model = vcpu_model.model
5149         cpu_config.mode = vcpu_model.mode
5150         cpu_config.match = vcpu_model.match
5151         cpu_config.vendor = vcpu_model.vendor
5152         if vcpu_model.topology:
5153             cpu_config.sockets = vcpu_model.topology.sockets
5154             cpu_config.cores = vcpu_model.topology.cores
5155             cpu_config.threads = vcpu_model.topology.threads
5156         if vcpu_model.features:
5157             for f in vcpu_model.features:
5158                 xf = vconfig.LibvirtConfigGuestCPUFeature()
5159                 xf.name = f.name
5160                 xf.policy = f.policy
5161                 cpu_config.features.add(xf)
5162         return cpu_config
5163 
5164     def _guest_add_pcie_root_ports(self, guest):
5165         """Add PCI Express root ports.
5166 
5167         PCI Express machine can have as many PCIe devices as it has
5168         pcie-root-port controllers (slots in virtual motherboard).
5169 
5170         If we want to have more PCIe slots for hotplug then we need to create
5171         whole PCIe structure (libvirt limitation).
5172         """
5173 
5174         pcieroot = vconfig.LibvirtConfigGuestPCIeRootController()
5175         guest.add_device(pcieroot)
5176 
5177         for x in range(0, CONF.libvirt.num_pcie_ports):
5178             pcierootport = vconfig.LibvirtConfigGuestPCIeRootPortController()
5179             guest.add_device(pcierootport)
5180 
5181     def _guest_needs_pcie(self, guest, caps):
5182         """Check for prerequisites for adding PCIe root port
5183         controllers
5184         """
5185 
5186         # TODO(kchamart) In the third 'if' conditional below, for 'x86'
5187         # arch, we're assuming: when 'os_mach_type' is 'None', you'll
5188         # have "pc" machine type.  That assumption, although it is
5189         # correct for the "forseeable future", it will be invalid when
5190         # libvirt / QEMU changes the default machine types.
5191         #
5192         # From libvirt 4.7.0 onwards (September 2018), it will ensure
5193         # that *if* 'pc' is available, it will be used as the default --
5194         # to not break existing applications.  (Refer:
5195         # https://libvirt.org/git/?p=libvirt.git;a=commit;h=26cfb1a3
5196         # --"qemu: ensure default machine types don't change if QEMU
5197         # changes").
5198         #
5199         # But even if libvirt (>=v4.7.0) handled the default case,
5200         # relying on such assumptions is not robust.  Instead we should
5201         # get the default machine type for a given architecture reliably
5202         # -- by Nova setting it explicitly (we already do it for Arm /
5203         # AArch64 & s390x).  A part of this bug is being tracked here:
5204         # https://bugs.launchpad.net/nova/+bug/1780138).
5205 
5206         # Add PCIe root port controllers for PCI Express machines
5207         # but only if their amount is configured
5208 
5209         if not CONF.libvirt.num_pcie_ports:
5210             return False
5211         if (caps.host.cpu.arch == fields.Architecture.AARCH64
5212                 and guest.os_mach_type.startswith('virt')):
5213             return True
5214         if (caps.host.cpu.arch == fields.Architecture.X86_64
5215                 and guest.os_mach_type is not None
5216                 and 'q35' in guest.os_mach_type):
5217             return True
5218         return False
5219 
5220     def _guest_add_usb_host_keyboard(self, guest):
5221         """Add USB Host controller and keyboard for graphical console use.
5222 
5223         Add USB keyboard as PS/2 support may not be present on non-x86
5224         architectures.
5225         """
5226         keyboard = vconfig.LibvirtConfigGuestInput()
5227         keyboard.type = "keyboard"
5228         keyboard.bus = "usb"
5229         guest.add_device(keyboard)
5230 
5231         usbhost = vconfig.LibvirtConfigGuestUSBHostController()
5232         usbhost.index = 0
5233         guest.add_device(usbhost)
5234 
5235     def _get_guest_config(self, instance, network_info, image_meta,
5236                           disk_info, rescue=None, block_device_info=None,
5237                           context=None, mdevs=None):
5238         """Get config data for parameters.
5239 
5240         :param rescue: optional dictionary that should contain the key
5241             'ramdisk_id' if a ramdisk is needed for the rescue image and
5242             'kernel_id' if a kernel is needed for the rescue image.
5243 
5244         :param mdevs: optional list of mediated devices to assign to the guest.
5245         """
5246         flavor = instance.flavor
5247         inst_path = libvirt_utils.get_instance_path(instance)
5248         disk_mapping = disk_info['mapping']
5249 
5250         virt_type = CONF.libvirt.virt_type
5251         guest = vconfig.LibvirtConfigGuest()
5252         guest.virt_type = virt_type
5253         guest.name = instance.name
5254         guest.uuid = instance.uuid
5255         # We are using default unit for memory: KiB
5256         guest.memory = flavor.memory_mb * units.Ki
5257         guest.vcpus = flavor.vcpus
5258         allowed_cpus = hardware.get_vcpu_pin_set()
5259 
5260         guest_numa_config = self._get_guest_numa_config(
5261             instance.numa_topology, flavor, allowed_cpus, image_meta)
5262 
5263         guest.cpuset = guest_numa_config.cpuset
5264         guest.cputune = guest_numa_config.cputune
5265         guest.numatune = guest_numa_config.numatune
5266 
5267         guest.membacking = self._get_guest_memory_backing_config(
5268             instance.numa_topology,
5269             guest_numa_config.numatune,
5270             flavor)
5271 
5272         guest.metadata.append(self._get_guest_config_meta(instance))
5273         guest.idmaps = self._get_guest_idmaps()
5274 
5275         for event in self._supported_perf_events:
5276             guest.add_perf_event(event)
5277 
5278         self._update_guest_cputune(guest, flavor, virt_type)
5279 
5280         guest.cpu = self._get_guest_cpu_config(
5281             flavor, image_meta, guest_numa_config.numaconfig,
5282             instance.numa_topology)
5283 
5284         # Notes(yjiang5): we always sync the instance's vcpu model with
5285         # the corresponding config file.
5286         instance.vcpu_model = self._cpu_config_to_vcpu_model(
5287             guest.cpu, instance.vcpu_model)
5288 
5289         if 'root' in disk_mapping:
5290             root_device_name = block_device.prepend_dev(
5291                 disk_mapping['root']['dev'])
5292         else:
5293             root_device_name = None
5294 
5295         if root_device_name:
5296             instance.root_device_name = root_device_name
5297 
5298         guest.os_type = (fields.VMMode.get_from_instance(instance) or
5299                 self._get_guest_os_type(virt_type))
5300         caps = self._host.get_capabilities()
5301 
5302         self._configure_guest_by_virt_type(guest, virt_type, caps, instance,
5303                                            image_meta, flavor,
5304                                            root_device_name)
5305         if virt_type not in ('lxc', 'uml'):
5306             self._conf_non_lxc_uml(virt_type, guest, root_device_name, rescue,
5307                     instance, inst_path, image_meta, disk_info)
5308 
5309         self._set_features(guest, instance.os_type, caps, virt_type,
5310                            image_meta, flavor)
5311         self._set_clock(guest, instance.os_type, image_meta, virt_type)
5312 
5313         storage_configs = self._get_guest_storage_config(context,
5314                 instance, image_meta, disk_info, rescue, block_device_info,
5315                 flavor, guest.os_type)
5316         for config in storage_configs:
5317             guest.add_device(config)
5318 
5319         for vif in network_info:
5320             config = self.vif_driver.get_config(
5321                 instance, vif, image_meta,
5322                 flavor, virt_type, self._host)
5323             guest.add_device(config)
5324 
5325         self._create_consoles(virt_type, guest, instance, flavor, image_meta)
5326 
5327         pointer = self._get_guest_pointer_model(guest.os_type, image_meta)
5328         if pointer:
5329             guest.add_device(pointer)
5330 
5331         self._guest_add_spice_channel(guest)
5332 
5333         if self._guest_add_video_device(guest):
5334             self._add_video_driver(guest, image_meta, flavor)
5335 
5336             # We want video == we want graphical console. Some architectures
5337             # do not have input devices attached in default configuration.
5338             # Let then add USB Host controller and USB keyboard.
5339             # x86(-64) and ppc64 have usb host controller and keyboard
5340             # s390x does not support USB
5341             if caps.host.cpu.arch == fields.Architecture.AARCH64:
5342                 self._guest_add_usb_host_keyboard(guest)
5343 
5344         # Qemu guest agent only support 'qemu' and 'kvm' hypervisor
5345         if virt_type in ('qemu', 'kvm'):
5346             self._set_qemu_guest_agent(guest, flavor, instance, image_meta)
5347 
5348         if self._guest_needs_pcie(guest, caps):
5349             self._guest_add_pcie_root_ports(guest)
5350 
5351         self._guest_add_pci_devices(guest, instance)
5352 
5353         self._guest_add_watchdog_action(guest, flavor, image_meta)
5354 
5355         self._guest_add_memory_balloon(guest)
5356 
5357         if mdevs:
5358             self._guest_add_mdevs(guest, mdevs)
5359 
5360         return guest
5361 
5362     def _guest_add_mdevs(self, guest, chosen_mdevs):
5363         for chosen_mdev in chosen_mdevs:
5364             mdev = vconfig.LibvirtConfigGuestHostdevMDEV()
5365             mdev.uuid = chosen_mdev
5366             guest.add_device(mdev)
5367 
5368     @staticmethod
5369     def _guest_add_spice_channel(guest):
5370         if (CONF.spice.enabled and CONF.spice.agent_enabled
5371                 and guest.virt_type not in ('lxc', 'uml', 'xen')):
5372             channel = vconfig.LibvirtConfigGuestChannel()
5373             channel.type = 'spicevmc'
5374             channel.target_name = "com.redhat.spice.0"
5375             guest.add_device(channel)
5376 
5377     @staticmethod
5378     def _guest_add_memory_balloon(guest):
5379         virt_type = guest.virt_type
5380         # Memory balloon device only support 'qemu/kvm' and 'xen' hypervisor
5381         if (virt_type in ('xen', 'qemu', 'kvm') and
5382                     CONF.libvirt.mem_stats_period_seconds > 0):
5383             balloon = vconfig.LibvirtConfigMemoryBalloon()
5384             if virt_type in ('qemu', 'kvm'):
5385                 balloon.model = 'virtio'
5386             else:
5387                 balloon.model = 'xen'
5388             balloon.period = CONF.libvirt.mem_stats_period_seconds
5389             guest.add_device(balloon)
5390 
5391     @staticmethod
5392     def _guest_add_watchdog_action(guest, flavor, image_meta):
5393         # image meta takes precedence over flavor extra specs; disable the
5394         # watchdog action by default
5395         watchdog_action = (flavor.extra_specs.get('hw:watchdog_action')
5396                            or 'disabled')
5397         watchdog_action = image_meta.properties.get('hw_watchdog_action',
5398                                                     watchdog_action)
5399         # NB(sross): currently only actually supported by KVM/QEmu
5400         if watchdog_action != 'disabled':
5401             if watchdog_action in fields.WatchdogAction.ALL:
5402                 bark = vconfig.LibvirtConfigGuestWatchdog()
5403                 bark.action = watchdog_action
5404                 guest.add_device(bark)
5405             else:
5406                 raise exception.InvalidWatchdogAction(action=watchdog_action)
5407 
5408     def _guest_add_pci_devices(self, guest, instance):
5409         virt_type = guest.virt_type
5410         if virt_type in ('xen', 'qemu', 'kvm'):
5411             # Get all generic PCI devices (non-SR-IOV).
5412             for pci_dev in pci_manager.get_instance_pci_devs(instance):
5413                 guest.add_device(self._get_guest_pci_device(pci_dev))
5414         else:
5415             # PCI devices is only supported for hypervisors
5416             #  'xen', 'qemu' and 'kvm'.
5417             if pci_manager.get_instance_pci_devs(instance, 'all'):
5418                 raise exception.PciDeviceUnsupportedHypervisor(type=virt_type)
5419 
5420     @staticmethod
5421     def _guest_add_video_device(guest):
5422         # NB some versions of libvirt support both SPICE and VNC
5423         # at the same time. We're not trying to second guess which
5424         # those versions are. We'll just let libvirt report the
5425         # errors appropriately if the user enables both.
5426         add_video_driver = False
5427         if CONF.vnc.enabled and guest.virt_type not in ('lxc', 'uml'):
5428             graphics = vconfig.LibvirtConfigGuestGraphics()
5429             graphics.type = "vnc"
5430             if CONF.vnc.keymap:
5431                 graphics.keymap = CONF.vnc.keymap
5432             graphics.listen = CONF.vnc.server_listen
5433             guest.add_device(graphics)
5434             add_video_driver = True
5435         if CONF.spice.enabled and guest.virt_type not in ('lxc', 'uml', 'xen'):
5436             graphics = vconfig.LibvirtConfigGuestGraphics()
5437             graphics.type = "spice"
5438             if CONF.spice.keymap:
5439                 graphics.keymap = CONF.spice.keymap
5440             graphics.listen = CONF.spice.server_listen
5441             guest.add_device(graphics)
5442             add_video_driver = True
5443         return add_video_driver
5444 
5445     def _get_guest_pointer_model(self, os_type, image_meta):
5446         pointer_model = image_meta.properties.get(
5447             'hw_pointer_model', CONF.pointer_model)
5448         if pointer_model is None and CONF.libvirt.use_usb_tablet:
5449             # TODO(sahid): We set pointer_model to keep compatibility
5450             # until the next release O*. It means operators can continue
5451             # to use the deprecated option "use_usb_tablet" or set a
5452             # specific device to use
5453             pointer_model = "usbtablet"
5454             LOG.warning('The option "use_usb_tablet" has been '
5455                         'deprecated for Newton in favor of the more '
5456                         'generic "pointer_model". Please update '
5457                         'nova.conf to address this change.')
5458 
5459         if pointer_model == "usbtablet":
5460             # We want a tablet if VNC is enabled, or SPICE is enabled and
5461             # the SPICE agent is disabled. If the SPICE agent is enabled
5462             # it provides a paravirt mouse which drastically reduces
5463             # overhead (by eliminating USB polling).
5464             if CONF.vnc.enabled or (
5465                     CONF.spice.enabled and not CONF.spice.agent_enabled):
5466                 return self._get_guest_usb_tablet(os_type)
5467             else:
5468                 if CONF.pointer_model or CONF.libvirt.use_usb_tablet:
5469                     # For backward compatibility We don't want to break
5470                     # process of booting an instance if host is configured
5471                     # to use USB tablet without VNC or SPICE and SPICE
5472                     # agent disable.
5473                     LOG.warning('USB tablet requested for guests by host '
5474                                 'configuration. In order to accept this '
5475                                 'request VNC should be enabled or SPICE '
5476                                 'and SPICE agent disabled on host.')
5477                 else:
5478                     raise exception.UnsupportedPointerModelRequested(
5479                         model="usbtablet")
5480 
5481     def _get_guest_usb_tablet(self, os_type):
5482         tablet = None
5483         if os_type == fields.VMMode.HVM:
5484             tablet = vconfig.LibvirtConfigGuestInput()
5485             tablet.type = "tablet"
5486             tablet.bus = "usb"
5487         else:
5488             if CONF.pointer_model or CONF.libvirt.use_usb_tablet:
5489                 # For backward compatibility We don't want to break
5490                 # process of booting an instance if virtual machine mode
5491                 # is not configured as HVM.
5492                 LOG.warning('USB tablet requested for guests by host '
5493                             'configuration. In order to accept this '
5494                             'request the machine mode should be '
5495                             'configured as HVM.')
5496             else:
5497                 raise exception.UnsupportedPointerModelRequested(
5498                     model="usbtablet")
5499         return tablet
5500 
5501     def _get_guest_xml(self, context, instance, network_info, disk_info,
5502                        image_meta, rescue=None,
5503                        block_device_info=None,
5504                        mdevs=None):
5505         # NOTE(danms): Stringifying a NetworkInfo will take a lock. Do
5506         # this ahead of time so that we don't acquire it while also
5507         # holding the logging lock.
5508         network_info_str = str(network_info)
5509         msg = ('Start _get_guest_xml '
5510                'network_info=%(network_info)s '
5511                'disk_info=%(disk_info)s '
5512                'image_meta=%(image_meta)s rescue=%(rescue)s '
5513                'block_device_info=%(block_device_info)s' %
5514                {'network_info': network_info_str, 'disk_info': disk_info,
5515                 'image_meta': image_meta, 'rescue': rescue,
5516                 'block_device_info': block_device_info})
5517         # NOTE(mriedem): block_device_info can contain auth_password so we
5518         # need to sanitize the password in the message.
5519         LOG.debug(strutils.mask_password(msg), instance=instance)
5520         conf = self._get_guest_config(instance, network_info, image_meta,
5521                                       disk_info, rescue, block_device_info,
5522                                       context, mdevs)
5523         xml = conf.to_xml()
5524 
5525         LOG.debug('End _get_guest_xml xml=%(xml)s',
5526                   {'xml': xml}, instance=instance)
5527         return xml
5528 
5529     def get_info(self, instance):
5530         """Retrieve information from libvirt for a specific instance.
5531 
5532         If a libvirt error is encountered during lookup, we might raise a
5533         NotFound exception or Error exception depending on how severe the
5534         libvirt error is.
5535 
5536         :param instance: nova.objects.instance.Instance object
5537         :returns: An InstanceInfo object
5538         """
5539         guest = self._host.get_guest(instance)
5540         # Kind of ugly but we need to pass host to get_info as for a
5541         # workaround, see libvirt/compat.py
5542         return guest.get_info(self._host)
5543 
5544     def _create_domain_setup_lxc(self, context, instance, image_meta,
5545                                  block_device_info):
5546         inst_path = libvirt_utils.get_instance_path(instance)
5547         block_device_mapping = driver.block_device_info_get_mapping(
5548             block_device_info)
5549         root_disk = block_device.get_root_bdm(block_device_mapping)
5550         if root_disk:
5551             self._connect_volume(context, root_disk['connection_info'],
5552                                  instance)
5553             disk_path = root_disk['connection_info']['data']['device_path']
5554 
5555             # NOTE(apmelton) - Even though the instance is being booted from a
5556             # cinder volume, it is still presented as a local block device.
5557             # LocalBlockImage is used here to indicate that the instance's
5558             # disk is backed by a local block device.
5559             image_model = imgmodel.LocalBlockImage(disk_path)
5560         else:
5561             root_disk = self.image_backend.by_name(instance, 'disk')
5562             image_model = root_disk.get_model(self._conn)
5563 
5564         container_dir = os.path.join(inst_path, 'rootfs')
5565         fileutils.ensure_tree(container_dir)
5566         rootfs_dev = disk_api.setup_container(image_model,
5567                                               container_dir=container_dir)
5568 
5569         try:
5570             # Save rootfs device to disconnect it when deleting the instance
5571             if rootfs_dev:
5572                 instance.system_metadata['rootfs_device_name'] = rootfs_dev
5573             if CONF.libvirt.uid_maps or CONF.libvirt.gid_maps:
5574                 id_maps = self._get_guest_idmaps()
5575                 libvirt_utils.chown_for_id_maps(container_dir, id_maps)
5576         except Exception:
5577             with excutils.save_and_reraise_exception():
5578                 self._create_domain_cleanup_lxc(instance)
5579 
5580     def _create_domain_cleanup_lxc(self, instance):
5581         inst_path = libvirt_utils.get_instance_path(instance)
5582         container_dir = os.path.join(inst_path, 'rootfs')
5583 
5584         try:
5585             state = self.get_info(instance).state
5586         except exception.InstanceNotFound:
5587             # The domain may not be present if the instance failed to start
5588             state = None
5589 
5590         if state == power_state.RUNNING:
5591             # NOTE(uni): Now the container is running with its own private
5592             # mount namespace and so there is no need to keep the container
5593             # rootfs mounted in the host namespace
5594             LOG.debug('Attempting to unmount container filesystem: %s',
5595                       container_dir, instance=instance)
5596             disk_api.clean_lxc_namespace(container_dir=container_dir)
5597         else:
5598             disk_api.teardown_container(container_dir=container_dir)
5599 
5600     @contextlib.contextmanager
5601     def _lxc_disk_handler(self, context, instance, image_meta,
5602                           block_device_info):
5603         """Context manager to handle the pre and post instance boot,
5604            LXC specific disk operations.
5605 
5606            An image or a volume path will be prepared and setup to be
5607            used by the container, prior to starting it.
5608            The disk will be disconnected and unmounted if a container has
5609            failed to start.
5610         """
5611 
5612         if CONF.libvirt.virt_type != 'lxc':
5613             yield
5614             return
5615 
5616         self._create_domain_setup_lxc(context, instance, image_meta,
5617                                       block_device_info)
5618 
5619         try:
5620             yield
5621         finally:
5622             self._create_domain_cleanup_lxc(instance)
5623 
5624     # TODO(sahid): Consider renaming this to _create_guest.
5625     def _create_domain(self, xml=None, domain=None,
5626                        power_on=True, pause=False, post_xml_callback=None):
5627         """Create a domain.
5628 
5629         Either domain or xml must be passed in. If both are passed, then
5630         the domain definition is overwritten from the xml.
5631 
5632         :returns guest.Guest: Guest just created
5633         """
5634         if xml:
5635             guest = libvirt_guest.Guest.create(xml, self._host)
5636             if post_xml_callback is not None:
5637                 post_xml_callback()
5638         else:
5639             guest = libvirt_guest.Guest(domain)
5640 
5641         if power_on or pause:
5642             guest.launch(pause=pause)
5643 
5644         if not utils.is_neutron():
5645             guest.enable_hairpin()
5646 
5647         return guest
5648 
5649     def _neutron_failed_callback(self, event_name, instance):
5650         LOG.error('Neutron Reported failure on event '
5651                   '%(event)s for instance %(uuid)s',
5652                   {'event': event_name, 'uuid': instance.uuid},
5653                   instance=instance)
5654         if CONF.vif_plugging_is_fatal:
5655             raise exception.VirtualInterfaceCreateException()
5656 
5657     def _get_neutron_events(self, network_info):
5658         # NOTE(danms): We need to collect any VIFs that are currently
5659         # down that we expect a down->up event for. Anything that is
5660         # already up will not undergo that transition, and for
5661         # anything that might be stale (cache-wise) assume it's
5662         # already up so we don't block on it.
5663         return [('network-vif-plugged', vif['id'])
5664                 for vif in network_info if vif.get('active', True) is False]
5665 
5666     def _cleanup_failed_start(self, context, instance, network_info,
5667                               block_device_info, guest, destroy_disks):
5668         try:
5669             if guest and guest.is_active():
5670                 guest.poweroff()
5671         finally:
5672             self.cleanup(context, instance, network_info=network_info,
5673                          block_device_info=block_device_info,
5674                          destroy_disks=destroy_disks)
5675 
5676     def _create_domain_and_network(self, context, xml, instance, network_info,
5677                                    block_device_info=None, power_on=True,
5678                                    vifs_already_plugged=False,
5679                                    post_xml_callback=None,
5680                                    destroy_disks_on_failure=False):
5681 
5682         """Do required network setup and create domain."""
5683         timeout = CONF.vif_plugging_timeout
5684         if (self._conn_supports_start_paused and
5685             utils.is_neutron() and not
5686             vifs_already_plugged and power_on and timeout):
5687             events = self._get_neutron_events(network_info)
5688         else:
5689             events = []
5690 
5691         pause = bool(events)
5692         guest = None
5693         try:
5694             with self.virtapi.wait_for_instance_event(
5695                     instance, events, deadline=timeout,
5696                     error_callback=self._neutron_failed_callback):
5697                 self.plug_vifs(instance, network_info)
5698                 self.firewall_driver.setup_basic_filtering(instance,
5699                                                            network_info)
5700                 self.firewall_driver.prepare_instance_filter(instance,
5701                                                              network_info)
5702                 with self._lxc_disk_handler(context, instance,
5703                                             instance.image_meta,
5704                                             block_device_info):
5705                     guest = self._create_domain(
5706                         xml, pause=pause, power_on=power_on,
5707                         post_xml_callback=post_xml_callback)
5708 
5709                 self.firewall_driver.apply_instance_filter(instance,
5710                                                            network_info)
5711         except exception.VirtualInterfaceCreateException:
5712             # Neutron reported failure and we didn't swallow it, so
5713             # bail here
5714             with excutils.save_and_reraise_exception():
5715                 self._cleanup_failed_start(context, instance, network_info,
5716                                            block_device_info, guest,
5717                                            destroy_disks_on_failure)
5718         except eventlet.timeout.Timeout:
5719             # We never heard from Neutron
5720             LOG.warning('Timeout waiting for %(events)s for '
5721                         'instance with vm_state %(vm_state)s and '
5722                         'task_state %(task_state)s.',
5723                         {'events': events,
5724                          'vm_state': instance.vm_state,
5725                          'task_state': instance.task_state},
5726                         instance=instance)
5727             if CONF.vif_plugging_is_fatal:
5728                 self._cleanup_failed_start(context, instance, network_info,
5729                                            block_device_info, guest,
5730                                            destroy_disks_on_failure)
5731                 raise exception.VirtualInterfaceCreateException()
5732         except Exception:
5733             # Any other error, be sure to clean up
5734             LOG.error('Failed to start libvirt guest', instance=instance)
5735             with excutils.save_and_reraise_exception():
5736                 self._cleanup_failed_start(context, instance, network_info,
5737                                            block_device_info, guest,
5738                                            destroy_disks_on_failure)
5739 
5740         # Resume only if domain has been paused
5741         if pause:
5742             guest.resume()
5743         return guest
5744 
5745     def _get_vcpu_total(self):
5746         """Get available vcpu number of physical computer.
5747 
5748         :returns: the number of cpu core instances can be used.
5749 
5750         """
5751         try:
5752             total_pcpus = self._host.get_cpu_count()
5753         except libvirt.libvirtError:
5754             LOG.warning("Cannot get the number of cpu, because this "
5755                         "function is not implemented for this platform.")
5756             return 0
5757 
5758         if not CONF.vcpu_pin_set:
5759             return total_pcpus
5760 
5761         available_ids = hardware.get_vcpu_pin_set()
5762         # We get the list of online CPUs on the host and see if the requested
5763         # set falls under these. If not, we retain the old behavior.
5764         online_pcpus = None
5765         try:
5766             online_pcpus = self._host.get_online_cpus()
5767         except libvirt.libvirtError as ex:
5768             error_code = ex.get_error_code()
5769             err_msg = encodeutils.exception_to_unicode(ex)
5770             LOG.warning(
5771                 "Couldn't retrieve the online CPUs due to a Libvirt "
5772                 "error: %(error)s with error code: %(error_code)s",
5773                 {'error': err_msg, 'error_code': error_code})
5774         if online_pcpus:
5775             if not (available_ids <= online_pcpus):
5776                 msg = (_("Invalid vcpu_pin_set config, one or more of the "
5777                          "specified cpuset is not online. Online cpuset(s): "
5778                          "%(online)s, requested cpuset(s): %(req)s"),
5779                        {'online': sorted(online_pcpus),
5780                         'req': sorted(available_ids)})
5781                 raise exception.Invalid(msg)
5782         elif sorted(available_ids)[-1] >= total_pcpus:
5783             raise exception.Invalid(_("Invalid vcpu_pin_set config, "
5784                                       "out of hypervisor cpu range."))
5785         return len(available_ids)
5786 
5787     @staticmethod
5788     def _get_local_gb_info():
5789         """Get local storage info of the compute node in GB.
5790 
5791         :returns: A dict containing:
5792              :total: How big the overall usable filesystem is (in gigabytes)
5793              :free: How much space is free (in gigabytes)
5794              :used: How much space is used (in gigabytes)
5795         """
5796 
5797         if CONF.libvirt.images_type == 'lvm':
5798             info = lvm.get_volume_group_info(
5799                                CONF.libvirt.images_volume_group)
5800         elif CONF.libvirt.images_type == 'rbd':
5801             info = LibvirtDriver._get_rbd_driver().get_pool_info()
5802         else:
5803             info = libvirt_utils.get_fs_info(CONF.instances_path)
5804 
5805         for (k, v) in info.items():
5806             info[k] = v / units.Gi
5807 
5808         return info
5809 
5810     def _get_vcpu_used(self):
5811         """Get vcpu usage number of physical computer.
5812 
5813         :returns: The total number of vcpu(s) that are currently being used.
5814 
5815         """
5816 
5817         total = 0
5818 
5819         # Not all libvirt drivers will support the get_vcpus_info()
5820         #
5821         # For example, LXC does not have a concept of vCPUs, while
5822         # QEMU (TCG) traditionally handles all vCPUs in a single
5823         # thread. So both will report an exception when the vcpus()
5824         # API call is made. In such a case we should report the
5825         # guest as having 1 vCPU, since that lets us still do
5826         # CPU over commit calculations that apply as the total
5827         # guest count scales.
5828         #
5829         # It is also possible that we might see an exception if
5830         # the guest is just in middle of shutting down. Technically
5831         # we should report 0 for vCPU usage in this case, but we
5832         # we can't reliably distinguish the vcpu not supported
5833         # case from the just shutting down case. Thus we don't know
5834         # whether to report 1 or 0 for vCPU count.
5835         #
5836         # Under-reporting vCPUs is bad because it could conceivably
5837         # let the scheduler place too many guests on the host. Over-
5838         # reporting vCPUs is not a problem as it'll auto-correct on
5839         # the next refresh of usage data.
5840         #
5841         # Thus when getting an exception we always report 1 as the
5842         # vCPU count, as the least worst value.
5843         for guest in self._host.list_guests():
5844             try:
5845                 vcpus = guest.get_vcpus_info()
5846                 total += len(list(vcpus))
5847             except libvirt.libvirtError:
5848                 total += 1
5849             # NOTE(gtt116): give other tasks a chance.
5850             greenthread.sleep(0)
5851         return total
5852 
5853     def _get_supported_vgpu_types(self):
5854         if not CONF.devices.enabled_vgpu_types:
5855             return []
5856         # TODO(sbauza): Move this check up to compute_manager.init_host
5857         if len(CONF.devices.enabled_vgpu_types) > 1:
5858             LOG.warning('libvirt only supports one GPU type per compute node,'
5859                         ' only first type will be used.')
5860         requested_types = CONF.devices.enabled_vgpu_types[:1]
5861         return requested_types
5862 
5863     def _get_vgpu_total(self):
5864         """Returns the number of total available vGPUs for any GPU type that is
5865         enabled with the enabled_vgpu_types CONF option.
5866         """
5867         requested_types = self._get_supported_vgpu_types()
5868         # Bail out early if operator doesn't care about providing vGPUs
5869         if not requested_types:
5870             return 0
5871         # Filter how many available mdevs we can create for all the supported
5872         # types.
5873         mdev_capable_devices = self._get_mdev_capable_devices(requested_types)
5874         vgpus = 0
5875         for dev in mdev_capable_devices:
5876             for _type in dev['types']:
5877                 vgpus += dev['types'][_type]['availableInstances']
5878         # Count the already created (but possibly not assigned to a guest)
5879         # mdevs for all the supported types
5880         mediated_devices = self._get_mediated_devices(requested_types)
5881         vgpus += len(mediated_devices)
5882         return vgpus
5883 
5884     def _get_instance_capabilities(self):
5885         """Get hypervisor instance capabilities
5886 
5887         Returns a list of tuples that describe instances the
5888         hypervisor is capable of hosting.  Each tuple consists
5889         of the triplet (arch, hypervisor_type, vm_mode).
5890 
5891         Supported hypervisor_type is filtered by virt_type,
5892         a parameter set by operators via `nova.conf`.
5893 
5894         :returns: List of tuples describing instance capabilities
5895         """
5896         caps = self._host.get_capabilities()
5897         instance_caps = list()
5898         for g in caps.guests:
5899             for dt in g.domtype:
5900                 if dt != CONF.libvirt.virt_type:
5901                     continue
5902                 instance_cap = (
5903                     fields.Architecture.canonicalize(g.arch),
5904                     fields.HVType.canonicalize(dt),
5905                     fields.VMMode.canonicalize(g.ostype))
5906                 instance_caps.append(instance_cap)
5907 
5908         return instance_caps
5909 
5910     def _get_cpu_info(self):
5911         """Get cpuinfo information.
5912 
5913         Obtains cpu feature from virConnect.getCapabilities.
5914 
5915         :return: see above description
5916 
5917         """
5918 
5919         caps = self._host.get_capabilities()
5920         cpu_info = dict()
5921 
5922         cpu_info['arch'] = caps.host.cpu.arch
5923         cpu_info['model'] = caps.host.cpu.model
5924         cpu_info['vendor'] = caps.host.cpu.vendor
5925 
5926         topology = dict()
5927         topology['cells'] = len(getattr(caps.host.topology, 'cells', [1]))
5928         topology['sockets'] = caps.host.cpu.sockets
5929         topology['cores'] = caps.host.cpu.cores
5930         topology['threads'] = caps.host.cpu.threads
5931         cpu_info['topology'] = topology
5932 
5933         features = set()
5934         for f in caps.host.cpu.features:
5935             features.add(f.name)
5936         cpu_info['features'] = features
5937         return cpu_info
5938 
5939     def _get_pcinet_info(self, vf_address):
5940         """Returns a dict of NET device."""
5941         devname = pci_utils.get_net_name_by_vf_pci_address(vf_address)
5942         if not devname:
5943             return
5944 
5945         virtdev = self._host.device_lookup_by_name(devname)
5946         xmlstr = virtdev.XMLDesc(0)
5947         cfgdev = vconfig.LibvirtConfigNodeDevice()
5948         cfgdev.parse_str(xmlstr)
5949         return {'name': cfgdev.name,
5950                 'capabilities': cfgdev.pci_capability.features}
5951 
5952     def _get_pcidev_info(self, devname):
5953         """Returns a dict of PCI device."""
5954 
5955         def _get_device_type(cfgdev, pci_address):
5956             """Get a PCI device's device type.
5957 
5958             An assignable PCI device can be a normal PCI device,
5959             a SR-IOV Physical Function (PF), or a SR-IOV Virtual
5960             Function (VF). Only normal PCI devices or SR-IOV VFs
5961             are assignable, while SR-IOV PFs are always owned by
5962             hypervisor.
5963             """
5964             for fun_cap in cfgdev.pci_capability.fun_capability:
5965                 if fun_cap.type == 'virt_functions':
5966                     return {
5967                         'dev_type': fields.PciDeviceType.SRIOV_PF,
5968                     }
5969                 if (fun_cap.type == 'phys_function' and
5970                     len(fun_cap.device_addrs) != 0):
5971                     phys_address = "%04x:%02x:%02x.%01x" % (
5972                         fun_cap.device_addrs[0][0],
5973                         fun_cap.device_addrs[0][1],
5974                         fun_cap.device_addrs[0][2],
5975                         fun_cap.device_addrs[0][3])
5976                     return {
5977                         'dev_type': fields.PciDeviceType.SRIOV_VF,
5978                         'parent_addr': phys_address,
5979                     }
5980 
5981             return {'dev_type': fields.PciDeviceType.STANDARD}
5982 
5983         def _get_device_capabilities(device, address):
5984             """Get PCI VF device's additional capabilities.
5985 
5986             If a PCI device is a virtual function, this function reads the PCI
5987             parent's network capabilities (must be always a NIC device) and
5988             appends this information to the device's dictionary.
5989             """
5990             if device.get('dev_type') == fields.PciDeviceType.SRIOV_VF:
5991                 pcinet_info = self._get_pcinet_info(address)
5992                 if pcinet_info:
5993                     return {'capabilities':
5994                                 {'network': pcinet_info.get('capabilities')}}
5995             return {}
5996 
5997         virtdev = self._host.device_lookup_by_name(devname)
5998         xmlstr = virtdev.XMLDesc(0)
5999         cfgdev = vconfig.LibvirtConfigNodeDevice()
6000         cfgdev.parse_str(xmlstr)
6001 
6002         address = "%04x:%02x:%02x.%1x" % (
6003             cfgdev.pci_capability.domain,
6004             cfgdev.pci_capability.bus,
6005             cfgdev.pci_capability.slot,
6006             cfgdev.pci_capability.function)
6007 
6008         device = {
6009             "dev_id": cfgdev.name,
6010             "address": address,
6011             "product_id": "%04x" % cfgdev.pci_capability.product_id,
6012             "vendor_id": "%04x" % cfgdev.pci_capability.vendor_id,
6013             }
6014 
6015         device["numa_node"] = cfgdev.pci_capability.numa_node
6016 
6017         # requirement by DataBase Model
6018         device['label'] = 'label_%(vendor_id)s_%(product_id)s' % device
6019         device.update(_get_device_type(cfgdev, address))
6020         device.update(_get_device_capabilities(device, address))
6021         return device
6022 
6023     def _get_pci_passthrough_devices(self):
6024         """Get host PCI devices information.
6025 
6026         Obtains pci devices information from libvirt, and returns
6027         as a JSON string.
6028 
6029         Each device information is a dictionary, with mandatory keys
6030         of 'address', 'vendor_id', 'product_id', 'dev_type', 'dev_id',
6031         'label' and other optional device specific information.
6032 
6033         Refer to the objects/pci_device.py for more idea of these keys.
6034 
6035         :returns: a JSON string containing a list of the assignable PCI
6036                   devices information
6037         """
6038         # Bail early if we know we can't support `listDevices` to avoid
6039         # repeated warnings within a periodic task
6040         if not getattr(self, '_list_devices_supported', True):
6041             return jsonutils.dumps([])
6042 
6043         try:
6044             dev_names = self._host.list_pci_devices() or []
6045         except libvirt.libvirtError as ex:
6046             error_code = ex.get_error_code()
6047             if error_code == libvirt.VIR_ERR_NO_SUPPORT:
6048                 self._list_devices_supported = False
6049                 LOG.warning("URI %(uri)s does not support "
6050                             "listDevices: %(error)s",
6051                             {'uri': self._uri(),
6052                              'error': encodeutils.exception_to_unicode(ex)})
6053                 return jsonutils.dumps([])
6054             else:
6055                 raise
6056 
6057         pci_info = []
6058         for name in dev_names:
6059             pci_info.append(self._get_pcidev_info(name))
6060 
6061         return jsonutils.dumps(pci_info)
6062 
6063     def _get_mdev_capabilities_for_dev(self, devname, types=None):
6064         """Returns a dict of MDEV capable device with the ID as first key
6065         and then a list of supported types, each of them being a dict.
6066 
6067         :param types: Only return those specific types.
6068         """
6069         virtdev = self._host.device_lookup_by_name(devname)
6070         xmlstr = virtdev.XMLDesc(0)
6071         cfgdev = vconfig.LibvirtConfigNodeDevice()
6072         cfgdev.parse_str(xmlstr)
6073 
6074         device = {
6075             "dev_id": cfgdev.name,
6076             "types": {},
6077             "vendor_id": cfgdev.pci_capability.vendor_id,
6078         }
6079         for mdev_cap in cfgdev.pci_capability.mdev_capability:
6080             for cap in mdev_cap.mdev_types:
6081                 if not types or cap['type'] in types:
6082                     device["types"].update({cap['type']: {
6083                         'availableInstances': cap['availableInstances'],
6084                         'name': cap['name'],
6085                         'deviceAPI': cap['deviceAPI']}})
6086         return device
6087 
6088     def _get_mdev_capable_devices(self, types=None):
6089         """Get host devices supporting mdev types.
6090 
6091         Obtain devices information from libvirt and returns a list of
6092         dictionaries.
6093 
6094         :param types: Filter only devices supporting those types.
6095         """
6096         if not self._host.has_min_version(MIN_LIBVIRT_MDEV_SUPPORT):
6097             return []
6098         dev_names = self._host.list_mdev_capable_devices() or []
6099         mdev_capable_devices = []
6100         for name in dev_names:
6101             device = self._get_mdev_capabilities_for_dev(name, types)
6102             if not device["types"]:
6103                 continue
6104             mdev_capable_devices.append(device)
6105         return mdev_capable_devices
6106 
6107     def _get_mediated_device_information(self, devname):
6108         """Returns a dict of a mediated device."""
6109         virtdev = self._host.device_lookup_by_name(devname)
6110         xmlstr = virtdev.XMLDesc(0)
6111         cfgdev = vconfig.LibvirtConfigNodeDevice()
6112         cfgdev.parse_str(xmlstr)
6113 
6114         device = {
6115             "dev_id": cfgdev.name,
6116             # name is like mdev_00ead764_fdc0_46b6_8db9_2963f5c815b4
6117             "uuid": str(uuid.UUID(cfgdev.name[5:].replace('_', '-'))),
6118             # the physical GPU PCI device
6119             "parent": cfgdev.parent,
6120             "type": cfgdev.mdev_information.type,
6121             "iommu_group": cfgdev.mdev_information.iommu_group,
6122         }
6123         return device
6124 
6125     def _get_mediated_devices(self, types=None):
6126         """Get host mediated devices.
6127 
6128         Obtain devices information from libvirt and returns a list of
6129         dictionaries.
6130 
6131         :param types: Filter only devices supporting those types.
6132         """
6133         if not self._host.has_min_version(MIN_LIBVIRT_MDEV_SUPPORT):
6134             return []
6135         dev_names = self._host.list_mediated_devices() or []
6136         mediated_devices = []
6137         for name in dev_names:
6138             device = self._get_mediated_device_information(name)
6139             if not types or device["type"] in types:
6140                 mediated_devices.append(device)
6141         return mediated_devices
6142 
6143     def _get_all_assigned_mediated_devices(self, instance=None):
6144         """Lookup all instances from the host and return all the mediated
6145         devices that are assigned to a guest.
6146 
6147         :param instance: Only return mediated devices for that instance.
6148 
6149         :returns: A dictionary of keys being mediated device UUIDs and their
6150                   respective values the instance UUID of the guest using it.
6151         """
6152         allocated_mdevs = {}
6153         if instance:
6154             # NOTE(sbauza): In some cases (like a migration issue), the
6155             # instance can exist in the Nova database but libvirt doesn't know
6156             # about it. For such cases, the way to fix that is to hard reboot
6157             # the instance, which will recreate the libvirt guest.
6158             # For that reason, we need to support that case by making sure
6159             # we don't raise an exception if the libvirt guest doesn't exist.
6160             try:
6161                 guest = self._host.get_guest(instance)
6162             except exception.InstanceNotFound:
6163                 # Bail out early if libvirt doesn't know about it since we
6164                 # can't know the existing mediated devices
6165                 return {}
6166             guests = [guest]
6167         else:
6168             guests = self._host.list_guests(only_running=False)
6169         for guest in guests:
6170             cfg = guest.get_config()
6171             for device in cfg.devices:
6172                 if isinstance(device, vconfig.LibvirtConfigGuestHostdevMDEV):
6173                     allocated_mdevs[device.uuid] = guest.uuid
6174         return allocated_mdevs
6175 
6176     @staticmethod
6177     def _vgpu_allocations(allocations):
6178         """Filtering only the VGPU allocations from a list of allocations.
6179 
6180         :param allocations: Information about resources allocated to the
6181                             instance via placement, of the form returned by
6182                             SchedulerReportClient.get_allocations_for_consumer.
6183         """
6184         if not allocations:
6185             # If no allocations, there is no vGPU request.
6186             return {}
6187         RC_VGPU = rc_fields.ResourceClass.VGPU
6188         vgpu_allocations = {}
6189         for rp in allocations:
6190             res = allocations[rp]['resources']
6191             if RC_VGPU in res and res[RC_VGPU] > 0:
6192                 vgpu_allocations[rp] = {'resources': {RC_VGPU: res[RC_VGPU]}}
6193         return vgpu_allocations
6194 
6195     def _get_existing_mdevs_not_assigned(self, requested_types=None):
6196         """Returns the already created mediated devices that are not assigned
6197         to a guest yet.
6198 
6199         :param requested_types: Filter out the result for only mediated devices
6200                                 having those types.
6201         """
6202         allocated_mdevs = self._get_all_assigned_mediated_devices()
6203         mdevs = self._get_mediated_devices(requested_types)
6204         available_mdevs = set([mdev["uuid"]
6205                                for mdev in mdevs]) - set(allocated_mdevs)
6206         return available_mdevs
6207 
6208     def _create_new_mediated_device(self, requested_types, uuid=None):
6209         """Find a physical device that can support a new mediated device and
6210         create it.
6211 
6212         :param requested_types: Filter only capable devices supporting those
6213                                 types.
6214         :param uuid: The possible mdev UUID we want to create again
6215 
6216         :returns: the newly created mdev UUID or None if not possible
6217         """
6218         # Try to see if we can still create a new mediated device
6219         devices = self._get_mdev_capable_devices(requested_types)
6220         for device in devices:
6221             # For the moment, the libvirt driver only supports one
6222             # type per host
6223             # TODO(sbauza): Once we support more than one type, make
6224             # sure we look at the flavor/trait for the asked type.
6225             asked_type = requested_types[0]
6226             if device['types'][asked_type]['availableInstances'] > 0:
6227                 # That physical GPU has enough room for a new mdev
6228                 dev_name = device['dev_id']
6229                 # We need the PCI address, not the libvirt name
6230                 # The libvirt name is like 'pci_0000_84_00_0'
6231                 pci_addr = "{}:{}:{}.{}".format(*dev_name[4:].split('_'))
6232                 chosen_mdev = nova.privsep.libvirt.create_mdev(pci_addr,
6233                                                                asked_type,
6234                                                                uuid=uuid)
6235                 return chosen_mdev
6236 
6237     @utils.synchronized(VGPU_RESOURCE_SEMAPHORE)
6238     def _allocate_mdevs(self, allocations):
6239         """Returns a list of mediated device UUIDs corresponding to available
6240         resources we can assign to the guest(s) corresponding to the allocation
6241         requests passed as argument.
6242 
6243         That method can either find an existing but unassigned mediated device
6244         it can allocate, or create a new mediated device from a capable
6245         physical device if the latter has enough left capacity.
6246 
6247         :param allocations: Information about resources allocated to the
6248                             instance via placement, of the form returned by
6249                             SchedulerReportClient.get_allocations_for_consumer.
6250                             That code is supporting Placement API version 1.12
6251         """
6252         vgpu_allocations = self._vgpu_allocations(allocations)
6253         if not vgpu_allocations:
6254             return
6255         # TODO(sbauza): Once we have nested resource providers, find which one
6256         # is having the related allocation for the specific VGPU type.
6257         # For the moment, we should only have one allocation for
6258         # ResourceProvider.
6259         # TODO(sbauza): Iterate over all the allocations once we have
6260         # nested Resource Providers. For the moment, just take the first.
6261         if len(vgpu_allocations) > 1:
6262             LOG.warning('More than one allocation was passed over to libvirt '
6263                         'while at the moment libvirt only supports one. Only '
6264                         'the first allocation will be looked up.')
6265         alloc = six.next(six.itervalues(vgpu_allocations))
6266         vgpus_asked = alloc['resources'][rc_fields.ResourceClass.VGPU]
6267 
6268         requested_types = self._get_supported_vgpu_types()
6269         # Which mediated devices are created but not assigned to a guest ?
6270         mdevs_available = self._get_existing_mdevs_not_assigned(
6271             requested_types)
6272 
6273         chosen_mdevs = []
6274         for c in six.moves.range(vgpus_asked):
6275             chosen_mdev = None
6276             if mdevs_available:
6277                 # Take the first available mdev
6278                 chosen_mdev = mdevs_available.pop()
6279             else:
6280                 chosen_mdev = self._create_new_mediated_device(requested_types)
6281             if not chosen_mdev:
6282                 # If we can't find devices having available VGPUs, just raise
6283                 raise exception.ComputeResourcesUnavailable(
6284                     reason='vGPU resource is not available')
6285             else:
6286                 chosen_mdevs.append(chosen_mdev)
6287         return chosen_mdevs
6288 
6289     def _detach_mediated_devices(self, guest):
6290         mdevs = guest.get_all_devices(
6291             devtype=vconfig.LibvirtConfigGuestHostdevMDEV)
6292         for mdev_cfg in mdevs:
6293             try:
6294                 guest.detach_device(mdev_cfg, live=True)
6295             except libvirt.libvirtError as ex:
6296                 error_code = ex.get_error_code()
6297                 # NOTE(sbauza): There is a pending issue with libvirt that
6298                 # doesn't allow to hot-unplug mediated devices. Let's
6299                 # short-circuit the suspend action and set the instance back
6300                 # to ACTIVE.
6301                 # TODO(sbauza): Once libvirt supports this, amend the resume()
6302                 # operation to support reallocating mediated devices.
6303                 if error_code == libvirt.VIR_ERR_CONFIG_UNSUPPORTED:
6304                     reason = _("Suspend is not supported for instances having "
6305                                "attached vGPUs.")
6306                     raise exception.InstanceFaultRollback(
6307                         exception.InstanceSuspendFailure(reason=reason))
6308                 else:
6309                     raise
6310 
6311     def _has_numa_support(self):
6312         # This means that the host can support LibvirtConfigGuestNUMATune
6313         # and the nodeset field in LibvirtConfigGuestMemoryBackingPage
6314         caps = self._host.get_capabilities()
6315 
6316         if (caps.host.cpu.arch in (fields.Architecture.I686,
6317                                    fields.Architecture.X86_64,
6318                                    fields.Architecture.AARCH64) and
6319                 self._host.has_min_version(hv_type=host.HV_DRIVER_QEMU)):
6320             return True
6321         elif (caps.host.cpu.arch in (fields.Architecture.PPC64,
6322                                      fields.Architecture.PPC64LE)):
6323             return True
6324 
6325         return False
6326 
6327     def _get_host_numa_topology(self):
6328         if not self._has_numa_support():
6329             return
6330 
6331         caps = self._host.get_capabilities()
6332         topology = caps.host.topology
6333 
6334         if topology is None or not topology.cells:
6335             return
6336 
6337         cells = []
6338         allowed_cpus = hardware.get_vcpu_pin_set()
6339         online_cpus = self._host.get_online_cpus()
6340         if allowed_cpus:
6341             allowed_cpus &= online_cpus
6342         else:
6343             allowed_cpus = online_cpus
6344 
6345         def _get_reserved_memory_for_cell(self, cell_id, page_size):
6346             cell = self._reserved_hugepages.get(cell_id, {})
6347             return cell.get(page_size, 0)
6348 
6349         def _get_physnet_numa_affinity():
6350             affinities = {cell.id: set() for cell in topology.cells}
6351             for physnet in CONF.neutron.physnets:
6352                 # This will error out if the group is not registered, which is
6353                 # exactly what we want as that would be a bug
6354                 group = getattr(CONF, 'neutron_physnet_%s' % physnet)
6355 
6356                 if not group.numa_nodes:
6357                     msg = ("the physnet '%s' was listed in '[neutron] "
6358                            "physnets' but no corresponding "
6359                            "'[neutron_physnet_%s] numa_nodes' option was "
6360                            "defined." % (physnet, physnet))
6361                     raise exception.InvalidNetworkNUMAAffinity(reason=msg)
6362 
6363                 for node in group.numa_nodes:
6364                     if node not in affinities:
6365                         msg = ("node %d for physnet %s is not present in host "
6366                                "affinity set %r" % (node, physnet, affinities))
6367                         # The config option referenced an invalid node
6368                         raise exception.InvalidNetworkNUMAAffinity(reason=msg)
6369                     affinities[node].add(physnet)
6370 
6371             return affinities
6372 
6373         def _get_tunnel_numa_affinity():
6374             affinities = {cell.id: False for cell in topology.cells}
6375 
6376             for node in CONF.neutron_tunnel.numa_nodes:
6377                 if node not in affinities:
6378                     msg = ("node %d for tunneled networks is not present "
6379                            "in host affinity set %r" % (node, affinities))
6380                     # The config option referenced an invalid node
6381                     raise exception.InvalidNetworkNUMAAffinity(reason=msg)
6382                 affinities[node] = True
6383 
6384             return affinities
6385 
6386         physnet_affinities = _get_physnet_numa_affinity()
6387         tunnel_affinities = _get_tunnel_numa_affinity()
6388 
6389         for cell in topology.cells:
6390             cpuset = set(cpu.id for cpu in cell.cpus)
6391             siblings = sorted(map(set,
6392                                   set(tuple(cpu.siblings)
6393                                         if cpu.siblings else ()
6394                                       for cpu in cell.cpus)
6395                                   ))
6396             cpuset &= allowed_cpus
6397             siblings = [sib & allowed_cpus for sib in siblings]
6398             # Filter out empty sibling sets that may be left
6399             siblings = [sib for sib in siblings if len(sib) > 0]
6400 
6401             mempages = [
6402                 objects.NUMAPagesTopology(
6403                     size_kb=pages.size,
6404                     total=pages.total,
6405                     used=0,
6406                     reserved=_get_reserved_memory_for_cell(
6407                         self, cell.id, pages.size))
6408                 for pages in cell.mempages]
6409 
6410             network_metadata = objects.NetworkMetadata(
6411                 physnets=physnet_affinities[cell.id],
6412                 tunneled=tunnel_affinities[cell.id])
6413 
6414             cell = objects.NUMACell(id=cell.id, cpuset=cpuset,
6415                                     memory=cell.memory / units.Ki,
6416                                     cpu_usage=0, memory_usage=0,
6417                                     siblings=siblings,
6418                                     pinned_cpus=set([]),
6419                                     mempages=mempages,
6420                                     network_metadata=network_metadata)
6421             cells.append(cell)
6422 
6423         return objects.NUMATopology(cells=cells)
6424 
6425     def get_all_volume_usage(self, context, compute_host_bdms):
6426         """Return usage info for volumes attached to vms on
6427            a given host.
6428         """
6429         vol_usage = []
6430 
6431         for instance_bdms in compute_host_bdms:
6432             instance = instance_bdms['instance']
6433 
6434             for bdm in instance_bdms['instance_bdms']:
6435                 mountpoint = bdm['device_name']
6436                 if mountpoint.startswith('/dev/'):
6437                     mountpoint = mountpoint[5:]
6438                 volume_id = bdm['volume_id']
6439 
6440                 LOG.debug("Trying to get stats for the volume %s",
6441                           volume_id, instance=instance)
6442                 vol_stats = self.block_stats(instance, mountpoint)
6443 
6444                 if vol_stats:
6445                     stats = dict(volume=volume_id,
6446                                  instance=instance,
6447                                  rd_req=vol_stats[0],
6448                                  rd_bytes=vol_stats[1],
6449                                  wr_req=vol_stats[2],
6450                                  wr_bytes=vol_stats[3])
6451                     LOG.debug(
6452                         "Got volume usage stats for the volume=%(volume)s,"
6453                         " rd_req=%(rd_req)d, rd_bytes=%(rd_bytes)d, "
6454                         "wr_req=%(wr_req)d, wr_bytes=%(wr_bytes)d",
6455                         stats, instance=instance)
6456                     vol_usage.append(stats)
6457 
6458         return vol_usage
6459 
6460     def block_stats(self, instance, disk_id):
6461         """Note that this function takes an instance name."""
6462         try:
6463             guest = self._host.get_guest(instance)
6464             dev = guest.get_block_device(disk_id)
6465             return dev.blockStats()
6466         except libvirt.libvirtError as e:
6467             errcode = e.get_error_code()
6468             LOG.info('Getting block stats failed, device might have '
6469                      'been detached. Instance=%(instance_name)s '
6470                      'Disk=%(disk)s Code=%(errcode)s Error=%(e)s',
6471                      {'instance_name': instance.name, 'disk': disk_id,
6472                       'errcode': errcode, 'e': e},
6473                      instance=instance)
6474         except exception.InstanceNotFound:
6475             LOG.info('Could not find domain in libvirt for instance %s. '
6476                      'Cannot get block stats for device', instance.name,
6477                      instance=instance)
6478 
6479     def get_console_pool_info(self, console_type):
6480         # TODO(mdragon): console proxy should be implemented for libvirt,
6481         #                in case someone wants to use it with kvm or
6482         #                such. For now return fake data.
6483         return {'address': '127.0.0.1',
6484                 'username': 'fakeuser',
6485                 'password': 'fakepassword'}
6486 
6487     def refresh_security_group_rules(self, security_group_id):
6488         self.firewall_driver.refresh_security_group_rules(security_group_id)
6489 
6490     def refresh_instance_security_rules(self, instance):
6491         self.firewall_driver.refresh_instance_security_rules(instance)
6492 
6493     def update_provider_tree(self, provider_tree, nodename, allocations=None):
6494         """Update a ProviderTree object with current resource provider,
6495         inventory information and CPU traits.
6496 
6497         :param nova.compute.provider_tree.ProviderTree provider_tree:
6498             A nova.compute.provider_tree.ProviderTree object representing all
6499             the providers in the tree associated with the compute node, and any
6500             sharing providers (those with the ``MISC_SHARES_VIA_AGGREGATE``
6501             trait) associated via aggregate with any of those providers (but
6502             not *their* tree- or aggregate-associated providers), as currently
6503             known by placement.
6504         :param nodename:
6505             String name of the compute node (i.e.
6506             ComputeNode.hypervisor_hostname) for which the caller is requesting
6507             updated provider information.
6508         :param allocations:
6509             Dict of allocation data of the form:
6510               { $CONSUMER_UUID: {
6511                     # The shape of each "allocations" dict below is identical
6512                     # to the return from GET /allocations/{consumer_uuid}
6513                     "allocations": {
6514                         $RP_UUID: {
6515                             "generation": $RP_GEN,
6516                             "resources": {
6517                                 $RESOURCE_CLASS: $AMOUNT,
6518                                 ...
6519                             },
6520                         },
6521                         ...
6522                     },
6523                     "project_id": $PROJ_ID,
6524                     "user_id": $USER_ID,
6525                     "consumer_generation": $CONSUMER_GEN,
6526                 },
6527                 ...
6528               }
6529             If None, and the method determines that any inventory needs to be
6530             moved (from one provider to another and/or to a different resource
6531             class), the ReshapeNeeded exception must be raised. Otherwise, this
6532             dict must be edited in place to indicate the desired final state of
6533             allocations.
6534         :raises ReshapeNeeded: If allocations is None and any inventory needs
6535             to be moved from one provider to another and/or to a different
6536             resource class.
6537         """
6538         disk_gb = int(self._get_local_gb_info()['total'])
6539         memory_mb = int(self._host.get_memory_mb_total())
6540         vcpus = self._get_vcpu_total()
6541 
6542         # NOTE(sbauza): For the moment, the libvirt driver only supports
6543         # providing the total number of virtual GPUs for a single GPU type. If
6544         # you have multiple physical GPUs, each of them providing multiple GPU
6545         # types, libvirt will return the total sum of virtual GPUs
6546         # corresponding to the single type passed in enabled_vgpu_types
6547         # configuration option. Eg. if you have 2 pGPUs supporting 'nvidia-35',
6548         # each of them having 16 available instances, the total here will be
6549         # 32.
6550         # If one of the 2 pGPUs doesn't support 'nvidia-35', it won't be used.
6551         # TODO(sbauza): Use traits to make a better world.
6552         vgpus = self._get_vgpu_total()
6553 
6554         # NOTE(yikun): If the inv record does not exists, the allocation_ratio
6555         # will use the CONF.xxx_allocation_ratio value if xxx_allocation_ratio
6556         # is set, and fallback to use the initial_xxx_allocation_ratio
6557         # otherwise.
6558         inv = provider_tree.data(nodename).inventory
6559         ratios = self._get_allocation_ratios(inv)
6560         result = {
6561             rc_fields.ResourceClass.VCPU: {
6562                 'total': vcpus,
6563                 'min_unit': 1,
6564                 'max_unit': vcpus,
6565                 'step_size': 1,
6566                 'allocation_ratio': ratios[rc_fields.ResourceClass.VCPU],
6567                 'reserved': CONF.reserved_host_cpus,
6568             },
6569             rc_fields.ResourceClass.MEMORY_MB: {
6570                 'total': memory_mb,
6571                 'min_unit': 1,
6572                 'max_unit': memory_mb,
6573                 'step_size': 1,
6574                 'allocation_ratio': ratios[rc_fields.ResourceClass.MEMORY_MB],
6575                 'reserved': CONF.reserved_host_memory_mb,
6576             },
6577         }
6578 
6579         # If a sharing DISK_GB provider exists in the provider tree, then our
6580         # storage is shared, and we should not report the DISK_GB inventory in
6581         # the compute node provider.
6582         # TODO(efried): Reinstate non-reporting of shared resource by the
6583         # compute RP once the issues from bug #1784020 have been resolved.
6584         if provider_tree.has_sharing_provider(rc_fields.ResourceClass.DISK_GB):
6585             LOG.debug('Ignoring sharing provider - see bug #1784020')
6586         result[rc_fields.ResourceClass.DISK_GB] = {
6587             'total': disk_gb,
6588             'min_unit': 1,
6589             'max_unit': disk_gb,
6590             'step_size': 1,
6591             'allocation_ratio': ratios[rc_fields.ResourceClass.DISK_GB],
6592             'reserved': self._get_reserved_host_disk_gb_from_config(),
6593         }
6594 
6595         if vgpus > 0:
6596             # Only provide VGPU resource classes if the driver supports it.
6597             result[rc_fields.ResourceClass.VGPU] = {
6598                 'total': vgpus,
6599                 'min_unit': 1,
6600                 'max_unit': vgpus,
6601                 'step_size': 1,
6602                 }
6603 
6604         provider_tree.update_inventory(nodename, result)
6605 
6606         traits = self._get_cpu_traits()
6607         if traits is not None:
6608             # _get_cpu_traits returns a dict of trait names mapped to boolean
6609             # values. Add traits equal to True to provider tree, remove
6610             # those False traits from provider tree.
6611             traits_to_add = [t for t in traits if traits[t]]
6612             traits_to_remove = set(traits) - set(traits_to_add)
6613             provider_tree.add_traits(nodename, *traits_to_add)
6614             provider_tree.remove_traits(nodename, *traits_to_remove)
6615 
6616     def get_available_resource(self, nodename):
6617         """Retrieve resource information.
6618 
6619         This method is called when nova-compute launches, and
6620         as part of a periodic task that records the results in the DB.
6621 
6622         :param nodename: unused in this driver
6623         :returns: dictionary containing resource info
6624         """
6625 
6626         disk_info_dict = self._get_local_gb_info()
6627         data = {}
6628 
6629         # NOTE(dprince): calling capabilities before getVersion works around
6630         # an initialization issue with some versions of Libvirt (1.0.5.5).
6631         # See: https://bugzilla.redhat.com/show_bug.cgi?id=1000116
6632         # See: https://bugs.launchpad.net/nova/+bug/1215593
6633         data["supported_instances"] = self._get_instance_capabilities()
6634 
6635         data["vcpus"] = self._get_vcpu_total()
6636         data["memory_mb"] = self._host.get_memory_mb_total()
6637         data["local_gb"] = disk_info_dict['total']
6638         data["vcpus_used"] = self._get_vcpu_used()
6639         data["memory_mb_used"] = self._host.get_memory_mb_used()
6640         data["local_gb_used"] = disk_info_dict['used']
6641         data["hypervisor_type"] = self._host.get_driver_type()
6642         data["hypervisor_version"] = self._host.get_version()
6643         data["hypervisor_hostname"] = self._host.get_hostname()
6644         # TODO(berrange): why do we bother converting the
6645         # libvirt capabilities XML into a special JSON format ?
6646         # The data format is different across all the drivers
6647         # so we could just return the raw capabilities XML
6648         # which 'compare_cpu' could use directly
6649         #
6650         # That said, arch_filter.py now seems to rely on
6651         # the libvirt drivers format which suggests this
6652         # data format needs to be standardized across drivers
6653         data["cpu_info"] = jsonutils.dumps(self._get_cpu_info())
6654 
6655         disk_free_gb = disk_info_dict['free']
6656         disk_over_committed = self._get_disk_over_committed_size_total()
6657         available_least = disk_free_gb * units.Gi - disk_over_committed
6658         data['disk_available_least'] = available_least / units.Gi
6659 
6660         data['pci_passthrough_devices'] = \
6661             self._get_pci_passthrough_devices()
6662 
6663         numa_topology = self._get_host_numa_topology()
6664         if numa_topology:
6665             data['numa_topology'] = numa_topology._to_json()
6666         else:
6667             data['numa_topology'] = None
6668 
6669         return data
6670 
6671     def check_instance_shared_storage_local(self, context, instance):
6672         """Check if instance files located on shared storage.
6673 
6674         This runs check on the destination host, and then calls
6675         back to the source host to check the results.
6676 
6677         :param context: security context
6678         :param instance: nova.objects.instance.Instance object
6679         :returns:
6680          - tempfile: A dict containing the tempfile info on the destination
6681                      host
6682          - None:
6683 
6684             1. If the instance path is not existing.
6685             2. If the image backend is shared block storage type.
6686         """
6687         if self.image_backend.backend().is_shared_block_storage():
6688             return None
6689 
6690         dirpath = libvirt_utils.get_instance_path(instance)
6691 
6692         if not os.path.exists(dirpath):
6693             return None
6694 
6695         fd, tmp_file = tempfile.mkstemp(dir=dirpath)
6696         LOG.debug("Creating tmpfile %s to verify with other "
6697                   "compute node that the instance is on "
6698                   "the same shared storage.",
6699                   tmp_file, instance=instance)
6700         os.close(fd)
6701         return {"filename": tmp_file}
6702 
6703     def check_instance_shared_storage_remote(self, context, data):
6704         return os.path.exists(data['filename'])
6705 
6706     def check_instance_shared_storage_cleanup(self, context, data):
6707         fileutils.delete_if_exists(data["filename"])
6708 
6709     def check_can_live_migrate_destination(self, context, instance,
6710                                            src_compute_info, dst_compute_info,
6711                                            block_migration=False,
6712                                            disk_over_commit=False):
6713         """Check if it is possible to execute live migration.
6714 
6715         This runs checks on the destination host, and then calls
6716         back to the source host to check the results.
6717 
6718         :param context: security context
6719         :param instance: nova.db.sqlalchemy.models.Instance
6720         :param block_migration: if true, prepare for block migration
6721         :param disk_over_commit: if true, allow disk over commit
6722         :returns: a LibvirtLiveMigrateData object
6723         """
6724 
6725         # TODO(zcornelius): Remove this check in Stein, as we'll only support
6726         #                   Rocky and newer computes.
6727         # If file_backed_memory is enabled on this host, we have to make sure
6728         # the source is new enough to support it. Since the source generates
6729         # the XML for the destination, we depend on the source generating a
6730         # file-backed XML for us, so fail if it won't do that.
6731         if CONF.libvirt.file_backed_memory > 0:
6732             srv = objects.Service.get_by_compute_host(context, instance.host)
6733             if srv.version < 32:
6734                 msg = ("Cannot migrate instance %(uuid)s from node %(node)s. "
6735                        "Node %(node)s is not compatible with "
6736                        "file_backed_memory" % {"uuid": instance.uuid,
6737                                                "node": srv.host})
6738                 raise exception.MigrationPreCheckError(reason=msg)
6739 
6740         if disk_over_commit:
6741             disk_available_gb = dst_compute_info['free_disk_gb']
6742         else:
6743             disk_available_gb = dst_compute_info['disk_available_least']
6744         disk_available_mb = (
6745             (disk_available_gb * units.Ki) - CONF.reserved_host_disk_mb)
6746 
6747         # Compare CPU
6748         if not instance.vcpu_model or not instance.vcpu_model.model:
6749             source_cpu_info = src_compute_info['cpu_info']
6750             self._compare_cpu(None, source_cpu_info, instance)
6751         else:
6752             self._compare_cpu(instance.vcpu_model, None, instance)
6753 
6754         # Create file on storage, to be checked on source host
6755         filename = self._create_shared_storage_test_file(instance)
6756 
6757         data = objects.LibvirtLiveMigrateData()
6758         data.filename = filename
6759         data.image_type = CONF.libvirt.images_type
6760         data.graphics_listen_addr_vnc = CONF.vnc.server_listen
6761         data.graphics_listen_addr_spice = CONF.spice.server_listen
6762         if CONF.serial_console.enabled:
6763             data.serial_listen_addr = CONF.serial_console.proxyclient_address
6764         else:
6765             data.serial_listen_addr = None
6766         # Notes(eliqiao): block_migration and disk_over_commit are not
6767         # nullable, so just don't set them if they are None
6768         if block_migration is not None:
6769             data.block_migration = block_migration
6770         if disk_over_commit is not None:
6771             data.disk_over_commit = disk_over_commit
6772         data.disk_available_mb = disk_available_mb
6773         data.dst_wants_file_backed_memory = \
6774                 CONF.libvirt.file_backed_memory > 0
6775         data.file_backed_memory_discard = (CONF.libvirt.file_backed_memory and
6776             self._host.has_min_version(MIN_LIBVIRT_FILE_BACKED_DISCARD_VERSION,
6777                                        MIN_QEMU_FILE_BACKED_DISCARD_VERSION))
6778 
6779         return data
6780 
6781     def cleanup_live_migration_destination_check(self, context,
6782                                                  dest_check_data):
6783         """Do required cleanup on dest host after check_can_live_migrate calls
6784 
6785         :param context: security context
6786         """
6787         filename = dest_check_data.filename
6788         self._cleanup_shared_storage_test_file(filename)
6789 
6790     def check_can_live_migrate_source(self, context, instance,
6791                                       dest_check_data,
6792                                       block_device_info=None):
6793         """Check if it is possible to execute live migration.
6794 
6795         This checks if the live migration can succeed, based on the
6796         results from check_can_live_migrate_destination.
6797 
6798         :param context: security context
6799         :param instance: nova.db.sqlalchemy.models.Instance
6800         :param dest_check_data: result of check_can_live_migrate_destination
6801         :param block_device_info: result of _get_instance_block_device_info
6802         :returns: a LibvirtLiveMigrateData object
6803         """
6804         if not isinstance(dest_check_data, migrate_data_obj.LiveMigrateData):
6805             md_obj = objects.LibvirtLiveMigrateData()
6806             md_obj.from_legacy_dict(dest_check_data)
6807             dest_check_data = md_obj
6808 
6809         # Checking shared storage connectivity
6810         # if block migration, instances_path should not be on shared storage.
6811         source = CONF.host
6812 
6813         dest_check_data.is_shared_instance_path = (
6814             self._check_shared_storage_test_file(
6815                 dest_check_data.filename, instance))
6816 
6817         dest_check_data.is_shared_block_storage = (
6818             self._is_shared_block_storage(instance, dest_check_data,
6819                                           block_device_info))
6820 
6821         if 'block_migration' not in dest_check_data:
6822             dest_check_data.block_migration = (
6823                 not dest_check_data.is_on_shared_storage())
6824 
6825         if dest_check_data.block_migration:
6826             # TODO(eliqiao): Once block_migration flag is removed from the API
6827             # we can safely remove the if condition
6828             if dest_check_data.is_on_shared_storage():
6829                 reason = _("Block migration can not be used "
6830                            "with shared storage.")
6831                 raise exception.InvalidLocalStorage(reason=reason, path=source)
6832             if 'disk_over_commit' in dest_check_data:
6833                 self._assert_dest_node_has_enough_disk(context, instance,
6834                                         dest_check_data.disk_available_mb,
6835                                         dest_check_data.disk_over_commit,
6836                                         block_device_info)
6837             if block_device_info:
6838                 bdm = block_device_info.get('block_device_mapping')
6839                 # NOTE(eliqiao): Selective disk migrations are not supported
6840                 # with tunnelled block migrations so we can block them early.
6841                 if (bdm and
6842                     (self._block_migration_flags &
6843                      libvirt.VIR_MIGRATE_TUNNELLED != 0)):
6844                     msg = (_('Cannot block migrate instance %(uuid)s with'
6845                              ' mapped volumes. Selective block device'
6846                              ' migration is not supported with tunnelled'
6847                              ' block migrations.') % {'uuid': instance.uuid})
6848                     LOG.error(msg, instance=instance)
6849                     raise exception.MigrationPreCheckError(reason=msg)
6850         elif not (dest_check_data.is_shared_block_storage or
6851                   dest_check_data.is_shared_instance_path):
6852             reason = _("Shared storage live-migration requires either shared "
6853                        "storage or boot-from-volume with no local disks.")
6854             raise exception.InvalidSharedStorage(reason=reason, path=source)
6855 
6856         # NOTE(mikal): include the instance directory name here because it
6857         # doesn't yet exist on the destination but we want to force that
6858         # same name to be used
6859         instance_path = libvirt_utils.get_instance_path(instance,
6860                                                         relative=True)
6861         dest_check_data.instance_relative_path = instance_path
6862 
6863         # NOTE(lyarwood): Used to indicate to the dest that the src is capable
6864         # of wiring up the encrypted disk configuration for the domain.
6865         # Note that this does not require the QEMU and Libvirt versions to
6866         # decrypt LUKS to be installed on the source node. Only the Nova
6867         # utility code to generate the correct XML is required, so we can
6868         # default to True here for all computes >= Queens.
6869         dest_check_data.src_supports_native_luks = True
6870 
6871         return dest_check_data
6872 
6873     def _is_shared_block_storage(self, instance, dest_check_data,
6874                                  block_device_info=None):
6875         """Check if all block storage of an instance can be shared
6876         between source and destination of a live migration.
6877 
6878         Returns true if the instance is volume backed and has no local disks,
6879         or if the image backend is the same on source and destination and the
6880         backend shares block storage between compute nodes.
6881 
6882         :param instance: nova.objects.instance.Instance object
6883         :param dest_check_data: dict with boolean fields image_type,
6884                                 is_shared_instance_path, and is_volume_backed
6885         """
6886         if (dest_check_data.obj_attr_is_set('image_type') and
6887                 CONF.libvirt.images_type == dest_check_data.image_type and
6888                 self.image_backend.backend().is_shared_block_storage()):
6889             # NOTE(dgenin): currently true only for RBD image backend
6890             return True
6891 
6892         if (dest_check_data.is_shared_instance_path and
6893                 self.image_backend.backend().is_file_in_instance_path()):
6894             # NOTE(angdraug): file based image backends (Flat, Qcow2)
6895             # place block device files under the instance path
6896             return True
6897 
6898         if (dest_check_data.is_volume_backed and
6899                 not bool(self._get_instance_disk_info(instance,
6900                                                       block_device_info))):
6901             return True
6902 
6903         return False
6904 
6905     def _assert_dest_node_has_enough_disk(self, context, instance,
6906                                              available_mb, disk_over_commit,
6907                                              block_device_info):
6908         """Checks if destination has enough disk for block migration."""
6909         # Libvirt supports qcow2 disk format,which is usually compressed
6910         # on compute nodes.
6911         # Real disk image (compressed) may enlarged to "virtual disk size",
6912         # that is specified as the maximum disk size.
6913         # (See qemu-img -f path-to-disk)
6914         # Scheduler recognizes destination host still has enough disk space
6915         # if real disk size < available disk size
6916         # if disk_over_commit is True,
6917         #  otherwise virtual disk size < available disk size.
6918 
6919         available = 0
6920         if available_mb:
6921             available = available_mb * units.Mi
6922 
6923         disk_infos = self._get_instance_disk_info(instance, block_device_info)
6924 
6925         necessary = 0
6926         if disk_over_commit:
6927             for info in disk_infos:
6928                 necessary += int(info['disk_size'])
6929         else:
6930             for info in disk_infos:
6931                 necessary += int(info['virt_disk_size'])
6932 
6933         # Check that available disk > necessary disk
6934         if (available - necessary) < 0:
6935             reason = (_('Unable to migrate %(instance_uuid)s: '
6936                         'Disk of instance is too large(available'
6937                         ' on destination host:%(available)s '
6938                         '< need:%(necessary)s)') %
6939                       {'instance_uuid': instance.uuid,
6940                        'available': available,
6941                        'necessary': necessary})
6942             raise exception.MigrationPreCheckError(reason=reason)
6943 
6944     def _compare_cpu(self, guest_cpu, host_cpu_str, instance):
6945         """Check the host is compatible with the requested CPU
6946 
6947         :param guest_cpu: nova.objects.VirtCPUModel or None
6948         :param host_cpu_str: JSON from _get_cpu_info() method
6949 
6950         If the 'guest_cpu' parameter is not None, this will be
6951         validated for migration compatibility with the host.
6952         Otherwise the 'host_cpu_str' JSON string will be used for
6953         validation.
6954 
6955         :returns:
6956             None. if given cpu info is not compatible to this server,
6957             raise exception.
6958         """
6959 
6960         # NOTE(kchamart): Comparing host to guest CPU model for emulated
6961         # guests (<domain type='qemu'>) should not matter -- in this
6962         # mode (QEMU "TCG") the CPU is fully emulated in software and no
6963         # hardware acceleration, like KVM, is involved. So, skip the CPU
6964         # compatibility check for the QEMU domain type, and retain it for
6965         # KVM guests.
6966         if CONF.libvirt.virt_type not in ['kvm']:
6967             return
6968 
6969         if guest_cpu is None:
6970             info = jsonutils.loads(host_cpu_str)
6971             LOG.info('Instance launched has CPU info: %s', host_cpu_str)
6972             cpu = vconfig.LibvirtConfigCPU()
6973             cpu.arch = info['arch']
6974             cpu.model = info['model']
6975             cpu.vendor = info['vendor']
6976             cpu.sockets = info['topology']['sockets']
6977             cpu.cores = info['topology']['cores']
6978             cpu.threads = info['topology']['threads']
6979             for f in info['features']:
6980                 cpu.add_feature(vconfig.LibvirtConfigCPUFeature(f))
6981         else:
6982             cpu = self._vcpu_model_to_cpu_config(guest_cpu)
6983 
6984         u = ("http://libvirt.org/html/libvirt-libvirt-host.html#"
6985              "virCPUCompareResult")
6986         m = _("CPU doesn't have compatibility.\n\n%(ret)s\n\nRefer to %(u)s")
6987         # unknown character exists in xml, then libvirt complains
6988         try:
6989             cpu_xml = cpu.to_xml()
6990             LOG.debug("cpu compare xml: %s", cpu_xml, instance=instance)
6991             ret = self._host.compare_cpu(cpu_xml)
6992         except libvirt.libvirtError as e:
6993             error_code = e.get_error_code()
6994             if error_code == libvirt.VIR_ERR_NO_SUPPORT:
6995                 LOG.debug("URI %(uri)s does not support cpu comparison. "
6996                           "It will be proceeded though. Error: %(error)s",
6997                           {'uri': self._uri(), 'error': e})
6998                 return
6999             else:
7000                 LOG.error(m, {'ret': e, 'u': u})
7001                 raise exception.MigrationPreCheckError(
7002                     reason=m % {'ret': e, 'u': u})
7003 
7004         if ret <= 0:
7005             LOG.error(m, {'ret': ret, 'u': u})
7006             raise exception.InvalidCPUInfo(reason=m % {'ret': ret, 'u': u})
7007 
7008     def _create_shared_storage_test_file(self, instance):
7009         """Makes tmpfile under CONF.instances_path."""
7010         dirpath = CONF.instances_path
7011         fd, tmp_file = tempfile.mkstemp(dir=dirpath)
7012         LOG.debug("Creating tmpfile %s to notify to other "
7013                   "compute nodes that they should mount "
7014                   "the same storage.", tmp_file, instance=instance)
7015         os.close(fd)
7016         return os.path.basename(tmp_file)
7017 
7018     def _check_shared_storage_test_file(self, filename, instance):
7019         """Confirms existence of the tmpfile under CONF.instances_path.
7020 
7021         Cannot confirm tmpfile return False.
7022         """
7023         # NOTE(tpatzig): if instances_path is a shared volume that is
7024         # under heavy IO (many instances on many compute nodes),
7025         # then checking the existence of the testfile fails,
7026         # just because it takes longer until the client refreshes and new
7027         # content gets visible.
7028         # os.utime (like touch) on the directory forces the client to refresh.
7029         os.utime(CONF.instances_path, None)
7030 
7031         tmp_file = os.path.join(CONF.instances_path, filename)
7032         if not os.path.exists(tmp_file):
7033             exists = False
7034         else:
7035             exists = True
7036         LOG.debug('Check if temp file %s exists to indicate shared storage '
7037                   'is being used for migration. Exists? %s', tmp_file, exists,
7038                   instance=instance)
7039         return exists
7040 
7041     def _cleanup_shared_storage_test_file(self, filename):
7042         """Removes existence of the tmpfile under CONF.instances_path."""
7043         tmp_file = os.path.join(CONF.instances_path, filename)
7044         os.remove(tmp_file)
7045 
7046     def ensure_filtering_rules_for_instance(self, instance, network_info):
7047         """Ensure that an instance's filtering rules are enabled.
7048 
7049         When migrating an instance, we need the filtering rules to
7050         be configured on the destination host before starting the
7051         migration.
7052 
7053         Also, when restarting the compute service, we need to ensure
7054         that filtering rules exist for all running services.
7055         """
7056 
7057         self.firewall_driver.setup_basic_filtering(instance, network_info)
7058         self.firewall_driver.prepare_instance_filter(instance,
7059                 network_info)
7060 
7061         # nwfilters may be defined in a separate thread in the case
7062         # of libvirt non-blocking mode, so we wait for completion
7063         timeout_count = list(range(CONF.live_migration_retry_count))
7064         while timeout_count:
7065             if self.firewall_driver.instance_filter_exists(instance,
7066                                                            network_info):
7067                 break
7068             timeout_count.pop()
7069             if len(timeout_count) == 0:
7070                 msg = _('The firewall filter for %s does not exist')
7071                 raise exception.InternalError(msg % instance.name)
7072             greenthread.sleep(1)
7073 
7074     def filter_defer_apply_on(self):
7075         self.firewall_driver.filter_defer_apply_on()
7076 
7077     def filter_defer_apply_off(self):
7078         self.firewall_driver.filter_defer_apply_off()
7079 
7080     def live_migration(self, context, instance, dest,
7081                        post_method, recover_method, block_migration=False,
7082                        migrate_data=None):
7083         """Spawning live_migration operation for distributing high-load.
7084 
7085         :param context: security context
7086         :param instance:
7087             nova.db.sqlalchemy.models.Instance object
7088             instance object that is migrated.
7089         :param dest: destination host
7090         :param post_method:
7091             post operation method.
7092             expected nova.compute.manager._post_live_migration.
7093         :param recover_method:
7094             recovery method when any exception occurs.
7095             expected nova.compute.manager._rollback_live_migration.
7096         :param block_migration: if true, do block migration.
7097         :param migrate_data: a LibvirtLiveMigrateData object
7098 
7099         """
7100 
7101         # 'dest' will be substituted into 'migration_uri' so ensure
7102         # it does't contain any characters that could be used to
7103         # exploit the URI accepted by libivrt
7104         if not libvirt_utils.is_valid_hostname(dest):
7105             raise exception.InvalidHostname(hostname=dest)
7106 
7107         self._live_migration(context, instance, dest,
7108                              post_method, recover_method, block_migration,
7109                              migrate_data)
7110 
7111     def live_migration_abort(self, instance):
7112         """Aborting a running live-migration.
7113 
7114         :param instance: instance object that is in migration
7115 
7116         """
7117 
7118         guest = self._host.get_guest(instance)
7119         dom = guest._domain
7120 
7121         try:
7122             dom.abortJob()
7123         except libvirt.libvirtError as e:
7124             LOG.error("Failed to cancel migration %s",
7125                     encodeutils.exception_to_unicode(e), instance=instance)
7126             raise
7127 
7128     def _verify_serial_console_is_disabled(self):
7129         if CONF.serial_console.enabled:
7130 
7131             msg = _('Your destination node does not support'
7132                     ' retrieving listen addresses. In order'
7133                     ' for live migration to work properly you'
7134                     ' must disable serial console.')
7135             raise exception.MigrationError(reason=msg)
7136 
7137     def _live_migration_operation(self, context, instance, dest,
7138                                   block_migration, migrate_data, guest,
7139                                   device_names):
7140         """Invoke the live migration operation
7141 
7142         :param context: security context
7143         :param instance:
7144             nova.db.sqlalchemy.models.Instance object
7145             instance object that is migrated.
7146         :param dest: destination host
7147         :param block_migration: if true, do block migration.
7148         :param migrate_data: a LibvirtLiveMigrateData object
7149         :param guest: the guest domain object
7150         :param device_names: list of device names that are being migrated with
7151             instance
7152 
7153         This method is intended to be run in a background thread and will
7154         block that thread until the migration is finished or failed.
7155         """
7156         try:
7157             if migrate_data.block_migration:
7158                 migration_flags = self._block_migration_flags
7159             else:
7160                 migration_flags = self._live_migration_flags
7161 
7162             serial_listen_addr = libvirt_migrate.serial_listen_addr(
7163                 migrate_data)
7164             if not serial_listen_addr:
7165                 # In this context we want to ensure that serial console is
7166                 # disabled on source node. This is because nova couldn't
7167                 # retrieve serial listen address from destination node, so we
7168                 # consider that destination node might have serial console
7169                 # disabled as well.
7170                 self._verify_serial_console_is_disabled()
7171 
7172             # NOTE(aplanas) migrate_uri will have a value only in the
7173             # case that `live_migration_inbound_addr` parameter is
7174             # set, and we propose a non tunneled migration.
7175             migrate_uri = None
7176             if ('target_connect_addr' in migrate_data and
7177                     migrate_data.target_connect_addr is not None):
7178                 dest = migrate_data.target_connect_addr
7179                 if (migration_flags &
7180                     libvirt.VIR_MIGRATE_TUNNELLED == 0):
7181                     migrate_uri = self._migrate_uri(dest)
7182 
7183             new_xml_str = None
7184             if CONF.libvirt.virt_type != "parallels":
7185                 # If the migrate_data has port binding information for the
7186                 # destination host, we need to prepare the guest vif config
7187                 # for the destination before we start migrating the guest.
7188                 get_vif_config = None
7189                 if 'vifs' in migrate_data and migrate_data.vifs:
7190                     # NOTE(mriedem): The vif kwarg must be built on the fly
7191                     # within get_updated_guest_xml based on migrate_data.vifs.
7192                     # We could stash the virt_type from the destination host
7193                     # into LibvirtLiveMigrateData but the host kwarg is a
7194                     # nova.virt.libvirt.host.Host object and is used to check
7195                     # information like libvirt version on the destination.
7196                     # If this becomes a problem, what we could do is get the
7197                     # VIF configs while on the destination host during
7198                     # pre_live_migration() and store those in the
7199                     # LibvirtLiveMigrateData object. For now we just use the
7200                     # source host information for virt_type and
7201                     # host (version) since the conductor live_migrate method
7202                     # _check_compatible_with_source_hypervisor() ensures that
7203                     # the hypervisor types and versions are compatible.
7204                     get_vif_config = functools.partial(
7205                         self.vif_driver.get_config,
7206                         instance=instance,
7207                         image_meta=instance.image_meta,
7208                         inst_type=instance.flavor,
7209                         virt_type=CONF.libvirt.virt_type,
7210                         host=self._host)
7211                 new_xml_str = libvirt_migrate.get_updated_guest_xml(
7212                     # TODO(sahid): It's not a really good idea to pass
7213                     # the method _get_volume_config and we should to find
7214                     # a way to avoid this in future.
7215                     guest, migrate_data, self._get_volume_config,
7216                     get_vif_config=get_vif_config)
7217 
7218             # NOTE(pkoniszewski): Because of precheck which blocks
7219             # tunnelled block live migration with mapped volumes we
7220             # can safely remove migrate_disks when tunnelling is on.
7221             # Otherwise we will block all tunnelled block migrations,
7222             # even when an instance does not have volumes mapped.
7223             # This is because selective disk migration is not
7224             # supported in tunnelled block live migration. Also we
7225             # cannot fallback to migrateToURI2 in this case because of
7226             # bug #1398999
7227             #
7228             # TODO(kchamart) Move the following bit to guest.migrate()
7229             if (migration_flags & libvirt.VIR_MIGRATE_TUNNELLED != 0):
7230                 device_names = []
7231 
7232             # TODO(sahid): This should be in
7233             # post_live_migration_at_source but no way to retrieve
7234             # ports acquired on the host for the guest at this
7235             # step. Since the domain is going to be removed from
7236             # libvird on source host after migration, we backup the
7237             # serial ports to release them if all went well.
7238             serial_ports = []
7239             if CONF.serial_console.enabled:
7240                 serial_ports = list(self._get_serial_ports_from_guest(guest))
7241 
7242             LOG.debug("About to invoke the migrate API", instance=instance)
7243             guest.migrate(self._live_migration_uri(dest),
7244                           migrate_uri=migrate_uri,
7245                           flags=migration_flags,
7246                           migrate_disks=device_names,
7247                           destination_xml=new_xml_str,
7248                           bandwidth=CONF.libvirt.live_migration_bandwidth)
7249             LOG.debug("Migrate API has completed", instance=instance)
7250 
7251             for hostname, port in serial_ports:
7252                 serial_console.release_port(host=hostname, port=port)
7253         except Exception as e:
7254             with excutils.save_and_reraise_exception():
7255                 LOG.error("Live Migration failure: %s", e, instance=instance)
7256 
7257         # If 'migrateToURI' fails we don't know what state the
7258         # VM instances on each host are in. Possibilities include
7259         #
7260         #  1. src==running, dst==none
7261         #
7262         #     Migration failed & rolled back, or never started
7263         #
7264         #  2. src==running, dst==paused
7265         #
7266         #     Migration started but is still ongoing
7267         #
7268         #  3. src==paused,  dst==paused
7269         #
7270         #     Migration data transfer completed, but switchover
7271         #     is still ongoing, or failed
7272         #
7273         #  4. src==paused,  dst==running
7274         #
7275         #     Migration data transfer completed, switchover
7276         #     happened but cleanup on source failed
7277         #
7278         #  5. src==none,    dst==running
7279         #
7280         #     Migration fully succeeded.
7281         #
7282         # Libvirt will aim to complete any migration operation
7283         # or roll it back. So even if the migrateToURI call has
7284         # returned an error, if the migration was not finished
7285         # libvirt should clean up.
7286         #
7287         # So we take the error raise here with a pinch of salt
7288         # and rely on the domain job info status to figure out
7289         # what really happened to the VM, which is a much more
7290         # reliable indicator.
7291         #
7292         # In particular we need to try very hard to ensure that
7293         # Nova does not "forget" about the guest. ie leaving it
7294         # running on a different host to the one recorded in
7295         # the database, as that would be a serious resource leak
7296 
7297         LOG.debug("Migration operation thread has finished",
7298                   instance=instance)
7299 
7300     def _live_migration_copy_disk_paths(self, context, instance, guest):
7301         '''Get list of disks to copy during migration
7302 
7303         :param context: security context
7304         :param instance: the instance being migrated
7305         :param guest: the Guest instance being migrated
7306 
7307         Get the list of disks to copy during migration.
7308 
7309         :returns: a list of local source paths and a list of device names to
7310             copy
7311         '''
7312 
7313         disk_paths = []
7314         device_names = []
7315         block_devices = []
7316 
7317         if (self._block_migration_flags &
7318                 libvirt.VIR_MIGRATE_TUNNELLED == 0):
7319             bdm_list = objects.BlockDeviceMappingList.get_by_instance_uuid(
7320                 context, instance.uuid)
7321             block_device_info = driver.get_block_device_info(instance,
7322                                                              bdm_list)
7323 
7324             block_device_mappings = driver.block_device_info_get_mapping(
7325                 block_device_info)
7326             for bdm in block_device_mappings:
7327                 device_name = str(bdm['mount_device'].rsplit('/', 1)[1])
7328                 block_devices.append(device_name)
7329 
7330         for dev in guest.get_all_disks():
7331             if dev.readonly or dev.shareable:
7332                 continue
7333             if dev.source_type not in ["file", "block"]:
7334                 continue
7335             if dev.target_dev in block_devices:
7336                 continue
7337             disk_paths.append(dev.source_path)
7338             device_names.append(dev.target_dev)
7339         return (disk_paths, device_names)
7340 
7341     def _live_migration_data_gb(self, instance, disk_paths):
7342         '''Calculate total amount of data to be transferred
7343 
7344         :param instance: the nova.objects.Instance being migrated
7345         :param disk_paths: list of disk paths that are being migrated
7346         with instance
7347 
7348         Calculates the total amount of data that needs to be
7349         transferred during the live migration. The actual
7350         amount copied will be larger than this, due to the
7351         guest OS continuing to dirty RAM while the migration
7352         is taking place. So this value represents the minimal
7353         data size possible.
7354 
7355         :returns: data size to be copied in GB
7356         '''
7357 
7358         ram_gb = instance.flavor.memory_mb * units.Mi / units.Gi
7359         if ram_gb < 2:
7360             ram_gb = 2
7361 
7362         disk_gb = 0
7363         for path in disk_paths:
7364             try:
7365                 size = os.stat(path).st_size
7366                 size_gb = (size / units.Gi)
7367                 if size_gb < 2:
7368                     size_gb = 2
7369                 disk_gb += size_gb
7370             except OSError as e:
7371                 LOG.warning("Unable to stat %(disk)s: %(ex)s",
7372                             {'disk': path, 'ex': e})
7373                 # Ignore error since we don't want to break
7374                 # the migration monitoring thread operation
7375 
7376         return ram_gb + disk_gb
7377 
7378     def _get_migration_flags(self, is_block_migration):
7379         if is_block_migration:
7380             return self._block_migration_flags
7381         return self._live_migration_flags
7382 
7383     def _live_migration_monitor(self, context, instance, guest,
7384                                 dest, post_method,
7385                                 recover_method, block_migration,
7386                                 migrate_data, finish_event,
7387                                 disk_paths):
7388         on_migration_failure = deque()
7389         data_gb = self._live_migration_data_gb(instance, disk_paths)
7390         downtime_steps = list(libvirt_migrate.downtime_steps(data_gb))
7391         migration = migrate_data.migration
7392         curdowntime = None
7393 
7394         migration_flags = self._get_migration_flags(
7395                                   migrate_data.block_migration)
7396 
7397         n = 0
7398         start = time.time()
7399         is_post_copy_enabled = self._is_post_copy_enabled(migration_flags)
7400         while True:
7401             info = guest.get_job_info()
7402 
7403             if info.type == libvirt.VIR_DOMAIN_JOB_NONE:
7404                 # Either still running, or failed or completed,
7405                 # lets untangle the mess
7406                 if not finish_event.ready():
7407                     LOG.debug("Operation thread is still running",
7408                               instance=instance)
7409                 else:
7410                     info.type = libvirt_migrate.find_job_type(guest, instance)
7411                     LOG.debug("Fixed incorrect job type to be %d",
7412                               info.type, instance=instance)
7413 
7414             if info.type == libvirt.VIR_DOMAIN_JOB_NONE:
7415                 # Migration is not yet started
7416                 LOG.debug("Migration not running yet",
7417                           instance=instance)
7418             elif info.type == libvirt.VIR_DOMAIN_JOB_UNBOUNDED:
7419                 # Migration is still running
7420                 #
7421                 # This is where we wire up calls to change live
7422                 # migration status. eg change max downtime, cancel
7423                 # the operation, change max bandwidth
7424                 libvirt_migrate.run_tasks(guest, instance,
7425                                           self.active_migrations,
7426                                           on_migration_failure,
7427                                           migration,
7428                                           is_post_copy_enabled)
7429 
7430                 now = time.time()
7431                 elapsed = now - start
7432 
7433                 completion_timeout = int(
7434                     CONF.libvirt.live_migration_completion_timeout * data_gb)
7435                 # NOTE(yikun): Check the completion timeout to determine
7436                 # should trigger the timeout action, and there are two choices
7437                 # ``abort`` (default) or ``force_complete``. If the action is
7438                 # set to ``force_complete``, the post-copy will be triggered
7439                 # if available else the VM will be suspended, otherwise the
7440                 # live migrate operation will be aborted.
7441                 if libvirt_migrate.should_trigger_timeout_action(
7442                         instance, elapsed, completion_timeout,
7443                         migration.status):
7444                     timeout_act = CONF.libvirt.live_migration_timeout_action
7445                     if timeout_act == 'force_complete':
7446                         self.live_migration_force_complete(instance)
7447                     else:
7448                         # timeout action is 'abort'
7449                         try:
7450                             guest.abort_job()
7451                         except libvirt.libvirtError as e:
7452                             LOG.warning("Failed to abort migration %s",
7453                                     encodeutils.exception_to_unicode(e),
7454                                     instance=instance)
7455                             self._clear_empty_migration(instance)
7456                             raise
7457 
7458                 curdowntime = libvirt_migrate.update_downtime(
7459                     guest, instance, curdowntime,
7460                     downtime_steps, elapsed)
7461 
7462                 # We loop every 500ms, so don't log on every
7463                 # iteration to avoid spamming logs for long
7464                 # running migrations. Just once every 5 secs
7465                 # is sufficient for developers to debug problems.
7466                 # We log once every 30 seconds at info to help
7467                 # admins see slow running migration operations
7468                 # when debug logs are off.
7469                 if (n % 10) == 0:
7470                     # Ignoring memory_processed, as due to repeated
7471                     # dirtying of data, this can be way larger than
7472                     # memory_total. Best to just look at what's
7473                     # remaining to copy and ignore what's done already
7474                     #
7475                     # TODO(berrange) perhaps we could include disk
7476                     # transfer stats in the progress too, but it
7477                     # might make memory info more obscure as large
7478                     # disk sizes might dwarf memory size
7479                     remaining = 100
7480                     if info.memory_total != 0:
7481                         remaining = round(info.memory_remaining *
7482                                           100 / info.memory_total)
7483 
7484                     libvirt_migrate.save_stats(instance, migration,
7485                                                info, remaining)
7486 
7487                     lg = LOG.debug
7488                     if (n % 60) == 0:
7489                         lg = LOG.info
7490 
7491                     lg("Migration running for %(secs)d secs, "
7492                        "memory %(remaining)d%% remaining; "
7493                        "(bytes processed=%(processed_memory)d, "
7494                        "remaining=%(remaining_memory)d, "
7495                        "total=%(total_memory)d)",
7496                        {"secs": n / 2, "remaining": remaining,
7497                         "processed_memory": info.memory_processed,
7498                         "remaining_memory": info.memory_remaining,
7499                         "total_memory": info.memory_total}, instance=instance)
7500 
7501                 n = n + 1
7502             elif info.type == libvirt.VIR_DOMAIN_JOB_COMPLETED:
7503                 # Migration is all done
7504                 LOG.info("Migration operation has completed",
7505                          instance=instance)
7506                 post_method(context, instance, dest, block_migration,
7507                             migrate_data)
7508                 break
7509             elif info.type == libvirt.VIR_DOMAIN_JOB_FAILED:
7510                 # Migration did not succeed
7511                 LOG.error("Migration operation has aborted", instance=instance)
7512                 libvirt_migrate.run_recover_tasks(self._host, guest, instance,
7513                                                   on_migration_failure)
7514                 recover_method(context, instance, dest, migrate_data)
7515                 break
7516             elif info.type == libvirt.VIR_DOMAIN_JOB_CANCELLED:
7517                 # Migration was stopped by admin
7518                 LOG.warning("Migration operation was cancelled",
7519                             instance=instance)
7520                 libvirt_migrate.run_recover_tasks(self._host, guest, instance,
7521                                                   on_migration_failure)
7522                 recover_method(context, instance, dest, migrate_data,
7523                                migration_status='cancelled')
7524                 break
7525             else:
7526                 LOG.warning("Unexpected migration job type: %d",
7527                             info.type, instance=instance)
7528 
7529             time.sleep(0.5)
7530         self._clear_empty_migration(instance)
7531 
7532     def _clear_empty_migration(self, instance):
7533         try:
7534             del self.active_migrations[instance.uuid]
7535         except KeyError:
7536             LOG.warning("There are no records in active migrations "
7537                         "for instance", instance=instance)
7538 
7539     def _live_migration(self, context, instance, dest, post_method,
7540                         recover_method, block_migration,
7541                         migrate_data):
7542         """Do live migration.
7543 
7544         :param context: security context
7545         :param instance:
7546             nova.db.sqlalchemy.models.Instance object
7547             instance object that is migrated.
7548         :param dest: destination host
7549         :param post_method:
7550             post operation method.
7551             expected nova.compute.manager._post_live_migration.
7552         :param recover_method:
7553             recovery method when any exception occurs.
7554             expected nova.compute.manager._rollback_live_migration.
7555         :param block_migration: if true, do block migration.
7556         :param migrate_data: a LibvirtLiveMigrateData object
7557 
7558         This fires off a new thread to run the blocking migration
7559         operation, and then this thread monitors the progress of
7560         migration and controls its operation
7561         """
7562 
7563         guest = self._host.get_guest(instance)
7564 
7565         disk_paths = []
7566         device_names = []
7567         if (migrate_data.block_migration and
7568                 CONF.libvirt.virt_type != "parallels"):
7569             disk_paths, device_names = self._live_migration_copy_disk_paths(
7570                 context, instance, guest)
7571 
7572         opthread = utils.spawn(self._live_migration_operation,
7573                                      context, instance, dest,
7574                                      block_migration,
7575                                      migrate_data, guest,
7576                                      device_names)
7577 
7578         finish_event = eventlet.event.Event()
7579         self.active_migrations[instance.uuid] = deque()
7580 
7581         def thread_finished(thread, event):
7582             LOG.debug("Migration operation thread notification",
7583                       instance=instance)
7584             event.send()
7585         opthread.link(thread_finished, finish_event)
7586 
7587         # Let eventlet schedule the new thread right away
7588         time.sleep(0)
7589 
7590         try:
7591             LOG.debug("Starting monitoring of live migration",
7592                       instance=instance)
7593             self._live_migration_monitor(context, instance, guest, dest,
7594                                          post_method, recover_method,
7595                                          block_migration, migrate_data,
7596                                          finish_event, disk_paths)
7597         except Exception as ex:
7598             LOG.warning("Error monitoring migration: %(ex)s",
7599                         {"ex": ex}, instance=instance, exc_info=True)
7600             raise
7601         finally:
7602             LOG.debug("Live migration monitoring is all done",
7603                       instance=instance)
7604 
7605     def _is_post_copy_enabled(self, migration_flags):
7606         if self._is_post_copy_available():
7607             if (migration_flags & libvirt.VIR_MIGRATE_POSTCOPY) != 0:
7608                 return True
7609         return False
7610 
7611     def live_migration_force_complete(self, instance):
7612         try:
7613             self.active_migrations[instance.uuid].append('force-complete')
7614         except KeyError:
7615             raise exception.NoActiveMigrationForInstance(
7616                 instance_id=instance.uuid)
7617 
7618     def _try_fetch_image(self, context, path, image_id, instance,
7619                          fallback_from_host=None):
7620         try:
7621             libvirt_utils.fetch_image(context, path, image_id,
7622                                       instance.trusted_certs)
7623         except exception.ImageNotFound:
7624             if not fallback_from_host:
7625                 raise
7626             LOG.debug("Image %(image_id)s doesn't exist anymore on "
7627                       "image service, attempting to copy image "
7628                       "from %(host)s",
7629                       {'image_id': image_id, 'host': fallback_from_host})
7630             libvirt_utils.copy_image(src=path, dest=path,
7631                                      host=fallback_from_host,
7632                                      receive=True)
7633 
7634     def _fetch_instance_kernel_ramdisk(self, context, instance,
7635                                        fallback_from_host=None):
7636         """Download kernel and ramdisk for instance in instance directory."""
7637         instance_dir = libvirt_utils.get_instance_path(instance)
7638         if instance.kernel_id:
7639             kernel_path = os.path.join(instance_dir, 'kernel')
7640             # NOTE(dsanders): only fetch image if it's not available at
7641             # kernel_path. This also avoids ImageNotFound exception if
7642             # the image has been deleted from glance
7643             if not os.path.exists(kernel_path):
7644                 self._try_fetch_image(context,
7645                                       kernel_path,
7646                                       instance.kernel_id,
7647                                       instance, fallback_from_host)
7648             if instance.ramdisk_id:
7649                 ramdisk_path = os.path.join(instance_dir, 'ramdisk')
7650                 # NOTE(dsanders): only fetch image if it's not available at
7651                 # ramdisk_path. This also avoids ImageNotFound exception if
7652                 # the image has been deleted from glance
7653                 if not os.path.exists(ramdisk_path):
7654                     self._try_fetch_image(context,
7655                                           ramdisk_path,
7656                                           instance.ramdisk_id,
7657                                           instance, fallback_from_host)
7658 
7659     def rollback_live_migration_at_destination(self, context, instance,
7660                                                network_info,
7661                                                block_device_info,
7662                                                destroy_disks=True,
7663                                                migrate_data=None):
7664         """Clean up destination node after a failed live migration."""
7665         try:
7666             self.destroy(context, instance, network_info, block_device_info,
7667                          destroy_disks)
7668         finally:
7669             # NOTE(gcb): Failed block live migration may leave instance
7670             # directory at destination node, ensure it is always deleted.
7671             is_shared_instance_path = True
7672             if migrate_data:
7673                 is_shared_instance_path = migrate_data.is_shared_instance_path
7674                 if (migrate_data.obj_attr_is_set("serial_listen_ports")
7675                     and migrate_data.serial_listen_ports):
7676                     # Releases serial ports reserved.
7677                     for port in migrate_data.serial_listen_ports:
7678                         serial_console.release_port(
7679                             host=migrate_data.serial_listen_addr, port=port)
7680 
7681             if not is_shared_instance_path:
7682                 instance_dir = libvirt_utils.get_instance_path_at_destination(
7683                     instance, migrate_data)
7684                 if os.path.exists(instance_dir):
7685                     shutil.rmtree(instance_dir)
7686 
7687     def _pre_live_migration_plug_vifs(self, instance, network_info,
7688                                       migrate_data):
7689         if 'vifs' in migrate_data and migrate_data.vifs:
7690             LOG.debug('Plugging VIFs using destination host port bindings '
7691                       'before live migration.', instance=instance)
7692             vif_plug_nw_info = network_model.NetworkInfo([])
7693             for migrate_vif in migrate_data.vifs:
7694                 vif_plug_nw_info.append(migrate_vif.get_dest_vif())
7695         else:
7696             LOG.debug('Plugging VIFs before live migration.',
7697                       instance=instance)
7698             vif_plug_nw_info = network_info
7699         # Retry operation is necessary because continuous live migration
7700         # requests to the same host cause concurrent requests to iptables,
7701         # then it complains.
7702         max_retry = CONF.live_migration_retry_count
7703         for cnt in range(max_retry):
7704             try:
7705                 self.plug_vifs(instance, vif_plug_nw_info)
7706                 break
7707             except processutils.ProcessExecutionError:
7708                 if cnt == max_retry - 1:
7709                     raise
7710                 else:
7711                     LOG.warning('plug_vifs() failed %(cnt)d. Retry up to '
7712                                 '%(max_retry)d.',
7713                                 {'cnt': cnt, 'max_retry': max_retry},
7714                                 instance=instance)
7715                     greenthread.sleep(1)
7716 
7717     def pre_live_migration(self, context, instance, block_device_info,
7718                            network_info, disk_info, migrate_data):
7719         """Preparation live migration."""
7720         if disk_info is not None:
7721             disk_info = jsonutils.loads(disk_info)
7722 
7723         LOG.debug('migrate_data in pre_live_migration: %s', migrate_data,
7724                   instance=instance)
7725         is_shared_block_storage = migrate_data.is_shared_block_storage
7726         is_shared_instance_path = migrate_data.is_shared_instance_path
7727         is_block_migration = migrate_data.block_migration
7728 
7729         if not is_shared_instance_path:
7730             instance_dir = libvirt_utils.get_instance_path_at_destination(
7731                             instance, migrate_data)
7732 
7733             if os.path.exists(instance_dir):
7734                 raise exception.DestinationDiskExists(path=instance_dir)
7735 
7736             LOG.debug('Creating instance directory: %s', instance_dir,
7737                       instance=instance)
7738             os.mkdir(instance_dir)
7739 
7740             # Recreate the disk.info file and in doing so stop the
7741             # imagebackend from recreating it incorrectly by inspecting the
7742             # contents of each file when using the Raw backend.
7743             if disk_info:
7744                 image_disk_info = {}
7745                 for info in disk_info:
7746                     image_file = os.path.basename(info['path'])
7747                     image_path = os.path.join(instance_dir, image_file)
7748                     image_disk_info[image_path] = info['type']
7749 
7750                 LOG.debug('Creating disk.info with the contents: %s',
7751                           image_disk_info, instance=instance)
7752 
7753                 image_disk_info_path = os.path.join(instance_dir,
7754                                                     'disk.info')
7755                 libvirt_utils.write_to_file(image_disk_info_path,
7756                                             jsonutils.dumps(image_disk_info))
7757 
7758             if not is_shared_block_storage:
7759                 # Ensure images and backing files are present.
7760                 LOG.debug('Checking to make sure images and backing files are '
7761                           'present before live migration.', instance=instance)
7762                 self._create_images_and_backing(
7763                     context, instance, instance_dir, disk_info,
7764                     fallback_from_host=instance.host)
7765                 if (configdrive.required_by(instance) and
7766                         CONF.config_drive_format == 'iso9660'):
7767                     # NOTE(pkoniszewski): Due to a bug in libvirt iso config
7768                     # drive needs to be copied to destination prior to
7769                     # migration when instance path is not shared and block
7770                     # storage is not shared. Files that are already present
7771                     # on destination are excluded from a list of files that
7772                     # need to be copied to destination. If we don't do that
7773                     # live migration will fail on copying iso config drive to
7774                     # destination and writing to read-only device.
7775                     # Please see bug/1246201 for more details.
7776                     src = "%s:%s/disk.config" % (instance.host, instance_dir)
7777                     self._remotefs.copy_file(src, instance_dir)
7778 
7779             if not is_block_migration:
7780                 # NOTE(angdraug): when block storage is shared between source
7781                 # and destination and instance path isn't (e.g. volume backed
7782                 # or rbd backed instance), instance path on destination has to
7783                 # be prepared
7784 
7785                 # Required by Quobyte CI
7786                 self._ensure_console_log_for_instance(instance)
7787 
7788                 # if image has kernel and ramdisk, just download
7789                 # following normal way.
7790                 self._fetch_instance_kernel_ramdisk(context, instance)
7791 
7792         # Establishing connection to volume server.
7793         block_device_mapping = driver.block_device_info_get_mapping(
7794             block_device_info)
7795 
7796         if len(block_device_mapping):
7797             LOG.debug('Connecting volumes before live migration.',
7798                       instance=instance)
7799 
7800         for bdm in block_device_mapping:
7801             connection_info = bdm['connection_info']
7802             # NOTE(lyarwood): Handle the P to Q LM during upgrade use case
7803             # where an instance has encrypted volumes attached using the
7804             # os-brick encryptors. Do not attempt to attach the encrypted
7805             # volume using native LUKS decryption on the destionation.
7806             src_native_luks = False
7807             if migrate_data.obj_attr_is_set('src_supports_native_luks'):
7808                 src_native_luks = migrate_data.src_supports_native_luks
7809             dest_native_luks = self._is_native_luks_available()
7810             allow_native_luks = src_native_luks and dest_native_luks
7811             self._connect_volume(context, connection_info, instance,
7812                                  allow_native_luks=allow_native_luks)
7813 
7814         self._pre_live_migration_plug_vifs(
7815             instance, network_info, migrate_data)
7816 
7817         # Store server_listen and latest disk device info
7818         if not migrate_data:
7819             migrate_data = objects.LibvirtLiveMigrateData(bdms=[])
7820         else:
7821             migrate_data.bdms = []
7822         # Store live_migration_inbound_addr
7823         migrate_data.target_connect_addr = \
7824             CONF.libvirt.live_migration_inbound_addr
7825         migrate_data.supported_perf_events = self._supported_perf_events
7826 
7827         migrate_data.serial_listen_ports = []
7828         if CONF.serial_console.enabled:
7829             num_ports = hardware.get_number_of_serial_ports(
7830                 instance.flavor, instance.image_meta)
7831             for port in six.moves.range(num_ports):
7832                 migrate_data.serial_listen_ports.append(
7833                     serial_console.acquire_port(
7834                         migrate_data.serial_listen_addr))
7835 
7836         for vol in block_device_mapping:
7837             connection_info = vol['connection_info']
7838             if connection_info.get('serial'):
7839                 disk_info = blockinfo.get_info_from_bdm(
7840                     instance, CONF.libvirt.virt_type,
7841                     instance.image_meta, vol)
7842 
7843                 bdmi = objects.LibvirtLiveMigrateBDMInfo()
7844                 bdmi.serial = connection_info['serial']
7845                 bdmi.connection_info = connection_info
7846                 bdmi.bus = disk_info['bus']
7847                 bdmi.dev = disk_info['dev']
7848                 bdmi.type = disk_info['type']
7849                 bdmi.format = disk_info.get('format')
7850                 bdmi.boot_index = disk_info.get('boot_index')
7851                 volume_secret = self._host.find_secret('volume', vol.volume_id)
7852                 if volume_secret:
7853                     bdmi.encryption_secret_uuid = volume_secret.UUIDString()
7854 
7855                 migrate_data.bdms.append(bdmi)
7856 
7857         return migrate_data
7858 
7859     def _try_fetch_image_cache(self, image, fetch_func, context, filename,
7860                                image_id, instance, size,
7861                                fallback_from_host=None):
7862         try:
7863             image.cache(fetch_func=fetch_func,
7864                         context=context,
7865                         filename=filename,
7866                         image_id=image_id,
7867                         size=size,
7868                         trusted_certs=instance.trusted_certs)
7869         except exception.ImageNotFound:
7870             if not fallback_from_host:
7871                 raise
7872             LOG.debug("Image %(image_id)s doesn't exist anymore "
7873                       "on image service, attempting to copy "
7874                       "image from %(host)s",
7875                       {'image_id': image_id, 'host': fallback_from_host},
7876                       instance=instance)
7877 
7878             def copy_from_host(target):
7879                 libvirt_utils.copy_image(src=target,
7880                                          dest=target,
7881                                          host=fallback_from_host,
7882                                          receive=True)
7883             image.cache(fetch_func=copy_from_host,
7884                         filename=filename)
7885 
7886     def _create_images_and_backing(self, context, instance, instance_dir,
7887                                    disk_info, fallback_from_host=None):
7888         """:param context: security context
7889            :param instance:
7890                nova.db.sqlalchemy.models.Instance object
7891                instance object that is migrated.
7892            :param instance_dir:
7893                instance path to use, calculated externally to handle block
7894                migrating an instance with an old style instance path
7895            :param disk_info:
7896                disk info specified in _get_instance_disk_info_from_config
7897                (list of dicts)
7898            :param fallback_from_host:
7899                host where we can retrieve images if the glance images are
7900                not available.
7901 
7902         """
7903 
7904         # Virtuozzo containers don't use backing file
7905         if (CONF.libvirt.virt_type == "parallels" and
7906                 instance.vm_mode == fields.VMMode.EXE):
7907             return
7908 
7909         if not disk_info:
7910             disk_info = []
7911 
7912         for info in disk_info:
7913             base = os.path.basename(info['path'])
7914             # Get image type and create empty disk image, and
7915             # create backing file in case of qcow2.
7916             instance_disk = os.path.join(instance_dir, base)
7917             if not info['backing_file'] and not os.path.exists(instance_disk):
7918                 libvirt_utils.create_image(info['type'], instance_disk,
7919                                            info['virt_disk_size'])
7920             elif info['backing_file']:
7921                 # Creating backing file follows same way as spawning instances.
7922                 cache_name = os.path.basename(info['backing_file'])
7923 
7924                 disk = self.image_backend.by_name(instance, instance_disk,
7925                                                   CONF.libvirt.images_type)
7926                 if cache_name.startswith('ephemeral'):
7927                     # The argument 'size' is used by image.cache to
7928                     # validate disk size retrieved from cache against
7929                     # the instance disk size (should always return OK)
7930                     # and ephemeral_size is used by _create_ephemeral
7931                     # to build the image if the disk is not already
7932                     # cached.
7933                     disk.cache(
7934                         fetch_func=self._create_ephemeral,
7935                         fs_label=cache_name,
7936                         os_type=instance.os_type,
7937                         filename=cache_name,
7938                         size=info['virt_disk_size'],
7939                         ephemeral_size=info['virt_disk_size'] / units.Gi)
7940                 elif cache_name.startswith('swap'):
7941                     inst_type = instance.get_flavor()
7942                     swap_mb = inst_type.swap
7943                     disk.cache(fetch_func=self._create_swap,
7944                                 filename="swap_%s" % swap_mb,
7945                                 size=swap_mb * units.Mi,
7946                                 swap_mb=swap_mb)
7947                 else:
7948                     self._try_fetch_image_cache(disk,
7949                                                 libvirt_utils.fetch_image,
7950                                                 context, cache_name,
7951                                                 instance.image_ref,
7952                                                 instance,
7953                                                 info['virt_disk_size'],
7954                                                 fallback_from_host)
7955 
7956         # if disk has kernel and ramdisk, just download
7957         # following normal way.
7958         self._fetch_instance_kernel_ramdisk(
7959             context, instance, fallback_from_host=fallback_from_host)
7960 
7961     def post_live_migration(self, context, instance, block_device_info,
7962                             migrate_data=None):
7963         # Disconnect from volume server
7964         block_device_mapping = driver.block_device_info_get_mapping(
7965                 block_device_info)
7966         volume_api = self._volume_api
7967         for vol in block_device_mapping:
7968             volume_id = vol['connection_info']['serial']
7969             if vol['attachment_id'] is None:
7970                 # Cinder v2 api flow: Retrieve connection info from Cinder's
7971                 # initialize_connection API. The info returned will be
7972                 # accurate for the source server.
7973                 connector = self.get_volume_connector(instance)
7974                 connection_info = volume_api.initialize_connection(
7975                     context, volume_id, connector)
7976             else:
7977                 # cinder v3.44 api flow: Retrieve the connection_info for
7978                 # the old attachment from cinder.
7979                 old_attachment_id = \
7980                     migrate_data.old_vol_attachment_ids[volume_id]
7981                 old_attachment = volume_api.attachment_get(
7982                     context, old_attachment_id)
7983                 connection_info = old_attachment['connection_info']
7984 
7985             # TODO(leeantho) The following multipath_id logic is temporary
7986             # and will be removed in the future once os-brick is updated
7987             # to handle multipath for drivers in a more efficient way.
7988             # For now this logic is needed to ensure the connection info
7989             # data is correct.
7990 
7991             # Pull out multipath_id from the bdm information. The
7992             # multipath_id can be placed into the connection info
7993             # because it is based off of the volume and will be the
7994             # same on the source and destination hosts.
7995             if 'multipath_id' in vol['connection_info']['data']:
7996                 multipath_id = vol['connection_info']['data']['multipath_id']
7997                 connection_info['data']['multipath_id'] = multipath_id
7998 
7999             self._disconnect_volume(context, connection_info, instance)
8000 
8001     def post_live_migration_at_source(self, context, instance, network_info):
8002         """Unplug VIFs from networks at source.
8003 
8004         :param context: security context
8005         :param instance: instance object reference
8006         :param network_info: instance network information
8007         """
8008         self.unplug_vifs(instance, network_info)
8009 
8010     def post_live_migration_at_destination(self, context,
8011                                            instance,
8012                                            network_info,
8013                                            block_migration=False,
8014                                            block_device_info=None):
8015         """Post operation of live migration at destination host.
8016 
8017         :param context: security context
8018         :param instance:
8019             nova.db.sqlalchemy.models.Instance object
8020             instance object that is migrated.
8021         :param network_info: instance network information
8022         :param block_migration: if true, post operation of block_migration.
8023         """
8024         # The source node set the VIR_MIGRATE_PERSIST_DEST flag when live
8025         # migrating so the guest xml should already be persisted on the
8026         # destination host, so just perform a sanity check to make sure it
8027         # made it as expected.
8028         self._host.get_guest(instance)
8029 
8030     def _get_instance_disk_info_from_config(self, guest_config,
8031                                             block_device_info):
8032         """Get the non-volume disk information from the domain xml
8033 
8034         :param LibvirtConfigGuest guest_config: the libvirt domain config
8035                                                 for the instance
8036         :param dict block_device_info: block device info for BDMs
8037         :returns disk_info: list of dicts with keys:
8038 
8039           * 'type': the disk type (str)
8040           * 'path': the disk path (str)
8041           * 'virt_disk_size': the virtual disk size (int)
8042           * 'backing_file': backing file of a disk image (str)
8043           * 'disk_size': physical disk size (int)
8044           * 'over_committed_disk_size': virt_disk_size - disk_size or 0
8045         """
8046         block_device_mapping = driver.block_device_info_get_mapping(
8047             block_device_info)
8048 
8049         volume_devices = set()
8050         for vol in block_device_mapping:
8051             disk_dev = vol['mount_device'].rpartition("/")[2]
8052             volume_devices.add(disk_dev)
8053 
8054         disk_info = []
8055 
8056         if (guest_config.virt_type == 'parallels' and
8057                 guest_config.os_type == fields.VMMode.EXE):
8058             node_type = 'filesystem'
8059         else:
8060             node_type = 'disk'
8061 
8062         for device in guest_config.devices:
8063             if device.root_name != node_type:
8064                 continue
8065             disk_type = device.source_type
8066             if device.root_name == 'filesystem':
8067                 target = device.target_dir
8068                 if device.source_type == 'file':
8069                     path = device.source_file
8070                 elif device.source_type == 'block':
8071                     path = device.source_dev
8072                 else:
8073                     path = None
8074             else:
8075                 target = device.target_dev
8076                 path = device.source_path
8077 
8078             if not path:
8079                 LOG.debug('skipping disk for %s as it does not have a path',
8080                           guest_config.name)
8081                 continue
8082 
8083             if disk_type not in ['file', 'block']:
8084                 LOG.debug('skipping disk because it looks like a volume', path)
8085                 continue
8086 
8087             if target in volume_devices:
8088                 LOG.debug('skipping disk %(path)s (%(target)s) as it is a '
8089                           'volume', {'path': path, 'target': target})
8090                 continue
8091 
8092             if device.root_name == 'filesystem':
8093                 driver_type = device.driver_type
8094             else:
8095                 driver_type = device.driver_format
8096             # get the real disk size or
8097             # raise a localized error if image is unavailable
8098             if disk_type == 'file' and driver_type == 'ploop':
8099                 dk_size = 0
8100                 for dirpath, dirnames, filenames in os.walk(path):
8101                     for f in filenames:
8102                         fp = os.path.join(dirpath, f)
8103                         dk_size += os.path.getsize(fp)
8104                 qemu_img_info = disk_api.get_disk_info(path)
8105                 virt_size = qemu_img_info.virtual_size
8106                 backing_file = libvirt_utils.get_disk_backing_file(path)
8107                 over_commit_size = int(virt_size) - dk_size
8108 
8109             elif disk_type == 'file' and driver_type == 'qcow2':
8110                 qemu_img_info = disk_api.get_disk_info(path)
8111                 dk_size = qemu_img_info.disk_size
8112                 virt_size = qemu_img_info.virtual_size
8113                 backing_file = libvirt_utils.get_disk_backing_file(path)
8114                 over_commit_size = int(virt_size) - dk_size
8115 
8116             elif disk_type == 'file':
8117                 dk_size = os.stat(path).st_blocks * 512
8118                 virt_size = os.path.getsize(path)
8119                 backing_file = ""
8120                 over_commit_size = 0
8121 
8122             elif disk_type == 'block' and block_device_info:
8123                 dk_size = lvm.get_volume_size(path)
8124                 virt_size = dk_size
8125                 backing_file = ""
8126                 over_commit_size = 0
8127 
8128             else:
8129                 LOG.debug('skipping disk %(path)s (%(target)s) - unable to '
8130                           'determine if volume',
8131                           {'path': path, 'target': target})
8132                 continue
8133 
8134             disk_info.append({'type': driver_type,
8135                               'path': path,
8136                               'virt_disk_size': virt_size,
8137                               'backing_file': backing_file,
8138                               'disk_size': dk_size,
8139                               'over_committed_disk_size': over_commit_size})
8140         return disk_info
8141 
8142     def _get_instance_disk_info(self, instance, block_device_info):
8143         try:
8144             guest = self._host.get_guest(instance)
8145             config = guest.get_config()
8146         except libvirt.libvirtError as ex:
8147             error_code = ex.get_error_code()
8148             LOG.warning('Error from libvirt while getting description of '
8149                         '%(instance_name)s: [Error Code %(error_code)s] '
8150                         '%(ex)s',
8151                         {'instance_name': instance.name,
8152                          'error_code': error_code,
8153                          'ex': encodeutils.exception_to_unicode(ex)},
8154                         instance=instance)
8155             raise exception.InstanceNotFound(instance_id=instance.uuid)
8156 
8157         return self._get_instance_disk_info_from_config(config,
8158                                                         block_device_info)
8159 
8160     def get_instance_disk_info(self, instance,
8161                                block_device_info=None):
8162         return jsonutils.dumps(
8163             self._get_instance_disk_info(instance, block_device_info))
8164 
8165     def _get_disk_over_committed_size_total(self):
8166         """Return total over committed disk size for all instances."""
8167         # Disk size that all instance uses : virtual_size - disk_size
8168         disk_over_committed_size = 0
8169         instance_domains = self._host.list_instance_domains(only_running=False)
8170         if not instance_domains:
8171             return disk_over_committed_size
8172 
8173         # Get all instance uuids
8174         instance_uuids = [dom.UUIDString() for dom in instance_domains]
8175         ctx = nova_context.get_admin_context()
8176         # Get instance object list by uuid filter
8177         filters = {'uuid': instance_uuids}
8178         # NOTE(ankit): objects.InstanceList.get_by_filters method is
8179         # getting called twice one is here and another in the
8180         # _update_available_resource method of resource_tracker. Since
8181         # _update_available_resource method is synchronized, there is a
8182         # possibility the instances list retrieved here to calculate
8183         # disk_over_committed_size would differ to the list you would get
8184         # in _update_available_resource method for calculating usages based
8185         # on instance utilization.
8186         local_instance_list = objects.InstanceList.get_by_filters(
8187             ctx, filters, use_slave=True)
8188         # Convert instance list to dictionary with instance uuid as key.
8189         local_instances = {inst.uuid: inst for inst in local_instance_list}
8190 
8191         # Get bdms by instance uuids
8192         bdms = objects.BlockDeviceMappingList.bdms_by_instance_uuid(
8193             ctx, instance_uuids)
8194 
8195         for dom in instance_domains:
8196             try:
8197                 guest = libvirt_guest.Guest(dom)
8198                 config = guest.get_config()
8199 
8200                 block_device_info = None
8201                 if guest.uuid in local_instances \
8202                         and (bdms and guest.uuid in bdms):
8203                     # Get block device info for instance
8204                     block_device_info = driver.get_block_device_info(
8205                         local_instances[guest.uuid], bdms[guest.uuid])
8206 
8207                 disk_infos = self._get_instance_disk_info_from_config(
8208                     config, block_device_info)
8209                 if not disk_infos:
8210                     continue
8211 
8212                 for info in disk_infos:
8213                     disk_over_committed_size += int(
8214                         info['over_committed_disk_size'])
8215             except libvirt.libvirtError as ex:
8216                 error_code = ex.get_error_code()
8217                 LOG.warning(
8218                     'Error from libvirt while getting description of '
8219                     '%(instance_name)s: [Error Code %(error_code)s] %(ex)s',
8220                     {'instance_name': guest.name,
8221                      'error_code': error_code,
8222                      'ex': encodeutils.exception_to_unicode(ex)})
8223             except OSError as e:
8224                 if e.errno in (errno.ENOENT, errno.ESTALE):
8225                     LOG.warning('Periodic task is updating the host stat, '
8226                                 'it is trying to get disk %(i_name)s, '
8227                                 'but disk file was removed by concurrent '
8228                                 'operations such as resize.',
8229                                 {'i_name': guest.name})
8230                 elif e.errno == errno.EACCES:
8231                     LOG.warning('Periodic task is updating the host stat, '
8232                                 'it is trying to get disk %(i_name)s, '
8233                                 'but access is denied. It is most likely '
8234                                 'due to a VM that exists on the compute '
8235                                 'node but is not managed by Nova.',
8236                                 {'i_name': guest.name})
8237                 else:
8238                     raise
8239             except exception.VolumeBDMPathNotFound as e:
8240                 LOG.warning('Periodic task is updating the host stats, '
8241                             'it is trying to get disk info for %(i_name)s, '
8242                             'but the backing volume block device was removed '
8243                             'by concurrent operations such as resize. '
8244                             'Error: %(error)s',
8245                             {'i_name': guest.name, 'error': e})
8246             except exception.DiskNotFound:
8247                 with excutils.save_and_reraise_exception() as err_ctxt:
8248                     # If the instance is undergoing a task state transition,
8249                     # like moving to another host or is being deleted, we
8250                     # should ignore this instance and move on.
8251                     if guest.uuid in local_instances:
8252                         inst = local_instances[guest.uuid]
8253                         if inst.task_state is not None:
8254                             LOG.info('Periodic task is updating the host '
8255                                      'stats; it is trying to get disk info '
8256                                      'for %(i_name)s, but the backing disk '
8257                                      'was removed by a concurrent operation '
8258                                      '(task_state=%(task_state)s)',
8259                                      {'i_name': guest.name,
8260                                       'task_state': inst.task_state},
8261                                      instance=inst)
8262                             err_ctxt.reraise = False
8263 
8264             # NOTE(gtt116): give other tasks a chance.
8265             greenthread.sleep(0)
8266         return disk_over_committed_size
8267 
8268     def unfilter_instance(self, instance, network_info):
8269         """See comments of same method in firewall_driver."""
8270         self.firewall_driver.unfilter_instance(instance,
8271                                                network_info=network_info)
8272 
8273     def get_available_nodes(self, refresh=False):
8274         return [self._host.get_hostname()]
8275 
8276     def get_host_cpu_stats(self):
8277         """Return the current CPU state of the host."""
8278         return self._host.get_cpu_stats()
8279 
8280     def get_host_uptime(self):
8281         """Returns the result of calling "uptime"."""
8282         out, err = processutils.execute('env', 'LANG=C', 'uptime')
8283         return out
8284 
8285     def manage_image_cache(self, context, all_instances):
8286         """Manage the local cache of images."""
8287         self.image_cache_manager.update(context, all_instances)
8288 
8289     def _cleanup_remote_migration(self, dest, inst_base, inst_base_resize,
8290                                   shared_storage=False):
8291         """Used only for cleanup in case migrate_disk_and_power_off fails."""
8292         try:
8293             if os.path.exists(inst_base_resize):
8294                 shutil.rmtree(inst_base, ignore_errors=True)
8295                 os.rename(inst_base_resize, inst_base)
8296                 if not shared_storage:
8297                     self._remotefs.remove_dir(dest, inst_base)
8298         except Exception:
8299             pass
8300 
8301     def _is_storage_shared_with(self, dest, inst_base):
8302         # NOTE (rmk): There are two methods of determining whether we are
8303         #             on the same filesystem: the source and dest IP are the
8304         #             same, or we create a file on the dest system via SSH
8305         #             and check whether the source system can also see it.
8306         # NOTE (drwahl): Actually, there is a 3rd way: if images_type is rbd,
8307         #                it will always be shared storage
8308         if CONF.libvirt.images_type == 'rbd':
8309             return True
8310         shared_storage = (dest == self.get_host_ip_addr())
8311         if not shared_storage:
8312             tmp_file = uuidutils.generate_uuid(dashed=False) + '.tmp'
8313             tmp_path = os.path.join(inst_base, tmp_file)
8314 
8315             try:
8316                 self._remotefs.create_file(dest, tmp_path)
8317                 if os.path.exists(tmp_path):
8318                     shared_storage = True
8319                     os.unlink(tmp_path)
8320                 else:
8321                     self._remotefs.remove_file(dest, tmp_path)
8322             except Exception:
8323                 pass
8324         return shared_storage
8325 
8326     def migrate_disk_and_power_off(self, context, instance, dest,
8327                                    flavor, network_info,
8328                                    block_device_info=None,
8329                                    timeout=0, retry_interval=0):
8330         LOG.debug("Starting migrate_disk_and_power_off",
8331                    instance=instance)
8332 
8333         ephemerals = driver.block_device_info_get_ephemerals(block_device_info)
8334 
8335         # get_bdm_ephemeral_disk_size() will return 0 if the new
8336         # instance's requested block device mapping contain no
8337         # ephemeral devices. However, we still want to check if
8338         # the original instance's ephemeral_gb property was set and
8339         # ensure that the new requested flavor ephemeral size is greater
8340         eph_size = (block_device.get_bdm_ephemeral_disk_size(ephemerals) or
8341                     instance.flavor.ephemeral_gb)
8342 
8343         # Checks if the migration needs a disk resize down.
8344         root_down = flavor.root_gb < instance.flavor.root_gb
8345         ephemeral_down = flavor.ephemeral_gb < eph_size
8346         booted_from_volume = self._is_booted_from_volume(block_device_info)
8347 
8348         if (root_down and not booted_from_volume) or ephemeral_down:
8349             reason = _("Unable to resize disk down.")
8350             raise exception.InstanceFaultRollback(
8351                 exception.ResizeError(reason=reason))
8352 
8353         # NOTE(dgenin): Migration is not implemented for LVM backed instances.
8354         if CONF.libvirt.images_type == 'lvm' and not booted_from_volume:
8355             reason = _("Migration is not supported for LVM backed instances")
8356             raise exception.InstanceFaultRollback(
8357                 exception.MigrationPreCheckError(reason=reason))
8358 
8359         # copy disks to destination
8360         # rename instance dir to +_resize at first for using
8361         # shared storage for instance dir (eg. NFS).
8362         inst_base = libvirt_utils.get_instance_path(instance)
8363         inst_base_resize = inst_base + "_resize"
8364         shared_storage = self._is_storage_shared_with(dest, inst_base)
8365 
8366         # try to create the directory on the remote compute node
8367         # if this fails we pass the exception up the stack so we can catch
8368         # failures here earlier
8369         if not shared_storage:
8370             try:
8371                 self._remotefs.create_dir(dest, inst_base)
8372             except processutils.ProcessExecutionError as e:
8373                 reason = _("not able to execute ssh command: %s") % e
8374                 raise exception.InstanceFaultRollback(
8375                     exception.ResizeError(reason=reason))
8376 
8377         self.power_off(instance, timeout, retry_interval)
8378 
8379         block_device_mapping = driver.block_device_info_get_mapping(
8380             block_device_info)
8381         for vol in block_device_mapping:
8382             connection_info = vol['connection_info']
8383             self._disconnect_volume(context, connection_info, instance)
8384 
8385         disk_info = self._get_instance_disk_info(instance, block_device_info)
8386 
8387         try:
8388             os.rename(inst_base, inst_base_resize)
8389             # if we are migrating the instance with shared storage then
8390             # create the directory.  If it is a remote node the directory
8391             # has already been created
8392             if shared_storage:
8393                 dest = None
8394                 fileutils.ensure_tree(inst_base)
8395 
8396             on_execute = lambda process: \
8397                 self.job_tracker.add_job(instance, process.pid)
8398             on_completion = lambda process: \
8399                 self.job_tracker.remove_job(instance, process.pid)
8400 
8401             for info in disk_info:
8402                 # assume inst_base == dirname(info['path'])
8403                 img_path = info['path']
8404                 fname = os.path.basename(img_path)
8405                 from_path = os.path.join(inst_base_resize, fname)
8406 
8407                 # We will not copy over the swap disk here, and rely on
8408                 # finish_migration to re-create it for us. This is ok because
8409                 # the OS is shut down, and as recreating a swap disk is very
8410                 # cheap it is more efficient than copying either locally or
8411                 # over the network. This also means we don't have to resize it.
8412                 if fname == 'disk.swap':
8413                     continue
8414 
8415                 compression = info['type'] not in NO_COMPRESSION_TYPES
8416                 libvirt_utils.copy_image(from_path, img_path, host=dest,
8417                                          on_execute=on_execute,
8418                                          on_completion=on_completion,
8419                                          compression=compression)
8420 
8421             # Ensure disk.info is written to the new path to avoid disks being
8422             # reinspected and potentially changing format.
8423             src_disk_info_path = os.path.join(inst_base_resize, 'disk.info')
8424             if os.path.exists(src_disk_info_path):
8425                 dst_disk_info_path = os.path.join(inst_base, 'disk.info')
8426                 libvirt_utils.copy_image(src_disk_info_path,
8427                                          dst_disk_info_path,
8428                                          host=dest, on_execute=on_execute,
8429                                          on_completion=on_completion)
8430         except Exception:
8431             with excutils.save_and_reraise_exception():
8432                 self._cleanup_remote_migration(dest, inst_base,
8433                                                inst_base_resize,
8434                                                shared_storage)
8435 
8436         return jsonutils.dumps(disk_info)
8437 
8438     def _wait_for_running(self, instance):
8439         state = self.get_info(instance).state
8440 
8441         if state == power_state.RUNNING:
8442             LOG.info("Instance running successfully.", instance=instance)
8443             raise loopingcall.LoopingCallDone()
8444 
8445     @staticmethod
8446     def _disk_raw_to_qcow2(path):
8447         """Converts a raw disk to qcow2."""
8448         path_qcow = path + '_qcow'
8449         # execute operation with disk concurrency semaphore
8450         with compute_utils.disk_ops_semaphore:
8451             processutils.execute('qemu-img', 'convert', '-f', 'raw',
8452                                  '-O', 'qcow2', path, path_qcow)
8453         os.rename(path_qcow, path)
8454 
8455     def finish_migration(self, context, migration, instance, disk_info,
8456                          network_info, image_meta, resize_instance,
8457                          block_device_info=None, power_on=True):
8458         LOG.debug("Starting finish_migration", instance=instance)
8459 
8460         block_disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
8461                                                   instance,
8462                                                   image_meta,
8463                                                   block_device_info)
8464         # assume _create_image does nothing if a target file exists.
8465         # NOTE: This has the intended side-effect of fetching a missing
8466         # backing file.
8467         self._create_image(context, instance, block_disk_info['mapping'],
8468                            block_device_info=block_device_info,
8469                            ignore_bdi_for_swap=True,
8470                            fallback_from_host=migration.source_compute)
8471 
8472         # Required by Quobyte CI
8473         self._ensure_console_log_for_instance(instance)
8474 
8475         gen_confdrive = functools.partial(
8476             self._create_configdrive, context, instance,
8477             InjectionInfo(admin_pass=None, network_info=network_info,
8478                           files=None))
8479 
8480         # Convert raw disks to qcow2 if migrating to host which uses
8481         # qcow2 from host which uses raw.
8482         disk_info = jsonutils.loads(disk_info)
8483         for info in disk_info:
8484             path = info['path']
8485             disk_name = os.path.basename(path)
8486 
8487             # NOTE(mdbooth): The code below looks wrong, but is actually
8488             # required to prevent a security hole when migrating from a host
8489             # with use_cow_images=False to one with use_cow_images=True.
8490             # Imagebackend uses use_cow_images to select between the
8491             # atrociously-named-Raw and Qcow2 backends. The Qcow2 backend
8492             # writes to disk.info, but does not read it as it assumes qcow2.
8493             # Therefore if we don't convert raw to qcow2 here, a raw disk will
8494             # be incorrectly assumed to be qcow2, which is a severe security
8495             # flaw. The reverse is not true, because the atrociously-named-Raw
8496             # backend supports both qcow2 and raw disks, and will choose
8497             # appropriately between them as long as disk.info exists and is
8498             # correctly populated, which it is because Qcow2 writes to
8499             # disk.info.
8500             #
8501             # In general, we do not yet support format conversion during
8502             # migration. For example:
8503             #   * Converting from use_cow_images=True to use_cow_images=False
8504             #     isn't handled. This isn't a security bug, but is almost
8505             #     certainly buggy in other cases, as the 'Raw' backend doesn't
8506             #     expect a backing file.
8507             #   * Converting to/from lvm and rbd backends is not supported.
8508             #
8509             # This behaviour is inconsistent, and therefore undesirable for
8510             # users. It is tightly-coupled to implementation quirks of 2
8511             # out of 5 backends in imagebackend and defends against a severe
8512             # security flaw which is not at all obvious without deep analysis,
8513             # and is therefore undesirable to developers. We should aim to
8514             # remove it. This will not be possible, though, until we can
8515             # represent the storage layout of a specific instance
8516             # independent of the default configuration of the local compute
8517             # host.
8518 
8519             # Config disks are hard-coded to be raw even when
8520             # use_cow_images=True (see _get_disk_config_image_type),so don't
8521             # need to be converted.
8522             if (disk_name != 'disk.config' and
8523                         info['type'] == 'raw' and CONF.use_cow_images):
8524                 self._disk_raw_to_qcow2(info['path'])
8525 
8526         xml = self._get_guest_xml(context, instance, network_info,
8527                                   block_disk_info, image_meta,
8528                                   block_device_info=block_device_info)
8529         # NOTE(mriedem): vifs_already_plugged=True here, regardless of whether
8530         # or not we've migrated to another host, because we unplug VIFs locally
8531         # and the status change in the port might go undetected by the neutron
8532         # L2 agent (or neutron server) so neutron may not know that the VIF was
8533         # unplugged in the first place and never send an event.
8534         guest = self._create_domain_and_network(context, xml, instance,
8535                                         network_info,
8536                                         block_device_info=block_device_info,
8537                                         power_on=power_on,
8538                                         vifs_already_plugged=True,
8539                                         post_xml_callback=gen_confdrive)
8540         if power_on:
8541             timer = loopingcall.FixedIntervalLoopingCall(
8542                                                     self._wait_for_running,
8543                                                     instance)
8544             timer.start(interval=0.5).wait()
8545 
8546             # Sync guest time after migration.
8547             guest.sync_guest_time()
8548 
8549         LOG.debug("finish_migration finished successfully.", instance=instance)
8550 
8551     def _cleanup_failed_migration(self, inst_base):
8552         """Make sure that a failed migrate doesn't prevent us from rolling
8553         back in a revert.
8554         """
8555         try:
8556             shutil.rmtree(inst_base)
8557         except OSError as e:
8558             if e.errno != errno.ENOENT:
8559                 raise
8560 
8561     def finish_revert_migration(self, context, instance, network_info,
8562                                 block_device_info=None, power_on=True):
8563         LOG.debug("Starting finish_revert_migration",
8564                   instance=instance)
8565 
8566         inst_base = libvirt_utils.get_instance_path(instance)
8567         inst_base_resize = inst_base + "_resize"
8568 
8569         # NOTE(danms): if we're recovering from a failed migration,
8570         # make sure we don't have a left-over same-host base directory
8571         # that would conflict. Also, don't fail on the rename if the
8572         # failure happened early.
8573         if os.path.exists(inst_base_resize):
8574             self._cleanup_failed_migration(inst_base)
8575             os.rename(inst_base_resize, inst_base)
8576 
8577         root_disk = self.image_backend.by_name(instance, 'disk')
8578         # Once we rollback, the snapshot is no longer needed, so remove it
8579         if root_disk.exists():
8580             root_disk.rollback_to_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME)
8581             root_disk.remove_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME)
8582 
8583         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
8584                                             instance,
8585                                             instance.image_meta,
8586                                             block_device_info)
8587         xml = self._get_guest_xml(context, instance, network_info, disk_info,
8588                                   instance.image_meta,
8589                                   block_device_info=block_device_info)
8590         self._create_domain_and_network(context, xml, instance, network_info,
8591                                         block_device_info=block_device_info,
8592                                         power_on=power_on)
8593 
8594         if power_on:
8595             timer = loopingcall.FixedIntervalLoopingCall(
8596                                                     self._wait_for_running,
8597                                                     instance)
8598             timer.start(interval=0.5).wait()
8599 
8600         LOG.debug("finish_revert_migration finished successfully.",
8601                   instance=instance)
8602 
8603     def confirm_migration(self, context, migration, instance, network_info):
8604         """Confirms a resize, destroying the source VM."""
8605         self._cleanup_resize(context, instance, network_info)
8606 
8607     @staticmethod
8608     def _get_io_devices(xml_doc):
8609         """get the list of io devices from the xml document."""
8610         result = {"volumes": [], "ifaces": []}
8611         try:
8612             doc = etree.fromstring(xml_doc)
8613         except Exception:
8614             return result
8615         blocks = [('./devices/disk', 'volumes'),
8616             ('./devices/interface', 'ifaces')]
8617         for block, key in blocks:
8618             section = doc.findall(block)
8619             for node in section:
8620                 for child in node.getchildren():
8621                     if child.tag == 'target' and child.get('dev'):
8622                         result[key].append(child.get('dev'))
8623         return result
8624 
8625     def get_diagnostics(self, instance):
8626         guest = self._host.get_guest(instance)
8627 
8628         # TODO(sahid): We are converting all calls from a
8629         # virDomain object to use nova.virt.libvirt.Guest.
8630         # We should be able to remove domain at the end.
8631         domain = guest._domain
8632         output = {}
8633         # get cpu time, might launch an exception if the method
8634         # is not supported by the underlying hypervisor being
8635         # used by libvirt
8636         try:
8637             for vcpu in guest.get_vcpus_info():
8638                 output["cpu" + str(vcpu.id) + "_time"] = vcpu.time
8639         except libvirt.libvirtError:
8640             pass
8641         # get io status
8642         xml = guest.get_xml_desc()
8643         dom_io = LibvirtDriver._get_io_devices(xml)
8644         for guest_disk in dom_io["volumes"]:
8645             try:
8646                 # blockStats might launch an exception if the method
8647                 # is not supported by the underlying hypervisor being
8648                 # used by libvirt
8649                 stats = domain.blockStats(guest_disk)
8650                 output[guest_disk + "_read_req"] = stats[0]
8651                 output[guest_disk + "_read"] = stats[1]
8652                 output[guest_disk + "_write_req"] = stats[2]
8653                 output[guest_disk + "_write"] = stats[3]
8654                 output[guest_disk + "_errors"] = stats[4]
8655             except libvirt.libvirtError:
8656                 pass
8657         for interface in dom_io["ifaces"]:
8658             try:
8659                 # interfaceStats might launch an exception if the method
8660                 # is not supported by the underlying hypervisor being
8661                 # used by libvirt
8662                 stats = domain.interfaceStats(interface)
8663                 output[interface + "_rx"] = stats[0]
8664                 output[interface + "_rx_packets"] = stats[1]
8665                 output[interface + "_rx_errors"] = stats[2]
8666                 output[interface + "_rx_drop"] = stats[3]
8667                 output[interface + "_tx"] = stats[4]
8668                 output[interface + "_tx_packets"] = stats[5]
8669                 output[interface + "_tx_errors"] = stats[6]
8670                 output[interface + "_tx_drop"] = stats[7]
8671             except libvirt.libvirtError:
8672                 pass
8673         output["memory"] = domain.maxMemory()
8674         # memoryStats might launch an exception if the method
8675         # is not supported by the underlying hypervisor being
8676         # used by libvirt
8677         try:
8678             mem = domain.memoryStats()
8679             for key in mem.keys():
8680                 output["memory-" + key] = mem[key]
8681         except (libvirt.libvirtError, AttributeError):
8682             pass
8683         return output
8684 
8685     def get_instance_diagnostics(self, instance):
8686         guest = self._host.get_guest(instance)
8687 
8688         # TODO(sahid): We are converting all calls from a
8689         # virDomain object to use nova.virt.libvirt.Guest.
8690         # We should be able to remove domain at the end.
8691         domain = guest._domain
8692 
8693         xml = guest.get_xml_desc()
8694         xml_doc = etree.fromstring(xml)
8695 
8696         # TODO(sahid): Needs to use get_info but more changes have to
8697         # be done since a mapping STATE_MAP LIBVIRT_POWER_STATE is
8698         # needed.
8699         (state, max_mem, mem, num_cpu, cpu_time) = \
8700             guest._get_domain_info(self._host)
8701         config_drive = configdrive.required_by(instance)
8702         launched_at = timeutils.normalize_time(instance.launched_at)
8703         uptime = timeutils.delta_seconds(launched_at,
8704                                          timeutils.utcnow())
8705         diags = diagnostics_obj.Diagnostics(state=power_state.STATE_MAP[state],
8706                                         driver='libvirt',
8707                                         config_drive=config_drive,
8708                                         hypervisor=CONF.libvirt.virt_type,
8709                                         hypervisor_os='linux',
8710                                         uptime=uptime)
8711         diags.memory_details = diagnostics_obj.MemoryDiagnostics(
8712             maximum=max_mem / units.Mi,
8713             used=mem / units.Mi)
8714 
8715         # get cpu time, might launch an exception if the method
8716         # is not supported by the underlying hypervisor being
8717         # used by libvirt
8718         try:
8719             for vcpu in guest.get_vcpus_info():
8720                 diags.add_cpu(id=vcpu.id, time=vcpu.time)
8721         except libvirt.libvirtError:
8722             pass
8723         # get io status
8724         dom_io = LibvirtDriver._get_io_devices(xml)
8725         for guest_disk in dom_io["volumes"]:
8726             try:
8727                 # blockStats might launch an exception if the method
8728                 # is not supported by the underlying hypervisor being
8729                 # used by libvirt
8730                 stats = domain.blockStats(guest_disk)
8731                 diags.add_disk(read_bytes=stats[1],
8732                                read_requests=stats[0],
8733                                write_bytes=stats[3],
8734                                write_requests=stats[2],
8735                                errors_count=stats[4])
8736             except libvirt.libvirtError:
8737                 pass
8738         for interface in dom_io["ifaces"]:
8739             try:
8740                 # interfaceStats might launch an exception if the method
8741                 # is not supported by the underlying hypervisor being
8742                 # used by libvirt
8743                 stats = domain.interfaceStats(interface)
8744                 diags.add_nic(rx_octets=stats[0],
8745                               rx_errors=stats[2],
8746                               rx_drop=stats[3],
8747                               rx_packets=stats[1],
8748                               tx_octets=stats[4],
8749                               tx_errors=stats[6],
8750                               tx_drop=stats[7],
8751                               tx_packets=stats[5])
8752             except libvirt.libvirtError:
8753                 pass
8754 
8755         # Update mac addresses of interface if stats have been reported
8756         if diags.nic_details:
8757             nodes = xml_doc.findall('./devices/interface/mac')
8758             for index, node in enumerate(nodes):
8759                 diags.nic_details[index].mac_address = node.get('address')
8760         return diags
8761 
8762     @staticmethod
8763     def _prepare_device_bus(dev):
8764         """Determines the device bus and its hypervisor assigned address
8765         """
8766         bus = None
8767         address = (dev.device_addr.format_address() if
8768                    dev.device_addr else None)
8769         if isinstance(dev.device_addr,
8770                       vconfig.LibvirtConfigGuestDeviceAddressPCI):
8771             bus = objects.PCIDeviceBus()
8772         elif isinstance(dev, vconfig.LibvirtConfigGuestDisk):
8773             if dev.target_bus == 'scsi':
8774                 bus = objects.SCSIDeviceBus()
8775             elif dev.target_bus == 'ide':
8776                 bus = objects.IDEDeviceBus()
8777             elif dev.target_bus == 'usb':
8778                 bus = objects.USBDeviceBus()
8779         if address is not None and bus is not None:
8780             bus.address = address
8781         return bus
8782 
8783     def _build_interface_metadata(self, dev, vifs_to_expose, vlans_by_mac,
8784                                   trusted_by_mac):
8785         """Builds a metadata object for a network interface
8786 
8787         :param dev: The LibvirtConfigGuestInterface to build metadata for.
8788         :param vifs_to_expose: The list of tagged and/or vlan'ed
8789                                VirtualInterface objects.
8790         :param vlans_by_mac: A dictionary of mac address -> vlan associations.
8791         :param trusted_by_mac: A dictionary of mac address -> vf_trusted
8792                                associations.
8793         :return: A NetworkInterfaceMetadata object, or None.
8794         """
8795         vif = vifs_to_expose.get(dev.mac_addr)
8796         if not vif:
8797             LOG.debug('No VIF found with MAC %s, not building metadata',
8798                       dev.mac_addr)
8799             return None
8800         bus = self._prepare_device_bus(dev)
8801         device = objects.NetworkInterfaceMetadata(mac=vif.address)
8802         if 'tag' in vif and vif.tag:
8803             device.tags = [vif.tag]
8804         if bus:
8805             device.bus = bus
8806         vlan = vlans_by_mac.get(vif.address)
8807         if vlan:
8808             device.vlan = int(vlan)
8809         device.vf_trusted = trusted_by_mac.get(vif.address, False)
8810         return device
8811 
8812     def _build_disk_metadata(self, dev, tagged_bdms):
8813         """Builds a metadata object for a disk
8814 
8815         :param dev: The vconfig.LibvirtConfigGuestDisk to build metadata for.
8816         :param tagged_bdms: The list of tagged BlockDeviceMapping objects.
8817         :return: A DiskMetadata object, or None.
8818         """
8819         bdm = tagged_bdms.get(dev.target_dev)
8820         if not bdm:
8821             LOG.debug('No BDM found with device name %s, not building '
8822                       'metadata.', dev.target_dev)
8823             return None
8824         bus = self._prepare_device_bus(dev)
8825         device = objects.DiskMetadata(tags=[bdm.tag])
8826         # NOTE(artom) Setting the serial (which corresponds to
8827         # volume_id in BlockDeviceMapping) in DiskMetadata allows us to
8828         # find the disks's BlockDeviceMapping object when we detach the
8829         # volume and want to clean up its metadata.
8830         device.serial = bdm.volume_id
8831         if bus:
8832             device.bus = bus
8833         return device
8834 
8835     def _build_hostdev_metadata(self, dev, vifs_to_expose, vlans_by_mac):
8836         """Builds a metadata object for a hostdev. This can only be a PF, so we
8837         don't need trusted_by_mac like in _build_interface_metadata because
8838         only VFs can be trusted.
8839 
8840         :param dev: The LibvirtConfigGuestHostdevPCI to build metadata for.
8841         :param vifs_to_expose: The list of tagged and/or vlan'ed
8842                                VirtualInterface objects.
8843         :param vlans_by_mac: A dictionary of mac address -> vlan associations.
8844         :return: A NetworkInterfaceMetadata object, or None.
8845         """
8846         # Strip out the leading '0x'
8847         pci_address = pci_utils.get_pci_address(
8848             *[x[2:] for x in (dev.domain, dev.bus, dev.slot, dev.function)])
8849         try:
8850             mac = pci_utils.get_mac_by_pci_address(pci_address,
8851                                                    pf_interface=True)
8852         except exception.PciDeviceNotFoundById:
8853             LOG.debug('Not exposing metadata for not found PCI device %s',
8854                       pci_address)
8855             return None
8856 
8857         vif = vifs_to_expose.get(mac)
8858         if not vif:
8859             LOG.debug('No VIF found with MAC %s, not building metadata', mac)
8860             return None
8861 
8862         device = objects.NetworkInterfaceMetadata(mac=mac)
8863         device.bus = objects.PCIDeviceBus(address=pci_address)
8864         if 'tag' in vif and vif.tag:
8865             device.tags = [vif.tag]
8866         vlan = vlans_by_mac.get(mac)
8867         if vlan:
8868             device.vlan = int(vlan)
8869         return device
8870 
8871     def _build_device_metadata(self, context, instance):
8872         """Builds a metadata object for instance devices, that maps the user
8873            provided tag to the hypervisor assigned device address.
8874         """
8875         def _get_device_name(bdm):
8876             return block_device.strip_dev(bdm.device_name)
8877 
8878         network_info = instance.info_cache.network_info
8879         vlans_by_mac = netutils.get_cached_vifs_with_vlan(network_info)
8880         trusted_by_mac = netutils.get_cached_vifs_with_trusted(network_info)
8881         vifs = objects.VirtualInterfaceList.get_by_instance_uuid(context,
8882                                                                  instance.uuid)
8883         vifs_to_expose = {vif.address: vif for vif in vifs
8884                           if ('tag' in vif and vif.tag) or
8885                              vlans_by_mac.get(vif.address)}
8886         # TODO(mriedem): We should be able to avoid the DB query here by using
8887         # block_device_info['block_device_mapping'] which is passed into most
8888         # methods that call this function.
8889         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
8890             context, instance.uuid)
8891         tagged_bdms = {_get_device_name(bdm): bdm for bdm in bdms if bdm.tag}
8892 
8893         devices = []
8894         guest = self._host.get_guest(instance)
8895         xml = guest.get_xml_desc()
8896         xml_dom = etree.fromstring(xml)
8897         guest_config = vconfig.LibvirtConfigGuest()
8898         guest_config.parse_dom(xml_dom)
8899 
8900         for dev in guest_config.devices:
8901             device = None
8902             if isinstance(dev, vconfig.LibvirtConfigGuestInterface):
8903                 device = self._build_interface_metadata(dev, vifs_to_expose,
8904                                                         vlans_by_mac,
8905                                                         trusted_by_mac)
8906             if isinstance(dev, vconfig.LibvirtConfigGuestDisk):
8907                 device = self._build_disk_metadata(dev, tagged_bdms)
8908             if isinstance(dev, vconfig.LibvirtConfigGuestHostdevPCI):
8909                 device = self._build_hostdev_metadata(dev, vifs_to_expose,
8910                                                       vlans_by_mac)
8911             if device:
8912                 devices.append(device)
8913         if devices:
8914             dev_meta = objects.InstanceDeviceMetadata(devices=devices)
8915             return dev_meta
8916 
8917     def instance_on_disk(self, instance):
8918         # ensure directories exist and are writable
8919         instance_path = libvirt_utils.get_instance_path(instance)
8920         LOG.debug('Checking instance files accessibility %s', instance_path,
8921                   instance=instance)
8922         shared_instance_path = os.access(instance_path, os.W_OK)
8923         # NOTE(flwang): For shared block storage scenario, the file system is
8924         # not really shared by the two hosts, but the volume of evacuated
8925         # instance is reachable.
8926         shared_block_storage = (self.image_backend.backend().
8927                                 is_shared_block_storage())
8928         return shared_instance_path or shared_block_storage
8929 
8930     def inject_network_info(self, instance, nw_info):
8931         self.firewall_driver.setup_basic_filtering(instance, nw_info)
8932 
8933     def delete_instance_files(self, instance):
8934         target = libvirt_utils.get_instance_path(instance)
8935         # A resize may be in progress
8936         target_resize = target + '_resize'
8937         # Other threads may attempt to rename the path, so renaming the path
8938         # to target + '_del' (because it is atomic) and iterating through
8939         # twice in the unlikely event that a concurrent rename occurs between
8940         # the two rename attempts in this method. In general this method
8941         # should be fairly thread-safe without these additional checks, since
8942         # other operations involving renames are not permitted when the task
8943         # state is not None and the task state should be set to something
8944         # other than None by the time this method is invoked.
8945         target_del = target + '_del'
8946         for i in range(2):
8947             try:
8948                 os.rename(target, target_del)
8949                 break
8950             except Exception:
8951                 pass
8952             try:
8953                 os.rename(target_resize, target_del)
8954                 break
8955             except Exception:
8956                 pass
8957         # Either the target or target_resize path may still exist if all
8958         # rename attempts failed.
8959         remaining_path = None
8960         for p in (target, target_resize):
8961             if os.path.exists(p):
8962                 remaining_path = p
8963                 break
8964 
8965         # A previous delete attempt may have been interrupted, so target_del
8966         # may exist even if all rename attempts during the present method
8967         # invocation failed due to the absence of both target and
8968         # target_resize.
8969         if not remaining_path and os.path.exists(target_del):
8970             self.job_tracker.terminate_jobs(instance)
8971 
8972             LOG.info('Deleting instance files %s', target_del,
8973                      instance=instance)
8974             remaining_path = target_del
8975             try:
8976                 shutil.rmtree(target_del)
8977             except OSError as e:
8978                 LOG.error('Failed to cleanup directory %(target)s: %(e)s',
8979                           {'target': target_del, 'e': e}, instance=instance)
8980 
8981         # It is possible that the delete failed, if so don't mark the instance
8982         # as cleaned.
8983         if remaining_path and os.path.exists(remaining_path):
8984             LOG.info('Deletion of %s failed', remaining_path,
8985                      instance=instance)
8986             return False
8987 
8988         LOG.info('Deletion of %s complete', target_del, instance=instance)
8989         return True
8990 
8991     @property
8992     def need_legacy_block_device_info(self):
8993         return False
8994 
8995     def default_root_device_name(self, instance, image_meta, root_bdm):
8996         disk_bus = blockinfo.get_disk_bus_for_device_type(
8997             instance, CONF.libvirt.virt_type, image_meta, "disk")
8998         cdrom_bus = blockinfo.get_disk_bus_for_device_type(
8999             instance, CONF.libvirt.virt_type, image_meta, "cdrom")
9000         root_info = blockinfo.get_root_info(
9001             instance, CONF.libvirt.virt_type, image_meta,
9002             root_bdm, disk_bus, cdrom_bus)
9003         return block_device.prepend_dev(root_info['dev'])
9004 
9005     def default_device_names_for_instance(self, instance, root_device_name,
9006                                           *block_device_lists):
9007         block_device_mapping = list(itertools.chain(*block_device_lists))
9008         # NOTE(ndipanov): Null out the device names so that blockinfo code
9009         #                 will assign them
9010         for bdm in block_device_mapping:
9011             if bdm.device_name is not None:
9012                 LOG.info(
9013                     "Ignoring supplied device name: %(device_name)s. "
9014                     "Libvirt can't honour user-supplied dev names",
9015                     {'device_name': bdm.device_name}, instance=instance)
9016                 bdm.device_name = None
9017         block_device_info = driver.get_block_device_info(instance,
9018                                                          block_device_mapping)
9019 
9020         blockinfo.default_device_names(CONF.libvirt.virt_type,
9021                                        nova_context.get_admin_context(),
9022                                        instance,
9023                                        block_device_info,
9024                                        instance.image_meta)
9025 
9026     def get_device_name_for_instance(self, instance, bdms, block_device_obj):
9027         block_device_info = driver.get_block_device_info(instance, bdms)
9028         instance_info = blockinfo.get_disk_info(
9029                 CONF.libvirt.virt_type, instance,
9030                 instance.image_meta, block_device_info=block_device_info)
9031 
9032         suggested_dev_name = block_device_obj.device_name
9033         if suggested_dev_name is not None:
9034             LOG.info(
9035                 'Ignoring supplied device name: %(suggested_dev)s',
9036                 {'suggested_dev': suggested_dev_name}, instance=instance)
9037 
9038         # NOTE(ndipanov): get_info_from_bdm will generate the new device name
9039         #                 only when it's actually not set on the bd object
9040         block_device_obj.device_name = None
9041         disk_info = blockinfo.get_info_from_bdm(
9042             instance, CONF.libvirt.virt_type, instance.image_meta,
9043             block_device_obj, mapping=instance_info['mapping'])
9044         return block_device.prepend_dev(disk_info['dev'])
9045 
9046     def is_supported_fs_format(self, fs_type):
9047         return fs_type in [nova.privsep.fs.FS_FORMAT_EXT2,
9048                            nova.privsep.fs.FS_FORMAT_EXT3,
9049                            nova.privsep.fs.FS_FORMAT_EXT4,
9050                            nova.privsep.fs.FS_FORMAT_XFS]
9051 
9052     def _get_cpu_traits(self):
9053         """Get CPU traits of VMs based on guest CPU model config:
9054         1. if mode is 'host-model' or 'host-passthrough', use host's
9055         CPU features.
9056         2. if mode is None, choose a default CPU model based on CPU
9057         architecture.
9058         3. if mode is 'custom', use cpu_model to generate CPU features.
9059         The code also accounts for cpu_model_extra_flags configuration when
9060         cpu_mode is 'host-model', 'host-passthrough' or 'custom', this
9061         ensures user specified CPU feature flags to be included.
9062         :return: A dict of trait names mapped to boolean values or None.
9063         """
9064         cpu = self._get_guest_cpu_model_config()
9065         if not cpu:
9066             LOG.info('The current libvirt hypervisor %(virt_type)s '
9067                      'does not support reporting CPU traits.',
9068                      {'virt_type': CONF.libvirt.virt_type})
9069             return
9070 
9071         caps = deepcopy(self._host.get_capabilities())
9072         if cpu.mode in ('host-model', 'host-passthrough'):
9073             # Account for features in cpu_model_extra_flags conf
9074             host_features = [f.name for f in
9075                              caps.host.cpu.features | cpu.features]
9076             return libvirt_utils.cpu_features_to_traits(host_features)
9077 
9078         # Choose a default CPU model when cpu_mode is not specified
9079         if cpu.mode is None:
9080             caps.host.cpu.model = libvirt_utils.get_cpu_model_from_arch(
9081                 caps.host.cpu.arch)
9082             caps.host.cpu.features = set()
9083         else:
9084             # For custom mode, set model to guest CPU model
9085             caps.host.cpu.model = cpu.model
9086             caps.host.cpu.features = set()
9087             # Account for features in cpu_model_extra_flags conf
9088             for f in cpu.features:
9089                 caps.host.cpu.add_feature(
9090                     vconfig.LibvirtConfigCPUFeature(name=f.name))
9091 
9092         xml_str = caps.host.cpu.to_xml()
9093         LOG.debug("Libvirt baseline CPU %s", xml_str)
9094         # TODO(lei-zh): baselineCPU is not supported on all platforms.
9095         # There is some work going on in the libvirt community to replace the
9096         # baseline call. Consider using the new apis when they are ready. See
9097         # https://www.redhat.com/archives/libvir-list/2018-May/msg01204.html.
9098         try:
9099             if hasattr(libvirt, 'VIR_CONNECT_BASELINE_CPU_EXPAND_FEATURES'):
9100                 features = self._host.get_connection().baselineCPU(
9101                     [xml_str],
9102                     libvirt.VIR_CONNECT_BASELINE_CPU_EXPAND_FEATURES)
9103             else:
9104                 features = self._host.get_connection().baselineCPU([xml_str])
9105         except libvirt.libvirtError as ex:
9106             with excutils.save_and_reraise_exception() as ctxt:
9107                 error_code = ex.get_error_code()
9108                 if error_code == libvirt.VIR_ERR_NO_SUPPORT:
9109                     ctxt.reraise = False
9110                     LOG.info('URI %(uri)s does not support full set'
9111                              ' of host capabilities: %(error)s',
9112                              {'uri': self._host._uri, 'error': ex})
9113                     return libvirt_utils.cpu_features_to_traits([])
9114 
9115         cpu.parse_str(features)
9116         return libvirt_utils.cpu_features_to_traits(
9117             [f.name for f in cpu.features])
