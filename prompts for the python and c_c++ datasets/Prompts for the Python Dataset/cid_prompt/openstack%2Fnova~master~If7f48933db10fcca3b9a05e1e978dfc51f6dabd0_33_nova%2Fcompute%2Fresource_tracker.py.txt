Please review the code below for security defects. You can consider defect types in terms of:
1.CWE-284 (Improper Access Control)
2.CWE-435 (Improper Interaction Between Multiple Entities)
3.CWE-664 (Improper Control of a Resource Through its Lifetime)
4.CWE-682 (Incorrect Calculation)
5.CWE-691 (Insufficient Control Flow Management)
6.CWE-693 (Protection Mechanism Failure)
7.CWE-697 (Incorrect Comparison)
8.CWE-703 (Improper Check or Handling of Exceptional Conditions)
9.CWE-707 (Improper Neutralization)
10.CWE-710 (Improper Adherence to Coding Standards)
If any are found, please describe the security defect in detail and indicate the corresponding line number of code and solution. If none are detected, states: 'No security defects are detected in the code'.

1 # Copyright (c) 2012 OpenStack Foundation
2 # All Rights Reserved.
3 #
4 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
5 #    not use this file except in compliance with the License. You may obtain
6 #    a copy of the License at
7 #
8 #         http://www.apache.org/licenses/LICENSE-2.0
9 #
10 #    Unless required by applicable law or agreed to in writing, software
11 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
12 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
13 #    License for the specific language governing permissions and limitations
14 #    under the License.
15 
16 """
17 Track resources like memory and disk for a compute host.  Provides the
18 scheduler with useful information about availability through the ComputeNode
19 model.
20 """
21 import collections
22 import copy
23 
24 from keystoneauth1 import exceptions as ks_exc
25 import os_resource_classes as orc
26 import os_traits
27 from oslo_log import log as logging
28 from oslo_serialization import jsonutils
29 from oslo_utils import excutils
30 import retrying
31 
32 from nova.compute import claims
33 from nova.compute import monitors
34 from nova.compute import stats as compute_stats
35 from nova.compute import task_states
36 from nova.compute import utils as compute_utils
37 from nova.compute import vm_states
38 import nova.conf
39 from nova import exception
40 from nova.i18n import _
41 from nova import objects
42 from nova.objects import base as obj_base
43 from nova.objects import migration as migration_obj
44 from nova.pci import manager as pci_manager
45 from nova.pci import request as pci_request
46 from nova import rpc
47 from nova.scheduler.client import report
48 from nova import utils
49 from nova.virt import hardware
50 
51 
52 CONF = nova.conf.CONF
53 
54 LOG = logging.getLogger(__name__)
55 COMPUTE_RESOURCE_SEMAPHORE = "compute_resources"
56 
57 
58 def _instance_in_resize_state(instance):
59     """Returns True if the instance is in one of the resizing states.
60 
61     :param instance: `nova.objects.Instance` object
62     """
63     vm = instance.vm_state
64     task = instance.task_state
65 
66     if vm == vm_states.RESIZED:
67         return True
68 
69     if vm in [vm_states.ACTIVE, vm_states.STOPPED] and task in (
70             task_states.resizing_states + task_states.rebuild_states):
71         return True
72 
73     return False
74 
75 
76 def _is_trackable_migration(migration):
77     # Only look at resize/migrate migration and evacuation records
78     # NOTE(danms): RT should probably examine live migration
79     # records as well and do something smart. However, ignore
80     # those for now to avoid them being included in below calculations.
81     return migration.migration_type in ('resize', 'migration',
82                                         'evacuation')
83 
84 
85 def _normalize_inventory_from_cn_obj(inv_data, cn):
86     """Helper function that injects various information from a compute node
87     object into the inventory dict returned from the virt driver's
88     get_inventory() method. This function allows us to marry information like
89     *_allocation_ratio and reserved memory amounts that are in the
90     compute_nodes DB table and that the virt driver doesn't know about with the
91     information the virt driver *does* know about.
92 
93     Note that if the supplied inv_data contains allocation_ratio, reserved or
94     other fields, we DO NOT override the value with that of the compute node.
95     This is to ensure that the virt driver is the single source of truth
96     regarding inventory information. For instance, the Ironic virt driver will
97     always return a very specific inventory with allocation_ratios pinned to
98     1.0.
99 
100     :param inv_data: Dict, keyed by resource class, of inventory information
101                      returned from virt driver's get_inventory() method
102     :param compute_node: `objects.ComputeNode` describing the compute node
103     """
104     if orc.VCPU in inv_data:
105         cpu_inv = inv_data[orc.VCPU]
106         if 'allocation_ratio' not in cpu_inv:
107             cpu_inv['allocation_ratio'] = cn.cpu_allocation_ratio
108         if 'reserved' not in cpu_inv:
109             cpu_inv['reserved'] = CONF.reserved_host_cpus
110 
111     if orc.MEMORY_MB in inv_data:
112         mem_inv = inv_data[orc.MEMORY_MB]
113         if 'allocation_ratio' not in mem_inv:
114             mem_inv['allocation_ratio'] = cn.ram_allocation_ratio
115         if 'reserved' not in mem_inv:
116             mem_inv['reserved'] = CONF.reserved_host_memory_mb
117 
118     if orc.DISK_GB in inv_data:
119         disk_inv = inv_data[orc.DISK_GB]
120         if 'allocation_ratio' not in disk_inv:
121             disk_inv['allocation_ratio'] = cn.disk_allocation_ratio
122         if 'reserved' not in disk_inv:
123             # TODO(johngarbutt) We should either move to reserved_host_disk_gb
124             # or start tracking DISK_MB.
125             reserved_mb = CONF.reserved_host_disk_mb
126             reserved_gb = compute_utils.convert_mb_to_ceil_gb(reserved_mb)
127             disk_inv['reserved'] = reserved_gb
128 
129 
130 class ResourceTracker(object):
131     """Compute helper class for keeping track of resource usage as instances
132     are built and destroyed.
133     """
134 
135     def __init__(self, host, driver, reportclient=None):
136         self.host = host
137         self.driver = driver
138         self.pci_tracker = None
139         # Dict of objects.ComputeNode objects, keyed by nodename
140         self.compute_nodes = {}
141         # Dict of Stats objects, keyed by nodename
142         self.stats = collections.defaultdict(compute_stats.Stats)
143         # Set of UUIDs of instances tracked on this host.
144         self.tracked_instances = set()
145         self.tracked_migrations = {}
146         self.is_bfv = {}  # dict, keyed by instance uuid, to is_bfv boolean
147         monitor_handler = monitors.MonitorHandler(self)
148         self.monitors = monitor_handler.monitors
149         self.old_resources = collections.defaultdict(objects.ComputeNode)
150         self.reportclient = reportclient or report.SchedulerReportClient()
151         self.ram_allocation_ratio = CONF.ram_allocation_ratio
152         self.cpu_allocation_ratio = CONF.cpu_allocation_ratio
153         self.disk_allocation_ratio = CONF.disk_allocation_ratio
154         self.provider_tree = None
155         # Dict of assigned_resources, keyed by resource provider uuid
156         # the value is a dict again, keyed by resource class
157         # and value of this sub-dict is a set of Resource obj
158         self.assigned_resources = collections.defaultdict(
159             lambda: collections.defaultdict(set))
160 
161     @utils.synchronized(COMPUTE_RESOURCE_SEMAPHORE)
162     def instance_claim(self, context, instance, nodename, allocations,
163                        limits=None):
164         """Indicate that some resources are needed for an upcoming compute
165         instance build operation.
166 
167         This should be called before the compute node is about to perform
168         an instance build operation that will consume additional resources.
169 
170         :param context: security context
171         :param instance: instance to reserve resources for.
172         :type instance: nova.objects.instance.Instance object
173         :param nodename: The Ironic nodename selected by the scheduler
174         :param allocations: The placement allocation records for the instance.
175         :param limits: Dict of oversubscription limits for memory, disk,
176                        and CPUs.
177         :returns: A Claim ticket representing the reserved resources.  It can
178                   be used to revert the resource usage if an error occurs
179                   during the instance build.
180         """
181         if self.disabled(nodename):
182             # instance_claim() was called before update_available_resource()
183             # (which ensures that a compute node exists for nodename). We
184             # shouldn't get here but in case we do, just set the instance's
185             # host and nodename attribute (probably incorrect) and return a
186             # NoopClaim.
187             # TODO(jaypipes): Remove all the disabled junk from the resource
188             # tracker. Servicegroup API-level active-checking belongs in the
189             # nova-compute manager.
190             self._set_instance_host_and_node(instance, nodename)
191             return claims.NopClaim()
192 
193         # sanity checks:
194         if instance.host:
195             LOG.warning("Host field should not be set on the instance "
196                         "until resources have been claimed.",
197                         instance=instance)
198 
199         if instance.node:
200             LOG.warning("Node field should not be set on the instance "
201                         "until resources have been claimed.",
202                         instance=instance)
203 
204         cn = self.compute_nodes[nodename]
205         pci_requests = objects.InstancePCIRequests.get_by_instance_uuid(
206             context, instance.uuid)
207         claim = claims.Claim(context, instance, nodename, self, cn,
208                              pci_requests, limits=limits)
209 
210         # self._set_instance_host_and_node() will save instance to the DB
211         # so set instance.numa_topology first.  We need to make sure
212         # that numa_topology is saved while under COMPUTE_RESOURCE_SEMAPHORE
213         # so that the resource audit knows about any cpus we've pinned.
214         instance_numa_topology = claim.claimed_numa_topology
215         instance.numa_topology = instance_numa_topology
216         self._set_instance_host_and_node(instance, nodename)
217 
218         if self.pci_tracker:
219             # NOTE(jaypipes): ComputeNode.pci_device_pools is set below
220             # in _update_usage_from_instance().
221             self.pci_tracker.claim_instance(context, pci_requests,
222                                             instance_numa_topology)
223 
224         claimed_resources = self._claim_resources(allocations)
225         instance.resources = claimed_resources
226 
227         # Mark resources in-use and update stats
228         self._update_usage_from_instance(context, instance, nodename)
229 
230         elevated = context.elevated()
231         # persist changes to the compute node:
232         self._update(elevated, cn)
233 
234         return claim
235 
236     @utils.synchronized(COMPUTE_RESOURCE_SEMAPHORE)
237     def rebuild_claim(self, context, instance, nodename, allocations,
238                       limits=None, image_meta=None, migration=None):
239         """Create a claim for a rebuild operation."""
240         instance_type = instance.flavor
241         return self._move_claim(context, instance, instance_type, nodename,
242                                 migration, allocations, move_type='evacuation',
243                                 limits=limits, image_meta=image_meta)
244 
245     @utils.synchronized(COMPUTE_RESOURCE_SEMAPHORE)
246     def resize_claim(self, context, instance, instance_type, nodename,
247                      migration, allocations, image_meta=None, limits=None):
248         """Create a claim for a resize or cold-migration move.
249 
250         Note that this code assumes ``instance.new_flavor`` is set when
251         resizing with a new flavor.
252         """
253         return self._move_claim(context, instance, instance_type, nodename,
254                                 migration, allocations, image_meta=image_meta,
255                                 limits=limits)
256 
257     @utils.synchronized(COMPUTE_RESOURCE_SEMAPHORE)
258     def live_migration_claim(self, context, instance, nodename, migration,
259                              limits):
260         """Builds a MoveClaim for a live migration.
261 
262         :param context: The request context.
263         :param instance: The instance being live migrated.
264         :param nodename: The nodename of the destination host.
265         :param migration: The Migration object associated with this live
266                           migration.
267         :param limits: A SchedulerLimits object from when the scheduler
268                        selected the destination host.
269         :returns: A MoveClaim for this live migration.
270         """
271         # Flavor and image cannot change during a live migration.
272         instance_type = instance.flavor
273         image_meta = instance.image_meta
274         # TODO(Luyao) will pass allocations to live_migration_claim after the
275         # live migration change is done, now just set it None to _move_claim
276         return self._move_claim(context, instance, instance_type, nodename,
277                                 migration, None, move_type='live-migration',
278                                 image_meta=image_meta, limits=limits)
279 
280     def _move_claim(self, context, instance, new_instance_type, nodename,
281                     migration, allocations, move_type=None,
282                     image_meta=None, limits=None):
283         """Indicate that resources are needed for a move to this host.
284 
285         Move can be either a migrate/resize, live-migrate or an
286         evacuate/rebuild operation.
287 
288         :param context: security context
289         :param instance: instance object to reserve resources for
290         :param new_instance_type: new instance_type being resized to
291         :param nodename: The Ironic nodename selected by the scheduler
292         :param migration: A migration object if one was already created
293                           elsewhere for this operation (otherwise None)
294         :param allocations: the placement allocation records.
295         :param move_type: move type - can be one of 'migration', 'resize',
296                          'live-migration', 'evacuate'
297         :param image_meta: instance image metadata
298         :param limits: Dict of oversubscription limits for memory, disk,
299         and CPUs
300         :returns: A Claim ticket representing the reserved resources.  This
301         should be turned into finalize  a resource claim or free
302         resources after the compute operation is finished.
303         """
304         image_meta = image_meta or {}
305         if migration:
306             self._claim_existing_migration(migration, nodename)
307         else:
308             migration = self._create_migration(context, instance,
309                                                new_instance_type,
310                                                nodename, move_type)
311 
312         if self.disabled(nodename):
313             # compute_driver doesn't support resource tracking, just
314             # generate the migration record and continue the resize:
315             return claims.NopClaim(migration=migration)
316 
317         cn = self.compute_nodes[nodename]
318 
319         # TODO(moshele): we are recreating the pci requests even if
320         # there was no change on resize. This will cause allocating
321         # the old/new pci device in the resize phase. In the future
322         # we would like to optimise this.
323         new_pci_requests = pci_request.get_pci_requests_from_flavor(
324             new_instance_type)
325         new_pci_requests.instance_uuid = instance.uuid
326         # On resize merge the SR-IOV ports pci_requests
327         # with the new instance flavor pci_requests.
328         if instance.pci_requests:
329             for request in instance.pci_requests.requests:
330                 if request.source == objects.InstancePCIRequest.NEUTRON_PORT:
331                     new_pci_requests.requests.append(request)
332         claim = claims.MoveClaim(context, instance, nodename,
333                                  new_instance_type, image_meta, self, cn,
334                                  new_pci_requests, migration, limits=limits)
335 
336         claimed_pci_devices_objs = []
337         # TODO(artom) The second part of this condition should not be
338         # necessary, but since SRIOV live migration is currently handled
339         # elsewhere - see for example _claim_pci_for_instance_vifs() in the
340         # compute manager - we don't do any PCI claims if this is a live
341         # migration to avoid stepping on that code's toes. Ideally,
342         # MoveClaim/this method would be used for all live migration resource
343         # claims.
344         if self.pci_tracker and migration.migration_type != 'live-migration':
345             # NOTE(jaypipes): ComputeNode.pci_device_pools is set below
346             # in _update_usage_from_instance().
347             claimed_pci_devices_objs = self.pci_tracker.claim_instance(
348                     context, new_pci_requests, claim.claimed_numa_topology)
349         claimed_pci_devices = objects.PciDeviceList(
350                 objects=claimed_pci_devices_objs)
351 
352         claimed_resources = self._claim_resources(allocations)
353         old_resources = instance.resources
354 
355         # TODO(jaypipes): Move claimed_numa_topology out of the Claim's
356         # constructor flow so the Claim constructor only tests whether
357         # resources can be claimed, not consume the resources directly.
358         mig_context = objects.MigrationContext(
359             context=context, instance_uuid=instance.uuid,
360             migration_id=migration.id,
361             old_numa_topology=instance.numa_topology,
362             new_numa_topology=claim.claimed_numa_topology,
363             old_pci_devices=instance.pci_devices,
364             new_pci_devices=claimed_pci_devices,
365             old_pci_requests=instance.pci_requests,
366             new_pci_requests=new_pci_requests,
367             old_resources=old_resources,
368             new_resources=claimed_resources)
369 
370         instance.migration_context = mig_context
371         instance.save()
372 
373         # Mark the resources in-use for the resize landing on this
374         # compute host:
375         self._update_usage_from_migration(context, instance, migration,
376                                           nodename)
377         elevated = context.elevated()
378         self._update(elevated, cn)
379 
380         return claim
381 
382     def _create_migration(self, context, instance, new_instance_type,
383                           nodename, move_type=None):
384         """Create a migration record for the upcoming resize.  This should
385         be done while the COMPUTE_RESOURCES_SEMAPHORE is held so the resource
386         claim will not be lost if the audit process starts.
387         """
388         migration = objects.Migration(context=context.elevated())
389         migration.dest_compute = self.host
390         migration.dest_node = nodename
391         migration.dest_host = self.driver.get_host_ip_addr()
392         migration.old_instance_type_id = instance.flavor.id
393         migration.new_instance_type_id = new_instance_type.id
394         migration.status = 'pre-migrating'
395         migration.instance_uuid = instance.uuid
396         migration.source_compute = instance.host
397         migration.source_node = instance.node
398         if move_type:
399             migration.migration_type = move_type
400         else:
401             migration.migration_type = migration_obj.determine_migration_type(
402                 migration)
403         migration.create()
404         return migration
405 
406     def _claim_existing_migration(self, migration, nodename):
407         """Make an existing migration record count for resource tracking.
408 
409         If a migration record was created already before the request made
410         it to this compute host, only set up the migration so it's included in
411         resource tracking. This should be done while the
412         COMPUTE_RESOURCES_SEMAPHORE is held.
413         """
414         migration.dest_compute = self.host
415         migration.dest_node = nodename
416         migration.dest_host = self.driver.get_host_ip_addr()
417         # NOTE(artom) Migration objects for live migrations are created with
418         # status 'accepted' by the conductor in live_migrate_instance() and do
419         # not have a 'pre-migrating' status.
420         if migration.migration_type != 'live-migration':
421             migration.status = 'pre-migrating'
422         migration.save()
423 
424     def _claim_resources(self, allocations):
425         """Claim resources according to assigned resources from allocations
426         and available resources in provider tree
427         """
428         if not allocations:
429             return None
430         claimed_resources = []
431         for rp_uuid, alloc_dict in allocations.items():
432             try:
433                 provider_data = self.provider_tree.data(rp_uuid)
434             except ValueError:
435                 # If an instance is in evacuating, it will hold new and old
436                 # allocations, but the provider UUIDs in old allocations won't
437                 # exist in the current provider tree, so skip it.
438                 LOG.debug("Skip claiming resources of provider %(rp_uuid)s, "
439                           "since the provider UUIDs are not in provider tree.",
440                           {'rp_uuid': rp_uuid})
441                 continue
442             for rc, amount in alloc_dict['resources'].items():
443                 if rc not in provider_data.resources:
444                     # This means we don't use provider_data.resources to
445                     # assign this kind of resource class, such as 'VCPU' for
446                     # now, otherwise the provider_data.resources will be
447                     # populated with this resource class when updating
448                     # provider tree.
449                     continue
450                 assigned = self.assigned_resources[rp_uuid][rc]
451                 free = provider_data.resources[rc] - assigned
452                 if amount > len(free):
453                     reason = (_("Needed %(amount)d units of resource class "
454                                 "%(rc)s, but %(avail)d are available.") %
455                                 {'amount': amount,
456                                  'rc': rc,
457                                  'avail': len(free)})
458                     raise exception.ComputeResourcesUnavailable(reason=reason)
459                 for i in range(amount):
460                     claimed_resources.append(free.pop())
461 
462         if claimed_resources:
463             self._add_assigned_resources(claimed_resources)
464             return objects.ResourceList(objects=claimed_resources)
465 
466     def _populate_assigned_resources(self, context, instance_by_uuid):
467         """Populate self.assigned_resources organized by resource class and
468         reource provider uuid, which is as following format:
469         {
470         $RP_UUID: {
471             $RESOURCE_CLASS: [objects.Resource, ...],
472             $RESOURCE_CLASS: [...]},
473         ...}
474         """
475         resources = []
476 
477         # Get resources assigned to migrations
478         for mig in self.tracked_migrations.values():
479             mig_ctx = mig.instance.migration_context
480             if mig.source_compute == self.host and 'old_resources' in mig_ctx:
481                 resources.extend(mig_ctx.old_resources or [])
482             if mig.dest_compute == self.host and 'new_resources' in mig_ctx:
483                 resources.extend(mig_ctx.new_resources or [])
484 
485         # Get resources assigned to instances
486         for uuid in self.tracked_instances:
487             resources.extend(instance_by_uuid[uuid].resources or [])
488 
489         self.assigned_resources.clear()
490         self._add_assigned_resources(resources)
491 
492     def _check_resources(self, context):
493         """Check if there are assigned resources not found in provider tree"""
494         notfound = set()
495         for rp_uuid in self.assigned_resources:
496             provider_data = self.provider_tree.data(rp_uuid)
497             for rc, assigned in self.assigned_resources[rp_uuid].items():
498                 notfound |= (assigned - provider_data.resources[rc])
499 
500         if not notfound:
501             return
502 
503         # This only happens when assigned resources are removed
504         # from the configuration and the compute service is SIGHUP'd
505         # or restarted.
506         resources = [(res.identifier, res.resource_class) for res in notfound]
507         reason = _("The following resources are assigned to instances, "
508                    "but were not listed in the configuration: %s "
509                    "Please check if this will influence your instances, "
510                    "and restore your configuration if necessary") % resources
511         raise exception.AssignedResourceNotFound(reason=reason)
512 
513     def _release_assigned_resources(self, resources):
514         """Remove resources from self.assigned_resources."""
515         if not resources:
516             return
517         for resource in resources:
518             rp_uuid = resource.provider_uuid
519             rc = resource.resource_class
520             try:
521                 self.assigned_resources[rp_uuid][rc].remove(resource)
522             except KeyError:
523                 LOG.warning("Release resource %(rc)s: %(id)s of provider "
524                             "%(rp_uuid)s, not tracked in "
525                             "ResourceTracker.assigned_resources.",
526                             {'rc': rc, 'id': resource.identifier,
527                              'rp_uuid': rp_uuid})
528 
529     def _add_assigned_resources(self, resources):
530         """Add resources to self.assigned_resources"""
531         if not resources:
532             return
533         for resource in resources:
534             rp_uuid = resource.provider_uuid
535             rc = resource.resource_class
536             self.assigned_resources[rp_uuid][rc].add(resource)
537 
538     def _set_instance_host_and_node(self, instance, nodename):
539         """Tag the instance as belonging to this host.  This should be done
540         while the COMPUTE_RESOURCES_SEMAPHORE is held so the resource claim
541         will not be lost if the audit process starts.
542         """
543         instance.host = self.host
544         instance.launched_on = self.host
545         instance.node = nodename
546         instance.save()
547 
548     def _unset_instance_host_and_node(self, instance):
549         """Untag the instance so it no longer belongs to the host.
550 
551         This should be done while the COMPUTE_RESOURCES_SEMAPHORE is held so
552         the resource claim will not be lost if the audit process starts.
553         """
554         instance.host = None
555         instance.node = None
556         instance.save()
557 
558     @utils.synchronized(COMPUTE_RESOURCE_SEMAPHORE)
559     def abort_instance_claim(self, context, instance, nodename):
560         """Remove usage from the given instance."""
561         self._update_usage_from_instance(context, instance, nodename,
562                                          is_removed=True)
563 
564         instance.clear_numa_topology()
565         self._unset_instance_host_and_node(instance)
566 
567         self._update(context.elevated(), self.compute_nodes[nodename])
568 
569     def _drop_pci_devices(self, instance, nodename, prefix):
570         if self.pci_tracker:
571             # free old/new allocated pci devices
572             pci_devices = self._get_migration_context_resource(
573                 'pci_devices', instance, prefix=prefix)
574             if pci_devices:
575                 for pci_device in pci_devices:
576                     self.pci_tracker.free_device(pci_device, instance)
577 
578                 dev_pools_obj = self.pci_tracker.stats.to_device_pools_obj()
579                 self.compute_nodes[nodename].pci_device_pools = dev_pools_obj
580 
581     @utils.synchronized(COMPUTE_RESOURCE_SEMAPHORE)
582     def drop_move_claim(self, context, instance, nodename,
583                         instance_type=None, prefix='new_'):
584         """Remove usage for an incoming/outgoing migration.
585 
586         :param context: Security context.
587         :param instance: The instance whose usage is to be removed.
588         :param nodename: Host on which to remove usage. If the migration
589                          completed successfully, this is normally the source.
590                          If it did not complete successfully (failed or
591                          reverted), this is normally the destination.
592         :param instance_type: The flavor that determines the usage to remove.
593                               If the migration completed successfully, this is
594                               the old flavor to be removed from the source. If
595                               the migration did not complete successfully, this
596                               is the new flavor to be removed from the
597                               destination.
598         :param prefix: Prefix to use when accessing migration context
599                        attributes. 'old_' or 'new_', with 'new_' being the
600                        default.
601         """
602         # Remove usage for an instance that is tracked in migrations, such as
603         # on the dest node during revert resize.
604         if instance['uuid'] in self.tracked_migrations:
605             migration = self.tracked_migrations.pop(instance['uuid'])
606             if not instance_type:
607                 instance_type = self._get_instance_type(instance, prefix,
608                                                         migration)
609         # Remove usage for an instance that is not tracked in migrations (such
610         # as on the source node after a migration).
611         # NOTE(lbeliveau): On resize on the same node, the instance is
612         # included in both tracked_migrations and tracked_instances.
613         elif instance['uuid'] in self.tracked_instances:
614             self.tracked_instances.remove(instance['uuid'])
615 
616         if instance_type is not None:
617             numa_topology = self._get_migration_context_resource(
618                 'numa_topology', instance, prefix=prefix)
619             usage = self._get_usage_dict(
620                     instance_type, instance, numa_topology=numa_topology)
621             self._drop_pci_devices(instance, nodename, prefix)
622             resources = self._get_migration_context_resource(
623                 'resources', instance, prefix=prefix)
624             self._release_assigned_resources(resources)
625             self._update_usage(usage, nodename, sign=-1)
626 
627             ctxt = context.elevated()
628             self._update(ctxt, self.compute_nodes[nodename])
629 
630     @utils.synchronized(COMPUTE_RESOURCE_SEMAPHORE)
631     def update_usage(self, context, instance, nodename):
632         """Update the resource usage and stats after a change in an
633         instance
634         """
635         if self.disabled(nodename):
636             return
637 
638         uuid = instance['uuid']
639 
640         # don't update usage for this instance unless it submitted a resource
641         # claim first:
642         if uuid in self.tracked_instances:
643             self._update_usage_from_instance(context, instance, nodename)
644             self._update(context.elevated(), self.compute_nodes[nodename])
645 
646     def disabled(self, nodename):
647         return (nodename not in self.compute_nodes or
648                 not self.driver.node_is_available(nodename))
649 
650     def _check_for_nodes_rebalance(self, context, resources, nodename):
651         """Check if nodes rebalance has happened.
652 
653         The ironic driver maintains a hash ring mapping bare metal nodes
654         to compute nodes. If a compute dies, the hash ring is rebuilt, and
655         some of its bare metal nodes (more precisely, those not in ACTIVE
656         state) are assigned to other computes.
657 
658         This method checks for this condition and adjusts the database
659         accordingly.
660 
661         :param context: security context
662         :param resources: initial values
663         :param nodename: node name
664         :returns: True if a suitable compute node record was found, else False
665         """
666         if not self.driver.rebalances_nodes:
667             return False
668 
669         # Its possible ironic just did a node re-balance, so let's
670         # check if there is a compute node that already has the correct
671         # hypervisor_hostname. We can re-use that rather than create a
672         # new one and have to move existing placement allocations
673         cn_candidates = objects.ComputeNodeList.get_by_hypervisor(
674             context, nodename)
675 
676         if len(cn_candidates) == 1:
677             cn = cn_candidates[0]
678             LOG.info("ComputeNode %(name)s moving from %(old)s to %(new)s",
679                      {"name": nodename, "old": cn.host, "new": self.host})
680             cn.host = self.host
681             self.compute_nodes[nodename] = cn
682             self._copy_resources(cn, resources)
683             self._setup_pci_tracker(context, cn, resources)
684             self._update(context, cn)
685             return True
686         elif len(cn_candidates) > 1:
687             LOG.error(
688                 "Found more than one ComputeNode for nodename %s. "
689                 "Please clean up the orphaned ComputeNode records in your DB.",
690                 nodename)
691 
692         return False
693 
694     def _init_compute_node(self, context, resources):
695         """Initialize the compute node if it does not already exist.
696 
697         The resource tracker will be inoperable if compute_node
698         is not defined. The compute_node will remain undefined if
699         we fail to create it or if there is no associated service
700         registered.
701 
702         If this method has to create a compute node it needs initial
703         values - these come from resources.
704 
705         :param context: security context
706         :param resources: initial values
707         :returns: True if a new compute_nodes table record was created,
708             False otherwise
709         """
710         nodename = resources['hypervisor_hostname']
711 
712         # if there is already a compute node just use resources
713         # to initialize
714         if nodename in self.compute_nodes:
715             cn = self.compute_nodes[nodename]
716             self._copy_resources(cn, resources)
717             self._setup_pci_tracker(context, cn, resources)
718             return False
719 
720         # now try to get the compute node record from the
721         # database. If we get one we use resources to initialize
722         cn = self._get_compute_node(context, nodename)
723         if cn:
724             self.compute_nodes[nodename] = cn
725             self._copy_resources(cn, resources)
726             self._setup_pci_tracker(context, cn, resources)
727             return False
728 
729         if self._check_for_nodes_rebalance(context, resources, nodename):
730             return False
731 
732         # there was no local copy and none in the database
733         # so we need to create a new compute node. This needs
734         # to be initialized with resource values.
735         cn = objects.ComputeNode(context)
736         cn.host = self.host
737         self._copy_resources(cn, resources, initial=True)
738         cn.create()
739         # Only map the ComputeNode into compute_nodes if create() was OK
740         # because if create() fails, on the next run through here nodename
741         # would be in compute_nodes and we won't try to create again (because
742         # of the logic above).
743         self.compute_nodes[nodename] = cn
744         LOG.info('Compute node record created for '
745                  '%(host)s:%(node)s with uuid: %(uuid)s',
746                  {'host': self.host, 'node': nodename, 'uuid': cn.uuid})
747 
748         self._setup_pci_tracker(context, cn, resources)
749         return True
750 
751     def _setup_pci_tracker(self, context, compute_node, resources):
752         if not self.pci_tracker:
753             n_id = compute_node.id
754             self.pci_tracker = pci_manager.PciDevTracker(context, node_id=n_id)
755             if 'pci_passthrough_devices' in resources:
756                 dev_json = resources.pop('pci_passthrough_devices')
757                 self.pci_tracker.update_devices_from_hypervisor_resources(
758                         dev_json)
759 
760             dev_pools_obj = self.pci_tracker.stats.to_device_pools_obj()
761             compute_node.pci_device_pools = dev_pools_obj
762 
763     def _copy_resources(self, compute_node, resources, initial=False):
764         """Copy resource values to supplied compute_node."""
765         nodename = resources['hypervisor_hostname']
766         stats = self.stats[nodename]
767         # purge old stats and init with anything passed in by the driver
768         # NOTE(danms): Preserve 'failed_builds' across the stats clearing,
769         # as that is not part of resources
770         # TODO(danms): Stop doing this when we get a column to store this
771         # directly
772         prev_failed_builds = stats.get('failed_builds', 0)
773         stats.clear()
774         stats['failed_builds'] = prev_failed_builds
775         stats.digest_stats(resources.get('stats'))
776         compute_node.stats = stats
777 
778         # Update the allocation ratios for the related ComputeNode object
779         # but only if the configured values are not the default; the
780         # ComputeNode._from_db_object method takes care of providing default
781         # allocation ratios when the config is left at the default, so
782         # we'll really end up with something like a
783         # ComputeNode.cpu_allocation_ratio of 16.0. We want to avoid
784         # resetting the ComputeNode fields to None because that will make
785         # the _resource_change method think something changed when really it
786         # didn't.
787         # NOTE(yikun): The CONF.initial_(cpu|ram|disk)_allocation_ratio would
788         # be used when we initialize the compute node object, that means the
789         # ComputeNode.(cpu|ram|disk)_allocation_ratio will be set to
790         # CONF.initial_(cpu|ram|disk)_allocation_ratio when initial flag is
791         # True.
792         for res in ('cpu', 'disk', 'ram'):
793             attr = '%s_allocation_ratio' % res
794             if initial:
795                 conf_alloc_ratio = getattr(CONF, 'initial_%s' % attr)
796             else:
797                 conf_alloc_ratio = getattr(self, attr)
798             # NOTE(yikun): In Stein version, we change the default value of
799             # (cpu|ram|disk)_allocation_ratio from 0.0 to None, but we still
800             # should allow 0.0 to keep compatibility, and this 0.0 condition
801             # will be removed in the next version (T version).
802             if conf_alloc_ratio not in (0.0, None):
803                 setattr(compute_node, attr, conf_alloc_ratio)
804 
805         # now copy rest to compute_node
806         compute_node.update_from_virt_driver(resources)
807 
808     def remove_node(self, nodename):
809         """Handle node removal/rebalance.
810 
811         Clean up any stored data about a compute node no longer
812         managed by this host.
813         """
814         self.stats.pop(nodename, None)
815         self.compute_nodes.pop(nodename, None)
816         self.old_resources.pop(nodename, None)
817 
818     def _get_host_metrics(self, context, nodename):
819         """Get the metrics from monitors and
820         notify information to message bus.
821         """
822         metrics = objects.MonitorMetricList()
823         metrics_info = {}
824         for monitor in self.monitors:
825             try:
826                 monitor.populate_metrics(metrics)
827             except NotImplementedError:
828                 LOG.debug("The compute driver doesn't support host "
829                           "metrics for  %(mon)s", {'mon': monitor})
830             except Exception as exc:
831                 LOG.warning("Cannot get the metrics from %(mon)s; "
832                             "error: %(exc)s",
833                             {'mon': monitor, 'exc': exc})
834         # TODO(jaypipes): Remove this when compute_node.metrics doesn't need
835         # to be populated as a JSONified string.
836         metric_list = metrics.to_list()
837         if len(metric_list):
838             metrics_info['nodename'] = nodename
839             metrics_info['metrics'] = metric_list
840             metrics_info['host'] = self.host
841             metrics_info['host_ip'] = CONF.my_ip
842             notifier = rpc.get_notifier(service='compute', host=nodename)
843             notifier.info(context, 'compute.metrics.update', metrics_info)
844             compute_utils.notify_about_metrics_update(
845                 context, self.host, CONF.my_ip, nodename, metrics)
846         return metric_list
847 
848     def update_available_resource(self, context, nodename, startup=False):
849         """Override in-memory calculations of compute node resource usage based
850         on data audited from the hypervisor layer.
851 
852         Add in resource claims in progress to account for operations that have
853         declared a need for resources, but not necessarily retrieved them from
854         the hypervisor layer yet.
855 
856         :param nodename: Temporary parameter representing the Ironic resource
857                          node. This parameter will be removed once Ironic
858                          baremetal resource nodes are handled like any other
859                          resource in the system.
860         :param startup: Boolean indicating whether we're running this on
861                         on startup (True) or periodic (False).
862         """
863         LOG.debug("Auditing locally available compute resources for "
864                   "%(host)s (node: %(node)s)",
865                  {'node': nodename,
866                   'host': self.host})
867         resources = self.driver.get_available_resource(nodename)
868         # NOTE(jaypipes): The resources['hypervisor_hostname'] field now
869         # contains a non-None value, even for non-Ironic nova-compute hosts. It
870         # is this value that will be populated in the compute_nodes table.
871         resources['host_ip'] = CONF.my_ip
872 
873         # We want the 'cpu_info' to be None from the POV of the
874         # virt driver, but the DB requires it to be non-null so
875         # just force it to empty string
876         if "cpu_info" not in resources or resources["cpu_info"] is None:
877             resources["cpu_info"] = ''
878 
879         self._verify_resources(resources)
880 
881         self._report_hypervisor_resource_view(resources)
882 
883         self._update_available_resource(context, resources, startup=startup)
884 
885     def _pair_instances_to_migrations(self, migrations, instance_by_uuid):
886         for migration in migrations:
887             try:
888                 migration.instance = instance_by_uuid[migration.instance_uuid]
889             except KeyError:
890                 # NOTE(danms): If this happens, we don't set it here, and
891                 # let the code either fail or lazy-load the instance later
892                 # which is what happened before we added this optimization.
893                 # NOTE(tdurakov) this situation is possible for resize/cold
894                 # migration when migration is finished but haven't yet
895                 # confirmed/reverted in that case instance already changed host
896                 # to destination and no matching happens
897                 LOG.debug('Migration for instance %(uuid)s refers to '
898                               'another host\'s instance!',
899                           {'uuid': migration.instance_uuid})
900 
901     @utils.synchronized(COMPUTE_RESOURCE_SEMAPHORE)
902     def _update_available_resource(self, context, resources, startup=False):
903 
904         # initialize the compute node object, creating it
905         # if it does not already exist.
906         is_new_compute_node = self._init_compute_node(context, resources)
907 
908         nodename = resources['hypervisor_hostname']
909 
910         # if we could not init the compute node the tracker will be
911         # disabled and we should quit now
912         if self.disabled(nodename):
913             return
914 
915         # Grab all instances assigned to this node:
916         instances = objects.InstanceList.get_by_host_and_node(
917             context, self.host, nodename,
918             expected_attrs=['system_metadata',
919                             'numa_topology',
920                             'flavor', 'migration_context',
921                             'resources'])
922 
923         # Now calculate usage based on instance utilization:
924         instance_by_uuid = self._update_usage_from_instances(
925             context, instances, nodename)
926 
927         # Grab all in-progress migrations:
928         migrations = objects.MigrationList.get_in_progress_by_host_and_node(
929                 context, self.host, nodename)
930 
931         self._pair_instances_to_migrations(migrations, instance_by_uuid)
932         self._update_usage_from_migrations(context, migrations, nodename)
933 
934         # A new compute node means there won't be a resource provider yet since
935         # that would be created via the _update() call below, and if there is
936         # no resource provider then there are no allocations against it.
937         if not is_new_compute_node:
938             self._remove_deleted_instances_allocations(
939                 context, self.compute_nodes[nodename], migrations,
940                 instance_by_uuid)
941 
942         # Detect and account for orphaned instances that may exist on the
943         # hypervisor, but are not in the DB:
944         orphans = self._find_orphaned_instances()
945         self._update_usage_from_orphans(orphans, nodename)
946 
947         cn = self.compute_nodes[nodename]
948 
949         # NOTE(yjiang5): Because pci device tracker status is not cleared in
950         # this periodic task, and also because the resource tracker is not
951         # notified when instances are deleted, we need remove all usages
952         # from deleted instances.
953         self.pci_tracker.clean_usage(instances, migrations, orphans)
954         dev_pools_obj = self.pci_tracker.stats.to_device_pools_obj()
955         cn.pci_device_pools = dev_pools_obj
956 
957         self._report_final_resource_view(nodename)
958 
959         metrics = self._get_host_metrics(context, nodename)
960         # TODO(pmurray): metrics should not be a json string in ComputeNode,
961         # but it is. This should be changed in ComputeNode
962         cn.metrics = jsonutils.dumps(metrics)
963 
964         # Update assigned resources to self.assigned_resources
965         self._populate_assigned_resources(context, instance_by_uuid)
966 
967         # update the compute_node
968         self._update(context, cn, startup=startup)
969         LOG.debug('Compute_service record updated for %(host)s:%(node)s',
970                   {'host': self.host, 'node': nodename})
971 
972         # Check if there is any resource assigned but not found
973         # in provider tree
974         if startup:
975             self._check_resources(context)
976 
977     def _get_compute_node(self, context, nodename):
978         """Returns compute node for the host and nodename."""
979         try:
980             return objects.ComputeNode.get_by_host_and_nodename(
981                 context, self.host, nodename)
982         except exception.NotFound:
983             LOG.warning("No compute node record for %(host)s:%(node)s",
984                         {'host': self.host, 'node': nodename})
985 
986     def _report_hypervisor_resource_view(self, resources):
987         """Log the hypervisor's view of free resources.
988 
989         This is just a snapshot of resource usage recorded by the
990         virt driver.
991 
992         The following resources are logged:
993             - free memory
994             - free disk
995             - free CPUs
996             - assignable PCI devices
997         """
998         nodename = resources['hypervisor_hostname']
999         free_ram_mb = resources['memory_mb'] - resources['memory_mb_used']
1000         free_disk_gb = resources['local_gb'] - resources['local_gb_used']
1001         vcpus = resources['vcpus']
1002         if vcpus:
1003             free_vcpus = vcpus - resources['vcpus_used']
1004         else:
1005             free_vcpus = 'unknown'
1006 
1007         pci_devices = resources.get('pci_passthrough_devices')
1008 
1009         LOG.debug("Hypervisor/Node resource view: "
1010                   "name=%(node)s "
1011                   "free_ram=%(free_ram)sMB "
1012                   "free_disk=%(free_disk)sGB "
1013                   "free_vcpus=%(free_vcpus)s "
1014                   "pci_devices=%(pci_devices)s",
1015                   {'node': nodename,
1016                    'free_ram': free_ram_mb,
1017                    'free_disk': free_disk_gb,
1018                    'free_vcpus': free_vcpus,
1019                    'pci_devices': pci_devices})
1020 
1021     def _report_final_resource_view(self, nodename):
1022         """Report final calculate of physical memory, used virtual memory,
1023         disk, usable vCPUs, used virtual CPUs and PCI devices,
1024         including instance calculations and in-progress resource claims. These
1025         values will be exposed via the compute node table to the scheduler.
1026         """
1027         cn = self.compute_nodes[nodename]
1028         vcpus = cn.vcpus
1029         if vcpus:
1030             tcpu = vcpus
1031             ucpu = cn.vcpus_used
1032             LOG.debug("Total usable vcpus: %(tcpu)s, "
1033                       "total allocated vcpus: %(ucpu)s",
1034                       {'tcpu': vcpus,
1035                        'ucpu': ucpu})
1036         else:
1037             tcpu = 0
1038             ucpu = 0
1039         pci_stats = (list(cn.pci_device_pools) if
1040             cn.pci_device_pools else [])
1041         LOG.debug("Final resource view: "
1042                   "name=%(node)s "
1043                   "phys_ram=%(phys_ram)sMB "
1044                   "used_ram=%(used_ram)sMB "
1045                   "phys_disk=%(phys_disk)sGB "
1046                   "used_disk=%(used_disk)sGB "
1047                   "total_vcpus=%(total_vcpus)s "
1048                   "used_vcpus=%(used_vcpus)s "
1049                   "pci_stats=%(pci_stats)s",
1050                   {'node': nodename,
1051                    'phys_ram': cn.memory_mb,
1052                    'used_ram': cn.memory_mb_used,
1053                    'phys_disk': cn.local_gb,
1054                    'used_disk': cn.local_gb_used,
1055                    'total_vcpus': tcpu,
1056                    'used_vcpus': ucpu,
1057                    'pci_stats': pci_stats})
1058 
1059     def _resource_change(self, compute_node):
1060         """Check to see if any resources have changed."""
1061         nodename = compute_node.hypervisor_hostname
1062         old_compute = self.old_resources[nodename]
1063         if not obj_base.obj_equal_prims(
1064                 compute_node, old_compute, ['updated_at']):
1065             self.old_resources[nodename] = copy.deepcopy(compute_node)
1066             return True
1067         return False
1068 
1069     def _sync_compute_service_disabled_trait(self, context, traits):
1070         """Synchronize the COMPUTE_STATUS_DISABLED trait on the node provider.
1071 
1072         Determines if the COMPUTE_STATUS_DISABLED trait should be added to
1073         or removed from the provider's set of traits based on the related
1074         nova-compute service disabled status.
1075 
1076         :param context: RequestContext for cell database access
1077         :param traits: set of traits for the compute node resource provider;
1078             this is modified by reference
1079         """
1080         trait = os_traits.COMPUTE_STATUS_DISABLED
1081         try:
1082             service = objects.Service.get_by_compute_host(context, self.host)
1083             if service.disabled:
1084                 # The service is disabled so make sure the trait is reported.
1085                 traits.add(trait)
1086             else:
1087                 # The service is not disabled so do not report the trait.
1088                 traits.discard(trait)
1089         except exception.NotFound:
1090             # This should not happen but handle it gracefully. The scheduler
1091             # should ignore this node if the compute service record is gone.
1092             LOG.error('Unable to find services table record for nova-compute '
1093                       'host %s', self.host)
1094 
1095     def _get_traits(self, context, nodename, provider_tree):
1096         """Synchronizes internal and external traits for the node provider.
1097 
1098         This works in conjunction with the ComptueDriver.update_provider_tree
1099         flow and is used to synchronize traits reported by the compute driver,
1100         traits based on information in the ComputeNode record, and traits set
1101         externally using the placement REST API.
1102 
1103         :param context: RequestContext for cell database access
1104         :param nodename: ComputeNode.hypervisor_hostname for the compute node
1105             resource provider whose traits are being synchronized; the node
1106             must be in the ProviderTree.
1107         :param provider_tree: ProviderTree being updated
1108         """
1109         # Get the traits from the ProviderTree which will be the set
1110         # of virt-owned traits plus any externally defined traits set
1111         # on the provider that aren't owned by the virt driver.
1112         traits = provider_tree.data(nodename).traits
1113 
1114         # Now get the driver's capabilities and add any supported
1115         # traits that are missing, and remove any existing set traits
1116         # that are not currently supported.
1117         for trait, supported in self.driver.capabilities_as_traits().items():
1118             if supported:
1119                 traits.add(trait)
1120             elif trait in traits:
1121                 traits.remove(trait)
1122 
1123         self._sync_compute_service_disabled_trait(context, traits)
1124 
1125         return list(traits)
1126 
1127     @retrying.retry(stop_max_attempt_number=4,
1128                     retry_on_exception=lambda e: isinstance(
1129                         e, exception.ResourceProviderUpdateConflict))
1130     def _update_to_placement(self, context, compute_node, startup):
1131         """Send resource and inventory changes to placement."""
1132         # NOTE(jianghuaw): Some resources(e.g. VGPU) are not saved in the
1133         # object of compute_node; instead the inventory data for these
1134         # resource is reported by driver's get_inventory(). So even there
1135         # is no resource change for compute_node as above, we need proceed
1136         # to get inventory and use report client interfaces to update
1137         # inventory to placement. It's report client's responsibility to
1138         # ensure the update request to placement only happens when inventory
1139         # is changed.
1140         nodename = compute_node.hypervisor_hostname
1141         # Persist the stats to the Scheduler
1142         # First try update_provider_tree
1143         # Retrieve the provider tree associated with this compute node.  If
1144         # it doesn't exist yet, this will create it with a (single, root)
1145         # provider corresponding to the compute node.
1146         prov_tree = self.reportclient.get_provider_tree_and_ensure_root(
1147             context, compute_node.uuid, name=compute_node.hypervisor_hostname)
1148         # Let the virt driver rearrange the provider tree and set/update
1149         # the inventory, traits, and aggregates throughout.
1150         allocs = None
1151         try:
1152             try:
1153                 self.driver.update_provider_tree(prov_tree, nodename)
1154             except exception.ReshapeNeeded:
1155                 if not startup:
1156                     # This isn't supposed to happen during periodic, so raise
1157                     # it up; the compute manager will treat it specially.
1158                     raise
1159                 LOG.info("Performing resource provider inventory and "
1160                          "allocation data migration during compute service "
1161                          "startup or fast-forward upgrade.")
1162                 allocs = self.reportclient.get_allocations_for_provider_tree(
1163                     context, nodename)
1164                 self.driver.update_provider_tree(prov_tree, nodename,
1165                                                  allocations=allocs)
1166 
1167             # Inject driver capabilities traits into the provider
1168             # tree.  We need to determine the traits that the virt
1169             # driver owns - so those that come from the tree itself
1170             # (via the virt driver) plus the compute capabilities
1171             # traits, and then merge those with the traits set
1172             # externally that the driver does not own - and remove any
1173             # set on the provider externally that the virt owns but
1174             # aren't in the current list of supported traits.  For
1175             # example, let's say we reported multiattach support as a
1176             # trait at t1 and then at t2 it's not, so we need to
1177             # remove it.  But at both t1 and t2 there is a
1178             # CUSTOM_VENDOR_TRAIT_X which we can't touch because it
1179             # was set externally on the provider.
1180             # We also want to sync the COMPUTE_STATUS_DISABLED trait based
1181             # on the related nova-compute service's disabled status.
1182             traits = self._get_traits(
1183                 context, nodename, provider_tree=prov_tree)
1184             prov_tree.update_traits(nodename, traits)
1185         except NotImplementedError:
1186             # TODO(mriedem): Remove the compatibility code in the U release.
1187             LOG.warning('Compute driver "%s" does not implement the '
1188                         '"update_provider_tree" interface. Compatibility for '
1189                         'non-update_provider_tree interfaces will be removed '
1190                         'in a future release and result in an error to report '
1191                         'inventory for this compute service.',
1192                         CONF.compute_driver)
1193             # update_provider_tree isn't implemented yet - try get_inventory
1194             try:
1195                 inv_data = self.driver.get_inventory(nodename)
1196                 _normalize_inventory_from_cn_obj(inv_data, compute_node)
1197             except NotImplementedError:
1198                 # Eventually all virt drivers will return an inventory dict in
1199                 # the format that the placement API expects and we'll be able
1200                 # to remove this code branch
1201                 inv_data = compute_utils.compute_node_to_inventory_dict(
1202                     compute_node)
1203 
1204             prov_tree.update_inventory(nodename, inv_data)
1205 
1206         self.provider_tree = prov_tree
1207 
1208         # Flush any changes. If we processed ReshapeNeeded above, allocs is not
1209         # None, and this will hit placement's POST /reshaper route.
1210         self.reportclient.update_from_provider_tree(context, prov_tree,
1211                                                     allocations=allocs)
1212 
1213     def _update(self, context, compute_node, startup=False):
1214         """Update partial stats locally and populate them to Scheduler."""
1215         # _resource_change will update self.old_resources if it detects changes
1216         # but we want to restore those if compute_node.save() fails.
1217         nodename = compute_node.hypervisor_hostname
1218         old_compute = self.old_resources[nodename]
1219         if self._resource_change(compute_node):
1220             # If the compute_node's resource changed, update to DB.
1221             # NOTE(jianghuaw): Once we completely move to use get_inventory()
1222             # for all resource provider's inv data. We can remove this check.
1223             # At the moment we still need this check and save compute_node.
1224             try:
1225                 compute_node.save()
1226             except Exception:
1227                 # Restore the previous state in self.old_resources so that on
1228                 # the next trip through here _resource_change does not have
1229                 # stale data to compare.
1230                 with excutils.save_and_reraise_exception(logger=LOG):
1231                     self.old_resources[nodename] = old_compute
1232 
1233         self._update_to_placement(context, compute_node, startup)
1234 
1235         if self.pci_tracker:
1236             self.pci_tracker.save(context)
1237 
1238     def _update_usage(self, usage, nodename, sign=1):
1239         # TODO(stephenfin): We don't use the CPU, RAM and disk fields for much
1240         # except 'Aggregate(Core|Ram|Disk)Filter', the 'os-hypervisors' API,
1241         # and perhaps some out-of-tree filters. Once the in-tree stuff is
1242         # removed or updated to use information from placement, we can think
1243         # about dropping the fields from the 'ComputeNode' object entirely
1244         mem_usage = usage['memory_mb']
1245         disk_usage = usage.get('root_gb', 0)
1246         vcpus_usage = usage.get('vcpus', 0)
1247 
1248         cn = self.compute_nodes[nodename]
1249         cn.memory_mb_used += sign * mem_usage
1250         cn.local_gb_used += sign * disk_usage
1251         cn.local_gb_used += sign * usage.get('ephemeral_gb', 0)
1252         cn.local_gb_used += sign * usage.get('swap', 0) / 1024
1253         cn.vcpus_used += sign * vcpus_usage
1254 
1255         # free ram and disk may be negative, depending on policy:
1256         cn.free_ram_mb = cn.memory_mb - cn.memory_mb_used
1257         cn.free_disk_gb = cn.local_gb - cn.local_gb_used
1258 
1259         stats = self.stats[nodename]
1260         cn.running_vms = stats.num_instances
1261 
1262         # calculate the NUMA usage, assuming the instance is actually using
1263         # NUMA, of course
1264         if cn.numa_topology and usage.get('numa_topology'):
1265             instance_numa_topology = usage.get('numa_topology')
1266             # the ComputeNode.numa_topology field is a StringField, so
1267             # deserialize
1268             host_numa_topology = objects.NUMATopology.obj_from_db_obj(
1269                 cn.numa_topology)
1270 
1271             free = sign == -1
1272 
1273             # ...and reserialize once we save it back
1274             cn.numa_topology = hardware.numa_usage_from_instance_numa(
1275                 host_numa_topology, instance_numa_topology, free)._to_json()
1276 
1277     def _get_migration_context_resource(self, resource, instance,
1278                                         prefix='new_'):
1279         migration_context = instance.migration_context
1280         resource = prefix + resource
1281         if migration_context and resource in migration_context:
1282             return getattr(migration_context, resource)
1283         return None
1284 
1285     def _update_usage_from_migration(self, context, instance, migration,
1286                                      nodename):
1287         """Update usage for a single migration.  The record may
1288         represent an incoming or outbound migration.
1289         """
1290         if not _is_trackable_migration(migration):
1291             return
1292 
1293         uuid = migration.instance_uuid
1294         LOG.info("Updating resource usage from migration %s", migration.uuid,
1295                  instance_uuid=uuid)
1296 
1297         incoming = (migration.dest_compute == self.host and
1298                     migration.dest_node == nodename)
1299         outbound = (migration.source_compute == self.host and
1300                     migration.source_node == nodename)
1301         same_node = (incoming and outbound)
1302 
1303         tracked = uuid in self.tracked_instances
1304         itype = None
1305         numa_topology = None
1306         sign = 0
1307         if same_node:
1308             # Same node resize. Record usage for the 'new_' resources.  This
1309             # is executed on resize_claim().
1310             if (instance['instance_type_id'] ==
1311                     migration.old_instance_type_id):
1312                 itype = self._get_instance_type(instance, 'new_', migration)
1313                 numa_topology = self._get_migration_context_resource(
1314                     'numa_topology', instance)
1315                 # Allocate pci device(s) for the instance.
1316                 sign = 1
1317             else:
1318                 # The instance is already set to the new flavor (this is done
1319                 # by the compute manager on finish_resize()), hold space for a
1320                 # possible revert to the 'old_' resources.
1321                 # NOTE(lbeliveau): When the periodic audit timer gets
1322                 # triggered, the compute usage gets reset.  The usage for an
1323                 # instance that is migrated to the new flavor but not yet
1324                 # confirmed/reverted will first get accounted for by
1325                 # _update_usage_from_instances().  This method will then be
1326                 # called, and we need to account for the '_old' resources
1327                 # (just in case).
1328                 itype = self._get_instance_type(instance, 'old_', migration)
1329                 numa_topology = self._get_migration_context_resource(
1330                     'numa_topology', instance, prefix='old_')
1331 
1332         elif incoming and not tracked:
1333             # instance has not yet migrated here:
1334             itype = self._get_instance_type(instance, 'new_', migration)
1335             numa_topology = self._get_migration_context_resource(
1336                 'numa_topology', instance)
1337             # Allocate pci device(s) for the instance.
1338             sign = 1
1339             LOG.debug('Starting to track incoming migration %s with flavor %s',
1340                       migration.uuid, itype.flavorid, instance=instance)
1341 
1342         elif outbound and not tracked:
1343             # instance migrated, but record usage for a possible revert:
1344             itype = self._get_instance_type(instance, 'old_', migration)
1345             numa_topology = self._get_migration_context_resource(
1346                 'numa_topology', instance, prefix='old_')
1347             # We could be racing with confirm_resize setting the
1348             # instance.old_flavor field to None before the migration status
1349             # is "confirmed" so if we did not find the flavor in the outgoing
1350             # resized instance we won't track it.
1351             if itype:
1352                 LOG.debug('Starting to track outgoing migration %s with '
1353                           'flavor %s', migration.uuid, itype.flavorid,
1354                           instance=instance)
1355 
1356         if itype:
1357             cn = self.compute_nodes[nodename]
1358             usage = self._get_usage_dict(
1359                         itype, instance, numa_topology=numa_topology)
1360             if self.pci_tracker and sign:
1361                 self.pci_tracker.update_pci_for_instance(
1362                     context, instance, sign=sign)
1363             self._update_usage(usage, nodename)
1364             if self.pci_tracker:
1365                 obj = self.pci_tracker.stats.to_device_pools_obj()
1366                 cn.pci_device_pools = obj
1367             else:
1368                 obj = objects.PciDevicePoolList()
1369                 cn.pci_device_pools = obj
1370             self.tracked_migrations[uuid] = migration
1371 
1372     def _update_usage_from_migrations(self, context, migrations, nodename):
1373         filtered = {}
1374         instances = {}
1375         self.tracked_migrations.clear()
1376 
1377         # do some defensive filtering against bad migrations records in the
1378         # database:
1379         for migration in migrations:
1380             uuid = migration.instance_uuid
1381 
1382             try:
1383                 if uuid not in instances:
1384                     instances[uuid] = migration.instance
1385             except exception.InstanceNotFound as e:
1386                 # migration referencing deleted instance
1387                 LOG.debug('Migration instance not found: %s', e)
1388                 continue
1389 
1390             # skip migration if instance isn't in a resize state:
1391             if not _instance_in_resize_state(instances[uuid]):
1392                 LOG.warning("Instance not resizing, skipping migration.",
1393                             instance_uuid=uuid)
1394                 continue
1395 
1396             # filter to most recently updated migration for each instance:
1397             other_migration = filtered.get(uuid, None)
1398             # NOTE(claudiub): In Python 3, you cannot compare NoneTypes.
1399             if other_migration:
1400                 om = other_migration
1401                 other_time = om.updated_at or om.created_at
1402                 migration_time = migration.updated_at or migration.created_at
1403                 if migration_time > other_time:
1404                     filtered[uuid] = migration
1405             else:
1406                 filtered[uuid] = migration
1407 
1408         for migration in filtered.values():
1409             instance = instances[migration.instance_uuid]
1410             # Skip migration (and mark it as error) if it doesn't match the
1411             # instance migration id.
1412             # This can happen if we have a stale migration record.
1413             # We want to proceed if instance.migration_context is None
1414             if (instance.migration_context is not None and
1415                     instance.migration_context.migration_id != migration.id):
1416                 LOG.info("Current instance migration %(im)s doesn't match "
1417                              "migration %(m)s, marking migration as error. "
1418                              "This can occur if a previous migration for this "
1419                              "instance did not complete.",
1420                     {'im': instance.migration_context.migration_id,
1421                      'm': migration.id})
1422                 migration.status = "error"
1423                 migration.save()
1424                 continue
1425 
1426             try:
1427                 self._update_usage_from_migration(context, instance, migration,
1428                                                   nodename)
1429             except exception.FlavorNotFound:
1430                 LOG.warning("Flavor could not be found, skipping migration.",
1431                             instance_uuid=instance.uuid)
1432                 continue
1433 
1434     def _update_usage_from_instance(self, context, instance, nodename,
1435             is_removed=False):
1436         """Update usage for a single instance."""
1437 
1438         uuid = instance['uuid']
1439         is_new_instance = uuid not in self.tracked_instances
1440         # NOTE(sfinucan): Both brand new instances as well as instances that
1441         # are being unshelved will have is_new_instance == True
1442         is_removed_instance = not is_new_instance and (is_removed or
1443             instance['vm_state'] in vm_states.ALLOW_RESOURCE_REMOVAL)
1444 
1445         if is_new_instance:
1446             self.tracked_instances.add(uuid)
1447             sign = 1
1448 
1449         if is_removed_instance:
1450             self.tracked_instances.remove(uuid)
1451             self._release_assigned_resources(instance.resources)
1452             sign = -1
1453 
1454         cn = self.compute_nodes[nodename]
1455         stats = self.stats[nodename]
1456         stats.update_stats_for_instance(instance, is_removed_instance)
1457         cn.stats = stats
1458 
1459         # if it's a new or deleted instance:
1460         if is_new_instance or is_removed_instance:
1461             if self.pci_tracker:
1462                 self.pci_tracker.update_pci_for_instance(context,
1463                                                          instance,
1464                                                          sign=sign)
1465             # new instance, update compute node resource usage:
1466             self._update_usage(self._get_usage_dict(instance, instance),
1467                                nodename, sign=sign)
1468 
1469         # Stop tracking removed instances in the is_bfv cache. This needs to
1470         # happen *after* calling _get_usage_dict() since that relies on the
1471         # is_bfv cache.
1472         if is_removed_instance and uuid in self.is_bfv:
1473             del self.is_bfv[uuid]
1474 
1475         cn.current_workload = stats.calculate_workload()
1476         if self.pci_tracker:
1477             obj = self.pci_tracker.stats.to_device_pools_obj()
1478             cn.pci_device_pools = obj
1479         else:
1480             cn.pci_device_pools = objects.PciDevicePoolList()
1481 
1482     def _update_usage_from_instances(self, context, instances, nodename):
1483         """Calculate resource usage based on instance utilization.  This is
1484         different than the hypervisor's view as it will account for all
1485         instances assigned to the local compute host, even if they are not
1486         currently powered on.
1487         """
1488         self.tracked_instances.clear()
1489 
1490         cn = self.compute_nodes[nodename]
1491         # set some initial values, reserve room for host/hypervisor:
1492         cn.local_gb_used = CONF.reserved_host_disk_mb / 1024
1493         cn.memory_mb_used = CONF.reserved_host_memory_mb
1494         cn.vcpus_used = CONF.reserved_host_cpus
1495         cn.free_ram_mb = (cn.memory_mb - cn.memory_mb_used)
1496         cn.free_disk_gb = (cn.local_gb - cn.local_gb_used)
1497         cn.current_workload = 0
1498         cn.running_vms = 0
1499 
1500         instance_by_uuid = {}
1501         for instance in instances:
1502             if instance.vm_state not in vm_states.ALLOW_RESOURCE_REMOVAL:
1503                 self._update_usage_from_instance(context, instance, nodename)
1504             instance_by_uuid[instance.uuid] = instance
1505         return instance_by_uuid
1506 
1507     def _remove_deleted_instances_allocations(self, context, cn,
1508                                               migrations, instance_by_uuid):
1509         migration_uuids = [migration.uuid for migration in migrations
1510                            if 'uuid' in migration]
1511         # NOTE(jaypipes): All of this code sucks. It's basically dealing with
1512         # all the corner cases in move, local delete, unshelve and rebuild
1513         # operations for when allocations should be deleted when things didn't
1514         # happen according to the normal flow of events where the scheduler
1515         # always creates allocations for an instance
1516         try:
1517             # pai: report.ProviderAllocInfo namedtuple
1518             pai = self.reportclient.get_allocations_for_resource_provider(
1519                 context, cn.uuid)
1520         except (exception.ResourceProviderAllocationRetrievalFailed,
1521                 ks_exc.ClientException) as e:
1522             LOG.error("Skipping removal of allocations for deleted instances: "
1523                       "%s", e)
1524             return
1525         allocations = pai.allocations
1526         if not allocations:
1527             # The main loop below would short-circuit anyway, but this saves us
1528             # the (potentially expensive) context.elevated construction below.
1529             return
1530         read_deleted_context = context.elevated(read_deleted='yes')
1531         for consumer_uuid, alloc in allocations.items():
1532             if consumer_uuid in self.tracked_instances:
1533                 LOG.debug("Instance %s actively managed on this compute host "
1534                           "and has allocations in placement: %s.",
1535                           consumer_uuid, alloc)
1536                 continue
1537             if consumer_uuid in migration_uuids:
1538                 LOG.debug("Migration %s is active on this compute host "
1539                           "and has allocations in placement: %s.",
1540                           consumer_uuid, alloc)
1541                 continue
1542 
1543             # We know these are instances now, so proceed
1544             instance_uuid = consumer_uuid
1545             instance = instance_by_uuid.get(instance_uuid)
1546             if not instance:
1547                 try:
1548                     instance = objects.Instance.get_by_uuid(
1549                         read_deleted_context, consumer_uuid,
1550                         expected_attrs=[])
1551                 except exception.InstanceNotFound:
1552                     # The instance isn't even in the database. Either the
1553                     # scheduler _just_ created an allocation for it and we're
1554                     # racing with the creation in the cell database, or the
1555                     #  instance was deleted and fully archived before we got a
1556                     # chance to run this. The former is far more likely than
1557                     # the latter. Avoid deleting allocations for a building
1558                     # instance here.
1559                     LOG.info("Instance %(uuid)s has allocations against this "
1560                              "compute host but is not found in the database.",
1561                              {'uuid': instance_uuid},
1562                              exc_info=False)
1563                     continue
1564 
1565             if instance.deleted:
1566                 # The instance is gone, so we definitely want to remove
1567                 # allocations associated with it.
1568                 # NOTE(jaypipes): This will not be true if/when we support
1569                 # cross-cell migrations...
1570                 LOG.debug("Instance %s has been deleted (perhaps locally). "
1571                           "Deleting allocations that remained for this "
1572                           "instance against this compute host: %s.",
1573                           instance_uuid, alloc)
1574                 self.reportclient.delete_allocation_for_instance(context,
1575                                                                  instance_uuid)
1576                 continue
1577             if not instance.host:
1578                 # Allocations related to instances being scheduled should not
1579                 # be deleted if we already wrote the allocation previously.
1580                 LOG.debug("Instance %s has been scheduled to this compute "
1581                           "host, the scheduler has made an allocation "
1582                           "against this compute node but the instance has "
1583                           "yet to start. Skipping heal of allocation: %s.",
1584                           instance_uuid, alloc)
1585                 continue
1586             if (instance.host == cn.host and
1587                     instance.node == cn.hypervisor_hostname):
1588                 # The instance is supposed to be on this compute host but is
1589                 # not in the list of actively managed instances. This could be
1590                 # because we are racing with an instance_claim call during
1591                 # initial build or unshelve where the instance host/node is set
1592                 # before the instance is added to tracked_instances. If the
1593                 # task_state is set, then consider things in motion and log at
1594                 # debug level instead of warning.
1595                 if instance.task_state:
1596                     LOG.debug('Instance with task_state "%s" is not being '
1597                               'actively managed by this compute host but has '
1598                               'allocations referencing this compute node '
1599                               '(%s): %s. Skipping heal of allocations during '
1600                               'the task state transition.',
1601                               instance.task_state, cn.uuid, alloc,
1602                               instance=instance)
1603                 else:
1604                     LOG.warning("Instance %s is not being actively managed by "
1605                                 "this compute host but has allocations "
1606                                 "referencing this compute host: %s. Skipping "
1607                                 "heal of allocation because we do not know "
1608                                 "what to do.", instance_uuid, alloc)
1609                 continue
1610             if instance.host != cn.host:
1611                 # The instance has been moved to another host either via a
1612                 # migration, evacuation or unshelve in between the time when we
1613                 # ran InstanceList.get_by_host_and_node(), added those
1614                 # instances to RT.tracked_instances and the above
1615                 # Instance.get_by_uuid() call. We SHOULD attempt to remove any
1616                 # allocations that reference this compute host if the VM is in
1617                 # a stable terminal state (i.e. it isn't in a state of waiting
1618                 # for resize to confirm/revert), however if the destination
1619                 # host is an Ocata compute host, it will delete the allocation
1620                 # that contains this source compute host information anyway and
1621                 # recreate an allocation that only refers to itself. So we
1622                 # don't need to do anything in that case. Just log the
1623                 # situation here for information but don't attempt to delete or
1624                 # change the allocation.
1625                 LOG.warning("Instance %s has been moved to another host "
1626                             "%s(%s). There are allocations remaining against "
1627                             "the source host that might need to be removed: "
1628                             "%s.",
1629                             instance_uuid, instance.host, instance.node, alloc)
1630 
1631     def delete_allocation_for_evacuated_instance(self, context, instance, node,
1632                                                  node_type='source'):
1633         # Clean up the instance allocation from this node in placement
1634         cn_uuid = self.compute_nodes[node].uuid
1635         if not self.reportclient.remove_provider_tree_from_instance_allocation(
1636                 context, instance.uuid, cn_uuid):
1637             LOG.error("Failed to clean allocation of evacuated "
1638                       "instance on the %s node %s",
1639                       node_type, cn_uuid, instance=instance)
1640 
1641     def _find_orphaned_instances(self):
1642         """Given the set of instances and migrations already account for
1643         by resource tracker, sanity check the hypervisor to determine
1644         if there are any "orphaned" instances left hanging around.
1645 
1646         Orphans could be consuming memory and should be accounted for in
1647         usage calculations to guard against potential out of memory
1648         errors.
1649         """
1650         uuids1 = frozenset(self.tracked_instances)
1651         uuids2 = frozenset(self.tracked_migrations.keys())
1652         uuids = uuids1 | uuids2
1653 
1654         usage = self.driver.get_per_instance_usage()
1655         vuuids = frozenset(usage.keys())
1656 
1657         orphan_uuids = vuuids - uuids
1658         orphans = [usage[uuid] for uuid in orphan_uuids]
1659 
1660         return orphans
1661 
1662     def _update_usage_from_orphans(self, orphans, nodename):
1663         """Include orphaned instances in usage."""
1664         for orphan in orphans:
1665             memory_mb = orphan['memory_mb']
1666 
1667             LOG.warning("Detected running orphan instance: %(uuid)s "
1668                         "(consuming %(memory_mb)s MB memory)",
1669                         {'uuid': orphan['uuid'], 'memory_mb': memory_mb})
1670 
1671             # just record memory usage for the orphan
1672             usage = {'memory_mb': memory_mb}
1673             self._update_usage(usage, nodename)
1674 
1675     def delete_allocation_for_shelve_offloaded_instance(self, context,
1676                                                         instance):
1677         self.reportclient.delete_allocation_for_instance(context,
1678                                                          instance.uuid)
1679 
1680     def _verify_resources(self, resources):
1681         resource_keys = ["vcpus", "memory_mb", "local_gb", "cpu_info",
1682                          "vcpus_used", "memory_mb_used", "local_gb_used",
1683                          "numa_topology"]
1684 
1685         missing_keys = [k for k in resource_keys if k not in resources]
1686         if missing_keys:
1687             reason = _("Missing keys: %s") % missing_keys
1688             raise exception.InvalidInput(reason=reason)
1689 
1690     def _get_instance_type(self, instance, prefix, migration):
1691         """Get the instance type from instance."""
1692         stashed_flavors = migration.migration_type in ('resize',)
1693         if stashed_flavors:
1694             return getattr(instance, '%sflavor' % prefix)
1695         else:
1696             # NOTE(ndipanov): Certain migration types (all but resize)
1697             # do not change flavors so there is no need to stash
1698             # them. In that case - just get the instance flavor.
1699             return instance.flavor
1700 
1701     def _get_usage_dict(self, object_or_dict, instance, **updates):
1702         """Make a usage dict _update methods expect.
1703 
1704         Accepts a dict or an Instance or Flavor object, and a set of updates.
1705         Converts the object to a dict and applies the updates.
1706 
1707         :param object_or_dict: instance or flavor as an object or just a dict
1708         :param instance: nova.objects.Instance for the related operation; this
1709                          is needed to determine if the instance is
1710                          volume-backed
1711         :param updates: key-value pairs to update the passed object.
1712                         Currently only considers 'numa_topology', all other
1713                         keys are ignored.
1714 
1715         :returns: a dict with all the information from object_or_dict updated
1716                   with updates
1717         """
1718 
1719         def _is_bfv():
1720             # Check to see if we have the is_bfv value cached.
1721             if instance.uuid in self.is_bfv:
1722                 is_bfv = self.is_bfv[instance.uuid]
1723             else:
1724                 is_bfv = compute_utils.is_volume_backed_instance(
1725                     instance._context, instance)
1726                 self.is_bfv[instance.uuid] = is_bfv
1727             return is_bfv
1728 
1729         usage = {}
1730         if isinstance(object_or_dict, objects.Instance):
1731             is_bfv = _is_bfv()
1732             usage = {'memory_mb': object_or_dict.flavor.memory_mb,
1733                      'swap': object_or_dict.flavor.swap,
1734                      'vcpus': object_or_dict.flavor.vcpus,
1735                      'root_gb': (0 if is_bfv else
1736                                  object_or_dict.flavor.root_gb),
1737                      'ephemeral_gb': object_or_dict.flavor.ephemeral_gb,
1738                      'numa_topology': object_or_dict.numa_topology}
1739         elif isinstance(object_or_dict, objects.Flavor):
1740             usage = obj_base.obj_to_primitive(object_or_dict)
1741             if _is_bfv():
1742                 usage['root_gb'] = 0
1743         else:
1744             usage.update(object_or_dict)
1745 
1746         for key in ('numa_topology',):
1747             if key in updates:
1748                 usage[key] = updates[key]
1749         return usage
1750 
1751     def build_failed(self, nodename):
1752         """Increments the failed_builds stats for the given node."""
1753         self.stats[nodename].build_failed()
1754 
1755     def build_succeeded(self, nodename):
1756         """Resets the failed_builds stats for the given node."""
1757         self.stats[nodename].build_succeeded()
1758 
1759     @utils.synchronized(COMPUTE_RESOURCE_SEMAPHORE)
1760     def claim_pci_devices(self, context, pci_requests):
1761         """Claim instance PCI resources
1762 
1763         :param context: security context
1764         :param pci_requests: a list of nova.objects.InstancePCIRequests
1765         :returns: a list of nova.objects.PciDevice objects
1766         """
1767         result = self.pci_tracker.claim_instance(
1768             context, pci_requests, None)
1769         self.pci_tracker.save(context)
1770         return result
1771 
1772     @utils.synchronized(COMPUTE_RESOURCE_SEMAPHORE)
1773     def allocate_pci_devices_for_instance(self, context, instance):
1774         """Allocate instance claimed PCI resources
1775 
1776         :param context: security context
1777         :param instance: instance object
1778         """
1779         self.pci_tracker.allocate_instance(instance)
1780         self.pci_tracker.save(context)
1781 
1782     @utils.synchronized(COMPUTE_RESOURCE_SEMAPHORE)
1783     def free_pci_device_allocations_for_instance(self, context, instance):
1784         """Free instance allocated PCI resources
1785 
1786         :param context: security context
1787         :param instance: instance object
1788         """
1789         self.pci_tracker.free_instance_allocations(context, instance)
1790         self.pci_tracker.save(context)
1791 
1792     @utils.synchronized(COMPUTE_RESOURCE_SEMAPHORE)
1793     def free_pci_device_claims_for_instance(self, context, instance):
1794         """Free instance claimed PCI resources
1795 
1796         :param context: security context
1797         :param instance: instance object
1798         """
1799         self.pci_tracker.free_instance_claims(context, instance)
1800         self.pci_tracker.save(context)
