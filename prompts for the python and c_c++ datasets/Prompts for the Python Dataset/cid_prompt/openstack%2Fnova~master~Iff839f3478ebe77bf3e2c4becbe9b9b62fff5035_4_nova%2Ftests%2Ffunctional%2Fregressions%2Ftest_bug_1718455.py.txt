Please review the code below for security defects. You can consider defect types in terms of:
1.CWE-284 (Improper Access Control)
2.CWE-435 (Improper Interaction Between Multiple Entities)
3.CWE-664 (Improper Control of a Resource Through its Lifetime)
4.CWE-682 (Incorrect Calculation)
5.CWE-691 (Insufficient Control Flow Management)
6.CWE-693 (Protection Mechanism Failure)
7.CWE-697 (Incorrect Comparison)
8.CWE-703 (Improper Check or Handling of Exceptional Conditions)
9.CWE-707 (Improper Neutralization)
10.CWE-710 (Improper Adherence to Coding Standards)
If any are found, please describe the security defect in detail and indicate the corresponding line number of code and solution. If none are detected, states: 'No security defects are detected in the code'.

1 # Licensed under the Apache License, Version 2.0 (the "License");
2 # you may not use this file except in compliance with the License.
3 # You may obtain a copy of the License at
4 #
5 #    http://www.apache.org/licenses/LICENSE-2.0
6 #
7 # Unless required by applicable law or agreed to in writing, software
8 # distributed under the License is distributed on an "AS IS" BASIS,
9 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
10 # See the License for the specific language governing permissions and
11 # limitations under the License.
12 
13 import time
14 
15 from nova import test
16 from nova.tests import fixtures as nova_fixtures
17 from nova.tests.functional import integrated_helpers
18 from nova.tests.unit import fake_network
19 import nova.tests.unit.image.fake
20 from nova.tests.unit import policy_fixture
21 from nova.virt import fake
22 
23 
24 class TestLiveMigrateOneOfConcurrentlyCreatedInstances(
25         test.TestCase, integrated_helpers.InstanceHelperMixin):
26     """Regression tests for bug #1718455
27 
28     When creating multiple instances at the same time, the RequestSpec record
29     is persisting the number of concurrent instances.
30     When moving one of those instances, the scheduler should not include
31     num_instances > 1 in the request spec.
32     It was partially fixed by bug #1708961 but we forgot to amend
33     some place in the scheduler so that the right number of hosts was returned
34     to the scheduler method calling both the Placement API and filters/weighers
35     but we were verifying that returned size of hosts against a wrong number,
36     which is the number of instances created concurrently.
37 
38     That test will create 2 concurrent instances and verify that when
39     live-migrating one of them, we end up with a correct move operation.
40     """
41 
42     microversion = 'latest'
43 
44     def setUp(self):
45         super(TestLiveMigrateOneOfConcurrentlyCreatedInstances, self).setUp()
46 
47         self.useFixture(policy_fixture.RealPolicyFixture())
48         self.useFixture(nova_fixtures.NeutronFixture(self))
49         self.useFixture(nova_fixtures.PlacementFixture())
50 
51         api_fixture = self.useFixture(nova_fixtures.OSAPIFixture(
52             api_version='v2.1'))
53 
54         self.api = api_fixture.admin_api
55         self.api.microversion = self.microversion
56 
57         nova.tests.unit.image.fake.stub_out_image_service(self)
58         self.addCleanup(nova.tests.unit.image.fake.FakeImageService_reset)
59 
60         self.start_service('conductor')
61         self.start_service('scheduler')
62 
63         # set_nodes() is needed to have each compute service return a
64         # different nodename, so we get two hosts in the list of candidates
65         # for scheduling. Otherwise both hosts will have the same default
66         # nodename "fake-mini". The host passed to start_service controls the
67         # "host" attribute and set_nodes() sets the "nodename" attribute.
68         # We set_nodes() to make host and nodename the same for each compute.
69         fake.set_nodes(['host1'])
70         self.addCleanup(fake.restore_nodes)
71         self.start_service('compute', host='host1')
72         fake.set_nodes(['host2'])
73         self.addCleanup(fake.restore_nodes)
74         self.start_service('compute', host='host2')
75 
76         fake_network.set_stub_network_methods(self)
77 
78         flavors = self.api.get_flavors()
79         self.flavor1 = flavors[0]
80 
81     def _boot_servers(self, num_servers=1):
82         server_req = self._build_minimal_create_server_request(
83             self.api, 'some-server', flavor_id=self.flavor1['id'],
84             image_uuid='155d900f-4e14-4e4c-a73d-069cbf4541e6',
85             networks='none')
86         server_req.update({'min_count': str(num_servers),
87                            'return_reservation_id': 'True'})
88         response = self.api.post_server({'server': server_req})
89         reservation_id = response['reservation_id']
90         # lookup servers created by the multi-create request.
91         servers = self.api.get_servers(detail=True,
92                 search_opts={'reservation_id': reservation_id})
93         for idx, server in enumerate(servers):
94             servers[idx] = self._wait_for_state_change(self.api, server,
95                                                        'ACTIVE')
96         return servers
97 
98     def _wait_for_migration_status(self, server, expected_status):
99         """Waits for a migration record with the given status to be found
100         for the given server, else the test fails. The migration record, if
101         found, is returned.
102         """
103         for attempt in range(10):
104             migrations = self.api.get_migrations()
105             for migration in migrations:
106                 if (migration['instance_uuid'] == server['id'] and
107                         migration['status'].lower() ==
108                         expected_status.lower()):
109                     return migration
110             time.sleep(0.5)
111         self.fail('Timed out waiting for migration with status "%s" for '
112                   'instance: %s' % (expected_status, server['id']))
113 
114     def test_live_migrate_one_multi_created_instance(self):
115         # Boot two servers in a multi-create request
116         servers = self._boot_servers(num_servers=2)
117 
118         # Take the first instance and verify which host the instance is there
119         server = servers[0]
120         original_host = server['OS-EXT-SRV-ATTR:host']
121         target_host = 'host1' if original_host == 'host2' else 'host2'
122 
123         # Initiate live migration for that instance by targeting the other host
124         post = {'os-migrateLive': {'block_migration': 'auto',
125                                    'host': target_host}}
126 
127         # NOTE(sbauza): Since API version 2.34, live-migration pre-checks are
128         # now done asynchronously so even if we hit a NoValidHost exception by
129         # the conductor, the API call will always succeed with a HTTP202
130         # response code. In order to verify whether the migration succeeded,
131         # we need to lookup the migrations API.
132         self.api.post_server_action(server['id'], post)
133 
134         # Poll the migration until it is done.
135         migration = self._wait_for_migration_status(server, 'running')
136 
137         self.assertEqual('live-migration', migration['migration_type'])
138         self.assertEqual(original_host, migration['source_compute'])
139 
140         # Verify that the migration succeeded as the instance is now on the
141         # destination node.
142         server = self.api.get_server(server['id'])
143         self.assertEqual(target_host, server['OS-EXT-SRV-ATTR:host'])
