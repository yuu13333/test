Please review the code below for security defects. You can consider defect types in terms of:
1.CWE-284 (Improper Access Control)
2.CWE-435 (Improper Interaction Between Multiple Entities)
3.CWE-664 (Improper Control of a Resource Through its Lifetime)
4.CWE-682 (Incorrect Calculation)
5.CWE-691 (Insufficient Control Flow Management)
6.CWE-693 (Protection Mechanism Failure)
7.CWE-697 (Incorrect Comparison)
8.CWE-703 (Improper Check or Handling of Exceptional Conditions)
9.CWE-707 (Improper Neutralization)
10.CWE-710 (Improper Adherence to Coding Standards)
If any are found, please describe the security defect in detail and indicate the corresponding line number of code and solution. If none are detected, states: 'No security defects are detected in the code'.

1 # Copyright 2011 Justin Santa Barbara
2 # All Rights Reserved.
3 #
4 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
5 #    not use this file except in compliance with the License. You may obtain
6 #    a copy of the License at
7 #
8 #         http://www.apache.org/licenses/LICENSE-2.0
9 #
10 #    Unless required by applicable law or agreed to in writing, software
11 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
12 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
13 #    License for the specific language governing permissions and limitations
14 #    under the License.
15 from __future__ import absolute_import
16 
17 import collections
18 import copy
19 import datetime
20 import time
21 import zlib
22 
23 from keystoneauth1 import adapter
24 import mock
25 import os_resource_classes as orc
26 from oslo_config import cfg
27 from oslo_log import log as logging
28 from oslo_serialization import base64
29 from oslo_serialization import jsonutils
30 from oslo_utils.fixture import uuidsentinel as uuids
31 from oslo_utils import timeutils
32 import six
33 
34 from nova.compute import api as compute_api
35 from nova.compute import instance_actions
36 from nova.compute import manager as compute_manager
37 from nova import context
38 from nova import exception
39 from nova.network.neutronv2 import constants
40 from nova import objects
41 from nova.objects import block_device as block_device_obj
42 from nova.scheduler import utils
43 from nova.scheduler import weights
44 from nova import test
45 from nova.tests import fixtures as nova_fixtures
46 from nova.tests.functional.api import client
47 from nova.tests.functional import integrated_helpers
48 from nova.tests.unit.api.openstack import fakes
49 from nova.tests.unit import fake_block_device
50 from nova.tests.unit import fake_notifier
51 from nova.tests.unit import fake_requests
52 import nova.tests.unit.image.fake
53 from nova.tests.unit.objects import test_instance_info_cache
54 from nova.virt import fake
55 from nova import volume
56 
57 CONF = cfg.CONF
58 
59 LOG = logging.getLogger(__name__)
60 
61 
62 class AltHostWeigher(weights.BaseHostWeigher):
63     """Used in the alternate host tests to return a pre-determined list of
64     hosts.
65     """
66     def _weigh_object(self, host_state, weight_properties):
67         """Return a defined order of hosts."""
68         weights = {"selection": 999, "alt_host1": 888, "alt_host2": 777,
69                    "alt_host3": 666, "host1": 0, "host2": 0}
70         return weights.get(host_state.host, 0)
71 
72 
73 class ServersTestBase(integrated_helpers._IntegratedTestBase):
74     api_major_version = 'v2'
75     _force_delete_parameter = 'forceDelete'
76     _image_ref_parameter = 'imageRef'
77     _flavor_ref_parameter = 'flavorRef'
78     _access_ipv4_parameter = 'accessIPv4'
79     _access_ipv6_parameter = 'accessIPv6'
80     _return_resv_id_parameter = 'return_reservation_id'
81     _min_count_parameter = 'min_count'
82 
83     USE_NEUTRON = True
84 
85     def setUp(self):
86         self.computes = {}
87         super(ServersTestBase, self).setUp()
88 
89     def _wait_for_state_change(self, server, from_status):
90         for i in range(0, 50):
91             server = self.api.get_server(server['id'])
92             if server['status'] != from_status:
93                 break
94             time.sleep(.1)
95 
96         return server
97 
98     def _wait_for_deletion(self, server_id):
99         # Wait (briefly) for deletion
100         for _retries in range(50):
101             try:
102                 found_server = self.api.get_server(server_id)
103             except client.OpenStackApiNotFoundException:
104                 found_server = None
105                 LOG.debug("Got 404, proceeding")
106                 break
107 
108             LOG.debug("Found_server=%s", found_server)
109 
110             # TODO(justinsb): Mock doesn't yet do accurate state changes
111             # if found_server['status'] != 'deleting':
112             #    break
113             time.sleep(.1)
114 
115         # Should be gone
116         self.assertFalse(found_server)
117 
118     def _delete_server(self, server_id):
119         # Delete the server
120         self.api.delete_server(server_id)
121         self._wait_for_deletion(server_id)
122 
123     def _get_access_ips_params(self):
124         return {self._access_ipv4_parameter: "172.19.0.2",
125                 self._access_ipv6_parameter: "fe80::2"}
126 
127     def _verify_access_ips(self, server):
128         self.assertEqual('172.19.0.2',
129                          server[self._access_ipv4_parameter])
130         self.assertEqual('fe80::2', server[self._access_ipv6_parameter])
131 
132 
133 class ServersTest(ServersTestBase):
134 
135     def test_get_servers(self):
136         # Simple check that listing servers works.
137         servers = self.api.get_servers()
138         for server in servers:
139             LOG.debug("server: %s", server)
140 
141     def _get_node_build_failures(self):
142         ctxt = context.get_admin_context()
143         computes = objects.ComputeNodeList.get_all(ctxt)
144         return {
145             node.hypervisor_hostname: int(node.stats.get('failed_builds', 0))
146             for node in computes}
147 
148     def _run_periodics(self):
149         """Run the update_available_resource task on every compute manager
150 
151         This runs periodics on the computes in an undefined order; some child
152         class redefined this function to force a specific order.
153         """
154 
155         if self.compute.host not in self.computes:
156             self.computes[self.compute.host] = self.compute
157 
158         ctx = context.get_admin_context()
159         for compute in self.computes.values():
160             LOG.info('Running periodic for compute (%s)',
161                 compute.manager.host)
162             compute.manager.update_available_resource(ctx)
163         LOG.info('Finished with periodics')
164 
165     def test_create_server_with_error(self):
166         # Create a server which will enter error state.
167 
168         def throw_error(*args, **kwargs):
169             raise exception.BuildAbortException(reason='',
170                     instance_uuid='fake')
171 
172         self.stub_out('nova.virt.fake.FakeDriver.spawn', throw_error)
173 
174         server = self._build_minimal_create_server_request()
175         created_server = self.api.post_server({"server": server})
176         created_server_id = created_server['id']
177 
178         found_server = self.api.get_server(created_server_id)
179         self.assertEqual(created_server_id, found_server['id'])
180 
181         found_server = self._wait_for_state_change(found_server, 'BUILD')
182 
183         self.assertEqual('ERROR', found_server['status'])
184         self._delete_server(created_server_id)
185 
186         # We should have no (persisted) build failures until we update
187         # resources, after which we should have one
188         self.assertEqual([0], list(self._get_node_build_failures().values()))
189         self._run_periodics()
190         self.assertEqual([1], list(self._get_node_build_failures().values()))
191 
192     def test_create_server_with_image_type_filter(self):
193         self.flags(query_placement_for_image_type_support=True,
194                    group='scheduler')
195 
196         raw_image = '155d900f-4e14-4e4c-a73d-069cbf4541e6'
197         vhd_image = 'a440c04b-79fa-479c-bed1-0b816eaec379'
198 
199         server = self._build_minimal_create_server_request(
200             image_uuid=vhd_image)
201         server = self.api.post_server({'server': server})
202         server = self.api.get_server(server['id'])
203         errored_server = self._wait_for_state_change(server, server['status'])
204         self.assertEqual('ERROR', errored_server['status'])
205         self.assertIn('No valid host', errored_server['fault']['message'])
206 
207         server = self._build_minimal_create_server_request(
208             image_uuid=raw_image)
209         server = self.api.post_server({'server': server})
210         server = self.api.get_server(server['id'])
211         created_server = self._wait_for_state_change(server, server['status'])
212         self.assertEqual('ACTIVE', created_server['status'])
213 
214     def _test_create_server_with_error_with_retries(self):
215         # Create a server which will enter error state.
216 
217         self.compute2 = self.start_service('compute', host='host2')
218         self.computes['compute2'] = self.compute2
219 
220         fails = []
221 
222         def throw_error(*args, **kwargs):
223             fails.append('one')
224             raise test.TestingException('Please retry me')
225 
226         self.stub_out('nova.virt.fake.FakeDriver.spawn', throw_error)
227 
228         server = self._build_minimal_create_server_request()
229         created_server = self.api.post_server({"server": server})
230         created_server_id = created_server['id']
231 
232         found_server = self.api.get_server(created_server_id)
233         self.assertEqual(created_server_id, found_server['id'])
234 
235         found_server = self._wait_for_state_change(found_server, 'BUILD')
236 
237         self.assertEqual('ERROR', found_server['status'])
238         self._delete_server(created_server_id)
239 
240         return len(fails)
241 
242     def test_create_server_with_error_with_retries(self):
243         self.flags(max_attempts=2, group='scheduler')
244         fails = self._test_create_server_with_error_with_retries()
245         self.assertEqual(2, fails)
246         self._run_periodics()
247         self.assertEqual(
248             [1, 1], list(self._get_node_build_failures().values()))
249 
250     def test_create_server_with_error_with_no_retries(self):
251         self.flags(max_attempts=1, group='scheduler')
252         fails = self._test_create_server_with_error_with_retries()
253         self.assertEqual(1, fails)
254         self._run_periodics()
255         self.assertEqual(
256             [0, 1], list(sorted(self._get_node_build_failures().values())))
257 
258     def test_create_and_delete_server(self):
259         # Creates and deletes a server.
260 
261         # Create server
262         # Build the server data gradually, checking errors along the way
263         server = {}
264         good_server = self._build_minimal_create_server_request()
265 
266         post = {'server': server}
267 
268         # Without an imageRef, this throws 500.
269         # TODO(justinsb): Check whatever the spec says should be thrown here
270         self.assertRaises(client.OpenStackApiException,
271                           self.api.post_server, post)
272 
273         # With an invalid imageRef, this throws 500.
274         server[self._image_ref_parameter] = self.get_invalid_image()
275         # TODO(justinsb): Check whatever the spec says should be thrown here
276         self.assertRaises(client.OpenStackApiException,
277                           self.api.post_server, post)
278 
279         # Add a valid imageRef
280         server[self._image_ref_parameter] = good_server.get(
281             self._image_ref_parameter)
282 
283         # Without flavorRef, this throws 500
284         # TODO(justinsb): Check whatever the spec says should be thrown here
285         self.assertRaises(client.OpenStackApiException,
286                           self.api.post_server, post)
287 
288         server[self._flavor_ref_parameter] = good_server.get(
289             self._flavor_ref_parameter)
290 
291         # Without a name, this throws 500
292         # TODO(justinsb): Check whatever the spec says should be thrown here
293         self.assertRaises(client.OpenStackApiException,
294                           self.api.post_server, post)
295 
296         # Set a valid server name
297         server['name'] = good_server['name']
298 
299         created_server = self.api.post_server(post)
300         LOG.debug("created_server: %s", created_server)
301         self.assertTrue(created_server['id'])
302         created_server_id = created_server['id']
303 
304         # Check it's there
305         found_server = self.api.get_server(created_server_id)
306         self.assertEqual(created_server_id, found_server['id'])
307 
308         # It should also be in the all-servers list
309         servers = self.api.get_servers()
310         server_ids = [s['id'] for s in servers]
311         self.assertIn(created_server_id, server_ids)
312 
313         found_server = self._wait_for_state_change(found_server, 'BUILD')
314         # It should be available...
315         # TODO(justinsb): Mock doesn't yet do this...
316         self.assertEqual('ACTIVE', found_server['status'])
317         servers = self.api.get_servers(detail=True)
318         for server in servers:
319             self.assertIn("image", server)
320             self.assertIn("flavor", server)
321 
322         self._delete_server(created_server_id)
323 
324     def _force_reclaim(self):
325         # Make sure that compute manager thinks the instance is
326         # old enough to be expired
327         the_past = timeutils.utcnow() + datetime.timedelta(hours=1)
328         timeutils.set_time_override(override_time=the_past)
329         self.addCleanup(timeutils.clear_time_override)
330         ctxt = context.get_admin_context()
331         self.compute._reclaim_queued_deletes(ctxt)
332 
333     def test_deferred_delete(self):
334         # Creates, deletes and waits for server to be reclaimed.
335         self.flags(reclaim_instance_interval=1)
336 
337         # Create server
338         server = self._build_minimal_create_server_request()
339 
340         created_server = self.api.post_server({'server': server})
341         LOG.debug("created_server: %s", created_server)
342         self.assertTrue(created_server['id'])
343         created_server_id = created_server['id']
344 
345         # Wait for it to finish being created
346         found_server = self._wait_for_state_change(created_server, 'BUILD')
347 
348         # It should be available...
349         self.assertEqual('ACTIVE', found_server['status'])
350 
351         # Cannot restore unless instance is deleted
352         self.assertRaises(client.OpenStackApiException,
353                           self.api.post_server_action, created_server_id,
354                           {'restore': {}})
355 
356         # Delete the server
357         self.api.delete_server(created_server_id)
358 
359         # Wait for queued deletion
360         found_server = self._wait_for_state_change(found_server, 'ACTIVE')
361         self.assertEqual('SOFT_DELETED', found_server['status'])
362 
363         self._force_reclaim()
364 
365         # Wait for real deletion
366         self._wait_for_deletion(created_server_id)
367 
368     def test_deferred_delete_restore(self):
369         # Creates, deletes and restores a server.
370         self.flags(reclaim_instance_interval=3600)
371 
372         # Create server
373         server = self._build_minimal_create_server_request()
374 
375         created_server = self.api.post_server({'server': server})
376         LOG.debug("created_server: %s", created_server)
377         self.assertTrue(created_server['id'])
378         created_server_id = created_server['id']
379 
380         # Wait for it to finish being created
381         found_server = self._wait_for_state_change(created_server, 'BUILD')
382 
383         # It should be available...
384         self.assertEqual('ACTIVE', found_server['status'])
385 
386         # Delete the server
387         self.api.delete_server(created_server_id)
388 
389         # Wait for queued deletion
390         found_server = self._wait_for_state_change(found_server, 'ACTIVE')
391         self.assertEqual('SOFT_DELETED', found_server['status'])
392 
393         # Restore server
394         self.api.post_server_action(created_server_id, {'restore': {}})
395 
396         # Wait for server to become active again
397         found_server = self._wait_for_state_change(found_server, 'DELETED')
398         self.assertEqual('ACTIVE', found_server['status'])
399 
400     def test_deferred_delete_restore_overquota(self):
401         # Test that a restore that would put the user over quota fails
402         self.flags(instances=1, group='quota')
403         # Creates, deletes and restores a server.
404         self.flags(reclaim_instance_interval=3600)
405 
406         # Create server
407         server = self._build_minimal_create_server_request()
408 
409         created_server1 = self.api.post_server({'server': server})
410         LOG.debug("created_server: %s", created_server1)
411         self.assertTrue(created_server1['id'])
412         created_server_id1 = created_server1['id']
413 
414         # Wait for it to finish being created
415         found_server1 = self._wait_for_state_change(created_server1, 'BUILD')
416 
417         # It should be available...
418         self.assertEqual('ACTIVE', found_server1['status'])
419 
420         # Delete the server
421         self.api.delete_server(created_server_id1)
422 
423         # Wait for queued deletion
424         found_server1 = self._wait_for_state_change(found_server1, 'ACTIVE')
425         self.assertEqual('SOFT_DELETED', found_server1['status'])
426 
427         # Create a second server
428         server = self._build_minimal_create_server_request()
429 
430         created_server2 = self.api.post_server({'server': server})
431         LOG.debug("created_server: %s", created_server2)
432         self.assertTrue(created_server2['id'])
433 
434         # Wait for it to finish being created
435         found_server2 = self._wait_for_state_change(created_server2, 'BUILD')
436 
437         # It should be available...
438         self.assertEqual('ACTIVE', found_server2['status'])
439 
440         # Try to restore the first server, it should fail
441         ex = self.assertRaises(client.OpenStackApiException,
442                                self.api.post_server_action,
443                                created_server_id1, {'restore': {}})
444         self.assertEqual(403, ex.response.status_code)
445         self.assertEqual('SOFT_DELETED', found_server1['status'])
446 
447     def test_deferred_delete_force(self):
448         # Creates, deletes and force deletes a server.
449         self.flags(reclaim_instance_interval=3600)
450 
451         # Create server
452         server = self._build_minimal_create_server_request()
453 
454         created_server = self.api.post_server({'server': server})
455         LOG.debug("created_server: %s", created_server)
456         self.assertTrue(created_server['id'])
457         created_server_id = created_server['id']
458 
459         # Wait for it to finish being created
460         found_server = self._wait_for_state_change(created_server, 'BUILD')
461 
462         # It should be available...
463         self.assertEqual('ACTIVE', found_server['status'])
464 
465         # Delete the server
466         self.api.delete_server(created_server_id)
467 
468         # Wait for queued deletion
469         found_server = self._wait_for_state_change(found_server, 'ACTIVE')
470         self.assertEqual('SOFT_DELETED', found_server['status'])
471 
472         # Force delete server
473         self.api.post_server_action(created_server_id,
474                                     {self._force_delete_parameter: {}})
475 
476         # Wait for real deletion
477         self._wait_for_deletion(created_server_id)
478 
479     def test_create_server_with_metadata(self):
480         # Creates a server with metadata.
481 
482         # Build the server data gradually, checking errors along the way
483         server = self._build_minimal_create_server_request()
484 
485         metadata = {}
486         for i in range(30):
487             metadata['key_%s' % i] = 'value_%s' % i
488 
489         server['metadata'] = metadata
490 
491         post = {'server': server}
492         created_server = self.api.post_server(post)
493         LOG.debug("created_server: %s", created_server)
494         self.assertTrue(created_server['id'])
495         created_server_id = created_server['id']
496 
497         found_server = self.api.get_server(created_server_id)
498         self.assertEqual(created_server_id, found_server['id'])
499         self.assertEqual(metadata, found_server.get('metadata'))
500 
501         # The server should also be in the all-servers details list
502         servers = self.api.get_servers(detail=True)
503         server_map = {server['id']: server for server in servers}
504         found_server = server_map.get(created_server_id)
505         self.assertTrue(found_server)
506         # Details do include metadata
507         self.assertEqual(metadata, found_server.get('metadata'))
508 
509         # The server should also be in the all-servers summary list
510         servers = self.api.get_servers(detail=False)
511         server_map = {server['id']: server for server in servers}
512         found_server = server_map.get(created_server_id)
513         self.assertTrue(found_server)
514         # Summary should not include metadata
515         self.assertFalse(found_server.get('metadata'))
516 
517         # Cleanup
518         self._delete_server(created_server_id)
519 
520     def test_server_metadata_actions_negative_invalid_state(self):
521         # Create server with metadata
522         server = self._build_minimal_create_server_request()
523 
524         metadata = {'key_1': 'value_1'}
525 
526         server['metadata'] = metadata
527 
528         post = {'server': server}
529         created_server = self.api.post_server(post)
530 
531         found_server = self._wait_for_state_change(created_server, 'BUILD')
532         self.assertEqual('ACTIVE', found_server['status'])
533         self.assertEqual(metadata, found_server.get('metadata'))
534         server_id = found_server['id']
535 
536         # Change status from ACTIVE to SHELVED for negative test
537         self.flags(shelved_offload_time = -1)
538         self.api.post_server_action(server_id, {'shelve': {}})
539         found_server = self._wait_for_state_change(found_server, 'ACTIVE')
540         self.assertEqual('SHELVED', found_server['status'])
541 
542         metadata = {'key_2': 'value_2'}
543 
544         # Update Metadata item in SHELVED (not ACTIVE, etc.)
545         ex = self.assertRaises(client.OpenStackApiException,
546                                self.api.post_server_metadata,
547                                server_id, metadata)
548         self.assertEqual(409, ex.response.status_code)
549         self.assertEqual('SHELVED', found_server['status'])
550 
551         # Delete Metadata item in SHELVED (not ACTIVE, etc.)
552         ex = self.assertRaises(client.OpenStackApiException,
553                                self.api.delete_server_metadata,
554                                server_id, 'key_1')
555         self.assertEqual(409, ex.response.status_code)
556         self.assertEqual('SHELVED', found_server['status'])
557 
558         # Cleanup
559         self._delete_server(server_id)
560 
561     def test_create_and_rebuild_server(self):
562         # Rebuild a server with metadata.
563 
564         # create a server with initially has no metadata
565         server = self._build_minimal_create_server_request()
566         server_post = {'server': server}
567 
568         metadata = {}
569         for i in range(30):
570             metadata['key_%s' % i] = 'value_%s' % i
571 
572         server_post['server']['metadata'] = metadata
573 
574         created_server = self.api.post_server(server_post)
575         LOG.debug("created_server: %s", created_server)
576         self.assertTrue(created_server['id'])
577         created_server_id = created_server['id']
578 
579         created_server = self._wait_for_state_change(created_server, 'BUILD')
580 
581         # rebuild the server with metadata and other server attributes
582         post = {}
583         post['rebuild'] = {
584             self._image_ref_parameter: "76fa36fc-c930-4bf3-8c8a-ea2a2420deb6",
585             "name": "blah",
586             self._access_ipv4_parameter: "172.19.0.2",
587             self._access_ipv6_parameter: "fe80::2",
588             "metadata": {'some': 'thing'},
589         }
590         post['rebuild'].update(self._get_access_ips_params())
591 
592         self.api.post_server_action(created_server_id, post)
593         LOG.debug("rebuilt server: %s", created_server)
594         self.assertTrue(created_server['id'])
595 
596         found_server = self.api.get_server(created_server_id)
597         self.assertEqual(created_server_id, found_server['id'])
598         self.assertEqual({'some': 'thing'}, found_server.get('metadata'))
599         self.assertEqual('blah', found_server.get('name'))
600         self.assertEqual(post['rebuild'][self._image_ref_parameter],
601                          found_server.get('image')['id'])
602         self._verify_access_ips(found_server)
603 
604         # rebuild the server with empty metadata and nothing else
605         post = {}
606         post['rebuild'] = {
607             self._image_ref_parameter: "76fa36fc-c930-4bf3-8c8a-ea2a2420deb6",
608             "metadata": {},
609         }
610 
611         self.api.post_server_action(created_server_id, post)
612         LOG.debug("rebuilt server: %s", created_server)
613         self.assertTrue(created_server['id'])
614 
615         found_server = self.api.get_server(created_server_id)
616         self.assertEqual(created_server_id, found_server['id'])
617         self.assertEqual({}, found_server.get('metadata'))
618         self.assertEqual('blah', found_server.get('name'))
619         self.assertEqual(post['rebuild'][self._image_ref_parameter],
620                          found_server.get('image')['id'])
621         self._verify_access_ips(found_server)
622 
623         # Cleanup
624         self._delete_server(created_server_id)
625 
626     def test_rename_server(self):
627         # Test building and renaming a server.
628 
629         # Create a server
630         server = self._build_minimal_create_server_request()
631         created_server = self.api.post_server({'server': server})
632         LOG.debug("created_server: %s", created_server)
633         server_id = created_server['id']
634         self.assertTrue(server_id)
635 
636         # Rename the server to 'new-name'
637         self.api.put_server(server_id, {'server': {'name': 'new-name'}})
638 
639         # Check the name of the server
640         created_server = self.api.get_server(server_id)
641         self.assertEqual(created_server['name'], 'new-name')
642 
643         # Cleanup
644         self._delete_server(server_id)
645 
646     def test_create_multiple_servers(self):
647         # Creates multiple servers and checks for reservation_id.
648 
649         # Create 2 servers, setting 'return_reservation_id, which should
650         # return a reservation_id
651         server = self._build_minimal_create_server_request()
652         server[self._min_count_parameter] = 2
653         server[self._return_resv_id_parameter] = True
654         post = {'server': server}
655         response = self.api.post_server(post)
656         self.assertIn('reservation_id', response)
657         reservation_id = response['reservation_id']
658         self.assertNotIn(reservation_id, ['', None])
659         # Assert that the reservation_id itself has the expected format
660         self.assertRegex(reservation_id, 'r-[0-9a-zA-Z]{8}')
661 
662         # Create 1 more server, which should not return a reservation_id
663         server = self._build_minimal_create_server_request()
664         post = {'server': server}
665         created_server = self.api.post_server(post)
666         self.assertTrue(created_server['id'])
667         created_server_id = created_server['id']
668 
669         # lookup servers created by the first request.
670         servers = self.api.get_servers(detail=True,
671                 search_opts={'reservation_id': reservation_id})
672         server_map = {server['id']: server for server in servers}
673         found_server = server_map.get(created_server_id)
674         # The server from the 2nd request should not be there.
675         self.assertIsNone(found_server)
676         # Should have found 2 servers.
677         self.assertEqual(len(server_map), 2)
678 
679         # Cleanup
680         self._delete_server(created_server_id)
681         for server_id in server_map:
682             self._delete_server(server_id)
683 
684     def test_create_server_with_injected_files(self):
685         # Creates a server with injected_files.
686         personality = []
687 
688         # Inject a text file
689         data = 'Hello, World!'
690         personality.append({
691             'path': '/helloworld.txt',
692             'contents': base64.encode_as_bytes(data),
693         })
694 
695         # Inject a binary file
696         data = zlib.compress(b'Hello, World!')
697         personality.append({
698             'path': '/helloworld.zip',
699             'contents': base64.encode_as_bytes(data),
700         })
701 
702         # Create server
703         server = self._build_minimal_create_server_request()
704         server['personality'] = personality
705 
706         post = {'server': server}
707 
708         created_server = self.api.post_server(post)
709         LOG.debug("created_server: %s", created_server)
710         self.assertTrue(created_server['id'])
711         created_server_id = created_server['id']
712 
713         # Check it's there
714         found_server = self.api.get_server(created_server_id)
715         self.assertEqual(created_server_id, found_server['id'])
716 
717         found_server = self._wait_for_state_change(found_server, 'BUILD')
718         self.assertEqual('ACTIVE', found_server['status'])
719 
720         # Cleanup
721         self._delete_server(created_server_id)
722 
723     def test_stop_start_servers_negative_invalid_state(self):
724         # Create server
725         server = self._build_minimal_create_server_request()
726         created_server = self.api.post_server({"server": server})
727         created_server_id = created_server['id']
728 
729         found_server = self._wait_for_state_change(created_server, 'BUILD')
730         self.assertEqual('ACTIVE', found_server['status'])
731 
732         # Start server in ACTIVE
733         # NOTE(mkoshiya): When os-start API runs, the server status
734         # must be SHUTOFF.
735         # By returning 409, I want to confirm that the ACTIVE server does not
736         # cause unexpected behavior.
737         post = {'os-start': {}}
738         ex = self.assertRaises(client.OpenStackApiException,
739                                self.api.post_server_action,
740                                created_server_id, post)
741         self.assertEqual(409, ex.response.status_code)
742         self.assertEqual('ACTIVE', found_server['status'])
743 
744         # Stop server
745         post = {'os-stop': {}}
746         self.api.post_server_action(created_server_id, post)
747         found_server = self._wait_for_state_change(found_server, 'ACTIVE')
748         self.assertEqual('SHUTOFF', found_server['status'])
749 
750         # Stop server in SHUTOFF
751         # NOTE(mkoshiya): When os-stop API runs, the server status
752         # must be ACTIVE or ERROR.
753         # By returning 409, I want to confirm that the SHUTOFF server does not
754         # cause unexpected behavior.
755         post = {'os-stop': {}}
756         ex = self.assertRaises(client.OpenStackApiException,
757                                self.api.post_server_action,
758                                created_server_id, post)
759         self.assertEqual(409, ex.response.status_code)
760         self.assertEqual('SHUTOFF', found_server['status'])
761 
762         # Cleanup
763         self._delete_server(created_server_id)
764 
765     def test_revert_resized_server_negative_invalid_state(self):
766         # Create server
767         server = self._build_minimal_create_server_request()
768         created_server = self.api.post_server({"server": server})
769         created_server_id = created_server['id']
770         found_server = self._wait_for_state_change(created_server, 'BUILD')
771         self.assertEqual('ACTIVE', found_server['status'])
772 
773         # Revert resized server in ACTIVE
774         # NOTE(yatsumi): When revert resized server API runs,
775         # the server status must be VERIFY_RESIZE.
776         # By returning 409, I want to confirm that the ACTIVE server does not
777         # cause unexpected behavior.
778         post = {'revertResize': {}}
779         ex = self.assertRaises(client.OpenStackApiException,
780                                self.api.post_server_action,
781                                created_server_id, post)
782         self.assertEqual(409, ex.response.status_code)
783         self.assertEqual('ACTIVE', found_server['status'])
784 
785         # Cleanup
786         self._delete_server(created_server_id)
787 
788     def test_resize_server_negative_invalid_state(self):
789         # Avoid migration
790         self.flags(allow_resize_to_same_host=True)
791 
792         # Create server
793         server = self._build_minimal_create_server_request()
794         created_server = self.api.post_server({"server": server})
795         created_server_id = created_server['id']
796         found_server = self._wait_for_state_change(created_server, 'BUILD')
797         self.assertEqual('ACTIVE', found_server['status'])
798 
799         # Resize server(flavorRef: 1 -> 2)
800         post = {'resize': {"flavorRef": "2", "OS-DCF:diskConfig": "AUTO"}}
801         self.api.post_server_action(created_server_id, post)
802         found_server = self._wait_for_state_change(found_server, 'RESIZE')
803         self.assertEqual('VERIFY_RESIZE', found_server['status'])
804 
805         # Resize server in VERIFY_RESIZE(flavorRef: 2 -> 1)
806         # NOTE(yatsumi): When resize API runs, the server status
807         # must be ACTIVE or SHUTOFF.
808         # By returning 409, I want to confirm that the VERIFY_RESIZE server
809         # does not cause unexpected behavior.
810         post = {'resize': {"flavorRef": "1", "OS-DCF:diskConfig": "AUTO"}}
811         ex = self.assertRaises(client.OpenStackApiException,
812                                self.api.post_server_action,
813                                created_server_id, post)
814         self.assertEqual(409, ex.response.status_code)
815         self.assertEqual('VERIFY_RESIZE', found_server['status'])
816 
817         # Cleanup
818         self._delete_server(created_server_id)
819 
820     def test_confirm_resized_server_negative_invalid_state(self):
821         # Create server
822         server = self._build_minimal_create_server_request()
823         created_server = self.api.post_server({"server": server})
824         created_server_id = created_server['id']
825         found_server = self._wait_for_state_change(created_server, 'BUILD')
826         self.assertEqual('ACTIVE', found_server['status'])
827 
828         # Confirm resized server in ACTIVE
829         # NOTE(yatsumi): When confirm resized server API runs,
830         # the server status must be VERIFY_RESIZE.
831         # By returning 409, I want to confirm that the ACTIVE server does not
832         # cause unexpected behavior.
833         post = {'confirmResize': {}}
834         ex = self.assertRaises(client.OpenStackApiException,
835                                self.api.post_server_action,
836                                created_server_id, post)
837         self.assertEqual(409, ex.response.status_code)
838         self.assertEqual('ACTIVE', found_server['status'])
839 
840         # Cleanup
841         self._delete_server(created_server_id)
842 
843     def test_resize_server_overquota(self):
844         self.flags(cores=1, group='quota')
845         self.flags(ram=512, group='quota')
846         # Create server with default flavor, 1 core, 512 ram
847         server = self._build_minimal_create_server_request()
848         created_server = self.api.post_server({"server": server})
849         created_server_id = created_server['id']
850 
851         found_server = self._wait_for_state_change(created_server, 'BUILD')
852         self.assertEqual('ACTIVE', found_server['status'])
853 
854         # Try to resize to flavorid 2, 1 core, 2048 ram
855         post = {'resize': {'flavorRef': '2'}}
856         ex = self.assertRaises(client.OpenStackApiException,
857                                self.api.post_server_action,
858                                created_server_id, post)
859         self.assertEqual(403, ex.response.status_code)
860 
861     def test_attach_vol_maximum_disk_devices_exceeded(self):
862         self.useFixture(nova_fixtures.CinderFixture(self))
863 
864         server = self._build_minimal_create_server_request()
865         created_server = self.api.post_server({"server": server})
866         server_id = created_server['id']
867         self._wait_for_state_change(created_server, 'BUILD')
868 
869         volume_id = '9a695496-44aa-4404-b2cc-ccab2501f87e'
870         LOG.info('Attaching volume %s to server %s', volume_id, server_id)
871 
872         # The fake driver doesn't implement get_device_name_for_instance, so
873         # we'll just raise the exception directly here, instead of simuluating
874         # an instance with 26 disk devices already attached.
875         with mock.patch.object(self.compute.driver,
876                                'get_device_name_for_instance') as mock_get:
877             mock_get.side_effect = exception.TooManyDiskDevices(maximum=26)
878             ex = self.assertRaises(
879                 client.OpenStackApiException, self.api.post_server_volume,
880                 server_id, dict(volumeAttachment=dict(volumeId=volume_id)))
881             expected = ('The maximum allowed number of disk devices (26) to '
882                         'attach to a single instance has been exceeded.')
883             self.assertEqual(403, ex.response.status_code)
884             self.assertIn(expected, six.text_type(ex))
885 
886 
887 class ServersTestV21(ServersTest):
888     api_major_version = 'v2.1'
889 
890 
891 class ServersTestV219(ServersTestBase):
892     api_major_version = 'v2.1'
893 
894     def _create_server(self, set_desc = True, desc = None):
895         server = self._build_minimal_create_server_request()
896         if set_desc:
897             server['description'] = desc
898         post = {'server': server}
899         response = self.api.api_post('/servers', post).body
900         return (server, response['server'])
901 
902     def _update_server(self, server_id, set_desc = True, desc = None):
903         new_name = integrated_helpers.generate_random_alphanumeric(8)
904         server = {'server': {'name': new_name}}
905         if set_desc:
906             server['server']['description'] = desc
907         self.api.api_put('/servers/%s' % server_id, server)
908 
909     def _rebuild_server(self, server_id, set_desc = True, desc = None):
910         new_name = integrated_helpers.generate_random_alphanumeric(8)
911         post = {}
912         post['rebuild'] = {
913             "name": new_name,
914             self._image_ref_parameter: "76fa36fc-c930-4bf3-8c8a-ea2a2420deb6",
915             self._access_ipv4_parameter: "172.19.0.2",
916             self._access_ipv6_parameter: "fe80::2",
917             "metadata": {'some': 'thing'},
918         }
919         post['rebuild'].update(self._get_access_ips_params())
920         if set_desc:
921             post['rebuild']['description'] = desc
922         self.api.api_post('/servers/%s/action' % server_id, post)
923 
924     def _create_server_and_verify(self, set_desc = True, expected_desc = None):
925         # Creates a server with a description and verifies it is
926         # in the GET responses.
927         created_server_id = self._create_server(set_desc,
928                                                 expected_desc)[1]['id']
929         self._verify_server_description(created_server_id, expected_desc)
930         self._delete_server(created_server_id)
931 
932     def _update_server_and_verify(self, server_id, set_desc = True,
933                                   expected_desc = None):
934         # Updates a server with a description and verifies it is
935         # in the GET responses.
936         self._update_server(server_id, set_desc, expected_desc)
937         self._verify_server_description(server_id, expected_desc)
938 
939     def _rebuild_server_and_verify(self, server_id, set_desc = True,
940                                   expected_desc = None):
941         # Rebuilds a server with a description and verifies it is
942         # in the GET responses.
943         self._rebuild_server(server_id, set_desc, expected_desc)
944         self._verify_server_description(server_id, expected_desc)
945 
946     def _verify_server_description(self, server_id, expected_desc = None,
947                                    desc_in_resp = True):
948         # Calls GET on the servers and verifies that the description
949         # is set as expected in the response, or not set at all.
950         response = self.api.api_get('/servers/%s' % server_id)
951         found_server = response.body['server']
952         self.assertEqual(server_id, found_server['id'])
953         if desc_in_resp:
954             # Verify the description is set as expected (can be None)
955             self.assertEqual(expected_desc, found_server.get('description'))
956         else:
957             # Verify the description is not included in the response.
958             self.assertNotIn('description', found_server)
959 
960         servers = self.api.api_get('/servers/detail').body['servers']
961         server_map = {server['id']: server for server in servers}
962         found_server = server_map.get(server_id)
963         self.assertTrue(found_server)
964         if desc_in_resp:
965             # Verify the description is set as expected (can be None)
966             self.assertEqual(expected_desc, found_server.get('description'))
967         else:
968             # Verify the description is not included in the response.
969             self.assertNotIn('description', found_server)
970 
971     def _create_assertRaisesRegex(self, desc):
972         # Verifies that a 400 error is thrown on create server
973         with self.assertRaisesRegex(client.OpenStackApiException,
974                                     ".*Unexpected status code.*") as cm:
975             self._create_server(True, desc)
976             self.assertEqual(400, cm.exception.response.status_code)
977 
978     def _update_assertRaisesRegex(self, server_id, desc):
979         # Verifies that a 400 error is thrown on update server
980         with self.assertRaisesRegex(client.OpenStackApiException,
981                                     ".*Unexpected status code.*") as cm:
982             self._update_server(server_id, True, desc)
983             self.assertEqual(400, cm.exception.response.status_code)
984 
985     def _rebuild_assertRaisesRegex(self, server_id, desc):
986         # Verifies that a 400 error is thrown on rebuild server
987         with self.assertRaisesRegex(client.OpenStackApiException,
988                                     ".*Unexpected status code.*") as cm:
989             self._rebuild_server(server_id, True, desc)
990             self.assertEqual(400, cm.exception.response.status_code)
991 
992     def test_create_server_with_description(self):
993         self.api.microversion = '2.19'
994         # Create and get a server with a description
995         self._create_server_and_verify(True, 'test description')
996         # Create and get a server with an empty description
997         self._create_server_and_verify(True, '')
998         # Create and get a server with description set to None
999         self._create_server_and_verify()
1000         # Create and get a server without setting the description
1001         self._create_server_and_verify(False)
1002 
1003     def test_update_server_with_description(self):
1004         self.api.microversion = '2.19'
1005         # Create a server with an initial description
1006         server_id = self._create_server(True, 'test desc 1')[1]['id']
1007 
1008         # Update and get the server with a description
1009         self._update_server_and_verify(server_id, True, 'updated desc')
1010         # Update and get the server name without changing the description
1011         self._update_server_and_verify(server_id, False, 'updated desc')
1012         # Update and get the server with an empty description
1013         self._update_server_and_verify(server_id, True, '')
1014         # Update and get the server by removing the description (set to None)
1015         self._update_server_and_verify(server_id)
1016         # Update and get the server with a 2nd new description
1017         self._update_server_and_verify(server_id, True, 'updated desc2')
1018 
1019         # Cleanup
1020         self._delete_server(server_id)
1021 
1022     def test_rebuild_server_with_description(self):
1023         self.api.microversion = '2.19'
1024 
1025         # Create a server with an initial description
1026         server = self._create_server(True, 'test desc 1')[1]
1027         server_id = server['id']
1028         self._wait_for_state_change(server, 'BUILD')
1029 
1030         # Rebuild and get the server with a description
1031         self._rebuild_server_and_verify(server_id, True, 'updated desc')
1032         # Rebuild and get the server name without changing the description
1033         self._rebuild_server_and_verify(server_id, False, 'updated desc')
1034         # Rebuild and get the server with an empty description
1035         self._rebuild_server_and_verify(server_id, True, '')
1036         # Rebuild and get the server by removing the description (set to None)
1037         self._rebuild_server_and_verify(server_id)
1038         # Rebuild and get the server with a 2nd new description
1039         self._rebuild_server_and_verify(server_id, True, 'updated desc2')
1040 
1041         # Cleanup
1042         self._delete_server(server_id)
1043 
1044     def test_version_compatibility(self):
1045         # Create a server with microversion v2.19 and a description.
1046         self.api.microversion = '2.19'
1047         server_id = self._create_server(True, 'test desc 1')[1]['id']
1048         # Verify that the description is not included on V2.18 GETs
1049         self.api.microversion = '2.18'
1050         self._verify_server_description(server_id, desc_in_resp = False)
1051         # Verify that updating the server with description on V2.18
1052         # results in a 400 error
1053         self._update_assertRaisesRegex(server_id, 'test update 2.18')
1054         # Verify that rebuilding the server with description on V2.18
1055         # results in a 400 error
1056         self._rebuild_assertRaisesRegex(server_id, 'test rebuild 2.18')
1057 
1058         # Cleanup
1059         self._delete_server(server_id)
1060 
1061         # Create a server on V2.18 and verify that the description
1062         # defaults to the name on a V2.19 GET
1063         server_req, response = self._create_server(False)
1064         server_id = response['id']
1065         self.api.microversion = '2.19'
1066         self._verify_server_description(server_id, server_req['name'])
1067 
1068         # Cleanup
1069         self._delete_server(server_id)
1070 
1071         # Verify that creating a server with description on V2.18
1072         # results in a 400 error
1073         self.api.microversion = '2.18'
1074         self._create_assertRaisesRegex('test create 2.18')
1075 
1076     def test_description_errors(self):
1077         self.api.microversion = '2.19'
1078         # Create servers with invalid descriptions.  These throw 400.
1079         # Invalid unicode with non-printable control char
1080         self._create_assertRaisesRegex(u'invalid\0dstring')
1081         # Description is longer than 255 chars
1082         self._create_assertRaisesRegex('x' * 256)
1083 
1084         # Update and rebuild servers with invalid descriptions.
1085         # These throw 400.
1086         server_id = self._create_server(True, "desc")[1]['id']
1087         # Invalid unicode with non-printable control char
1088         self._update_assertRaisesRegex(server_id, u'invalid\u0604string')
1089         self._rebuild_assertRaisesRegex(server_id, u'invalid\u0604string')
1090         # Description is longer than 255 chars
1091         self._update_assertRaisesRegex(server_id, 'x' * 256)
1092         self._rebuild_assertRaisesRegex(server_id, 'x' * 256)
1093 
1094 
1095 class ServerTestV220(ServersTestBase):
1096     api_major_version = 'v2.1'
1097 
1098     def setUp(self):
1099         super(ServerTestV220, self).setUp()
1100         self.api.microversion = '2.20'
1101         self.ctxt = context.get_admin_context()
1102 
1103     def _create_server(self):
1104         server = self._build_minimal_create_server_request()
1105         post = {'server': server}
1106         response = self.api.api_post('/servers', post).body
1107         return (server, response['server'])
1108 
1109     def _shelve_server(self):
1110         server = self._create_server()[1]
1111         server_id = server['id']
1112         self._wait_for_state_change(server, 'BUILD')
1113         self.api.post_server_action(server_id, {'shelve': None})
1114         return self._wait_for_state_change(server, 'ACTIVE')
1115 
1116     def _get_fake_bdms(self, ctxt):
1117         return block_device_obj.block_device_make_list(self.ctxt,
1118                     [fake_block_device.FakeDbBlockDeviceDict(
1119                     {'device_name': '/dev/vda',
1120                      'source_type': 'volume',
1121                      'destination_type': 'volume',
1122                      'volume_id': '5d721593-f033-4f6d-ab6f-b5b067e61bc4'})])
1123 
1124     def test_attach_detach_vol_to_shelved_offloaded_server_new_flow(self):
1125         self.flags(shelved_offload_time=0)
1126         found_server = self._shelve_server()
1127         self.assertEqual('SHELVED_OFFLOADED', found_server['status'])
1128         server_id = found_server['id']
1129         fake_bdms = self._get_fake_bdms(self.ctxt)
1130 
1131         # Test attach volume
1132         self.stub_out('nova.volume.cinder.API.get', fakes.stub_volume_get)
1133         with test.nested(mock.patch.object(compute_api.API,
1134                             '_check_volume_already_attached_to_instance'),
1135                          mock.patch.object(volume.cinder.API,
1136                                         'check_availability_zone'),
1137                          mock.patch.object(volume.cinder.API,
1138                                         'attachment_create'),
1139                          mock.patch.object(volume.cinder.API,
1140                                         'attachment_complete')
1141                          ) as (mock_check_vol_attached,
1142                                mock_check_av_zone, mock_attach_create,
1143                                mock_attachment_complete):
1144             mock_attach_create.return_value = {'id': uuids.volume}
1145             volume_attachment = {"volumeAttachment": {"volumeId":
1146                                        "5d721593-f033-4f6d-ab6f-b5b067e61bc4"}}
1147             attach_response = self.api.api_post(
1148                              '/servers/%s/os-volume_attachments' % (server_id),
1149                              volume_attachment).body['volumeAttachment']
1150             self.assertTrue(mock_attach_create.called)
1151             mock_attachment_complete.assert_called_once_with(
1152                 mock.ANY, uuids.volume)
1153             self.assertIsNone(attach_response['device'])
1154 
1155         # Test detach volume
1156         with test.nested(mock.patch.object(objects.BlockDeviceMappingList,
1157                                            'get_by_instance_uuid'),
1158                          mock.patch.object(compute_api.API,
1159                                            '_local_cleanup_bdm_volumes')
1160                          ) as (mock_get_bdms, mock_clean_vols):
1161 
1162             mock_get_bdms.return_value = fake_bdms
1163             attachment_id = mock_get_bdms.return_value[0]['volume_id']
1164             self.api.api_delete('/servers/%s/os-volume_attachments/%s' %
1165                             (server_id, attachment_id))
1166             self.assertTrue(mock_clean_vols.called)
1167 
1168         self._delete_server(server_id)
1169 
1170 
1171 class ServerTestV269(ServersTestBase):
1172     api_major_version = 'v2.1'
1173     NUMBER_OF_CELLS = 3
1174 
1175     def setUp(self):
1176         super(ServerTestV269, self).setUp()
1177         self.api.microversion = '2.69'
1178 
1179         self.ctxt = context.get_admin_context()
1180         self.project_id = self.api.project_id
1181         self.cells = objects.CellMappingList.get_all(self.ctxt)
1182         self.down_cell_insts = []
1183         self.up_cell_insts = []
1184         self.down_cell_mappings = objects.CellMappingList()
1185         flavor = objects.Flavor(id=1, name='flavor1',
1186                                 memory_mb=256, vcpus=1,
1187                                 root_gb=1, ephemeral_gb=1,
1188                                 flavorid='1',
1189                                 swap=0, rxtx_factor=1.0,
1190                                 vcpu_weight=1,
1191                                 disabled=False,
1192                                 is_public=True,
1193                                 extra_specs={},
1194                                 projects=[])
1195         _info_cache = objects.InstanceInfoCache(context)
1196         objects.InstanceInfoCache._from_db_object(context, _info_cache,
1197             test_instance_info_cache.fake_info_cache)
1198         # cell1 and cell2 will be the down cells while
1199         # cell0 and cell3 will be the up cells.
1200         down_cell_names = ['cell1', 'cell2']
1201         for cell in self.cells:
1202             # create 2 instances and their mappings in all the 4 cells
1203             for i in range(2):
1204                 with context.target_cell(self.ctxt, cell) as cctxt:
1205                     inst = objects.Instance(
1206                         context=cctxt,
1207                         project_id=self.project_id,
1208                         user_id=self.project_id,
1209                         instance_type_id=flavor.id,
1210                         hostname='%s-inst%i' % (cell.name, i),
1211                         flavor=flavor,
1212                         info_cache=_info_cache,
1213                         display_name='server-test')
1214                     inst.create()
1215                 im = objects.InstanceMapping(context=self.ctxt,
1216                                              instance_uuid=inst.uuid,
1217                                              cell_mapping=cell,
1218                                              project_id=self.project_id,
1219                                              queued_for_delete=False)
1220                 im.create()
1221                 if cell.name in down_cell_names:
1222                     self.down_cell_insts.append(inst.uuid)
1223                 else:
1224                     self.up_cell_insts.append(inst.uuid)
1225             # In cell1 and cell3 add a third instance in a different project
1226             # to show the --all-tenants case.
1227             if cell.name == 'cell1' or cell.name == 'cell3':
1228                 with context.target_cell(self.ctxt, cell) as cctxt:
1229                     inst = objects.Instance(
1230                         context=cctxt,
1231                         project_id='faker',
1232                         user_id='faker',
1233                         instance_type_id=flavor.id,
1234                         hostname='%s-inst%i' % (cell.name, 3),
1235                         flavor=flavor,
1236                         info_cache=_info_cache,
1237                         display_name='server-test')
1238                     inst.create()
1239                 im = objects.InstanceMapping(context=self.ctxt,
1240                                              instance_uuid=inst.uuid,
1241                                              cell_mapping=cell,
1242                                              project_id='faker',
1243                                              queued_for_delete=False)
1244                 im.create()
1245             if cell.name in down_cell_names:
1246                 self.down_cell_mappings.objects.append(cell)
1247         self.useFixture(nova_fixtures.DownCellFixture(self.down_cell_mappings))
1248 
1249     def test_get_servers_with_down_cells(self):
1250         servers = self.api.get_servers(detail=False)
1251         # 4 servers from the up cells and 4 servers from the down cells
1252         self.assertEqual(8, len(servers))
1253         for server in servers:
1254             if 'name' not in server:
1255                 # server is in the down cell.
1256                 self.assertEqual('UNKNOWN', server['status'])
1257                 self.assertIn(server['id'], self.down_cell_insts)
1258                 self.assertIn('links', server)
1259                 # the partial construct will have only the above 3 keys
1260                 self.assertEqual(3, len(server))
1261             else:
1262                 # server in up cell
1263                 self.assertIn(server['id'], self.up_cell_insts)
1264                 # has all the keys
1265                 self.assertEqual(server['name'], 'server-test')
1266                 self.assertIn('links', server)
1267 
1268     def test_get_servers_detail_with_down_cells(self):
1269         servers = self.api.get_servers()
1270         # 4 servers from the up cells and 4 servers from the down cells
1271         self.assertEqual(8, len(servers))
1272         for server in servers:
1273             if 'user_id' not in server:
1274                 # server is in the down cell.
1275                 self.assertEqual('UNKNOWN', server['status'])
1276                 self.assertIn(server['id'], self.down_cell_insts)
1277                 # the partial construct will have only 5 keys:
1278                 # created, tenant_id, status, id and links.
1279                 self.assertEqual(5, len(server))
1280             else:
1281                 # server in up cell
1282                 self.assertIn(server['id'], self.up_cell_insts)
1283                 # has all the keys
1284                 self.assertEqual(server['user_id'], self.project_id)
1285                 self.assertIn('image', server)
1286 
1287     def test_get_servers_detail_limits_with_down_cells(self):
1288         servers = self.api.get_servers(search_opts={'limit': 5})
1289         # 4 servers from the up cells since we skip down cell
1290         # results by default for paging.
1291         self.assertEqual(4, len(servers), servers)
1292         for server in servers:
1293             # server in up cell
1294             self.assertIn(server['id'], self.up_cell_insts)
1295             # has all the keys
1296             self.assertEqual(server['user_id'], self.project_id)
1297             self.assertIn('image', server)
1298 
1299     def test_get_servers_detail_limits_with_down_cells_the_500_gift(self):
1300         self.flags(list_records_by_skipping_down_cells=False, group='api')
1301         # We get an API error with a 500 response code since the
1302         # list_records_by_skipping_down_cells config option is False.
1303         exp = self.assertRaises(client.OpenStackApiException,
1304                                 self.api.get_servers,
1305                                 search_opts={'limit': 5})
1306         self.assertEqual(500, exp.response.status_code)
1307         self.assertIn('NovaException', six.text_type(exp))
1308 
1309     def test_get_servers_detail_marker_in_down_cells(self):
1310         marker = self.down_cell_insts[2]
1311         # It will fail with a 500 if the marker is in the down cell.
1312         exp = self.assertRaises(client.OpenStackApiException,
1313                                 self.api.get_servers,
1314                                 search_opts={'marker': marker})
1315         self.assertEqual(500, exp.response.status_code)
1316         self.assertIn('oslo_db.exception.DBError', six.text_type(exp))
1317 
1318     def test_get_servers_detail_marker_sorting(self):
1319         marker = self.up_cell_insts[1]
1320         # It will give the results from the up cell if
1321         # list_records_by_skipping_down_cells config option is True.
1322         servers = self.api.get_servers(search_opts={'marker': marker,
1323                                                     'sort_key': "created_at",
1324                                                     'sort_dir': "asc"})
1325         # since there are 4 servers from the up cells, when giving the
1326         # second instance as marker, sorted by creation time in ascending
1327         # third and fourth instances will be returned.
1328         self.assertEqual(2, len(servers))
1329         for server in servers:
1330             self.assertIn(
1331                 server['id'], [self.up_cell_insts[2], self.up_cell_insts[3]])
1332 
1333     def test_get_servers_detail_non_admin_with_deleted_flag(self):
1334         # if list_records_by_skipping_down_cells config option is True
1335         # this deleted option should be ignored and the rest of the instances
1336         # from the up cells and the partial results from the down cells should
1337         # be returned.
1338         # Set the policy so we don't have permission to allow
1339         # all filters but are able to get server details.
1340         servers_rule = 'os_compute_api:servers:detail'
1341         extraspec_rule = 'os_compute_api:servers:allow_all_filters'
1342         self.policy.set_rules({
1343             extraspec_rule: 'rule:admin_api',
1344             servers_rule: '@'})
1345         servers = self.api.get_servers(search_opts={'deleted': True})
1346         # gets 4 results from up cells and 4 from down cells.
1347         self.assertEqual(8, len(servers))
1348         for server in servers:
1349             if "image" not in server:
1350                 self.assertIn(server['id'], self.down_cell_insts)
1351             else:
1352                 self.assertIn(server['id'], self.up_cell_insts)
1353 
1354     def test_get_servers_detail_filters(self):
1355         # We get the results only from the up cells, this ignoring the down
1356         # cells if list_records_by_skipping_down_cells config option is True.
1357         api_fixture = self.useFixture(nova_fixtures.OSAPIFixture(
1358             api_version='v2.1'))
1359         self.admin_api = api_fixture.admin_api
1360         self.admin_api.microversion = '2.69'
1361         servers = self.admin_api.get_servers(
1362             search_opts={'hostname': "cell3-inst0"})
1363         self.assertEqual(1, len(servers))
1364         self.assertEqual(self.up_cell_insts[2], servers[0]['id'])
1365 
1366     def test_get_servers_detail_all_tenants_with_down_cells(self):
1367         api_fixture = self.useFixture(nova_fixtures.OSAPIFixture(
1368             api_version='v2.1'))
1369         self.admin_api = api_fixture.admin_api
1370         self.admin_api.microversion = '2.69'
1371         servers = self.admin_api.get_servers(search_opts={'all_tenants': True})
1372         # 4 servers from the up cells and 4 servers from the down cells
1373         # plus the 2 instances from cell1 and cell3 which are in a different
1374         # project.
1375         self.assertEqual(10, len(servers))
1376         for server in servers:
1377             if 'user_id' not in server:
1378                 # server is in the down cell.
1379                 self.assertEqual('UNKNOWN', server['status'])
1380                 if server['tenant_id'] != 'faker':
1381                     self.assertIn(server['id'], self.down_cell_insts)
1382                 # the partial construct will have only 5 keys:
1383                 # created, tenant_id, status, id and links
1384                 self.assertEqual(5, len(server))
1385             else:
1386                 # server in up cell
1387                 if server['tenant_id'] != 'faker':
1388                     self.assertIn(server['id'], self.up_cell_insts)
1389                     self.assertEqual(server['user_id'], self.project_id)
1390                 self.assertIn('image', server)
1391 
1392 
1393 class ServerRebuildTestCase(integrated_helpers._IntegratedTestBase,
1394                             integrated_helpers.InstanceHelperMixin):
1395     api_major_version = 'v2.1'
1396     # We have to cap the microversion at 2.38 because that's the max we
1397     # can use to update image metadata via our compute images proxy API.
1398     microversion = '2.38'
1399 
1400     def _disable_compute_for(self, server):
1401         # Refresh to get its host
1402         server = self.api.get_server(server['id'])
1403         host = server['OS-EXT-SRV-ATTR:host']
1404 
1405         # Disable the service it is on
1406         self.api_fixture.admin_api.put_service('disable',
1407                                                {'host': host,
1408                                                 'binary': 'nova-compute'})
1409 
1410     def test_rebuild_with_image_novalidhost(self):
1411         """Creates a server with an image that is valid for the single compute
1412         that we have. Then rebuilds the server, passing in an image with
1413         metadata that does not fit the single compute which should result in
1414         a NoValidHost error. The ImagePropertiesFilter filter is enabled by
1415         default so that should filter out the host based on the image meta.
1416         """
1417 
1418         self.compute2 = self.start_service('compute', host='host2')
1419 
1420         # We hard-code from a fake image since we can't get images
1421         # via the compute /images proxy API with microversion > 2.35.
1422         original_image_ref = '155d900f-4e14-4e4c-a73d-069cbf4541e6'
1423         server_req_body = {
1424             'server': {
1425                 'imageRef': original_image_ref,
1426                 'flavorRef': '1',   # m1.tiny from DefaultFlavorsFixture,
1427                 'name': 'test_rebuild_with_image_novalidhost',
1428                 # We don't care about networking for this test. This requires
1429                 # microversion >= 2.37.
1430                 'networks': 'none'
1431             }
1432         }
1433         server = self.api.post_server(server_req_body)
1434         self._wait_for_state_change(self.api, server, 'ACTIVE')
1435 
1436         # Disable the host we're on so ComputeFilter would have ruled it out
1437         # normally
1438         self._disable_compute_for(server)
1439 
1440         # Now update the image metadata to be something that won't work with
1441         # the fake compute driver we're using since the fake driver has an
1442         # "x86_64" architecture.
1443         rebuild_image_ref = (
1444             nova.tests.unit.image.fake.AUTO_DISK_CONFIG_ENABLED_IMAGE_UUID)
1445         self.api.put_image_meta_key(
1446             rebuild_image_ref, 'hw_architecture', 'unicore32')
1447         # Now rebuild the server with that updated image and it should result
1448         # in a NoValidHost failure from the scheduler.
1449         rebuild_req_body = {
1450             'rebuild': {
1451                 'imageRef': rebuild_image_ref
1452             }
1453         }
1454         # Since we're using the CastAsCall fixture, the NoValidHost error
1455         # should actually come back to the API and result in a 500 error.
1456         # Normally the user would get a 202 response because nova-api RPC casts
1457         # to nova-conductor which RPC calls the scheduler which raises the
1458         # NoValidHost. We can mimic the end user way to figure out the failure
1459         # by looking for the failed 'rebuild' instance action event.
1460         self.api.api_post('/servers/%s/action' % server['id'],
1461                           rebuild_req_body, check_response_status=[500])
1462         # Look for the failed rebuild action.
1463         self._wait_for_action_fail_completion(
1464             server, instance_actions.REBUILD, 'rebuild_server',
1465             # Before microversion 2.51 events are only returned for instance
1466             # actions if you're an admin.
1467             self.api_fixture.admin_api)
1468         # Assert the server image_ref was rolled back on failure.
1469         server = self.api.get_server(server['id'])
1470         self.assertEqual(original_image_ref, server['image']['id'])
1471 
1472         # The server should be in ERROR state
1473         self.assertEqual('ERROR', server['status'])
1474         self.assertIn('No valid host', server['fault']['message'])
1475 
1476         # Rebuild it again with the same bad image to make sure it's rejected
1477         # again. Since we're using CastAsCall here, there is no 202 from the
1478         # API, and the exception from conductor gets passed back through the
1479         # API.
1480         ex = self.assertRaises(
1481             client.OpenStackApiException, self.api.api_post,
1482             '/servers/%s/action' % server['id'], rebuild_req_body)
1483         self.assertIn('NoValidHost', six.text_type(ex))
1484 
1485     # A rebuild to the same host should never attempt a rebuild claim.
1486     @mock.patch('nova.compute.resource_tracker.ResourceTracker.rebuild_claim',
1487                 new_callable=mock.NonCallableMock)
1488     def test_rebuild_with_new_image(self, mock_rebuild_claim):
1489         """Rebuilds a server with a different image which will run it through
1490         the scheduler to validate the image is still OK with the compute host
1491         that the instance is running on.
1492 
1493         Validates that additional resources are not allocated against the
1494         instance.host in Placement due to the rebuild on same host.
1495         """
1496         admin_api = self.api_fixture.admin_api
1497         admin_api.microversion = '2.53'
1498 
1499         def _get_provider_uuid_by_host(host):
1500             resp = admin_api.api_get(
1501                 'os-hypervisors?hypervisor_hostname_pattern=%s' % host).body
1502             return resp['hypervisors'][0]['id']
1503 
1504         def _get_provider_usages(provider_uuid):
1505             return self.placement_api.get(
1506                 '/resource_providers/%s/usages' % provider_uuid).body['usages']
1507 
1508         def _get_allocations_by_server_uuid(server_uuid):
1509             return self.placement_api.get(
1510                 '/allocations/%s' % server_uuid).body['allocations']
1511 
1512         def _set_provider_inventory(rp_uuid, resource_class, inventory):
1513             # Get the resource provider generation for the inventory update.
1514             rp = self.placement_api.get(
1515                 '/resource_providers/%s' % rp_uuid).body
1516             inventory['resource_provider_generation'] = rp['generation']
1517             return self.placement_api.put(
1518                 '/resource_providers/%s/inventories/%s' %
1519                 (rp_uuid, resource_class), inventory).body
1520 
1521         def assertFlavorMatchesAllocation(flavor, allocation):
1522             self.assertEqual(flavor['vcpus'], allocation['VCPU'])
1523             self.assertEqual(flavor['ram'], allocation['MEMORY_MB'])
1524             self.assertEqual(flavor['disk'], allocation['DISK_GB'])
1525 
1526         nodename = self.compute.manager._get_nodename(None)
1527         rp_uuid = _get_provider_uuid_by_host(nodename)
1528         # make sure we start with no usage on the compute node
1529         rp_usages = _get_provider_usages(rp_uuid)
1530         self.assertEqual({'VCPU': 0, 'MEMORY_MB': 0, 'DISK_GB': 0}, rp_usages)
1531 
1532         server_req_body = {
1533             'server': {
1534                 # We hard-code from a fake image since we can't get images
1535                 # via the compute /images proxy API with microversion > 2.35.
1536                 'imageRef': '155d900f-4e14-4e4c-a73d-069cbf4541e6',
1537                 'flavorRef': '1',   # m1.tiny from DefaultFlavorsFixture,
1538                 'name': 'test_rebuild_with_new_image',
1539                 # We don't care about networking for this test. This requires
1540                 # microversion >= 2.37.
1541                 'networks': 'none'
1542             }
1543         }
1544         server = self.api.post_server(server_req_body)
1545         self._wait_for_state_change(self.api, server, 'ACTIVE')
1546 
1547         flavor = self.api.api_get('/flavors/1').body['flavor']
1548 
1549         # make the compute node full and ensure rebuild still succeed
1550         _set_provider_inventory(rp_uuid, "VCPU", {"total": 1})
1551 
1552         # There should be usage for the server on the compute node now.
1553         rp_usages = _get_provider_usages(rp_uuid)
1554         assertFlavorMatchesAllocation(flavor, rp_usages)
1555         allocs = _get_allocations_by_server_uuid(server['id'])
1556         self.assertIn(rp_uuid, allocs)
1557         allocs = allocs[rp_uuid]['resources']
1558         assertFlavorMatchesAllocation(flavor, allocs)
1559 
1560         rebuild_image_ref = (
1561             nova.tests.unit.image.fake.AUTO_DISK_CONFIG_ENABLED_IMAGE_UUID)
1562         # Now rebuild the server with a different image.
1563         rebuild_req_body = {
1564             'rebuild': {
1565                 'imageRef': rebuild_image_ref
1566             }
1567         }
1568         self.api.api_post('/servers/%s/action' % server['id'],
1569                           rebuild_req_body)
1570         self._wait_for_server_parameter(
1571             self.api, server, {'OS-EXT-STS:task_state': None})
1572 
1573         # The usage and allocations should not have changed.
1574         rp_usages = _get_provider_usages(rp_uuid)
1575         assertFlavorMatchesAllocation(flavor, rp_usages)
1576 
1577         allocs = _get_allocations_by_server_uuid(server['id'])
1578         self.assertIn(rp_uuid, allocs)
1579         allocs = allocs[rp_uuid]['resources']
1580         assertFlavorMatchesAllocation(flavor, allocs)
1581 
1582     def test_volume_backed_rebuild_different_image(self):
1583         """Tests that trying to rebuild a volume-backed instance with a
1584         different image than what is in the root disk of the root volume
1585         will result in a 400 BadRequest error.
1586         """
1587         self.useFixture(nova_fixtures.CinderFixture(self))
1588         # First create our server as normal.
1589         server_req_body = {
1590             # There is no imageRef because this is boot from volume.
1591             'server': {
1592                 'flavorRef': '1',  # m1.tiny from DefaultFlavorsFixture,
1593                 'name': 'test_volume_backed_rebuild_different_image',
1594                 # We don't care about networking for this test. This requires
1595                 # microversion >= 2.37.
1596                 'networks': 'none',
1597                 'block_device_mapping_v2': [{
1598                     'boot_index': 0,
1599                     'uuid':
1600                     nova_fixtures.CinderFixture.IMAGE_BACKED_VOL,
1601                     'source_type': 'volume',
1602                     'destination_type': 'volume'
1603                 }]
1604             }
1605         }
1606         server = self.api.post_server(server_req_body)
1607         server = self._wait_for_state_change(self.api, server, 'ACTIVE')
1608         # For a volume-backed server, the image ref will be an empty string
1609         # in the server response.
1610         self.assertEqual('', server['image'])
1611 
1612         # Now rebuild the server with a different image than was used to create
1613         # our fake volume.
1614         rebuild_image_ref = (
1615             nova.tests.unit.image.fake.AUTO_DISK_CONFIG_ENABLED_IMAGE_UUID)
1616         rebuild_req_body = {
1617             'rebuild': {
1618                 'imageRef': rebuild_image_ref
1619             }
1620         }
1621         resp = self.api.api_post('/servers/%s/action' % server['id'],
1622                                  rebuild_req_body, check_response_status=[400])
1623         # Assert that we failed because of the image change and not something
1624         # else.
1625         self.assertIn('Unable to rebuild with a different image for a '
1626                       'volume-backed server', six.text_type(resp))
1627 
1628 
1629 class ServerMovingTests(integrated_helpers.ProviderUsageBaseTestCase):
1630     """Tests moving servers while checking the resource allocations and usages
1631 
1632     These tests use two compute hosts. Boot a server on one of them then try to
1633     move the server to the other. At every step resource allocation of the
1634     server and the resource usages of the computes are queried from placement
1635     API and asserted.
1636     """
1637 
1638     REQUIRES_LOCKING = True
1639     # NOTE(danms): The test defaults to using SmallFakeDriver,
1640     # which only has one vcpu, which can't take the doubled allocation
1641     # we're now giving it. So, use the bigger MediumFakeDriver here.
1642     compute_driver = 'fake.MediumFakeDriver'
1643 
1644     def setUp(self):
1645         super(ServerMovingTests, self).setUp()
1646         fake_notifier.stub_notifier(self)
1647         self.addCleanup(fake_notifier.reset)
1648 
1649         self.compute1 = self._start_compute(host='host1')
1650         self.compute2 = self._start_compute(host='host2')
1651 
1652         flavors = self.api.get_flavors()
1653         self.flavor1 = flavors[0]
1654         self.flavor2 = flavors[1]
1655         # create flavor3 which has less MEMORY_MB but more DISK_GB than flavor2
1656         flavor_body = {'flavor':
1657                            {'name': 'test_flavor3',
1658                             'ram': int(self.flavor2['ram'] / 2),
1659                             'vcpus': 1,
1660                             'disk': self.flavor2['disk'] * 2,
1661                             'id': 'a22d5517-147c-4147-a0d1-e698df5cd4e3'
1662                             }}
1663 
1664         self.flavor3 = self.api.post_flavor(flavor_body)
1665 
1666     def _other_hostname(self, host):
1667         other_host = {'host1': 'host2',
1668                       'host2': 'host1'}
1669         return other_host[host]
1670 
1671     def _run_periodics(self):
1672         # NOTE(jaypipes): We always run periodics in the same order: first on
1673         # compute1, then on compute2. However, we want to test scenarios when
1674         # the periodics run at different times during mover operations. This is
1675         # why we have the "reverse" tests which simply switch the source and
1676         # dest host while keeping the order in which we run the
1677         # periodics. This effectively allows us to test the matrix of timing
1678         # scenarios during move operations.
1679         ctx = context.get_admin_context()
1680         LOG.info('Running periodic for compute1 (%s)',
1681             self.compute1.manager.host)
1682         self.compute1.manager.update_available_resource(ctx)
1683         LOG.info('Running periodic for compute2 (%s)',
1684             self.compute2.manager.host)
1685         self.compute2.manager.update_available_resource(ctx)
1686         LOG.info('Finished with periodics')
1687 
1688     def test_resize_revert(self):
1689         self._test_resize_revert(dest_hostname='host1')
1690 
1691     def test_resize_revert_reverse(self):
1692         self._test_resize_revert(dest_hostname='host2')
1693 
1694     def test_resize_confirm(self):
1695         self._test_resize_confirm(dest_hostname='host1')
1696 
1697     def test_resize_confirm_reverse(self):
1698         self._test_resize_confirm(dest_hostname='host2')
1699 
1700     def _resize_and_check_allocations(self, server, old_flavor, new_flavor,
1701             source_rp_uuid, dest_rp_uuid):
1702         self.flags(allow_resize_to_same_host=False)
1703         resize_req = {
1704             'resize': {
1705                 'flavorRef': new_flavor['id']
1706             }
1707         }
1708         self._move_and_check_allocations(
1709             server, request=resize_req, old_flavor=old_flavor,
1710             new_flavor=new_flavor, source_rp_uuid=source_rp_uuid,
1711             dest_rp_uuid=dest_rp_uuid)
1712 
1713     def test_migration_confirm_resize_error(self):
1714         source_hostname = self.compute1.host
1715         dest_hostname = self.compute2.host
1716 
1717         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
1718         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
1719 
1720         server = self._boot_and_check_allocations(self.flavor1,
1721                                                   source_hostname)
1722 
1723         self._move_and_check_allocations(
1724             server, request={'migrate': None}, old_flavor=self.flavor1,
1725             new_flavor=self.flavor1, source_rp_uuid=source_rp_uuid,
1726             dest_rp_uuid=dest_rp_uuid)
1727 
1728         # Mock failure
1729         def fake_confirm_migration(context, migration, instance, network_info):
1730             raise exception.MigrationPreCheckError(
1731                 reason='test_migration_confirm_resize_error')
1732 
1733         with mock.patch('nova.virt.fake.FakeDriver.'
1734                         'confirm_migration',
1735                         side_effect=fake_confirm_migration):
1736 
1737             # Confirm the migration/resize and check the usages
1738             post = {'confirmResize': None}
1739             self.api.post_server_action(
1740                 server['id'], post, check_response_status=[204])
1741             server = self._wait_for_state_change(self.api, server, 'ERROR')
1742 
1743         # After confirming and error, we should have an allocation only on the
1744         # destination host
1745 
1746         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
1747         self.assertRequestMatchesUsage({'VCPU': 0,
1748                                         'MEMORY_MB': 0,
1749                                         'DISK_GB': 0}, source_rp_uuid)
1750         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
1751                                            dest_rp_uuid)
1752 
1753         self._run_periodics()
1754 
1755         # Check we're still accurate after running the periodics
1756 
1757         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
1758         self.assertRequestMatchesUsage({'VCPU': 0,
1759                                         'MEMORY_MB': 0,
1760                                         'DISK_GB': 0}, source_rp_uuid)
1761         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
1762                                            dest_rp_uuid)
1763 
1764         self._delete_and_check_allocations(server)
1765 
1766     def _test_resize_revert(self, dest_hostname):
1767         source_hostname = self._other_hostname(dest_hostname)
1768         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
1769         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
1770 
1771         server = self._boot_and_check_allocations(self.flavor1,
1772             source_hostname)
1773 
1774         self._resize_and_check_allocations(server, self.flavor1, self.flavor2,
1775             source_rp_uuid, dest_rp_uuid)
1776 
1777         # Revert the resize and check the usages
1778         post = {'revertResize': None}
1779         self.api.post_server_action(server['id'], post)
1780         self._wait_for_state_change(self.api, server, 'ACTIVE')
1781 
1782         # Make sure the RequestSpec.flavor matches the original flavor.
1783         ctxt = context.get_admin_context()
1784         reqspec = objects.RequestSpec.get_by_instance_uuid(ctxt, server['id'])
1785         self.assertEqual(self.flavor1['id'], reqspec.flavor.flavorid)
1786 
1787         self._run_periodics()
1788 
1789         # the original host expected to have the old resource allocation
1790         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
1791 
1792         self.assertRequestMatchesUsage({'VCPU': 0,
1793                                         'MEMORY_MB': 0,
1794                                         'DISK_GB': 0}, dest_rp_uuid)
1795 
1796         # Check that the server only allocates resource from the original host
1797         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
1798                                            source_rp_uuid)
1799 
1800         self._delete_and_check_allocations(server)
1801 
1802     def _test_resize_confirm(self, dest_hostname):
1803         source_hostname = self._other_hostname(dest_hostname)
1804         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
1805         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
1806 
1807         server = self._boot_and_check_allocations(self.flavor1,
1808             source_hostname)
1809 
1810         self._resize_and_check_allocations(server, self.flavor1, self.flavor2,
1811             source_rp_uuid, dest_rp_uuid)
1812 
1813         # Confirm the resize and check the usages
1814         self._confirm_resize(server)
1815 
1816         # After confirming, we should have an allocation only on the
1817         # destination host
1818 
1819         # The target host usage should be according to the new flavor
1820         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor2)
1821         self.assertRequestMatchesUsage({'VCPU': 0,
1822                                         'MEMORY_MB': 0,
1823                                         'DISK_GB': 0}, source_rp_uuid)
1824 
1825         # and the target host allocation should be according to the new flavor
1826         self.assertFlavorMatchesAllocation(self.flavor2, server['id'],
1827                                            dest_rp_uuid)
1828 
1829         self._run_periodics()
1830 
1831         # Check we're still accurate after running the periodics
1832 
1833         # and the target host usage should be according to the new flavor
1834         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor2)
1835         self.assertRequestMatchesUsage({'VCPU': 0,
1836                                         'MEMORY_MB': 0,
1837                                         'DISK_GB': 0}, source_rp_uuid)
1838 
1839         # and the server allocates only from the target host
1840         self.assertFlavorMatchesAllocation(self.flavor2, server['id'],
1841                                            dest_rp_uuid)
1842 
1843         self._delete_and_check_allocations(server)
1844 
1845     def test_resize_revert_same_host(self):
1846         # make sure that the test only uses a single host
1847         compute2_service_id = self.admin_api.get_services(
1848             host=self.compute2.host, binary='nova-compute')[0]['id']
1849         self.admin_api.put_service(compute2_service_id, {'status': 'disabled'})
1850 
1851         hostname = self.compute1.manager.host
1852         rp_uuid = self._get_provider_uuid_by_host(hostname)
1853 
1854         server = self._boot_and_check_allocations(self.flavor2, hostname)
1855 
1856         self._resize_to_same_host_and_check_allocations(
1857             server, self.flavor2, self.flavor3, rp_uuid)
1858 
1859         # Revert the resize and check the usages
1860         post = {'revertResize': None}
1861         self.api.post_server_action(server['id'], post)
1862         self._wait_for_state_change(self.api, server, 'ACTIVE')
1863 
1864         self._run_periodics()
1865 
1866         # after revert only allocations due to the old flavor should remain
1867         self.assertFlavorMatchesUsage(rp_uuid, self.flavor2)
1868 
1869         self.assertFlavorMatchesAllocation(self.flavor2, server['id'],
1870                                            rp_uuid)
1871 
1872         self._delete_and_check_allocations(server)
1873 
1874     def test_resize_confirm_same_host(self):
1875         # make sure that the test only uses a single host
1876         compute2_service_id = self.admin_api.get_services(
1877             host=self.compute2.host, binary='nova-compute')[0]['id']
1878         self.admin_api.put_service(compute2_service_id, {'status': 'disabled'})
1879 
1880         hostname = self.compute1.manager.host
1881         rp_uuid = self._get_provider_uuid_by_host(hostname)
1882 
1883         server = self._boot_and_check_allocations(self.flavor2, hostname)
1884 
1885         self._resize_to_same_host_and_check_allocations(
1886             server, self.flavor2, self.flavor3, rp_uuid)
1887 
1888         # Confirm the resize and check the usages
1889         self._confirm_resize(server)
1890 
1891         self._run_periodics()
1892 
1893         # after confirm only allocations due to the new flavor should remain
1894         self.assertFlavorMatchesUsage(rp_uuid, self.flavor3)
1895 
1896         self.assertFlavorMatchesAllocation(self.flavor3, server['id'],
1897                                            rp_uuid)
1898 
1899         self._delete_and_check_allocations(server)
1900 
1901     def test_resize_not_enough_resource(self):
1902         # Try to resize to a flavor that requests more VCPU than what the
1903         # compute hosts has available and expect the resize to fail
1904 
1905         flavor_body = {'flavor':
1906                            {'name': 'test_too_big_flavor',
1907                             'ram': 1024,
1908                             'vcpus': fake.MediumFakeDriver.vcpus + 1,
1909                             'disk': 20,
1910                             }}
1911 
1912         big_flavor = self.api.post_flavor(flavor_body)
1913 
1914         dest_hostname = self.compute2.host
1915         source_hostname = self._other_hostname(dest_hostname)
1916         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
1917         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
1918 
1919         server = self._boot_and_check_allocations(
1920             self.flavor1, source_hostname)
1921 
1922         self.flags(allow_resize_to_same_host=False)
1923         resize_req = {
1924             'resize': {
1925                 'flavorRef': big_flavor['id']
1926             }
1927         }
1928 
1929         resp = self.api.post_server_action(
1930             server['id'], resize_req, check_response_status=[400])
1931         self.assertEqual(
1932             resp['badRequest']['message'],
1933             "No valid host was found. No valid host found for resize")
1934         server = self.admin_api.get_server(server['id'])
1935         self.assertEqual(source_hostname, server['OS-EXT-SRV-ATTR:host'])
1936 
1937         # only the source host shall have usages after the failed resize
1938         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
1939 
1940         # Check that the other provider has no usage
1941         self.assertRequestMatchesUsage(
1942             {'VCPU': 0,
1943              'MEMORY_MB': 0,
1944              'DISK_GB': 0}, dest_rp_uuid)
1945 
1946         # Check that the server only allocates resource from the host it is
1947         # booted on
1948         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
1949                                            source_rp_uuid)
1950 
1951         self._delete_and_check_allocations(server)
1952 
1953     def test_resize_delete_while_verify(self):
1954         """Test scenario where the server is deleted while in the
1955         VERIFY_RESIZE state and ensures the allocations are properly
1956         cleaned up from the source and target compute node resource providers.
1957         The _confirm_resize_on_deleting() method in the API is actually
1958         responsible for making sure the migration-based allocations get
1959         cleaned up by confirming the resize on the source host before deleting
1960         the server from the target host.
1961         """
1962         dest_hostname = 'host2'
1963         source_hostname = self._other_hostname(dest_hostname)
1964         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
1965         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
1966 
1967         server = self._boot_and_check_allocations(self.flavor1,
1968                                                   source_hostname)
1969 
1970         self._resize_and_check_allocations(server, self.flavor1, self.flavor2,
1971                                            source_rp_uuid, dest_rp_uuid)
1972 
1973         self._delete_and_check_allocations(server)
1974 
1975     def test_resize_confirm_assert_hypervisor_usage_no_periodics(self):
1976         """Resize confirm test for bug 1818914 to make sure the tracked
1977         resource usage in the os-hypervisors API (not placement) is as
1978         expected during a confirmed resize. This intentionally does not
1979         use _test_resize_confirm in order to avoid running periodics.
1980         """
1981         # There should be no usage from a server on either hypervisor.
1982         source_rp_uuid = self._get_provider_uuid_by_host('host1')
1983         dest_rp_uuid = self._get_provider_uuid_by_host('host2')
1984         no_usage = {'vcpus': 0, 'disk': 0, 'ram': 0}
1985         for rp_uuid in (source_rp_uuid, dest_rp_uuid):
1986             self.assert_hypervisor_usage(
1987                 rp_uuid, no_usage, volume_backed=False)
1988 
1989         # Create the server and wait for it to be ACTIVE.
1990         server = self._boot_and_check_allocations(self.flavor1, 'host1')
1991 
1992         # There should be resource usage for flavor1 on the source host.
1993         self.assert_hypervisor_usage(
1994             source_rp_uuid, self.flavor1, volume_backed=False)
1995         # And still no usage on the dest host.
1996         self.assert_hypervisor_usage(
1997             dest_rp_uuid, no_usage, volume_backed=False)
1998 
1999         # Resize the server to flavor2 and wait for VERIFY_RESIZE.
2000         self.flags(allow_resize_to_same_host=False)
2001         resize_req = {
2002             'resize': {
2003                 'flavorRef': self.flavor2['id']
2004             }
2005         }
2006         self.api.post_server_action(server['id'], resize_req)
2007         self._wait_for_state_change(self.api, server, 'VERIFY_RESIZE')
2008 
2009         # There should be resource usage for flavor1 on the source host.
2010         self.assert_hypervisor_usage(
2011             source_rp_uuid, self.flavor1, volume_backed=False)
2012         # And resource usage for flavor2 on the target host.
2013         self.assert_hypervisor_usage(
2014             dest_rp_uuid, self.flavor2, volume_backed=False)
2015 
2016         # Now confirm the resize and check hypervisor usage again.
2017         self._confirm_resize(server)
2018 
2019         # There should no resource usage for flavor1 on the source host.
2020         self.assert_hypervisor_usage(
2021             source_rp_uuid, no_usage, volume_backed=False)
2022         # And resource usage for flavor2 should still be on the target host.
2023         self.assert_hypervisor_usage(
2024             dest_rp_uuid, self.flavor2, volume_backed=False)
2025 
2026         # Run periodics and make sure usage is still as expected.
2027         self._run_periodics()
2028         self.assert_hypervisor_usage(
2029             source_rp_uuid, no_usage, volume_backed=False)
2030         self.assert_hypervisor_usage(
2031             dest_rp_uuid, self.flavor2, volume_backed=False)
2032 
2033     def _wait_for_notification_event_type(self, event_type, max_retries=50):
2034         retry_counter = 0
2035         while True:
2036             if len(fake_notifier.NOTIFICATIONS) > 0:
2037                 for notification in fake_notifier.NOTIFICATIONS:
2038                     if notification.event_type == event_type:
2039                         return
2040             if retry_counter == max_retries:
2041                 self.fail('Wait for notification event type (%s) failed'
2042                           % event_type)
2043             retry_counter += 1
2044             time.sleep(0.1)
2045 
2046     def test_evacuate_with_no_compute(self):
2047         source_hostname = self.compute1.host
2048         dest_hostname = self.compute2.host
2049         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2050         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
2051 
2052         # Disable compute service on destination host
2053         compute2_service_id = self.admin_api.get_services(
2054             host=dest_hostname, binary='nova-compute')[0]['id']
2055         self.admin_api.put_service(compute2_service_id, {'status': 'disabled'})
2056 
2057         server = self._boot_and_check_allocations(
2058             self.flavor1, source_hostname)
2059 
2060         # Force source compute down
2061         source_compute_id = self.admin_api.get_services(
2062             host=source_hostname, binary='nova-compute')[0]['id']
2063         self.compute1.stop()
2064         self.admin_api.put_service(
2065             source_compute_id, {'forced_down': 'true'})
2066 
2067         # Initialize fake_notifier
2068         fake_notifier.stub_notifier(self)
2069         fake_notifier.reset()
2070 
2071         # Initiate evacuation
2072         post = {'evacuate': {}}
2073         self.api.post_server_action(server['id'], post)
2074 
2075         # NOTE(elod.illes): Should be changed to non-polling solution when
2076         # patch https://review.opendev.org/#/c/482629/ gets merged:
2077         # fake_notifier.wait_for_versioned_notifications(
2078         #     'compute_task.rebuild_server')
2079         self._wait_for_notification_event_type('compute_task.rebuild_server')
2080 
2081         self._run_periodics()
2082 
2083         # There is no other host to evacuate to so the rebuild should put the
2084         # VM to ERROR state, but it should remain on source compute
2085         expected_params = {'OS-EXT-SRV-ATTR:host': source_hostname,
2086                            'status': 'ERROR'}
2087         server = self._wait_for_server_parameter(self.api, server,
2088                                                  expected_params)
2089 
2090         # Check migrations
2091         migrations = self.api.get_migrations()
2092         self.assertEqual(1, len(migrations))
2093         self.assertEqual('evacuation', migrations[0]['migration_type'])
2094         self.assertEqual(server['id'], migrations[0]['instance_uuid'])
2095         self.assertEqual(source_hostname, migrations[0]['source_compute'])
2096         self.assertEqual('error', migrations[0]['status'])
2097 
2098         # Restart source host
2099         self.admin_api.put_service(
2100             source_compute_id, {'forced_down': 'false'})
2101         self.compute1.start()
2102 
2103         self._run_periodics()
2104 
2105         # Check allocation and usages: should only use resources on source host
2106         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
2107 
2108         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
2109                                            source_rp_uuid)
2110         zero_usage = {'VCPU': 0, 'DISK_GB': 0, 'MEMORY_MB': 0}
2111         self.assertRequestMatchesUsage(zero_usage, dest_rp_uuid)
2112 
2113         self._delete_and_check_allocations(server)
2114 
2115     def test_migrate_no_valid_host(self):
2116         source_hostname = self.compute1.host
2117         dest_hostname = self.compute2.host
2118         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2119         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
2120 
2121         server = self._boot_and_check_allocations(
2122             self.flavor1, source_hostname)
2123 
2124         dest_compute_id = self.admin_api.get_services(
2125             host=dest_hostname, binary='nova-compute')[0]['id']
2126         self.compute2.stop()
2127         # force it down to avoid waiting for the service group to time out
2128         self.admin_api.put_service(
2129             dest_compute_id, {'forced_down': 'true'})
2130 
2131         # migrate the server
2132         post = {'migrate': None}
2133         ex = self.assertRaises(client.OpenStackApiException,
2134                                self.api.post_server_action,
2135                                server['id'], post)
2136         self.assertIn('No valid host', six.text_type(ex))
2137         expected_params = {'OS-EXT-SRV-ATTR:host': source_hostname,
2138                            'status': 'ACTIVE'}
2139         self._wait_for_server_parameter(self.api, server, expected_params)
2140 
2141         self._run_periodics()
2142 
2143         # Expect to have allocation only on source_host
2144         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
2145         zero_usage = {'VCPU': 0, 'DISK_GB': 0, 'MEMORY_MB': 0}
2146         self.assertRequestMatchesUsage(zero_usage, dest_rp_uuid)
2147 
2148         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
2149                                            source_rp_uuid)
2150 
2151         self._delete_and_check_allocations(server)
2152 
2153     def _test_evacuate(self, keep_hypervisor_state):
2154         source_hostname = self.compute1.host
2155         dest_hostname = self.compute2.host
2156         server = self._boot_and_check_allocations(
2157             self.flavor1, source_hostname)
2158 
2159         source_compute_id = self.admin_api.get_services(
2160             host=source_hostname, binary='nova-compute')[0]['id']
2161 
2162         self.compute1.stop()
2163         # force it down to avoid waiting for the service group to time out
2164         self.admin_api.put_service(
2165             source_compute_id, {'forced_down': 'true'})
2166 
2167         # evacuate the server
2168         post = {'evacuate': {}}
2169         self.api.post_server_action(
2170             server['id'], post)
2171         expected_params = {'OS-EXT-SRV-ATTR:host': dest_hostname,
2172                            'status': 'ACTIVE'}
2173         server = self._wait_for_server_parameter(self.api, server,
2174                                                  expected_params)
2175 
2176         # Expect to have allocation and usages on both computes as the
2177         # source compute is still down
2178         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2179         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
2180 
2181         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
2182 
2183         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
2184 
2185         self._check_allocation_during_evacuate(
2186             self.flavor1, server['id'], source_rp_uuid, dest_rp_uuid)
2187 
2188         # restart the source compute
2189         self.compute1 = self.restart_compute_service(
2190             self.compute1, keep_hypervisor_state=keep_hypervisor_state)
2191 
2192         self.admin_api.put_service(
2193             source_compute_id, {'forced_down': 'false'})
2194 
2195         source_usages = self._get_provider_usages(source_rp_uuid)
2196         self.assertEqual({'VCPU': 0,
2197                           'MEMORY_MB': 0,
2198                           'DISK_GB': 0},
2199                          source_usages)
2200 
2201         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
2202 
2203         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
2204                                            dest_rp_uuid)
2205 
2206         self._delete_and_check_allocations(server)
2207 
2208     def test_evacuate_instance_kept_on_the_hypervisor(self):
2209         self._test_evacuate(keep_hypervisor_state=True)
2210 
2211     def test_evacuate_clean_hypervisor(self):
2212         self._test_evacuate(keep_hypervisor_state=False)
2213 
2214     def _test_evacuate_forced_host(self, keep_hypervisor_state):
2215         """Evacuating a server with a forced host bypasses the scheduler
2216         which means conductor has to create the allocations against the
2217         destination node. This test recreates the scenarios and asserts
2218         the allocations on the source and destination nodes are as expected.
2219         """
2220         source_hostname = self.compute1.host
2221         dest_hostname = self.compute2.host
2222 
2223         # the ability to force evacuate a server is removed entirely in 2.68
2224         self.api.microversion = '2.67'
2225 
2226         server = self._boot_and_check_allocations(
2227             self.flavor1, source_hostname)
2228 
2229         source_compute_id = self.admin_api.get_services(
2230             host=source_hostname, binary='nova-compute')[0]['id']
2231 
2232         self.compute1.stop()
2233         # force it down to avoid waiting for the service group to time out
2234         self.admin_api.put_service(
2235             source_compute_id, {'forced_down': 'true'})
2236 
2237         # evacuate the server and force the destination host which bypasses
2238         # the scheduler
2239         post = {
2240             'evacuate': {
2241                 'host': dest_hostname,
2242                 'force': True
2243             }
2244         }
2245         self.api.post_server_action(server['id'], post)
2246         expected_params = {'OS-EXT-SRV-ATTR:host': dest_hostname,
2247                            'status': 'ACTIVE'}
2248         server = self._wait_for_server_parameter(self.api, server,
2249                                                  expected_params)
2250 
2251         # Run the periodics to show those don't modify allocations.
2252         self._run_periodics()
2253 
2254         # Expect to have allocation and usages on both computes as the
2255         # source compute is still down
2256         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2257         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
2258 
2259         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
2260 
2261         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
2262 
2263         self._check_allocation_during_evacuate(
2264             self.flavor1, server['id'], source_rp_uuid, dest_rp_uuid)
2265 
2266         # restart the source compute
2267         self.compute1 = self.restart_compute_service(
2268             self.compute1, keep_hypervisor_state=keep_hypervisor_state)
2269         self.admin_api.put_service(
2270             source_compute_id, {'forced_down': 'false'})
2271 
2272         # Run the periodics again to show they don't change anything.
2273         self._run_periodics()
2274 
2275         # When the source node starts up, the instance has moved so the
2276         # ResourceTracker should cleanup allocations for the source node.
2277         source_usages = self._get_provider_usages(source_rp_uuid)
2278         self.assertEqual(
2279             {'VCPU': 0, 'MEMORY_MB': 0, 'DISK_GB': 0}, source_usages)
2280 
2281         # The usages/allocations should still exist on the destination node
2282         # after the source node starts back up.
2283         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
2284 
2285         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
2286                                            dest_rp_uuid)
2287 
2288         self._delete_and_check_allocations(server)
2289 
2290     def test_evacuate_forced_host_instance_kept_on_the_hypervisor(self):
2291         self._test_evacuate_forced_host(keep_hypervisor_state=True)
2292 
2293     def test_evacuate_forced_host_clean_hypervisor(self):
2294         self._test_evacuate_forced_host(keep_hypervisor_state=False)
2295 
2296     def test_evacuate_forced_host_v268(self):
2297         """Evacuating a server with a forced host was removed in API
2298         microversion 2.68. This test ensures that the request is rejected.
2299         """
2300         source_hostname = self.compute1.host
2301         dest_hostname = self.compute2.host
2302 
2303         server = self._boot_and_check_allocations(
2304             self.flavor1, source_hostname)
2305 
2306         # evacuate the server and force the destination host which bypasses
2307         # the scheduler
2308         post = {
2309             'evacuate': {
2310                 'host': dest_hostname,
2311                 'force': True
2312             }
2313         }
2314         ex = self.assertRaises(client.OpenStackApiException,
2315                                self.api.post_server_action,
2316                                server['id'], post)
2317         self.assertIn("'force' was unexpected", six.text_type(ex))
2318 
2319     # NOTE(gibi): there is a similar test in SchedulerOnlyChecksTargetTest but
2320     # we want this test here as well because ServerMovingTest is a parent class
2321     # of multiple test classes that run this test case with different compute
2322     # node setups.
2323     def test_evacuate_host_specified_but_not_forced(self):
2324         """Evacuating a server with a host but using the scheduler to create
2325         the allocations against the destination node. This test recreates the
2326         scenarios and asserts the allocations on the source and destination
2327         nodes are as expected.
2328         """
2329         source_hostname = self.compute1.host
2330         dest_hostname = self.compute2.host
2331 
2332         server = self._boot_and_check_allocations(
2333             self.flavor1, source_hostname)
2334 
2335         source_compute_id = self.admin_api.get_services(
2336             host=source_hostname, binary='nova-compute')[0]['id']
2337 
2338         self.compute1.stop()
2339         # force it down to avoid waiting for the service group to time out
2340         self.admin_api.put_service(
2341             source_compute_id, {'forced_down': 'true'})
2342 
2343         # evacuate the server specify the target but do not force the
2344         # destination host to use the scheduler to validate the target host
2345         post = {
2346             'evacuate': {
2347                 'host': dest_hostname,
2348             }
2349         }
2350         self.api.post_server_action(server['id'], post)
2351         expected_params = {'OS-EXT-SRV-ATTR:host': dest_hostname,
2352                            'status': 'ACTIVE'}
2353         server = self._wait_for_server_parameter(self.api, server,
2354                                                  expected_params)
2355 
2356         # Run the periodics to show those don't modify allocations.
2357         self._run_periodics()
2358 
2359         # Expect to have allocation and usages on both computes as the
2360         # source compute is still down
2361         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2362         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
2363 
2364         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
2365 
2366         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
2367 
2368         self._check_allocation_during_evacuate(
2369             self.flavor1, server['id'], source_rp_uuid, dest_rp_uuid)
2370 
2371         # restart the source compute
2372         self.compute1 = self.restart_compute_service(self.compute1)
2373         self.admin_api.put_service(
2374             source_compute_id, {'forced_down': 'false'})
2375 
2376         # Run the periodics again to show they don't change anything.
2377         self._run_periodics()
2378 
2379         # When the source node starts up, the instance has moved so the
2380         # ResourceTracker should cleanup allocations for the source node.
2381         source_usages = self._get_provider_usages(source_rp_uuid)
2382         self.assertEqual(
2383             {'VCPU': 0, 'MEMORY_MB': 0, 'DISK_GB': 0}, source_usages)
2384 
2385         # The usages/allocations should still exist on the destination node
2386         # after the source node starts back up.
2387         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
2388 
2389         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
2390                                            dest_rp_uuid)
2391 
2392         self._delete_and_check_allocations(server)
2393 
2394     def _test_evacuate_claim_on_dest_fails(self, keep_hypervisor_state):
2395         """Tests that the allocations on the destination node are cleaned up
2396         when the rebuild move claim fails due to insufficient resources.
2397         """
2398         source_hostname = self.compute1.host
2399         dest_hostname = self.compute2.host
2400         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
2401 
2402         server = self._boot_and_check_allocations(
2403             self.flavor1, source_hostname)
2404 
2405         source_compute_id = self.admin_api.get_services(
2406             host=source_hostname, binary='nova-compute')[0]['id']
2407 
2408         self.compute1.stop()
2409         # force it down to avoid waiting for the service group to time out
2410         self.admin_api.put_service(
2411             source_compute_id, {'forced_down': 'true'})
2412 
2413         # NOTE(mriedem): This isn't great, and I'd like to fake out the driver
2414         # to make the claim fail, by doing something like returning a too high
2415         # memory_mb overhead, but the limits dict passed to the claim is empty
2416         # so the claim test is considering it as unlimited and never actually
2417         # performs a claim test.
2418         def fake_move_claim(*args, **kwargs):
2419             # Assert the destination node allocation exists.
2420             self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
2421             raise exception.ComputeResourcesUnavailable(
2422                     reason='test_evacuate_claim_on_dest_fails')
2423 
2424         with mock.patch('nova.compute.claims.MoveClaim', fake_move_claim):
2425             # evacuate the server
2426             self.api.post_server_action(server['id'], {'evacuate': {}})
2427             # the migration will fail on the dest node and the instance will
2428             # go into error state
2429             server = self._wait_for_state_change(self.api, server, 'ERROR')
2430 
2431         # Run the periodics to show those don't modify allocations.
2432         self._run_periodics()
2433 
2434         # The allocation should still exist on the source node since it's
2435         # still down, and the allocation on the destination node should be
2436         # cleaned up.
2437         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2438 
2439         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
2440 
2441         self.assertRequestMatchesUsage(
2442             {'VCPU': 0,
2443              'MEMORY_MB': 0,
2444              'DISK_GB': 0}, dest_rp_uuid)
2445 
2446         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
2447                                            source_rp_uuid)
2448 
2449         # restart the source compute
2450         self.compute1 = self.restart_compute_service(
2451             self.compute1, keep_hypervisor_state=keep_hypervisor_state)
2452         self.admin_api.put_service(
2453             source_compute_id, {'forced_down': 'false'})
2454 
2455         # Run the periodics again to show they don't change anything.
2456         self._run_periodics()
2457 
2458         # The source compute shouldn't have cleaned up the allocation for
2459         # itself since the instance didn't move.
2460         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
2461 
2462         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
2463                                            source_rp_uuid)
2464 
2465     def test_evacuate_claim_on_dest_fails_instance_kept_on_the_hypervisor(
2466             self):
2467         self._test_evacuate_claim_on_dest_fails(keep_hypervisor_state=True)
2468 
2469     def test_evacuate_claim_on_dest_fails_clean_hypervisor(self):
2470         self._test_evacuate_claim_on_dest_fails(keep_hypervisor_state=False)
2471 
2472     def _test_evacuate_rebuild_on_dest_fails(self, keep_hypervisor_state):
2473         """Tests that the allocations on the destination node are cleaned up
2474         automatically when the claim is made but the actual rebuild
2475         via the driver fails.
2476 
2477         """
2478         source_hostname = self.compute1.host
2479         dest_hostname = self.compute2.host
2480         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
2481 
2482         server = self._boot_and_check_allocations(
2483             self.flavor1, source_hostname)
2484 
2485         source_compute_id = self.admin_api.get_services(
2486             host=source_hostname, binary='nova-compute')[0]['id']
2487 
2488         self.compute1.stop()
2489         # force it down to avoid waiting for the service group to time out
2490         self.admin_api.put_service(
2491             source_compute_id, {'forced_down': 'true'})
2492 
2493         def fake_rebuild(*args, **kwargs):
2494             # Assert the destination node allocation exists.
2495             self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
2496             raise test.TestingException('test_evacuate_rebuild_on_dest_fails')
2497 
2498         with mock.patch.object(
2499                 self.compute2.driver, 'rebuild', fake_rebuild):
2500             # evacuate the server
2501             self.api.post_server_action(server['id'], {'evacuate': {}})
2502             # the migration will fail on the dest node and the instance will
2503             # go into error state
2504             server = self._wait_for_state_change(self.api, server, 'ERROR')
2505 
2506         # Run the periodics to show those don't modify allocations.
2507         self._run_periodics()
2508 
2509         # The allocation should still exist on the source node since it's
2510         # still down, and the allocation on the destination node should be
2511         # cleaned up.
2512         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2513 
2514         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
2515 
2516         self.assertRequestMatchesUsage(
2517             {'VCPU': 0,
2518              'MEMORY_MB': 0,
2519              'DISK_GB': 0}, dest_rp_uuid)
2520 
2521         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
2522                                            source_rp_uuid)
2523 
2524         # restart the source compute
2525         self.compute1 = self.restart_compute_service(
2526             self.compute1, keep_hypervisor_state=keep_hypervisor_state)
2527         self.admin_api.put_service(
2528             source_compute_id, {'forced_down': 'false'})
2529 
2530         # Run the periodics again to show they don't change anything.
2531         self._run_periodics()
2532 
2533         # The source compute shouldn't have cleaned up the allocation for
2534         # itself since the instance didn't move.
2535         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
2536 
2537         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
2538                                            source_rp_uuid)
2539 
2540     def test_evacuate_rebuild_on_dest_fails_instance_kept_on_the_hypervisor(
2541             self):
2542         self._test_evacuate_rebuild_on_dest_fails(keep_hypervisor_state=True)
2543 
2544     def test_evacuate_rebuild_on_dest_fails_clean_hypervisor(self):
2545         self._test_evacuate_rebuild_on_dest_fails(keep_hypervisor_state=False)
2546 
2547     def _boot_then_shelve_and_check_allocations(self, hostname, rp_uuid):
2548         # avoid automatic shelve offloading
2549         self.flags(shelved_offload_time=-1)
2550         server = self._boot_and_check_allocations(
2551             self.flavor1, hostname)
2552         req = {
2553             'shelve': {}
2554         }
2555         self.api.post_server_action(server['id'], req)
2556         self._wait_for_state_change(self.api, server, 'SHELVED')
2557         # the host should maintain the existing allocation for this instance
2558         # while the instance is shelved
2559         self.assertFlavorMatchesUsage(rp_uuid, self.flavor1)
2560         # Check that the server only allocates resource from the host it is
2561         # booted on
2562         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
2563                                            rp_uuid)
2564         return server
2565 
2566     def test_shelve_unshelve(self):
2567         source_hostname = self.compute1.host
2568         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2569         server = self._boot_then_shelve_and_check_allocations(
2570             source_hostname, source_rp_uuid)
2571 
2572         req = {
2573             'unshelve': None
2574         }
2575         self.api.post_server_action(server['id'], req)
2576         self._wait_for_state_change(self.api, server, 'ACTIVE')
2577 
2578         # the host should have resource usage as the instance is ACTIVE
2579         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
2580 
2581         # Check that the server only allocates resource from the host it is
2582         # booted on
2583         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
2584                                            source_rp_uuid)
2585 
2586         self._delete_and_check_allocations(server)
2587 
2588     def _shelve_offload_and_check_allocations(self, server, source_rp_uuid):
2589         req = {
2590             'shelveOffload': {}
2591         }
2592         self.api.post_server_action(server['id'], req)
2593         self._wait_for_server_parameter(
2594             self.api, server, {'status': 'SHELVED_OFFLOADED',
2595                                'OS-EXT-SRV-ATTR:host': None,
2596                                'OS-EXT-AZ:availability_zone': ''})
2597         source_usages = self._get_provider_usages(source_rp_uuid)
2598         self.assertEqual({'VCPU': 0,
2599                           'MEMORY_MB': 0,
2600                           'DISK_GB': 0},
2601                          source_usages)
2602 
2603         allocations = self._get_allocations_by_server_uuid(server['id'])
2604         self.assertEqual(0, len(allocations))
2605 
2606     def test_shelve_offload_unshelve_diff_host(self):
2607         source_hostname = self.compute1.host
2608         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2609         server = self._boot_then_shelve_and_check_allocations(
2610             source_hostname, source_rp_uuid)
2611 
2612         self._shelve_offload_and_check_allocations(server, source_rp_uuid)
2613 
2614         # unshelve after shelve offload will do scheduling. this test case
2615         # wants to test the scenario when the scheduler select a different host
2616         # to ushelve the instance. So we disable the original host.
2617         source_service_id = self.admin_api.get_services(
2618             host=source_hostname, binary='nova-compute')[0]['id']
2619         self.admin_api.put_service(source_service_id, {'status': 'disabled'})
2620 
2621         req = {
2622             'unshelve': None
2623         }
2624         self.api.post_server_action(server['id'], req)
2625         server = self._wait_for_state_change(self.api, server, 'ACTIVE')
2626         # unshelving an offloaded instance will call the scheduler so the
2627         # instance might end up on a different host
2628         current_hostname = server['OS-EXT-SRV-ATTR:host']
2629         self.assertEqual(current_hostname, self._other_hostname(
2630             source_hostname))
2631 
2632         # the host running the instance should have resource usage
2633         current_rp_uuid = self._get_provider_uuid_by_host(current_hostname)
2634         self.assertFlavorMatchesUsage(current_rp_uuid, self.flavor1)
2635 
2636         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
2637                                            current_rp_uuid)
2638 
2639         self._delete_and_check_allocations(server)
2640 
2641     def test_shelve_offload_unshelve_same_host(self):
2642         source_hostname = self.compute1.host
2643         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2644         server = self._boot_then_shelve_and_check_allocations(
2645             source_hostname, source_rp_uuid)
2646 
2647         self._shelve_offload_and_check_allocations(server, source_rp_uuid)
2648 
2649         # unshelve after shelve offload will do scheduling. this test case
2650         # wants to test the scenario when the scheduler select the same host
2651         # to ushelve the instance. So we disable the other host.
2652         source_service_id = self.admin_api.get_services(
2653             host=self._other_hostname(source_hostname),
2654             binary='nova-compute')[0]['id']
2655         self.admin_api.put_service(source_service_id, {'status': 'disabled'})
2656 
2657         req = {
2658             'unshelve': None
2659         }
2660         self.api.post_server_action(server['id'], req)
2661         server = self._wait_for_state_change(self.api, server, 'ACTIVE')
2662         # unshelving an offloaded instance will call the scheduler so the
2663         # instance might end up on a different host
2664         current_hostname = server['OS-EXT-SRV-ATTR:host']
2665         self.assertEqual(current_hostname, source_hostname)
2666 
2667         # the host running the instance should have resource usage
2668         current_rp_uuid = self._get_provider_uuid_by_host(current_hostname)
2669         self.assertFlavorMatchesUsage(current_rp_uuid, self.flavor1)
2670 
2671         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
2672                                            current_rp_uuid)
2673 
2674         self._delete_and_check_allocations(server)
2675 
2676     def test_live_migrate_force(self):
2677         source_hostname = self.compute1.host
2678         dest_hostname = self.compute2.host
2679         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2680         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
2681 
2682         # the ability to force live migrate a server is removed entirely in
2683         # 2.68
2684         self.api.microversion = '2.67'
2685 
2686         server = self._boot_and_check_allocations(
2687             self.flavor1, source_hostname)
2688 
2689         # live migrate the server and force the destination host which bypasses
2690         # the scheduler
2691         post = {
2692             'os-migrateLive': {
2693                 'host': dest_hostname,
2694                 'block_migration': True,
2695                 'force': True,
2696             }
2697         }
2698 
2699         self.api.post_server_action(server['id'], post)
2700         self._wait_for_server_parameter(self.api, server,
2701             {'OS-EXT-SRV-ATTR:host': dest_hostname,
2702              'status': 'ACTIVE'})
2703 
2704         self._run_periodics()
2705 
2706         # NOTE(danms): There should be no usage for the source
2707         self.assertRequestMatchesUsage(
2708             {'VCPU': 0,
2709              'MEMORY_MB': 0,
2710              'DISK_GB': 0}, source_rp_uuid)
2711 
2712         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
2713 
2714         # the server has an allocation on only the dest node
2715         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
2716                                            dest_rp_uuid)
2717 
2718         self._delete_and_check_allocations(server)
2719 
2720     def test_live_migrate_forced_v268(self):
2721         """Live migrating a server with a forced host was removed in API
2722         microversion 2.68. This test ensures that the request is rejected.
2723         """
2724         source_hostname = self.compute1.host
2725         dest_hostname = self.compute2.host
2726 
2727         server = self._boot_and_check_allocations(
2728             self.flavor1, source_hostname)
2729 
2730         # live migrate the server and force the destination host which bypasses
2731         # the scheduler
2732         post = {
2733             'os-migrateLive': {
2734                 'host': dest_hostname,
2735                 'block_migration': True,
2736                 'force': True,
2737             }
2738         }
2739 
2740         ex = self.assertRaises(client.OpenStackApiException,
2741                                self.api.post_server_action,
2742                                server['id'], post)
2743         self.assertIn("'force' was unexpected", six.text_type(ex))
2744 
2745     def test_live_migrate(self):
2746         source_hostname = self.compute1.host
2747         dest_hostname = self.compute2.host
2748         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2749         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
2750 
2751         server = self._boot_and_check_allocations(
2752             self.flavor1, source_hostname)
2753         post = {
2754             'os-migrateLive': {
2755                 'host': dest_hostname,
2756                 'block_migration': True,
2757             }
2758         }
2759 
2760         self.api.post_server_action(server['id'], post)
2761         self._wait_for_server_parameter(self.api, server,
2762                                         {'OS-EXT-SRV-ATTR:host': dest_hostname,
2763                                          'status': 'ACTIVE'})
2764 
2765         self._run_periodics()
2766 
2767         # NOTE(danms): There should be no usage for the source
2768         self.assertRequestMatchesUsage(
2769             {'VCPU': 0,
2770              'MEMORY_MB': 0,
2771              'DISK_GB': 0}, source_rp_uuid)
2772 
2773         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
2774 
2775         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
2776                                            dest_rp_uuid)
2777 
2778         self._delete_and_check_allocations(server)
2779 
2780     def test_live_migrate_pre_check_fails(self):
2781         """Tests the case that the LiveMigrationTask in conductor has
2782         called the scheduler which picked a host and created allocations
2783         against it in Placement, but then when the conductor task calls
2784         check_can_live_migrate_destination on the destination compute it
2785         fails. The allocations on the destination compute node should be
2786         cleaned up before the conductor task asks the scheduler for another
2787         host to try the live migration.
2788         """
2789         self.failed_hostname = None
2790         source_hostname = self.compute1.host
2791         dest_hostname = self.compute2.host
2792         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2793         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
2794 
2795         server = self._boot_and_check_allocations(
2796             self.flavor1, source_hostname)
2797 
2798         def fake_check_can_live_migrate_destination(
2799                 context, instance, src_compute_info, dst_compute_info,
2800                 block_migration=False, disk_over_commit=False):
2801             self.failed_hostname = dst_compute_info['host']
2802             raise exception.MigrationPreCheckError(
2803                 reason='test_live_migrate_pre_check_fails')
2804 
2805         with mock.patch('nova.virt.fake.FakeDriver.'
2806                         'check_can_live_migrate_destination',
2807                         side_effect=fake_check_can_live_migrate_destination):
2808             post = {
2809                 'os-migrateLive': {
2810                     'host': dest_hostname,
2811                     'block_migration': True,
2812                 }
2813             }
2814             self.api.post_server_action(server['id'], post)
2815             # As there are only two computes and we failed to live migrate to
2816             # the only other destination host, the LiveMigrationTask raises
2817             # MaxRetriesExceeded back to the conductor manager which handles it
2818             # generically and sets the instance back to ACTIVE status and
2819             # clears the task_state. The migration record status is set to
2820             # 'error', so that's what we need to look for to know when this
2821             # is done.
2822             migration = self._wait_for_migration_status(server, ['error'])
2823 
2824         # The source_compute should be set on the migration record, but the
2825         # destination shouldn't be as we never made it to one.
2826         self.assertEqual(source_hostname, migration['source_compute'])
2827         self.assertIsNone(migration['dest_compute'])
2828         # Make sure the destination host (the only other host) is the failed
2829         # host.
2830         self.assertEqual(dest_hostname, self.failed_hostname)
2831 
2832         # Since the instance didn't move, assert the allocations are still
2833         # on the source node.
2834         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
2835 
2836         # Assert the allocations, created by the scheduler, are cleaned up
2837         # after the migration pre-check error happens.
2838         self.assertRequestMatchesUsage(
2839             {'VCPU': 0,
2840              'MEMORY_MB': 0,
2841              'DISK_GB': 0}, dest_rp_uuid)
2842 
2843         # There should only be 1 allocation for the instance on the source node
2844         self.assertFlavorMatchesAllocation(
2845             self.flavor1, server['id'], source_rp_uuid)
2846 
2847         self._delete_and_check_allocations(server)
2848 
2849     @mock.patch('nova.virt.fake.FakeDriver.pre_live_migration')
2850     def test_live_migrate_rollback_cleans_dest_node_allocations(
2851             self, mock_pre_live_migration, force=False):
2852         """Tests the case that when live migration fails, either during the
2853         call to pre_live_migration on the destination, or during the actual
2854         live migration in the virt driver, the allocations on the destination
2855         node are rolled back since the instance is still on the source node.
2856         """
2857         source_hostname = self.compute1.host
2858         dest_hostname = self.compute2.host
2859         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2860         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
2861 
2862         # the ability to force live migrate a server is removed entirely in
2863         # 2.68
2864         self.api.microversion = '2.67'
2865 
2866         server = self._boot_and_check_allocations(
2867             self.flavor1, source_hostname)
2868 
2869         def stub_pre_live_migration(context, instance, block_device_info,
2870                                     network_info, disk_info, migrate_data):
2871             # Make sure the source node allocations are against the migration
2872             # record and the dest node allocations are against the instance.
2873             self.assertFlavorMatchesAllocation(
2874                 self.flavor1, migrate_data.migration.uuid, source_rp_uuid)
2875 
2876             self.assertFlavorMatchesAllocation(
2877                 self.flavor1, server['id'], dest_rp_uuid)
2878             # The actual type of exception here doesn't matter. The point
2879             # is that the virt driver raised an exception from the
2880             # pre_live_migration method on the destination host.
2881             raise test.TestingException(
2882                 'test_live_migrate_rollback_cleans_dest_node_allocations')
2883 
2884         mock_pre_live_migration.side_effect = stub_pre_live_migration
2885 
2886         post = {
2887             'os-migrateLive': {
2888                 'host': dest_hostname,
2889                 'block_migration': True,
2890                 'force': force
2891             }
2892         }
2893         self.api.post_server_action(server['id'], post)
2894         # The compute manager will put the migration record into error status
2895         # when pre_live_migration fails, so wait for that to happen.
2896         migration = self._wait_for_migration_status(server, ['error'])
2897         # The _rollback_live_migration method in the compute manager will reset
2898         # the task_state on the instance, so wait for that to happen.
2899         server = self._wait_for_server_parameter(
2900             self.api, server, {'OS-EXT-STS:task_state': None})
2901 
2902         self.assertEqual(source_hostname, migration['source_compute'])
2903         self.assertEqual(dest_hostname, migration['dest_compute'])
2904 
2905         # Since the instance didn't move, assert the allocations are still
2906         # on the source node.
2907         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
2908 
2909         # Assert the allocations, created by the scheduler, are cleaned up
2910         # after the rollback happens.
2911         self.assertRequestMatchesUsage(
2912             {'VCPU': 0,
2913              'MEMORY_MB': 0,
2914              'DISK_GB': 0}, dest_rp_uuid)
2915 
2916         # There should only be 1 allocation for the instance on the source node
2917         self.assertFlavorMatchesAllocation(
2918             self.flavor1, server['id'], source_rp_uuid)
2919 
2920         self._delete_and_check_allocations(server)
2921 
2922     def test_live_migrate_rollback_cleans_dest_node_allocations_forced(self):
2923         """Tests the case that when a forced host live migration fails, either
2924         during the call to pre_live_migration on the destination, or during
2925         the actual live migration in the virt driver, the allocations on the
2926         destination node are rolled back since the instance is still on the
2927         source node.
2928         """
2929         self.test_live_migrate_rollback_cleans_dest_node_allocations(
2930             force=True)
2931 
2932     def test_rescheduling_when_migrating_instance(self):
2933         """Tests that allocations are removed from the destination node by
2934         the compute service when a cold migrate / resize fails and a reschedule
2935         request is sent back to conductor.
2936         """
2937         source_hostname = self.compute1.manager.host
2938         server = self._boot_and_check_allocations(
2939             self.flavor1, source_hostname)
2940 
2941         def fake_prep_resize(*args, **kwargs):
2942             dest_hostname = self._other_hostname(source_hostname)
2943             dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
2944             self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
2945             allocations = self._get_allocations_by_server_uuid(server['id'])
2946             self.assertIn(dest_rp_uuid, allocations)
2947 
2948             source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2949             self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
2950             migration_uuid = self.get_migration_uuid_for_instance(server['id'])
2951             allocations = self._get_allocations_by_server_uuid(migration_uuid)
2952             self.assertIn(source_rp_uuid, allocations)
2953 
2954             raise test.TestingException('Simulated _prep_resize failure.')
2955 
2956         # Yes this isn't great in a functional test, but it's simple.
2957         self.stub_out('nova.compute.manager.ComputeManager._prep_resize',
2958                       fake_prep_resize)
2959 
2960         # Now migrate the server which is going to fail on the destination.
2961         self.api.post_server_action(server['id'], {'migrate': None})
2962 
2963         self._wait_for_action_fail_completion(
2964             server, instance_actions.MIGRATE, 'compute_prep_resize')
2965 
2966         dest_hostname = self._other_hostname(source_hostname)
2967         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
2968 
2969         # Expects no allocation records on the failed host.
2970         self.assertRequestMatchesUsage(
2971             {'VCPU': 0,
2972              'MEMORY_MB': 0,
2973              'DISK_GB': 0}, dest_rp_uuid)
2974 
2975         # Ensure the allocation records still exist on the source host.
2976         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2977         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
2978         allocations = self._get_allocations_by_server_uuid(server['id'])
2979         self.assertIn(source_rp_uuid, allocations)
2980 
2981     def _test_resize_to_same_host_instance_fails(self, failing_method,
2982                                                  event_name):
2983         """Tests that when we resize to the same host and resize fails in
2984         the given method, we cleanup the allocations before rescheduling.
2985         """
2986         # make sure that the test only uses a single host
2987         compute2_service_id = self.admin_api.get_services(
2988             host=self.compute2.host, binary='nova-compute')[0]['id']
2989         self.admin_api.put_service(compute2_service_id, {'status': 'disabled'})
2990 
2991         hostname = self.compute1.manager.host
2992         rp_uuid = self._get_provider_uuid_by_host(hostname)
2993 
2994         server = self._boot_and_check_allocations(self.flavor1, hostname)
2995 
2996         def fake_resize_method(*args, **kwargs):
2997             # Ensure the allocations are doubled now before we fail.
2998             self.assertFlavorMatchesUsage(rp_uuid, self.flavor1, self.flavor2)
2999             raise test.TestingException('Simulated resize failure.')
3000 
3001         # Yes this isn't great in a functional test, but it's simple.
3002         self.stub_out(
3003             'nova.compute.manager.ComputeManager.%s' % failing_method,
3004             fake_resize_method)
3005 
3006         self.flags(allow_resize_to_same_host=True)
3007         resize_req = {
3008             'resize': {
3009                 'flavorRef': self.flavor2['id']
3010             }
3011         }
3012         self.api.post_server_action(server['id'], resize_req)
3013 
3014         self._wait_for_action_fail_completion(
3015             server, instance_actions.RESIZE, event_name)
3016 
3017         # Ensure the allocation records still exist on the host.
3018         source_rp_uuid = self._get_provider_uuid_by_host(hostname)
3019         if failing_method == '_finish_resize':
3020             # finish_resize will drop the old flavor allocations.
3021             self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor2)
3022         else:
3023             # The new_flavor should have been subtracted from the doubled
3024             # allocation which just leaves us with the original flavor.
3025             self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
3026 
3027     def test_resize_to_same_host_prep_resize_fails(self):
3028         self._test_resize_to_same_host_instance_fails(
3029             '_prep_resize', 'compute_prep_resize')
3030 
3031     def test_resize_instance_fails_allocation_cleanup(self):
3032         self._test_resize_to_same_host_instance_fails(
3033             '_resize_instance', 'compute_resize_instance')
3034 
3035     def test_finish_resize_fails_allocation_cleanup(self):
3036         self._test_resize_to_same_host_instance_fails(
3037             '_finish_resize', 'compute_finish_resize')
3038 
3039     def _server_created_with_host(self):
3040         hostname = self.compute1.host
3041         server_req = self._build_minimal_create_server_request(
3042             self.api, "some-server", flavor_id=self.flavor1["id"],
3043             image_uuid="155d900f-4e14-4e4c-a73d-069cbf4541e6",
3044             networks='none')
3045         server_req['host'] = hostname
3046 
3047         created_server = self.api.post_server({"server": server_req})
3048         server = self._wait_for_state_change(
3049             self.api, created_server, "ACTIVE")
3050         return server
3051 
3052     def test_live_migration_after_server_created_with_host(self):
3053         """Test after creating server with requested host, and then
3054         do live-migration for the server. The requested host will not
3055         effect the new moving operation.
3056         """
3057         dest_hostname = self.compute2.host
3058         created_server = self._server_created_with_host()
3059 
3060         post = {
3061             'os-migrateLive': {
3062                 'host': None,
3063                 'block_migration': 'auto'
3064             }
3065         }
3066         self.api.post_server_action(created_server['id'], post)
3067         new_server = self._wait_for_server_parameter(
3068             self.api, created_server, {'status': 'ACTIVE'})
3069         inst_dest_host = new_server["OS-EXT-SRV-ATTR:host"]
3070 
3071         self.assertEqual(dest_hostname, inst_dest_host)
3072 
3073     def test_evacuate_after_server_created_with_host(self):
3074         """Test after creating server with requested host, and then
3075         do evacuation for the server. The requested host will not
3076         effect the new moving operation.
3077         """
3078         dest_hostname = self.compute2.host
3079         created_server = self._server_created_with_host()
3080 
3081         source_compute_id = self.admin_api.get_services(
3082             host=created_server["OS-EXT-SRV-ATTR:host"],
3083             binary='nova-compute')[0]['id']
3084 
3085         self.compute1.stop()
3086         # force it down to avoid waiting for the service group to time out
3087         self.admin_api.put_service(
3088             source_compute_id, {'forced_down': 'true'})
3089 
3090         post = {
3091             'evacuate': {}
3092         }
3093         self.api.post_server_action(created_server['id'], post)
3094         expected_params = {'OS-EXT-SRV-ATTR:host': dest_hostname,
3095                            'status': 'ACTIVE'}
3096         new_server = self._wait_for_server_parameter(self.api, created_server,
3097                                                      expected_params)
3098         inst_dest_host = new_server["OS-EXT-SRV-ATTR:host"]
3099 
3100         self.assertEqual(dest_hostname, inst_dest_host)
3101 
3102     def test_resize_and_confirm_after_server_created_with_host(self):
3103         """Test after creating server with requested host, and then
3104         do resize for the server. The requested host will not
3105         effect the new moving operation.
3106         """
3107         dest_hostname = self.compute2.host
3108         created_server = self._server_created_with_host()
3109 
3110         # resize server
3111         self.flags(allow_resize_to_same_host=False)
3112         resize_req = {
3113             'resize': {
3114                 'flavorRef': self.flavor2['id']
3115             }
3116         }
3117         self.api.post_server_action(created_server['id'], resize_req)
3118         self._wait_for_state_change(self.api, created_server, 'VERIFY_RESIZE')
3119 
3120         # Confirm the resize
3121         new_server = self._confirm_resize(created_server)
3122         inst_dest_host = new_server["OS-EXT-SRV-ATTR:host"]
3123 
3124         self.assertEqual(dest_hostname, inst_dest_host)
3125 
3126     def test_shelve_unshelve_after_server_created_with_host(self):
3127         """Test after creating server with requested host, and then
3128         do shelve and unshelve for the server. The requested host
3129         will not effect the new moving operation.
3130         """
3131         dest_hostname = self.compute2.host
3132         created_server = self._server_created_with_host()
3133 
3134         self.flags(shelved_offload_time=-1)
3135         req = {'shelve': {}}
3136         self.api.post_server_action(created_server['id'], req)
3137         self._wait_for_state_change(self.api, created_server, 'SHELVED')
3138 
3139         req = {'shelveOffload': {}}
3140         self.api.post_server_action(created_server['id'], req)
3141         self._wait_for_server_parameter(
3142             self.api, created_server, {'status': 'SHELVED_OFFLOADED',
3143                                        'OS-EXT-SRV-ATTR:host': None,
3144                                        'OS-EXT-AZ:availability_zone': ''})
3145 
3146         # unshelve after shelve offload will do scheduling. this test case
3147         # wants to test the scenario when the scheduler select a different host
3148         # to ushelve the instance. So we disable the original host.
3149         source_service_id = self.admin_api.get_services(
3150             host=created_server["OS-EXT-SRV-ATTR:host"],
3151             binary='nova-compute')[0]['id']
3152         self.admin_api.put_service(source_service_id, {'status': 'disabled'})
3153 
3154         req = {'unshelve': None}
3155         self.api.post_server_action(created_server['id'], req)
3156         new_server = self._wait_for_state_change(
3157             self.api, created_server, 'ACTIVE')
3158         inst_dest_host = new_server["OS-EXT-SRV-ATTR:host"]
3159 
3160         self.assertEqual(dest_hostname, inst_dest_host)
3161 
3162     @mock.patch.object(utils, 'fill_provider_mapping',
3163                        wraps=utils.fill_provider_mapping)
3164     def _test_resize_reschedule_uses_host_lists(self, mock_fill_provider_map,
3165                                                 fails, num_alts=None):
3166         """Test that when a resize attempt fails, the retry comes from the
3167         supplied host_list, and does not call the scheduler.
3168         """
3169         server_req = self._build_minimal_create_server_request(
3170                 self.api, "some-server", flavor_id=self.flavor1["id"],
3171                 image_uuid="155d900f-4e14-4e4c-a73d-069cbf4541e6",
3172                 networks='none')
3173 
3174         created_server = self.api.post_server({"server": server_req})
3175         server = self._wait_for_state_change(self.api, created_server,
3176                 "ACTIVE")
3177         inst_host = server["OS-EXT-SRV-ATTR:host"]
3178         uuid_orig = self._get_provider_uuid_by_host(inst_host)
3179 
3180         # We will need four new compute nodes to test the resize, representing
3181         # the host selected by select_destinations(), along with 3 alternates.
3182         self._start_compute(host="selection")
3183         self._start_compute(host="alt_host1")
3184         self._start_compute(host="alt_host2")
3185         self._start_compute(host="alt_host3")
3186         uuid_sel = self._get_provider_uuid_by_host("selection")
3187         uuid_alt1 = self._get_provider_uuid_by_host("alt_host1")
3188         uuid_alt2 = self._get_provider_uuid_by_host("alt_host2")
3189         uuid_alt3 = self._get_provider_uuid_by_host("alt_host3")
3190         hosts = [{"name": "selection", "uuid": uuid_sel},
3191                  {"name": "alt_host1", "uuid": uuid_alt1},
3192                  {"name": "alt_host2", "uuid": uuid_alt2},
3193                  {"name": "alt_host3", "uuid": uuid_alt3},
3194                 ]
3195 
3196         self.flags(weight_classes=[__name__ + '.AltHostWeigher'],
3197                    group='filter_scheduler')
3198         self.scheduler_service.stop()
3199         self.scheduler_service = self.start_service('scheduler')
3200 
3201         def fake_prep_resize(*args, **kwargs):
3202             if self.num_fails < fails:
3203                 self.num_fails += 1
3204                 raise Exception("fake_prep_resize")
3205             actual_prep_resize(*args, **kwargs)
3206 
3207         # Yes this isn't great in a functional test, but it's simple.
3208         actual_prep_resize = compute_manager.ComputeManager._prep_resize
3209         self.stub_out("nova.compute.manager.ComputeManager._prep_resize",
3210                       fake_prep_resize)
3211         self.num_fails = 0
3212         num_alts = 4 if num_alts is None else num_alts
3213         # Make sure we have enough retries available for the number of
3214         # requested fails.
3215         attempts = min(fails + 2, num_alts)
3216         self.flags(max_attempts=attempts, group='scheduler')
3217         server_uuid = server["id"]
3218         data = {"resize": {"flavorRef": self.flavor2["id"]}}
3219         self.api.post_server_action(server_uuid, data)
3220 
3221         # fill_provider_mapping should have been called once for the initial
3222         # build, once for the resize scheduling to the primary host and then
3223         # once per reschedule.
3224         expected_fill_count = 2
3225         if num_alts > 1:
3226             expected_fill_count += self.num_fails - 1
3227         self.assertGreaterEqual(mock_fill_provider_map.call_count,
3228                                 expected_fill_count)
3229 
3230         if num_alts < fails:
3231             # We will run out of alternates before populate_retry will
3232             # raise a MaxRetriesExceeded exception, so the migration will
3233             # fail and the server should be in status "ERROR"
3234             server = self._wait_for_state_change(self.api, created_server,
3235                     "ERROR")
3236             # The usage should be unchanged from the original flavor
3237             self.assertFlavorMatchesUsage(uuid_orig, self.flavor1)
3238             # There should be no usages on any of the hosts
3239             target_uuids = (uuid_sel, uuid_alt1, uuid_alt2, uuid_alt3)
3240             empty_usage = {"VCPU": 0, "MEMORY_MB": 0, "DISK_GB": 0}
3241             for target_uuid in target_uuids:
3242                 usage = self._get_provider_usages(target_uuid)
3243                 self.assertEqual(empty_usage, usage)
3244         else:
3245             server = self._wait_for_state_change(self.api, created_server,
3246                     "VERIFY_RESIZE")
3247             # Verify that the selected host failed, and was rescheduled to
3248             # an alternate host.
3249             new_server_host = server.get("OS-EXT-SRV-ATTR:host")
3250             expected_host = hosts[fails]["name"]
3251             self.assertEqual(expected_host, new_server_host)
3252             uuid_dest = hosts[fails]["uuid"]
3253             # The usage should match the resized flavor
3254             self.assertFlavorMatchesUsage(uuid_dest, self.flavor2)
3255             # Verify that the other host have no allocations
3256             target_uuids = (uuid_sel, uuid_alt1, uuid_alt2, uuid_alt3)
3257             empty_usage = {"VCPU": 0, "MEMORY_MB": 0, "DISK_GB": 0}
3258             for target_uuid in target_uuids:
3259                 if target_uuid == uuid_dest:
3260                     continue
3261                 usage = self._get_provider_usages(target_uuid)
3262                 self.assertEqual(empty_usage, usage)
3263 
3264             # Verify that there is only one migration record for the instance.
3265             ctxt = context.get_admin_context()
3266             filters = {"instance_uuid": server["id"]}
3267             migrations = objects.MigrationList.get_by_filters(ctxt, filters)
3268             self.assertEqual(1, len(migrations.objects))
3269 
3270     def test_resize_reschedule_uses_host_lists_1_fail(self):
3271         self._test_resize_reschedule_uses_host_lists(fails=1)
3272 
3273     def test_resize_reschedule_uses_host_lists_3_fails(self):
3274         self._test_resize_reschedule_uses_host_lists(fails=3)
3275 
3276     def test_resize_reschedule_uses_host_lists_not_enough_alts(self):
3277         self._test_resize_reschedule_uses_host_lists(fails=3, num_alts=1)
3278 
3279     def test_migrate_confirm(self):
3280         source_hostname = self.compute1.host
3281         dest_hostname = self.compute2.host
3282         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
3283         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
3284 
3285         server = self._boot_and_check_allocations(
3286             self.flavor1, source_hostname)
3287 
3288         self._migrate_and_check_allocations(
3289             server, self.flavor1, source_rp_uuid, dest_rp_uuid)
3290 
3291         # Confirm the move and check the usages
3292         self._confirm_resize(server)
3293 
3294         def _check_allocation():
3295             # the target host usage should be according to the flavor
3296             self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
3297             # the source host has no usage
3298             self.assertRequestMatchesUsage({'VCPU': 0,
3299                                             'MEMORY_MB': 0,
3300                                             'DISK_GB': 0}, source_rp_uuid)
3301 
3302             # and the target host allocation should be according to the flavor
3303             self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
3304                                                dest_rp_uuid)
3305 
3306         # After confirming, we should have an allocation only on the
3307         # destination host
3308         _check_allocation()
3309         self._run_periodics()
3310 
3311         # Check we're still accurate after running the periodics
3312         _check_allocation()
3313 
3314         self._delete_and_check_allocations(server)
3315 
3316     def test_migrate_revert(self):
3317         source_hostname = self.compute1.host
3318         dest_hostname = self.compute2.host
3319         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
3320         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
3321 
3322         server = self._boot_and_check_allocations(
3323             self.flavor1, source_hostname)
3324 
3325         self._migrate_and_check_allocations(
3326             server, self.flavor1, source_rp_uuid, dest_rp_uuid)
3327 
3328         # Revert the move and check the usages
3329         post = {'revertResize': None}
3330         self.api.post_server_action(server['id'], post)
3331         self._wait_for_state_change(self.api, server, 'ACTIVE')
3332 
3333         def _check_allocation():
3334             self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
3335             self.assertRequestMatchesUsage({'VCPU': 0,
3336                                             'MEMORY_MB': 0,
3337                                             'DISK_GB': 0}, dest_rp_uuid)
3338 
3339             # Check that the server only allocates resource from the original
3340             # host
3341             self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
3342                                                source_rp_uuid)
3343 
3344         # the original host expected to have the old resource allocation
3345         _check_allocation()
3346         self._run_periodics()
3347         _check_allocation()
3348 
3349         self._delete_and_check_allocations(server)
3350 
3351 
3352 class ServerLiveMigrateForceAndAbort(
3353         integrated_helpers.ProviderUsageBaseTestCase):
3354     """Test Server live migrations, which delete the migration or
3355     force_complete it, and check the allocations after the operations.
3356 
3357     The test are using fakedriver to handle the force_completion and deletion
3358     of live migration.
3359     """
3360 
3361     compute_driver = 'fake.FakeLiveMigrateDriver'
3362 
3363     def setUp(self):
3364         super(ServerLiveMigrateForceAndAbort, self).setUp()
3365 
3366         self.compute1 = self._start_compute(host='host1')
3367         self.compute2 = self._start_compute(host='host2')
3368 
3369         flavors = self.api.get_flavors()
3370         self.flavor1 = flavors[0]
3371 
3372     def test_live_migrate_force_complete(self):
3373         source_hostname = self.compute1.host
3374         dest_hostname = self.compute2.host
3375         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
3376         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
3377 
3378         server = self._boot_and_check_allocations(
3379             self.flavor1, source_hostname)
3380 
3381         post = {
3382             'os-migrateLive': {
3383                 'host': dest_hostname,
3384                 'block_migration': True,
3385             }
3386         }
3387         self.api.post_server_action(server['id'], post)
3388 
3389         migration = self._wait_for_migration_status(server, ['running'])
3390         self.api.force_complete_migration(server['id'],
3391                                           migration['id'])
3392 
3393         self._wait_for_server_parameter(self.api, server,
3394                                         {'OS-EXT-SRV-ATTR:host': dest_hostname,
3395                                          'status': 'ACTIVE'})
3396 
3397         self._run_periodics()
3398 
3399         self.assertRequestMatchesUsage(
3400             {'VCPU': 0,
3401              'MEMORY_MB': 0,
3402              'DISK_GB': 0}, source_rp_uuid)
3403 
3404         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
3405         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
3406                                            dest_rp_uuid)
3407 
3408         self._delete_and_check_allocations(server)
3409 
3410     def test_live_migrate_delete(self):
3411         source_hostname = self.compute1.host
3412         dest_hostname = self.compute2.host
3413         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
3414         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
3415 
3416         server = self._boot_and_check_allocations(
3417             self.flavor1, source_hostname)
3418 
3419         post = {
3420             'os-migrateLive': {
3421                 'host': dest_hostname,
3422                 'block_migration': True,
3423             }
3424         }
3425         self.api.post_server_action(server['id'], post)
3426 
3427         migration = self._wait_for_migration_status(server, ['running'])
3428 
3429         self.api.delete_migration(server['id'], migration['id'])
3430         self._wait_for_server_parameter(self.api, server,
3431             {'OS-EXT-SRV-ATTR:host': source_hostname,
3432              'status': 'ACTIVE'})
3433 
3434         self._run_periodics()
3435 
3436         allocations = self._get_allocations_by_server_uuid(server['id'])
3437         self.assertNotIn(dest_rp_uuid, allocations)
3438 
3439         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
3440 
3441         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
3442                                            source_rp_uuid)
3443 
3444         self.assertRequestMatchesUsage({'VCPU': 0,
3445                                         'MEMORY_MB': 0,
3446                                         'DISK_GB': 0}, dest_rp_uuid)
3447 
3448         self._delete_and_check_allocations(server)
3449 
3450 
3451 class ServerLiveMigrateForceAndAbortWithNestedResourcesRequest(
3452         ServerLiveMigrateForceAndAbort):
3453     compute_driver = 'fake.FakeLiveMigrateDriverWithNestedCustomResources'
3454 
3455     def setUp(self):
3456         super(ServerLiveMigrateForceAndAbortWithNestedResourcesRequest,
3457               self).setUp()
3458         # modify the flavor used in the test base class to require one piece of
3459         # CUSTOM_MAGIC resource as well.
3460 
3461         self.api.post_extra_spec(
3462             self.flavor1['id'], {'extra_specs': {'resources:CUSTOM_MAGIC': 1}})
3463         # save the extra_specs in the flavor stored in the test case as
3464         # well
3465         self.flavor1['extra_specs'] = {'resources:CUSTOM_MAGIC': 1}
3466 
3467 
3468 class ServerRescheduleTests(integrated_helpers.ProviderUsageBaseTestCase):
3469     """Tests server create scenarios which trigger a reschedule during
3470     a server build and validates that allocations in Placement
3471     are properly cleaned up.
3472 
3473     Uses a fake virt driver that fails the build on the first attempt.
3474     """
3475 
3476     compute_driver = 'fake.FakeRescheduleDriver'
3477 
3478     def setUp(self):
3479         super(ServerRescheduleTests, self).setUp()
3480         self.compute1 = self._start_compute(host='host1')
3481         self.compute2 = self._start_compute(host='host2')
3482 
3483         flavors = self.api.get_flavors()
3484         self.flavor1 = flavors[0]
3485 
3486     def _other_hostname(self, host):
3487         other_host = {'host1': 'host2',
3488                       'host2': 'host1'}
3489         return other_host[host]
3490 
3491     def test_rescheduling_when_booting_instance(self):
3492         """Tests that allocations, created by the scheduler, are cleaned
3493         from the source node when the build fails on that node and is
3494         rescheduled to another node.
3495         """
3496         server_req = self._build_minimal_create_server_request(
3497                 self.api, 'some-server', flavor_id=self.flavor1['id'],
3498                 image_uuid='155d900f-4e14-4e4c-a73d-069cbf4541e6',
3499                 networks='none')
3500 
3501         created_server = self.api.post_server({'server': server_req})
3502         server = self._wait_for_state_change(
3503                 self.api, created_server, 'ACTIVE')
3504         dest_hostname = server['OS-EXT-SRV-ATTR:host']
3505         failed_hostname = self._other_hostname(dest_hostname)
3506 
3507         LOG.info('failed on %s', failed_hostname)
3508         LOG.info('booting on %s', dest_hostname)
3509 
3510         failed_rp_uuid = self._get_provider_uuid_by_host(failed_hostname)
3511         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
3512 
3513         # Expects no allocation records on the failed host.
3514         self.assertRequestMatchesUsage(
3515             {'VCPU': 0,
3516              'MEMORY_MB': 0,
3517              'DISK_GB': 0}, failed_rp_uuid)
3518 
3519         # Ensure the allocation records on the destination host.
3520         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
3521 
3522     def test_allocation_fails_during_reschedule(self):
3523         """Verify that if nova fails to allocate resources during re-schedule
3524         then the server is put into ERROR state properly.
3525         """
3526 
3527         server_req = self._build_minimal_create_server_request(
3528             self.api, 'some-server', flavor_id=self.flavor1['id'],
3529             image_uuid='155d900f-4e14-4e4c-a73d-069cbf4541e6',
3530             networks='none')
3531 
3532         orig_claim = utils.claim_resources
3533         # First call is during boot, we want that to succeed normally. Then the
3534         # fake virt driver triggers a re-schedule. During that re-schedule we
3535         # simulate that the placement call fails.
3536         with mock.patch('nova.scheduler.utils.claim_resources',
3537                         side_effect=[
3538                             orig_claim,
3539                             exception.AllocationUpdateFailed(
3540                                 consumer_uuid=uuids.inst1, error='testing')]):
3541 
3542             server = self.api.post_server({'server': server_req})
3543             server = self._wait_for_state_change(
3544                 self.admin_api, server, 'ERROR')
3545 
3546         self._delete_and_check_allocations(server)
3547 
3548 
3549 class ServerRescheduleTestsWithNestedResourcesRequest(ServerRescheduleTests):
3550     compute_driver = 'fake.FakeRescheduleDriverWithNestedCustomResources'
3551 
3552     def setUp(self):
3553         super(ServerRescheduleTestsWithNestedResourcesRequest, self).setUp()
3554         # modify the flavor used in the test base class to require one piece of
3555         # CUSTOM_MAGIC resource as well.
3556 
3557         self.api.post_extra_spec(
3558             self.flavor1['id'], {'extra_specs': {'resources:CUSTOM_MAGIC': 1}})
3559         # save the extra_specs in the flavor stored in the test case as
3560         # well
3561         self.flavor1['extra_specs'] = {'resources:CUSTOM_MAGIC': 1}
3562 
3563 
3564 class ServerBuildAbortTests(integrated_helpers.ProviderUsageBaseTestCase):
3565     """Tests server create scenarios which trigger a build abort during
3566     a server build and validates that allocations in Placement
3567     are properly cleaned up.
3568 
3569     Uses a fake virt driver that aborts the build on the first attempt.
3570     """
3571 
3572     compute_driver = 'fake.FakeBuildAbortDriver'
3573 
3574     def setUp(self):
3575         super(ServerBuildAbortTests, self).setUp()
3576         # We only need one compute service/host/node for these tests.
3577         self.compute1 = self._start_compute(host='host1')
3578 
3579         flavors = self.api.get_flavors()
3580         self.flavor1 = flavors[0]
3581 
3582     def test_abort_when_booting_instance(self):
3583         """Tests that allocations, created by the scheduler, are cleaned
3584         from the source node when the build is aborted on that node.
3585         """
3586         server_req = self._build_minimal_create_server_request(
3587                 self.api, 'some-server', flavor_id=self.flavor1['id'],
3588                 image_uuid='155d900f-4e14-4e4c-a73d-069cbf4541e6',
3589                 networks='none')
3590 
3591         created_server = self.api.post_server({'server': server_req})
3592         self._wait_for_state_change(self.api, created_server, 'ERROR')
3593 
3594         failed_hostname = self.compute1.manager.host
3595 
3596         failed_rp_uuid = self._get_provider_uuid_by_host(failed_hostname)
3597         # Expects no allocation records on the failed host.
3598         self.assertRequestMatchesUsage({'VCPU': 0,
3599                                         'MEMORY_MB': 0,
3600                                         'DISK_GB': 0}, failed_rp_uuid)
3601 
3602 
3603 class ServerBuildAbortTestsWithNestedResourceRequest(ServerBuildAbortTests):
3604     compute_driver = 'fake.FakeBuildAbortDriverWithNestedCustomResources'
3605 
3606     def setUp(self):
3607         super(ServerBuildAbortTestsWithNestedResourceRequest, self).setUp()
3608         # modify the flavor used in the test base class to require one piece of
3609         # CUSTOM_MAGIC resource as well.
3610 
3611         self.api.post_extra_spec(
3612             self.flavor1['id'], {'extra_specs': {'resources:CUSTOM_MAGIC': 1}})
3613         # save the extra_specs in the flavor stored in the test case as
3614         # well
3615         self.flavor1['extra_specs'] = {'resources:CUSTOM_MAGIC': 1}
3616 
3617 
3618 class ServerUnshelveSpawnFailTests(
3619         integrated_helpers.ProviderUsageBaseTestCase):
3620     """Tests server unshelve scenarios which trigger a
3621     VirtualInterfaceCreateException during driver.spawn() and validates that
3622     allocations in Placement are properly cleaned up.
3623     """
3624 
3625     compute_driver = 'fake.FakeUnshelveSpawnFailDriver'
3626 
3627     def setUp(self):
3628         super(ServerUnshelveSpawnFailTests, self).setUp()
3629         # We only need one compute service/host/node for these tests.
3630         self.compute1 = self._start_compute('host1')
3631 
3632         flavors = self.api.get_flavors()
3633         self.flavor1 = flavors[0]
3634 
3635     def test_driver_spawn_fail_when_unshelving_instance(self):
3636         """Tests that allocations, created by the scheduler, are cleaned
3637         from the target node when the unshelve driver.spawn fails on that node.
3638         """
3639         hostname = self.compute1.manager.host
3640         rp_uuid = self._get_provider_uuid_by_host(hostname)
3641         # We start with no usages on the host.
3642         self.assertRequestMatchesUsage(
3643             {'VCPU': 0,
3644              'MEMORY_MB': 0,
3645              'DISK_GB': 0}, rp_uuid)
3646 
3647         server_req = self._build_minimal_create_server_request(
3648             self.api, 'unshelve-spawn-fail', flavor_id=self.flavor1['id'],
3649             image_uuid='155d900f-4e14-4e4c-a73d-069cbf4541e6',
3650             networks='none')
3651 
3652         server = self.api.post_server({'server': server_req})
3653         self._wait_for_state_change(self.api, server, 'ACTIVE')
3654 
3655         # assert allocations exist for the host
3656         self.assertFlavorMatchesUsage(rp_uuid, self.flavor1)
3657 
3658         # shelve offload the server
3659         self.flags(shelved_offload_time=0)
3660         self.api.post_server_action(server['id'], {'shelve': None})
3661         self._wait_for_server_parameter(
3662             self.api, server, {'status': 'SHELVED_OFFLOADED',
3663                                'OS-EXT-SRV-ATTR:host': None})
3664 
3665         # assert allocations were removed from the host
3666         self.assertRequestMatchesUsage(
3667             {'VCPU': 0,
3668              'MEMORY_MB': 0,
3669              'DISK_GB': 0}, rp_uuid)
3670 
3671         # unshelve the server, which should fail
3672         self.api.post_server_action(server['id'], {'unshelve': None})
3673         self._wait_for_action_fail_completion(
3674             server, instance_actions.UNSHELVE, 'compute_unshelve_instance')
3675 
3676         # assert allocations were removed from the host
3677         self.assertRequestMatchesUsage(
3678             {'VCPU': 0,
3679              'MEMORY_MB': 0,
3680              'DISK_GB': 0}, rp_uuid)
3681 
3682 
3683 class ServerUnshelveSpawnFailTestsWithNestedResourceRequest(
3684     ServerUnshelveSpawnFailTests):
3685     compute_driver = ('fake.'
3686                       'FakeUnshelveSpawnFailDriverWithNestedCustomResources')
3687 
3688     def setUp(self):
3689         super(ServerUnshelveSpawnFailTestsWithNestedResourceRequest,
3690               self).setUp()
3691         # modify the flavor used in the test base class to require one piece of
3692         # CUSTOM_MAGIC resource as well.
3693 
3694         self.api.post_extra_spec(
3695             self.flavor1['id'], {'extra_specs': {'resources:CUSTOM_MAGIC': 1}})
3696         # save the extra_specs in the flavor stored in the test case as
3697         # well
3698         self.flavor1['extra_specs'] = {'resources:CUSTOM_MAGIC': 1}
3699 
3700 
3701 class ServerSoftDeleteTests(integrated_helpers.ProviderUsageBaseTestCase):
3702 
3703     compute_driver = 'fake.SmallFakeDriver'
3704 
3705     def setUp(self):
3706         super(ServerSoftDeleteTests, self).setUp()
3707         # We only need one compute service/host/node for these tests.
3708         self.compute1 = self._start_compute('host1')
3709 
3710         flavors = self.api.get_flavors()
3711         self.flavor1 = flavors[0]
3712 
3713     def _soft_delete_and_check_allocation(self, server, hostname):
3714         self.api.delete_server(server['id'])
3715         server = self._wait_for_state_change(self.api, server, 'SOFT_DELETED')
3716 
3717         self._run_periodics()
3718 
3719         # in soft delete state nova should keep the resource allocation as
3720         # the instance can be restored
3721         rp_uuid = self._get_provider_uuid_by_host(hostname)
3722 
3723         self.assertFlavorMatchesUsage(rp_uuid, self.flavor1)
3724 
3725         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
3726                                            rp_uuid)
3727 
3728         # run the periodic reclaim but as time isn't advanced it should not
3729         # reclaim the instance
3730         ctxt = context.get_admin_context()
3731         self.compute1._reclaim_queued_deletes(ctxt)
3732 
3733         self._run_periodics()
3734 
3735         self.assertFlavorMatchesUsage(rp_uuid, self.flavor1)
3736 
3737         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
3738                                            rp_uuid)
3739 
3740     def test_soft_delete_then_reclaim(self):
3741         """Asserts that the automatic reclaim of soft deleted instance cleans
3742         up the allocations in placement.
3743         """
3744 
3745         # make sure that instance will go to SOFT_DELETED state instead of
3746         # deleted immediately
3747         self.flags(reclaim_instance_interval=30)
3748 
3749         hostname = self.compute1.host
3750         rp_uuid = self._get_provider_uuid_by_host(hostname)
3751 
3752         server = self._boot_and_check_allocations(self.flavor1, hostname)
3753 
3754         self._soft_delete_and_check_allocation(server, hostname)
3755 
3756         # advance the time and run periodic reclaim, instance should be deleted
3757         # and resources should be freed
3758         the_past = timeutils.utcnow() + datetime.timedelta(hours=1)
3759         timeutils.set_time_override(override_time=the_past)
3760         self.addCleanup(timeutils.clear_time_override)
3761         ctxt = context.get_admin_context()
3762         self.compute1._reclaim_queued_deletes(ctxt)
3763 
3764         # Wait for real deletion
3765         self._wait_until_deleted(server)
3766 
3767         usages = self._get_provider_usages(rp_uuid)
3768         self.assertEqual({'VCPU': 0,
3769                           'MEMORY_MB': 0,
3770                           'DISK_GB': 0}, usages)
3771         allocations = self._get_allocations_by_server_uuid(server['id'])
3772         self.assertEqual(0, len(allocations))
3773 
3774     def test_soft_delete_then_restore(self):
3775         """Asserts that restoring a soft deleted instance keeps the proper
3776         allocation in placement.
3777         """
3778 
3779         # make sure that instance will go to SOFT_DELETED state instead of
3780         # deleted immediately
3781         self.flags(reclaim_instance_interval=30)
3782 
3783         hostname = self.compute1.host
3784         rp_uuid = self._get_provider_uuid_by_host(hostname)
3785 
3786         server = self._boot_and_check_allocations(
3787             self.flavor1, hostname)
3788 
3789         self._soft_delete_and_check_allocation(server, hostname)
3790 
3791         post = {'restore': {}}
3792         self.api.post_server_action(server['id'], post)
3793         server = self._wait_for_state_change(self.api, server, 'ACTIVE')
3794 
3795         # after restore the allocations should be kept
3796         self.assertFlavorMatchesUsage(rp_uuid, self.flavor1)
3797 
3798         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
3799                                            rp_uuid)
3800 
3801         # Now we want a real delete
3802         self.flags(reclaim_instance_interval=0)
3803         self._delete_and_check_allocations(server)
3804 
3805 
3806 class ServerSoftDeleteTestsWithNestedResourceRequest(ServerSoftDeleteTests):
3807     compute_driver = 'fake.MediumFakeDriverWithNestedCustomResources'
3808 
3809     def setUp(self):
3810         super(ServerSoftDeleteTestsWithNestedResourceRequest, self).setUp()
3811         # modify the flavor used in the test base class to require one piece of
3812         # CUSTOM_MAGIC resource as well.
3813 
3814         self.api.post_extra_spec(
3815             self.flavor1['id'], {'extra_specs': {'resources:CUSTOM_MAGIC': 1}})
3816         # save the extra_specs in the flavor stored in the test case as
3817         # well
3818         self.flavor1['extra_specs'] = {'resources:CUSTOM_MAGIC': 1}
3819 
3820 
3821 class VolumeBackedServerTest(integrated_helpers.ProviderUsageBaseTestCase):
3822     """Tests for volume-backed servers."""
3823 
3824     compute_driver = 'fake.SmallFakeDriver'
3825 
3826     def setUp(self):
3827         super(VolumeBackedServerTest, self).setUp()
3828         self.compute1 = self._start_compute('host1')
3829         self.flavor_id = self._create_flavor()
3830 
3831     def _create_flavor(self):
3832         body = {
3833             'flavor': {
3834                 'id': 'vbst',
3835                 'name': 'special',
3836                 'ram': 512,
3837                 'vcpus': 1,
3838                 'disk': 10,
3839                 'OS-FLV-EXT-DATA:ephemeral': 20,
3840                 'swap': 5 * 1024,
3841                 'rxtx_factor': 1.0,
3842                 'os-flavor-access:is_public': True,
3843             },
3844         }
3845         self.admin_api.post_flavor(body)
3846         return body['flavor']['id']
3847 
3848     def _create_server(self):
3849         with nova.utils.temporary_mutation(self.api, microversion='2.35'):
3850             image_id = self.api.get_images()[0]['id']
3851         server_req = self._build_minimal_create_server_request(
3852             self.api, 'trait-based-server',
3853             image_uuid=image_id,
3854             flavor_id=self.flavor_id, networks='none')
3855         server = self.api.post_server({'server': server_req})
3856         server = self._wait_for_state_change(self.api, server, 'ACTIVE')
3857         return server
3858 
3859     def _create_volume_backed_server(self):
3860         self.useFixture(nova_fixtures.CinderFixture(self))
3861         volume_id = nova_fixtures.CinderFixture.IMAGE_BACKED_VOL
3862         server_req_body = {
3863             # There is no imageRef because this is boot from volume.
3864             'server': {
3865                 'flavorRef': self.flavor_id,
3866                 'name': 'test_volume_backed',
3867                 # We don't care about networking for this test. This
3868                 # requires microversion >= 2.37.
3869                 'networks': 'none',
3870                 'block_device_mapping_v2': [{
3871                     'boot_index': 0,
3872                     'uuid': volume_id,
3873                     'source_type': 'volume',
3874                     'destination_type': 'volume'
3875                 }]
3876             }
3877         }
3878         server = self.api.post_server(server_req_body)
3879         server = self._wait_for_state_change(self.api, server, 'ACTIVE')
3880         return server
3881 
3882     def test_ephemeral_has_disk_allocation(self):
3883         server = self._create_server()
3884         allocs = self._get_allocations_by_server_uuid(server['id'])
3885         resources = list(allocs.values())[0]['resources']
3886         self.assertIn('MEMORY_MB', resources)
3887         # 10gb root, 20gb ephemeral, 5gb swap
3888         expected_usage = 35
3889         self.assertEqual(expected_usage, resources['DISK_GB'])
3890         # Ensure the compute node is reporting the correct disk usage
3891         self.assertEqual(
3892             expected_usage,
3893             self.admin_api.get_hypervisor_stats()['local_gb_used'])
3894 
3895     def test_volume_backed_image_type_filter(self):
3896         # Enable the image type support filter and ensure that a
3897         # non-image-having volume-backed server can still boot
3898         self.flags(query_placement_for_image_type_support=True,
3899                    group='scheduler')
3900         server = self._create_volume_backed_server()
3901         created_server = self.api.get_server(server['id'])
3902         self.assertEqual('ACTIVE', created_server['status'])
3903 
3904     def test_volume_backed_no_disk_allocation(self):
3905         server = self._create_volume_backed_server()
3906         allocs = self._get_allocations_by_server_uuid(server['id'])
3907         resources = list(allocs.values())[0]['resources']
3908         self.assertIn('MEMORY_MB', resources)
3909         # 0gb root, 20gb ephemeral, 5gb swap
3910         expected_usage = 25
3911         self.assertEqual(expected_usage, resources['DISK_GB'])
3912         # Ensure the compute node is reporting the correct disk usage
3913         self.assertEqual(
3914             expected_usage,
3915             self.admin_api.get_hypervisor_stats()['local_gb_used'])
3916 
3917         # Now let's hack the RequestSpec.is_bfv field to mimic migrating an
3918         # old instance created before RequestSpec.is_bfv was set in the API,
3919         # move the instance and verify that the RequestSpec.is_bfv is set
3920         # and the instance still reports the same DISK_GB allocations as during
3921         # the initial create.
3922         ctxt = context.get_admin_context()
3923         reqspec = objects.RequestSpec.get_by_instance_uuid(ctxt, server['id'])
3924         # Make sure it's set.
3925         self.assertTrue(reqspec.is_bfv)
3926         del reqspec.is_bfv
3927         reqspec.save()
3928         reqspec = objects.RequestSpec.get_by_instance_uuid(ctxt, server['id'])
3929         # Make sure it's not set.
3930         self.assertNotIn('is_bfv', reqspec)
3931         # Now migrate the instance to another host and check the request spec
3932         # and allocations after the migration.
3933         self._start_compute('host2')
3934         self.admin_api.post_server_action(server['id'], {'migrate': None})
3935         # Wait for the server to complete the cold migration.
3936         server = self._wait_for_state_change(
3937             self.admin_api, server, 'VERIFY_RESIZE')
3938         self.assertEqual('host2', server['OS-EXT-SRV-ATTR:host'])
3939         # Confirm the cold migration and check usage and the request spec.
3940         self._confirm_resize(server)
3941         reqspec = objects.RequestSpec.get_by_instance_uuid(ctxt, server['id'])
3942         # Make sure it's set.
3943         self.assertTrue(reqspec.is_bfv)
3944         allocs = self._get_allocations_by_server_uuid(server['id'])
3945         resources = list(allocs.values())[0]['resources']
3946         self.assertEqual(expected_usage, resources['DISK_GB'])
3947 
3948         # Now shelve and unshelve the server to make sure root_gb DISK_GB
3949         # isn't reported for allocations after we unshelve the server.
3950         fake_notifier.stub_notifier(self)
3951         self.addCleanup(fake_notifier.reset)
3952         self.api.post_server_action(server['id'], {'shelve': None})
3953         self._wait_for_state_change(self.api, server, 'SHELVED_OFFLOADED')
3954         fake_notifier.wait_for_versioned_notifications(
3955                 'instance.shelve_offload.end')
3956         # The server should not have any allocations since it's not currently
3957         # hosted on any compute service.
3958         allocs = self._get_allocations_by_server_uuid(server['id'])
3959         self.assertDictEqual({}, allocs)
3960         # Now unshelve the server and make sure there are still no DISK_GB
3961         # allocations for the root disk.
3962         self.api.post_server_action(server['id'], {'unshelve': None})
3963         self._wait_for_state_change(self.api, server, 'ACTIVE')
3964         allocs = self._get_allocations_by_server_uuid(server['id'])
3965         resources = list(allocs.values())[0]['resources']
3966         self.assertEqual(expected_usage, resources['DISK_GB'])
3967 
3968 
3969 class TraitsBasedSchedulingTest(integrated_helpers.ProviderUsageBaseTestCase):
3970     """Tests for requesting a server with required traits in Placement"""
3971 
3972     compute_driver = 'fake.SmallFakeDriver'
3973 
3974     def setUp(self):
3975         super(TraitsBasedSchedulingTest, self).setUp()
3976         self.compute1 = self._start_compute('host1')
3977         self.compute2 = self._start_compute('host2')
3978         # Using a standard trait from the os-traits library, set a required
3979         # trait extra spec on the flavor.
3980         flavors = self.api.get_flavors()
3981         self.flavor_with_trait = flavors[0]
3982         self.admin_api.post_extra_spec(
3983             self.flavor_with_trait['id'],
3984             {'extra_specs': {'trait:HW_CPU_X86_VMX': 'required'}})
3985         self.flavor_without_trait = flavors[1]
3986         self.flavor_with_forbidden_trait = flavors[2]
3987         self.admin_api.post_extra_spec(
3988             self.flavor_with_forbidden_trait['id'],
3989             {'extra_specs': {'trait:HW_CPU_X86_SGX': 'forbidden'}})
3990 
3991         # Note that we're using v2.35 explicitly as the api returns 404
3992         # starting with 2.36
3993         with nova.utils.temporary_mutation(self.api, microversion='2.35'):
3994             images = self.api.get_images()
3995             self.image_id_with_trait = images[0]['id']
3996             self.api.api_put('/images/%s/metadata' % self.image_id_with_trait,
3997                              {'metadata': {
3998                                  'trait:HW_CPU_X86_SGX': 'required'}})
3999             self.image_id_without_trait = images[1]['id']
4000 
4001     def _create_server_with_traits(self, flavor_id, image_id):
4002         """Create a server with given flavor and image id's
4003         :param flavor_id: the flavor id
4004         :param image_id: the image id
4005         :return: create server response
4006         """
4007 
4008         server_req = self._build_minimal_create_server_request(
4009             self.api, 'trait-based-server',
4010             image_uuid=image_id,
4011             flavor_id=flavor_id, networks='none')
4012         return self.api.post_server({'server': server_req})
4013 
4014     def _create_volume_backed_server_with_traits(self, flavor_id, volume_id):
4015         """Create a server with block device mapping(volume) with the given
4016         flavor and volume id's. Either the flavor or the image backing the
4017         volume is expected to have the traits
4018         :param flavor_id: the flavor id
4019         :param volume_id: the volume id
4020         :return: create server response
4021         """
4022 
4023         server_req_body = {
4024             # There is no imageRef because this is boot from volume.
4025             'server': {
4026                 'flavorRef': flavor_id,
4027                 'name': 'test_image_trait_on_volume_backed',
4028                 # We don't care about networking for this test. This
4029                 # requires microversion >= 2.37.
4030                 'networks': 'none',
4031                 'block_device_mapping_v2': [{
4032                     'boot_index': 0,
4033                     'uuid': volume_id,
4034                     'source_type': 'volume',
4035                     'destination_type': 'volume'
4036                 }]
4037             }
4038         }
4039         server = self.api.post_server(server_req_body)
4040         return server
4041 
4042     def test_flavor_traits_based_scheduling(self):
4043         """Tests that a server create request using a required trait in the
4044         flavor ends up on the single compute node resource provider that also
4045         has that trait in Placement. That test will however pass half of the
4046         times even if the trait is not taken into consideration, so we are
4047         also disabling the compute node that has the required trait and try
4048         again, which should result in a no valid host error.
4049         """
4050 
4051         # Decorate compute1 resource provider with the required trait.
4052         rp_uuid = self._get_provider_uuid_by_host(self.compute1.host)
4053         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_VMX'])
4054 
4055         # Create server using flavor with required trait
4056         server = self._create_server_with_traits(self.flavor_with_trait['id'],
4057                                                  self.image_id_without_trait)
4058         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
4059         # Assert the server ended up on the expected compute host that has
4060         # the required trait.
4061         self.assertEqual(self.compute1.host, server['OS-EXT-SRV-ATTR:host'])
4062 
4063         # Disable the compute node that has the required trait
4064         compute1_service_id = self.admin_api.get_services(
4065             host=self.compute1.host, binary='nova-compute')[0]['id']
4066         self.admin_api.put_service(compute1_service_id, {'status': 'disabled'})
4067 
4068         # Create server using flavor with required trait
4069         server = self._create_server_with_traits(self.flavor_with_trait['id'],
4070                                                  self.image_id_without_trait)
4071 
4072         # The server should go to ERROR state because there is no valid host.
4073         server = self._wait_for_state_change(self.admin_api, server, 'ERROR')
4074         self.assertIsNone(server['OS-EXT-SRV-ATTR:host'])
4075         # Make sure the failure was due to NoValidHost by checking the fault.
4076         self.assertIn('fault', server)
4077         self.assertIn('No valid host', server['fault']['message'])
4078 
4079     def test_flavor_forbidden_traits_based_scheduling(self):
4080         """Tests that a server create request using a forbidden trait in the
4081         flavor ends up on the single compute host that doesn't have that
4082         trait in Placement. That test will however pass half of the times even
4083         if the trait is not taken into consideration, so we are also disabling
4084         the compute node that doesn't have the forbidden trait and try again,
4085         which should result in a no valid host error.
4086         """
4087 
4088         # Decorate compute1 resource provider with forbidden trait
4089         rp_uuid = self._get_provider_uuid_by_host(self.compute1.host)
4090         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_SGX'])
4091 
4092         # Create server using flavor with forbidden trait
4093         server = self._create_server_with_traits(
4094             self.flavor_with_forbidden_trait['id'],
4095             self.image_id_without_trait
4096         )
4097 
4098         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
4099 
4100         # Assert the server ended up on the expected compute host that doesn't
4101         # have the forbidden trait.
4102         self.assertEqual(self.compute2.host, server['OS-EXT-SRV-ATTR:host'])
4103 
4104         # Disable the compute node that doesn't have the forbidden trait
4105         compute2_service_id = self.admin_api.get_services(
4106             host=self.compute2.host, binary='nova-compute')[0]['id']
4107         self.admin_api.put_service(compute2_service_id, {'status': 'disabled'})
4108 
4109         # Create server using flavor with forbidden trait
4110         server = self._create_server_with_traits(
4111             self.flavor_with_forbidden_trait['id'],
4112             self.image_id_without_trait
4113         )
4114 
4115         # The server should go to ERROR state because there is no valid host.
4116         server = self._wait_for_state_change(self.admin_api, server, 'ERROR')
4117         self.assertIsNone(server['OS-EXT-SRV-ATTR:host'])
4118         # Make sure the failure was due to NoValidHost by checking the fault.
4119         self.assertIn('fault', server)
4120         self.assertIn('No valid host', server['fault']['message'])
4121 
4122     def test_image_traits_based_scheduling(self):
4123         """Tests that a server create request using a required trait on image
4124         ends up on the single compute node resource provider that also has that
4125         trait in Placement.
4126         """
4127 
4128         # Decorate compute2 resource provider with image trait.
4129         rp_uuid = self._get_provider_uuid_by_host(self.compute2.host)
4130         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_SGX'])
4131 
4132         # Create server using only image trait
4133         server = self._create_server_with_traits(
4134             self.flavor_without_trait['id'], self.image_id_with_trait)
4135         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
4136         # Assert the server ended up on the expected compute host that has
4137         # the required trait.
4138         self.assertEqual(self.compute2.host, server['OS-EXT-SRV-ATTR:host'])
4139 
4140     def test_flavor_image_traits_based_scheduling(self):
4141         """Tests that a server create request using a required trait on flavor
4142         AND a required trait on the image ends up on the single compute node
4143         resource provider that also has that trait in Placement.
4144         """
4145 
4146         # Decorate compute2 resource provider with both flavor and image trait.
4147         rp_uuid = self._get_provider_uuid_by_host(self.compute2.host)
4148         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_VMX',
4149                                             'HW_CPU_X86_SGX'])
4150 
4151         # Create server using flavor and image trait
4152         server = self._create_server_with_traits(
4153             self.flavor_with_trait['id'], self.image_id_with_trait)
4154         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
4155         # Assert the server ended up on the expected compute host that has
4156         # the required trait.
4157         self.assertEqual(self.compute2.host, server['OS-EXT-SRV-ATTR:host'])
4158 
4159     def test_image_trait_on_volume_backed_instance(self):
4160         """Tests that when trying to launch a volume-backed instance with a
4161         required trait on the image metadata contained within the volume,
4162         the instance ends up on the single compute node resource provider
4163         that also has that trait in Placement.
4164         """
4165         # Decorate compute2 resource provider with volume image metadata trait.
4166         rp_uuid = self._get_provider_uuid_by_host(self.compute2.host)
4167         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_SGX'])
4168 
4169         self.useFixture(nova_fixtures.CinderFixture(self))
4170         # Create our server with a volume containing the image meta data with a
4171         # required trait
4172         server = self._create_volume_backed_server_with_traits(
4173             self.flavor_without_trait['id'],
4174             nova_fixtures.CinderFixture.
4175             IMAGE_WITH_TRAITS_BACKED_VOL)
4176 
4177         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
4178         # Assert the server ended up on the expected compute host that has
4179         # the required trait.
4180         self.assertEqual(self.compute2.host, server['OS-EXT-SRV-ATTR:host'])
4181 
4182     def test_flavor_image_trait_on_volume_backed_instance(self):
4183         """Tests that when trying to launch a volume-backed instance with a
4184         required trait on flavor AND a required trait on the image metadata
4185         contained within the volume, the instance ends up on the single
4186         compute node resource provider that also has those traits in Placement.
4187         """
4188         # Decorate compute2 resource provider with volume image metadata trait.
4189         rp_uuid = self._get_provider_uuid_by_host(self.compute2.host)
4190         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_VMX',
4191                                             'HW_CPU_X86_SGX'])
4192 
4193         self.useFixture(nova_fixtures.CinderFixture(self))
4194         # Create our server with a flavor trait and a volume containing the
4195         # image meta data with a required trait
4196         server = self._create_volume_backed_server_with_traits(
4197             self.flavor_with_trait['id'],
4198             nova_fixtures.CinderFixture.
4199             IMAGE_WITH_TRAITS_BACKED_VOL)
4200 
4201         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
4202         # Assert the server ended up on the expected compute host that has
4203         # the required trait.
4204         self.assertEqual(self.compute2.host, server['OS-EXT-SRV-ATTR:host'])
4205 
4206     def test_flavor_traits_based_scheduling_no_valid_host(self):
4207         """Tests that a server create request using a required trait expressed
4208          in flavor fails to find a valid host since no compute node resource
4209          providers have the trait.
4210         """
4211 
4212         # Decorate compute1 resource provider with the image trait.
4213         rp_uuid = self._get_provider_uuid_by_host(self.compute1.host)
4214         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_SGX'])
4215 
4216         server = self._create_server_with_traits(self.flavor_with_trait['id'],
4217                                                  self.image_id_without_trait)
4218         # The server should go to ERROR state because there is no valid host.
4219         server = self._wait_for_state_change(self.admin_api, server, 'ERROR')
4220         self.assertIsNone(server['OS-EXT-SRV-ATTR:host'])
4221         # Make sure the failure was due to NoValidHost by checking the fault.
4222         self.assertIn('fault', server)
4223         self.assertIn('No valid host', server['fault']['message'])
4224 
4225     def test_image_traits_based_scheduling_no_valid_host(self):
4226         """Tests that a server create request using a required trait expressed
4227          in image fails to find a valid host since no compute node resource
4228          providers have the trait.
4229         """
4230 
4231         # Decorate compute1 resource provider with that flavor trait.
4232         rp_uuid = self._get_provider_uuid_by_host(self.compute1.host)
4233         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_VMX'])
4234 
4235         server = self._create_server_with_traits(
4236             self.flavor_without_trait['id'], self.image_id_with_trait)
4237         # The server should go to ERROR state because there is no valid host.
4238         server = self._wait_for_state_change(self.admin_api, server, 'ERROR')
4239         self.assertIsNone(server['OS-EXT-SRV-ATTR:host'])
4240         # Make sure the failure was due to NoValidHost by checking the fault.
4241         self.assertIn('fault', server)
4242         self.assertIn('No valid host', server['fault']['message'])
4243 
4244     def test_flavor_image_traits_based_scheduling_no_valid_host(self):
4245         """Tests that a server create request using a required trait expressed
4246          in flavor AND a required trait expressed in the image fails to find a
4247          valid host since no compute node resource providers have the trait.
4248         """
4249 
4250         server = self._create_server_with_traits(
4251             self.flavor_with_trait['id'], self.image_id_with_trait)
4252         # The server should go to ERROR state because there is no valid host.
4253         server = self._wait_for_state_change(self.admin_api, server, 'ERROR')
4254         self.assertIsNone(server['OS-EXT-SRV-ATTR:host'])
4255         # Make sure the failure was due to NoValidHost by checking the fault.
4256         self.assertIn('fault', server)
4257         self.assertIn('No valid host', server['fault']['message'])
4258 
4259     def test_image_trait_on_volume_backed_instance_no_valid_host(self):
4260         """Tests that when trying to launch a volume-backed instance with a
4261         required trait on the image metadata contained within the volume
4262         fails to find a valid host since no compute node resource providers
4263         have the trait.
4264         """
4265         self.useFixture(nova_fixtures.CinderFixture(self))
4266         # Create our server with a volume
4267         server = self._create_volume_backed_server_with_traits(
4268             self.flavor_without_trait['id'],
4269             nova_fixtures.CinderFixture.
4270             IMAGE_WITH_TRAITS_BACKED_VOL)
4271 
4272         # The server should go to ERROR state because there is no valid host.
4273         server = self._wait_for_state_change(self.admin_api, server, 'ERROR')
4274         self.assertIsNone(server['OS-EXT-SRV-ATTR:host'])
4275         # Make sure the failure was due to NoValidHost by checking the fault.
4276         self.assertIn('fault', server)
4277         self.assertIn('No valid host', server['fault']['message'])
4278 
4279     def test_rebuild_instance_with_image_traits(self):
4280         """Rebuilds a server with a different image which has traits
4281         associated with it and which will run it through the scheduler to
4282         validate the image is still OK with the compute host that the
4283         instance is running on.
4284          """
4285         # Decorate compute2 resource provider with both flavor and image trait.
4286         rp_uuid = self._get_provider_uuid_by_host(self.compute2.host)
4287         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_VMX',
4288                                             'HW_CPU_X86_SGX'])
4289         # make sure we start with no usage on the compute node
4290         rp_usages = self._get_provider_usages(rp_uuid)
4291         self.assertEqual({'VCPU': 0, 'MEMORY_MB': 0, 'DISK_GB': 0}, rp_usages)
4292 
4293         # create a server without traits on image and with traits on flavour
4294         server = self._create_server_with_traits(
4295             self.flavor_with_trait['id'], self.image_id_without_trait)
4296         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
4297 
4298         # make the compute node full and ensure rebuild still succeed
4299         inv = {"resource_class": "VCPU",
4300                "total": 1}
4301         self._set_inventory(rp_uuid, inv)
4302 
4303         # Now rebuild the server with a different image with traits
4304         rebuild_req_body = {
4305             'rebuild': {
4306                 'imageRef': self.image_id_with_trait
4307             }
4308         }
4309         self.api.api_post('/servers/%s/action' % server['id'],
4310                           rebuild_req_body)
4311         self._wait_for_server_parameter(
4312             self.api, server, {'OS-EXT-STS:task_state': None})
4313 
4314         allocs = self._get_allocations_by_server_uuid(server['id'])
4315         self.assertIn(rp_uuid, allocs)
4316 
4317         # Assert the server ended up on the expected compute host that has
4318         # the required trait.
4319         self.assertEqual(self.compute2.host, server['OS-EXT-SRV-ATTR:host'])
4320 
4321     def test_rebuild_instance_with_image_traits_no_host(self):
4322         """Rebuilding a server with a different image which has required
4323         traits on the image fails to valid the host that this server is
4324         currently running, cause the compute host resource provider is not
4325         associated with similar trait.
4326         """
4327         # Decorate compute2 resource provider with traits on flavor
4328         rp_uuid = self._get_provider_uuid_by_host(self.compute2.host)
4329         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_VMX'])
4330 
4331         # make sure we start with no usage on the compute node
4332         rp_usages = self._get_provider_usages(rp_uuid)
4333         self.assertEqual({'VCPU': 0, 'MEMORY_MB': 0, 'DISK_GB': 0}, rp_usages)
4334 
4335         # create a server without traits on image and with traits on flavour
4336         server = self._create_server_with_traits(
4337             self.flavor_with_trait['id'], self.image_id_without_trait)
4338         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
4339 
4340         # Now rebuild the server with a different image with traits
4341         rebuild_req_body = {
4342             'rebuild': {
4343                 'imageRef': self.image_id_with_trait
4344             }
4345         }
4346 
4347         self.api.api_post('/servers/%s/action' % server['id'],
4348                           rebuild_req_body)
4349         # Look for the failed rebuild action.
4350         self._wait_for_action_fail_completion(
4351             server, instance_actions.REBUILD, 'rebuild_server', self.admin_api)
4352         # Assert the server image_ref was rolled back on failure.
4353         server = self.api.get_server(server['id'])
4354         self.assertEqual(self.image_id_without_trait, server['image']['id'])
4355 
4356         # The server should be in ERROR state
4357         self.assertEqual('ERROR', server['status'])
4358         self.assertEqual("No valid host was found. Image traits cannot be "
4359                          "satisfied by the current resource providers. "
4360                          "Either specify a different image during rebuild "
4361                          "or create a new server with the specified image.",
4362                          server['fault']['message'])
4363 
4364     def test_rebuild_instance_with_image_traits_no_image_change(self):
4365         """Rebuilds a server with a same image which has traits
4366         associated with it and which will run it through the scheduler to
4367         validate the image is still OK with the compute host that the
4368         instance is running on.
4369          """
4370         # Decorate compute2 resource provider with both flavor and image trait.
4371         rp_uuid = self._get_provider_uuid_by_host(self.compute2.host)
4372         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_VMX',
4373                                             'HW_CPU_X86_SGX'])
4374         # make sure we start with no usage on the compute node
4375         rp_usages = self._get_provider_usages(rp_uuid)
4376         self.assertEqual({'VCPU': 0, 'MEMORY_MB': 0, 'DISK_GB': 0},
4377                          rp_usages)
4378 
4379         # create a server with traits in both image and flavour
4380         server = self._create_server_with_traits(
4381             self.flavor_with_trait['id'], self.image_id_with_trait)
4382         server = self._wait_for_state_change(self.admin_api, server,
4383                                              'ACTIVE')
4384 
4385         # Now rebuild the server with a different image with traits
4386         rebuild_req_body = {
4387             'rebuild': {
4388                 'imageRef': self.image_id_with_trait
4389             }
4390         }
4391         self.api.api_post('/servers/%s/action' % server['id'],
4392                           rebuild_req_body)
4393         self._wait_for_server_parameter(
4394             self.api, server, {'OS-EXT-STS:task_state': None})
4395 
4396         allocs = self._get_allocations_by_server_uuid(server['id'])
4397         self.assertIn(rp_uuid, allocs)
4398 
4399         # Assert the server ended up on the expected compute host that has
4400         # the required trait.
4401         self.assertEqual(self.compute2.host,
4402                          server['OS-EXT-SRV-ATTR:host'])
4403 
4404     def test_rebuild_instance_with_image_traits_and_forbidden_flavor_traits(
4405                                                                         self):
4406         """Rebuilding a server with a different image which has required
4407         traits on the image fails to validate image traits because flavor
4408         associated with the current instance has the similar trait that is
4409         forbidden
4410         """
4411         # Decorate compute2 resource provider with traits on flavor
4412         rp_uuid = self._get_provider_uuid_by_host(self.compute2.host)
4413         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_VMX'])
4414 
4415         # make sure we start with no usage on the compute node
4416         rp_usages = self._get_provider_usages(rp_uuid)
4417         self.assertEqual({'VCPU': 0, 'MEMORY_MB': 0, 'DISK_GB': 0}, rp_usages)
4418 
4419         # create a server with forbidden traits on flavor and no triats on
4420         # image
4421         server = self._create_server_with_traits(
4422             self.flavor_with_forbidden_trait['id'],
4423             self.image_id_without_trait)
4424         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
4425 
4426         # Now rebuild the server with a different image with traits
4427         rebuild_req_body = {
4428             'rebuild': {
4429                 'imageRef': self.image_id_with_trait
4430             }
4431         }
4432 
4433         self.api.api_post('/servers/%s/action' % server['id'],
4434                           rebuild_req_body)
4435         # Look for the failed rebuild action.
4436         self._wait_for_action_fail_completion(
4437             server, instance_actions.REBUILD, 'rebuild_server', self.admin_api)
4438         # Assert the server image_ref was rolled back on failure.
4439         server = self.api.get_server(server['id'])
4440         self.assertEqual(self.image_id_without_trait, server['image']['id'])
4441 
4442         # The server should be in ERROR state
4443         self.assertEqual('ERROR', server['status'])
4444         self.assertEqual("No valid host was found. Image traits are part of "
4445                          "forbidden traits in flavor associated with the "
4446                          "server. Either specify a different image during "
4447                          "rebuild or create a new server with the specified "
4448                          "image and a compatible flavor.",
4449                          server['fault']['message'])
4450 
4451 
4452 class ServerTestV256Common(ServersTestBase):
4453     api_major_version = 'v2.1'
4454     microversion = '2.56'
4455     ADMIN_API = True
4456 
4457     def _setup_compute_service(self):
4458         # Set up 3 compute services in the same cell
4459         for host in ('host1', 'host2', 'host3'):
4460             self.start_service('compute', host=host)
4461 
4462     def _create_server(self, target_host=None):
4463         server = self._build_minimal_create_server_request(
4464             image_uuid='a2459075-d96c-40d5-893e-577ff92e721c')
4465         server.update({'networks': 'auto'})
4466         if target_host is not None:
4467             server['availability_zone'] = 'nova:%s' % target_host
4468         post = {'server': server}
4469         response = self.api.api_post('/servers', post).body
4470         return response['server']
4471 
4472     @staticmethod
4473     def _get_target_and_other_hosts(host):
4474         target_other_hosts = {'host1': ['host2', 'host3'],
4475                               'host2': ['host3', 'host1'],
4476                               'host3': ['host1', 'host2']}
4477         return target_other_hosts[host]
4478 
4479 
4480 class ServerTestV256MultiCellTestCase(ServerTestV256Common):
4481     """Negative test to ensure we fail with ComputeHostNotFound if we try to
4482     target a host in another cell from where the instance lives.
4483     """
4484     NUMBER_OF_CELLS = 2
4485 
4486     def _setup_compute_service(self):
4487         # Set up 2 compute services in different cells
4488         host_to_cell_mappings = {
4489             'host1': 'cell1',
4490             'host2': 'cell2'}
4491         for host in sorted(host_to_cell_mappings):
4492             self.start_service('compute', host=host,
4493                                cell=host_to_cell_mappings[host])
4494 
4495     def test_migrate_server_to_host_in_different_cell(self):
4496         # We target host1 specifically so that we have a predictable target for
4497         # the cold migration in cell2.
4498         server = self._create_server(target_host='host1')
4499         server = self._wait_for_state_change(server, 'BUILD')
4500 
4501         self.assertEqual('host1', server['OS-EXT-SRV-ATTR:host'])
4502         ex = self.assertRaises(client.OpenStackApiException,
4503                                self.api.post_server_action,
4504                                server['id'],
4505                                {'migrate': {'host': 'host2'}})
4506         # When the API pulls the instance out of cell1, the context is targeted
4507         # to cell1, so when the compute API resize() method attempts to lookup
4508         # the target host in cell1, it will result in a ComputeHostNotFound
4509         # error.
4510         self.assertEqual(400, ex.response.status_code)
4511         self.assertIn('Compute host host2 could not be found',
4512                       six.text_type(ex))
4513 
4514 
4515 class ServerTestV256SingleCellMultiHostTestCase(ServerTestV256Common):
4516     """Happy path test where we create a server on one host, migrate it to
4517     another host of our choosing and ensure it lands there.
4518     """
4519     def test_migrate_server_to_host_in_same_cell(self):
4520         server = self._create_server()
4521         server = self._wait_for_state_change(server, 'BUILD')
4522         source_host = server['OS-EXT-SRV-ATTR:host']
4523         target_host = self._get_target_and_other_hosts(source_host)[0]
4524         self.api.post_server_action(server['id'],
4525                                     {'migrate': {'host': target_host}})
4526         # Assert the server is now on the target host.
4527         server = self.api.get_server(server['id'])
4528         self.assertEqual(target_host, server['OS-EXT-SRV-ATTR:host'])
4529 
4530 
4531 class ServerTestV256RescheduleTestCase(ServerTestV256Common):
4532 
4533     @mock.patch.object(compute_manager.ComputeManager, '_prep_resize',
4534                        side_effect=exception.MigrationError(
4535                            reason='Test Exception'))
4536     def test_migrate_server_not_reschedule(self, mock_prep_resize):
4537         server = self._create_server()
4538         found_server = self._wait_for_state_change(server, 'BUILD')
4539 
4540         target_host, other_host = self._get_target_and_other_hosts(
4541             found_server['OS-EXT-SRV-ATTR:host'])
4542 
4543         self.assertRaises(client.OpenStackApiException,
4544                           self.api.post_server_action,
4545                           server['id'],
4546                           {'migrate': {'host': target_host}})
4547         self.assertEqual(1, mock_prep_resize.call_count)
4548         found_server = self.api.get_server(server['id'])
4549         # Check that rescheduling is not occurred.
4550         self.assertNotEqual(other_host, found_server['OS-EXT-SRV-ATTR:host'])
4551 
4552 
4553 class ConsumerGenerationConflictTest(
4554         integrated_helpers.ProviderUsageBaseTestCase):
4555 
4556     # we need the medium driver to be able to allocate resource not just for
4557     # a single instance
4558     compute_driver = 'fake.MediumFakeDriver'
4559 
4560     def setUp(self):
4561         super(ConsumerGenerationConflictTest, self).setUp()
4562         flavors = self.api.get_flavors()
4563         self.flavor = flavors[0]
4564         self.other_flavor = flavors[1]
4565         self.compute1 = self._start_compute('compute1')
4566         self.compute2 = self._start_compute('compute2')
4567 
4568     def test_create_server_fails_as_placement_reports_consumer_conflict(self):
4569         server_req = self._build_minimal_create_server_request(
4570             self.api, 'some-server', flavor_id=self.flavor['id'],
4571             image_uuid='155d900f-4e14-4e4c-a73d-069cbf4541e6',
4572             networks='none')
4573 
4574         # We cannot pre-create a consumer with the uuid of the instance created
4575         # below as that uuid is generated. Instead we have to simulate that
4576         # Placement returns 409, consumer generation conflict for the PUT
4577         # /allocation request the scheduler does for the instance.
4578         with mock.patch('keystoneauth1.adapter.Adapter.put') as mock_put:
4579             rsp = fake_requests.FakeResponse(
4580                 409,
4581                 jsonutils.dumps(
4582                     {'errors': [
4583                         {'code': 'placement.concurrent_update',
4584                          'detail': 'consumer generation conflict'}]}))
4585             mock_put.return_value = rsp
4586 
4587             created_server = self.api.post_server({'server': server_req})
4588             server = self._wait_for_state_change(
4589                 self.admin_api, created_server, 'ERROR')
4590 
4591         # This is not a conflict that the API user can ever resolve. It is a
4592         # serious inconsistency in our database or a bug in the scheduler code
4593         # doing the claim.
4594         self.assertEqual(500, server['fault']['code'])
4595         self.assertIn('Failed to update allocations for consumer',
4596                       server['fault']['message'])
4597 
4598         allocations = self._get_allocations_by_server_uuid(server['id'])
4599         self.assertEqual(0, len(allocations))
4600 
4601         self._delete_and_check_allocations(server)
4602 
4603     def test_migrate_claim_on_dest_fails(self):
4604         source_hostname = self.compute1.host
4605         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
4606 
4607         server = self._boot_and_check_allocations(self.flavor, source_hostname)
4608 
4609         # We have to simulate that Placement returns 409, consumer generation
4610         # conflict for the PUT /allocation request the scheduler does on the
4611         # destination host for the instance.
4612         with mock.patch('keystoneauth1.adapter.Adapter.put') as mock_put:
4613             rsp = fake_requests.FakeResponse(
4614                 409,
4615                 jsonutils.dumps(
4616                     {'errors': [
4617                         {'code': 'placement.concurrent_update',
4618                          'detail': 'consumer generation conflict'}]}))
4619             mock_put.return_value = rsp
4620 
4621             request = {'migrate': None}
4622             exception = self.assertRaises(client.OpenStackApiException,
4623                                           self.api.post_server_action,
4624                                           server['id'], request)
4625 
4626         # I know that HTTP 500 is harsh code but I think this conflict case
4627         # signals either a serious db inconsistency or a bug in nova's
4628         # claim code.
4629         self.assertEqual(500, exception.response.status_code)
4630 
4631         # The migration is aborted so the instance is ACTIVE on the source
4632         # host instead of being in VERIFY_RESIZE state.
4633         server = self.api.get_server(server['id'])
4634         self.assertEqual('ACTIVE', server['status'])
4635         self.assertEqual(source_hostname, server['OS-EXT-SRV-ATTR:host'])
4636 
4637         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor)
4638 
4639         self.assertFlavorMatchesAllocation(self.flavor, server['id'],
4640                                            source_rp_uuid)
4641 
4642         self._delete_and_check_allocations(server)
4643 
4644     def test_migrate_move_allocation_fails_due_to_conflict(self):
4645         source_hostname = self.compute1.host
4646         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
4647 
4648         server = self._boot_and_check_allocations(self.flavor, source_hostname)
4649 
4650         rsp = fake_requests.FakeResponse(
4651             409,
4652             jsonutils.dumps(
4653                 {'errors': [
4654                     {'code': 'placement.concurrent_update',
4655                      'detail': 'consumer generation conflict'}]}))
4656 
4657         with mock.patch('keystoneauth1.adapter.Adapter.post',
4658                         autospec=True) as mock_post:
4659             mock_post.return_value = rsp
4660 
4661             request = {'migrate': None}
4662             exception = self.assertRaises(client.OpenStackApiException,
4663                                           self.api.post_server_action,
4664                                           server['id'], request)
4665 
4666         self.assertEqual(1, mock_post.call_count)
4667 
4668         self.assertEqual(409, exception.response.status_code)
4669         self.assertIn('Failed to move allocations', exception.response.text)
4670 
4671         migrations = self.api.get_migrations()
4672         self.assertEqual(1, len(migrations))
4673         self.assertEqual('migration', migrations[0]['migration_type'])
4674         self.assertEqual(server['id'], migrations[0]['instance_uuid'])
4675         self.assertEqual(source_hostname, migrations[0]['source_compute'])
4676         self.assertEqual('error', migrations[0]['status'])
4677 
4678         # The migration is aborted so the instance is ACTIVE on the source
4679         # host instead of being in VERIFY_RESIZE state.
4680         server = self.api.get_server(server['id'])
4681         self.assertEqual('ACTIVE', server['status'])
4682         self.assertEqual(source_hostname, server['OS-EXT-SRV-ATTR:host'])
4683 
4684         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor)
4685 
4686         self.assertFlavorMatchesAllocation(self.flavor, server['id'],
4687                                            source_rp_uuid)
4688 
4689         self._delete_and_check_allocations(server)
4690 
4691     def test_confirm_migrate_delete_alloc_on_source_fails(self):
4692         source_hostname = self.compute1.host
4693         dest_hostname = self.compute2.host
4694         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
4695         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
4696 
4697         server = self._boot_and_check_allocations(self.flavor, source_hostname)
4698         self._migrate_and_check_allocations(
4699             server, self.flavor, source_rp_uuid, dest_rp_uuid)
4700 
4701         rsp = fake_requests.FakeResponse(
4702             409,
4703             jsonutils.dumps(
4704                 {'errors': [
4705                     {'code': 'placement.concurrent_update',
4706                      'detail': 'consumer generation conflict'}]}))
4707 
4708         with mock.patch('keystoneauth1.adapter.Adapter.put',
4709                         autospec=True) as mock_put:
4710             mock_put.return_value = rsp
4711 
4712             post = {'confirmResize': None}
4713             self.api.post_server_action(
4714                 server['id'], post, check_response_status=[204])
4715             server = self._wait_for_state_change(self.api, server, 'ERROR')
4716             self.assertIn('Failed to delete allocations',
4717                           server['fault']['message'])
4718 
4719         self.assertEqual(1, mock_put.call_count)
4720 
4721         migrations = self.api.get_migrations()
4722         self.assertEqual(1, len(migrations))
4723         self.assertEqual('migration', migrations[0]['migration_type'])
4724         self.assertEqual(server['id'], migrations[0]['instance_uuid'])
4725         self.assertEqual(source_hostname, migrations[0]['source_compute'])
4726         self.assertEqual('error', migrations[0]['status'])
4727 
4728         # NOTE(gibi): Nova leaks the allocation held by the migration_uuid even
4729         # after the instance is deleted. At least nova logs a fat ERROR.
4730         self.assertIn('Deleting allocation in placement for migration %s '
4731                       'failed. The instance %s will be put to ERROR state but '
4732                       'the allocation held by the migration is leaked.' %
4733                       (migrations[0]['uuid'], server['id']),
4734                       self.stdlog.logger.output)
4735         self.api.delete_server(server['id'])
4736         self._wait_until_deleted(server)
4737         fake_notifier.wait_for_versioned_notifications('instance.delete.end')
4738 
4739         allocations = self._get_allocations_by_server_uuid(
4740             migrations[0]['uuid'])
4741         self.assertEqual(1, len(allocations))
4742 
4743     def test_revert_migrate_delete_dest_allocation_fails_due_to_conflict(self):
4744         source_hostname = self.compute1.host
4745         dest_hostname = self.compute2.host
4746         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
4747         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
4748 
4749         server = self._boot_and_check_allocations(self.flavor, source_hostname)
4750         self._migrate_and_check_allocations(
4751             server, self.flavor, source_rp_uuid, dest_rp_uuid)
4752 
4753         rsp = fake_requests.FakeResponse(
4754             409,
4755             jsonutils.dumps(
4756                 {'errors': [
4757                     {'code': 'placement.concurrent_update',
4758                      'detail': 'consumer generation conflict'}]}))
4759 
4760         with mock.patch('keystoneauth1.adapter.Adapter.post',
4761                         autospec=True) as mock_post:
4762             mock_post.return_value = rsp
4763 
4764             post = {'revertResize': None}
4765             self.api.post_server_action(server['id'], post)
4766             server = self._wait_for_state_change(self.api, server, 'ERROR')
4767 
4768         self.assertEqual(1, mock_post.call_count)
4769 
4770         migrations = self.api.get_migrations()
4771         self.assertEqual(1, len(migrations))
4772         self.assertEqual('migration', migrations[0]['migration_type'])
4773         self.assertEqual(server['id'], migrations[0]['instance_uuid'])
4774         self.assertEqual(source_hostname, migrations[0]['source_compute'])
4775         self.assertEqual('error', migrations[0]['status'])
4776 
4777         # NOTE(gibi): Nova leaks the allocation held by the migration_uuid even
4778         # after the instance is deleted. At least nova logs a fat ERROR.
4779         self.assertIn('Reverting allocation in placement for migration %s '
4780                       'failed. The instance %s will be put into ERROR state '
4781                       'but the allocation held by the migration is leaked.' %
4782                       (migrations[0]['uuid'], server['id']),
4783                       self.stdlog.logger.output)
4784         self.api.delete_server(server['id'])
4785         self._wait_until_deleted(server)
4786         fake_notifier.wait_for_versioned_notifications('instance.delete.end')
4787 
4788         allocations = self._get_allocations_by_server_uuid(
4789             migrations[0]['uuid'])
4790         self.assertEqual(1, len(allocations))
4791 
4792     def test_revert_resize_same_host_delete_dest_fails_due_to_conflict(self):
4793         # make sure that the test only uses a single host
4794         compute2_service_id = self.admin_api.get_services(
4795             host=self.compute2.host, binary='nova-compute')[0]['id']
4796         self.admin_api.put_service(compute2_service_id, {'status': 'disabled'})
4797 
4798         hostname = self.compute1.manager.host
4799         rp_uuid = self._get_provider_uuid_by_host(hostname)
4800 
4801         server = self._boot_and_check_allocations(self.flavor, hostname)
4802 
4803         self._resize_to_same_host_and_check_allocations(
4804             server, self.flavor, self.other_flavor, rp_uuid)
4805 
4806         rsp = fake_requests.FakeResponse(
4807             409,
4808             jsonutils.dumps(
4809                 {'errors': [
4810                     {'code': 'placement.concurrent_update',
4811                      'detail': 'consumer generation conflict'}]}))
4812         with mock.patch('keystoneauth1.adapter.Adapter.post',
4813                         autospec=True) as mock_post:
4814             mock_post.return_value = rsp
4815 
4816             post = {'revertResize': None}
4817             self.api.post_server_action(server['id'], post)
4818             server = self._wait_for_state_change(self.api, server, 'ERROR',)
4819 
4820         self.assertEqual(1, mock_post.call_count)
4821 
4822         migrations = self.api.get_migrations()
4823         self.assertEqual(1, len(migrations))
4824         self.assertEqual('resize', migrations[0]['migration_type'])
4825         self.assertEqual(server['id'], migrations[0]['instance_uuid'])
4826         self.assertEqual(hostname, migrations[0]['source_compute'])
4827         self.assertEqual('error', migrations[0]['status'])
4828 
4829         # NOTE(gibi): Nova leaks the allocation held by the migration_uuid even
4830         # after the instance is deleted. At least nova logs a fat ERROR.
4831         self.assertIn('Reverting allocation in placement for migration %s '
4832                       'failed. The instance %s will be put into ERROR state '
4833                       'but the allocation held by the migration is leaked.' %
4834                       (migrations[0]['uuid'], server['id']),
4835                       self.stdlog.logger.output)
4836         self.api.delete_server(server['id'])
4837         self._wait_until_deleted(server)
4838         fake_notifier.wait_for_versioned_notifications('instance.delete.end')
4839 
4840         allocations = self._get_allocations_by_server_uuid(
4841             migrations[0]['uuid'])
4842         self.assertEqual(1, len(allocations))
4843 
4844     def test_force_live_migrate_claim_on_dest_fails(self):
4845         # Normal live migrate moves source allocation from instance to
4846         # migration like a normal migrate tested above.
4847         # Normal live migrate claims on dest like a normal boot tested above.
4848         source_hostname = self.compute1.host
4849         dest_hostname = self.compute2.host
4850 
4851         # the ability to force live migrate a server is removed entirely in
4852         # 2.68
4853         self.api.microversion = '2.67'
4854 
4855         server = self._boot_and_check_allocations(
4856             self.flavor, source_hostname)
4857 
4858         rsp = fake_requests.FakeResponse(
4859             409,
4860             jsonutils.dumps(
4861                 {'errors': [
4862                     {'code': 'placement.concurrent_update',
4863                      'detail': 'consumer generation conflict'}]}))
4864         with mock.patch('keystoneauth1.adapter.Adapter.put',
4865                         autospec=True) as mock_put:
4866             mock_put.return_value = rsp
4867 
4868             post = {
4869                 'os-migrateLive': {
4870                     'host': dest_hostname,
4871                     'block_migration': True,
4872                     'force': True,
4873                 }
4874             }
4875 
4876             self.api.post_server_action(server['id'], post)
4877             server = self._wait_for_state_change(self.api, server, 'ERROR')
4878 
4879         self.assertEqual(1, mock_put.call_count)
4880 
4881         # This is not a conflict that the API user can ever resolve. It is a
4882         # serious inconsistency in our database or a bug in the scheduler code
4883         # doing the claim.
4884         self.assertEqual(500, server['fault']['code'])
4885         # The instance is in ERROR state so the allocations are in limbo but
4886         # at least we expect that when the instance is deleted the allocations
4887         # are cleaned up properly.
4888         self._delete_and_check_allocations(server)
4889 
4890     def test_live_migrate_drop_allocation_on_source_fails(self):
4891         source_hostname = self.compute1.host
4892         dest_hostname = self.compute2.host
4893         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
4894         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
4895 
4896         # the ability to force live migrate a server is removed entirely in
4897         # 2.68
4898         self.api.microversion = '2.67'
4899 
4900         server = self._boot_and_check_allocations(
4901             self.flavor, source_hostname)
4902 
4903         fake_notifier.stub_notifier(self)
4904         self.addCleanup(fake_notifier.reset)
4905 
4906         orig_put = adapter.Adapter.put
4907 
4908         rsp = fake_requests.FakeResponse(
4909             409,
4910             jsonutils.dumps(
4911                 {'errors': [
4912                     {'code': 'placement.concurrent_update',
4913                      'detail': 'consumer generation conflict'}]}))
4914 
4915         self.adapter_put_call_count = 0
4916 
4917         def fake_put(_self, url, **kwargs):
4918             self.adapter_put_call_count += 1
4919             migration_uuid = self.get_migration_uuid_for_instance(server['id'])
4920             if url == '/allocations/%s' % migration_uuid:
4921                 return rsp
4922             else:
4923                 return orig_put(_self, url, **kwargs)
4924 
4925         with mock.patch('keystoneauth1.adapter.Adapter.put', new=fake_put):
4926             post = {
4927                 'os-migrateLive': {
4928                     'host': dest_hostname,
4929                     'block_migration': True,
4930                     'force': True,
4931                 }
4932             }
4933 
4934             self.api.post_server_action(server['id'], post)
4935 
4936             # nova does the source host cleanup _after_ setting the migration
4937             # to completed and sending end notifications so we have to wait
4938             # here a bit.
4939             time.sleep(1)
4940 
4941             # Nova failed to clean up on the source host. This right now puts
4942             # the instance to ERROR state and fails the migration.
4943             server = self._wait_for_server_parameter(self.api, server,
4944                 {'OS-EXT-SRV-ATTR:host': dest_hostname,
4945                  'status': 'ERROR'})
4946             self._wait_for_migration_status(server, ['error'])
4947             fake_notifier.wait_for_versioned_notifications(
4948                 'instance.live_migration_post.end')
4949 
4950         # 1 claim on destination, 1 normal delete on dest that fails,
4951         self.assertEqual(2, self.adapter_put_call_count)
4952 
4953         # As the cleanup on the source host failed Nova leaks the allocation
4954         # held by the migration.
4955         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor)
4956         migration_uuid = self.get_migration_uuid_for_instance(server['id'])
4957         self.assertFlavorMatchesAllocation(self.flavor, migration_uuid,
4958                                            source_rp_uuid)
4959 
4960         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor)
4961 
4962         self.assertFlavorMatchesAllocation(self.flavor, server['id'],
4963                                            dest_rp_uuid)
4964 
4965         # NOTE(gibi): Nova leaks the allocation held by the migration_uuid even
4966         # after the instance is deleted. At least nova logs a fat ERROR.
4967         self.assertIn('Deleting allocation in placement for migration %s '
4968                       'failed. The instance %s will be put to ERROR state but '
4969                       'the allocation held by the migration is leaked.' %
4970                       (migration_uuid, server['id']),
4971                       self.stdlog.logger.output)
4972 
4973         self.api.delete_server(server['id'])
4974         self._wait_until_deleted(server)
4975         fake_notifier.wait_for_versioned_notifications('instance.delete.end')
4976 
4977         self.assertFlavorMatchesAllocation(self.flavor, migration_uuid,
4978                                            source_rp_uuid)
4979 
4980     def _test_evacuate_fails_allocating_on_dest_host(self, force):
4981         source_hostname = self.compute1.host
4982         dest_hostname = self.compute2.host
4983 
4984         # the ability to force evacuate a server is removed entirely in 2.68
4985         self.api.microversion = '2.67'
4986 
4987         server = self._boot_and_check_allocations(
4988             self.flavor, source_hostname)
4989 
4990         source_compute_id = self.admin_api.get_services(
4991             host=source_hostname, binary='nova-compute')[0]['id']
4992 
4993         self.compute1.stop()
4994         # force it down to avoid waiting for the service group to time out
4995         self.admin_api.put_service(
4996             source_compute_id, {'forced_down': 'true'})
4997 
4998         rsp = fake_requests.FakeResponse(
4999             409,
5000             jsonutils.dumps(
5001                 {'errors': [
5002                     {'code': 'placement.concurrent_update',
5003                      'detail': 'consumer generation conflict'}]}))
5004 
5005         with mock.patch('keystoneauth1.adapter.Adapter.put',
5006                         autospec=True) as mock_put:
5007             mock_put.return_value = rsp
5008             post = {
5009                 'evacuate': {
5010                     'force': force
5011                 }
5012             }
5013             if force:
5014                 post['evacuate']['host'] = dest_hostname
5015 
5016             self.api.post_server_action(server['id'], post)
5017             server = self._wait_for_state_change(self.api, server, 'ERROR')
5018 
5019         self.assertEqual(1, mock_put.call_count)
5020 
5021         # As nova failed to allocate on the dest host we only expect allocation
5022         # on the source
5023         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
5024         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
5025 
5026         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor)
5027 
5028         self.assertRequestMatchesUsage({'VCPU': 0,
5029                                         'MEMORY_MB': 0,
5030                                         'DISK_GB': 0}, dest_rp_uuid)
5031 
5032         self.assertFlavorMatchesAllocation(self.flavor, server['id'],
5033                                            source_rp_uuid)
5034 
5035         self._delete_and_check_allocations(server)
5036 
5037     def test_force_evacuate_fails_allocating_on_dest_host(self):
5038         self._test_evacuate_fails_allocating_on_dest_host(force=True)
5039 
5040     def test_evacuate_fails_allocating_on_dest_host(self):
5041         self._test_evacuate_fails_allocating_on_dest_host(force=False)
5042 
5043     def test_server_delete_fails_due_to_conflict(self):
5044         source_hostname = self.compute1.host
5045 
5046         server = self._boot_and_check_allocations(self.flavor, source_hostname)
5047 
5048         rsp = fake_requests.FakeResponse(
5049             409, jsonutils.dumps({'text': 'consumer generation conflict'}))
5050 
5051         with mock.patch('keystoneauth1.adapter.Adapter.put',
5052                         autospec=True) as mock_put:
5053             mock_put.return_value = rsp
5054 
5055             self.api.delete_server(server['id'])
5056             server = self._wait_for_state_change(self.admin_api, server,
5057                                                  'ERROR')
5058             self.assertEqual(1, mock_put.call_count)
5059 
5060         # We still have the allocations as deletion failed
5061         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
5062         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor)
5063 
5064         self.assertFlavorMatchesAllocation(self.flavor, server['id'],
5065                                            source_rp_uuid)
5066 
5067         # retry the delete to make sure that allocations are removed this time
5068         self._delete_and_check_allocations(server)
5069 
5070     def test_server_local_delete_fails_due_to_conflict(self):
5071         source_hostname = self.compute1.host
5072 
5073         server = self._boot_and_check_allocations(self.flavor, source_hostname)
5074         source_compute_id = self.admin_api.get_services(
5075             host=self.compute1.host, binary='nova-compute')[0]['id']
5076         self.compute1.stop()
5077         self.admin_api.put_service(
5078             source_compute_id, {'forced_down': 'true'})
5079 
5080         rsp = fake_requests.FakeResponse(
5081             409, jsonutils.dumps({'text': 'consumer generation conflict'}))
5082 
5083         with mock.patch('keystoneauth1.adapter.Adapter.put',
5084                         autospec=True) as mock_put:
5085             mock_put.return_value = rsp
5086 
5087             ex = self.assertRaises(client.OpenStackApiException,
5088                                    self.api.delete_server, server['id'])
5089             self.assertEqual(409, ex.response.status_code)
5090             self.assertIn('Failed to delete allocations for consumer',
5091                           jsonutils.loads(ex.response.content)[
5092                               'conflictingRequest']['message'])
5093             self.assertEqual(1, mock_put.call_count)
5094 
5095         # We still have the allocations as deletion failed
5096         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
5097         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor)
5098 
5099         self.assertFlavorMatchesAllocation(self.flavor, server['id'],
5100                                            source_rp_uuid)
5101 
5102         # retry the delete to make sure that allocations are removed this time
5103         self._delete_and_check_allocations(server)
5104 
5105 
5106 class ServerMovingTestsWithNestedComputes(ServerMovingTests):
5107     """Runs all the server moving tests while the computes have nested trees.
5108     The servers still do not request resources from any child provider though.
5109     """
5110     compute_driver = 'fake.MediumFakeDriverWithNestedCustomResources'
5111 
5112 
5113 class ServerMovingTestsWithNestedResourceRequests(
5114     ServerMovingTestsWithNestedComputes):
5115     """Runs all the server moving tests while the computes have nested trees.
5116     The servers also request resources from child providers.
5117     """
5118 
5119     def setUp(self):
5120         super(ServerMovingTestsWithNestedResourceRequests, self).setUp()
5121         # modify the flavors used in the ServerMoving test base class to
5122         # require one piece of CUSTOM_MAGIC resource as well.
5123 
5124         for flavor in [self.flavor1, self.flavor2, self.flavor3]:
5125             self.api.post_extra_spec(
5126                 flavor['id'], {'extra_specs': {'resources:CUSTOM_MAGIC': 1}})
5127             # save the extra_specs in the flavor stored in the test case as
5128             # well
5129             flavor['extra_specs'] = {'resources:CUSTOM_MAGIC': 1}
5130 
5131     def _check_allocation_during_evacuate(
5132             self, flavor, server_uuid, source_root_rp_uuid, dest_root_rp_uuid):
5133         # NOTE(gibi): evacuate is the only case when the same consumer has
5134         # allocation from two different RP trees so we need a special check
5135         # here.
5136         allocations = self._get_allocations_by_server_uuid(server_uuid)
5137         source_rps = self._get_all_rp_uuids_in_a_tree(source_root_rp_uuid)
5138         dest_rps = self._get_all_rp_uuids_in_a_tree(dest_root_rp_uuid)
5139 
5140         self.assertEqual(set(source_rps + dest_rps), set(allocations))
5141 
5142         total_source_allocation = collections.defaultdict(int)
5143         total_dest_allocation = collections.defaultdict(int)
5144         for rp, alloc in allocations.items():
5145             for rc, value in alloc['resources'].items():
5146                 if rp in source_rps:
5147                     total_source_allocation[rc] += value
5148                 else:
5149                     total_dest_allocation[rc] += value
5150 
5151         self.assertEqual(
5152             self._resources_from_flavor(flavor), total_source_allocation)
5153         self.assertEqual(
5154             self._resources_from_flavor(flavor), total_dest_allocation)
5155 
5156     def test_live_migrate_force(self):
5157         # Nova intentionally does not support force live-migrating server
5158         # with nested allocations.
5159 
5160         source_hostname = self.compute1.host
5161         dest_hostname = self.compute2.host
5162         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
5163         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
5164 
5165         # the ability to force live migrate a server is removed entirely in
5166         # 2.68
5167         self.api.microversion = '2.67'
5168 
5169         server = self._boot_and_check_allocations(
5170             self.flavor1, source_hostname)
5171         post = {
5172             'os-migrateLive': {
5173                 'host': dest_hostname,
5174                 'block_migration': True,
5175                 'force': True,
5176             }
5177         }
5178 
5179         self.api.post_server_action(server['id'], post)
5180         self._wait_for_migration_status(server, ['error'])
5181         self._wait_for_server_parameter(self.api, server,
5182             {'OS-EXT-SRV-ATTR:host': source_hostname,
5183              'status': 'ACTIVE'})
5184         self.assertIn('Unable to move instance %s to host host2. The instance '
5185                       'has complex allocations on the source host so move '
5186                       'cannot be forced.' %
5187                       server['id'],
5188                       self.stdlog.logger.output)
5189 
5190         self._run_periodics()
5191 
5192         # NOTE(danms): There should be no usage for the dest
5193         self.assertRequestMatchesUsage(
5194             {'VCPU': 0,
5195              'MEMORY_MB': 0,
5196              'DISK_GB': 0}, dest_rp_uuid)
5197 
5198         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
5199 
5200         # the server has an allocation on only the source node
5201         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
5202                                            source_rp_uuid)
5203 
5204         self._delete_and_check_allocations(server)
5205 
5206     def _test_evacuate_forced_host(self, keep_hypervisor_state):
5207         # Nova intentionally does not support force evacuating server
5208         # with nested allocations.
5209 
5210         source_hostname = self.compute1.host
5211         dest_hostname = self.compute2.host
5212 
5213         # the ability to force evacuate a server is removed entirely in 2.68
5214         self.api.microversion = '2.67'
5215 
5216         server = self._boot_and_check_allocations(
5217             self.flavor1, source_hostname)
5218 
5219         source_compute_id = self.admin_api.get_services(
5220             host=source_hostname, binary='nova-compute')[0]['id']
5221 
5222         self.compute1.stop()
5223         # force it down to avoid waiting for the service group to time out
5224         self.admin_api.put_service(
5225             source_compute_id, {'forced_down': 'true'})
5226 
5227         # evacuate the server and force the destination host which bypasses
5228         # the scheduler
5229         post = {
5230             'evacuate': {
5231                 'host': dest_hostname,
5232                 'force': True
5233             }
5234         }
5235         self.api.post_server_action(server['id'], post)
5236         self._wait_for_migration_status(server, ['error'])
5237         expected_params = {'OS-EXT-SRV-ATTR:host': source_hostname,
5238                            'status': 'ACTIVE'}
5239         server = self._wait_for_server_parameter(self.api, server,
5240                                                  expected_params)
5241         self.assertIn('Unable to move instance %s to host host2. The instance '
5242                       'has complex allocations on the source host so move '
5243                       'cannot be forced.' %
5244                       server['id'],
5245                       self.stdlog.logger.output)
5246 
5247         # Run the periodics to show those don't modify allocations.
5248         self._run_periodics()
5249 
5250         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
5251         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
5252 
5253         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
5254 
5255         self.assertRequestMatchesUsage(
5256             {'VCPU': 0,
5257              'MEMORY_MB': 0,
5258              'DISK_GB': 0}, dest_rp_uuid)
5259 
5260         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
5261                                            source_rp_uuid)
5262 
5263         # restart the source compute
5264         self.compute1 = self.restart_compute_service(
5265             self.compute1, keep_hypervisor_state=keep_hypervisor_state)
5266         self.admin_api.put_service(
5267             source_compute_id, {'forced_down': 'false'})
5268 
5269         # Run the periodics again to show they don't change anything.
5270         self._run_periodics()
5271 
5272         # When the source node starts up nothing should change as the
5273         # evacuation failed
5274         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
5275 
5276         self.assertRequestMatchesUsage(
5277             {'VCPU': 0,
5278              'MEMORY_MB': 0,
5279              'DISK_GB': 0}, dest_rp_uuid)
5280 
5281         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
5282                                            source_rp_uuid)
5283 
5284         self._delete_and_check_allocations(server)
5285 
5286 
5287 # NOTE(gibi): There is another case NestedToFlat but that leads to the same
5288 # code path that NestedToNested as in both cases the instance will have
5289 # complex allocation on the source host which is already covered in
5290 # ServerMovingTestsWithNestedResourceRequests
5291 class ServerMovingTestsFromFlatToNested(
5292         integrated_helpers.ProviderUsageBaseTestCase):
5293     """Tests trying to move servers from a compute with a flat RP tree to a
5294     compute with a nested RP tree and assert that the blind allocation copy
5295     fails cleanly.
5296     """
5297 
5298     REQUIRES_LOCKING = True
5299     compute_driver = 'fake.MediumFakeDriver'
5300 
5301     def setUp(self):
5302         super(ServerMovingTestsFromFlatToNested, self).setUp()
5303         flavors = self.api.get_flavors()
5304         self.flavor1 = flavors[0]
5305         self.api.post_extra_spec(
5306             self.flavor1['id'], {'extra_specs': {'resources:CUSTOM_MAGIC': 1}})
5307         self.flavor1['extra_specs'] = {'resources:CUSTOM_MAGIC': 1}
5308 
5309     def test_force_live_migrate_from_flat_to_nested(self):
5310         # first compute will start with the flat RP tree but we add
5311         # CUSTOM_MAGIC inventory to the root compute RP
5312         orig_update_provider_tree = fake.MediumFakeDriver.update_provider_tree
5313 
5314         # the ability to force live migrate a server is removed entirely in
5315         # 2.68
5316         self.api.microversion = '2.67'
5317 
5318         def stub_update_provider_tree(self, provider_tree, nodename,
5319                                       allocations=None):
5320             # do the regular inventory update
5321             orig_update_provider_tree(
5322                 self, provider_tree, nodename, allocations)
5323             if nodename == 'host1':
5324                 # add the extra resource
5325                 inv = provider_tree.data(nodename).inventory
5326                 inv['CUSTOM_MAGIC'] = {
5327                     'total': 10,
5328                     'reserved': 0,
5329                     'min_unit': 1,
5330                     'max_unit': 10,
5331                     'step_size': 1,
5332                     'allocation_ratio': 1,
5333                 }
5334                 provider_tree.update_inventory(nodename, inv)
5335 
5336         self.stub_out('nova.virt.fake.FakeDriver.update_provider_tree',
5337                       stub_update_provider_tree)
5338         self.compute1 = self._start_compute(host='host1')
5339         source_rp_uuid = self._get_provider_uuid_by_host('host1')
5340 
5341         server = self._boot_and_check_allocations(self.flavor1, 'host1')
5342         # start the second compute with nested RP tree
5343         self.flags(
5344             compute_driver='fake.MediumFakeDriverWithNestedCustomResources')
5345         self.compute2 = self._start_compute(host='host2')
5346 
5347         # try to force live migrate from flat to nested.
5348         post = {
5349             'os-migrateLive': {
5350                 'host': 'host2',
5351                 'block_migration': True,
5352                 'force': True,
5353             }
5354         }
5355 
5356         self.api.post_server_action(server['id'], post)
5357         # We expect that the migration will fail as force migrate tries to
5358         # blindly copy the source allocation to the destination but on the
5359         # destination there is no inventory of CUSTOM_MAGIC on the compute node
5360         # provider as that resource is reported on a child provider.
5361         self._wait_for_server_parameter(self.api, server,
5362             {'OS-EXT-SRV-ATTR:host': 'host1',
5363              'status': 'ACTIVE'})
5364 
5365         migration = self._wait_for_migration_status(server, ['error'])
5366         self.assertEqual('host1', migration['source_compute'])
5367         self.assertEqual('host2', migration['dest_compute'])
5368 
5369         # Nova fails the migration because it ties to allocation CUSTOM_MAGIC
5370         # from the dest node root RP and placement rejects the that allocation.
5371         self.assertIn("Unable to allocate inventory: Inventory for "
5372                       "'CUSTOM_MAGIC'", self.stdlog.logger.output)
5373         self.assertIn('No valid host was found. Unable to move instance %s to '
5374                       'host host2. There is not enough capacity on the host '
5375                       'for the instance.' % server['id'],
5376                       self.stdlog.logger.output)
5377 
5378         dest_rp_uuid = self._get_provider_uuid_by_host('host2')
5379 
5380         # There should be no usage for the dest
5381         self.assertRequestMatchesUsage(
5382             {'VCPU': 0,
5383              'MEMORY_MB': 0,
5384              'DISK_GB': 0}, dest_rp_uuid)
5385 
5386         # and everything stays at the source
5387         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
5388         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
5389                                            source_rp_uuid)
5390 
5391         self._delete_and_check_allocations(server)
5392 
5393     def test_force_evacuate_from_flat_to_nested(self):
5394         # first compute will start with the flat RP tree but we add
5395         # CUSTOM_MAGIC inventory to the root compute RP
5396         orig_update_provider_tree = fake.MediumFakeDriver.update_provider_tree
5397 
5398         # the ability to force evacuate a server is removed entirely in 2.68
5399         self.api.microversion = '2.67'
5400 
5401         def stub_update_provider_tree(self, provider_tree, nodename,
5402                                       allocations=None):
5403             # do the regular inventory update
5404             orig_update_provider_tree(
5405                 self, provider_tree, nodename, allocations)
5406             if nodename == 'host1':
5407                 # add the extra resource
5408                 inv = provider_tree.data(nodename).inventory
5409                 inv['CUSTOM_MAGIC'] = {
5410                     'total': 10,
5411                     'reserved': 0,
5412                     'min_unit': 1,
5413                     'max_unit': 10,
5414                     'step_size': 1,
5415                     'allocation_ratio': 1,
5416                 }
5417                 provider_tree.update_inventory(nodename, inv)
5418 
5419         self.stub_out('nova.virt.fake.FakeDriver.update_provider_tree',
5420                       stub_update_provider_tree)
5421         self.compute1 = self._start_compute(host='host1')
5422         source_rp_uuid = self._get_provider_uuid_by_host('host1')
5423 
5424         server = self._boot_and_check_allocations(self.flavor1, 'host1')
5425         # start the second compute with nested RP tree
5426         self.flags(
5427             compute_driver='fake.MediumFakeDriverWithNestedCustomResources')
5428         self.compute2 = self._start_compute(host='host2')
5429 
5430         source_compute_id = self.admin_api.get_services(
5431             host='host1', binary='nova-compute')[0]['id']
5432         self.compute1.stop()
5433         # force it down to avoid waiting for the service group to time out
5434         self.admin_api.put_service(
5435             source_compute_id, {'forced_down': 'true'})
5436 
5437         # try to force evacuate from flat to nested.
5438         post = {
5439             'evacuate': {
5440                 'host': 'host2',
5441                 'force': True,
5442             }
5443         }
5444 
5445         self.api.post_server_action(server['id'], post)
5446         # We expect that the evacuation will fail as force evacuate tries to
5447         # blindly copy the source allocation to the destination but on the
5448         # destination there is no inventory of CUSTOM_MAGIC on the compute node
5449         # provider as that resource is reported on a child provider.
5450         self._wait_for_server_parameter(self.api, server,
5451             {'OS-EXT-SRV-ATTR:host': 'host1',
5452              'status': 'ACTIVE'})
5453 
5454         migration = self._wait_for_migration_status(server, ['error'])
5455         self.assertEqual('host1', migration['source_compute'])
5456         self.assertEqual('host2', migration['dest_compute'])
5457 
5458         # Nova fails the migration because it ties to allocation CUSTOM_MAGIC
5459         # from the dest node root RP and placement rejects the that allocation.
5460         self.assertIn("Unable to allocate inventory: Inventory for "
5461                       "'CUSTOM_MAGIC'", self.stdlog.logger.output)
5462         self.assertIn('No valid host was found. Unable to move instance %s to '
5463                       'host host2. There is not enough capacity on the host '
5464                       'for the instance.' % server['id'],
5465                       self.stdlog.logger.output)
5466 
5467         dest_rp_uuid = self._get_provider_uuid_by_host('host2')
5468 
5469         # There should be no usage for the dest
5470         self.assertRequestMatchesUsage(
5471             {'VCPU': 0,
5472              'MEMORY_MB': 0,
5473              'DISK_GB': 0}, dest_rp_uuid)
5474 
5475         # and everything stays at the source
5476         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
5477         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
5478                                            source_rp_uuid)
5479 
5480         self._delete_and_check_allocations(server)
5481 
5482 
5483 class PortResourceRequestBasedSchedulingTestBase(
5484         integrated_helpers.ProviderUsageBaseTestCase):
5485 
5486     compute_driver = 'fake.FakeDriverWithPciResources'
5487 
5488     CUSTOM_VNIC_TYPE_NORMAL = 'CUSTOM_VNIC_TYPE_NORMAL'
5489     CUSTOM_VNIC_TYPE_DIRECT = 'CUSTOM_VNIC_TYPE_DIRECT'
5490     CUSTOM_VNIC_TYPE_MACVTAP = 'CUSTOM_VNIC_TYPE_MACVTAP'
5491     CUSTOM_PHYSNET1 = 'CUSTOM_PHYSNET1'
5492     CUSTOM_PHYSNET2 = 'CUSTOM_PHYSNET2'
5493     CUSTOM_PHYSNET3 = 'CUSTOM_PHYSNET3'
5494 
5495     def setUp(self):
5496         # enable PciPassthroughFilter to support SRIOV before the base class
5497         # starts the scheduler
5498         if 'PciPassthroughFilter' not in CONF.filter_scheduler.enabled_filters:
5499             self.flags(
5500                 enabled_filters=CONF.filter_scheduler.enabled_filters +
5501                                 ['PciPassthroughFilter'],
5502                 group='filter_scheduler')
5503 
5504         self.useFixture(
5505             fake.FakeDriverWithPciResources.
5506                 FakeDriverWithPciResourcesConfigFixture())
5507 
5508         super(PortResourceRequestBasedSchedulingTestBase, self).setUp()
5509         self.compute1 = self._start_compute('host1')
5510         self.compute1_rp_uuid = self._get_provider_uuid_by_host('host1')
5511         self.ovs_bridge_rp_per_host = {}
5512         self.sriov_dev_rp_per_host = {}
5513         self.flavor = self.api.get_flavors()[0]
5514         self.flavor_with_group_policy = self.api.get_flavors()[1]
5515 
5516         # Setting group policy for placement. This is mandatory when more than
5517         # one request group is included in the allocation candidate request and
5518         # we have tests with two ports both having resource request modelled as
5519         # two separate request groups.
5520         self.admin_api.post_extra_spec(
5521             self.flavor_with_group_policy['id'],
5522             {'extra_specs': {'group_policy': 'isolate'}})
5523 
5524         self._create_networking_rp_tree('host1', self.compute1_rp_uuid)
5525 
5526         # add extra ports and the related network to the neutron fixture
5527         # specifically for these tests. It cannot be added globally in the
5528         # fixture init as it adds a second network that makes auto allocation
5529         # based test to fail due to ambiguous networks.
5530         self.neutron._ports[
5531             self.neutron.port_with_sriov_resource_request['id']] = \
5532             copy.deepcopy(self.neutron.port_with_sriov_resource_request)
5533         self.neutron._ports[self.neutron.sriov_port['id']] = \
5534             copy.deepcopy(self.neutron.sriov_port)
5535         self.neutron._networks[
5536             self.neutron.network_2['id']] = self.neutron.network_2
5537         self.neutron._subnets[
5538             self.neutron.subnet_2['id']] = self.neutron.subnet_2
5539         macvtap = self.neutron.port_macvtap_with_resource_request
5540         self.neutron._ports[macvtap['id']] = copy.deepcopy(macvtap)
5541 
5542     def assertComputeAllocationMatchesFlavor(
5543             self, allocations, compute_rp_uuid, flavor):
5544         compute_allocations = allocations[compute_rp_uuid]['resources']
5545         self.assertEqual(
5546             self._resources_from_flavor(flavor),
5547             compute_allocations)
5548 
5549     def _create_server(self, flavor, networks, host=None):
5550         server_req = self._build_minimal_create_server_request(
5551             self.api, 'bandwidth-aware-server',
5552             image_uuid='76fa36fc-c930-4bf3-8c8a-ea2a2420deb6',
5553             flavor_id=flavor['id'], networks=networks,
5554             host=host)
5555         return self.api.post_server({'server': server_req})
5556 
5557     def _set_provider_inventories(self, rp_uuid, inventories):
5558         rp = self.placement_api.get(
5559             '/resource_providers/%s' % rp_uuid).body
5560         inventories['resource_provider_generation'] = rp['generation']
5561         return self._update_inventory(rp_uuid, inventories)
5562 
5563     def _create_ovs_networking_rp_tree(self, compute_rp_uuid):
5564         # we need uuid sentinel for the test to make pep8 happy but we need a
5565         # unique one per compute so here is some ugliness
5566         ovs_agent_rp_uuid = getattr(uuids, compute_rp_uuid + 'ovs agent')
5567         agent_rp_req = {
5568             "name": ovs_agent_rp_uuid,
5569             "uuid": ovs_agent_rp_uuid,
5570             "parent_provider_uuid": compute_rp_uuid
5571         }
5572         self.placement_api.post('/resource_providers',
5573                                 body=agent_rp_req,
5574                                 version='1.20')
5575         ovs_bridge_rp_uuid = getattr(uuids, ovs_agent_rp_uuid + 'ovs br')
5576         ovs_bridge_req = {
5577             "name": ovs_bridge_rp_uuid,
5578             "uuid": ovs_bridge_rp_uuid,
5579             "parent_provider_uuid": ovs_agent_rp_uuid
5580         }
5581         self.placement_api.post('/resource_providers',
5582                                 body=ovs_bridge_req,
5583                                 version='1.20')
5584         self.ovs_bridge_rp_per_host[compute_rp_uuid] = ovs_bridge_rp_uuid
5585 
5586         self._set_provider_inventories(
5587             ovs_bridge_rp_uuid,
5588             {"inventories": {
5589                 orc.NET_BW_IGR_KILOBIT_PER_SEC: {"total": 10000},
5590                 orc.NET_BW_EGR_KILOBIT_PER_SEC: {"total": 10000},
5591             }})
5592 
5593         self._create_trait(self.CUSTOM_VNIC_TYPE_NORMAL)
5594         self._create_trait(self.CUSTOM_PHYSNET2)
5595 
5596         self._set_provider_traits(
5597             ovs_bridge_rp_uuid,
5598             [self.CUSTOM_VNIC_TYPE_NORMAL, self.CUSTOM_PHYSNET2])
5599 
5600     def _create_pf_device_rp(
5601             self, device_rp_uuid, parent_rp_uuid, inventories, traits,
5602             device_rp_name=None):
5603         """Create a RP in placement for a physical function network device with
5604         traits and inventories.
5605         """
5606 
5607         if not device_rp_name:
5608             device_rp_name = device_rp_uuid
5609 
5610         sriov_pf_req = {
5611             "name": device_rp_name,
5612             "uuid": device_rp_uuid,
5613             "parent_provider_uuid": parent_rp_uuid
5614         }
5615         self.placement_api.post('/resource_providers',
5616                                 body=sriov_pf_req,
5617                                 version='1.20')
5618 
5619         self._set_provider_inventories(
5620             device_rp_uuid,
5621             {"inventories": inventories})
5622 
5623         for trait in traits:
5624             self._create_trait(trait)
5625 
5626         self._set_provider_traits(
5627             device_rp_uuid,
5628             traits)
5629 
5630     def _create_sriov_networking_rp_tree(self, hostname, compute_rp_uuid):
5631         # Create a matching RP tree in placement for the PCI devices added to
5632         # the passthrough_whitelist config during setUp() and PCI devices
5633         # present in the FakeDriverWithPciResources virt driver.
5634         #
5635         # * PF1 represents the PCI device 0000:01:00, it will be mapped to
5636         # physnet1 and it will have bandwidth inventory.
5637         # * PF2 represents the PCI device 0000:02:00, it will be mapped to
5638         # physnet2 it will have bandwidth inventory.
5639         # * PF3 represents the PCI device 0000:03:00 and, it will be mapped to
5640         # physnet2 but it will not have bandwidth inventory.
5641         self.sriov_dev_rp_per_host[compute_rp_uuid] = {}
5642 
5643         sriov_agent_rp_uuid = getattr(uuids, compute_rp_uuid + 'sriov agent')
5644         agent_rp_req = {
5645             "name": "%s:NIC Switch agent" % hostname,
5646             "uuid": sriov_agent_rp_uuid,
5647             "parent_provider_uuid": compute_rp_uuid
5648         }
5649         self.placement_api.post('/resource_providers',
5650                                 body=agent_rp_req,
5651                                 version='1.20')
5652         dev_rp_name_prefix = ("%s:NIC Switch agent:" % hostname)
5653 
5654         sriov_pf1_rp_uuid = getattr(uuids, sriov_agent_rp_uuid + 'PF1')
5655         self.sriov_dev_rp_per_host[compute_rp_uuid]['pf1'] = sriov_pf1_rp_uuid
5656 
5657         inventories = {
5658             orc.NET_BW_IGR_KILOBIT_PER_SEC: {"total": 100000},
5659             orc.NET_BW_EGR_KILOBIT_PER_SEC: {"total": 100000},
5660         }
5661         traits = [self.CUSTOM_VNIC_TYPE_DIRECT, self.CUSTOM_PHYSNET1]
5662         self._create_pf_device_rp(
5663             sriov_pf1_rp_uuid, sriov_agent_rp_uuid, inventories, traits,
5664             device_rp_name=dev_rp_name_prefix + "%s-ens1" % hostname)
5665 
5666         sriov_pf2_rp_uuid = getattr(uuids, sriov_agent_rp_uuid + 'PF2')
5667         self.sriov_dev_rp_per_host[compute_rp_uuid]['pf2'] = sriov_pf2_rp_uuid
5668         inventories = {
5669             orc.NET_BW_IGR_KILOBIT_PER_SEC: {"total": 100000},
5670             orc.NET_BW_EGR_KILOBIT_PER_SEC: {"total": 100000},
5671         }
5672         traits = [self.CUSTOM_VNIC_TYPE_DIRECT, self.CUSTOM_VNIC_TYPE_MACVTAP,
5673                   self.CUSTOM_PHYSNET2]
5674         self._create_pf_device_rp(
5675             sriov_pf2_rp_uuid, sriov_agent_rp_uuid, inventories, traits,
5676             device_rp_name=dev_rp_name_prefix + "%s-ens2" % hostname)
5677 
5678         sriov_pf3_rp_uuid = getattr(uuids, sriov_agent_rp_uuid + 'PF3')
5679         self.sriov_dev_rp_per_host[compute_rp_uuid]['pf3'] = sriov_pf3_rp_uuid
5680         inventories = {}
5681         traits = [self.CUSTOM_VNIC_TYPE_DIRECT, self.CUSTOM_PHYSNET2]
5682         self._create_pf_device_rp(
5683             sriov_pf3_rp_uuid, sriov_agent_rp_uuid, inventories, traits,
5684             device_rp_name=dev_rp_name_prefix + "%s-ens3" % hostname)
5685 
5686     def _create_networking_rp_tree(self, hostname, compute_rp_uuid):
5687         # let's simulate what the neutron would do
5688         self._create_ovs_networking_rp_tree(compute_rp_uuid)
5689         self._create_sriov_networking_rp_tree(hostname, compute_rp_uuid)
5690 
5691     def assertPortMatchesAllocation(self, port, allocations):
5692         port_request = port[constants.RESOURCE_REQUEST]['resources']
5693         for rc, amount in allocations.items():
5694             self.assertEqual(port_request[rc], amount,
5695                              'port %s requested %d %s '
5696                              'resources but got allocation %d' %
5697                              (port['id'], port_request[rc], rc,
5698                               amount))
5699 
5700 
5701 class UnsupportedPortResourceRequestBasedSchedulingTest(
5702         PortResourceRequestBasedSchedulingTestBase):
5703     """Tests for handling servers with ports having resource requests """
5704 
5705     def _add_resource_request_to_a_bound_port(self, port_id):
5706         # NOTE(gibi): self.neutron._ports contains a copy of each neutron port
5707         # defined on class level in the fixture. So modifying what is in the
5708         # _ports list is safe as it is re-created for each Neutron fixture
5709         # instance therefore for each individual test using that fixture.
5710         bound_port = self.neutron._ports[port_id]
5711         bound_port[constants.RESOURCE_REQUEST] = (
5712             self.neutron.port_with_resource_request[
5713                 constants.RESOURCE_REQUEST])
5714 
5715     def test_interface_attach_with_port_resource_request(self):
5716         # create a server
5717         server = self._create_server(
5718             flavor=self.flavor,
5719             networks=[{'port': self.neutron.port_1['id']}])
5720         self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
5721 
5722         # try to add a port with resource request
5723         post = {
5724             'interfaceAttachment': {
5725                 'port_id': self.neutron.port_with_resource_request['id']
5726         }}
5727         ex = self.assertRaises(client.OpenStackApiException,
5728                                self.api.attach_interface,
5729                                server['id'], post)
5730         self.assertEqual(400, ex.response.status_code)
5731         self.assertIn('Attaching interfaces with QoS policy is '
5732                       'not supported for instance',
5733                       six.text_type(ex))
5734 
5735     @mock.patch('nova.tests.fixtures.NeutronFixture.create_port')
5736     def test_interface_attach_with_network_create_port_has_resource_request(
5737             self, mock_neutron_create_port):
5738         # create a server
5739         server = self._create_server(
5740             flavor=self.flavor,
5741             networks=[{'port': self.neutron.port_1['id']}])
5742         self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
5743 
5744         # the interfaceAttach operation below will result in a new port being
5745         # created in the network that is attached. Make sure that neutron
5746         # returns a port that has resource request.
5747         mock_neutron_create_port.return_value = (
5748             {'port': copy.deepcopy(self.neutron.port_with_resource_request)})
5749 
5750         # try to attach a network
5751         post = {
5752             'interfaceAttachment': {
5753                 'net_id': self.neutron.network_1['id']
5754         }}
5755         ex = self.assertRaises(client.OpenStackApiException,
5756                                self.api.attach_interface,
5757                                server['id'], post)
5758         self.assertEqual(400, ex.response.status_code)
5759         self.assertIn('Using networks with QoS policy is not supported for '
5760                       'instance',
5761                       six.text_type(ex))
5762 
5763     @mock.patch('nova.tests.fixtures.NeutronFixture.create_port')
5764     def test_create_server_with_network_create_port_has_resource_request(
5765             self, mock_neutron_create_port):
5766         # the server create operation below will result in a new port being
5767         # created in the network. Make sure that neutron returns a port that
5768         # has resource request.
5769         mock_neutron_create_port.return_value = (
5770             {'port': copy.deepcopy(self.neutron.port_with_resource_request)})
5771 
5772         server = self._create_server(
5773             flavor=self.flavor,
5774             networks=[{'uuid': self.neutron.network_1['id']}])
5775         server = self._wait_for_state_change(self.admin_api, server, 'ERROR')
5776 
5777         self.assertEqual(500, server['fault']['code'])
5778         self.assertIn('Failed to allocate the network',
5779                       server['fault']['message'])
5780 
5781     def test_create_server_with_port_resource_request_old_microversion(self):
5782 
5783         # NOTE(gibi): 2.71 is the last microversion where nova does not support
5784         # this kind of create server
5785         self.api.microversion = '2.71'
5786         ex = self.assertRaises(
5787             client.OpenStackApiException, self._create_server,
5788             flavor=self.flavor,
5789             networks=[{'port': self.neutron.port_with_resource_request['id']}])
5790 
5791         self.assertEqual(400, ex.response.status_code)
5792         self.assertIn(
5793             "Creating servers with ports having resource requests, like a "
5794             "port with a QoS minimum bandwidth policy, is not supported "
5795             "until microversion 2.72.",
5796             six.text_type(ex))
5797 
5798     def test_resize_server_with_port_resource_request_old_microversion(self):
5799         server = self._create_server(
5800             flavor=self.flavor,
5801             networks=[{'port': self.neutron.port_1['id']}])
5802         self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
5803 
5804         # We need to simulate that the above server has a port that has
5805         # resource request; we cannot boot with such a port but legacy servers
5806         # can exist with such a port.
5807         self._add_resource_request_to_a_bound_port(self.neutron.port_1['id'])
5808 
5809         resize_req = {
5810             'resize': {
5811                 'flavorRef': self.flavor['id']
5812             }
5813         }
5814         ex = self.assertRaises(
5815             client.OpenStackApiException,
5816             self.api.post_server_action, server['id'], resize_req)
5817 
5818         self.assertEqual(400, ex.response.status_code)
5819         self.assertIn(
5820             'The resize action on a server with ports having resource '
5821             'requests', six.text_type(ex))
5822 
5823     def test_migrate_server_with_port_resource_request_old_microversion(self):
5824         server = self._create_server(
5825             flavor=self.flavor,
5826             networks=[{'port': self.neutron.port_1['id']}])
5827         self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
5828 
5829         # We need to simulate that the above server has a port that has
5830         # resource request; we cannot boot with such a port but legacy servers
5831         # can exist with such a port.
5832         self._add_resource_request_to_a_bound_port(self.neutron.port_1['id'])
5833 
5834         ex = self.assertRaises(
5835             client.OpenStackApiException,
5836             self.api.post_server_action, server['id'], {'migrate': None})
5837 
5838         self.assertEqual(400, ex.response.status_code)
5839         self.assertIn(
5840             'The migrate action on a server with ports having resource '
5841             'requests', six.text_type(ex))
5842 
5843     def test_live_migrate_server_with_port_resource_request_old_microversion(
5844             self):
5845         server = self._create_server(
5846             flavor=self.flavor,
5847             networks=[{'port': self.neutron.port_1['id']}])
5848         self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
5849 
5850         # We need to simulate that the above server has a port that has
5851         # resource request; we cannot boot with such a port but legacy servers
5852         # can exist with such a port.
5853         self._add_resource_request_to_a_bound_port(self.neutron.port_1['id'])
5854 
5855         post = {
5856             'os-migrateLive': {
5857                 'host': None,
5858                 'block_migration': False,
5859             }
5860         }
5861         ex = self.assertRaises(
5862             client.OpenStackApiException,
5863             self.api.post_server_action, server['id'], post)
5864 
5865         self.assertEqual(400, ex.response.status_code)
5866         self.assertIn(
5867             'The os-migrateLive action on a server with ports having resource '
5868             'requests', six.text_type(ex))
5869 
5870     def test_evacuate_server_with_port_resource_request_old_microversion(
5871             self):
5872         server = self._create_server(
5873             flavor=self.flavor,
5874             networks=[{'port': self.neutron.port_1['id']}])
5875         self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
5876 
5877         # We need to simulate that the above server has a port that has
5878         # resource request; we cannot boot with such a port but legacy servers
5879         # can exist with such a port.
5880         self._add_resource_request_to_a_bound_port(self.neutron.port_1['id'])
5881 
5882         ex = self.assertRaises(
5883             client.OpenStackApiException,
5884             self.api.post_server_action, server['id'], {'evacuate': {}})
5885 
5886         self.assertEqual(400, ex.response.status_code)
5887         self.assertIn(
5888             'The evacuate action on a server with ports having resource '
5889             'requests', six.text_type(ex))
5890 
5891     def test_unshelve_offloaded_server_with_port_resource_request_old_version(
5892             self):
5893         server = self._create_server(
5894             flavor=self.flavor,
5895             networks=[{'port': self.neutron.port_1['id']}])
5896         self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
5897 
5898         # with default config shelve means immediate offload as well
5899         req = {
5900             'shelve': {}
5901         }
5902         self.api.post_server_action(server['id'], req)
5903         self._wait_for_server_parameter(
5904             self.api, server, {'status': 'SHELVED_OFFLOADED'})
5905 
5906         # We need to simulate that the above server has a port that has
5907         # resource request; we cannot boot with such a port but legacy servers
5908         # can exist with such a port.
5909         self._add_resource_request_to_a_bound_port(self.neutron.port_1['id'])
5910 
5911         ex = self.assertRaises(
5912             client.OpenStackApiException,
5913             self.api.post_server_action, server['id'], {'unshelve': None})
5914 
5915         self.assertEqual(400, ex.response.status_code)
5916         self.assertIn(
5917             'The unshelve action on a server with ports having resource '
5918             'requests', six.text_type(ex))
5919 
5920     def test_unshelve_not_offloaded_server_with_port_resource_request(
5921             self):
5922         """If the server is not offloaded then unshelving does not cause a new
5923         resource allocation therefore having port resource request is
5924         irrelevant. This test asserts that such unshelve request is not
5925         rejected.
5926         """
5927         server = self._create_server(
5928             flavor=self.flavor,
5929             networks=[{'port': self.neutron.port_1['id']}])
5930         self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
5931 
5932         # avoid automatic shelve offloading
5933         self.flags(shelved_offload_time=-1)
5934         req = {
5935             'shelve': {}
5936         }
5937         self.api.post_server_action(server['id'], req)
5938         self._wait_for_server_parameter(
5939             self.api, server, {'status': 'SHELVED'})
5940 
5941         # We need to simulate that the above server has a port that has
5942         # resource request; we cannot boot with such a port but legacy servers
5943         # can exist with such a port.
5944         self._add_resource_request_to_a_bound_port(self.neutron.port_1['id'])
5945 
5946         self.api.post_server_action(server['id'], {'unshelve': None})
5947         self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
5948 
5949 
5950 class PortResourceRequestBasedSchedulingTest(
5951         PortResourceRequestBasedSchedulingTestBase):
5952     """Tests creating a server with a pre-existing port that has a resource
5953     request for a QoS minimum bandwidth policy.
5954     """
5955 
5956     def test_boot_server_with_two_ports_one_having_resource_request(self):
5957         non_qos_port = self.neutron.port_1
5958         qos_port = self.neutron.port_with_resource_request
5959 
5960         server = self._create_server(
5961             flavor=self.flavor,
5962             networks=[{'port': non_qos_port['id']},
5963                       {'port': qos_port['id']}])
5964         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
5965         updated_non_qos_port = self.neutron.show_port(
5966             non_qos_port['id'])['port']
5967         updated_qos_port = self.neutron.show_port(qos_port['id'])['port']
5968 
5969         allocations = self.placement_api.get(
5970             '/allocations/%s' % server['id']).body['allocations']
5971 
5972         # We expect one set of allocations for the compute resources on the
5973         # compute rp and one set for the networking resources on the ovs bridge
5974         # rp due to the qos_port resource request
5975         self.assertEqual(2, len(allocations))
5976 
5977         self.assertComputeAllocationMatchesFlavor(
5978             allocations, self.compute1_rp_uuid, self.flavor)
5979         network_allocations = allocations[
5980             self.ovs_bridge_rp_per_host[self.compute1_rp_uuid]]['resources']
5981         self.assertPortMatchesAllocation(qos_port, network_allocations)
5982 
5983         # We expect that only the RP uuid of the networking RP having the port
5984         # allocation is sent in the port binding for the port having resource
5985         # request
5986         qos_binding_profile = updated_qos_port['binding:profile']
5987         self.assertEqual(self.ovs_bridge_rp_per_host[self.compute1_rp_uuid],
5988                          qos_binding_profile['allocation'])
5989 
5990         # And we expect not to have any allocation set in the port binding for
5991         # the port that doesn't have resource request
5992         self.assertNotIn('binding:profile', updated_non_qos_port)
5993 
5994         self._delete_and_check_allocations(server)
5995 
5996         # assert that unbind removes the allocation from the binding of the
5997         # port that got allocation during the bind
5998         updated_qos_port = self.neutron.show_port(qos_port['id'])['port']
5999         binding_profile = updated_qos_port['binding:profile']
6000         self.assertNotIn('allocation', binding_profile)
6001 
6002     def test_one_ovs_one_sriov_port(self):
6003         ovs_port = self.neutron.port_with_resource_request
6004         sriov_port = self.neutron.port_with_sriov_resource_request
6005 
6006         server = self._create_server(flavor=self.flavor_with_group_policy,
6007                                      networks=[{'port': ovs_port['id']},
6008                                                {'port': sriov_port['id']}])
6009 
6010         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6011 
6012         ovs_port = self.neutron.show_port(ovs_port['id'])['port']
6013         sriov_port = self.neutron.show_port(sriov_port['id'])['port']
6014 
6015         allocations = self.placement_api.get(
6016             '/allocations/%s' % server['id']).body['allocations']
6017 
6018         # We expect one set of allocations for the compute resources on the
6019         # compute rp and one set for the networking resources on the ovs bridge
6020         # rp and on the sriov PF rp.
6021         self.assertEqual(3, len(allocations))
6022 
6023         self.assertComputeAllocationMatchesFlavor(
6024             allocations, self.compute1_rp_uuid, self.flavor_with_group_policy)
6025 
6026         ovs_allocations = allocations[
6027             self.ovs_bridge_rp_per_host[self.compute1_rp_uuid]]['resources']
6028         sriov_allocations = allocations[
6029             self.sriov_dev_rp_per_host[
6030                 self.compute1_rp_uuid]['pf2']]['resources']
6031 
6032         self.assertPortMatchesAllocation(ovs_port, ovs_allocations)
6033         self.assertPortMatchesAllocation(sriov_port, sriov_allocations)
6034 
6035         # We expect that only the RP uuid of the networking RP having the port
6036         # allocation is sent in the port binding for the port having resource
6037         # request
6038         ovs_binding = ovs_port['binding:profile']
6039         self.assertEqual(self.ovs_bridge_rp_per_host[self.compute1_rp_uuid],
6040                          ovs_binding['allocation'])
6041         sriov_binding = sriov_port['binding:profile']
6042         self.assertEqual(
6043             self.sriov_dev_rp_per_host[self.compute1_rp_uuid]['pf2'],
6044             sriov_binding['allocation'])
6045 
6046     def test_interface_detach_with_port_with_bandwidth_request(self):
6047         port = self.neutron.port_with_resource_request
6048 
6049         # create a server
6050         server = self._create_server(
6051             flavor=self.flavor,
6052             networks=[{'port': port['id']}])
6053         self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6054 
6055         allocations = self.placement_api.get(
6056             '/allocations/%s' % server['id']).body['allocations']
6057         # We expect one set of allocations for the compute resources on the
6058         # compute rp and one set for the networking resources on the ovs bridge
6059         # rp due to the port resource request
6060         self.assertEqual(2, len(allocations))
6061 
6062         self.assertComputeAllocationMatchesFlavor(
6063             allocations, self.compute1_rp_uuid, self.flavor)
6064 
6065         network_allocations = allocations[
6066             self.ovs_bridge_rp_per_host[self.compute1_rp_uuid]]['resources']
6067         self.assertPortMatchesAllocation(port, network_allocations)
6068 
6069         # We expect that only the RP uuid of the networking RP having the port
6070         # allocation is sent in the port binding for the port having resource
6071         # request
6072         updated_port = self.neutron.show_port(port['id'])['port']
6073         binding_profile = updated_port['binding:profile']
6074         self.assertEqual(self.ovs_bridge_rp_per_host[self.compute1_rp_uuid],
6075                          binding_profile['allocation'])
6076 
6077         self.api.detach_interface(
6078             server['id'], self.neutron.port_with_resource_request['id'])
6079 
6080         fake_notifier.wait_for_versioned_notifications(
6081             'instance.interface_detach.end')
6082 
6083         updated_port = self.neutron.show_port(
6084             self.neutron.port_with_resource_request['id'])['port']
6085 
6086         allocations = self.placement_api.get(
6087             '/allocations/%s' % server['id']).body['allocations']
6088 
6089         # We expect that the port related resource allocations are removed
6090         self.assertEqual(1, len(allocations))
6091 
6092         self.assertComputeAllocationMatchesFlavor(
6093             allocations, self.compute1_rp_uuid, self.flavor)
6094 
6095         # We expect that the allocation is removed from the port too
6096         binding_profile = updated_port['binding:profile']
6097         self.assertNotIn('allocation', binding_profile)
6098 
6099     def test_delete_bound_port_in_neutron_with_resource_request(self):
6100         """Neutron sends a network-vif-deleted os-server-external-events
6101         notification to nova when a bound port is deleted. Nova detaches the
6102         vif from the server. If the port had a resource allocation then that
6103         allocation is leaked. This test makes sure that 1) an ERROR is logged
6104         when the leak happens. 2) the leaked resource is reclaimed when the
6105         server is deleted.
6106         """
6107         port = self.neutron.port_with_resource_request
6108 
6109         # create a server
6110         server = self._create_server(
6111             flavor=self.flavor,
6112             networks=[{'port': port['id']}])
6113         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6114 
6115         allocations = self.placement_api.get(
6116             '/allocations/%s' % server['id']).body['allocations']
6117         # We expect one set of allocations for the compute resources on the
6118         # compute rp and one set for the networking resources on the ovs bridge
6119         # rp due to the port resource request
6120         self.assertEqual(2, len(allocations))
6121         compute_allocations = allocations[self.compute1_rp_uuid]['resources']
6122         network_allocations = allocations[
6123             self.ovs_bridge_rp_per_host[self.compute1_rp_uuid]]['resources']
6124 
6125         self.assertEqual(self._resources_from_flavor(self.flavor),
6126                          compute_allocations)
6127         self.assertPortMatchesAllocation(port, network_allocations)
6128 
6129         # We expect that only the RP uuid of the networking RP having the port
6130         # allocation is sent in the port binding for the port having resource
6131         # request
6132         updated_port = self.neutron.show_port(port['id'])['port']
6133         binding_profile = updated_port['binding:profile']
6134         self.assertEqual(self.ovs_bridge_rp_per_host[self.compute1_rp_uuid],
6135                          binding_profile['allocation'])
6136 
6137         # neutron is faked in the functional test so this test just sends in
6138         # a os-server-external-events notification to trigger the
6139         # detach + ERROR log.
6140         events = {
6141             "events": [
6142                 {
6143                     "name": "network-vif-deleted",
6144                     "server_uuid": server['id'],
6145                     "tag": port['id'],
6146                 }
6147             ]
6148         }
6149         response = self.api.api_post('/os-server-external-events', events).body
6150         self.assertEqual(200, response['events'][0]['code'])
6151 
6152         port_rp_uuid = self.ovs_bridge_rp_per_host[self.compute1_rp_uuid]
6153 
6154         # 1) Nova logs an ERROR about the leak
6155         self._wait_for_log(
6156             'ERROR [nova.compute.manager] The bound port %(port_id)s is '
6157             'deleted in Neutron but the resource allocation on the resource '
6158             'provider %(rp_uuid)s is leaked until the server %(server_uuid)s '
6159             'is deleted.'
6160             % {'port_id': port['id'],
6161                'rp_uuid': port_rp_uuid,
6162                'server_uuid': server['id']})
6163 
6164         allocations = self.placement_api.get(
6165             '/allocations/%s' % server['id']).body['allocations']
6166 
6167         # Nova leaks the port allocation so the server still has the same
6168         # allocation before the port delete.
6169         self.assertEqual(2, len(allocations))
6170         compute_allocations = allocations[self.compute1_rp_uuid]['resources']
6171         network_allocations = allocations[port_rp_uuid]['resources']
6172 
6173         self.assertEqual(self._resources_from_flavor(self.flavor),
6174                          compute_allocations)
6175         self.assertPortMatchesAllocation(port, network_allocations)
6176 
6177         # 2) Also nova will reclaim the leaked resource during the server
6178         # delete
6179         self._delete_and_check_allocations(server)
6180 
6181     def test_two_sriov_ports_one_with_request_two_available_pfs(self):
6182         """Verify that the port's bandwidth allocated from the same PF as
6183         the allocated VF.
6184 
6185         One compute host:
6186         * PF1 (0000:01:00) is configured for physnet1
6187         * PF2 (0000:02:00) is configured for physnet2, with 1 VF and bandwidth
6188           inventory
6189         * PF3 (0000:03:00) is configured for physnet2, with 1 VF but without
6190           bandwidth inventory
6191 
6192         One instance will be booted with two neutron ports, both ports
6193         requested to be connected to physnet2. One port has resource request
6194         the other does not have resource request. The port having the resource
6195         request cannot be allocated to PF3 and PF1 while the other port that
6196         does not have resource request can be allocated to PF2 or PF3.
6197 
6198         For the detailed compute host config see the FakeDriverWithPciResources
6199         class. For the necessary passthrough_whitelist config see the setUp of
6200         the PortResourceRequestBasedSchedulingTestBase class.
6201         """
6202 
6203         sriov_port = self.neutron.sriov_port
6204         sriov_port_with_res_req = self.neutron.port_with_sriov_resource_request
6205         server = self._create_server(
6206             flavor=self.flavor_with_group_policy,
6207             networks=[
6208                 {'port': sriov_port_with_res_req['id']},
6209                 {'port': sriov_port['id']}])
6210 
6211         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6212 
6213         sriov_port = self.neutron.show_port(sriov_port['id'])['port']
6214         sriov_port_with_res_req = self.neutron.show_port(
6215             sriov_port_with_res_req['id'])['port']
6216 
6217         allocations = self.placement_api.get(
6218             '/allocations/%s' % server['id']).body['allocations']
6219 
6220         # We expect one set of allocations for the compute resources on the
6221         # compute rp and one set for the networking resources on the sriov PF2
6222         # rp.
6223         self.assertEqual(2, len(allocations))
6224 
6225         self.assertComputeAllocationMatchesFlavor(
6226             allocations, self.compute1_rp_uuid, self.flavor_with_group_policy)
6227 
6228         sriov_allocations = allocations[
6229             self.sriov_dev_rp_per_host[
6230                 self.compute1_rp_uuid]['pf2']]['resources']
6231         self.assertPortMatchesAllocation(
6232             sriov_port_with_res_req, sriov_allocations)
6233 
6234         # We expect that only the RP uuid of the networking RP having the port
6235         # allocation is sent in the port binding for the port having resource
6236         # request
6237         sriov_with_req_binding = sriov_port_with_res_req['binding:profile']
6238         self.assertEqual(
6239             self.sriov_dev_rp_per_host[self.compute1_rp_uuid]['pf2'],
6240             sriov_with_req_binding['allocation'])
6241         # and the port without resource request does not have allocation
6242         sriov_binding = sriov_port['binding:profile']
6243         self.assertNotIn('allocation', sriov_binding)
6244 
6245         # We expect that the selected PCI device matches with the RP from
6246         # where the bandwidth is allocated from. The bandwidth is allocated
6247         # from 0000:02:00 (PF2) so the PCI device should be a VF of that PF
6248         self.assertEqual(
6249             fake.FakeDriverWithPciResources.PCI_ADDR_PF2_VF1,
6250             sriov_with_req_binding['pci_slot'])
6251         # But also the port that has no resource request still gets a pci slot
6252         # allocated. The 0000:02:00 has no more VF available but 0000:03:00 has
6253         # one VF available and that PF is also on physnet2
6254         self.assertEqual(
6255             fake.FakeDriverWithPciResources.PCI_ADDR_PF3_VF1,
6256             sriov_binding['pci_slot'])
6257 
6258     def test_one_sriov_port_no_vf_and_bandwidth_available_on_the_same_pf(self):
6259         """Verify that if there is no PF that both provides bandwidth and VFs
6260         then the boot will fail.
6261         """
6262 
6263         # boot a server with a single sriov port that has no resource request
6264         sriov_port = self.neutron.sriov_port
6265         server = self._create_server(
6266             flavor=self.flavor_with_group_policy,
6267             networks=[{'port': sriov_port['id']}])
6268 
6269         self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6270         sriov_port = self.neutron.show_port(sriov_port['id'])['port']
6271         sriov_binding = sriov_port['binding:profile']
6272 
6273         # We expect that this consume the last available VF from the PF2
6274         self.assertEqual(
6275             fake.FakeDriverWithPciResources.PCI_ADDR_PF2_VF1,
6276             sriov_binding['pci_slot'])
6277 
6278         # Now boot a second server with a port that has resource request
6279         # At this point PF2 has available bandwidth but no available VF
6280         # and PF3 has available VF but no available bandwidth so we expect
6281         # the boot to fail.
6282 
6283         sriov_port_with_res_req = self.neutron.port_with_sriov_resource_request
6284         server = self._create_server(
6285             flavor=self.flavor_with_group_policy,
6286             networks=[{'port': sriov_port_with_res_req['id']}])
6287 
6288         # NOTE(gibi): It should be NoValidHost in an ideal world but that would
6289         # require the scheduler to detect the situation instead of the pci
6290         # claim. However that is pretty hard as the scheduler does not know
6291         # anything about allocation candidates (e.g. that the only candidate
6292         # for the port in this case is PF2) it see the whole host as a
6293         # candidate and in our host there is available VF for the request even
6294         # if that is on the wrong PF.
6295         server = self._wait_for_state_change(self.admin_api, server, 'ERROR')
6296         self.assertIn(
6297             'Exceeded maximum number of retries. Exhausted all hosts '
6298             'available for retrying build failures for instance',
6299             server['fault']['message'])
6300 
6301     def test_sriov_macvtap_port_with_resource_request(self):
6302         """Verify that vnic type macvtap is also supported"""
6303 
6304         port = self.neutron.port_macvtap_with_resource_request
6305 
6306         server = self._create_server(
6307             flavor=self.flavor,
6308             networks=[{'port': port['id']}])
6309 
6310         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6311 
6312         port = self.neutron.show_port(port['id'])['port']
6313 
6314         allocations = self.placement_api.get(
6315             '/allocations/%s' % server['id']).body['allocations']
6316 
6317         # We expect one set of allocations for the compute resources on the
6318         # compute rp and one set for the networking resources on the sriov PF2
6319         # rp.
6320         self.assertEqual(2, len(allocations))
6321 
6322         self.assertComputeAllocationMatchesFlavor(
6323             allocations, self.compute1_rp_uuid, self.flavor)
6324 
6325         sriov_allocations = allocations[self.sriov_dev_rp_per_host[
6326                 self.compute1_rp_uuid]['pf2']]['resources']
6327         self.assertPortMatchesAllocation(
6328             port, sriov_allocations)
6329 
6330         # We expect that only the RP uuid of the networking RP having the port
6331         # allocation is sent in the port binding for the port having resource
6332         # request
6333         port_binding = port['binding:profile']
6334         self.assertEqual(
6335             self.sriov_dev_rp_per_host[self.compute1_rp_uuid]['pf2'],
6336             port_binding['allocation'])
6337 
6338         # We expect that the selected PCI device matches with the RP from
6339         # where the bandwidth is allocated from. The bandwidth is allocated
6340         # from 0000:02:00 (PF2) so the PCI device should be a VF of that PF
6341         self.assertEqual(
6342             fake.FakeDriverWithPciResources.PCI_ADDR_PF2_VF1,
6343             port_binding['pci_slot'])
6344 
6345 
6346 class HostNameWeigher(weights.BaseHostWeigher):
6347     # Weigher to make the scheduler alternate host list deterministic
6348     _weights = {'host1': 100, 'host2': 50, 'host3': 10}
6349 
6350     def _weigh_object(self, host_state, weight_properties):
6351         # Any undefined host gets no weight.
6352         return self._weights.get(host_state.host, 0)
6353 
6354 
6355 class ServerMoveWithPortResourceRequestTest(
6356         PortResourceRequestBasedSchedulingTestBase):
6357 
6358     def setUp(self):
6359         # Use our custom weigher defined above to make sure that we have
6360         # a predictable host order in the alternate list returned by the
6361         # scheduler for migration.
6362         self.flags(weight_classes=[__name__ + '.HostNameWeigher'],
6363                    group='filter_scheduler')
6364         super(ServerMoveWithPortResourceRequestTest, self).setUp()
6365 
6366         # The API actively rejecting the move operations with resource
6367         # request so we have to turn off that check.
6368         # TODO(gibi): Remove this when the move operations are supported and
6369         # the API check is removed.
6370         patcher = mock.patch(
6371             'nova.api.openstack.common.'
6372             'supports_port_resource_request_during_move',
6373             return_value=True)
6374         self.addCleanup(patcher.stop)
6375         patcher.start()
6376 
6377         self.compute2 = self._start_compute('host2')
6378         self.compute2_rp_uuid = self._get_provider_uuid_by_host('host2')
6379         self._create_networking_rp_tree('host2', self.compute2_rp_uuid)
6380         self.compute2_service_id = self.admin_api.get_services(
6381             host='host2', binary='nova-compute')[0]['id']
6382 
6383     def _check_allocation(
6384             self, server, compute_rp_uuid, non_qos_port, qos_port,
6385             qos_sriov_port, migration_uuid=None, source_compute_rp_uuid=None):
6386 
6387         updated_non_qos_port = self.neutron.show_port(
6388             non_qos_port['id'])['port']
6389         updated_qos_port = self.neutron.show_port(qos_port['id'])['port']
6390         updated_qos_sriov_port = self.neutron.show_port(
6391             qos_sriov_port['id'])['port']
6392 
6393         allocations = self.placement_api.get(
6394             '/allocations/%s' % server['id']).body['allocations']
6395 
6396         # We expect one set of allocations for the compute resources on the
6397         # compute rp and two sets for the networking resources one on the ovs
6398         # bridge rp due to the qos_port resource request and one one the
6399         # sriov pf2 due to qos_sriov_port resource request
6400         self.assertEqual(3, len(allocations))
6401         self.assertComputeAllocationMatchesFlavor(
6402             allocations, compute_rp_uuid, self.flavor_with_group_policy)
6403         ovs_allocations = allocations[
6404             self.ovs_bridge_rp_per_host[compute_rp_uuid]]['resources']
6405         self.assertPortMatchesAllocation(qos_port, ovs_allocations)
6406         sriov_allocations = allocations[
6407             self.sriov_dev_rp_per_host[compute_rp_uuid]['pf2']]['resources']
6408         self.assertPortMatchesAllocation(qos_sriov_port, sriov_allocations)
6409 
6410         # We expect that only the RP uuid of the networking RP having the port
6411         # allocation is sent in the port binding for the port having resource
6412         # request
6413         qos_binding_profile = updated_qos_port['binding:profile']
6414         self.assertEqual(self.ovs_bridge_rp_per_host[compute_rp_uuid],
6415                          qos_binding_profile['allocation'])
6416         qos_sriov_binding_profile = updated_qos_sriov_port['binding:profile']
6417         self.assertEqual(self.sriov_dev_rp_per_host[compute_rp_uuid]['pf2'],
6418                          qos_sriov_binding_profile['allocation'])
6419 
6420         # And we expect not to have any allocation set in the port binding for
6421         # the port that doesn't have resource request
6422         self.assertNotIn('binding:profile', updated_non_qos_port)
6423 
6424         if migration_uuid:
6425             migration_allocations = self.placement_api.get(
6426                 '/allocations/%s' % migration_uuid).body['allocations']
6427 
6428             # We expect one set of allocations for the compute resources on the
6429             # compute rp and two sets for the networking resources one on the
6430             # ovs bridge rp due to the qos_port resource request and one one
6431             # the sriov pf2 due to qos_sriov_port resource request
6432             self.assertEqual(3, len(migration_allocations))
6433             self.assertComputeAllocationMatchesFlavor(
6434                 migration_allocations, source_compute_rp_uuid,
6435                 self.flavor_with_group_policy)
6436             ovs_allocations = migration_allocations[
6437                 self.ovs_bridge_rp_per_host[
6438                     source_compute_rp_uuid]]['resources']
6439             self.assertPortMatchesAllocation(qos_port, ovs_allocations)
6440             sriov_allocations = migration_allocations[
6441                 self.sriov_dev_rp_per_host[
6442                     source_compute_rp_uuid]['pf2']]['resources']
6443             self.assertPortMatchesAllocation(qos_sriov_port, sriov_allocations)
6444 
6445     def _create_server_with_ports(self, *ports):
6446         server = self._create_server(
6447             flavor=self.flavor_with_group_policy,
6448             networks=[{'port': port['id']} for port in ports],
6449             host='host1')
6450         return self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6451 
6452     def _delete_server_and_check_allocations(
6453             self, qos_port, qos_sriov_port, server):
6454         self._delete_and_check_allocations(server)
6455 
6456         # assert that unbind removes the allocation from the binding of the
6457         # ports that got allocation during the bind
6458         updated_qos_port = self.neutron.show_port(qos_port['id'])['port']
6459         binding_profile = updated_qos_port['binding:profile']
6460         self.assertNotIn('allocation', binding_profile)
6461         updated_qos_sriov_port = self.neutron.show_port(
6462             qos_sriov_port['id'])['port']
6463         binding_profile = updated_qos_sriov_port['binding:profile']
6464         self.assertNotIn('allocation', binding_profile)
6465 
6466     def test_migrate_server_with_qos_port_old_dest_compute_no_alternate(self):
6467         """Create a situation where the only migration target host returned
6468         by the scheduler is too old and therefore the migration fails.
6469         """
6470         non_qos_normal_port = self.neutron.port_1
6471         qos_normal_port = self.neutron.port_with_resource_request
6472         qos_sriov_port = self.neutron.port_with_sriov_resource_request
6473 
6474         server = self._create_server_with_ports(
6475             non_qos_normal_port, qos_normal_port, qos_sriov_port)
6476 
6477         # check that the server allocates from the current host properly
6478         self._check_allocation(
6479             server, self.compute1_rp_uuid, non_qos_normal_port,
6480             qos_normal_port, qos_sriov_port)
6481 
6482         orig_get_service = nova.objects.Service.get_by_host_and_binary
6483 
6484         def fake_get_service(context, host, binary):
6485             if host == 'host1':
6486                 return orig_get_service(context, host, binary)
6487             if host == 'host2':
6488                 service = orig_get_service(context, host, binary)
6489                 service.version = 38
6490                 return service
6491 
6492         with mock.patch(
6493                 'nova.objects.Service.get_by_host_and_binary',
6494                 side_effect=fake_get_service):
6495 
6496             ex = self.assertRaises(
6497                 client.OpenStackApiException,
6498                 self.api.post_server_action, server['id'], {'migrate': None})
6499 
6500         self.assertEqual(400, ex.response.status_code)
6501         self.assertIn('No valid host was found.', six.text_type(ex))
6502 
6503         # check that the server still allocates from the original host
6504         self._check_allocation(
6505             server, self.compute1_rp_uuid, non_qos_normal_port,
6506             qos_normal_port, qos_sriov_port)
6507 
6508         # but the migration allocation is gone
6509         migration_uuid = self.get_migration_uuid_for_instance(server['id'])
6510         migration_allocations = self.placement_api.get(
6511             '/allocations/%s' % migration_uuid).body['allocations']
6512         self.assertEqual({}, migration_allocations)
6513 
6514         self._delete_server_and_check_allocations(
6515             qos_normal_port, qos_sriov_port, server)
6516 
6517     def test_migrate_server_with_qos_port_old_dest_compute_alternate(self):
6518         """Create a situation where the first migration target host returned
6519         by the scheduler is too old and therefore the second host is selected
6520         by the MigrationTask.
6521         """
6522         self._start_compute('host3')
6523         compute3_rp_uuid = self._get_provider_uuid_by_host('host3')
6524         self._create_networking_rp_tree('host3', compute3_rp_uuid)
6525 
6526         non_qos_normal_port = self.neutron.port_1
6527         qos_normal_port = self.neutron.port_with_resource_request
6528         qos_sriov_port = self.neutron.port_with_sriov_resource_request
6529 
6530         server = self._create_server_with_ports(
6531             non_qos_normal_port, qos_normal_port, qos_sriov_port)
6532 
6533         # check that the server allocates from the current host properly
6534         self._check_allocation(
6535             server, self.compute1_rp_uuid, non_qos_normal_port,
6536             qos_normal_port, qos_sriov_port)
6537 
6538         orig_get_service = nova.objects.Service.get_by_host_and_binary
6539 
6540         def fake_get_service(context, host, binary):
6541             if host == 'host1':
6542                 return orig_get_service(context, host, binary)
6543             if host == 'host2':
6544                 service = orig_get_service(context, host, binary)
6545                 service.version = 38
6546                 return service
6547             if host == 'host3':
6548                 service = orig_get_service(context, host, binary)
6549                 service.version = 39
6550                 return service
6551 
6552         with mock.patch(
6553                 'nova.objects.Service.get_by_host_and_binary',
6554                 side_effect=fake_get_service):
6555 
6556             self.api.post_server_action(server['id'], {'migrate': None})
6557 
6558         self._wait_for_state_change(self.api, server, 'VERIFY_RESIZE')
6559 
6560         migration_uuid = self.get_migration_uuid_for_instance(server['id'])
6561 
6562         # check that server allocates from host3
6563         self._check_allocation(
6564             server, compute3_rp_uuid, non_qos_normal_port, qos_normal_port,
6565             qos_sriov_port, migration_uuid,
6566             source_compute_rp_uuid=self.compute1_rp_uuid)
6567 
6568         self._confirm_resize(server)
6569         # check that allocation is still OK
6570         self._check_allocation(
6571             server, compute3_rp_uuid, non_qos_normal_port,
6572             qos_normal_port, qos_sriov_port)
6573         # but the migration allocation is gone
6574         migration_allocations = self.placement_api.get(
6575             '/allocations/%s' % migration_uuid).body['allocations']
6576         self.assertEqual({}, migration_allocations)
6577 
6578         self._delete_server_and_check_allocations(
6579             qos_normal_port, qos_sriov_port, server)
6580 
6581     def test_migrate_server_with_qos_ports(self):
6582         non_qos_normal_port = self.neutron.port_1
6583         qos_normal_port = self.neutron.port_with_resource_request
6584         qos_sriov_port = self.neutron.port_with_sriov_resource_request
6585 
6586         server = self._create_server_with_ports(
6587             non_qos_normal_port, qos_normal_port, qos_sriov_port)
6588 
6589         # check that the server allocates from the current host properly
6590         self._check_allocation(
6591             server, self.compute1_rp_uuid, non_qos_normal_port,
6592             qos_normal_port, qos_sriov_port)
6593 
6594         self.api.post_server_action(server['id'], {'migrate': None})
6595         self._wait_for_state_change(self.api, server, 'VERIFY_RESIZE')
6596 
6597         migration_uuid = self.get_migration_uuid_for_instance(server['id'])
6598 
6599         # check that server allocates from the new host properly
6600         self._check_allocation(
6601             server, self.compute2_rp_uuid, non_qos_normal_port,
6602             qos_normal_port, qos_sriov_port, migration_uuid,
6603             source_compute_rp_uuid=self.compute1_rp_uuid)
6604 
6605         self._confirm_resize(server)
6606 
6607         # check that allocation is still OK
6608         self._check_allocation(
6609             server, self.compute2_rp_uuid, non_qos_normal_port,
6610             qos_normal_port, qos_sriov_port)
6611         migration_allocations = self.placement_api.get(
6612             '/allocations/%s' % migration_uuid).body['allocations']
6613         self.assertEqual({}, migration_allocations)
6614 
6615         self._delete_server_and_check_allocations(
6616             qos_normal_port, qos_sriov_port, server)
6617 
6618     def test_migrate_revert_with_qos_port(self):
6619         non_qos_port = self.neutron.port_1
6620         qos_port = self.neutron.port_with_resource_request
6621         qos_sriov_port = self.neutron.port_with_sriov_resource_request
6622 
6623         server = self._create_server_with_ports(
6624             non_qos_port, qos_port, qos_sriov_port)
6625 
6626         # check that the server allocates from the current host properly
6627         self._check_allocation(
6628             server, self.compute1_rp_uuid, non_qos_port, qos_port,
6629             qos_sriov_port)
6630 
6631         self.api.post_server_action(server['id'], {'migrate': None})
6632         self._wait_for_state_change(self.api, server, 'VERIFY_RESIZE')
6633 
6634         migration_uuid = self.get_migration_uuid_for_instance(server['id'])
6635 
6636         # check that server allocates from the new host properly
6637         self._check_allocation(
6638             server, self.compute2_rp_uuid, non_qos_port, qos_port,
6639             qos_sriov_port, migration_uuid,
6640             source_compute_rp_uuid=self.compute1_rp_uuid)
6641 
6642         self.api.post_server_action(server['id'], {'revertResize': None})
6643         self._wait_for_state_change(self.api, server, 'ACTIVE')
6644 
6645         # check that allocation is moved back to the source host
6646         self._check_allocation(
6647             server, self.compute1_rp_uuid, non_qos_port, qos_port,
6648             qos_sriov_port)
6649 
6650         # check that the target host allocation is cleaned up.
6651         self.assertRequestMatchesUsage(
6652             {'VCPU': 0, 'MEMORY_MB': 0, 'DISK_GB': 0,
6653              'NET_BW_IGR_KILOBIT_PER_SEC': 0, 'NET_BW_EGR_KILOBIT_PER_SEC': 0},
6654             self.compute2_rp_uuid)
6655         migration_allocations = self.placement_api.get(
6656             '/allocations/%s' % migration_uuid).body['allocations']
6657         self.assertEqual({}, migration_allocations)
6658 
6659         self._delete_server_and_check_allocations(
6660             qos_port, qos_sriov_port, server)
6661 
6662     def test_migrate_server_with_qos_port_reschedule_success(self):
6663         self._start_compute('host3')
6664         compute3_rp_uuid = self._get_provider_uuid_by_host('host3')
6665         self._create_networking_rp_tree('host3', compute3_rp_uuid)
6666 
6667         non_qos_port = self.neutron.port_1
6668         qos_port = self.neutron.port_with_resource_request
6669         qos_sriov_port = self.neutron.port_with_sriov_resource_request
6670 
6671         server = self._create_server_with_ports(
6672             non_qos_port, qos_port, qos_sriov_port)
6673 
6674         # check that the server allocates from the current host properly
6675         self._check_allocation(
6676             server, self.compute1_rp_uuid, non_qos_port, qos_port,
6677             qos_sriov_port)
6678 
6679         # Yes this isn't great in a functional test, but it's simple.
6680         original_prep_resize = compute_manager.ComputeManager._prep_resize
6681 
6682         prep_resize_calls = []
6683 
6684         def fake_prep_resize(_self, *args, **kwargs):
6685             # Make the first prep_resize fail and the rest passing through
6686             # the original _prep_resize call
6687             if not prep_resize_calls:
6688                 prep_resize_calls.append(_self.host)
6689                 raise test.TestingException('Simulated prep_resize failure.')
6690             prep_resize_calls.append(_self.host)
6691             original_prep_resize(_self, *args, **kwargs)
6692 
6693         # The patched compute manager will raise from _prep_resize on the
6694         # first host of the migration. Then the migration
6695         # is reschedule on the other host where it will succeed
6696         with mock.patch.object(
6697                 compute_manager.ComputeManager, '_prep_resize',
6698                 new=fake_prep_resize):
6699             self.api.post_server_action(server['id'], {'migrate': None})
6700             self._wait_for_state_change(self.api, server, 'VERIFY_RESIZE')
6701 
6702         # ensure that resize is tried on two hosts, so we had a re-schedule
6703         self.assertEqual(['host2', 'host3'], prep_resize_calls)
6704 
6705         migration_uuid = self.get_migration_uuid_for_instance(server['id'])
6706 
6707         # check that server allocates from the final host properly while
6708         # the migration holds the allocation on the source host
6709         self._check_allocation(
6710             server, compute3_rp_uuid, non_qos_port, qos_port, qos_sriov_port,
6711             migration_uuid, source_compute_rp_uuid=self.compute1_rp_uuid)
6712 
6713         self._confirm_resize(server)
6714 
6715         # check that allocation is still OK
6716         self._check_allocation(
6717             server, compute3_rp_uuid, non_qos_port, qos_port, qos_sriov_port)
6718         migration_allocations = self.placement_api.get(
6719             '/allocations/%s' % migration_uuid).body['allocations']
6720         self.assertEqual({}, migration_allocations)
6721 
6722         self._delete_server_and_check_allocations(
6723             qos_port, qos_sriov_port, server)
6724 
6725     def test_migrate_server_with_qos_port_reschedule_failure(self):
6726         non_qos_port = self.neutron.port_1
6727         qos_port = self.neutron.port_with_resource_request
6728         qos_sriov_port = self.neutron.port_with_sriov_resource_request
6729 
6730         server = self._create_server_with_ports(
6731             non_qos_port, qos_port, qos_sriov_port)
6732 
6733         # check that the server allocates from the current host properly
6734         self._check_allocation(
6735             server, self.compute1_rp_uuid, non_qos_port, qos_port,
6736             qos_sriov_port)
6737 
6738         # The patched compute manager on host2 will raise from _prep_resize.
6739         # Then the migration is reschedule but there is no other host to
6740         # choose from.
6741         with mock.patch.object(
6742                 compute_manager.ComputeManager, '_prep_resize',
6743                 side_effect=test.TestingException(
6744                     'Simulated prep_resize failure.')):
6745             self.api.post_server_action(server['id'], {'migrate': None})
6746             self._wait_for_server_parameter(
6747                 self.api, server,
6748                 {'OS-EXT-SRV-ATTR:host': 'host1',
6749                  'status': 'ERROR'})
6750             self._wait_for_migration_status(server, ['error'])
6751 
6752         migration_uuid = self.get_migration_uuid_for_instance(server['id'])
6753 
6754         # as the migration is failed we expect that the migration allocation
6755         # is deleted
6756         migration_allocations = self.placement_api.get(
6757             '/allocations/%s' % migration_uuid).body['allocations']
6758         self.assertEqual({}, migration_allocations)
6759 
6760         # and the instance allocates from the source host
6761         self._check_allocation(
6762             server, self.compute1_rp_uuid, non_qos_port, qos_port,
6763             qos_sriov_port)
6764 
6765     def test_migrate_server_with_qos_port_pci_update_fail_not_reschedule(self):
6766         self._start_compute('host3')
6767         compute3_rp_uuid = self._get_provider_uuid_by_host('host3')
6768         self._create_networking_rp_tree('host3', compute3_rp_uuid)
6769 
6770         non_qos_port = self.neutron.port_1
6771         qos_port = self.neutron.port_with_resource_request
6772         qos_sriov_port = self.neutron.port_with_sriov_resource_request
6773 
6774         server = self._create_server_with_ports(
6775             non_qos_port, qos_port, qos_sriov_port)
6776 
6777         # check that the server allocates from the current host properly
6778         self._check_allocation(
6779             server, self.compute1_rp_uuid, non_qos_port, qos_port,
6780             qos_sriov_port)
6781 
6782         # The patched compute manager on host2 will raise from
6783         # _update_pci_request_spec_with_allocated_interface_name which will
6784         # intentionally not trigger a re-schedule even if there is host3 as an
6785         # alternate.
6786         with mock.patch.object(
6787                 self.computes['host2'].manager,
6788                 '_update_pci_request_spec_with_allocated_interface_name',
6789                 side_effect=exception.BuildAbortException(
6790                     instance_uuid=server['id'], reason='Testing')):
6791             self.api.post_server_action(server['id'], {'migrate': None})
6792             server = self._wait_for_server_parameter(
6793                 self.api, server,
6794                 {'OS-EXT-SRV-ATTR:host': 'host1',
6795                  'status': 'ERROR'})
6796             self._wait_for_migration_status(server, ['error'])
6797 
6798         self.assertIn(
6799             'Build of instance %s aborted' % server['id'],
6800             server['fault']['message'])
6801 
6802         self._wait_for_action_fail_completion(
6803             server, instance_actions.MIGRATE, 'compute_prep_resize',
6804             self.admin_api)
6805 
6806         fake_notifier.wait_for_versioned_notifications(
6807             'instance.resize_prep.end')
6808         fake_notifier.wait_for_versioned_notifications(
6809             'compute.exception')
6810 
6811         migration_uuid = self.get_migration_uuid_for_instance(server['id'])
6812 
6813         # as the migration is failed we expect that the migration allocation
6814         # is deleted
6815         migration_allocations = self.placement_api.get(
6816             '/allocations/%s' % migration_uuid).body['allocations']
6817         self.assertEqual({}, migration_allocations)
6818 
6819         # and the instance allocates from the source host
6820         self._check_allocation(
6821             server, self.compute1_rp_uuid, non_qos_port, qos_port,
6822             qos_sriov_port)
6823 
6824 
6825 class PortResourceRequestReSchedulingTest(
6826         PortResourceRequestBasedSchedulingTestBase):
6827     """Similar to PortResourceRequestBasedSchedulingTest
6828     except this test uses FakeRescheduleDriver which will test reschedules
6829     during server create work as expected, i.e. that the resource request
6830     allocations are moved from the initially selected compute to the
6831     alternative compute.
6832     """
6833 
6834     compute_driver = 'fake.FakeRescheduleDriver'
6835 
6836     def setUp(self):
6837         super(PortResourceRequestReSchedulingTest, self).setUp()
6838         self.compute2 = self._start_compute('host2')
6839         self.compute2_rp_uuid = self._get_provider_uuid_by_host('host2')
6840         self._create_networking_rp_tree('host2', self.compute2_rp_uuid)
6841 
6842     def _create_networking_rp_tree(self, hostname, compute_rp_uuid):
6843         # let's simulate what the neutron would do
6844         self._create_ovs_networking_rp_tree(compute_rp_uuid)
6845 
6846     def test_boot_reschedule_success(self):
6847         port = self.neutron.port_with_resource_request
6848 
6849         server = self._create_server(
6850             flavor=self.flavor,
6851             networks=[{'port': port['id']}])
6852         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6853         updated_port = self.neutron.show_port(port['id'])['port']
6854 
6855         dest_hostname = server['OS-EXT-SRV-ATTR:host']
6856         dest_compute_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
6857 
6858         failed_compute_rp = (self.compute1_rp_uuid
6859                              if dest_compute_rp_uuid == self.compute2_rp_uuid
6860                              else self.compute2_rp_uuid)
6861 
6862         allocations = self.placement_api.get(
6863             '/allocations/%s' % server['id']).body['allocations']
6864 
6865         # We expect one set of allocations for the compute resources on the
6866         # compute rp and one set for the networking resources on the ovs bridge
6867         # rp
6868         self.assertEqual(2, len(allocations))
6869 
6870         self.assertComputeAllocationMatchesFlavor(
6871             allocations, dest_compute_rp_uuid, self.flavor)
6872 
6873         network_allocations = allocations[
6874             self.ovs_bridge_rp_per_host[dest_compute_rp_uuid]]['resources']
6875         self.assertPortMatchesAllocation(port, network_allocations)
6876 
6877         # assert that the allocations against the host where the spawn
6878         # failed are cleaned up properly
6879         self.assertEqual(
6880             {'VCPU': 0, 'MEMORY_MB': 0, 'DISK_GB': 0},
6881             self._get_provider_usages(failed_compute_rp))
6882         self.assertEqual(
6883             {'NET_BW_EGR_KILOBIT_PER_SEC': 0, 'NET_BW_IGR_KILOBIT_PER_SEC': 0},
6884             self._get_provider_usages(
6885                 self.ovs_bridge_rp_per_host[failed_compute_rp]))
6886 
6887         # We expect that only the RP uuid of the networking RP having the port
6888         # allocation is sent in the port binding
6889         binding_profile = updated_port['binding:profile']
6890         self.assertEqual(self.ovs_bridge_rp_per_host[dest_compute_rp_uuid],
6891                          binding_profile['allocation'])
6892 
6893         self._delete_and_check_allocations(server)
6894 
6895         # assert that unbind removes the allocation from the binding
6896         updated_port = self.neutron.show_port(port['id'])['port']
6897         binding_profile = updated_port['binding:profile']
6898         self.assertNotIn('allocation', binding_profile)
6899 
6900     def test_boot_reschedule_fill_provider_mapping_raises(self):
6901         """Verify that if the  _fill_provider_mapping raises during re-schedule
6902         then the instance is properly put into ERROR state.
6903         """
6904 
6905         port = self.neutron.port_with_resource_request
6906 
6907         # First call is during boot, we want that to succeed normally. Then the
6908         # fake virt driver triggers a re-schedule. During that re-schedule the
6909         # fill is called again, and we simulate that call raises.
6910         fill = nova.scheduler.utils.fill_provider_mapping
6911 
6912         with mock.patch(
6913                 'nova.scheduler.utils.fill_provider_mapping',
6914                 side_effect=[
6915                     fill,
6916                     exception.ResourceProviderTraitRetrievalFailed(
6917                         uuid=uuids.rp1)],
6918                 autospec=True):
6919             server = self._create_server(
6920                 flavor=self.flavor,
6921                 networks=[{'port': port['id']}])
6922             server = self._wait_for_state_change(
6923                 self.admin_api, server, 'ERROR')
6924 
6925         self.assertIn(
6926             'Failed to get traits for resource provider',
6927             server['fault']['message'])
6928 
6929         self._delete_and_check_allocations(server)
6930 
6931         # assert that unbind removes the allocation from the binding
6932         updated_port = self.neutron.show_port(port['id'])['port']
6933         binding_profile = updated_port['binding:profile']
6934         self.assertNotIn('allocation', binding_profile)
