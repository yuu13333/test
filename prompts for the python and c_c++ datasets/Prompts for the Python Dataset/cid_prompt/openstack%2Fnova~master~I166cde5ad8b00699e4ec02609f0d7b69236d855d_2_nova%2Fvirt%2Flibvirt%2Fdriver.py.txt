Please review the code below for security defects. You can consider defect types in terms of:
1.CWE-284 (Improper Access Control)
2.CWE-435 (Improper Interaction Between Multiple Entities)
3.CWE-664 (Improper Control of a Resource Through its Lifetime)
4.CWE-682 (Incorrect Calculation)
5.CWE-691 (Insufficient Control Flow Management)
6.CWE-693 (Protection Mechanism Failure)
7.CWE-697 (Incorrect Comparison)
8.CWE-703 (Improper Check or Handling of Exceptional Conditions)
9.CWE-707 (Improper Neutralization)
10.CWE-710 (Improper Adherence to Coding Standards)
If any are found, please describe the security defect in detail and indicate the corresponding line number of code and solution. If none are detected, states: 'No security defects are detected in the code'.

1 # Copyright 2010 United States Government as represented by the
2 # Administrator of the National Aeronautics and Space Administration.
3 # All Rights Reserved.
4 # Copyright (c) 2010 Citrix Systems, Inc.
5 # Copyright (c) 2011 Piston Cloud Computing, Inc
6 # Copyright (c) 2012 University Of Minho
7 # (c) Copyright 2013 Hewlett-Packard Development Company, L.P.
8 #
9 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
10 #    not use this file except in compliance with the License. You may obtain
11 #    a copy of the License at
12 #
13 #         http://www.apache.org/licenses/LICENSE-2.0
14 #
15 #    Unless required by applicable law or agreed to in writing, software
16 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
17 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
18 #    License for the specific language governing permissions and limitations
19 #    under the License.
20 
21 """
22 A connection to a hypervisor through libvirt.
23 
24 Supports KVM, LXC, QEMU, UML, XEN and Parallels.
25 
26 """
27 
28 import collections
29 from collections import deque
30 import contextlib
31 import errno
32 import functools
33 import glob
34 import itertools
35 import mmap
36 import operator
37 import os
38 import shutil
39 import tempfile
40 import time
41 import uuid
42 
43 import eventlet
44 from eventlet import greenthread
45 from eventlet import tpool
46 from lxml import etree
47 from os_brick import encryptors
48 from os_brick import exception as brick_exception
49 from os_brick.initiator import connector
50 from oslo_concurrency import processutils
51 from oslo_log import log as logging
52 from oslo_serialization import jsonutils
53 from oslo_service import loopingcall
54 from oslo_utils import excutils
55 from oslo_utils import fileutils
56 from oslo_utils import importutils
57 from oslo_utils import strutils
58 from oslo_utils import timeutils
59 from oslo_utils import units
60 import six
61 from six.moves import range
62 
63 from nova.api.metadata import base as instance_metadata
64 from nova import block_device
65 from nova.compute import power_state
66 from nova.compute import task_states
67 from nova.compute import utils as compute_utils
68 import nova.conf
69 from nova.console import serial as serial_console
70 from nova.console import type as ctype
71 from nova import context as nova_context
72 from nova import exception
73 from nova.i18n import _
74 from nova import image
75 from nova import keymgr
76 from nova.network import model as network_model
77 from nova import objects
78 from nova.objects import diagnostics as diagnostics_obj
79 from nova.objects import fields
80 from nova.objects import migrate_data as migrate_data_obj
81 from nova.pci import manager as pci_manager
82 from nova.pci import utils as pci_utils
83 from nova import utils
84 from nova import version
85 from nova.virt import block_device as driver_block_device
86 from nova.virt import configdrive
87 from nova.virt.disk import api as disk_api
88 from nova.virt.disk.vfs import guestfs
89 from nova.virt import driver
90 from nova.virt import firewall
91 from nova.virt import hardware
92 from nova.virt.image import model as imgmodel
93 from nova.virt import images
94 from nova.virt.libvirt import blockinfo
95 from nova.virt.libvirt import config as vconfig
96 from nova.virt.libvirt import firewall as libvirt_firewall
97 from nova.virt.libvirt import guest as libvirt_guest
98 from nova.virt.libvirt import host
99 from nova.virt.libvirt import imagebackend
100 from nova.virt.libvirt import imagecache
101 from nova.virt.libvirt import instancejobtracker
102 from nova.virt.libvirt import migration as libvirt_migrate
103 from nova.virt.libvirt.storage import dmcrypt
104 from nova.virt.libvirt.storage import lvm
105 from nova.virt.libvirt.storage import rbd_utils
106 from nova.virt.libvirt import utils as libvirt_utils
107 from nova.virt.libvirt import vif as libvirt_vif
108 from nova.virt.libvirt.volume import mount
109 from nova.virt.libvirt.volume import remotefs
110 from nova.virt import netutils
111 from nova.volume import cinder
112 
113 libvirt = None
114 
115 uefi_logged = False
116 
117 LOG = logging.getLogger(__name__)
118 
119 CONF = nova.conf.CONF
120 
121 DEFAULT_FIREWALL_DRIVER = "%s.%s" % (
122     libvirt_firewall.__name__,
123     libvirt_firewall.IptablesFirewallDriver.__name__)
124 
125 DEFAULT_UEFI_LOADER_PATH = {
126     "x86_64": "/usr/share/OVMF/OVMF_CODE.fd",
127     "aarch64": "/usr/share/AAVMF/AAVMF_CODE.fd"
128 }
129 
130 MAX_CONSOLE_BYTES = 100 * units.Ki
131 
132 # The libvirt driver will prefix any disable reason codes with this string.
133 DISABLE_PREFIX = 'AUTO: '
134 # Disable reason for the service which was enabled or disabled without reason
135 DISABLE_REASON_UNDEFINED = None
136 
137 # Guest config console string
138 CONSOLE = "console=tty0 console=ttyS0 console=hvc0"
139 
140 GuestNumaConfig = collections.namedtuple(
141     'GuestNumaConfig', ['cpuset', 'cputune', 'numaconfig', 'numatune'])
142 
143 InjectionInfo = collections.namedtuple(
144     'InjectionInfo', ['network_info', 'files', 'admin_pass'])
145 
146 libvirt_volume_drivers = [
147     'iscsi=nova.virt.libvirt.volume.iscsi.LibvirtISCSIVolumeDriver',
148     'iser=nova.virt.libvirt.volume.iser.LibvirtISERVolumeDriver',
149     'local=nova.virt.libvirt.volume.volume.LibvirtVolumeDriver',
150     'drbd=nova.virt.libvirt.volume.drbd.LibvirtDRBDVolumeDriver',
151     'fake=nova.virt.libvirt.volume.volume.LibvirtFakeVolumeDriver',
152     'rbd=nova.virt.libvirt.volume.net.LibvirtNetVolumeDriver',
153     'sheepdog=nova.virt.libvirt.volume.net.LibvirtNetVolumeDriver',
154     'nfs=nova.virt.libvirt.volume.nfs.LibvirtNFSVolumeDriver',
155     'smbfs=nova.virt.libvirt.volume.smbfs.LibvirtSMBFSVolumeDriver',
156     'aoe=nova.virt.libvirt.volume.aoe.LibvirtAOEVolumeDriver',
157     'fibre_channel='
158         'nova.virt.libvirt.volume.fibrechannel.'
159         'LibvirtFibreChannelVolumeDriver',
160     'gpfs=nova.virt.libvirt.volume.gpfs.LibvirtGPFSVolumeDriver',
161     'quobyte=nova.virt.libvirt.volume.quobyte.LibvirtQuobyteVolumeDriver',
162     'hgst=nova.virt.libvirt.volume.hgst.LibvirtHGSTVolumeDriver',
163     'scaleio=nova.virt.libvirt.volume.scaleio.LibvirtScaleIOVolumeDriver',
164     'disco=nova.virt.libvirt.volume.disco.LibvirtDISCOVolumeDriver',
165     'vzstorage='
166         'nova.virt.libvirt.volume.vzstorage.LibvirtVZStorageVolumeDriver',
167     'veritas_hyperscale='
168         'nova.virt.libvirt.volume.vrtshyperscale.'
169         'LibvirtHyperScaleVolumeDriver',
170 ]
171 
172 
173 def patch_tpool_proxy():
174     """eventlet.tpool.Proxy doesn't work with old-style class in __str__()
175     or __repr__() calls. See bug #962840 for details.
176     We perform a monkey patch to replace those two instance methods.
177     """
178     def str_method(self):
179         return str(self._obj)
180 
181     def repr_method(self):
182         return repr(self._obj)
183 
184     tpool.Proxy.__str__ = str_method
185     tpool.Proxy.__repr__ = repr_method
186 
187 
188 patch_tpool_proxy()
189 
190 # For information about when MIN_LIBVIRT_VERSION and
191 # NEXT_MIN_LIBVIRT_VERSION can be changed, consult
192 #
193 #   https://wiki.openstack.org/wiki/LibvirtDistroSupportMatrix
194 #
195 # Currently this is effectively the min version for i686/x86_64
196 # + KVM/QEMU, as other architectures/hypervisors require newer
197 # versions. Over time, this will become a common min version
198 # for all architectures/hypervisors, as this value rises to
199 # meet them.
200 MIN_LIBVIRT_VERSION = (1, 2, 9)
201 MIN_QEMU_VERSION = (2, 1, 0)
202 # TODO(berrange): Re-evaluate this at start of each release cycle
203 # to decide if we want to plan a future min version bump.
204 # MIN_LIBVIRT_VERSION can be updated to match this after
205 # NEXT_MIN_LIBVIRT_VERSION  has been at a higher value for
206 # one cycle
207 NEXT_MIN_LIBVIRT_VERSION = (1, 3, 1)
208 NEXT_MIN_QEMU_VERSION = (2, 5, 0)
209 
210 # When the above version matches/exceeds this version
211 # delete it & corresponding code using it
212 # Libvirt version 1.2.17 is required for successful block live migration
213 # of vm booted from image with attached devices
214 MIN_LIBVIRT_BLOCK_LM_WITH_VOLUMES_VERSION = (1, 2, 17)
215 # PowerPC based hosts that support NUMA using libvirt
216 MIN_LIBVIRT_NUMA_VERSION_PPC = (1, 2, 19)
217 # Versions of libvirt with known NUMA topology issues
218 # See bug #1449028
219 BAD_LIBVIRT_NUMA_VERSIONS = [(1, 2, 9, 2)]
220 # Versions of libvirt with broken cpu pinning support. This excludes
221 # versions of libvirt with broken NUMA support since pinning needs
222 # NUMA
223 # See bug #1438226
224 BAD_LIBVIRT_CPU_POLICY_VERSIONS = [(1, 2, 10)]
225 
226 # Virtuozzo driver support
227 MIN_VIRTUOZZO_VERSION = (7, 0, 0)
228 MIN_LIBVIRT_VIRTUOZZO_VERSION = (1, 2, 12)
229 
230 # Ability to set the user guest password with Qemu
231 MIN_LIBVIRT_SET_ADMIN_PASSWD = (1, 2, 16)
232 
233 # Ability to set the user guest password with parallels
234 MIN_LIBVIRT_PARALLELS_SET_ADMIN_PASSWD = (2, 0, 0)
235 
236 # s/390 & s/390x architectures with KVM
237 MIN_LIBVIRT_KVM_S390_VERSION = (1, 2, 13)
238 MIN_QEMU_S390_VERSION = (2, 3, 0)
239 
240 # libvirt < 1.3 reported virt_functions capability
241 # only when VFs are enabled.
242 # libvirt 1.3 fix f391889f4e942e22b9ef8ecca492de05106ce41e
243 MIN_LIBVIRT_PF_WITH_NO_VFS_CAP_VERSION = (1, 3, 0)
244 
245 # Use the "logd" backend for handling stdout/stderr from QEMU processes.
246 MIN_LIBVIRT_VIRTLOGD = (1, 3, 3)
247 MIN_QEMU_VIRTLOGD = (2, 7, 0)
248 
249 # ppc64/ppc64le architectures with KVM
250 # NOTE(rfolco): Same levels for Libvirt/Qemu on Big Endian and Little
251 # Endian giving the nuance around guest vs host architectures
252 MIN_LIBVIRT_KVM_PPC64_VERSION = (1, 2, 12)
253 
254 # Names of the types that do not get compressed during migration
255 NO_COMPRESSION_TYPES = ('qcow2',)
256 
257 
258 # number of serial console limit
259 QEMU_MAX_SERIAL_PORTS = 4
260 # Qemu supports 4 serial consoles, we remove 1 because of the PTY one defined
261 ALLOWED_QEMU_SERIAL_PORTS = QEMU_MAX_SERIAL_PORTS - 1
262 
263 # realtime support
264 MIN_LIBVIRT_REALTIME_VERSION = (1, 2, 13)
265 
266 # libvirt postcopy support
267 MIN_LIBVIRT_POSTCOPY_VERSION = (1, 3, 3)
268 
269 # qemu postcopy support
270 MIN_QEMU_POSTCOPY_VERSION = (2, 5, 0)
271 
272 MIN_LIBVIRT_OTHER_ARCH = {
273     fields.Architecture.S390: MIN_LIBVIRT_KVM_S390_VERSION,
274     fields.Architecture.S390X: MIN_LIBVIRT_KVM_S390_VERSION,
275     fields.Architecture.PPC: MIN_LIBVIRT_KVM_PPC64_VERSION,
276     fields.Architecture.PPC64: MIN_LIBVIRT_KVM_PPC64_VERSION,
277     fields.Architecture.PPC64LE: MIN_LIBVIRT_KVM_PPC64_VERSION,
278 }
279 
280 MIN_QEMU_OTHER_ARCH = {
281     fields.Architecture.S390: MIN_QEMU_S390_VERSION,
282     fields.Architecture.S390X: MIN_QEMU_S390_VERSION,
283 }
284 
285 # perf events support
286 MIN_LIBVIRT_PERF_VERSION = (2, 0, 0)
287 LIBVIRT_PERF_EVENT_PREFIX = 'VIR_PERF_PARAM_'
288 
289 PERF_EVENTS_CPU_FLAG_MAPPING = {'cmt': 'cmt',
290                                 'mbml': 'mbm_local',
291                                 'mbmt': 'mbm_total',
292                                }
293 
294 
295 class LibvirtDriver(driver.ComputeDriver):
296     capabilities = {
297         "has_imagecache": True,
298         "supports_recreate": True,
299         "supports_migrate_to_same_host": False,
300         "supports_attach_interface": True,
301         "supports_device_tagging": True,
302         "supports_tagged_attach_interface": True,
303         "supports_tagged_attach_volume": True,
304         "supports_extend_volume": True,
305     }
306 
307     def __init__(self, virtapi, read_only=False):
308         super(LibvirtDriver, self).__init__(virtapi)
309 
310         global libvirt
311         if libvirt is None:
312             libvirt = importutils.import_module('libvirt')
313             libvirt_migrate.libvirt = libvirt
314 
315         self._host = host.Host(self._uri(), read_only,
316                                lifecycle_event_handler=self.emit_event,
317                                conn_event_handler=self._handle_conn_event)
318         self._initiator = None
319         self._fc_wwnns = None
320         self._fc_wwpns = None
321         self._caps = None
322         self._supported_perf_events = []
323         self.firewall_driver = firewall.load_driver(
324             DEFAULT_FIREWALL_DRIVER,
325             host=self._host)
326 
327         self.vif_driver = libvirt_vif.LibvirtGenericVIFDriver()
328 
329         # TODO(mriedem): Long-term we should load up the volume drivers on
330         # demand as needed rather than doing this on startup, as there might
331         # be unsupported volume drivers in this list based on the underlying
332         # platform.
333         self.volume_drivers = self._get_volume_drivers()
334 
335         self._disk_cachemode = None
336         self.image_cache_manager = imagecache.ImageCacheManager()
337         self.image_backend = imagebackend.Backend(CONF.use_cow_images)
338 
339         self.disk_cachemodes = {}
340 
341         self.valid_cachemodes = ["default",
342                                  "none",
343                                  "writethrough",
344                                  "writeback",
345                                  "directsync",
346                                  "unsafe",
347                                 ]
348         self._conn_supports_start_paused = CONF.libvirt.virt_type in ('kvm',
349                                                                       'qemu')
350 
351         for mode_str in CONF.libvirt.disk_cachemodes:
352             disk_type, sep, cache_mode = mode_str.partition('=')
353             if cache_mode not in self.valid_cachemodes:
354                 LOG.warning('Invalid cachemode %(cache_mode)s specified '
355                             'for disk type %(disk_type)s.',
356                             {'cache_mode': cache_mode, 'disk_type': disk_type})
357                 continue
358             self.disk_cachemodes[disk_type] = cache_mode
359 
360         self._volume_api = cinder.API()
361         self._image_api = image.API()
362 
363         sysinfo_serial_funcs = {
364             'none': lambda: None,
365             'hardware': self._get_host_sysinfo_serial_hardware,
366             'os': self._get_host_sysinfo_serial_os,
367             'auto': self._get_host_sysinfo_serial_auto,
368         }
369 
370         self._sysinfo_serial_func = sysinfo_serial_funcs.get(
371             CONF.libvirt.sysinfo_serial)
372 
373         self.job_tracker = instancejobtracker.InstanceJobTracker()
374         self._remotefs = remotefs.RemoteFilesystem()
375 
376         self._live_migration_flags = self._block_migration_flags = 0
377         self.active_migrations = {}
378 
379         # Compute reserved hugepages from conf file at the very
380         # beginning to ensure any syntax error will be reported and
381         # avoid any re-calculation when computing resources.
382         self._reserved_hugepages = hardware.numa_get_reserved_huge_pages()
383 
384     def _get_volume_drivers(self):
385         driver_registry = dict()
386 
387         for driver_str in libvirt_volume_drivers:
388             driver_type, _sep, driver = driver_str.partition('=')
389             driver_class = importutils.import_class(driver)
390             try:
391                 driver_registry[driver_type] = driver_class(self._host)
392             except brick_exception.InvalidConnectorProtocol:
393                 LOG.debug('Unable to load volume driver %s. It is not '
394                           'supported on this host.', driver)
395 
396         return driver_registry
397 
398     @property
399     def disk_cachemode(self):
400         if self._disk_cachemode is None:
401             # We prefer 'none' for consistent performance, host crash
402             # safety & migration correctness by avoiding host page cache.
403             # Some filesystems don't support O_DIRECT though. For those we
404             # fallback to 'writethrough' which gives host crash safety, and
405             # is safe for migration provided the filesystem is cache coherent
406             # (cluster filesystems typically are, but things like NFS are not).
407             self._disk_cachemode = "none"
408             if not self._supports_direct_io(CONF.instances_path):
409                 self._disk_cachemode = "writethrough"
410         return self._disk_cachemode
411 
412     def _set_cache_mode(self, conf):
413         """Set cache mode on LibvirtConfigGuestDisk object."""
414         try:
415             source_type = conf.source_type
416             driver_cache = conf.driver_cache
417         except AttributeError:
418             return
419 
420         cache_mode = self.disk_cachemodes.get(source_type,
421                                               driver_cache)
422         conf.driver_cache = cache_mode
423 
424     def _do_quality_warnings(self):
425         """Warn about untested driver configurations.
426 
427         This will log a warning message about untested driver or host arch
428         configurations to indicate to administrators that the quality is
429         unknown. Currently, only qemu or kvm on intel 32- or 64-bit systems
430         is tested upstream.
431         """
432         caps = self._host.get_capabilities()
433         hostarch = caps.host.cpu.arch
434         if (CONF.libvirt.virt_type not in ('qemu', 'kvm') or
435             hostarch not in (fields.Architecture.I686,
436                              fields.Architecture.X86_64)):
437             LOG.warning('The libvirt driver is not tested on '
438                         '%(type)s/%(arch)s by the OpenStack project and '
439                         'thus its quality can not be ensured. For more '
440                         'information, see: http://docs.openstack.org/'
441                         'developer/nova/support-matrix.html',
442                         {'type': CONF.libvirt.virt_type, 'arch': hostarch})
443 
444     def _handle_conn_event(self, enabled, reason):
445         LOG.info("Connection event '%(enabled)d' reason '%(reason)s'",
446                  {'enabled': enabled, 'reason': reason})
447         self._set_host_enabled(enabled, reason)
448 
449     def _version_to_string(self, version):
450         return '.'.join([str(x) for x in version])
451 
452     def init_host(self, host):
453         self._host.initialize()
454 
455         self._do_quality_warnings()
456 
457         self._parse_migration_flags()
458 
459         self._supported_perf_events = self._get_supported_perf_events()
460 
461         if (CONF.libvirt.virt_type == 'lxc' and
462                 not (CONF.libvirt.uid_maps and CONF.libvirt.gid_maps)):
463             LOG.warning("Running libvirt-lxc without user namespaces is "
464                         "dangerous. Containers spawned by Nova will be run "
465                         "as the host's root user. It is highly suggested "
466                         "that user namespaces be used in a public or "
467                         "multi-tenant environment.")
468 
469         # Stop libguestfs using KVM unless we're also configured
470         # to use this. This solves problem where people need to
471         # stop Nova use of KVM because nested-virt is broken
472         if CONF.libvirt.virt_type != "kvm":
473             guestfs.force_tcg()
474 
475         if not self._host.has_min_version(MIN_LIBVIRT_VERSION):
476             raise exception.InternalError(
477                 _('Nova requires libvirt version %s or greater.') %
478                 self._version_to_string(MIN_LIBVIRT_VERSION))
479 
480         if (CONF.libvirt.virt_type in ("qemu", "kvm") and
481             not self._host.has_min_version(hv_ver=MIN_QEMU_VERSION)):
482             raise exception.InternalError(
483                 _('Nova requires QEMU version %s or greater.') %
484                 self._version_to_string(MIN_QEMU_VERSION))
485 
486         if CONF.libvirt.virt_type == 'parallels':
487             if not self._host.has_min_version(hv_ver=MIN_VIRTUOZZO_VERSION):
488                 raise exception.InternalError(
489                     _('Nova requires Virtuozzo version %s or greater.') %
490                     self._version_to_string(MIN_VIRTUOZZO_VERSION))
491             if not self._host.has_min_version(MIN_LIBVIRT_VIRTUOZZO_VERSION):
492                 raise exception.InternalError(
493                     _('Running Nova with parallels virt_type requires '
494                       'libvirt version %s') %
495                     self._version_to_string(MIN_LIBVIRT_VIRTUOZZO_VERSION))
496 
497         # Give the cloud admin a heads up if we are intending to
498         # change the MIN_LIBVIRT_VERSION in the next release.
499         if not self._host.has_min_version(NEXT_MIN_LIBVIRT_VERSION):
500             LOG.warning('Running Nova with a libvirt version less than '
501                         '%(version)s is deprecated. The required minimum '
502                         'version of libvirt will be raised to %(version)s '
503                         'in the next release.',
504                         {'version': self._version_to_string(
505                             NEXT_MIN_LIBVIRT_VERSION)})
506         if (CONF.libvirt.virt_type in ("qemu", "kvm") and
507             not self._host.has_min_version(hv_ver=NEXT_MIN_QEMU_VERSION)):
508             LOG.warning('Running Nova with a QEMU version less than '
509                         '%(version)s is deprecated. The required minimum '
510                         'version of QEMU will be raised to %(version)s '
511                         'in the next release.',
512                         {'version': self._version_to_string(
513                             NEXT_MIN_QEMU_VERSION)})
514 
515         kvm_arch = fields.Architecture.from_host()
516         if (CONF.libvirt.virt_type in ('kvm', 'qemu') and
517             kvm_arch in MIN_LIBVIRT_OTHER_ARCH and
518                 not self._host.has_min_version(
519                                         MIN_LIBVIRT_OTHER_ARCH.get(kvm_arch),
520                                         MIN_QEMU_OTHER_ARCH.get(kvm_arch))):
521             if MIN_QEMU_OTHER_ARCH.get(kvm_arch):
522                 raise exception.InternalError(
523                     _('Running Nova with qemu/kvm virt_type on %(arch)s '
524                       'requires libvirt version %(libvirt_ver)s and '
525                       'qemu version %(qemu_ver)s, or greater') %
526                     {'arch': kvm_arch,
527                      'libvirt_ver': self._version_to_string(
528                          MIN_LIBVIRT_OTHER_ARCH.get(kvm_arch)),
529                      'qemu_ver': self._version_to_string(
530                          MIN_QEMU_OTHER_ARCH.get(kvm_arch))})
531             # no qemu version in the error message
532             raise exception.InternalError(
533                 _('Running Nova with qemu/kvm virt_type on %(arch)s '
534                   'requires libvirt version %(libvirt_ver)s or greater') %
535                 {'arch': kvm_arch,
536                  'libvirt_ver': self._version_to_string(
537                      MIN_LIBVIRT_OTHER_ARCH.get(kvm_arch))})
538 
539     def _prepare_migration_flags(self):
540         migration_flags = 0
541 
542         migration_flags |= libvirt.VIR_MIGRATE_LIVE
543 
544         # Adding p2p flag only if xen is not in use, because xen does not
545         # support p2p migrations
546         if CONF.libvirt.virt_type != 'xen':
547             migration_flags |= libvirt.VIR_MIGRATE_PEER2PEER
548 
549         # Adding VIR_MIGRATE_UNDEFINE_SOURCE because, without it, migrated
550         # instance will remain defined on the source host
551         migration_flags |= libvirt.VIR_MIGRATE_UNDEFINE_SOURCE
552 
553         # Adding VIR_MIGRATE_PERSIST_DEST to persist the VM on the
554         # destination host
555         migration_flags |= libvirt.VIR_MIGRATE_PERSIST_DEST
556 
557         live_migration_flags = block_migration_flags = migration_flags
558 
559         # Adding VIR_MIGRATE_NON_SHARED_INC, otherwise all block-migrations
560         # will be live-migrations instead
561         block_migration_flags |= libvirt.VIR_MIGRATE_NON_SHARED_INC
562 
563         return (live_migration_flags, block_migration_flags)
564 
565     def _handle_live_migration_tunnelled(self, migration_flags):
566         if (CONF.libvirt.live_migration_tunnelled is None or
567                 CONF.libvirt.live_migration_tunnelled):
568             migration_flags |= libvirt.VIR_MIGRATE_TUNNELLED
569         return migration_flags
570 
571     def _is_post_copy_available(self):
572         if self._host.has_min_version(lv_ver=MIN_LIBVIRT_POSTCOPY_VERSION,
573                                       hv_ver=MIN_QEMU_POSTCOPY_VERSION):
574             return True
575         return False
576 
577     def _is_virtlogd_available(self):
578         return self._host.has_min_version(MIN_LIBVIRT_VIRTLOGD,
579                                           MIN_QEMU_VIRTLOGD)
580 
581     def _handle_live_migration_post_copy(self, migration_flags):
582         if CONF.libvirt.live_migration_permit_post_copy:
583             if self._is_post_copy_available():
584                 migration_flags |= libvirt.VIR_MIGRATE_POSTCOPY
585             else:
586                 LOG.info('The live_migration_permit_post_copy is set '
587                          'to True, but it is not supported.')
588         return migration_flags
589 
590     def _handle_live_migration_auto_converge(self, migration_flags):
591         if (self._is_post_copy_available() and
592                 (migration_flags & libvirt.VIR_MIGRATE_POSTCOPY) != 0):
593             LOG.info('The live_migration_permit_post_copy is set to '
594                      'True and post copy live migration is available '
595                      'so auto-converge will not be in use.')
596         elif CONF.libvirt.live_migration_permit_auto_converge:
597             migration_flags |= libvirt.VIR_MIGRATE_AUTO_CONVERGE
598         return migration_flags
599 
600     def _parse_migration_flags(self):
601         (live_migration_flags,
602             block_migration_flags) = self._prepare_migration_flags()
603 
604         live_migration_flags = self._handle_live_migration_tunnelled(
605             live_migration_flags)
606         block_migration_flags = self._handle_live_migration_tunnelled(
607             block_migration_flags)
608 
609         live_migration_flags = self._handle_live_migration_post_copy(
610             live_migration_flags)
611         block_migration_flags = self._handle_live_migration_post_copy(
612             block_migration_flags)
613 
614         live_migration_flags = self._handle_live_migration_auto_converge(
615             live_migration_flags)
616         block_migration_flags = self._handle_live_migration_auto_converge(
617             block_migration_flags)
618 
619         self._live_migration_flags = live_migration_flags
620         self._block_migration_flags = block_migration_flags
621 
622     # TODO(sahid): This method is targeted for removal when the tests
623     # have been updated to avoid its use
624     #
625     # All libvirt API calls on the libvirt.Connect object should be
626     # encapsulated by methods on the nova.virt.libvirt.host.Host
627     # object, rather than directly invoking the libvirt APIs. The goal
628     # is to avoid a direct dependency on the libvirt API from the
629     # driver.py file.
630     def _get_connection(self):
631         return self._host.get_connection()
632 
633     _conn = property(_get_connection)
634 
635     @staticmethod
636     def _uri():
637         if CONF.libvirt.virt_type == 'uml':
638             uri = CONF.libvirt.connection_uri or 'uml:///system'
639         elif CONF.libvirt.virt_type == 'xen':
640             uri = CONF.libvirt.connection_uri or 'xen:///'
641         elif CONF.libvirt.virt_type == 'lxc':
642             uri = CONF.libvirt.connection_uri or 'lxc:///'
643         elif CONF.libvirt.virt_type == 'parallels':
644             uri = CONF.libvirt.connection_uri or 'parallels:///system'
645         else:
646             uri = CONF.libvirt.connection_uri or 'qemu:///system'
647         return uri
648 
649     @staticmethod
650     def _live_migration_uri(dest):
651         uris = {
652             'kvm': 'qemu+%s://%s/system',
653             'qemu': 'qemu+%s://%s/system',
654             'xen': 'xenmigr://%s/system',
655             'parallels': 'parallels+tcp://%s/system',
656         }
657         virt_type = CONF.libvirt.virt_type
658         # TODO(pkoniszewski): Remove fetching live_migration_uri in Pike
659         uri = CONF.libvirt.live_migration_uri
660         if uri:
661             return uri % dest
662 
663         uri = uris.get(virt_type)
664         if uri is None:
665             raise exception.LiveMigrationURINotAvailable(virt_type=virt_type)
666 
667         str_format = (dest,)
668         if virt_type in ('kvm', 'qemu'):
669             scheme = CONF.libvirt.live_migration_scheme or 'tcp'
670             str_format = (scheme, dest)
671         return uris.get(virt_type) % str_format
672 
673     @staticmethod
674     def _migrate_uri(dest):
675         uri = None
676         # Only QEMU live migrations supports migrate-uri parameter
677         virt_type = CONF.libvirt.virt_type
678         if virt_type in ('qemu', 'kvm'):
679             # QEMU accept two schemes: tcp and rdma.  By default
680             # libvirt build the URI using the remote hostname and the
681             # tcp schema.
682             uri = 'tcp://%s' % dest
683         # Because dest might be of type unicode, here we might return value of
684         # type unicode as well which is not acceptable by libvirt python
685         # binding when Python 2.7 is in use, so let's convert it explicitly
686         # back to string. When Python 3.x is in use, libvirt python binding
687         # accepts unicode type so it is completely fine to do a no-op str(uri)
688         # conversion which will return value of type unicode.
689         return uri and str(uri)
690 
691     def instance_exists(self, instance):
692         """Efficient override of base instance_exists method."""
693         try:
694             self._host.get_guest(instance)
695             return True
696         except (exception.InternalError, exception.InstanceNotFound):
697             return False
698 
699     def estimate_instance_overhead(self, instance_info):
700         overhead = super(LibvirtDriver, self).estimate_instance_overhead(
701             instance_info)
702         if isinstance(instance_info, objects.Flavor):
703             # A flavor object is passed during case of migrate
704             # TODO(sahid): We do not have any way to retrieve the
705             # image meta related to the instance so if the cpu_policy
706             # has been set in image_meta we will get an
707             # exception. Until we fix it we specifically set the
708             # cpu_policy in dedicated in an ImageMeta object so if the
709             # emulator threads has been requested nothing is going to
710             # fail.
711             image_meta = objects.ImageMeta.from_dict({"properties": {
712                 "hw_cpu_policy": fields.CPUAllocationPolicy.DEDICATED,
713             }})
714             if (hardware.get_emulator_threads_constraint(
715                     instance_info, image_meta)
716                 == fields.CPUEmulatorThreadsPolicy.ISOLATE):
717                 overhead['vcpus'] += 1
718         else:
719             # An instance object is passed during case of spawing or a
720             # dict is passed when computing resource for an instance
721             numa_topology = hardware.instance_topology_from_instance(
722                 instance_info)
723             if numa_topology and numa_topology.emulator_threads_isolated:
724                 overhead['vcpus'] += 1
725         return overhead
726 
727     def list_instances(self):
728         names = []
729         for guest in self._host.list_guests(only_running=False):
730             names.append(guest.name)
731 
732         return names
733 
734     def list_instance_uuids(self):
735         uuids = []
736         for guest in self._host.list_guests(only_running=False):
737             uuids.append(guest.uuid)
738 
739         return uuids
740 
741     def plug_vifs(self, instance, network_info):
742         """Plug VIFs into networks."""
743         for vif in network_info:
744             self.vif_driver.plug(instance, vif)
745 
746     def _unplug_vifs(self, instance, network_info, ignore_errors):
747         """Unplug VIFs from networks."""
748         for vif in network_info:
749             try:
750                 self.vif_driver.unplug(instance, vif)
751             except exception.NovaException:
752                 if not ignore_errors:
753                     raise
754 
755     def unplug_vifs(self, instance, network_info):
756         self._unplug_vifs(instance, network_info, False)
757 
758     def _teardown_container(self, instance):
759         inst_path = libvirt_utils.get_instance_path(instance)
760         container_dir = os.path.join(inst_path, 'rootfs')
761         rootfs_dev = instance.system_metadata.get('rootfs_device_name')
762         LOG.debug('Attempting to teardown container at path %(dir)s with '
763                   'root device: %(rootfs_dev)s',
764                   {'dir': container_dir, 'rootfs_dev': rootfs_dev},
765                   instance=instance)
766         disk_api.teardown_container(container_dir, rootfs_dev)
767 
768     def _destroy(self, instance, attempt=1):
769         try:
770             guest = self._host.get_guest(instance)
771             if CONF.serial_console.enabled:
772                 # This method is called for several events: destroy,
773                 # rebuild, hard-reboot, power-off - For all of these
774                 # events we want to release the serial ports acquired
775                 # for the guest before destroying it.
776                 serials = self._get_serial_ports_from_guest(guest)
777                 for hostname, port in serials:
778                     serial_console.release_port(host=hostname, port=port)
779         except exception.InstanceNotFound:
780             guest = None
781 
782         # If the instance is already terminated, we're still happy
783         # Otherwise, destroy it
784         old_domid = -1
785         if guest is not None:
786             try:
787                 old_domid = guest.id
788                 guest.poweroff()
789 
790             except libvirt.libvirtError as e:
791                 is_okay = False
792                 errcode = e.get_error_code()
793                 if errcode == libvirt.VIR_ERR_NO_DOMAIN:
794                     # Domain already gone. This can safely be ignored.
795                     is_okay = True
796                 elif errcode == libvirt.VIR_ERR_OPERATION_INVALID:
797                     # If the instance is already shut off, we get this:
798                     # Code=55 Error=Requested operation is not valid:
799                     # domain is not running
800 
801                     state = guest.get_power_state(self._host)
802                     if state == power_state.SHUTDOWN:
803                         is_okay = True
804                 elif errcode == libvirt.VIR_ERR_INTERNAL_ERROR:
805                     errmsg = e.get_error_message()
806                     if (CONF.libvirt.virt_type == 'lxc' and
807                         errmsg == 'internal error: '
808                                   'Some processes refused to die'):
809                         # Some processes in the container didn't die
810                         # fast enough for libvirt. The container will
811                         # eventually die. For now, move on and let
812                         # the wait_for_destroy logic take over.
813                         is_okay = True
814                 elif errcode == libvirt.VIR_ERR_OPERATION_TIMEOUT:
815                     LOG.warning("Cannot destroy instance, operation time out",
816                                 instance=instance)
817                     reason = _("operation time out")
818                     raise exception.InstancePowerOffFailure(reason=reason)
819                 elif errcode == libvirt.VIR_ERR_SYSTEM_ERROR:
820                     if e.get_int1() == errno.EBUSY:
821                         # NOTE(danpb): When libvirt kills a process it sends it
822                         # SIGTERM first and waits 10 seconds. If it hasn't gone
823                         # it sends SIGKILL and waits another 5 seconds. If it
824                         # still hasn't gone then you get this EBUSY error.
825                         # Usually when a QEMU process fails to go away upon
826                         # SIGKILL it is because it is stuck in an
827                         # uninterruptible kernel sleep waiting on I/O from
828                         # some non-responsive server.
829                         # Given the CPU load of the gate tests though, it is
830                         # conceivable that the 15 second timeout is too short,
831                         # particularly if the VM running tempest has a high
832                         # steal time from the cloud host. ie 15 wallclock
833                         # seconds may have passed, but the VM might have only
834                         # have a few seconds of scheduled run time.
835                         LOG.warning('Error from libvirt during destroy. '
836                                     'Code=%(errcode)s Error=%(e)s; '
837                                     'attempt %(attempt)d of 3',
838                                     {'errcode': errcode, 'e': e,
839                                      'attempt': attempt},
840                                     instance=instance)
841                         with excutils.save_and_reraise_exception() as ctxt:
842                             # Try up to 3 times before giving up.
843                             if attempt < 3:
844                                 ctxt.reraise = False
845                                 self._destroy(instance, attempt + 1)
846                                 return
847 
848                 if not is_okay:
849                     with excutils.save_and_reraise_exception():
850                         LOG.error('Error from libvirt during destroy. '
851                                   'Code=%(errcode)s Error=%(e)s',
852                                   {'errcode': errcode, 'e': e},
853                                   instance=instance)
854 
855         def _wait_for_destroy(expected_domid):
856             """Called at an interval until the VM is gone."""
857             # NOTE(vish): If the instance disappears during the destroy
858             #             we ignore it so the cleanup can still be
859             #             attempted because we would prefer destroy to
860             #             never fail.
861             try:
862                 dom_info = self.get_info(instance)
863                 state = dom_info.state
864                 new_domid = dom_info.id
865             except exception.InstanceNotFound:
866                 LOG.debug("During wait destroy, instance disappeared.",
867                           instance=instance)
868                 state = power_state.SHUTDOWN
869 
870             if state == power_state.SHUTDOWN:
871                 LOG.info("Instance destroyed successfully.", instance=instance)
872                 raise loopingcall.LoopingCallDone()
873 
874             # NOTE(wangpan): If the instance was booted again after destroy,
875             #                this may be an endless loop, so check the id of
876             #                domain here, if it changed and the instance is
877             #                still running, we should destroy it again.
878             # see https://bugs.launchpad.net/nova/+bug/1111213 for more details
879             if new_domid != expected_domid:
880                 LOG.info("Instance may be started again.", instance=instance)
881                 kwargs['is_running'] = True
882                 raise loopingcall.LoopingCallDone()
883 
884         kwargs = {'is_running': False}
885         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_destroy,
886                                                      old_domid)
887         timer.start(interval=0.5).wait()
888         if kwargs['is_running']:
889             LOG.info("Going to destroy instance again.", instance=instance)
890             self._destroy(instance)
891         else:
892             # NOTE(GuanQiang): teardown container to avoid resource leak
893             if CONF.libvirt.virt_type == 'lxc':
894                 self._teardown_container(instance)
895 
896     def destroy(self, context, instance, network_info, block_device_info=None,
897                 destroy_disks=True):
898         self._destroy(instance)
899         self.cleanup(context, instance, network_info, block_device_info,
900                      destroy_disks)
901 
902     def _undefine_domain(self, instance):
903         try:
904             guest = self._host.get_guest(instance)
905             try:
906                 support_uefi = self._has_uefi_support()
907                 guest.delete_configuration(support_uefi)
908             except libvirt.libvirtError as e:
909                 with excutils.save_and_reraise_exception() as ctxt:
910                     errcode = e.get_error_code()
911                     if errcode == libvirt.VIR_ERR_NO_DOMAIN:
912                         LOG.debug("Called undefine, but domain already gone.",
913                                   instance=instance)
914                         ctxt.reraise = False
915                     else:
916                         LOG.error('Error from libvirt during undefine. '
917                                   'Code=%(errcode)s Error=%(e)s',
918                                   {'errcode': errcode, 'e': e},
919                                   instance=instance)
920         except exception.InstanceNotFound:
921             pass
922 
923     def cleanup(self, context, instance, network_info, block_device_info=None,
924                 destroy_disks=True, migrate_data=None, destroy_vifs=True):
925         if destroy_vifs:
926             self._unplug_vifs(instance, network_info, True)
927 
928         retry = True
929         while retry:
930             try:
931                 self.unfilter_instance(instance, network_info)
932             except libvirt.libvirtError as e:
933                 try:
934                     state = self.get_info(instance).state
935                 except exception.InstanceNotFound:
936                     state = power_state.SHUTDOWN
937 
938                 if state != power_state.SHUTDOWN:
939                     LOG.warning("Instance may be still running, destroy "
940                                 "it again.", instance=instance)
941                     self._destroy(instance)
942                 else:
943                     retry = False
944                     errcode = e.get_error_code()
945                     LOG.exception(_('Error from libvirt during unfilter. '
946                                     'Code=%(errcode)s Error=%(e)s'),
947                                   {'errcode': errcode, 'e': e},
948                                   instance=instance)
949                     reason = _("Error unfiltering instance.")
950                     raise exception.InstanceTerminationFailure(reason=reason)
951             except Exception:
952                 retry = False
953                 raise
954             else:
955                 retry = False
956 
957         # FIXME(wangpan): if the instance is booted again here, such as the
958         #                 soft reboot operation boot it here, it will become
959         #                 "running deleted", should we check and destroy it
960         #                 at the end of this method?
961 
962         # NOTE(vish): we disconnect from volumes regardless
963         block_device_mapping = driver.block_device_info_get_mapping(
964             block_device_info)
965         for vol in block_device_mapping:
966             connection_info = vol['connection_info']
967             disk_dev = vol['mount_device']
968             if disk_dev is not None:
969                 disk_dev = disk_dev.rpartition("/")[2]
970 
971             if ('data' in connection_info and
972                     'volume_id' in connection_info['data']):
973                 volume_id = connection_info['data']['volume_id']
974                 encryption = encryptors.get_encryption_metadata(
975                     context, self._volume_api, volume_id, connection_info)
976 
977                 if encryption:
978                     # The volume must be detached from the VM before
979                     # disconnecting it from its encryptor. Otherwise, the
980                     # encryptor may report that the volume is still in use.
981                     encryptor = self._get_volume_encryptor(connection_info,
982                                                            encryption)
983                     encryptor.detach_volume(**encryption)
984 
985             try:
986                 self._disconnect_volume(connection_info, disk_dev, instance)
987             except Exception as exc:
988                 with excutils.save_and_reraise_exception() as ctxt:
989                     if destroy_disks:
990                         # Don't block on Volume errors if we're trying to
991                         # delete the instance as we may be partially created
992                         # or deleted
993                         ctxt.reraise = False
994                         LOG.warning(
995                             "Ignoring Volume Error on vol %(vol_id)s "
996                             "during delete %(exc)s",
997                             {'vol_id': vol.get('volume_id'), 'exc': exc},
998                             instance=instance)
999 
1000         if destroy_disks:
1001             # NOTE(haomai): destroy volumes if needed
1002             if CONF.libvirt.images_type == 'lvm':
1003                 self._cleanup_lvm(instance, block_device_info)
1004             if CONF.libvirt.images_type == 'rbd':
1005                 self._cleanup_rbd(instance)
1006 
1007         is_shared_block_storage = False
1008         if migrate_data and 'is_shared_block_storage' in migrate_data:
1009             is_shared_block_storage = migrate_data.is_shared_block_storage
1010         if destroy_disks or is_shared_block_storage:
1011             attempts = int(instance.system_metadata.get('clean_attempts',
1012                                                         '0'))
1013             success = self.delete_instance_files(instance)
1014             # NOTE(mriedem): This is used in the _run_pending_deletes periodic
1015             # task in the compute manager. The tight coupling is not great...
1016             instance.system_metadata['clean_attempts'] = str(attempts + 1)
1017             if success:
1018                 instance.cleaned = True
1019             instance.save()
1020 
1021         self._undefine_domain(instance)
1022 
1023     def _detach_encrypted_volumes(self, instance, block_device_info):
1024         """Detaches encrypted volumes attached to instance."""
1025         disks = self._get_instance_disk_info(instance, block_device_info)
1026         encrypted_volumes = filter(dmcrypt.is_encrypted,
1027                                    [disk['path'] for disk in disks])
1028         for path in encrypted_volumes:
1029             dmcrypt.delete_volume(path)
1030 
1031     def _get_serial_ports_from_guest(self, guest, mode=None):
1032         """Returns an iterator over serial port(s) configured on guest.
1033 
1034         :param mode: Should be a value in (None, bind, connect)
1035         """
1036         xml = guest.get_xml_desc()
1037         tree = etree.fromstring(xml)
1038 
1039         # The 'serial' device is the base for x86 platforms. Other platforms
1040         # (e.g. kvm on system z = S390X) can only use 'console' devices.
1041         xpath_mode = "[@mode='%s']" % mode if mode else ""
1042         serial_tcp = "./devices/serial[@type='tcp']/source" + xpath_mode
1043         console_tcp = "./devices/console[@type='tcp']/source" + xpath_mode
1044 
1045         tcp_devices = tree.findall(serial_tcp)
1046         if len(tcp_devices) == 0:
1047             tcp_devices = tree.findall(console_tcp)
1048         for source in tcp_devices:
1049             yield (source.get("host"), int(source.get("service")))
1050 
1051     def _get_scsi_controller_max_unit(self, guest):
1052         """Returns the max disk unit used by scsi controller"""
1053         xml = guest.get_xml_desc()
1054         tree = etree.fromstring(xml)
1055         addrs = "./devices/disk[@device='disk']/address[@type='drive']"
1056 
1057         ret = []
1058         for obj in tree.findall(addrs):
1059             ret.append(int(obj.get('unit', 0)))
1060         return max(ret)
1061 
1062     @staticmethod
1063     def _get_rbd_driver():
1064         return rbd_utils.RBDDriver(
1065                 pool=CONF.libvirt.images_rbd_pool,
1066                 ceph_conf=CONF.libvirt.images_rbd_ceph_conf,
1067                 rbd_user=CONF.libvirt.rbd_user)
1068 
1069     def _cleanup_rbd(self, instance):
1070         # NOTE(nic): On revert_resize, the cleanup steps for the root
1071         # volume are handled with an "rbd snap rollback" command,
1072         # and none of this is needed (and is, in fact, harmful) so
1073         # filter out non-ephemerals from the list
1074         if instance.task_state == task_states.RESIZE_REVERTING:
1075             filter_fn = lambda disk: (disk.startswith(instance.uuid) and
1076                                       disk.endswith('disk.local'))
1077         else:
1078             filter_fn = lambda disk: disk.startswith(instance.uuid)
1079         LibvirtDriver._get_rbd_driver().cleanup_volumes(filter_fn)
1080 
1081     def _cleanup_lvm(self, instance, block_device_info):
1082         """Delete all LVM disks for given instance object."""
1083         if instance.get('ephemeral_key_uuid') is not None:
1084             self._detach_encrypted_volumes(instance, block_device_info)
1085 
1086         disks = self._lvm_disks(instance)
1087         if disks:
1088             lvm.remove_volumes(disks)
1089 
1090     def _lvm_disks(self, instance):
1091         """Returns all LVM disks for given instance object."""
1092         if CONF.libvirt.images_volume_group:
1093             vg = os.path.join('/dev', CONF.libvirt.images_volume_group)
1094             if not os.path.exists(vg):
1095                 return []
1096             pattern = '%s_' % instance.uuid
1097 
1098             def belongs_to_instance(disk):
1099                 return disk.startswith(pattern)
1100 
1101             def fullpath(name):
1102                 return os.path.join(vg, name)
1103 
1104             logical_volumes = lvm.list_volumes(vg)
1105 
1106             disks = [fullpath(disk) for disk in logical_volumes
1107                      if belongs_to_instance(disk)]
1108             return disks
1109         return []
1110 
1111     def get_volume_connector(self, instance):
1112         root_helper = utils.get_root_helper()
1113         return connector.get_connector_properties(
1114             root_helper, CONF.my_block_storage_ip,
1115             CONF.libvirt.volume_use_multipath,
1116             enforce_multipath=True,
1117             host=CONF.host)
1118 
1119     def _cleanup_resize(self, instance, network_info):
1120         inst_base = libvirt_utils.get_instance_path(instance)
1121         target = inst_base + '_resize'
1122 
1123         if os.path.exists(target):
1124             # Deletion can fail over NFS, so retry the deletion as required.
1125             # Set maximum attempt as 5, most test can remove the directory
1126             # for the second time.
1127             utils.execute('rm', '-rf', target, delay_on_retry=True,
1128                           attempts=5)
1129 
1130         root_disk = self.image_backend.by_name(instance, 'disk')
1131         # TODO(nic): Set ignore_errors=False in a future release.
1132         # It is set to True here to avoid any upgrade issues surrounding
1133         # instances being in pending resize state when the software is updated;
1134         # in that case there will be no snapshot to remove.  Once it can be
1135         # reasonably assumed that no such instances exist in the wild
1136         # anymore, it should be set back to False (the default) so it will
1137         # throw errors, like it should.
1138         if root_disk.exists():
1139             root_disk.remove_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME,
1140                                   ignore_errors=True)
1141 
1142         # NOTE(mjozefcz):
1143         # self.image_backend.image for some backends recreates instance
1144         # directory and image disk.info - remove it here if exists
1145         if os.path.exists(inst_base) and not root_disk.exists():
1146             try:
1147                 shutil.rmtree(inst_base)
1148             except OSError as e:
1149                 if e.errno != errno.ENOENT:
1150                     raise
1151 
1152         if instance.host != CONF.host:
1153             self._undefine_domain(instance)
1154             self.unplug_vifs(instance, network_info)
1155             self.unfilter_instance(instance, network_info)
1156 
1157     def _get_volume_driver(self, connection_info):
1158         driver_type = connection_info.get('driver_volume_type')
1159         if driver_type not in self.volume_drivers:
1160             raise exception.VolumeDriverNotFound(driver_type=driver_type)
1161         return self.volume_drivers[driver_type]
1162 
1163     def _connect_volume(self, connection_info, disk_info, instance):
1164         vol_driver = self._get_volume_driver(connection_info)
1165         vol_driver.connect_volume(connection_info, disk_info, instance)
1166 
1167     def _disconnect_volume(self, connection_info, disk_dev, instance):
1168         vol_driver = self._get_volume_driver(connection_info)
1169         vol_driver.disconnect_volume(connection_info, disk_dev, instance)
1170 
1171     def _extend_volume(self, connection_info, instance):
1172         vol_driver = self._get_volume_driver(connection_info)
1173         return vol_driver.extend_volume(connection_info, instance)
1174 
1175     def _get_volume_config(self, connection_info, disk_info):
1176         vol_driver = self._get_volume_driver(connection_info)
1177         conf = vol_driver.get_config(connection_info, disk_info)
1178         self._set_cache_mode(conf)
1179         return conf
1180 
1181     def _get_volume_encryptor(self, connection_info, encryption):
1182         root_helper = utils.get_root_helper()
1183         key_manager = keymgr.API(CONF)
1184         return encryptors.get_volume_encryptor(root_helper=root_helper,
1185                                                keymgr=key_manager,
1186                                                connection_info=connection_info,
1187                                                **encryption)
1188 
1189     def _check_discard_for_attach_volume(self, conf, instance):
1190         """Perform some checks for volumes configured for discard support.
1191 
1192         If discard is configured for the volume, and the guest is using a
1193         configuration known to not work, we will log a message explaining
1194         the reason why.
1195         """
1196         if conf.driver_discard == 'unmap' and conf.target_bus == 'virtio':
1197             LOG.debug('Attempting to attach volume %(id)s with discard '
1198                       'support enabled to an instance using an '
1199                       'unsupported configuration. target_bus = '
1200                       '%(bus)s. Trim commands will not be issued to '
1201                       'the storage device.',
1202                       {'bus': conf.target_bus,
1203                        'id': conf.serial},
1204                       instance=instance)
1205 
1206     def attach_volume(self, context, connection_info, instance, mountpoint,
1207                       disk_bus=None, device_type=None, encryption=None):
1208         guest = self._host.get_guest(instance)
1209 
1210         disk_dev = mountpoint.rpartition("/")[2]
1211         bdm = {
1212             'device_name': disk_dev,
1213             'disk_bus': disk_bus,
1214             'device_type': device_type}
1215 
1216         # Note(cfb): If the volume has a custom block size, check that
1217         #            that we are using QEMU/KVM and libvirt >= 0.10.2. The
1218         #            presence of a block size is considered mandatory by
1219         #            cinder so we fail if we can't honor the request.
1220         data = {}
1221         if ('data' in connection_info):
1222             data = connection_info['data']
1223         if ('logical_block_size' in data or 'physical_block_size' in data):
1224             if ((CONF.libvirt.virt_type != "kvm" and
1225                  CONF.libvirt.virt_type != "qemu")):
1226                 msg = _("Volume sets block size, but the current "
1227                         "libvirt hypervisor '%s' does not support custom "
1228                         "block size") % CONF.libvirt.virt_type
1229                 raise exception.InvalidHypervisorType(msg)
1230 
1231         disk_info = blockinfo.get_info_from_bdm(
1232             instance, CONF.libvirt.virt_type, instance.image_meta, bdm)
1233         self._connect_volume(connection_info, disk_info, instance)
1234         if disk_info['bus'] == 'scsi':
1235             disk_info['unit'] = self._get_scsi_controller_max_unit(guest) + 1
1236 
1237         conf = self._get_volume_config(connection_info, disk_info)
1238 
1239         self._check_discard_for_attach_volume(conf, instance)
1240 
1241         try:
1242             state = guest.get_power_state(self._host)
1243             live = state in (power_state.RUNNING, power_state.PAUSED)
1244 
1245             if encryption:
1246                 encryptor = self._get_volume_encryptor(connection_info,
1247                                                        encryption)
1248                 encryptor.attach_volume(context, **encryption)
1249 
1250             guest.attach_device(conf, persistent=True, live=live)
1251             # NOTE(artom) If we're attaching with a device role tag, we need to
1252             # rebuild device_metadata. If we're attaching without a role
1253             # tag, we're rebuilding it here needlessly anyways. This isn't a
1254             # massive deal, and it helps reduce code complexity by not having
1255             # to indicate to the virt driver that the attach is tagged. The
1256             # really important optimization of not calling the database unless
1257             # device_metadata has actually changed is done for us by
1258             # instance.save().
1259             instance.device_metadata = self._build_device_metadata(
1260                 context, instance)
1261             instance.save()
1262         except Exception:
1263             LOG.exception(_('Failed to attach volume at mountpoint: %s'),
1264                           mountpoint, instance=instance)
1265             with excutils.save_and_reraise_exception():
1266                 self._disconnect_volume(connection_info, disk_dev, instance)
1267 
1268     def _swap_volume(self, guest, disk_path, conf, resize_to):
1269         """Swap existing disk with a new block device."""
1270         dev = guest.get_block_device(disk_path)
1271 
1272         # Save a copy of the domain's persistent XML file. We'll use this
1273         # to redefine the domain if anything fails during the volume swap.
1274         xml = guest.get_xml_desc(dump_inactive=True, dump_sensitive=True)
1275 
1276         # Abort is an idempotent operation, so make sure any block
1277         # jobs which may have failed are ended.
1278         try:
1279             dev.abort_job()
1280         except Exception:
1281             pass
1282 
1283         try:
1284             # NOTE (rmk): blockRebase cannot be executed on persistent
1285             #             domains, so we need to temporarily undefine it.
1286             #             If any part of this block fails, the domain is
1287             #             re-defined regardless.
1288             if guest.has_persistent_configuration():
1289                 support_uefi = self._has_uefi_support()
1290                 guest.delete_configuration(support_uefi)
1291 
1292             try:
1293                 # Start copy with VIR_DOMAIN_BLOCK_REBASE_REUSE_EXT flag to
1294                 # allow writing to existing external volume file. Use
1295                 # VIR_DOMAIN_BLOCK_REBASE_COPY_DEV if it's a block device to
1296                 # make sure XML is generated correctly (bug 1691195)
1297                 copy_dev = conf.source_type == 'block'
1298                 dev.rebase(conf.source_path, copy=True, reuse_ext=True,
1299                            copy_dev=copy_dev)
1300                 while not dev.is_job_complete():
1301                     time.sleep(0.5)
1302 
1303                 dev.abort_job(pivot=True)
1304 
1305             except Exception as exc:
1306                 LOG.exception("Failure rebasing volume %(new_path)s on "
1307                     "%(old_path)s.", {'new_path': conf.source_path,
1308                                       'old_path': disk_path})
1309                 raise exception.VolumeRebaseFailed(reason=six.text_type(exc))
1310 
1311             if resize_to:
1312                 dev.resize(resize_to * units.Gi / units.Ki)
1313 
1314             # Make sure we will redefine the domain using the updated
1315             # configuration after the volume was swapped.
1316             xml = guest.get_xml_desc(dump_sensitive=True)
1317         finally:
1318             self._host.write_instance_config(xml)
1319 
1320     def swap_volume(self, old_connection_info,
1321                     new_connection_info, instance, mountpoint, resize_to):
1322 
1323         guest = self._host.get_guest(instance)
1324 
1325         disk_dev = mountpoint.rpartition("/")[2]
1326         if not guest.get_disk(disk_dev):
1327             raise exception.DiskNotFound(location=disk_dev)
1328         disk_info = {
1329             'dev': disk_dev,
1330             'bus': blockinfo.get_disk_bus_for_disk_dev(
1331                 CONF.libvirt.virt_type, disk_dev),
1332             'type': 'disk',
1333             }
1334         # NOTE (lyarwood): new_connection_info will be modified by the
1335         # following _connect_volume call down into the volume drivers. The
1336         # majority of the volume drivers will add a device_path that is in turn
1337         # used by _get_volume_config to set the source_path of the
1338         # LibvirtConfigGuestDisk object it returns. We do not explicitly save
1339         # this to the BDM here as the upper compute swap_volume method will
1340         # eventually do this for us.
1341         self._connect_volume(new_connection_info, disk_info, instance)
1342         conf = self._get_volume_config(new_connection_info, disk_info)
1343         if not conf.source_path:
1344             self._disconnect_volume(new_connection_info, disk_dev, instance)
1345             raise NotImplementedError(_("Swap only supports host devices"))
1346 
1347         try:
1348             self._swap_volume(guest, disk_dev, conf, resize_to)
1349         except exception.VolumeRebaseFailed:
1350             with excutils.save_and_reraise_exception():
1351                 self._disconnect_volume(new_connection_info, disk_dev,
1352                                         instance)
1353 
1354         self._disconnect_volume(old_connection_info, disk_dev, instance)
1355 
1356     def _get_existing_domain_xml(self, instance, network_info,
1357                                  block_device_info=None):
1358         try:
1359             guest = self._host.get_guest(instance)
1360             xml = guest.get_xml_desc()
1361         except exception.InstanceNotFound:
1362             disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
1363                                                 instance,
1364                                                 instance.image_meta,
1365                                                 block_device_info)
1366             xml = self._get_guest_xml(nova_context.get_admin_context(),
1367                                       instance, network_info, disk_info,
1368                                       instance.image_meta,
1369                                       block_device_info=block_device_info)
1370         return xml
1371 
1372     def detach_volume(self, connection_info, instance, mountpoint,
1373                       encryption=None):
1374         disk_dev = mountpoint.rpartition("/")[2]
1375         try:
1376             guest = self._host.get_guest(instance)
1377 
1378             state = guest.get_power_state(self._host)
1379             live = state in (power_state.RUNNING, power_state.PAUSED)
1380 
1381             # The volume must be detached from the VM before disconnecting it
1382             # from its encryptor. Otherwise, the encryptor may report that the
1383             # volume is still in use.
1384             wait_for_detach = guest.detach_device_with_retry(guest.get_disk,
1385                                                              disk_dev,
1386                                                              live=live)
1387             wait_for_detach()
1388 
1389             if encryption:
1390                 encryptor = self._get_volume_encryptor(connection_info,
1391                                                        encryption)
1392                 encryptor.detach_volume(**encryption)
1393 
1394         except exception.InstanceNotFound:
1395             # NOTE(zhaoqin): If the instance does not exist, _lookup_by_name()
1396             #                will throw InstanceNotFound exception. Need to
1397             #                disconnect volume under this circumstance.
1398             LOG.warning("During detach_volume, instance disappeared.",
1399                         instance=instance)
1400         except exception.DeviceNotFound:
1401             raise exception.DiskNotFound(location=disk_dev)
1402         except libvirt.libvirtError as ex:
1403             # NOTE(vish): This is called to cleanup volumes after live
1404             #             migration, so we should still disconnect even if
1405             #             the instance doesn't exist here anymore.
1406             error_code = ex.get_error_code()
1407             if error_code == libvirt.VIR_ERR_NO_DOMAIN:
1408                 # NOTE(vish):
1409                 LOG.warning("During detach_volume, instance disappeared.",
1410                             instance=instance)
1411             else:
1412                 raise
1413 
1414         self._disconnect_volume(connection_info, disk_dev, instance)
1415 
1416     def extend_volume(self, connection_info, instance):
1417         try:
1418             new_size = self._extend_volume(connection_info, instance)
1419         except NotImplementedError:
1420             raise exception.ExtendVolumeNotSupported()
1421 
1422         # Resize the device in QEMU so its size is updated and
1423         # detected by the instance without rebooting.
1424         try:
1425             guest = self._host.get_guest(instance)
1426             state = guest.get_power_state(self._host)
1427             active_state = state in (power_state.RUNNING, power_state.PAUSED)
1428             if active_state:
1429                 disk_path = connection_info['data']['device_path']
1430                 LOG.debug('resizing block device %(dev)s to %(size)u kb',
1431                           {'dev': disk_path, 'size': new_size})
1432                 dev = guest.get_block_device(disk_path)
1433                 dev.resize(new_size // units.Ki)
1434             else:
1435                 LOG.debug('Skipping block device resize, guest is not running',
1436                           instance=instance)
1437         except exception.InstanceNotFound:
1438             with excutils.save_and_reraise_exception():
1439                 LOG.warning('During extend_volume, instance disappeared.',
1440                             instance=instance)
1441         except libvirt.libvirtError:
1442             with excutils.save_and_reraise_exception():
1443                 LOG.exception('resizing block device failed.',
1444                               instance=instance)
1445 
1446     def attach_interface(self, context, instance, image_meta, vif):
1447         guest = self._host.get_guest(instance)
1448 
1449         self.vif_driver.plug(instance, vif)
1450         self.firewall_driver.setup_basic_filtering(instance, [vif])
1451         cfg = self.vif_driver.get_config(instance, vif, image_meta,
1452                                          instance.flavor,
1453                                          CONF.libvirt.virt_type,
1454                                          self._host)
1455         try:
1456             state = guest.get_power_state(self._host)
1457             live = state in (power_state.RUNNING, power_state.PAUSED)
1458             guest.attach_device(cfg, persistent=True, live=live)
1459         except libvirt.libvirtError:
1460             LOG.error('attaching network adapter failed.',
1461                       instance=instance, exc_info=True)
1462             self.vif_driver.unplug(instance, vif)
1463             raise exception.InterfaceAttachFailed(
1464                     instance_uuid=instance.uuid)
1465         try:
1466             # NOTE(artom) If we're attaching with a device role tag, we need to
1467             # rebuild device_metadata. If we're attaching without a role
1468             # tag, we're rebuilding it here needlessly anyways. This isn't a
1469             # massive deal, and it helps reduce code complexity by not having
1470             # to indicate to the virt driver that the attach is tagged. The
1471             # really important optimization of not calling the database unless
1472             # device_metadata has actually changed is done for us by
1473             # instance.save().
1474             instance.device_metadata = self._build_device_metadata(
1475                 context, instance)
1476             instance.save()
1477         except Exception:
1478             # NOTE(artom) If we fail here it means the interface attached
1479             # successfully but building and/or saving the device metadata
1480             # failed. Just unplugging the vif is therefore not enough cleanup,
1481             # we need to detach the interface.
1482             with excutils.save_and_reraise_exception(reraise=False):
1483                 LOG.error('Interface attached successfully but building '
1484                           'and/or saving device metadata failed.',
1485                           instance=instance, exc_info=True)
1486                 self.detach_interface(context, instance, vif)
1487                 raise exception.InterfaceAttachFailed(
1488                     instance_uuid=instance.uuid)
1489 
1490     def detach_interface(self, context, instance, vif):
1491         guest = self._host.get_guest(instance)
1492         cfg = self.vif_driver.get_config(instance, vif,
1493                                          instance.image_meta,
1494                                          instance.flavor,
1495                                          CONF.libvirt.virt_type, self._host)
1496         interface = guest.get_interface_by_cfg(cfg)
1497         try:
1498             self.vif_driver.unplug(instance, vif)
1499             # NOTE(mriedem): When deleting an instance and using Neutron,
1500             # we can be racing against Neutron deleting the port and
1501             # sending the vif-deleted event which then triggers a call to
1502             # detach the interface, so if the interface is not found then
1503             # we can just log it as a warning.
1504             if not interface:
1505                 mac = vif.get('address')
1506                 # The interface is gone so just log it as a warning.
1507                 LOG.warning('Detaching interface %(mac)s failed because '
1508                             'the device is no longer found on the guest.',
1509                             {'mac': mac}, instance=instance)
1510                 return
1511 
1512             state = guest.get_power_state(self._host)
1513             live = state in (power_state.RUNNING, power_state.PAUSED)
1514             # Now we are going to loop until the interface is detached or we
1515             # timeout.
1516             wait_for_detach = guest.detach_device_with_retry(
1517                 guest.get_interface_by_cfg, cfg, live=live,
1518                 alternative_device_name=self.vif_driver.get_vif_devname(vif))
1519             wait_for_detach()
1520         except exception.DeviceDetachFailed:
1521             # We failed to detach the device even with the retry loop, so let's
1522             # dump some debug information to the logs before raising back up.
1523             with excutils.save_and_reraise_exception():
1524                 devname = self.vif_driver.get_vif_devname(vif)
1525                 interface = guest.get_interface_by_cfg(cfg)
1526                 if interface:
1527                     LOG.warning(
1528                         'Failed to detach interface %(devname)s after '
1529                         'repeated attempts. Final interface xml:\n'
1530                         '%(interface_xml)s\nFinal guest xml:\n%(guest_xml)s',
1531                         {'devname': devname,
1532                          'interface_xml': interface.to_xml(),
1533                          'guest_xml': guest.get_xml_desc()},
1534                         instance=instance)
1535         except exception.DeviceNotFound:
1536             # The interface is gone so just log it as a warning.
1537             LOG.warning('Detaching interface %(mac)s failed because '
1538                         'the device is no longer found on the guest.',
1539                         {'mac': vif.get('address')}, instance=instance)
1540         except libvirt.libvirtError as ex:
1541             error_code = ex.get_error_code()
1542             if error_code == libvirt.VIR_ERR_NO_DOMAIN:
1543                 LOG.warning("During detach_interface, instance disappeared.",
1544                             instance=instance)
1545             else:
1546                 # NOTE(mriedem): When deleting an instance and using Neutron,
1547                 # we can be racing against Neutron deleting the port and
1548                 # sending the vif-deleted event which then triggers a call to
1549                 # detach the interface, so we might have failed because the
1550                 # network device no longer exists. Libvirt will fail with
1551                 # "operation failed: no matching network device was found"
1552                 # which unfortunately does not have a unique error code so we
1553                 # need to look up the interface by config and if it's not found
1554                 # then we can just log it as a warning rather than tracing an
1555                 # error.
1556                 mac = vif.get('address')
1557                 interface = guest.get_interface_by_cfg(cfg)
1558                 if interface:
1559                     LOG.error('detaching network adapter failed.',
1560                               instance=instance, exc_info=True)
1561                     raise exception.InterfaceDetachFailed(
1562                             instance_uuid=instance.uuid)
1563 
1564                 # The interface is gone so just log it as a warning.
1565                 LOG.warning('Detaching interface %(mac)s failed because '
1566                             'the device is no longer found on the guest.',
1567                             {'mac': mac}, instance=instance)
1568 
1569     def _create_snapshot_metadata(self, image_meta, instance,
1570                                   img_fmt, snp_name):
1571         metadata = {'is_public': False,
1572                     'status': 'active',
1573                     'name': snp_name,
1574                     'properties': {
1575                                    'kernel_id': instance.kernel_id,
1576                                    'image_location': 'snapshot',
1577                                    'image_state': 'available',
1578                                    'owner_id': instance.project_id,
1579                                    'ramdisk_id': instance.ramdisk_id,
1580                                    }
1581                     }
1582         if instance.os_type:
1583             metadata['properties']['os_type'] = instance.os_type
1584 
1585         # NOTE(vish): glance forces ami disk format to be ami
1586         if image_meta.disk_format == 'ami':
1587             metadata['disk_format'] = 'ami'
1588         else:
1589             metadata['disk_format'] = img_fmt
1590 
1591         if image_meta.obj_attr_is_set("container_format"):
1592             metadata['container_format'] = image_meta.container_format
1593         else:
1594             metadata['container_format'] = "bare"
1595 
1596         return metadata
1597 
1598     def snapshot(self, context, instance, image_id, update_task_state):
1599         """Create snapshot from a running VM instance.
1600 
1601         This command only works with qemu 0.14+
1602         """
1603         try:
1604             guest = self._host.get_guest(instance)
1605 
1606             # TODO(sahid): We are converting all calls from a
1607             # virDomain object to use nova.virt.libvirt.Guest.
1608             # We should be able to remove virt_dom at the end.
1609             virt_dom = guest._domain
1610         except exception.InstanceNotFound:
1611             raise exception.InstanceNotRunning(instance_id=instance.uuid)
1612 
1613         snapshot = self._image_api.get(context, image_id)
1614 
1615         # source_format is an on-disk format
1616         # source_type is a backend type
1617         disk_path, source_format = libvirt_utils.find_disk(guest)
1618         source_type = libvirt_utils.get_disk_type_from_path(disk_path)
1619 
1620         # We won't have source_type for raw or qcow2 disks, because we can't
1621         # determine that from the path. We should have it from the libvirt
1622         # xml, though.
1623         if source_type is None:
1624             source_type = source_format
1625         # For lxc instances we won't have it either from libvirt xml
1626         # (because we just gave libvirt the mounted filesystem), or the path,
1627         # so source_type is still going to be None. In this case,
1628         # root_disk is going to default to CONF.libvirt.images_type
1629         # below, which is still safe.
1630 
1631         image_format = CONF.libvirt.snapshot_image_format or source_type
1632 
1633         # NOTE(bfilippov): save lvm and rbd as raw
1634         if image_format == 'lvm' or image_format == 'rbd':
1635             image_format = 'raw'
1636 
1637         metadata = self._create_snapshot_metadata(instance.image_meta,
1638                                                   instance,
1639                                                   image_format,
1640                                                   snapshot['name'])
1641 
1642         snapshot_name = uuid.uuid4().hex
1643 
1644         state = guest.get_power_state(self._host)
1645 
1646         # NOTE(dgenin): Instances with LVM encrypted ephemeral storage require
1647         #               cold snapshots. Currently, checking for encryption is
1648         #               redundant because LVM supports only cold snapshots.
1649         #               It is necessary in case this situation changes in the
1650         #               future.
1651         if (self._host.has_min_version(hv_type=host.HV_DRIVER_QEMU)
1652              and source_type not in ('lvm')
1653              and not CONF.ephemeral_storage_encryption.enabled
1654              and not CONF.workarounds.disable_libvirt_livesnapshot):
1655             live_snapshot = True
1656             # Abort is an idempotent operation, so make sure any block
1657             # jobs which may have failed are ended. This operation also
1658             # confirms the running instance, as opposed to the system as a
1659             # whole, has a new enough version of the hypervisor (bug 1193146).
1660             try:
1661                 guest.get_block_device(disk_path).abort_job()
1662             except libvirt.libvirtError as ex:
1663                 error_code = ex.get_error_code()
1664                 if error_code == libvirt.VIR_ERR_CONFIG_UNSUPPORTED:
1665                     live_snapshot = False
1666                 else:
1667                     pass
1668         else:
1669             live_snapshot = False
1670 
1671         # NOTE(rmk): We cannot perform live snapshots when a managedSave
1672         #            file is present, so we will use the cold/legacy method
1673         #            for instances which are shutdown.
1674         if state == power_state.SHUTDOWN:
1675             live_snapshot = False
1676 
1677         self._prepare_domain_for_snapshot(context, live_snapshot, state,
1678                                           instance)
1679 
1680         root_disk = self.image_backend.by_libvirt_path(
1681             instance, disk_path, image_type=source_type)
1682 
1683         if live_snapshot:
1684             LOG.info("Beginning live snapshot process", instance=instance)
1685         else:
1686             LOG.info("Beginning cold snapshot process", instance=instance)
1687 
1688         update_task_state(task_state=task_states.IMAGE_PENDING_UPLOAD)
1689 
1690         try:
1691             update_task_state(task_state=task_states.IMAGE_UPLOADING,
1692                               expected_state=task_states.IMAGE_PENDING_UPLOAD)
1693             metadata['location'] = root_disk.direct_snapshot(
1694                 context, snapshot_name, image_format, image_id,
1695                 instance.image_ref)
1696             self._snapshot_domain(context, live_snapshot, virt_dom, state,
1697                                   instance)
1698             self._image_api.update(context, image_id, metadata,
1699                                    purge_props=False)
1700         except (NotImplementedError, exception.ImageUnacceptable,
1701                 exception.Forbidden) as e:
1702             if type(e) != NotImplementedError:
1703                 LOG.warning('Performing standard snapshot because direct '
1704                             'snapshot failed: %(error)s', {'error': e})
1705             failed_snap = metadata.pop('location', None)
1706             if failed_snap:
1707                 failed_snap = {'url': str(failed_snap)}
1708             root_disk.cleanup_direct_snapshot(failed_snap,
1709                                                   also_destroy_volume=True,
1710                                                   ignore_errors=True)
1711             update_task_state(task_state=task_states.IMAGE_PENDING_UPLOAD,
1712                               expected_state=task_states.IMAGE_UPLOADING)
1713 
1714             # TODO(nic): possibly abstract this out to the root_disk
1715             if source_type == 'rbd' and live_snapshot:
1716                 # Standard snapshot uses qemu-img convert from RBD which is
1717                 # not safe to run with live_snapshot.
1718                 live_snapshot = False
1719                 # Suspend the guest, so this is no longer a live snapshot
1720                 self._prepare_domain_for_snapshot(context, live_snapshot,
1721                                                   state, instance)
1722 
1723             snapshot_directory = CONF.libvirt.snapshots_directory
1724             fileutils.ensure_tree(snapshot_directory)
1725             with utils.tempdir(dir=snapshot_directory) as tmpdir:
1726                 try:
1727                     out_path = os.path.join(tmpdir, snapshot_name)
1728                     if live_snapshot:
1729                         # NOTE(xqueralt): libvirt needs o+x in the tempdir
1730                         os.chmod(tmpdir, 0o701)
1731                         self._live_snapshot(context, instance, guest,
1732                                             disk_path, out_path, source_format,
1733                                             image_format, instance.image_meta)
1734                     else:
1735                         root_disk.snapshot_extract(out_path, image_format)
1736                 finally:
1737                     self._snapshot_domain(context, live_snapshot, virt_dom,
1738                                           state, instance)
1739                     LOG.info("Snapshot extracted, beginning image upload",
1740                              instance=instance)
1741 
1742                 # Upload that image to the image service
1743                 update_task_state(task_state=task_states.IMAGE_UPLOADING,
1744                         expected_state=task_states.IMAGE_PENDING_UPLOAD)
1745                 with libvirt_utils.file_open(out_path, 'rb') as image_file:
1746                     self._image_api.update(context,
1747                                            image_id,
1748                                            metadata,
1749                                            image_file)
1750         except Exception:
1751             with excutils.save_and_reraise_exception():
1752                 LOG.exception(_("Failed to snapshot image"))
1753                 failed_snap = metadata.pop('location', None)
1754                 if failed_snap:
1755                     failed_snap = {'url': str(failed_snap)}
1756                 root_disk.cleanup_direct_snapshot(
1757                         failed_snap, also_destroy_volume=True,
1758                         ignore_errors=True)
1759 
1760         LOG.info("Snapshot image upload complete", instance=instance)
1761 
1762     def _prepare_domain_for_snapshot(self, context, live_snapshot, state,
1763                                      instance):
1764         # NOTE(dkang): managedSave does not work for LXC
1765         if CONF.libvirt.virt_type != 'lxc' and not live_snapshot:
1766             if state == power_state.RUNNING or state == power_state.PAUSED:
1767                 self.suspend(context, instance)
1768 
1769     def _snapshot_domain(self, context, live_snapshot, virt_dom, state,
1770                          instance):
1771         guest = None
1772         # NOTE(dkang): because previous managedSave is not called
1773         #              for LXC, _create_domain must not be called.
1774         if CONF.libvirt.virt_type != 'lxc' and not live_snapshot:
1775             if state == power_state.RUNNING:
1776                 guest = self._create_domain(domain=virt_dom)
1777             elif state == power_state.PAUSED:
1778                 guest = self._create_domain(domain=virt_dom, pause=True)
1779 
1780             if guest is not None:
1781                 self._attach_pci_devices(
1782                     guest, pci_manager.get_instance_pci_devs(instance))
1783                 self._attach_direct_passthrough_ports(
1784                     context, instance, guest)
1785 
1786     def _can_set_admin_password(self, image_meta):
1787 
1788         if CONF.libvirt.virt_type == 'parallels':
1789             if not self._host.has_min_version(
1790                    MIN_LIBVIRT_PARALLELS_SET_ADMIN_PASSWD):
1791                 raise exception.SetAdminPasswdNotSupported()
1792         elif CONF.libvirt.virt_type in ('kvm', 'qemu'):
1793             if not self._host.has_min_version(
1794                    MIN_LIBVIRT_SET_ADMIN_PASSWD):
1795                 raise exception.SetAdminPasswdNotSupported()
1796             if not image_meta.properties.get('hw_qemu_guest_agent', False):
1797                 raise exception.QemuGuestAgentNotEnabled()
1798         else:
1799             raise exception.SetAdminPasswdNotSupported()
1800 
1801     def set_admin_password(self, instance, new_pass):
1802         self._can_set_admin_password(instance.image_meta)
1803 
1804         guest = self._host.get_guest(instance)
1805         user = instance.image_meta.properties.get("os_admin_user")
1806         if not user:
1807             if instance.os_type == "windows":
1808                 user = "Administrator"
1809             else:
1810                 user = "root"
1811         try:
1812             guest.set_user_password(user, new_pass)
1813         except libvirt.libvirtError as ex:
1814             error_code = ex.get_error_code()
1815             msg = (_('Error from libvirt while set password for username '
1816                      '"%(user)s": [Error Code %(error_code)s] %(ex)s')
1817                    % {'user': user, 'error_code': error_code, 'ex': ex})
1818             raise exception.InternalError(msg)
1819 
1820     def _can_quiesce(self, instance, image_meta):
1821         if CONF.libvirt.virt_type not in ('kvm', 'qemu'):
1822             raise exception.InstanceQuiesceNotSupported(
1823                 instance_id=instance.uuid)
1824 
1825         if not image_meta.properties.get('hw_qemu_guest_agent', False):
1826             raise exception.QemuGuestAgentNotEnabled()
1827 
1828     def _requires_quiesce(self, image_meta):
1829         return image_meta.properties.get('os_require_quiesce', False)
1830 
1831     def _set_quiesced(self, context, instance, image_meta, quiesced):
1832         self._can_quiesce(instance, image_meta)
1833         try:
1834             guest = self._host.get_guest(instance)
1835             if quiesced:
1836                 guest.freeze_filesystems()
1837             else:
1838                 guest.thaw_filesystems()
1839         except libvirt.libvirtError as ex:
1840             error_code = ex.get_error_code()
1841             msg = (_('Error from libvirt while quiescing %(instance_name)s: '
1842                      '[Error Code %(error_code)s] %(ex)s')
1843                    % {'instance_name': instance.name,
1844                       'error_code': error_code, 'ex': ex})
1845             raise exception.InternalError(msg)
1846 
1847     def quiesce(self, context, instance, image_meta):
1848         """Freeze the guest filesystems to prepare for snapshot.
1849 
1850         The qemu-guest-agent must be setup to execute fsfreeze.
1851         """
1852         self._set_quiesced(context, instance, image_meta, True)
1853 
1854     def unquiesce(self, context, instance, image_meta):
1855         """Thaw the guest filesystems after snapshot."""
1856         self._set_quiesced(context, instance, image_meta, False)
1857 
1858     def _live_snapshot(self, context, instance, guest, disk_path, out_path,
1859                        source_format, image_format, image_meta):
1860         """Snapshot an instance without downtime."""
1861         dev = guest.get_block_device(disk_path)
1862 
1863         # Save a copy of the domain's persistent XML file
1864         xml = guest.get_xml_desc(dump_inactive=True, dump_sensitive=True)
1865 
1866         # Abort is an idempotent operation, so make sure any block
1867         # jobs which may have failed are ended.
1868         try:
1869             dev.abort_job()
1870         except Exception:
1871             pass
1872 
1873         # NOTE (rmk): We are using shallow rebases as a workaround to a bug
1874         #             in QEMU 1.3. In order to do this, we need to create
1875         #             a destination image with the original backing file
1876         #             and matching size of the instance root disk.
1877         src_disk_size = libvirt_utils.get_disk_size(disk_path,
1878                                                     format=source_format)
1879         src_back_path = libvirt_utils.get_disk_backing_file(disk_path,
1880                                                         format=source_format,
1881                                                         basename=False)
1882         disk_delta = out_path + '.delta'
1883         libvirt_utils.create_cow_image(src_back_path, disk_delta,
1884                                        src_disk_size)
1885 
1886         quiesced = False
1887         try:
1888             self._set_quiesced(context, instance, image_meta, True)
1889             quiesced = True
1890         except exception.NovaException as err:
1891             if self._requires_quiesce(image_meta):
1892                 raise
1893             LOG.info('Skipping quiescing instance: %(reason)s.',
1894                      {'reason': err}, instance=instance)
1895 
1896         try:
1897             # NOTE (rmk): blockRebase cannot be executed on persistent
1898             #             domains, so we need to temporarily undefine it.
1899             #             If any part of this block fails, the domain is
1900             #             re-defined regardless.
1901             if guest.has_persistent_configuration():
1902                 support_uefi = self._has_uefi_support()
1903                 guest.delete_configuration(support_uefi)
1904 
1905             # NOTE (rmk): Establish a temporary mirror of our root disk and
1906             #             issue an abort once we have a complete copy.
1907             dev.rebase(disk_delta, copy=True, reuse_ext=True, shallow=True)
1908 
1909             while not dev.is_job_complete():
1910                 time.sleep(0.5)
1911 
1912             dev.abort_job()
1913             libvirt_utils.chown(disk_delta, os.getuid())
1914         finally:
1915             self._host.write_instance_config(xml)
1916             if quiesced:
1917                 self._set_quiesced(context, instance, image_meta, False)
1918 
1919         # Convert the delta (CoW) image with a backing file to a flat
1920         # image with no backing file.
1921         libvirt_utils.extract_snapshot(disk_delta, 'qcow2',
1922                                        out_path, image_format)
1923 
1924     def _volume_snapshot_update_status(self, context, snapshot_id, status):
1925         """Send a snapshot status update to Cinder.
1926 
1927         This method captures and logs exceptions that occur
1928         since callers cannot do anything useful with these exceptions.
1929 
1930         Operations on the Cinder side waiting for this will time out if
1931         a failure occurs sending the update.
1932 
1933         :param context: security context
1934         :param snapshot_id: id of snapshot being updated
1935         :param status: new status value
1936 
1937         """
1938 
1939         try:
1940             self._volume_api.update_snapshot_status(context,
1941                                                     snapshot_id,
1942                                                     status)
1943         except Exception:
1944             LOG.exception(_('Failed to send updated snapshot status '
1945                             'to volume service.'))
1946 
1947     def _volume_snapshot_create(self, context, instance, guest,
1948                                 volume_id, new_file):
1949         """Perform volume snapshot.
1950 
1951            :param guest: VM that volume is attached to
1952            :param volume_id: volume UUID to snapshot
1953            :param new_file: relative path to new qcow2 file present on share
1954 
1955         """
1956         xml = guest.get_xml_desc()
1957         xml_doc = etree.fromstring(xml)
1958 
1959         device_info = vconfig.LibvirtConfigGuest()
1960         device_info.parse_dom(xml_doc)
1961 
1962         disks_to_snap = []          # to be snapshotted by libvirt
1963         network_disks_to_snap = []  # network disks (netfs, etc.)
1964         disks_to_skip = []          # local disks not snapshotted
1965 
1966         for guest_disk in device_info.devices:
1967             if (guest_disk.root_name != 'disk'):
1968                 continue
1969 
1970             if (guest_disk.target_dev is None):
1971                 continue
1972 
1973             if (guest_disk.serial is None or guest_disk.serial != volume_id):
1974                 disks_to_skip.append(guest_disk.target_dev)
1975                 continue
1976 
1977             # disk is a Cinder volume with the correct volume_id
1978 
1979             disk_info = {
1980                 'dev': guest_disk.target_dev,
1981                 'serial': guest_disk.serial,
1982                 'current_file': guest_disk.source_path,
1983                 'source_protocol': guest_disk.source_protocol,
1984                 'source_name': guest_disk.source_name,
1985                 'source_hosts': guest_disk.source_hosts,
1986                 'source_ports': guest_disk.source_ports
1987             }
1988 
1989             # Determine path for new_file based on current path
1990             if disk_info['current_file'] is not None:
1991                 current_file = disk_info['current_file']
1992                 new_file_path = os.path.join(os.path.dirname(current_file),
1993                                              new_file)
1994                 disks_to_snap.append((current_file, new_file_path))
1995             # NOTE(mriedem): This used to include a check for gluster in
1996             # addition to netfs since they were added together. Support for
1997             # gluster was removed in the 16.0.0 Pike release. It is unclear,
1998             # however, if other volume drivers rely on the netfs disk source
1999             # protocol.
2000             elif disk_info['source_protocol'] == 'netfs':
2001                 network_disks_to_snap.append((disk_info, new_file))
2002 
2003         if not disks_to_snap and not network_disks_to_snap:
2004             msg = _('Found no disk to snapshot.')
2005             raise exception.InternalError(msg)
2006 
2007         snapshot = vconfig.LibvirtConfigGuestSnapshot()
2008 
2009         for current_name, new_filename in disks_to_snap:
2010             snap_disk = vconfig.LibvirtConfigGuestSnapshotDisk()
2011             snap_disk.name = current_name
2012             snap_disk.source_path = new_filename
2013             snap_disk.source_type = 'file'
2014             snap_disk.snapshot = 'external'
2015             snap_disk.driver_name = 'qcow2'
2016 
2017             snapshot.add_disk(snap_disk)
2018 
2019         for disk_info, new_filename in network_disks_to_snap:
2020             snap_disk = vconfig.LibvirtConfigGuestSnapshotDisk()
2021             snap_disk.name = disk_info['dev']
2022             snap_disk.source_type = 'network'
2023             snap_disk.source_protocol = disk_info['source_protocol']
2024             snap_disk.snapshot = 'external'
2025             snap_disk.source_path = new_filename
2026             old_dir = disk_info['source_name'].split('/')[0]
2027             snap_disk.source_name = '%s/%s' % (old_dir, new_filename)
2028             snap_disk.source_hosts = disk_info['source_hosts']
2029             snap_disk.source_ports = disk_info['source_ports']
2030 
2031             snapshot.add_disk(snap_disk)
2032 
2033         for dev in disks_to_skip:
2034             snap_disk = vconfig.LibvirtConfigGuestSnapshotDisk()
2035             snap_disk.name = dev
2036             snap_disk.snapshot = 'no'
2037 
2038             snapshot.add_disk(snap_disk)
2039 
2040         snapshot_xml = snapshot.to_xml()
2041         LOG.debug("snap xml: %s", snapshot_xml, instance=instance)
2042 
2043         image_meta = instance.image_meta
2044         try:
2045             # Check to see if we can quiesce the guest before taking the
2046             # snapshot.
2047             self._can_quiesce(instance, image_meta)
2048             try:
2049                 guest.snapshot(snapshot, no_metadata=True, disk_only=True,
2050                                reuse_ext=True, quiesce=True)
2051                 return
2052             except libvirt.libvirtError:
2053                 # If the image says that quiesce is required then we fail.
2054                 if self._requires_quiesce(image_meta):
2055                     raise
2056                 LOG.exception(_('Unable to create quiesced VM snapshot, '
2057                                 'attempting again with quiescing disabled.'),
2058                               instance=instance)
2059         except (exception.InstanceQuiesceNotSupported,
2060                 exception.QemuGuestAgentNotEnabled) as err:
2061             # If the image says that quiesce is required then we need to fail.
2062             if self._requires_quiesce(image_meta):
2063                 raise
2064             LOG.info('Skipping quiescing instance: %(reason)s.',
2065                      {'reason': err}, instance=instance)
2066 
2067         try:
2068             guest.snapshot(snapshot, no_metadata=True, disk_only=True,
2069                            reuse_ext=True, quiesce=False)
2070         except libvirt.libvirtError:
2071             LOG.exception(_('Unable to create VM snapshot, '
2072                             'failing volume_snapshot operation.'),
2073                           instance=instance)
2074 
2075             raise
2076 
2077     def _volume_refresh_connection_info(self, context, instance, volume_id):
2078         bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
2079                   context, volume_id, instance.uuid)
2080 
2081         driver_bdm = driver_block_device.convert_volume(bdm)
2082         if driver_bdm:
2083             driver_bdm.refresh_connection_info(context, instance,
2084                                                self._volume_api, self)
2085 
2086     def volume_snapshot_create(self, context, instance, volume_id,
2087                                create_info):
2088         """Create snapshots of a Cinder volume via libvirt.
2089 
2090         :param instance: VM instance object reference
2091         :param volume_id: id of volume being snapshotted
2092         :param create_info: dict of information used to create snapshots
2093                      - snapshot_id : ID of snapshot
2094                      - type : qcow2 / <other>
2095                      - new_file : qcow2 file created by Cinder which
2096                      becomes the VM's active image after
2097                      the snapshot is complete
2098         """
2099 
2100         LOG.debug("volume_snapshot_create: create_info: %(c_info)s",
2101                   {'c_info': create_info}, instance=instance)
2102 
2103         try:
2104             guest = self._host.get_guest(instance)
2105         except exception.InstanceNotFound:
2106             raise exception.InstanceNotRunning(instance_id=instance.uuid)
2107 
2108         if create_info['type'] != 'qcow2':
2109             msg = _('Unknown type: %s') % create_info['type']
2110             raise exception.InternalError(msg)
2111 
2112         snapshot_id = create_info.get('snapshot_id', None)
2113         if snapshot_id is None:
2114             msg = _('snapshot_id required in create_info')
2115             raise exception.InternalError(msg)
2116 
2117         try:
2118             self._volume_snapshot_create(context, instance, guest,
2119                                          volume_id, create_info['new_file'])
2120         except Exception:
2121             with excutils.save_and_reraise_exception():
2122                 LOG.exception(_('Error occurred during '
2123                                 'volume_snapshot_create, '
2124                                 'sending error status to Cinder.'),
2125                               instance=instance)
2126                 self._volume_snapshot_update_status(
2127                     context, snapshot_id, 'error')
2128 
2129         self._volume_snapshot_update_status(
2130             context, snapshot_id, 'creating')
2131 
2132         def _wait_for_snapshot():
2133             snapshot = self._volume_api.get_snapshot(context, snapshot_id)
2134 
2135             if snapshot.get('status') != 'creating':
2136                 self._volume_refresh_connection_info(context, instance,
2137                                                      volume_id)
2138                 raise loopingcall.LoopingCallDone()
2139 
2140         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_snapshot)
2141         timer.start(interval=0.5).wait()
2142 
2143     @staticmethod
2144     def _rebase_with_qemu_img(guest, device, active_disk_object,
2145                               rebase_base):
2146         """Rebase a device tied to a guest using qemu-img.
2147 
2148         :param guest:the Guest which owns the device being rebased
2149         :type guest: nova.virt.libvirt.guest.Guest
2150         :param device: the guest block device to rebase
2151         :type device: nova.virt.libvirt.guest.BlockDevice
2152         :param active_disk_object: the guest block device to rebase
2153         :type active_disk_object: nova.virt.libvirt.config.\
2154                                     LibvirtConfigGuestDisk
2155         :param rebase_base: the new parent in the backing chain
2156         :type rebase_base: None or string
2157         """
2158 
2159         # It's unsure how well qemu-img handles network disks for
2160         # every protocol. So let's be safe.
2161         active_protocol = active_disk_object.source_protocol
2162         if active_protocol is not None:
2163             msg = _("Something went wrong when deleting a volume snapshot: "
2164                     "rebasing a %(protocol)s network disk using qemu-img "
2165                     "has not been fully tested") % {'protocol':
2166                     active_protocol}
2167             LOG.error(msg)
2168             raise exception.InternalError(msg)
2169 
2170         if rebase_base is None:
2171             # If backing_file is specified as "" (the empty string), then
2172             # the image is rebased onto no backing file (i.e. it will exist
2173             # independently of any backing file).
2174             backing_file = ""
2175             qemu_img_extra_arg = []
2176         else:
2177             # If the rebased image is going to have a backing file then
2178             # explicitly set the backing file format to avoid any security
2179             # concerns related to file format auto detection.
2180             backing_file = rebase_base
2181             b_file_fmt = images.qemu_img_info(backing_file).file_format
2182             qemu_img_extra_arg = ['-F', b_file_fmt]
2183 
2184         qemu_img_extra_arg.append(active_disk_object.source_path)
2185         utils.execute("qemu-img", "rebase", "-b", backing_file,
2186                       *qemu_img_extra_arg)
2187 
2188     def _volume_snapshot_delete(self, context, instance, volume_id,
2189                                 snapshot_id, delete_info=None):
2190         """Note:
2191             if file being merged into == active image:
2192                 do a blockRebase (pull) operation
2193             else:
2194                 do a blockCommit operation
2195             Files must be adjacent in snap chain.
2196 
2197         :param instance: instance object reference
2198         :param volume_id: volume UUID
2199         :param snapshot_id: snapshot UUID (unused currently)
2200         :param delete_info: {
2201             'type':              'qcow2',
2202             'file_to_merge':     'a.img',
2203             'merge_target_file': 'b.img' or None (if merging file_to_merge into
2204                                                   active image)
2205           }
2206         """
2207 
2208         LOG.debug('volume_snapshot_delete: delete_info: %s', delete_info,
2209                   instance=instance)
2210 
2211         if delete_info['type'] != 'qcow2':
2212             msg = _('Unknown delete_info type %s') % delete_info['type']
2213             raise exception.InternalError(msg)
2214 
2215         try:
2216             guest = self._host.get_guest(instance)
2217         except exception.InstanceNotFound:
2218             raise exception.InstanceNotRunning(instance_id=instance.uuid)
2219 
2220         # Find dev name
2221         my_dev = None
2222         active_disk = None
2223 
2224         xml = guest.get_xml_desc()
2225         xml_doc = etree.fromstring(xml)
2226 
2227         device_info = vconfig.LibvirtConfigGuest()
2228         device_info.parse_dom(xml_doc)
2229 
2230         active_disk_object = None
2231 
2232         for guest_disk in device_info.devices:
2233             if (guest_disk.root_name != 'disk'):
2234                 continue
2235 
2236             if (guest_disk.target_dev is None or guest_disk.serial is None):
2237                 continue
2238 
2239             if guest_disk.serial == volume_id:
2240                 my_dev = guest_disk.target_dev
2241 
2242                 active_disk = guest_disk.source_path
2243                 active_protocol = guest_disk.source_protocol
2244                 active_disk_object = guest_disk
2245                 break
2246 
2247         if my_dev is None or (active_disk is None and active_protocol is None):
2248             LOG.debug('Domain XML: %s', xml, instance=instance)
2249             msg = (_('Disk with id: %s not found attached to instance.')
2250                    % volume_id)
2251             raise exception.InternalError(msg)
2252 
2253         LOG.debug("found device at %s", my_dev, instance=instance)
2254 
2255         def _get_snap_dev(filename, backing_store):
2256             if filename is None:
2257                 msg = _('filename cannot be None')
2258                 raise exception.InternalError(msg)
2259 
2260             # libgfapi delete
2261             LOG.debug("XML: %s", xml)
2262 
2263             LOG.debug("active disk object: %s", active_disk_object)
2264 
2265             # determine reference within backing store for desired image
2266             filename_to_merge = filename
2267             matched_name = None
2268             b = backing_store
2269             index = None
2270 
2271             current_filename = active_disk_object.source_name.split('/')[1]
2272             if current_filename == filename_to_merge:
2273                 return my_dev + '[0]'
2274 
2275             while b is not None:
2276                 source_filename = b.source_name.split('/')[1]
2277                 if source_filename == filename_to_merge:
2278                     LOG.debug('found match: %s', b.source_name)
2279                     matched_name = b.source_name
2280                     index = b.index
2281                     break
2282 
2283                 b = b.backing_store
2284 
2285             if matched_name is None:
2286                 msg = _('no match found for %s') % (filename_to_merge)
2287                 raise exception.InternalError(msg)
2288 
2289             LOG.debug('index of match (%s) is %s', b.source_name, index)
2290 
2291             my_snap_dev = '%s[%s]' % (my_dev, index)
2292             return my_snap_dev
2293 
2294         if delete_info['merge_target_file'] is None:
2295             # pull via blockRebase()
2296 
2297             # Merge the most recent snapshot into the active image
2298 
2299             rebase_disk = my_dev
2300             rebase_base = delete_info['file_to_merge']  # often None
2301             if (active_protocol is not None) and (rebase_base is not None):
2302                 rebase_base = _get_snap_dev(rebase_base,
2303                                             active_disk_object.backing_store)
2304 
2305             # NOTE(deepakcs): libvirt added support for _RELATIVE in v1.2.7,
2306             # and when available this flag _must_ be used to ensure backing
2307             # paths are maintained relative by qemu.
2308             #
2309             # If _RELATIVE flag not found, continue with old behaviour
2310             # (relative backing path seems to work for this case)
2311             try:
2312                 libvirt.VIR_DOMAIN_BLOCK_REBASE_RELATIVE
2313                 relative = rebase_base is not None
2314             except AttributeError:
2315                 LOG.warning(
2316                     "Relative blockrebase support was not detected. "
2317                     "Continuing with old behaviour.")
2318                 relative = False
2319 
2320             LOG.debug(
2321                 'disk: %(disk)s, base: %(base)s, '
2322                 'bw: %(bw)s, relative: %(relative)s',
2323                 {'disk': rebase_disk,
2324                  'base': rebase_base,
2325                  'bw': libvirt_guest.BlockDevice.REBASE_DEFAULT_BANDWIDTH,
2326                  'relative': str(relative)}, instance=instance)
2327 
2328             dev = guest.get_block_device(rebase_disk)
2329             if guest.is_active():
2330                 result = dev.rebase(rebase_base, relative=relative)
2331                 if result == 0:
2332                     LOG.debug('blockRebase started successfully',
2333                               instance=instance)
2334 
2335                 while not dev.is_job_complete():
2336                     LOG.debug('waiting for blockRebase job completion',
2337                               instance=instance)
2338                     time.sleep(0.5)
2339 
2340             # If the guest is not running libvirt won't do a blockRebase.
2341             # In that case, let's ask qemu-img to rebase the disk.
2342             else:
2343                 LOG.debug('Guest is not running so doing a block rebase '
2344                           'using "qemu-img rebase"', instance=instance)
2345                 self._rebase_with_qemu_img(guest, dev, active_disk_object,
2346                                            rebase_base)
2347 
2348         else:
2349             # commit with blockCommit()
2350             my_snap_base = None
2351             my_snap_top = None
2352             commit_disk = my_dev
2353 
2354             if active_protocol is not None:
2355                 my_snap_base = _get_snap_dev(delete_info['merge_target_file'],
2356                                              active_disk_object.backing_store)
2357                 my_snap_top = _get_snap_dev(delete_info['file_to_merge'],
2358                                             active_disk_object.backing_store)
2359 
2360             commit_base = my_snap_base or delete_info['merge_target_file']
2361             commit_top = my_snap_top or delete_info['file_to_merge']
2362 
2363             LOG.debug('will call blockCommit with commit_disk=%(commit_disk)s '
2364                       'commit_base=%(commit_base)s '
2365                       'commit_top=%(commit_top)s ',
2366                       {'commit_disk': commit_disk,
2367                        'commit_base': commit_base,
2368                        'commit_top': commit_top}, instance=instance)
2369 
2370             dev = guest.get_block_device(commit_disk)
2371             result = dev.commit(commit_base, commit_top, relative=True)
2372 
2373             if result == 0:
2374                 LOG.debug('blockCommit started successfully',
2375                           instance=instance)
2376 
2377             while not dev.is_job_complete():
2378                 LOG.debug('waiting for blockCommit job completion',
2379                           instance=instance)
2380                 time.sleep(0.5)
2381 
2382     def volume_snapshot_delete(self, context, instance, volume_id, snapshot_id,
2383                                delete_info):
2384         try:
2385             self._volume_snapshot_delete(context, instance, volume_id,
2386                                          snapshot_id, delete_info=delete_info)
2387         except Exception:
2388             with excutils.save_and_reraise_exception():
2389                 LOG.exception(_('Error occurred during '
2390                                 'volume_snapshot_delete, '
2391                                 'sending error status to Cinder.'),
2392                               instance=instance)
2393                 self._volume_snapshot_update_status(
2394                     context, snapshot_id, 'error_deleting')
2395 
2396         self._volume_snapshot_update_status(context, snapshot_id, 'deleting')
2397         self._volume_refresh_connection_info(context, instance, volume_id)
2398 
2399     def reboot(self, context, instance, network_info, reboot_type,
2400                block_device_info=None, bad_volumes_callback=None):
2401         """Reboot a virtual machine, given an instance reference."""
2402         if reboot_type == 'SOFT':
2403             # NOTE(vish): This will attempt to do a graceful shutdown/restart.
2404             try:
2405                 soft_reboot_success = self._soft_reboot(instance)
2406             except libvirt.libvirtError as e:
2407                 LOG.debug("Instance soft reboot failed: %s", e,
2408                           instance=instance)
2409                 soft_reboot_success = False
2410 
2411             if soft_reboot_success:
2412                 LOG.info("Instance soft rebooted successfully.",
2413                          instance=instance)
2414                 return
2415             else:
2416                 LOG.warning("Failed to soft reboot instance. "
2417                             "Trying hard reboot.",
2418                             instance=instance)
2419         return self._hard_reboot(context, instance, network_info,
2420                                  block_device_info)
2421 
2422     def _soft_reboot(self, instance):
2423         """Attempt to shutdown and restart the instance gracefully.
2424 
2425         We use shutdown and create here so we can return if the guest
2426         responded and actually rebooted. Note that this method only
2427         succeeds if the guest responds to acpi. Therefore we return
2428         success or failure so we can fall back to a hard reboot if
2429         necessary.
2430 
2431         :returns: True if the reboot succeeded
2432         """
2433         guest = self._host.get_guest(instance)
2434 
2435         state = guest.get_power_state(self._host)
2436         old_domid = guest.id
2437         # NOTE(vish): This check allows us to reboot an instance that
2438         #             is already shutdown.
2439         if state == power_state.RUNNING:
2440             guest.shutdown()
2441         # NOTE(vish): This actually could take slightly longer than the
2442         #             FLAG defines depending on how long the get_info
2443         #             call takes to return.
2444         self._prepare_pci_devices_for_use(
2445             pci_manager.get_instance_pci_devs(instance, 'all'))
2446         for x in range(CONF.libvirt.wait_soft_reboot_seconds):
2447             guest = self._host.get_guest(instance)
2448 
2449             state = guest.get_power_state(self._host)
2450             new_domid = guest.id
2451 
2452             # NOTE(ivoks): By checking domain IDs, we make sure we are
2453             #              not recreating domain that's already running.
2454             if old_domid != new_domid:
2455                 if state in [power_state.SHUTDOWN,
2456                              power_state.CRASHED]:
2457                     LOG.info("Instance shutdown successfully.",
2458                              instance=instance)
2459                     self._create_domain(domain=guest._domain)
2460                     timer = loopingcall.FixedIntervalLoopingCall(
2461                         self._wait_for_running, instance)
2462                     timer.start(interval=0.5).wait()
2463                     return True
2464                 else:
2465                     LOG.info("Instance may have been rebooted during soft "
2466                              "reboot, so return now.", instance=instance)
2467                     return True
2468             greenthread.sleep(1)
2469         return False
2470 
2471     def _hard_reboot(self, context, instance, network_info,
2472                      block_device_info=None):
2473         """Reboot a virtual machine, given an instance reference.
2474 
2475         Performs a Libvirt reset (if supported) on the domain.
2476 
2477         If Libvirt reset is unavailable this method actually destroys and
2478         re-creates the domain to ensure the reboot happens, as the guest
2479         OS cannot ignore this action.
2480         """
2481 
2482         self._destroy(instance)
2483         # Domain XML will be redefined so we can safely undefine it
2484         # from libvirt. This ensure that such process as create serial
2485         # console for guest will run smoothly.
2486         self._undefine_domain(instance)
2487 
2488         # Convert the system metadata to image metadata
2489         # NOTE(mdbooth): This is a workaround for stateless Nova compute
2490         #                https://bugs.launchpad.net/nova/+bug/1349978
2491         instance_dir = libvirt_utils.get_instance_path(instance)
2492         fileutils.ensure_tree(instance_dir)
2493 
2494         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
2495                                             instance,
2496                                             instance.image_meta,
2497                                             block_device_info)
2498         # NOTE(vish): This could generate the wrong device_format if we are
2499         #             using the raw backend and the images don't exist yet.
2500         #             The create_images_and_backing below doesn't properly
2501         #             regenerate raw backend images, however, so when it
2502         #             does we need to (re)generate the xml after the images
2503         #             are in place.
2504         xml = self._get_guest_xml(context, instance, network_info, disk_info,
2505                                   instance.image_meta,
2506                                   block_device_info=block_device_info)
2507 
2508         # NOTE(mdbooth): context.auth_token will not be set when we call
2509         #                _hard_reboot from resume_state_on_host_boot()
2510         if context.auth_token is not None:
2511             # NOTE (rmk): Re-populate any missing backing files.
2512             config = vconfig.LibvirtConfigGuest()
2513             config.parse_str(xml)
2514             backing_disk_info = self._get_instance_disk_info_from_config(
2515                 config, block_device_info)
2516             self._create_images_and_backing(context, instance, instance_dir,
2517                                             backing_disk_info)
2518 
2519         # Initialize all the necessary networking, block devices and
2520         # start the instance.
2521         self._create_domain_and_network(context, xml, instance, network_info,
2522                                         block_device_info=block_device_info,
2523                                         reboot=True,
2524                                         vifs_already_plugged=True)
2525         self._prepare_pci_devices_for_use(
2526             pci_manager.get_instance_pci_devs(instance, 'all'))
2527 
2528         def _wait_for_reboot():
2529             """Called at an interval until the VM is running again."""
2530             state = self.get_info(instance).state
2531 
2532             if state == power_state.RUNNING:
2533                 LOG.info("Instance rebooted successfully.",
2534                          instance=instance)
2535                 raise loopingcall.LoopingCallDone()
2536 
2537         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_reboot)
2538         timer.start(interval=0.5).wait()
2539 
2540     def pause(self, instance):
2541         """Pause VM instance."""
2542         self._host.get_guest(instance).pause()
2543 
2544     def unpause(self, instance):
2545         """Unpause paused VM instance."""
2546         guest = self._host.get_guest(instance)
2547         guest.resume()
2548         guest.sync_guest_time()
2549 
2550     def _clean_shutdown(self, instance, timeout, retry_interval):
2551         """Attempt to shutdown the instance gracefully.
2552 
2553         :param instance: The instance to be shutdown
2554         :param timeout: How long to wait in seconds for the instance to
2555                         shutdown
2556         :param retry_interval: How often in seconds to signal the instance
2557                                to shutdown while waiting
2558 
2559         :returns: True if the shutdown succeeded
2560         """
2561 
2562         # List of states that represent a shutdown instance
2563         SHUTDOWN_STATES = [power_state.SHUTDOWN,
2564                            power_state.CRASHED]
2565 
2566         try:
2567             guest = self._host.get_guest(instance)
2568         except exception.InstanceNotFound:
2569             # If the instance has gone then we don't need to
2570             # wait for it to shutdown
2571             return True
2572 
2573         state = guest.get_power_state(self._host)
2574         if state in SHUTDOWN_STATES:
2575             LOG.info("Instance already shutdown.", instance=instance)
2576             return True
2577 
2578         LOG.debug("Shutting down instance from state %s", state,
2579                   instance=instance)
2580         guest.shutdown()
2581         retry_countdown = retry_interval
2582 
2583         for sec in range(timeout):
2584 
2585             guest = self._host.get_guest(instance)
2586             state = guest.get_power_state(self._host)
2587 
2588             if state in SHUTDOWN_STATES:
2589                 LOG.info("Instance shutdown successfully after %d seconds.",
2590                          sec, instance=instance)
2591                 return True
2592 
2593             # Note(PhilD): We can't assume that the Guest was able to process
2594             #              any previous shutdown signal (for example it may
2595             #              have still been startingup, so within the overall
2596             #              timeout we re-trigger the shutdown every
2597             #              retry_interval
2598             if retry_countdown == 0:
2599                 retry_countdown = retry_interval
2600                 # Instance could shutdown at any time, in which case we
2601                 # will get an exception when we call shutdown
2602                 try:
2603                     LOG.debug("Instance in state %s after %d seconds - "
2604                               "resending shutdown", state, sec,
2605                               instance=instance)
2606                     guest.shutdown()
2607                 except libvirt.libvirtError:
2608                     # Assume this is because its now shutdown, so loop
2609                     # one more time to clean up.
2610                     LOG.debug("Ignoring libvirt exception from shutdown "
2611                               "request.", instance=instance)
2612                     continue
2613             else:
2614                 retry_countdown -= 1
2615 
2616             time.sleep(1)
2617 
2618         LOG.info("Instance failed to shutdown in %d seconds.",
2619                  timeout, instance=instance)
2620         return False
2621 
2622     def power_off(self, instance, timeout=0, retry_interval=0):
2623         """Power off the specified instance."""
2624         if timeout:
2625             self._clean_shutdown(instance, timeout, retry_interval)
2626         self._destroy(instance)
2627 
2628     def power_on(self, context, instance, network_info,
2629                  block_device_info=None):
2630         """Power on the specified instance."""
2631         # We use _hard_reboot here to ensure that all backing files,
2632         # network, and block device connections, etc. are established
2633         # and available before we attempt to start the instance.
2634         self._hard_reboot(context, instance, network_info, block_device_info)
2635 
2636     def trigger_crash_dump(self, instance):
2637 
2638         """Trigger crash dump by injecting an NMI to the specified instance."""
2639         try:
2640             self._host.get_guest(instance).inject_nmi()
2641         except libvirt.libvirtError as ex:
2642             error_code = ex.get_error_code()
2643 
2644             if error_code == libvirt.VIR_ERR_NO_SUPPORT:
2645                 raise exception.TriggerCrashDumpNotSupported()
2646             elif error_code == libvirt.VIR_ERR_OPERATION_INVALID:
2647                 raise exception.InstanceNotRunning(instance_id=instance.uuid)
2648 
2649             LOG.exception(_('Error from libvirt while injecting an NMI to '
2650                             '%(instance_uuid)s: '
2651                             '[Error Code %(error_code)s] %(ex)s'),
2652                           {'instance_uuid': instance.uuid,
2653                            'error_code': error_code, 'ex': ex})
2654             raise
2655 
2656     def suspend(self, context, instance):
2657         """Suspend the specified instance."""
2658         guest = self._host.get_guest(instance)
2659 
2660         self._detach_pci_devices(guest,
2661             pci_manager.get_instance_pci_devs(instance))
2662         self._detach_direct_passthrough_ports(context, instance, guest)
2663         guest.save_memory_state()
2664 
2665     def resume(self, context, instance, network_info, block_device_info=None):
2666         """resume the specified instance."""
2667         xml = self._get_existing_domain_xml(instance, network_info,
2668                                             block_device_info)
2669         guest = self._create_domain_and_network(context, xml, instance,
2670                            network_info, block_device_info=block_device_info,
2671                            vifs_already_plugged=True)
2672         self._attach_pci_devices(guest,
2673             pci_manager.get_instance_pci_devs(instance))
2674         self._attach_direct_passthrough_ports(
2675             context, instance, guest, network_info)
2676         timer = loopingcall.FixedIntervalLoopingCall(self._wait_for_running,
2677                                                      instance)
2678         timer.start(interval=0.5).wait()
2679         guest.sync_guest_time()
2680 
2681     def resume_state_on_host_boot(self, context, instance, network_info,
2682                                   block_device_info=None):
2683         """resume guest state when a host is booted."""
2684         # Check if the instance is running already and avoid doing
2685         # anything if it is.
2686         try:
2687             guest = self._host.get_guest(instance)
2688             state = guest.get_power_state(self._host)
2689 
2690             ignored_states = (power_state.RUNNING,
2691                               power_state.SUSPENDED,
2692                               power_state.NOSTATE,
2693                               power_state.PAUSED)
2694 
2695             if state in ignored_states:
2696                 return
2697         except (exception.InternalError, exception.InstanceNotFound):
2698             pass
2699 
2700         # Instance is not up and could be in an unknown state.
2701         # Be as absolute as possible about getting it back into
2702         # a known and running state.
2703         self._hard_reboot(context, instance, network_info, block_device_info)
2704 
2705     def rescue(self, context, instance, network_info, image_meta,
2706                rescue_password):
2707         """Loads a VM using rescue images.
2708 
2709         A rescue is normally performed when something goes wrong with the
2710         primary images and data needs to be corrected/recovered. Rescuing
2711         should not edit or over-ride the original image, only allow for
2712         data recovery.
2713 
2714         """
2715         instance_dir = libvirt_utils.get_instance_path(instance)
2716         unrescue_xml = self._get_existing_domain_xml(instance, network_info)
2717         unrescue_xml_path = os.path.join(instance_dir, 'unrescue.xml')
2718         libvirt_utils.write_to_file(unrescue_xml_path, unrescue_xml)
2719 
2720         rescue_image_id = None
2721         if image_meta.obj_attr_is_set("id"):
2722             rescue_image_id = image_meta.id
2723 
2724         rescue_images = {
2725             'image_id': (rescue_image_id or
2726                         CONF.libvirt.rescue_image_id or instance.image_ref),
2727             'kernel_id': (CONF.libvirt.rescue_kernel_id or
2728                           instance.kernel_id),
2729             'ramdisk_id': (CONF.libvirt.rescue_ramdisk_id or
2730                            instance.ramdisk_id),
2731         }
2732         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
2733                                             instance,
2734                                             image_meta,
2735                                             rescue=True)
2736         injection_info = InjectionInfo(network_info=network_info,
2737                                        admin_pass=rescue_password,
2738                                        files=None)
2739         gen_confdrive = functools.partial(self._create_configdrive,
2740                                           context, instance, injection_info,
2741                                           rescue=True)
2742         self._create_image(context, instance, disk_info['mapping'],
2743                            injection_info=injection_info, suffix='.rescue',
2744                            disk_images=rescue_images)
2745         xml = self._get_guest_xml(context, instance, network_info, disk_info,
2746                                   image_meta, rescue=rescue_images)
2747         self._destroy(instance)
2748         self._create_domain(xml, post_xml_callback=gen_confdrive)
2749 
2750     def unrescue(self, instance, network_info):
2751         """Reboot the VM which is being rescued back into primary images.
2752         """
2753         instance_dir = libvirt_utils.get_instance_path(instance)
2754         unrescue_xml_path = os.path.join(instance_dir, 'unrescue.xml')
2755         xml = libvirt_utils.load_file(unrescue_xml_path)
2756         guest = self._host.get_guest(instance)
2757 
2758         # TODO(sahid): We are converting all calls from a
2759         # virDomain object to use nova.virt.libvirt.Guest.
2760         # We should be able to remove virt_dom at the end.
2761         virt_dom = guest._domain
2762         self._destroy(instance)
2763         self._create_domain(xml, virt_dom)
2764         libvirt_utils.file_delete(unrescue_xml_path)
2765         rescue_files = os.path.join(instance_dir, "*.rescue")
2766         for rescue_file in glob.iglob(rescue_files):
2767             if os.path.isdir(rescue_file):
2768                 shutil.rmtree(rescue_file)
2769             else:
2770                 libvirt_utils.file_delete(rescue_file)
2771         # cleanup rescue volume
2772         lvm.remove_volumes([lvmdisk for lvmdisk in self._lvm_disks(instance)
2773                                 if lvmdisk.endswith('.rescue')])
2774         if CONF.libvirt.images_type == 'rbd':
2775             filter_fn = lambda disk: (disk.startswith(instance.uuid) and
2776                                       disk.endswith('.rescue'))
2777             LibvirtDriver._get_rbd_driver().cleanup_volumes(filter_fn)
2778 
2779     def poll_rebooting_instances(self, timeout, instances):
2780         pass
2781 
2782     # NOTE(ilyaalekseyev): Implementation like in multinics
2783     # for xenapi(tr3buchet)
2784     def spawn(self, context, instance, image_meta, injected_files,
2785               admin_password, network_info=None, block_device_info=None):
2786         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
2787                                             instance,
2788                                             image_meta,
2789                                             block_device_info)
2790         injection_info = InjectionInfo(network_info=network_info,
2791                                        files=injected_files,
2792                                        admin_pass=admin_password)
2793         gen_confdrive = functools.partial(self._create_configdrive,
2794                                           context, instance,
2795                                           injection_info)
2796         self._create_image(context, instance, disk_info['mapping'],
2797                            injection_info=injection_info,
2798                            block_device_info=block_device_info)
2799 
2800         # Required by Quobyte CI
2801         self._ensure_console_log_for_instance(instance)
2802 
2803         xml = self._get_guest_xml(context, instance, network_info,
2804                                   disk_info, image_meta,
2805                                   block_device_info=block_device_info)
2806         self._create_domain_and_network(
2807             context, xml, instance, network_info,
2808             block_device_info=block_device_info,
2809             post_xml_callback=gen_confdrive,
2810             destroy_disks_on_failure=True)
2811         LOG.debug("Instance is running", instance=instance)
2812 
2813         def _wait_for_boot():
2814             """Called at an interval until the VM is running."""
2815             state = self.get_info(instance).state
2816 
2817             if state == power_state.RUNNING:
2818                 LOG.info("Instance spawned successfully.", instance=instance)
2819                 raise loopingcall.LoopingCallDone()
2820 
2821         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_boot)
2822         timer.start(interval=0.5).wait()
2823 
2824     def _flush_libvirt_console(self, pty):
2825         out, err = utils.execute('dd',
2826                                  'if=%s' % pty,
2827                                  'iflag=nonblock',
2828                                  run_as_root=True,
2829                                  check_exit_code=False)
2830         return out
2831 
2832     def _append_to_file(self, data, fpath):
2833         LOG.info('data: %(data)r, fpath: %(fpath)r',
2834                  {'data': data, 'fpath': fpath})
2835         with open(fpath, 'a+') as fp:
2836             fp.write(data)
2837 
2838         return fpath
2839 
2840     def _get_console_output_file(self, instance, console_log):
2841         bytes_to_read = MAX_CONSOLE_BYTES
2842         log_data = b""  # The last N read bytes
2843         i = 0  # in case there is a log rotation (like "virtlogd")
2844         path = console_log
2845         while bytes_to_read > 0 and os.path.exists(path):
2846             libvirt_utils.chown(path, os.getuid())
2847             with libvirt_utils.file_open(path, 'rb') as fp:
2848                 read_log_data, remaining = libvirt_utils.last_bytes(
2849                                                 fp, bytes_to_read)
2850                 # We need the log file content in chronological order,
2851                 # that's why we *prepend* the log data.
2852                 log_data = read_log_data + log_data
2853                 bytes_to_read -= len(read_log_data)
2854                 path = console_log + "." + str(i)
2855                 i += 1
2856             if remaining > 0:
2857                 LOG.info('Truncated console log returned, '
2858                          '%d bytes ignored', remaining, instance=instance)
2859         return log_data
2860 
2861     def get_console_output(self, context, instance):
2862         guest = self._host.get_guest(instance)
2863 
2864         xml = guest.get_xml_desc()
2865         tree = etree.fromstring(xml)
2866 
2867         # If the guest has a console logging to a file prefer to use that
2868         file_consoles = tree.findall("./devices/console[@type='file']")
2869         if file_consoles:
2870             for file_console in file_consoles:
2871                 source_node = file_console.find('./source')
2872                 if source_node is None:
2873                     continue
2874                 path = source_node.get("path")
2875                 if not path:
2876                     continue
2877 
2878                 if not os.path.exists(path):
2879                     LOG.info('Instance is configured with a file console, '
2880                              'but the backing file is not (yet?) present',
2881                              instance=instance)
2882                     return ""
2883 
2884                 return self._get_console_output_file(instance, path)
2885 
2886         # Try 'pty' types
2887         pty_consoles = tree.findall("./devices/console[@type='pty']")
2888         if pty_consoles:
2889             for pty_console in pty_consoles:
2890                 source_node = pty_console.find('./source')
2891                 if source_node is None:
2892                     continue
2893                 pty = source_node.get("path")
2894                 if not pty:
2895                     continue
2896                 break
2897             else:
2898                 raise exception.ConsoleNotAvailable()
2899         else:
2900             raise exception.ConsoleNotAvailable()
2901 
2902         console_log = self._get_console_log_path(instance)
2903         # By default libvirt chowns the console log when it starts a domain.
2904         # We need to chown it back before attempting to read from or write
2905         # to it.
2906         if os.path.exists(console_log):
2907             libvirt_utils.chown(console_log, os.getuid())
2908 
2909         data = self._flush_libvirt_console(pty)
2910         # NOTE(markus_z): The virt_types kvm and qemu are the only ones
2911         # which create a dedicated file device for the console logging.
2912         # Other virt_types like xen, lxc, uml, parallels depend on the
2913         # flush of that pty device into the "console.log" file to ensure
2914         # that a series of "get_console_output" calls return the complete
2915         # content even after rebooting a guest.
2916         fpath = self._append_to_file(data, console_log)
2917 
2918         return self._get_console_output_file(instance, fpath)
2919 
2920     def get_host_ip_addr(self):
2921         ips = compute_utils.get_machine_ips()
2922         if CONF.my_ip not in ips:
2923             LOG.warning('my_ip address (%(my_ip)s) was not found on '
2924                         'any of the interfaces: %(ifaces)s',
2925                         {'my_ip': CONF.my_ip, 'ifaces': ", ".join(ips)})
2926         return CONF.my_ip
2927 
2928     def get_vnc_console(self, context, instance):
2929         def get_vnc_port_for_instance(instance_name):
2930             guest = self._host.get_guest(instance)
2931 
2932             xml = guest.get_xml_desc()
2933             xml_dom = etree.fromstring(xml)
2934 
2935             graphic = xml_dom.find("./devices/graphics[@type='vnc']")
2936             if graphic is not None:
2937                 return graphic.get('port')
2938             # NOTE(rmk): We had VNC consoles enabled but the instance in
2939             # question is not actually listening for connections.
2940             raise exception.ConsoleTypeUnavailable(console_type='vnc')
2941 
2942         port = get_vnc_port_for_instance(instance.name)
2943         host = CONF.vnc.vncserver_proxyclient_address
2944 
2945         return ctype.ConsoleVNC(host=host, port=port)
2946 
2947     def get_spice_console(self, context, instance):
2948         def get_spice_ports_for_instance(instance_name):
2949             guest = self._host.get_guest(instance)
2950 
2951             xml = guest.get_xml_desc()
2952             xml_dom = etree.fromstring(xml)
2953 
2954             graphic = xml_dom.find("./devices/graphics[@type='spice']")
2955             if graphic is not None:
2956                 return (graphic.get('port'), graphic.get('tlsPort'))
2957             # NOTE(rmk): We had Spice consoles enabled but the instance in
2958             # question is not actually listening for connections.
2959             raise exception.ConsoleTypeUnavailable(console_type='spice')
2960 
2961         ports = get_spice_ports_for_instance(instance.name)
2962         host = CONF.spice.server_proxyclient_address
2963 
2964         return ctype.ConsoleSpice(host=host, port=ports[0], tlsPort=ports[1])
2965 
2966     def get_serial_console(self, context, instance):
2967         guest = self._host.get_guest(instance)
2968         for hostname, port in self._get_serial_ports_from_guest(
2969                 guest, mode='bind'):
2970             return ctype.ConsoleSerial(host=hostname, port=port)
2971         raise exception.ConsoleTypeUnavailable(console_type='serial')
2972 
2973     @staticmethod
2974     def _supports_direct_io(dirpath):
2975 
2976         if not hasattr(os, 'O_DIRECT'):
2977             LOG.debug("This python runtime does not support direct I/O")
2978             return False
2979 
2980         testfile = os.path.join(dirpath, ".directio.test")
2981 
2982         hasDirectIO = True
2983         fd = None
2984         try:
2985             fd = os.open(testfile, os.O_CREAT | os.O_WRONLY | os.O_DIRECT)
2986             # Check is the write allowed with 512 byte alignment
2987             align_size = 512
2988             m = mmap.mmap(-1, align_size)
2989             m.write(b"x" * align_size)
2990             os.write(fd, m)
2991             LOG.debug("Path '%(path)s' supports direct I/O",
2992                       {'path': dirpath})
2993         except OSError as e:
2994             if e.errno == errno.EINVAL:
2995                 LOG.debug("Path '%(path)s' does not support direct I/O: "
2996                           "'%(ex)s'", {'path': dirpath, 'ex': e})
2997                 hasDirectIO = False
2998             else:
2999                 with excutils.save_and_reraise_exception():
3000                     LOG.error("Error on '%(path)s' while checking "
3001                               "direct I/O: '%(ex)s'",
3002                               {'path': dirpath, 'ex': e})
3003         except Exception as e:
3004             with excutils.save_and_reraise_exception():
3005                 LOG.error("Error on '%(path)s' while checking direct I/O: "
3006                           "'%(ex)s'", {'path': dirpath, 'ex': e})
3007         finally:
3008             # ensure unlink(filepath) will actually remove the file by deleting
3009             # the remaining link to it in close(fd)
3010             if fd is not None:
3011                 os.close(fd)
3012 
3013             try:
3014                 os.unlink(testfile)
3015             except Exception:
3016                 pass
3017 
3018         return hasDirectIO
3019 
3020     @staticmethod
3021     def _create_ephemeral(target, ephemeral_size,
3022                           fs_label, os_type, is_block_dev=False,
3023                           context=None, specified_fs=None,
3024                           vm_mode=None):
3025         if not is_block_dev:
3026             if (CONF.libvirt.virt_type == "parallels" and
3027                     vm_mode == fields.VMMode.EXE):
3028 
3029                 libvirt_utils.create_ploop_image('expanded', target,
3030                                                  '%dG' % ephemeral_size,
3031                                                  specified_fs)
3032                 return
3033             libvirt_utils.create_image('raw', target, '%dG' % ephemeral_size)
3034 
3035         # Run as root only for block devices.
3036         disk_api.mkfs(os_type, fs_label, target, run_as_root=is_block_dev,
3037                       specified_fs=specified_fs)
3038 
3039     @staticmethod
3040     def _create_swap(target, swap_mb, context=None):
3041         """Create a swap file of specified size."""
3042         libvirt_utils.create_image('raw', target, '%dM' % swap_mb)
3043         utils.mkfs('swap', target)
3044 
3045     @staticmethod
3046     def _get_console_log_path(instance):
3047         return os.path.join(libvirt_utils.get_instance_path(instance),
3048                             'console.log')
3049 
3050     def _ensure_console_log_for_instance(self, instance):
3051         # NOTE(mdbooth): Although libvirt will create this file for us
3052         # automatically when it starts, it will initially create it with
3053         # root ownership and then chown it depending on the configuration of
3054         # the domain it is launching. Quobyte CI explicitly disables the
3055         # chown by setting dynamic_ownership=0 in libvirt's config.
3056         # Consequently when the domain starts it is unable to write to its
3057         # console.log. See bug https://bugs.launchpad.net/nova/+bug/1597644
3058         #
3059         # To work around this, we create the file manually before starting
3060         # the domain so it has the same ownership as Nova. This works
3061         # for Quobyte CI because it is also configured to run qemu as the same
3062         # user as the Nova service. Installations which don't set
3063         # dynamic_ownership=0 are not affected because libvirt will always
3064         # correctly configure permissions regardless of initial ownership.
3065         #
3066         # Setting dynamic_ownership=0 is dubious and potentially broken in
3067         # more ways than console.log (see comment #22 on the above bug), so
3068         # Future Maintainer who finds this code problematic should check to see
3069         # if we still support it.
3070         console_file = self._get_console_log_path(instance)
3071         LOG.debug('Ensure instance console log exists: %s', console_file,
3072                   instance=instance)
3073         try:
3074             libvirt_utils.file_open(console_file, 'a').close()
3075         # NOTE(sfinucan): We can safely ignore permission issues here and
3076         # assume that it is libvirt that has taken ownership of this file.
3077         except IOError as ex:
3078             if ex.errno != errno.EACCES:
3079                 raise
3080             LOG.debug('Console file already exists: %s.', console_file)
3081 
3082     @staticmethod
3083     def _get_disk_config_image_type():
3084         # TODO(mikal): there is a bug here if images_type has
3085         # changed since creation of the instance, but I am pretty
3086         # sure that this bug already exists.
3087         return 'rbd' if CONF.libvirt.images_type == 'rbd' else 'raw'
3088 
3089     @staticmethod
3090     def _is_booted_from_volume(block_device_info):
3091         """Determines whether the VM is booting from volume
3092 
3093         Determines whether the block device info indicates that the VM
3094         is booting from a volume.
3095         """
3096         block_device_mapping = driver.block_device_info_get_mapping(
3097             block_device_info)
3098         return bool(block_device.get_root_bdm(block_device_mapping))
3099 
3100     def _inject_data(self, disk, instance, injection_info):
3101         """Injects data in a disk image
3102 
3103         Helper used for injecting data in a disk image file system.
3104 
3105         :param disk: The disk we're injecting into (an Image object)
3106         :param instance: The instance we're injecting into
3107         :param injection_info: Injection info
3108         """
3109         # Handles the partition need to be used.
3110         LOG.debug('Checking root disk injection %(info)s',
3111                   info=str(injection_info), instance=instance)
3112         target_partition = None
3113         if not instance.kernel_id:
3114             target_partition = CONF.libvirt.inject_partition
3115             if target_partition == 0:
3116                 target_partition = None
3117         if CONF.libvirt.virt_type == 'lxc':
3118             target_partition = None
3119 
3120         # Handles the key injection.
3121         if CONF.libvirt.inject_key and instance.get('key_data'):
3122             key = str(instance.key_data)
3123         else:
3124             key = None
3125 
3126         # Handles the admin password injection.
3127         if not CONF.libvirt.inject_password:
3128             admin_pass = None
3129         else:
3130             admin_pass = injection_info.admin_pass
3131 
3132         # Handles the network injection.
3133         net = netutils.get_injected_network_template(
3134             injection_info.network_info,
3135             libvirt_virt_type=CONF.libvirt.virt_type)
3136 
3137         # Handles the metadata injection
3138         metadata = instance.get('metadata')
3139 
3140         if any((key, net, metadata, admin_pass, injection_info.files)):
3141             LOG.debug('Injecting %(info)s', info=str(injection_info),
3142                       instance=instance)
3143             img_id = instance.image_ref
3144             try:
3145                 disk_api.inject_data(disk.get_model(self._conn),
3146                                      key, net, metadata, admin_pass,
3147                                      injection_info.files,
3148                                      partition=target_partition,
3149                                      mandatory=('files',))
3150             except Exception as e:
3151                 with excutils.save_and_reraise_exception():
3152                     LOG.error('Error injecting data into image '
3153                               '%(img_id)s (%(e)s)',
3154                               {'img_id': img_id, 'e': e},
3155                               instance=instance)
3156 
3157     # NOTE(sileht): many callers of this method assume that this
3158     # method doesn't fail if an image already exists but instead
3159     # think that it will be reused (ie: (live)-migration/resize)
3160     def _create_image(self, context, instance,
3161                       disk_mapping, injection_info=None, suffix='',
3162                       disk_images=None, block_device_info=None,
3163                       fallback_from_host=None,
3164                       ignore_bdi_for_swap=False):
3165         booted_from_volume = self._is_booted_from_volume(block_device_info)
3166 
3167         def image(fname, image_type=CONF.libvirt.images_type):
3168             return self.image_backend.by_name(instance,
3169                                               fname + suffix, image_type)
3170 
3171         def raw(fname):
3172             return image(fname, image_type='raw')
3173 
3174         # ensure directories exist and are writable
3175         fileutils.ensure_tree(libvirt_utils.get_instance_path(instance))
3176 
3177         LOG.info('Creating image', instance=instance)
3178 
3179         inst_type = instance.get_flavor()
3180         swap_mb = 0
3181         if 'disk.swap' in disk_mapping:
3182             mapping = disk_mapping['disk.swap']
3183 
3184             if ignore_bdi_for_swap:
3185                 # This is a workaround to support legacy swap resizing,
3186                 # which does not touch swap size specified in bdm,
3187                 # but works with flavor specified size only.
3188                 # In this case we follow the legacy logic and ignore block
3189                 # device info completely.
3190                 # NOTE(ft): This workaround must be removed when a correct
3191                 # implementation of resize operation changing sizes in bdms is
3192                 # developed. Also at that stage we probably may get rid of
3193                 # the direct usage of flavor swap size here,
3194                 # leaving the work with bdm only.
3195                 swap_mb = inst_type['swap']
3196             else:
3197                 swap = driver.block_device_info_get_swap(block_device_info)
3198                 if driver.swap_is_usable(swap):
3199                     swap_mb = swap['swap_size']
3200                 elif (inst_type['swap'] > 0 and
3201                       not block_device.volume_in_mapping(
3202                         mapping['dev'], block_device_info)):
3203                     swap_mb = inst_type['swap']
3204 
3205             if swap_mb > 0:
3206                 if (CONF.libvirt.virt_type == "parallels" and
3207                         instance.vm_mode == fields.VMMode.EXE):
3208                     msg = _("Swap disk is not supported "
3209                             "for Virtuozzo container")
3210                     raise exception.Invalid(msg)
3211 
3212         if not disk_images:
3213             disk_images = {'image_id': instance.image_ref,
3214                            'kernel_id': instance.kernel_id,
3215                            'ramdisk_id': instance.ramdisk_id}
3216 
3217         if disk_images['kernel_id']:
3218             fname = imagecache.get_cache_fname(disk_images['kernel_id'])
3219             raw('kernel').cache(fetch_func=libvirt_utils.fetch_raw_image,
3220                                 context=context,
3221                                 filename=fname,
3222                                 image_id=disk_images['kernel_id'])
3223             if disk_images['ramdisk_id']:
3224                 fname = imagecache.get_cache_fname(disk_images['ramdisk_id'])
3225                 raw('ramdisk').cache(fetch_func=libvirt_utils.fetch_raw_image,
3226                                      context=context,
3227                                      filename=fname,
3228                                      image_id=disk_images['ramdisk_id'])
3229 
3230         if CONF.libvirt.virt_type == 'uml':
3231             libvirt_utils.chown(image('disk').path, 'root')
3232 
3233         self._create_and_inject_local_root(context, instance,
3234                                            booted_from_volume, suffix,
3235                                            disk_images, injection_info,
3236                                            fallback_from_host)
3237 
3238         # Lookup the filesystem type if required
3239         os_type_with_default = disk_api.get_fs_type_for_os_type(
3240             instance.os_type)
3241         # Generate a file extension based on the file system
3242         # type and the mkfs commands configured if any
3243         file_extension = disk_api.get_file_extension_for_os_type(
3244                                                           os_type_with_default)
3245 
3246         vm_mode = fields.VMMode.get_from_instance(instance)
3247         ephemeral_gb = instance.flavor.ephemeral_gb
3248         if 'disk.local' in disk_mapping:
3249             disk_image = image('disk.local')
3250             fn = functools.partial(self._create_ephemeral,
3251                                    fs_label='ephemeral0',
3252                                    os_type=instance.os_type,
3253                                    is_block_dev=disk_image.is_block_dev,
3254                                    vm_mode=vm_mode)
3255             fname = "ephemeral_%s_%s" % (ephemeral_gb, file_extension)
3256             size = ephemeral_gb * units.Gi
3257             disk_image.cache(fetch_func=fn,
3258                              context=context,
3259                              filename=fname,
3260                              size=size,
3261                              ephemeral_size=ephemeral_gb)
3262 
3263         for idx, eph in enumerate(driver.block_device_info_get_ephemerals(
3264                 block_device_info)):
3265             disk_image = image(blockinfo.get_eph_disk(idx))
3266 
3267             specified_fs = eph.get('guest_format')
3268             if specified_fs and not self.is_supported_fs_format(specified_fs):
3269                 msg = _("%s format is not supported") % specified_fs
3270                 raise exception.InvalidBDMFormat(details=msg)
3271 
3272             fn = functools.partial(self._create_ephemeral,
3273                                    fs_label='ephemeral%d' % idx,
3274                                    os_type=instance.os_type,
3275                                    is_block_dev=disk_image.is_block_dev,
3276                                    vm_mode=vm_mode)
3277             size = eph['size'] * units.Gi
3278             fname = "ephemeral_%s_%s" % (eph['size'], file_extension)
3279             disk_image.cache(fetch_func=fn,
3280                              context=context,
3281                              filename=fname,
3282                              size=size,
3283                              ephemeral_size=eph['size'],
3284                              specified_fs=specified_fs)
3285 
3286         if swap_mb > 0:
3287             size = swap_mb * units.Mi
3288             image('disk.swap').cache(fetch_func=self._create_swap,
3289                                      context=context,
3290                                      filename="swap_%s" % swap_mb,
3291                                      size=size,
3292                                      swap_mb=swap_mb)
3293 
3294     def _create_and_inject_local_root(self, context, instance,
3295                                       booted_from_volume, suffix, disk_images,
3296                                       injection_info, fallback_from_host):
3297         # File injection only if needed
3298         need_inject = (not configdrive.required_by(instance) and
3299                        injection_info is not None and
3300                        CONF.libvirt.inject_partition != -2)
3301 
3302         # NOTE(ndipanov): Even if disk_mapping was passed in, which
3303         # currently happens only on rescue - we still don't want to
3304         # create a base image.
3305         if not booted_from_volume:
3306             root_fname = imagecache.get_cache_fname(disk_images['image_id'])
3307             size = instance.flavor.root_gb * units.Gi
3308 
3309             if size == 0 or suffix == '.rescue':
3310                 size = None
3311 
3312             backend = self.image_backend.by_name(instance, 'disk' + suffix,
3313                                                  CONF.libvirt.images_type)
3314             if instance.task_state == task_states.RESIZE_FINISH:
3315                 backend.create_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME)
3316             if backend.SUPPORTS_CLONE:
3317                 def clone_fallback_to_fetch(*args, **kwargs):
3318                     try:
3319                         backend.clone(context, disk_images['image_id'])
3320                     except exception.ImageUnacceptable:
3321                         libvirt_utils.fetch_image(*args, **kwargs)
3322                 fetch_func = clone_fallback_to_fetch
3323             else:
3324                 fetch_func = libvirt_utils.fetch_image
3325             self._try_fetch_image_cache(backend, fetch_func, context,
3326                                         root_fname, disk_images['image_id'],
3327                                         instance, size, fallback_from_host)
3328 
3329             if need_inject:
3330                 self._inject_data(backend, instance, injection_info)
3331 
3332         elif need_inject:
3333             LOG.warning('File injection into a boot from volume '
3334                         'instance is not supported', instance=instance)
3335 
3336     def _create_configdrive(self, context, instance, injection_info,
3337                             rescue=False):
3338         # As this method being called right after the definition of a
3339         # domain, but before its actual launch, device metadata will be built
3340         # and saved in the instance for it to be used by the config drive and
3341         # the metadata service.
3342         instance.device_metadata = self._build_device_metadata(context,
3343                                                                instance)
3344         if configdrive.required_by(instance):
3345             LOG.info('Using config drive', instance=instance)
3346 
3347             name = 'disk.config'
3348             if rescue:
3349                 name += '.rescue'
3350 
3351             config_disk = self.image_backend.by_name(
3352                 instance, name, self._get_disk_config_image_type())
3353 
3354             # Don't overwrite an existing config drive
3355             if not config_disk.exists():
3356                 extra_md = {}
3357                 if injection_info.admin_pass:
3358                     extra_md['admin_pass'] = injection_info.admin_pass
3359 
3360                 inst_md = instance_metadata.InstanceMetadata(
3361                     instance, content=injection_info.files, extra_md=extra_md,
3362                     network_info=injection_info.network_info,
3363                     request_context=context)
3364 
3365                 cdb = configdrive.ConfigDriveBuilder(instance_md=inst_md)
3366                 with cdb:
3367                     # NOTE(mdbooth): We're hardcoding here the path of the
3368                     # config disk when using the flat backend. This isn't
3369                     # good, but it's required because we need a local path we
3370                     # know we can write to in case we're subsequently
3371                     # importing into rbd. This will be cleaned up when we
3372                     # replace this with a call to create_from_func, but that
3373                     # can't happen until we've updated the backends and we
3374                     # teach them not to cache config disks. This isn't
3375                     # possible while we're still using cache() under the hood.
3376                     config_disk_local_path = os.path.join(
3377                         libvirt_utils.get_instance_path(instance), name)
3378                     LOG.info('Creating config drive at %(path)s',
3379                              {'path': config_disk_local_path},
3380                              instance=instance)
3381 
3382                     try:
3383                         cdb.make_drive(config_disk_local_path)
3384                     except processutils.ProcessExecutionError as e:
3385                         with excutils.save_and_reraise_exception():
3386                             LOG.error('Creating config drive failed with '
3387                                       'error: %s', e, instance=instance)
3388 
3389                 try:
3390                     config_disk.import_file(
3391                         instance, config_disk_local_path, name)
3392                 finally:
3393                     # NOTE(mikal): if the config drive was imported into RBD,
3394                     # then we no longer need the local copy
3395                     if CONF.libvirt.images_type == 'rbd':
3396                         LOG.info('Deleting local config drive %(path)s '
3397                                  'because it was imported into RBD.',
3398                                  {'path': config_disk_local_path},
3399                                  instance=instance)
3400                         os.unlink(config_disk_local_path)
3401 
3402     def _prepare_pci_devices_for_use(self, pci_devices):
3403         # kvm , qemu support managed mode
3404         # In managed mode, the configured device will be automatically
3405         # detached from the host OS drivers when the guest is started,
3406         # and then re-attached when the guest shuts down.
3407         if CONF.libvirt.virt_type != 'xen':
3408             # we do manual detach only for xen
3409             return
3410         try:
3411             for dev in pci_devices:
3412                 libvirt_dev_addr = dev['hypervisor_name']
3413                 libvirt_dev = \
3414                         self._host.device_lookup_by_name(libvirt_dev_addr)
3415                 # Note(yjiang5) Spelling for 'dettach' is correct, see
3416                 # http://libvirt.org/html/libvirt-libvirt.html.
3417                 libvirt_dev.dettach()
3418 
3419             # Note(yjiang5): A reset of one PCI device may impact other
3420             # devices on the same bus, thus we need two separated loops
3421             # to detach and then reset it.
3422             for dev in pci_devices:
3423                 libvirt_dev_addr = dev['hypervisor_name']
3424                 libvirt_dev = \
3425                         self._host.device_lookup_by_name(libvirt_dev_addr)
3426                 libvirt_dev.reset()
3427 
3428         except libvirt.libvirtError as exc:
3429             raise exception.PciDevicePrepareFailed(id=dev['id'],
3430                                                    instance_uuid=
3431                                                    dev['instance_uuid'],
3432                                                    reason=six.text_type(exc))
3433 
3434     def _detach_pci_devices(self, guest, pci_devs):
3435         try:
3436             for dev in pci_devs:
3437                 guest.detach_device(self._get_guest_pci_device(dev), live=True)
3438                 # after detachDeviceFlags returned, we should check the dom to
3439                 # ensure the detaching is finished
3440                 xml = guest.get_xml_desc()
3441                 xml_doc = etree.fromstring(xml)
3442                 guest_config = vconfig.LibvirtConfigGuest()
3443                 guest_config.parse_dom(xml_doc)
3444 
3445                 for hdev in [d for d in guest_config.devices
3446                     if isinstance(d, vconfig.LibvirtConfigGuestHostdevPCI)]:
3447                     hdbsf = [hdev.domain, hdev.bus, hdev.slot, hdev.function]
3448                     dbsf = pci_utils.parse_address(dev.address)
3449                     if [int(x, 16) for x in hdbsf] ==\
3450                             [int(x, 16) for x in dbsf]:
3451                         raise exception.PciDeviceDetachFailed(reason=
3452                                                               "timeout",
3453                                                               dev=dev)
3454 
3455         except libvirt.libvirtError as ex:
3456             error_code = ex.get_error_code()
3457             if error_code == libvirt.VIR_ERR_NO_DOMAIN:
3458                 LOG.warning("Instance disappeared while detaching "
3459                             "a PCI device from it.")
3460             else:
3461                 raise
3462 
3463     def _attach_pci_devices(self, guest, pci_devs):
3464         try:
3465             for dev in pci_devs:
3466                 guest.attach_device(self._get_guest_pci_device(dev))
3467 
3468         except libvirt.libvirtError:
3469             LOG.error('Attaching PCI devices %(dev)s to %(dom)s failed.',
3470                       {'dev': pci_devs, 'dom': guest.id})
3471             raise
3472 
3473     @staticmethod
3474     def _has_direct_passthrough_port(network_info):
3475         for vif in network_info:
3476             if (vif['vnic_type'] in
3477                 network_model.VNIC_TYPES_DIRECT_PASSTHROUGH):
3478                 return True
3479         return False
3480 
3481     def _attach_direct_passthrough_ports(
3482         self, context, instance, guest, network_info=None):
3483         if network_info is None:
3484             network_info = instance.info_cache.network_info
3485         if network_info is None:
3486             return
3487 
3488         if self._has_direct_passthrough_port(network_info):
3489             for vif in network_info:
3490                 if (vif['vnic_type'] in
3491                     network_model.VNIC_TYPES_DIRECT_PASSTHROUGH):
3492                     cfg = self.vif_driver.get_config(instance,
3493                                                      vif,
3494                                                      instance.image_meta,
3495                                                      instance.flavor,
3496                                                      CONF.libvirt.virt_type,
3497                                                      self._host)
3498                     LOG.debug('Attaching direct passthrough port %(port)s '
3499                               'to %(dom)s', {'port': vif, 'dom': guest.id},
3500                               instance=instance)
3501                     guest.attach_device(cfg)
3502 
3503     def _detach_direct_passthrough_ports(self, context, instance, guest):
3504         network_info = instance.info_cache.network_info
3505         if network_info is None:
3506             return
3507 
3508         if self._has_direct_passthrough_port(network_info):
3509             # In case of VNIC_TYPES_DIRECT_PASSTHROUGH ports we create
3510             # pci request per direct passthrough port. Therefore we can trust
3511             # that pci_slot value in the vif is correct.
3512             direct_passthrough_pci_addresses = [
3513                 vif['profile']['pci_slot']
3514                 for vif in network_info
3515                 if (vif['vnic_type'] in
3516                     network_model.VNIC_TYPES_DIRECT_PASSTHROUGH and
3517                     vif['profile'].get('pci_slot') is not None)
3518             ]
3519 
3520             # use detach_pci_devices to avoid failure in case of
3521             # multiple guest direct passthrough ports with the same MAC
3522             # (protection use-case, ports are on different physical
3523             # interfaces)
3524             pci_devs = pci_manager.get_instance_pci_devs(instance, 'all')
3525             direct_passthrough_pci_addresses = (
3526                 [pci_dev for pci_dev in pci_devs
3527                  if pci_dev.address in direct_passthrough_pci_addresses])
3528             self._detach_pci_devices(guest, direct_passthrough_pci_addresses)
3529 
3530     def _set_host_enabled(self, enabled,
3531                           disable_reason=DISABLE_REASON_UNDEFINED):
3532         """Enables / Disables the compute service on this host.
3533 
3534            This doesn't override non-automatic disablement with an automatic
3535            setting; thereby permitting operators to keep otherwise
3536            healthy hosts out of rotation.
3537         """
3538 
3539         status_name = {True: 'disabled',
3540                        False: 'enabled'}
3541 
3542         disable_service = not enabled
3543 
3544         ctx = nova_context.get_admin_context()
3545         try:
3546             service = objects.Service.get_by_compute_host(ctx, CONF.host)
3547 
3548             if service.disabled != disable_service:
3549                 # Note(jang): this is a quick fix to stop operator-
3550                 # disabled compute hosts from re-enabling themselves
3551                 # automatically. We prefix any automatic reason code
3552                 # with a fixed string. We only re-enable a host
3553                 # automatically if we find that string in place.
3554                 # This should probably be replaced with a separate flag.
3555                 if not service.disabled or (
3556                         service.disabled_reason and
3557                         service.disabled_reason.startswith(DISABLE_PREFIX)):
3558                     service.disabled = disable_service
3559                     service.disabled_reason = (
3560                        DISABLE_PREFIX + disable_reason
3561                        if disable_service and disable_reason else
3562                            DISABLE_REASON_UNDEFINED)
3563                     service.save()
3564                     LOG.debug('Updating compute service status to %s',
3565                               status_name[disable_service])
3566                 else:
3567                     LOG.debug('Not overriding manual compute service '
3568                               'status with: %s',
3569                               status_name[disable_service])
3570         except exception.ComputeHostNotFound:
3571             LOG.warning('Cannot update service status on host "%s" '
3572                         'since it is not registered.', CONF.host)
3573         except Exception:
3574             LOG.warning('Cannot update service status on host "%s" '
3575                         'due to an unexpected exception.', CONF.host,
3576                         exc_info=True)
3577 
3578         if enabled:
3579             mount.get_manager().host_up(self._host)
3580         else:
3581             mount.get_manager().host_down()
3582 
3583     def _get_guest_cpu_model_config(self):
3584         mode = CONF.libvirt.cpu_mode
3585         model = CONF.libvirt.cpu_model
3586 
3587         if (CONF.libvirt.virt_type == "kvm" or
3588             CONF.libvirt.virt_type == "qemu"):
3589             if mode is None:
3590                 mode = "host-model"
3591             if mode == "none":
3592                 return vconfig.LibvirtConfigGuestCPU()
3593         else:
3594             if mode is None or mode == "none":
3595                 return None
3596 
3597         if ((CONF.libvirt.virt_type != "kvm" and
3598              CONF.libvirt.virt_type != "qemu")):
3599             msg = _("Config requested an explicit CPU model, but "
3600                     "the current libvirt hypervisor '%s' does not "
3601                     "support selecting CPU models") % CONF.libvirt.virt_type
3602             raise exception.Invalid(msg)
3603 
3604         if mode == "custom" and model is None:
3605             msg = _("Config requested a custom CPU model, but no "
3606                     "model name was provided")
3607             raise exception.Invalid(msg)
3608         elif mode != "custom" and model is not None:
3609             msg = _("A CPU model name should not be set when a "
3610                     "host CPU model is requested")
3611             raise exception.Invalid(msg)
3612 
3613         LOG.debug("CPU mode '%(mode)s' model '%(model)s' was chosen",
3614                   {'mode': mode, 'model': (model or "")})
3615 
3616         cpu = vconfig.LibvirtConfigGuestCPU()
3617         cpu.mode = mode
3618         cpu.model = model
3619 
3620         return cpu
3621 
3622     def _get_guest_cpu_config(self, flavor, image_meta,
3623                               guest_cpu_numa_config, instance_numa_topology):
3624         cpu = self._get_guest_cpu_model_config()
3625 
3626         if cpu is None:
3627             return None
3628 
3629         topology = hardware.get_best_cpu_topology(
3630                 flavor, image_meta, numa_topology=instance_numa_topology)
3631 
3632         cpu.sockets = topology.sockets
3633         cpu.cores = topology.cores
3634         cpu.threads = topology.threads
3635         cpu.numa = guest_cpu_numa_config
3636 
3637         return cpu
3638 
3639     def _get_guest_disk_config(self, instance, name, disk_mapping, inst_type,
3640                                image_type=None):
3641         disk_unit = None
3642         disk = self.image_backend.by_name(instance, name, image_type)
3643         if (name == 'disk.config' and image_type == 'rbd' and
3644                 not disk.exists()):
3645             # This is likely an older config drive that has not been migrated
3646             # to rbd yet. Try to fall back on 'flat' image type.
3647             # TODO(melwitt): Add online migration of some sort so we can
3648             # remove this fall back once we know all config drives are in rbd.
3649             # NOTE(vladikr): make sure that the flat image exist, otherwise
3650             # the image will be created after the domain definition.
3651             flat_disk = self.image_backend.by_name(instance, name, 'flat')
3652             if flat_disk.exists():
3653                 disk = flat_disk
3654                 LOG.debug('Config drive not found in RBD, falling back to the '
3655                           'instance directory', instance=instance)
3656         disk_info = disk_mapping[name]
3657         if 'unit' in disk_mapping:
3658             disk_unit = disk_mapping['unit']
3659             disk_mapping['unit'] += 1  # Increments for the next disk added
3660         conf = disk.libvirt_info(disk_info['bus'],
3661                                  disk_info['dev'],
3662                                  disk_info['type'],
3663                                  self.disk_cachemode,
3664                                  inst_type['extra_specs'],
3665                                  self._host.get_version(),
3666                                  disk_unit=disk_unit)
3667         return conf
3668 
3669     def _get_guest_fs_config(self, instance, name, image_type=None):
3670         disk = self.image_backend.by_name(instance, name, image_type)
3671         return disk.libvirt_fs_info("/", "ploop")
3672 
3673     def _get_guest_storage_config(self, instance, image_meta,
3674                                   disk_info,
3675                                   rescue, block_device_info,
3676                                   inst_type, os_type):
3677         devices = []
3678         disk_mapping = disk_info['mapping']
3679 
3680         block_device_mapping = driver.block_device_info_get_mapping(
3681             block_device_info)
3682         mount_rootfs = CONF.libvirt.virt_type == "lxc"
3683         scsi_controller = self._get_scsi_controller(image_meta)
3684 
3685         if scsi_controller and scsi_controller.model == 'virtio-scsi':
3686             # The virtio-scsi can handle up to 256 devices but the
3687             # optional element "address" must be defined to describe
3688             # where the device is placed on the controller (see:
3689             # LibvirtConfigGuestDeviceAddressDrive).
3690             #
3691             # Note about why it's added in disk_mapping: It's not
3692             # possible to pass an 'int' by reference in Python, so we
3693             # use disk_mapping as container to keep reference of the
3694             # unit added and be able to increment it for each disk
3695             # added.
3696             disk_mapping['unit'] = 0
3697 
3698         def _get_ephemeral_devices():
3699             eph_devices = []
3700             for idx, eph in enumerate(
3701                 driver.block_device_info_get_ephemerals(
3702                     block_device_info)):
3703                 diskeph = self._get_guest_disk_config(
3704                     instance,
3705                     blockinfo.get_eph_disk(idx),
3706                     disk_mapping, inst_type)
3707                 eph_devices.append(diskeph)
3708             return eph_devices
3709 
3710         if mount_rootfs:
3711             fs = vconfig.LibvirtConfigGuestFilesys()
3712             fs.source_type = "mount"
3713             fs.source_dir = os.path.join(
3714                 libvirt_utils.get_instance_path(instance), 'rootfs')
3715             devices.append(fs)
3716         elif (os_type == fields.VMMode.EXE and
3717               CONF.libvirt.virt_type == "parallels"):
3718             if rescue:
3719                 fsrescue = self._get_guest_fs_config(instance, "disk.rescue")
3720                 devices.append(fsrescue)
3721 
3722                 fsos = self._get_guest_fs_config(instance, "disk")
3723                 fsos.target_dir = "/mnt/rescue"
3724                 devices.append(fsos)
3725             else:
3726                 if 'disk' in disk_mapping:
3727                     fs = self._get_guest_fs_config(instance, "disk")
3728                     devices.append(fs)
3729                 devices = devices + _get_ephemeral_devices()
3730         else:
3731 
3732             if rescue:
3733                 diskrescue = self._get_guest_disk_config(instance,
3734                                                          'disk.rescue',
3735                                                          disk_mapping,
3736                                                          inst_type)
3737                 devices.append(diskrescue)
3738 
3739                 diskos = self._get_guest_disk_config(instance,
3740                                                      'disk',
3741                                                      disk_mapping,
3742                                                      inst_type)
3743                 devices.append(diskos)
3744             else:
3745                 if 'disk' in disk_mapping:
3746                     diskos = self._get_guest_disk_config(instance,
3747                                                          'disk',
3748                                                          disk_mapping,
3749                                                          inst_type)
3750                     devices.append(diskos)
3751 
3752                 if 'disk.local' in disk_mapping:
3753                     disklocal = self._get_guest_disk_config(instance,
3754                                                             'disk.local',
3755                                                             disk_mapping,
3756                                                             inst_type)
3757                     devices.append(disklocal)
3758                     instance.default_ephemeral_device = (
3759                         block_device.prepend_dev(disklocal.target_dev))
3760 
3761                 devices = devices + _get_ephemeral_devices()
3762 
3763                 if 'disk.swap' in disk_mapping:
3764                     diskswap = self._get_guest_disk_config(instance,
3765                                                            'disk.swap',
3766                                                            disk_mapping,
3767                                                            inst_type)
3768                     devices.append(diskswap)
3769                     instance.default_swap_device = (
3770                         block_device.prepend_dev(diskswap.target_dev))
3771 
3772             config_name = 'disk.config.rescue' if rescue else 'disk.config'
3773             if config_name in disk_mapping:
3774                 diskconfig = self._get_guest_disk_config(
3775                     instance, config_name, disk_mapping, inst_type,
3776                     self._get_disk_config_image_type())
3777                 devices.append(diskconfig)
3778 
3779         for vol in block_device.get_bdms_to_connect(block_device_mapping,
3780                                                    mount_rootfs):
3781             connection_info = vol['connection_info']
3782             vol_dev = block_device.prepend_dev(vol['mount_device'])
3783             info = disk_mapping[vol_dev]
3784             self._connect_volume(connection_info, info, instance)
3785             if scsi_controller and scsi_controller.model == 'virtio-scsi':
3786                 info['unit'] = disk_mapping['unit']
3787                 disk_mapping['unit'] += 1
3788             cfg = self._get_volume_config(connection_info, info)
3789             devices.append(cfg)
3790             vol['connection_info'] = connection_info
3791             vol.save()
3792 
3793         if scsi_controller:
3794             devices.append(scsi_controller)
3795 
3796         return devices
3797 
3798     @staticmethod
3799     def _get_scsi_controller(image_meta):
3800         """Return scsi controller or None based on image meta"""
3801         # TODO(sahid): should raise an exception for an invalid controller
3802         if image_meta.properties.get('hw_scsi_model'):
3803             hw_scsi_model = image_meta.properties.hw_scsi_model
3804             scsi_controller = vconfig.LibvirtConfigGuestController()
3805             scsi_controller.type = 'scsi'
3806             scsi_controller.model = hw_scsi_model
3807             scsi_controller.index = 0
3808             return scsi_controller
3809 
3810     def _get_host_sysinfo_serial_hardware(self):
3811         """Get a UUID from the host hardware
3812 
3813         Get a UUID for the host hardware reported by libvirt.
3814         This is typically from the SMBIOS data, unless it has
3815         been overridden in /etc/libvirt/libvirtd.conf
3816         """
3817         caps = self._host.get_capabilities()
3818         return caps.host.uuid
3819 
3820     def _get_host_sysinfo_serial_os(self):
3821         """Get a UUID from the host operating system
3822 
3823         Get a UUID for the host operating system. Modern Linux
3824         distros based on systemd provide a /etc/machine-id
3825         file containing a UUID. This is also provided inside
3826         systemd based containers and can be provided by other
3827         init systems too, since it is just a plain text file.
3828         """
3829         if not os.path.exists("/etc/machine-id"):
3830             msg = _("Unable to get host UUID: /etc/machine-id does not exist")
3831             raise exception.InternalError(msg)
3832 
3833         with open("/etc/machine-id") as f:
3834             # We want to have '-' in the right place
3835             # so we parse & reformat the value
3836             lines = f.read().split()
3837             if not lines:
3838                 msg = _("Unable to get host UUID: /etc/machine-id is empty")
3839                 raise exception.InternalError(msg)
3840 
3841             return str(uuid.UUID(lines[0]))
3842 
3843     def _get_host_sysinfo_serial_auto(self):
3844         if os.path.exists("/etc/machine-id"):
3845             return self._get_host_sysinfo_serial_os()
3846         else:
3847             return self._get_host_sysinfo_serial_hardware()
3848 
3849     def _get_guest_config_sysinfo(self, instance):
3850         sysinfo = vconfig.LibvirtConfigGuestSysinfo()
3851 
3852         sysinfo.system_manufacturer = version.vendor_string()
3853         sysinfo.system_product = version.product_string()
3854         sysinfo.system_version = version.version_string_with_package()
3855 
3856         sysinfo.system_serial = self._sysinfo_serial_func()
3857         sysinfo.system_uuid = instance.uuid
3858 
3859         sysinfo.system_family = "Virtual Machine"
3860 
3861         return sysinfo
3862 
3863     def _get_guest_pci_device(self, pci_device):
3864 
3865         dbsf = pci_utils.parse_address(pci_device.address)
3866         dev = vconfig.LibvirtConfigGuestHostdevPCI()
3867         dev.domain, dev.bus, dev.slot, dev.function = dbsf
3868 
3869         # only kvm support managed mode
3870         if CONF.libvirt.virt_type in ('xen', 'parallels',):
3871             dev.managed = 'no'
3872         if CONF.libvirt.virt_type in ('kvm', 'qemu'):
3873             dev.managed = 'yes'
3874 
3875         return dev
3876 
3877     def _get_guest_config_meta(self, instance):
3878         """Get metadata config for guest."""
3879 
3880         meta = vconfig.LibvirtConfigGuestMetaNovaInstance()
3881         meta.package = version.version_string_with_package()
3882         meta.name = instance.display_name
3883         meta.creationTime = time.time()
3884 
3885         if instance.image_ref not in ("", None):
3886             meta.roottype = "image"
3887             meta.rootid = instance.image_ref
3888 
3889         system_meta = instance.system_metadata
3890         ometa = vconfig.LibvirtConfigGuestMetaNovaOwner()
3891         ometa.userid = instance.user_id
3892         ometa.username = system_meta.get('owner_user_name', 'N/A')
3893         ometa.projectid = instance.project_id
3894         ometa.projectname = system_meta.get('owner_project_name', 'N/A')
3895         meta.owner = ometa
3896 
3897         fmeta = vconfig.LibvirtConfigGuestMetaNovaFlavor()
3898         flavor = instance.flavor
3899         fmeta.name = flavor.name
3900         fmeta.memory = flavor.memory_mb
3901         fmeta.vcpus = flavor.vcpus
3902         fmeta.ephemeral = flavor.ephemeral_gb
3903         fmeta.disk = flavor.root_gb
3904         fmeta.swap = flavor.swap
3905 
3906         meta.flavor = fmeta
3907 
3908         return meta
3909 
3910     def _machine_type_mappings(self):
3911         mappings = {}
3912         for mapping in CONF.libvirt.hw_machine_type:
3913             host_arch, _, machine_type = mapping.partition('=')
3914             mappings[host_arch] = machine_type
3915         return mappings
3916 
3917     def _get_machine_type(self, image_meta, caps):
3918         # The underlying machine type can be set as an image attribute,
3919         # or otherwise based on some architecture specific defaults
3920 
3921         mach_type = None
3922 
3923         if image_meta.properties.get('hw_machine_type') is not None:
3924             mach_type = image_meta.properties.hw_machine_type
3925         else:
3926             # For ARM systems we will default to vexpress-a15 for armv7
3927             # and virt for aarch64
3928             if caps.host.cpu.arch == fields.Architecture.ARMV7:
3929                 mach_type = "vexpress-a15"
3930 
3931             if caps.host.cpu.arch == fields.Architecture.AARCH64:
3932                 mach_type = "virt"
3933 
3934             if caps.host.cpu.arch in (fields.Architecture.S390,
3935                                       fields.Architecture.S390X):
3936                 mach_type = 's390-ccw-virtio'
3937 
3938             # If set in the config, use that as the default.
3939             if CONF.libvirt.hw_machine_type:
3940                 mappings = self._machine_type_mappings()
3941                 mach_type = mappings.get(caps.host.cpu.arch)
3942 
3943         return mach_type
3944 
3945     @staticmethod
3946     def _create_idmaps(klass, map_strings):
3947         idmaps = []
3948         if len(map_strings) > 5:
3949             map_strings = map_strings[0:5]
3950             LOG.warning("Too many id maps, only included first five.")
3951         for map_string in map_strings:
3952             try:
3953                 idmap = klass()
3954                 values = [int(i) for i in map_string.split(":")]
3955                 idmap.start = values[0]
3956                 idmap.target = values[1]
3957                 idmap.count = values[2]
3958                 idmaps.append(idmap)
3959             except (ValueError, IndexError):
3960                 LOG.warning("Invalid value for id mapping %s", map_string)
3961         return idmaps
3962 
3963     def _get_guest_idmaps(self):
3964         id_maps = []
3965         if CONF.libvirt.virt_type == 'lxc' and CONF.libvirt.uid_maps:
3966             uid_maps = self._create_idmaps(vconfig.LibvirtConfigGuestUIDMap,
3967                                            CONF.libvirt.uid_maps)
3968             id_maps.extend(uid_maps)
3969         if CONF.libvirt.virt_type == 'lxc' and CONF.libvirt.gid_maps:
3970             gid_maps = self._create_idmaps(vconfig.LibvirtConfigGuestGIDMap,
3971                                            CONF.libvirt.gid_maps)
3972             id_maps.extend(gid_maps)
3973         return id_maps
3974 
3975     def _update_guest_cputune(self, guest, flavor, virt_type):
3976         is_able = self._host.is_cpu_control_policy_capable()
3977 
3978         cputuning = ['shares', 'period', 'quota']
3979         wants_cputune = any([k for k in cputuning
3980             if "quota:cpu_" + k in flavor.extra_specs.keys()])
3981 
3982         if wants_cputune and not is_able:
3983             raise exception.UnsupportedHostCPUControlPolicy()
3984 
3985         if not is_able or virt_type not in ('lxc', 'kvm', 'qemu'):
3986             return
3987 
3988         if guest.cputune is None:
3989             guest.cputune = vconfig.LibvirtConfigGuestCPUTune()
3990             # Setting the default cpu.shares value to be a value
3991             # dependent on the number of vcpus
3992         guest.cputune.shares = 1024 * guest.vcpus
3993 
3994         for name in cputuning:
3995             key = "quota:cpu_" + name
3996             if key in flavor.extra_specs:
3997                 setattr(guest.cputune, name,
3998                         int(flavor.extra_specs[key]))
3999 
4000     def _get_cpu_numa_config_from_instance(self, instance_numa_topology,
4001                                            wants_hugepages):
4002         if instance_numa_topology:
4003             guest_cpu_numa = vconfig.LibvirtConfigGuestCPUNUMA()
4004             for instance_cell in instance_numa_topology.cells:
4005                 guest_cell = vconfig.LibvirtConfigGuestCPUNUMACell()
4006                 guest_cell.id = instance_cell.id
4007                 guest_cell.cpus = instance_cell.cpuset
4008                 guest_cell.memory = instance_cell.memory * units.Ki
4009 
4010                 # The vhost-user network backend requires file backed
4011                 # guest memory (ie huge pages) to be marked as shared
4012                 # access, not private, so an external process can read
4013                 # and write the pages.
4014                 #
4015                 # You can't change the shared vs private flag for an
4016                 # already running guest, and since we can't predict what
4017                 # types of NIC may be hotplugged, we have no choice but
4018                 # to unconditionally turn on the shared flag. This has
4019                 # no real negative functional effect on the guest, so
4020                 # is a reasonable approach to take
4021                 if wants_hugepages:
4022                     guest_cell.memAccess = "shared"
4023                 guest_cpu_numa.cells.append(guest_cell)
4024             return guest_cpu_numa
4025 
4026     def _has_cpu_policy_support(self):
4027         for ver in BAD_LIBVIRT_CPU_POLICY_VERSIONS:
4028             if self._host.has_version(ver):
4029                 ver_ = self._version_to_string(ver)
4030                 raise exception.CPUPinningNotSupported(reason=_(
4031                     'Invalid libvirt version %(version)s') % {'version': ver_})
4032         return True
4033 
4034     def _wants_hugepages(self, host_topology, instance_topology):
4035         """Determine if the guest / host topology implies the
4036            use of huge pages for guest RAM backing
4037         """
4038 
4039         if host_topology is None or instance_topology is None:
4040             return False
4041 
4042         avail_pagesize = [page.size_kb
4043                           for page in host_topology.cells[0].mempages]
4044         avail_pagesize.sort()
4045         # Remove smallest page size as that's not classed as a largepage
4046         avail_pagesize = avail_pagesize[1:]
4047 
4048         # See if we have page size set
4049         for cell in instance_topology.cells:
4050             if (cell.pagesize is not None and
4051                 cell.pagesize in avail_pagesize):
4052                 return True
4053 
4054         return False
4055 
4056     def _get_guest_numa_config(self, instance_numa_topology, flavor,
4057                                allowed_cpus=None, image_meta=None):
4058         """Returns the config objects for the guest NUMA specs.
4059 
4060         Determines the CPUs that the guest can be pinned to if the guest
4061         specifies a cell topology and the host supports it. Constructs the
4062         libvirt XML config object representing the NUMA topology selected
4063         for the guest. Returns a tuple of:
4064 
4065             (cpu_set, guest_cpu_tune, guest_cpu_numa, guest_numa_tune)
4066 
4067         With the following caveats:
4068 
4069             a) If there is no specified guest NUMA topology, then
4070                all tuple elements except cpu_set shall be None. cpu_set
4071                will be populated with the chosen CPUs that the guest
4072                allowed CPUs fit within, which could be the supplied
4073                allowed_cpus value if the host doesn't support NUMA
4074                topologies.
4075 
4076             b) If there is a specified guest NUMA topology, then
4077                cpu_set will be None and guest_cpu_numa will be the
4078                LibvirtConfigGuestCPUNUMA object representing the guest's
4079                NUMA topology. If the host supports NUMA, then guest_cpu_tune
4080                will contain a LibvirtConfigGuestCPUTune object representing
4081                the optimized chosen cells that match the host capabilities
4082                with the instance's requested topology. If the host does
4083                not support NUMA, then guest_cpu_tune and guest_numa_tune
4084                will be None.
4085         """
4086 
4087         if (not self._has_numa_support() and
4088                 instance_numa_topology is not None):
4089             # We should not get here, since we should have avoided
4090             # reporting NUMA topology from _get_host_numa_topology
4091             # in the first place. Just in case of a scheduler
4092             # mess up though, raise an exception
4093             raise exception.NUMATopologyUnsupported()
4094 
4095         topology = self._get_host_numa_topology()
4096 
4097         # We have instance NUMA so translate it to the config class
4098         guest_cpu_numa_config = self._get_cpu_numa_config_from_instance(
4099                 instance_numa_topology,
4100                 self._wants_hugepages(topology, instance_numa_topology))
4101 
4102         if not guest_cpu_numa_config:
4103             # No NUMA topology defined for instance - let the host kernel deal
4104             # with the NUMA effects.
4105             # TODO(ndipanov): Attempt to spread the instance
4106             # across NUMA nodes and expose the topology to the
4107             # instance as an optimisation
4108             return GuestNumaConfig(allowed_cpus, None, None, None)
4109         else:
4110             if topology:
4111                 # Now get the CpuTune configuration from the numa_topology
4112                 guest_cpu_tune = vconfig.LibvirtConfigGuestCPUTune()
4113                 guest_numa_tune = vconfig.LibvirtConfigGuestNUMATune()
4114                 emupcpus = []
4115 
4116                 numa_mem = vconfig.LibvirtConfigGuestNUMATuneMemory()
4117                 numa_memnodes = [vconfig.LibvirtConfigGuestNUMATuneMemNode()
4118                                  for _ in guest_cpu_numa_config.cells]
4119 
4120                 emulator_threads_isolated = (
4121                     instance_numa_topology.emulator_threads_isolated)
4122 
4123                 vcpus_rt = set([])
4124                 wants_realtime = hardware.is_realtime_enabled(flavor)
4125                 if wants_realtime:
4126                     if not self._host.has_min_version(
4127                             MIN_LIBVIRT_REALTIME_VERSION):
4128                         raise exception.RealtimePolicyNotSupported()
4129                     # Prepare realtime config for libvirt
4130                     vcpus_rt = hardware.vcpus_realtime_topology(
4131                         flavor, image_meta)
4132                     vcpusched = vconfig.LibvirtConfigGuestCPUTuneVCPUSched()
4133                     vcpusched.vcpus = vcpus_rt
4134                     vcpusched.scheduler = "fifo"
4135                     vcpusched.priority = (
4136                         CONF.libvirt.realtime_scheduler_priority)
4137                     guest_cpu_tune.vcpusched.append(vcpusched)
4138 
4139                 # TODO(sahid): Defining domain topology should be
4140                 # refactored.
4141                 for host_cell in topology.cells:
4142                     for guest_node_id, guest_config_cell in enumerate(
4143                             guest_cpu_numa_config.cells):
4144                         if guest_config_cell.id == host_cell.id:
4145                             node = numa_memnodes[guest_node_id]
4146                             node.cellid = guest_node_id
4147                             node.nodeset = [host_cell.id]
4148                             node.mode = "strict"
4149 
4150                             numa_mem.nodeset.append(host_cell.id)
4151 
4152                             object_numa_cell = (
4153                                     instance_numa_topology.cells[guest_node_id]
4154                                 )
4155                             for cpu in guest_config_cell.cpus:
4156                                 pin_cpuset = (
4157                                     vconfig.LibvirtConfigGuestCPUTuneVCPUPin())
4158                                 pin_cpuset.id = cpu
4159                                 # If there is pinning information in the cell
4160                                 # we pin to individual CPUs, otherwise we float
4161                                 # over the whole host NUMA node
4162 
4163                                 if (object_numa_cell.cpu_pinning and
4164                                         self._has_cpu_policy_support()):
4165                                     pcpu = object_numa_cell.cpu_pinning[cpu]
4166                                     pin_cpuset.cpuset = set([pcpu])
4167                                 else:
4168                                     pin_cpuset.cpuset = host_cell.cpuset
4169                                 if emulator_threads_isolated:
4170                                     emupcpus.extend(
4171                                         object_numa_cell.cpuset_reserved)
4172                                 elif not wants_realtime or cpu not in vcpus_rt:
4173                                     # - If realtime IS NOT enabled, the
4174                                     #   emulator threads are allowed to float
4175                                     #   across all the pCPUs associated with
4176                                     #   the guest vCPUs ("not wants_realtime"
4177                                     #   is true, so we add all pcpus)
4178                                     # - If realtime IS enabled, then at least
4179                                     #   1 vCPU is required to be set aside for
4180                                     #   non-realtime usage. The emulator
4181                                     #   threads are allowed to float acros the
4182                                     #   pCPUs that are associated with the
4183                                     #   non-realtime VCPUs (the "cpu not in
4184                                     #   vcpu_rt" check deals with this
4185                                     #   filtering)
4186                                     emupcpus.extend(pin_cpuset.cpuset)
4187                                 guest_cpu_tune.vcpupin.append(pin_cpuset)
4188 
4189                 # TODO(berrange) When the guest has >1 NUMA node, it will
4190                 # span multiple host NUMA nodes. By pinning emulator threads
4191                 # to the union of all nodes, we guarantee there will be
4192                 # cross-node memory access by the emulator threads when
4193                 # responding to guest I/O operations. The only way to avoid
4194                 # this would be to pin emulator threads to a single node and
4195                 # tell the guest OS to only do I/O from one of its virtual
4196                 # NUMA nodes. This is not even remotely practical.
4197                 #
4198                 # The long term solution is to make use of a new QEMU feature
4199                 # called "I/O Threads" which will let us configure an explicit
4200                 # I/O thread for each guest vCPU or guest NUMA node. It is
4201                 # still TBD how to make use of this feature though, especially
4202                 # how to associate IO threads with guest devices to eliminate
4203                 # cross NUMA node traffic. This is an area of investigation
4204                 # for QEMU community devs.
4205                 emulatorpin = vconfig.LibvirtConfigGuestCPUTuneEmulatorPin()
4206                 emulatorpin.cpuset = set(emupcpus)
4207                 guest_cpu_tune.emulatorpin = emulatorpin
4208                 # Sort the vcpupin list per vCPU id for human-friendlier XML
4209                 guest_cpu_tune.vcpupin.sort(key=operator.attrgetter("id"))
4210 
4211                 guest_numa_tune.memory = numa_mem
4212                 guest_numa_tune.memnodes = numa_memnodes
4213 
4214                 # normalize cell.id
4215                 for i, (cell, memnode) in enumerate(
4216                                             zip(guest_cpu_numa_config.cells,
4217                                                 guest_numa_tune.memnodes)):
4218                     cell.id = i
4219                     memnode.cellid = i
4220 
4221                 return GuestNumaConfig(None, guest_cpu_tune,
4222                                        guest_cpu_numa_config,
4223                                        guest_numa_tune)
4224             else:
4225                 return GuestNumaConfig(allowed_cpus, None,
4226                                        guest_cpu_numa_config, None)
4227 
4228     def _get_guest_os_type(self, virt_type):
4229         """Returns the guest OS type based on virt type."""
4230         if virt_type == "lxc":
4231             ret = fields.VMMode.EXE
4232         elif virt_type == "uml":
4233             ret = fields.VMMode.UML
4234         elif virt_type == "xen":
4235             ret = fields.VMMode.XEN
4236         else:
4237             ret = fields.VMMode.HVM
4238         return ret
4239 
4240     def _set_guest_for_rescue(self, rescue, guest, inst_path, virt_type,
4241                               root_device_name):
4242         if rescue.get('kernel_id'):
4243             guest.os_kernel = os.path.join(inst_path, "kernel.rescue")
4244             guest.os_cmdline = ("root=%s %s" % (root_device_name, CONSOLE))
4245             if virt_type == "qemu":
4246                 guest.os_cmdline += " no_timer_check"
4247         if rescue.get('ramdisk_id'):
4248             guest.os_initrd = os.path.join(inst_path, "ramdisk.rescue")
4249 
4250     def _set_guest_for_inst_kernel(self, instance, guest, inst_path, virt_type,
4251                                 root_device_name, image_meta):
4252         guest.os_kernel = os.path.join(inst_path, "kernel")
4253         guest.os_cmdline = ("root=%s %s" % (root_device_name, CONSOLE))
4254         if virt_type == "qemu":
4255             guest.os_cmdline += " no_timer_check"
4256         if instance.ramdisk_id:
4257             guest.os_initrd = os.path.join(inst_path, "ramdisk")
4258         # we only support os_command_line with images with an explicit
4259         # kernel set and don't want to break nova if there's an
4260         # os_command_line property without a specified kernel_id param
4261         if image_meta.properties.get("os_command_line"):
4262             guest.os_cmdline = image_meta.properties.os_command_line
4263 
4264     def _set_clock(self, guest, os_type, image_meta, virt_type):
4265         # NOTE(mikal): Microsoft Windows expects the clock to be in
4266         # "localtime". If the clock is set to UTC, then you can use a
4267         # registry key to let windows know, but Microsoft says this is
4268         # buggy in http://support.microsoft.com/kb/2687252
4269         clk = vconfig.LibvirtConfigGuestClock()
4270         if os_type == 'windows':
4271             LOG.info('Configuring timezone for windows instance to localtime')
4272             clk.offset = 'localtime'
4273         else:
4274             clk.offset = 'utc'
4275         guest.set_clock(clk)
4276 
4277         if virt_type == "kvm":
4278             self._set_kvm_timers(clk, os_type, image_meta)
4279 
4280     def _set_kvm_timers(self, clk, os_type, image_meta):
4281         # TODO(berrange) One day this should be per-guest
4282         # OS type configurable
4283         tmpit = vconfig.LibvirtConfigGuestTimer()
4284         tmpit.name = "pit"
4285         tmpit.tickpolicy = "delay"
4286 
4287         tmrtc = vconfig.LibvirtConfigGuestTimer()
4288         tmrtc.name = "rtc"
4289         tmrtc.tickpolicy = "catchup"
4290 
4291         clk.add_timer(tmpit)
4292         clk.add_timer(tmrtc)
4293 
4294         guestarch = libvirt_utils.get_arch(image_meta)
4295         if guestarch in (fields.Architecture.I686,
4296                          fields.Architecture.X86_64):
4297             # NOTE(rfolco): HPET is a hardware timer for x86 arch.
4298             # qemu -no-hpet is not supported on non-x86 targets.
4299             tmhpet = vconfig.LibvirtConfigGuestTimer()
4300             tmhpet.name = "hpet"
4301             tmhpet.present = False
4302             clk.add_timer(tmhpet)
4303 
4304         # Provide Windows guests with the paravirtualized hyperv timer source.
4305         # This is the windows equiv of kvm-clock, allowing Windows
4306         # guests to accurately keep time.
4307         if os_type == 'windows':
4308             tmhyperv = vconfig.LibvirtConfigGuestTimer()
4309             tmhyperv.name = "hypervclock"
4310             tmhyperv.present = True
4311             clk.add_timer(tmhyperv)
4312 
4313     def _set_features(self, guest, os_type, caps, virt_type, image_meta):
4314         if virt_type == "xen":
4315             # PAE only makes sense in X86
4316             if caps.host.cpu.arch in (fields.Architecture.I686,
4317                                       fields.Architecture.X86_64):
4318                 guest.features.append(vconfig.LibvirtConfigGuestFeaturePAE())
4319 
4320         if (virt_type not in ("lxc", "uml", "parallels", "xen") or
4321                 (virt_type == "xen" and guest.os_type == fields.VMMode.HVM)):
4322             guest.features.append(vconfig.LibvirtConfigGuestFeatureACPI())
4323             guest.features.append(vconfig.LibvirtConfigGuestFeatureAPIC())
4324 
4325         if (virt_type in ("qemu", "kvm") and
4326                 os_type == 'windows'):
4327             hv = vconfig.LibvirtConfigGuestFeatureHyperV()
4328             hv.relaxed = True
4329 
4330             hv.spinlocks = True
4331             # Increase spinlock retries - value recommended by
4332             # KVM maintainers who certify Windows guests
4333             # with Microsoft
4334             hv.spinlock_retries = 8191
4335             hv.vapic = True
4336             guest.features.append(hv)
4337 
4338         if (virt_type in ("qemu", "kvm") and
4339                 image_meta.properties.get('img_hide_hypervisor_id')):
4340             guest.features.append(vconfig.LibvirtConfigGuestFeatureKvmHidden())
4341 
4342     def _check_number_of_serial_console(self, num_ports):
4343         virt_type = CONF.libvirt.virt_type
4344         if (virt_type in ("kvm", "qemu") and
4345             num_ports > ALLOWED_QEMU_SERIAL_PORTS):
4346             raise exception.SerialPortNumberLimitExceeded(
4347                 allowed=ALLOWED_QEMU_SERIAL_PORTS, virt_type=virt_type)
4348 
4349     def _add_video_driver(self, guest, image_meta, flavor):
4350         VALID_VIDEO_DEVICES = ("vga", "cirrus", "vmvga", "xen", "qxl")
4351         video = vconfig.LibvirtConfigGuestVideo()
4352         # NOTE(ldbragst): The following logic sets the video.type
4353         # depending on supported defaults given the architecture,
4354         # virtualization type, and features. The video.type attribute can
4355         # be overridden by the user with image_meta.properties, which
4356         # is carried out in the next if statement below this one.
4357         guestarch = libvirt_utils.get_arch(image_meta)
4358         if guest.os_type == fields.VMMode.XEN:
4359             video.type = 'xen'
4360         elif CONF.libvirt.virt_type == 'parallels':
4361             video.type = 'vga'
4362         elif guestarch in (fields.Architecture.PPC,
4363                            fields.Architecture.PPC64,
4364                            fields.Architecture.PPC64LE):
4365             # NOTE(ldbragst): PowerKVM doesn't support 'cirrus' be default
4366             # so use 'vga' instead when running on Power hardware.
4367             video.type = 'vga'
4368         elif CONF.spice.enabled:
4369             video.type = 'qxl'
4370         if image_meta.properties.get('hw_video_model'):
4371             video.type = image_meta.properties.hw_video_model
4372             if (video.type not in VALID_VIDEO_DEVICES):
4373                 raise exception.InvalidVideoMode(model=video.type)
4374 
4375         # Set video memory, only if the flavor's limit is set
4376         video_ram = image_meta.properties.get('hw_video_ram', 0)
4377         max_vram = int(flavor.extra_specs.get('hw_video:ram_max_mb', 0))
4378         if video_ram > max_vram:
4379             raise exception.RequestedVRamTooHigh(req_vram=video_ram,
4380                                                  max_vram=max_vram)
4381         if max_vram and video_ram:
4382             video.vram = video_ram * units.Mi / units.Ki
4383         guest.add_device(video)
4384 
4385     def _add_qga_device(self, guest, instance):
4386         qga = vconfig.LibvirtConfigGuestChannel()
4387         qga.type = "unix"
4388         qga.target_name = "org.qemu.guest_agent.0"
4389         qga.source_path = ("/var/lib/libvirt/qemu/%s.%s.sock" %
4390                           ("org.qemu.guest_agent.0", instance.name))
4391         guest.add_device(qga)
4392 
4393     def _add_rng_device(self, guest, flavor):
4394         rng_device = vconfig.LibvirtConfigGuestRng()
4395         rate_bytes = flavor.extra_specs.get('hw_rng:rate_bytes', 0)
4396         period = flavor.extra_specs.get('hw_rng:rate_period', 0)
4397         if rate_bytes:
4398             rng_device.rate_bytes = int(rate_bytes)
4399             rng_device.rate_period = int(period)
4400         rng_path = CONF.libvirt.rng_dev_path
4401         if (rng_path and not os.path.exists(rng_path)):
4402             raise exception.RngDeviceNotExist(path=rng_path)
4403         rng_device.backend = rng_path
4404         guest.add_device(rng_device)
4405 
4406     def _set_qemu_guest_agent(self, guest, flavor, instance, image_meta):
4407         # Enable qga only if the 'hw_qemu_guest_agent' is equal to yes
4408         if image_meta.properties.get('hw_qemu_guest_agent', False):
4409             LOG.debug("Qemu guest agent is enabled through image "
4410                       "metadata", instance=instance)
4411             self._add_qga_device(guest, instance)
4412         rng_is_virtio = image_meta.properties.get('hw_rng_model') == 'virtio'
4413         rng_allowed_str = flavor.extra_specs.get('hw_rng:allowed', '')
4414         rng_allowed = strutils.bool_from_string(rng_allowed_str)
4415         if rng_is_virtio and rng_allowed:
4416             self._add_rng_device(guest, flavor)
4417 
4418     def _get_guest_memory_backing_config(
4419             self, inst_topology, numatune, flavor):
4420         wantsmempages = False
4421         if inst_topology:
4422             for cell in inst_topology.cells:
4423                 if cell.pagesize:
4424                     wantsmempages = True
4425                     break
4426 
4427         wantsrealtime = hardware.is_realtime_enabled(flavor)
4428 
4429         membacking = None
4430         if wantsmempages:
4431             pages = self._get_memory_backing_hugepages_support(
4432                 inst_topology, numatune)
4433             if pages:
4434                 membacking = vconfig.LibvirtConfigGuestMemoryBacking()
4435                 membacking.hugepages = pages
4436         if wantsrealtime:
4437             if not membacking:
4438                 membacking = vconfig.LibvirtConfigGuestMemoryBacking()
4439             membacking.locked = True
4440             membacking.sharedpages = False
4441 
4442         return membacking
4443 
4444     def _get_memory_backing_hugepages_support(self, inst_topology, numatune):
4445         if not self._has_numa_support():
4446             # We should not get here, since we should have avoided
4447             # reporting NUMA topology from _get_host_numa_topology
4448             # in the first place. Just in case of a scheduler
4449             # mess up though, raise an exception
4450             raise exception.MemoryPagesUnsupported()
4451 
4452         host_topology = self._get_host_numa_topology()
4453 
4454         if host_topology is None:
4455             # As above, we should not get here but just in case...
4456             raise exception.MemoryPagesUnsupported()
4457 
4458         # Currently libvirt does not support the smallest
4459         # pagesize set as a backend memory.
4460         # https://bugzilla.redhat.com/show_bug.cgi?id=1173507
4461         avail_pagesize = [page.size_kb
4462                           for page in host_topology.cells[0].mempages]
4463         avail_pagesize.sort()
4464         smallest = avail_pagesize[0]
4465 
4466         pages = []
4467         for guest_cellid, inst_cell in enumerate(inst_topology.cells):
4468             if inst_cell.pagesize and inst_cell.pagesize > smallest:
4469                 for memnode in numatune.memnodes:
4470                     if guest_cellid == memnode.cellid:
4471                         page = (
4472                             vconfig.LibvirtConfigGuestMemoryBackingPage())
4473                         page.nodeset = [guest_cellid]
4474                         page.size_kb = inst_cell.pagesize
4475                         pages.append(page)
4476                         break  # Quit early...
4477         return pages
4478 
4479     def _get_flavor(self, ctxt, instance, flavor):
4480         if flavor is not None:
4481             return flavor
4482         return instance.flavor
4483 
4484     def _has_uefi_support(self):
4485         # This means that the host can support uefi booting for guests
4486         supported_archs = [fields.Architecture.X86_64,
4487                            fields.Architecture.AARCH64]
4488         caps = self._host.get_capabilities()
4489         return ((caps.host.cpu.arch in supported_archs) and
4490                 os.path.exists(DEFAULT_UEFI_LOADER_PATH[caps.host.cpu.arch]))
4491 
4492     def _get_supported_perf_events(self):
4493 
4494         if (len(CONF.libvirt.enabled_perf_events) == 0 or
4495              not self._host.has_min_version(MIN_LIBVIRT_PERF_VERSION)):
4496             return []
4497 
4498         supported_events = []
4499         host_cpu_info = self._get_cpu_info()
4500         for event in CONF.libvirt.enabled_perf_events:
4501             if self._supported_perf_event(event, host_cpu_info['features']):
4502                 supported_events.append(event)
4503         return supported_events
4504 
4505     def _supported_perf_event(self, event, cpu_features):
4506 
4507         libvirt_perf_event_name = LIBVIRT_PERF_EVENT_PREFIX + event.upper()
4508 
4509         if not hasattr(libvirt, libvirt_perf_event_name):
4510             LOG.warning("Libvirt doesn't support event type %s.", event)
4511             return False
4512 
4513         if (event in PERF_EVENTS_CPU_FLAG_MAPPING
4514             and PERF_EVENTS_CPU_FLAG_MAPPING[event] not in cpu_features):
4515             LOG.warning("Host does not support event type %s.", event)
4516             return False
4517 
4518         return True
4519 
4520     def _configure_guest_by_virt_type(self, guest, virt_type, caps, instance,
4521                                       image_meta, flavor, root_device_name):
4522         if virt_type == "xen":
4523             if guest.os_type == fields.VMMode.HVM:
4524                 guest.os_loader = CONF.libvirt.xen_hvmloader_path
4525             else:
4526                 guest.os_cmdline = CONSOLE
4527         elif virt_type in ("kvm", "qemu"):
4528             if caps.host.cpu.arch in (fields.Architecture.I686,
4529                                       fields.Architecture.X86_64):
4530                 guest.sysinfo = self._get_guest_config_sysinfo(instance)
4531                 guest.os_smbios = vconfig.LibvirtConfigGuestSMBIOS()
4532             hw_firmware_type = image_meta.properties.get('hw_firmware_type')
4533             if hw_firmware_type == fields.FirmwareType.UEFI:
4534                 if self._has_uefi_support():
4535                     global uefi_logged
4536                     if not uefi_logged:
4537                         LOG.warning("uefi support is without some kind of "
4538                                     "functional testing and therefore "
4539                                     "considered experimental.")
4540                         uefi_logged = True
4541                     guest.os_loader = DEFAULT_UEFI_LOADER_PATH[
4542                         caps.host.cpu.arch]
4543                     guest.os_loader_type = "pflash"
4544                 else:
4545                     raise exception.UEFINotSupported()
4546             guest.os_mach_type = self._get_machine_type(image_meta, caps)
4547             if image_meta.properties.get('hw_boot_menu') is None:
4548                 guest.os_bootmenu = strutils.bool_from_string(
4549                     flavor.extra_specs.get('hw:boot_menu', 'no'))
4550             else:
4551                 guest.os_bootmenu = image_meta.properties.hw_boot_menu
4552 
4553         elif virt_type == "lxc":
4554             guest.os_init_path = "/sbin/init"
4555             guest.os_cmdline = CONSOLE
4556         elif virt_type == "uml":
4557             guest.os_kernel = "/usr/bin/linux"
4558             guest.os_root = root_device_name
4559         elif virt_type == "parallels":
4560             if guest.os_type == fields.VMMode.EXE:
4561                 guest.os_init_path = "/sbin/init"
4562 
4563     def _conf_non_lxc_uml(self, virt_type, guest, root_device_name, rescue,
4564                     instance, inst_path, image_meta, disk_info):
4565         if rescue:
4566             self._set_guest_for_rescue(rescue, guest, inst_path, virt_type,
4567                                        root_device_name)
4568         elif instance.kernel_id:
4569             self._set_guest_for_inst_kernel(instance, guest, inst_path,
4570                                             virt_type, root_device_name,
4571                                             image_meta)
4572         else:
4573             guest.os_boot_dev = blockinfo.get_boot_order(disk_info)
4574 
4575     def _create_consoles(self, virt_type, guest_cfg, instance, flavor,
4576                          image_meta):
4577         # NOTE(markus_z): Beware! Below are so many conditionals that it is
4578         # easy to lose track. Use this chart to figure out your case:
4579         #
4580         # case | is serial | has       | is qemu | resulting
4581         #      | enabled?  | virtlogd? | or kvm? | devices
4582         # --------------------------------------------------
4583         #    1 |        no |        no |     no  | pty*
4584         #    2 |        no |        no |     yes | file + pty
4585         #    3 |        no |       yes |      no | see case 1
4586         #    4 |        no |       yes |     yes | pty with logd
4587         #    5 |       yes |        no |      no | see case 1
4588         #    6 |       yes |        no |     yes | tcp + pty
4589         #    7 |       yes |       yes |      no | see case 1
4590         #    8 |       yes |       yes |     yes | tcp with logd
4591         #    * exception: virt_type "parallels" doesn't create a device
4592         if virt_type == 'parallels':
4593             pass
4594         elif virt_type not in ("qemu", "kvm"):
4595             log_path = self._get_console_log_path(instance)
4596             self._create_pty_device(guest_cfg,
4597                                     vconfig.LibvirtConfigGuestConsole,
4598                                     log_path=log_path)
4599         elif (virt_type in ("qemu", "kvm") and
4600                   self._is_s390x_guest(image_meta)):
4601             self._create_consoles_s390x(guest_cfg, instance,
4602                                         flavor, image_meta)
4603         elif virt_type in ("qemu", "kvm"):
4604             self._create_consoles_qemu_kvm(guest_cfg, instance,
4605                                         flavor, image_meta)
4606 
4607     def _is_s390x_guest(self, image_meta):
4608         s390x_archs = (fields.Architecture.S390, fields.Architecture.S390X)
4609         return libvirt_utils.get_arch(image_meta) in s390x_archs
4610 
4611     def _create_consoles_qemu_kvm(self, guest_cfg, instance, flavor,
4612                                   image_meta):
4613         char_dev_cls = vconfig.LibvirtConfigGuestSerial
4614         log_path = self._get_console_log_path(instance)
4615         if CONF.serial_console.enabled:
4616             if not self._serial_ports_already_defined(instance):
4617                 num_ports = hardware.get_number_of_serial_ports(flavor,
4618                                                                 image_meta)
4619                 self._check_number_of_serial_console(num_ports)
4620                 self._create_serial_consoles(guest_cfg, num_ports,
4621                                              char_dev_cls, log_path)
4622         else:
4623             self._create_file_device(guest_cfg, instance, char_dev_cls)
4624         self._create_pty_device(guest_cfg, char_dev_cls, log_path=log_path)
4625 
4626     def _create_consoles_s390x(self, guest_cfg, instance, flavor, image_meta):
4627         char_dev_cls = vconfig.LibvirtConfigGuestConsole
4628         log_path = self._get_console_log_path(instance)
4629         if CONF.serial_console.enabled:
4630             if not self._serial_ports_already_defined(instance):
4631                 num_ports = hardware.get_number_of_serial_ports(flavor,
4632                                                                 image_meta)
4633                 self._create_serial_consoles(guest_cfg, num_ports,
4634                                              char_dev_cls, log_path)
4635         else:
4636             self._create_file_device(guest_cfg, instance, char_dev_cls,
4637                                      "sclplm")
4638         self._create_pty_device(guest_cfg, char_dev_cls, "sclp", log_path)
4639 
4640     def _create_pty_device(self, guest_cfg, char_dev_cls, target_type=None,
4641                            log_path=None):
4642         def _create_base_dev():
4643             consolepty = char_dev_cls()
4644             consolepty.target_type = target_type
4645             consolepty.type = "pty"
4646             return consolepty
4647 
4648         def _create_logd_dev():
4649             consolepty = _create_base_dev()
4650             log = vconfig.LibvirtConfigGuestCharDeviceLog()
4651             log.file = log_path
4652             consolepty.log = log
4653             return consolepty
4654 
4655         if CONF.serial_console.enabled:
4656             if self._is_virtlogd_available():
4657                 return
4658             else:
4659                 # NOTE(markus_z): You may wonder why this is necessary and
4660                 # so do I. I'm certain that this is *not* needed in any
4661                 # real use case. It is, however, useful if you want to
4662                 # pypass the Nova API and use "virsh console <guest>" on
4663                 # an hypervisor, as this CLI command doesn't work with TCP
4664                 # devices (like the serial console is).
4665                 #     https://bugzilla.redhat.com/show_bug.cgi?id=781467
4666                 # Pypassing the Nova API however is a thing we don't want.
4667                 # Future changes should remove this and fix the unit tests
4668                 # which ask for the existence.
4669                 guest_cfg.add_device(_create_base_dev())
4670         else:
4671             if self._is_virtlogd_available():
4672                 guest_cfg.add_device(_create_logd_dev())
4673             else:
4674                 guest_cfg.add_device(_create_base_dev())
4675 
4676     def _create_file_device(self, guest_cfg, instance, char_dev_cls,
4677                             target_type=None):
4678         if self._is_virtlogd_available():
4679             return
4680 
4681         consolelog = char_dev_cls()
4682         consolelog.target_type = target_type
4683         consolelog.type = "file"
4684         consolelog.source_path = self._get_console_log_path(instance)
4685         guest_cfg.add_device(consolelog)
4686 
4687     def _serial_ports_already_defined(self, instance):
4688         try:
4689             guest = self._host.get_guest(instance)
4690             if list(self._get_serial_ports_from_guest(guest)):
4691                 # Serial port are already configured for instance that
4692                 # means we are in a context of migration.
4693                 return True
4694         except exception.InstanceNotFound:
4695             LOG.debug(
4696                 "Instance does not exist yet on libvirt, we can "
4697                 "safely pass on looking for already defined serial "
4698                 "ports in its domain XML", instance=instance)
4699         return False
4700 
4701     def _create_serial_consoles(self, guest_cfg, num_ports, char_dev_cls,
4702                                 log_path):
4703         for port in six.moves.range(num_ports):
4704             console = char_dev_cls()
4705             console.port = port
4706             console.type = "tcp"
4707             console.listen_host = CONF.serial_console.proxyclient_address
4708             listen_port = serial_console.acquire_port(console.listen_host)
4709             console.listen_port = listen_port
4710             # NOTE: only the first serial console gets the boot messages,
4711             # that's why we attach the logd subdevice only to that.
4712             if port == 0 and self._is_virtlogd_available():
4713                 log = vconfig.LibvirtConfigGuestCharDeviceLog()
4714                 log.file = log_path
4715                 console.log = log
4716             guest_cfg.add_device(console)
4717 
4718     def _cpu_config_to_vcpu_model(self, cpu_config, vcpu_model):
4719         """Update VirtCPUModel object according to libvirt CPU config.
4720 
4721         :param:cpu_config: vconfig.LibvirtConfigGuestCPU presenting the
4722                            instance's virtual cpu configuration.
4723         :param:vcpu_model: VirtCPUModel object. A new object will be created
4724                            if None.
4725 
4726         :return: Updated VirtCPUModel object, or None if cpu_config is None
4727 
4728         """
4729 
4730         if not cpu_config:
4731             return
4732         if not vcpu_model:
4733             vcpu_model = objects.VirtCPUModel()
4734 
4735         vcpu_model.arch = cpu_config.arch
4736         vcpu_model.vendor = cpu_config.vendor
4737         vcpu_model.model = cpu_config.model
4738         vcpu_model.mode = cpu_config.mode
4739         vcpu_model.match = cpu_config.match
4740 
4741         if cpu_config.sockets:
4742             vcpu_model.topology = objects.VirtCPUTopology(
4743                 sockets=cpu_config.sockets,
4744                 cores=cpu_config.cores,
4745                 threads=cpu_config.threads)
4746         else:
4747             vcpu_model.topology = None
4748 
4749         features = [objects.VirtCPUFeature(
4750             name=f.name,
4751             policy=f.policy) for f in cpu_config.features]
4752         vcpu_model.features = features
4753 
4754         return vcpu_model
4755 
4756     def _vcpu_model_to_cpu_config(self, vcpu_model):
4757         """Create libvirt CPU config according to VirtCPUModel object.
4758 
4759         :param:vcpu_model: VirtCPUModel object.
4760 
4761         :return: vconfig.LibvirtConfigGuestCPU.
4762 
4763         """
4764 
4765         cpu_config = vconfig.LibvirtConfigGuestCPU()
4766         cpu_config.arch = vcpu_model.arch
4767         cpu_config.model = vcpu_model.model
4768         cpu_config.mode = vcpu_model.mode
4769         cpu_config.match = vcpu_model.match
4770         cpu_config.vendor = vcpu_model.vendor
4771         if vcpu_model.topology:
4772             cpu_config.sockets = vcpu_model.topology.sockets
4773             cpu_config.cores = vcpu_model.topology.cores
4774             cpu_config.threads = vcpu_model.topology.threads
4775         if vcpu_model.features:
4776             for f in vcpu_model.features:
4777                 xf = vconfig.LibvirtConfigGuestCPUFeature()
4778                 xf.name = f.name
4779                 xf.policy = f.policy
4780                 cpu_config.features.add(xf)
4781         return cpu_config
4782 
4783     def _get_guest_config(self, instance, network_info, image_meta,
4784                           disk_info, rescue=None, block_device_info=None,
4785                           context=None):
4786         """Get config data for parameters.
4787 
4788         :param rescue: optional dictionary that should contain the key
4789             'ramdisk_id' if a ramdisk is needed for the rescue image and
4790             'kernel_id' if a kernel is needed for the rescue image.
4791         """
4792         flavor = instance.flavor
4793         inst_path = libvirt_utils.get_instance_path(instance)
4794         disk_mapping = disk_info['mapping']
4795 
4796         virt_type = CONF.libvirt.virt_type
4797         guest = vconfig.LibvirtConfigGuest()
4798         guest.virt_type = virt_type
4799         guest.name = instance.name
4800         guest.uuid = instance.uuid
4801         # We are using default unit for memory: KiB
4802         guest.memory = flavor.memory_mb * units.Ki
4803         guest.vcpus = flavor.vcpus
4804         allowed_cpus = hardware.get_vcpu_pin_set()
4805 
4806         guest_numa_config = self._get_guest_numa_config(
4807             instance.numa_topology, flavor, allowed_cpus, image_meta)
4808 
4809         guest.cpuset = guest_numa_config.cpuset
4810         guest.cputune = guest_numa_config.cputune
4811         guest.numatune = guest_numa_config.numatune
4812 
4813         guest.membacking = self._get_guest_memory_backing_config(
4814             instance.numa_topology,
4815             guest_numa_config.numatune,
4816             flavor)
4817 
4818         guest.metadata.append(self._get_guest_config_meta(instance))
4819         guest.idmaps = self._get_guest_idmaps()
4820 
4821         for event in self._supported_perf_events:
4822             guest.add_perf_event(event)
4823 
4824         self._update_guest_cputune(guest, flavor, virt_type)
4825 
4826         guest.cpu = self._get_guest_cpu_config(
4827             flavor, image_meta, guest_numa_config.numaconfig,
4828             instance.numa_topology)
4829 
4830         # Notes(yjiang5): we always sync the instance's vcpu model with
4831         # the corresponding config file.
4832         instance.vcpu_model = self._cpu_config_to_vcpu_model(
4833             guest.cpu, instance.vcpu_model)
4834 
4835         if 'root' in disk_mapping:
4836             root_device_name = block_device.prepend_dev(
4837                 disk_mapping['root']['dev'])
4838         else:
4839             root_device_name = None
4840 
4841         if root_device_name:
4842             # NOTE(yamahata):
4843             # for nova.api.ec2.cloud.CloudController.get_metadata()
4844             instance.root_device_name = root_device_name
4845 
4846         guest.os_type = (fields.VMMode.get_from_instance(instance) or
4847                 self._get_guest_os_type(virt_type))
4848         caps = self._host.get_capabilities()
4849 
4850         self._configure_guest_by_virt_type(guest, virt_type, caps, instance,
4851                                            image_meta, flavor,
4852                                            root_device_name)
4853         if virt_type not in ('lxc', 'uml'):
4854             self._conf_non_lxc_uml(virt_type, guest, root_device_name, rescue,
4855                     instance, inst_path, image_meta, disk_info)
4856 
4857         self._set_features(guest, instance.os_type, caps, virt_type,
4858                            image_meta)
4859         self._set_clock(guest, instance.os_type, image_meta, virt_type)
4860 
4861         storage_configs = self._get_guest_storage_config(
4862                 instance, image_meta, disk_info, rescue, block_device_info,
4863                 flavor, guest.os_type)
4864         for config in storage_configs:
4865             guest.add_device(config)
4866 
4867         for vif in network_info:
4868             config = self.vif_driver.get_config(
4869                 instance, vif, image_meta,
4870                 flavor, virt_type, self._host)
4871             guest.add_device(config)
4872 
4873         self._create_consoles(virt_type, guest, instance, flavor, image_meta)
4874 
4875         pointer = self._get_guest_pointer_model(guest.os_type, image_meta)
4876         if pointer:
4877             guest.add_device(pointer)
4878 
4879         self._guest_add_spice_channel(guest)
4880 
4881         if self._guest_add_video_device(guest):
4882             self._add_video_driver(guest, image_meta, flavor)
4883 
4884         # Qemu guest agent only support 'qemu' and 'kvm' hypervisor
4885         if virt_type in ('qemu', 'kvm'):
4886             self._set_qemu_guest_agent(guest, flavor, instance, image_meta)
4887 
4888         self._guest_add_pci_devices(guest, instance)
4889 
4890         self._guest_add_watchdog_action(guest, flavor, image_meta)
4891 
4892         self._guest_add_memory_balloon(guest)
4893 
4894         return guest
4895 
4896     @staticmethod
4897     def _guest_add_spice_channel(guest):
4898         if (CONF.spice.enabled and CONF.spice.agent_enabled
4899                 and guest.virt_type not in ('lxc', 'uml', 'xen')):
4900             channel = vconfig.LibvirtConfigGuestChannel()
4901             channel.type = 'spicevmc'
4902             channel.target_name = "com.redhat.spice.0"
4903             guest.add_device(channel)
4904 
4905     @staticmethod
4906     def _guest_add_memory_balloon(guest):
4907         virt_type = guest.virt_type
4908         # Memory balloon device only support 'qemu/kvm' and 'xen' hypervisor
4909         if (virt_type in ('xen', 'qemu', 'kvm') and
4910                     CONF.libvirt.mem_stats_period_seconds > 0):
4911             balloon = vconfig.LibvirtConfigMemoryBalloon()
4912             if virt_type in ('qemu', 'kvm'):
4913                 balloon.model = 'virtio'
4914             else:
4915                 balloon.model = 'xen'
4916             balloon.period = CONF.libvirt.mem_stats_period_seconds
4917             guest.add_device(balloon)
4918 
4919     @staticmethod
4920     def _guest_add_watchdog_action(guest, flavor, image_meta):
4921         # image meta takes precedence over flavor extra specs; disable the
4922         # watchdog action by default
4923         watchdog_action = (flavor.extra_specs.get('hw:watchdog_action')
4924                            or 'disabled')
4925         watchdog_action = image_meta.properties.get('hw_watchdog_action',
4926                                                     watchdog_action)
4927         # NB(sross): currently only actually supported by KVM/QEmu
4928         if watchdog_action != 'disabled':
4929             if watchdog_action in fields.WatchdogAction.ALL:
4930                 bark = vconfig.LibvirtConfigGuestWatchdog()
4931                 bark.action = watchdog_action
4932                 guest.add_device(bark)
4933             else:
4934                 raise exception.InvalidWatchdogAction(action=watchdog_action)
4935 
4936     def _guest_add_pci_devices(self, guest, instance):
4937         virt_type = guest.virt_type
4938         if virt_type in ('xen', 'qemu', 'kvm'):
4939             # Get all generic PCI devices (non-SR-IOV).
4940             for pci_dev in pci_manager.get_instance_pci_devs(instance):
4941                 guest.add_device(self._get_guest_pci_device(pci_dev))
4942         else:
4943             # PCI devices is only supported for hypervisors
4944             #  'xen', 'qemu' and 'kvm'.
4945             if pci_manager.get_instance_pci_devs(instance, 'all'):
4946                 raise exception.PciDeviceUnsupportedHypervisor(type=virt_type)
4947 
4948     @staticmethod
4949     def _guest_add_video_device(guest):
4950         # NB some versions of libvirt support both SPICE and VNC
4951         # at the same time. We're not trying to second guess which
4952         # those versions are. We'll just let libvirt report the
4953         # errors appropriately if the user enables both.
4954         add_video_driver = False
4955         if CONF.vnc.enabled and guest.virt_type not in ('lxc', 'uml'):
4956             graphics = vconfig.LibvirtConfigGuestGraphics()
4957             graphics.type = "vnc"
4958             if CONF.vnc.keymap:
4959                 # TODO(stephenfin): There are some issues here that may
4960                 # necessitate deprecating this option entirely in the future.
4961                 # Refer to bug #1682020 for more information.
4962                 graphics.keymap = CONF.vnc.keymap
4963             graphics.listen = CONF.vnc.vncserver_listen
4964             guest.add_device(graphics)
4965             add_video_driver = True
4966         if CONF.spice.enabled and guest.virt_type not in ('lxc', 'uml', 'xen'):
4967             graphics = vconfig.LibvirtConfigGuestGraphics()
4968             graphics.type = "spice"
4969             if CONF.spice.keymap:
4970                 # TODO(stephenfin): There are some issues here that may
4971                 # necessitate deprecating this option entirely in the future.
4972                 # Refer to bug #1682020 for more information.
4973                 graphics.keymap = CONF.spice.keymap
4974             graphics.listen = CONF.spice.server_listen
4975             guest.add_device(graphics)
4976             add_video_driver = True
4977         return add_video_driver
4978 
4979     def _get_guest_pointer_model(self, os_type, image_meta):
4980         pointer_model = image_meta.properties.get(
4981             'hw_pointer_model', CONF.pointer_model)
4982         if pointer_model is None and CONF.libvirt.use_usb_tablet:
4983             # TODO(sahid): We set pointer_model to keep compatibility
4984             # until the next release O*. It means operators can continue
4985             # to use the deprecated option "use_usb_tablet" or set a
4986             # specific device to use
4987             pointer_model = "usbtablet"
4988             LOG.warning('The option "use_usb_tablet" has been '
4989                         'deprecated for Newton in favor of the more '
4990                         'generic "pointer_model". Please update '
4991                         'nova.conf to address this change.')
4992 
4993         if pointer_model == "usbtablet":
4994             # We want a tablet if VNC is enabled, or SPICE is enabled and
4995             # the SPICE agent is disabled. If the SPICE agent is enabled
4996             # it provides a paravirt mouse which drastically reduces
4997             # overhead (by eliminating USB polling).
4998             if CONF.vnc.enabled or (
4999                     CONF.spice.enabled and not CONF.spice.agent_enabled):
5000                 return self._get_guest_usb_tablet(os_type)
5001             else:
5002                 if CONF.pointer_model or CONF.libvirt.use_usb_tablet:
5003                     # For backward compatibility We don't want to break
5004                     # process of booting an instance if host is configured
5005                     # to use USB tablet without VNC or SPICE and SPICE
5006                     # agent disable.
5007                     LOG.warning('USB tablet requested for guests by host '
5008                                 'configuration. In order to accept this '
5009                                 'request VNC should be enabled or SPICE '
5010                                 'and SPICE agent disabled on host.')
5011                 else:
5012                     raise exception.UnsupportedPointerModelRequested(
5013                         model="usbtablet")
5014 
5015     def _get_guest_usb_tablet(self, os_type):
5016         tablet = None
5017         if os_type == fields.VMMode.HVM:
5018             tablet = vconfig.LibvirtConfigGuestInput()
5019             tablet.type = "tablet"
5020             tablet.bus = "usb"
5021         else:
5022             if CONF.pointer_model or CONF.libvirt.use_usb_tablet:
5023                 # For backward compatibility We don't want to break
5024                 # process of booting an instance if virtual machine mode
5025                 # is not configured as HVM.
5026                 LOG.warning('USB tablet requested for guests by host '
5027                             'configuration. In order to accept this '
5028                             'request the machine mode should be '
5029                             'configured as HVM.')
5030             else:
5031                 raise exception.UnsupportedPointerModelRequested(
5032                     model="usbtablet")
5033         return tablet
5034 
5035     def _get_guest_xml(self, context, instance, network_info, disk_info,
5036                        image_meta, rescue=None,
5037                        block_device_info=None):
5038         # NOTE(danms): Stringifying a NetworkInfo will take a lock. Do
5039         # this ahead of time so that we don't acquire it while also
5040         # holding the logging lock.
5041         network_info_str = str(network_info)
5042         msg = ('Start _get_guest_xml '
5043                'network_info=%(network_info)s '
5044                'disk_info=%(disk_info)s '
5045                'image_meta=%(image_meta)s rescue=%(rescue)s '
5046                'block_device_info=%(block_device_info)s' %
5047                {'network_info': network_info_str, 'disk_info': disk_info,
5048                 'image_meta': image_meta, 'rescue': rescue,
5049                 'block_device_info': block_device_info})
5050         # NOTE(mriedem): block_device_info can contain auth_password so we
5051         # need to sanitize the password in the message.
5052         LOG.debug(strutils.mask_password(msg), instance=instance)
5053         conf = self._get_guest_config(instance, network_info, image_meta,
5054                                       disk_info, rescue, block_device_info,
5055                                       context)
5056         xml = conf.to_xml()
5057 
5058         LOG.debug('End _get_guest_xml xml=%(xml)s',
5059                   {'xml': xml}, instance=instance)
5060         return xml
5061 
5062     def get_info(self, instance):
5063         """Retrieve information from libvirt for a specific instance.
5064 
5065         If a libvirt error is encountered during lookup, we might raise a
5066         NotFound exception or Error exception depending on how severe the
5067         libvirt error is.
5068 
5069         :param instance: nova.objects.instance.Instance object
5070         :returns: An InstanceInfo object
5071         """
5072         guest = self._host.get_guest(instance)
5073         # Kind of ugly but we need to pass host to get_info as for a
5074         # workaround, see libvirt/compat.py
5075         return guest.get_info(self._host)
5076 
5077     def _create_domain_setup_lxc(self, instance, image_meta,
5078                                  block_device_info):
5079         inst_path = libvirt_utils.get_instance_path(instance)
5080         block_device_mapping = driver.block_device_info_get_mapping(
5081             block_device_info)
5082         root_disk = block_device.get_root_bdm(block_device_mapping)
5083         if root_disk:
5084             disk_info = blockinfo.get_info_from_bdm(
5085                 instance, CONF.libvirt.virt_type, image_meta, root_disk)
5086             self._connect_volume(root_disk['connection_info'], disk_info,
5087                                  instance)
5088             disk_path = root_disk['connection_info']['data']['device_path']
5089 
5090             # NOTE(apmelton) - Even though the instance is being booted from a
5091             # cinder volume, it is still presented as a local block device.
5092             # LocalBlockImage is used here to indicate that the instance's
5093             # disk is backed by a local block device.
5094             image_model = imgmodel.LocalBlockImage(disk_path)
5095         else:
5096             root_disk = self.image_backend.by_name(instance, 'disk')
5097             image_model = root_disk.get_model(self._conn)
5098 
5099         container_dir = os.path.join(inst_path, 'rootfs')
5100         fileutils.ensure_tree(container_dir)
5101         rootfs_dev = disk_api.setup_container(image_model,
5102                                               container_dir=container_dir)
5103 
5104         try:
5105             # Save rootfs device to disconnect it when deleting the instance
5106             if rootfs_dev:
5107                 instance.system_metadata['rootfs_device_name'] = rootfs_dev
5108             if CONF.libvirt.uid_maps or CONF.libvirt.gid_maps:
5109                 id_maps = self._get_guest_idmaps()
5110                 libvirt_utils.chown_for_id_maps(container_dir, id_maps)
5111         except Exception:
5112             with excutils.save_and_reraise_exception():
5113                 self._create_domain_cleanup_lxc(instance)
5114 
5115     def _create_domain_cleanup_lxc(self, instance):
5116         inst_path = libvirt_utils.get_instance_path(instance)
5117         container_dir = os.path.join(inst_path, 'rootfs')
5118 
5119         try:
5120             state = self.get_info(instance).state
5121         except exception.InstanceNotFound:
5122             # The domain may not be present if the instance failed to start
5123             state = None
5124 
5125         if state == power_state.RUNNING:
5126             # NOTE(uni): Now the container is running with its own private
5127             # mount namespace and so there is no need to keep the container
5128             # rootfs mounted in the host namespace
5129             LOG.debug('Attempting to unmount container filesystem: %s',
5130                       container_dir, instance=instance)
5131             disk_api.clean_lxc_namespace(container_dir=container_dir)
5132         else:
5133             disk_api.teardown_container(container_dir=container_dir)
5134 
5135     @contextlib.contextmanager
5136     def _lxc_disk_handler(self, instance, image_meta, block_device_info):
5137         """Context manager to handle the pre and post instance boot,
5138            LXC specific disk operations.
5139 
5140            An image or a volume path will be prepared and setup to be
5141            used by the container, prior to starting it.
5142            The disk will be disconnected and unmounted if a container has
5143            failed to start.
5144         """
5145 
5146         if CONF.libvirt.virt_type != 'lxc':
5147             yield
5148             return
5149 
5150         self._create_domain_setup_lxc(instance, image_meta, block_device_info)
5151 
5152         try:
5153             yield
5154         finally:
5155             self._create_domain_cleanup_lxc(instance)
5156 
5157     # TODO(sahid): Consider renaming this to _create_guest.
5158     def _create_domain(self, xml=None, domain=None,
5159                        power_on=True, pause=False, post_xml_callback=None):
5160         """Create a domain.
5161 
5162         Either domain or xml must be passed in. If both are passed, then
5163         the domain definition is overwritten from the xml.
5164 
5165         :returns guest.Guest: Guest just created
5166         """
5167         if xml:
5168             guest = libvirt_guest.Guest.create(xml, self._host)
5169             if post_xml_callback is not None:
5170                 post_xml_callback()
5171         else:
5172             guest = libvirt_guest.Guest(domain)
5173 
5174         if power_on or pause:
5175             guest.launch(pause=pause)
5176 
5177         if not utils.is_neutron():
5178             guest.enable_hairpin()
5179 
5180         return guest
5181 
5182     def _neutron_failed_callback(self, event_name, instance):
5183         LOG.error('Neutron Reported failure on event '
5184                   '%(event)s for instance %(uuid)s',
5185                   {'event': event_name, 'uuid': instance.uuid},
5186                   instance=instance)
5187         if CONF.vif_plugging_is_fatal:
5188             raise exception.VirtualInterfaceCreateException()
5189 
5190     def _get_neutron_events(self, network_info):
5191         # NOTE(danms): We need to collect any VIFs that are currently
5192         # down that we expect a down->up event for. Anything that is
5193         # already up will not undergo that transition, and for
5194         # anything that might be stale (cache-wise) assume it's
5195         # already up so we don't block on it.
5196         return [('network-vif-plugged', vif['id'])
5197                 for vif in network_info if vif.get('active', True) is False]
5198 
5199     def _cleanup_failed_start(self, context, instance, network_info,
5200                               block_device_info, guest, destroy_disks):
5201         try:
5202             if guest and guest.is_active():
5203                 guest.poweroff()
5204         finally:
5205             self.cleanup(context, instance, network_info=network_info,
5206                          block_device_info=block_device_info,
5207                          destroy_disks=destroy_disks)
5208 
5209     def _create_domain_and_network(self, context, xml, instance, network_info,
5210                                    block_device_info=None,
5211                                    power_on=True, reboot=False,
5212                                    vifs_already_plugged=False,
5213                                    post_xml_callback=None,
5214                                    destroy_disks_on_failure=False):
5215 
5216         """Do required network setup and create domain."""
5217         block_device_mapping = driver.block_device_info_get_mapping(
5218             block_device_info)
5219 
5220         for vol in block_device_mapping:
5221             connection_info = vol['connection_info']
5222 
5223             if (not reboot and 'data' in connection_info and
5224                     'volume_id' in connection_info['data']):
5225                 volume_id = connection_info['data']['volume_id']
5226                 encryption = encryptors.get_encryption_metadata(
5227                     context, self._volume_api, volume_id, connection_info)
5228 
5229                 if encryption:
5230                     encryptor = self._get_volume_encryptor(connection_info,
5231                                                            encryption)
5232                     encryptor.attach_volume(context, **encryption)
5233 
5234         timeout = CONF.vif_plugging_timeout
5235         if (self._conn_supports_start_paused and
5236             utils.is_neutron() and not
5237             vifs_already_plugged and power_on and timeout):
5238             events = self._get_neutron_events(network_info)
5239         else:
5240             events = []
5241 
5242         pause = bool(events)
5243         guest = None
5244         try:
5245             with self.virtapi.wait_for_instance_event(
5246                     instance, events, deadline=timeout,
5247                     error_callback=self._neutron_failed_callback):
5248                 self.plug_vifs(instance, network_info)
5249                 self.firewall_driver.setup_basic_filtering(instance,
5250                                                            network_info)
5251                 self.firewall_driver.prepare_instance_filter(instance,
5252                                                              network_info)
5253                 with self._lxc_disk_handler(instance, instance.image_meta,
5254                                             block_device_info):
5255                     guest = self._create_domain(
5256                         xml, pause=pause, power_on=power_on,
5257                         post_xml_callback=post_xml_callback)
5258 
5259                 self.firewall_driver.apply_instance_filter(instance,
5260                                                            network_info)
5261         except exception.VirtualInterfaceCreateException:
5262             # Neutron reported failure and we didn't swallow it, so
5263             # bail here
5264             with excutils.save_and_reraise_exception():
5265                 self._cleanup_failed_start(context, instance, network_info,
5266                                            block_device_info, guest,
5267                                            destroy_disks_on_failure)
5268         except eventlet.timeout.Timeout:
5269             # We never heard from Neutron
5270             LOG.warning('Timeout waiting for vif plugging callback for '
5271                         'instance with vm_state %(vm_state)s and '
5272                         'task_state %(task_state)s.',
5273                         {'vm_state': instance.vm_state,
5274                          'task_state': instance.task_state},
5275                         instance=instance)
5276             if CONF.vif_plugging_is_fatal:
5277                 self._cleanup_failed_start(context, instance, network_info,
5278                                            block_device_info, guest,
5279                                            destroy_disks_on_failure)
5280                 raise exception.VirtualInterfaceCreateException()
5281         except Exception:
5282             # Any other error, be sure to clean up
5283             LOG.error('Failed to start libvirt guest', instance=instance)
5284             with excutils.save_and_reraise_exception():
5285                 self._cleanup_failed_start(context, instance, network_info,
5286                                            block_device_info, guest,
5287                                            destroy_disks_on_failure)
5288 
5289         # Resume only if domain has been paused
5290         if pause:
5291             guest.resume()
5292         return guest
5293 
5294     def _get_vcpu_total(self):
5295         """Get available vcpu number of physical computer.
5296 
5297         :returns: the number of cpu core instances can be used.
5298 
5299         """
5300         try:
5301             total_pcpus = self._host.get_cpu_count()
5302         except libvirt.libvirtError:
5303             LOG.warning("Cannot get the number of cpu, because this "
5304                         "function is not implemented for this platform. ")
5305             return 0
5306 
5307         if not CONF.vcpu_pin_set:
5308             return total_pcpus
5309 
5310         available_ids = hardware.get_vcpu_pin_set()
5311         # We get the list of online CPUs on the host and see if the requested
5312         # set falls under these. If not, we retain the old behavior.
5313         online_pcpus = None
5314         try:
5315             online_pcpus = self._host.get_online_cpus()
5316         except libvirt.libvirtError as ex:
5317             error_code = ex.get_error_code()
5318             LOG.warning(
5319                 "Couldn't retrieve the online CPUs due to a Libvirt "
5320                 "error: %(error)s with error code: %(error_code)s",
5321                 {'error': ex, 'error_code': error_code})
5322         if online_pcpus:
5323             if not (available_ids <= online_pcpus):
5324                 msg = (_("Invalid vcpu_pin_set config, one or more of the "
5325                          "specified cpuset is not online. Online cpuset(s): "
5326                          "%(online)s, requested cpuset(s): %(req)s"),
5327                        {'online': sorted(online_pcpus),
5328                         'req': sorted(available_ids)})
5329                 raise exception.Invalid(msg)
5330         elif sorted(available_ids)[-1] >= total_pcpus:
5331             raise exception.Invalid(_("Invalid vcpu_pin_set config, "
5332                                       "out of hypervisor cpu range."))
5333         return len(available_ids)
5334 
5335     @staticmethod
5336     def _get_local_gb_info():
5337         """Get local storage info of the compute node in GB.
5338 
5339         :returns: A dict containing:
5340              :total: How big the overall usable filesystem is (in gigabytes)
5341              :free: How much space is free (in gigabytes)
5342              :used: How much space is used (in gigabytes)
5343         """
5344 
5345         if CONF.libvirt.images_type == 'lvm':
5346             info = lvm.get_volume_group_info(
5347                                CONF.libvirt.images_volume_group)
5348         elif CONF.libvirt.images_type == 'rbd':
5349             info = LibvirtDriver._get_rbd_driver().get_pool_info()
5350         else:
5351             info = libvirt_utils.get_fs_info(CONF.instances_path)
5352 
5353         for (k, v) in info.items():
5354             info[k] = v / units.Gi
5355 
5356         return info
5357 
5358     def _get_vcpu_used(self):
5359         """Get vcpu usage number of physical computer.
5360 
5361         :returns: The total number of vcpu(s) that are currently being used.
5362 
5363         """
5364 
5365         total = 0
5366 
5367         # Not all libvirt drivers will support the get_vcpus_info()
5368         #
5369         # For example, LXC does not have a concept of vCPUs, while
5370         # QEMU (TCG) traditionally handles all vCPUs in a single
5371         # thread. So both will report an exception when the vcpus()
5372         # API call is made. In such a case we should report the
5373         # guest as having 1 vCPU, since that lets us still do
5374         # CPU over commit calculations that apply as the total
5375         # guest count scales.
5376         #
5377         # It is also possible that we might see an exception if
5378         # the guest is just in middle of shutting down. Technically
5379         # we should report 0 for vCPU usage in this case, but we
5380         # we can't reliably distinguish the vcpu not supported
5381         # case from the just shutting down case. Thus we don't know
5382         # whether to report 1 or 0 for vCPU count.
5383         #
5384         # Under-reporting vCPUs is bad because it could conceivably
5385         # let the scheduler place too many guests on the host. Over-
5386         # reporting vCPUs is not a problem as it'll auto-correct on
5387         # the next refresh of usage data.
5388         #
5389         # Thus when getting an exception we always report 1 as the
5390         # vCPU count, as the least worst value.
5391         for guest in self._host.list_guests():
5392             try:
5393                 vcpus = guest.get_vcpus_info()
5394                 total += len(list(vcpus))
5395             except libvirt.libvirtError:
5396                 total += 1
5397             # NOTE(gtt116): give other tasks a chance.
5398             greenthread.sleep(0)
5399         return total
5400 
5401     def _get_instance_capabilities(self):
5402         """Get hypervisor instance capabilities
5403 
5404         Returns a list of tuples that describe instances the
5405         hypervisor is capable of hosting.  Each tuple consists
5406         of the triplet (arch, hypervisor_type, vm_mode).
5407 
5408         :returns: List of tuples describing instance capabilities
5409         """
5410         caps = self._host.get_capabilities()
5411         instance_caps = list()
5412         for g in caps.guests:
5413             for dt in g.domtype:
5414                 instance_cap = (
5415                     fields.Architecture.canonicalize(g.arch),
5416                     fields.HVType.canonicalize(dt),
5417                     fields.VMMode.canonicalize(g.ostype))
5418                 instance_caps.append(instance_cap)
5419 
5420         return instance_caps
5421 
5422     def _get_cpu_info(self):
5423         """Get cpuinfo information.
5424 
5425         Obtains cpu feature from virConnect.getCapabilities.
5426 
5427         :return: see above description
5428 
5429         """
5430 
5431         caps = self._host.get_capabilities()
5432         cpu_info = dict()
5433 
5434         cpu_info['arch'] = caps.host.cpu.arch
5435         cpu_info['model'] = caps.host.cpu.model
5436         cpu_info['vendor'] = caps.host.cpu.vendor
5437 
5438         topology = dict()
5439         topology['cells'] = len(getattr(caps.host.topology, 'cells', [1]))
5440         topology['sockets'] = caps.host.cpu.sockets
5441         topology['cores'] = caps.host.cpu.cores
5442         topology['threads'] = caps.host.cpu.threads
5443         cpu_info['topology'] = topology
5444 
5445         features = set()
5446         for f in caps.host.cpu.features:
5447             features.add(f.name)
5448         cpu_info['features'] = features
5449         return cpu_info
5450 
5451     def _get_pcinet_info(self, vf_address):
5452         """Returns a dict of NET device."""
5453         devname = pci_utils.get_net_name_by_vf_pci_address(vf_address)
5454         if not devname:
5455             return
5456 
5457         virtdev = self._host.device_lookup_by_name(devname)
5458         xmlstr = virtdev.XMLDesc(0)
5459         cfgdev = vconfig.LibvirtConfigNodeDevice()
5460         cfgdev.parse_str(xmlstr)
5461         return {'name': cfgdev.name,
5462                 'capabilities': cfgdev.pci_capability.features}
5463 
5464     def _get_pcidev_info(self, devname):
5465         """Returns a dict of PCI device."""
5466 
5467         def _get_device_type(cfgdev, pci_address):
5468             """Get a PCI device's device type.
5469 
5470             An assignable PCI device can be a normal PCI device,
5471             a SR-IOV Physical Function (PF), or a SR-IOV Virtual
5472             Function (VF). Only normal PCI devices or SR-IOV VFs
5473             are assignable, while SR-IOV PFs are always owned by
5474             hypervisor.
5475             """
5476             for fun_cap in cfgdev.pci_capability.fun_capability:
5477                 if fun_cap.type == 'virt_functions':
5478                     return {
5479                         'dev_type': fields.PciDeviceType.SRIOV_PF,
5480                     }
5481                 if (fun_cap.type == 'phys_function' and
5482                     len(fun_cap.device_addrs) != 0):
5483                     phys_address = "%04x:%02x:%02x.%01x" % (
5484                         fun_cap.device_addrs[0][0],
5485                         fun_cap.device_addrs[0][1],
5486                         fun_cap.device_addrs[0][2],
5487                         fun_cap.device_addrs[0][3])
5488                     return {
5489                         'dev_type': fields.PciDeviceType.SRIOV_VF,
5490                         'parent_addr': phys_address,
5491                     }
5492 
5493             # Note(moshele): libvirt < 1.3 reported virt_functions capability
5494             # only when VFs are enabled. The check below is a workaround
5495             # to get the correct report regardless of whether or not any
5496             # VFs are enabled for the device.
5497             if not self._host.has_min_version(
5498                 MIN_LIBVIRT_PF_WITH_NO_VFS_CAP_VERSION):
5499                 is_physical_function = pci_utils.is_physical_function(
5500                     *pci_utils.get_pci_address_fields(pci_address))
5501                 if is_physical_function:
5502                     return {'dev_type': fields.PciDeviceType.SRIOV_PF}
5503 
5504             return {'dev_type': fields.PciDeviceType.STANDARD}
5505 
5506         def _get_device_capabilities(device, address):
5507             """Get PCI VF device's additional capabilities.
5508 
5509             If a PCI device is a virtual function, this function reads the PCI
5510             parent's network capabilities (must be always a NIC device) and
5511             appends this information to the device's dictionary.
5512             """
5513             if device.get('dev_type') == fields.PciDeviceType.SRIOV_VF:
5514                 pcinet_info = self._get_pcinet_info(address)
5515                 if pcinet_info:
5516                     return {'capabilities':
5517                                 {'network': pcinet_info.get('capabilities')}}
5518             return {}
5519 
5520         virtdev = self._host.device_lookup_by_name(devname)
5521         xmlstr = virtdev.XMLDesc(0)
5522         cfgdev = vconfig.LibvirtConfigNodeDevice()
5523         cfgdev.parse_str(xmlstr)
5524 
5525         address = "%04x:%02x:%02x.%1x" % (
5526             cfgdev.pci_capability.domain,
5527             cfgdev.pci_capability.bus,
5528             cfgdev.pci_capability.slot,
5529             cfgdev.pci_capability.function)
5530 
5531         device = {
5532             "dev_id": cfgdev.name,
5533             "address": address,
5534             "product_id": "%04x" % cfgdev.pci_capability.product_id,
5535             "vendor_id": "%04x" % cfgdev.pci_capability.vendor_id,
5536             }
5537 
5538         device["numa_node"] = cfgdev.pci_capability.numa_node
5539 
5540         # requirement by DataBase Model
5541         device['label'] = 'label_%(vendor_id)s_%(product_id)s' % device
5542         device.update(_get_device_type(cfgdev, address))
5543         device.update(_get_device_capabilities(device, address))
5544         return device
5545 
5546     def _get_pci_passthrough_devices(self):
5547         """Get host PCI devices information.
5548 
5549         Obtains pci devices information from libvirt, and returns
5550         as a JSON string.
5551 
5552         Each device information is a dictionary, with mandatory keys
5553         of 'address', 'vendor_id', 'product_id', 'dev_type', 'dev_id',
5554         'label' and other optional device specific information.
5555 
5556         Refer to the objects/pci_device.py for more idea of these keys.
5557 
5558         :returns: a JSON string containing a list of the assignable PCI
5559                   devices information
5560         """
5561         # Bail early if we know we can't support `listDevices` to avoid
5562         # repeated warnings within a periodic task
5563         if not getattr(self, '_list_devices_supported', True):
5564             return jsonutils.dumps([])
5565 
5566         try:
5567             dev_names = self._host.list_pci_devices() or []
5568         except libvirt.libvirtError as ex:
5569             error_code = ex.get_error_code()
5570             if error_code == libvirt.VIR_ERR_NO_SUPPORT:
5571                 self._list_devices_supported = False
5572                 LOG.warning("URI %(uri)s does not support "
5573                             "listDevices: %(error)s",
5574                             {'uri': self._uri(), 'error': ex})
5575                 return jsonutils.dumps([])
5576             else:
5577                 raise
5578 
5579         pci_info = []
5580         for name in dev_names:
5581             pci_info.append(self._get_pcidev_info(name))
5582 
5583         return jsonutils.dumps(pci_info)
5584 
5585     def _has_numa_support(self):
5586         # This means that the host can support LibvirtConfigGuestNUMATune
5587         # and the nodeset field in LibvirtConfigGuestMemoryBackingPage
5588         for ver in BAD_LIBVIRT_NUMA_VERSIONS:
5589             if self._host.has_version(ver):
5590                 if not getattr(self, '_bad_libvirt_numa_version_warn', False):
5591                     LOG.warning('You are running with libvirt version %s '
5592                                 'which is known to have broken NUMA support. '
5593                                 'Consider patching or updating libvirt on '
5594                                 'this host if you need NUMA support.',
5595                                 self._version_to_string(ver))
5596                     self._bad_libvirt_numa_version_warn = True
5597                 return False
5598 
5599         caps = self._host.get_capabilities()
5600 
5601         if (caps.host.cpu.arch in (fields.Architecture.I686,
5602                                    fields.Architecture.X86_64,
5603                                    fields.Architecture.AARCH64) and
5604                 self._host.has_min_version(hv_type=host.HV_DRIVER_QEMU)):
5605             return True
5606         elif (caps.host.cpu.arch in (fields.Architecture.PPC64,
5607                                      fields.Architecture.PPC64LE) and
5608                 self._host.has_min_version(MIN_LIBVIRT_NUMA_VERSION_PPC,
5609                                            hv_type=host.HV_DRIVER_QEMU)):
5610             return True
5611 
5612         return False
5613 
5614     def _get_host_numa_topology(self):
5615         if not self._has_numa_support():
5616             return
5617 
5618         caps = self._host.get_capabilities()
5619         topology = caps.host.topology
5620 
5621         if topology is None or not topology.cells:
5622             return
5623 
5624         cells = []
5625         allowed_cpus = hardware.get_vcpu_pin_set()
5626         online_cpus = self._host.get_online_cpus()
5627         if allowed_cpus:
5628             allowed_cpus &= online_cpus
5629         else:
5630             allowed_cpus = online_cpus
5631 
5632         def _get_reserved_memory_for_cell(self, cell_id, page_size):
5633             cell = self._reserved_hugepages.get(cell_id, {})
5634             return cell.get(page_size, 0)
5635 
5636         for cell in topology.cells:
5637             cpuset = set(cpu.id for cpu in cell.cpus)
5638             siblings = sorted(map(set,
5639                                   set(tuple(cpu.siblings)
5640                                         if cpu.siblings else ()
5641                                       for cpu in cell.cpus)
5642                                   ))
5643             cpuset &= allowed_cpus
5644             siblings = [sib & allowed_cpus for sib in siblings]
5645             # Filter out singles and empty sibling sets that may be left
5646             siblings = [sib for sib in siblings if len(sib) > 1]
5647 
5648             mempages = [
5649                 objects.NUMAPagesTopology(
5650                     size_kb=pages.size,
5651                     total=pages.total,
5652                     used=0,
5653                     reserved=_get_reserved_memory_for_cell(
5654                         self, cell.id, pages.size))
5655                 for pages in cell.mempages]
5656 
5657             cell = objects.NUMACell(id=cell.id, cpuset=cpuset,
5658                                     memory=cell.memory / units.Ki,
5659                                     cpu_usage=0, memory_usage=0,
5660                                     siblings=siblings,
5661                                     pinned_cpus=set([]),
5662                                     mempages=mempages)
5663             cells.append(cell)
5664 
5665         return objects.NUMATopology(cells=cells)
5666 
5667     def get_all_volume_usage(self, context, compute_host_bdms):
5668         """Return usage info for volumes attached to vms on
5669            a given host.
5670         """
5671         vol_usage = []
5672 
5673         for instance_bdms in compute_host_bdms:
5674             instance = instance_bdms['instance']
5675 
5676             for bdm in instance_bdms['instance_bdms']:
5677                 mountpoint = bdm['device_name']
5678                 if mountpoint.startswith('/dev/'):
5679                     mountpoint = mountpoint[5:]
5680                 volume_id = bdm['volume_id']
5681 
5682                 LOG.debug("Trying to get stats for the volume %s",
5683                           volume_id, instance=instance)
5684                 vol_stats = self.block_stats(instance, mountpoint)
5685 
5686                 if vol_stats:
5687                     stats = dict(volume=volume_id,
5688                                  instance=instance,
5689                                  rd_req=vol_stats[0],
5690                                  rd_bytes=vol_stats[1],
5691                                  wr_req=vol_stats[2],
5692                                  wr_bytes=vol_stats[3])
5693                     LOG.debug(
5694                         "Got volume usage stats for the volume=%(volume)s,"
5695                         " rd_req=%(rd_req)d, rd_bytes=%(rd_bytes)d, "
5696                         "wr_req=%(wr_req)d, wr_bytes=%(wr_bytes)d",
5697                         stats, instance=instance)
5698                     vol_usage.append(stats)
5699 
5700         return vol_usage
5701 
5702     def block_stats(self, instance, disk_id):
5703         """Note that this function takes an instance name."""
5704         try:
5705             guest = self._host.get_guest(instance)
5706 
5707             # TODO(sahid): We are converting all calls from a
5708             # virDomain object to use nova.virt.libvirt.Guest.
5709             # We should be able to remove domain at the end.
5710             domain = guest._domain
5711             return domain.blockStats(disk_id)
5712         except libvirt.libvirtError as e:
5713             errcode = e.get_error_code()
5714             LOG.info('Getting block stats failed, device might have '
5715                      'been detached. Instance=%(instance_name)s '
5716                      'Disk=%(disk)s Code=%(errcode)s Error=%(e)s',
5717                      {'instance_name': instance.name, 'disk': disk_id,
5718                       'errcode': errcode, 'e': e},
5719                      instance=instance)
5720         except exception.InstanceNotFound:
5721             LOG.info('Could not find domain in libvirt for instance %s. '
5722                      'Cannot get block stats for device', instance.name,
5723                      instance=instance)
5724 
5725     def get_console_pool_info(self, console_type):
5726         # TODO(mdragon): console proxy should be implemented for libvirt,
5727         #                in case someone wants to use it with kvm or
5728         #                such. For now return fake data.
5729         return {'address': '127.0.0.1',
5730                 'username': 'fakeuser',
5731                 'password': 'fakepassword'}
5732 
5733     def refresh_security_group_rules(self, security_group_id):
5734         self.firewall_driver.refresh_security_group_rules(security_group_id)
5735 
5736     def refresh_instance_security_rules(self, instance):
5737         self.firewall_driver.refresh_instance_security_rules(instance)
5738 
5739     def get_inventory(self, nodename):
5740         """Return a dict, keyed by resource class, of inventory information for
5741         the supplied node.
5742         """
5743         disk_gb = int(self._get_local_gb_info()['total'])
5744         memory_mb = int(self._host.get_memory_mb_total())
5745         vcpus = self._get_vcpu_total()
5746         # NOTE(jaypipes): We leave some fields like allocation_ratio and
5747         # reserved out of the returned dicts here because, for now at least,
5748         # the RT injects those values into the inventory dict based on the
5749         # compute_nodes record values.
5750         result = {
5751             fields.ResourceClass.VCPU: {
5752                 'total': vcpus,
5753                 'min_unit': 1,
5754                 'max_unit': vcpus,
5755                 'step_size': 1,
5756             },
5757             fields.ResourceClass.MEMORY_MB: {
5758                 'total': memory_mb,
5759                 'min_unit': 1,
5760                 'max_unit': memory_mb,
5761                 'step_size': 1,
5762             },
5763             fields.ResourceClass.DISK_GB: {
5764                 'total': disk_gb,
5765                 'min_unit': 1,
5766                 'max_unit': disk_gb,
5767                 'step_size': 1,
5768             },
5769         }
5770         return result
5771 
5772     def get_available_resource(self, nodename):
5773         """Retrieve resource information.
5774 
5775         This method is called when nova-compute launches, and
5776         as part of a periodic task that records the results in the DB.
5777 
5778         :param nodename: unused in this driver
5779         :returns: dictionary containing resource info
5780         """
5781 
5782         disk_info_dict = self._get_local_gb_info()
5783         data = {}
5784 
5785         # NOTE(dprince): calling capabilities before getVersion works around
5786         # an initialization issue with some versions of Libvirt (1.0.5.5).
5787         # See: https://bugzilla.redhat.com/show_bug.cgi?id=1000116
5788         # See: https://bugs.launchpad.net/nova/+bug/1215593
5789         data["supported_instances"] = self._get_instance_capabilities()
5790 
5791         data["vcpus"] = self._get_vcpu_total()
5792         data["memory_mb"] = self._host.get_memory_mb_total()
5793         data["local_gb"] = disk_info_dict['total']
5794         data["vcpus_used"] = self._get_vcpu_used()
5795         data["memory_mb_used"] = self._host.get_memory_mb_used()
5796         data["local_gb_used"] = disk_info_dict['used']
5797         data["hypervisor_type"] = self._host.get_driver_type()
5798         data["hypervisor_version"] = self._host.get_version()
5799         data["hypervisor_hostname"] = self._host.get_hostname()
5800         # TODO(berrange): why do we bother converting the
5801         # libvirt capabilities XML into a special JSON format ?
5802         # The data format is different across all the drivers
5803         # so we could just return the raw capabilities XML
5804         # which 'compare_cpu' could use directly
5805         #
5806         # That said, arch_filter.py now seems to rely on
5807         # the libvirt drivers format which suggests this
5808         # data format needs to be standardized across drivers
5809         data["cpu_info"] = jsonutils.dumps(self._get_cpu_info())
5810 
5811         disk_free_gb = disk_info_dict['free']
5812         disk_over_committed = self._get_disk_over_committed_size_total()
5813         available_least = disk_free_gb * units.Gi - disk_over_committed
5814         data['disk_available_least'] = available_least / units.Gi
5815 
5816         data['pci_passthrough_devices'] = \
5817             self._get_pci_passthrough_devices()
5818 
5819         numa_topology = self._get_host_numa_topology()
5820         if numa_topology:
5821             data['numa_topology'] = numa_topology._to_json()
5822         else:
5823             data['numa_topology'] = None
5824 
5825         return data
5826 
5827     def check_instance_shared_storage_local(self, context, instance):
5828         """Check if instance files located on shared storage.
5829 
5830         This runs check on the destination host, and then calls
5831         back to the source host to check the results.
5832 
5833         :param context: security context
5834         :param instance: nova.objects.instance.Instance object
5835         :returns:
5836          - tempfile: A dict containing the tempfile info on the destination
5837                      host
5838          - None:
5839 
5840             1. If the instance path is not existing.
5841             2. If the image backend is shared block storage type.
5842         """
5843         if self.image_backend.backend().is_shared_block_storage():
5844             return None
5845 
5846         dirpath = libvirt_utils.get_instance_path(instance)
5847 
5848         if not os.path.exists(dirpath):
5849             return None
5850 
5851         fd, tmp_file = tempfile.mkstemp(dir=dirpath)
5852         LOG.debug("Creating tmpfile %s to verify with other "
5853                   "compute node that the instance is on "
5854                   "the same shared storage.",
5855                   tmp_file, instance=instance)
5856         os.close(fd)
5857         return {"filename": tmp_file}
5858 
5859     def check_instance_shared_storage_remote(self, context, data):
5860         return os.path.exists(data['filename'])
5861 
5862     def check_instance_shared_storage_cleanup(self, context, data):
5863         fileutils.delete_if_exists(data["filename"])
5864 
5865     def check_can_live_migrate_destination(self, context, instance,
5866                                            src_compute_info, dst_compute_info,
5867                                            block_migration=False,
5868                                            disk_over_commit=False):
5869         """Check if it is possible to execute live migration.
5870 
5871         This runs checks on the destination host, and then calls
5872         back to the source host to check the results.
5873 
5874         :param context: security context
5875         :param instance: nova.db.sqlalchemy.models.Instance
5876         :param block_migration: if true, prepare for block migration
5877         :param disk_over_commit: if true, allow disk over commit
5878         :returns: a LibvirtLiveMigrateData object
5879         """
5880         disk_available_gb = dst_compute_info['disk_available_least']
5881         disk_available_mb = (
5882             (disk_available_gb * units.Ki) - CONF.reserved_host_disk_mb)
5883 
5884         # Compare CPU
5885         if not instance.vcpu_model or not instance.vcpu_model.model:
5886             source_cpu_info = src_compute_info['cpu_info']
5887             self._compare_cpu(None, source_cpu_info, instance)
5888         else:
5889             self._compare_cpu(instance.vcpu_model, None, instance)
5890 
5891         # Create file on storage, to be checked on source host
5892         filename = self._create_shared_storage_test_file(instance)
5893 
5894         data = objects.LibvirtLiveMigrateData()
5895         data.filename = filename
5896         data.image_type = CONF.libvirt.images_type
5897         data.graphics_listen_addr_vnc = CONF.vnc.vncserver_listen
5898         data.graphics_listen_addr_spice = CONF.spice.server_listen
5899         if CONF.serial_console.enabled:
5900             data.serial_listen_addr = CONF.serial_console.proxyclient_address
5901         else:
5902             data.serial_listen_addr = None
5903         # Notes(eliqiao): block_migration and disk_over_commit are not
5904         # nullable, so just don't set them if they are None
5905         if block_migration is not None:
5906             data.block_migration = block_migration
5907         if disk_over_commit is not None:
5908             data.disk_over_commit = disk_over_commit
5909         data.disk_available_mb = disk_available_mb
5910         return data
5911 
5912     def cleanup_live_migration_destination_check(self, context,
5913                                                  dest_check_data):
5914         """Do required cleanup on dest host after check_can_live_migrate calls
5915 
5916         :param context: security context
5917         """
5918         filename = dest_check_data.filename
5919         self._cleanup_shared_storage_test_file(filename)
5920 
5921     def check_can_live_migrate_source(self, context, instance,
5922                                       dest_check_data,
5923                                       block_device_info=None):
5924         """Check if it is possible to execute live migration.
5925 
5926         This checks if the live migration can succeed, based on the
5927         results from check_can_live_migrate_destination.
5928 
5929         :param context: security context
5930         :param instance: nova.db.sqlalchemy.models.Instance
5931         :param dest_check_data: result of check_can_live_migrate_destination
5932         :param block_device_info: result of _get_instance_block_device_info
5933         :returns: a LibvirtLiveMigrateData object
5934         """
5935         if not isinstance(dest_check_data, migrate_data_obj.LiveMigrateData):
5936             md_obj = objects.LibvirtLiveMigrateData()
5937             md_obj.from_legacy_dict(dest_check_data)
5938             dest_check_data = md_obj
5939 
5940         # Checking shared storage connectivity
5941         # if block migration, instances_path should not be on shared storage.
5942         source = CONF.host
5943 
5944         dest_check_data.is_shared_instance_path = (
5945             self._check_shared_storage_test_file(
5946                 dest_check_data.filename, instance))
5947 
5948         dest_check_data.is_shared_block_storage = (
5949             self._is_shared_block_storage(instance, dest_check_data,
5950                                           block_device_info))
5951 
5952         if 'block_migration' not in dest_check_data:
5953             dest_check_data.block_migration = (
5954                 not dest_check_data.is_on_shared_storage())
5955 
5956         if dest_check_data.block_migration:
5957             # TODO(eliqiao): Once block_migration flag is removed from the API
5958             # we can safely remove the if condition
5959             if dest_check_data.is_on_shared_storage():
5960                 reason = _("Block migration can not be used "
5961                            "with shared storage.")
5962                 raise exception.InvalidLocalStorage(reason=reason, path=source)
5963             if 'disk_over_commit' in dest_check_data:
5964                 self._assert_dest_node_has_enough_disk(context, instance,
5965                                         dest_check_data.disk_available_mb,
5966                                         dest_check_data.disk_over_commit,
5967                                         block_device_info)
5968             if block_device_info:
5969                 bdm = block_device_info.get('block_device_mapping')
5970                 # NOTE(pkoniszewski): libvirt from version 1.2.17 upwards
5971                 # supports selective block device migration. It means that it
5972                 # is possible to define subset of block devices to be copied
5973                 # during migration. If they are not specified - block devices
5974                 # won't be migrated. However, it does not work when live
5975                 # migration is tunnelled through libvirt.
5976                 if bdm and not self._host.has_min_version(
5977                         MIN_LIBVIRT_BLOCK_LM_WITH_VOLUMES_VERSION):
5978                     # NOTE(stpierre): if this instance has mapped volumes,
5979                     # we can't do a block migration, since that will result
5980                     # in volumes being copied from themselves to themselves,
5981                     # which is a recipe for disaster.
5982                     ver = ".".join([str(x) for x in
5983                                     MIN_LIBVIRT_BLOCK_LM_WITH_VOLUMES_VERSION])
5984                     msg = (_('Cannot block migrate instance %(uuid)s with'
5985                              ' mapped volumes. Selective block device'
5986                              ' migration feature requires libvirt version'
5987                              ' %(libvirt_ver)s') %
5988                            {'uuid': instance.uuid, 'libvirt_ver': ver})
5989                     LOG.error(msg, instance=instance)
5990                     raise exception.MigrationPreCheckError(reason=msg)
5991                 # NOTE(eliqiao): Selective disk migrations are not supported
5992                 # with tunnelled block migrations so we can block them early.
5993                 if (bdm and
5994                     (self._block_migration_flags &
5995                      libvirt.VIR_MIGRATE_TUNNELLED != 0)):
5996                     msg = (_('Cannot block migrate instance %(uuid)s with'
5997                              ' mapped volumes. Selective block device'
5998                              ' migration is not supported with tunnelled'
5999                              ' block migrations.') % {'uuid': instance.uuid})
6000                     LOG.error(msg, instance=instance)
6001                     raise exception.MigrationPreCheckError(reason=msg)
6002         elif not (dest_check_data.is_shared_block_storage or
6003                   dest_check_data.is_shared_instance_path):
6004             reason = _("Shared storage live-migration requires either shared "
6005                        "storage or boot-from-volume with no local disks.")
6006             raise exception.InvalidSharedStorage(reason=reason, path=source)
6007 
6008         # NOTE(mikal): include the instance directory name here because it
6009         # doesn't yet exist on the destination but we want to force that
6010         # same name to be used
6011         instance_path = libvirt_utils.get_instance_path(instance,
6012                                                         relative=True)
6013         dest_check_data.instance_relative_path = instance_path
6014 
6015         return dest_check_data
6016 
6017     def _is_shared_block_storage(self, instance, dest_check_data,
6018                                  block_device_info=None):
6019         """Check if all block storage of an instance can be shared
6020         between source and destination of a live migration.
6021 
6022         Returns true if the instance is volume backed and has no local disks,
6023         or if the image backend is the same on source and destination and the
6024         backend shares block storage between compute nodes.
6025 
6026         :param instance: nova.objects.instance.Instance object
6027         :param dest_check_data: dict with boolean fields image_type,
6028                                 is_shared_instance_path, and is_volume_backed
6029         """
6030         if (dest_check_data.obj_attr_is_set('image_type') and
6031                 CONF.libvirt.images_type == dest_check_data.image_type and
6032                 self.image_backend.backend().is_shared_block_storage()):
6033             # NOTE(dgenin): currently true only for RBD image backend
6034             return True
6035 
6036         if (dest_check_data.is_shared_instance_path and
6037                 self.image_backend.backend().is_file_in_instance_path()):
6038             # NOTE(angdraug): file based image backends (Flat, Qcow2)
6039             # place block device files under the instance path
6040             return True
6041 
6042         if (dest_check_data.is_volume_backed and
6043                 not bool(self._get_instance_disk_info(instance,
6044                                                       block_device_info))):
6045             return True
6046 
6047         return False
6048 
6049     def _assert_dest_node_has_enough_disk(self, context, instance,
6050                                              available_mb, disk_over_commit,
6051                                              block_device_info):
6052         """Checks if destination has enough disk for block migration."""
6053         # Libvirt supports qcow2 disk format,which is usually compressed
6054         # on compute nodes.
6055         # Real disk image (compressed) may enlarged to "virtual disk size",
6056         # that is specified as the maximum disk size.
6057         # (See qemu-img -f path-to-disk)
6058         # Scheduler recognizes destination host still has enough disk space
6059         # if real disk size < available disk size
6060         # if disk_over_commit is True,
6061         #  otherwise virtual disk size < available disk size.
6062 
6063         available = 0
6064         if available_mb:
6065             available = available_mb * units.Mi
6066 
6067         disk_infos = self._get_instance_disk_info(instance, block_device_info)
6068 
6069         necessary = 0
6070         if disk_over_commit:
6071             for info in disk_infos:
6072                 necessary += int(info['disk_size'])
6073         else:
6074             for info in disk_infos:
6075                 necessary += int(info['virt_disk_size'])
6076 
6077         # Check that available disk > necessary disk
6078         if (available - necessary) < 0:
6079             reason = (_('Unable to migrate %(instance_uuid)s: '
6080                         'Disk of instance is too large(available'
6081                         ' on destination host:%(available)s '
6082                         '< need:%(necessary)s)') %
6083                       {'instance_uuid': instance.uuid,
6084                        'available': available,
6085                        'necessary': necessary})
6086             raise exception.MigrationPreCheckError(reason=reason)
6087 
6088     def _compare_cpu(self, guest_cpu, host_cpu_str, instance):
6089         """Check the host is compatible with the requested CPU
6090 
6091         :param guest_cpu: nova.objects.VirtCPUModel or None
6092         :param host_cpu_str: JSON from _get_cpu_info() method
6093 
6094         If the 'guest_cpu' parameter is not None, this will be
6095         validated for migration compatibility with the host.
6096         Otherwise the 'host_cpu_str' JSON string will be used for
6097         validation.
6098 
6099         :returns:
6100             None. if given cpu info is not compatible to this server,
6101             raise exception.
6102         """
6103 
6104         # NOTE(kchamart): Comparing host to guest CPU model for emulated
6105         # guests (<domain type='qemu'>) should not matter -- in this
6106         # mode (QEMU "TCG") the CPU is fully emulated in software and no
6107         # hardware acceleration, like KVM, is involved. So, skip the CPU
6108         # compatibility check for the QEMU domain type, and retain it for
6109         # KVM guests.
6110         if CONF.libvirt.virt_type not in ['kvm']:
6111             return
6112 
6113         if guest_cpu is None:
6114             info = jsonutils.loads(host_cpu_str)
6115             LOG.info('Instance launched has CPU info: %s', host_cpu_str)
6116             cpu = vconfig.LibvirtConfigCPU()
6117             cpu.arch = info['arch']
6118             cpu.model = info['model']
6119             cpu.vendor = info['vendor']
6120             cpu.sockets = info['topology']['sockets']
6121             cpu.cores = info['topology']['cores']
6122             cpu.threads = info['topology']['threads']
6123             for f in info['features']:
6124                 cpu.add_feature(vconfig.LibvirtConfigCPUFeature(f))
6125         else:
6126             cpu = self._vcpu_model_to_cpu_config(guest_cpu)
6127 
6128         u = ("http://libvirt.org/html/libvirt-libvirt-host.html#"
6129              "virCPUCompareResult")
6130         m = _("CPU doesn't have compatibility.\n\n%(ret)s\n\nRefer to %(u)s")
6131         # unknown character exists in xml, then libvirt complains
6132         try:
6133             cpu_xml = cpu.to_xml()
6134             LOG.debug("cpu compare xml: %s", cpu_xml, instance=instance)
6135             ret = self._host.compare_cpu(cpu_xml)
6136         except libvirt.libvirtError as e:
6137             error_code = e.get_error_code()
6138             if error_code == libvirt.VIR_ERR_NO_SUPPORT:
6139                 LOG.debug("URI %(uri)s does not support cpu comparison. "
6140                           "It will be proceeded though. Error: %(error)s",
6141                           {'uri': self._uri(), 'error': e})
6142                 return
6143             else:
6144                 LOG.error(m, {'ret': e, 'u': u})
6145                 raise exception.MigrationPreCheckError(
6146                     reason=m % {'ret': e, 'u': u})
6147 
6148         if ret <= 0:
6149             LOG.error(m, {'ret': ret, 'u': u})
6150             raise exception.InvalidCPUInfo(reason=m % {'ret': ret, 'u': u})
6151 
6152     def _create_shared_storage_test_file(self, instance):
6153         """Makes tmpfile under CONF.instances_path."""
6154         dirpath = CONF.instances_path
6155         fd, tmp_file = tempfile.mkstemp(dir=dirpath)
6156         LOG.debug("Creating tmpfile %s to notify to other "
6157                   "compute nodes that they should mount "
6158                   "the same storage.", tmp_file, instance=instance)
6159         os.close(fd)
6160         return os.path.basename(tmp_file)
6161 
6162     def _check_shared_storage_test_file(self, filename, instance):
6163         """Confirms existence of the tmpfile under CONF.instances_path.
6164 
6165         Cannot confirm tmpfile return False.
6166         """
6167         # NOTE(tpatzig): if instances_path is a shared volume that is
6168         # under heavy IO (many instances on many compute nodes),
6169         # then checking the existence of the testfile fails,
6170         # just because it takes longer until the client refreshes and new
6171         # content gets visible.
6172         # os.utime (like touch) on the directory forces the client to refresh.
6173         os.utime(CONF.instances_path, None)
6174 
6175         tmp_file = os.path.join(CONF.instances_path, filename)
6176         if not os.path.exists(tmp_file):
6177             exists = False
6178         else:
6179             exists = True
6180         LOG.debug('Check if temp file %s exists to indicate shared storage '
6181                   'is being used for migration. Exists? %s', tmp_file, exists,
6182                   instance=instance)
6183         return exists
6184 
6185     def _cleanup_shared_storage_test_file(self, filename):
6186         """Removes existence of the tmpfile under CONF.instances_path."""
6187         tmp_file = os.path.join(CONF.instances_path, filename)
6188         os.remove(tmp_file)
6189 
6190     def ensure_filtering_rules_for_instance(self, instance, network_info):
6191         """Ensure that an instance's filtering rules are enabled.
6192 
6193         When migrating an instance, we need the filtering rules to
6194         be configured on the destination host before starting the
6195         migration.
6196 
6197         Also, when restarting the compute service, we need to ensure
6198         that filtering rules exist for all running services.
6199         """
6200 
6201         self.firewall_driver.setup_basic_filtering(instance, network_info)
6202         self.firewall_driver.prepare_instance_filter(instance,
6203                 network_info)
6204 
6205         # nwfilters may be defined in a separate thread in the case
6206         # of libvirt non-blocking mode, so we wait for completion
6207         timeout_count = list(range(CONF.live_migration_retry_count))
6208         while timeout_count:
6209             if self.firewall_driver.instance_filter_exists(instance,
6210                                                            network_info):
6211                 break
6212             timeout_count.pop()
6213             if len(timeout_count) == 0:
6214                 msg = _('The firewall filter for %s does not exist')
6215                 raise exception.InternalError(msg % instance.name)
6216             greenthread.sleep(1)
6217 
6218     def filter_defer_apply_on(self):
6219         self.firewall_driver.filter_defer_apply_on()
6220 
6221     def filter_defer_apply_off(self):
6222         self.firewall_driver.filter_defer_apply_off()
6223 
6224     def live_migration(self, context, instance, dest,
6225                        post_method, recover_method, block_migration=False,
6226                        migrate_data=None):
6227         """Spawning live_migration operation for distributing high-load.
6228 
6229         :param context: security context
6230         :param instance:
6231             nova.db.sqlalchemy.models.Instance object
6232             instance object that is migrated.
6233         :param dest: destination host
6234         :param post_method:
6235             post operation method.
6236             expected nova.compute.manager._post_live_migration.
6237         :param recover_method:
6238             recovery method when any exception occurs.
6239             expected nova.compute.manager._rollback_live_migration.
6240         :param block_migration: if true, do block migration.
6241         :param migrate_data: a LibvirtLiveMigrateData object
6242 
6243         """
6244 
6245         # 'dest' will be substituted into 'migration_uri' so ensure
6246         # it does't contain any characters that could be used to
6247         # exploit the URI accepted by libivrt
6248         if not libvirt_utils.is_valid_hostname(dest):
6249             raise exception.InvalidHostname(hostname=dest)
6250 
6251         self._live_migration(context, instance, dest,
6252                              post_method, recover_method, block_migration,
6253                              migrate_data)
6254 
6255     def live_migration_abort(self, instance):
6256         """Aborting a running live-migration.
6257 
6258         :param instance: instance object that is in migration
6259 
6260         """
6261 
6262         guest = self._host.get_guest(instance)
6263         dom = guest._domain
6264 
6265         try:
6266             dom.abortJob()
6267         except libvirt.libvirtError as e:
6268             LOG.error("Failed to cancel migration %s", e, instance=instance)
6269             raise
6270 
6271     def _verify_serial_console_is_disabled(self):
6272         if CONF.serial_console.enabled:
6273 
6274             msg = _('Your destination node does not support'
6275                     ' retrieving listen addresses.  In order'
6276                     ' for live migration to work properly you'
6277                     ' must disable serial console.')
6278             raise exception.MigrationError(reason=msg)
6279 
6280     def _live_migration_operation(self, context, instance, dest,
6281                                   block_migration, migrate_data, guest,
6282                                   device_names):
6283         """Invoke the live migration operation
6284 
6285         :param context: security context
6286         :param instance:
6287             nova.db.sqlalchemy.models.Instance object
6288             instance object that is migrated.
6289         :param dest: destination host
6290         :param block_migration: if true, do block migration.
6291         :param migrate_data: a LibvirtLiveMigrateData object
6292         :param guest: the guest domain object
6293         :param device_names: list of device names that are being migrated with
6294             instance
6295 
6296         This method is intended to be run in a background thread and will
6297         block that thread until the migration is finished or failed.
6298         """
6299         try:
6300             if migrate_data.block_migration:
6301                 migration_flags = self._block_migration_flags
6302             else:
6303                 migration_flags = self._live_migration_flags
6304 
6305             serial_listen_addr = libvirt_migrate.serial_listen_addr(
6306                 migrate_data)
6307             if not serial_listen_addr:
6308                 # In this context we want to ensure that serial console is
6309                 # disabled on source node. This is because nova couldn't
6310                 # retrieve serial listen address from destination node, so we
6311                 # consider that destination node might have serial console
6312                 # disabled as well.
6313                 self._verify_serial_console_is_disabled()
6314 
6315             # NOTE(aplanas) migrate_uri will have a value only in the
6316             # case that `live_migration_inbound_addr` parameter is
6317             # set, and we propose a non tunneled migration.
6318             migrate_uri = None
6319             if ('target_connect_addr' in migrate_data and
6320                     migrate_data.target_connect_addr is not None):
6321                 dest = migrate_data.target_connect_addr
6322                 if (migration_flags &
6323                     libvirt.VIR_MIGRATE_TUNNELLED == 0):
6324                     migrate_uri = self._migrate_uri(dest)
6325 
6326             params = None
6327             new_xml_str = None
6328             if CONF.libvirt.virt_type != "parallels":
6329                 new_xml_str = libvirt_migrate.get_updated_guest_xml(
6330                     # TODO(sahid): It's not a really good idea to pass
6331                     # the method _get_volume_config and we should to find
6332                     # a way to avoid this in future.
6333                     guest, migrate_data, self._get_volume_config)
6334             if self._host.has_min_version(
6335                     MIN_LIBVIRT_BLOCK_LM_WITH_VOLUMES_VERSION):
6336                 params = {
6337                     'bandwidth': CONF.libvirt.live_migration_bandwidth,
6338                     'destination_xml': new_xml_str,
6339                     'migrate_disks': device_names,
6340                 }
6341                 # NOTE(pkoniszewski): Because of precheck which blocks
6342                 # tunnelled block live migration with mapped volumes we
6343                 # can safely remove migrate_disks when tunnelling is on.
6344                 # Otherwise we will block all tunnelled block migrations,
6345                 # even when an instance does not have volumes mapped.
6346                 # This is because selective disk migration is not
6347                 # supported in tunnelled block live migration. Also we
6348                 # cannot fallback to migrateToURI2 in this case because of
6349                 # bug #1398999
6350                 if (migration_flags &
6351                     libvirt.VIR_MIGRATE_TUNNELLED != 0):
6352                     params.pop('migrate_disks')
6353 
6354             # TODO(sahid): This should be in
6355             # post_live_migration_at_source but no way to retrieve
6356             # ports acquired on the host for the guest at this
6357             # step. Since the domain is going to be removed from
6358             # libvird on source host after migration, we backup the
6359             # serial ports to release them if all went well.
6360             serial_ports = []
6361             if CONF.serial_console.enabled:
6362                 serial_ports = list(self._get_serial_ports_from_guest(guest))
6363 
6364             guest.migrate(self._live_migration_uri(dest),
6365                           migrate_uri=migrate_uri,
6366                           flags=migration_flags,
6367                           params=params,
6368                           domain_xml=new_xml_str,
6369                           bandwidth=CONF.libvirt.live_migration_bandwidth)
6370 
6371             for hostname, port in serial_ports:
6372                 serial_console.release_port(host=hostname, port=port)
6373         except Exception as e:
6374             with excutils.save_and_reraise_exception():
6375                 LOG.error("Live Migration failure: %s", e, instance=instance)
6376 
6377         # If 'migrateToURI' fails we don't know what state the
6378         # VM instances on each host are in. Possibilities include
6379         #
6380         #  1. src==running, dst==none
6381         #
6382         #     Migration failed & rolled back, or never started
6383         #
6384         #  2. src==running, dst==paused
6385         #
6386         #     Migration started but is still ongoing
6387         #
6388         #  3. src==paused,  dst==paused
6389         #
6390         #     Migration data transfer completed, but switchover
6391         #     is still ongoing, or failed
6392         #
6393         #  4. src==paused,  dst==running
6394         #
6395         #     Migration data transfer completed, switchover
6396         #     happened but cleanup on source failed
6397         #
6398         #  5. src==none,    dst==running
6399         #
6400         #     Migration fully succeeded.
6401         #
6402         # Libvirt will aim to complete any migration operation
6403         # or roll it back. So even if the migrateToURI call has
6404         # returned an error, if the migration was not finished
6405         # libvirt should clean up.
6406         #
6407         # So we take the error raise here with a pinch of salt
6408         # and rely on the domain job info status to figure out
6409         # what really happened to the VM, which is a much more
6410         # reliable indicator.
6411         #
6412         # In particular we need to try very hard to ensure that
6413         # Nova does not "forget" about the guest. ie leaving it
6414         # running on a different host to the one recorded in
6415         # the database, as that would be a serious resource leak
6416 
6417         LOG.debug("Migration operation thread has finished",
6418                   instance=instance)
6419 
6420     def _live_migration_copy_disk_paths(self, context, instance, guest):
6421         '''Get list of disks to copy during migration
6422 
6423         :param context: security context
6424         :param instance: the instance being migrated
6425         :param guest: the Guest instance being migrated
6426 
6427         Get the list of disks to copy during migration.
6428 
6429         :returns: a list of local source paths and a list of device names to
6430             copy
6431         '''
6432 
6433         disk_paths = []
6434         device_names = []
6435         block_devices = []
6436 
6437         # TODO(pkoniszewski): Remove version check when we bump min libvirt
6438         # version to >= 1.2.17.
6439         if (self._block_migration_flags &
6440                 libvirt.VIR_MIGRATE_TUNNELLED == 0 and
6441                 self._host.has_min_version(
6442                     MIN_LIBVIRT_BLOCK_LM_WITH_VOLUMES_VERSION)):
6443             bdm_list = objects.BlockDeviceMappingList.get_by_instance_uuid(
6444                 context, instance.uuid)
6445             block_device_info = driver.get_block_device_info(instance,
6446                                                              bdm_list)
6447 
6448             block_device_mappings = driver.block_device_info_get_mapping(
6449                 block_device_info)
6450             for bdm in block_device_mappings:
6451                 device_name = str(bdm['mount_device'].rsplit('/', 1)[1])
6452                 block_devices.append(device_name)
6453 
6454         for dev in guest.get_all_disks():
6455             if dev.readonly or dev.shareable:
6456                 continue
6457             if dev.source_type not in ["file", "block"]:
6458                 continue
6459             if dev.target_dev in block_devices:
6460                 continue
6461             disk_paths.append(dev.source_path)
6462             device_names.append(dev.target_dev)
6463         return (disk_paths, device_names)
6464 
6465     def _live_migration_data_gb(self, instance, disk_paths):
6466         '''Calculate total amount of data to be transferred
6467 
6468         :param instance: the nova.objects.Instance being migrated
6469         :param disk_paths: list of disk paths that are being migrated
6470         with instance
6471 
6472         Calculates the total amount of data that needs to be
6473         transferred during the live migration. The actual
6474         amount copied will be larger than this, due to the
6475         guest OS continuing to dirty RAM while the migration
6476         is taking place. So this value represents the minimal
6477         data size possible.
6478 
6479         :returns: data size to be copied in GB
6480         '''
6481 
6482         ram_gb = instance.flavor.memory_mb * units.Mi / units.Gi
6483         if ram_gb < 2:
6484             ram_gb = 2
6485 
6486         disk_gb = 0
6487         for path in disk_paths:
6488             try:
6489                 size = os.stat(path).st_size
6490                 size_gb = (size / units.Gi)
6491                 if size_gb < 2:
6492                     size_gb = 2
6493                 disk_gb += size_gb
6494             except OSError as e:
6495                 LOG.warning("Unable to stat %(disk)s: %(ex)s",
6496                             {'disk': path, 'ex': e})
6497                 # Ignore error since we don't want to break
6498                 # the migration monitoring thread operation
6499 
6500         return ram_gb + disk_gb
6501 
6502     def _get_migration_flags(self, is_block_migration):
6503         if is_block_migration:
6504             return self._block_migration_flags
6505         return self._live_migration_flags
6506 
6507     def _live_migration_monitor(self, context, instance, guest,
6508                                 dest, post_method,
6509                                 recover_method, block_migration,
6510                                 migrate_data, finish_event,
6511                                 disk_paths):
6512         on_migration_failure = deque()
6513         data_gb = self._live_migration_data_gb(instance, disk_paths)
6514         downtime_steps = list(libvirt_migrate.downtime_steps(data_gb))
6515         migration = migrate_data.migration
6516         curdowntime = None
6517 
6518         migration_flags = self._get_migration_flags(
6519                                   migrate_data.block_migration)
6520 
6521         n = 0
6522         start = time.time()
6523         progress_time = start
6524         progress_watermark = None
6525         previous_data_remaining = -1
6526         is_post_copy_enabled = self._is_post_copy_enabled(migration_flags)
6527         while True:
6528             info = guest.get_job_info()
6529 
6530             if info.type == libvirt.VIR_DOMAIN_JOB_NONE:
6531                 # Either still running, or failed or completed,
6532                 # lets untangle the mess
6533                 if not finish_event.ready():
6534                     LOG.debug("Operation thread is still running",
6535                               instance=instance)
6536                 else:
6537                     info.type = libvirt_migrate.find_job_type(guest, instance)
6538                     LOG.debug("Fixed incorrect job type to be %d",
6539                               info.type, instance=instance)
6540 
6541             if info.type == libvirt.VIR_DOMAIN_JOB_NONE:
6542                 # Migration is not yet started
6543                 LOG.debug("Migration not running yet",
6544                           instance=instance)
6545             elif info.type == libvirt.VIR_DOMAIN_JOB_UNBOUNDED:
6546                 # Migration is still running
6547                 #
6548                 # This is where we wire up calls to change live
6549                 # migration status. eg change max downtime, cancel
6550                 # the operation, change max bandwidth
6551                 libvirt_migrate.run_tasks(guest, instance,
6552                                           self.active_migrations,
6553                                           on_migration_failure,
6554                                           migration,
6555                                           is_post_copy_enabled)
6556 
6557                 now = time.time()
6558                 elapsed = now - start
6559 
6560                 if ((progress_watermark is None) or
6561                     (progress_watermark == 0) or
6562                     (progress_watermark > info.data_remaining)):
6563                     progress_watermark = info.data_remaining
6564                     progress_time = now
6565 
6566                 progress_timeout = CONF.libvirt.live_migration_progress_timeout
6567                 completion_timeout = int(
6568                     CONF.libvirt.live_migration_completion_timeout * data_gb)
6569                 if libvirt_migrate.should_abort(instance, now, progress_time,
6570                                                 progress_timeout, elapsed,
6571                                                 completion_timeout,
6572                                                 migration.status):
6573                     try:
6574                         guest.abort_job()
6575                     except libvirt.libvirtError as e:
6576                         LOG.warning("Failed to abort migration %s",
6577                                     e, instance=instance)
6578                         self._clear_empty_migration(instance)
6579                         raise
6580 
6581                 if (is_post_copy_enabled and
6582                     libvirt_migrate.should_switch_to_postcopy(
6583                     info.memory_iteration, info.data_remaining,
6584                     previous_data_remaining, migration.status)):
6585                     libvirt_migrate.trigger_postcopy_switch(guest,
6586                                                             instance,
6587                                                             migration)
6588                 previous_data_remaining = info.data_remaining
6589 
6590                 curdowntime = libvirt_migrate.update_downtime(
6591                     guest, instance, curdowntime,
6592                     downtime_steps, elapsed)
6593 
6594                 # We loop every 500ms, so don't log on every
6595                 # iteration to avoid spamming logs for long
6596                 # running migrations. Just once every 5 secs
6597                 # is sufficient for developers to debug problems.
6598                 # We log once every 30 seconds at info to help
6599                 # admins see slow running migration operations
6600                 # when debug logs are off.
6601                 if (n % 10) == 0:
6602                     # Ignoring memory_processed, as due to repeated
6603                     # dirtying of data, this can be way larger than
6604                     # memory_total. Best to just look at what's
6605                     # remaining to copy and ignore what's done already
6606                     #
6607                     # TODO(berrange) perhaps we could include disk
6608                     # transfer stats in the progress too, but it
6609                     # might make memory info more obscure as large
6610                     # disk sizes might dwarf memory size
6611                     remaining = 100
6612                     if info.memory_total != 0:
6613                         remaining = round(info.memory_remaining *
6614                                           100 / info.memory_total)
6615 
6616                     libvirt_migrate.save_stats(instance, migration,
6617                                                info, remaining)
6618 
6619                     lg = LOG.debug
6620                     if (n % 60) == 0:
6621                         lg = LOG.info
6622 
6623                     lg("Migration running for %(secs)d secs, "
6624                        "memory %(remaining)d%% remaining; "
6625                        "(bytes processed=%(processed_memory)d, "
6626                        "remaining=%(remaining_memory)d, "
6627                        "total=%(total_memory)d)",
6628                        {"secs": n / 2, "remaining": remaining,
6629                         "processed_memory": info.memory_processed,
6630                         "remaining_memory": info.memory_remaining,
6631                         "total_memory": info.memory_total}, instance=instance)
6632                     if info.data_remaining > progress_watermark:
6633                         lg("Data remaining %(remaining)d bytes, "
6634                            "low watermark %(watermark)d bytes "
6635                            "%(last)d seconds ago",
6636                            {"remaining": info.data_remaining,
6637                             "watermark": progress_watermark,
6638                             "last": (now - progress_time)}, instance=instance)
6639 
6640                 n = n + 1
6641             elif info.type == libvirt.VIR_DOMAIN_JOB_COMPLETED:
6642                 # Migration is all done
6643                 LOG.info("Migration operation has completed",
6644                          instance=instance)
6645                 post_method(context, instance, dest, block_migration,
6646                             migrate_data)
6647                 break
6648             elif info.type == libvirt.VIR_DOMAIN_JOB_FAILED:
6649                 # Migration did not succeed
6650                 LOG.error("Migration operation has aborted", instance=instance)
6651                 libvirt_migrate.run_recover_tasks(self._host, guest, instance,
6652                                                   on_migration_failure)
6653                 recover_method(context, instance, dest, migrate_data)
6654                 break
6655             elif info.type == libvirt.VIR_DOMAIN_JOB_CANCELLED:
6656                 # Migration was stopped by admin
6657                 LOG.warning("Migration operation was cancelled",
6658                             instance=instance)
6659                 libvirt_migrate.run_recover_tasks(self._host, guest, instance,
6660                                                   on_migration_failure)
6661                 recover_method(context, instance, dest, migrate_data,
6662                                migration_status='cancelled')
6663                 break
6664             else:
6665                 LOG.warning("Unexpected migration job type: %d",
6666                             info.type, instance=instance)
6667 
6668             time.sleep(0.5)
6669         self._clear_empty_migration(instance)
6670 
6671     def _clear_empty_migration(self, instance):
6672         try:
6673             del self.active_migrations[instance.uuid]
6674         except KeyError:
6675             LOG.warning("There are no records in active migrations "
6676                         "for instance", instance=instance)
6677 
6678     def _live_migration(self, context, instance, dest, post_method,
6679                         recover_method, block_migration,
6680                         migrate_data):
6681         """Do live migration.
6682 
6683         :param context: security context
6684         :param instance:
6685             nova.db.sqlalchemy.models.Instance object
6686             instance object that is migrated.
6687         :param dest: destination host
6688         :param post_method:
6689             post operation method.
6690             expected nova.compute.manager._post_live_migration.
6691         :param recover_method:
6692             recovery method when any exception occurs.
6693             expected nova.compute.manager._rollback_live_migration.
6694         :param block_migration: if true, do block migration.
6695         :param migrate_data: a LibvirtLiveMigrateData object
6696 
6697         This fires off a new thread to run the blocking migration
6698         operation, and then this thread monitors the progress of
6699         migration and controls its operation
6700         """
6701 
6702         guest = self._host.get_guest(instance)
6703 
6704         disk_paths = []
6705         device_names = []
6706         if (migrate_data.block_migration and
6707                 CONF.libvirt.virt_type != "parallels"):
6708             disk_paths, device_names = self._live_migration_copy_disk_paths(
6709                 context, instance, guest)
6710 
6711         opthread = utils.spawn(self._live_migration_operation,
6712                                      context, instance, dest,
6713                                      block_migration,
6714                                      migrate_data, guest,
6715                                      device_names)
6716 
6717         finish_event = eventlet.event.Event()
6718         self.active_migrations[instance.uuid] = deque()
6719 
6720         def thread_finished(thread, event):
6721             LOG.debug("Migration operation thread notification",
6722                       instance=instance)
6723             event.send()
6724         opthread.link(thread_finished, finish_event)
6725 
6726         # Let eventlet schedule the new thread right away
6727         time.sleep(0)
6728 
6729         try:
6730             LOG.debug("Starting monitoring of live migration",
6731                       instance=instance)
6732             self._live_migration_monitor(context, instance, guest, dest,
6733                                          post_method, recover_method,
6734                                          block_migration, migrate_data,
6735                                          finish_event, disk_paths)
6736         except Exception as ex:
6737             LOG.warning("Error monitoring migration: %(ex)s",
6738                         {"ex": ex}, instance=instance, exc_info=True)
6739             raise
6740         finally:
6741             LOG.debug("Live migration monitoring is all done",
6742                       instance=instance)
6743 
6744     def _is_post_copy_enabled(self, migration_flags):
6745         if self._is_post_copy_available():
6746             if (migration_flags & libvirt.VIR_MIGRATE_POSTCOPY) != 0:
6747                 return True
6748         return False
6749 
6750     def live_migration_force_complete(self, instance):
6751         try:
6752             self.active_migrations[instance.uuid].append('force-complete')
6753         except KeyError:
6754             raise exception.NoActiveMigrationForInstance(
6755                 instance_id=instance.uuid)
6756 
6757     def _try_fetch_image(self, context, path, image_id, instance,
6758                          fallback_from_host=None):
6759         try:
6760             libvirt_utils.fetch_image(context, path, image_id)
6761         except exception.ImageNotFound:
6762             if not fallback_from_host:
6763                 raise
6764             LOG.debug("Image %(image_id)s doesn't exist anymore on "
6765                       "image service, attempting to copy image "
6766                       "from %(host)s",
6767                       {'image_id': image_id, 'host': fallback_from_host})
6768             libvirt_utils.copy_image(src=path, dest=path,
6769                                      host=fallback_from_host,
6770                                      receive=True)
6771 
6772     def _fetch_instance_kernel_ramdisk(self, context, instance,
6773                                        fallback_from_host=None):
6774         """Download kernel and ramdisk for instance in instance directory."""
6775         instance_dir = libvirt_utils.get_instance_path(instance)
6776         if instance.kernel_id:
6777             kernel_path = os.path.join(instance_dir, 'kernel')
6778             # NOTE(dsanders): only fetch image if it's not available at
6779             # kernel_path. This also avoids ImageNotFound exception if
6780             # the image has been deleted from glance
6781             if not os.path.exists(kernel_path):
6782                 self._try_fetch_image(context,
6783                                       kernel_path,
6784                                       instance.kernel_id,
6785                                       instance, fallback_from_host)
6786             if instance.ramdisk_id:
6787                 ramdisk_path = os.path.join(instance_dir, 'ramdisk')
6788                 # NOTE(dsanders): only fetch image if it's not available at
6789                 # ramdisk_path. This also avoids ImageNotFound exception if
6790                 # the image has been deleted from glance
6791                 if not os.path.exists(ramdisk_path):
6792                     self._try_fetch_image(context,
6793                                           ramdisk_path,
6794                                           instance.ramdisk_id,
6795                                           instance, fallback_from_host)
6796 
6797     def rollback_live_migration_at_destination(self, context, instance,
6798                                                network_info,
6799                                                block_device_info,
6800                                                destroy_disks=True,
6801                                                migrate_data=None):
6802         """Clean up destination node after a failed live migration."""
6803         try:
6804             self.destroy(context, instance, network_info, block_device_info,
6805                          destroy_disks)
6806         finally:
6807             # NOTE(gcb): Failed block live migration may leave instance
6808             # directory at destination node, ensure it is always deleted.
6809             is_shared_instance_path = True
6810             if migrate_data:
6811                 is_shared_instance_path = migrate_data.is_shared_instance_path
6812                 if (migrate_data.obj_attr_is_set("serial_listen_ports")
6813                     and migrate_data.serial_listen_ports):
6814                     # Releases serial ports reserved.
6815                     for port in migrate_data.serial_listen_ports:
6816                         serial_console.release_port(
6817                             host=migrate_data.serial_listen_addr, port=port)
6818 
6819             if not is_shared_instance_path:
6820                 instance_dir = libvirt_utils.get_instance_path_at_destination(
6821                     instance, migrate_data)
6822                 if os.path.exists(instance_dir):
6823                     shutil.rmtree(instance_dir)
6824 
6825     def pre_live_migration(self, context, instance, block_device_info,
6826                            network_info, disk_info, migrate_data):
6827         """Preparation live migration."""
6828         if disk_info is not None:
6829             disk_info = jsonutils.loads(disk_info)
6830 
6831         LOG.debug('migrate_data in pre_live_migration: %s', migrate_data,
6832                   instance=instance)
6833         is_shared_block_storage = migrate_data.is_shared_block_storage
6834         is_shared_instance_path = migrate_data.is_shared_instance_path
6835         is_block_migration = migrate_data.block_migration
6836 
6837         if not is_shared_instance_path:
6838             instance_dir = libvirt_utils.get_instance_path_at_destination(
6839                             instance, migrate_data)
6840 
6841             if os.path.exists(instance_dir):
6842                 raise exception.DestinationDiskExists(path=instance_dir)
6843 
6844             LOG.debug('Creating instance directory: %s', instance_dir,
6845                       instance=instance)
6846             os.mkdir(instance_dir)
6847 
6848             # Recreate the disk.info file and in doing so stop the
6849             # imagebackend from recreating it incorrectly by inspecting the
6850             # contents of each file when using the Raw backend.
6851             if disk_info:
6852                 image_disk_info = {}
6853                 for info in disk_info:
6854                     image_file = os.path.basename(info['path'])
6855                     image_path = os.path.join(instance_dir, image_file)
6856                     image_disk_info[image_path] = info['type']
6857 
6858                 LOG.debug('Creating disk.info with the contents: %s',
6859                           image_disk_info, instance=instance)
6860 
6861                 image_disk_info_path = os.path.join(instance_dir,
6862                                                     'disk.info')
6863                 libvirt_utils.write_to_file(image_disk_info_path,
6864                                             jsonutils.dumps(image_disk_info))
6865 
6866             if not is_shared_block_storage:
6867                 # Ensure images and backing files are present.
6868                 LOG.debug('Checking to make sure images and backing files are '
6869                           'present before live migration.', instance=instance)
6870                 self._create_images_and_backing(
6871                     context, instance, instance_dir, disk_info,
6872                     fallback_from_host=instance.host)
6873                 if (configdrive.required_by(instance) and
6874                         CONF.config_drive_format == 'iso9660'):
6875                     # NOTE(pkoniszewski): Due to a bug in libvirt iso config
6876                     # drive needs to be copied to destination prior to
6877                     # migration when instance path is not shared and block
6878                     # storage is not shared. Files that are already present
6879                     # on destination are excluded from a list of files that
6880                     # need to be copied to destination. If we don't do that
6881                     # live migration will fail on copying iso config drive to
6882                     # destination and writing to read-only device.
6883                     # Please see bug/1246201 for more details.
6884                     src = "%s:%s/disk.config" % (instance.host, instance_dir)
6885                     self._remotefs.copy_file(src, instance_dir)
6886 
6887             if not is_block_migration:
6888                 # NOTE(angdraug): when block storage is shared between source
6889                 # and destination and instance path isn't (e.g. volume backed
6890                 # or rbd backed instance), instance path on destination has to
6891                 # be prepared
6892 
6893                 # Required by Quobyte CI
6894                 self._ensure_console_log_for_instance(instance)
6895 
6896                 # if image has kernel and ramdisk, just download
6897                 # following normal way.
6898                 self._fetch_instance_kernel_ramdisk(context, instance)
6899 
6900         # Establishing connection to volume server.
6901         block_device_mapping = driver.block_device_info_get_mapping(
6902             block_device_info)
6903 
6904         if len(block_device_mapping):
6905             LOG.debug('Connecting volumes before live migration.',
6906                       instance=instance)
6907 
6908         for bdm in block_device_mapping:
6909             connection_info = bdm['connection_info']
6910             disk_info = blockinfo.get_info_from_bdm(
6911                 instance, CONF.libvirt.virt_type,
6912                 instance.image_meta, bdm)
6913             self._connect_volume(connection_info, disk_info, instance)
6914 
6915         # We call plug_vifs before the compute manager calls
6916         # ensure_filtering_rules_for_instance, to ensure bridge is set up
6917         # Retry operation is necessary because continuously request comes,
6918         # concurrent request occurs to iptables, then it complains.
6919         LOG.debug('Plugging VIFs before live migration.', instance=instance)
6920         max_retry = CONF.live_migration_retry_count
6921         for cnt in range(max_retry):
6922             try:
6923                 self.plug_vifs(instance, network_info)
6924                 break
6925             except processutils.ProcessExecutionError:
6926                 if cnt == max_retry - 1:
6927                     raise
6928                 else:
6929                     LOG.warning('plug_vifs() failed %(cnt)d. Retry up to '
6930                                 '%(max_retry)d.',
6931                                 {'cnt': cnt, 'max_retry': max_retry},
6932                                 instance=instance)
6933                     greenthread.sleep(1)
6934 
6935         # Store vncserver_listen and latest disk device info
6936         if not migrate_data:
6937             migrate_data = objects.LibvirtLiveMigrateData(bdms=[])
6938         else:
6939             migrate_data.bdms = []
6940         # Store live_migration_inbound_addr
6941         migrate_data.target_connect_addr = \
6942             CONF.libvirt.live_migration_inbound_addr
6943         migrate_data.supported_perf_events = self._supported_perf_events
6944 
6945         migrate_data.serial_listen_ports = []
6946         if CONF.serial_console.enabled:
6947             num_ports = hardware.get_number_of_serial_ports(
6948                 instance.flavor, instance.image_meta)
6949             for port in six.moves.range(num_ports):
6950                 migrate_data.serial_listen_ports.append(
6951                     serial_console.acquire_port(
6952                         migrate_data.serial_listen_addr))
6953 
6954         for vol in block_device_mapping:
6955             connection_info = vol['connection_info']
6956             if connection_info.get('serial'):
6957                 disk_info = blockinfo.get_info_from_bdm(
6958                     instance, CONF.libvirt.virt_type,
6959                     instance.image_meta, vol)
6960 
6961                 bdmi = objects.LibvirtLiveMigrateBDMInfo()
6962                 bdmi.serial = connection_info['serial']
6963                 bdmi.connection_info = connection_info
6964                 bdmi.bus = disk_info['bus']
6965                 bdmi.dev = disk_info['dev']
6966                 bdmi.type = disk_info['type']
6967                 bdmi.format = disk_info.get('format')
6968                 bdmi.boot_index = disk_info.get('boot_index')
6969                 migrate_data.bdms.append(bdmi)
6970 
6971         return migrate_data
6972 
6973     def _try_fetch_image_cache(self, image, fetch_func, context, filename,
6974                                image_id, instance, size,
6975                                fallback_from_host=None):
6976         try:
6977             image.cache(fetch_func=fetch_func,
6978                         context=context,
6979                         filename=filename,
6980                         image_id=image_id,
6981                         size=size)
6982         except exception.ImageNotFound:
6983             if not fallback_from_host:
6984                 raise
6985             LOG.debug("Image %(image_id)s doesn't exist anymore "
6986                       "on image service, attempting to copy "
6987                       "image from %(host)s",
6988                       {'image_id': image_id, 'host': fallback_from_host},
6989                       instance=instance)
6990 
6991             def copy_from_host(target):
6992                 libvirt_utils.copy_image(src=target,
6993                                          dest=target,
6994                                          host=fallback_from_host,
6995                                          receive=True)
6996             image.cache(fetch_func=copy_from_host,
6997                         filename=filename)
6998 
6999     def _create_images_and_backing(self, context, instance, instance_dir,
7000                                    disk_info, fallback_from_host=None):
7001         """:param context: security context
7002            :param instance:
7003                nova.db.sqlalchemy.models.Instance object
7004                instance object that is migrated.
7005            :param instance_dir:
7006                instance path to use, calculated externally to handle block
7007                migrating an instance with an old style instance path
7008            :param disk_info:
7009                disk info specified in _get_instance_disk_info_from_config
7010                (list of dicts)
7011            :param fallback_from_host:
7012                host where we can retrieve images if the glance images are
7013                not available.
7014 
7015         """
7016 
7017         # Virtuozzo containers don't use backing file
7018         if (CONF.libvirt.virt_type == "parallels" and
7019                 instance.vm_mode == fields.VMMode.EXE):
7020             return
7021 
7022         if not disk_info:
7023             disk_info = []
7024 
7025         for info in disk_info:
7026             base = os.path.basename(info['path'])
7027             # Get image type and create empty disk image, and
7028             # create backing file in case of qcow2.
7029             instance_disk = os.path.join(instance_dir, base)
7030             if not info['backing_file'] and not os.path.exists(instance_disk):
7031                 libvirt_utils.create_image(info['type'], instance_disk,
7032                                            info['virt_disk_size'])
7033             elif info['backing_file']:
7034                 # Creating backing file follows same way as spawning instances.
7035                 cache_name = os.path.basename(info['backing_file'])
7036 
7037                 disk = self.image_backend.by_name(instance, instance_disk,
7038                                                   CONF.libvirt.images_type)
7039                 if cache_name.startswith('ephemeral'):
7040                     # The argument 'size' is used by image.cache to
7041                     # validate disk size retrieved from cache against
7042                     # the instance disk size (should always return OK)
7043                     # and ephemeral_size is used by _create_ephemeral
7044                     # to build the image if the disk is not already
7045                     # cached.
7046                     disk.cache(
7047                         fetch_func=self._create_ephemeral,
7048                         fs_label=cache_name,
7049                         os_type=instance.os_type,
7050                         filename=cache_name,
7051                         size=info['virt_disk_size'],
7052                         ephemeral_size=info['virt_disk_size'] / units.Gi)
7053                 elif cache_name.startswith('swap'):
7054                     inst_type = instance.get_flavor()
7055                     swap_mb = inst_type.swap
7056                     disk.cache(fetch_func=self._create_swap,
7057                                 filename="swap_%s" % swap_mb,
7058                                 size=swap_mb * units.Mi,
7059                                 swap_mb=swap_mb)
7060                 else:
7061                     self._try_fetch_image_cache(disk,
7062                                                 libvirt_utils.fetch_image,
7063                                                 context, cache_name,
7064                                                 instance.image_ref,
7065                                                 instance,
7066                                                 info['virt_disk_size'],
7067                                                 fallback_from_host)
7068 
7069         # if disk has kernel and ramdisk, just download
7070         # following normal way.
7071         self._fetch_instance_kernel_ramdisk(
7072             context, instance, fallback_from_host=fallback_from_host)
7073 
7074     def post_live_migration(self, context, instance, block_device_info,
7075                             migrate_data=None):
7076         # Disconnect from volume server
7077         block_device_mapping = driver.block_device_info_get_mapping(
7078                 block_device_info)
7079         connector = self.get_volume_connector(instance)
7080         volume_api = self._volume_api
7081         for vol in block_device_mapping:
7082             # Retrieve connection info from Cinder's initialize_connection API.
7083             # The info returned will be accurate for the source server.
7084             volume_id = vol['connection_info']['serial']
7085             connection_info = volume_api.initialize_connection(context,
7086                                                                volume_id,
7087                                                                connector)
7088 
7089             # TODO(leeantho) The following multipath_id logic is temporary
7090             # and will be removed in the future once os-brick is updated
7091             # to handle multipath for drivers in a more efficient way.
7092             # For now this logic is needed to ensure the connection info
7093             # data is correct.
7094 
7095             # Pull out multipath_id from the bdm information. The
7096             # multipath_id can be placed into the connection info
7097             # because it is based off of the volume and will be the
7098             # same on the source and destination hosts.
7099             if 'multipath_id' in vol['connection_info']['data']:
7100                 multipath_id = vol['connection_info']['data']['multipath_id']
7101                 connection_info['data']['multipath_id'] = multipath_id
7102 
7103             disk_dev = vol['mount_device'].rpartition("/")[2]
7104             self._disconnect_volume(connection_info, disk_dev, instance)
7105 
7106     def post_live_migration_at_source(self, context, instance, network_info):
7107         """Unplug VIFs from networks at source.
7108 
7109         :param context: security context
7110         :param instance: instance object reference
7111         :param network_info: instance network information
7112         """
7113         self.unplug_vifs(instance, network_info)
7114 
7115     def post_live_migration_at_destination(self, context,
7116                                            instance,
7117                                            network_info,
7118                                            block_migration=False,
7119                                            block_device_info=None):
7120         """Post operation of live migration at destination host.
7121 
7122         :param context: security context
7123         :param instance:
7124             nova.db.sqlalchemy.models.Instance object
7125             instance object that is migrated.
7126         :param network_info: instance network information
7127         :param block_migration: if true, post operation of block_migration.
7128         """
7129         guest = self._host.get_guest(instance)
7130 
7131         # TODO(sahid): In Ocata we have added the migration flag
7132         # VIR_MIGRATE_PERSIST_DEST to libvirt, which means that the
7133         # guest XML is going to be set in libvirtd on destination node
7134         # automatically. However we do not remove that part until P*
7135         # because during an upgrade, to ensure migrating instances
7136         # from node running Newton is still going to set the guest XML
7137         # in libvirtd on destination node.
7138 
7139         # Make sure we define the migrated instance in libvirt
7140         xml = guest.get_xml_desc()
7141         self._host.write_instance_config(xml)
7142 
7143     def _get_instance_disk_info_from_config(self, guest_config,
7144                                             block_device_info):
7145         """Get the non-volume disk information from the domain xml
7146 
7147         :param LibvirtConfigGuest guest_config: the libvirt domain config
7148                                                 for the instance
7149         :param dict block_device_info: block device info for BDMs
7150         :returns disk_info: list of dicts with keys:
7151 
7152           * 'type': the disk type (str)
7153           * 'path': the disk path (str)
7154           * 'virt_disk_size': the virtual disk size (int)
7155           * 'backing_file': backing file of a disk image (str)
7156           * 'disk_size': physical disk size (int)
7157           * 'over_committed_disk_size': virt_disk_size - disk_size or 0
7158         """
7159         block_device_mapping = driver.block_device_info_get_mapping(
7160             block_device_info)
7161 
7162         volume_devices = set()
7163         for vol in block_device_mapping:
7164             disk_dev = vol['mount_device'].rpartition("/")[2]
7165             volume_devices.add(disk_dev)
7166 
7167         disk_info = []
7168 
7169         if (guest_config.virt_type == 'parallels' and
7170                 guest_config.os_type == fields.VMMode.EXE):
7171             node_type = 'filesystem'
7172         else:
7173             node_type = 'disk'
7174 
7175         for device in guest_config.devices:
7176             if device.root_name != node_type:
7177                 continue
7178             disk_type = device.source_type
7179             if device.root_name == 'filesystem':
7180                 target = device.target_dir
7181                 if device.source_type == 'file':
7182                     path = device.source_file
7183                 elif device.source_type == 'block':
7184                     path = device.source_dev
7185                 else:
7186                     path = None
7187             else:
7188                 target = device.target_dev
7189                 path = device.source_path
7190 
7191             if not path:
7192                 LOG.debug('skipping disk for %s as it does not have a path',
7193                           guest_config.name)
7194                 continue
7195 
7196             if disk_type not in ['file', 'block']:
7197                 LOG.debug('skipping disk because it looks like a volume', path)
7198                 continue
7199 
7200             if target in volume_devices:
7201                 LOG.debug('skipping disk %(path)s (%(target)s) as it is a '
7202                           'volume', {'path': path, 'target': target})
7203                 continue
7204 
7205             if device.root_name == 'filesystem':
7206                 driver_type = device.driver_type
7207             else:
7208                 driver_type = device.driver_format
7209             # get the real disk size or
7210             # raise a localized error if image is unavailable
7211             if disk_type == 'file':
7212                 if driver_type == 'ploop':
7213                     dk_size = 0
7214                     for dirpath, dirnames, filenames in os.walk(path):
7215                         for f in filenames:
7216                             fp = os.path.join(dirpath, f)
7217                             dk_size += os.path.getsize(fp)
7218                 else:
7219                     dk_size = int(os.path.getsize(path))
7220             elif disk_type == 'block' and block_device_info:
7221                 dk_size = lvm.get_volume_size(path)
7222             else:
7223                 LOG.debug('skipping disk %(path)s (%(target)s) - unable to '
7224                           'determine if volume',
7225                           {'path': path, 'target': target})
7226                 continue
7227 
7228             if driver_type in ("qcow2", "ploop"):
7229                 backing_file = libvirt_utils.get_disk_backing_file(path)
7230                 virt_size = disk_api.get_disk_size(path)
7231                 over_commit_size = int(virt_size) - dk_size
7232             else:
7233                 backing_file = ""
7234                 virt_size = dk_size
7235                 over_commit_size = 0
7236 
7237             disk_info.append({'type': driver_type,
7238                               'path': path,
7239                               'virt_disk_size': virt_size,
7240                               'backing_file': backing_file,
7241                               'disk_size': dk_size,
7242                               'over_committed_disk_size': over_commit_size})
7243         return disk_info
7244 
7245     def _get_instance_disk_info(self, instance, block_device_info):
7246         try:
7247             guest = self._host.get_guest(instance)
7248             config = guest.get_config()
7249         except libvirt.libvirtError as ex:
7250             error_code = ex.get_error_code()
7251             LOG.warning('Error from libvirt while getting description of '
7252                         '%(instance_name)s: [Error Code %(error_code)s] '
7253                         '%(ex)s',
7254                         {'instance_name': instance.name,
7255                          'error_code': error_code,
7256                          'ex': ex},
7257                         instance=instance)
7258             raise exception.InstanceNotFound(instance_id=instance.uuid)
7259 
7260         return self._get_instance_disk_info_from_config(config,
7261                                                         block_device_info)
7262 
7263     def get_instance_disk_info(self, instance,
7264                                block_device_info=None):
7265         return jsonutils.dumps(
7266             self._get_instance_disk_info(instance, block_device_info))
7267 
7268     def _get_disk_over_committed_size_total(self):
7269         """Return total over committed disk size for all instances."""
7270         # Disk size that all instance uses : virtual_size - disk_size
7271         disk_over_committed_size = 0
7272         instance_domains = self._host.list_instance_domains(only_running=False)
7273         if not instance_domains:
7274             return disk_over_committed_size
7275 
7276         # Get all instance uuids
7277         instance_uuids = [dom.UUIDString() for dom in instance_domains]
7278         ctx = nova_context.get_admin_context()
7279         # Get instance object list by uuid filter
7280         filters = {'uuid': instance_uuids}
7281         # NOTE(ankit): objects.InstanceList.get_by_filters method is
7282         # getting called twice one is here and another in the
7283         # _update_available_resource method of resource_tracker. Since
7284         # _update_available_resource method is synchronized, there is a
7285         # possibility the instances list retrieved here to calculate
7286         # disk_over_committed_size would differ to the list you would get
7287         # in _update_available_resource method for calculating usages based
7288         # on instance utilization.
7289         local_instance_list = objects.InstanceList.get_by_filters(
7290             ctx, filters, use_slave=True)
7291         # Convert instance list to dictionary with instance uuid as key.
7292         local_instances = {inst.uuid: inst for inst in local_instance_list}
7293 
7294         # Get bdms by instance uuids
7295         bdms = objects.BlockDeviceMappingList.bdms_by_instance_uuid(
7296             ctx, instance_uuids)
7297 
7298         for dom in instance_domains:
7299             try:
7300                 guest = libvirt_guest.Guest(dom)
7301                 config = guest.get_config()
7302 
7303                 block_device_info = None
7304                 if guest.uuid in local_instances \
7305                         and (bdms and guest.uuid in bdms):
7306                     # Get block device info for instance
7307                     block_device_info = driver.get_block_device_info(
7308                         local_instances[guest.uuid], bdms[guest.uuid])
7309 
7310                 disk_infos = self._get_instance_disk_info_from_config(
7311                     config, block_device_info)
7312                 if not disk_infos:
7313                     continue
7314 
7315                 for info in disk_infos:
7316                     disk_over_committed_size += int(
7317                         info['over_committed_disk_size'])
7318             except libvirt.libvirtError as ex:
7319                 error_code = ex.get_error_code()
7320                 LOG.warning(
7321                     'Error from libvirt while getting description of '
7322                     '%(instance_name)s: [Error Code %(error_code)s] %(ex)s',
7323                     {'instance_name': guest.name,
7324                      'error_code': error_code,
7325                      'ex': ex})
7326             except OSError as e:
7327                 if e.errno in (errno.ENOENT, errno.ESTALE):
7328                     LOG.warning('Periodic task is updating the host stat, '
7329                                 'it is trying to get disk %(i_name)s, '
7330                                 'but disk file was removed by concurrent '
7331                                 'operations such as resize.',
7332                                 {'i_name': guest.name})
7333                 elif e.errno == errno.EACCES:
7334                     LOG.warning('Periodic task is updating the host stat, '
7335                                 'it is trying to get disk %(i_name)s, '
7336                                 'but access is denied. It is most likely '
7337                                 'due to a VM that exists on the compute '
7338                                 'node but is not managed by Nova.',
7339                                 {'i_name': guest.name})
7340                 else:
7341                     raise
7342             except exception.VolumeBDMPathNotFound as e:
7343                 LOG.warning('Periodic task is updating the host stats, '
7344                             'it is trying to get disk info for %(i_name)s, '
7345                             'but the backing volume block device was removed '
7346                             'by concurrent operations such as resize. '
7347                             'Error: %(error)s',
7348                             {'i_name': guest.name, 'error': e})
7349             # NOTE(gtt116): give other tasks a chance.
7350             greenthread.sleep(0)
7351         return disk_over_committed_size
7352 
7353     def unfilter_instance(self, instance, network_info):
7354         """See comments of same method in firewall_driver."""
7355         self.firewall_driver.unfilter_instance(instance,
7356                                                network_info=network_info)
7357 
7358     def get_available_nodes(self, refresh=False):
7359         return [self._host.get_hostname()]
7360 
7361     def get_host_cpu_stats(self):
7362         """Return the current CPU state of the host."""
7363         return self._host.get_cpu_stats()
7364 
7365     def get_host_uptime(self):
7366         """Returns the result of calling "uptime"."""
7367         out, err = utils.execute('env', 'LANG=C', 'uptime')
7368         return out
7369 
7370     def manage_image_cache(self, context, all_instances):
7371         """Manage the local cache of images."""
7372         self.image_cache_manager.update(context, all_instances)
7373 
7374     def _cleanup_remote_migration(self, dest, inst_base, inst_base_resize,
7375                                   shared_storage=False):
7376         """Used only for cleanup in case migrate_disk_and_power_off fails."""
7377         try:
7378             if os.path.exists(inst_base_resize):
7379                 utils.execute('rm', '-rf', inst_base)
7380                 utils.execute('mv', inst_base_resize, inst_base)
7381                 if not shared_storage:
7382                     self._remotefs.remove_dir(dest, inst_base)
7383         except Exception:
7384             pass
7385 
7386     def _is_storage_shared_with(self, dest, inst_base):
7387         # NOTE (rmk): There are two methods of determining whether we are
7388         #             on the same filesystem: the source and dest IP are the
7389         #             same, or we create a file on the dest system via SSH
7390         #             and check whether the source system can also see it.
7391         # NOTE (drwahl): Actually, there is a 3rd way: if images_type is rbd,
7392         #                it will always be shared storage
7393         if CONF.libvirt.images_type == 'rbd':
7394             return True
7395         shared_storage = (dest == self.get_host_ip_addr())
7396         if not shared_storage:
7397             tmp_file = uuid.uuid4().hex + '.tmp'
7398             tmp_path = os.path.join(inst_base, tmp_file)
7399 
7400             try:
7401                 self._remotefs.create_file(dest, tmp_path)
7402                 if os.path.exists(tmp_path):
7403                     shared_storage = True
7404                     os.unlink(tmp_path)
7405                 else:
7406                     self._remotefs.remove_file(dest, tmp_path)
7407             except Exception:
7408                 pass
7409         return shared_storage
7410 
7411     def migrate_disk_and_power_off(self, context, instance, dest,
7412                                    flavor, network_info,
7413                                    block_device_info=None,
7414                                    timeout=0, retry_interval=0):
7415         LOG.debug("Starting migrate_disk_and_power_off",
7416                    instance=instance)
7417 
7418         ephemerals = driver.block_device_info_get_ephemerals(block_device_info)
7419 
7420         # get_bdm_ephemeral_disk_size() will return 0 if the new
7421         # instance's requested block device mapping contain no
7422         # ephemeral devices. However, we still want to check if
7423         # the original instance's ephemeral_gb property was set and
7424         # ensure that the new requested flavor ephemeral size is greater
7425         eph_size = (block_device.get_bdm_ephemeral_disk_size(ephemerals) or
7426                     instance.flavor.ephemeral_gb)
7427 
7428         # Checks if the migration needs a disk resize down.
7429         root_down = flavor.root_gb < instance.flavor.root_gb
7430         ephemeral_down = flavor.ephemeral_gb < eph_size
7431         booted_from_volume = self._is_booted_from_volume(block_device_info)
7432 
7433         if (root_down and not booted_from_volume) or ephemeral_down:
7434             reason = _("Unable to resize disk down.")
7435             raise exception.InstanceFaultRollback(
7436                 exception.ResizeError(reason=reason))
7437 
7438         # NOTE(dgenin): Migration is not implemented for LVM backed instances.
7439         if CONF.libvirt.images_type == 'lvm' and not booted_from_volume:
7440             reason = _("Migration is not supported for LVM backed instances")
7441             raise exception.InstanceFaultRollback(
7442                 exception.MigrationPreCheckError(reason=reason))
7443 
7444         # copy disks to destination
7445         # rename instance dir to +_resize at first for using
7446         # shared storage for instance dir (eg. NFS).
7447         inst_base = libvirt_utils.get_instance_path(instance)
7448         inst_base_resize = inst_base + "_resize"
7449         shared_storage = self._is_storage_shared_with(dest, inst_base)
7450 
7451         # try to create the directory on the remote compute node
7452         # if this fails we pass the exception up the stack so we can catch
7453         # failures here earlier
7454         if not shared_storage:
7455             try:
7456                 self._remotefs.create_dir(dest, inst_base)
7457             except processutils.ProcessExecutionError as e:
7458                 reason = _("not able to execute ssh command: %s") % e
7459                 raise exception.InstanceFaultRollback(
7460                     exception.ResizeError(reason=reason))
7461 
7462         self.power_off(instance, timeout, retry_interval)
7463 
7464         block_device_mapping = driver.block_device_info_get_mapping(
7465             block_device_info)
7466         for vol in block_device_mapping:
7467             connection_info = vol['connection_info']
7468             disk_dev = vol['mount_device'].rpartition("/")[2]
7469             self._disconnect_volume(connection_info, disk_dev, instance)
7470 
7471         disk_info = self._get_instance_disk_info(instance, block_device_info)
7472 
7473         try:
7474             utils.execute('mv', inst_base, inst_base_resize)
7475             # if we are migrating the instance with shared storage then
7476             # create the directory.  If it is a remote node the directory
7477             # has already been created
7478             if shared_storage:
7479                 dest = None
7480                 utils.execute('mkdir', '-p', inst_base)
7481 
7482             on_execute = lambda process: \
7483                 self.job_tracker.add_job(instance, process.pid)
7484             on_completion = lambda process: \
7485                 self.job_tracker.remove_job(instance, process.pid)
7486 
7487             for info in disk_info:
7488                 # assume inst_base == dirname(info['path'])
7489                 img_path = info['path']
7490                 fname = os.path.basename(img_path)
7491                 from_path = os.path.join(inst_base_resize, fname)
7492 
7493                 # We will not copy over the swap disk here, and rely on
7494                 # finish_migration to re-create it for us. This is ok because
7495                 # the OS is shut down, and as recreating a swap disk is very
7496                 # cheap it is more efficient than copying either locally or
7497                 # over the network. This also means we don't have to resize it.
7498                 if fname == 'disk.swap':
7499                     continue
7500 
7501                 compression = info['type'] not in NO_COMPRESSION_TYPES
7502                 libvirt_utils.copy_image(from_path, img_path, host=dest,
7503                                          on_execute=on_execute,
7504                                          on_completion=on_completion,
7505                                          compression=compression)
7506 
7507             # Ensure disk.info is written to the new path to avoid disks being
7508             # reinspected and potentially changing format.
7509             src_disk_info_path = os.path.join(inst_base_resize, 'disk.info')
7510             if os.path.exists(src_disk_info_path):
7511                 dst_disk_info_path = os.path.join(inst_base, 'disk.info')
7512                 libvirt_utils.copy_image(src_disk_info_path,
7513                                          dst_disk_info_path,
7514                                          host=dest, on_execute=on_execute,
7515                                          on_completion=on_completion)
7516         except Exception:
7517             with excutils.save_and_reraise_exception():
7518                 self._cleanup_remote_migration(dest, inst_base,
7519                                                inst_base_resize,
7520                                                shared_storage)
7521 
7522         return jsonutils.dumps(disk_info)
7523 
7524     def _wait_for_running(self, instance):
7525         state = self.get_info(instance).state
7526 
7527         if state == power_state.RUNNING:
7528             LOG.info("Instance running successfully.", instance=instance)
7529             raise loopingcall.LoopingCallDone()
7530 
7531     @staticmethod
7532     def _disk_raw_to_qcow2(path):
7533         """Converts a raw disk to qcow2."""
7534         path_qcow = path + '_qcow'
7535         utils.execute('qemu-img', 'convert', '-f', 'raw',
7536                       '-O', 'qcow2', path, path_qcow)
7537         utils.execute('mv', path_qcow, path)
7538 
7539     @staticmethod
7540     def _disk_qcow2_to_raw(path):
7541         """Converts a qcow2 disk to raw."""
7542         path_raw = path + '_raw'
7543         utils.execute('qemu-img', 'convert', '-f', 'qcow2',
7544                       '-O', 'raw', path, path_raw)
7545         utils.execute('mv', path_raw, path)
7546 
7547     def finish_migration(self, context, migration, instance, disk_info,
7548                          network_info, image_meta, resize_instance,
7549                          block_device_info=None, power_on=True):
7550         LOG.debug("Starting finish_migration", instance=instance)
7551 
7552         block_disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
7553                                                   instance,
7554                                                   image_meta,
7555                                                   block_device_info)
7556         # assume _create_image does nothing if a target file exists.
7557         # NOTE: This has the intended side-effect of fetching a missing
7558         # backing file.
7559         self._create_image(context, instance, block_disk_info['mapping'],
7560                            block_device_info=block_device_info,
7561                            ignore_bdi_for_swap=True,
7562                            fallback_from_host=migration.source_compute)
7563 
7564         # Required by Quobyte CI
7565         self._ensure_console_log_for_instance(instance)
7566 
7567         gen_confdrive = functools.partial(
7568             self._create_configdrive, context, instance,
7569             InjectionInfo(admin_pass=None, network_info=network_info,
7570                           files=None))
7571 
7572         # Convert raw disks to qcow2 if migrating to host which uses
7573         # qcow2 from host which uses raw.
7574         disk_info = jsonutils.loads(disk_info)
7575         for info in disk_info:
7576             path = info['path']
7577             disk_name = os.path.basename(path)
7578 
7579             # NOTE(mdbooth): The code below looks wrong, but is actually
7580             # required to prevent a security hole when migrating from a host
7581             # with use_cow_images=False to one with use_cow_images=True.
7582             # Imagebackend uses use_cow_images to select between the
7583             # atrociously-named-Raw and Qcow2 backends. The Qcow2 backend
7584             # writes to disk.info, but does not read it as it assumes qcow2.
7585             # Therefore if we don't convert raw to qcow2 here, a raw disk will
7586             # be incorrectly assumed to be qcow2, which is a severe security
7587             # flaw. The reverse is not true, because the atrociously-named-Raw
7588             # backend supports both qcow2 and raw disks, and will choose
7589             # appropriately between them as long as disk.info exists and is
7590             # correctly populated, which it is because Qcow2 writes to
7591             # disk.info.
7592             #
7593             # In general, we do not yet support format conversion during
7594             # migration. For example:
7595             #   * Converting from use_cow_images=True to use_cow_images=False
7596             #     isn't handled. This isn't a security bug, but is almost
7597             #     certainly buggy in other cases, as the 'Raw' backend doesn't
7598             #     expect a backing file.
7599             #   * Converting to/from lvm and rbd backends is not supported.
7600             #
7601             # This behaviour is inconsistent, and therefore undesirable for
7602             # users. It is tightly-coupled to implementation quirks of 2
7603             # out of 5 backends in imagebackend and defends against a severe
7604             # security flaw which is not at all obvious without deep analysis,
7605             # and is therefore undesirable to developers. We should aim to
7606             # remove it. This will not be possible, though, until we can
7607             # represent the storage layout of a specific instance
7608             # independent of the default configuration of the local compute
7609             # host.
7610 
7611             # Config disks are hard-coded to be raw even when
7612             # use_cow_images=True (see _get_disk_config_image_type),so don't
7613             # need to be converted.
7614             if (disk_name != 'disk.config' and
7615                         info['type'] == 'raw' and CONF.use_cow_images):
7616                 self._disk_raw_to_qcow2(info['path'])
7617 
7618         xml = self._get_guest_xml(context, instance, network_info,
7619                                   block_disk_info, image_meta,
7620                                   block_device_info=block_device_info)
7621         # NOTE(mriedem): vifs_already_plugged=True here, regardless of whether
7622         # or not we've migrated to another host, because we unplug VIFs locally
7623         # and the status change in the port might go undetected by the neutron
7624         # L2 agent (or neutron server) so neutron may not know that the VIF was
7625         # unplugged in the first place and never send an event.
7626         guest = self._create_domain_and_network(context, xml, instance,
7627                                         network_info,
7628                                         block_device_info=block_device_info,
7629                                         power_on=power_on,
7630                                         vifs_already_plugged=True,
7631                                         post_xml_callback=gen_confdrive)
7632         if power_on:
7633             timer = loopingcall.FixedIntervalLoopingCall(
7634                                                     self._wait_for_running,
7635                                                     instance)
7636             timer.start(interval=0.5).wait()
7637 
7638             # Sync guest time after migration.
7639             guest.sync_guest_time()
7640 
7641         LOG.debug("finish_migration finished successfully.", instance=instance)
7642 
7643     def _cleanup_failed_migration(self, inst_base):
7644         """Make sure that a failed migrate doesn't prevent us from rolling
7645         back in a revert.
7646         """
7647         try:
7648             shutil.rmtree(inst_base)
7649         except OSError as e:
7650             if e.errno != errno.ENOENT:
7651                 raise
7652 
7653     def finish_revert_migration(self, context, instance, network_info,
7654                                 block_device_info=None, power_on=True):
7655         LOG.debug("Starting finish_revert_migration",
7656                   instance=instance)
7657 
7658         inst_base = libvirt_utils.get_instance_path(instance)
7659         inst_base_resize = inst_base + "_resize"
7660 
7661         # NOTE(danms): if we're recovering from a failed migration,
7662         # make sure we don't have a left-over same-host base directory
7663         # that would conflict. Also, don't fail on the rename if the
7664         # failure happened early.
7665         if os.path.exists(inst_base_resize):
7666             self._cleanup_failed_migration(inst_base)
7667             utils.execute('mv', inst_base_resize, inst_base)
7668 
7669         root_disk = self.image_backend.by_name(instance, 'disk')
7670         # Once we rollback, the snapshot is no longer needed, so remove it
7671         # TODO(nic): Remove the try/except/finally in a future release
7672         # To avoid any upgrade issues surrounding instances being in pending
7673         # resize state when the software is updated, this portion of the
7674         # method logs exceptions rather than failing on them.  Once it can be
7675         # reasonably assumed that no such instances exist in the wild
7676         # anymore, the try/except/finally should be removed,
7677         # and ignore_errors should be set back to False (the default) so
7678         # that problems throw errors, like they should.
7679         if root_disk.exists():
7680             try:
7681                 root_disk.rollback_to_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME)
7682             except exception.SnapshotNotFound:
7683                 LOG.warning("Failed to rollback snapshot (%s)",
7684                             libvirt_utils.RESIZE_SNAPSHOT_NAME)
7685             finally:
7686                 root_disk.remove_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME,
7687                                       ignore_errors=True)
7688 
7689         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
7690                                             instance,
7691                                             instance.image_meta,
7692                                             block_device_info)
7693         xml = self._get_guest_xml(context, instance, network_info, disk_info,
7694                                   instance.image_meta,
7695                                   block_device_info=block_device_info)
7696         self._create_domain_and_network(context, xml, instance, network_info,
7697                                         block_device_info=block_device_info,
7698                                         power_on=power_on,
7699                                         vifs_already_plugged=True)
7700 
7701         if power_on:
7702             timer = loopingcall.FixedIntervalLoopingCall(
7703                                                     self._wait_for_running,
7704                                                     instance)
7705             timer.start(interval=0.5).wait()
7706 
7707         LOG.debug("finish_revert_migration finished successfully.",
7708                   instance=instance)
7709 
7710     def confirm_migration(self, context, migration, instance, network_info):
7711         """Confirms a resize, destroying the source VM."""
7712         self._cleanup_resize(instance, network_info)
7713 
7714     @staticmethod
7715     def _get_io_devices(xml_doc):
7716         """get the list of io devices from the xml document."""
7717         result = {"volumes": [], "ifaces": []}
7718         try:
7719             doc = etree.fromstring(xml_doc)
7720         except Exception:
7721             return result
7722         blocks = [('./devices/disk', 'volumes'),
7723             ('./devices/interface', 'ifaces')]
7724         for block, key in blocks:
7725             section = doc.findall(block)
7726             for node in section:
7727                 for child in node.getchildren():
7728                     if child.tag == 'target' and child.get('dev'):
7729                         result[key].append(child.get('dev'))
7730         return result
7731 
7732     def get_diagnostics(self, instance):
7733         guest = self._host.get_guest(instance)
7734 
7735         # TODO(sahid): We are converting all calls from a
7736         # virDomain object to use nova.virt.libvirt.Guest.
7737         # We should be able to remove domain at the end.
7738         domain = guest._domain
7739         output = {}
7740         # get cpu time, might launch an exception if the method
7741         # is not supported by the underlying hypervisor being
7742         # used by libvirt
7743         try:
7744             for vcpu in guest.get_vcpus_info():
7745                 output["cpu" + str(vcpu.id) + "_time"] = vcpu.time
7746         except libvirt.libvirtError:
7747             pass
7748         # get io status
7749         xml = guest.get_xml_desc()
7750         dom_io = LibvirtDriver._get_io_devices(xml)
7751         for guest_disk in dom_io["volumes"]:
7752             try:
7753                 # blockStats might launch an exception if the method
7754                 # is not supported by the underlying hypervisor being
7755                 # used by libvirt
7756                 stats = domain.blockStats(guest_disk)
7757                 output[guest_disk + "_read_req"] = stats[0]
7758                 output[guest_disk + "_read"] = stats[1]
7759                 output[guest_disk + "_write_req"] = stats[2]
7760                 output[guest_disk + "_write"] = stats[3]
7761                 output[guest_disk + "_errors"] = stats[4]
7762             except libvirt.libvirtError:
7763                 pass
7764         for interface in dom_io["ifaces"]:
7765             try:
7766                 # interfaceStats might launch an exception if the method
7767                 # is not supported by the underlying hypervisor being
7768                 # used by libvirt
7769                 stats = domain.interfaceStats(interface)
7770                 output[interface + "_rx"] = stats[0]
7771                 output[interface + "_rx_packets"] = stats[1]
7772                 output[interface + "_rx_errors"] = stats[2]
7773                 output[interface + "_rx_drop"] = stats[3]
7774                 output[interface + "_tx"] = stats[4]
7775                 output[interface + "_tx_packets"] = stats[5]
7776                 output[interface + "_tx_errors"] = stats[6]
7777                 output[interface + "_tx_drop"] = stats[7]
7778             except libvirt.libvirtError:
7779                 pass
7780         output["memory"] = domain.maxMemory()
7781         # memoryStats might launch an exception if the method
7782         # is not supported by the underlying hypervisor being
7783         # used by libvirt
7784         try:
7785             mem = domain.memoryStats()
7786             for key in mem.keys():
7787                 output["memory-" + key] = mem[key]
7788         except (libvirt.libvirtError, AttributeError):
7789             pass
7790         return output
7791 
7792     def get_instance_diagnostics(self, instance):
7793         guest = self._host.get_guest(instance)
7794 
7795         # TODO(sahid): We are converting all calls from a
7796         # virDomain object to use nova.virt.libvirt.Guest.
7797         # We should be able to remove domain at the end.
7798         domain = guest._domain
7799 
7800         xml = guest.get_xml_desc()
7801         xml_doc = etree.fromstring(xml)
7802 
7803         # TODO(sahid): Needs to use get_info but more changes have to
7804         # be done since a mapping STATE_MAP LIBVIRT_POWER_STATE is
7805         # needed.
7806         (state, max_mem, mem, num_cpu, cpu_time) = \
7807             guest._get_domain_info(self._host)
7808         config_drive = configdrive.required_by(instance)
7809         launched_at = timeutils.normalize_time(instance.launched_at)
7810         uptime = timeutils.delta_seconds(launched_at,
7811                                          timeutils.utcnow())
7812         diags = diagnostics_obj.Diagnostics(state=power_state.STATE_MAP[state],
7813                                         driver='libvirt',
7814                                         config_drive=config_drive,
7815                                         hypervisor=CONF.libvirt.virt_type,
7816                                         hypervisor_os='linux',
7817                                         uptime=uptime)
7818         diags.memory_details = diagnostics_obj.MemoryDiagnostics(
7819             maximum=max_mem / units.Mi,
7820             used=mem / units.Mi)
7821 
7822         # get cpu time, might launch an exception if the method
7823         # is not supported by the underlying hypervisor being
7824         # used by libvirt
7825         try:
7826             for vcpu in guest.get_vcpus_info():
7827                 diags.add_cpu(id=vcpu.id, time=vcpu.time)
7828         except libvirt.libvirtError:
7829             pass
7830         # get io status
7831         dom_io = LibvirtDriver._get_io_devices(xml)
7832         for guest_disk in dom_io["volumes"]:
7833             try:
7834                 # blockStats might launch an exception if the method
7835                 # is not supported by the underlying hypervisor being
7836                 # used by libvirt
7837                 stats = domain.blockStats(guest_disk)
7838                 diags.add_disk(read_bytes=stats[1],
7839                                read_requests=stats[0],
7840                                write_bytes=stats[3],
7841                                write_requests=stats[2],
7842                                errors_count=stats[4])
7843             except libvirt.libvirtError:
7844                 pass
7845         for interface in dom_io["ifaces"]:
7846             try:
7847                 # interfaceStats might launch an exception if the method
7848                 # is not supported by the underlying hypervisor being
7849                 # used by libvirt
7850                 stats = domain.interfaceStats(interface)
7851                 diags.add_nic(rx_octets=stats[0],
7852                               rx_errors=stats[2],
7853                               rx_drop=stats[3],
7854                               rx_packets=stats[1],
7855                               tx_octets=stats[4],
7856                               tx_errors=stats[6],
7857                               tx_drop=stats[7],
7858                               tx_packets=stats[5])
7859             except libvirt.libvirtError:
7860                 pass
7861 
7862         # Update mac addresses of interface if stats have been reported
7863         if diags.nic_details:
7864             nodes = xml_doc.findall('./devices/interface/mac')
7865             for index, node in enumerate(nodes):
7866                 diags.nic_details[index].mac_address = node.get('address')
7867         return diags
7868 
7869     @staticmethod
7870     def _prepare_device_bus(dev):
7871         """Determines the device bus and its hypervisor assigned address
7872         """
7873         bus = None
7874         address = (dev.device_addr.format_address() if
7875                    dev.device_addr else None)
7876         if isinstance(dev.device_addr,
7877                       vconfig.LibvirtConfigGuestDeviceAddressPCI):
7878             bus = objects.PCIDeviceBus()
7879         elif isinstance(dev, vconfig.LibvirtConfigGuestDisk):
7880             if dev.target_bus == 'scsi':
7881                 bus = objects.SCSIDeviceBus()
7882             elif dev.target_bus == 'ide':
7883                 bus = objects.IDEDeviceBus()
7884             elif dev.target_bus == 'usb':
7885                 bus = objects.USBDeviceBus()
7886         if address is not None and bus is not None:
7887             bus.address = address
7888         return bus
7889 
7890     def _build_device_metadata(self, context, instance):
7891         """Builds a metadata object for instance devices, that maps the user
7892            provided tag to the hypervisor assigned device address.
7893         """
7894         def _get_device_name(bdm):
7895             return block_device.strip_dev(bdm.device_name)
7896 
7897         network_info = instance.info_cache.network_info
7898         vlans_by_mac = netutils.get_cached_vifs_with_vlan(network_info)
7899         vifs = objects.VirtualInterfaceList.get_by_instance_uuid(context,
7900                                                                  instance.uuid)
7901         vifs_to_expose = {vif.address: vif for vif in vifs
7902                           if ('tag' in vif and vif.tag) or
7903                              vlans_by_mac.get(vif.address)}
7904         # TODO(mriedem): We should be able to avoid the DB query here by using
7905         # block_device_info['block_device_mapping'] which is passed into most
7906         # methods that call this function.
7907         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
7908             context, instance.uuid)
7909         tagged_bdms = {_get_device_name(bdm): bdm for bdm in bdms if bdm.tag}
7910 
7911         devices = []
7912         guest = self._host.get_guest(instance)
7913         xml = guest.get_xml_desc()
7914         xml_dom = etree.fromstring(xml)
7915         guest_config = vconfig.LibvirtConfigGuest()
7916         guest_config.parse_dom(xml_dom)
7917 
7918         for dev in guest_config.devices:
7919             # Build network interfaces related metadata
7920             if isinstance(dev, vconfig.LibvirtConfigGuestInterface):
7921                 vif = vifs_to_expose.get(dev.mac_addr)
7922                 if not vif:
7923                     continue
7924                 bus = self._prepare_device_bus(dev)
7925                 device = objects.NetworkInterfaceMetadata(mac=vif.address)
7926                 if 'tag' in vif and vif.tag:
7927                     device.tags = [vif.tag]
7928                 if bus:
7929                     device.bus = bus
7930                 vlan = vlans_by_mac.get(vif.address)
7931                 if vlan:
7932                     device.vlan = int(vlan)
7933                 devices.append(device)
7934 
7935             # Build disks related metadata
7936             if isinstance(dev, vconfig.LibvirtConfigGuestDisk):
7937                 bdm = tagged_bdms.get(dev.target_dev)
7938                 if not bdm:
7939                     continue
7940                 bus = self._prepare_device_bus(dev)
7941                 device = objects.DiskMetadata(tags=[bdm.tag])
7942                 # NOTE(artom) Setting the serial (which corresponds to
7943                 # volume_id in BlockDeviceMapping) in DiskMetadata allows us to
7944                 # find the disks's BlockDeviceMapping object when we detach the
7945                 # volume and want to clean up its metadata.
7946                 device.serial = bdm.volume_id
7947                 if bus:
7948                     device.bus = bus
7949                 devices.append(device)
7950         if devices:
7951             dev_meta = objects.InstanceDeviceMetadata(devices=devices)
7952             return dev_meta
7953 
7954     def instance_on_disk(self, instance):
7955         # ensure directories exist and are writable
7956         instance_path = libvirt_utils.get_instance_path(instance)
7957         LOG.debug('Checking instance files accessibility %s', instance_path,
7958                   instance=instance)
7959         shared_instance_path = os.access(instance_path, os.W_OK)
7960         # NOTE(flwang): For shared block storage scenario, the file system is
7961         # not really shared by the two hosts, but the volume of evacuated
7962         # instance is reachable.
7963         shared_block_storage = (self.image_backend.backend().
7964                                 is_shared_block_storage())
7965         return shared_instance_path or shared_block_storage
7966 
7967     def inject_network_info(self, instance, nw_info):
7968         self.firewall_driver.setup_basic_filtering(instance, nw_info)
7969 
7970     def delete_instance_files(self, instance):
7971         target = libvirt_utils.get_instance_path(instance)
7972         # A resize may be in progress
7973         target_resize = target + '_resize'
7974         # Other threads may attempt to rename the path, so renaming the path
7975         # to target + '_del' (because it is atomic) and iterating through
7976         # twice in the unlikely event that a concurrent rename occurs between
7977         # the two rename attempts in this method. In general this method
7978         # should be fairly thread-safe without these additional checks, since
7979         # other operations involving renames are not permitted when the task
7980         # state is not None and the task state should be set to something
7981         # other than None by the time this method is invoked.
7982         target_del = target + '_del'
7983         for i in range(2):
7984             try:
7985                 utils.execute('mv', target, target_del)
7986                 break
7987             except Exception:
7988                 pass
7989             try:
7990                 utils.execute('mv', target_resize, target_del)
7991                 break
7992             except Exception:
7993                 pass
7994         # Either the target or target_resize path may still exist if all
7995         # rename attempts failed.
7996         remaining_path = None
7997         for p in (target, target_resize):
7998             if os.path.exists(p):
7999                 remaining_path = p
8000                 break
8001 
8002         # A previous delete attempt may have been interrupted, so target_del
8003         # may exist even if all rename attempts during the present method
8004         # invocation failed due to the absence of both target and
8005         # target_resize.
8006         if not remaining_path and os.path.exists(target_del):
8007             self.job_tracker.terminate_jobs(instance)
8008 
8009             LOG.info('Deleting instance files %s', target_del,
8010                      instance=instance)
8011             remaining_path = target_del
8012             try:
8013                 shutil.rmtree(target_del)
8014             except OSError as e:
8015                 LOG.error('Failed to cleanup directory %(target)s: %(e)s',
8016                           {'target': target_del, 'e': e}, instance=instance)
8017 
8018         # It is possible that the delete failed, if so don't mark the instance
8019         # as cleaned.
8020         if remaining_path and os.path.exists(remaining_path):
8021             LOG.info('Deletion of %s failed', remaining_path,
8022                      instance=instance)
8023             return False
8024 
8025         LOG.info('Deletion of %s complete', target_del, instance=instance)
8026         return True
8027 
8028     @property
8029     def need_legacy_block_device_info(self):
8030         return False
8031 
8032     def default_root_device_name(self, instance, image_meta, root_bdm):
8033         disk_bus = blockinfo.get_disk_bus_for_device_type(
8034             instance, CONF.libvirt.virt_type, image_meta, "disk")
8035         cdrom_bus = blockinfo.get_disk_bus_for_device_type(
8036             instance, CONF.libvirt.virt_type, image_meta, "cdrom")
8037         root_info = blockinfo.get_root_info(
8038             instance, CONF.libvirt.virt_type, image_meta,
8039             root_bdm, disk_bus, cdrom_bus)
8040         return block_device.prepend_dev(root_info['dev'])
8041 
8042     def default_device_names_for_instance(self, instance, root_device_name,
8043                                           *block_device_lists):
8044         block_device_mapping = list(itertools.chain(*block_device_lists))
8045         # NOTE(ndipanov): Null out the device names so that blockinfo code
8046         #                 will assign them
8047         for bdm in block_device_mapping:
8048             if bdm.device_name is not None:
8049                 LOG.warning(
8050                     "Ignoring supplied device name: %(device_name)s. "
8051                     "Libvirt can't honour user-supplied dev names",
8052                     {'device_name': bdm.device_name}, instance=instance)
8053                 bdm.device_name = None
8054         block_device_info = driver.get_block_device_info(instance,
8055                                                          block_device_mapping)
8056 
8057         blockinfo.default_device_names(CONF.libvirt.virt_type,
8058                                        nova_context.get_admin_context(),
8059                                        instance,
8060                                        block_device_info,
8061                                        instance.image_meta)
8062 
8063     def get_device_name_for_instance(self, instance, bdms, block_device_obj):
8064         block_device_info = driver.get_block_device_info(instance, bdms)
8065         instance_info = blockinfo.get_disk_info(
8066                 CONF.libvirt.virt_type, instance,
8067                 instance.image_meta, block_device_info=block_device_info)
8068 
8069         suggested_dev_name = block_device_obj.device_name
8070         if suggested_dev_name is not None:
8071             LOG.warning(
8072                 'Ignoring supplied device name: %(suggested_dev)s',
8073                 {'suggested_dev': suggested_dev_name}, instance=instance)
8074 
8075         # NOTE(ndipanov): get_info_from_bdm will generate the new device name
8076         #                 only when it's actually not set on the bd object
8077         block_device_obj.device_name = None
8078         disk_info = blockinfo.get_info_from_bdm(
8079             instance, CONF.libvirt.virt_type, instance.image_meta,
8080             block_device_obj, mapping=instance_info['mapping'])
8081         return block_device.prepend_dev(disk_info['dev'])
8082 
8083     def is_supported_fs_format(self, fs_type):
8084         return fs_type in [disk_api.FS_FORMAT_EXT2, disk_api.FS_FORMAT_EXT3,
8085                            disk_api.FS_FORMAT_EXT4, disk_api.FS_FORMAT_XFS]
