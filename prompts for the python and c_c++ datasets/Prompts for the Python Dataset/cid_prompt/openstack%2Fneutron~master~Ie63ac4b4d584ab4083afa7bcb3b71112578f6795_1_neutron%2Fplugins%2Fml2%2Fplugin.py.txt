Please review the code below for security defects. You can consider defect types in terms of:
1.CWE-284 (Improper Access Control)
2.CWE-435 (Improper Interaction Between Multiple Entities)
3.CWE-664 (Improper Control of a Resource Through its Lifetime)
4.CWE-682 (Incorrect Calculation)
5.CWE-691 (Insufficient Control Flow Management)
6.CWE-693 (Protection Mechanism Failure)
7.CWE-697 (Incorrect Comparison)
8.CWE-703 (Improper Check or Handling of Exceptional Conditions)
9.CWE-707 (Improper Neutralization)
10.CWE-710 (Improper Adherence to Coding Standards)
If any are found, please describe the security defect in detail and indicate the corresponding line number of code and solution. If none are detected, states: 'No security defects are detected in the code'.

1 # Copyright (c) 2013 OpenStack Foundation
2 # All Rights Reserved.
3 #
4 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
5 #    not use this file except in compliance with the License. You may obtain
6 #    a copy of the License at
7 #
8 #         http://www.apache.org/licenses/LICENSE-2.0
9 #
10 #    Unless required by applicable law or agreed to in writing, software
11 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
12 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
13 #    License for the specific language governing permissions and limitations
14 #    under the License.
15 
16 from eventlet import greenthread
17 from neutron_lib.api.definitions import extra_dhcp_opt as edo_ext
18 from neutron_lib.api.definitions import port_security as psec
19 from neutron_lib.api.definitions import portbindings
20 from neutron_lib.api.definitions import provider_net
21 from neutron_lib.api import validators
22 from neutron_lib.callbacks import events
23 from neutron_lib.callbacks import exceptions
24 from neutron_lib.callbacks import registry
25 from neutron_lib.callbacks import resources
26 from neutron_lib import constants as const
27 from neutron_lib import exceptions as exc
28 from neutron_lib.exceptions import port_security as psec_exc
29 from neutron_lib.plugins import directory
30 from neutron_lib.plugins.ml2 import api
31 from oslo_config import cfg
32 from oslo_db import exception as os_db_exception
33 from oslo_log import helpers as log_helpers
34 from oslo_log import log
35 from oslo_serialization import jsonutils
36 from oslo_utils import excutils
37 from oslo_utils import importutils
38 from oslo_utils import uuidutils
39 import sqlalchemy
40 from sqlalchemy.orm import exc as sa_exc
41 
42 from neutron._i18n import _, _LE, _LI, _LW
43 from neutron.agent import securitygroups_rpc as sg_rpc
44 from neutron.api.rpc.agentnotifiers import dhcp_rpc_agent_api
45 from neutron.api.rpc.handlers import dhcp_rpc
46 from neutron.api.rpc.handlers import dvr_rpc
47 from neutron.api.rpc.handlers import metadata_rpc
48 from neutron.api.rpc.handlers import resources_rpc
49 from neutron.api.rpc.handlers import securitygroups_rpc
50 from neutron.api.v2 import attributes
51 from neutron.common import constants as n_const
52 from neutron.common import rpc as n_rpc
53 from neutron.common import topics
54 from neutron.common import utils
55 from neutron.db import _model_query as model_query
56 from neutron.db import _resource_extend as resource_extend
57 from neutron.db import _utils as db_utils
58 from neutron.db import address_scope_db
59 from neutron.db import agents_db
60 from neutron.db import agentschedulers_db
61 from neutron.db import allowedaddresspairs_db as addr_pair_db
62 from neutron.db import api as db_api
63 from neutron.db import db_base_plugin_v2
64 from neutron.db import dvr_mac_db
65 from neutron.db import external_net_db
66 from neutron.db import extradhcpopt_db
67 from neutron.db.models import securitygroup as sg_models
68 from neutron.db import models_v2
69 from neutron.db import provisioning_blocks
70 from neutron.db.quota import driver  # noqa
71 from neutron.db import securitygroups_rpc_base as sg_db_rpc
72 from neutron.db import segments_db
73 from neutron.db import subnet_service_type_db_models as service_type_db
74 from neutron.db import vlantransparent_db
75 from neutron.extensions import allowedaddresspairs as addr_pair
76 from neutron.extensions import availability_zone as az_ext
77 from neutron.extensions import multiprovidernet as mpnet
78 from neutron.extensions import providernet as provider
79 from neutron.extensions import vlantransparent
80 from neutron.plugins.ml2.common import exceptions as ml2_exc
81 from neutron.plugins.ml2 import config  # noqa
82 from neutron.plugins.ml2 import db
83 from neutron.plugins.ml2 import driver_context
84 from neutron.plugins.ml2.extensions import qos as qos_ext
85 from neutron.plugins.ml2 import managers
86 from neutron.plugins.ml2 import models
87 from neutron.plugins.ml2 import ovo_rpc
88 from neutron.plugins.ml2 import rpc
89 from neutron.quota import resource_registry
90 from neutron.services.qos import qos_consts
91 from neutron.services.segments import plugin as segments_plugin
92 
93 LOG = log.getLogger(__name__)
94 
95 MAX_BIND_TRIES = 10
96 
97 
98 SERVICE_PLUGINS_REQUIRED_DRIVERS = {
99     'qos': [qos_ext.QOS_EXT_DRIVER_ALIAS]
100 }
101 
102 
103 def _ml2_port_result_filter_hook(query, filters):
104     values = filters and filters.get(portbindings.HOST_ID, [])
105     if not values:
106         return query
107     bind_criteria = models.PortBinding.host.in_(values)
108     return query.filter(models_v2.Port.port_binding.has(bind_criteria))
109 
110 
111 @resource_extend.has_resource_extenders
112 @registry.has_registry_receivers
113 class Ml2Plugin(db_base_plugin_v2.NeutronDbPluginV2,
114                 dvr_mac_db.DVRDbMixin,
115                 external_net_db.External_net_db_mixin,
116                 sg_db_rpc.SecurityGroupServerRpcMixin,
117                 agentschedulers_db.AZDhcpAgentSchedulerDbMixin,
118                 addr_pair_db.AllowedAddressPairsMixin,
119                 vlantransparent_db.Vlantransparent_db_mixin,
120                 extradhcpopt_db.ExtraDhcpOptMixin,
121                 address_scope_db.AddressScopeDbMixin,
122                 service_type_db.SubnetServiceTypeMixin):
123 
124     """Implement the Neutron L2 abstractions using modules.
125 
126     Ml2Plugin is a Neutron plugin based on separately extensible sets
127     of network types and mechanisms for connecting to networks of
128     those types. The network types and mechanisms are implemented as
129     drivers loaded via Python entry points. Networks can be made up of
130     multiple segments (not yet fully implemented).
131     """
132 
133     # This attribute specifies whether the plugin supports or not
134     # bulk/pagination/sorting operations. Name mangling is used in
135     # order to ensure it is qualified by class
136     __native_bulk_support = True
137     __native_pagination_support = True
138     __native_sorting_support = True
139 
140     # List of supported extensions
141     _supported_extension_aliases = ["provider", "external-net", "binding",
142                                     "quotas", "security-group", "agent",
143                                     "dhcp_agent_scheduler",
144                                     "multi-provider", "allowed-address-pairs",
145                                     "extra_dhcp_opt", "subnet_allocation",
146                                     "net-mtu", "vlan-transparent",
147                                     "address-scope",
148                                     "availability_zone",
149                                     "network_availability_zone",
150                                     "default-subnetpools",
151                                     "subnet-service-types"]
152 
153     @property
154     def supported_extension_aliases(self):
155         if not hasattr(self, '_aliases'):
156             aliases = self._supported_extension_aliases[:]
157             aliases += self.extension_manager.extension_aliases()
158             sg_rpc.disable_security_group_extension_by_config(aliases)
159             vlantransparent.disable_extension_by_config(aliases)
160             self._aliases = aliases
161         return self._aliases
162 
163     def __new__(cls, *args, **kwargs):
164         model_query.register_hook(
165             models_v2.Port,
166             "ml2_port_bindings",
167             query_hook=None,
168             filter_hook=None,
169             result_filters=_ml2_port_result_filter_hook)
170         return super(Ml2Plugin, cls).__new__(cls, *args, **kwargs)
171 
172     @resource_registry.tracked_resources(
173         network=models_v2.Network,
174         port=models_v2.Port,
175         subnet=models_v2.Subnet,
176         subnetpool=models_v2.SubnetPool,
177         security_group=sg_models.SecurityGroup,
178         security_group_rule=sg_models.SecurityGroupRule)
179     def __init__(self):
180         # First load drivers, then initialize DB, then initialize drivers
181         self.type_manager = managers.TypeManager()
182         self.extension_manager = managers.ExtensionManager()
183         self.mechanism_manager = managers.MechanismManager()
184         super(Ml2Plugin, self).__init__()
185         self.type_manager.initialize()
186         self.extension_manager.initialize()
187         self.mechanism_manager.initialize()
188         self._setup_dhcp()
189         self._start_rpc_notifiers()
190         self.add_agent_status_check_worker(self.agent_health_check)
191         self.add_workers(self.mechanism_manager.get_workers())
192         self._verify_service_plugins_requirements()
193         LOG.info(_LI("Modular L2 Plugin initialization complete"))
194 
195     def _setup_rpc(self):
196         """Initialize components to support agent communication."""
197         self.endpoints = [
198             rpc.RpcCallbacks(self.notifier, self.type_manager),
199             securitygroups_rpc.SecurityGroupServerRpcCallback(),
200             dvr_rpc.DVRServerRpcCallback(),
201             dhcp_rpc.DhcpRpcCallback(),
202             agents_db.AgentExtRpcCallback(),
203             metadata_rpc.MetadataRpcCallback(),
204             resources_rpc.ResourcesPullRpcCallback()
205         ]
206 
207     def _setup_dhcp(self):
208         """Initialize components to support DHCP."""
209         self.network_scheduler = importutils.import_object(
210             cfg.CONF.network_scheduler_driver
211         )
212         self.add_periodic_dhcp_agent_status_check()
213 
214     def _verify_service_plugins_requirements(self):
215         for service_plugin in cfg.CONF.service_plugins:
216             extension_drivers = SERVICE_PLUGINS_REQUIRED_DRIVERS.get(
217                 service_plugin, []
218             )
219             for extension_driver in extension_drivers:
220                 if extension_driver not in self.extension_manager.names():
221                     raise ml2_exc.ExtensionDriverNotFound(
222                         driver=extension_driver, service_plugin=service_plugin
223                     )
224 
225     @registry.receives(resources.PORT,
226                        [provisioning_blocks.PROVISIONING_COMPLETE])
227     def _port_provisioned(self, rtype, event, trigger, context, object_id,
228                           **kwargs):
229         port_id = object_id
230         port = db.get_port(context, port_id)
231         if not port or not port.port_binding:
232             LOG.debug("Port %s was deleted so its status cannot be updated.",
233                       port_id)
234             return
235         if port.port_binding.vif_type in (portbindings.VIF_TYPE_BINDING_FAILED,
236                                           portbindings.VIF_TYPE_UNBOUND):
237             # NOTE(kevinbenton): we hit here when a port is created without
238             # a host ID and the dhcp agent notifies that its wiring is done
239             LOG.debug("Port %s cannot update to ACTIVE because it "
240                       "is not bound.", port_id)
241             return
242         else:
243             # port is bound, but we have to check for new provisioning blocks
244             # one last time to detect the case where we were triggered by an
245             # unbound port and the port became bound with new provisioning
246             # blocks before 'get_port' was called above
247             if provisioning_blocks.is_object_blocked(context, port_id,
248                                                      resources.PORT):
249                 LOG.debug("Port %s had new provisioning blocks added so it "
250                           "will not transition to active.", port_id)
251                 return
252         self.update_port_status(context, port_id, const.PORT_STATUS_ACTIVE)
253 
254     @log_helpers.log_method_call
255     def _start_rpc_notifiers(self):
256         """Initialize RPC notifiers for agents."""
257         self.ovo_notifier = ovo_rpc.OVOServerRpcInterface()
258         self.notifier = rpc.AgentNotifierApi(topics.AGENT)
259         self.agent_notifiers[const.AGENT_TYPE_DHCP] = (
260             dhcp_rpc_agent_api.DhcpAgentNotifyAPI()
261         )
262 
263     @log_helpers.log_method_call
264     def start_rpc_listeners(self):
265         """Start the RPC loop to let the plugin communicate with agents."""
266         self._setup_rpc()
267         self.topic = topics.PLUGIN
268         self.conn = n_rpc.create_connection()
269         self.conn.create_consumer(self.topic, self.endpoints, fanout=False)
270         self.conn.create_consumer(
271             topics.SERVER_RESOURCE_VERSIONS,
272             [resources_rpc.ResourcesPushToServerRpcCallback()],
273             fanout=True)
274         # process state reports despite dedicated rpc workers
275         self.conn.create_consumer(topics.REPORTS,
276                                   [agents_db.AgentExtRpcCallback()],
277                                   fanout=False)
278         return self.conn.consume_in_threads()
279 
280     def start_rpc_state_reports_listener(self):
281         self.conn_reports = n_rpc.create_connection()
282         self.conn_reports.create_consumer(topics.REPORTS,
283                                           [agents_db.AgentExtRpcCallback()],
284                                           fanout=False)
285         return self.conn_reports.consume_in_threads()
286 
287     def _filter_nets_provider(self, context, networks, filters):
288         return [network
289                 for network in networks
290                 if self.type_manager.network_matches_filters(network, filters)
291                 ]
292 
293     def _check_mac_update_allowed(self, orig_port, port, binding):
294         unplugged_types = (portbindings.VIF_TYPE_BINDING_FAILED,
295                            portbindings.VIF_TYPE_UNBOUND)
296         new_mac = port.get('mac_address')
297         mac_change = (new_mac is not None and
298                       orig_port['mac_address'] != new_mac)
299         if (mac_change and binding.vif_type not in unplugged_types):
300             raise exc.PortBound(port_id=orig_port['id'],
301                                 vif_type=binding.vif_type,
302                                 old_mac=orig_port['mac_address'],
303                                 new_mac=port['mac_address'])
304         return mac_change
305 
306     def _process_port_binding(self, mech_context, attrs):
307         plugin_context = mech_context._plugin_context
308         binding = mech_context._binding
309         port = mech_context.current
310         port_id = port['id']
311         changes = False
312 
313         host = const.ATTR_NOT_SPECIFIED
314         if attrs and portbindings.HOST_ID in attrs:
315             host = attrs.get(portbindings.HOST_ID) or ''
316 
317         original_host = binding.host
318         if (validators.is_attr_set(host) and
319             original_host != host):
320             binding.host = host
321             changes = True
322 
323         vnic_type = attrs and attrs.get(portbindings.VNIC_TYPE)
324         if (validators.is_attr_set(vnic_type) and
325             binding.vnic_type != vnic_type):
326             binding.vnic_type = vnic_type
327             changes = True
328 
329         # treat None as clear of profile.
330         profile = None
331         if attrs and portbindings.PROFILE in attrs:
332             profile = attrs.get(portbindings.PROFILE) or {}
333 
334         if profile not in (None, const.ATTR_NOT_SPECIFIED,
335                            self._get_profile(binding)):
336             binding.profile = jsonutils.dumps(profile)
337             if len(binding.profile) > models.BINDING_PROFILE_LEN:
338                 msg = _("binding:profile value too large")
339                 raise exc.InvalidInput(error_message=msg)
340             changes = True
341 
342         # Unbind the port if needed.
343         if changes:
344             binding.vif_type = portbindings.VIF_TYPE_UNBOUND
345             binding.vif_details = ''
346             db.clear_binding_levels(plugin_context, port_id, original_host)
347             mech_context._clear_binding_levels()
348             port['status'] = const.PORT_STATUS_DOWN
349             super(Ml2Plugin, self).update_port(
350                 mech_context._plugin_context, port_id,
351                 {attributes.PORT: {'status': const.PORT_STATUS_DOWN}})
352 
353         if port['device_owner'] == const.DEVICE_OWNER_DVR_INTERFACE:
354             binding.vif_type = portbindings.VIF_TYPE_UNBOUND
355             binding.vif_details = ''
356             db.clear_binding_levels(plugin_context, port_id, original_host)
357             mech_context._clear_binding_levels()
358             binding.host = ''
359 
360         self._update_port_dict_binding(port, binding)
361         binding.persist_state_to_session(plugin_context.session)
362         return changes
363 
364     @db_api.retry_db_errors
365     def _bind_port_if_needed(self, context, allow_notify=False,
366                              need_notify=False):
367         if not context.network.network_segments:
368             LOG.debug("Network %s has no segments, skipping binding",
369                       context.network.current['id'])
370             return context
371         for count in range(1, MAX_BIND_TRIES + 1):
372             if count > 1:
373                 # yield for binding retries so that we give other threads a
374                 # chance to do their work
375                 greenthread.sleep(0)
376 
377                 # multiple attempts shouldn't happen very often so we log each
378                 # attempt after the 1st.
379                 LOG.info(_LI("Attempt %(count)s to bind port %(port)s"),
380                          {'count': count, 'port': context.current['id']})
381 
382             bind_context, need_notify, try_again = self._attempt_binding(
383                 context, need_notify)
384 
385             if count == MAX_BIND_TRIES or not try_again:
386                 if self._should_bind_port(context):
387                     # At this point, we attempted to bind a port and reached
388                     # its final binding state. Binding either succeeded or
389                     # exhausted all attempts, thus no need to try again.
390                     # Now, the port and its binding state should be committed.
391                     context, need_notify, try_again = (
392                         self._commit_port_binding(context, bind_context,
393                                                   need_notify, try_again))
394                 else:
395                     context = bind_context
396 
397             if not try_again:
398                 if allow_notify and need_notify:
399                     self._notify_port_updated(context)
400                 return context
401 
402         LOG.error(_LE("Failed to commit binding results for %(port)s "
403                       "after %(max)s tries"),
404                   {'port': context.current['id'], 'max': MAX_BIND_TRIES})
405         return context
406 
407     def _should_bind_port(self, context):
408         return (context._binding.host and context._binding.vif_type
409                 in (portbindings.VIF_TYPE_UNBOUND,
410                     portbindings.VIF_TYPE_BINDING_FAILED))
411 
412     def _attempt_binding(self, context, need_notify):
413         try_again = False
414 
415         if self._should_bind_port(context):
416             bind_context = self._bind_port(context)
417 
418             if bind_context.vif_type != portbindings.VIF_TYPE_BINDING_FAILED:
419                 # Binding succeeded. Suggest notifying of successful binding.
420                 need_notify = True
421             else:
422                 # Current attempt binding failed, try to bind again.
423                 try_again = True
424             context = bind_context
425 
426         return context, need_notify, try_again
427 
428     def _bind_port(self, orig_context):
429         # Construct a new PortContext from the one from the previous
430         # transaction.
431         port = orig_context.current
432         orig_binding = orig_context._binding
433         new_binding = models.PortBinding(
434             host=orig_binding.host,
435             vnic_type=orig_binding.vnic_type,
436             profile=orig_binding.profile,
437             vif_type=portbindings.VIF_TYPE_UNBOUND,
438             vif_details=''
439         )
440         self._update_port_dict_binding(port, new_binding)
441         new_context = driver_context.PortContext(
442             self, orig_context._plugin_context, port,
443             orig_context.network.current, new_binding, None,
444             original_port=orig_context.original)
445 
446         # Attempt to bind the port and return the context with the
447         # result.
448         self.mechanism_manager.bind_port(new_context)
449         return new_context
450 
451     def _commit_port_binding(self, orig_context, bind_context,
452                              need_notify, try_again):
453         port_id = orig_context.current['id']
454         plugin_context = orig_context._plugin_context
455         orig_binding = orig_context._binding
456         new_binding = bind_context._binding
457 
458         # After we've attempted to bind the port, we begin a
459         # transaction, get the current port state, and decide whether
460         # to commit the binding results.
461         with db_api.context_manager.writer.using(plugin_context):
462             # Get the current port state and build a new PortContext
463             # reflecting this state as original state for subsequent
464             # mechanism driver update_port_*commit() calls.
465             try:
466                 port_db = self._get_port(plugin_context, port_id)
467                 cur_binding = port_db.port_binding
468             except exc.PortNotFound:
469                 port_db, cur_binding = None, None
470             if not port_db or not cur_binding:
471                 # The port has been deleted concurrently, so just
472                 # return the unbound result from the initial
473                 # transaction that completed before the deletion.
474                 LOG.debug("Port %s has been deleted concurrently", port_id)
475                 return orig_context, False, False
476             # Since the mechanism driver bind_port() calls must be made
477             # outside a DB transaction locking the port state, it is
478             # possible (but unlikely) that the port's state could change
479             # concurrently while these calls are being made. If another
480             # thread or process succeeds in binding the port before this
481             # thread commits its results, the already committed results are
482             # used. If attributes such as binding:host_id, binding:profile,
483             # or binding:vnic_type are updated concurrently, the try_again
484             # flag is returned to indicate that the commit was unsuccessful.
485             oport = self._make_port_dict(port_db)
486             port = self._make_port_dict(port_db)
487             network = bind_context.network.current
488             if port['device_owner'] == const.DEVICE_OWNER_DVR_INTERFACE:
489                 # REVISIT(rkukura): The PortBinding instance from the
490                 # ml2_port_bindings table, returned as cur_binding
491                 # from port_db.port_binding above, is
492                 # currently not used for DVR distributed ports, and is
493                 # replaced here with the DistributedPortBinding instance from
494                 # the ml2_distributed_port_bindings table specific to the host
495                 # on which the distributed port is being bound. It
496                 # would be possible to optimize this code to avoid
497                 # fetching the PortBinding instance in the DVR case,
498                 # and even to avoid creating the unused entry in the
499                 # ml2_port_bindings table. But the upcoming resolution
500                 # for bug 1367391 will eliminate the
501                 # ml2_distributed_port_bindings table, use the
502                 # ml2_port_bindings table to store non-host-specific
503                 # fields for both distributed and non-distributed
504                 # ports, and introduce a new ml2_port_binding_hosts
505                 # table for the fields that need to be host-specific
506                 # in the distributed case. Since the PortBinding
507                 # instance will then be needed, it does not make sense
508                 # to optimize this code to avoid fetching it.
509                 cur_binding = db.get_distributed_port_binding_by_host(
510                     plugin_context, port_id, orig_binding.host)
511             cur_context = driver_context.PortContext(
512                 self, plugin_context, port, network, cur_binding, None,
513                 original_port=oport)
514 
515             # Commit our binding results only if port has not been
516             # successfully bound concurrently by another thread or
517             # process and no binding inputs have been changed.
518             commit = ((cur_binding.vif_type in
519                        [portbindings.VIF_TYPE_UNBOUND,
520                         portbindings.VIF_TYPE_BINDING_FAILED]) and
521                       orig_binding.host == cur_binding.host and
522                       orig_binding.vnic_type == cur_binding.vnic_type and
523                       orig_binding.profile == cur_binding.profile)
524 
525             if commit:
526                 # Update the port's binding state with our binding
527                 # results.
528                 cur_binding.vif_type = new_binding.vif_type
529                 cur_binding.vif_details = new_binding.vif_details
530                 db.clear_binding_levels(plugin_context, port_id,
531                                         cur_binding.host)
532                 db.set_binding_levels(plugin_context,
533                                       bind_context._binding_levels)
534                 # refresh context with a snapshot of updated state
535                 cur_context._binding = driver_context.InstanceSnapshot(
536                     cur_binding)
537                 cur_context._binding_levels = bind_context._binding_levels
538 
539                 # Update PortContext's port dictionary to reflect the
540                 # updated binding state.
541                 self._update_port_dict_binding(port, cur_binding)
542 
543                 # Update the port status if requested by the bound driver.
544                 if (bind_context._binding_levels and
545                     bind_context._new_port_status):
546                     port_db.status = bind_context._new_port_status
547                     port['status'] = bind_context._new_port_status
548 
549                 # Call the mechanism driver precommit methods, commit
550                 # the results, and call the postcommit methods.
551                 self.mechanism_manager.update_port_precommit(cur_context)
552         if commit:
553             # Continue, using the port state as of the transaction that
554             # just finished, whether that transaction committed new
555             # results or discovered concurrent port state changes.
556             # Also, Trigger notification for successful binding commit.
557             kwargs = {
558                 'context': plugin_context,
559                 'port': self._make_port_dict(port_db),  # ensure latest state
560                 'mac_address_updated': False,
561                 'original_port': oport,
562             }
563             registry.notify(resources.PORT, events.AFTER_UPDATE,
564                             self, **kwargs)
565             self.mechanism_manager.update_port_postcommit(cur_context)
566             need_notify = True
567             try_again = False
568         else:
569             try_again = True
570 
571         return cur_context, need_notify, try_again
572 
573     def _update_port_dict_binding(self, port, binding):
574         port[portbindings.VNIC_TYPE] = binding.vnic_type
575         port[portbindings.PROFILE] = self._get_profile(binding)
576         if port['device_owner'] == const.DEVICE_OWNER_DVR_INTERFACE:
577             port[portbindings.HOST_ID] = ''
578             port[portbindings.VIF_TYPE] = portbindings.VIF_TYPE_DISTRIBUTED
579             port[portbindings.VIF_DETAILS] = {}
580         else:
581             port[portbindings.HOST_ID] = binding.host
582             port[portbindings.VIF_TYPE] = binding.vif_type
583             port[portbindings.VIF_DETAILS] = self._get_vif_details(binding)
584 
585     def _get_vif_details(self, binding):
586         if binding.vif_details:
587             try:
588                 return jsonutils.loads(binding.vif_details)
589             except Exception:
590                 LOG.error(_LE("Serialized vif_details DB value '%(value)s' "
591                               "for port %(port)s is invalid"),
592                           {'value': binding.vif_details,
593                            'port': binding.port_id})
594         return {}
595 
596     def _get_profile(self, binding):
597         if binding.profile:
598             try:
599                 return jsonutils.loads(binding.profile)
600             except Exception:
601                 LOG.error(_LE("Serialized profile DB value '%(value)s' for "
602                               "port %(port)s is invalid"),
603                           {'value': binding.profile,
604                            'port': binding.port_id})
605         return {}
606 
607     @staticmethod
608     @resource_extend.extends([attributes.PORTS])
609     def _ml2_extend_port_dict_binding(port_res, port_db):
610         plugin = directory.get_plugin()
611         # None when called during unit tests for other plugins.
612         if port_db.port_binding:
613             plugin._update_port_dict_binding(port_res, port_db.port_binding)
614 
615     # ML2's resource extend functions allow extension drivers that extend
616     # attributes for the resources to add those attributes to the result.
617 
618     @staticmethod
619     @resource_extend.extends([attributes.NETWORKS])
620     def _ml2_md_extend_network_dict(result, netdb):
621         plugin = directory.get_plugin()
622         session = plugin._object_session_or_new_session(netdb)
623         plugin.extension_manager.extend_network_dict(session, netdb, result)
624 
625     @staticmethod
626     @resource_extend.extends([attributes.PORTS])
627     def _ml2_md_extend_port_dict(result, portdb):
628         plugin = directory.get_plugin()
629         session = plugin._object_session_or_new_session(portdb)
630         plugin.extension_manager.extend_port_dict(session, portdb, result)
631 
632     @staticmethod
633     @resource_extend.extends([attributes.SUBNETS])
634     def _ml2_md_extend_subnet_dict(result, subnetdb):
635         plugin = directory.get_plugin()
636         session = plugin._object_session_or_new_session(subnetdb)
637         plugin.extension_manager.extend_subnet_dict(session, subnetdb, result)
638 
639     @staticmethod
640     def _object_session_or_new_session(sql_obj):
641         session = sqlalchemy.inspect(sql_obj).session
642         if not session:
643             session = db_api.get_reader_session()
644         return session
645 
646     def _notify_port_updated(self, mech_context):
647         port = mech_context.current
648         segment = mech_context.bottom_bound_segment
649         if not segment:
650             # REVISIT(rkukura): This should notify agent to unplug port
651             network = mech_context.network.current
652             LOG.debug("In _notify_port_updated(), no bound segment for "
653                       "port %(port_id)s on network %(network_id)s",
654                       {'port_id': port['id'], 'network_id': network['id']})
655             return
656         self.notifier.port_update(mech_context._plugin_context, port,
657                                   segment[api.NETWORK_TYPE],
658                                   segment[api.SEGMENTATION_ID],
659                                   segment[api.PHYSICAL_NETWORK])
660 
661     def _delete_objects(self, context, resource, objects):
662         delete_op = getattr(self, 'delete_%s' % resource)
663         for obj in objects:
664             try:
665                 delete_op(context, obj['result']['id'])
666             except KeyError:
667                 LOG.exception(_LE("Could not find %s to delete."),
668                               resource)
669             except Exception:
670                 LOG.exception(_LE("Could not delete %(res)s %(id)s."),
671                               {'res': resource,
672                                'id': obj['result']['id']})
673 
674     def _create_bulk_ml2(self, resource, context, request_items):
675         objects = []
676         collection = "%ss" % resource
677         items = request_items[collection]
678         obj_before_create = getattr(self, '_before_create_%s' % resource)
679         for item in items:
680             obj_before_create(context, item)
681         with db_api.context_manager.writer.using(context):
682             obj_creator = getattr(self, '_create_%s_db' % resource)
683             for item in items:
684                 try:
685                     attrs = item[resource]
686                     result, mech_context = obj_creator(context, item)
687                     objects.append({'mech_context': mech_context,
688                                     'result': result,
689                                     'attributes': attrs})
690 
691                 except Exception as e:
692                     with excutils.save_and_reraise_exception():
693                         utils.attach_exc_details(
694                             e, _LE("An exception occurred while creating "
695                                    "the %(resource)s:%(item)s"),
696                             {'resource': resource, 'item': item})
697 
698         postcommit_op = getattr(self, '_after_create_%s' % resource)
699         for obj in objects:
700             try:
701                 postcommit_op(context, obj['result'], obj['mech_context'])
702             except Exception:
703                 with excutils.save_and_reraise_exception():
704                     resource_ids = [res['result']['id'] for res in objects]
705                     LOG.exception(_LE("ML2 _after_create_%(res)s "
706                                       "failed for %(res)s: "
707                                       "'%(failed_id)s'. Deleting "
708                                       "%(res)ss %(resource_ids)s"),
709                                   {'res': resource,
710                                    'failed_id': obj['result']['id'],
711                                    'resource_ids': ', '.join(resource_ids)})
712                     # _after_handler will have deleted the object that threw
713                     to_delete = [o for o in objects if o != obj]
714                     self._delete_objects(context, resource, to_delete)
715         return objects
716 
717     def _get_network_mtu(self, network):
718         mtus = []
719         try:
720             segments = network[mpnet.SEGMENTS]
721         except KeyError:
722             segments = [network]
723         for s in segments:
724             segment_type = s[provider_net.NETWORK_TYPE]
725             try:
726                 type_driver = self.type_manager.drivers[segment_type].obj
727             except KeyError:
728                 # NOTE(ihrachys) This can happen when type driver is not loaded
729                 # for an existing segment, or simply when the network has no
730                 # segments at the specific time this is computed.
731                 # In the former case, while it's probably an indication of
732                 # a bad setup, it's better to be safe than sorry here. Also,
733                 # several unit tests use non-existent driver types that may
734                 # trigger the exception here.
735                 if segment_type and s[provider_net.SEGMENTATION_ID]:
736                     LOG.warning(
737                         _LW("Failed to determine MTU for segment "
738                             "%(segment_type)s:%(segment_id)s; network "
739                             "%(network_id)s MTU calculation may be not "
740                             "accurate"),
741                         {
742                             'segment_type': segment_type,
743                             'segment_id': s[provider_net.SEGMENTATION_ID],
744                             'network_id': network['id'],
745                         }
746                     )
747             else:
748                 mtu = type_driver.get_mtu(s[provider_net.PHYSICAL_NETWORK])
749                 # Some drivers, like 'local', may return None; the assumption
750                 # then is that for the segment type, MTU has no meaning or
751                 # unlimited, and so we should then ignore those values.
752                 if mtu:
753                     mtus.append(mtu)
754         return min(mtus) if mtus else 0
755 
756     def _before_create_network(self, context, network):
757         net_data = network[attributes.NETWORK]
758         registry.notify(resources.NETWORK, events.BEFORE_CREATE, self,
759                         context=context, network=net_data)
760 
761     def _create_network_db(self, context, network):
762         net_data = network[attributes.NETWORK]
763         tenant_id = net_data['tenant_id']
764         with db_api.context_manager.writer.using(context):
765             net_db = self.create_network_db(context, network)
766             result = self._make_network_dict(net_db, process_extensions=False,
767                                              context=context)
768             self.extension_manager.process_create_network(context, net_data,
769                                                           result)
770             self._process_l3_create(context, result, net_data)
771             net_data['id'] = result['id']
772             self.type_manager.create_network_segments(context, net_data,
773                                                       tenant_id)
774             self.type_manager.extend_network_dict_provider(context, result)
775             # Update the transparent vlan if configured
776             if utils.is_extension_supported(self, 'vlan-transparent'):
777                 vlt = vlantransparent.get_vlan_transparent(net_data)
778                 net_db['vlan_transparent'] = vlt
779                 result['vlan_transparent'] = vlt
780 
781             result[api.MTU] = self._get_network_mtu(result)
782 
783             if az_ext.AZ_HINTS in net_data:
784                 self.validate_availability_zones(context, 'network',
785                                                  net_data[az_ext.AZ_HINTS])
786                 az_hints = az_ext.convert_az_list_to_string(
787                                                 net_data[az_ext.AZ_HINTS])
788                 net_db[az_ext.AZ_HINTS] = az_hints
789                 result[az_ext.AZ_HINTS] = az_hints
790             registry.notify(resources.NETWORK, events.PRECOMMIT_CREATE, self,
791                             context=context, request=net_data, network=result)
792 
793             resource_extend.apply_funcs('networks', result, net_db)
794             mech_context = driver_context.NetworkContext(self, context,
795                                                          result)
796             self.mechanism_manager.create_network_precommit(mech_context)
797         return result, mech_context
798 
799     @utils.transaction_guard
800     @db_api.retry_if_session_inactive()
801     def create_network(self, context, network):
802         self._before_create_network(context, network)
803         result, mech_context = self._create_network_db(context, network)
804         return self._after_create_network(context, result, mech_context)
805 
806     def _after_create_network(self, context, result, mech_context):
807         kwargs = {'context': context, 'network': result}
808         registry.notify(resources.NETWORK, events.AFTER_CREATE, self, **kwargs)
809         try:
810             self.mechanism_manager.create_network_postcommit(mech_context)
811         except ml2_exc.MechanismDriverError:
812             with excutils.save_and_reraise_exception():
813                 LOG.error(_LE("mechanism_manager.create_network_postcommit "
814                               "failed, deleting network '%s'"), result['id'])
815                 self.delete_network(context, result['id'])
816 
817         return result
818 
819     @utils.transaction_guard
820     @db_api.retry_if_session_inactive()
821     def create_network_bulk(self, context, networks):
822         objects = self._create_bulk_ml2(attributes.NETWORK, context, networks)
823         return [obj['result'] for obj in objects]
824 
825     @utils.transaction_guard
826     @db_api.retry_if_session_inactive()
827     def update_network(self, context, id, network):
828         net_data = network[attributes.NETWORK]
829         provider._raise_if_updates_provider_attributes(net_data)
830 
831         with db_api.context_manager.writer.using(context):
832             original_network = super(Ml2Plugin, self).get_network(context, id)
833             updated_network = super(Ml2Plugin, self).update_network(context,
834                                                                     id,
835                                                                     network)
836             self.extension_manager.process_update_network(context, net_data,
837                                                           updated_network)
838             self._process_l3_update(context, updated_network, net_data)
839             self.type_manager.extend_network_dict_provider(context,
840                                                            updated_network)
841 
842             # ToDO(QoS): This would change once EngineFacade moves out
843             db_network = self._get_network(context, id)
844             # Expire the db_network in current transaction, so that the join
845             # relationship can be updated.
846             context.session.expire(db_network)
847             updated_network = self._make_network_dict(
848                 db_network, context=context)
849 
850             kwargs = {'context': context, 'network': updated_network,
851                       'original_network': original_network,
852                       'request': net_data}
853             registry.notify(
854                 resources.NETWORK, events.PRECOMMIT_UPDATE, self, **kwargs)
855 
856             # TODO(QoS): Move out to the extension framework somehow.
857             need_network_update_notify = (
858                 qos_consts.QOS_POLICY_ID in net_data and
859                 original_network[qos_consts.QOS_POLICY_ID] !=
860                 updated_network[qos_consts.QOS_POLICY_ID])
861 
862             mech_context = driver_context.NetworkContext(
863                 self, context, updated_network,
864                 original_network=original_network)
865             self.mechanism_manager.update_network_precommit(mech_context)
866 
867         # TODO(apech) - handle errors raised by update_network, potentially
868         # by re-calling update_network with the previous attributes. For
869         # now the error is propagated to the caller, which is expected to
870         # either undo/retry the operation or delete the resource.
871         kwargs = {'context': context, 'network': updated_network,
872                   'original_network': original_network}
873         registry.notify(resources.NETWORK, events.AFTER_UPDATE, self, **kwargs)
874         self.mechanism_manager.update_network_postcommit(mech_context)
875         if need_network_update_notify:
876             self.notifier.network_update(context, updated_network)
877         return updated_network
878 
879     @db_api.retry_if_session_inactive()
880     def get_network(self, context, id, fields=None):
881         with db_api.context_manager.reader.using(context):
882             result = super(Ml2Plugin, self).get_network(context, id, None)
883             self.type_manager.extend_network_dict_provider(context, result)
884             result[api.MTU] = self._get_network_mtu(result)
885 
886         return db_utils.resource_fields(result, fields)
887 
888     @db_api.retry_if_session_inactive()
889     def get_networks(self, context, filters=None, fields=None,
890                      sorts=None, limit=None, marker=None, page_reverse=False):
891         with db_api.context_manager.reader.using(context):
892             nets = super(Ml2Plugin,
893                          self).get_networks(context, filters, None, sorts,
894                                             limit, marker, page_reverse)
895             self.type_manager.extend_networks_dict_provider(context, nets)
896 
897             nets = self._filter_nets_provider(context, nets, filters)
898 
899             for net in nets:
900                 net[api.MTU] = self._get_network_mtu(net)
901 
902         return [db_utils.resource_fields(net, fields) for net in nets]
903 
904     def get_network_contexts(self, context, network_ids):
905         """Return a map of network_id to NetworkContext for network_ids."""
906         net_filters = {'id': list(set(network_ids))}
907         nets_by_netid = {
908             n['id']: n for n in self.get_networks(context,
909                                                   filters=net_filters)
910         }
911         segments_by_netid = segments_db.get_networks_segments(
912             context, list(nets_by_netid.keys()))
913         netctxs_by_netid = {
914             net_id: driver_context.NetworkContext(
915                 self, context, nets_by_netid[net_id],
916                 segments=segments_by_netid[net_id])
917             for net_id in nets_by_netid.keys()
918         }
919         return netctxs_by_netid
920 
921     @utils.transaction_guard
922     def delete_network(self, context, id):
923         # the only purpose of this override is to protect this from being
924         # called inside of a transaction.
925         return super(Ml2Plugin, self).delete_network(context, id)
926 
927     @registry.receives(resources.NETWORK, [events.PRECOMMIT_DELETE])
928     def _network_delete_precommit_handler(self, rtype, event, trigger,
929                                           context, network_id, **kwargs):
930         network = self.get_network(context, network_id)
931         mech_context = driver_context.NetworkContext(self,
932                                                      context,
933                                                      network)
934         # TODO(kevinbenton): move this mech context into something like
935         # a 'delete context' so it's not polluting the real context object
936         setattr(context, '_mech_context', mech_context)
937         self.mechanism_manager.delete_network_precommit(
938             mech_context)
939 
940     @registry.receives(resources.NETWORK, [events.AFTER_DELETE])
941     def _network_delete_after_delete_handler(self, rtype, event, trigger,
942                                              context, network, **kwargs):
943         try:
944             self.mechanism_manager.delete_network_postcommit(
945                 context._mech_context)
946         except ml2_exc.MechanismDriverError:
947             # TODO(apech) - One or more mechanism driver failed to
948             # delete the network.  Ideally we'd notify the caller of
949             # the fact that an error occurred.
950             LOG.error(_LE("mechanism_manager.delete_network_postcommit"
951                           " failed"))
952         self.notifier.network_delete(context, network['id'])
953 
954     def _before_create_subnet(self, context, subnet):
955         # TODO(kevinbenton): BEFORE notification should be added here
956         pass
957 
958     def _create_subnet_db(self, context, subnet):
959         with db_api.context_manager.writer.using(context):
960             result, net_db, ipam_sub = self._create_subnet_precommit(
961                 context, subnet)
962             self.extension_manager.process_create_subnet(
963                 context, subnet[attributes.SUBNET], result)
964             network = self._make_network_dict(net_db, context=context)
965             self.type_manager.extend_network_dict_provider(context, network)
966             network[api.MTU] = self._get_network_mtu(network)
967             mech_context = driver_context.SubnetContext(self, context,
968                                                         result, network)
969             self.mechanism_manager.create_subnet_precommit(mech_context)
970 
971         # TODO(kevinbenton): move this to '_after_subnet_create'
972         # db base plugin post commit ops
973         self._create_subnet_postcommit(context, result, net_db, ipam_sub)
974 
975         return result, mech_context
976 
977     @utils.transaction_guard
978     @db_api.retry_if_session_inactive()
979     def create_subnet(self, context, subnet):
980         self._before_create_subnet(context, subnet)
981         result, mech_context = self._create_subnet_db(context, subnet)
982         return self._after_create_subnet(context, result, mech_context)
983 
984     def _after_create_subnet(self, context, result, mech_context):
985         kwargs = {'context': context, 'subnet': result}
986         registry.notify(resources.SUBNET, events.AFTER_CREATE, self, **kwargs)
987         try:
988             self.mechanism_manager.create_subnet_postcommit(mech_context)
989         except ml2_exc.MechanismDriverError:
990             with excutils.save_and_reraise_exception():
991                 LOG.error(_LE("mechanism_manager.create_subnet_postcommit "
992                               "failed, deleting subnet '%s'"), result['id'])
993                 self.delete_subnet(context, result['id'])
994         return result
995 
996     @utils.transaction_guard
997     @db_api.retry_if_session_inactive()
998     def create_subnet_bulk(self, context, subnets):
999         objects = self._create_bulk_ml2(attributes.SUBNET, context, subnets)
1000         return [obj['result'] for obj in objects]
1001 
1002     @utils.transaction_guard
1003     @db_api.retry_if_session_inactive()
1004     def update_subnet(self, context, id, subnet):
1005         with db_api.context_manager.writer.using(context):
1006             updated_subnet, original_subnet = self._update_subnet_precommit(
1007                 context, id, subnet)
1008             self.extension_manager.process_update_subnet(
1009                 context, subnet[attributes.SUBNET], updated_subnet)
1010             updated_subnet = self.get_subnet(context, id)
1011             mech_context = driver_context.SubnetContext(
1012                 self, context, updated_subnet, network=None,
1013                 original_subnet=original_subnet)
1014             self.mechanism_manager.update_subnet_precommit(mech_context)
1015 
1016         self._update_subnet_postcommit(context, original_subnet,
1017                                        updated_subnet)
1018         # TODO(apech) - handle errors raised by update_subnet, potentially
1019         # by re-calling update_subnet with the previous attributes. For
1020         # now the error is propagated to the caller, which is expected to
1021         # either undo/retry the operation or delete the resource.
1022         self.mechanism_manager.update_subnet_postcommit(mech_context)
1023         return updated_subnet
1024 
1025     @utils.transaction_guard
1026     def delete_subnet(self, context, id):
1027         # the only purpose of this override is to protect this from being
1028         # called inside of a transaction.
1029         return super(Ml2Plugin, self).delete_subnet(context, id)
1030 
1031     @registry.receives(resources.SUBNET, [events.PRECOMMIT_DELETE])
1032     def _subnet_delete_precommit_handler(self, rtype, event, trigger,
1033                                          context, subnet_id, **kwargs):
1034         record = self._get_subnet(context, subnet_id)
1035         subnet = self._make_subnet_dict(record, context=context)
1036         network = self.get_network(context, subnet['network_id'])
1037         mech_context = driver_context.SubnetContext(self, context,
1038                                                     subnet, network)
1039         # TODO(kevinbenton): move this mech context into something like
1040         # a 'delete context' so it's not polluting the real context object
1041         setattr(context, '_mech_context', mech_context)
1042         self.mechanism_manager.delete_subnet_precommit(mech_context)
1043 
1044     @registry.receives(resources.SUBNET, [events.AFTER_DELETE])
1045     def _subnet_delete_after_delete_handler(self, rtype, event, trigger,
1046                                             context, subnet, **kwargs):
1047         try:
1048             self.mechanism_manager.delete_subnet_postcommit(
1049                 context._mech_context)
1050         except ml2_exc.MechanismDriverError:
1051             # TODO(apech) - One or more mechanism driver failed to
1052             # delete the subnet.  Ideally we'd notify the caller of
1053             # the fact that an error occurred.
1054             LOG.error(_LE("mechanism_manager.delete_subnet_postcommit failed"))
1055 
1056     # TODO(yalei) - will be simplified after security group and address pair be
1057     # converted to ext driver too.
1058     def _portsec_ext_port_create_processing(self, context, port_data, port):
1059         attrs = port[attributes.PORT]
1060         port_security = ((port_data.get(psec.PORTSECURITY) is None) or
1061                          port_data[psec.PORTSECURITY])
1062 
1063         # allowed address pair checks
1064         if self._check_update_has_allowed_address_pairs(port):
1065             if not port_security:
1066                 raise addr_pair.AddressPairAndPortSecurityRequired()
1067         else:
1068             # remove ATTR_NOT_SPECIFIED
1069             attrs[addr_pair.ADDRESS_PAIRS] = []
1070 
1071         if port_security:
1072             self._ensure_default_security_group_on_port(context, port)
1073         elif self._check_update_has_security_groups(port):
1074             raise psec_exc.PortSecurityAndIPRequiredForSecurityGroups()
1075 
1076     def _setup_dhcp_agent_provisioning_component(self, context, port):
1077         subnet_ids = [f['subnet_id'] for f in port['fixed_ips']]
1078         if (db.is_dhcp_active_on_any_subnet(context, subnet_ids) and
1079             len(self.get_dhcp_agents_hosting_networks(context,
1080                                                       [port['network_id']]))):
1081             # the agents will tell us when the dhcp config is ready so we setup
1082             # a provisioning component to prevent the port from going ACTIVE
1083             # until a dhcp_ready_on_port notification is received.
1084             provisioning_blocks.add_provisioning_component(
1085                 context, port['id'], resources.PORT,
1086                 provisioning_blocks.DHCP_ENTITY)
1087         else:
1088             provisioning_blocks.remove_provisioning_component(
1089                 context, port['id'], resources.PORT,
1090                 provisioning_blocks.DHCP_ENTITY)
1091 
1092     def _before_create_port(self, context, port):
1093         attrs = port[attributes.PORT]
1094         if not attrs.get('status'):
1095             attrs['status'] = const.PORT_STATUS_DOWN
1096 
1097         registry.notify(resources.PORT, events.BEFORE_CREATE, self,
1098                         context=context, port=attrs)
1099         # NOTE(kevinbenton): triggered outside of transaction since it
1100         # emits 'AFTER' events if it creates.
1101         self._ensure_default_security_group(context, attrs['tenant_id'])
1102 
1103     def _create_port_db(self, context, port):
1104         attrs = port[attributes.PORT]
1105         with db_api.context_manager.writer.using(context):
1106             dhcp_opts = attrs.get(edo_ext.EXTRADHCPOPTS, [])
1107             port_db = self.create_port_db(context, port)
1108             result = self._make_port_dict(port_db, process_extensions=False)
1109             self.extension_manager.process_create_port(context, attrs, result)
1110             self._portsec_ext_port_create_processing(context, result, port)
1111 
1112             # sgids must be got after portsec checked with security group
1113             sgids = self._get_security_groups_on_port(context, port)
1114             self._process_port_create_security_group(context, result, sgids)
1115             network = self.get_network(context, result['network_id'])
1116             binding = db.add_port_binding(context, result['id'])
1117             mech_context = driver_context.PortContext(self, context, result,
1118                                                       network, binding, None)
1119             self._process_port_binding(mech_context, attrs)
1120 
1121             result[addr_pair.ADDRESS_PAIRS] = (
1122                 self._process_create_allowed_address_pairs(
1123                     context, result,
1124                     attrs.get(addr_pair.ADDRESS_PAIRS)))
1125             self._process_port_create_extra_dhcp_opts(context, result,
1126                                                       dhcp_opts)
1127             kwargs = {'context': context, 'port': result}
1128             registry.notify(
1129                 resources.PORT, events.PRECOMMIT_CREATE, self, **kwargs)
1130             self.mechanism_manager.create_port_precommit(mech_context)
1131             self._setup_dhcp_agent_provisioning_component(context, result)
1132 
1133         resource_extend.apply_funcs('ports', result, port_db)
1134         return result, mech_context
1135 
1136     @utils.transaction_guard
1137     @db_api.retry_if_session_inactive()
1138     def create_port(self, context, port):
1139         self._before_create_port(context, port)
1140         result, mech_context = self._create_port_db(context, port)
1141         return self._after_create_port(context, result, mech_context)
1142 
1143     def _after_create_port(self, context, result, mech_context):
1144         # notify any plugin that is interested in port create events
1145         kwargs = {'context': context, 'port': result}
1146         registry.notify(resources.PORT, events.AFTER_CREATE, self, **kwargs)
1147 
1148         try:
1149             self.mechanism_manager.create_port_postcommit(mech_context)
1150         except ml2_exc.MechanismDriverError:
1151             with excutils.save_and_reraise_exception():
1152                 LOG.error(_LE("mechanism_manager.create_port_postcommit "
1153                               "failed, deleting port '%s'"), result['id'])
1154                 self.delete_port(context, result['id'], l3_port_check=False)
1155         try:
1156             bound_context = self._bind_port_if_needed(mech_context)
1157         except ml2_exc.MechanismDriverError:
1158             with excutils.save_and_reraise_exception():
1159                 LOG.error(_LE("_bind_port_if_needed "
1160                               "failed, deleting port '%s'"), result['id'])
1161                 self.delete_port(context, result['id'], l3_port_check=False)
1162 
1163         return bound_context.current
1164 
1165     @utils.transaction_guard
1166     @db_api.retry_if_session_inactive()
1167     def create_port_bulk(self, context, ports):
1168         objects = self._create_bulk_ml2(attributes.PORT, context, ports)
1169         return [obj['result'] for obj in objects]
1170 
1171     # TODO(yalei) - will be simplified after security group and address pair be
1172     # converted to ext driver too.
1173     def _portsec_ext_port_update_processing(self, updated_port, context, port,
1174                                             id):
1175         port_security = ((updated_port.get(psec.PORTSECURITY) is None) or
1176                          updated_port[psec.PORTSECURITY])
1177 
1178         if port_security:
1179             return
1180 
1181         # check the address-pairs
1182         if self._check_update_has_allowed_address_pairs(port):
1183             #  has address pairs in request
1184             raise addr_pair.AddressPairAndPortSecurityRequired()
1185         elif (not
1186          self._check_update_deletes_allowed_address_pairs(port)):
1187             # not a request for deleting the address-pairs
1188             updated_port[addr_pair.ADDRESS_PAIRS] = (
1189                     self.get_allowed_address_pairs(context, id))
1190 
1191             # check if address pairs has been in db, if address pairs could
1192             # be put in extension driver, we can refine here.
1193             if updated_port[addr_pair.ADDRESS_PAIRS]:
1194                 raise addr_pair.AddressPairAndPortSecurityRequired()
1195 
1196         # checks if security groups were updated adding/modifying
1197         # security groups, port security is set
1198         if self._check_update_has_security_groups(port):
1199             raise psec_exc.PortSecurityAndIPRequiredForSecurityGroups()
1200         elif (not
1201           self._check_update_deletes_security_groups(port)):
1202             if not utils.is_extension_supported(self, 'security-group'):
1203                 return
1204             # Update did not have security groups passed in. Check
1205             # that port does not have any security groups already on it.
1206             filters = {'port_id': [id]}
1207             security_groups = (
1208                 super(Ml2Plugin, self)._get_port_security_group_bindings(
1209                         context, filters)
1210                      )
1211             if security_groups:
1212                 raise psec_exc.PortSecurityPortHasSecurityGroup()
1213 
1214     @utils.transaction_guard
1215     @db_api.retry_if_session_inactive()
1216     def update_port(self, context, id, port):
1217         attrs = port[attributes.PORT]
1218         need_port_update_notify = False
1219         bound_mech_contexts = []
1220         with db_api.context_manager.writer.using(context):
1221             port_db = self._get_port(context, id)
1222             binding = port_db.port_binding
1223             if not binding:
1224                 raise exc.PortNotFound(port_id=id)
1225             mac_address_updated = self._check_mac_update_allowed(
1226                 port_db, attrs, binding)
1227             need_port_update_notify |= mac_address_updated
1228             original_port = self._make_port_dict(port_db)
1229             updated_port = super(Ml2Plugin, self).update_port(context, id,
1230                                                               port)
1231             self.extension_manager.process_update_port(context, attrs,
1232                                                        updated_port)
1233             self._portsec_ext_port_update_processing(updated_port, context,
1234                                                      port, id)
1235 
1236             if (psec.PORTSECURITY in attrs) and (
1237                         original_port[psec.PORTSECURITY] !=
1238                         updated_port[psec.PORTSECURITY]):
1239                 need_port_update_notify = True
1240             # TODO(QoS): Move out to the extension framework somehow.
1241             # Follow https://review.openstack.org/#/c/169223 for a solution.
1242             if (qos_consts.QOS_POLICY_ID in attrs and
1243                     original_port[qos_consts.QOS_POLICY_ID] !=
1244                     updated_port[qos_consts.QOS_POLICY_ID]):
1245                 need_port_update_notify = True
1246 
1247             if addr_pair.ADDRESS_PAIRS in attrs:
1248                 need_port_update_notify |= (
1249                     self.update_address_pairs_on_port(context, id, port,
1250                                                       original_port,
1251                                                       updated_port))
1252             need_port_update_notify |= self.update_security_group_on_port(
1253                 context, id, port, original_port, updated_port)
1254             network = self.get_network(context, original_port['network_id'])
1255             need_port_update_notify |= self._update_extra_dhcp_opts_on_port(
1256                 context, id, port, updated_port)
1257             levels = db.get_binding_levels(context, id, binding.host)
1258             # one of the operations above may have altered the model call
1259             # _make_port_dict again to ensure latest state is reflected so mech
1260             # drivers, callback handlers, and the API caller see latest state.
1261             # We expire here to reflect changed relationships on the obj.
1262             # Repeatable read will ensure we still get the state from this
1263             # transaction in spite of concurrent updates/deletes.
1264             context.session.expire(port_db)
1265             updated_port.update(self._make_port_dict(port_db))
1266             mech_context = driver_context.PortContext(
1267                 self, context, updated_port, network, binding, levels,
1268                 original_port=original_port)
1269             need_port_update_notify |= self._process_port_binding(
1270                 mech_context, attrs)
1271 
1272             kwargs = {
1273                 'context': context,
1274                 'port': updated_port,
1275                 'original_port': original_port,
1276             }
1277             registry.notify(
1278                 resources.PORT, events.PRECOMMIT_UPDATE, self, **kwargs)
1279 
1280             # For DVR router interface ports we need to retrieve the
1281             # DVRPortbinding context instead of the normal port context.
1282             # The normal Portbinding context does not have the status
1283             # of the ports that are required by the l2pop to process the
1284             # postcommit events.
1285 
1286             # NOTE:Sometimes during the update_port call, the DVR router
1287             # interface port may not have the port binding, so we cannot
1288             # create a generic bindinglist that will address both the
1289             # DVR and non-DVR cases here.
1290             # TODO(Swami): This code need to be revisited.
1291             if port_db['device_owner'] == const.DEVICE_OWNER_DVR_INTERFACE:
1292                 dist_binding_list = db.get_distributed_port_bindings(context,
1293                                                                      id)
1294                 for dist_binding in dist_binding_list:
1295                     levels = db.get_binding_levels(context, id,
1296                                                    dist_binding.host)
1297                     dist_mech_context = driver_context.PortContext(
1298                         self, context, updated_port, network,
1299                         dist_binding, levels, original_port=original_port)
1300                     self.mechanism_manager.update_port_precommit(
1301                         dist_mech_context)
1302                     bound_mech_contexts.append(dist_mech_context)
1303             else:
1304                 self.mechanism_manager.update_port_precommit(mech_context)
1305                 self._setup_dhcp_agent_provisioning_component(
1306                     context, updated_port)
1307                 bound_mech_contexts.append(mech_context)
1308 
1309         # Commit hooks may have modified the port object. Reload it.
1310         port_db = self._get_port(context, id)
1311         updated_port.update(self._make_port_dict(port_db))
1312 
1313         # Notifications must be sent after the above transaction is complete
1314         kwargs = {
1315             'context': context,
1316             'port': updated_port,
1317             'mac_address_updated': mac_address_updated,
1318             'original_port': original_port,
1319         }
1320         registry.notify(resources.PORT, events.AFTER_UPDATE, self, **kwargs)
1321 
1322         # Note that DVR Interface ports will have bindings on
1323         # multiple hosts, and so will have multiple mech_contexts,
1324         # while other ports typically have just one.
1325         # Since bound_mech_contexts has both the DVR and non-DVR
1326         # contexts we can manage just with a single for loop.
1327         try:
1328             for mech_context in bound_mech_contexts:
1329                 self.mechanism_manager.update_port_postcommit(
1330                     mech_context)
1331         except ml2_exc.MechanismDriverError:
1332             LOG.error(_LE("mechanism_manager.update_port_postcommit "
1333                           "failed for port %s"), id)
1334 
1335         self.check_and_notify_security_group_member_changed(
1336             context, original_port, updated_port)
1337         need_port_update_notify |= self.is_security_group_member_updated(
1338             context, original_port, updated_port)
1339 
1340         if original_port['admin_state_up'] != updated_port['admin_state_up']:
1341             need_port_update_notify = True
1342         # NOTE: In the case of DVR ports, the port-binding is done after
1343         # router scheduling when sync_routers is called and so this call
1344         # below may not be required for DVR routed interfaces. But still
1345         # since we don't have the mech_context for the DVR router interfaces
1346         # at certain times, we just pass the port-context and return it, so
1347         # that we don't disturb other methods that are expecting a return
1348         # value.
1349         bound_context = self._bind_port_if_needed(
1350             mech_context,
1351             allow_notify=True,
1352             need_notify=need_port_update_notify)
1353         return bound_context.current
1354 
1355     def _process_distributed_port_binding(self, mech_context, context, attrs):
1356         plugin_context = mech_context._plugin_context
1357         binding = mech_context._binding
1358         port = mech_context.current
1359         port_id = port['id']
1360 
1361         if binding.vif_type != portbindings.VIF_TYPE_UNBOUND:
1362             binding.vif_details = ''
1363             binding.vif_type = portbindings.VIF_TYPE_UNBOUND
1364             if binding.host:
1365                 db.clear_binding_levels(plugin_context, port_id, binding.host)
1366             binding.host = ''
1367 
1368         self._update_port_dict_binding(port, binding)
1369         binding.host = attrs and attrs.get(portbindings.HOST_ID)
1370         binding.router_id = attrs and attrs.get('device_id')
1371         # merge into session to reflect changes
1372         binding.persist_state_to_session(plugin_context.session)
1373 
1374     @utils.transaction_guard
1375     @db_api.retry_if_session_inactive()
1376     def update_distributed_port_binding(self, context, id, port):
1377         attrs = port[attributes.PORT]
1378 
1379         host = attrs and attrs.get(portbindings.HOST_ID)
1380         host_set = validators.is_attr_set(host)
1381 
1382         if not host_set:
1383             LOG.error(_LE("No Host supplied to bind DVR Port %s"), id)
1384             return
1385 
1386         binding = db.get_distributed_port_binding_by_host(context,
1387                                                           id, host)
1388         device_id = attrs and attrs.get('device_id')
1389         router_id = binding and binding.get('router_id')
1390         update_required = (not binding or
1391             binding.vif_type == portbindings.VIF_TYPE_BINDING_FAILED or
1392             router_id != device_id)
1393         if update_required:
1394             try:
1395                 with db_api.context_manager.writer.using(context):
1396                     orig_port = self.get_port(context, id)
1397                     if not binding:
1398                         binding = db.ensure_distributed_port_binding(
1399                             context, id, host, router_id=device_id)
1400                     network = self.get_network(context,
1401                                                orig_port['network_id'])
1402                     levels = db.get_binding_levels(context, id, host)
1403                     mech_context = driver_context.PortContext(self,
1404                         context, orig_port, network,
1405                         binding, levels, original_port=orig_port)
1406                     self._process_distributed_port_binding(
1407                         mech_context, context, attrs)
1408             except (os_db_exception.DBReferenceError, exc.PortNotFound):
1409                 LOG.debug("DVR Port %s has been deleted concurrently", id)
1410                 return
1411             self._bind_port_if_needed(mech_context)
1412 
1413     def _pre_delete_port(self, context, port_id, port_check):
1414         """Do some preliminary operations before deleting the port."""
1415         LOG.debug("Deleting port %s", port_id)
1416         try:
1417             # notify interested parties of imminent port deletion;
1418             # a failure here prevents the operation from happening
1419             kwargs = {
1420                 'context': context,
1421                 'port_id': port_id,
1422                 'port_check': port_check
1423             }
1424             registry.notify(
1425                 resources.PORT, events.BEFORE_DELETE, self, **kwargs)
1426         except exceptions.CallbackFailure as e:
1427             # NOTE(armax): preserve old check's behavior
1428             if len(e.errors) == 1:
1429                 raise e.errors[0].error
1430             raise exc.ServicePortInUse(port_id=port_id, reason=e)
1431 
1432     @utils.transaction_guard
1433     @db_api.retry_if_session_inactive()
1434     def delete_port(self, context, id, l3_port_check=True):
1435         self._pre_delete_port(context, id, l3_port_check)
1436         # TODO(armax): get rid of the l3 dependency in the with block
1437         router_ids = []
1438         l3plugin = directory.get_plugin(const.L3)
1439 
1440         with db_api.context_manager.writer.using(context):
1441             try:
1442                 port_db = self._get_port(context, id)
1443                 binding = port_db.port_binding
1444             except exc.PortNotFound:
1445                 LOG.debug("The port '%s' was deleted", id)
1446                 return
1447             port = self._make_port_dict(port_db)
1448 
1449             network = self.get_network(context, port['network_id'])
1450             bound_mech_contexts = []
1451             device_owner = port['device_owner']
1452             if device_owner == const.DEVICE_OWNER_DVR_INTERFACE:
1453                 bindings = db.get_distributed_port_bindings(context,
1454                                                             id)
1455                 for bind in bindings:
1456                     levels = db.get_binding_levels(context, id,
1457                                                    bind.host)
1458                     mech_context = driver_context.PortContext(
1459                         self, context, port, network, bind, levels)
1460                     self.mechanism_manager.delete_port_precommit(mech_context)
1461                     bound_mech_contexts.append(mech_context)
1462             else:
1463                 levels = db.get_binding_levels(context, id,
1464                                                binding.host)
1465                 mech_context = driver_context.PortContext(
1466                     self, context, port, network, binding, levels)
1467                 self.mechanism_manager.delete_port_precommit(mech_context)
1468                 bound_mech_contexts.append(mech_context)
1469             if l3plugin:
1470                 router_ids = l3plugin.disassociate_floatingips(
1471                     context, id, do_notify=False)
1472 
1473             LOG.debug("Calling delete_port for %(port_id)s owned by %(owner)s",
1474                       {"port_id": id, "owner": device_owner})
1475             super(Ml2Plugin, self).delete_port(context, id)
1476 
1477         self._post_delete_port(
1478             context, port, router_ids, bound_mech_contexts)
1479 
1480     def _post_delete_port(
1481         self, context, port, router_ids, bound_mech_contexts):
1482         kwargs = {
1483             'context': context,
1484             'port': port,
1485             'router_ids': router_ids,
1486         }
1487         registry.notify(resources.PORT, events.AFTER_DELETE, self, **kwargs)
1488         try:
1489             # Note that DVR Interface ports will have bindings on
1490             # multiple hosts, and so will have multiple mech_contexts,
1491             # while other ports typically have just one.
1492             for mech_context in bound_mech_contexts:
1493                 self.mechanism_manager.delete_port_postcommit(mech_context)
1494         except ml2_exc.MechanismDriverError:
1495             # TODO(apech) - One or more mechanism driver failed to
1496             # delete the port.  Ideally we'd notify the caller of the
1497             # fact that an error occurred.
1498             LOG.error(_LE("mechanism_manager.delete_port_postcommit failed for"
1499                           " port %s"), port['id'])
1500         self.notifier.port_delete(context, port['id'])
1501 
1502     @utils.transaction_guard
1503     @db_api.retry_if_session_inactive(context_var_name='plugin_context')
1504     def get_bound_port_context(self, plugin_context, port_id, host=None,
1505                                cached_networks=None):
1506         with db_api.context_manager.reader.using(plugin_context) as session:
1507             try:
1508                 port_db = (session.query(models_v2.Port).
1509                            enable_eagerloads(False).
1510                            filter(models_v2.Port.id.startswith(port_id)).
1511                            one())
1512             except sa_exc.NoResultFound:
1513                 LOG.info(_LI("No ports have port_id starting with %s"),
1514                          port_id)
1515                 return
1516             except sa_exc.MultipleResultsFound:
1517                 LOG.error(_LE("Multiple ports have port_id starting with %s"),
1518                           port_id)
1519                 return
1520             port = self._make_port_dict(port_db)
1521             network = (cached_networks or {}).get(port['network_id'])
1522 
1523             if not network:
1524                 network = self.get_network(plugin_context, port['network_id'])
1525 
1526             if port['device_owner'] == const.DEVICE_OWNER_DVR_INTERFACE:
1527                 binding = db.get_distributed_port_binding_by_host(
1528                     plugin_context, port['id'], host)
1529                 if not binding:
1530                     LOG.error(_LE("Binding info for DVR port %s not found"),
1531                               port_id)
1532                     return None
1533                 levels = db.get_binding_levels(plugin_context,
1534                                                port_db.id, host)
1535                 port_context = driver_context.PortContext(
1536                     self, plugin_context, port, network, binding, levels)
1537             else:
1538                 # since eager loads are disabled in port_db query
1539                 # related attribute port_binding could disappear in
1540                 # concurrent port deletion.
1541                 # It's not an error condition.
1542                 binding = port_db.port_binding
1543                 if not binding:
1544                     LOG.info(_LI("Binding info for port %s was not found, "
1545                                  "it might have been deleted already."),
1546                              port_id)
1547                     return
1548                 levels = db.get_binding_levels(plugin_context, port_db.id,
1549                                                port_db.port_binding.host)
1550                 port_context = driver_context.PortContext(
1551                     self, plugin_context, port, network, binding, levels)
1552 
1553         return self._bind_port_if_needed(port_context)
1554 
1555     @utils.transaction_guard
1556     @db_api.retry_if_session_inactive(context_var_name='plugin_context')
1557     def get_bound_ports_contexts(self, plugin_context, dev_ids, host=None):
1558         result = {}
1559         with db_api.context_manager.reader.using(plugin_context):
1560             dev_to_full_pids = db.partial_port_ids_to_full_ids(
1561                 plugin_context, dev_ids)
1562             # get all port objects for IDs
1563             port_dbs_by_id = db.get_port_db_objects(
1564                 plugin_context, dev_to_full_pids.values())
1565             # get all networks for PortContext construction
1566             netctxs_by_netid = self.get_network_contexts(
1567                 plugin_context,
1568                 {p.network_id for p in port_dbs_by_id.values()})
1569             for dev_id in dev_ids:
1570                 port_id = dev_to_full_pids.get(dev_id)
1571                 port_db = port_dbs_by_id.get(port_id)
1572                 if (not port_id or not port_db or
1573                         port_db.network_id not in netctxs_by_netid):
1574                     result[dev_id] = None
1575                     continue
1576                 port = self._make_port_dict(port_db)
1577                 if port['device_owner'] == const.DEVICE_OWNER_DVR_INTERFACE:
1578                     binding = db.get_distributed_port_binding_by_host(
1579                         plugin_context, port['id'], host)
1580                     bindlevelhost_match = host
1581                 else:
1582                     binding = port_db.port_binding
1583                     bindlevelhost_match = binding.host if binding else None
1584                 if not binding:
1585                     LOG.info(_LI("Binding info for port %s was not found, "
1586                                  "it might have been deleted already."),
1587                              port_id)
1588                     result[dev_id] = None
1589                     continue
1590                 levels = [l for l in port_db.binding_levels
1591                           if l.host == bindlevelhost_match]
1592                 levels = sorted(levels, key=lambda l: l.level)
1593                 network_ctx = netctxs_by_netid.get(port_db.network_id)
1594                 port_context = driver_context.PortContext(
1595                     self, plugin_context, port, network_ctx, binding, levels)
1596                 result[dev_id] = port_context
1597 
1598         return {d: self._bind_port_if_needed(pctx) if pctx else None
1599                 for d, pctx in result.items()}
1600 
1601     def update_port_status(self, context, port_id, status, host=None,
1602                            network=None):
1603         """
1604         Returns port_id (non-truncated uuid) if the port exists.
1605         Otherwise returns None.
1606         'network' is deprecated and has no effect
1607         """
1608         full = db.partial_port_ids_to_full_ids(context, [port_id])
1609         if port_id not in full:
1610             return None
1611         port_id = full[port_id]
1612         return self.update_port_statuses(
1613             context, {port_id: status}, host)[port_id]
1614 
1615     @utils.transaction_guard
1616     @db_api.retry_if_session_inactive()
1617     def update_port_statuses(self, context, port_id_to_status, host=None):
1618         result = {}
1619         port_ids = port_id_to_status.keys()
1620         port_dbs_by_id = db.get_port_db_objects(context, port_ids)
1621         for port_id, status in port_id_to_status.items():
1622             if not port_dbs_by_id.get(port_id):
1623                 LOG.debug("Port %(port)s update to %(val)s by agent not found",
1624                           {'port': port_id, 'val': status})
1625                 result[port_id] = None
1626                 continue
1627             result[port_id] = self._safe_update_individual_port_db_status(
1628                 context, port_dbs_by_id[port_id], status, host)
1629         return result
1630 
1631     def _safe_update_individual_port_db_status(self, context, port,
1632                                                status, host):
1633         port_id = port.id
1634         try:
1635             return self._update_individual_port_db_status(
1636                 context, port, status, host)
1637         except Exception:
1638             with excutils.save_and_reraise_exception() as ectx:
1639                 # don't reraise if port doesn't exist anymore
1640                 ectx.reraise = bool(db.get_port(context, port_id))
1641 
1642     def _update_individual_port_db_status(self, context, port, status, host):
1643         updated = False
1644         network = None
1645         port_id = port.id
1646         with db_api.context_manager.writer.using(context):
1647             context.session.add(port)  # bring port into writer session
1648             if (port.status != status and
1649                 port['device_owner'] != const.DEVICE_OWNER_DVR_INTERFACE):
1650                 original_port = self._make_port_dict(port)
1651                 port.status = status
1652                 # explicit flush before _make_port_dict to ensure extensions
1653                 # listening for db events can modify the port if necessary
1654                 context.session.flush()
1655                 updated_port = self._make_port_dict(port)
1656                 levels = db.get_binding_levels(context, port.id,
1657                                                port.port_binding.host)
1658                 mech_context = driver_context.PortContext(
1659                     self, context, updated_port, network, port.port_binding,
1660                     levels, original_port=original_port)
1661                 self.mechanism_manager.update_port_precommit(mech_context)
1662                 updated = True
1663             elif port['device_owner'] == const.DEVICE_OWNER_DVR_INTERFACE:
1664                 binding = db.get_distributed_port_binding_by_host(
1665                     context, port['id'], host)
1666                 if not binding:
1667                     return
1668                 binding.status = status
1669                 updated = True
1670 
1671         if (updated and
1672             port['device_owner'] == const.DEVICE_OWNER_DVR_INTERFACE):
1673             with db_api.context_manager.writer.using(context):
1674                 port = db.get_port(context, port_id)
1675                 if not port:
1676                     LOG.warning(_LW("Port %s not found during update"),
1677                                 port_id)
1678                     return
1679                 original_port = self._make_port_dict(port)
1680                 network = network or self.get_network(
1681                     context, original_port['network_id'])
1682                 port.status = db.generate_distributed_port_status(context,
1683                                                                   port['id'])
1684                 updated_port = self._make_port_dict(port)
1685                 levels = db.get_binding_levels(context, port_id, host)
1686                 mech_context = (driver_context.PortContext(
1687                     self, context, updated_port, network,
1688                     binding, levels, original_port=original_port))
1689                 self.mechanism_manager.update_port_precommit(mech_context)
1690 
1691         if updated:
1692             self.mechanism_manager.update_port_postcommit(mech_context)
1693             kwargs = {'context': context, 'port': mech_context.current,
1694                       'original_port': original_port}
1695             if status == const.PORT_STATUS_ACTIVE:
1696                 # NOTE(kevinbenton): this kwarg was carried over from
1697                 # the RPC handler that used to call this. it's not clear
1698                 # who uses it so maybe it can be removed. added in commit
1699                 # 3f3874717c07e2b469ea6c6fd52bcb4da7b380c7
1700                 kwargs['update_device_up'] = True
1701             registry.notify(resources.PORT, events.AFTER_UPDATE, self,
1702                             **kwargs)
1703 
1704         if port['device_owner'] == const.DEVICE_OWNER_DVR_INTERFACE:
1705             db.delete_distributed_port_binding_if_stale(context, binding)
1706 
1707         return port['id']
1708 
1709     @db_api.retry_if_session_inactive()
1710     def port_bound_to_host(self, context, port_id, host):
1711         if not host:
1712             return
1713         port = db.get_port(context, port_id)
1714         if not port:
1715             LOG.debug("No Port match for: %s", port_id)
1716             return
1717         if port['device_owner'] == const.DEVICE_OWNER_DVR_INTERFACE:
1718             bindings = db.get_distributed_port_bindings(context,
1719                                                         port_id)
1720             for b in bindings:
1721                 if b.host == host:
1722                     return port
1723             LOG.debug("No binding found for DVR port %s", port['id'])
1724             return
1725         else:
1726             port_host = db.get_port_binding_host(context, port_id)
1727             return port if (port_host == host) else None
1728 
1729     @db_api.retry_if_session_inactive()
1730     def get_ports_from_devices(self, context, devices):
1731         port_ids_to_devices = dict(
1732             (self._device_to_port_id(context, device), device)
1733             for device in devices)
1734         port_ids = list(port_ids_to_devices.keys())
1735         ports = db.get_ports_and_sgs(context, port_ids)
1736         for port in ports:
1737             # map back to original requested id
1738             port_id = next((port_id for port_id in port_ids
1739                            if port['id'].startswith(port_id)), None)
1740             port['device'] = port_ids_to_devices.get(port_id)
1741 
1742         return ports
1743 
1744     @staticmethod
1745     def _device_to_port_id(context, device):
1746         # REVISIT(rkukura): Consider calling into MechanismDrivers to
1747         # process device names, or having MechanismDrivers supply list
1748         # of device prefixes to strip.
1749         for prefix in n_const.INTERFACE_PREFIXES:
1750             if device.startswith(prefix):
1751                 return device[len(prefix):]
1752         # REVISIT(irenab): Consider calling into bound MD to
1753         # handle the get_device_details RPC
1754         if not uuidutils.is_uuid_like(device):
1755             port = db.get_port_from_device_mac(context, device)
1756             if port:
1757                 return port.id
1758         return device
1759 
1760     def filter_hosts_with_network_access(
1761             self, context, network_id, candidate_hosts):
1762         segments = segments_db.get_network_segments(context, network_id)
1763         return self.mechanism_manager.filter_hosts_with_segment_access(
1764             context, segments, candidate_hosts, self.get_agents)
1765 
1766     def check_segment_for_agent(self, segment, agent):
1767         for mech_driver in self.mechanism_manager.ordered_mech_drivers:
1768             driver_agent_type = getattr(mech_driver.obj, 'agent_type', None)
1769             if driver_agent_type and driver_agent_type == agent['agent_type']:
1770                 if mech_driver.obj.check_segment_for_agent(segment, agent):
1771                     return True
1772         return False
1773 
1774     @registry.receives(resources.SEGMENT, (events.PRECOMMIT_CREATE,
1775                                            events.PRECOMMIT_DELETE,
1776                                            events.AFTER_CREATE,
1777                                            events.AFTER_DELETE))
1778     def _handle_segment_change(self, rtype, event, trigger, context, segment):
1779         if (event == events.PRECOMMIT_CREATE and
1780             not isinstance(trigger, segments_plugin.Plugin)):
1781             # TODO(xiaohhui): Now, when create network, ml2 will reserve
1782             # segment and trigger this event handler. This event handler
1783             # will reserve segment again, which will lead to error as the
1784             # segment has already been reserved. This check could be removed
1785             # by unifying segment creation procedure.
1786             return
1787 
1788         network_id = segment.get('network_id')
1789 
1790         if event == events.PRECOMMIT_CREATE:
1791             updated_segment = self.type_manager.reserve_network_segment(
1792                 context, segment)
1793             # The segmentation id might be from ML2 type driver, update it
1794             # in the original segment.
1795             segment[api.SEGMENTATION_ID] = updated_segment[api.SEGMENTATION_ID]
1796         elif event == events.PRECOMMIT_DELETE:
1797             self.type_manager.release_network_segment(context, segment)
1798 
1799         try:
1800             self._notify_mechanism_driver_for_segment_change(
1801                 event, context, network_id)
1802         except ml2_exc.MechanismDriverError:
1803             with excutils.save_and_reraise_exception():
1804                 LOG.error(_LE("mechanism_manager error occurred when "
1805                               "handle event %(event)s for segment "
1806                               "'%(segment)s'"),
1807                           {'event': event, 'segment': segment['id']})
1808 
1809     def _notify_mechanism_driver_for_segment_change(self, event,
1810                                                     context, network_id):
1811         network_with_segments = self.get_network(context, network_id)
1812         mech_context = driver_context.NetworkContext(
1813             self, context, network_with_segments,
1814             original_network=network_with_segments)
1815         if (event == events.PRECOMMIT_CREATE or
1816             event == events.PRECOMMIT_DELETE):
1817             self.mechanism_manager.update_network_precommit(mech_context)
1818         elif event == events.AFTER_CREATE or event == events.AFTER_DELETE:
1819             self.mechanism_manager.update_network_postcommit(mech_context)
