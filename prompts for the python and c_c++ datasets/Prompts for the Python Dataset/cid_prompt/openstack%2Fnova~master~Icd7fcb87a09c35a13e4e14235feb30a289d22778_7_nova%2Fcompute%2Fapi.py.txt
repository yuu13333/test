Please review the code below for security defects. You can consider defect types in terms of:
1.CWE-284 (Improper Access Control)
2.CWE-435 (Improper Interaction Between Multiple Entities)
3.CWE-664 (Improper Control of a Resource Through its Lifetime)
4.CWE-682 (Incorrect Calculation)
5.CWE-691 (Insufficient Control Flow Management)
6.CWE-693 (Protection Mechanism Failure)
7.CWE-697 (Incorrect Comparison)
8.CWE-703 (Improper Check or Handling of Exceptional Conditions)
9.CWE-707 (Improper Neutralization)
10.CWE-710 (Improper Adherence to Coding Standards)
If any are found, please describe the security defect in detail and indicate the corresponding line number of code and solution. If none are detected, states: 'No security defects are detected in the code'.

1 # Copyright 2010 United States Government as represented by the
2 # Administrator of the National Aeronautics and Space Administration.
3 # Copyright 2011 Piston Cloud Computing, Inc.
4 # Copyright 2012-2013 Red Hat, Inc.
5 # All Rights Reserved.
6 #
7 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
8 #    not use this file except in compliance with the License. You may obtain
9 #    a copy of the License at
10 #
11 #         http://www.apache.org/licenses/LICENSE-2.0
12 #
13 #    Unless required by applicable law or agreed to in writing, software
14 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
15 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
16 #    License for the specific language governing permissions and limitations
17 #    under the License.
18 
19 """Handles all requests relating to compute resources (e.g. guest VMs,
20 networking and storage of VMs, and compute hosts on which they run)."""
21 
22 import collections
23 import copy
24 import functools
25 import re
26 import string
27 
28 from castellan import key_manager
29 from oslo_log import log as logging
30 from oslo_messaging import exceptions as oslo_exceptions
31 from oslo_serialization import base64 as base64utils
32 from oslo_utils import excutils
33 from oslo_utils import strutils
34 from oslo_utils import timeutils
35 from oslo_utils import units
36 from oslo_utils import uuidutils
37 import six
38 from six.moves import range
39 
40 from nova import availability_zones
41 from nova import block_device
42 from nova.cells import opts as cells_opts
43 from nova.compute import flavors
44 from nova.compute import instance_actions
45 from nova.compute import instance_list
46 from nova.compute import migration_list
47 from nova.compute import power_state
48 from nova.compute import rpcapi as compute_rpcapi
49 from nova.compute import task_states
50 from nova.compute import utils as compute_utils
51 from nova.compute.utils import wrap_instance_event
52 from nova.compute import vm_states
53 from nova import conductor
54 import nova.conf
55 from nova.consoleauth import rpcapi as consoleauth_rpcapi
56 from nova import context as nova_context
57 from nova import crypto
58 from nova.db import base
59 from nova import exception
60 from nova import exception_wrapper
61 from nova import hooks
62 from nova.i18n import _
63 from nova import image
64 from nova import network
65 from nova.network import model as network_model
66 from nova.network.security_group import openstack_driver
67 from nova.network.security_group import security_group_base
68 from nova import objects
69 from nova.objects import base as obj_base
70 from nova.objects import block_device as block_device_obj
71 from nova.objects import fields as fields_obj
72 from nova.objects import keypair as keypair_obj
73 from nova.objects import quotas as quotas_obj
74 from nova.pci import request as pci_request
75 import nova.policy
76 from nova import profiler
77 from nova import rpc
78 from nova.scheduler import client as scheduler_client
79 from nova.scheduler import utils as scheduler_utils
80 from nova import servicegroup
81 from nova import utils
82 from nova.virt import hardware
83 from nova.volume import cinder
84 
85 LOG = logging.getLogger(__name__)
86 
87 get_notifier = functools.partial(rpc.get_notifier, service='compute')
88 # NOTE(gibi): legacy notification used compute as a service but these
89 # calls still run on the client side of the compute service which is
90 # nova-api. By setting the binary to nova-api below, we can make sure
91 # that the new versioned notifications has the right publisher_id but the
92 # legacy notifications does not change.
93 wrap_exception = functools.partial(exception_wrapper.wrap_exception,
94                                    get_notifier=get_notifier,
95                                    binary='nova-api')
96 CONF = nova.conf.CONF
97 
98 RO_SECURITY_GROUPS = ['default']
99 
100 AGGREGATE_ACTION_UPDATE = 'Update'
101 AGGREGATE_ACTION_UPDATE_META = 'UpdateMeta'
102 AGGREGATE_ACTION_DELETE = 'Delete'
103 AGGREGATE_ACTION_ADD = 'Add'
104 BFV_RESERVE_MIN_COMPUTE_VERSION = 17
105 CINDER_V3_ATTACH_MIN_COMPUTE_VERSION = 24
106 MIN_COMPUTE_MULTIATTACH = 27
107 
108 # FIXME(danms): Keep a global cache of the cells we find the
109 # first time we look. This needs to be refreshed on a timer or
110 # trigger.
111 CELLS = []
112 
113 
114 def check_instance_state(vm_state=None, task_state=(None,),
115                          must_have_launched=True):
116     """Decorator to check VM and/or task state before entry to API functions.
117 
118     If the instance is in the wrong state, or has not been successfully
119     started at least once the wrapper will raise an exception.
120     """
121 
122     if vm_state is not None and not isinstance(vm_state, set):
123         vm_state = set(vm_state)
124     if task_state is not None and not isinstance(task_state, set):
125         task_state = set(task_state)
126 
127     def outer(f):
128         @six.wraps(f)
129         def inner(self, context, instance, *args, **kw):
130             if vm_state is not None and instance.vm_state not in vm_state:
131                 raise exception.InstanceInvalidState(
132                     attr='vm_state',
133                     instance_uuid=instance.uuid,
134                     state=instance.vm_state,
135                     method=f.__name__)
136             if (task_state is not None and
137                     instance.task_state not in task_state):
138                 raise exception.InstanceInvalidState(
139                     attr='task_state',
140                     instance_uuid=instance.uuid,
141                     state=instance.task_state,
142                     method=f.__name__)
143             if must_have_launched and not instance.launched_at:
144                 raise exception.InstanceInvalidState(
145                     attr='launched_at',
146                     instance_uuid=instance.uuid,
147                     state=instance.launched_at,
148                     method=f.__name__)
149 
150             return f(self, context, instance, *args, **kw)
151         return inner
152     return outer
153 
154 
155 def _set_or_none(q):
156     return q if q is None or isinstance(q, set) else set(q)
157 
158 
159 def reject_instance_state(vm_state=None, task_state=None):
160     """Decorator.  Raise InstanceInvalidState if instance is in any of the
161     given states.
162     """
163 
164     vm_state = _set_or_none(vm_state)
165     task_state = _set_or_none(task_state)
166 
167     def outer(f):
168         @six.wraps(f)
169         def inner(self, context, instance, *args, **kw):
170             _InstanceInvalidState = functools.partial(
171                 exception.InstanceInvalidState,
172                 instance_uuid=instance.uuid,
173                 method=f.__name__)
174 
175             if vm_state is not None and instance.vm_state in vm_state:
176                 raise _InstanceInvalidState(
177                     attr='vm_state', state=instance.vm_state)
178 
179             if task_state is not None and instance.task_state in task_state:
180                 raise _InstanceInvalidState(
181                     attr='task_state', state=instance.task_state)
182 
183             return f(self, context, instance, *args, **kw)
184         return inner
185     return outer
186 
187 
188 def check_instance_host(function):
189     @six.wraps(function)
190     def wrapped(self, context, instance, *args, **kwargs):
191         if not instance.host:
192             raise exception.InstanceNotReady(instance_id=instance.uuid)
193         return function(self, context, instance, *args, **kwargs)
194     return wrapped
195 
196 
197 def check_instance_lock(function):
198     @six.wraps(function)
199     def inner(self, context, instance, *args, **kwargs):
200         if instance.locked and not context.is_admin:
201             raise exception.InstanceIsLocked(instance_uuid=instance.uuid)
202         return function(self, context, instance, *args, **kwargs)
203     return inner
204 
205 
206 def check_instance_cell(fn):
207     @six.wraps(fn)
208     def _wrapped(self, context, instance, *args, **kwargs):
209         self._validate_cell(instance)
210         return fn(self, context, instance, *args, **kwargs)
211     return _wrapped
212 
213 
214 def _diff_dict(orig, new):
215     """Return a dict describing how to change orig to new.  The keys
216     correspond to values that have changed; the value will be a list
217     of one or two elements.  The first element of the list will be
218     either '+' or '-', indicating whether the key was updated or
219     deleted; if the key was updated, the list will contain a second
220     element, giving the updated value.
221     """
222     # Figure out what keys went away
223     result = {k: ['-'] for k in set(orig.keys()) - set(new.keys())}
224     # Compute the updates
225     for key, value in new.items():
226         if key not in orig or value != orig[key]:
227             result[key] = ['+', value]
228     return result
229 
230 
231 def load_cells():
232     global CELLS
233     if not CELLS:
234         CELLS = objects.CellMappingList.get_all(
235             nova_context.get_admin_context())
236         LOG.debug('Found %(count)i cells: %(cells)s',
237                   dict(count=len(CELLS),
238                        cells=','.join([c.identity for c in CELLS])))
239 
240     if not CELLS:
241         LOG.error('No cells are configured, unable to continue')
242 
243 
244 @profiler.trace_cls("compute_api")
245 class API(base.Base):
246     """API for interacting with the compute manager."""
247 
248     def __init__(self, image_api=None, network_api=None, volume_api=None,
249                  security_group_api=None, **kwargs):
250         self.image_api = image_api or image.API()
251         self.network_api = network_api or network.API()
252         self.volume_api = volume_api or cinder.API()
253         # NOTE(mriedem): This looks a bit weird but we get the reportclient
254         # via SchedulerClient since it lazy-loads SchedulerReportClient on
255         # the first usage which helps to avoid a bunch of lockutils spam in
256         # the nova-api logs every time the service is restarted (remember
257         # that pretty much all of the REST API controllers construct this
258         # API class).
259         self.placementclient = scheduler_client.SchedulerClient().reportclient
260         self.security_group_api = (security_group_api or
261             openstack_driver.get_openstack_security_group_driver())
262         self.consoleauth_rpcapi = consoleauth_rpcapi.ConsoleAuthAPI()
263         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
264         self.compute_task_api = conductor.ComputeTaskAPI()
265         self.servicegroup_api = servicegroup.API()
266         self.notifier = rpc.get_notifier('compute', CONF.host)
267         if CONF.ephemeral_storage_encryption.enabled:
268             self.key_manager = key_manager.API()
269         # Help us to record host in EventReporter
270         self.host = CONF.host
271         super(API, self).__init__(**kwargs)
272 
273     @property
274     def cell_type(self):
275         try:
276             return getattr(self, '_cell_type')
277         except AttributeError:
278             self._cell_type = cells_opts.get_cell_type()
279             return self._cell_type
280 
281     def _validate_cell(self, instance):
282         if self.cell_type != 'api':
283             return
284         cell_name = instance.cell_name
285         if not cell_name:
286             raise exception.InstanceUnknownCell(
287                     instance_uuid=instance.uuid)
288 
289     def _record_action_start(self, context, instance, action):
290         objects.InstanceAction.action_start(context, instance.uuid,
291                                             action, want_result=False)
292 
293     def _check_injected_file_quota(self, context, injected_files):
294         """Enforce quota limits on injected files.
295 
296         Raises a QuotaError if any limit is exceeded.
297         """
298         if injected_files is None:
299             return
300 
301         # Check number of files first
302         try:
303             objects.Quotas.limit_check(context,
304                                        injected_files=len(injected_files))
305         except exception.OverQuota:
306             raise exception.OnsetFileLimitExceeded()
307 
308         # OK, now count path and content lengths; we're looking for
309         # the max...
310         max_path = 0
311         max_content = 0
312         for path, content in injected_files:
313             max_path = max(max_path, len(path))
314             max_content = max(max_content, len(content))
315 
316         try:
317             objects.Quotas.limit_check(context,
318                                        injected_file_path_bytes=max_path,
319                                        injected_file_content_bytes=max_content)
320         except exception.OverQuota as exc:
321             # Favor path limit over content limit for reporting
322             # purposes
323             if 'injected_file_path_bytes' in exc.kwargs['overs']:
324                 raise exception.OnsetFilePathLimitExceeded(
325                       allowed=exc.kwargs['quotas']['injected_file_path_bytes'])
326             else:
327                 raise exception.OnsetFileContentLimitExceeded(
328                    allowed=exc.kwargs['quotas']['injected_file_content_bytes'])
329 
330     def _check_metadata_properties_quota(self, context, metadata=None):
331         """Enforce quota limits on metadata properties."""
332         if not metadata:
333             metadata = {}
334         if not isinstance(metadata, dict):
335             msg = (_("Metadata type should be dict."))
336             raise exception.InvalidMetadata(reason=msg)
337         num_metadata = len(metadata)
338         try:
339             objects.Quotas.limit_check(context, metadata_items=num_metadata)
340         except exception.OverQuota as exc:
341             quota_metadata = exc.kwargs['quotas']['metadata_items']
342             raise exception.MetadataLimitExceeded(allowed=quota_metadata)
343 
344         # Because metadata is stored in the DB, we hard-code the size limits
345         # In future, we may support more variable length strings, so we act
346         #  as if this is quota-controlled for forwards compatibility.
347         # Those are only used in V2 API, from V2.1 API, those checks are
348         # validated at API layer schema validation.
349         for k, v in metadata.items():
350             try:
351                 utils.check_string_length(v)
352                 utils.check_string_length(k, min_length=1)
353             except exception.InvalidInput as e:
354                 raise exception.InvalidMetadata(reason=e.format_message())
355 
356             if len(k) > 255:
357                 msg = _("Metadata property key greater than 255 characters")
358                 raise exception.InvalidMetadataSize(reason=msg)
359             if len(v) > 255:
360                 msg = _("Metadata property value greater than 255 characters")
361                 raise exception.InvalidMetadataSize(reason=msg)
362 
363     def _check_requested_secgroups(self, context, secgroups):
364         """Check if the security group requested exists and belongs to
365         the project.
366 
367         :param context: The nova request context.
368         :type context: nova.context.RequestContext
369         :param secgroups: list of requested security group names, or uuids in
370             the case of Neutron.
371         :type secgroups: list
372         :returns: list of requested security group names unmodified if using
373             nova-network. If using Neutron, the list returned is all uuids.
374             Note that 'default' is a special case and will be unmodified if
375             it's requested.
376         """
377         security_groups = []
378         for secgroup in secgroups:
379             # NOTE(sdague): default is handled special
380             if secgroup == "default":
381                 security_groups.append(secgroup)
382                 continue
383             secgroup_dict = self.security_group_api.get(context, secgroup)
384             if not secgroup_dict:
385                 raise exception.SecurityGroupNotFoundForProject(
386                     project_id=context.project_id, security_group_id=secgroup)
387 
388             # Check to see if it's a nova-network or neutron type.
389             if isinstance(secgroup_dict['id'], int):
390                 # This is nova-network so just return the requested name.
391                 security_groups.append(secgroup)
392             else:
393                 # The id for neutron is a uuid, so we return the id (uuid).
394                 security_groups.append(secgroup_dict['id'])
395 
396         return security_groups
397 
398     def _check_requested_networks(self, context, requested_networks,
399                                   max_count):
400         """Check if the networks requested belongs to the project
401         and the fixed IP address for each network provided is within
402         same the network block
403         """
404         if requested_networks is not None:
405             if requested_networks.no_allocate:
406                 # If the network request was specifically 'none' meaning don't
407                 # allocate any networks, we just return the number of requested
408                 # instances since quotas don't change at all.
409                 return max_count
410 
411             # NOTE(danms): Temporary transition
412             requested_networks = requested_networks.as_tuples()
413 
414         return self.network_api.validate_networks(context, requested_networks,
415                                                   max_count)
416 
417     def _handle_kernel_and_ramdisk(self, context, kernel_id, ramdisk_id,
418                                    image):
419         """Choose kernel and ramdisk appropriate for the instance.
420 
421         The kernel and ramdisk can be chosen in one of two ways:
422 
423             1. Passed in with create-instance request.
424 
425             2. Inherited from image metadata.
426 
427         If inherited from image metadata, and if that image metadata value is
428         set to 'nokernel', both kernel and ramdisk will default to None.
429         """
430         # Inherit from image if not specified
431         image_properties = image.get('properties', {})
432 
433         if kernel_id is None:
434             kernel_id = image_properties.get('kernel_id')
435 
436         if ramdisk_id is None:
437             ramdisk_id = image_properties.get('ramdisk_id')
438 
439         # Force to None if kernel_id indicates that a kernel is not to be used
440         if kernel_id == 'nokernel':
441             kernel_id = None
442             ramdisk_id = None
443 
444         # Verify kernel and ramdisk exist (fail-fast)
445         if kernel_id is not None:
446             kernel_image = self.image_api.get(context, kernel_id)
447             # kernel_id could have been a URI, not a UUID, so to keep behaviour
448             # from before, which leaked that implementation detail out to the
449             # caller, we return the image UUID of the kernel image and ramdisk
450             # image (below) and not any image URIs that might have been
451             # supplied.
452             # TODO(jaypipes): Get rid of this silliness once we move to a real
453             # Image object and hide all of that stuff within nova.image.api.
454             kernel_id = kernel_image['id']
455 
456         if ramdisk_id is not None:
457             ramdisk_image = self.image_api.get(context, ramdisk_id)
458             ramdisk_id = ramdisk_image['id']
459 
460         return kernel_id, ramdisk_id
461 
462     @staticmethod
463     def parse_availability_zone(context, availability_zone):
464         # NOTE(vish): We have a legacy hack to allow admins to specify hosts
465         #             via az using az:host:node. It might be nice to expose an
466         #             api to specify specific hosts to force onto, but for
467         #             now it just supports this legacy hack.
468         # NOTE(deva): It is also possible to specify az::node, in which case
469         #             the host manager will determine the correct host.
470         forced_host = None
471         forced_node = None
472         if availability_zone and ':' in availability_zone:
473             c = availability_zone.count(':')
474             if c == 1:
475                 availability_zone, forced_host = availability_zone.split(':')
476             elif c == 2:
477                 if '::' in availability_zone:
478                     availability_zone, forced_node = \
479                             availability_zone.split('::')
480                 else:
481                     availability_zone, forced_host, forced_node = \
482                             availability_zone.split(':')
483             else:
484                 raise exception.InvalidInput(
485                         reason="Unable to parse availability_zone")
486 
487         if not availability_zone:
488             availability_zone = CONF.default_schedule_zone
489 
490         return availability_zone, forced_host, forced_node
491 
492     def _ensure_auto_disk_config_is_valid(self, auto_disk_config_img,
493                                           auto_disk_config, image):
494         auto_disk_config_disabled = \
495                 utils.is_auto_disk_config_disabled(auto_disk_config_img)
496         if auto_disk_config_disabled and auto_disk_config:
497             raise exception.AutoDiskConfigDisabledByImage(image=image)
498 
499     def _inherit_properties_from_image(self, image, auto_disk_config):
500         image_properties = image.get('properties', {})
501         auto_disk_config_img = \
502                 utils.get_auto_disk_config_from_image_props(image_properties)
503         self._ensure_auto_disk_config_is_valid(auto_disk_config_img,
504                                                auto_disk_config,
505                                                image.get("id"))
506         if auto_disk_config is None:
507             auto_disk_config = strutils.bool_from_string(auto_disk_config_img)
508 
509         return {
510             'os_type': image_properties.get('os_type'),
511             'architecture': image_properties.get('architecture'),
512             'vm_mode': image_properties.get('vm_mode'),
513             'auto_disk_config': auto_disk_config
514         }
515 
516     def _check_config_drive(self, config_drive):
517         if config_drive:
518             try:
519                 bool_val = strutils.bool_from_string(config_drive,
520                                                      strict=True)
521             except ValueError:
522                 raise exception.ConfigDriveInvalidValue(option=config_drive)
523         else:
524             bool_val = False
525         # FIXME(comstud):  Bug ID 1193438 filed for this. This looks silly,
526         # but this is because the config drive column is a String.  False
527         # is represented by using an empty string.  And for whatever
528         # reason, we rely on the DB to cast True to a String.
529         return True if bool_val else ''
530 
531     def _check_requested_image(self, context, image_id, image,
532                                instance_type, root_bdm):
533         if not image:
534             return
535 
536         if image['status'] != 'active':
537             raise exception.ImageNotActive(image_id=image_id)
538 
539         image_properties = image.get('properties', {})
540         config_drive_option = image_properties.get(
541             'img_config_drive', 'optional')
542         if config_drive_option not in ['optional', 'mandatory']:
543             raise exception.InvalidImageConfigDrive(
544                 config_drive=config_drive_option)
545 
546         if instance_type['memory_mb'] < int(image.get('min_ram') or 0):
547             raise exception.FlavorMemoryTooSmall()
548 
549         # Image min_disk is in gb, size is in bytes. For sanity, have them both
550         # in bytes.
551         image_min_disk = int(image.get('min_disk') or 0) * units.Gi
552         image_size = int(image.get('size') or 0)
553 
554         # Target disk is a volume. Don't check flavor disk size because it
555         # doesn't make sense, and check min_disk against the volume size.
556         if (root_bdm is not None and root_bdm.is_volume):
557             # There are 2 possibilities here: either the target volume already
558             # exists, or it doesn't, in which case the bdm will contain the
559             # intended volume size.
560             #
561             # Cinder does its own check against min_disk, so if the target
562             # volume already exists this has already been done and we don't
563             # need to check it again here. In this case, volume_size may not be
564             # set on the bdm.
565             #
566             # If we're going to create the volume, the bdm will contain
567             # volume_size. Therefore we should check it if it exists. This will
568             # still be checked again by cinder when the volume is created, but
569             # that will not happen until the request reaches a host. By
570             # checking it here, the user gets an immediate and useful failure
571             # indication.
572             #
573             # The third possibility is that we have failed to consider
574             # something, and there are actually more than 2 possibilities. In
575             # this case cinder will still do the check at volume creation time.
576             # The behaviour will still be correct, but the user will not get an
577             # immediate failure from the api, and will instead have to
578             # determine why the instance is in an error state with a task of
579             # block_device_mapping.
580             #
581             # We could reasonably refactor this check into _validate_bdm at
582             # some future date, as the various size logic is already split out
583             # in there.
584             dest_size = root_bdm.volume_size
585             if dest_size is not None:
586                 dest_size *= units.Gi
587 
588                 if image_min_disk > dest_size:
589                     raise exception.VolumeSmallerThanMinDisk(
590                         volume_size=dest_size, image_min_disk=image_min_disk)
591 
592         # Target disk is a local disk whose size is taken from the flavor
593         else:
594             dest_size = instance_type['root_gb'] * units.Gi
595 
596             # NOTE(johannes): root_gb is allowed to be 0 for legacy reasons
597             # since libvirt interpreted the value differently than other
598             # drivers. A value of 0 means don't check size.
599             if dest_size != 0:
600                 if image_size > dest_size:
601                     raise exception.FlavorDiskSmallerThanImage(
602                         flavor_size=dest_size, image_size=image_size)
603 
604                 if image_min_disk > dest_size:
605                     raise exception.FlavorDiskSmallerThanMinDisk(
606                         flavor_size=dest_size, image_min_disk=image_min_disk)
607 
608     def _get_image_defined_bdms(self, instance_type, image_meta,
609                                 root_device_name):
610         image_properties = image_meta.get('properties', {})
611 
612         # Get the block device mappings defined by the image.
613         image_defined_bdms = image_properties.get('block_device_mapping', [])
614         legacy_image_defined = not image_properties.get('bdm_v2', False)
615 
616         image_mapping = image_properties.get('mappings', [])
617 
618         if legacy_image_defined:
619             image_defined_bdms = block_device.from_legacy_mapping(
620                 image_defined_bdms, None, root_device_name)
621         else:
622             image_defined_bdms = list(map(block_device.BlockDeviceDict,
623                                           image_defined_bdms))
624 
625         if image_mapping:
626             image_mapping = self._prepare_image_mapping(instance_type,
627                                                         image_mapping)
628             image_defined_bdms = self._merge_bdms_lists(
629                 image_mapping, image_defined_bdms)
630 
631         return image_defined_bdms
632 
633     def _get_flavor_defined_bdms(self, instance_type, block_device_mapping):
634         flavor_defined_bdms = []
635 
636         have_ephemeral_bdms = any(filter(
637             block_device.new_format_is_ephemeral, block_device_mapping))
638         have_swap_bdms = any(filter(
639             block_device.new_format_is_swap, block_device_mapping))
640 
641         if instance_type.get('ephemeral_gb') and not have_ephemeral_bdms:
642             flavor_defined_bdms.append(
643                 block_device.create_blank_bdm(instance_type['ephemeral_gb']))
644         if instance_type.get('swap') and not have_swap_bdms:
645             flavor_defined_bdms.append(
646                 block_device.create_blank_bdm(instance_type['swap'], 'swap'))
647 
648         return flavor_defined_bdms
649 
650     def _merge_bdms_lists(self, overridable_mappings, overrider_mappings):
651         """Override any block devices from the first list by device name
652 
653         :param overridable_mappings: list which items are overridden
654         :param overrider_mappings: list which items override
655 
656         :returns: A merged list of bdms
657         """
658         device_names = set(bdm['device_name'] for bdm in overrider_mappings
659                            if bdm['device_name'])
660         return (overrider_mappings +
661                 [bdm for bdm in overridable_mappings
662                  if bdm['device_name'] not in device_names])
663 
664     def _check_and_transform_bdm(self, context, base_options, instance_type,
665                                  image_meta, min_count, max_count,
666                                  block_device_mapping, legacy_bdm):
667         # NOTE (ndipanov): Assume root dev name is 'vda' if not supplied.
668         #                  It's needed for legacy conversion to work.
669         root_device_name = (base_options.get('root_device_name') or 'vda')
670         image_ref = base_options.get('image_ref', '')
671         # If the instance is booted by image and has a volume attached,
672         # the volume cannot have the same device name as root_device_name
673         if image_ref:
674             for bdm in block_device_mapping:
675                 if (bdm.get('destination_type') == 'volume' and
676                     block_device.strip_dev(bdm.get(
677                     'device_name')) == root_device_name):
678                     msg = _('The volume cannot be assigned the same device'
679                             ' name as the root device %s') % root_device_name
680                     raise exception.InvalidRequest(msg)
681 
682         image_defined_bdms = self._get_image_defined_bdms(
683             instance_type, image_meta, root_device_name)
684         root_in_image_bdms = (
685             block_device.get_root_bdm(image_defined_bdms) is not None)
686 
687         if legacy_bdm:
688             block_device_mapping = block_device.from_legacy_mapping(
689                 block_device_mapping, image_ref, root_device_name,
690                 no_root=root_in_image_bdms)
691         elif root_in_image_bdms:
692             # NOTE (ndipanov): client will insert an image mapping into the v2
693             # block_device_mapping, but if there is a bootable device in image
694             # mappings - we need to get rid of the inserted image
695             # NOTE (gibi): another case is when a server is booted with an
696             # image to bdm mapping where the image only contains a bdm to a
697             # snapshot. In this case the other image to bdm mapping
698             # contains an unnecessary device with boot_index == 0.
699             # Also in this case the image_ref is None as we are booting from
700             # an image to volume bdm.
701             def not_image_and_root_bdm(bdm):
702                 return not (bdm.get('boot_index') == 0 and
703                             bdm.get('source_type') == 'image')
704 
705             block_device_mapping = list(
706                 filter(not_image_and_root_bdm, block_device_mapping))
707 
708         block_device_mapping = self._merge_bdms_lists(
709             image_defined_bdms, block_device_mapping)
710 
711         if min_count > 1 or max_count > 1:
712             if any(map(lambda bdm: bdm['source_type'] == 'volume',
713                        block_device_mapping)):
714                 msg = _('Cannot attach one or more volumes to multiple'
715                         ' instances')
716                 raise exception.InvalidRequest(msg)
717 
718         block_device_mapping += self._get_flavor_defined_bdms(
719             instance_type, block_device_mapping)
720 
721         return block_device_obj.block_device_make_list_from_dicts(
722                 context, block_device_mapping)
723 
724     def _get_image(self, context, image_href):
725         if not image_href:
726             return None, {}
727 
728         image = self.image_api.get(context, image_href)
729         return image['id'], image
730 
731     def _checks_for_create_and_rebuild(self, context, image_id, image,
732                                        instance_type, metadata,
733                                        files_to_inject, root_bdm):
734         self._check_metadata_properties_quota(context, metadata)
735         self._check_injected_file_quota(context, files_to_inject)
736         self._check_requested_image(context, image_id, image,
737                                     instance_type, root_bdm)
738 
739     def _validate_and_build_base_options(self, context, instance_type,
740                                          boot_meta, image_href, image_id,
741                                          kernel_id, ramdisk_id, display_name,
742                                          display_description, key_name,
743                                          key_data, security_groups,
744                                          availability_zone, user_data,
745                                          metadata, access_ip_v4, access_ip_v6,
746                                          requested_networks, config_drive,
747                                          auto_disk_config, reservation_id,
748                                          max_count):
749         """Verify all the input parameters regardless of the provisioning
750         strategy being performed.
751         """
752         if instance_type['disabled']:
753             raise exception.FlavorNotFound(flavor_id=instance_type['id'])
754 
755         if user_data:
756             try:
757                 base64utils.decode_as_bytes(user_data)
758             except TypeError:
759                 raise exception.InstanceUserDataMalformed()
760 
761         # When using Neutron, _check_requested_secgroups will translate and
762         # return any requested security group names to uuids.
763         security_groups = (
764             self._check_requested_secgroups(context, security_groups))
765 
766         # Note:  max_count is the number of instances requested by the user,
767         # max_network_count is the maximum number of instances taking into
768         # account any network quotas
769         max_network_count = self._check_requested_networks(context,
770                                      requested_networks, max_count)
771 
772         kernel_id, ramdisk_id = self._handle_kernel_and_ramdisk(
773                 context, kernel_id, ramdisk_id, boot_meta)
774 
775         config_drive = self._check_config_drive(config_drive)
776 
777         if key_data is None and key_name is not None:
778             key_pair = objects.KeyPair.get_by_name(context,
779                                                    context.user_id,
780                                                    key_name)
781             key_data = key_pair.public_key
782         else:
783             key_pair = None
784 
785         root_device_name = block_device.prepend_dev(
786                 block_device.properties_root_device_name(
787                     boot_meta.get('properties', {})))
788 
789         try:
790             image_meta = objects.ImageMeta.from_dict(boot_meta)
791         except ValueError as e:
792             # there must be invalid values in the image meta properties so
793             # consider this an invalid request
794             msg = _('Invalid image metadata. Error: %s') % six.text_type(e)
795             raise exception.InvalidRequest(msg)
796         numa_topology = hardware.numa_get_constraints(
797                 instance_type, image_meta)
798 
799         system_metadata = {}
800 
801         # PCI requests come from two sources: instance flavor and
802         # requested_networks. The first call in below returns an
803         # InstancePCIRequests object which is a list of InstancePCIRequest
804         # objects. The second call in below creates an InstancePCIRequest
805         # object for each SR-IOV port, and append it to the list in the
806         # InstancePCIRequests object
807         pci_request_info = pci_request.get_pci_requests_from_flavor(
808             instance_type)
809         self.network_api.create_pci_requests_for_sriov_ports(context,
810             pci_request_info, requested_networks)
811 
812         base_options = {
813             'reservation_id': reservation_id,
814             'image_ref': image_href,
815             'kernel_id': kernel_id or '',
816             'ramdisk_id': ramdisk_id or '',
817             'power_state': power_state.NOSTATE,
818             'vm_state': vm_states.BUILDING,
819             'config_drive': config_drive,
820             'user_id': context.user_id,
821             'project_id': context.project_id,
822             'instance_type_id': instance_type['id'],
823             'memory_mb': instance_type['memory_mb'],
824             'vcpus': instance_type['vcpus'],
825             'root_gb': instance_type['root_gb'],
826             'ephemeral_gb': instance_type['ephemeral_gb'],
827             'display_name': display_name,
828             'display_description': display_description,
829             'user_data': user_data,
830             'key_name': key_name,
831             'key_data': key_data,
832             'locked': False,
833             'metadata': metadata or {},
834             'access_ip_v4': access_ip_v4,
835             'access_ip_v6': access_ip_v6,
836             'availability_zone': availability_zone,
837             'root_device_name': root_device_name,
838             'progress': 0,
839             'pci_requests': pci_request_info,
840             'numa_topology': numa_topology,
841             'system_metadata': system_metadata}
842 
843         options_from_image = self._inherit_properties_from_image(
844                 boot_meta, auto_disk_config)
845 
846         base_options.update(options_from_image)
847 
848         # return the validated options and maximum number of instances allowed
849         # by the network quotas
850         return base_options, max_network_count, key_pair, security_groups
851 
852     def _provision_instances(self, context, instance_type, min_count,
853             max_count, base_options, boot_meta, security_groups,
854             block_device_mapping, shutdown_terminate,
855             instance_group, check_server_group_quota, filter_properties,
856             key_pair, tags, supports_multiattach=False):
857         # Check quotas
858         num_instances = compute_utils.check_num_instances_quota(
859                 context, instance_type, min_count, max_count)
860         security_groups = self.security_group_api.populate_security_groups(
861                 security_groups)
862         self.security_group_api.ensure_default(context)
863         LOG.debug("Going to run %s instances...", num_instances)
864         instances_to_build = []
865         try:
866             for i in range(num_instances):
867                 # Create a uuid for the instance so we can store the
868                 # RequestSpec before the instance is created.
869                 instance_uuid = uuidutils.generate_uuid()
870                 # Store the RequestSpec that will be used for scheduling.
871                 req_spec = objects.RequestSpec.from_components(context,
872                         instance_uuid, boot_meta, instance_type,
873                         base_options['numa_topology'],
874                         base_options['pci_requests'], filter_properties,
875                         instance_group, base_options['availability_zone'],
876                         security_groups=security_groups)
877                 # NOTE(danms): We need to record num_instances on the request
878                 # spec as this is how the conductor knows how many were in this
879                 # batch.
880                 req_spec.num_instances = num_instances
881                 req_spec.create()
882 
883                 # Create an instance object, but do not store in db yet.
884                 instance = objects.Instance(context=context)
885                 instance.uuid = instance_uuid
886                 instance.update(base_options)
887                 instance.keypairs = objects.KeyPairList(objects=[])
888                 if key_pair:
889                     instance.keypairs.objects.append(key_pair)
890                 instance = self.create_db_entry_for_new_instance(context,
891                         instance_type, boot_meta, instance, security_groups,
892                         block_device_mapping, num_instances, i,
893                         shutdown_terminate, create_instance=False)
894                 block_device_mapping = (
895                     self._bdm_validate_set_size_and_instance(context,
896                         instance, instance_type, block_device_mapping,
897                         supports_multiattach))
898                 instance_tags = self._transform_tags(tags, instance.uuid)
899 
900                 build_request = objects.BuildRequest(context,
901                         instance=instance, instance_uuid=instance.uuid,
902                         project_id=instance.project_id,
903                         block_device_mappings=block_device_mapping,
904                         tags=instance_tags)
905                 build_request.create()
906 
907                 # Create an instance_mapping.  The null cell_mapping indicates
908                 # that the instance doesn't yet exist in a cell, and lookups
909                 # for it need to instead look for the RequestSpec.
910                 # cell_mapping will be populated after scheduling, with a
911                 # scheduling failure using the cell_mapping for the special
912                 # cell0.
913                 inst_mapping = objects.InstanceMapping(context=context)
914                 inst_mapping.instance_uuid = instance_uuid
915                 inst_mapping.project_id = context.project_id
916                 inst_mapping.cell_mapping = None
917                 inst_mapping.create()
918 
919                 instances_to_build.append(
920                     (req_spec, build_request, inst_mapping))
921 
922                 if instance_group:
923                     if check_server_group_quota:
924                         try:
925                             objects.Quotas.check_deltas(
926                                 context, {'server_group_members': 1},
927                                 instance_group, context.user_id)
928                         except exception.OverQuota:
929                             msg = _("Quota exceeded, too many servers in "
930                                     "group")
931                             raise exception.QuotaError(msg)
932 
933                     members = objects.InstanceGroup.add_members(
934                         context, instance_group.uuid, [instance.uuid])
935 
936                     # NOTE(melwitt): We recheck the quota after creating the
937                     # object to prevent users from allocating more resources
938                     # than their allowed quota in the event of a race. This is
939                     # configurable because it can be expensive if strict quota
940                     # limits are not required in a deployment.
941                     if CONF.quota.recheck_quota and check_server_group_quota:
942                         try:
943                             objects.Quotas.check_deltas(
944                                 context, {'server_group_members': 0},
945                                 instance_group, context.user_id)
946                         except exception.OverQuota:
947                             objects.InstanceGroup._remove_members_in_db(
948                                 context, instance_group.id, [instance.uuid])
949                             msg = _("Quota exceeded, too many servers in "
950                                     "group")
951                             raise exception.QuotaError(msg)
952                     # list of members added to servers group in this iteration
953                     # is needed to check quota of server group during add next
954                     # instance
955                     instance_group.members.extend(members)
956 
957         # In the case of any exceptions, attempt DB cleanup
958         except Exception:
959             with excutils.save_and_reraise_exception():
960                 self._cleanup_build_artifacts(None, instances_to_build)
961 
962         return instances_to_build
963 
964     def _get_bdm_image_metadata(self, context, block_device_mapping,
965                                 legacy_bdm=True):
966         """If we are booting from a volume, we need to get the
967         volume details from Cinder and make sure we pass the
968         metadata back accordingly.
969         """
970         if not block_device_mapping:
971             return {}
972 
973         for bdm in block_device_mapping:
974             if (legacy_bdm and
975                     block_device.get_device_letter(
976                        bdm.get('device_name', '')) != 'a'):
977                 continue
978             elif not legacy_bdm and bdm.get('boot_index') != 0:
979                 continue
980 
981             volume_id = bdm.get('volume_id')
982             snapshot_id = bdm.get('snapshot_id')
983             if snapshot_id:
984                 # NOTE(alaski): A volume snapshot inherits metadata from the
985                 # originating volume, but the API does not expose metadata
986                 # on the snapshot itself.  So we query the volume for it below.
987                 snapshot = self.volume_api.get_snapshot(context, snapshot_id)
988                 volume_id = snapshot['volume_id']
989 
990             if bdm.get('image_id'):
991                 try:
992                     image_id = bdm['image_id']
993                     image_meta = self.image_api.get(context, image_id)
994                     return image_meta
995                 except Exception:
996                     raise exception.InvalidBDMImage(id=image_id)
997             elif volume_id:
998                 try:
999                     volume = self.volume_api.get(context, volume_id)
1000                 except exception.CinderConnectionFailed:
1001                     raise
1002                 except Exception:
1003                     raise exception.InvalidBDMVolume(id=volume_id)
1004 
1005                 if not volume.get('bootable', True):
1006                     raise exception.InvalidBDMVolumeNotBootable(id=volume_id)
1007 
1008                 return utils.get_image_metadata_from_volume(volume)
1009         return {}
1010 
1011     @staticmethod
1012     def _get_requested_instance_group(context, filter_properties):
1013         if (not filter_properties or
1014                 not filter_properties.get('scheduler_hints')):
1015             return
1016 
1017         group_hint = filter_properties.get('scheduler_hints').get('group')
1018         if not group_hint:
1019             return
1020 
1021         return objects.InstanceGroup.get_by_uuid(context, group_hint)
1022 
1023     def _create_instance(self, context, instance_type,
1024                image_href, kernel_id, ramdisk_id,
1025                min_count, max_count,
1026                display_name, display_description,
1027                key_name, key_data, security_groups,
1028                availability_zone, user_data, metadata, injected_files,
1029                admin_password, access_ip_v4, access_ip_v6,
1030                requested_networks, config_drive,
1031                block_device_mapping, auto_disk_config, filter_properties,
1032                reservation_id=None, legacy_bdm=True, shutdown_terminate=False,
1033                check_server_group_quota=False, tags=None,
1034                supports_multiattach=False):
1035         """Verify all the input parameters regardless of the provisioning
1036         strategy being performed and schedule the instance(s) for
1037         creation.
1038         """
1039 
1040         # Normalize and setup some parameters
1041         if reservation_id is None:
1042             reservation_id = utils.generate_uid('r')
1043         security_groups = security_groups or ['default']
1044         min_count = min_count or 1
1045         max_count = max_count or min_count
1046         block_device_mapping = block_device_mapping or []
1047         tags = tags or []
1048 
1049         if image_href:
1050             image_id, boot_meta = self._get_image(context, image_href)
1051         else:
1052             image_id = None
1053             boot_meta = self._get_bdm_image_metadata(
1054                 context, block_device_mapping, legacy_bdm)
1055 
1056         self._check_auto_disk_config(image=boot_meta,
1057                                      auto_disk_config=auto_disk_config)
1058 
1059         base_options, max_net_count, key_pair, security_groups = \
1060                 self._validate_and_build_base_options(
1061                     context, instance_type, boot_meta, image_href, image_id,
1062                     kernel_id, ramdisk_id, display_name, display_description,
1063                     key_name, key_data, security_groups, availability_zone,
1064                     user_data, metadata, access_ip_v4, access_ip_v6,
1065                     requested_networks, config_drive, auto_disk_config,
1066                     reservation_id, max_count)
1067 
1068         # max_net_count is the maximum number of instances requested by the
1069         # user adjusted for any network quota constraints, including
1070         # consideration of connections to each requested network
1071         if max_net_count < min_count:
1072             raise exception.PortLimitExceeded()
1073         elif max_net_count < max_count:
1074             LOG.info("max count reduced from %(max_count)d to "
1075                      "%(max_net_count)d due to network port quota",
1076                      {'max_count': max_count,
1077                       'max_net_count': max_net_count})
1078             max_count = max_net_count
1079 
1080         block_device_mapping = self._check_and_transform_bdm(context,
1081             base_options, instance_type, boot_meta, min_count, max_count,
1082             block_device_mapping, legacy_bdm)
1083 
1084         # We can't do this check earlier because we need bdms from all sources
1085         # to have been merged in order to get the root bdm.
1086         self._checks_for_create_and_rebuild(context, image_id, boot_meta,
1087                 instance_type, metadata, injected_files,
1088                 block_device_mapping.root_bdm())
1089 
1090         instance_group = self._get_requested_instance_group(context,
1091                                    filter_properties)
1092 
1093         tags = self._create_tag_list_obj(context, tags)
1094 
1095         instances_to_build = self._provision_instances(
1096             context, instance_type, min_count, max_count, base_options,
1097             boot_meta, security_groups, block_device_mapping,
1098             shutdown_terminate, instance_group, check_server_group_quota,
1099             filter_properties, key_pair, tags, supports_multiattach)
1100 
1101         instances = []
1102         request_specs = []
1103         build_requests = []
1104         for rs, build_request, im in instances_to_build:
1105             build_requests.append(build_request)
1106             instance = build_request.get_new_instance(context)
1107             instances.append(instance)
1108             request_specs.append(rs)
1109 
1110         if CONF.cells.enable:
1111             # NOTE(danms): CellsV1 can't do the new thing, so we
1112             # do the old thing here. We can remove this path once
1113             # we stop supporting v1.
1114             for instance in instances:
1115                 instance.create()
1116             # NOTE(melwitt): We recheck the quota after creating the objects
1117             # to prevent users from allocating more resources than their
1118             # allowed quota in the event of a race. This is configurable
1119             # because it can be expensive if strict quota limits are not
1120             # required in a deployment.
1121             if CONF.quota.recheck_quota:
1122                 try:
1123                     compute_utils.check_num_instances_quota(
1124                         context, instance_type, 0, 0,
1125                         orig_num_req=len(instances))
1126                 except exception.TooManyInstances:
1127                     with excutils.save_and_reraise_exception():
1128                         # Need to clean up all the instances we created
1129                         # along with the build requests, request specs,
1130                         # and instance mappings.
1131                         self._cleanup_build_artifacts(instances,
1132                                                       instances_to_build)
1133 
1134             self.compute_task_api.build_instances(context,
1135                 instances=instances, image=boot_meta,
1136                 filter_properties=filter_properties,
1137                 admin_password=admin_password,
1138                 injected_files=injected_files,
1139                 requested_networks=requested_networks,
1140                 security_groups=security_groups,
1141                 block_device_mapping=block_device_mapping,
1142                 legacy_bdm=False)
1143         else:
1144             self.compute_task_api.schedule_and_build_instances(
1145                 context,
1146                 build_requests=build_requests,
1147                 request_spec=request_specs,
1148                 image=boot_meta,
1149                 admin_password=admin_password,
1150                 injected_files=injected_files,
1151                 requested_networks=requested_networks,
1152                 block_device_mapping=block_device_mapping,
1153                 tags=tags)
1154 
1155         return instances, reservation_id
1156 
1157     @staticmethod
1158     def _cleanup_build_artifacts(instances, instances_to_build):
1159         # instances_to_build is a list of tuples:
1160         # (RequestSpec, BuildRequest, InstanceMapping)
1161 
1162         # Be paranoid about artifacts being deleted underneath us.
1163         for instance in instances or []:
1164             try:
1165                 instance.destroy()
1166             except exception.InstanceNotFound:
1167                 pass
1168         for rs, build_request, im in instances_to_build or []:
1169             try:
1170                 rs.destroy()
1171             except exception.RequestSpecNotFound:
1172                 pass
1173             try:
1174                 build_request.destroy()
1175             except exception.BuildRequestNotFound:
1176                 pass
1177             try:
1178                 im.destroy()
1179             except exception.InstanceMappingNotFound:
1180                 pass
1181 
1182     @staticmethod
1183     def _volume_size(instance_type, bdm):
1184         size = bdm.get('volume_size')
1185         # NOTE (ndipanov): inherit flavor size only for swap and ephemeral
1186         if (size is None and bdm.get('source_type') == 'blank' and
1187                 bdm.get('destination_type') == 'local'):
1188             if bdm.get('guest_format') == 'swap':
1189                 size = instance_type.get('swap', 0)
1190             else:
1191                 size = instance_type.get('ephemeral_gb', 0)
1192         return size
1193 
1194     def _prepare_image_mapping(self, instance_type, mappings):
1195         """Extract and format blank devices from image mappings."""
1196 
1197         prepared_mappings = []
1198 
1199         for bdm in block_device.mappings_prepend_dev(mappings):
1200             LOG.debug("Image bdm %s", bdm)
1201 
1202             virtual_name = bdm['virtual']
1203             if virtual_name == 'ami' or virtual_name == 'root':
1204                 continue
1205 
1206             if not block_device.is_swap_or_ephemeral(virtual_name):
1207                 continue
1208 
1209             guest_format = bdm.get('guest_format')
1210             if virtual_name == 'swap':
1211                 guest_format = 'swap'
1212             if not guest_format:
1213                 guest_format = CONF.default_ephemeral_format
1214 
1215             values = block_device.BlockDeviceDict({
1216                 'device_name': bdm['device'],
1217                 'source_type': 'blank',
1218                 'destination_type': 'local',
1219                 'device_type': 'disk',
1220                 'guest_format': guest_format,
1221                 'delete_on_termination': True,
1222                 'boot_index': -1})
1223 
1224             values['volume_size'] = self._volume_size(
1225                 instance_type, values)
1226             if values['volume_size'] == 0:
1227                 continue
1228 
1229             prepared_mappings.append(values)
1230 
1231         return prepared_mappings
1232 
1233     def _bdm_validate_set_size_and_instance(self, context, instance,
1234                                             instance_type,
1235                                             block_device_mapping,
1236                                             supports_multiattach=False):
1237         """Ensure the bdms are valid, then set size and associate with instance
1238 
1239         Because this method can be called multiple times when more than one
1240         instance is booted in a single request it makes a copy of the bdm list.
1241         """
1242         LOG.debug("block_device_mapping %s", list(block_device_mapping),
1243                   instance_uuid=instance.uuid)
1244         self._validate_bdm(
1245             context, instance, instance_type, block_device_mapping,
1246             supports_multiattach)
1247         instance_block_device_mapping = block_device_mapping.obj_clone()
1248         for bdm in instance_block_device_mapping:
1249             bdm.volume_size = self._volume_size(instance_type, bdm)
1250             bdm.instance_uuid = instance.uuid
1251         return instance_block_device_mapping
1252 
1253     def _create_block_device_mapping(self, block_device_mapping):
1254         # Copy the block_device_mapping because this method can be called
1255         # multiple times when more than one instance is booted in a single
1256         # request. This avoids 'id' being set and triggering the object dupe
1257         # detection
1258         db_block_device_mapping = copy.deepcopy(block_device_mapping)
1259         # Create the BlockDeviceMapping objects in the db.
1260         for bdm in db_block_device_mapping:
1261             # TODO(alaski): Why is this done?
1262             if bdm.volume_size == 0:
1263                 continue
1264 
1265             bdm.update_or_create()
1266 
1267     def _validate_bdm(self, context, instance, instance_type,
1268                       block_device_mappings, supports_multiattach=False):
1269         # Make sure that the boot indexes make sense.
1270         # Setting a negative value or None indicates that the device should not
1271         # be used for booting.
1272         boot_indexes = sorted([bdm.boot_index
1273                                for bdm in block_device_mappings
1274                                if bdm.boot_index is not None
1275                                and bdm.boot_index >= 0])
1276 
1277         # Each device which is capable of being used as boot device should
1278         # be given a unique boot index, starting from 0 in ascending order, and
1279         # there needs to be at least one boot device.
1280         if not boot_indexes or any(i != v for i, v in enumerate(boot_indexes)):
1281             # Convert the BlockDeviceMappingList to a list for repr details.
1282             LOG.debug('Invalid block device mapping boot sequence for '
1283                       'instance: %s', list(block_device_mappings),
1284                       instance=instance)
1285             raise exception.InvalidBDMBootSequence()
1286 
1287         for bdm in block_device_mappings:
1288             # NOTE(vish): For now, just make sure the volumes are accessible.
1289             # Additionally, check that the volume can be attached to this
1290             # instance.
1291             snapshot_id = bdm.snapshot_id
1292             volume_id = bdm.volume_id
1293             image_id = bdm.image_id
1294             if (image_id is not None and
1295                     image_id != instance.get('image_ref')):
1296                 try:
1297                     self._get_image(context, image_id)
1298                 except Exception:
1299                     raise exception.InvalidBDMImage(id=image_id)
1300                 if (bdm.source_type == 'image' and
1301                         bdm.destination_type == 'volume' and
1302                         not bdm.volume_size):
1303                     raise exception.InvalidBDM(message=_("Images with "
1304                         "destination_type 'volume' need to have a non-zero "
1305                         "size specified"))
1306             elif volume_id is not None:
1307                 # The instance is being created and we don't know which
1308                 # cell it's going to land in, so check all cells.
1309                 min_compute_version = \
1310                     objects.service.get_minimum_version_all_cells(
1311                         context, ['nova-compute'])
1312                 try:
1313                     # NOTE(ildikov): The boot from volume operation did not
1314                     # reserve the volume before Pike and as the older computes
1315                     # are running 'check_attach' which will fail if the volume
1316                     # is in 'attaching' state; if the compute service version
1317                     # is not high enough we will just perform the old check as
1318                     # opposed to reserving the volume here.
1319                     volume = self.volume_api.get(context, volume_id)
1320                     if (min_compute_version >=
1321                         BFV_RESERVE_MIN_COMPUTE_VERSION):
1322                         self._check_attach_and_reserve_volume(
1323                             context, volume, instance, bdm,
1324                             supports_multiattach)
1325                     else:
1326                         # NOTE(ildikov): This call is here only for backward
1327                         # compatibility can be removed after Ocata EOL.
1328                         self._check_attach(context, volume, instance)
1329                     bdm.volume_size = volume.get('size')
1330 
1331                     # NOTE(mnaser): If we end up reserving the volume, it will
1332                     #               not have an attachment_id which is needed
1333                     #               for cleanups.  This can be removed once
1334                     #               all calls to reserve_volume are gone.
1335                     if 'attachment_id' not in bdm:
1336                         bdm.attachment_id = None
1337                 except (exception.CinderConnectionFailed,
1338                         exception.InvalidVolume,
1339                         exception.MultiattachNotSupportedOldMicroversion,
1340                         exception.MultiattachSupportNotYetAvailable):
1341                     raise
1342                 except exception.InvalidInput as exc:
1343                     raise exception.InvalidVolume(reason=exc.format_message())
1344                 except Exception:
1345                     raise exception.InvalidBDMVolume(id=volume_id)
1346             elif snapshot_id is not None:
1347                 try:
1348                     snap = self.volume_api.get_snapshot(context, snapshot_id)
1349                     bdm.volume_size = bdm.volume_size or snap.get('size')
1350                 except exception.CinderConnectionFailed:
1351                     raise
1352                 except Exception:
1353                     raise exception.InvalidBDMSnapshot(id=snapshot_id)
1354             elif (bdm.source_type == 'blank' and
1355                     bdm.destination_type == 'volume' and
1356                     not bdm.volume_size):
1357                 raise exception.InvalidBDM(message=_("Blank volumes "
1358                     "(source: 'blank', dest: 'volume') need to have non-zero "
1359                     "size"))
1360 
1361         ephemeral_size = sum(bdm.volume_size or instance_type['ephemeral_gb']
1362                 for bdm in block_device_mappings
1363                 if block_device.new_format_is_ephemeral(bdm))
1364         if ephemeral_size > instance_type['ephemeral_gb']:
1365             raise exception.InvalidBDMEphemeralSize()
1366 
1367         # There should be only one swap
1368         swap_list = block_device.get_bdm_swap_list(block_device_mappings)
1369         if len(swap_list) > 1:
1370             msg = _("More than one swap drive requested.")
1371             raise exception.InvalidBDMFormat(details=msg)
1372 
1373         if swap_list:
1374             swap_size = swap_list[0].volume_size or 0
1375             if swap_size > instance_type['swap']:
1376                 raise exception.InvalidBDMSwapSize()
1377 
1378         max_local = CONF.max_local_block_devices
1379         if max_local >= 0:
1380             num_local = len([bdm for bdm in block_device_mappings
1381                              if bdm.destination_type == 'local'])
1382             if num_local > max_local:
1383                 raise exception.InvalidBDMLocalsLimit()
1384 
1385     def _check_attach(self, context, volume, instance):
1386         # TODO(ildikov): This check_attach code is kept only for backward
1387         # compatibility and should be removed after Ocata EOL.
1388         if volume['status'] != 'available':
1389             msg = _("volume '%(vol)s' status must be 'available'. Currently "
1390                     "in '%(status)s'") % {'vol': volume['id'],
1391                                           'status': volume['status']}
1392             raise exception.InvalidVolume(reason=msg)
1393         if volume['attach_status'] == 'attached':
1394             msg = _("volume %s already attached") % volume['id']
1395             raise exception.InvalidVolume(reason=msg)
1396         self.volume_api.check_availability_zone(context, volume,
1397                                                 instance=instance)
1398 
1399     def _populate_instance_names(self, instance, num_instances, index):
1400         """Populate instance display_name and hostname."""
1401         display_name = instance.get('display_name')
1402         if instance.obj_attr_is_set('hostname'):
1403             hostname = instance.get('hostname')
1404         else:
1405             hostname = None
1406 
1407         # NOTE(mriedem): This is only here for test simplicity since a server
1408         # name is required in the REST API.
1409         if display_name is None:
1410             display_name = self._default_display_name(instance.uuid)
1411             instance.display_name = display_name
1412 
1413         if hostname is None and num_instances == 1:
1414             hostname = display_name
1415             default_hostname = self._default_host_name(instance.uuid)
1416             instance.hostname = utils.sanitize_hostname(hostname,
1417                                                         default_hostname)
1418 
1419         if num_instances > 1 and self.cell_type != 'api':
1420             original_name = instance.display_name
1421             new_name = '%s-%d' % (original_name, index + 1)
1422             instance.display_name = new_name
1423             if not instance.get('hostname', None):
1424                 if utils.sanitize_hostname(original_name) == "":
1425                     instance.hostname = self._default_host_name(instance.uuid)
1426                 else:
1427                     instance.hostname = utils.sanitize_hostname(new_name)
1428 
1429     def _default_display_name(self, instance_uuid):
1430         return "Server %s" % instance_uuid
1431 
1432     def _default_host_name(self, instance_uuid):
1433         return "Server-%s" % instance_uuid
1434 
1435     def _populate_instance_for_create(self, context, instance, image,
1436                                       index, security_groups, instance_type,
1437                                       num_instances, shutdown_terminate):
1438         """Build the beginning of a new instance."""
1439 
1440         instance.launch_index = index
1441         instance.vm_state = vm_states.BUILDING
1442         instance.task_state = task_states.SCHEDULING
1443         info_cache = objects.InstanceInfoCache()
1444         info_cache.instance_uuid = instance.uuid
1445         info_cache.network_info = network_model.NetworkInfo()
1446         instance.info_cache = info_cache
1447         instance.flavor = instance_type
1448         instance.old_flavor = None
1449         instance.new_flavor = None
1450         if CONF.ephemeral_storage_encryption.enabled:
1451             # NOTE(kfarr): dm-crypt expects the cipher in a
1452             # hyphenated format: cipher-chainmode-ivmode
1453             # (ex: aes-xts-plain64). The algorithm needs
1454             # to be parsed out to pass to the key manager (ex: aes).
1455             cipher = CONF.ephemeral_storage_encryption.cipher
1456             algorithm = cipher.split('-')[0] if cipher else None
1457             instance.ephemeral_key_uuid = self.key_manager.create_key(
1458                 context,
1459                 algorithm=algorithm,
1460                 length=CONF.ephemeral_storage_encryption.key_size)
1461         else:
1462             instance.ephemeral_key_uuid = None
1463 
1464         # Store image properties so we can use them later
1465         # (for notifications, etc).  Only store what we can.
1466         if not instance.obj_attr_is_set('system_metadata'):
1467             instance.system_metadata = {}
1468         # Make sure we have the dict form that we need for instance_update.
1469         instance.system_metadata = utils.instance_sys_meta(instance)
1470 
1471         system_meta = utils.get_system_metadata_from_image(
1472             image, instance_type)
1473 
1474         # In case we couldn't find any suitable base_image
1475         system_meta.setdefault('image_base_image_ref', instance.image_ref)
1476 
1477         system_meta['owner_user_name'] = context.user_name
1478         system_meta['owner_project_name'] = context.project_name
1479 
1480         instance.system_metadata.update(system_meta)
1481 
1482         if CONF.use_neutron:
1483             # For Neutron we don't actually store anything in the database, we
1484             # proxy the security groups on the instance from the ports
1485             # attached to the instance.
1486             instance.security_groups = objects.SecurityGroupList()
1487         else:
1488             instance.security_groups = security_groups
1489 
1490         self._populate_instance_names(instance, num_instances, index)
1491         instance.shutdown_terminate = shutdown_terminate
1492 
1493         return instance
1494 
1495     def _create_tag_list_obj(self, context, tags):
1496         """Create TagList objects from simple string tags.
1497 
1498         :param context: security context.
1499         :param tags: simple string tags from API request.
1500         :returns: TagList object.
1501         """
1502         tag_list = [objects.Tag(context=context, tag=t) for t in tags]
1503         tag_list_obj = objects.TagList(objects=tag_list)
1504         return tag_list_obj
1505 
1506     def _transform_tags(self, tags, resource_id):
1507         """Change the resource_id of the tags according to the input param.
1508 
1509         Because this method can be called multiple times when more than one
1510         instance is booted in a single request it makes a copy of the tags
1511         list.
1512 
1513         :param tags: TagList object.
1514         :param resource_id: string.
1515         :returns: TagList object.
1516         """
1517         instance_tags = tags.obj_clone()
1518         for tag in instance_tags:
1519             tag.resource_id = resource_id
1520         return instance_tags
1521 
1522     # This method remains because cellsv1 uses it in the scheduler
1523     def create_db_entry_for_new_instance(self, context, instance_type, image,
1524             instance, security_group, block_device_mapping, num_instances,
1525             index, shutdown_terminate=False, create_instance=True):
1526         """Create an entry in the DB for this new instance,
1527         including any related table updates (such as security group,
1528         etc).
1529 
1530         This is called by the scheduler after a location for the
1531         instance has been determined.
1532 
1533         :param create_instance: Determines if the instance is created here or
1534             just populated for later creation. This is done so that this code
1535             can be shared with cellsv1 which needs the instance creation to
1536             happen here. It should be removed and this method cleaned up when
1537             cellsv1 is a distant memory.
1538         """
1539         self._populate_instance_for_create(context, instance, image, index,
1540                                            security_group, instance_type,
1541                                            num_instances, shutdown_terminate)
1542 
1543         if create_instance:
1544             instance.create()
1545 
1546         return instance
1547 
1548     def _check_multiple_instances_with_neutron_ports(self,
1549                                                      requested_networks):
1550         """Check whether multiple instances are created from port id(s)."""
1551         for requested_net in requested_networks:
1552             if requested_net.port_id:
1553                 msg = _("Unable to launch multiple instances with"
1554                         " a single configured port ID. Please launch your"
1555                         " instance one by one with different ports.")
1556                 raise exception.MultiplePortsNotApplicable(reason=msg)
1557 
1558     def _check_multiple_instances_with_specified_ip(self, requested_networks):
1559         """Check whether multiple instances are created with specified ip."""
1560 
1561         for requested_net in requested_networks:
1562             if requested_net.network_id and requested_net.address:
1563                 msg = _("max_count cannot be greater than 1 if an fixed_ip "
1564                         "is specified.")
1565                 raise exception.InvalidFixedIpAndMaxCountRequest(reason=msg)
1566 
1567     @hooks.add_hook("create_instance")
1568     def create(self, context, instance_type,
1569                image_href, kernel_id=None, ramdisk_id=None,
1570                min_count=None, max_count=None,
1571                display_name=None, display_description=None,
1572                key_name=None, key_data=None, security_groups=None,
1573                availability_zone=None, forced_host=None, forced_node=None,
1574                user_data=None, metadata=None, injected_files=None,
1575                admin_password=None, block_device_mapping=None,
1576                access_ip_v4=None, access_ip_v6=None, requested_networks=None,
1577                config_drive=None, auto_disk_config=None, scheduler_hints=None,
1578                legacy_bdm=True, shutdown_terminate=False,
1579                check_server_group_quota=False, tags=None,
1580                supports_multiattach=False):
1581         """Provision instances, sending instance information to the
1582         scheduler.  The scheduler will determine where the instance(s)
1583         go and will handle creating the DB entries.
1584 
1585         Returns a tuple of (instances, reservation_id)
1586         """
1587         if requested_networks and max_count is not None and max_count > 1:
1588             self._check_multiple_instances_with_specified_ip(
1589                 requested_networks)
1590             if utils.is_neutron():
1591                 self._check_multiple_instances_with_neutron_ports(
1592                     requested_networks)
1593 
1594         if availability_zone:
1595             available_zones = availability_zones.\
1596                 get_availability_zones(context.elevated(), True)
1597             if forced_host is None and availability_zone not in \
1598                     available_zones:
1599                 msg = _('The requested availability zone is not available')
1600                 raise exception.InvalidRequest(msg)
1601 
1602         filter_properties = scheduler_utils.build_filter_properties(
1603                 scheduler_hints, forced_host, forced_node, instance_type)
1604 
1605         return self._create_instance(
1606                        context, instance_type,
1607                        image_href, kernel_id, ramdisk_id,
1608                        min_count, max_count,
1609                        display_name, display_description,
1610                        key_name, key_data, security_groups,
1611                        availability_zone, user_data, metadata,
1612                        injected_files, admin_password,
1613                        access_ip_v4, access_ip_v6,
1614                        requested_networks, config_drive,
1615                        block_device_mapping, auto_disk_config,
1616                        filter_properties=filter_properties,
1617                        legacy_bdm=legacy_bdm,
1618                        shutdown_terminate=shutdown_terminate,
1619                        check_server_group_quota=check_server_group_quota,
1620                        tags=tags, supports_multiattach=supports_multiattach)
1621 
1622     def _check_auto_disk_config(self, instance=None, image=None,
1623                                 **extra_instance_updates):
1624         auto_disk_config = extra_instance_updates.get("auto_disk_config")
1625         if auto_disk_config is None:
1626             return
1627         if not image and not instance:
1628             return
1629 
1630         if image:
1631             image_props = image.get("properties", {})
1632             auto_disk_config_img = \
1633                 utils.get_auto_disk_config_from_image_props(image_props)
1634             image_ref = image.get("id")
1635         else:
1636             sys_meta = utils.instance_sys_meta(instance)
1637             image_ref = sys_meta.get('image_base_image_ref')
1638             auto_disk_config_img = \
1639                 utils.get_auto_disk_config_from_instance(sys_meta=sys_meta)
1640 
1641         self._ensure_auto_disk_config_is_valid(auto_disk_config_img,
1642                                                auto_disk_config,
1643                                                image_ref)
1644 
1645     def _lookup_instance(self, context, uuid):
1646         '''Helper method for pulling an instance object from a database.
1647 
1648         During the transition to cellsv2 there is some complexity around
1649         retrieving an instance from the database which this method hides. If
1650         there is an instance mapping then query the cell for the instance, if
1651         no mapping exists then query the configured nova database.
1652 
1653         Once we are past the point that all deployments can be assumed to be
1654         migrated to cellsv2 this method can go away.
1655         '''
1656         inst_map = None
1657         try:
1658             inst_map = objects.InstanceMapping.get_by_instance_uuid(
1659                 context, uuid)
1660         except exception.InstanceMappingNotFound:
1661             # TODO(alaski): This exception block can be removed once we're
1662             # guaranteed everyone is using cellsv2.
1663             pass
1664 
1665         if (inst_map is None or inst_map.cell_mapping is None or
1666                 CONF.cells.enable):
1667             # If inst_map is None then the deployment has not migrated to
1668             # cellsv2 yet.
1669             # If inst_map.cell_mapping is None then the instance is not in a
1670             # cell yet. Until instance creation moves to the conductor the
1671             # instance can be found in the configured database, so attempt
1672             # to look it up.
1673             # If we're on cellsv1, we can't yet short-circuit the cells
1674             # messaging path
1675             cell = None
1676             try:
1677                 instance = objects.Instance.get_by_uuid(context, uuid)
1678             except exception.InstanceNotFound:
1679                 # If we get here then the conductor is in charge of writing the
1680                 # instance to the database and hasn't done that yet. It's up to
1681                 # the caller of this method to determine what to do with that
1682                 # information.
1683                 return None, None
1684         else:
1685             cell = inst_map.cell_mapping
1686             with nova_context.target_cell(context, cell) as cctxt:
1687                 try:
1688                     instance = objects.Instance.get_by_uuid(cctxt, uuid)
1689                 except exception.InstanceNotFound:
1690                     # Since the cell_mapping exists we know the instance is in
1691                     # the cell, however InstanceNotFound means it's already
1692                     # deleted.
1693                     return None, None
1694         return cell, instance
1695 
1696     def _delete_while_booting(self, context, instance):
1697         """Handle deletion if the instance has not reached a cell yet
1698 
1699         Deletion before an instance reaches a cell needs to be handled
1700         differently. What we're attempting to do is delete the BuildRequest
1701         before the api level conductor does.  If we succeed here then the boot
1702         request stops before reaching a cell.  If not then the instance will
1703         need to be looked up in a cell db and the normal delete path taken.
1704         """
1705         deleted = self._attempt_delete_of_buildrequest(context, instance)
1706 
1707         # After service version 15 deletion of the BuildRequest will halt the
1708         # build process in the conductor. In that case run the rest of this
1709         # method and consider the instance deleted. If we have not yet reached
1710         # service version 15 then just return False so the rest of the delete
1711         # process will proceed usually.
1712         service_version = objects.Service.get_minimum_version(
1713             context, 'nova-osapi_compute')
1714         if service_version < 15:
1715             return False
1716 
1717         if deleted:
1718             # If we've reached this block the successful deletion of the
1719             # buildrequest indicates that the build process should be halted by
1720             # the conductor.
1721 
1722             # NOTE(alaski): Though the conductor halts the build process it
1723             # does not currently delete the instance record. This is
1724             # because in the near future the instance record will not be
1725             # created if the buildrequest has been deleted here. For now we
1726             # ensure the instance has been set to deleted at this point.
1727             # Yes this directly contradicts the comment earlier in this
1728             # method, but this is a temporary measure.
1729             # Look up the instance because the current instance object was
1730             # stashed on the buildrequest and therefore not complete enough
1731             # to run .destroy().
1732             try:
1733                 instance_uuid = instance.uuid
1734                 cell, instance = self._lookup_instance(context, instance_uuid)
1735                 if instance is not None:
1736                     # If instance is None it has already been deleted.
1737                     if cell:
1738                         with nova_context.target_cell(context, cell) as cctxt:
1739                             # FIXME: When the instance context is targeted,
1740                             # we can remove this
1741                             with compute_utils.notify_about_instance_delete(
1742                                     self.notifier, cctxt, instance):
1743                                 instance.destroy()
1744                     else:
1745                         instance.destroy()
1746             except exception.InstanceNotFound:
1747                 pass
1748 
1749             return True
1750         return False
1751 
1752     def _attempt_delete_of_buildrequest(self, context, instance):
1753         # If there is a BuildRequest then the instance may not have been
1754         # written to a cell db yet. Delete the BuildRequest here, which
1755         # will indicate that the Instance build should not proceed.
1756         try:
1757             build_req = objects.BuildRequest.get_by_instance_uuid(
1758                 context, instance.uuid)
1759             build_req.destroy()
1760         except exception.BuildRequestNotFound:
1761             # This means that conductor has deleted the BuildRequest so the
1762             # instance is now in a cell and the delete needs to proceed
1763             # normally.
1764             return False
1765 
1766         # We need to detach from any volumes so they aren't orphaned.
1767         self._local_cleanup_bdm_volumes(
1768             build_req.block_device_mappings, instance, context)
1769 
1770         return True
1771 
1772     def _delete(self, context, instance, delete_type, cb, **instance_attrs):
1773         if instance.disable_terminate:
1774             LOG.info('instance termination disabled', instance=instance)
1775             return
1776 
1777         cell = None
1778         # If there is an instance.host (or the instance is shelved-offloaded or
1779         # in error state), the instance has been scheduled and sent to a
1780         # cell/compute which means it was pulled from the cell db.
1781         # Normal delete should be attempted.
1782         may_have_ports_or_volumes = compute_utils.may_have_ports_or_volumes(
1783             instance)
1784         if not instance.host and not may_have_ports_or_volumes:
1785             try:
1786                 if self._delete_while_booting(context, instance):
1787                     return
1788                 # If instance.host was not set it's possible that the Instance
1789                 # object here was pulled from a BuildRequest object and is not
1790                 # fully populated. Notably it will be missing an 'id' field
1791                 # which will prevent instance.destroy from functioning
1792                 # properly. A lookup is attempted which will either return a
1793                 # full Instance or None if not found. If not found then it's
1794                 # acceptable to skip the rest of the delete processing.
1795                 cell, instance = self._lookup_instance(context, instance.uuid)
1796                 if cell and instance:
1797                     try:
1798                         # Now destroy the instance from the cell it lives in.
1799                         with compute_utils.notify_about_instance_delete(
1800                                 self.notifier, context, instance):
1801                             instance.destroy()
1802                     except exception.InstanceNotFound:
1803                         pass
1804                     # The instance was deleted or is already gone.
1805                     return
1806                 if not instance:
1807                     # Instance is already deleted.
1808                     return
1809             except exception.ObjectActionError:
1810                 # NOTE(melwitt): This means the instance.host changed
1811                 # under us indicating the instance became scheduled
1812                 # during the destroy(). Refresh the instance from the DB and
1813                 # continue on with the delete logic for a scheduled instance.
1814                 # NOTE(danms): If instance.host is set, we should be able to
1815                 # do the following lookup. If not, there's not much we can
1816                 # do to recover.
1817                 cell, instance = self._lookup_instance(context, instance.uuid)
1818                 if not instance:
1819                     # Instance is already deleted
1820                     return
1821 
1822         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
1823                 context, instance.uuid)
1824 
1825         # At these states an instance has a snapshot associate.
1826         if instance.vm_state in (vm_states.SHELVED,
1827                                  vm_states.SHELVED_OFFLOADED):
1828             snapshot_id = instance.system_metadata.get('shelved_image_id')
1829             LOG.info("Working on deleting snapshot %s "
1830                      "from shelved instance...",
1831                      snapshot_id, instance=instance)
1832             try:
1833                 self.image_api.delete(context, snapshot_id)
1834             except (exception.ImageNotFound,
1835                     exception.ImageNotAuthorized) as exc:
1836                 LOG.warning("Failed to delete snapshot "
1837                             "from shelved instance (%s).",
1838                             exc.format_message(), instance=instance)
1839             except Exception:
1840                 LOG.exception("Something wrong happened when trying to "
1841                               "delete snapshot from shelved instance.",
1842                               instance=instance)
1843 
1844         original_task_state = instance.task_state
1845         try:
1846             # NOTE(maoy): no expected_task_state needs to be set
1847             instance.update(instance_attrs)
1848             instance.progress = 0
1849             instance.save()
1850 
1851             # NOTE(dtp): cells.enable = False means "use cells v2".
1852             # Run everywhere except v1 compute cells.
1853             if not CONF.cells.enable or self.cell_type == 'api':
1854                 # TODO(melwitt): In Rocky, we store console authorizations
1855                 # in both the consoleauth service and the database while
1856                 # we convert to using the database. Remove the consoleauth
1857                 # line below when authorizations are no longer being
1858                 # stored in consoleauth, in Stein.
1859                 self.consoleauth_rpcapi.delete_tokens_for_instance(
1860                     context, instance.uuid)
1861 
1862             if self.cell_type == 'api':
1863                 # NOTE(comstud): If we're in the API cell, we need to
1864                 # skip all remaining logic and just call the callback,
1865                 # which will cause a cast to the child cell.
1866                 cb(context, instance, bdms)
1867                 return
1868             if not instance.host and not may_have_ports_or_volumes:
1869                 try:
1870                     with compute_utils.notify_about_instance_delete(
1871                             self.notifier, context, instance,
1872                             delete_type
1873                             if delete_type != 'soft_delete'
1874                             else 'delete'):
1875                         instance.destroy()
1876                     LOG.info('Instance deleted and does not have host '
1877                              'field, its vm_state is %(state)s.',
1878                              {'state': instance.vm_state},
1879                               instance=instance)
1880                     return
1881                 except exception.ObjectActionError as ex:
1882                     # The instance's host likely changed under us as
1883                     # this instance could be building and has since been
1884                     # scheduled. Continue with attempts to delete it.
1885                     LOG.debug('Refreshing instance because: %s', ex,
1886                               instance=instance)
1887                     instance.refresh()
1888 
1889             if instance.vm_state == vm_states.RESIZED:
1890                 self._confirm_resize_on_deleting(context, instance)
1891 
1892             is_local_delete = True
1893             try:
1894                 # instance.host must be set in order to look up the service.
1895                 if instance.host is not None:
1896                     service = objects.Service.get_by_compute_host(
1897                         context.elevated(), instance.host)
1898                     is_local_delete = not self.servicegroup_api.service_is_up(
1899                         service)
1900                 if not is_local_delete:
1901                     if original_task_state in (task_states.DELETING,
1902                                                   task_states.SOFT_DELETING):
1903                         LOG.info('Instance is already in deleting state, '
1904                                  'ignoring this request',
1905                                  instance=instance)
1906                         return
1907                     self._record_action_start(context, instance,
1908                                               instance_actions.DELETE)
1909 
1910                     cb(context, instance, bdms)
1911             except exception.ComputeHostNotFound:
1912                 LOG.debug('Compute host %s not found during service up check, '
1913                           'going to local delete instance', instance.host,
1914                           instance=instance)
1915 
1916             if is_local_delete:
1917                 # If instance is in shelved_offloaded state or compute node
1918                 # isn't up, delete instance from db and clean bdms info and
1919                 # network info
1920                 if cell is None:
1921                     # NOTE(danms): If we didn't get our cell from one of the
1922                     # paths above, look it up now.
1923                     try:
1924                         im = objects.InstanceMapping.get_by_instance_uuid(
1925                             context, instance.uuid)
1926                         cell = im.cell_mapping
1927                     except exception.InstanceMappingNotFound:
1928                         LOG.warning('During local delete, failed to find '
1929                                     'instance mapping', instance=instance)
1930                         return
1931 
1932                 LOG.debug('Doing local delete in cell %s', cell.identity,
1933                           instance=instance)
1934                 with nova_context.target_cell(context, cell) as cctxt:
1935                     self._local_delete(cctxt, instance, bdms, delete_type, cb)
1936 
1937         except exception.InstanceNotFound:
1938             # NOTE(comstud): Race condition. Instance already gone.
1939             pass
1940 
1941     def _confirm_resize_on_deleting(self, context, instance):
1942         # If in the middle of a resize, use confirm_resize to
1943         # ensure the original instance is cleaned up too
1944         migration = None
1945         for status in ('finished', 'confirming'):
1946             try:
1947                 migration = objects.Migration.get_by_instance_and_status(
1948                         context.elevated(), instance.uuid, status)
1949                 LOG.info('Found an unconfirmed migration during delete, '
1950                          'id: %(id)s, status: %(status)s',
1951                          {'id': migration.id,
1952                           'status': migration.status},
1953                          instance=instance)
1954                 break
1955             except exception.MigrationNotFoundByStatus:
1956                 pass
1957 
1958         if not migration:
1959             LOG.info('Instance may have been confirmed during delete',
1960                      instance=instance)
1961             return
1962 
1963         src_host = migration.source_compute
1964 
1965         self._record_action_start(context, instance,
1966                                   instance_actions.CONFIRM_RESIZE)
1967 
1968         self.compute_rpcapi.confirm_resize(context,
1969                 instance, migration, src_host, cast=False)
1970 
1971     def _local_cleanup_bdm_volumes(self, bdms, instance, context):
1972         """The method deletes the bdm records and, if a bdm is a volume, call
1973         the terminate connection and the detach volume via the Volume API.
1974         """
1975         elevated = context.elevated()
1976         for bdm in bdms:
1977             if bdm.is_volume:
1978                 try:
1979                     if bdm.attachment_id:
1980                         self.volume_api.attachment_delete(context,
1981                                                           bdm.attachment_id)
1982                     else:
1983                         connector = compute_utils.get_stashed_volume_connector(
1984                             bdm, instance)
1985                         if connector:
1986                             self.volume_api.terminate_connection(context,
1987                                                                  bdm.volume_id,
1988                                                                  connector)
1989                         else:
1990                             LOG.debug('Unable to find connector for volume %s,'
1991                                       ' not attempting terminate_connection.',
1992                                       bdm.volume_id, instance=instance)
1993                         # Attempt to detach the volume. If there was no
1994                         # connection made in the first place this is just
1995                         # cleaning up the volume state in the Cinder DB.
1996                         self.volume_api.detach(elevated, bdm.volume_id,
1997                                                instance.uuid)
1998 
1999                     if bdm.delete_on_termination:
2000                         self.volume_api.delete(context, bdm.volume_id)
2001                 except Exception as exc:
2002                     LOG.warning("Ignoring volume cleanup failure due to %s",
2003                                 exc, instance=instance)
2004             # If we're cleaning up volumes from an instance that wasn't yet
2005             # created in a cell, i.e. the user deleted the server while
2006             # the BuildRequest still existed, then the BDM doesn't actually
2007             # exist in the DB to destroy it.
2008             if 'id' in bdm:
2009                 bdm.destroy()
2010 
2011     def _local_delete(self, context, instance, bdms, delete_type, cb):
2012         if instance.vm_state == vm_states.SHELVED_OFFLOADED:
2013             LOG.info("instance is in SHELVED_OFFLOADED state, cleanup"
2014                      " the instance's info from database.",
2015                      instance=instance)
2016         else:
2017             LOG.warning("instance's host %s is down, deleting from "
2018                         "database", instance.host, instance=instance)
2019         with compute_utils.notify_about_instance_delete(
2020                 self.notifier, context, instance,
2021                 delete_type if delete_type != 'soft_delete' else 'delete'):
2022 
2023             elevated = context.elevated()
2024             if self.cell_type != 'api':
2025                 # NOTE(liusheng): In nova-network multi_host scenario,deleting
2026                 # network info of the instance may need instance['host'] as
2027                 # destination host of RPC call. If instance in
2028                 # SHELVED_OFFLOADED state, instance['host'] is None, here, use
2029                 # shelved_host as host to deallocate network info and reset
2030                 # instance['host'] after that. Here we shouldn't use
2031                 # instance.save(), because this will mislead user who may think
2032                 # the instance's host has been changed, and actually, the
2033                 # instance.host is always None.
2034                 orig_host = instance.host
2035                 try:
2036                     if instance.vm_state == vm_states.SHELVED_OFFLOADED:
2037                         sysmeta = getattr(instance,
2038                                           obj_base.get_attrname(
2039                                               'system_metadata'))
2040                         instance.host = sysmeta.get('shelved_host')
2041                     self.network_api.deallocate_for_instance(elevated,
2042                                                              instance)
2043                 finally:
2044                     instance.host = orig_host
2045 
2046             # cleanup volumes
2047             self._local_cleanup_bdm_volumes(bdms, instance, context)
2048             # Cleanup allocations in Placement since we can't do it from the
2049             # compute service.
2050             self.placementclient.delete_allocation_for_instance(
2051                 context, instance.uuid)
2052             cb(context, instance, bdms, local=True)
2053             instance.destroy()
2054 
2055     def _do_delete(self, context, instance, bdms, local=False):
2056         if local:
2057             instance.vm_state = vm_states.DELETED
2058             instance.task_state = None
2059             instance.terminated_at = timeutils.utcnow()
2060             instance.save()
2061         else:
2062             self.compute_rpcapi.terminate_instance(context, instance, bdms,
2063                                                    delete_type='delete')
2064 
2065     def _do_force_delete(self, context, instance, bdms, local=False):
2066         if local:
2067             instance.vm_state = vm_states.DELETED
2068             instance.task_state = None
2069             instance.terminated_at = timeutils.utcnow()
2070             instance.save()
2071         else:
2072             self.compute_rpcapi.terminate_instance(context, instance, bdms,
2073                                                    delete_type='force_delete')
2074 
2075     def _do_soft_delete(self, context, instance, bdms, local=False):
2076         if local:
2077             instance.vm_state = vm_states.SOFT_DELETED
2078             instance.task_state = None
2079             instance.terminated_at = timeutils.utcnow()
2080             instance.save()
2081         else:
2082             self.compute_rpcapi.soft_delete_instance(context, instance)
2083 
2084     # NOTE(maoy): we allow delete to be called no matter what vm_state says.
2085     @check_instance_lock
2086     @check_instance_cell
2087     @check_instance_state(vm_state=None, task_state=None,
2088                           must_have_launched=True)
2089     def soft_delete(self, context, instance):
2090         """Terminate an instance."""
2091         LOG.debug('Going to try to soft delete instance',
2092                   instance=instance)
2093 
2094         self._delete(context, instance, 'soft_delete', self._do_soft_delete,
2095                      task_state=task_states.SOFT_DELETING,
2096                      deleted_at=timeutils.utcnow())
2097 
2098     def _delete_instance(self, context, instance):
2099         self._delete(context, instance, 'delete', self._do_delete,
2100                      task_state=task_states.DELETING)
2101 
2102     @check_instance_lock
2103     @check_instance_cell
2104     @check_instance_state(vm_state=None, task_state=None,
2105                           must_have_launched=False)
2106     def delete(self, context, instance):
2107         """Terminate an instance."""
2108         LOG.debug("Going to try to terminate instance", instance=instance)
2109         self._delete_instance(context, instance)
2110 
2111     @check_instance_lock
2112     @check_instance_state(vm_state=[vm_states.SOFT_DELETED])
2113     def restore(self, context, instance):
2114         """Restore a previously deleted (but not reclaimed) instance."""
2115         # Check quotas
2116         flavor = instance.get_flavor()
2117         project_id, user_id = quotas_obj.ids_from_instance(context, instance)
2118         compute_utils.check_num_instances_quota(context, flavor, 1, 1,
2119                 project_id=project_id, user_id=user_id)
2120 
2121         self._record_action_start(context, instance, instance_actions.RESTORE)
2122 
2123         if instance.host:
2124             instance.task_state = task_states.RESTORING
2125             instance.deleted_at = None
2126             instance.save(expected_task_state=[None])
2127             # TODO(melwitt): We're not rechecking for strict quota here to
2128             # guard against going over quota during a race at this time because
2129             # the resource consumption for this operation is written to the
2130             # database by compute.
2131             self.compute_rpcapi.restore_instance(context, instance)
2132         else:
2133             instance.vm_state = vm_states.ACTIVE
2134             instance.task_state = None
2135             instance.deleted_at = None
2136             instance.save(expected_task_state=[None])
2137 
2138     @check_instance_lock
2139     @check_instance_state(task_state=None,
2140                           must_have_launched=False)
2141     def force_delete(self, context, instance):
2142         """Force delete an instance in any vm_state/task_state."""
2143         self._delete(context, instance, 'force_delete', self._do_force_delete,
2144                      task_state=task_states.DELETING)
2145 
2146     def force_stop(self, context, instance, do_cast=True, clean_shutdown=True):
2147         LOG.debug("Going to try to stop instance", instance=instance)
2148 
2149         instance.task_state = task_states.POWERING_OFF
2150         instance.progress = 0
2151         instance.save(expected_task_state=[None])
2152 
2153         self._record_action_start(context, instance, instance_actions.STOP)
2154 
2155         self.compute_rpcapi.stop_instance(context, instance, do_cast=do_cast,
2156                                           clean_shutdown=clean_shutdown)
2157 
2158     @check_instance_lock
2159     @check_instance_host
2160     @check_instance_cell
2161     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.ERROR])
2162     def stop(self, context, instance, do_cast=True, clean_shutdown=True):
2163         """Stop an instance."""
2164         self.force_stop(context, instance, do_cast, clean_shutdown)
2165 
2166     @check_instance_lock
2167     @check_instance_host
2168     @check_instance_cell
2169     @check_instance_state(vm_state=[vm_states.STOPPED])
2170     def start(self, context, instance):
2171         """Start an instance."""
2172         LOG.debug("Going to try to start instance", instance=instance)
2173 
2174         instance.task_state = task_states.POWERING_ON
2175         instance.save(expected_task_state=[None])
2176 
2177         self._record_action_start(context, instance, instance_actions.START)
2178         # TODO(yamahata): injected_files isn't supported right now.
2179         #                 It is used only for osapi. not for ec2 api.
2180         #                 availability_zone isn't used by run_instance.
2181         self.compute_rpcapi.start_instance(context, instance)
2182 
2183     @check_instance_lock
2184     @check_instance_host
2185     @check_instance_cell
2186     @check_instance_state(vm_state=vm_states.ALLOW_TRIGGER_CRASH_DUMP)
2187     def trigger_crash_dump(self, context, instance):
2188         """Trigger crash dump in an instance."""
2189         LOG.debug("Try to trigger crash dump", instance=instance)
2190 
2191         self._record_action_start(context, instance,
2192                                   instance_actions.TRIGGER_CRASH_DUMP)
2193 
2194         self.compute_rpcapi.trigger_crash_dump(context, instance)
2195 
2196     def _get_instance_map_or_none(self, context, instance_uuid):
2197         try:
2198             inst_map = objects.InstanceMapping.get_by_instance_uuid(
2199                     context, instance_uuid)
2200         except exception.InstanceMappingNotFound:
2201             # InstanceMapping should always be found generally. This exception
2202             # may be raised if a deployment has partially migrated the nova-api
2203             # services.
2204             inst_map = None
2205         return inst_map
2206 
2207     def _get_instance(self, context, instance_uuid, expected_attrs):
2208         # Before service version 15 the BuildRequest is not cleaned up during
2209         # a delete request so there is no reason to look it up here as we can't
2210         # trust that it's not referencing a deleted instance. Also even if
2211         # there is an instance mapping we don't need to honor it for older
2212         # service versions.
2213         service_version = objects.Service.get_minimum_version(
2214             context, 'nova-osapi_compute')
2215         # If we're on cellsv1, we also need to consult the top-level
2216         # merged replica instead of the cell directly, so fall through
2217         # here in that case as well.
2218         if service_version < 15 or CONF.cells.enable:
2219             # If not using cells v1, we need to log a warning about the API
2220             # service version being less than 15 (that check was added in
2221             # newton), which indicates there is some lingering data during the
2222             # transition to cells v2 which could cause an InstanceNotFound
2223             # here. The warning message is a sort of breadcrumb.
2224             # This can all go away once we drop cells v1 and assert that all
2225             # deployments have upgraded from a base cells v2 setup with
2226             # mappings.
2227             if not CONF.cells.enable:
2228                 LOG.warning('The nova-osapi_compute service version is from '
2229                             'before Ocata and may cause problems looking up '
2230                             'instances in a cells v2 setup. Check your '
2231                             'nova-api service configuration and cell '
2232                             'mappings. You may need to remove stale '
2233                             'nova-osapi_compute service records from the cell '
2234                             'database.')
2235             return objects.Instance.get_by_uuid(context, instance_uuid,
2236                                                 expected_attrs=expected_attrs)
2237         inst_map = self._get_instance_map_or_none(context, instance_uuid)
2238         if inst_map and (inst_map.cell_mapping is not None):
2239             nova_context.set_target_cell(context, inst_map.cell_mapping)
2240             instance = objects.Instance.get_by_uuid(
2241                 context, instance_uuid, expected_attrs=expected_attrs)
2242         elif inst_map and (inst_map.cell_mapping is None):
2243             # This means the instance has not been scheduled and put in
2244             # a cell yet. For now it also may mean that the deployer
2245             # has not created their cell(s) yet.
2246             try:
2247                 build_req = objects.BuildRequest.get_by_instance_uuid(
2248                         context, instance_uuid)
2249                 instance = build_req.instance
2250             except exception.BuildRequestNotFound:
2251                 # Instance was mapped and the BuildRequest was deleted
2252                 # while fetching. Try again.
2253                 inst_map = self._get_instance_map_or_none(context,
2254                                                           instance_uuid)
2255                 if inst_map and (inst_map.cell_mapping is not None):
2256                     nova_context.set_target_cell(context,
2257                                                  inst_map.cell_mapping)
2258                     instance = objects.Instance.get_by_uuid(
2259                         context, instance_uuid,
2260                         expected_attrs=expected_attrs)
2261                 else:
2262                     raise exception.InstanceNotFound(instance_id=instance_uuid)
2263         else:
2264             raise exception.InstanceNotFound(instance_id=instance_uuid)
2265 
2266         return instance
2267 
2268     def get(self, context, instance_id, expected_attrs=None):
2269         """Get a single instance with the given instance_id."""
2270         if not expected_attrs:
2271             expected_attrs = []
2272         expected_attrs.extend(['metadata', 'system_metadata',
2273                                'security_groups', 'info_cache'])
2274         # NOTE(ameade): we still need to support integer ids for ec2
2275         try:
2276             if uuidutils.is_uuid_like(instance_id):
2277                 LOG.debug("Fetching instance by UUID",
2278                            instance_uuid=instance_id)
2279 
2280                 instance = self._get_instance(context, instance_id,
2281                                               expected_attrs)
2282             else:
2283                 LOG.debug("Failed to fetch instance by id %s", instance_id)
2284                 raise exception.InstanceNotFound(instance_id=instance_id)
2285         except exception.InvalidID:
2286             LOG.debug("Invalid instance id %s", instance_id)
2287             raise exception.InstanceNotFound(instance_id=instance_id)
2288 
2289         return instance
2290 
2291     def get_all(self, context, search_opts=None, limit=None, marker=None,
2292                 expected_attrs=None, sort_keys=None, sort_dirs=None):
2293         """Get all instances filtered by one of the given parameters.
2294 
2295         If there is no filter and the context is an admin, it will retrieve
2296         all instances in the system.
2297 
2298         Deleted instances will be returned by default, unless there is a
2299         search option that says otherwise.
2300 
2301         The results will be sorted based on the list of sort keys in the
2302         'sort_keys' parameter (first value is primary sort key, second value is
2303         secondary sort ket, etc.). For each sort key, the associated sort
2304         direction is based on the list of sort directions in the 'sort_dirs'
2305         parameter.
2306         """
2307         if search_opts is None:
2308             search_opts = {}
2309 
2310         LOG.debug("Searching by: %s", str(search_opts))
2311 
2312         # Fixups for the DB call
2313         filters = {}
2314 
2315         def _remap_flavor_filter(flavor_id):
2316             flavor = objects.Flavor.get_by_flavor_id(context, flavor_id)
2317             filters['instance_type_id'] = flavor.id
2318 
2319         def _remap_fixed_ip_filter(fixed_ip):
2320             # Turn fixed_ip into a regexp match. Since '.' matches
2321             # any character, we need to use regexp escaping for it.
2322             filters['ip'] = '^%s$' % fixed_ip.replace('.', '\\.')
2323 
2324         # search_option to filter_name mapping.
2325         filter_mapping = {
2326                 'image': 'image_ref',
2327                 'name': 'display_name',
2328                 'tenant_id': 'project_id',
2329                 'flavor': _remap_flavor_filter,
2330                 'fixed_ip': _remap_fixed_ip_filter}
2331 
2332         # copy from search_opts, doing various remappings as necessary
2333         for opt, value in search_opts.items():
2334             # Do remappings.
2335             # Values not in the filter_mapping table are copied as-is.
2336             # If remapping is None, option is not copied
2337             # If the remapping is a string, it is the filter_name to use
2338             try:
2339                 remap_object = filter_mapping[opt]
2340             except KeyError:
2341                 filters[opt] = value
2342             else:
2343                 # Remaps are strings to translate to, or functions to call
2344                 # to do the translating as defined by the table above.
2345                 if isinstance(remap_object, six.string_types):
2346                     filters[remap_object] = value
2347                 else:
2348                     try:
2349                         remap_object(value)
2350 
2351                     # We already know we can't match the filter, so
2352                     # return an empty list
2353                     except ValueError:
2354                         return objects.InstanceList()
2355 
2356         # IP address filtering cannot be applied at the DB layer, remove any DB
2357         # limit so that it can be applied after the IP filter.
2358         filter_ip = 'ip6' in filters or 'ip' in filters
2359         skip_build_request = False
2360         orig_limit = limit
2361         if filter_ip:
2362             skip_build_request = True
2363             if self.network_api.has_substr_port_filtering_extension(context):
2364                 # We're going to filter by IP using Neutron so set filter_ip
2365                 # to False so we don't attempt post-DB query filtering in
2366                 # memory below.
2367                 filter_ip = False
2368                 instance_uuids = self._ip_filter_using_neutron(context,
2369                                                                filters)
2370                 if instance_uuids:
2371                     # Note that 'uuid' is not in the 2.1 GET /servers query
2372                     # parameter schema, however, we allow additionalProperties
2373                     # so someone could filter instances by uuid, which doesn't
2374                     # make a lot of sense but we have to account for it.
2375                     if 'uuid' in filters and filters['uuid']:
2376                         filter_uuids = filters['uuid']
2377                         if isinstance(filter_uuids, list):
2378                             instance_uuids.extend(filter_uuids)
2379                         else:
2380                             # Assume a string. If it's a dict or tuple or
2381                             # something, well...that's too bad. This is why
2382                             # we have query parameter schema definitions.
2383                             if filter_uuids not in instance_uuids:
2384                                 instance_uuids.append(filter_uuids)
2385                     filters['uuid'] = instance_uuids
2386                 else:
2387                     # No matches on the ip filter(s), return an empty list.
2388                     return objects.InstanceList()
2389             elif limit:
2390                 LOG.debug('Removing limit for DB query due to IP filter')
2391                 limit = None
2392 
2393         # Skip get BuildRequest if filtering by IP address, as building
2394         # instances will not have IP addresses.
2395         if skip_build_request:
2396             build_requests = objects.BuildRequestList()
2397         else:
2398             # The ordering of instances will be
2399             # [sorted instances with no host] + [sorted instances with host].
2400             # This means BuildRequest and cell0 instances first, then cell
2401             # instances
2402             try:
2403                 build_requests = objects.BuildRequestList.get_by_filters(
2404                     context, filters, limit=limit, marker=marker,
2405                     sort_keys=sort_keys, sort_dirs=sort_dirs)
2406                 # If we found the marker in we need to set it to None
2407                 # so we don't expect to find it in the cells below.
2408                 marker = None
2409             except exception.MarkerNotFound:
2410                 # If we didn't find the marker in the build requests then keep
2411                 # looking for it in the cells.
2412                 build_requests = objects.BuildRequestList()
2413 
2414         build_req_instances = objects.InstanceList(
2415             objects=[build_req.instance for build_req in build_requests])
2416         # Only subtract from limit if it is not None
2417         limit = (limit - len(build_req_instances)) if limit else limit
2418 
2419         # We could arguably avoid joining on security_groups if we're using
2420         # neutron (which is the default) but if you're using neutron then the
2421         # security_group_instance_association table should be empty anyway
2422         # and the DB should optimize out that join, making it insignificant.
2423         fields = ['metadata', 'info_cache', 'security_groups']
2424         if expected_attrs:
2425             fields.extend(expected_attrs)
2426 
2427         if CONF.cells.enable:
2428             insts = self._do_old_style_instance_list_for_poor_cellsv1_users(
2429                 context, filters, limit, marker, fields, sort_keys,
2430                 sort_dirs)
2431         else:
2432             insts = instance_list.get_instance_objects_sorted(
2433                 context, filters, limit, marker, fields, sort_keys, sort_dirs)
2434 
2435         def _get_unique_filter_method():
2436             seen_uuids = set()
2437 
2438             def _filter(instance):
2439                 if instance.uuid in seen_uuids:
2440                     return False
2441                 seen_uuids.add(instance.uuid)
2442                 return True
2443 
2444             return _filter
2445 
2446         filter_method = _get_unique_filter_method()
2447         # Only subtract from limit if it is not None
2448         limit = (limit - len(insts)) if limit else limit
2449         # TODO(alaski): Clean up the objects concatenation when List objects
2450         # support it natively.
2451         instances = objects.InstanceList(
2452             objects=list(filter(filter_method,
2453                            build_req_instances.objects +
2454                            insts.objects)))
2455 
2456         if filter_ip:
2457             instances = self._ip_filter(instances, filters, orig_limit)
2458 
2459         return instances
2460 
2461     def _do_old_style_instance_list_for_poor_cellsv1_users(self,
2462                                                            context, filters,
2463                                                            limit, marker,
2464                                                            fields,
2465                                                            sort_keys,
2466                                                            sort_dirs):
2467         try:
2468             cell0_mapping = objects.CellMapping.get_by_uuid(context,
2469                 objects.CellMapping.CELL0_UUID)
2470         except exception.CellMappingNotFound:
2471             cell0_instances = objects.InstanceList(objects=[])
2472         else:
2473             with nova_context.target_cell(context, cell0_mapping) as cctxt:
2474                 try:
2475                     cell0_instances = self._get_instances_by_filters(
2476                         cctxt, filters, limit=limit, marker=marker,
2477                         fields=fields, sort_keys=sort_keys,
2478                         sort_dirs=sort_dirs)
2479                     # If we found the marker in cell0 we need to set it to None
2480                     # so we don't expect to find it in the cells below.
2481                     marker = None
2482                 except exception.MarkerNotFound:
2483                     # We can ignore this since we need to look in the cell DB
2484                     cell0_instances = objects.InstanceList(objects=[])
2485         # Only subtract from limit if it is not None
2486         limit = (limit - len(cell0_instances)) if limit else limit
2487 
2488         # There is only planned support for a single cell here. Multiple cell
2489         # instance lists should be proxied to project Searchlight, or a similar
2490         # alternative.
2491         if limit is None or limit > 0:
2492             # NOTE(melwitt): If we're on cells v1, we need to read
2493             # instances from the top-level database because reading from
2494             # cells results in changed behavior, because of the syncing.
2495             # We can remove this path once we stop supporting cells v1.
2496             cell_instances = self._get_instances_by_filters(
2497                 context, filters, limit=limit, marker=marker,
2498                 fields=fields, sort_keys=sort_keys,
2499                 sort_dirs=sort_dirs)
2500         else:
2501             LOG.debug('Limit excludes any results from real cells')
2502             cell_instances = objects.InstanceList(objects=[])
2503 
2504         return cell0_instances + cell_instances
2505 
2506     @staticmethod
2507     def _ip_filter(inst_models, filters, limit):
2508         ipv4_f = re.compile(str(filters.get('ip')))
2509         ipv6_f = re.compile(str(filters.get('ip6')))
2510 
2511         def _match_instance(instance):
2512             nw_info = instance.get_network_info()
2513             for vif in nw_info:
2514                 for fixed_ip in vif.fixed_ips():
2515                     address = fixed_ip.get('address')
2516                     if not address:
2517                         continue
2518                     version = fixed_ip.get('version')
2519                     if ((version == 4 and ipv4_f.match(address)) or
2520                         (version == 6 and ipv6_f.match(address))):
2521                         return True
2522             return False
2523 
2524         result_objs = []
2525         for instance in inst_models:
2526             if _match_instance(instance):
2527                 result_objs.append(instance)
2528                 if limit and len(result_objs) == limit:
2529                     break
2530         return objects.InstanceList(objects=result_objs)
2531 
2532     def _ip_filter_using_neutron(self, context, filters):
2533         ip4_address = filters.get('ip')
2534         ip6_address = filters.get('ip6')
2535         addresses = [ip4_address, ip6_address]
2536         uuids = []
2537         for address in addresses:
2538             if address:
2539                 try:
2540                     ports = self.network_api.list_ports(
2541                         context, fixed_ips='ip_address_substr=' + address,
2542                         fields=['device_id'])['ports']
2543                     for port in ports:
2544                         uuids.append(port['device_id'])
2545                 except Exception as e:
2546                     LOG.error('An error occurred while listing ports '
2547                               'with an ip_address filter value of "%s". '
2548                               'Error: %s',
2549                               address, six.text_type(e))
2550         return uuids
2551 
2552     def _get_instances_by_filters(self, context, filters,
2553                                   limit=None, marker=None, fields=None,
2554                                   sort_keys=None, sort_dirs=None):
2555         return objects.InstanceList.get_by_filters(
2556             context, filters=filters, limit=limit, marker=marker,
2557             expected_attrs=fields, sort_keys=sort_keys, sort_dirs=sort_dirs)
2558 
2559     def update_instance(self, context, instance, updates):
2560         """Updates a single Instance object with some updates dict.
2561 
2562         Returns the updated instance.
2563         """
2564 
2565         # NOTE(sbauza): Given we only persist the Instance object after we
2566         # create the BuildRequest, we are sure that if the Instance object
2567         # has an ID field set, then it was persisted in the right Cell DB.
2568         if instance.obj_attr_is_set('id'):
2569             instance.update(updates)
2570             # Instance has been scheduled and the BuildRequest has been deleted
2571             # we can directly write the update down to the right cell.
2572             inst_map = self._get_instance_map_or_none(context, instance.uuid)
2573             # If we have a cell_mapping and we're not on cells v1, then
2574             # look up the instance in the cell database
2575             if inst_map and (inst_map.cell_mapping is not None) and (
2576                     not CONF.cells.enable):
2577                 with nova_context.target_cell(context,
2578                                               inst_map.cell_mapping) as cctxt:
2579                     with instance.obj_alternate_context(cctxt):
2580                         instance.save()
2581             else:
2582                 # If inst_map.cell_mapping does not point at a cell then cell
2583                 # migration has not happened yet.
2584                 # TODO(alaski): Make this a failure case after we put in
2585                 # a block that requires migrating to cellsv2.
2586                 instance.save()
2587         else:
2588             # Instance is not yet mapped to a cell, so we need to update
2589             # BuildRequest instead
2590             # TODO(sbauza): Fix the possible race conditions where BuildRequest
2591             # could be deleted because of either a concurrent instance delete
2592             # or because the scheduler just returned a destination right
2593             # after we called the instance in the API.
2594             try:
2595                 build_req = objects.BuildRequest.get_by_instance_uuid(
2596                     context, instance.uuid)
2597                 instance = build_req.instance
2598                 instance.update(updates)
2599                 # FIXME(sbauza): Here we are updating the current
2600                 # thread-related BuildRequest object. Given that another worker
2601                 # could have looking up at that BuildRequest in the API, it
2602                 # means that it could pass it down to the conductor without
2603                 # making sure that it's not updated, we could have some race
2604                 # condition where it would missing the updated fields, but
2605                 # that's something we could discuss once the instance record
2606                 # is persisted by the conductor.
2607                 build_req.save()
2608             except exception.BuildRequestNotFound:
2609                 # Instance was mapped and the BuildRequest was deleted
2610                 # while fetching (and possibly the instance could have been
2611                 # deleted as well). We need to lookup again the Instance object
2612                 # in order to correctly update it.
2613                 # TODO(sbauza): Figure out a good way to know the expected
2614                 # attributes by checking which fields are set or not.
2615                 expected_attrs = ['flavor', 'pci_devices', 'numa_topology',
2616                                   'tags', 'metadata', 'system_metadata',
2617                                   'security_groups', 'info_cache']
2618                 inst_map = self._get_instance_map_or_none(context,
2619                                                           instance.uuid)
2620                 if inst_map and (inst_map.cell_mapping is not None):
2621                     with nova_context.target_cell(
2622                             context,
2623                             inst_map.cell_mapping) as cctxt:
2624                         instance = objects.Instance.get_by_uuid(
2625                             cctxt, instance.uuid,
2626                             expected_attrs=expected_attrs)
2627                         instance.update(updates)
2628                         instance.save()
2629                 else:
2630                     # If inst_map.cell_mapping does not point at a cell then
2631                     # cell migration has not happened yet.
2632                     # TODO(alaski): Make this a failure case after we put in
2633                     # a block that requires migrating to cellsv2.
2634                     instance = objects.Instance.get_by_uuid(
2635                         context, instance.uuid, expected_attrs=expected_attrs)
2636                     instance.update(updates)
2637                     instance.save()
2638         return instance
2639 
2640     # NOTE(melwitt): We don't check instance lock for backup because lock is
2641     #                intended to prevent accidental change/delete of instances
2642     @check_instance_cell
2643     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
2644                                     vm_states.PAUSED, vm_states.SUSPENDED])
2645     def backup(self, context, instance, name, backup_type, rotation,
2646                extra_properties=None):
2647         """Backup the given instance
2648 
2649         :param instance: nova.objects.instance.Instance object
2650         :param name: name of the backup
2651         :param backup_type: 'daily' or 'weekly'
2652         :param rotation: int representing how many backups to keep around;
2653             None if rotation shouldn't be used (as in the case of snapshots)
2654         :param extra_properties: dict of extra image properties to include
2655                                  when creating the image.
2656         :returns: A dict containing image metadata
2657         """
2658         props_copy = dict(extra_properties, backup_type=backup_type)
2659 
2660         if compute_utils.is_volume_backed_instance(context, instance):
2661             LOG.info("It's not supported to backup volume backed "
2662                      "instance.", instance=instance)
2663             raise exception.InvalidRequest(
2664                 _('Backup is not supported for volume-backed instances.'))
2665         else:
2666             image_meta = self._create_image(context, instance,
2667                                             name, 'backup',
2668                                             extra_properties=props_copy)
2669 
2670         # NOTE(comstud): Any changes to this method should also be made
2671         # to the backup_instance() method in nova/cells/messaging.py
2672 
2673         instance.task_state = task_states.IMAGE_BACKUP
2674         instance.save(expected_task_state=[None])
2675 
2676         self._record_action_start(context, instance,
2677                                   instance_actions.BACKUP)
2678 
2679         self.compute_rpcapi.backup_instance(context, instance,
2680                                             image_meta['id'],
2681                                             backup_type,
2682                                             rotation)
2683         return image_meta
2684 
2685     # NOTE(melwitt): We don't check instance lock for snapshot because lock is
2686     #                intended to prevent accidental change/delete of instances
2687     @check_instance_cell
2688     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
2689                                     vm_states.PAUSED, vm_states.SUSPENDED])
2690     def snapshot(self, context, instance, name, extra_properties=None):
2691         """Snapshot the given instance.
2692 
2693         :param instance: nova.objects.instance.Instance object
2694         :param name: name of the snapshot
2695         :param extra_properties: dict of extra image properties to include
2696                                  when creating the image.
2697         :returns: A dict containing image metadata
2698         """
2699         image_meta = self._create_image(context, instance, name,
2700                                         'snapshot',
2701                                         extra_properties=extra_properties)
2702 
2703         # NOTE(comstud): Any changes to this method should also be made
2704         # to the snapshot_instance() method in nova/cells/messaging.py
2705         instance.task_state = task_states.IMAGE_SNAPSHOT_PENDING
2706         try:
2707             instance.save(expected_task_state=[None])
2708         except (exception.InstanceNotFound,
2709                 exception.UnexpectedDeletingTaskStateError) as ex:
2710             # Changing the instance task state to use in raising the
2711             # InstanceInvalidException below
2712             LOG.debug('Instance disappeared during snapshot.',
2713                       instance=instance)
2714             try:
2715                 image_id = image_meta['id']
2716                 self.image_api.delete(context, image_id)
2717                 LOG.info('Image %s deleted because instance '
2718                          'deleted before snapshot started.',
2719                          image_id, instance=instance)
2720             except exception.ImageNotFound:
2721                 pass
2722             except Exception as exc:
2723                 LOG.warning("Error while trying to clean up image %(img_id)s: "
2724                             "%(error_msg)s",
2725                             {"img_id": image_meta['id'],
2726                              "error_msg": six.text_type(exc)})
2727             attr = 'task_state'
2728             state = task_states.DELETING
2729             if type(ex) == exception.InstanceNotFound:
2730                 attr = 'vm_state'
2731                 state = vm_states.DELETED
2732             raise exception.InstanceInvalidState(attr=attr,
2733                                            instance_uuid=instance.uuid,
2734                                            state=state,
2735                                            method='snapshot')
2736 
2737         self._record_action_start(context, instance,
2738                                   instance_actions.CREATE_IMAGE)
2739 
2740         self.compute_rpcapi.snapshot_instance(context, instance,
2741                                               image_meta['id'])
2742 
2743         return image_meta
2744 
2745     def _create_image(self, context, instance, name, image_type,
2746                       extra_properties=None):
2747         """Create new image entry in the image service.  This new image
2748         will be reserved for the compute manager to upload a snapshot
2749         or backup.
2750 
2751         :param context: security context
2752         :param instance: nova.objects.instance.Instance object
2753         :param name: string for name of the snapshot
2754         :param image_type: snapshot | backup
2755         :param extra_properties: dict of extra image properties to include
2756 
2757         """
2758         properties = {
2759             'instance_uuid': instance.uuid,
2760             'user_id': str(context.user_id),
2761             'image_type': image_type,
2762         }
2763         properties.update(extra_properties or {})
2764 
2765         image_meta = self._initialize_instance_snapshot_metadata(
2766             instance, name, properties)
2767         # if we're making a snapshot, omit the disk and container formats,
2768         # since the image may have been converted to another format, and the
2769         # original values won't be accurate.  The driver will populate these
2770         # with the correct values later, on image upload.
2771         if image_type == 'snapshot':
2772             image_meta.pop('disk_format', None)
2773             image_meta.pop('container_format', None)
2774         return self.image_api.create(context, image_meta)
2775 
2776     def _initialize_instance_snapshot_metadata(self, instance, name,
2777                                                extra_properties=None):
2778         """Initialize new metadata for a snapshot of the given instance.
2779 
2780         :param instance: nova.objects.instance.Instance object
2781         :param name: string for name of the snapshot
2782         :param extra_properties: dict of extra metadata properties to include
2783 
2784         :returns: the new instance snapshot metadata
2785         """
2786         image_meta = utils.get_image_from_system_metadata(
2787             instance.system_metadata)
2788         image_meta.update({'name': name,
2789                            'is_public': False})
2790 
2791         # Delete properties that are non-inheritable
2792         properties = image_meta['properties']
2793         for key in CONF.non_inheritable_image_properties:
2794             properties.pop(key, None)
2795 
2796         # The properties in extra_properties have precedence
2797         properties.update(extra_properties or {})
2798 
2799         return image_meta
2800 
2801     # NOTE(melwitt): We don't check instance lock for snapshot because lock is
2802     #                intended to prevent accidental change/delete of instances
2803     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
2804                                     vm_states.SUSPENDED])
2805     def snapshot_volume_backed(self, context, instance, name,
2806                                extra_properties=None):
2807         """Snapshot the given volume-backed instance.
2808 
2809         :param instance: nova.objects.instance.Instance object
2810         :param name: name of the backup or snapshot
2811         :param extra_properties: dict of extra image properties to include
2812 
2813         :returns: the new image metadata
2814         """
2815         image_meta = self._initialize_instance_snapshot_metadata(
2816             instance, name, extra_properties)
2817         # the new image is simply a bucket of properties (particularly the
2818         # block device mapping, kernel and ramdisk IDs) with no image data,
2819         # hence the zero size
2820         image_meta['size'] = 0
2821         for attr in ('container_format', 'disk_format'):
2822             image_meta.pop(attr, None)
2823         properties = image_meta['properties']
2824         # clean properties before filling
2825         for key in ('block_device_mapping', 'bdm_v2', 'root_device_name'):
2826             properties.pop(key, None)
2827         if instance.root_device_name:
2828             properties['root_device_name'] = instance.root_device_name
2829 
2830         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
2831                 context, instance.uuid)
2832 
2833         mapping = []  # list of BDM dicts that can go into the image properties
2834         # Do some up-front filtering of the list of BDMs from
2835         # which we are going to create snapshots.
2836         volume_bdms = []
2837         for bdm in bdms:
2838             if bdm.no_device:
2839                 continue
2840             if bdm.is_volume:
2841                 # These will be handled below.
2842                 volume_bdms.append(bdm)
2843             else:
2844                 mapping.append(bdm.get_image_mapping())
2845 
2846         # Check limits in Cinder before creating snapshots to avoid going over
2847         # quota in the middle of a list of volumes. This is a best-effort check
2848         # but concurrently running snapshot requests from the same project
2849         # could still fail to create volume snapshots if they go over limit.
2850         if volume_bdms:
2851             limits = self.volume_api.get_absolute_limits(context)
2852             total_snapshots_used = limits['totalSnapshotsUsed']
2853             max_snapshots = limits['maxTotalSnapshots']
2854             # -1 means there is unlimited quota for snapshots
2855             if (max_snapshots > -1 and
2856                     len(volume_bdms) + total_snapshots_used > max_snapshots):
2857                 LOG.debug('Unable to create volume snapshots for instance. '
2858                           'Currently has %s snapshots, requesting %s new '
2859                           'snapshots, with a limit of %s.',
2860                           total_snapshots_used, len(volume_bdms),
2861                           max_snapshots, instance=instance)
2862                 raise exception.OverQuota(overs='snapshots')
2863 
2864         quiesced = False
2865         if instance.vm_state == vm_states.ACTIVE:
2866             try:
2867                 LOG.info("Attempting to quiesce instance before volume "
2868                          "snapshot.", instance=instance)
2869                 self.compute_rpcapi.quiesce_instance(context, instance)
2870                 quiesced = True
2871             except (exception.InstanceQuiesceNotSupported,
2872                     exception.QemuGuestAgentNotEnabled,
2873                     exception.NovaException, NotImplementedError) as err:
2874                 if strutils.bool_from_string(instance.system_metadata.get(
2875                         'image_os_require_quiesce')):
2876                     raise
2877                 else:
2878                     LOG.info('Skipping quiescing instance: %(reason)s.',
2879                              {'reason': err},
2880                              instance=instance)
2881 
2882         @wrap_instance_event(prefix='api')
2883         def snapshot_instance(self, context, instance, bdms):
2884             try:
2885                 for bdm in volume_bdms:
2886                     # create snapshot based on volume_id
2887                     volume = self.volume_api.get(context, bdm.volume_id)
2888                     # NOTE(yamahata): Should we wait for snapshot creation?
2889                     #                 Linux LVM snapshot creation completes in
2890                     #                 short time, it doesn't matter for now.
2891                     name = _('snapshot for %s') % image_meta['name']
2892                     LOG.debug('Creating snapshot from volume %s.',
2893                               volume['id'], instance=instance)
2894                     snapshot = self.volume_api.create_snapshot_force(
2895                         context, volume['id'],
2896                         name, volume['display_description'])
2897                     mapping_dict = block_device.snapshot_from_bdm(
2898                         snapshot['id'], bdm)
2899                     mapping_dict = mapping_dict.get_image_mapping()
2900                     mapping.append(mapping_dict)
2901                 return mapping
2902             # NOTE(tasker): No error handling is done in the above for loop.
2903             # This means that if the snapshot fails and throws an exception
2904             # the traceback will skip right over the unquiesce needed below.
2905             # Here, catch any exception, unquiesce the instance, and raise the
2906             # error so that the calling function can do what it needs to in
2907             # order to properly treat a failed snap.
2908             except Exception:
2909                 with excutils.save_and_reraise_exception():
2910                     if quiesced:
2911                         LOG.info("Unquiescing instance after volume snapshot "
2912                                  "failure.", instance=instance)
2913                         self.compute_rpcapi.unquiesce_instance(
2914                             context, instance, mapping)
2915 
2916         self._record_action_start(context, instance,
2917                                   instance_actions.CREATE_IMAGE)
2918         mapping = snapshot_instance(self, context, instance, bdms)
2919 
2920         if quiesced:
2921             self.compute_rpcapi.unquiesce_instance(context, instance, mapping)
2922 
2923         if mapping:
2924             properties['block_device_mapping'] = mapping
2925             properties['bdm_v2'] = True
2926 
2927         return self.image_api.create(context, image_meta)
2928 
2929     @check_instance_lock
2930     def reboot(self, context, instance, reboot_type):
2931         """Reboot the given instance."""
2932         if reboot_type == 'SOFT':
2933             self._soft_reboot(context, instance)
2934         else:
2935             self._hard_reboot(context, instance)
2936 
2937     @check_instance_state(vm_state=set(vm_states.ALLOW_SOFT_REBOOT),
2938                           task_state=[None])
2939     def _soft_reboot(self, context, instance):
2940         expected_task_state = [None]
2941         instance.task_state = task_states.REBOOTING
2942         instance.save(expected_task_state=expected_task_state)
2943 
2944         self._record_action_start(context, instance, instance_actions.REBOOT)
2945 
2946         self.compute_rpcapi.reboot_instance(context, instance=instance,
2947                                             block_device_info=None,
2948                                             reboot_type='SOFT')
2949 
2950     @check_instance_state(vm_state=set(vm_states.ALLOW_HARD_REBOOT),
2951                           task_state=task_states.ALLOW_REBOOT)
2952     def _hard_reboot(self, context, instance):
2953         instance.task_state = task_states.REBOOTING_HARD
2954         expected_task_state = [None,
2955                                task_states.REBOOTING,
2956                                task_states.REBOOT_PENDING,
2957                                task_states.REBOOT_STARTED,
2958                                task_states.REBOOTING_HARD,
2959                                task_states.RESUMING,
2960                                task_states.UNPAUSING,
2961                                task_states.SUSPENDING]
2962         instance.save(expected_task_state = expected_task_state)
2963 
2964         self._record_action_start(context, instance, instance_actions.REBOOT)
2965 
2966         self.compute_rpcapi.reboot_instance(context, instance=instance,
2967                                             block_device_info=None,
2968                                             reboot_type='HARD')
2969 
2970     @check_instance_lock
2971     @check_instance_cell
2972     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
2973                                     vm_states.ERROR])
2974     def rebuild(self, context, instance, image_href, admin_password,
2975                 files_to_inject=None, **kwargs):
2976         """Rebuild the given instance with the provided attributes."""
2977         files_to_inject = files_to_inject or []
2978         metadata = kwargs.get('metadata', {})
2979         preserve_ephemeral = kwargs.get('preserve_ephemeral', False)
2980         auto_disk_config = kwargs.get('auto_disk_config')
2981 
2982         if 'key_name' in kwargs:
2983             key_name = kwargs.pop('key_name')
2984             if key_name:
2985                 # NOTE(liuyulong): we are intentionally using the user_id from
2986                 # the request context rather than the instance.user_id because
2987                 # users own keys but instances are owned by projects, and
2988                 # another user in the same project can rebuild an instance
2989                 # even if they didn't create it.
2990                 key_pair = objects.KeyPair.get_by_name(context,
2991                                                        context.user_id,
2992                                                        key_name)
2993                 instance.key_name = key_pair.name
2994                 instance.key_data = key_pair.public_key
2995                 instance.keypairs = objects.KeyPairList(objects=[key_pair])
2996             else:
2997                 instance.key_name = None
2998                 instance.key_data = None
2999                 instance.keypairs = objects.KeyPairList(objects=[])
3000 
3001         image_id, image = self._get_image(context, image_href)
3002         self._check_auto_disk_config(image=image, **kwargs)
3003 
3004         flavor = instance.get_flavor()
3005         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
3006             context, instance.uuid)
3007         root_bdm = compute_utils.get_root_bdm(context, instance, bdms)
3008 
3009         # Check to see if the image is changing and we have a volume-backed
3010         # server. The compute doesn't support changing the image in the
3011         # root disk of a volume-backed server, so we need to just fail fast.
3012         is_volume_backed = compute_utils.is_volume_backed_instance(
3013             context, instance, bdms)
3014         if is_volume_backed:
3015             # For boot from volume, instance.image_ref is empty, so we need to
3016             # query the image from the volume.
3017             if root_bdm is None:
3018                 # This shouldn't happen and is an error, we need to fail. This
3019                 # is not the users fault, it's an internal error. Without a
3020                 # root BDM we have no way of knowing the backing volume (or
3021                 # image in that volume) for this instance.
3022                 raise exception.NovaException(
3023                     _('Unable to find root block device mapping for '
3024                       'volume-backed instance.'))
3025 
3026             volume = self.volume_api.get(context, root_bdm.volume_id)
3027             volume_image_metadata = volume.get('volume_image_metadata', {})
3028             orig_image_ref = volume_image_metadata.get('image_id')
3029 
3030             if orig_image_ref != image_href:
3031                 # Leave a breadcrumb.
3032                 LOG.debug('Requested to rebuild instance with a new image %s '
3033                           'for a volume-backed server with image %s in its '
3034                           'root volume which is not supported.', image_href,
3035                           orig_image_ref, instance=instance)
3036                 msg = _('Unable to rebuild with a different image for a '
3037                         'volume-backed server.')
3038                 raise exception.ImageUnacceptable(
3039                     image_id=image_href, reason=msg)
3040         else:
3041             orig_image_ref = instance.image_ref
3042 
3043         self._checks_for_create_and_rebuild(context, image_id, image,
3044                 flavor, metadata, files_to_inject, root_bdm)
3045 
3046         kernel_id, ramdisk_id = self._handle_kernel_and_ramdisk(
3047                 context, None, None, image)
3048 
3049         def _reset_image_metadata():
3050             """Remove old image properties that we're storing as instance
3051             system metadata.  These properties start with 'image_'.
3052             Then add the properties for the new image.
3053             """
3054             # FIXME(comstud): There's a race condition here in that if
3055             # the system_metadata for this instance is updated after
3056             # we do the previous save() and before we update.. those
3057             # other updates will be lost. Since this problem exists in
3058             # a lot of other places, I think it should be addressed in
3059             # a DB layer overhaul.
3060 
3061             orig_sys_metadata = dict(instance.system_metadata)
3062             # Remove the old keys
3063             for key in list(instance.system_metadata.keys()):
3064                 if key.startswith(utils.SM_IMAGE_PROP_PREFIX):
3065                     del instance.system_metadata[key]
3066 
3067             # Add the new ones
3068             new_sys_metadata = utils.get_system_metadata_from_image(
3069                 image, flavor)
3070 
3071             instance.system_metadata.update(new_sys_metadata)
3072             instance.save()
3073             return orig_sys_metadata
3074 
3075         # Since image might have changed, we may have new values for
3076         # os_type, vm_mode, etc
3077         options_from_image = self._inherit_properties_from_image(
3078                 image, auto_disk_config)
3079         instance.update(options_from_image)
3080 
3081         instance.task_state = task_states.REBUILDING
3082         # An empty instance.image_ref is currently used as an indication
3083         # of BFV.  Preserve that over a rebuild to not break users.
3084         if not is_volume_backed:
3085             instance.image_ref = image_href
3086         instance.kernel_id = kernel_id or ""
3087         instance.ramdisk_id = ramdisk_id or ""
3088         instance.progress = 0
3089         instance.update(kwargs)
3090         instance.save(expected_task_state=[None])
3091 
3092         # On a rebuild, since we're potentially changing images, we need to
3093         # wipe out the old image properties that we're storing as instance
3094         # system metadata... and copy in the properties for the new image.
3095         orig_sys_metadata = _reset_image_metadata()
3096 
3097         self._record_action_start(context, instance, instance_actions.REBUILD)
3098 
3099         # NOTE(sbauza): The migration script we provided in Newton should make
3100         # sure that all our instances are currently migrated to have an
3101         # attached RequestSpec object but let's consider that the operator only
3102         # half migrated all their instances in the meantime.
3103         host = instance.host
3104         try:
3105             request_spec = objects.RequestSpec.get_by_instance_uuid(
3106                 context, instance.uuid)
3107             # If a new image is provided on rebuild, we will need to run
3108             # through the scheduler again, but we want the instance to be
3109             # rebuilt on the same host it's already on.
3110             if orig_image_ref != image_href:
3111                 # We have to modify the request spec that goes to the scheduler
3112                 # to contain the new image. We persist this since we've already
3113                 # changed the instance.image_ref above so we're being
3114                 # consistent.
3115                 request_spec.image = objects.ImageMeta.from_dict(image)
3116                 request_spec.save()
3117                 if 'scheduler_hints' not in request_spec:
3118                     request_spec.scheduler_hints = {}
3119                 # Nuke the id on this so we can't accidentally save
3120                 # this hint hack later
3121                 del request_spec.id
3122 
3123                 # NOTE(danms): Passing host=None tells conductor to
3124                 # call the scheduler. The _nova_check_type hint
3125                 # requires that the scheduler returns only the same
3126                 # host that we are currently on and only checks
3127                 # rebuild-related filters.
3128                 request_spec.scheduler_hints['_nova_check_type'] = ['rebuild']
3129                 request_spec.force_hosts = [instance.host]
3130                 request_spec.force_nodes = [instance.node]
3131                 host = None
3132         except exception.RequestSpecNotFound:
3133             # Some old instances can still have no RequestSpec object attached
3134             # to them, we need to support the old way
3135             request_spec = None
3136 
3137         self.compute_task_api.rebuild_instance(context, instance=instance,
3138                 new_pass=admin_password, injected_files=files_to_inject,
3139                 image_ref=image_href, orig_image_ref=orig_image_ref,
3140                 orig_sys_metadata=orig_sys_metadata, bdms=bdms,
3141                 preserve_ephemeral=preserve_ephemeral, host=host,
3142                 request_spec=request_spec,
3143                 kwargs=kwargs)
3144 
3145     @staticmethod
3146     def _check_quota_for_upsize(context, instance, current_flavor, new_flavor):
3147         project_id, user_id = quotas_obj.ids_from_instance(context,
3148                                                            instance)
3149         # Deltas will be empty if the resize is not an upsize.
3150         deltas = compute_utils.upsize_quota_delta(new_flavor,
3151                                                   current_flavor)
3152         if deltas:
3153             try:
3154                 res_deltas = {'cores': deltas.get('cores', 0),
3155                               'ram': deltas.get('ram', 0)}
3156                 objects.Quotas.check_deltas(context, res_deltas,
3157                                             project_id, user_id=user_id,
3158                                             check_project_id=project_id,
3159                                             check_user_id=user_id)
3160             except exception.OverQuota as exc:
3161                 quotas = exc.kwargs['quotas']
3162                 overs = exc.kwargs['overs']
3163                 usages = exc.kwargs['usages']
3164                 headroom = compute_utils.get_headroom(quotas, usages,
3165                                                       deltas)
3166                 (overs, reqs, total_alloweds,
3167                  useds) = compute_utils.get_over_quota_detail(headroom,
3168                                                               overs,
3169                                                               quotas,
3170                                                               deltas)
3171                 LOG.warning("%(overs)s quota exceeded for %(pid)s,"
3172                             " tried to resize instance.",
3173                             {'overs': overs, 'pid': context.project_id})
3174                 raise exception.TooManyInstances(overs=overs,
3175                                                  req=reqs,
3176                                                  used=useds,
3177                                                  allowed=total_alloweds)
3178 
3179     @check_instance_lock
3180     @check_instance_cell
3181     @check_instance_state(vm_state=[vm_states.RESIZED])
3182     def revert_resize(self, context, instance):
3183         """Reverts a resize, deleting the 'new' instance in the process."""
3184         elevated = context.elevated()
3185         migration = objects.Migration.get_by_instance_and_status(
3186             elevated, instance.uuid, 'finished')
3187 
3188         # If this is a resize down, a revert might go over quota.
3189         self._check_quota_for_upsize(context, instance, instance.flavor,
3190                                      instance.old_flavor)
3191 
3192         instance.task_state = task_states.RESIZE_REVERTING
3193         instance.save(expected_task_state=[None])
3194 
3195         migration.status = 'reverting'
3196         migration.save()
3197 
3198         self._record_action_start(context, instance,
3199                                   instance_actions.REVERT_RESIZE)
3200 
3201         # TODO(melwitt): We're not rechecking for strict quota here to guard
3202         # against going over quota during a race at this time because the
3203         # resource consumption for this operation is written to the database
3204         # by compute.
3205         self.compute_rpcapi.revert_resize(context, instance,
3206                                           migration,
3207                                           migration.dest_compute)
3208 
3209     @check_instance_lock
3210     @check_instance_cell
3211     @check_instance_state(vm_state=[vm_states.RESIZED])
3212     def confirm_resize(self, context, instance, migration=None):
3213         """Confirms a migration/resize and deletes the 'old' instance."""
3214         elevated = context.elevated()
3215         # NOTE(melwitt): We're not checking quota here because there isn't a
3216         # change in resource usage when confirming a resize. Resource
3217         # consumption for resizes are written to the database by compute, so
3218         # a confirm resize is just a clean up of the migration objects and a
3219         # state change in compute.
3220         if migration is None:
3221             migration = objects.Migration.get_by_instance_and_status(
3222                 elevated, instance.uuid, 'finished')
3223 
3224         migration.status = 'confirming'
3225         migration.save()
3226 
3227         self._record_action_start(context, instance,
3228                                   instance_actions.CONFIRM_RESIZE)
3229 
3230         self.compute_rpcapi.confirm_resize(context,
3231                                            instance,
3232                                            migration,
3233                                            migration.source_compute)
3234 
3235     @staticmethod
3236     def _resize_cells_support(context, instance,
3237                               current_instance_type, new_instance_type):
3238         """Special API cell logic for resize."""
3239         # NOTE(johannes/comstud): The API cell needs a local migration
3240         # record for later resize_confirm and resize_reverts.
3241         # We don't need source and/or destination
3242         # information, just the old and new flavors. Status is set to
3243         # 'finished' since nothing else will update the status along
3244         # the way.
3245         mig = objects.Migration(context=context.elevated())
3246         mig.instance_uuid = instance.uuid
3247         mig.old_instance_type_id = current_instance_type['id']
3248         mig.new_instance_type_id = new_instance_type['id']
3249         mig.status = 'finished'
3250         mig.migration_type = (
3251             mig.old_instance_type_id != mig.new_instance_type_id and
3252             'resize' or 'migration')
3253         mig.create()
3254 
3255     @check_instance_lock
3256     @check_instance_cell
3257     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED])
3258     def resize(self, context, instance, flavor_id=None, clean_shutdown=True,
3259                host_name=None, **extra_instance_updates):
3260         """Resize (ie, migrate) a running instance.
3261 
3262         If flavor_id is None, the process is considered a migration, keeping
3263         the original flavor_id. If flavor_id is not None, the instance should
3264         be migrated to a new host and resized to the new flavor_id.
3265         host_name is always None in the resize case.
3266         host_name can be set in the cold migration case only.
3267         """
3268         if host_name is not None:
3269             # Cannot migrate to the host where the instance exists
3270             # because it is useless.
3271             if host_name == instance.host:
3272                 raise exception.CannotMigrateToSameHost()
3273 
3274             # Check whether host exists or not.
3275             node = objects.ComputeNode.get_first_node_by_host_for_old_compat(
3276                 context, host_name, use_slave=True)
3277 
3278         self._check_auto_disk_config(instance, **extra_instance_updates)
3279 
3280         current_instance_type = instance.get_flavor()
3281 
3282         # If flavor_id is not provided, only migrate the instance.
3283         if not flavor_id:
3284             LOG.debug("flavor_id is None. Assuming migration.",
3285                       instance=instance)
3286             new_instance_type = current_instance_type
3287         else:
3288             new_instance_type = flavors.get_flavor_by_flavor_id(
3289                     flavor_id, read_deleted="no")
3290             if (new_instance_type.get('root_gb') == 0 and
3291                 current_instance_type.get('root_gb') != 0 and
3292                 not compute_utils.is_volume_backed_instance(context,
3293                     instance)):
3294                 reason = _('Resize to zero disk flavor is not allowed.')
3295                 raise exception.CannotResizeDisk(reason=reason)
3296 
3297         if not new_instance_type:
3298             raise exception.FlavorNotFound(flavor_id=flavor_id)
3299 
3300         current_instance_type_name = current_instance_type['name']
3301         new_instance_type_name = new_instance_type['name']
3302         LOG.debug("Old instance type %(current_instance_type_name)s, "
3303                   "new instance type %(new_instance_type_name)s",
3304                   {'current_instance_type_name': current_instance_type_name,
3305                    'new_instance_type_name': new_instance_type_name},
3306                   instance=instance)
3307 
3308         same_instance_type = (current_instance_type['id'] ==
3309                               new_instance_type['id'])
3310 
3311         # NOTE(sirp): We don't want to force a customer to change their flavor
3312         # when Ops is migrating off of a failed host.
3313         if not same_instance_type and new_instance_type.get('disabled'):
3314             raise exception.FlavorNotFound(flavor_id=flavor_id)
3315 
3316         if same_instance_type and flavor_id and self.cell_type != 'compute':
3317             raise exception.CannotResizeToSameFlavor()
3318 
3319         # ensure there is sufficient headroom for upsizes
3320         if flavor_id:
3321             self._check_quota_for_upsize(context, instance,
3322                                          current_instance_type,
3323                                          new_instance_type)
3324 
3325         instance.task_state = task_states.RESIZE_PREP
3326         instance.progress = 0
3327         instance.update(extra_instance_updates)
3328         instance.save(expected_task_state=[None])
3329 
3330         filter_properties = {'ignore_hosts': []}
3331 
3332         if not CONF.allow_resize_to_same_host:
3333             filter_properties['ignore_hosts'].append(instance.host)
3334 
3335         if self.cell_type == 'api':
3336             # Create migration record.
3337             self._resize_cells_support(context, instance,
3338                                        current_instance_type,
3339                                        new_instance_type)
3340 
3341         if not flavor_id:
3342             self._record_action_start(context, instance,
3343                                       instance_actions.MIGRATE)
3344         else:
3345             self._record_action_start(context, instance,
3346                                       instance_actions.RESIZE)
3347 
3348         # NOTE(sbauza): The migration script we provided in Newton should make
3349         # sure that all our instances are currently migrated to have an
3350         # attached RequestSpec object but let's consider that the operator only
3351         # half migrated all their instances in the meantime.
3352         try:
3353             request_spec = objects.RequestSpec.get_by_instance_uuid(
3354                 context, instance.uuid)
3355             request_spec.ignore_hosts = filter_properties['ignore_hosts']
3356         except exception.RequestSpecNotFound:
3357             # Some old instances can still have no RequestSpec object attached
3358             # to them, we need to support the old way
3359             if host_name is not None:
3360                 # If there is no request spec we cannot honor the request
3361                 # and we need to fail.
3362                 raise exception.CannotMigrateWithTargetHost()
3363             request_spec = None
3364 
3365         # TODO(melwitt): We're not rechecking for strict quota here to guard
3366         # against going over quota during a race at this time because the
3367         # resource consumption for this operation is written to the database
3368         # by compute.
3369         scheduler_hint = {'filter_properties': filter_properties}
3370 
3371         if request_spec:
3372             if host_name is None:
3373                 # If 'host_name' is not specified,
3374                 # clear the 'requested_destination' field of the RequestSpec.
3375                 request_spec.requested_destination = None
3376             else:
3377                 # Set the host and the node so that the scheduler will
3378                 # validate them.
3379                 # TODO(takashin): It will be added to check whether
3380                 # the specified host is within the same cell as
3381                 # the instance or not. If not, raise specific error message
3382                 # that is clear to the caller.
3383                 request_spec.requested_destination = objects.Destination(
3384                     host=node.host, node=node.hypervisor_hostname)
3385 
3386         self.compute_task_api.resize_instance(context, instance,
3387                 extra_instance_updates, scheduler_hint=scheduler_hint,
3388                 flavor=new_instance_type,
3389                 clean_shutdown=clean_shutdown,
3390                 request_spec=request_spec)
3391 
3392     @check_instance_lock
3393     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
3394                                     vm_states.PAUSED, vm_states.SUSPENDED])
3395     def shelve(self, context, instance, clean_shutdown=True):
3396         """Shelve an instance.
3397 
3398         Shuts down an instance and frees it up to be removed from the
3399         hypervisor.
3400         """
3401         instance.task_state = task_states.SHELVING
3402         instance.save(expected_task_state=[None])
3403 
3404         self._record_action_start(context, instance, instance_actions.SHELVE)
3405 
3406         if not compute_utils.is_volume_backed_instance(context, instance):
3407             name = '%s-shelved' % instance.display_name
3408             image_meta = self._create_image(context, instance, name,
3409                     'snapshot')
3410             image_id = image_meta['id']
3411             self.compute_rpcapi.shelve_instance(context, instance=instance,
3412                     image_id=image_id, clean_shutdown=clean_shutdown)
3413         else:
3414             self.compute_rpcapi.shelve_offload_instance(context,
3415                     instance=instance, clean_shutdown=clean_shutdown)
3416 
3417     @check_instance_lock
3418     @check_instance_state(vm_state=[vm_states.SHELVED])
3419     def shelve_offload(self, context, instance, clean_shutdown=True):
3420         """Remove a shelved instance from the hypervisor."""
3421         instance.task_state = task_states.SHELVING_OFFLOADING
3422         instance.save(expected_task_state=[None])
3423 
3424         self._record_action_start(context, instance,
3425                                   instance_actions.SHELVE_OFFLOAD)
3426 
3427         self.compute_rpcapi.shelve_offload_instance(context, instance=instance,
3428             clean_shutdown=clean_shutdown)
3429 
3430     @check_instance_lock
3431     @check_instance_state(vm_state=[vm_states.SHELVED,
3432         vm_states.SHELVED_OFFLOADED])
3433     def unshelve(self, context, instance):
3434         """Restore a shelved instance."""
3435         instance.task_state = task_states.UNSHELVING
3436         instance.save(expected_task_state=[None])
3437 
3438         self._record_action_start(context, instance, instance_actions.UNSHELVE)
3439 
3440         try:
3441             request_spec = objects.RequestSpec.get_by_instance_uuid(
3442                 context, instance.uuid)
3443         except exception.RequestSpecNotFound:
3444             # Some old instances can still have no RequestSpec object attached
3445             # to them, we need to support the old way
3446             request_spec = None
3447         self.compute_task_api.unshelve_instance(context, instance,
3448                                                 request_spec)
3449 
3450     @check_instance_lock
3451     def add_fixed_ip(self, context, instance, network_id):
3452         """Add fixed_ip from specified network to given instance."""
3453         self.compute_rpcapi.add_fixed_ip_to_instance(context,
3454                 instance=instance, network_id=network_id)
3455 
3456     @check_instance_lock
3457     def remove_fixed_ip(self, context, instance, address):
3458         """Remove fixed_ip from specified network to given instance."""
3459         self.compute_rpcapi.remove_fixed_ip_from_instance(context,
3460                 instance=instance, address=address)
3461 
3462     @check_instance_lock
3463     @check_instance_cell
3464     @check_instance_state(vm_state=[vm_states.ACTIVE])
3465     def pause(self, context, instance):
3466         """Pause the given instance."""
3467         instance.task_state = task_states.PAUSING
3468         instance.save(expected_task_state=[None])
3469         self._record_action_start(context, instance, instance_actions.PAUSE)
3470         self.compute_rpcapi.pause_instance(context, instance)
3471 
3472     @check_instance_lock
3473     @check_instance_cell
3474     @check_instance_state(vm_state=[vm_states.PAUSED])
3475     def unpause(self, context, instance):
3476         """Unpause the given instance."""
3477         instance.task_state = task_states.UNPAUSING
3478         instance.save(expected_task_state=[None])
3479         self._record_action_start(context, instance, instance_actions.UNPAUSE)
3480         self.compute_rpcapi.unpause_instance(context, instance)
3481 
3482     @check_instance_host
3483     def get_diagnostics(self, context, instance):
3484         """Retrieve diagnostics for the given instance."""
3485         return self.compute_rpcapi.get_diagnostics(context, instance=instance)
3486 
3487     @check_instance_host
3488     def get_instance_diagnostics(self, context, instance):
3489         """Retrieve diagnostics for the given instance."""
3490         return self.compute_rpcapi.get_instance_diagnostics(context,
3491                                                             instance=instance)
3492 
3493     @check_instance_lock
3494     @check_instance_cell
3495     @check_instance_state(vm_state=[vm_states.ACTIVE])
3496     def suspend(self, context, instance):
3497         """Suspend the given instance."""
3498         instance.task_state = task_states.SUSPENDING
3499         instance.save(expected_task_state=[None])
3500         self._record_action_start(context, instance, instance_actions.SUSPEND)
3501         self.compute_rpcapi.suspend_instance(context, instance)
3502 
3503     @check_instance_lock
3504     @check_instance_cell
3505     @check_instance_state(vm_state=[vm_states.SUSPENDED])
3506     def resume(self, context, instance):
3507         """Resume the given instance."""
3508         instance.task_state = task_states.RESUMING
3509         instance.save(expected_task_state=[None])
3510         self._record_action_start(context, instance, instance_actions.RESUME)
3511         self.compute_rpcapi.resume_instance(context, instance)
3512 
3513     @check_instance_lock
3514     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
3515                                     vm_states.ERROR])
3516     def rescue(self, context, instance, rescue_password=None,
3517                rescue_image_ref=None, clean_shutdown=True):
3518         """Rescue the given instance."""
3519 
3520         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
3521                     context, instance.uuid)
3522         for bdm in bdms:
3523             if bdm.volume_id:
3524                 vol = self.volume_api.get(context, bdm.volume_id)
3525                 self.volume_api.check_attached(context, vol)
3526         if compute_utils.is_volume_backed_instance(context, instance, bdms):
3527             reason = _("Cannot rescue a volume-backed instance")
3528             raise exception.InstanceNotRescuable(instance_id=instance.uuid,
3529                                                  reason=reason)
3530 
3531         instance.task_state = task_states.RESCUING
3532         instance.save(expected_task_state=[None])
3533 
3534         self._record_action_start(context, instance, instance_actions.RESCUE)
3535 
3536         self.compute_rpcapi.rescue_instance(context, instance=instance,
3537             rescue_password=rescue_password, rescue_image_ref=rescue_image_ref,
3538             clean_shutdown=clean_shutdown)
3539 
3540     @check_instance_lock
3541     @check_instance_state(vm_state=[vm_states.RESCUED])
3542     def unrescue(self, context, instance):
3543         """Unrescue the given instance."""
3544         instance.task_state = task_states.UNRESCUING
3545         instance.save(expected_task_state=[None])
3546 
3547         self._record_action_start(context, instance, instance_actions.UNRESCUE)
3548 
3549         self.compute_rpcapi.unrescue_instance(context, instance=instance)
3550 
3551     @check_instance_lock
3552     @check_instance_cell
3553     @check_instance_state(vm_state=[vm_states.ACTIVE])
3554     def set_admin_password(self, context, instance, password=None):
3555         """Set the root/admin password for the given instance.
3556 
3557         @param context: Nova auth context.
3558         @param instance: Nova instance object.
3559         @param password: The admin password for the instance.
3560         """
3561         instance.task_state = task_states.UPDATING_PASSWORD
3562         instance.save(expected_task_state=[None])
3563 
3564         self._record_action_start(context, instance,
3565                                   instance_actions.CHANGE_PASSWORD)
3566 
3567         self.compute_rpcapi.set_admin_password(context,
3568                                                instance=instance,
3569                                                new_pass=password)
3570 
3571     @check_instance_host
3572     @reject_instance_state(
3573         task_state=[task_states.DELETING, task_states.MIGRATING])
3574     def get_vnc_console(self, context, instance, console_type):
3575         """Get a url to an instance Console."""
3576         connect_info = self.compute_rpcapi.get_vnc_console(context,
3577                 instance=instance, console_type=console_type)
3578 
3579         # TODO(melwitt): In Rocky, the compute manager puts the
3580         # console authorization in the database in the above method.
3581         # The following will be removed when everything has been
3582         # converted to use the database, in Stein.
3583         self.consoleauth_rpcapi.authorize_console(context,
3584                 connect_info['token'], console_type,
3585                 connect_info['host'], connect_info['port'],
3586                 connect_info['internal_access_path'], instance.uuid,
3587                 access_url=connect_info['access_url'])
3588 
3589         return {'url': connect_info['access_url']}
3590 
3591     @check_instance_host
3592     def get_vnc_connect_info(self, context, instance, console_type):
3593         """Used in a child cell to get console info."""
3594         connect_info = self.compute_rpcapi.get_vnc_console(context,
3595                 instance=instance, console_type=console_type)
3596         return connect_info
3597 
3598     @check_instance_host
3599     @reject_instance_state(
3600         task_state=[task_states.DELETING, task_states.MIGRATING])
3601     def get_spice_console(self, context, instance, console_type):
3602         """Get a url to an instance Console."""
3603         connect_info = self.compute_rpcapi.get_spice_console(context,
3604                 instance=instance, console_type=console_type)
3605         # TODO(melwitt): In Rocky, the compute manager puts the
3606         # console authorization in the database in the above method.
3607         # The following will be removed when everything has been
3608         # converted to use the database, in Stein.
3609         self.consoleauth_rpcapi.authorize_console(context,
3610                 connect_info['token'], console_type,
3611                 connect_info['host'], connect_info['port'],
3612                 connect_info['internal_access_path'], instance.uuid,
3613                 access_url=connect_info['access_url'])
3614 
3615         return {'url': connect_info['access_url']}
3616 
3617     @check_instance_host
3618     def get_spice_connect_info(self, context, instance, console_type):
3619         """Used in a child cell to get console info."""
3620         connect_info = self.compute_rpcapi.get_spice_console(context,
3621                 instance=instance, console_type=console_type)
3622         return connect_info
3623 
3624     @check_instance_host
3625     @reject_instance_state(
3626         task_state=[task_states.DELETING, task_states.MIGRATING])
3627     def get_rdp_console(self, context, instance, console_type):
3628         """Get a url to an instance Console."""
3629         connect_info = self.compute_rpcapi.get_rdp_console(context,
3630                 instance=instance, console_type=console_type)
3631         # TODO(melwitt): In Rocky, the compute manager puts the
3632         # console authorization in the database in the above method.
3633         # The following will be removed when everything has been
3634         # converted to use the database, in Stein.
3635         self.consoleauth_rpcapi.authorize_console(context,
3636                 connect_info['token'], console_type,
3637                 connect_info['host'], connect_info['port'],
3638                 connect_info['internal_access_path'], instance.uuid,
3639                 access_url=connect_info['access_url'])
3640 
3641         return {'url': connect_info['access_url']}
3642 
3643     @check_instance_host
3644     def get_rdp_connect_info(self, context, instance, console_type):
3645         """Used in a child cell to get console info."""
3646         connect_info = self.compute_rpcapi.get_rdp_console(context,
3647                 instance=instance, console_type=console_type)
3648         return connect_info
3649 
3650     @check_instance_host
3651     @reject_instance_state(
3652         task_state=[task_states.DELETING, task_states.MIGRATING])
3653     def get_serial_console(self, context, instance, console_type):
3654         """Get a url to a serial console."""
3655         connect_info = self.compute_rpcapi.get_serial_console(context,
3656                 instance=instance, console_type=console_type)
3657 
3658         # TODO(melwitt): In Rocky, the compute manager puts the
3659         # console authorization in the database in the above method.
3660         # The following will be removed when everything has been
3661         # converted to use the database, in Stein.
3662         self.consoleauth_rpcapi.authorize_console(context,
3663                 connect_info['token'], console_type,
3664                 connect_info['host'], connect_info['port'],
3665                 connect_info['internal_access_path'], instance.uuid,
3666                 access_url=connect_info['access_url'])
3667         return {'url': connect_info['access_url']}
3668 
3669     @check_instance_host
3670     def get_serial_console_connect_info(self, context, instance, console_type):
3671         """Used in a child cell to get serial console."""
3672         connect_info = self.compute_rpcapi.get_serial_console(context,
3673                 instance=instance, console_type=console_type)
3674         return connect_info
3675 
3676     @check_instance_host
3677     @reject_instance_state(
3678         task_state=[task_states.DELETING, task_states.MIGRATING])
3679     def get_mks_console(self, context, instance, console_type):
3680         """Get a url to a MKS console."""
3681         connect_info = self.compute_rpcapi.get_mks_console(context,
3682                 instance=instance, console_type=console_type)
3683         # TODO(melwitt): In Rocky, the compute manager puts the
3684         # console authorization in the database in the above method.
3685         # The following will be removed when everything has been
3686         # converted to use the database, in Stein.
3687         self.consoleauth_rpcapi.authorize_console(context,
3688                 connect_info['token'], console_type,
3689                 connect_info['host'], connect_info['port'],
3690                 connect_info['internal_access_path'], instance.uuid,
3691                 access_url=connect_info['access_url'])
3692         return {'url': connect_info['access_url']}
3693 
3694     @check_instance_host
3695     def get_console_output(self, context, instance, tail_length=None):
3696         """Get console output for an instance."""
3697         return self.compute_rpcapi.get_console_output(context,
3698                 instance=instance, tail_length=tail_length)
3699 
3700     def lock(self, context, instance):
3701         """Lock the given instance."""
3702         # Only update the lock if we are an admin (non-owner)
3703         is_owner = instance.project_id == context.project_id
3704         if instance.locked and is_owner:
3705             return
3706 
3707         context = context.elevated()
3708         self._record_action_start(context, instance,
3709                                   instance_actions.LOCK)
3710 
3711         @wrap_instance_event(prefix='api')
3712         def lock(self, context, instance):
3713             LOG.debug('Locking', instance=instance)
3714             instance.locked = True
3715             instance.locked_by = 'owner' if is_owner else 'admin'
3716             instance.save()
3717 
3718         lock(self, context, instance)
3719 
3720     def is_expected_locked_by(self, context, instance):
3721         is_owner = instance.project_id == context.project_id
3722         expect_locked_by = 'owner' if is_owner else 'admin'
3723         locked_by = instance.locked_by
3724         if locked_by and locked_by != expect_locked_by:
3725             return False
3726         return True
3727 
3728     def unlock(self, context, instance):
3729         """Unlock the given instance."""
3730         context = context.elevated()
3731         self._record_action_start(context, instance,
3732                                   instance_actions.UNLOCK)
3733 
3734         @wrap_instance_event(prefix='api')
3735         def unlock(self, context, instance):
3736             LOG.debug('Unlocking', instance=instance)
3737             instance.locked = False
3738             instance.locked_by = None
3739             instance.save()
3740 
3741         unlock(self, context, instance)
3742 
3743     @check_instance_lock
3744     @check_instance_cell
3745     def reset_network(self, context, instance):
3746         """Reset networking on the instance."""
3747         self.compute_rpcapi.reset_network(context, instance=instance)
3748 
3749     @check_instance_lock
3750     @check_instance_cell
3751     def inject_network_info(self, context, instance):
3752         """Inject network info for the instance."""
3753         self.compute_rpcapi.inject_network_info(context, instance=instance)
3754 
3755     def _create_volume_bdm(self, context, instance, device, volume,
3756                            disk_bus, device_type, is_local_creation=False,
3757                            tag=None):
3758         volume_id = volume['id']
3759         if is_local_creation:
3760             # when the creation is done locally we can't specify the device
3761             # name as we do not have a way to check that the name specified is
3762             # a valid one.
3763             # We leave the setting of that value when the actual attach
3764             # happens on the compute manager
3765             # NOTE(artom) Local attach (to a shelved-offload instance) cannot
3766             # support device tagging because we have no way to call the compute
3767             # manager to check that it supports device tagging. In fact, we
3768             # don't even know which computer manager the instance will
3769             # eventually end up on when it's unshelved.
3770             volume_bdm = objects.BlockDeviceMapping(
3771                 context=context,
3772                 source_type='volume', destination_type='volume',
3773                 instance_uuid=instance.uuid, boot_index=None,
3774                 volume_id=volume_id,
3775                 device_name=None, guest_format=None,
3776                 disk_bus=disk_bus, device_type=device_type)
3777             volume_bdm.create()
3778         else:
3779             # NOTE(vish): This is done on the compute host because we want
3780             #             to avoid a race where two devices are requested at
3781             #             the same time. When db access is removed from
3782             #             compute, the bdm will be created here and we will
3783             #             have to make sure that they are assigned atomically.
3784             volume_bdm = self.compute_rpcapi.reserve_block_device_name(
3785                 context, instance, device, volume_id, disk_bus=disk_bus,
3786                 device_type=device_type, tag=tag,
3787                 multiattach=volume['multiattach'])
3788         return volume_bdm
3789 
3790     def _check_volume_already_attached_to_instance(self, context, instance,
3791                                                    volume_id):
3792         """Avoid attaching the same volume to the same instance twice.
3793 
3794            As the new Cinder flow (microversion 3.44) is handling the checks
3795            differently and allows to attach the same volume to the same
3796            instance twice to enable live_migrate we are checking whether the
3797            BDM already exists for this combination for the new flow and fail
3798            if it does.
3799         """
3800 
3801         try:
3802             objects.BlockDeviceMapping.get_by_volume_and_instance(
3803                 context, volume_id, instance.uuid)
3804 
3805             msg = _("volume %s already attached") % volume_id
3806             raise exception.InvalidVolume(reason=msg)
3807         except exception.VolumeBDMNotFound:
3808             pass
3809 
3810     def _check_attach_and_reserve_volume(self, context, volume, instance,
3811                                          bdm, supports_multiattach=False):
3812         volume_id = volume['id']
3813         self.volume_api.check_availability_zone(context, volume,
3814                                                 instance=instance)
3815         # If volume.multiattach=True and the microversion to
3816         # support multiattach is not used, fail the request.
3817         if volume['multiattach'] and not supports_multiattach:
3818             raise exception.MultiattachNotSupportedOldMicroversion()
3819 
3820         if 'id' in instance:
3821             # This is a volume attach to an existing instance, so
3822             # we only care about the cell the instance is in.
3823             min_compute_version = objects.Service.get_minimum_version(
3824                 context, 'nova-compute')
3825         else:
3826             # The instance is being created and we don't know which
3827             # cell it's going to land in, so check all cells.
3828             # NOTE(danms): We don't require all cells to report here since
3829             # we're really concerned about the new-ness of cells that the
3830             # instance may be scheduled into. If a cell doesn't respond here,
3831             # then it won't be a candidate for the instance and thus doesn't
3832             # matter.
3833             min_compute_version = \
3834                 objects.service.get_minimum_version_all_cells(
3835                     context, ['nova-compute'])
3836             # Check to see if the computes have been upgraded to support
3837             # booting from a multiattach volume.
3838             if (volume['multiattach'] and
3839                     min_compute_version < MIN_COMPUTE_MULTIATTACH):
3840                 raise exception.MultiattachSupportNotYetAvailable()
3841 
3842         if min_compute_version >= CINDER_V3_ATTACH_MIN_COMPUTE_VERSION:
3843             # Attempt a new style volume attachment, but fallback to old-style
3844             # in case Cinder API 3.44 isn't available.
3845             try:
3846                 attachment_id = self.volume_api.attachment_create(
3847                     context, volume_id, instance.uuid)['id']
3848                 bdm.attachment_id = attachment_id
3849                 # NOTE(ildikov): In case of boot from volume the BDM at this
3850                 # point is not yet created in a cell database, so we can't
3851                 # call save().  When attaching a volume to an existing
3852                 # instance, the instance is already in a cell and the BDM has
3853                 # been created in that same cell so updating here in that case
3854                 # is "ok".
3855                 if bdm.obj_attr_is_set('id'):
3856                     bdm.save()
3857             except exception.CinderAPIVersionNotAvailable:
3858                 LOG.debug('The available Cinder microversion is not high '
3859                           'enough to create new style volume attachment.')
3860                 self.volume_api.reserve_volume(context, volume_id)
3861         else:
3862             LOG.debug('The compute service version is not high enough to '
3863                       'create a new style volume attachment.')
3864             self.volume_api.reserve_volume(context, volume_id)
3865 
3866     def _attach_volume(self, context, instance, volume, device,
3867                        disk_bus, device_type, tag=None,
3868                        supports_multiattach=False):
3869         """Attach an existing volume to an existing instance.
3870 
3871         This method is separated to make it possible for cells version
3872         to override it.
3873         """
3874         volume_bdm = self._create_volume_bdm(
3875             context, instance, device, volume, disk_bus=disk_bus,
3876             device_type=device_type, tag=tag)
3877         try:
3878             self._check_attach_and_reserve_volume(context, volume, instance,
3879                                                   volume_bdm,
3880                                                   supports_multiattach)
3881             self._record_action_start(
3882                 context, instance, instance_actions.ATTACH_VOLUME)
3883             self.compute_rpcapi.attach_volume(context, instance, volume_bdm)
3884         except Exception:
3885             with excutils.save_and_reraise_exception():
3886                 volume_bdm.destroy()
3887 
3888         return volume_bdm.device_name
3889 
3890     def _attach_volume_shelved_offloaded(self, context, instance, volume,
3891                                          device, disk_bus, device_type):
3892         """Attach an existing volume to an instance in shelved offloaded state.
3893 
3894         Attaching a volume for an instance in shelved offloaded state requires
3895         to perform the regular check to see if we can attach and reserve the
3896         volume then we need to call the attach method on the volume API
3897         to mark the volume as 'in-use'.
3898         The instance at this stage is not managed by a compute manager
3899         therefore the actual attachment will be performed once the
3900         instance will be unshelved.
3901         """
3902         volume_id = volume['id']
3903 
3904         @wrap_instance_event(prefix='api')
3905         def attach_volume(self, context, v_id, instance, dev, attachment_id):
3906             if attachment_id:
3907                 # Normally we wouldn't complete an attachment without a host
3908                 # connector, but we do this to make the volume status change
3909                 # to "in-use" to maintain the API semantics with the old flow.
3910                 # When unshelving the instance, the compute service will deal
3911                 # with this disconnected attachment.
3912                 self.volume_api.attachment_complete(context, attachment_id)
3913             else:
3914                 self.volume_api.attach(context,
3915                                        v_id,
3916                                        instance.uuid,
3917                                        dev)
3918 
3919         volume_bdm = self._create_volume_bdm(
3920             context, instance, device, volume, disk_bus=disk_bus,
3921             device_type=device_type, is_local_creation=True)
3922         try:
3923             self._check_attach_and_reserve_volume(context, volume, instance,
3924                                                   volume_bdm)
3925             self._record_action_start(
3926                 context, instance,
3927                 instance_actions.ATTACH_VOLUME)
3928             attach_volume(self, context, volume_id, instance, device,
3929                           volume_bdm.attachment_id)
3930         except Exception:
3931             with excutils.save_and_reraise_exception():
3932                 volume_bdm.destroy()
3933 
3934         return volume_bdm.device_name
3935 
3936     @check_instance_lock
3937     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
3938                                     vm_states.STOPPED, vm_states.RESIZED,
3939                                     vm_states.SOFT_DELETED, vm_states.SHELVED,
3940                                     vm_states.SHELVED_OFFLOADED])
3941     def attach_volume(self, context, instance, volume_id, device=None,
3942                       disk_bus=None, device_type=None, tag=None,
3943                       supports_multiattach=False):
3944         """Attach an existing volume to an existing instance."""
3945         # NOTE(vish): Fail fast if the device is not going to pass. This
3946         #             will need to be removed along with the test if we
3947         #             change the logic in the manager for what constitutes
3948         #             a valid device.
3949         if device and not block_device.match_device(device):
3950             raise exception.InvalidDevicePath(path=device)
3951 
3952         # Check to see if the computes in this cell can support new-style
3953         # volume attachments.
3954         min_compute_version = objects.Service.get_minimum_version(
3955             context, 'nova-compute')
3956         if min_compute_version >= CINDER_V3_ATTACH_MIN_COMPUTE_VERSION:
3957             try:
3958                 # Check to see if Cinder is new enough to create new-style
3959                 # attachments.
3960                 cinder.is_microversion_supported(context, '3.44')
3961             except exception.CinderAPIVersionNotAvailable:
3962                 pass
3963             else:
3964                 # Make sure the volume isn't already attached to this instance
3965                 # because based on the above checks, we'll use the new style
3966                 # attachment flow in _check_attach_and_reserve_volume and
3967                 # Cinder will allow multiple attachments between the same
3968                 # volume and instance but the old flow API semantics don't
3969                 # allow that so we enforce it here.
3970                 self._check_volume_already_attached_to_instance(context,
3971                                                                 instance,
3972                                                                 volume_id)
3973 
3974         volume = self.volume_api.get(context, volume_id)
3975         is_shelved_offloaded = instance.vm_state == vm_states.SHELVED_OFFLOADED
3976         if is_shelved_offloaded:
3977             if tag:
3978                 # NOTE(artom) Local attach (to a shelved-offload instance)
3979                 # cannot support device tagging because we have no way to call
3980                 # the compute manager to check that it supports device tagging.
3981                 # In fact, we don't even know which computer manager the
3982                 # instance will eventually end up on when it's unshelved.
3983                 raise exception.VolumeTaggedAttachToShelvedNotSupported()
3984             if volume['multiattach']:
3985                 # NOTE(mriedem): Similar to tagged attach, we don't support
3986                 # attaching a multiattach volume to shelved offloaded instances
3987                 # because we can't tell if the compute host (since there isn't
3988                 # one) supports it. This could possibly be supported in the
3989                 # future if the scheduler was made aware of which computes
3990                 # support multiattach volumes.
3991                 raise exception.MultiattachToShelvedNotSupported()
3992             return self._attach_volume_shelved_offloaded(context,
3993                                                          instance,
3994                                                          volume,
3995                                                          device,
3996                                                          disk_bus,
3997                                                          device_type)
3998 
3999         return self._attach_volume(context, instance, volume, device,
4000                                    disk_bus, device_type, tag=tag,
4001                                    supports_multiattach=supports_multiattach)
4002 
4003     def _detach_volume(self, context, instance, volume):
4004         """Detach volume from instance.
4005 
4006         This method is separated to make it easier for cells version
4007         to override.
4008         """
4009         try:
4010             self.volume_api.begin_detaching(context, volume['id'])
4011         except exception.InvalidInput as exc:
4012             raise exception.InvalidVolume(reason=exc.format_message())
4013         attachments = volume.get('attachments', {})
4014         attachment_id = None
4015         if attachments and instance.uuid in attachments:
4016             attachment_id = attachments[instance.uuid]['attachment_id']
4017         self._record_action_start(
4018             context, instance, instance_actions.DETACH_VOLUME)
4019         self.compute_rpcapi.detach_volume(context, instance=instance,
4020                 volume_id=volume['id'], attachment_id=attachment_id)
4021 
4022     def _detach_volume_shelved_offloaded(self, context, instance, volume):
4023         """Detach a volume from an instance in shelved offloaded state.
4024 
4025         If the instance is shelved offloaded we just need to cleanup volume
4026         calling the volume api detach, the volume api terminate_connection
4027         and delete the bdm record.
4028         If the volume has delete_on_termination option set then we call the
4029         volume api delete as well.
4030         """
4031         @wrap_instance_event(prefix='api')
4032         def detach_volume(self, context, instance, bdms):
4033             self._local_cleanup_bdm_volumes(bdms, instance, context)
4034 
4035         try:
4036             self.volume_api.begin_detaching(context, volume['id'])
4037         except exception.InvalidInput as exc:
4038             raise exception.InvalidVolume(reason=exc.format_message())
4039         bdms = [objects.BlockDeviceMapping.get_by_volume_id(
4040                 context, volume['id'], instance.uuid)]
4041         self._record_action_start(
4042             context, instance,
4043             instance_actions.DETACH_VOLUME)
4044         detach_volume(self, context, instance, bdms)
4045 
4046     @check_instance_lock
4047     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
4048                                     vm_states.STOPPED, vm_states.RESIZED,
4049                                     vm_states.SOFT_DELETED, vm_states.SHELVED,
4050                                     vm_states.SHELVED_OFFLOADED])
4051     def detach_volume(self, context, instance, volume):
4052         """Detach a volume from an instance."""
4053         if instance.vm_state == vm_states.SHELVED_OFFLOADED:
4054             self._detach_volume_shelved_offloaded(context, instance, volume)
4055         else:
4056             self._detach_volume(context, instance, volume)
4057 
4058     def _count_attachments_for_swap(self, ctxt, volume):
4059         """Counts the number of attachments for a swap-related volume.
4060 
4061         Attempts to only count read/write attachments if the volume attachment
4062         records exist, otherwise simply just counts the number of attachments
4063         regardless of attach mode.
4064 
4065         :param ctxt: nova.context.RequestContext - user request context
4066         :param volume: nova-translated volume dict from nova.volume.cinder.
4067         :returns: count of attachments for the volume
4068         """
4069         # This is a dict, keyed by server ID, to a dict of attachment_id and
4070         # mountpoint.
4071         attachments = volume.get('attachments', {})
4072         # Multiattach volumes can have more than one attachment, so if there
4073         # is more than one attachment, attempt to count the read/write
4074         # attachments.
4075         if len(attachments) > 1:
4076             count = 0
4077             for attachment in attachments.values():
4078                 attachment_id = attachment['attachment_id']
4079                 # Get the attachment record for this attachment so we can
4080                 # get the attach_mode.
4081                 # TODO(mriedem): This could be optimized if we had
4082                 # GET /attachments/detail?volume_id=volume['id'] in Cinder.
4083                 try:
4084                     attachment_record = self.volume_api.attachment_get(
4085                         ctxt, attachment_id)
4086                     # Note that the attachment record from Cinder has
4087                     # attach_mode in the top-level of the resource but the
4088                     # nova.volume.cinder code translates it and puts the
4089                     # attach_mode in the connection_info for some legacy
4090                     # reason...
4091                     if attachment_record.get(
4092                             'connection_info', {}).get(
4093                                 # attachments are read/write by default
4094                                 'attach_mode', 'rw') == 'rw':
4095                         count += 1
4096                 except exception.VolumeAttachmentNotFound:
4097                     # attachments are read/write by default so count it
4098                     count += 1
4099         else:
4100             count = len(attachments)
4101 
4102         return count
4103 
4104     @check_instance_lock
4105     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
4106                                     vm_states.RESIZED])
4107     def swap_volume(self, context, instance, old_volume, new_volume):
4108         """Swap volume attached to an instance."""
4109         # The caller likely got the instance from volume['attachments']
4110         # in the first place, but let's sanity check.
4111         if not old_volume.get('attachments', {}).get(instance.uuid):
4112             msg = _("Old volume is attached to a different instance.")
4113             raise exception.InvalidVolume(reason=msg)
4114         if new_volume['attach_status'] == 'attached':
4115             msg = _("New volume must be detached in order to swap.")
4116             raise exception.InvalidVolume(reason=msg)
4117         if int(new_volume['size']) < int(old_volume['size']):
4118             msg = _("New volume must be the same size or larger.")
4119             raise exception.InvalidVolume(reason=msg)
4120         self.volume_api.check_availability_zone(context, new_volume,
4121                                                 instance=instance)
4122 
4123         # Disallow swapping from multiattach volumes that have more than one
4124         # read/write attachment. We know the old_volume has at least one
4125         # attachment since it's attached to this server. The new_volume
4126         # can't have any attachments because of the attach_status check above.
4127         if self._count_attachments_for_swap(context, old_volume) > 1:
4128             raise exception.MultiattachSwapVolumeNotSupported()
4129 
4130         try:
4131             self.volume_api.begin_detaching(context, old_volume['id'])
4132         except exception.InvalidInput as exc:
4133             raise exception.InvalidVolume(reason=exc.format_message())
4134 
4135         # Get the BDM for the attached (old) volume so we can tell if it was
4136         # attached with the new-style Cinder 3.44 API.
4137         bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
4138             context, old_volume['id'], instance.uuid)
4139         new_attachment_id = None
4140         if bdm.attachment_id is None:
4141             # This is an old-style attachment so reserve the new volume before
4142             # we cast to the compute host.
4143             self.volume_api.reserve_volume(context, new_volume['id'])
4144         else:
4145             try:
4146                 self._check_volume_already_attached_to_instance(
4147                     context, instance, new_volume['id'])
4148             except exception.InvalidVolume:
4149                 with excutils.save_and_reraise_exception():
4150                     self.volume_api.roll_detaching(context, old_volume['id'])
4151 
4152             # This is a new-style attachment so for the volume that we are
4153             # going to swap to, create a new volume attachment.
4154             new_attachment_id = self.volume_api.attachment_create(
4155                 context, new_volume['id'], instance.uuid)['id']
4156 
4157         self._record_action_start(
4158             context, instance, instance_actions.SWAP_VOLUME)
4159 
4160         try:
4161             self.compute_rpcapi.swap_volume(
4162                     context, instance=instance,
4163                     old_volume_id=old_volume['id'],
4164                     new_volume_id=new_volume['id'],
4165                     new_attachment_id=new_attachment_id)
4166         except Exception:
4167             with excutils.save_and_reraise_exception():
4168                 self.volume_api.roll_detaching(context, old_volume['id'])
4169                 if new_attachment_id is None:
4170                     self.volume_api.unreserve_volume(context, new_volume['id'])
4171                 else:
4172                     self.volume_api.attachment_delete(
4173                         context, new_attachment_id)
4174 
4175     @check_instance_lock
4176     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
4177                                     vm_states.STOPPED],
4178                           task_state=[None])
4179     def attach_interface(self, context, instance, network_id, port_id,
4180                          requested_ip, tag=None):
4181         """Use hotplug to add an network adapter to an instance."""
4182         self._record_action_start(
4183             context, instance, instance_actions.ATTACH_INTERFACE)
4184         return self.compute_rpcapi.attach_interface(context,
4185             instance=instance, network_id=network_id, port_id=port_id,
4186             requested_ip=requested_ip, tag=tag)
4187 
4188     @check_instance_lock
4189     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
4190                                     vm_states.STOPPED],
4191                           task_state=[None])
4192     def detach_interface(self, context, instance, port_id):
4193         """Detach an network adapter from an instance."""
4194         self._record_action_start(
4195             context, instance, instance_actions.DETACH_INTERFACE)
4196         self.compute_rpcapi.detach_interface(context, instance=instance,
4197             port_id=port_id)
4198 
4199     def get_instance_metadata(self, context, instance):
4200         """Get all metadata associated with an instance."""
4201         return self.db.instance_metadata_get(context, instance.uuid)
4202 
4203     @check_instance_lock
4204     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
4205                                     vm_states.SUSPENDED, vm_states.STOPPED],
4206                           task_state=None)
4207     def delete_instance_metadata(self, context, instance, key):
4208         """Delete the given metadata item from an instance."""
4209         instance.delete_metadata_key(key)
4210         self.compute_rpcapi.change_instance_metadata(context,
4211                                                      instance=instance,
4212                                                      diff={key: ['-']})
4213 
4214     @check_instance_lock
4215     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
4216                                     vm_states.SUSPENDED, vm_states.STOPPED],
4217                           task_state=None)
4218     def update_instance_metadata(self, context, instance,
4219                                  metadata, delete=False):
4220         """Updates or creates instance metadata.
4221 
4222         If delete is True, metadata items that are not specified in the
4223         `metadata` argument will be deleted.
4224 
4225         """
4226         orig = dict(instance.metadata)
4227         if delete:
4228             _metadata = metadata
4229         else:
4230             _metadata = dict(instance.metadata)
4231             _metadata.update(metadata)
4232 
4233         self._check_metadata_properties_quota(context, _metadata)
4234         instance.metadata = _metadata
4235         instance.save()
4236         diff = _diff_dict(orig, instance.metadata)
4237         self.compute_rpcapi.change_instance_metadata(context,
4238                                                      instance=instance,
4239                                                      diff=diff)
4240         return _metadata
4241 
4242     @check_instance_lock
4243     @check_instance_cell
4244     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED])
4245     def live_migrate(self, context, instance, block_migration,
4246                      disk_over_commit, host_name, force=None, async=False):
4247         """Migrate a server lively to a new host."""
4248         LOG.debug("Going to try to live migrate instance to %s",
4249                   host_name or "another host", instance=instance)
4250 
4251         instance.task_state = task_states.MIGRATING
4252         instance.save(expected_task_state=[None])
4253 
4254         self._record_action_start(context, instance,
4255                                   instance_actions.LIVE_MIGRATION)
4256 
4257         # TODO(melwitt): In Rocky, we store console authorizations
4258         # in both the consoleauth service and the database while
4259         # we convert to using the database. Remove the consoleauth
4260         # line below when authorizations are no longer being
4261         # stored in consoleauth, in Stein.
4262         self.consoleauth_rpcapi.delete_tokens_for_instance(
4263             context, instance.uuid)
4264 
4265         try:
4266             request_spec = objects.RequestSpec.get_by_instance_uuid(
4267                 context, instance.uuid)
4268         except exception.RequestSpecNotFound:
4269             # Some old instances can still have no RequestSpec object attached
4270             # to them, we need to support the old way
4271             request_spec = None
4272 
4273         # NOTE(sbauza): Force is a boolean by the new related API version
4274         if force is False and host_name:
4275             nodes = objects.ComputeNodeList.get_all_by_host(context, host_name)
4276             # Unset the host to make sure we call the scheduler
4277             # from the conductor LiveMigrationTask. Yes this is tightly-coupled
4278             # to behavior in conductor and not great.
4279             host_name = None
4280             # FIXME(sbauza): Since only Ironic driver uses more than one
4281             # compute per service but doesn't support live migrations,
4282             # let's provide the first one.
4283             target = nodes[0]
4284             if request_spec:
4285                 # TODO(sbauza): Hydrate a fake spec for old instances not yet
4286                 # having a request spec attached to them (particularly true for
4287                 # cells v1). For the moment, let's keep the same behaviour for
4288                 # all the instances but provide the destination only if a spec
4289                 # is found.
4290                 destination = objects.Destination(
4291                     host=target.host,
4292                     node=target.hypervisor_hostname
4293                 )
4294                 # This is essentially a hint to the scheduler to only consider
4295                 # the specified host but still run it through the filters.
4296                 request_spec.requested_destination = destination
4297 
4298         try:
4299             self.compute_task_api.live_migrate_instance(context, instance,
4300                 host_name, block_migration=block_migration,
4301                 disk_over_commit=disk_over_commit,
4302                 request_spec=request_spec, async=async)
4303         except oslo_exceptions.MessagingTimeout as messaging_timeout:
4304             with excutils.save_and_reraise_exception():
4305                 # NOTE(pkoniszewski): It is possible that MessagingTimeout
4306                 # occurs, but LM will still be in progress, so write
4307                 # instance fault to database
4308                 compute_utils.add_instance_fault_from_exc(context,
4309                                                           instance,
4310                                                           messaging_timeout)
4311 
4312     @check_instance_lock
4313     @check_instance_cell
4314     @check_instance_state(vm_state=[vm_states.ACTIVE],
4315                           task_state=[task_states.MIGRATING])
4316     def live_migrate_force_complete(self, context, instance, migration_id):
4317         """Force live migration to complete.
4318 
4319         :param context: Security context
4320         :param instance: The instance that is being migrated
4321         :param migration_id: ID of ongoing migration
4322 
4323         """
4324         LOG.debug("Going to try to force live migration to complete",
4325                   instance=instance)
4326 
4327         # NOTE(pkoniszewski): Get migration object to check if there is ongoing
4328         # live migration for particular instance. Also pass migration id to
4329         # compute to double check and avoid possible race condition.
4330         migration = objects.Migration.get_by_id_and_instance(
4331             context, migration_id, instance.uuid)
4332         if migration.status != 'running':
4333             raise exception.InvalidMigrationState(migration_id=migration_id,
4334                                                   instance_uuid=instance.uuid,
4335                                                   state=migration.status,
4336                                                   method='force complete')
4337 
4338         self._record_action_start(
4339             context, instance, instance_actions.LIVE_MIGRATION_FORCE_COMPLETE)
4340 
4341         self.compute_rpcapi.live_migration_force_complete(
4342             context, instance, migration)
4343 
4344     @check_instance_lock
4345     @check_instance_cell
4346     @check_instance_state(task_state=[task_states.MIGRATING])
4347     def live_migrate_abort(self, context, instance, migration_id):
4348         """Abort an in-progress live migration.
4349 
4350         :param context: Security context
4351         :param instance: The instance that is being migrated
4352         :param migration_id: ID of in-progress live migration
4353 
4354         """
4355         migration = objects.Migration.get_by_id_and_instance(context,
4356                     migration_id, instance.uuid)
4357         LOG.debug("Going to cancel live migration %s",
4358                   migration.id, instance=instance)
4359 
4360         if migration.status != 'running':
4361             raise exception.InvalidMigrationState(migration_id=migration_id,
4362                     instance_uuid=instance.uuid,
4363                     state=migration.status,
4364                     method='abort live migration')
4365         self._record_action_start(context, instance,
4366                                   instance_actions.LIVE_MIGRATION_CANCEL)
4367 
4368         self.compute_rpcapi.live_migration_abort(context,
4369                 instance, migration.id)
4370 
4371     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
4372                                     vm_states.ERROR])
4373     def evacuate(self, context, instance, host, on_shared_storage,
4374                  admin_password=None, force=None):
4375         """Running evacuate to target host.
4376 
4377         Checking vm compute host state, if the host not in expected_state,
4378         raising an exception.
4379 
4380         :param instance: The instance to evacuate
4381         :param host: Target host. if not set, the scheduler will pick up one
4382         :param on_shared_storage: True if instance files on shared storage
4383         :param admin_password: password to set on rebuilt instance
4384         :param force: Force the evacuation to the specific host target
4385 
4386         """
4387         LOG.debug('vm evacuation scheduled', instance=instance)
4388         inst_host = instance.host
4389         service = objects.Service.get_by_compute_host(context, inst_host)
4390         if self.servicegroup_api.service_is_up(service):
4391             LOG.error('Instance compute service state on %s '
4392                       'expected to be down, but it was up.', inst_host)
4393             raise exception.ComputeServiceInUse(host=inst_host)
4394 
4395         instance.task_state = task_states.REBUILDING
4396         instance.save(expected_task_state=[None])
4397         self._record_action_start(context, instance, instance_actions.EVACUATE)
4398 
4399         # NOTE(danms): Create this as a tombstone for the source compute
4400         # to find and cleanup. No need to pass it anywhere else.
4401         migration = objects.Migration(context,
4402                                       source_compute=instance.host,
4403                                       source_node=instance.node,
4404                                       instance_uuid=instance.uuid,
4405                                       status='accepted',
4406                                       migration_type='evacuation')
4407         if host:
4408             migration.dest_compute = host
4409         migration.create()
4410 
4411         compute_utils.notify_about_instance_usage(
4412             self.notifier, context, instance, "evacuate")
4413         compute_utils.notify_about_instance_action(
4414             context, instance, CONF.host,
4415             action=fields_obj.NotificationAction.EVACUATE,
4416             source=fields_obj.NotificationSource.API)
4417 
4418         try:
4419             request_spec = objects.RequestSpec.get_by_instance_uuid(
4420                 context, instance.uuid)
4421         except exception.RequestSpecNotFound:
4422             # Some old instances can still have no RequestSpec object attached
4423             # to them, we need to support the old way
4424             request_spec = None
4425 
4426         # NOTE(sbauza): Force is a boolean by the new related API version
4427         if force is False and host:
4428             nodes = objects.ComputeNodeList.get_all_by_host(context, host)
4429             # NOTE(sbauza): Unset the host to make sure we call the scheduler
4430             host = None
4431             # FIXME(sbauza): Since only Ironic driver uses more than one
4432             # compute per service but doesn't support evacuations,
4433             # let's provide the first one.
4434             target = nodes[0]
4435             if request_spec:
4436                 # TODO(sbauza): Hydrate a fake spec for old instances not yet
4437                 # having a request spec attached to them (particularly true for
4438                 # cells v1). For the moment, let's keep the same behaviour for
4439                 # all the instances but provide the destination only if a spec
4440                 # is found.
4441                 destination = objects.Destination(
4442                     host=target.host,
4443                     node=target.hypervisor_hostname
4444                 )
4445                 request_spec.requested_destination = destination
4446 
4447         return self.compute_task_api.rebuild_instance(context,
4448                        instance=instance,
4449                        new_pass=admin_password,
4450                        injected_files=None,
4451                        image_ref=None,
4452                        orig_image_ref=None,
4453                        orig_sys_metadata=None,
4454                        bdms=None,
4455                        recreate=True,
4456                        on_shared_storage=on_shared_storage,
4457                        host=host,
4458                        request_spec=request_spec,
4459                        )
4460 
4461     def get_migrations(self, context, filters):
4462         """Get all migrations for the given filters."""
4463         load_cells()
4464 
4465         migrations = []
4466         for cell in CELLS:
4467             if cell.uuid == objects.CellMapping.CELL0_UUID:
4468                 continue
4469             with nova_context.target_cell(context, cell) as cctxt:
4470                 migrations.extend(objects.MigrationList.get_by_filters(
4471                     cctxt, filters).objects)
4472         return objects.MigrationList(objects=migrations)
4473 
4474     def get_migrations_sorted(self, context, filters, sort_dirs=None,
4475                               sort_keys=None, limit=None, marker=None):
4476         """Get all migrations for the given parameters."""
4477         mig_objs = migration_list.get_migration_objects_sorted(
4478             context, filters, limit, marker, sort_keys, sort_dirs)
4479         return mig_objs
4480 
4481     def get_migrations_in_progress_by_instance(self, context, instance_uuid,
4482                                                migration_type=None):
4483         """Get all migrations of an instance in progress."""
4484         return objects.MigrationList.get_in_progress_by_instance(
4485                 context, instance_uuid, migration_type)
4486 
4487     def get_migration_by_id_and_instance(self, context,
4488                                          migration_id, instance_uuid):
4489         """Get the migration of an instance by id."""
4490         return objects.Migration.get_by_id_and_instance(
4491                 context, migration_id, instance_uuid)
4492 
4493     def _get_bdm_by_volume_id(self, context, volume_id, expected_attrs=None):
4494         """Retrieve a BDM without knowing its cell.
4495 
4496         .. note:: The context will be targeted to the cell in which the
4497             BDM is found, if any.
4498 
4499         :param context: The API request context.
4500         :param volume_id: The ID of the volume.
4501         :param expected_attrs: list of any additional attributes that should
4502             be joined when the BDM is loaded from the database.
4503         :raises: nova.exception.VolumeBDMNotFound if not found in any cell
4504         """
4505         load_cells()
4506         for cell in CELLS:
4507             nova_context.set_target_cell(context, cell)
4508             try:
4509                 return objects.BlockDeviceMapping.get_by_volume(
4510                     context, volume_id, expected_attrs=expected_attrs)
4511             except exception.NotFound:
4512                 continue
4513         raise exception.VolumeBDMNotFound(volume_id=volume_id)
4514 
4515     def volume_snapshot_create(self, context, volume_id, create_info):
4516         bdm = self._get_bdm_by_volume_id(
4517             context, volume_id, expected_attrs=['instance'])
4518 
4519         # We allow creating the snapshot in any vm_state as long as there is
4520         # no task being performed on the instance and it has a host.
4521         @check_instance_host
4522         @check_instance_state(vm_state=None)
4523         def do_volume_snapshot_create(self, context, instance):
4524             self.compute_rpcapi.volume_snapshot_create(context, instance,
4525                     volume_id, create_info)
4526             snapshot = {
4527                 'snapshot': {
4528                     'id': create_info.get('id'),
4529                     'volumeId': volume_id
4530                 }
4531             }
4532             return snapshot
4533 
4534         return do_volume_snapshot_create(self, context, bdm.instance)
4535 
4536     def volume_snapshot_delete(self, context, volume_id, snapshot_id,
4537                                delete_info):
4538         bdm = self._get_bdm_by_volume_id(
4539             context, volume_id, expected_attrs=['instance'])
4540 
4541         # We allow deleting the snapshot in any vm_state as long as there is
4542         # no task being performed on the instance and it has a host.
4543         @check_instance_host
4544         @check_instance_state(vm_state=None)
4545         def do_volume_snapshot_delete(self, context, instance):
4546             self.compute_rpcapi.volume_snapshot_delete(context, instance,
4547                     volume_id, snapshot_id, delete_info)
4548 
4549         do_volume_snapshot_delete(self, context, bdm.instance)
4550 
4551     def external_instance_event(self, api_context, instances, events):
4552         # NOTE(danms): The external API consumer just provides events,
4553         # but doesn't know where they go. We need to collate lists
4554         # by the host the affected instance is on and dispatch them
4555         # according to host
4556         instances_by_host = collections.defaultdict(list)
4557         events_by_host = collections.defaultdict(list)
4558         hosts_by_instance = collections.defaultdict(list)
4559         cell_contexts_by_host = {}
4560         for instance in instances:
4561             # instance._context is used here since it's already targeted to
4562             # the cell that the instance lives in, and we need to use that
4563             # cell context to lookup any migrations associated to the instance.
4564             for host in self._get_relevant_hosts(instance._context, instance):
4565                 # NOTE(danms): All instances on a host must have the same
4566                 # mapping, so just use that
4567                 # NOTE(mdbooth): We don't currently support migrations between
4568                 # cells, and given that the Migration record is hosted in the
4569                 # cell _get_relevant_hosts will likely have to change before we
4570                 # do. Consequently we can currently assume that the context for
4571                 # both the source and destination hosts of a migration is the
4572                 # same.
4573                 if host not in cell_contexts_by_host:
4574                     cell_contexts_by_host[host] = instance._context
4575 
4576                 instances_by_host[host].append(instance)
4577                 hosts_by_instance[instance.uuid].append(host)
4578 
4579         for event in events:
4580             if event.name == 'volume-extended':
4581                 # Volume extend is a user-initiated operation starting in the
4582                 # Block Storage service API. We record an instance action so
4583                 # the user can monitor the operation to completion.
4584                 host = hosts_by_instance[event.instance_uuid][0]
4585                 cell_context = cell_contexts_by_host[host]
4586                 objects.InstanceAction.action_start(
4587                     cell_context, event.instance_uuid,
4588                     instance_actions.EXTEND_VOLUME, want_result=False)
4589             for host in hosts_by_instance[event.instance_uuid]:
4590                 events_by_host[host].append(event)
4591 
4592         for host in instances_by_host:
4593             cell_context = cell_contexts_by_host[host]
4594 
4595             # TODO(salv-orlando): Handle exceptions raised by the rpc api layer
4596             # in order to ensure that a failure in processing events on a host
4597             # will not prevent processing events on other hosts
4598             self.compute_rpcapi.external_instance_event(
4599                 cell_context, instances_by_host[host], events_by_host[host],
4600                 host=host)
4601 
4602     def _get_relevant_hosts(self, context, instance):
4603         hosts = set()
4604         hosts.add(instance.host)
4605         if instance.migration_context is not None:
4606             migration_id = instance.migration_context.migration_id
4607             migration = objects.Migration.get_by_id(context, migration_id)
4608             hosts.add(migration.dest_compute)
4609             hosts.add(migration.source_compute)
4610             LOG.debug('Instance %(instance)s is migrating, '
4611                       'copying events to all relevant hosts: '
4612                       '%(hosts)s', {'instance': instance.uuid,
4613                                     'hosts': hosts})
4614         return hosts
4615 
4616     def get_instance_host_status(self, instance):
4617         if instance.host:
4618             try:
4619                 service = [service for service in instance.services if
4620                            service.binary == 'nova-compute'][0]
4621                 if service.forced_down:
4622                     host_status = fields_obj.HostStatus.DOWN
4623                 elif service.disabled:
4624                     host_status = fields_obj.HostStatus.MAINTENANCE
4625                 else:
4626                     alive = self.servicegroup_api.service_is_up(service)
4627                     host_status = ((alive and fields_obj.HostStatus.UP) or
4628                                    fields_obj.HostStatus.UNKNOWN)
4629             except IndexError:
4630                 host_status = fields_obj.HostStatus.NONE
4631         else:
4632             host_status = fields_obj.HostStatus.NONE
4633         return host_status
4634 
4635     def get_instances_host_statuses(self, instance_list):
4636         host_status_dict = dict()
4637         host_statuses = dict()
4638         for instance in instance_list:
4639             if instance.host:
4640                 if instance.host not in host_status_dict:
4641                     host_status = self.get_instance_host_status(instance)
4642                     host_status_dict[instance.host] = host_status
4643                 else:
4644                     host_status = host_status_dict[instance.host]
4645             else:
4646                 host_status = fields_obj.HostStatus.NONE
4647             host_statuses[instance.uuid] = host_status
4648         return host_statuses
4649 
4650 
4651 def target_host_cell(fn):
4652     """Target a host-based function to a cell.
4653 
4654     Expects to wrap a function of signature:
4655 
4656        func(self, context, host, ...)
4657     """
4658 
4659     @functools.wraps(fn)
4660     def targeted(self, context, host, *args, **kwargs):
4661         mapping = objects.HostMapping.get_by_host(context, host)
4662         nova_context.set_target_cell(context, mapping.cell_mapping)
4663         return fn(self, context, host, *args, **kwargs)
4664     return targeted
4665 
4666 
4667 def _find_service_in_cell(context, service_id=None, service_host=None):
4668     """Find a service by id or hostname by searching all cells.
4669 
4670     If one matching service is found, return it. If none or multiple
4671     are found, raise an exception.
4672 
4673     :param context: A context.RequestContext
4674     :param service_id: If not none, the DB ID of the service to find
4675     :param service_host: If not None, the hostname of the service to find
4676     :returns: An objects.Service
4677     :raises: ServiceNotUnique if multiple matching IDs are found
4678     :raises: NotFound if no matches are found
4679     :raises: NovaException if called with neither search option
4680     """
4681 
4682     load_cells()
4683     service = None
4684     found_in_cell = None
4685 
4686     is_uuid = False
4687     if service_id is not None:
4688         is_uuid = uuidutils.is_uuid_like(service_id)
4689         if is_uuid:
4690             lookup_fn = lambda c: objects.Service.get_by_uuid(c, service_id)
4691         else:
4692             lookup_fn = lambda c: objects.Service.get_by_id(c, service_id)
4693     elif service_host is not None:
4694         lookup_fn = lambda c: (
4695             objects.Service.get_by_compute_host(c, service_host))
4696     else:
4697         LOG.exception('_find_service_in_cell called with no search parameters')
4698         # This is intentionally cryptic so we don't leak implementation details
4699         # out of the API.
4700         raise exception.NovaException()
4701 
4702     for cell in CELLS:
4703         # NOTE(danms): Services can be in cell0, so don't skip it here
4704         try:
4705             with nova_context.target_cell(context, cell) as cctxt:
4706                 cell_service = lookup_fn(cctxt)
4707         except exception.NotFound:
4708             # NOTE(danms): Keep looking in other cells
4709             continue
4710         if service and cell_service:
4711             raise exception.ServiceNotUnique()
4712         service = cell_service
4713         found_in_cell = cell
4714         if service and is_uuid:
4715             break
4716 
4717     if service:
4718         # NOTE(danms): Set the cell on the context so it remains
4719         # when we return to our caller
4720         nova_context.set_target_cell(context, found_in_cell)
4721         return service
4722     else:
4723         raise exception.NotFound()
4724 
4725 
4726 class HostAPI(base.Base):
4727     """Sub-set of the Compute Manager API for managing host operations."""
4728 
4729     def __init__(self, rpcapi=None):
4730         self.rpcapi = rpcapi or compute_rpcapi.ComputeAPI()
4731         self.servicegroup_api = servicegroup.API()
4732         super(HostAPI, self).__init__()
4733 
4734     def _assert_host_exists(self, context, host_name, must_be_up=False):
4735         """Raise HostNotFound if compute host doesn't exist."""
4736         service = objects.Service.get_by_compute_host(context, host_name)
4737         if not service:
4738             raise exception.HostNotFound(host=host_name)
4739         if must_be_up and not self.servicegroup_api.service_is_up(service):
4740             raise exception.ComputeServiceUnavailable(host=host_name)
4741         return service['host']
4742 
4743     @wrap_exception()
4744     @target_host_cell
4745     def set_host_enabled(self, context, host_name, enabled):
4746         """Sets the specified host's ability to accept new instances."""
4747         host_name = self._assert_host_exists(context, host_name)
4748         payload = {'host_name': host_name, 'enabled': enabled}
4749         compute_utils.notify_about_host_update(context,
4750                                                'set_enabled.start',
4751                                                payload)
4752         result = self.rpcapi.set_host_enabled(context, enabled=enabled,
4753                 host=host_name)
4754         compute_utils.notify_about_host_update(context,
4755                                                'set_enabled.end',
4756                                                payload)
4757         return result
4758 
4759     @target_host_cell
4760     def get_host_uptime(self, context, host_name):
4761         """Returns the result of calling "uptime" on the target host."""
4762         host_name = self._assert_host_exists(context, host_name,
4763                          must_be_up=True)
4764         return self.rpcapi.get_host_uptime(context, host=host_name)
4765 
4766     @wrap_exception()
4767     @target_host_cell
4768     def host_power_action(self, context, host_name, action):
4769         """Reboots, shuts down or powers up the host."""
4770         host_name = self._assert_host_exists(context, host_name)
4771         payload = {'host_name': host_name, 'action': action}
4772         compute_utils.notify_about_host_update(context,
4773                                                'power_action.start',
4774                                                payload)
4775         result = self.rpcapi.host_power_action(context, action=action,
4776                 host=host_name)
4777         compute_utils.notify_about_host_update(context,
4778                                                'power_action.end',
4779                                                payload)
4780         return result
4781 
4782     @wrap_exception()
4783     @target_host_cell
4784     def set_host_maintenance(self, context, host_name, mode):
4785         """Start/Stop host maintenance window. On start, it triggers
4786         guest VMs evacuation.
4787         """
4788         host_name = self._assert_host_exists(context, host_name)
4789         payload = {'host_name': host_name, 'mode': mode}
4790         compute_utils.notify_about_host_update(context,
4791                                                'set_maintenance.start',
4792                                                payload)
4793         result = self.rpcapi.host_maintenance_mode(context,
4794                 host_param=host_name, mode=mode, host=host_name)
4795         compute_utils.notify_about_host_update(context,
4796                                                'set_maintenance.end',
4797                                                payload)
4798         return result
4799 
4800     def service_get_all(self, context, filters=None, set_zones=False,
4801                         all_cells=False):
4802         """Returns a list of services, optionally filtering the results.
4803 
4804         If specified, 'filters' should be a dictionary containing services
4805         attributes and matching values.  Ie, to get a list of services for
4806         the 'compute' topic, use filters={'topic': 'compute'}.
4807 
4808         If all_cells=True, then scan all cells and merge the results.
4809         """
4810         if filters is None:
4811             filters = {}
4812         disabled = filters.pop('disabled', None)
4813         if 'availability_zone' in filters:
4814             set_zones = True
4815 
4816         # NOTE(danms): Eventually this all_cells nonsense should go away
4817         # and we should always iterate over the cells. However, certain
4818         # callers need the legacy behavior for now.
4819         if all_cells:
4820             services = []
4821             service_dict = nova_context.scatter_gather_all_cells(context,
4822                 objects.ServiceList.get_all, disabled, set_zones=set_zones)
4823             for service in service_dict.values():
4824                 if service not in (nova_context.did_not_respond_sentinel,
4825                                    nova_context.raised_exception_sentinel):
4826                     services.extend(service)
4827         else:
4828             services = objects.ServiceList.get_all(context, disabled,
4829                                                    set_zones=set_zones)
4830         ret_services = []
4831         for service in services:
4832             for key, val in filters.items():
4833                 if service[key] != val:
4834                     break
4835             else:
4836                 # All filters matched.
4837                 ret_services.append(service)
4838         return ret_services
4839 
4840     def service_get_by_id(self, context, service_id):
4841         """Get service entry for the given service id or uuid."""
4842         try:
4843             return _find_service_in_cell(context, service_id=service_id)
4844         except exception.NotFound:
4845             raise exception.ServiceNotFound(service_id=service_id)
4846 
4847     @target_host_cell
4848     def service_get_by_compute_host(self, context, host_name):
4849         """Get service entry for the given compute hostname."""
4850         return objects.Service.get_by_compute_host(context, host_name)
4851 
4852     def _service_update(self, context, host_name, binary, params_to_update):
4853         """Performs the actual service update operation."""
4854         service = objects.Service.get_by_args(context, host_name, binary)
4855         service.update(params_to_update)
4856         service.save()
4857         return service
4858 
4859     @target_host_cell
4860     def service_update(self, context, host_name, binary, params_to_update):
4861         """Enable / Disable a service.
4862 
4863         For compute services, this stops new builds and migrations going to
4864         the host.
4865         """
4866         return self._service_update(context, host_name, binary,
4867                                     params_to_update)
4868 
4869     def _service_delete(self, context, service_id):
4870         """Performs the actual Service deletion operation."""
4871         try:
4872             service = _find_service_in_cell(context, service_id=service_id)
4873         except exception.NotFound:
4874             raise exception.ServiceNotFound(service_id=service_id)
4875         service.destroy()
4876 
4877     def service_delete(self, context, service_id):
4878         """Deletes the specified service found via id or uuid."""
4879         self._service_delete(context, service_id)
4880 
4881     @target_host_cell
4882     def instance_get_all_by_host(self, context, host_name):
4883         """Return all instances on the given host."""
4884         return objects.InstanceList.get_by_host(context, host_name)
4885 
4886     def task_log_get_all(self, context, task_name, period_beginning,
4887                          period_ending, host=None, state=None):
4888         """Return the task logs within a given range, optionally
4889         filtering by host and/or state.
4890         """
4891         return self.db.task_log_get_all(context, task_name,
4892                                         period_beginning,
4893                                         period_ending,
4894                                         host=host,
4895                                         state=state)
4896 
4897     def compute_node_get(self, context, compute_id):
4898         """Return compute node entry for particular integer ID or UUID."""
4899         load_cells()
4900 
4901         # NOTE(danms): Unfortunately this API exposes database identifiers
4902         # which means we really can't do something efficient here
4903         is_uuid = uuidutils.is_uuid_like(compute_id)
4904         for cell in CELLS:
4905             if cell.uuid == objects.CellMapping.CELL0_UUID:
4906                 continue
4907             with nova_context.target_cell(context, cell) as cctxt:
4908                 try:
4909                     if is_uuid:
4910                         return objects.ComputeNode.get_by_uuid(cctxt,
4911                                                                compute_id)
4912                     return objects.ComputeNode.get_by_id(cctxt,
4913                                                          int(compute_id))
4914                 except exception.ComputeHostNotFound:
4915                     # NOTE(danms): Keep looking in other cells
4916                     continue
4917 
4918         raise exception.ComputeHostNotFound(host=compute_id)
4919 
4920     def compute_node_get_all(self, context, limit=None, marker=None):
4921         load_cells()
4922 
4923         computes = []
4924         uuid_marker = marker and uuidutils.is_uuid_like(marker)
4925         for cell in CELLS:
4926             if cell.uuid == objects.CellMapping.CELL0_UUID:
4927                 continue
4928             with nova_context.target_cell(context, cell) as cctxt:
4929 
4930                 # If we have a marker and it's a uuid, see if the compute node
4931                 # is in this cell.
4932                 if marker and uuid_marker:
4933                     try:
4934                         compute_marker = objects.ComputeNode.get_by_uuid(
4935                             cctxt, marker)
4936                         # we found the marker compute node, so use it's id
4937                         # for the actual marker for paging in this cell's db
4938                         marker = compute_marker.id
4939                     except exception.ComputeHostNotFound:
4940                         # The marker node isn't in this cell so keep looking.
4941                         continue
4942 
4943                 try:
4944                     cell_computes = objects.ComputeNodeList.get_by_pagination(
4945                         cctxt, limit=limit, marker=marker)
4946                 except exception.MarkerNotFound:
4947                     # NOTE(danms): Keep looking through cells
4948                     continue
4949                 computes.extend(cell_computes)
4950                 # NOTE(danms): We must have found the marker, so continue on
4951                 # without one
4952                 marker = None
4953                 if limit:
4954                     limit -= len(cell_computes)
4955                     if limit <= 0:
4956                         break
4957 
4958         if marker is not None and len(computes) == 0:
4959             # NOTE(danms): If we did not find the marker in any cell,
4960             # mimic the db_api behavior here.
4961             raise exception.MarkerNotFound(marker=marker)
4962 
4963         return objects.ComputeNodeList(objects=computes)
4964 
4965     def compute_node_search_by_hypervisor(self, context, hypervisor_match):
4966         load_cells()
4967 
4968         computes = []
4969         for cell in CELLS:
4970             if cell.uuid == objects.CellMapping.CELL0_UUID:
4971                 continue
4972             with nova_context.target_cell(context, cell) as cctxt:
4973                 cell_computes = objects.ComputeNodeList.get_by_hypervisor(
4974                     cctxt, hypervisor_match)
4975             computes.extend(cell_computes)
4976         return objects.ComputeNodeList(objects=computes)
4977 
4978     def compute_node_statistics(self, context):
4979         load_cells()
4980 
4981         cell_stats = []
4982         for cell in CELLS:
4983             if cell.uuid == objects.CellMapping.CELL0_UUID:
4984                 continue
4985             with nova_context.target_cell(context, cell) as cctxt:
4986                 cell_stats.append(self.db.compute_node_statistics(cctxt))
4987 
4988         if cell_stats:
4989             keys = cell_stats[0].keys()
4990             return {k: sum(stats[k] for stats in cell_stats)
4991                     for k in keys}
4992         else:
4993             return {}
4994 
4995 
4996 class InstanceActionAPI(base.Base):
4997     """Sub-set of the Compute Manager API for managing instance actions."""
4998 
4999     def actions_get(self, context, instance, limit=None, marker=None,
5000                     filters=None):
5001         return objects.InstanceActionList.get_by_instance_uuid(
5002             context, instance.uuid, limit, marker, filters)
5003 
5004     def action_get_by_request_id(self, context, instance, request_id):
5005         return objects.InstanceAction.get_by_request_id(
5006             context, instance.uuid, request_id)
5007 
5008     def action_events_get(self, context, instance, action_id):
5009         return objects.InstanceActionEventList.get_by_action(
5010             context, action_id)
5011 
5012 
5013 class AggregateAPI(base.Base):
5014     """Sub-set of the Compute Manager API for managing host aggregates."""
5015     def __init__(self, **kwargs):
5016         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
5017         self.scheduler_client = scheduler_client.SchedulerClient()
5018         self.placement_client = self.scheduler_client.reportclient
5019         super(AggregateAPI, self).__init__(**kwargs)
5020 
5021     @wrap_exception()
5022     def create_aggregate(self, context, aggregate_name, availability_zone):
5023         """Creates the model for the aggregate."""
5024 
5025         aggregate = objects.Aggregate(context=context)
5026         aggregate.name = aggregate_name
5027         if availability_zone:
5028             aggregate.metadata = {'availability_zone': availability_zone}
5029         aggregate.create()
5030         self.scheduler_client.update_aggregates(context, [aggregate])
5031         return aggregate
5032 
5033     def get_aggregate(self, context, aggregate_id):
5034         """Get an aggregate by id."""
5035         return objects.Aggregate.get_by_id(context, aggregate_id)
5036 
5037     def get_aggregate_list(self, context):
5038         """Get all the aggregates."""
5039         return objects.AggregateList.get_all(context)
5040 
5041     def get_aggregates_by_host(self, context, compute_host):
5042         """Get all the aggregates where the given host is presented."""
5043         return objects.AggregateList.get_by_host(context, compute_host)
5044 
5045     @wrap_exception()
5046     def update_aggregate(self, context, aggregate_id, values):
5047         """Update the properties of an aggregate."""
5048         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
5049         if 'name' in values:
5050             aggregate.name = values.pop('name')
5051             aggregate.save()
5052         self.is_safe_to_update_az(context, values, aggregate=aggregate,
5053                                   action_name=AGGREGATE_ACTION_UPDATE)
5054         if values:
5055             aggregate.update_metadata(values)
5056             aggregate.updated_at = timeutils.utcnow()
5057         self.scheduler_client.update_aggregates(context, [aggregate])
5058         # If updated values include availability_zones, then the cache
5059         # which stored availability_zones and host need to be reset
5060         if values.get('availability_zone'):
5061             availability_zones.reset_cache()
5062         return aggregate
5063 
5064     @wrap_exception()
5065     def update_aggregate_metadata(self, context, aggregate_id, metadata):
5066         """Updates the aggregate metadata."""
5067         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
5068         self.is_safe_to_update_az(context, metadata, aggregate=aggregate,
5069                                   action_name=AGGREGATE_ACTION_UPDATE_META)
5070         aggregate.update_metadata(metadata)
5071         self.scheduler_client.update_aggregates(context, [aggregate])
5072         # If updated metadata include availability_zones, then the cache
5073         # which stored availability_zones and host need to be reset
5074         if metadata and metadata.get('availability_zone'):
5075             availability_zones.reset_cache()
5076         aggregate.updated_at = timeutils.utcnow()
5077         return aggregate
5078 
5079     @wrap_exception()
5080     def delete_aggregate(self, context, aggregate_id):
5081         """Deletes the aggregate."""
5082         aggregate_payload = {'aggregate_id': aggregate_id}
5083         compute_utils.notify_about_aggregate_update(context,
5084                                                     "delete.start",
5085                                                     aggregate_payload)
5086         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
5087 
5088         compute_utils.notify_about_aggregate_action(
5089             context=context,
5090             aggregate=aggregate,
5091             action=fields_obj.NotificationAction.DELETE,
5092             phase=fields_obj.NotificationPhase.START)
5093 
5094         if len(aggregate.hosts) > 0:
5095             msg = _("Host aggregate is not empty")
5096             raise exception.InvalidAggregateActionDelete(
5097                 aggregate_id=aggregate_id, reason=msg)
5098         aggregate.destroy()
5099         self.scheduler_client.delete_aggregate(context, aggregate)
5100         compute_utils.notify_about_aggregate_update(context,
5101                                                     "delete.end",
5102                                                     aggregate_payload)
5103         compute_utils.notify_about_aggregate_action(
5104             context=context,
5105             aggregate=aggregate,
5106             action=fields_obj.NotificationAction.DELETE,
5107             phase=fields_obj.NotificationPhase.END)
5108 
5109     def is_safe_to_update_az(self, context, metadata, aggregate,
5110                              hosts=None,
5111                              action_name=AGGREGATE_ACTION_ADD):
5112         """Determine if updates alter an aggregate's availability zone.
5113 
5114             :param context: local context
5115             :param metadata: Target metadata for updating aggregate
5116             :param aggregate: Aggregate to update
5117             :param hosts: Hosts to check. If None, aggregate.hosts is used
5118             :type hosts: list
5119             :action_name: Calling method for logging purposes
5120 
5121         """
5122         if 'availability_zone' in metadata:
5123             if not metadata['availability_zone']:
5124                 msg = _("Aggregate %s does not support empty named "
5125                         "availability zone") % aggregate.name
5126                 self._raise_invalid_aggregate_exc(action_name, aggregate.id,
5127                                                   msg)
5128             _hosts = hosts or aggregate.hosts
5129             host_aggregates = objects.AggregateList.get_by_metadata_key(
5130                 context, 'availability_zone', hosts=_hosts)
5131             conflicting_azs = [
5132                 agg.availability_zone for agg in host_aggregates
5133                 if agg.availability_zone != metadata['availability_zone']
5134                 and agg.id != aggregate.id]
5135             if conflicting_azs:
5136                 msg = _("One or more hosts already in availability zone(s) "
5137                         "%s") % conflicting_azs
5138                 self._raise_invalid_aggregate_exc(action_name, aggregate.id,
5139                                                   msg)
5140 
5141     def _raise_invalid_aggregate_exc(self, action_name, aggregate_id, reason):
5142         if action_name == AGGREGATE_ACTION_ADD:
5143             raise exception.InvalidAggregateActionAdd(
5144                 aggregate_id=aggregate_id, reason=reason)
5145         elif action_name == AGGREGATE_ACTION_UPDATE:
5146             raise exception.InvalidAggregateActionUpdate(
5147                 aggregate_id=aggregate_id, reason=reason)
5148         elif action_name == AGGREGATE_ACTION_UPDATE_META:
5149             raise exception.InvalidAggregateActionUpdateMeta(
5150                 aggregate_id=aggregate_id, reason=reason)
5151         elif action_name == AGGREGATE_ACTION_DELETE:
5152             raise exception.InvalidAggregateActionDelete(
5153                 aggregate_id=aggregate_id, reason=reason)
5154 
5155         raise exception.NovaException(
5156             _("Unexpected aggregate action %s") % action_name)
5157 
5158     def _update_az_cache_for_host(self, context, host_name, aggregate_meta):
5159         # Update the availability_zone cache to avoid getting wrong
5160         # availability_zone in cache retention time when add/remove
5161         # host to/from aggregate.
5162         if aggregate_meta and aggregate_meta.get('availability_zone'):
5163             availability_zones.update_host_availability_zone_cache(context,
5164                                                                    host_name)
5165 
5166     @wrap_exception()
5167     def add_host_to_aggregate(self, context, aggregate_id, host_name):
5168         """Adds the host to an aggregate."""
5169         aggregate_payload = {'aggregate_id': aggregate_id,
5170                              'host_name': host_name}
5171         compute_utils.notify_about_aggregate_update(context,
5172                                                     "addhost.start",
5173                                                     aggregate_payload)
5174         # validates the host; HostMappingNotFound or ComputeHostNotFound
5175         # is raised if invalid
5176         try:
5177             mapping = objects.HostMapping.get_by_host(context, host_name)
5178             nova_context.set_target_cell(context, mapping.cell_mapping)
5179             objects.Service.get_by_compute_host(context, host_name)
5180         except exception.HostMappingNotFound:
5181             try:
5182                 # NOTE(danms): This targets our cell
5183                 _find_service_in_cell(context, service_host=host_name)
5184             except exception.NotFound:
5185                 raise exception.ComputeHostNotFound(host=host_name)
5186 
5187         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
5188 
5189         compute_utils.notify_about_aggregate_action(
5190             context=context,
5191             aggregate=aggregate,
5192             action=fields_obj.NotificationAction.ADD_HOST,
5193             phase=fields_obj.NotificationPhase.START)
5194 
5195         self.is_safe_to_update_az(context, aggregate.metadata,
5196                                   hosts=[host_name], aggregate=aggregate)
5197 
5198         aggregate.add_host(host_name)
5199         self.scheduler_client.update_aggregates(context, [aggregate])
5200         try:
5201             self.placement_client.aggregate_add_host(
5202                 context, aggregate.uuid, host_name)
5203         except exception.PlacementAPIConnectFailure:
5204             # NOTE(jaypipes): Rocky should be able to tolerate the nova-api
5205             # service not communicating with the Placement API, so just log a
5206             # warning here.
5207             # TODO(jaypipes): Remove this in Stein, when placement must be able
5208             # to be contacted from the nova-api service.
5209             LOG.warning("Failed to associate %s with a placement "
5210                         "aggregate: %s. There was a failure to communicate "
5211                         "with the placement service.",
5212                         host_name, aggregate.uuid)
5213         except (exception.ResourceProviderNotFound,
5214                 exception.ResourceProviderAggregateRetrievalFailed,
5215                 exception.ResourceProviderUpdateFailed) as err:
5216             # NOTE(jaypipes): We don't want a failure perform the mirroring
5217             # action in the placement service to be returned to the user (they
5218             # probably don't know anything about the placement service and
5219             # would just be confused). So, we just log a warning here, noting
5220             # that on the next run of nova-manage placement sync_aggregates
5221             # things will go back to normal
5222             LOG.warning("Failed to associate %s with a placement "
5223                         "aggregate: %s. This may be corrected after running "
5224                         "nova-manage placement sync_aggregates.",
5225                         host_name, err)
5226         self._update_az_cache_for_host(context, host_name, aggregate.metadata)
5227         # NOTE(jogo): Send message to host to support resource pools
5228         self.compute_rpcapi.add_aggregate_host(context,
5229                 aggregate=aggregate, host_param=host_name, host=host_name)
5230         aggregate_payload.update({'name': aggregate.name})
5231         compute_utils.notify_about_aggregate_update(context,
5232                                                     "addhost.end",
5233                                                     aggregate_payload)
5234         compute_utils.notify_about_aggregate_action(
5235             context=context,
5236             aggregate=aggregate,
5237             action=fields_obj.NotificationAction.ADD_HOST,
5238             phase=fields_obj.NotificationPhase.END)
5239 
5240         return aggregate
5241 
5242     @wrap_exception()
5243     def remove_host_from_aggregate(self, context, aggregate_id, host_name):
5244         """Removes host from the aggregate."""
5245         aggregate_payload = {'aggregate_id': aggregate_id,
5246                              'host_name': host_name}
5247         compute_utils.notify_about_aggregate_update(context,
5248                                                     "removehost.start",
5249                                                     aggregate_payload)
5250         # validates the host; HostMappingNotFound or ComputeHostNotFound
5251         # is raised if invalid
5252         mapping = objects.HostMapping.get_by_host(context, host_name)
5253         nova_context.set_target_cell(context, mapping.cell_mapping)
5254         objects.Service.get_by_compute_host(context, host_name)
5255         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
5256 
5257         compute_utils.notify_about_aggregate_action(
5258             context=context,
5259             aggregate=aggregate,
5260             action=fields_obj.NotificationAction.REMOVE_HOST,
5261             phase=fields_obj.NotificationPhase.START)
5262 
5263         aggregate.delete_host(host_name)
5264         self.scheduler_client.update_aggregates(context, [aggregate])
5265         try:
5266             self.placement_client.aggregate_remove_host(
5267                 context, aggregate.uuid, host_name)
5268         except exception.PlacementAPIConnectFailure:
5269             # NOTE(jaypipes): Rocky should be able to tolerate the nova-api
5270             # service not communicating with the Placement API, so just log a
5271             # warning here.
5272             # TODO(jaypipes): Remove this in Stein, when placement must be able
5273             # to be contacted from the nova-api service.
5274             LOG.warning("Failed to remove association of %s with a placement "
5275                         "aggregate: %s. There was a failure to communicate "
5276                         "with the placement service.",
5277                         host_name, aggregate.uuid)
5278         except (exception.ResourceProviderNotFound,
5279                 exception.ResourceProviderAggregateRetrievalFailed,
5280                 exception.ResourceProviderUpdateFailed) as err:
5281             # NOTE(jaypipes): We don't want a failure perform the mirroring
5282             # action in the placement service to be returned to the user (they
5283             # probably don't know anything about the placement service and
5284             # would just be confused). So, we just log a warning here, noting
5285             # that on the next run of nova-manage placement sync_aggregates
5286             # things will go back to normal
5287             LOG.warning("Failed to remove association of %s with a placement "
5288                         "aggregate: %s. This may be corrected after running "
5289                         "nova-manage placement sync_aggregates.",
5290                         host_name, err)
5291         self._update_az_cache_for_host(context, host_name, aggregate.metadata)
5292         self.compute_rpcapi.remove_aggregate_host(context,
5293                 aggregate=aggregate, host_param=host_name, host=host_name)
5294         compute_utils.notify_about_aggregate_update(context,
5295                                                     "removehost.end",
5296                                                     aggregate_payload)
5297         compute_utils.notify_about_aggregate_action(
5298             context=context,
5299             aggregate=aggregate,
5300             action=fields_obj.NotificationAction.REMOVE_HOST,
5301             phase=fields_obj.NotificationPhase.END)
5302         return aggregate
5303 
5304 
5305 class KeypairAPI(base.Base):
5306     """Subset of the Compute Manager API for managing key pairs."""
5307 
5308     get_notifier = functools.partial(rpc.get_notifier, service='api')
5309     wrap_exception = functools.partial(exception_wrapper.wrap_exception,
5310                                        get_notifier=get_notifier,
5311                                        binary='nova-api')
5312 
5313     def _notify(self, context, event_suffix, keypair_name):
5314         payload = {
5315             'tenant_id': context.project_id,
5316             'user_id': context.user_id,
5317             'key_name': keypair_name,
5318         }
5319         notify = self.get_notifier()
5320         notify.info(context, 'keypair.%s' % event_suffix, payload)
5321 
5322     def _validate_new_key_pair(self, context, user_id, key_name, key_type):
5323         safe_chars = "_- " + string.digits + string.ascii_letters
5324         clean_value = "".join(x for x in key_name if x in safe_chars)
5325         if clean_value != key_name:
5326             raise exception.InvalidKeypair(
5327                 reason=_("Keypair name contains unsafe characters"))
5328 
5329         try:
5330             utils.check_string_length(key_name, min_length=1, max_length=255)
5331         except exception.InvalidInput:
5332             raise exception.InvalidKeypair(
5333                 reason=_('Keypair name must be string and between '
5334                          '1 and 255 characters long'))
5335         try:
5336             objects.Quotas.check_deltas(context, {'key_pairs': 1}, user_id)
5337         except exception.OverQuota:
5338             raise exception.KeypairLimitExceeded()
5339 
5340     @wrap_exception()
5341     def import_key_pair(self, context, user_id, key_name, public_key,
5342                         key_type=keypair_obj.KEYPAIR_TYPE_SSH):
5343         """Import a key pair using an existing public key."""
5344         self._validate_new_key_pair(context, user_id, key_name, key_type)
5345 
5346         self._notify(context, 'import.start', key_name)
5347 
5348         keypair = objects.KeyPair(context)
5349         keypair.user_id = user_id
5350         keypair.name = key_name
5351         keypair.type = key_type
5352         keypair.fingerprint = None
5353         keypair.public_key = public_key
5354 
5355         compute_utils.notify_about_keypair_action(
5356             context=context,
5357             keypair=keypair,
5358             action=fields_obj.NotificationAction.IMPORT,
5359             phase=fields_obj.NotificationPhase.START)
5360 
5361         fingerprint = self._generate_fingerprint(public_key, key_type)
5362 
5363         keypair.fingerprint = fingerprint
5364         keypair.create()
5365 
5366         compute_utils.notify_about_keypair_action(
5367             context=context,
5368             keypair=keypair,
5369             action=fields_obj.NotificationAction.IMPORT,
5370             phase=fields_obj.NotificationPhase.END)
5371         self._notify(context, 'import.end', key_name)
5372 
5373         return keypair
5374 
5375     @wrap_exception()
5376     def create_key_pair(self, context, user_id, key_name,
5377                         key_type=keypair_obj.KEYPAIR_TYPE_SSH):
5378         """Create a new key pair."""
5379         self._validate_new_key_pair(context, user_id, key_name, key_type)
5380 
5381         keypair = objects.KeyPair(context)
5382         keypair.user_id = user_id
5383         keypair.name = key_name
5384         keypair.type = key_type
5385         keypair.fingerprint = None
5386         keypair.public_key = None
5387 
5388         self._notify(context, 'create.start', key_name)
5389         compute_utils.notify_about_keypair_action(
5390             context=context,
5391             keypair=keypair,
5392             action=fields_obj.NotificationAction.CREATE,
5393             phase=fields_obj.NotificationPhase.START)
5394 
5395         private_key, public_key, fingerprint = self._generate_key_pair(
5396             user_id, key_type)
5397 
5398         keypair.fingerprint = fingerprint
5399         keypair.public_key = public_key
5400         keypair.create()
5401 
5402         # NOTE(melwitt): We recheck the quota after creating the object to
5403         # prevent users from allocating more resources than their allowed quota
5404         # in the event of a race. This is configurable because it can be
5405         # expensive if strict quota limits are not required in a deployment.
5406         if CONF.quota.recheck_quota:
5407             try:
5408                 objects.Quotas.check_deltas(context, {'key_pairs': 0}, user_id)
5409             except exception.OverQuota:
5410                 keypair.destroy()
5411                 raise exception.KeypairLimitExceeded()
5412 
5413         compute_utils.notify_about_keypair_action(
5414             context=context,
5415             keypair=keypair,
5416             action=fields_obj.NotificationAction.CREATE,
5417             phase=fields_obj.NotificationPhase.END)
5418 
5419         self._notify(context, 'create.end', key_name)
5420 
5421         return keypair, private_key
5422 
5423     def _generate_fingerprint(self, public_key, key_type):
5424         if key_type == keypair_obj.KEYPAIR_TYPE_SSH:
5425             return crypto.generate_fingerprint(public_key)
5426         elif key_type == keypair_obj.KEYPAIR_TYPE_X509:
5427             return crypto.generate_x509_fingerprint(public_key)
5428 
5429     def _generate_key_pair(self, user_id, key_type):
5430         if key_type == keypair_obj.KEYPAIR_TYPE_SSH:
5431             return crypto.generate_key_pair()
5432         elif key_type == keypair_obj.KEYPAIR_TYPE_X509:
5433             return crypto.generate_winrm_x509_cert(user_id)
5434 
5435     @wrap_exception()
5436     def delete_key_pair(self, context, user_id, key_name):
5437         """Delete a keypair by name."""
5438         self._notify(context, 'delete.start', key_name)
5439         keypair = self.get_key_pair(context, user_id, key_name)
5440         compute_utils.notify_about_keypair_action(
5441             context=context,
5442             keypair=keypair,
5443             action=fields_obj.NotificationAction.DELETE,
5444             phase=fields_obj.NotificationPhase.START)
5445         objects.KeyPair.destroy_by_name(context, user_id, key_name)
5446         compute_utils.notify_about_keypair_action(
5447             context=context,
5448             keypair=keypair,
5449             action=fields_obj.NotificationAction.DELETE,
5450             phase=fields_obj.NotificationPhase.END)
5451         self._notify(context, 'delete.end', key_name)
5452 
5453     def get_key_pairs(self, context, user_id, limit=None, marker=None):
5454         """List key pairs."""
5455         return objects.KeyPairList.get_by_user(
5456             context, user_id, limit=limit, marker=marker)
5457 
5458     def get_key_pair(self, context, user_id, key_name):
5459         """Get a keypair by name."""
5460         return objects.KeyPair.get_by_name(context, user_id, key_name)
5461 
5462 
5463 class SecurityGroupAPI(base.Base, security_group_base.SecurityGroupBase):
5464     """Sub-set of the Compute API related to managing security groups
5465     and security group rules
5466     """
5467 
5468     # The nova security group api does not use a uuid for the id.
5469     id_is_uuid = False
5470 
5471     def __init__(self, **kwargs):
5472         super(SecurityGroupAPI, self).__init__(**kwargs)
5473         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
5474 
5475     def validate_property(self, value, property, allowed):
5476         """Validate given security group property.
5477 
5478         :param value:          the value to validate, as a string or unicode
5479         :param property:       the property, either 'name' or 'description'
5480         :param allowed:        the range of characters allowed
5481         """
5482 
5483         try:
5484             val = value.strip()
5485         except AttributeError:
5486             msg = _("Security group %s is not a string or unicode") % property
5487             self.raise_invalid_property(msg)
5488         utils.check_string_length(val, name=property, min_length=1,
5489                                   max_length=255)
5490 
5491         if allowed and not re.match(allowed, val):
5492             # Some validation to ensure that values match API spec.
5493             # - Alphanumeric characters, spaces, dashes, and underscores.
5494             # TODO(Daviey): LP: #813685 extend beyond group_name checking, and
5495             #  probably create a param validator that can be used elsewhere.
5496             msg = (_("Value (%(value)s) for parameter Group%(property)s is "
5497                      "invalid. Content limited to '%(allowed)s'.") %
5498                    {'value': value, 'allowed': allowed,
5499                     'property': property.capitalize()})
5500             self.raise_invalid_property(msg)
5501 
5502     def ensure_default(self, context):
5503         """Ensure that a context has a security group.
5504 
5505         Creates a security group for the security context if it does not
5506         already exist.
5507 
5508         :param context: the security context
5509         """
5510         self.db.security_group_ensure_default(context)
5511 
5512     def create_security_group(self, context, name, description):
5513         try:
5514             objects.Quotas.check_deltas(context, {'security_groups': 1},
5515                                         context.project_id,
5516                                         user_id=context.user_id)
5517         except exception.OverQuota:
5518             msg = _("Quota exceeded, too many security groups.")
5519             self.raise_over_quota(msg)
5520 
5521         LOG.info("Create Security Group %s", name)
5522 
5523         self.ensure_default(context)
5524 
5525         group = {'user_id': context.user_id,
5526                  'project_id': context.project_id,
5527                  'name': name,
5528                  'description': description}
5529         try:
5530             group_ref = self.db.security_group_create(context, group)
5531         except exception.SecurityGroupExists:
5532             msg = _('Security group %s already exists') % name
5533             self.raise_group_already_exists(msg)
5534 
5535         # NOTE(melwitt): We recheck the quota after creating the object to
5536         # prevent users from allocating more resources than their allowed quota
5537         # in the event of a race. This is configurable because it can be
5538         # expensive if strict quota limits are not required in a deployment.
5539         if CONF.quota.recheck_quota:
5540             try:
5541                 objects.Quotas.check_deltas(context, {'security_groups': 0},
5542                                             context.project_id,
5543                                             user_id=context.user_id)
5544             except exception.OverQuota:
5545                 self.db.security_group_destroy(context, group_ref['id'])
5546                 msg = _("Quota exceeded, too many security groups.")
5547                 self.raise_over_quota(msg)
5548 
5549         return group_ref
5550 
5551     def update_security_group(self, context, security_group,
5552                                 name, description):
5553         if security_group['name'] in RO_SECURITY_GROUPS:
5554             msg = (_("Unable to update system group '%s'") %
5555                     security_group['name'])
5556             self.raise_invalid_group(msg)
5557 
5558         group = {'name': name,
5559                  'description': description}
5560 
5561         columns_to_join = ['rules.grantee_group']
5562         group_ref = self.db.security_group_update(context,
5563                 security_group['id'],
5564                 group,
5565                 columns_to_join=columns_to_join)
5566         return group_ref
5567 
5568     def get(self, context, name=None, id=None, map_exception=False):
5569         self.ensure_default(context)
5570         cols = ['rules']
5571         try:
5572             if name:
5573                 return self.db.security_group_get_by_name(context,
5574                                                           context.project_id,
5575                                                           name,
5576                                                           columns_to_join=cols)
5577             elif id:
5578                 return self.db.security_group_get(context, id,
5579                                                   columns_to_join=cols)
5580         except exception.NotFound as exp:
5581             if map_exception:
5582                 msg = exp.format_message()
5583                 self.raise_not_found(msg)
5584             else:
5585                 raise
5586 
5587     def list(self, context, names=None, ids=None, project=None,
5588              search_opts=None):
5589         self.ensure_default(context)
5590 
5591         groups = []
5592         if names or ids:
5593             if names:
5594                 for name in names:
5595                     groups.append(self.db.security_group_get_by_name(context,
5596                                                                      project,
5597                                                                      name))
5598             if ids:
5599                 for id in ids:
5600                     groups.append(self.db.security_group_get(context, id))
5601 
5602         elif context.is_admin:
5603             # TODO(eglynn): support a wider set of search options than just
5604             # all_tenants, at least include the standard filters defined for
5605             # the EC2 DescribeSecurityGroups API for the non-admin case also
5606             if (search_opts and 'all_tenants' in search_opts):
5607                 groups = self.db.security_group_get_all(context)
5608             else:
5609                 groups = self.db.security_group_get_by_project(context,
5610                                                                project)
5611 
5612         elif project:
5613             groups = self.db.security_group_get_by_project(context, project)
5614 
5615         return groups
5616 
5617     def destroy(self, context, security_group):
5618         if security_group['name'] in RO_SECURITY_GROUPS:
5619             msg = _("Unable to delete system group '%s'") % \
5620                     security_group['name']
5621             self.raise_invalid_group(msg)
5622 
5623         if self.db.security_group_in_use(context, security_group['id']):
5624             msg = _("Security group is still in use")
5625             self.raise_invalid_group(msg)
5626 
5627         LOG.info("Delete security group %s", security_group['name'])
5628         self.db.security_group_destroy(context, security_group['id'])
5629 
5630     def is_associated_with_server(self, security_group, instance_uuid):
5631         """Check if the security group is already associated
5632            with the instance. If Yes, return True.
5633         """
5634 
5635         if not security_group:
5636             return False
5637 
5638         instances = security_group.get('instances')
5639         if not instances:
5640             return False
5641 
5642         for inst in instances:
5643             if (instance_uuid == inst['uuid']):
5644                 return True
5645 
5646         return False
5647 
5648     def add_to_instance(self, context, instance, security_group_name):
5649         """Add security group to the instance."""
5650         security_group = self.db.security_group_get_by_name(context,
5651                 context.project_id,
5652                 security_group_name)
5653 
5654         instance_uuid = instance.uuid
5655 
5656         # check if the security group is associated with the server
5657         if self.is_associated_with_server(security_group, instance_uuid):
5658             raise exception.SecurityGroupExistsForInstance(
5659                                         security_group_id=security_group['id'],
5660                                         instance_id=instance_uuid)
5661 
5662         self.db.instance_add_security_group(context.elevated(),
5663                                             instance_uuid,
5664                                             security_group['id'])
5665         if instance.host:
5666             self.compute_rpcapi.refresh_instance_security_rules(
5667                     context, instance, instance.host)
5668 
5669     def remove_from_instance(self, context, instance, security_group_name):
5670         """Remove the security group associated with the instance."""
5671         security_group = self.db.security_group_get_by_name(context,
5672                 context.project_id,
5673                 security_group_name)
5674 
5675         instance_uuid = instance.uuid
5676 
5677         # check if the security group is associated with the server
5678         if not self.is_associated_with_server(security_group, instance_uuid):
5679             raise exception.SecurityGroupNotExistsForInstance(
5680                                     security_group_id=security_group['id'],
5681                                     instance_id=instance_uuid)
5682 
5683         self.db.instance_remove_security_group(context.elevated(),
5684                                                instance_uuid,
5685                                                security_group['id'])
5686         if instance.host:
5687             self.compute_rpcapi.refresh_instance_security_rules(
5688                     context, instance, instance.host)
5689 
5690     def get_rule(self, context, id):
5691         self.ensure_default(context)
5692         try:
5693             return self.db.security_group_rule_get(context, id)
5694         except exception.NotFound:
5695             msg = _("Rule (%s) not found") % id
5696             self.raise_not_found(msg)
5697 
5698     def add_rules(self, context, id, name, vals):
5699         """Add security group rule(s) to security group.
5700 
5701         Note: the Nova security group API doesn't support adding multiple
5702         security group rules at once but the EC2 one does. Therefore,
5703         this function is written to support both.
5704         """
5705 
5706         try:
5707             objects.Quotas.check_deltas(context,
5708                                         {'security_group_rules': len(vals)},
5709                                         id)
5710         except exception.OverQuota:
5711             msg = _("Quota exceeded, too many security group rules.")
5712             self.raise_over_quota(msg)
5713 
5714         msg = ("Security group %(name)s added %(protocol)s ingress "
5715                "(%(from_port)s:%(to_port)s)")
5716         rules = []
5717         for v in vals:
5718             rule = self.db.security_group_rule_create(context, v)
5719 
5720             # NOTE(melwitt): We recheck the quota after creating the object to
5721             # prevent users from allocating more resources than their allowed
5722             # quota in the event of a race. This is configurable because it can
5723             # be expensive if strict quota limits are not required in a
5724             # deployment.
5725             if CONF.quota.recheck_quota:
5726                 try:
5727                     objects.Quotas.check_deltas(context,
5728                                                 {'security_group_rules': 0},
5729                                                 id)
5730                 except exception.OverQuota:
5731                     self.db.security_group_rule_destroy(context, rule['id'])
5732                     msg = _("Quota exceeded, too many security group rules.")
5733                     self.raise_over_quota(msg)
5734 
5735             rules.append(rule)
5736             LOG.info(msg, {'name': name,
5737                            'protocol': rule.protocol,
5738                            'from_port': rule.from_port,
5739                            'to_port': rule.to_port})
5740 
5741         self.trigger_rules_refresh(context, id=id)
5742         return rules
5743 
5744     def remove_rules(self, context, security_group, rule_ids):
5745         msg = ("Security group %(name)s removed %(protocol)s ingress "
5746                "(%(from_port)s:%(to_port)s)")
5747         for rule_id in rule_ids:
5748             rule = self.get_rule(context, rule_id)
5749             LOG.info(msg, {'name': security_group['name'],
5750                            'protocol': rule.protocol,
5751                            'from_port': rule.from_port,
5752                            'to_port': rule.to_port})
5753 
5754             self.db.security_group_rule_destroy(context, rule_id)
5755 
5756         # NOTE(vish): we removed some rules, so refresh
5757         self.trigger_rules_refresh(context, id=security_group['id'])
5758 
5759     def remove_default_rules(self, context, rule_ids):
5760         for rule_id in rule_ids:
5761             self.db.security_group_default_rule_destroy(context, rule_id)
5762 
5763     def add_default_rules(self, context, vals):
5764         rules = [self.db.security_group_default_rule_create(context, v)
5765                  for v in vals]
5766         return rules
5767 
5768     def default_rule_exists(self, context, values):
5769         """Indicates whether the specified rule values are already
5770            defined in the default security group rules.
5771         """
5772         for rule in self.db.security_group_default_rule_list(context):
5773             keys = ('cidr', 'from_port', 'to_port', 'protocol')
5774             for key in keys:
5775                 if rule.get(key) != values.get(key):
5776                     break
5777             else:
5778                 return rule.get('id') or True
5779         return False
5780 
5781     def get_all_default_rules(self, context):
5782         try:
5783             rules = self.db.security_group_default_rule_list(context)
5784         except Exception:
5785             msg = 'cannot get default security group rules'
5786             raise exception.SecurityGroupDefaultRuleNotFound(msg)
5787 
5788         return rules
5789 
5790     def get_default_rule(self, context, id):
5791         return self.db.security_group_default_rule_get(context, id)
5792 
5793     def validate_id(self, id):
5794         try:
5795             return int(id)
5796         except ValueError:
5797             msg = _("Security group id should be integer")
5798             self.raise_invalid_property(msg)
5799 
5800     def _refresh_instance_security_rules(self, context, instances):
5801         for instance in instances:
5802             if instance.host is not None:
5803                 self.compute_rpcapi.refresh_instance_security_rules(
5804                         context, instance, instance.host)
5805 
5806     def trigger_rules_refresh(self, context, id):
5807         """Called when a rule is added to or removed from a security_group."""
5808         instances = objects.InstanceList.get_by_security_group_id(context, id)
5809         self._refresh_instance_security_rules(context, instances)
5810 
5811     def trigger_members_refresh(self, context, group_ids):
5812         """Called when a security group gains a new or loses a member.
5813 
5814         Sends an update request to each compute node for each instance for
5815         which this is relevant.
5816         """
5817         instances = objects.InstanceList.get_by_grantee_security_group_ids(
5818             context, group_ids)
5819         self._refresh_instance_security_rules(context, instances)
5820 
5821     def get_instance_security_groups(self, context, instance, detailed=False):
5822         if detailed:
5823             return self.db.security_group_get_by_instance(context,
5824                                                           instance.uuid)
5825         return [{'name': group.name} for group in instance.security_groups]
