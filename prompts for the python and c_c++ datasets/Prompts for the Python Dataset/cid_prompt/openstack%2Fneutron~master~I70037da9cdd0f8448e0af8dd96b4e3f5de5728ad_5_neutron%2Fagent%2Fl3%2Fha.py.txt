Please review the code below for security defects. You can consider defect types in terms of:
1.CWE-284 (Improper Access Control)
2.CWE-435 (Improper Interaction Between Multiple Entities)
3.CWE-664 (Improper Control of a Resource Through its Lifetime)
4.CWE-682 (Incorrect Calculation)
5.CWE-691 (Insufficient Control Flow Management)
6.CWE-693 (Protection Mechanism Failure)
7.CWE-697 (Incorrect Comparison)
8.CWE-703 (Improper Check or Handling of Exceptional Conditions)
9.CWE-707 (Improper Neutralization)
10.CWE-710 (Improper Adherence to Coding Standards)
If any are found, please describe the security defect in detail and indicate the corresponding line number of code and solution. If none are detected, states: 'No security defects are detected in the code'.

1 # Copyright (c) 2014 OpenStack Foundation.
2 # All Rights Reserved.
3 #
4 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
5 #    not use this file except in compliance with the License. You may obtain
6 #    a copy of the License at
7 #
8 #         http://www.apache.org/licenses/LICENSE-2.0
9 #
10 #    Unless required by applicable law or agreed to in writing, software
11 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
12 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
13 #    License for the specific language governing permissions and limitations
14 #    under the License.
15 
16 import os
17 import threading
18 
19 import eventlet
20 from neutron_lib import constants
21 from oslo_log import log as logging
22 from oslo_utils import fileutils
23 import webob
24 
25 from neutron.agent.linux import utils as agent_utils
26 from neutron.notifiers import batch_notifier
27 
28 LOG = logging.getLogger(__name__)
29 
30 KEEPALIVED_STATE_CHANGE_SERVER_BACKLOG = 4096
31 
32 TRANSLATION_MAP = {'master': constants.HA_ROUTER_STATE_ACTIVE,
33                    'backup': constants.HA_ROUTER_STATE_STANDBY,
34                    'fault': constants.HA_ROUTER_STATE_STANDBY,
35                    'unknown': constants.HA_ROUTER_STATE_UNKNOWN}
36 
37 
38 class KeepalivedStateChangeHandler(object):
39     def __init__(self, agent):
40         self.agent = agent
41 
42     @webob.dec.wsgify(RequestClass=webob.Request)
43     def __call__(self, req):
44         router_id = req.headers['X-Neutron-Router-Id']
45         state = req.headers['X-Neutron-State']
46         self.enqueue(router_id, state)
47 
48     def enqueue(self, router_id, state):
49         LOG.debug('Handling notification for router '
50                   '%(router_id)s, state %(state)s', {'router_id': router_id,
51                                                      'state': state})
52         self.agent.enqueue_state_change(router_id, state)
53 
54 
55 class L3AgentKeepalivedStateChangeServer(object):
56     def __init__(self, agent, conf):
57         self.agent = agent
58         self.conf = conf
59 
60         agent_utils.ensure_directory_exists_without_file(
61             self.get_keepalived_state_change_socket_path(self.conf))
62 
63     @classmethod
64     def get_keepalived_state_change_socket_path(cls, conf):
65         return os.path.join(conf.state_path, 'keepalived-state-change')
66 
67     def run(self):
68         server = agent_utils.UnixDomainWSGIServer(
69             'neutron-keepalived-state-change',
70             num_threads=self.conf.ha_keepalived_state_change_server_threads)
71         server.start(KeepalivedStateChangeHandler(self.agent),
72                      self.get_keepalived_state_change_socket_path(self.conf),
73                      workers=0,
74                      backlog=KEEPALIVED_STATE_CHANGE_SERVER_BACKLOG)
75         server.wait()
76 
77 
78 class AgentMixin(object):
79     def __init__(self, host):
80         self._init_ha_conf_path()
81         super(AgentMixin, self).__init__(host)
82         # BatchNotifier queue is needed to ensure that the HA router
83         # state change sequence is under the proper order.
84         self.state_change_notifier = batch_notifier.BatchNotifier(
85             self._calculate_batch_duration(), self.notify_server)
86         eventlet.spawn(self._start_keepalived_notifications_server)
87         self._transition_states = {}
88         self._transition_state_mutex = threading.Lock()
89 
90     def _get_router_info(self, router_id):
91         try:
92             return self.router_info[router_id]
93         except KeyError:
94             LOG.info('Router %s is not managed by this agent. It was '
95                      'possibly deleted concurrently.', router_id)
96 
97     def check_ha_state_for_router(self, router_id, current_state):
98         ri = self._get_router_info(router_id)
99         if not ri:
100             return
101         ha_state = ri.ha_state
102         if current_state != TRANSLATION_MAP[ha_state]:
103             LOG.debug("Updating server with state %(state)s for router "
104                       "%(router_id)s", {'router_id': router_id,
105                                         'state': ha_state})
106             self.state_change_notifier.queue_event((router_id, ha_state))
107 
108     def _start_keepalived_notifications_server(self):
109         state_change_server = (
110             L3AgentKeepalivedStateChangeServer(self, self.conf))
111         state_change_server.run()
112 
113     def _calculate_batch_duration(self):
114         # Set the BatchNotifier interval to ha_vrrp_advert_int,
115         # default 2 seconds.
116         return self.conf.ha_vrrp_advert_int
117 
118     def _update_transition_state(self, router_id, new_state=None):
119         with self._transition_state_mutex:
120             transition_state = self._transition_states.get(router_id)
121             if new_state:
122                 self._transition_states[router_id] = new_state
123             else:
124                 self._transition_states.pop(router_id, None)
125         return transition_state
126 
127     def enqueue_state_change(self, router_id, state):
128         """Inform the server about the new router state
129 
130         This function will also update the metadata proxy, the radvd daemon,
131         process the prefix delegation and inform to the L3 extensions. If the
132         HA router changes to "master", this transition will be delayed for at
133         least "ha_vrrp_advert_int" seconds. When the "master" router
134         transitions to "backup", "keepalived" will set the rest of HA routers
135         to "master" until it decides which one should be the only "master".
136         The transition from "backup" to "master" and then to "backup" again,
137         should not be registered in the Neutron server.
138 
139         :param router_id: router ID
140         :param state: ['master', 'backup']
141         """
142         if not self._update_transition_state(router_id, state):
143             eventlet.spawn_n(self._enqueue_state_change, router_id, state)
144             eventlet.sleep(0)
145 
146     def _enqueue_state_change(self, router_id, state):
147         # NOTE(ralonsoh): move 'master' and 'backup' constants to n-lib
148         if state == 'master':
149             eventlet.sleep(self.conf.ha_vrrp_advert_int)
150         if self._update_transition_state(router_id) != state:
151             # If the current "transition state" is not the initial "state" sent
152             # to update the router, that means the actual router state is the
153             # same as the "transition state" (e.g.: backup-->master-->backup).
154             return
155 
156         ri = self._get_router_info(router_id)
157         if ri is None:
158             return
159 
160         state_change_data = {"router_id": router_id, "state": state,
161                              "host": ri.agent.host}
162         LOG.info('Router %(router_id)s transitioned to %(state)s on '
163                  'agent %(host)s',
164                  state_change_data)
165 
166         # TODO(dalvarez): Fix bug 1677279 by moving the IPv6 parameters
167         # configuration to keepalived-state-change in order to remove the
168         # dependency that currently exists on l3-agent running for the IPv6
169         # failover.
170         ri.ha_state = state
171         self._configure_ipv6_params(ri, state)
172         if self.conf.enable_metadata_proxy:
173             self._update_metadata_proxy(ri, router_id, state)
174         self._update_radvd_daemon(ri, state)
175         self.pd.process_ha_state(router_id, state == 'master')
176         self.state_change_notifier.queue_event((router_id, state))
177         self.l3_ext_manager.ha_state_change(self.context, state_change_data)
178 
179     def _configure_ipv6_params(self, ri, state):
180         if not self.use_ipv6:
181             return
182 
183         ipv6_forwarding_enable = state == 'master'
184         if ri.router.get('distributed', False):
185             namespace = ri.ha_namespace
186         else:
187             namespace = ri.ns_name
188 
189         if ipv6_forwarding_enable:
190             ri.driver.configure_ipv6_forwarding(
191                 namespace, 'all', ipv6_forwarding_enable)
192 
193         # If ipv6 is enabled on the platform, ipv6_gateway config flag is
194         # not set and external_network associated to the router does not
195         # include any IPv6 subnet, enable the gateway interface to accept
196         # Router Advts from upstream router for default route on master
197         # instances as well as ipv6 forwarding. Otherwise, disable them.
198         ex_gw_port_id = ri.ex_gw_port and ri.ex_gw_port['id']
199         if ex_gw_port_id:
200             interface_name = ri.get_external_device_name(ex_gw_port_id)
201             ri._configure_ipv6_params_on_gw(
202                 ri.ex_gw_port, namespace, interface_name,
203                 ipv6_forwarding_enable)
204 
205     def _update_metadata_proxy(self, ri, router_id, state):
206         # NOTE(slaweq): Since the metadata proxy is spawned in the qrouter
207         # namespace and not in the snat namespace, even standby DVR-HA
208         # routers needs to serve metadata requests to local ports.
209         if state == 'master' or ri.router.get('distributed', False):
210             LOG.debug('Spawning metadata proxy for router %s', router_id)
211             self.metadata_driver.spawn_monitored_metadata_proxy(
212                 self.process_monitor, ri.ns_name, self.conf.metadata_port,
213                 self.conf, router_id=ri.router_id)
214         else:
215             LOG.debug('Closing metadata proxy for router %s', router_id)
216             self.metadata_driver.destroy_monitored_metadata_proxy(
217                 self.process_monitor, ri.router_id, self.conf, ri.ns_name)
218 
219     def _update_radvd_daemon(self, ri, state):
220         # Radvd has to be spawned only on the Master HA Router. If there are
221         # any state transitions, we enable/disable radvd accordingly.
222         if state == 'master':
223             ri.enable_radvd()
224         else:
225             ri.disable_radvd()
226 
227     def notify_server(self, batched_events):
228         translated_states = dict((router_id, TRANSLATION_MAP[state]) for
229                                  router_id, state in batched_events)
230         LOG.debug('Updating server with HA routers states %s',
231                   translated_states)
232         self.plugin_rpc.update_ha_routers_states(
233             self.context, translated_states)
234 
235     def _init_ha_conf_path(self):
236         ha_full_path = os.path.dirname("/%s/" % self.conf.ha_confs_path)
237         fileutils.ensure_tree(ha_full_path, mode=0o755)
