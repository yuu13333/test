Please review the code below for security defects. You can consider defect types in terms of:
1.CWE-284 (Improper Access Control)
2.CWE-435 (Improper Interaction Between Multiple Entities)
3.CWE-664 (Improper Control of a Resource Through its Lifetime)
4.CWE-682 (Incorrect Calculation)
5.CWE-691 (Insufficient Control Flow Management)
6.CWE-693 (Protection Mechanism Failure)
7.CWE-697 (Incorrect Comparison)
8.CWE-703 (Improper Check or Handling of Exceptional Conditions)
9.CWE-707 (Improper Neutralization)
10.CWE-710 (Improper Adherence to Coding Standards)
If any are found, please describe the security defect in detail and indicate the corresponding line number of code and solution. If none are detected, states: 'No security defects are detected in the code'.

1 # Copyright 2010 United States Government as represented by the
2 # Administrator of the National Aeronautics and Space Administration.
3 # Copyright 2011 Justin Santa Barbara
4 # All Rights Reserved.
5 #
6 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
7 #    not use this file except in compliance with the License. You may obtain
8 #    a copy of the License at
9 #
10 #         http://www.apache.org/licenses/LICENSE-2.0
11 #
12 #    Unless required by applicable law or agreed to in writing, software
13 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
14 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
15 #    License for the specific language governing permissions and limitations
16 #    under the License.
17 
18 """Handles all processes relating to instances (guest vms).
19 
20 The :py:class:`ComputeManager` class is a :py:class:`nova.manager.Manager` that
21 handles RPC calls relating to creating instances.  It is responsible for
22 building a disk image, launching it via the underlying virtualization driver,
23 responding to calls to check its state, attaching persistent storage, and
24 terminating it.
25 
26 """
27 
28 import base64
29 import binascii
30 # If py2, concurrent.futures comes from the futures library otherwise it
31 # comes from the py3 standard library.
32 from concurrent import futures
33 import contextlib
34 import functools
35 import inspect
36 import sys
37 import time
38 import traceback
39 
40 from cinderclient import exceptions as cinder_exception
41 from cursive import exception as cursive_exception
42 import eventlet.event
43 from eventlet import greenthread
44 import eventlet.semaphore
45 import eventlet.timeout
46 from keystoneauth1 import exceptions as keystone_exception
47 from oslo_log import log as logging
48 import oslo_messaging as messaging
49 from oslo_serialization import jsonutils
50 from oslo_service import loopingcall
51 from oslo_service import periodic_task
52 from oslo_utils import excutils
53 from oslo_utils import strutils
54 from oslo_utils import timeutils
55 import six
56 from six.moves import range
57 
58 from nova import block_device
59 from nova.cells import rpcapi as cells_rpcapi
60 from nova import compute
61 from nova.compute import build_results
62 from nova.compute import claims
63 from nova.compute import power_state
64 from nova.compute import resource_tracker
65 from nova.compute import rpcapi as compute_rpcapi
66 from nova.compute import task_states
67 from nova.compute import utils as compute_utils
68 from nova.compute.utils import wrap_instance_event
69 from nova.compute import vm_states
70 from nova import conductor
71 import nova.conf
72 from nova.console import rpcapi as console_rpcapi
73 import nova.context
74 from nova import exception
75 from nova import exception_wrapper
76 from nova import hooks
77 from nova.i18n import _
78 from nova import image
79 from nova import manager
80 from nova import network
81 from nova.network import base_api as base_net_api
82 from nova.network import model as network_model
83 from nova.network.security_group import openstack_driver
84 from nova import objects
85 from nova.objects import base as obj_base
86 from nova.objects import fields
87 from nova.objects import instance as obj_instance
88 from nova.objects import migrate_data as migrate_data_obj
89 from nova.pci import whitelist
90 from nova import rpc
91 from nova import safe_utils
92 from nova.scheduler import client as scheduler_client
93 from nova.scheduler import utils as scheduler_utils
94 from nova import utils
95 from nova.virt import block_device as driver_block_device
96 from nova.virt import configdrive
97 from nova.virt import driver
98 from nova.virt import event as virtevent
99 from nova.virt import storage_users
100 from nova.virt import virtapi
101 from nova.volume import cinder
102 
103 CONF = nova.conf.CONF
104 
105 LOG = logging.getLogger(__name__)
106 
107 get_notifier = functools.partial(rpc.get_notifier, service='compute')
108 wrap_exception = functools.partial(exception_wrapper.wrap_exception,
109                                    get_notifier=get_notifier,
110                                    binary='nova-compute')
111 
112 
113 @contextlib.contextmanager
114 def errors_out_migration_ctxt(migration):
115     """Context manager to error out migration on failure."""
116 
117     try:
118         yield
119     except Exception:
120         with excutils.save_and_reraise_exception():
121             if migration:
122                 # We may have been passed None for our migration if we're
123                 # receiving from an older client. The migration will be
124                 # errored via the legacy path.
125                 migration.status = 'error'
126                 try:
127                     with migration.obj_as_admin():
128                         migration.save()
129                 except Exception:
130                     LOG.debug(
131                         'Error setting migration status for instance %s.',
132                         migration.instance_uuid, exc_info=True)
133 
134 
135 @utils.expects_func_args('migration')
136 def errors_out_migration(function):
137     """Decorator to error out migration on failure."""
138 
139     @functools.wraps(function)
140     def decorated_function(self, context, *args, **kwargs):
141         wrapped_func = safe_utils.get_wrapped_function(function)
142         keyed_args = inspect.getcallargs(wrapped_func, self, context,
143                                          *args, **kwargs)
144         migration = keyed_args['migration']
145         with errors_out_migration_ctxt(migration):
146             return function(self, context, *args, **kwargs)
147 
148     return decorated_function
149 
150 
151 @utils.expects_func_args('instance')
152 def reverts_task_state(function):
153     """Decorator to revert task_state on failure."""
154 
155     @functools.wraps(function)
156     def decorated_function(self, context, *args, **kwargs):
157         try:
158             return function(self, context, *args, **kwargs)
159         except exception.UnexpectedTaskStateError as e:
160             # Note(maoy): unexpected task state means the current
161             # task is preempted. Do not clear task state in this
162             # case.
163             with excutils.save_and_reraise_exception():
164                 LOG.info("Task possibly preempted: %s",
165                          e.format_message())
166         except Exception:
167             with excutils.save_and_reraise_exception():
168                 wrapped_func = safe_utils.get_wrapped_function(function)
169                 keyed_args = inspect.getcallargs(wrapped_func, self, context,
170                                                  *args, **kwargs)
171                 # NOTE(mriedem): 'instance' must be in keyed_args because we
172                 # have utils.expects_func_args('instance') decorating this
173                 # method.
174                 instance = keyed_args['instance']
175                 original_task_state = instance.task_state
176                 try:
177                     self._instance_update(context, instance, task_state=None)
178                     LOG.info("Successfully reverted task state from %s on "
179                              "failure for instance.",
180                              original_task_state, instance=instance)
181                 except exception.InstanceNotFound:
182                     # We might delete an instance that failed to build shortly
183                     # after it errored out this is an expected case and we
184                     # should not trace on it.
185                     pass
186                 except Exception as e:
187                     LOG.warning("Failed to revert task state for instance. "
188                                 "Error: %s", e, instance=instance)
189 
190     return decorated_function
191 
192 
193 @utils.expects_func_args('instance')
194 def wrap_instance_fault(function):
195     """Wraps a method to catch exceptions related to instances.
196 
197     This decorator wraps a method to catch any exceptions having to do with
198     an instance that may get thrown. It then logs an instance fault in the db.
199     """
200 
201     @functools.wraps(function)
202     def decorated_function(self, context, *args, **kwargs):
203         try:
204             return function(self, context, *args, **kwargs)
205         except exception.InstanceNotFound:
206             raise
207         except Exception as e:
208             # NOTE(gtt): If argument 'instance' is in args rather than kwargs,
209             # we will get a KeyError exception which will cover up the real
210             # exception. So, we update kwargs with the values from args first.
211             # then, we can get 'instance' from kwargs easily.
212             kwargs.update(dict(zip(function.__code__.co_varnames[2:], args)))
213 
214             with excutils.save_and_reraise_exception():
215                 compute_utils.add_instance_fault_from_exc(context,
216                         kwargs['instance'], e, sys.exc_info())
217 
218     return decorated_function
219 
220 
221 @utils.expects_func_args('image_id', 'instance')
222 def delete_image_on_error(function):
223     """Used for snapshot related method to ensure the image created in
224     compute.api is deleted when an error occurs.
225     """
226 
227     @functools.wraps(function)
228     def decorated_function(self, context, image_id, instance,
229                            *args, **kwargs):
230         try:
231             return function(self, context, image_id, instance,
232                             *args, **kwargs)
233         except Exception:
234             with excutils.save_and_reraise_exception():
235                 LOG.debug("Cleaning up image %s", image_id,
236                           exc_info=True, instance=instance)
237                 try:
238                     self.image_api.delete(context, image_id)
239                 except exception.ImageNotFound:
240                     # Since we're trying to cleanup an image, we don't care if
241                     # if it's already gone.
242                     pass
243                 except Exception:
244                     LOG.exception("Error while trying to clean up image %s",
245                                   image_id, instance=instance)
246 
247     return decorated_function
248 
249 
250 # TODO(danms): Remove me after Icehouse
251 # TODO(alaski): Actually remove this after Newton, assuming a major RPC bump
252 # NOTE(mikal): if the method being decorated has more than one decorator, then
253 # put this one first. Otherwise the various exception handling decorators do
254 # not function correctly.
255 def object_compat(function):
256     """Wraps a method that expects a new-world instance
257 
258     This provides compatibility for callers passing old-style dict
259     instances.
260     """
261 
262     @functools.wraps(function)
263     def decorated_function(self, context, *args, **kwargs):
264         def _load_instance(instance_or_dict):
265             if isinstance(instance_or_dict, dict):
266                 # try to get metadata and system_metadata for most cases but
267                 # only attempt to load those if the db instance already has
268                 # those fields joined
269                 metas = [meta for meta in ('metadata', 'system_metadata')
270                          if meta in instance_or_dict]
271                 instance = objects.Instance._from_db_object(
272                     context, objects.Instance(), instance_or_dict,
273                     expected_attrs=metas)
274                 instance._context = context
275                 return instance
276             return instance_or_dict
277 
278         try:
279             kwargs['instance'] = _load_instance(kwargs['instance'])
280         except KeyError:
281             args = (_load_instance(args[0]),) + args[1:]
282 
283         migration = kwargs.get('migration')
284         if isinstance(migration, dict):
285             migration = objects.Migration._from_db_object(
286                     context.elevated(), objects.Migration(),
287                     migration)
288             kwargs['migration'] = migration
289 
290         return function(self, context, *args, **kwargs)
291 
292     return decorated_function
293 
294 
295 class InstanceEvents(object):
296     def __init__(self):
297         self._events = {}
298 
299     @staticmethod
300     def _lock_name(instance):
301         return '%s-%s' % (instance.uuid, 'events')
302 
303     def prepare_for_instance_event(self, instance, name, tag):
304         """Prepare to receive an event for an instance.
305 
306         This will register an event for the given instance that we will
307         wait on later. This should be called before initiating whatever
308         action will trigger the event. The resulting eventlet.event.Event
309         object should be wait()'d on to ensure completion.
310 
311         :param instance: the instance for which the event will be generated
312         :param name: the name of the event we're expecting
313         :param tag: the tag associated with the event we're expecting
314         :returns: an event object that should be wait()'d on
315         """
316         if self._events is None:
317             # NOTE(danms): We really should have a more specific error
318             # here, but this is what we use for our default error case
319             raise exception.NovaException('In shutdown, no new events '
320                                           'can be scheduled')
321 
322         @utils.synchronized(self._lock_name(instance))
323         def _create_or_get_event():
324             instance_events = self._events.setdefault(instance.uuid, {})
325             return instance_events.setdefault((name, tag),
326                                               eventlet.event.Event())
327         LOG.debug('Preparing to wait for external event %(name)s-%(tag)s',
328                   {'name': name, 'tag': tag}, instance=instance)
329         return _create_or_get_event()
330 
331     def pop_instance_event(self, instance, event):
332         """Remove a pending event from the wait list.
333 
334         This will remove a pending event from the wait list so that it
335         can be used to signal the waiters to wake up.
336 
337         :param instance: the instance for which the event was generated
338         :param event: the nova.objects.external_event.InstanceExternalEvent
339                       that describes the event
340         :returns: the eventlet.event.Event object on which the waiters
341                   are blocked
342         """
343         no_events_sentinel = object()
344         no_matching_event_sentinel = object()
345 
346         @utils.synchronized(self._lock_name(instance))
347         def _pop_event():
348             if self._events is None:
349                 LOG.debug('Unexpected attempt to pop events during shutdown',
350                           instance=instance)
351                 return no_events_sentinel
352             events = self._events.get(instance.uuid)
353             if not events:
354                 return no_events_sentinel
355             _event = events.pop((event.name, event.tag), None)
356             if not events:
357                 del self._events[instance.uuid]
358             if _event is None:
359                 return no_matching_event_sentinel
360             return _event
361 
362         result = _pop_event()
363         if result is no_events_sentinel:
364             LOG.debug('No waiting events found dispatching %(event)s',
365                       {'event': event.key},
366                       instance=instance)
367             return None
368         elif result is no_matching_event_sentinel:
369             LOG.debug('No event matching %(event)s in %(events)s',
370                       {'event': event.key,
371                        'events': self._events.get(instance.uuid, {}).keys()},
372                       instance=instance)
373             return None
374         else:
375             return result
376 
377     def clear_events_for_instance(self, instance):
378         """Remove all pending events for an instance.
379 
380         This will remove all events currently pending for an instance
381         and return them (indexed by event name).
382 
383         :param instance: the instance for which events should be purged
384         :returns: a dictionary of {event_name: eventlet.event.Event}
385         """
386         @utils.synchronized(self._lock_name(instance))
387         def _clear_events():
388             if self._events is None:
389                 LOG.debug('Unexpected attempt to clear events during shutdown',
390                           instance=instance)
391                 return dict()
392             # NOTE(danms): We have historically returned the raw internal
393             # format here, which is {event.key: [events, ...])} so just
394             # trivially convert it here.
395             return {'%s-%s' % k: e
396                     for k, e in self._events.pop(instance.uuid, {}).items()}
397         return _clear_events()
398 
399     def cancel_all_events(self):
400         if self._events is None:
401             LOG.debug('Unexpected attempt to cancel events during shutdown.')
402             return
403         our_events = self._events
404         # NOTE(danms): Block new events
405         self._events = None
406 
407         for instance_uuid, events in our_events.items():
408             for (name, tag), eventlet_event in events.items():
409                 LOG.debug('Canceling in-flight event %(name)s-%(tag)s for '
410                           'instance %(instance_uuid)s',
411                           {'name': name,
412                            'tag': tag,
413                            'instance_uuid': instance_uuid})
414                 event = objects.InstanceExternalEvent(
415                     instance_uuid=instance_uuid,
416                     name=name, status='failed',
417                     tag=tag, data={})
418                 eventlet_event.send(event)
419 
420 
421 class ComputeVirtAPI(virtapi.VirtAPI):
422     def __init__(self, compute):
423         super(ComputeVirtAPI, self).__init__()
424         self._compute = compute
425 
426     def _default_error_callback(self, event_name, instance):
427         raise exception.NovaException(_('Instance event failed'))
428 
429     @contextlib.contextmanager
430     def wait_for_instance_event(self, instance, event_names, deadline=300,
431                                 error_callback=None):
432         """Plan to wait for some events, run some code, then wait.
433 
434         This context manager will first create plans to wait for the
435         provided event_names, yield, and then wait for all the scheduled
436         events to complete.
437 
438         Note that this uses an eventlet.timeout.Timeout to bound the
439         operation, so callers should be prepared to catch that
440         failure and handle that situation appropriately.
441 
442         If the event is not received by the specified timeout deadline,
443         eventlet.timeout.Timeout is raised.
444 
445         If the event is received but did not have a 'completed'
446         status, a NovaException is raised.  If an error_callback is
447         provided, instead of raising an exception as detailed above
448         for the failure case, the callback will be called with the
449         event_name and instance, and can return True to continue
450         waiting for the rest of the events, False to stop processing,
451         or raise an exception which will bubble up to the waiter.
452 
453         :param instance: The instance for which an event is expected
454         :param event_names: A list of event names. Each element is a
455                             tuple of strings to indicate (name, tag),
456                             where name is required, but tag may be None.
457         :param deadline: Maximum number of seconds we should wait for all
458                          of the specified events to arrive.
459         :param error_callback: A function to be called if an event arrives
460 
461         """
462 
463         if error_callback is None:
464             error_callback = self._default_error_callback
465         events = {}
466         for event_name in event_names:
467             name, tag = event_name
468             event_name = objects.InstanceExternalEvent.make_key(name, tag)
469             try:
470                 events[event_name] = (
471                     self._compute.instance_events.prepare_for_instance_event(
472                         instance, name, tag))
473             except exception.NovaException:
474                 error_callback(event_name, instance)
475                 # NOTE(danms): Don't wait for any of the events. They
476                 # should all be canceled and fired immediately below,
477                 # but don't stick around if not.
478                 deadline = 0
479         yield
480         with eventlet.timeout.Timeout(deadline):
481             for event_name, event in events.items():
482                 actual_event = event.wait()
483                 if actual_event.status == 'completed':
484                     continue
485                 decision = error_callback(event_name, instance)
486                 if decision is False:
487                     break
488 
489 
490 class ComputeManager(manager.Manager):
491     """Manages the running instances from creation to destruction."""
492 
493     target = messaging.Target(version='5.0')
494 
495     def __init__(self, compute_driver=None, *args, **kwargs):
496         """Load configuration options and connect to the hypervisor."""
497         self.virtapi = ComputeVirtAPI(self)
498         self.network_api = network.API()
499         self.volume_api = cinder.API()
500         self.image_api = image.API()
501         self._last_host_check = 0
502         self._last_bw_usage_poll = 0
503         self._bw_usage_supported = True
504         self._last_bw_usage_cell_update = 0
505         self.compute_api = compute.API()
506         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
507         self.conductor_api = conductor.API()
508         self.compute_task_api = conductor.ComputeTaskAPI()
509         self.is_neutron_security_groups = (
510             openstack_driver.is_neutron_security_groups())
511         self.cells_rpcapi = cells_rpcapi.CellsAPI()
512         self.scheduler_client = scheduler_client.SchedulerClient()
513         self.reportclient = self.scheduler_client.reportclient
514         self._resource_tracker = None
515         self.instance_events = InstanceEvents()
516         self._sync_power_pool = eventlet.GreenPool(
517             size=CONF.sync_power_state_pool_size)
518         self._syncs_in_progress = {}
519         self.send_instance_updates = (
520             CONF.filter_scheduler.track_instance_changes)
521         if CONF.max_concurrent_builds != 0:
522             self._build_semaphore = eventlet.semaphore.Semaphore(
523                 CONF.max_concurrent_builds)
524         else:
525             self._build_semaphore = compute_utils.UnlimitedSemaphore()
526         if max(CONF.max_concurrent_live_migrations, 0) != 0:
527             self._live_migration_executor = futures.ThreadPoolExecutor(
528                 max_workers=CONF.max_concurrent_live_migrations)
529         else:
530             # Starting in python 3.5, this is technically bounded, but it's
531             # ncpu * 5 which is probably much higher than anyone would sanely
532             # use for concurrently running live migrations.
533             self._live_migration_executor = futures.ThreadPoolExecutor()
534         # This is a dict, keyed by instance uuid, to a two-item tuple of
535         # migration object and Future for the queued live migration.
536         self._waiting_live_migrations = {}
537 
538         super(ComputeManager, self).__init__(service_name="compute",
539                                              *args, **kwargs)
540 
541         # NOTE(russellb) Load the driver last.  It may call back into the
542         # compute manager via the virtapi, so we want it to be fully
543         # initialized before that happens.
544         self.driver = driver.load_compute_driver(self.virtapi, compute_driver)
545         self.use_legacy_block_device_info = \
546                             self.driver.need_legacy_block_device_info
547 
548     def reset(self):
549         LOG.info('Reloading compute RPC API')
550         compute_rpcapi.LAST_VERSION = None
551         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
552 
553     def _get_resource_tracker(self):
554         if not self._resource_tracker:
555             rt = resource_tracker.ResourceTracker(self.host, self.driver)
556             self._resource_tracker = rt
557         return self._resource_tracker
558 
559     def _update_resource_tracker(self, context, instance):
560         """Let the resource tracker know that an instance has changed state."""
561 
562         if instance.host == self.host:
563             rt = self._get_resource_tracker()
564             rt.update_usage(context, instance, instance.node)
565 
566     def _instance_update(self, context, instance, **kwargs):
567         """Update an instance in the database using kwargs as value."""
568 
569         for k, v in kwargs.items():
570             setattr(instance, k, v)
571         instance.save()
572         self._update_resource_tracker(context, instance)
573 
574     def _nil_out_instance_obj_host_and_node(self, instance):
575         # NOTE(jwcroppe): We don't do instance.save() here for performance
576         # reasons; a call to this is expected to be immediately followed by
577         # another call that does instance.save(), thus avoiding two writes
578         # to the database layer.
579         instance.host = None
580         instance.node = None
581         # If the instance is not on a host, it's not in an aggregate and
582         # therefore is not in an availability zone.
583         instance.availability_zone = None
584 
585     def _set_instance_obj_error_state(self, context, instance,
586                                       clean_task_state=False):
587         try:
588             instance.vm_state = vm_states.ERROR
589             if clean_task_state:
590                 instance.task_state = None
591             instance.save()
592         except exception.InstanceNotFound:
593             LOG.debug('Instance has been destroyed from under us while '
594                       'trying to set it to ERROR', instance=instance)
595 
596     def _get_instances_on_driver(self, context, filters=None):
597         """Return a list of instance records for the instances found
598         on the hypervisor which satisfy the specified filters. If filters=None
599         return a list of instance records for all the instances found on the
600         hypervisor.
601         """
602         if not filters:
603             filters = {}
604         try:
605             driver_uuids = self.driver.list_instance_uuids()
606             if len(driver_uuids) == 0:
607                 # Short circuit, don't waste a DB call
608                 return objects.InstanceList()
609             filters['uuid'] = driver_uuids
610             local_instances = objects.InstanceList.get_by_filters(
611                 context, filters, use_slave=True)
612             return local_instances
613         except NotImplementedError:
614             pass
615 
616         # The driver doesn't support uuids listing, so we'll have
617         # to brute force.
618         driver_instances = self.driver.list_instances()
619         # NOTE(mjozefcz): In this case we need to apply host filter.
620         # Without this all instance data would be fetched from db.
621         filters['host'] = self.host
622         instances = objects.InstanceList.get_by_filters(context, filters,
623                                                         use_slave=True)
624         name_map = {instance.name: instance for instance in instances}
625         local_instances = []
626         for driver_instance in driver_instances:
627             instance = name_map.get(driver_instance)
628             if not instance:
629                 continue
630             local_instances.append(instance)
631         return local_instances
632 
633     def _destroy_evacuated_instances(self, context):
634         """Destroys evacuated instances.
635 
636         While nova-compute was down, the instances running on it could be
637         evacuated to another host. This method looks for evacuation migration
638         records where this is the source host and which were either started
639         (accepted), in-progress (pre-migrating) or migrated (done). From those
640         migration records, local instances reported by the hypervisor are
641         compared to the instances for the migration records and those local
642         guests are destroyed, along with instance allocation records in
643         Placement for this node.
644         """
645         filters = {
646             'source_compute': self.host,
647             # NOTE(mriedem): Migration records that have been accepted are
648             # included in case the source node comes back up while instances
649             # are being evacuated to another host. We don't want the same
650             # instance being reported from multiple hosts.
651             # NOTE(lyarwood): pre-migrating is also included here as the
652             # source compute can come back online shortly after the RT
653             # claims on the destination that in-turn moves the migration to
654             # pre-migrating. If the evacuate fails on the destination host,
655             # the user can rebuild the instance (in ERROR state) on the source
656             # host.
657             'status': ['accepted', 'pre-migrating', 'done'],
658             'migration_type': 'evacuation',
659         }
660         with utils.temporary_mutation(context, read_deleted='yes'):
661             evacuations = objects.MigrationList.get_by_filters(context,
662                                                                filters)
663         if not evacuations:
664             return
665         evacuations = {mig.instance_uuid: mig for mig in evacuations}
666 
667         local_instances = self._get_instances_on_driver(context)
668         evacuated = [inst for inst in local_instances
669                      if inst.uuid in evacuations]
670 
671         # NOTE(gibi): We are called from init_host and at this point the
672         # compute_nodes of the resource tracker has not been populated yet so
673         # we cannot rely on the resource tracker here.
674         compute_nodes = {}
675 
676         for instance in evacuated:
677             migration = evacuations[instance.uuid]
678             LOG.info('Deleting instance as it has been evacuated from '
679                      'this host', instance=instance)
680             try:
681                 network_info = self.network_api.get_instance_nw_info(
682                     context, instance)
683                 bdi = self._get_instance_block_device_info(context,
684                                                            instance)
685                 destroy_disks = not (self._is_instance_storage_shared(
686                     context, instance))
687             except exception.InstanceNotFound:
688                 network_info = network_model.NetworkInfo()
689                 bdi = {}
690                 LOG.info('Instance has been marked deleted already, '
691                          'removing it from the hypervisor.',
692                          instance=instance)
693                 # always destroy disks if the instance was deleted
694                 destroy_disks = True
695             self.driver.destroy(context, instance,
696                                 network_info,
697                                 bdi, destroy_disks)
698 
699             # delete the allocation of the evacuated instance from this host
700             if migration.source_node not in compute_nodes:
701                 try:
702                     cn_uuid = objects.ComputeNode.get_by_host_and_nodename(
703                         context, self.host, migration.source_node).uuid
704                     compute_nodes[migration.source_node] = cn_uuid
705                 except exception.ComputeHostNotFound:
706                     LOG.error("Failed to clean allocation of evacuated "
707                               "instance as the source node %s is not found",
708                               migration.source_node, instance=instance)
709                     continue
710             cn_uuid = compute_nodes[migration.source_node]
711 
712             if not scheduler_utils.remove_allocation_from_compute(
713                     context, instance, cn_uuid, self.reportclient):
714                 LOG.error("Failed to clean allocation of evacuated instance "
715                           "on the source node %s",
716                           cn_uuid, instance=instance)
717 
718             migration.status = 'completed'
719             migration.save()
720         return evacuations
721 
722     def _is_instance_storage_shared(self, context, instance, host=None):
723         shared_storage = True
724         data = None
725         try:
726             data = self.driver.check_instance_shared_storage_local(context,
727                                                        instance)
728             if data:
729                 shared_storage = (self.compute_rpcapi.
730                                   check_instance_shared_storage(context,
731                                   instance, data, host=host))
732         except NotImplementedError:
733             LOG.debug('Hypervisor driver does not support '
734                       'instance shared storage check, '
735                       'assuming it\'s not on shared storage',
736                       instance=instance)
737             shared_storage = False
738         except Exception:
739             LOG.exception('Failed to check if instance shared',
740                           instance=instance)
741         finally:
742             if data:
743                 self.driver.check_instance_shared_storage_cleanup(context,
744                                                                   data)
745         return shared_storage
746 
747     def _complete_partial_deletion(self, context, instance):
748         """Complete deletion for instances in DELETED status but not marked as
749         deleted in the DB
750         """
751         instance.destroy()
752         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
753                 context, instance.uuid)
754         self._complete_deletion(context,
755                                 instance)
756         self._notify_about_instance_usage(context, instance, "delete.end")
757         compute_utils.notify_about_instance_action(context, instance,
758                 self.host, action=fields.NotificationAction.DELETE,
759                 phase=fields.NotificationPhase.END, bdms=bdms)
760 
761     def _complete_deletion(self, context, instance):
762         self._update_resource_tracker(context, instance)
763 
764         rt = self._get_resource_tracker()
765         rt.reportclient.delete_allocation_for_instance(context, instance.uuid)
766 
767         self._clean_instance_console_tokens(context, instance)
768         self._delete_scheduler_instance_info(context, instance.uuid)
769 
770     def _init_instance(self, context, instance):
771         """Initialize this instance during service init."""
772 
773         # NOTE(danms): If the instance appears to not be owned by this
774         # host, it may have been evacuated away, but skipped by the
775         # evacuation cleanup code due to configuration. Thus, if that
776         # is a possibility, don't touch the instance in any way, but
777         # log the concern. This will help avoid potential issues on
778         # startup due to misconfiguration.
779         if instance.host != self.host:
780             LOG.warning('Instance %(uuid)s appears to not be owned '
781                         'by this host, but by %(host)s. Startup '
782                         'processing is being skipped.',
783                         {'uuid': instance.uuid,
784                          'host': instance.host})
785             return
786 
787         # Instances that are shut down, or in an error state can not be
788         # initialized and are not attempted to be recovered. The exception
789         # to this are instances that are in RESIZE_MIGRATING or DELETING,
790         # which are dealt with further down.
791         if (instance.vm_state == vm_states.SOFT_DELETED or
792             (instance.vm_state == vm_states.ERROR and
793             instance.task_state not in
794             (task_states.RESIZE_MIGRATING, task_states.DELETING))):
795             LOG.debug("Instance is in %s state.",
796                       instance.vm_state, instance=instance)
797             return
798 
799         if instance.vm_state == vm_states.DELETED:
800             try:
801                 self._complete_partial_deletion(context, instance)
802             except Exception:
803                 # we don't want that an exception blocks the init_host
804                 LOG.exception('Failed to complete a deletion',
805                               instance=instance)
806             return
807 
808         if (instance.vm_state == vm_states.BUILDING or
809             instance.task_state in [task_states.SCHEDULING,
810                                     task_states.BLOCK_DEVICE_MAPPING,
811                                     task_states.NETWORKING,
812                                     task_states.SPAWNING]):
813             # NOTE(dave-mcnally) compute stopped before instance was fully
814             # spawned so set to ERROR state. This is safe to do as the state
815             # may be set by the api but the host is not so if we get here the
816             # instance has already been scheduled to this particular host.
817             LOG.debug("Instance failed to spawn correctly, "
818                       "setting to ERROR state", instance=instance)
819             instance.task_state = None
820             instance.vm_state = vm_states.ERROR
821             instance.save()
822             return
823 
824         if (instance.vm_state in [vm_states.ACTIVE, vm_states.STOPPED] and
825             instance.task_state in [task_states.REBUILDING,
826                                     task_states.REBUILD_BLOCK_DEVICE_MAPPING,
827                                     task_states.REBUILD_SPAWNING]):
828             # NOTE(jichenjc) compute stopped before instance was fully
829             # spawned so set to ERROR state. This is consistent to BUILD
830             LOG.debug("Instance failed to rebuild correctly, "
831                       "setting to ERROR state", instance=instance)
832             instance.task_state = None
833             instance.vm_state = vm_states.ERROR
834             instance.save()
835             return
836 
837         if (instance.vm_state != vm_states.ERROR and
838             instance.task_state in [task_states.IMAGE_SNAPSHOT_PENDING,
839                                     task_states.IMAGE_PENDING_UPLOAD,
840                                     task_states.IMAGE_UPLOADING,
841                                     task_states.IMAGE_SNAPSHOT]):
842             LOG.debug("Instance in transitional state %s at start-up "
843                       "clearing task state",
844                       instance.task_state, instance=instance)
845             try:
846                 self._post_interrupted_snapshot_cleanup(context, instance)
847             except Exception:
848                 # we don't want that an exception blocks the init_host
849                 LOG.exception('Failed to cleanup snapshot.', instance=instance)
850             instance.task_state = None
851             instance.save()
852 
853         if (instance.vm_state != vm_states.ERROR and
854             instance.task_state in [task_states.RESIZE_PREP]):
855             LOG.debug("Instance in transitional state %s at start-up "
856                       "clearing task state",
857                       instance['task_state'], instance=instance)
858             instance.task_state = None
859             instance.save()
860 
861         if instance.task_state in [task_states.DELETING,
862                                    task_states.SOFT_DELETING]:
863             LOG.info('Service started deleting the instance during '
864                      'the previous run, but did not finish. Restarting'
865                      ' the deletion now.', instance=instance)
866             self._delete_instance_stuck_in_deleting_task(context, instance)
867             return
868 
869         current_power_state = self._get_power_state(context, instance)
870         try_reboot, reboot_type = self._retry_reboot(context, instance,
871                                                      current_power_state)
872 
873         if try_reboot:
874             LOG.debug("Instance in transitional state (%(task_state)s) at "
875                       "start-up and power state is (%(power_state)s), "
876                       "triggering reboot",
877                       {'task_state': instance.task_state,
878                        'power_state': current_power_state},
879                       instance=instance)
880 
881             # NOTE(mikal): if the instance was doing a soft reboot that got as
882             # far as shutting down the instance but not as far as starting it
883             # again, then we've just become a hard reboot. That means the
884             # task state for the instance needs to change so that we're in one
885             # of the expected task states for a hard reboot.
886             if (instance.task_state in task_states.soft_reboot_states and
887                 reboot_type == 'HARD'):
888                 instance.task_state = task_states.REBOOT_PENDING_HARD
889                 instance.save()
890 
891             self.reboot_instance(context, instance, block_device_info=None,
892                                  reboot_type=reboot_type)
893             return
894 
895         elif (current_power_state == power_state.RUNNING and
896               instance.task_state in [task_states.REBOOT_STARTED,
897                                       task_states.REBOOT_STARTED_HARD,
898                                       task_states.PAUSING,
899                                       task_states.UNPAUSING]):
900             LOG.warning("Instance in transitional state "
901                         "(%(task_state)s) at start-up and power state "
902                         "is (%(power_state)s), clearing task state",
903                         {'task_state': instance.task_state,
904                          'power_state': current_power_state},
905                         instance=instance)
906             instance.task_state = None
907             instance.vm_state = vm_states.ACTIVE
908             instance.save()
909         elif (current_power_state == power_state.PAUSED and
910               instance.task_state == task_states.UNPAUSING):
911             LOG.warning("Instance in transitional state "
912                         "(%(task_state)s) at start-up and power state "
913                         "is (%(power_state)s), clearing task state "
914                         "and unpausing the instance",
915                         {'task_state': instance.task_state,
916                          'power_state': current_power_state},
917                         instance=instance)
918             try:
919                 self.unpause_instance(context, instance)
920             except NotImplementedError:
921                 # Some virt driver didn't support pause and unpause
922                 pass
923             except Exception:
924                 LOG.exception('Failed to unpause instance', instance=instance)
925             return
926 
927         if instance.task_state == task_states.POWERING_OFF:
928             try:
929                 LOG.debug("Instance in transitional state %s at start-up "
930                           "retrying stop request",
931                           instance.task_state, instance=instance)
932                 self.stop_instance(context, instance, True)
933             except Exception:
934                 # we don't want that an exception blocks the init_host
935                 LOG.exception('Failed to stop instance', instance=instance)
936             return
937 
938         if instance.task_state == task_states.POWERING_ON:
939             try:
940                 LOG.debug("Instance in transitional state %s at start-up "
941                           "retrying start request",
942                           instance.task_state, instance=instance)
943                 self.start_instance(context, instance)
944             except Exception:
945                 # we don't want that an exception blocks the init_host
946                 LOG.exception('Failed to start instance', instance=instance)
947             return
948 
949         net_info = instance.get_network_info()
950         try:
951             self.driver.plug_vifs(instance, net_info)
952         except NotImplementedError as e:
953             LOG.debug(e, instance=instance)
954         except exception.VirtualInterfacePlugException:
955             # NOTE(mriedem): If we get here, it could be because the vif_type
956             # in the cache is "binding_failed". The only way to fix that is to
957             # try and bind the ports again, which would be expensive here on
958             # host startup. We could add a check to _heal_instance_info_cache
959             # to handle this, but probably only if the instance task_state is
960             # None.
961             LOG.exception('Virtual interface plugging failed for instance. '
962                           'The port binding:host_id may need to be manually '
963                           'updated.', instance=instance)
964             self._set_instance_obj_error_state(context, instance)
965             return
966 
967         if instance.task_state == task_states.RESIZE_MIGRATING:
968             # We crashed during resize/migration, so roll back for safety
969             try:
970                 # NOTE(mriedem): check old_vm_state for STOPPED here, if it's
971                 # not in system_metadata we default to True for backwards
972                 # compatibility
973                 power_on = (instance.system_metadata.get('old_vm_state') !=
974                             vm_states.STOPPED)
975 
976                 block_dev_info = self._get_instance_block_device_info(context,
977                                                                       instance)
978 
979                 self.driver.finish_revert_migration(context,
980                     instance, net_info, block_dev_info, power_on)
981 
982             except Exception:
983                 LOG.exception('Failed to revert crashed migration',
984                               instance=instance)
985             finally:
986                 LOG.info('Instance found in migrating state during '
987                          'startup. Resetting task_state',
988                          instance=instance)
989                 instance.task_state = None
990                 instance.save()
991         if instance.task_state == task_states.MIGRATING:
992             # Live migration did not complete, but instance is on this
993             # host, so reset the state.
994             instance.task_state = None
995             instance.save(expected_task_state=[task_states.MIGRATING])
996 
997         db_state = instance.power_state
998         drv_state = self._get_power_state(context, instance)
999         expect_running = (db_state == power_state.RUNNING and
1000                           drv_state != db_state)
1001 
1002         LOG.debug('Current state is %(drv_state)s, state in DB is '
1003                   '%(db_state)s.',
1004                   {'drv_state': drv_state, 'db_state': db_state},
1005                   instance=instance)
1006 
1007         if expect_running and CONF.resume_guests_state_on_host_boot:
1008             self._resume_guests_state(context, instance, net_info)
1009         elif drv_state == power_state.RUNNING:
1010             # VMwareAPI drivers will raise an exception
1011             try:
1012                 self.driver.ensure_filtering_rules_for_instance(
1013                                        instance, net_info)
1014             except NotImplementedError:
1015                 LOG.debug('Hypervisor driver does not support '
1016                           'firewall rules', instance=instance)
1017 
1018     def _resume_guests_state(self, context, instance, net_info):
1019         LOG.info('Rebooting instance after nova-compute restart.',
1020                  instance=instance)
1021         block_device_info = \
1022             self._get_instance_block_device_info(context, instance)
1023 
1024         try:
1025             self.driver.resume_state_on_host_boot(
1026                 context, instance, net_info, block_device_info)
1027         except NotImplementedError:
1028             LOG.warning('Hypervisor driver does not support '
1029                         'resume guests', instance=instance)
1030         except Exception:
1031             # NOTE(vish): The instance failed to resume, so we set the
1032             #             instance to error and attempt to continue.
1033             LOG.warning('Failed to resume instance',
1034                         instance=instance)
1035             self._set_instance_obj_error_state(context, instance)
1036 
1037     def _retry_reboot(self, context, instance, current_power_state):
1038         current_task_state = instance.task_state
1039         retry_reboot = False
1040         reboot_type = compute_utils.get_reboot_type(current_task_state,
1041                                                     current_power_state)
1042 
1043         pending_soft = (current_task_state == task_states.REBOOT_PENDING and
1044                         instance.vm_state in vm_states.ALLOW_SOFT_REBOOT)
1045         pending_hard = (current_task_state == task_states.REBOOT_PENDING_HARD
1046                         and instance.vm_state in vm_states.ALLOW_HARD_REBOOT)
1047         started_not_running = (current_task_state in
1048                                [task_states.REBOOT_STARTED,
1049                                 task_states.REBOOT_STARTED_HARD] and
1050                                current_power_state != power_state.RUNNING)
1051 
1052         if pending_soft or pending_hard or started_not_running:
1053             retry_reboot = True
1054 
1055         return retry_reboot, reboot_type
1056 
1057     def handle_lifecycle_event(self, event):
1058         LOG.info("VM %(state)s (Lifecycle Event)",
1059                  {'state': event.get_name()},
1060                  instance_uuid=event.get_instance_uuid())
1061         context = nova.context.get_admin_context(read_deleted='yes')
1062         vm_power_state = None
1063         event_transition = event.get_transition()
1064         if event_transition == virtevent.EVENT_LIFECYCLE_STOPPED:
1065             vm_power_state = power_state.SHUTDOWN
1066         elif event_transition == virtevent.EVENT_LIFECYCLE_STARTED:
1067             vm_power_state = power_state.RUNNING
1068         elif event_transition in (
1069                 virtevent.EVENT_LIFECYCLE_PAUSED,
1070                 virtevent.EVENT_LIFECYCLE_POSTCOPY_STARTED,
1071                 virtevent.EVENT_LIFECYCLE_MIGRATION_COMPLETED):
1072             vm_power_state = power_state.PAUSED
1073         elif event_transition == virtevent.EVENT_LIFECYCLE_RESUMED:
1074             vm_power_state = power_state.RUNNING
1075         elif event_transition == virtevent.EVENT_LIFECYCLE_SUSPENDED:
1076             vm_power_state = power_state.SUSPENDED
1077         else:
1078             LOG.warning("Unexpected lifecycle event: %d", event_transition)
1079 
1080         migrate_finish_statuses = {
1081             # This happens on the source node and indicates live migration
1082             # entered post-copy mode.
1083             virtevent.EVENT_LIFECYCLE_POSTCOPY_STARTED: 'running (post-copy)',
1084             # Suspended for offline migration.
1085             virtevent.EVENT_LIFECYCLE_MIGRATION_COMPLETED: 'running'
1086         }
1087 
1088         expected_attrs = []
1089         if event_transition in migrate_finish_statuses:
1090             # Join on info_cache since that's needed in migrate_instance_start.
1091             expected_attrs.append('info_cache')
1092         instance = objects.Instance.get_by_uuid(context,
1093                                                 event.get_instance_uuid(),
1094                                                 expected_attrs=expected_attrs)
1095 
1096         # Note(lpetrut): The event may be delayed, thus not reflecting
1097         # the current instance power state. In that case, ignore the event.
1098         current_power_state = self._get_power_state(context, instance)
1099         if current_power_state == vm_power_state:
1100             LOG.debug('Synchronizing instance power state after lifecycle '
1101                       'event "%(event)s"; current vm_state: %(vm_state)s, '
1102                       'current task_state: %(task_state)s, current DB '
1103                       'power_state: %(db_power_state)s, VM power_state: '
1104                       '%(vm_power_state)s',
1105                       {'event': event.get_name(),
1106                        'vm_state': instance.vm_state,
1107                        'task_state': instance.task_state,
1108                        'db_power_state': instance.power_state,
1109                        'vm_power_state': vm_power_state},
1110                       instance_uuid=instance.uuid)
1111             self._sync_instance_power_state(context,
1112                                             instance,
1113                                             vm_power_state)
1114 
1115         # The following checks are for live migration. We want to activate
1116         # the port binding for the destination host before the live migration
1117         # is resumed on the destination host in order to reduce network
1118         # downtime. Otherwise the ports are bound to the destination host
1119         # in post_live_migration_at_destination.
1120         # TODO(danms): Explore options for using a different live migration
1121         # specific callback for this instead of piggy-backing on the
1122         # handle_lifecycle_event callback.
1123         if (instance.task_state == task_states.MIGRATING and
1124                 event_transition in migrate_finish_statuses):
1125             status = migrate_finish_statuses[event_transition]
1126             try:
1127                 migration = objects.Migration.get_by_instance_and_status(
1128                             context, instance.uuid, status)
1129                 LOG.debug('Binding ports to destination host: %s',
1130                           migration.dest_compute, instance=instance)
1131                 # For neutron, migrate_instance_start will activate the
1132                 # destination host port bindings, if there are any created by
1133                 # conductor before live migration started.
1134                 self.network_api.migrate_instance_start(
1135                     context, instance, migration)
1136             except exception.MigrationNotFoundByStatus:
1137                 LOG.warning("Unable to find migration record with status "
1138                             "'%s' for instance. Port binding will happen in "
1139                             "post live migration.", status, instance=instance)
1140 
1141     def handle_events(self, event):
1142         if isinstance(event, virtevent.LifecycleEvent):
1143             try:
1144                 self.handle_lifecycle_event(event)
1145             except exception.InstanceNotFound:
1146                 LOG.debug("Event %s arrived for non-existent instance. The "
1147                           "instance was probably deleted.", event)
1148         else:
1149             LOG.debug("Ignoring event %s", event)
1150 
1151     def init_virt_events(self):
1152         if CONF.workarounds.handle_virt_lifecycle_events:
1153             self.driver.register_event_listener(self.handle_events)
1154         else:
1155             # NOTE(mriedem): If the _sync_power_states periodic task is
1156             # disabled we should emit a warning in the logs.
1157             if CONF.sync_power_state_interval < 0:
1158                 LOG.warning('Instance lifecycle events from the compute '
1159                             'driver have been disabled. Note that lifecycle '
1160                             'changes to an instance outside of the compute '
1161                             'service will not be synchronized '
1162                             'automatically since the _sync_power_states '
1163                             'periodic task is also disabled.')
1164             else:
1165                 LOG.info('Instance lifecycle events from the compute '
1166                          'driver have been disabled. Note that lifecycle '
1167                          'changes to an instance outside of the compute '
1168                          'service will only be synchronized by the '
1169                          '_sync_power_states periodic task.')
1170 
1171     def init_host(self):
1172         """Initialization for a standalone compute service."""
1173 
1174         if CONF.pci.passthrough_whitelist:
1175             # Simply loading the PCI passthrough whitelist will do a bunch of
1176             # validation that would otherwise wait until the PciDevTracker is
1177             # constructed when updating available resources for the compute
1178             # node(s) in the resource tracker, effectively killing that task.
1179             # So load up the whitelist when starting the compute service to
1180             # flush any invalid configuration early so we can kill the service
1181             # if the configuration is wrong.
1182             whitelist.Whitelist(CONF.pci.passthrough_whitelist)
1183 
1184         nova.conf.neutron.register_dynamic_opts(CONF)
1185 
1186         self.driver.init_host(host=self.host)
1187         context = nova.context.get_admin_context()
1188         instances = objects.InstanceList.get_by_host(
1189             context, self.host, expected_attrs=['info_cache', 'metadata'])
1190 
1191         if CONF.defer_iptables_apply:
1192             self.driver.filter_defer_apply_on()
1193 
1194         self.init_virt_events()
1195 
1196         try:
1197             # checking that instance was not already evacuated to other host
1198             evacuated_instances = self._destroy_evacuated_instances(context)
1199 
1200             # Initialise instances on the host that are not evacuating
1201             for instance in instances:
1202                 if (not evacuated_instances or
1203                         instance.uuid not in evacuated_instances):
1204                     self._init_instance(context, instance)
1205 
1206         finally:
1207             if CONF.defer_iptables_apply:
1208                 self.driver.filter_defer_apply_off()
1209             if instances:
1210                 # We only send the instance info to the scheduler on startup
1211                 # if there is anything to send, otherwise this host might
1212                 # not be mapped yet in a cell and the scheduler may have
1213                 # issues dealing with the information. Later changes to
1214                 # instances on this host will update the scheduler, or the
1215                 # _sync_scheduler_instance_info periodic task will.
1216                 self._update_scheduler_instance_info(context, instances)
1217 
1218     def cleanup_host(self):
1219         self.driver.register_event_listener(None)
1220         self.instance_events.cancel_all_events()
1221         self.driver.cleanup_host(host=self.host)
1222         self._cleanup_live_migrations_in_pool()
1223 
1224     def _cleanup_live_migrations_in_pool(self):
1225         # Shutdown the pool so we don't get new requests.
1226         self._live_migration_executor.shutdown(wait=False)
1227         # For any queued migrations, cancel the migration and update
1228         # its status.
1229         for migration, future in self._waiting_live_migrations.values():
1230             # If we got here before the Future was submitted then we need
1231             # to move on since there isn't anything we can do.
1232             if future is None:
1233                 continue
1234             if future.cancel():
1235                 self._set_migration_status(migration, 'cancelled')
1236                 LOG.info('Successfully cancelled queued live migration.',
1237                          instance_uuid=migration.instance_uuid)
1238             else:
1239                 LOG.warning('Unable to cancel live migration.',
1240                             instance_uuid=migration.instance_uuid)
1241         self._waiting_live_migrations.clear()
1242 
1243     def pre_start_hook(self):
1244         """After the service is initialized, but before we fully bring
1245         the service up by listening on RPC queues, make sure to update
1246         our available resources (and indirectly our available nodes).
1247         """
1248         self.update_available_resource(nova.context.get_admin_context(),
1249                                        startup=True)
1250 
1251     def _get_power_state(self, context, instance):
1252         """Retrieve the power state for the given instance."""
1253         LOG.debug('Checking state', instance=instance)
1254         try:
1255             return self.driver.get_info(instance).state
1256         except exception.InstanceNotFound:
1257             return power_state.NOSTATE
1258 
1259     def get_console_topic(self, context):
1260         """Retrieves the console host for a project on this host.
1261 
1262         Currently this is just set in the flags for each compute host.
1263 
1264         """
1265         # TODO(mdragon): perhaps make this variable by console_type?
1266         return '%s.%s' % (console_rpcapi.RPC_TOPIC, CONF.console_host)
1267 
1268     @wrap_exception()
1269     def get_console_pool_info(self, context, console_type):
1270         return self.driver.get_console_pool_info(console_type)
1271 
1272     @wrap_exception()
1273     def refresh_instance_security_rules(self, context, instance):
1274         """Tell the virtualization driver to refresh security rules for
1275         an instance.
1276 
1277         Passes straight through to the virtualization driver.
1278 
1279         Synchronize the call because we may still be in the middle of
1280         creating the instance.
1281         """
1282         @utils.synchronized(instance.uuid)
1283         def _sync_refresh():
1284             try:
1285                 return self.driver.refresh_instance_security_rules(instance)
1286             except NotImplementedError:
1287                 LOG.debug('Hypervisor driver does not support '
1288                           'security groups.', instance=instance)
1289 
1290         return _sync_refresh()
1291 
1292     def _await_block_device_map_created(self, context, vol_id):
1293         # TODO(yamahata): creating volume simultaneously
1294         #                 reduces creation time?
1295         # TODO(yamahata): eliminate dumb polling
1296         start = time.time()
1297         retries = CONF.block_device_allocate_retries
1298         if retries < 0:
1299             LOG.warning("Treating negative config value (%(retries)s) for "
1300                         "'block_device_retries' as 0.",
1301                         {'retries': retries})
1302         # (1) treat  negative config value as 0
1303         # (2) the configured value is 0, one attempt should be made
1304         # (3) the configured value is > 0, then the total number attempts
1305         #      is (retries + 1)
1306         attempts = 1
1307         if retries >= 1:
1308             attempts = retries + 1
1309         for attempt in range(1, attempts + 1):
1310             volume = self.volume_api.get(context, vol_id)
1311             volume_status = volume['status']
1312             if volume_status not in ['creating', 'downloading']:
1313                 if volume_status == 'available':
1314                     return attempt
1315                 LOG.warning("Volume id: %(vol_id)s finished being "
1316                             "created but its status is %(vol_status)s.",
1317                             {'vol_id': vol_id,
1318                              'vol_status': volume_status})
1319                 break
1320             greenthread.sleep(CONF.block_device_allocate_retries_interval)
1321         raise exception.VolumeNotCreated(volume_id=vol_id,
1322                                          seconds=int(time.time() - start),
1323                                          attempts=attempt,
1324                                          volume_status=volume_status)
1325 
1326     def _decode_files(self, injected_files):
1327         """Base64 decode the list of files to inject."""
1328         if not injected_files:
1329             return []
1330 
1331         def _decode(f):
1332             path, contents = f
1333             # Py3 raises binascii.Error instead of TypeError as in Py27
1334             try:
1335                 decoded = base64.b64decode(contents)
1336                 return path, decoded
1337             except (TypeError, binascii.Error):
1338                 raise exception.Base64Exception(path=path)
1339 
1340         return [_decode(f) for f in injected_files]
1341 
1342     def _validate_instance_group_policy(self, context, instance,
1343                                         scheduler_hints):
1344         # NOTE(russellb) Instance group policy is enforced by the scheduler.
1345         # However, there is a race condition with the enforcement of
1346         # the policy.  Since more than one instance may be scheduled at the
1347         # same time, it's possible that more than one instance with an
1348         # anti-affinity policy may end up here.  It's also possible that
1349         # multiple instances with an affinity policy could end up on different
1350         # hosts.  This is a validation step to make sure that starting the
1351         # instance here doesn't violate the policy.
1352         group_hint = scheduler_hints.get('group')
1353         if not group_hint:
1354             return
1355 
1356         # The RequestSpec stores scheduler_hints as key=list pairs so we need
1357         # to check the type on the value and pull the single entry out. The
1358         # API request schema validates that the 'group' hint is a single value.
1359         if isinstance(group_hint, list):
1360             group_hint = group_hint[0]
1361 
1362         @utils.synchronized(group_hint)
1363         def _do_validation(context, instance, group_hint):
1364             group = objects.InstanceGroup.get_by_hint(context, group_hint)
1365             if group.policy and 'anti-affinity' == group.policy:
1366                 instances_uuids = objects.InstanceList.get_uuids_by_host(
1367                     context, self.host)
1368                 ins_on_host = set(instances_uuids)
1369                 members = set(group.members)
1370                 # Determine the set of instance group members on this host
1371                 # which are not the instance in question. This is used to
1372                 # determine how many other members from the same anti-affinity
1373                 # group can be on this host.
1374                 members_on_host = ins_on_host & members - set([instance.uuid])
1375                 rules = group.rules
1376                 if rules and 'max_server_per_host' in rules:
1377                     max_server = rules['max_server_per_host']
1378                 else:
1379                     max_server = 1
1380                 if len(members_on_host) >= max_server:
1381                     msg = _("Anti-affinity instance group policy "
1382                             "was violated.")
1383                     raise exception.RescheduledException(
1384                             instance_uuid=instance.uuid,
1385                             reason=msg)
1386             elif group.policy and 'affinity' == group.policy:
1387                 group_hosts = group.get_hosts(exclude=[instance.uuid])
1388                 if group_hosts and self.host not in group_hosts:
1389                     msg = _("Affinity instance group policy was violated.")
1390                     raise exception.RescheduledException(
1391                             instance_uuid=instance.uuid,
1392                             reason=msg)
1393 
1394         if not CONF.workarounds.disable_group_policy_check_upcall:
1395             _do_validation(context, instance, group_hint)
1396 
1397     def _log_original_error(self, exc_info, instance_uuid):
1398         LOG.error('Error: %s', exc_info[1], instance_uuid=instance_uuid,
1399                   exc_info=exc_info)
1400 
1401     def _reschedule(self, context, request_spec, filter_properties,
1402             instance, reschedule_method, method_args, task_state,
1403             exc_info=None, host_list=None):
1404         """Attempt to re-schedule a compute operation."""
1405 
1406         instance_uuid = instance.uuid
1407         retry = filter_properties.get('retry')
1408         if not retry:
1409             # no retry information, do not reschedule.
1410             LOG.debug("Retry info not present, will not reschedule",
1411                       instance_uuid=instance_uuid)
1412             return
1413 
1414         if not request_spec:
1415             LOG.debug("No request spec, will not reschedule",
1416                       instance_uuid=instance_uuid)
1417             return
1418 
1419         LOG.debug("Re-scheduling %(method)s: attempt %(num)d",
1420                   {'method': reschedule_method.__name__,
1421                    'num': retry['num_attempts']}, instance_uuid=instance_uuid)
1422 
1423         # reset the task state:
1424         self._instance_update(context, instance, task_state=task_state)
1425 
1426         if exc_info:
1427             # stringify to avoid circular ref problem in json serialization:
1428             retry['exc'] = traceback.format_exception_only(exc_info[0],
1429                                     exc_info[1])
1430 
1431         reschedule_method(context, *method_args, host_list=host_list)
1432         return True
1433 
1434     @periodic_task.periodic_task
1435     def _check_instance_build_time(self, context):
1436         """Ensure that instances are not stuck in build."""
1437         timeout = CONF.instance_build_timeout
1438         if timeout == 0:
1439             return
1440 
1441         filters = {'vm_state': vm_states.BUILDING,
1442                    'host': self.host}
1443 
1444         building_insts = objects.InstanceList.get_by_filters(context,
1445                            filters, expected_attrs=[], use_slave=True)
1446 
1447         for instance in building_insts:
1448             if timeutils.is_older_than(instance.created_at, timeout):
1449                 self._set_instance_obj_error_state(context, instance)
1450                 LOG.warning("Instance build timed out. Set to error "
1451                             "state.", instance=instance)
1452 
1453     def _check_instance_exists(self, context, instance):
1454         """Ensure an instance with the same name is not already present."""
1455         if self.driver.instance_exists(instance):
1456             raise exception.InstanceExists(name=instance.name)
1457 
1458     def _allocate_network_async(self, context, instance, requested_networks,
1459                                 macs, security_groups, is_vpn):
1460         """Method used to allocate networks in the background.
1461 
1462         Broken out for testing.
1463         """
1464         # First check to see if we're specifically not supposed to allocate
1465         # networks because if so, we can exit early.
1466         if requested_networks and requested_networks.no_allocate:
1467             LOG.debug("Not allocating networking since 'none' was specified.",
1468                       instance=instance)
1469             return network_model.NetworkInfo([])
1470 
1471         LOG.debug("Allocating IP information in the background.",
1472                   instance=instance)
1473         retries = CONF.network_allocate_retries
1474         attempts = retries + 1
1475         retry_time = 1
1476         bind_host_id = self.driver.network_binding_host_id(context, instance)
1477         for attempt in range(1, attempts + 1):
1478             try:
1479                 nwinfo = self.network_api.allocate_for_instance(
1480                         context, instance, vpn=is_vpn,
1481                         requested_networks=requested_networks,
1482                         macs=macs,
1483                         security_groups=security_groups,
1484                         bind_host_id=bind_host_id)
1485                 LOG.debug('Instance network_info: |%s|', nwinfo,
1486                           instance=instance)
1487                 instance.system_metadata['network_allocated'] = 'True'
1488                 # NOTE(JoshNang) do not save the instance here, as it can cause
1489                 # races. The caller shares a reference to instance and waits
1490                 # for this async greenthread to finish before calling
1491                 # instance.save().
1492                 return nwinfo
1493             except Exception:
1494                 exc_info = sys.exc_info()
1495                 log_info = {'attempt': attempt,
1496                             'attempts': attempts}
1497                 if attempt == attempts:
1498                     LOG.exception('Instance failed network setup '
1499                                   'after %(attempts)d attempt(s)',
1500                                   log_info)
1501                     six.reraise(*exc_info)
1502                 LOG.warning('Instance failed network setup '
1503                             '(attempt %(attempt)d of %(attempts)d)',
1504                             log_info, instance=instance)
1505                 time.sleep(retry_time)
1506                 retry_time *= 2
1507                 if retry_time > 30:
1508                     retry_time = 30
1509         # Not reached.
1510 
1511     def _build_networks_for_instance(self, context, instance,
1512             requested_networks, security_groups):
1513 
1514         # If we're here from a reschedule the network may already be allocated.
1515         if strutils.bool_from_string(
1516                 instance.system_metadata.get('network_allocated', 'False')):
1517             # NOTE(alex_xu): The network_allocated is True means the network
1518             # resource already allocated at previous scheduling, and the
1519             # network setup is cleanup at previous. After rescheduling, the
1520             # network resource need setup on the new host.
1521             self.network_api.setup_instance_network_on_host(
1522                 context, instance, instance.host)
1523             return self.network_api.get_instance_nw_info(context, instance)
1524 
1525         if not self.is_neutron_security_groups:
1526             security_groups = []
1527 
1528         macs = self.driver.macs_for_instance(instance)
1529         network_info = self._allocate_network(context, instance,
1530                 requested_networks, macs, security_groups)
1531 
1532         return network_info
1533 
1534     def _allocate_network(self, context, instance, requested_networks, macs,
1535                           security_groups):
1536         """Start network allocation asynchronously.  Return an instance
1537         of NetworkInfoAsyncWrapper that can be used to retrieve the
1538         allocated networks when the operation has finished.
1539         """
1540         # NOTE(comstud): Since we're allocating networks asynchronously,
1541         # this task state has little meaning, as we won't be in this
1542         # state for very long.
1543         instance.vm_state = vm_states.BUILDING
1544         instance.task_state = task_states.NETWORKING
1545         instance.save(expected_task_state=[None])
1546 
1547         is_vpn = False
1548         return network_model.NetworkInfoAsyncWrapper(
1549                 self._allocate_network_async, context, instance,
1550                 requested_networks, macs, security_groups, is_vpn)
1551 
1552     def _default_root_device_name(self, instance, image_meta, root_bdm):
1553         try:
1554             return self.driver.default_root_device_name(instance,
1555                                                         image_meta,
1556                                                         root_bdm)
1557         except NotImplementedError:
1558             return compute_utils.get_next_device_name(instance, [])
1559 
1560     def _default_device_names_for_instance(self, instance,
1561                                            root_device_name,
1562                                            *block_device_lists):
1563         try:
1564             self.driver.default_device_names_for_instance(instance,
1565                                                           root_device_name,
1566                                                           *block_device_lists)
1567         except NotImplementedError:
1568             compute_utils.default_device_names_for_instance(
1569                 instance, root_device_name, *block_device_lists)
1570 
1571     def _get_device_name_for_instance(self, instance, bdms, block_device_obj):
1572         # NOTE(ndipanov): Copy obj to avoid changing the original
1573         block_device_obj = block_device_obj.obj_clone()
1574         try:
1575             return self.driver.get_device_name_for_instance(
1576                 instance, bdms, block_device_obj)
1577         except NotImplementedError:
1578             return compute_utils.get_device_name_for_instance(
1579                 instance, bdms, block_device_obj.get("device_name"))
1580 
1581     def _default_block_device_names(self, instance, image_meta, block_devices):
1582         """Verify that all the devices have the device_name set. If not,
1583         provide a default name.
1584 
1585         It also ensures that there is a root_device_name and is set to the
1586         first block device in the boot sequence (boot_index=0).
1587         """
1588         root_bdm = block_device.get_root_bdm(block_devices)
1589         if not root_bdm:
1590             return
1591 
1592         # Get the root_device_name from the root BDM or the instance
1593         root_device_name = None
1594         update_root_bdm = False
1595 
1596         if root_bdm.device_name:
1597             root_device_name = root_bdm.device_name
1598             instance.root_device_name = root_device_name
1599         elif instance.root_device_name:
1600             root_device_name = instance.root_device_name
1601             root_bdm.device_name = root_device_name
1602             update_root_bdm = True
1603         else:
1604             root_device_name = self._default_root_device_name(instance,
1605                                                               image_meta,
1606                                                               root_bdm)
1607 
1608             instance.root_device_name = root_device_name
1609             root_bdm.device_name = root_device_name
1610             update_root_bdm = True
1611 
1612         if update_root_bdm:
1613             root_bdm.save()
1614 
1615         ephemerals = list(filter(block_device.new_format_is_ephemeral,
1616                             block_devices))
1617         swap = list(filter(block_device.new_format_is_swap,
1618                       block_devices))
1619         block_device_mapping = list(filter(
1620               driver_block_device.is_block_device_mapping, block_devices))
1621 
1622         self._default_device_names_for_instance(instance,
1623                                                 root_device_name,
1624                                                 ephemerals,
1625                                                 swap,
1626                                                 block_device_mapping)
1627 
1628     def _block_device_info_to_legacy(self, block_device_info):
1629         """Convert BDI to the old format for drivers that need it."""
1630 
1631         if self.use_legacy_block_device_info:
1632             ephemerals = driver_block_device.legacy_block_devices(
1633                 driver.block_device_info_get_ephemerals(block_device_info))
1634             mapping = driver_block_device.legacy_block_devices(
1635                 driver.block_device_info_get_mapping(block_device_info))
1636             swap = block_device_info['swap']
1637             if swap:
1638                 swap = swap.legacy()
1639 
1640             block_device_info.update({
1641                 'ephemerals': ephemerals,
1642                 'swap': swap,
1643                 'block_device_mapping': mapping})
1644 
1645     def _add_missing_dev_names(self, bdms, instance):
1646         for bdm in bdms:
1647             if bdm.device_name is not None:
1648                 continue
1649 
1650             device_name = self._get_device_name_for_instance(instance,
1651                                                              bdms, bdm)
1652             values = {'device_name': device_name}
1653             bdm.update(values)
1654             bdm.save()
1655 
1656     def _prep_block_device(self, context, instance, bdms):
1657         """Set up the block device for an instance with error logging."""
1658         try:
1659             self._add_missing_dev_names(bdms, instance)
1660             block_device_info = driver.get_block_device_info(instance, bdms)
1661             mapping = driver.block_device_info_get_mapping(block_device_info)
1662             driver_block_device.attach_block_devices(
1663                 mapping, context, instance, self.volume_api, self.driver,
1664                 wait_func=self._await_block_device_map_created)
1665 
1666             self._block_device_info_to_legacy(block_device_info)
1667             return block_device_info
1668 
1669         except exception.OverQuota as e:
1670             LOG.warning('Failed to create block device for instance due'
1671                         ' to exceeding volume related resource quota.'
1672                         ' Error: %s', e.message, instance=instance)
1673             raise
1674 
1675         except Exception as ex:
1676             LOG.exception('Instance failed block device setup',
1677                           instance=instance)
1678             # InvalidBDM will eventually result in a BuildAbortException when
1679             # booting from volume, and will be recorded as an instance fault.
1680             # Maintain the original exception message which most likely has
1681             # useful details which the standard InvalidBDM error message lacks.
1682             raise exception.InvalidBDM(six.text_type(ex))
1683 
1684     def _update_instance_after_spawn(self, context, instance):
1685         instance.power_state = self._get_power_state(context, instance)
1686         instance.vm_state = vm_states.ACTIVE
1687         instance.task_state = None
1688         instance.launched_at = timeutils.utcnow()
1689         configdrive.update_instance(instance)
1690 
1691     def _update_scheduler_instance_info(self, context, instance):
1692         """Sends an InstanceList with created or updated Instance objects to
1693         the Scheduler client.
1694 
1695         In the case of init_host, the value passed will already be an
1696         InstanceList. Other calls will send individual Instance objects that
1697         have been created or resized. In this case, we create an InstanceList
1698         object containing that Instance.
1699         """
1700         if not self.send_instance_updates:
1701             return
1702         if isinstance(instance, obj_instance.Instance):
1703             instance = objects.InstanceList(objects=[instance])
1704         context = context.elevated()
1705         self.scheduler_client.update_instance_info(context, self.host,
1706                                                    instance)
1707 
1708     def _delete_scheduler_instance_info(self, context, instance_uuid):
1709         """Sends the uuid of the deleted Instance to the Scheduler client."""
1710         if not self.send_instance_updates:
1711             return
1712         context = context.elevated()
1713         self.scheduler_client.delete_instance_info(context, self.host,
1714                                                    instance_uuid)
1715 
1716     @periodic_task.periodic_task(spacing=CONF.scheduler_instance_sync_interval)
1717     def _sync_scheduler_instance_info(self, context):
1718         if not self.send_instance_updates:
1719             return
1720         context = context.elevated()
1721         instances = objects.InstanceList.get_by_host(context, self.host,
1722                                                      expected_attrs=[],
1723                                                      use_slave=True)
1724         uuids = [instance.uuid for instance in instances]
1725         self.scheduler_client.sync_instance_info(context, self.host, uuids)
1726 
1727     def _notify_about_instance_usage(self, context, instance, event_suffix,
1728                                      network_info=None, extra_usage_info=None,
1729                                      fault=None):
1730         compute_utils.notify_about_instance_usage(
1731             self.notifier, context, instance, event_suffix,
1732             network_info=network_info,
1733             extra_usage_info=extra_usage_info, fault=fault)
1734 
1735     def _deallocate_network(self, context, instance,
1736                             requested_networks=None):
1737         # If we were told not to allocate networks let's save ourselves
1738         # the trouble of calling the network API.
1739         if requested_networks and requested_networks.no_allocate:
1740             LOG.debug("Skipping network deallocation for instance since "
1741                       "networking was not requested.", instance=instance)
1742             return
1743 
1744         LOG.debug('Deallocating network for instance', instance=instance)
1745         with timeutils.StopWatch() as timer:
1746             self.network_api.deallocate_for_instance(
1747                 context, instance, requested_networks=requested_networks)
1748         # nova-network does an rpc call so we're OK tracking time spent here
1749         LOG.info('Took %0.2f seconds to deallocate network for instance.',
1750                  timer.elapsed(), instance=instance)
1751 
1752     def _get_instance_block_device_info(self, context, instance,
1753                                         refresh_conn_info=False,
1754                                         bdms=None):
1755         """Transform block devices to the driver block_device format."""
1756 
1757         if not bdms:
1758             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
1759                     context, instance.uuid)
1760         block_device_info = driver.get_block_device_info(instance, bdms)
1761 
1762         if not refresh_conn_info:
1763             # if the block_device_mapping has no value in connection_info
1764             # (returned as None), don't include in the mapping
1765             block_device_info['block_device_mapping'] = [
1766                 bdm for bdm in driver.block_device_info_get_mapping(
1767                                     block_device_info)
1768                 if bdm.get('connection_info')]
1769         else:
1770             driver_block_device.refresh_conn_infos(
1771                 driver.block_device_info_get_mapping(block_device_info),
1772                 context, instance, self.volume_api, self.driver)
1773 
1774         self._block_device_info_to_legacy(block_device_info)
1775 
1776         return block_device_info
1777 
1778     def _build_failed(self, node):
1779         if CONF.compute.consecutive_build_service_disable_threshold:
1780             rt = self._get_resource_tracker()
1781             # NOTE(danms): Update our counter, but wait for the next
1782             # update_available_resource() periodic to flush it to the DB
1783             rt.build_failed(node)
1784 
1785     def _build_succeeded(self, node):
1786         rt = self._get_resource_tracker()
1787         rt.build_succeeded(node)
1788 
1789     @wrap_exception()
1790     @reverts_task_state
1791     @wrap_instance_fault
1792     def build_and_run_instance(self, context, instance, image, request_spec,
1793                      filter_properties, admin_password=None,
1794                      injected_files=None, requested_networks=None,
1795                      security_groups=None, block_device_mapping=None,
1796                      node=None, limits=None, host_list=None):
1797 
1798         @utils.synchronized(instance.uuid)
1799         def _locked_do_build_and_run_instance(*args, **kwargs):
1800             # NOTE(danms): We grab the semaphore with the instance uuid
1801             # locked because we could wait in line to build this instance
1802             # for a while and we want to make sure that nothing else tries
1803             # to do anything with this instance while we wait.
1804             with self._build_semaphore:
1805                 try:
1806                     result = self._do_build_and_run_instance(*args, **kwargs)
1807                 except Exception:
1808                     # NOTE(mriedem): This should really only happen if
1809                     # _decode_files in _do_build_and_run_instance fails, and
1810                     # that's before a guest is spawned so it's OK to remove
1811                     # allocations for the instance for this node from Placement
1812                     # below as there is no guest consuming resources anyway.
1813                     # The _decode_files case could be handled more specifically
1814                     # but that's left for another day.
1815                     result = build_results.FAILED
1816                     raise
1817                 finally:
1818                     if result == build_results.FAILED:
1819                         # Remove the allocation records from Placement for the
1820                         # instance if the build failed. The instance.host is
1821                         # likely set to None in _do_build_and_run_instance
1822                         # which means if the user deletes the instance, it
1823                         # will be deleted in the API, not the compute service.
1824                         # Setting the instance.host to None in
1825                         # _do_build_and_run_instance means that the
1826                         # ResourceTracker will no longer consider this instance
1827                         # to be claiming resources against it, so we want to
1828                         # reflect that same thing in Placement.  No need to
1829                         # call this for a reschedule, as the allocations will
1830                         # have already been removed in
1831                         # self._do_build_and_run_instance().
1832                         self._delete_allocation_for_instance(context,
1833                                                              instance.uuid)
1834 
1835                     if result in (build_results.FAILED,
1836                                   build_results.RESCHEDULED):
1837                         self._build_failed(node)
1838                     else:
1839                         self._build_succeeded(node)
1840 
1841         # NOTE(danms): We spawn here to return the RPC worker thread back to
1842         # the pool. Since what follows could take a really long time, we don't
1843         # want to tie up RPC workers.
1844         utils.spawn_n(_locked_do_build_and_run_instance,
1845                       context, instance, image, request_spec,
1846                       filter_properties, admin_password, injected_files,
1847                       requested_networks, security_groups,
1848                       block_device_mapping, node, limits, host_list)
1849 
1850     def _delete_allocation_for_instance(self, context, instance_uuid):
1851         rt = self._get_resource_tracker()
1852         rt.reportclient.delete_allocation_for_instance(context, instance_uuid)
1853 
1854     def _check_device_tagging(self, requested_networks, block_device_mapping):
1855         tagging_requested = False
1856         if requested_networks:
1857             for net in requested_networks:
1858                 if 'tag' in net and net.tag is not None:
1859                     tagging_requested = True
1860                     break
1861         if block_device_mapping and not tagging_requested:
1862             for bdm in block_device_mapping:
1863                 if 'tag' in bdm and bdm.tag is not None:
1864                     tagging_requested = True
1865                     break
1866         if (tagging_requested and
1867                 not self.driver.capabilities.get('supports_device_tagging',
1868                                                  False)):
1869             raise exception.BuildAbortException('Attempt to boot guest with '
1870                                                 'tagged devices on host that '
1871                                                 'does not support tagging.')
1872 
1873     def _check_trusted_certs(self, instance):
1874         if (instance.trusted_certs and
1875                 not self.driver.capabilities.get('supports_trusted_certs',
1876                                                  False)):
1877             raise exception.BuildAbortException(
1878                 'Trusted image certificates provided on host that does not '
1879                 'support certificate validation.')
1880 
1881     @hooks.add_hook('build_instance')
1882     @wrap_exception()
1883     @reverts_task_state
1884     @wrap_instance_event(prefix='compute')
1885     @wrap_instance_fault
1886     def _do_build_and_run_instance(self, context, instance, image,
1887             request_spec, filter_properties, admin_password, injected_files,
1888             requested_networks, security_groups, block_device_mapping,
1889             node=None, limits=None, host_list=None):
1890 
1891         try:
1892             LOG.debug('Starting instance...', instance=instance)
1893             instance.vm_state = vm_states.BUILDING
1894             instance.task_state = None
1895             instance.save(expected_task_state=
1896                     (task_states.SCHEDULING, None))
1897         except exception.InstanceNotFound:
1898             msg = 'Instance disappeared before build.'
1899             LOG.debug(msg, instance=instance)
1900             return build_results.FAILED
1901         except exception.UnexpectedTaskStateError as e:
1902             LOG.debug(e.format_message(), instance=instance)
1903             return build_results.FAILED
1904 
1905         # b64 decode the files to inject:
1906         decoded_files = self._decode_files(injected_files)
1907 
1908         if limits is None:
1909             limits = {}
1910 
1911         if node is None:
1912             node = self._get_nodename(instance, refresh=True)
1913 
1914         try:
1915             with timeutils.StopWatch() as timer:
1916                 self._build_and_run_instance(context, instance, image,
1917                         decoded_files, admin_password, requested_networks,
1918                         security_groups, block_device_mapping, node, limits,
1919                         filter_properties, request_spec)
1920             LOG.info('Took %0.2f seconds to build instance.',
1921                      timer.elapsed(), instance=instance)
1922             return build_results.ACTIVE
1923         except exception.RescheduledException as e:
1924             retry = filter_properties.get('retry')
1925             if not retry:
1926                 # no retry information, do not reschedule.
1927                 LOG.debug("Retry info not present, will not reschedule",
1928                     instance=instance)
1929                 self._cleanup_allocated_networks(context, instance,
1930                     requested_networks)
1931                 self._cleanup_volumes(context, instance,
1932                     block_device_mapping, raise_exc=False)
1933                 compute_utils.add_instance_fault_from_exc(context,
1934                         instance, e, sys.exc_info(),
1935                         fault_message=e.kwargs['reason'])
1936                 self._nil_out_instance_obj_host_and_node(instance)
1937                 self._set_instance_obj_error_state(context, instance,
1938                                                    clean_task_state=True)
1939                 return build_results.FAILED
1940             LOG.debug(e.format_message(), instance=instance)
1941             # This will be used for logging the exception
1942             retry['exc'] = traceback.format_exception(*sys.exc_info())
1943             # This will be used for setting the instance fault message
1944             retry['exc_reason'] = e.kwargs['reason']
1945             # NOTE(comstud): Deallocate networks if the driver wants
1946             # us to do so.
1947             # NOTE(mriedem): Always deallocate networking when using Neutron.
1948             # This is to unbind any ports that the user supplied in the server
1949             # create request, or delete any ports that nova created which were
1950             # meant to be bound to this host. This check intentionally bypasses
1951             # the result of deallocate_networks_on_reschedule because the
1952             # default value in the driver is False, but that method was really
1953             # only meant for Ironic and should be removed when nova-network is
1954             # removed (since is_neutron() will then always be True).
1955             # NOTE(vladikr): SR-IOV ports should be deallocated to
1956             # allow new sriov pci devices to be allocated on a new host.
1957             # Otherwise, if devices with pci addresses are already allocated
1958             # on the destination host, the instance will fail to spawn.
1959             # info_cache.network_info should be present at this stage.
1960             if (self.driver.deallocate_networks_on_reschedule(instance) or
1961                 utils.is_neutron() or
1962                 self.deallocate_sriov_ports_on_reschedule(instance)):
1963                 self._cleanup_allocated_networks(context, instance,
1964                         requested_networks)
1965             else:
1966                 # NOTE(alex_xu): Network already allocated and we don't
1967                 # want to deallocate them before rescheduling. But we need
1968                 # to cleanup those network resources setup on this host before
1969                 # rescheduling.
1970                 self.network_api.cleanup_instance_network_on_host(
1971                     context, instance, self.host)
1972 
1973             self._nil_out_instance_obj_host_and_node(instance)
1974             instance.task_state = task_states.SCHEDULING
1975             instance.save()
1976             # The instance will have already claimed resources from this host
1977             # before this build was attempted. Now that it has failed, we need
1978             # to unclaim those resources before casting to the conductor, so
1979             # that if there are alternate hosts available for a retry, it can
1980             # claim resources on that new host for the instance.
1981             self._delete_allocation_for_instance(context, instance.uuid)
1982 
1983             self.compute_task_api.build_instances(context, [instance],
1984                     image, filter_properties, admin_password,
1985                     injected_files, requested_networks, security_groups,
1986                     block_device_mapping, request_spec=request_spec,
1987                     host_lists=[host_list])
1988             return build_results.RESCHEDULED
1989         except (exception.InstanceNotFound,
1990                 exception.UnexpectedDeletingTaskStateError):
1991             msg = 'Instance disappeared during build.'
1992             LOG.debug(msg, instance=instance)
1993             self._cleanup_allocated_networks(context, instance,
1994                     requested_networks)
1995             return build_results.FAILED
1996         except Exception as e:
1997             if isinstance(e, exception.BuildAbortException):
1998                 LOG.error(e.format_message(), instance=instance)
1999             else:
2000                 # Should not reach here.
2001                 LOG.exception('Unexpected build failure, not rescheduling '
2002                               'build.', instance=instance)
2003             self._cleanup_allocated_networks(context, instance,
2004                     requested_networks)
2005             self._cleanup_volumes(context, instance,
2006                     block_device_mapping, raise_exc=False)
2007             compute_utils.add_instance_fault_from_exc(context, instance,
2008                     e, sys.exc_info())
2009             self._nil_out_instance_obj_host_and_node(instance)
2010             self._set_instance_obj_error_state(context, instance,
2011                                                clean_task_state=True)
2012             return build_results.FAILED
2013 
2014     def deallocate_sriov_ports_on_reschedule(self, instance):
2015         """Determine if networks are needed to be deallocated before reschedule
2016 
2017         Check the cached network info for any assigned SR-IOV ports.
2018         SR-IOV ports should be deallocated prior to rescheduling
2019         in order to allow new sriov pci devices to be allocated on a new host.
2020         """
2021         info_cache = instance.info_cache
2022 
2023         def _has_sriov_port(vif):
2024             return vif['vnic_type'] in network_model.VNIC_TYPES_SRIOV
2025 
2026         if (info_cache and info_cache.network_info):
2027             for vif in info_cache.network_info:
2028                 if _has_sriov_port(vif):
2029                     return True
2030         return False
2031 
2032     @staticmethod
2033     def _get_scheduler_hints(filter_properties, request_spec=None):
2034         """Helper method to get scheduler hints.
2035 
2036         This method prefers to get the hints out of the request spec, but that
2037         might not be provided. Conductor will pass request_spec down to the
2038         first compute chosen for a build but older computes will not pass
2039         the request_spec to conductor's build_instances method for a
2040         a reschedule, so if we're on a host via a retry, request_spec may not
2041         be provided so we need to fallback to use the filter_properties
2042         to get scheduler hints.
2043         """
2044         hints = {}
2045         if request_spec is not None and 'scheduler_hints' in request_spec:
2046             hints = request_spec.scheduler_hints
2047         if not hints:
2048             hints = filter_properties.get('scheduler_hints') or {}
2049         return hints
2050 
2051     def _build_and_run_instance(self, context, instance, image, injected_files,
2052             admin_password, requested_networks, security_groups,
2053             block_device_mapping, node, limits, filter_properties,
2054             request_spec=None):
2055 
2056         image_name = image.get('name')
2057         self._notify_about_instance_usage(context, instance, 'create.start',
2058                 extra_usage_info={'image_name': image_name})
2059         compute_utils.notify_about_instance_create(
2060             context, instance, self.host,
2061             phase=fields.NotificationPhase.START,
2062             bdms=block_device_mapping)
2063 
2064         # NOTE(mikal): cache the keystone roles associated with the instance
2065         # at boot time for later reference
2066         instance.system_metadata.update(
2067             {'boot_roles': ','.join(context.roles)})
2068 
2069         self._check_device_tagging(requested_networks, block_device_mapping)
2070         self._check_trusted_certs(instance)
2071 
2072         try:
2073             scheduler_hints = self._get_scheduler_hints(filter_properties,
2074                                                         request_spec)
2075             rt = self._get_resource_tracker()
2076             with rt.instance_claim(context, instance, node, limits):
2077                 # NOTE(russellb) It's important that this validation be done
2078                 # *after* the resource tracker instance claim, as that is where
2079                 # the host is set on the instance.
2080                 self._validate_instance_group_policy(context, instance,
2081                                                      scheduler_hints)
2082                 image_meta = objects.ImageMeta.from_dict(image)
2083                 with self._build_resources(context, instance,
2084                         requested_networks, security_groups, image_meta,
2085                         block_device_mapping) as resources:
2086                     instance.vm_state = vm_states.BUILDING
2087                     instance.task_state = task_states.SPAWNING
2088                     # NOTE(JoshNang) This also saves the changes to the
2089                     # instance from _allocate_network_async, as they aren't
2090                     # saved in that function to prevent races.
2091                     instance.save(expected_task_state=
2092                             task_states.BLOCK_DEVICE_MAPPING)
2093                     block_device_info = resources['block_device_info']
2094                     network_info = resources['network_info']
2095                     allocs = resources['allocations']
2096                     LOG.debug('Start spawning the instance on the hypervisor.',
2097                               instance=instance)
2098                     with timeutils.StopWatch() as timer:
2099                         self.driver.spawn(context, instance, image_meta,
2100                                           injected_files, admin_password,
2101                                           allocs, network_info=network_info,
2102                                           block_device_info=block_device_info)
2103                     LOG.info('Took %0.2f seconds to spawn the instance on '
2104                              'the hypervisor.', timer.elapsed(),
2105                              instance=instance)
2106         except (exception.InstanceNotFound,
2107                 exception.UnexpectedDeletingTaskStateError) as e:
2108             with excutils.save_and_reraise_exception():
2109                 self._notify_about_instance_usage(context, instance,
2110                     'create.error', fault=e)
2111                 tb = traceback.format_exc()
2112                 compute_utils.notify_about_instance_create(
2113                     context, instance, self.host,
2114                     phase=fields.NotificationPhase.ERROR, exception=e,
2115                     bdms=block_device_mapping, tb=tb)
2116         except exception.ComputeResourcesUnavailable as e:
2117             LOG.debug(e.format_message(), instance=instance)
2118             self._notify_about_instance_usage(context, instance,
2119                     'create.error', fault=e)
2120             tb = traceback.format_exc()
2121             compute_utils.notify_about_instance_create(
2122                     context, instance, self.host,
2123                     phase=fields.NotificationPhase.ERROR, exception=e,
2124                     bdms=block_device_mapping, tb=tb)
2125             raise exception.RescheduledException(
2126                     instance_uuid=instance.uuid, reason=e.format_message())
2127         except exception.BuildAbortException as e:
2128             with excutils.save_and_reraise_exception():
2129                 LOG.debug(e.format_message(), instance=instance)
2130                 self._notify_about_instance_usage(context, instance,
2131                     'create.error', fault=e)
2132                 tb = traceback.format_exc()
2133                 compute_utils.notify_about_instance_create(
2134                     context, instance, self.host,
2135                     phase=fields.NotificationPhase.ERROR, exception=e,
2136                     bdms=block_device_mapping, tb=tb)
2137         except (exception.FixedIpLimitExceeded,
2138                 exception.NoMoreNetworks, exception.NoMoreFixedIps) as e:
2139             LOG.warning('No more network or fixed IP to be allocated',
2140                         instance=instance)
2141             self._notify_about_instance_usage(context, instance,
2142                     'create.error', fault=e)
2143             tb = traceback.format_exc()
2144             compute_utils.notify_about_instance_create(
2145                     context, instance, self.host,
2146                     phase=fields.NotificationPhase.ERROR, exception=e,
2147                     bdms=block_device_mapping, tb=tb)
2148             msg = _('Failed to allocate the network(s) with error %s, '
2149                     'not rescheduling.') % e.format_message()
2150             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2151                     reason=msg)
2152         except (exception.VirtualInterfaceCreateException,
2153                 exception.VirtualInterfaceMacAddressException,
2154                 exception.FixedIpInvalidOnHost,
2155                 exception.UnableToAutoAllocateNetwork) as e:
2156             LOG.exception('Failed to allocate network(s)',
2157                           instance=instance)
2158             self._notify_about_instance_usage(context, instance,
2159                     'create.error', fault=e)
2160             tb = traceback.format_exc()
2161             compute_utils.notify_about_instance_create(
2162                     context, instance, self.host,
2163                     phase=fields.NotificationPhase.ERROR, exception=e,
2164                     bdms=block_device_mapping, tb=tb)
2165             msg = _('Failed to allocate the network(s), not rescheduling.')
2166             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2167                     reason=msg)
2168         except (exception.FlavorDiskTooSmall,
2169                 exception.FlavorMemoryTooSmall,
2170                 exception.ImageNotActive,
2171                 exception.ImageUnacceptable,
2172                 exception.InvalidDiskInfo,
2173                 exception.InvalidDiskFormat,
2174                 cursive_exception.SignatureVerificationError,
2175                 exception.CertificateValidationFailed,
2176                 exception.VolumeEncryptionNotSupported,
2177                 exception.InvalidInput,
2178                 # TODO(mriedem): We should be validating RequestedVRamTooHigh
2179                 # in the API during server create and rebuild.
2180                 exception.RequestedVRamTooHigh) as e:
2181             self._notify_about_instance_usage(context, instance,
2182                     'create.error', fault=e)
2183             tb = traceback.format_exc()
2184             compute_utils.notify_about_instance_create(
2185                     context, instance, self.host,
2186                     phase=fields.NotificationPhase.ERROR, exception=e,
2187                     bdms=block_device_mapping, tb=tb)
2188             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2189                     reason=e.format_message())
2190         except Exception as e:
2191             self._notify_about_instance_usage(context, instance,
2192                     'create.error', fault=e)
2193             tb = traceback.format_exc()
2194             compute_utils.notify_about_instance_create(
2195                     context, instance, self.host,
2196                     phase=fields.NotificationPhase.ERROR, exception=e,
2197                     bdms=block_device_mapping, tb=tb)
2198             raise exception.RescheduledException(
2199                     instance_uuid=instance.uuid, reason=six.text_type(e))
2200 
2201         # NOTE(alaski): This is only useful during reschedules, remove it now.
2202         instance.system_metadata.pop('network_allocated', None)
2203 
2204         # If CONF.default_access_ip_network_name is set, grab the
2205         # corresponding network and set the access ip values accordingly.
2206         network_name = CONF.default_access_ip_network_name
2207         if (network_name and not instance.access_ip_v4 and
2208                 not instance.access_ip_v6):
2209             # Note that when there are multiple ips to choose from, an
2210             # arbitrary one will be chosen.
2211             for vif in network_info:
2212                 if vif['network']['label'] == network_name:
2213                     for ip in vif.fixed_ips():
2214                         if not instance.access_ip_v4 and ip['version'] == 4:
2215                             instance.access_ip_v4 = ip['address']
2216                         if not instance.access_ip_v6 and ip['version'] == 6:
2217                             instance.access_ip_v6 = ip['address']
2218                     break
2219 
2220         self._update_instance_after_spawn(context, instance)
2221 
2222         try:
2223             instance.save(expected_task_state=task_states.SPAWNING)
2224         except (exception.InstanceNotFound,
2225                 exception.UnexpectedDeletingTaskStateError) as e:
2226             with excutils.save_and_reraise_exception():
2227                 self._notify_about_instance_usage(context, instance,
2228                     'create.error', fault=e)
2229                 tb = traceback.format_exc()
2230                 compute_utils.notify_about_instance_create(
2231                     context, instance, self.host,
2232                     phase=fields.NotificationPhase.ERROR, exception=e,
2233                     bdms=block_device_mapping, tb=tb)
2234 
2235         self._update_scheduler_instance_info(context, instance)
2236         self._notify_about_instance_usage(context, instance, 'create.end',
2237                 extra_usage_info={'message': _('Success')},
2238                 network_info=network_info)
2239         compute_utils.notify_about_instance_create(context, instance,
2240                 self.host, phase=fields.NotificationPhase.END,
2241                 bdms=block_device_mapping)
2242 
2243     @contextlib.contextmanager
2244     def _build_resources(self, context, instance, requested_networks,
2245                          security_groups, image_meta, block_device_mapping):
2246         resources = {}
2247         network_info = None
2248         try:
2249             LOG.debug('Start building networks asynchronously for instance.',
2250                       instance=instance)
2251             network_info = self._build_networks_for_instance(context, instance,
2252                     requested_networks, security_groups)
2253             resources['network_info'] = network_info
2254         except (exception.InstanceNotFound,
2255                 exception.UnexpectedDeletingTaskStateError):
2256             raise
2257         except exception.UnexpectedTaskStateError as e:
2258             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2259                     reason=e.format_message())
2260         except Exception:
2261             # Because this allocation is async any failures are likely to occur
2262             # when the driver accesses network_info during spawn().
2263             LOG.exception('Failed to allocate network(s)',
2264                           instance=instance)
2265             msg = _('Failed to allocate the network(s), not rescheduling.')
2266             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2267                     reason=msg)
2268 
2269         try:
2270             # Perform any driver preparation work for the driver.
2271             self.driver.prepare_for_spawn(instance)
2272 
2273             # Depending on a virt driver, some network configuration is
2274             # necessary before preparing block devices.
2275             self.driver.prepare_networks_before_block_device_mapping(
2276                 instance, network_info)
2277 
2278             # Verify that all the BDMs have a device_name set and assign a
2279             # default to the ones missing it with the help of the driver.
2280             self._default_block_device_names(instance, image_meta,
2281                                              block_device_mapping)
2282 
2283             LOG.debug('Start building block device mappings for instance.',
2284                       instance=instance)
2285             instance.vm_state = vm_states.BUILDING
2286             instance.task_state = task_states.BLOCK_DEVICE_MAPPING
2287             instance.save()
2288 
2289             block_device_info = self._prep_block_device(context, instance,
2290                     block_device_mapping)
2291             resources['block_device_info'] = block_device_info
2292         except (exception.InstanceNotFound,
2293                 exception.UnexpectedDeletingTaskStateError):
2294             with excutils.save_and_reraise_exception():
2295                 # Make sure the async call finishes
2296                 if network_info is not None:
2297                     network_info.wait(do_raise=False)
2298                     self.driver.clean_networks_preparation(instance,
2299                                                            network_info)
2300                 self.driver.failed_spawn_cleanup(instance)
2301         except (exception.UnexpectedTaskStateError,
2302                 exception.OverQuota, exception.InvalidBDM) as e:
2303             # Make sure the async call finishes
2304             if network_info is not None:
2305                 network_info.wait(do_raise=False)
2306                 self.driver.clean_networks_preparation(instance, network_info)
2307             self.driver.failed_spawn_cleanup(instance)
2308             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2309                     reason=e.format_message())
2310         except Exception:
2311             LOG.exception('Failure prepping block device',
2312                           instance=instance)
2313             # Make sure the async call finishes
2314             if network_info is not None:
2315                 network_info.wait(do_raise=False)
2316                 self.driver.clean_networks_preparation(instance, network_info)
2317             self.driver.failed_spawn_cleanup(instance)
2318             msg = _('Failure prepping block device.')
2319             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2320                     reason=msg)
2321 
2322         try:
2323             resources['allocations'] = (
2324                 self.reportclient.get_allocations_for_consumer(context,
2325                                                                instance.uuid))
2326         except Exception:
2327             LOG.exception('Failure retrieving placement allocations',
2328                           instance=instance)
2329             # Make sure the async call finishes
2330             if network_info is not None:
2331                 network_info.wait(do_raise=False)
2332             self.driver.failed_spawn_cleanup(instance)
2333             msg = _('Failure retrieving placement allocations')
2334             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2335                                                 reason=msg)
2336 
2337         try:
2338             yield resources
2339         except Exception as exc:
2340             with excutils.save_and_reraise_exception() as ctxt:
2341                 if not isinstance(exc, (
2342                         exception.InstanceNotFound,
2343                         exception.UnexpectedDeletingTaskStateError)):
2344                     LOG.exception('Instance failed to spawn',
2345                                   instance=instance)
2346                 # Make sure the async call finishes
2347                 if network_info is not None:
2348                     network_info.wait(do_raise=False)
2349                 # if network_info is empty we're likely here because of
2350                 # network allocation failure. Since nothing can be reused on
2351                 # rescheduling it's better to deallocate network to eliminate
2352                 # the chance of orphaned ports in neutron
2353                 deallocate_networks = False if network_info else True
2354                 try:
2355                     self._shutdown_instance(context, instance,
2356                             block_device_mapping, requested_networks,
2357                             try_deallocate_networks=deallocate_networks)
2358                 except Exception as exc2:
2359                     ctxt.reraise = False
2360                     LOG.warning('Could not clean up failed build,'
2361                                 ' not rescheduling. Error: %s',
2362                                 six.text_type(exc2))
2363                     raise exception.BuildAbortException(
2364                             instance_uuid=instance.uuid,
2365                             reason=six.text_type(exc))
2366 
2367     def _cleanup_allocated_networks(self, context, instance,
2368             requested_networks):
2369         try:
2370             self._deallocate_network(context, instance, requested_networks)
2371         except Exception:
2372             LOG.exception('Failed to deallocate networks', instance=instance)
2373             return
2374 
2375         instance.system_metadata['network_allocated'] = 'False'
2376         try:
2377             instance.save()
2378         except exception.InstanceNotFound:
2379             # NOTE(alaski): It's possible that we're cleaning up the networks
2380             # because the instance was deleted.  If that's the case then this
2381             # exception will be raised by instance.save()
2382             pass
2383 
2384     def _try_deallocate_network(self, context, instance,
2385                                 requested_networks=None):
2386 
2387         # During auto-scale cleanup, we could be deleting a large number
2388         # of servers at the same time and overloading parts of the system,
2389         # so we retry a few times in case of connection failures to the
2390         # networking service.
2391         @loopingcall.RetryDecorator(
2392             max_retry_count=3, inc_sleep_time=2, max_sleep_time=12,
2393             exceptions=(keystone_exception.connection.ConnectFailure,))
2394         def _deallocate_network_with_retries():
2395             try:
2396                 self._deallocate_network(
2397                     context, instance, requested_networks)
2398             except keystone_exception.connection.ConnectFailure as e:
2399                 # Provide a warning that something is amiss.
2400                 with excutils.save_and_reraise_exception():
2401                     LOG.warning('Failed to deallocate network for instance; '
2402                                 'retrying. Error: %s', six.text_type(e),
2403                                 instance=instance)
2404 
2405         try:
2406             # tear down allocated network structure
2407             _deallocate_network_with_retries()
2408         except Exception as ex:
2409             with excutils.save_and_reraise_exception():
2410                 LOG.error('Failed to deallocate network for instance. '
2411                           'Error: %s', ex, instance=instance)
2412                 self._set_instance_obj_error_state(context, instance)
2413 
2414     def _get_power_off_values(self, context, instance, clean_shutdown):
2415         """Get the timing configuration for powering down this instance."""
2416         if clean_shutdown:
2417             timeout = compute_utils.get_value_from_system_metadata(instance,
2418                           key='image_os_shutdown_timeout', type=int,
2419                           default=CONF.shutdown_timeout)
2420             retry_interval = CONF.compute.shutdown_retry_interval
2421         else:
2422             timeout = 0
2423             retry_interval = 0
2424 
2425         return timeout, retry_interval
2426 
2427     def _power_off_instance(self, context, instance, clean_shutdown=True):
2428         """Power off an instance on this host."""
2429         timeout, retry_interval = self._get_power_off_values(context,
2430                                         instance, clean_shutdown)
2431         self.driver.power_off(instance, timeout, retry_interval)
2432 
2433     def _shutdown_instance(self, context, instance,
2434                            bdms, requested_networks=None, notify=True,
2435                            try_deallocate_networks=True):
2436         """Shutdown an instance on this host.
2437 
2438         :param:context: security context
2439         :param:instance: a nova.objects.Instance object
2440         :param:bdms: the block devices for the instance to be torn
2441                      down
2442         :param:requested_networks: the networks on which the instance
2443                                    has ports
2444         :param:notify: true if a final usage notification should be
2445                        emitted
2446         :param:try_deallocate_networks: false if we should avoid
2447                                         trying to teardown networking
2448         """
2449         context = context.elevated()
2450         LOG.info('Terminating instance', instance=instance)
2451 
2452         if notify:
2453             self._notify_about_instance_usage(context, instance,
2454                                               "shutdown.start")
2455             compute_utils.notify_about_instance_action(context, instance,
2456                     self.host, action=fields.NotificationAction.SHUTDOWN,
2457                     phase=fields.NotificationPhase.START, bdms=bdms)
2458 
2459         network_info = instance.get_network_info()
2460 
2461         # NOTE(vish) get bdms before destroying the instance
2462         vol_bdms = [bdm for bdm in bdms if bdm.is_volume]
2463         block_device_info = self._get_instance_block_device_info(
2464             context, instance, bdms=bdms)
2465 
2466         # NOTE(melwitt): attempt driver destroy before releasing ip, may
2467         #                want to keep ip allocated for certain failures
2468         try:
2469             LOG.debug('Start destroying the instance on the hypervisor.',
2470                       instance=instance)
2471             with timeutils.StopWatch() as timer:
2472                 self.driver.destroy(context, instance, network_info,
2473                                     block_device_info)
2474             LOG.info('Took %0.2f seconds to destroy the instance on the '
2475                      'hypervisor.', timer.elapsed(), instance=instance)
2476         except exception.InstancePowerOffFailure:
2477             # if the instance can't power off, don't release the ip
2478             with excutils.save_and_reraise_exception():
2479                 pass
2480         except Exception:
2481             with excutils.save_and_reraise_exception():
2482                 # deallocate ip and fail without proceeding to
2483                 # volume api calls, preserving current behavior
2484                 if try_deallocate_networks:
2485                     self._try_deallocate_network(context, instance,
2486                                                  requested_networks)
2487 
2488         if try_deallocate_networks:
2489             self._try_deallocate_network(context, instance, requested_networks)
2490 
2491         timer.restart()
2492         for bdm in vol_bdms:
2493             try:
2494                 if bdm.attachment_id:
2495                     self.volume_api.attachment_delete(context,
2496                                                       bdm.attachment_id)
2497                 else:
2498                     # NOTE(vish): actual driver detach done in driver.destroy,
2499                     #             so just tell cinder that we are done with it.
2500                     connector = self.driver.get_volume_connector(instance)
2501                     self.volume_api.terminate_connection(context,
2502                                                          bdm.volume_id,
2503                                                          connector)
2504                     self.volume_api.detach(context, bdm.volume_id,
2505                                            instance.uuid)
2506 
2507             except exception.VolumeAttachmentNotFound as exc:
2508                 LOG.debug('Ignoring VolumeAttachmentNotFound: %s', exc,
2509                           instance=instance)
2510             except exception.DiskNotFound as exc:
2511                 LOG.debug('Ignoring DiskNotFound: %s', exc,
2512                           instance=instance)
2513             except exception.VolumeNotFound as exc:
2514                 LOG.debug('Ignoring VolumeNotFound: %s', exc,
2515                           instance=instance)
2516             except (cinder_exception.EndpointNotFound,
2517                     keystone_exception.EndpointNotFound) as exc:
2518                 LOG.warning('Ignoring EndpointNotFound for '
2519                             'volume %(volume_id)s: %(exc)s',
2520                             {'exc': exc, 'volume_id': bdm.volume_id},
2521                             instance=instance)
2522             except cinder_exception.ClientException as exc:
2523                 LOG.warning('Ignoring unknown cinder exception for '
2524                             'volume %(volume_id)s: %(exc)s',
2525                             {'exc': exc, 'volume_id': bdm.volume_id},
2526                             instance=instance)
2527             except Exception as exc:
2528                 LOG.warning('Ignoring unknown exception for '
2529                             'volume %(volume_id)s: %(exc)s',
2530                             {'exc': exc, 'volume_id': bdm.volume_id},
2531                             instance=instance)
2532         if vol_bdms:
2533             LOG.info('Took %(time).2f seconds to detach %(num)s volumes '
2534                      'for instance.',
2535                      {'time': timer.elapsed(), 'num': len(vol_bdms)},
2536                      instance=instance)
2537 
2538         if notify:
2539             self._notify_about_instance_usage(context, instance,
2540                                               "shutdown.end")
2541             compute_utils.notify_about_instance_action(context, instance,
2542                     self.host, action=fields.NotificationAction.SHUTDOWN,
2543                     phase=fields.NotificationPhase.END, bdms=bdms)
2544 
2545     def _cleanup_volumes(self, context, instance, bdms, raise_exc=True,
2546                          detach=True):
2547         exc_info = None
2548         for bdm in bdms:
2549             if detach and bdm.volume_id:
2550                 try:
2551                     LOG.debug("Detaching volume: %s", bdm.volume_id,
2552                               instance_uuid=instance.uuid)
2553                     destroy = bdm.delete_on_termination
2554                     self._detach_volume(context, bdm, instance,
2555                                         destroy_bdm=destroy)
2556                 except Exception as exc:
2557                     exc_info = sys.exc_info()
2558                     LOG.warning('Failed to detach volume: %(volume_id)s '
2559                                 'due to %(exc)s',
2560                                 {'volume_id': bdm.volume_id, 'exc': exc})
2561 
2562             if bdm.volume_id and bdm.delete_on_termination:
2563                 try:
2564                     LOG.debug("Deleting volume: %s", bdm.volume_id,
2565                               instance_uuid=instance.uuid)
2566                     self.volume_api.delete(context, bdm.volume_id)
2567                 except Exception as exc:
2568                     exc_info = sys.exc_info()
2569                     LOG.warning('Failed to delete volume: %(volume_id)s '
2570                                 'due to %(exc)s',
2571                                 {'volume_id': bdm.volume_id, 'exc': exc})
2572         if exc_info is not None and raise_exc:
2573             six.reraise(exc_info[0], exc_info[1], exc_info[2])
2574 
2575     @hooks.add_hook("delete_instance")
2576     def _delete_instance(self, context, instance, bdms):
2577         """Delete an instance on this host.
2578 
2579         :param context: nova request context
2580         :param instance: nova.objects.instance.Instance object
2581         :param bdms: nova.objects.block_device.BlockDeviceMappingList object
2582         """
2583         events = self.instance_events.clear_events_for_instance(instance)
2584         if events:
2585             LOG.debug('Events pending at deletion: %(events)s',
2586                       {'events': ','.join(events.keys())},
2587                       instance=instance)
2588         self._notify_about_instance_usage(context, instance,
2589                                           "delete.start")
2590         compute_utils.notify_about_instance_action(context, instance,
2591                 self.host, action=fields.NotificationAction.DELETE,
2592                 phase=fields.NotificationPhase.START, bdms=bdms)
2593 
2594         self._shutdown_instance(context, instance, bdms)
2595 
2596         # NOTE(vish): We have already deleted the instance, so we have
2597         #             to ignore problems cleaning up the volumes. It
2598         #             would be nice to let the user know somehow that
2599         #             the volume deletion failed, but it is not
2600         #             acceptable to have an instance that can not be
2601         #             deleted. Perhaps this could be reworked in the
2602         #             future to set an instance fault the first time
2603         #             and to only ignore the failure if the instance
2604         #             is already in ERROR.
2605 
2606         # NOTE(ameeda): The volumes already detached during the above
2607         #               _shutdown_instance() call and this is why
2608         #               detach is not requested from _cleanup_volumes()
2609         #               in this case
2610 
2611         self._cleanup_volumes(context, instance, bdms,
2612                 raise_exc=False, detach=False)
2613         # if a delete task succeeded, always update vm state and task
2614         # state without expecting task state to be DELETING
2615         instance.vm_state = vm_states.DELETED
2616         instance.task_state = None
2617         instance.power_state = power_state.NOSTATE
2618         instance.terminated_at = timeutils.utcnow()
2619         instance.save()
2620 
2621         self._complete_deletion(context, instance)
2622         # only destroy the instance in the db if the _complete_deletion
2623         # doesn't raise and therefore allocation is successfully
2624         # deleted in placement
2625         instance.destroy()
2626 
2627         self._notify_about_instance_usage(context, instance, "delete.end")
2628         compute_utils.notify_about_instance_action(context, instance,
2629                 self.host, action=fields.NotificationAction.DELETE,
2630                 phase=fields.NotificationPhase.END, bdms=bdms)
2631 
2632     @wrap_exception()
2633     @reverts_task_state
2634     @wrap_instance_event(prefix='compute')
2635     @wrap_instance_fault
2636     def terminate_instance(self, context, instance, bdms):
2637         """Terminate an instance on this host."""
2638         @utils.synchronized(instance.uuid)
2639         def do_terminate_instance(instance, bdms):
2640             # NOTE(mriedem): If we are deleting the instance while it was
2641             # booting from volume, we could be racing with a database update of
2642             # the BDM volume_id. Since the compute API passes the BDMs over RPC
2643             # to compute here, the BDMs may be stale at this point. So check
2644             # for any volume BDMs that don't have volume_id set and if we
2645             # detect that, we need to refresh the BDM list before proceeding.
2646             # TODO(mriedem): Move this into _delete_instance and make the bdms
2647             # parameter optional.
2648             for bdm in list(bdms):
2649                 if bdm.is_volume and not bdm.volume_id:
2650                     LOG.debug('There are potentially stale BDMs during '
2651                               'delete, refreshing the BlockDeviceMappingList.',
2652                               instance=instance)
2653                     bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
2654                         context, instance.uuid)
2655                     break
2656             try:
2657                 self._delete_instance(context, instance, bdms)
2658             except exception.InstanceNotFound:
2659                 LOG.info("Instance disappeared during terminate",
2660                          instance=instance)
2661             except Exception:
2662                 # As we're trying to delete always go to Error if something
2663                 # goes wrong that _delete_instance can't handle.
2664                 with excutils.save_and_reraise_exception():
2665                     LOG.exception('Setting instance vm_state to ERROR',
2666                                   instance=instance)
2667                     self._set_instance_obj_error_state(context, instance)
2668 
2669         do_terminate_instance(instance, bdms)
2670 
2671     def _delete_instance_stuck_in_deleting_task(self, context, instance):
2672         try:
2673             if instance.task_state == task_states.DELETING:
2674                 instance.obj_load_attr('metadata')
2675                 instance.obj_load_attr('system_metadata')
2676                 bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
2677                     context, instance.uuid)
2678                 self._delete_instance(context, instance, bdms)
2679             else:
2680                 self.soft_delete_instance(context, instance)
2681         except Exception:
2682             # we don't want that an exception blocks the init_host
2683             # or periodic_task _run_pending_deletes
2684             LOG.exception('Failed to complete a deletion',
2685                           instance=instance)
2686             self._set_instance_obj_error_state(context, instance)
2687 
2688     # NOTE(johannes): This is probably better named power_off_instance
2689     # so it matches the driver method, but because of other issues, we
2690     # can't use that name in grizzly.
2691     @wrap_exception()
2692     @reverts_task_state
2693     @wrap_instance_event(prefix='compute')
2694     @wrap_instance_fault
2695     def stop_instance(self, context, instance, clean_shutdown):
2696         """Stopping an instance on this host."""
2697 
2698         @utils.synchronized(instance.uuid)
2699         def do_stop_instance():
2700             current_power_state = self._get_power_state(context, instance)
2701             LOG.debug('Stopping instance; current vm_state: %(vm_state)s, '
2702                       'current task_state: %(task_state)s, current DB '
2703                       'power_state: %(db_power_state)s, current VM '
2704                       'power_state: %(current_power_state)s',
2705                       {'vm_state': instance.vm_state,
2706                        'task_state': instance.task_state,
2707                        'db_power_state': instance.power_state,
2708                        'current_power_state': current_power_state},
2709                       instance_uuid=instance.uuid)
2710 
2711             # NOTE(mriedem): If the instance is already powered off, we are
2712             # possibly tearing down and racing with other operations, so we can
2713             # expect the task_state to be None if something else updates the
2714             # instance and we're not locking it.
2715             expected_task_state = [task_states.POWERING_OFF]
2716             # The list of power states is from _sync_instance_power_state.
2717             if current_power_state in (power_state.NOSTATE,
2718                                        power_state.SHUTDOWN,
2719                                        power_state.CRASHED):
2720                 LOG.info('Instance is already powered off in the '
2721                          'hypervisor when stop is called.',
2722                          instance=instance)
2723                 expected_task_state.append(None)
2724 
2725             self._notify_about_instance_usage(context, instance,
2726                                               "power_off.start")
2727 
2728             compute_utils.notify_about_instance_action(context, instance,
2729                         self.host, action=fields.NotificationAction.POWER_OFF,
2730                         phase=fields.NotificationPhase.START)
2731 
2732             self._power_off_instance(context, instance, clean_shutdown)
2733             instance.power_state = self._get_power_state(context, instance)
2734             instance.vm_state = vm_states.STOPPED
2735             instance.task_state = None
2736             instance.save(expected_task_state=expected_task_state)
2737             self._notify_about_instance_usage(context, instance,
2738                                               "power_off.end")
2739 
2740             compute_utils.notify_about_instance_action(context, instance,
2741                         self.host, action=fields.NotificationAction.POWER_OFF,
2742                         phase=fields.NotificationPhase.END)
2743 
2744         do_stop_instance()
2745 
2746     def _power_on(self, context, instance):
2747         network_info = self.network_api.get_instance_nw_info(context, instance)
2748         block_device_info = self._get_instance_block_device_info(context,
2749                                                                  instance)
2750         self.driver.power_on(context, instance,
2751                              network_info,
2752                              block_device_info)
2753 
2754     def _delete_snapshot_of_shelved_instance(self, context, instance,
2755                                              snapshot_id):
2756         """Delete snapshot of shelved instance."""
2757         try:
2758             self.image_api.delete(context, snapshot_id)
2759         except (exception.ImageNotFound,
2760                 exception.ImageNotAuthorized) as exc:
2761             LOG.warning("Failed to delete snapshot "
2762                         "from shelved instance (%s).",
2763                         exc.format_message(), instance=instance)
2764         except Exception:
2765             LOG.exception("Something wrong happened when trying to "
2766                           "delete snapshot from shelved instance.",
2767                           instance=instance)
2768 
2769     # NOTE(johannes): This is probably better named power_on_instance
2770     # so it matches the driver method, but because of other issues, we
2771     # can't use that name in grizzly.
2772     @wrap_exception()
2773     @reverts_task_state
2774     @wrap_instance_event(prefix='compute')
2775     @wrap_instance_fault
2776     def start_instance(self, context, instance):
2777         """Starting an instance on this host."""
2778         self._notify_about_instance_usage(context, instance, "power_on.start")
2779         compute_utils.notify_about_instance_action(context, instance,
2780             self.host, action=fields.NotificationAction.POWER_ON,
2781             phase=fields.NotificationPhase.START)
2782         self._power_on(context, instance)
2783         instance.power_state = self._get_power_state(context, instance)
2784         instance.vm_state = vm_states.ACTIVE
2785         instance.task_state = None
2786 
2787         # Delete an image(VM snapshot) for a shelved instance
2788         snapshot_id = instance.system_metadata.get('shelved_image_id')
2789         if snapshot_id:
2790             self._delete_snapshot_of_shelved_instance(context, instance,
2791                                                       snapshot_id)
2792 
2793         # Delete system_metadata for a shelved instance
2794         compute_utils.remove_shelved_keys_from_system_metadata(instance)
2795 
2796         instance.save(expected_task_state=task_states.POWERING_ON)
2797         self._notify_about_instance_usage(context, instance, "power_on.end")
2798         compute_utils.notify_about_instance_action(context, instance,
2799             self.host, action=fields.NotificationAction.POWER_ON,
2800             phase=fields.NotificationPhase.END)
2801 
2802     @messaging.expected_exceptions(NotImplementedError,
2803                                    exception.TriggerCrashDumpNotSupported,
2804                                    exception.InstanceNotRunning)
2805     @wrap_exception()
2806     @wrap_instance_event(prefix='compute')
2807     @wrap_instance_fault
2808     def trigger_crash_dump(self, context, instance):
2809         """Trigger crash dump in an instance."""
2810 
2811         self._notify_about_instance_usage(context, instance,
2812                                           "trigger_crash_dump.start")
2813         compute_utils.notify_about_instance_action(context, instance,
2814                 self.host, action=fields.NotificationAction.TRIGGER_CRASH_DUMP,
2815                 phase=fields.NotificationPhase.START)
2816 
2817         # This method does not change task_state and power_state because the
2818         # effect of a trigger depends on user's configuration.
2819         self.driver.trigger_crash_dump(instance)
2820 
2821         self._notify_about_instance_usage(context, instance,
2822                                           "trigger_crash_dump.end")
2823         compute_utils.notify_about_instance_action(context, instance,
2824                 self.host, action=fields.NotificationAction.TRIGGER_CRASH_DUMP,
2825                 phase=fields.NotificationPhase.END)
2826 
2827     @wrap_exception()
2828     @reverts_task_state
2829     @wrap_instance_event(prefix='compute')
2830     @wrap_instance_fault
2831     def soft_delete_instance(self, context, instance):
2832         """Soft delete an instance on this host."""
2833         with compute_utils.notify_about_instance_delete(
2834                 self.notifier, context, instance, 'soft_delete',
2835                 source=fields.NotificationSource.COMPUTE):
2836             try:
2837                 self.driver.soft_delete(instance)
2838             except NotImplementedError:
2839                 # Fallback to just powering off the instance if the
2840                 # hypervisor doesn't implement the soft_delete method
2841                 self.driver.power_off(instance)
2842             instance.power_state = self._get_power_state(context, instance)
2843             instance.vm_state = vm_states.SOFT_DELETED
2844             instance.task_state = None
2845             instance.save(expected_task_state=[task_states.SOFT_DELETING])
2846 
2847     @wrap_exception()
2848     @reverts_task_state
2849     @wrap_instance_event(prefix='compute')
2850     @wrap_instance_fault
2851     def restore_instance(self, context, instance):
2852         """Restore a soft-deleted instance on this host."""
2853         self._notify_about_instance_usage(context, instance, "restore.start")
2854         compute_utils.notify_about_instance_action(context, instance,
2855             self.host, action=fields.NotificationAction.RESTORE,
2856             phase=fields.NotificationPhase.START)
2857         try:
2858             self.driver.restore(instance)
2859         except NotImplementedError:
2860             # Fallback to just powering on the instance if the hypervisor
2861             # doesn't implement the restore method
2862             self._power_on(context, instance)
2863         instance.power_state = self._get_power_state(context, instance)
2864         instance.vm_state = vm_states.ACTIVE
2865         instance.task_state = None
2866         instance.save(expected_task_state=task_states.RESTORING)
2867         self._notify_about_instance_usage(context, instance, "restore.end")
2868         compute_utils.notify_about_instance_action(context, instance,
2869             self.host, action=fields.NotificationAction.RESTORE,
2870             phase=fields.NotificationPhase.END)
2871 
2872     @staticmethod
2873     def _set_migration_status(migration, status):
2874         """Set the status, and guard against a None being passed in.
2875 
2876         This is useful as some of the compute RPC calls will not pass
2877         a migration object in older versions. The check can be removed when
2878         we move past 4.x major version of the RPC API.
2879         """
2880         if migration:
2881             migration.status = status
2882             migration.save()
2883 
2884     def _rebuild_default_impl(self, context, instance, image_meta,
2885                               injected_files, admin_password, allocations,
2886                               bdms, detach_block_devices, attach_block_devices,
2887                               network_info=None,
2888                               evacuate=False, block_device_info=None,
2889                               preserve_ephemeral=False):
2890         if preserve_ephemeral:
2891             # The default code path does not support preserving ephemeral
2892             # partitions.
2893             raise exception.PreserveEphemeralNotSupported()
2894 
2895         if evacuate:
2896             detach_block_devices(context, bdms)
2897         else:
2898             self._power_off_instance(context, instance, clean_shutdown=True)
2899             detach_block_devices(context, bdms)
2900             self.driver.destroy(context, instance,
2901                                 network_info=network_info,
2902                                 block_device_info=block_device_info)
2903 
2904         instance.task_state = task_states.REBUILD_BLOCK_DEVICE_MAPPING
2905         instance.save(expected_task_state=[task_states.REBUILDING])
2906 
2907         new_block_device_info = attach_block_devices(context, instance, bdms)
2908 
2909         instance.task_state = task_states.REBUILD_SPAWNING
2910         instance.save(
2911             expected_task_state=[task_states.REBUILD_BLOCK_DEVICE_MAPPING])
2912 
2913         with instance.mutated_migration_context():
2914             self.driver.spawn(context, instance, image_meta, injected_files,
2915                               admin_password, allocations,
2916                               network_info=network_info,
2917                               block_device_info=new_block_device_info)
2918 
2919     def _notify_instance_rebuild_error(self, context, instance, error, bdms):
2920         tb = traceback.format_exc()
2921         self._notify_about_instance_usage(context, instance,
2922                                           'rebuild.error', fault=error)
2923         compute_utils.notify_about_instance_rebuild(
2924             context, instance, self.host,
2925             phase=fields.NotificationPhase.ERROR, exception=error, bdms=bdms,
2926             tb=tb)
2927 
2928     @messaging.expected_exceptions(exception.PreserveEphemeralNotSupported)
2929     @wrap_exception()
2930     @reverts_task_state
2931     @wrap_instance_event(prefix='compute')
2932     @wrap_instance_fault
2933     def rebuild_instance(self, context, instance, orig_image_ref, image_ref,
2934                          injected_files, new_pass, orig_sys_metadata,
2935                          bdms, recreate, on_shared_storage,
2936                          preserve_ephemeral, migration,
2937                          scheduled_node, limits, request_spec):
2938         """Destroy and re-make this instance.
2939 
2940         A 'rebuild' effectively purges all existing data from the system and
2941         remakes the VM with given 'metadata' and 'personalities'.
2942 
2943         :param context: `nova.RequestContext` object
2944         :param instance: Instance object
2945         :param orig_image_ref: Original image_ref before rebuild
2946         :param image_ref: New image_ref for rebuild
2947         :param injected_files: Files to inject
2948         :param new_pass: password to set on rebuilt instance
2949         :param orig_sys_metadata: instance system metadata from pre-rebuild
2950         :param bdms: block-device-mappings to use for rebuild
2951         :param recreate: True if the instance is being recreated (e.g. the
2952             hypervisor it was on failed) - cleanup of old state will be
2953             skipped.
2954         :param on_shared_storage: True if instance files on shared storage.
2955                                   If not provided then information from the
2956                                   driver will be used to decide if the instance
2957                                   files are available or not on the target host
2958         :param preserve_ephemeral: True if the default ephemeral storage
2959                                    partition must be preserved on rebuild
2960         :param migration: a Migration object if one was created for this
2961                           rebuild operation (if it's a part of evacuate)
2962         :param scheduled_node: A node of the host chosen by the scheduler. If a
2963                                host was specified by the user, this will be
2964                                None
2965         :param limits: Overcommit limits set by the scheduler. If a host was
2966                        specified by the user, this will be None
2967         :param request_spec: a RequestSpec object used to schedule the instance
2968 
2969         """
2970         # recreate=True means the instance is being evacuated from a failed
2971         # host to a new destination host (this host). The 'recreate' variable
2972         # name is confusing, so rename it to evacuate here at the top, which
2973         # is simpler than renaming a parameter in an RPC versioned method.
2974         evacuate = recreate
2975         context = context.elevated()
2976 
2977         if evacuate:
2978             LOG.info("Evacuating instance", instance=instance)
2979         else:
2980             LOG.info("Rebuilding instance", instance=instance)
2981 
2982         rt = self._get_resource_tracker()
2983         if evacuate:
2984             # This is an evacuation to a new host, so we need to perform a
2985             # resource claim.
2986             rebuild_claim = rt.rebuild_claim
2987         else:
2988             # This is a rebuild to the same host, so we don't need to make
2989             # a claim since the instance is already on this host.
2990             rebuild_claim = claims.NopClaim
2991 
2992         if image_ref:
2993             image_meta = objects.ImageMeta.from_image_ref(
2994                 context, self.image_api, image_ref)
2995         elif evacuate:
2996             # For evacuate the API does not send down the image_ref since the
2997             # image does not change so just get it from what was stashed in
2998             # the instance system_metadata when the instance was created (or
2999             # last rebuilt). This also works for volume-backed instances.
3000             image_meta = instance.image_meta
3001         else:
3002             image_meta = objects.ImageMeta()
3003 
3004         # NOTE(mriedem): On an evacuate, we need to update
3005         # the instance's host and node properties to reflect it's
3006         # destination node for the evacuate.
3007         if not scheduled_node:
3008             if evacuate:
3009                 try:
3010                     compute_node = self._get_compute_info(context, self.host)
3011                     scheduled_node = compute_node.hypervisor_hostname
3012                 except exception.ComputeHostNotFound:
3013                     LOG.exception('Failed to get compute_info for %s',
3014                                   self.host)
3015             else:
3016                 scheduled_node = instance.node
3017 
3018         with self._error_out_instance_on_exception(context, instance):
3019             try:
3020                 claim_ctxt = rebuild_claim(
3021                     context, instance, scheduled_node,
3022                     limits=limits, image_meta=image_meta,
3023                     migration=migration)
3024                 self._do_rebuild_instance_with_claim(
3025                     claim_ctxt, context, instance, orig_image_ref,
3026                     image_meta, injected_files, new_pass, orig_sys_metadata,
3027                     bdms, evacuate, on_shared_storage, preserve_ephemeral,
3028                     migration, request_spec)
3029             except (exception.ComputeResourcesUnavailable,
3030                     exception.RescheduledException) as e:
3031                 if isinstance(e, exception.ComputeResourcesUnavailable):
3032                     LOG.debug("Could not rebuild instance on this host, not "
3033                               "enough resources available.", instance=instance)
3034                 else:
3035                     # RescheduledException is raised by the late server group
3036                     # policy check during evacuation if a parallel scheduling
3037                     # violated the policy.
3038                     # We catch the RescheduledException here but we don't have
3039                     # the plumbing to do an actual reschedule so we abort the
3040                     # operation.
3041                     LOG.debug("Could not rebuild instance on this host, "
3042                               "late server group check failed.",
3043                               instance=instance)
3044                 # NOTE(ndipanov): We just abort the build for now and leave a
3045                 # migration record for potential cleanup later
3046                 self._set_migration_status(migration, 'failed')
3047                 # Since the claim failed, we need to remove the allocation
3048                 # created against the destination node. Note that we can only
3049                 # get here when evacuating to a destination node. Rebuilding
3050                 # on the same host (not evacuate) uses the NopClaim which will
3051                 # not raise ComputeResourcesUnavailable.
3052                 rt.delete_allocation_for_evacuated_instance(
3053                     context, instance, scheduled_node, node_type='destination')
3054                 self._notify_instance_rebuild_error(context, instance, e, bdms)
3055                 raise exception.BuildAbortException(
3056                     instance_uuid=instance.uuid, reason=e.format_message())
3057             except (exception.InstanceNotFound,
3058                     exception.UnexpectedDeletingTaskStateError) as e:
3059                 LOG.debug('Instance was deleted while rebuilding',
3060                           instance=instance)
3061                 self._set_migration_status(migration, 'failed')
3062                 self._notify_instance_rebuild_error(context, instance, e, bdms)
3063             except Exception as e:
3064                 self._set_migration_status(migration, 'failed')
3065                 if evacuate or scheduled_node is not None:
3066                     rt.delete_allocation_for_evacuated_instance(
3067                         context, instance, scheduled_node,
3068                         node_type='destination')
3069                 self._notify_instance_rebuild_error(context, instance, e, bdms)
3070                 raise
3071             else:
3072                 instance.apply_migration_context()
3073                 # NOTE (ndipanov): This save will now update the host and node
3074                 # attributes making sure that next RT pass is consistent since
3075                 # it will be based on the instance and not the migration DB
3076                 # entry.
3077                 instance.host = self.host
3078                 instance.node = scheduled_node
3079                 instance.save()
3080                 instance.drop_migration_context()
3081 
3082                 # NOTE (ndipanov): Mark the migration as done only after we
3083                 # mark the instance as belonging to this host.
3084                 self._set_migration_status(migration, 'done')
3085 
3086     def _do_rebuild_instance_with_claim(self, claim_context, *args, **kwargs):
3087         """Helper to avoid deep nesting in the top-level method."""
3088 
3089         with claim_context:
3090             self._do_rebuild_instance(*args, **kwargs)
3091 
3092     @staticmethod
3093     def _get_image_name(image_meta):
3094         if image_meta.obj_attr_is_set("name"):
3095             return image_meta.name
3096         else:
3097             return ''
3098 
3099     def _do_rebuild_instance(self, context, instance, orig_image_ref,
3100                              image_meta, injected_files, new_pass,
3101                              orig_sys_metadata, bdms, evacuate,
3102                              on_shared_storage, preserve_ephemeral,
3103                              migration, request_spec):
3104         orig_vm_state = instance.vm_state
3105 
3106         if evacuate:
3107             if request_spec:
3108                 # NOTE(gibi): Do a late check of server group policy as
3109                 # parallel scheduling could violate such policy. This will
3110                 # cause the evacuate to fail as rebuild does not implement
3111                 # reschedule.
3112                 hints = self._get_scheduler_hints({}, request_spec)
3113                 self._validate_instance_group_policy(context, instance, hints)
3114 
3115             if not self.driver.capabilities.get("supports_evacuate", False):
3116                 raise exception.InstanceEvacuateNotSupported
3117 
3118             self._check_instance_exists(context, instance)
3119 
3120             if on_shared_storage is None:
3121                 LOG.debug('on_shared_storage is not provided, using driver '
3122                           'information to decide if the instance needs to '
3123                           'be evacuated')
3124                 on_shared_storage = self.driver.instance_on_disk(instance)
3125 
3126             elif (on_shared_storage !=
3127                     self.driver.instance_on_disk(instance)):
3128                 # To cover case when admin expects that instance files are
3129                 # on shared storage, but not accessible and vice versa
3130                 raise exception.InvalidSharedStorage(
3131                         _("Invalid state of instance files on shared"
3132                             " storage"))
3133 
3134             if on_shared_storage:
3135                 LOG.info('disk on shared storage, evacuating using'
3136                          ' existing disk')
3137             elif instance.image_ref:
3138                 orig_image_ref = instance.image_ref
3139                 LOG.info("disk not on shared storage, evacuating from "
3140                          "image: '%s'", str(orig_image_ref))
3141             else:
3142                 LOG.info('disk on volume, evacuating using existing '
3143                          'volume')
3144 
3145         # We check trusted certs capabilities for both evacuate (rebuild on
3146         # another host) and rebuild (rebuild on the same host) because for
3147         # evacuate we need to make sure an instance with trusted certs can
3148         # have the image verified with those certs during rebuild, and for
3149         # rebuild we could be rebuilding a server that started out with no
3150         # trusted certs on this host, and then was rebuilt with trusted certs
3151         # for a new image, in which case we need to validate that new image
3152         # with the trusted certs during the rebuild.
3153         self._check_trusted_certs(instance)
3154 
3155         # This instance.exists message should contain the original
3156         # image_ref, not the new one.  Since the DB has been updated
3157         # to point to the new one... we have to override it.
3158         orig_image_ref_url = self.image_api.generate_image_url(orig_image_ref,
3159                                                                context)
3160         extra_usage_info = {'image_ref_url': orig_image_ref_url}
3161         compute_utils.notify_usage_exists(
3162                 self.notifier, context, instance, self.host,
3163                 current_period=True, system_metadata=orig_sys_metadata,
3164                 extra_usage_info=extra_usage_info)
3165 
3166         # This message should contain the new image_ref
3167         extra_usage_info = {'image_name': self._get_image_name(image_meta)}
3168         self._notify_about_instance_usage(context, instance,
3169                 "rebuild.start", extra_usage_info=extra_usage_info)
3170         # NOTE: image_name is not included in the versioned notification
3171         # because we already provide the image_uuid in the notification
3172         # payload and the image details can be looked up via the uuid.
3173         compute_utils.notify_about_instance_rebuild(
3174             context, instance, self.host,
3175             phase=fields.NotificationPhase.START,
3176             bdms=bdms)
3177 
3178         instance.power_state = self._get_power_state(context, instance)
3179         instance.task_state = task_states.REBUILDING
3180         instance.save(expected_task_state=[task_states.REBUILDING])
3181 
3182         if evacuate:
3183             self.network_api.setup_networks_on_host(
3184                     context, instance, self.host)
3185             # For nova-network this is needed to move floating IPs
3186             # For neutron this updates the host in the port binding
3187             # TODO(cfriesen): this network_api call and the one above
3188             # are so similar, we should really try to unify them.
3189             self.network_api.setup_instance_network_on_host(
3190                     context, instance, self.host, migration)
3191             # TODO(mriedem): Consider decorating setup_instance_network_on_host
3192             # with @base_api.refresh_cache and then we wouldn't need this
3193             # explicit call to get_instance_nw_info.
3194             network_info = self.network_api.get_instance_nw_info(context,
3195                                                                  instance)
3196         else:
3197             network_info = instance.get_network_info()
3198 
3199         allocations = self.reportclient.get_allocations_for_consumer(
3200             context, instance.uuid)
3201 
3202         if bdms is None:
3203             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
3204                     context, instance.uuid)
3205 
3206         block_device_info = \
3207             self._get_instance_block_device_info(
3208                     context, instance, bdms=bdms)
3209 
3210         def detach_block_devices(context, bdms):
3211             for bdm in bdms:
3212                 if bdm.is_volume:
3213                     # NOTE (ildikov): Having the attachment_id set in the BDM
3214                     # means that it's the new Cinder attach/detach flow
3215                     # (available from v3.44). In that case we explicitly
3216                     # attach and detach the volumes through attachment level
3217                     # operations. In this scenario _detach_volume will delete
3218                     # the existing attachment which would make the volume
3219                     # status change to 'available' if we don't pre-create
3220                     # another empty attachment before deleting the old one.
3221                     attachment_id = None
3222                     if bdm.attachment_id:
3223                         attachment_id = self.volume_api.attachment_create(
3224                             context, bdm['volume_id'], instance.uuid)['id']
3225                     self._detach_volume(context, bdm, instance,
3226                                         destroy_bdm=False)
3227                     if attachment_id:
3228                         bdm.attachment_id = attachment_id
3229                         bdm.save()
3230 
3231         files = self._decode_files(injected_files)
3232 
3233         kwargs = dict(
3234             context=context,
3235             instance=instance,
3236             image_meta=image_meta,
3237             injected_files=files,
3238             admin_password=new_pass,
3239             allocations=allocations,
3240             bdms=bdms,
3241             detach_block_devices=detach_block_devices,
3242             attach_block_devices=self._prep_block_device,
3243             block_device_info=block_device_info,
3244             network_info=network_info,
3245             preserve_ephemeral=preserve_ephemeral,
3246             evacuate=evacuate)
3247         try:
3248             with instance.mutated_migration_context():
3249                 self.driver.rebuild(**kwargs)
3250         except NotImplementedError:
3251             # NOTE(rpodolyaka): driver doesn't provide specialized version
3252             # of rebuild, fall back to the default implementation
3253             self._rebuild_default_impl(**kwargs)
3254         self._update_instance_after_spawn(context, instance)
3255         instance.save(expected_task_state=[task_states.REBUILD_SPAWNING])
3256 
3257         if orig_vm_state == vm_states.STOPPED:
3258             LOG.info("bringing vm to original state: '%s'",
3259                      orig_vm_state, instance=instance)
3260             instance.vm_state = vm_states.ACTIVE
3261             instance.task_state = task_states.POWERING_OFF
3262             instance.progress = 0
3263             instance.save()
3264             self.stop_instance(context, instance, False)
3265         # TODO(melwitt): We should clean up instance console tokens here in the
3266         # case of evacuate. The instance is on a new host and will need to
3267         # establish a new console connection.
3268         self._update_scheduler_instance_info(context, instance)
3269         self._notify_about_instance_usage(
3270                 context, instance, "rebuild.end",
3271                 network_info=network_info,
3272                 extra_usage_info=extra_usage_info)
3273         compute_utils.notify_about_instance_rebuild(
3274             context, instance, self.host,
3275             phase=fields.NotificationPhase.END,
3276             bdms=bdms)
3277 
3278     def _handle_bad_volumes_detached(self, context, instance, bad_devices,
3279                                      block_device_info):
3280         """Handle cases where the virt-layer had to detach non-working volumes
3281         in order to complete an operation.
3282         """
3283         for bdm in block_device_info['block_device_mapping']:
3284             if bdm.get('mount_device') in bad_devices:
3285                 try:
3286                     volume_id = bdm['connection_info']['data']['volume_id']
3287                 except KeyError:
3288                     continue
3289 
3290                 # NOTE(sirp): ideally we'd just call
3291                 # `compute_api.detach_volume` here but since that hits the
3292                 # DB directly, that's off limits from within the
3293                 # compute-manager.
3294                 #
3295                 # API-detach
3296                 LOG.info("Detaching from volume api: %s", volume_id)
3297                 self.volume_api.begin_detaching(context, volume_id)
3298 
3299                 # Manager-detach
3300                 self.detach_volume(context, volume_id, instance)
3301 
3302     @wrap_exception()
3303     @reverts_task_state
3304     @wrap_instance_event(prefix='compute')
3305     @wrap_instance_fault
3306     def reboot_instance(self, context, instance, block_device_info,
3307                         reboot_type):
3308         """Reboot an instance on this host."""
3309         # acknowledge the request made it to the manager
3310         if reboot_type == "SOFT":
3311             instance.task_state = task_states.REBOOT_PENDING
3312             expected_states = task_states.soft_reboot_states
3313         else:
3314             instance.task_state = task_states.REBOOT_PENDING_HARD
3315             expected_states = task_states.hard_reboot_states
3316 
3317         context = context.elevated()
3318         LOG.info("Rebooting instance", instance=instance)
3319 
3320         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
3321             context, instance.uuid)
3322         block_device_info = self._get_instance_block_device_info(
3323             context, instance, bdms=bdms)
3324 
3325         network_info = self.network_api.get_instance_nw_info(context, instance)
3326 
3327         self._notify_about_instance_usage(context, instance, "reboot.start")
3328         compute_utils.notify_about_instance_action(
3329             context, instance, self.host,
3330             action=fields.NotificationAction.REBOOT,
3331             phase=fields.NotificationPhase.START,
3332             bdms=bdms
3333         )
3334 
3335         instance.power_state = self._get_power_state(context, instance)
3336         instance.save(expected_task_state=expected_states)
3337 
3338         if instance.power_state != power_state.RUNNING:
3339             state = instance.power_state
3340             running = power_state.RUNNING
3341             LOG.warning('trying to reboot a non-running instance:'
3342                         ' (state: %(state)s expected: %(running)s)',
3343                         {'state': state, 'running': running},
3344                         instance=instance)
3345 
3346         def bad_volumes_callback(bad_devices):
3347             self._handle_bad_volumes_detached(
3348                     context, instance, bad_devices, block_device_info)
3349 
3350         try:
3351             # Don't change it out of rescue mode
3352             if instance.vm_state == vm_states.RESCUED:
3353                 new_vm_state = vm_states.RESCUED
3354             else:
3355                 new_vm_state = vm_states.ACTIVE
3356             new_power_state = None
3357             if reboot_type == "SOFT":
3358                 instance.task_state = task_states.REBOOT_STARTED
3359                 expected_state = task_states.REBOOT_PENDING
3360             else:
3361                 instance.task_state = task_states.REBOOT_STARTED_HARD
3362                 expected_state = task_states.REBOOT_PENDING_HARD
3363             instance.save(expected_task_state=expected_state)
3364             self.driver.reboot(context, instance,
3365                                network_info,
3366                                reboot_type,
3367                                block_device_info=block_device_info,
3368                                bad_volumes_callback=bad_volumes_callback)
3369 
3370         except Exception as error:
3371             with excutils.save_and_reraise_exception() as ctxt:
3372                 exc_info = sys.exc_info()
3373                 # if the reboot failed but the VM is running don't
3374                 # put it into an error state
3375                 new_power_state = self._get_power_state(context, instance)
3376                 if new_power_state == power_state.RUNNING:
3377                     LOG.warning('Reboot failed but instance is running',
3378                                 instance=instance)
3379                     compute_utils.add_instance_fault_from_exc(context,
3380                             instance, error, exc_info)
3381                     self._notify_about_instance_usage(context, instance,
3382                             'reboot.error', fault=error)
3383                     tb = traceback.format_exc()
3384                     compute_utils.notify_about_instance_action(
3385                         context, instance, self.host,
3386                         action=fields.NotificationAction.REBOOT,
3387                         phase=fields.NotificationPhase.ERROR,
3388                         exception=error, bdms=bdms, tb=tb
3389                     )
3390                     ctxt.reraise = False
3391                 else:
3392                     LOG.error('Cannot reboot instance: %s', error,
3393                               instance=instance)
3394                     self._set_instance_obj_error_state(context, instance)
3395 
3396         if not new_power_state:
3397             new_power_state = self._get_power_state(context, instance)
3398         try:
3399             instance.power_state = new_power_state
3400             instance.vm_state = new_vm_state
3401             instance.task_state = None
3402             instance.save()
3403         except exception.InstanceNotFound:
3404             LOG.warning("Instance disappeared during reboot",
3405                         instance=instance)
3406 
3407         self._notify_about_instance_usage(context, instance, "reboot.end")
3408         compute_utils.notify_about_instance_action(
3409             context, instance, self.host,
3410             action=fields.NotificationAction.REBOOT,
3411             phase=fields.NotificationPhase.END,
3412             bdms=bdms
3413         )
3414 
3415     @delete_image_on_error
3416     def _do_snapshot_instance(self, context, image_id, instance):
3417         self._snapshot_instance(context, image_id, instance,
3418                                 task_states.IMAGE_BACKUP)
3419 
3420     @wrap_exception()
3421     @reverts_task_state
3422     @wrap_instance_event(prefix='compute')
3423     @wrap_instance_fault
3424     def backup_instance(self, context, image_id, instance, backup_type,
3425                         rotation):
3426         """Backup an instance on this host.
3427 
3428         :param backup_type: daily | weekly
3429         :param rotation: int representing how many backups to keep around
3430         """
3431         self._do_snapshot_instance(context, image_id, instance)
3432         self._rotate_backups(context, instance, backup_type, rotation)
3433 
3434     @wrap_exception()
3435     @reverts_task_state
3436     @wrap_instance_event(prefix='compute')
3437     @wrap_instance_fault
3438     @delete_image_on_error
3439     def snapshot_instance(self, context, image_id, instance):
3440         """Snapshot an instance on this host.
3441 
3442         :param context: security context
3443         :param image_id: glance.db.sqlalchemy.models.Image.Id
3444         :param instance: a nova.objects.instance.Instance object
3445         """
3446         # NOTE(dave-mcnally) the task state will already be set by the api
3447         # but if the compute manager has crashed/been restarted prior to the
3448         # request getting here the task state may have been cleared so we set
3449         # it again and things continue normally
3450         try:
3451             instance.task_state = task_states.IMAGE_SNAPSHOT
3452             instance.save(
3453                         expected_task_state=task_states.IMAGE_SNAPSHOT_PENDING)
3454         except exception.InstanceNotFound:
3455             # possibility instance no longer exists, no point in continuing
3456             LOG.debug("Instance not found, could not set state %s "
3457                       "for instance.",
3458                       task_states.IMAGE_SNAPSHOT, instance=instance)
3459             return
3460 
3461         except exception.UnexpectedDeletingTaskStateError:
3462             LOG.debug("Instance being deleted, snapshot cannot continue",
3463                       instance=instance)
3464             return
3465 
3466         self._snapshot_instance(context, image_id, instance,
3467                                 task_states.IMAGE_SNAPSHOT)
3468 
3469     def _snapshot_instance(self, context, image_id, instance,
3470                            expected_task_state):
3471         context = context.elevated()
3472 
3473         instance.power_state = self._get_power_state(context, instance)
3474         try:
3475             instance.save()
3476 
3477             LOG.info('instance snapshotting', instance=instance)
3478 
3479             if instance.power_state != power_state.RUNNING:
3480                 state = instance.power_state
3481                 running = power_state.RUNNING
3482                 LOG.warning('trying to snapshot a non-running instance: '
3483                             '(state: %(state)s expected: %(running)s)',
3484                             {'state': state, 'running': running},
3485                             instance=instance)
3486 
3487             self._notify_about_instance_usage(
3488                 context, instance, "snapshot.start")
3489             compute_utils.notify_about_instance_snapshot(context, instance,
3490                 self.host, phase=fields.NotificationPhase.START,
3491                 snapshot_image_id=image_id)
3492 
3493             def update_task_state(task_state,
3494                                   expected_state=expected_task_state):
3495                 instance.task_state = task_state
3496                 instance.save(expected_task_state=expected_state)
3497 
3498             with timeutils.StopWatch() as timer:
3499                 self.driver.snapshot(context, instance, image_id,
3500                                      update_task_state)
3501             LOG.info('Took %0.2f seconds to snapshot the instance on '
3502                      'the hypervisor.', timer.elapsed(), instance=instance)
3503 
3504             instance.task_state = None
3505             instance.save(expected_task_state=task_states.IMAGE_UPLOADING)
3506 
3507             self._notify_about_instance_usage(context, instance,
3508                                               "snapshot.end")
3509             compute_utils.notify_about_instance_snapshot(context, instance,
3510                 self.host, phase=fields.NotificationPhase.END,
3511                 snapshot_image_id=image_id)
3512         except (exception.InstanceNotFound,
3513                 exception.UnexpectedDeletingTaskStateError):
3514             # the instance got deleted during the snapshot
3515             # Quickly bail out of here
3516             msg = 'Instance disappeared during snapshot'
3517             LOG.debug(msg, instance=instance)
3518             try:
3519                 image = self.image_api.get(context, image_id)
3520                 if image['status'] != 'active':
3521                     self.image_api.delete(context, image_id)
3522             except exception.ImageNotFound:
3523                 LOG.debug('Image not found during clean up %s', image_id)
3524             except Exception:
3525                 LOG.warning("Error while trying to clean up image %s",
3526                             image_id, instance=instance)
3527         except exception.ImageNotFound:
3528             instance.task_state = None
3529             instance.save()
3530             LOG.warning("Image not found during snapshot", instance=instance)
3531 
3532     def _post_interrupted_snapshot_cleanup(self, context, instance):
3533         self.driver.post_interrupted_snapshot_cleanup(context, instance)
3534 
3535     @messaging.expected_exceptions(NotImplementedError)
3536     @wrap_exception()
3537     def volume_snapshot_create(self, context, instance, volume_id,
3538                                create_info):
3539         self.driver.volume_snapshot_create(context, instance, volume_id,
3540                                            create_info)
3541 
3542     @messaging.expected_exceptions(NotImplementedError)
3543     @wrap_exception()
3544     def volume_snapshot_delete(self, context, instance, volume_id,
3545                                snapshot_id, delete_info):
3546         self.driver.volume_snapshot_delete(context, instance, volume_id,
3547                                            snapshot_id, delete_info)
3548 
3549     @wrap_instance_fault
3550     def _rotate_backups(self, context, instance, backup_type, rotation):
3551         """Delete excess backups associated to an instance.
3552 
3553         Instances are allowed a fixed number of backups (the rotation number);
3554         this method deletes the oldest backups that exceed the rotation
3555         threshold.
3556 
3557         :param context: security context
3558         :param instance: Instance dict
3559         :param backup_type: a user-defined type, like "daily" or "weekly" etc.
3560         :param rotation: int representing how many backups to keep around;
3561             None if rotation shouldn't be used (as in the case of snapshots)
3562         """
3563         filters = {'property-image_type': 'backup',
3564                    'property-backup_type': backup_type,
3565                    'property-instance_uuid': instance.uuid}
3566 
3567         images = self.image_api.get_all(context, filters=filters,
3568                                         sort_key='created_at', sort_dir='desc')
3569         num_images = len(images)
3570         LOG.debug("Found %(num_images)d images (rotation: %(rotation)d)",
3571                   {'num_images': num_images, 'rotation': rotation},
3572                   instance=instance)
3573 
3574         if num_images > rotation:
3575             # NOTE(sirp): this deletes all backups that exceed the rotation
3576             # limit
3577             excess = len(images) - rotation
3578             LOG.debug("Rotating out %d backups", excess,
3579                       instance=instance)
3580             for i in range(excess):
3581                 image = images.pop()
3582                 image_id = image['id']
3583                 LOG.debug("Deleting image %s", image_id,
3584                           instance=instance)
3585                 try:
3586                     self.image_api.delete(context, image_id)
3587                 except exception.ImageNotFound:
3588                     LOG.info("Failed to find image %(image_id)s to "
3589                              "delete", {'image_id': image_id},
3590                              instance=instance)
3591                 except (exception.ImageDeleteConflict, Exception) as exc:
3592                     LOG.info("Failed to delete image %(image_id)s during "
3593                              "deleting excess backups. "
3594                              "Continuing for next image.. %(exc)s",
3595                              {'image_id': image_id, 'exc': exc},
3596                              instance=instance)
3597 
3598     @wrap_exception()
3599     @reverts_task_state
3600     @wrap_instance_event(prefix='compute')
3601     @wrap_instance_fault
3602     def set_admin_password(self, context, instance, new_pass):
3603         """Set the root/admin password for an instance on this host.
3604 
3605         This is generally only called by API password resets after an
3606         image has been built.
3607 
3608         @param context: Nova auth context.
3609         @param instance: Nova instance object.
3610         @param new_pass: The admin password for the instance.
3611         """
3612 
3613         context = context.elevated()
3614         if new_pass is None:
3615             # Generate a random password
3616             new_pass = utils.generate_password()
3617 
3618         current_power_state = self._get_power_state(context, instance)
3619         expected_state = power_state.RUNNING
3620 
3621         if current_power_state != expected_state:
3622             instance.task_state = None
3623             instance.save(expected_task_state=task_states.UPDATING_PASSWORD)
3624             _msg = _('instance %s is not running') % instance.uuid
3625             raise exception.InstancePasswordSetFailed(
3626                 instance=instance.uuid, reason=_msg)
3627 
3628         try:
3629             self.driver.set_admin_password(instance, new_pass)
3630             LOG.info("Admin password set", instance=instance)
3631             instance.task_state = None
3632             instance.save(
3633                 expected_task_state=task_states.UPDATING_PASSWORD)
3634         except exception.InstanceAgentNotEnabled:
3635             with excutils.save_and_reraise_exception():
3636                 LOG.debug('Guest agent is not enabled for the instance.',
3637                           instance=instance)
3638                 instance.task_state = None
3639                 instance.save(
3640                     expected_task_state=task_states.UPDATING_PASSWORD)
3641         except exception.SetAdminPasswdNotSupported:
3642             with excutils.save_and_reraise_exception():
3643                 LOG.info('set_admin_password is not supported '
3644                          'by this driver or guest instance.',
3645                          instance=instance)
3646                 instance.task_state = None
3647                 instance.save(
3648                     expected_task_state=task_states.UPDATING_PASSWORD)
3649         except NotImplementedError:
3650             LOG.warning('set_admin_password is not implemented '
3651                         'by this driver or guest instance.',
3652                         instance=instance)
3653             instance.task_state = None
3654             instance.save(
3655                 expected_task_state=task_states.UPDATING_PASSWORD)
3656             raise NotImplementedError(_('set_admin_password is not '
3657                                         'implemented by this driver or guest '
3658                                         'instance.'))
3659         except exception.UnexpectedTaskStateError:
3660             # interrupted by another (most likely delete) task
3661             # do not retry
3662             raise
3663         except Exception:
3664             # Catch all here because this could be anything.
3665             LOG.exception('set_admin_password failed', instance=instance)
3666             # We create a new exception here so that we won't
3667             # potentially reveal password information to the
3668             # API caller.  The real exception is logged above
3669             _msg = _('error setting admin password')
3670             raise exception.InstancePasswordSetFailed(
3671                 instance=instance.uuid, reason=_msg)
3672 
3673     @wrap_exception()
3674     @reverts_task_state
3675     @wrap_instance_fault
3676     def inject_file(self, context, path, file_contents, instance):
3677         """Write a file to the specified path in an instance on this host."""
3678         # NOTE(russellb) Remove this method, as well as the underlying virt
3679         # driver methods, when the compute rpc interface is bumped to 4.x
3680         # as it is no longer used.
3681         context = context.elevated()
3682         current_power_state = self._get_power_state(context, instance)
3683         expected_state = power_state.RUNNING
3684         if current_power_state != expected_state:
3685             LOG.warning('trying to inject a file into a non-running '
3686                         '(state: %(current_state)s expected: '
3687                         '%(expected_state)s)',
3688                         {'current_state': current_power_state,
3689                          'expected_state': expected_state},
3690                         instance=instance)
3691         LOG.info('injecting file to %s', path, instance=instance)
3692         self.driver.inject_file(instance, path, file_contents)
3693 
3694     def _get_rescue_image(self, context, instance, rescue_image_ref=None):
3695         """Determine what image should be used to boot the rescue VM."""
3696         # 1. If rescue_image_ref is passed in, use that for rescue.
3697         # 2. Else, use the base image associated with instance's current image.
3698         #       The idea here is to provide the customer with a rescue
3699         #       environment which they are familiar with.
3700         #       So, if they built their instance off of a Debian image,
3701         #       their rescue VM will also be Debian.
3702         # 3. As a last resort, use instance's current image.
3703         if not rescue_image_ref:
3704             system_meta = utils.instance_sys_meta(instance)
3705             rescue_image_ref = system_meta.get('image_base_image_ref')
3706 
3707         if not rescue_image_ref:
3708             LOG.warning('Unable to find a different image to use for '
3709                         'rescue VM, using instance\'s current image',
3710                         instance=instance)
3711             rescue_image_ref = instance.image_ref
3712 
3713         return objects.ImageMeta.from_image_ref(
3714             context, self.image_api, rescue_image_ref)
3715 
3716     @wrap_exception()
3717     @reverts_task_state
3718     @wrap_instance_event(prefix='compute')
3719     @wrap_instance_fault
3720     def rescue_instance(self, context, instance, rescue_password,
3721                         rescue_image_ref, clean_shutdown):
3722         context = context.elevated()
3723         LOG.info('Rescuing', instance=instance)
3724 
3725         admin_password = (rescue_password if rescue_password else
3726                       utils.generate_password())
3727 
3728         network_info = self.network_api.get_instance_nw_info(context, instance)
3729 
3730         rescue_image_meta = self._get_rescue_image(context, instance,
3731                                                    rescue_image_ref)
3732 
3733         extra_usage_info = {'rescue_image_name':
3734                             self._get_image_name(rescue_image_meta)}
3735         self._notify_about_instance_usage(context, instance,
3736                 "rescue.start", extra_usage_info=extra_usage_info,
3737                 network_info=network_info)
3738         compute_utils.notify_about_instance_rescue_action(
3739             context, instance, self.host, rescue_image_ref,
3740             phase=fields.NotificationPhase.START)
3741 
3742         try:
3743             self._power_off_instance(context, instance, clean_shutdown)
3744 
3745             self.driver.rescue(context, instance,
3746                                network_info,
3747                                rescue_image_meta, admin_password)
3748         except Exception as e:
3749             LOG.exception("Error trying to Rescue Instance",
3750                           instance=instance)
3751             self._set_instance_obj_error_state(context, instance)
3752             raise exception.InstanceNotRescuable(
3753                 instance_id=instance.uuid,
3754                 reason=_("Driver Error: %s") % e)
3755 
3756         compute_utils.notify_usage_exists(self.notifier, context, instance,
3757                                           self.host, current_period=True)
3758 
3759         instance.vm_state = vm_states.RESCUED
3760         instance.task_state = None
3761         instance.power_state = self._get_power_state(context, instance)
3762         instance.launched_at = timeutils.utcnow()
3763         instance.save(expected_task_state=task_states.RESCUING)
3764 
3765         self._notify_about_instance_usage(context, instance,
3766                 "rescue.end", extra_usage_info=extra_usage_info,
3767                 network_info=network_info)
3768         compute_utils.notify_about_instance_rescue_action(
3769             context, instance, self.host, rescue_image_ref,
3770             phase=fields.NotificationPhase.END)
3771 
3772     @wrap_exception()
3773     @reverts_task_state
3774     @wrap_instance_event(prefix='compute')
3775     @wrap_instance_fault
3776     def unrescue_instance(self, context, instance):
3777         context = context.elevated()
3778         LOG.info('Unrescuing', instance=instance)
3779 
3780         network_info = self.network_api.get_instance_nw_info(context, instance)
3781         self._notify_about_instance_usage(context, instance,
3782                 "unrescue.start", network_info=network_info)
3783         compute_utils.notify_about_instance_action(context, instance,
3784             self.host, action=fields.NotificationAction.UNRESCUE,
3785             phase=fields.NotificationPhase.START)
3786 
3787         with self._error_out_instance_on_exception(context, instance):
3788             self.driver.unrescue(instance,
3789                                  network_info)
3790 
3791         instance.vm_state = vm_states.ACTIVE
3792         instance.task_state = None
3793         instance.power_state = self._get_power_state(context, instance)
3794         instance.save(expected_task_state=task_states.UNRESCUING)
3795 
3796         self._notify_about_instance_usage(context,
3797                                           instance,
3798                                           "unrescue.end",
3799                                           network_info=network_info)
3800         compute_utils.notify_about_instance_action(context, instance,
3801             self.host, action=fields.NotificationAction.UNRESCUE,
3802             phase=fields.NotificationPhase.END)
3803 
3804     @wrap_exception()
3805     @wrap_instance_fault
3806     def change_instance_metadata(self, context, diff, instance):
3807         """Update the metadata published to the instance."""
3808         LOG.debug("Changing instance metadata according to %r",
3809                   diff, instance=instance)
3810         self.driver.change_instance_metadata(context, instance, diff)
3811 
3812     @wrap_exception()
3813     @wrap_instance_event(prefix='compute')
3814     @wrap_instance_fault
3815     def confirm_resize(self, context, instance, migration):
3816         """Confirms a migration/resize and deletes the 'old' instance.
3817 
3818         This is called from the API and runs on the source host.
3819 
3820         Nothing needs to happen on the destination host at this point since
3821         the instance is already running there. This routine just cleans up the
3822         source host.
3823         """
3824         @utils.synchronized(instance.uuid)
3825         def do_confirm_resize(context, instance, migration_id):
3826             # NOTE(wangpan): Get the migration status from db, if it has been
3827             #                confirmed, we do nothing and return here
3828             LOG.debug("Going to confirm migration %s", migration_id,
3829                       instance=instance)
3830             try:
3831                 # TODO(russellb) Why are we sending the migration object just
3832                 # to turn around and look it up from the db again?
3833                 migration = objects.Migration.get_by_id(
3834                                     context.elevated(), migration_id)
3835             except exception.MigrationNotFound:
3836                 LOG.error("Migration %s is not found during confirmation",
3837                           migration_id, instance=instance)
3838                 return
3839 
3840             if migration.status == 'confirmed':
3841                 LOG.info("Migration %s is already confirmed",
3842                          migration_id, instance=instance)
3843                 return
3844             elif migration.status not in ('finished', 'confirming'):
3845                 LOG.warning("Unexpected confirmation status '%(status)s' "
3846                             "of migration %(id)s, exit confirmation process",
3847                             {"status": migration.status, "id": migration_id},
3848                             instance=instance)
3849                 return
3850 
3851             # NOTE(wangpan): Get the instance from db, if it has been
3852             #                deleted, we do nothing and return here
3853             expected_attrs = ['metadata', 'system_metadata', 'flavor']
3854             try:
3855                 instance = objects.Instance.get_by_uuid(
3856                         context, instance.uuid,
3857                         expected_attrs=expected_attrs)
3858             except exception.InstanceNotFound:
3859                 LOG.info("Instance is not found during confirmation",
3860                          instance=instance)
3861                 return
3862 
3863             self._confirm_resize(context, instance, migration=migration)
3864 
3865         do_confirm_resize(context, instance, migration.id)
3866 
3867     def _confirm_resize(self, context, instance, migration=None):
3868         """Destroys the source instance."""
3869         self._notify_about_instance_usage(context, instance,
3870                                           "resize.confirm.start")
3871         compute_utils.notify_about_instance_action(context, instance,
3872             self.host, action=fields.NotificationAction.RESIZE_CONFIRM,
3873             phase=fields.NotificationPhase.START)
3874 
3875         with self._error_out_instance_on_exception(context, instance):
3876             # NOTE(danms): delete stashed migration information
3877             old_instance_type = instance.old_flavor
3878             instance.old_flavor = None
3879             instance.new_flavor = None
3880             instance.system_metadata.pop('old_vm_state', None)
3881             instance.save()
3882 
3883             # NOTE(tr3buchet): tear down networks on source host
3884             self.network_api.setup_networks_on_host(context, instance,
3885                                migration.source_compute, teardown=True)
3886 
3887             network_info = self.network_api.get_instance_nw_info(context,
3888                                                                  instance)
3889             # TODO(mriedem): Get BDMs here and pass them to the driver.
3890             self.driver.confirm_migration(context, migration, instance,
3891                                           network_info)
3892 
3893             migration.status = 'confirmed'
3894             with migration.obj_as_admin():
3895                 migration.save()
3896 
3897             rt = self._get_resource_tracker()
3898             rt.drop_move_claim(context, instance, migration.source_node,
3899                                old_instance_type, prefix='old_')
3900             self._delete_allocation_after_move(context, instance,
3901                                                migration,
3902                                                old_instance_type,
3903                                                migration.source_node)
3904             instance.drop_migration_context()
3905 
3906             # NOTE(mriedem): The old_vm_state could be STOPPED but the user
3907             # might have manually powered up the instance to confirm the
3908             # resize/migrate, so we need to check the current power state
3909             # on the instance and set the vm_state appropriately. We default
3910             # to ACTIVE because if the power state is not SHUTDOWN, we
3911             # assume _sync_instance_power_state will clean it up.
3912             p_state = instance.power_state
3913             vm_state = None
3914             if p_state == power_state.SHUTDOWN:
3915                 vm_state = vm_states.STOPPED
3916                 LOG.debug("Resized/migrated instance is powered off. "
3917                           "Setting vm_state to '%s'.", vm_state,
3918                           instance=instance)
3919             else:
3920                 vm_state = vm_states.ACTIVE
3921 
3922             instance.vm_state = vm_state
3923             instance.task_state = None
3924             instance.save(expected_task_state=[None, task_states.DELETING,
3925                                                task_states.SOFT_DELETING])
3926 
3927             self._notify_about_instance_usage(
3928                 context, instance, "resize.confirm.end",
3929                 network_info=network_info)
3930             compute_utils.notify_about_instance_action(context, instance,
3931                    self.host, action=fields.NotificationAction.RESIZE_CONFIRM,
3932                    phase=fields.NotificationPhase.END)
3933 
3934     def _delete_allocation_after_move(self, context, instance, migration,
3935                                       flavor, nodename):
3936         rt = self._get_resource_tracker()
3937         cn_uuid = rt.get_node_uuid(nodename)
3938 
3939         if migration.source_node == nodename:
3940             if migration.status in ('confirmed', 'completed'):
3941                 try:
3942                     # NOTE(danms): We're finishing on the source node, so try
3943                     # to delete the allocation based on the migration uuid
3944                     deleted = self.reportclient.delete_allocation_for_instance(
3945                         context, migration.uuid)
3946                     if deleted:
3947                         LOG.info(_('Source node %(node)s confirmed migration '
3948                                    '%(mig)s; deleted migration-based '
3949                                    'allocation'),
3950                                  {'node': nodename, 'mig': migration.uuid})
3951                         # NOTE(danms): We succeeded, which means we do not
3952                         # need to do the complex double allocation dance
3953                         return
3954                 except exception.AllocationDeleteFailed:
3955                     LOG.error('Deleting allocation in placement for migration '
3956                               '%(migration_uuid)s failed. The instance '
3957                               '%(instance_uuid)s will be put to ERROR state '
3958                               'but the allocation held by the migration is '
3959                               'leaked.',
3960                               {'instance_uuid': instance.uuid,
3961                                'migration_uuid': migration.uuid})
3962                     raise
3963             else:
3964                 # We're reverting (or failed) on the source, so we
3965                 # need to check if our migration holds a claim and if
3966                 # so, avoid doing the legacy behavior below.
3967                 # NOTE(gibi): We are only hitting this in case of reverting
3968                 # a resize-on-same-host operation
3969                 mig_allocs = (
3970                     self.reportclient.get_allocations_for_consumer_by_provider(
3971                         context, cn_uuid, migration.uuid))
3972                 if mig_allocs:
3973                     LOG.info(_('Source node %(node)s reverted migration '
3974                                '%(mig)s; not deleting migration-based '
3975                                'allocation'),
3976                              {'node': nodename, 'mig': migration.uuid})
3977                     return
3978         elif migration.dest_node == nodename:
3979             # NOTE(danms): We're reverting on the destination node
3980             # (and we must not be doing a same-host migration if we
3981             # made it past the check above), so we need to check to
3982             # see if the source did migration-based allocation
3983             # accounting
3984             allocs = self.reportclient.get_allocations_for_consumer(
3985                 context, migration.uuid)
3986             if allocs:
3987                 # NOTE(danms): The source did migration-based allocation
3988                 # accounting, so we should let the source node rejigger
3989                 # the allocations in finish_resize_revert()
3990                 LOG.info(_('Destination node %(node)s reverted migration '
3991                            '%(mig)s; not deleting migration-based '
3992                            'allocation'),
3993                          {'node': nodename, 'mig': migration.uuid})
3994                 return
3995 
3996         # TODO(danms): Remove below this line when we remove compatibility
3997         # for double-accounting migrations (likely rocky)
3998         LOG.info(_('Doing legacy allocation math for migration %(mig)s after '
3999                    'instance move'),
4000                  {'mig': migration.uuid},
4001                  instance=instance)
4002 
4003         # NOTE(jaypipes): This sucks, but due to the fact that confirm_resize()
4004         # only runs on the source host and revert_resize() runs on the
4005         # destination host, we need to do this here. Basically, what we're
4006         # doing here is grabbing the existing allocations for this instance
4007         # from the placement API, dropping the resources in the doubled-up
4008         # allocation set that refer to the source host UUID and calling PUT
4009         # /allocations back to the placement API. The allocation that gets
4010         # PUT'd back to placement will only include the destination host and
4011         # any shared providers in the case of a confirm_resize operation and
4012         # the source host and shared providers for a revert_resize operation..
4013         if not scheduler_utils.remove_allocation_from_compute(
4014                 context, instance, cn_uuid, self.reportclient, flavor):
4015             LOG.error("Failed to save manipulated allocation",
4016                       instance=instance)
4017 
4018     @wrap_exception()
4019     @reverts_task_state
4020     @wrap_instance_event(prefix='compute')
4021     @errors_out_migration
4022     @wrap_instance_fault
4023     def revert_resize(self, context, instance, migration):
4024         """Destroys the new instance on the destination machine.
4025 
4026         Reverts the model changes, and powers on the old instance on the
4027         source machine.
4028 
4029         """
4030         # NOTE(comstud): A revert_resize is essentially a resize back to
4031         # the old size, so we need to send a usage event here.
4032         compute_utils.notify_usage_exists(self.notifier, context, instance,
4033                                           self.host, current_period=True)
4034 
4035         with self._error_out_instance_on_exception(context, instance):
4036             # NOTE(tr3buchet): tear down networks on destination host
4037             self.network_api.setup_networks_on_host(context, instance,
4038                                                     teardown=True)
4039 
4040             migration_p = obj_base.obj_to_primitive(migration)
4041             self.network_api.migrate_instance_start(context,
4042                                                     instance,
4043                                                     migration_p)
4044 
4045             network_info = self.network_api.get_instance_nw_info(context,
4046                                                                  instance)
4047             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
4048                     context, instance.uuid)
4049             block_device_info = self._get_instance_block_device_info(
4050                                 context, instance, bdms=bdms)
4051 
4052             destroy_disks = not self._is_instance_storage_shared(
4053                 context, instance, host=migration.source_compute)
4054             self.driver.destroy(context, instance, network_info,
4055                                 block_device_info, destroy_disks)
4056 
4057             self._terminate_volume_connections(context, instance, bdms)
4058 
4059             migration.status = 'reverted'
4060             with migration.obj_as_admin():
4061                 migration.save()
4062 
4063             # NOTE(ndipanov): We need to do this here because dropping the
4064             # claim means we lose the migration_context data. We really should
4065             # fix this by moving the drop_move_claim call to the
4066             # finish_revert_resize method as this is racy (revert is dropped,
4067             # but instance resources will be tracked with the new flavor until
4068             # it gets rolled back in finish_revert_resize, which is
4069             # potentially wrong for a period of time).
4070             instance.revert_migration_context()
4071             instance.save()
4072 
4073             rt = self._get_resource_tracker()
4074             rt.drop_move_claim(context, instance, instance.node)
4075             self._delete_allocation_after_move(context, instance, migration,
4076                                                instance.flavor,
4077                                                instance.node)
4078 
4079             # RPC cast back to the source host to finish the revert there.
4080             self.compute_rpcapi.finish_revert_resize(context, instance,
4081                     migration, migration.source_compute)
4082 
4083     @wrap_exception()
4084     @reverts_task_state
4085     @wrap_instance_event(prefix='compute')
4086     @errors_out_migration
4087     @wrap_instance_fault
4088     def finish_revert_resize(self, context, instance, migration):
4089         """Finishes the second half of reverting a resize on the source host.
4090 
4091         Bring the original source instance state back (active/shutoff) and
4092         revert the resized attributes in the database.
4093 
4094         """
4095         with self._error_out_instance_on_exception(context, instance):
4096             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
4097                 context, instance.uuid)
4098             self._notify_about_instance_usage(
4099                     context, instance, "resize.revert.start")
4100             compute_utils.notify_about_instance_action(context, instance,
4101                 self.host, action=fields.NotificationAction.RESIZE_REVERT,
4102                     phase=fields.NotificationPhase.START, bdms=bdms)
4103 
4104             # NOTE(mriedem): delete stashed old_vm_state information; we
4105             # default to ACTIVE for backwards compatibility if old_vm_state
4106             # is not set
4107             old_vm_state = instance.system_metadata.pop('old_vm_state',
4108                                                         vm_states.ACTIVE)
4109 
4110             self._set_instance_info(instance, instance.old_flavor)
4111             instance.old_flavor = None
4112             instance.new_flavor = None
4113             instance.host = migration.source_compute
4114             instance.node = migration.source_node
4115             instance.save()
4116 
4117             try:
4118                 self._revert_allocation(context, instance, migration)
4119             except exception.AllocationMoveFailed:
4120                 LOG.error('Reverting allocation in placement for migration '
4121                           '%(migration_uuid)s failed. The instance '
4122                           '%(instance_uuid)s will be put into ERROR state but '
4123                           'the allocation held by the migration is leaked.',
4124                           {'instance_uuid': instance.uuid,
4125                            'migration_uuid': migration.uuid})
4126                 raise
4127 
4128             self.network_api.setup_networks_on_host(context, instance,
4129                                                     migration.source_compute)
4130             migration_p = obj_base.obj_to_primitive(migration)
4131             # NOTE(hanrong): we need to change migration_p['dest_compute'] to
4132             # source host temporarily. "network_api.migrate_instance_finish"
4133             # will setup the network for the instance on the destination host.
4134             # For revert resize, the instance will back to the source host, the
4135             # setup of the network for instance should be on the source host.
4136             # So set the migration_p['dest_compute'] to source host at here.
4137             migration_p['dest_compute'] = migration.source_compute
4138             self.network_api.migrate_instance_finish(context,
4139                                                      instance,
4140                                                      migration_p)
4141             network_info = self.network_api.get_instance_nw_info(context,
4142                                                                  instance)
4143 
4144             # revert_resize deleted any volume attachments for the instance
4145             # and created new ones to be used on this host, but we
4146             # have to update those attachments with the host connector so the
4147             # BDM.connection_info will get set in the call to
4148             # _get_instance_block_device_info below with refresh_conn_info=True
4149             # and then the volumes can be re-connected via the driver on this
4150             # host.
4151             self._update_volume_attachments(context, instance, bdms)
4152 
4153             block_device_info = self._get_instance_block_device_info(
4154                     context, instance, refresh_conn_info=True, bdms=bdms)
4155 
4156             power_on = old_vm_state != vm_states.STOPPED
4157             self.driver.finish_revert_migration(context, instance,
4158                                        network_info,
4159                                        block_device_info, power_on)
4160 
4161             instance.drop_migration_context()
4162             instance.launched_at = timeutils.utcnow()
4163             instance.save(expected_task_state=task_states.RESIZE_REVERTING)
4164 
4165             # Complete any volume attachments so the volumes are in-use.
4166             self._complete_volume_attachments(context, bdms)
4167 
4168             # if the original vm state was STOPPED, set it back to STOPPED
4169             LOG.info("Updating instance to original state: '%s'",
4170                      old_vm_state, instance=instance)
4171             if power_on:
4172                 instance.vm_state = vm_states.ACTIVE
4173                 instance.task_state = None
4174                 instance.save()
4175             else:
4176                 instance.task_state = task_states.POWERING_OFF
4177                 instance.save()
4178                 self.stop_instance(context, instance=instance,
4179                                    clean_shutdown=True)
4180 
4181             self._notify_about_instance_usage(
4182                     context, instance, "resize.revert.end")
4183             compute_utils.notify_about_instance_action(context, instance,
4184                 self.host, action=fields.NotificationAction.RESIZE_REVERT,
4185                     phase=fields.NotificationPhase.END, bdms=bdms)
4186 
4187     def _revert_allocation(self, context, instance, migration):
4188         """Revert an allocation that is held by migration to our instance."""
4189 
4190         # Fetch the original allocation that the instance had on the source
4191         # node, which are now held by the migration
4192         orig_alloc = self.reportclient.get_allocations_for_consumer(
4193             context, migration.uuid)
4194         if not orig_alloc:
4195             # NOTE(danms): This migration did not do per-migration allocation
4196             # accounting, so nothing to do here.
4197             LOG.info('Old-style migration %(mig)s is being reverted; '
4198                      'no migration claims found on original node '
4199                      'to swap.',
4200                      {'mig': migration.uuid},
4201                      instance=instance)
4202             return False
4203 
4204         if len(orig_alloc) > 1:
4205             # NOTE(danms): This may change later if we have other allocations
4206             # against other providers that need to be held by the migration
4207             # as well. Perhaps something like shared storage resources that
4208             # will actually be duplicated during a resize type operation.
4209             LOG.error('New-style migration %(mig)s has allocations against '
4210                       'more than one provider %(rps)s. This should not be '
4211                       'possible, but reverting it anyway.',
4212                       {'mig': migration.uuid,
4213                        'rps': ','.join(orig_alloc.keys())},
4214                       instance=instance)
4215 
4216         # We only have a claim against one provider, it is the source node
4217         cn_uuid = list(orig_alloc.keys())[0]
4218 
4219         # FIXME(danms): This method is flawed in that it asssumes allocations
4220         # against only one provider. So, this may overwite allocations against
4221         # a shared provider, if we had one.
4222         LOG.info('Swapping old allocation on %(node)s held by migration '
4223                  '%(mig)s for instance',
4224                  {'node': cn_uuid, 'mig': migration.uuid},
4225                  instance=instance)
4226         # TODO(cdent): Should we be doing anything with return values here?
4227         self.reportclient.move_allocations(context, migration.uuid,
4228                                            instance.uuid)
4229         return True
4230 
4231     def _prep_resize(self, context, image, instance, instance_type,
4232                      filter_properties, node, migration, clean_shutdown=True):
4233 
4234         if not filter_properties:
4235             filter_properties = {}
4236 
4237         if not instance.host:
4238             self._set_instance_obj_error_state(context, instance)
4239             msg = _('Instance has no source host')
4240             raise exception.MigrationError(reason=msg)
4241 
4242         same_host = instance.host == self.host
4243         # if the flavor IDs match, it's migrate; otherwise resize
4244         if same_host and instance_type.id == instance['instance_type_id']:
4245             # check driver whether support migrate to same host
4246             if not self.driver.capabilities.get(
4247                     'supports_migrate_to_same_host', False):
4248                 raise exception.UnableToMigrateToSelf(
4249                     instance_id=instance.uuid, host=self.host)
4250 
4251         # NOTE(danms): Stash the new instance_type to avoid having to
4252         # look it up in the database later
4253         instance.new_flavor = instance_type
4254         # NOTE(mriedem): Stash the old vm_state so we can set the
4255         # resized/reverted instance back to the same state later.
4256         vm_state = instance.vm_state
4257         LOG.debug('Stashing vm_state: %s', vm_state, instance=instance)
4258         instance.system_metadata['old_vm_state'] = vm_state
4259         instance.save()
4260 
4261         limits = filter_properties.get('limits', {})
4262         rt = self._get_resource_tracker()
4263         with rt.resize_claim(context, instance, instance_type, node,
4264                              migration, image_meta=image,
4265                              limits=limits) as claim:
4266             LOG.info('Migrating', instance=instance)
4267             # RPC cast to the source host to start the actual resize/migration.
4268             self.compute_rpcapi.resize_instance(
4269                     context, instance, claim.migration, image,
4270                     instance_type, clean_shutdown)
4271 
4272     @wrap_exception()
4273     @reverts_task_state
4274     @wrap_instance_event(prefix='compute')
4275     @wrap_instance_fault
4276     def prep_resize(self, context, image, instance, instance_type,
4277                     request_spec, filter_properties, node,
4278                     clean_shutdown, migration, host_list):
4279         """Initiates the process of moving a running instance to another host.
4280 
4281         Possibly changes the VCPU, RAM and disk size in the process.
4282 
4283         This is initiated from conductor and runs on the destination host.
4284 
4285         The main purpose of this method is performing some checks on the
4286         destination host and making a claim for resources. If the claim fails
4287         then a reschedule to another host may be attempted which involves
4288         calling back to conductor to start the process over again.
4289         """
4290         if node is None:
4291             node = self._get_nodename(instance, refresh=True)
4292 
4293         with self._error_out_instance_on_exception(context, instance), \
4294                  errors_out_migration_ctxt(migration):
4295             compute_utils.notify_usage_exists(self.notifier, context, instance,
4296                                               self.host, current_period=True)
4297             self._notify_about_instance_usage(
4298                     context, instance, "resize.prep.start")
4299             compute_utils.notify_about_resize_prep_instance(
4300                 context, instance, self.host,
4301                 fields.NotificationPhase.START, instance_type)
4302             try:
4303                 self._prep_resize(context, image, instance,
4304                                   instance_type, filter_properties,
4305                                   node, migration, clean_shutdown)
4306             except Exception:
4307                 # Since we hit a failure, we're either rescheduling or dead
4308                 # and either way we need to cleanup any allocations created
4309                 # by the scheduler for the destination node.
4310                 if migration and not self._revert_allocation(
4311                         context, instance, migration):
4312                     # We did not do a migration-based
4313                     # allocation. Note that for a resize to the
4314                     # same host, the scheduler will merge the
4315                     # flavors, so here we'd be subtracting the new
4316                     # flavor from the allocated resources on this
4317                     # node.
4318                     # FIXME(danms): Remove this in Rocky
4319                     rt = self._get_resource_tracker()
4320                     rt.delete_allocation_for_failed_resize(
4321                         context, instance, node, instance_type)
4322                 # try to re-schedule the resize elsewhere:
4323                 exc_info = sys.exc_info()
4324                 self._reschedule_resize_or_reraise(context, image, instance,
4325                         exc_info, instance_type, request_spec,
4326                         filter_properties, host_list)
4327             finally:
4328                 extra_usage_info = dict(
4329                         new_instance_type=instance_type.name,
4330                         new_instance_type_id=instance_type.id)
4331 
4332                 self._notify_about_instance_usage(
4333                     context, instance, "resize.prep.end",
4334                     extra_usage_info=extra_usage_info)
4335                 compute_utils.notify_about_resize_prep_instance(
4336                     context, instance, self.host,
4337                     fields.NotificationPhase.END, instance_type)
4338 
4339     def _reschedule_resize_or_reraise(self, context, image, instance, exc_info,
4340             instance_type, request_spec, filter_properties, host_list):
4341         """Try to re-schedule the resize or re-raise the original error to
4342         error out the instance.
4343         """
4344         if not request_spec:
4345             request_spec = {}
4346         if not filter_properties:
4347             filter_properties = {}
4348 
4349         rescheduled = False
4350         instance_uuid = instance.uuid
4351 
4352         try:
4353             reschedule_method = self.compute_task_api.resize_instance
4354             scheduler_hint = dict(filter_properties=filter_properties)
4355             method_args = (instance, None, scheduler_hint, instance_type)
4356             task_state = task_states.RESIZE_PREP
4357 
4358             rescheduled = self._reschedule(context, request_spec,
4359                     filter_properties, instance, reschedule_method,
4360                     method_args, task_state, exc_info, host_list=host_list)
4361         except Exception as error:
4362             rescheduled = False
4363             LOG.exception("Error trying to reschedule",
4364                           instance_uuid=instance_uuid)
4365             compute_utils.add_instance_fault_from_exc(context,
4366                     instance, error,
4367                     exc_info=sys.exc_info())
4368             self._notify_about_instance_usage(context, instance,
4369                     'resize.error', fault=error)
4370             compute_utils.notify_about_instance_action(
4371                 context, instance, self.host,
4372                 action=fields.NotificationAction.RESIZE,
4373                 phase=fields.NotificationPhase.ERROR,
4374                 exception=error,
4375                 tb=','.join(traceback.format_exception(*exc_info)))
4376         if rescheduled:
4377             self._log_original_error(exc_info, instance_uuid)
4378             compute_utils.add_instance_fault_from_exc(context,
4379                     instance, exc_info[1], exc_info=exc_info)
4380             self._notify_about_instance_usage(context, instance,
4381                     'resize.error', fault=exc_info[1])
4382             compute_utils.notify_about_instance_action(
4383                 context, instance, self.host,
4384                 action=fields.NotificationAction.RESIZE,
4385                 phase=fields.NotificationPhase.ERROR,
4386                 exception=exc_info[1],
4387                 tb=','.join(traceback.format_exception(*exc_info)))
4388         else:
4389             # not re-scheduling
4390             six.reraise(*exc_info)
4391 
4392     @wrap_exception()
4393     @reverts_task_state
4394     @wrap_instance_event(prefix='compute')
4395     @wrap_instance_fault
4396     def resize_instance(self, context, instance, image,
4397                         migration, instance_type, clean_shutdown):
4398         """Starts the migration of a running instance to another host.
4399 
4400         This is initiated from the destination host's ``prep_resize`` routine
4401         and runs on the source host.
4402         """
4403         try:
4404             self._resize_instance(context, instance, image, migration,
4405                                   instance_type, clean_shutdown)
4406         except Exception:
4407             with excutils.save_and_reraise_exception():
4408                 self._revert_allocation(context, instance, migration)
4409 
4410     def _resize_instance(self, context, instance, image,
4411                          migration, instance_type, clean_shutdown):
4412         with self._error_out_instance_on_exception(context, instance), \
4413              errors_out_migration_ctxt(migration):
4414             network_info = self.network_api.get_instance_nw_info(context,
4415                                                                  instance)
4416 
4417             migration.status = 'migrating'
4418             with migration.obj_as_admin():
4419                 migration.save()
4420 
4421             instance.task_state = task_states.RESIZE_MIGRATING
4422             instance.save(expected_task_state=task_states.RESIZE_PREP)
4423 
4424             self._notify_about_instance_usage(
4425                 context, instance, "resize.start", network_info=network_info)
4426 
4427             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
4428                     context, instance.uuid)
4429 
4430             compute_utils.notify_about_instance_action(context, instance,
4431                    self.host, action=fields.NotificationAction.RESIZE,
4432                    phase=fields.NotificationPhase.START, bdms=bdms)
4433 
4434             block_device_info = self._get_instance_block_device_info(
4435                                 context, instance, bdms=bdms)
4436 
4437             timeout, retry_interval = self._get_power_off_values(context,
4438                                             instance, clean_shutdown)
4439             disk_info = self.driver.migrate_disk_and_power_off(
4440                     context, instance, migration.dest_host,
4441                     instance_type, network_info,
4442                     block_device_info,
4443                     timeout, retry_interval)
4444 
4445             self._terminate_volume_connections(context, instance, bdms)
4446 
4447             migration_p = obj_base.obj_to_primitive(migration)
4448             self.network_api.migrate_instance_start(context,
4449                                                     instance,
4450                                                     migration_p)
4451 
4452             migration.status = 'post-migrating'
4453             with migration.obj_as_admin():
4454                 migration.save()
4455 
4456             instance.host = migration.dest_compute
4457             instance.node = migration.dest_node
4458             instance.task_state = task_states.RESIZE_MIGRATED
4459             instance.save(expected_task_state=task_states.RESIZE_MIGRATING)
4460 
4461             # RPC cast to the destination host to finish the resize/migration.
4462             self.compute_rpcapi.finish_resize(context, instance,
4463                     migration, image, disk_info, migration.dest_compute)
4464 
4465         self._notify_about_instance_usage(context, instance, "resize.end",
4466                                           network_info=network_info)
4467 
4468         compute_utils.notify_about_instance_action(context, instance,
4469                self.host, action=fields.NotificationAction.RESIZE,
4470                phase=fields.NotificationPhase.END, bdms=bdms)
4471         self.instance_events.clear_events_for_instance(instance)
4472 
4473     def _terminate_volume_connections(self, context, instance, bdms):
4474         connector = None
4475         for bdm in bdms:
4476             if bdm.is_volume:
4477                 if bdm.attachment_id:
4478                     # NOTE(jdg): So here's the thing, the idea behind the new
4479                     # attach API's was to have a new code fork/path that we
4480                     # followed, we're not going to do that so we have to do
4481                     # some extra work in here to make it *behave* just like the
4482                     # old code. Cinder doesn't allow disconnect/reconnect (you
4483                     # just delete the attachment and get a new one)
4484                     # attachments in the new attach code so we have to do
4485                     # a delete and create without a connector (reserve),
4486                     # in other words, beware
4487                     attachment_id = self.volume_api.attachment_create(
4488                         context, bdm.volume_id, instance.uuid)['id']
4489                     self.volume_api.attachment_delete(context,
4490                                                       bdm.attachment_id)
4491                     bdm.attachment_id = attachment_id
4492                     bdm.save()
4493 
4494                 else:
4495                     if connector is None:
4496                         connector = self.driver.get_volume_connector(instance)
4497                     self.volume_api.terminate_connection(context,
4498                                                          bdm.volume_id,
4499                                                          connector)
4500 
4501     @staticmethod
4502     def _set_instance_info(instance, instance_type):
4503         instance.instance_type_id = instance_type.id
4504         instance.memory_mb = instance_type.memory_mb
4505         instance.vcpus = instance_type.vcpus
4506         instance.root_gb = instance_type.root_gb
4507         instance.ephemeral_gb = instance_type.ephemeral_gb
4508         instance.flavor = instance_type
4509 
4510     def _update_volume_attachments(self, context, instance, bdms):
4511         """Updates volume attachments using the virt driver host connector.
4512 
4513         :param context: nova.context.RequestContext - user request context
4514         :param instance: nova.objects.Instance
4515         :param bdms: nova.objects.BlockDeviceMappingList - the list of block
4516                      device mappings for the given instance
4517         """
4518         if bdms:
4519             connector = None
4520             for bdm in bdms:
4521                 if bdm.is_volume and bdm.attachment_id:
4522                     if connector is None:
4523                         connector = self.driver.get_volume_connector(instance)
4524                     self.volume_api.attachment_update(
4525                         context, bdm.attachment_id, connector, bdm.device_name)
4526 
4527     def _complete_volume_attachments(self, context, bdms):
4528         """Completes volume attachments for the instance
4529 
4530         :param context: nova.context.RequestContext - user request context
4531         :param bdms: nova.objects.BlockDeviceMappingList - the list of block
4532                      device mappings for the given instance
4533         """
4534         if bdms:
4535             for bdm in bdms:
4536                 if bdm.is_volume and bdm.attachment_id:
4537                     self.volume_api.attachment_complete(
4538                         context, bdm.attachment_id)
4539 
4540     def _finish_resize(self, context, instance, migration, disk_info,
4541                        image_meta, bdms):
4542         resize_instance = False
4543         old_instance_type_id = migration['old_instance_type_id']
4544         new_instance_type_id = migration['new_instance_type_id']
4545         old_instance_type = instance.get_flavor()
4546         # NOTE(mriedem): Get the old_vm_state so we know if we should
4547         # power on the instance. If old_vm_state is not set we need to default
4548         # to ACTIVE for backwards compatibility
4549         old_vm_state = instance.system_metadata.get('old_vm_state',
4550                                                     vm_states.ACTIVE)
4551         instance.old_flavor = old_instance_type
4552 
4553         if old_instance_type_id != new_instance_type_id:
4554             instance_type = instance.get_flavor('new')
4555             self._set_instance_info(instance, instance_type)
4556             for key in ('root_gb', 'swap', 'ephemeral_gb'):
4557                 if old_instance_type[key] != instance_type[key]:
4558                     resize_instance = True
4559                     break
4560         instance.apply_migration_context()
4561 
4562         # NOTE(tr3buchet): setup networks on destination host
4563         self.network_api.setup_networks_on_host(context, instance,
4564                                                 migration['dest_compute'])
4565 
4566         migration_p = obj_base.obj_to_primitive(migration)
4567         self.network_api.migrate_instance_finish(context,
4568                                                  instance,
4569                                                  migration_p)
4570 
4571         network_info = self.network_api.get_instance_nw_info(context, instance)
4572 
4573         instance.task_state = task_states.RESIZE_FINISH
4574         instance.save(expected_task_state=task_states.RESIZE_MIGRATED)
4575 
4576         self._notify_about_instance_usage(
4577             context, instance, "finish_resize.start",
4578             network_info=network_info)
4579         compute_utils.notify_about_instance_action(context, instance,
4580                self.host, action=fields.NotificationAction.RESIZE_FINISH,
4581                phase=fields.NotificationPhase.START, bdms=bdms)
4582 
4583         # We need to update any volume attachments using the destination
4584         # host connector so that we can update the BDM.connection_info
4585         # before calling driver.finish_migration otherwise the driver
4586         # won't know how to connect the volumes to this host.
4587         # Note that _get_instance_block_device_info with
4588         # refresh_conn_info=True will update the BDM.connection_info value
4589         # in the database so we must do this before calling that method.
4590         self._update_volume_attachments(context, instance, bdms)
4591 
4592         block_device_info = self._get_instance_block_device_info(
4593             context, instance, refresh_conn_info=True, bdms=bdms)
4594 
4595         # NOTE(mriedem): If the original vm_state was STOPPED, we don't
4596         # automatically power on the instance after it's migrated
4597         power_on = old_vm_state != vm_states.STOPPED
4598 
4599         try:
4600             self.driver.finish_migration(context, migration, instance,
4601                                          disk_info,
4602                                          network_info,
4603                                          image_meta, resize_instance,
4604                                          block_device_info, power_on)
4605         except Exception:
4606             with excutils.save_and_reraise_exception():
4607                 if old_instance_type_id != new_instance_type_id:
4608                     self._set_instance_info(instance,
4609                                             old_instance_type)
4610 
4611         # Now complete any volume attachments that were previously updated.
4612         self._complete_volume_attachments(context, bdms)
4613 
4614         migration.status = 'finished'
4615         with migration.obj_as_admin():
4616             migration.save()
4617 
4618         instance.vm_state = vm_states.RESIZED
4619         instance.task_state = None
4620         instance.launched_at = timeutils.utcnow()
4621         instance.save(expected_task_state=task_states.RESIZE_FINISH)
4622 
4623         return network_info
4624 
4625     @wrap_exception()
4626     @reverts_task_state
4627     @wrap_instance_event(prefix='compute')
4628     @wrap_instance_fault
4629     def finish_resize(self, context, disk_info, image, instance,
4630                       migration):
4631         """Completes the migration process.
4632 
4633         Sets up the newly transferred disk and turns on the instance at its
4634         new host machine.
4635 
4636         """
4637         try:
4638             self._finish_resize_helper(context, disk_info, image, instance,
4639                                        migration)
4640         except Exception:
4641             with excutils.save_and_reraise_exception():
4642                 self._revert_allocation(context, instance, migration)
4643 
4644     def _finish_resize_helper(self, context, disk_info, image, instance,
4645                               migration):
4646         """Completes the migration process.
4647 
4648         The caller must revert the instance's allocations if the migration
4649         process failed.
4650         """
4651         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
4652             context, instance.uuid)
4653 
4654         with self._error_out_instance_on_exception(context, instance), \
4655              errors_out_migration_ctxt(migration):
4656             image_meta = objects.ImageMeta.from_dict(image)
4657             network_info = self._finish_resize(context, instance, migration,
4658                                                disk_info, image_meta, bdms)
4659 
4660         # TODO(melwitt): We should clean up instance console tokens here. The
4661         # instance is on a new host and will need to establish a new console
4662         # connection.
4663         self._update_scheduler_instance_info(context, instance)
4664         self._notify_about_instance_usage(
4665             context, instance, "finish_resize.end",
4666             network_info=network_info)
4667         compute_utils.notify_about_instance_action(context, instance,
4668                self.host, action=fields.NotificationAction.RESIZE_FINISH,
4669                phase=fields.NotificationPhase.END, bdms=bdms)
4670 
4671     @wrap_exception()
4672     @wrap_instance_fault
4673     def add_fixed_ip_to_instance(self, context, network_id, instance):
4674         """Calls network_api to add new fixed_ip to instance
4675         then injects the new network info and resets instance networking.
4676 
4677         """
4678         self._notify_about_instance_usage(
4679                 context, instance, "create_ip.start")
4680 
4681         network_info = self.network_api.add_fixed_ip_to_instance(context,
4682                                                                  instance,
4683                                                                  network_id)
4684         self._inject_network_info(context, instance, network_info)
4685         self.reset_network(context, instance)
4686 
4687         # NOTE(russellb) We just want to bump updated_at.  See bug 1143466.
4688         instance.updated_at = timeutils.utcnow()
4689         instance.save()
4690 
4691         self._notify_about_instance_usage(
4692             context, instance, "create_ip.end", network_info=network_info)
4693 
4694     @wrap_exception()
4695     @wrap_instance_fault
4696     def remove_fixed_ip_from_instance(self, context, address, instance):
4697         """Calls network_api to remove existing fixed_ip from instance
4698         by injecting the altered network info and resetting
4699         instance networking.
4700         """
4701         self._notify_about_instance_usage(
4702                 context, instance, "delete_ip.start")
4703 
4704         network_info = self.network_api.remove_fixed_ip_from_instance(context,
4705                                                                       instance,
4706                                                                       address)
4707         self._inject_network_info(context, instance, network_info)
4708         self.reset_network(context, instance)
4709 
4710         # NOTE(russellb) We just want to bump updated_at.  See bug 1143466.
4711         instance.updated_at = timeutils.utcnow()
4712         instance.save()
4713 
4714         self._notify_about_instance_usage(
4715             context, instance, "delete_ip.end", network_info=network_info)
4716 
4717     @wrap_exception()
4718     @reverts_task_state
4719     @wrap_instance_event(prefix='compute')
4720     @wrap_instance_fault
4721     def pause_instance(self, context, instance):
4722         """Pause an instance on this host."""
4723         context = context.elevated()
4724         LOG.info('Pausing', instance=instance)
4725         self._notify_about_instance_usage(context, instance, 'pause.start')
4726         compute_utils.notify_about_instance_action(context, instance,
4727                self.host, action=fields.NotificationAction.PAUSE,
4728                phase=fields.NotificationPhase.START)
4729         self.driver.pause(instance)
4730         instance.power_state = self._get_power_state(context, instance)
4731         instance.vm_state = vm_states.PAUSED
4732         instance.task_state = None
4733         instance.save(expected_task_state=task_states.PAUSING)
4734         self._notify_about_instance_usage(context, instance, 'pause.end')
4735         compute_utils.notify_about_instance_action(context, instance,
4736                self.host, action=fields.NotificationAction.PAUSE,
4737                phase=fields.NotificationPhase.END)
4738 
4739     @wrap_exception()
4740     @reverts_task_state
4741     @wrap_instance_event(prefix='compute')
4742     @wrap_instance_fault
4743     def unpause_instance(self, context, instance):
4744         """Unpause a paused instance on this host."""
4745         context = context.elevated()
4746         LOG.info('Unpausing', instance=instance)
4747         self._notify_about_instance_usage(context, instance, 'unpause.start')
4748         compute_utils.notify_about_instance_action(context, instance,
4749             self.host, action=fields.NotificationAction.UNPAUSE,
4750             phase=fields.NotificationPhase.START)
4751         self.driver.unpause(instance)
4752         instance.power_state = self._get_power_state(context, instance)
4753         instance.vm_state = vm_states.ACTIVE
4754         instance.task_state = None
4755         instance.save(expected_task_state=task_states.UNPAUSING)
4756         self._notify_about_instance_usage(context, instance, 'unpause.end')
4757         compute_utils.notify_about_instance_action(context, instance,
4758             self.host, action=fields.NotificationAction.UNPAUSE,
4759             phase=fields.NotificationPhase.END)
4760 
4761     @wrap_exception()
4762     def host_power_action(self, context, action):
4763         """Reboots, shuts down or powers up the host."""
4764         return self.driver.host_power_action(action)
4765 
4766     @wrap_exception()
4767     def host_maintenance_mode(self, context, host, mode):
4768         """Start/Stop host maintenance window. On start, it triggers
4769         guest VMs evacuation.
4770         """
4771         return self.driver.host_maintenance_mode(host, mode)
4772 
4773     @wrap_exception()
4774     def set_host_enabled(self, context, enabled):
4775         """Sets the specified host's ability to accept new instances."""
4776         return self.driver.set_host_enabled(enabled)
4777 
4778     @wrap_exception()
4779     def get_host_uptime(self, context):
4780         """Returns the result of calling "uptime" on the target host."""
4781         return self.driver.get_host_uptime()
4782 
4783     @wrap_exception()
4784     @wrap_instance_fault
4785     def get_diagnostics(self, context, instance):
4786         """Retrieve diagnostics for an instance on this host."""
4787         current_power_state = self._get_power_state(context, instance)
4788         if current_power_state == power_state.RUNNING:
4789             LOG.info("Retrieving diagnostics", instance=instance)
4790             return self.driver.get_diagnostics(instance)
4791         else:
4792             raise exception.InstanceInvalidState(
4793                 attr='power state',
4794                 instance_uuid=instance.uuid,
4795                 state=power_state.STATE_MAP[instance.power_state],
4796                 method='get_diagnostics')
4797 
4798     @wrap_exception()
4799     @wrap_instance_fault
4800     def get_instance_diagnostics(self, context, instance):
4801         """Retrieve diagnostics for an instance on this host."""
4802         current_power_state = self._get_power_state(context, instance)
4803         if current_power_state == power_state.RUNNING:
4804             LOG.info("Retrieving diagnostics", instance=instance)
4805             return self.driver.get_instance_diagnostics(instance)
4806         else:
4807             raise exception.InstanceInvalidState(
4808                 attr='power state',
4809                 instance_uuid=instance.uuid,
4810                 state=power_state.STATE_MAP[instance.power_state],
4811                 method='get_diagnostics')
4812 
4813     @wrap_exception()
4814     @reverts_task_state
4815     @wrap_instance_event(prefix='compute')
4816     @wrap_instance_fault
4817     def suspend_instance(self, context, instance):
4818         """Suspend the given instance."""
4819         context = context.elevated()
4820 
4821         # Store the old state
4822         instance.system_metadata['old_vm_state'] = instance.vm_state
4823         self._notify_about_instance_usage(context, instance, 'suspend.start')
4824         compute_utils.notify_about_instance_action(context, instance,
4825                 self.host, action=fields.NotificationAction.SUSPEND,
4826                 phase=fields.NotificationPhase.START)
4827         with self._error_out_instance_on_exception(context, instance,
4828              instance_state=instance.vm_state):
4829             self.driver.suspend(context, instance)
4830         instance.power_state = self._get_power_state(context, instance)
4831         instance.vm_state = vm_states.SUSPENDED
4832         instance.task_state = None
4833         instance.save(expected_task_state=task_states.SUSPENDING)
4834         self._notify_about_instance_usage(context, instance, 'suspend.end')
4835         compute_utils.notify_about_instance_action(context, instance,
4836                 self.host, action=fields.NotificationAction.SUSPEND,
4837                 phase=fields.NotificationPhase.END)
4838 
4839     @wrap_exception()
4840     @reverts_task_state
4841     @wrap_instance_event(prefix='compute')
4842     @wrap_instance_fault
4843     def resume_instance(self, context, instance):
4844         """Resume the given suspended instance."""
4845         context = context.elevated()
4846         LOG.info('Resuming', instance=instance)
4847 
4848         self._notify_about_instance_usage(context, instance, 'resume.start')
4849 
4850         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
4851             context, instance.uuid)
4852         block_device_info = self._get_instance_block_device_info(
4853             context, instance, bdms=bdms)
4854 
4855         compute_utils.notify_about_instance_action(context, instance,
4856             self.host, action=fields.NotificationAction.RESUME,
4857             phase=fields.NotificationPhase.START, bdms=bdms)
4858 
4859         network_info = self.network_api.get_instance_nw_info(context, instance)
4860 
4861         with self._error_out_instance_on_exception(context, instance,
4862              instance_state=instance.vm_state):
4863             self.driver.resume(context, instance, network_info,
4864                                block_device_info)
4865 
4866         instance.power_state = self._get_power_state(context, instance)
4867 
4868         # We default to the ACTIVE state for backwards compatibility
4869         instance.vm_state = instance.system_metadata.pop('old_vm_state',
4870                                                          vm_states.ACTIVE)
4871 
4872         instance.task_state = None
4873         instance.save(expected_task_state=task_states.RESUMING)
4874         self._notify_about_instance_usage(context, instance, 'resume.end')
4875         compute_utils.notify_about_instance_action(context, instance,
4876             self.host, action=fields.NotificationAction.RESUME,
4877             phase=fields.NotificationPhase.END, bdms=bdms)
4878 
4879     @wrap_exception()
4880     @reverts_task_state
4881     @wrap_instance_event(prefix='compute')
4882     @wrap_instance_fault
4883     def shelve_instance(self, context, instance, image_id,
4884                         clean_shutdown):
4885         """Shelve an instance.
4886 
4887         This should be used when you want to take a snapshot of the instance.
4888         It also adds system_metadata that can be used by a periodic task to
4889         offload the shelved instance after a period of time.
4890 
4891         :param context: request context
4892         :param instance: an Instance object
4893         :param image_id: an image id to snapshot to.
4894         :param clean_shutdown: give the GuestOS a chance to stop
4895         """
4896 
4897         @utils.synchronized(instance.uuid)
4898         def do_shelve_instance():
4899             self._shelve_instance(context, instance, image_id, clean_shutdown)
4900         do_shelve_instance()
4901 
4902     def _shelve_instance(self, context, instance, image_id,
4903                          clean_shutdown):
4904         LOG.info('Shelving', instance=instance)
4905         offload = CONF.shelved_offload_time == 0
4906         if offload:
4907             # Get the BDMs early so we can pass them into versioned
4908             # notifications since _shelve_offload_instance needs the
4909             # BDMs anyway.
4910             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
4911                 context, instance.uuid)
4912         else:
4913             bdms = None
4914         compute_utils.notify_usage_exists(self.notifier, context, instance,
4915                                           self.host, current_period=True)
4916         self._notify_about_instance_usage(context, instance, 'shelve.start')
4917         compute_utils.notify_about_instance_action(context, instance,
4918                 self.host, action=fields.NotificationAction.SHELVE,
4919                 phase=fields.NotificationPhase.START, bdms=bdms)
4920 
4921         def update_task_state(task_state, expected_state=task_states.SHELVING):
4922             shelving_state_map = {
4923                     task_states.IMAGE_PENDING_UPLOAD:
4924                         task_states.SHELVING_IMAGE_PENDING_UPLOAD,
4925                     task_states.IMAGE_UPLOADING:
4926                         task_states.SHELVING_IMAGE_UPLOADING,
4927                     task_states.SHELVING: task_states.SHELVING}
4928             task_state = shelving_state_map[task_state]
4929             expected_state = shelving_state_map[expected_state]
4930             instance.task_state = task_state
4931             instance.save(expected_task_state=expected_state)
4932         # Do not attempt a clean shutdown of a paused guest since some
4933         # hypervisors will fail the clean shutdown if the guest is not
4934         # running.
4935         if instance.power_state == power_state.PAUSED:
4936             clean_shutdown = False
4937         self._power_off_instance(context, instance, clean_shutdown)
4938         self.driver.snapshot(context, instance, image_id, update_task_state)
4939 
4940         instance.system_metadata['shelved_at'] = timeutils.utcnow().isoformat()
4941         instance.system_metadata['shelved_image_id'] = image_id
4942         instance.system_metadata['shelved_host'] = self.host
4943         instance.vm_state = vm_states.SHELVED
4944         instance.task_state = None
4945         if CONF.shelved_offload_time == 0:
4946             instance.task_state = task_states.SHELVING_OFFLOADING
4947         instance.power_state = self._get_power_state(context, instance)
4948         instance.save(expected_task_state=[
4949                 task_states.SHELVING,
4950                 task_states.SHELVING_IMAGE_UPLOADING])
4951 
4952         self._notify_about_instance_usage(context, instance, 'shelve.end')
4953         compute_utils.notify_about_instance_action(context, instance,
4954                 self.host, action=fields.NotificationAction.SHELVE,
4955                 phase=fields.NotificationPhase.END, bdms=bdms)
4956 
4957         if offload:
4958             self._shelve_offload_instance(context, instance,
4959                                           clean_shutdown=False, bdms=bdms)
4960 
4961     @wrap_exception()
4962     @reverts_task_state
4963     @wrap_instance_event(prefix='compute')
4964     @wrap_instance_fault
4965     def shelve_offload_instance(self, context, instance, clean_shutdown):
4966         """Remove a shelved instance from the hypervisor.
4967 
4968         This frees up those resources for use by other instances, but may lead
4969         to slower unshelve times for this instance.  This method is used by
4970         volume backed instances since restoring them doesn't involve the
4971         potentially large download of an image.
4972 
4973         :param context: request context
4974         :param instance: nova.objects.instance.Instance
4975         :param clean_shutdown: give the GuestOS a chance to stop
4976         """
4977 
4978         @utils.synchronized(instance.uuid)
4979         def do_shelve_offload_instance():
4980             self._shelve_offload_instance(context, instance, clean_shutdown)
4981         do_shelve_offload_instance()
4982 
4983     def _shelve_offload_instance(self, context, instance, clean_shutdown,
4984                                  bdms=None):
4985         LOG.info('Shelve offloading', instance=instance)
4986         if bdms is None:
4987             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
4988                 context, instance.uuid)
4989         self._notify_about_instance_usage(context, instance,
4990                 'shelve_offload.start')
4991         compute_utils.notify_about_instance_action(context, instance,
4992                 self.host, action=fields.NotificationAction.SHELVE_OFFLOAD,
4993                 phase=fields.NotificationPhase.START, bdms=bdms)
4994 
4995         self._power_off_instance(context, instance, clean_shutdown)
4996         current_power_state = self._get_power_state(context, instance)
4997 
4998         self.network_api.cleanup_instance_network_on_host(context, instance,
4999                                                           instance.host)
5000         network_info = self.network_api.get_instance_nw_info(context, instance)
5001 
5002         block_device_info = self._get_instance_block_device_info(context,
5003                                                                  instance,
5004                                                                  bdms=bdms)
5005         self.driver.destroy(context, instance, network_info,
5006                 block_device_info)
5007 
5008         # the instance is going to be removed from the host so we want to
5009         # terminate all the connections with the volume server and the host
5010         self._terminate_volume_connections(context, instance, bdms)
5011 
5012         instance.power_state = current_power_state
5013         # NOTE(mriedem): The vm_state has to be set before updating the
5014         # resource tracker, see vm_states.ALLOW_RESOURCE_REMOVAL. The host/node
5015         # values cannot be nulled out until after updating the resource tracker
5016         # though.
5017         instance.vm_state = vm_states.SHELVED_OFFLOADED
5018         instance.task_state = None
5019         instance.save(expected_task_state=[task_states.SHELVING,
5020                                            task_states.SHELVING_OFFLOADING])
5021 
5022         # NOTE(ndipanov): Free resources from the resource tracker
5023         self._update_resource_tracker(context, instance)
5024 
5025         rt = self._get_resource_tracker()
5026         rt.delete_allocation_for_shelve_offloaded_instance(context, instance)
5027 
5028         # NOTE(sfinucan): RPC calls should no longer be attempted against this
5029         # instance, so ensure any calls result in errors
5030         self._nil_out_instance_obj_host_and_node(instance)
5031         instance.save(expected_task_state=None)
5032 
5033         # TODO(melwitt): We should clean up instance console tokens here. The
5034         # instance has no host at this point and will need to establish a new
5035         # console connection in the future after it is unshelved.
5036         self._delete_scheduler_instance_info(context, instance.uuid)
5037         self._notify_about_instance_usage(context, instance,
5038                 'shelve_offload.end')
5039         compute_utils.notify_about_instance_action(context, instance,
5040                 self.host, action=fields.NotificationAction.SHELVE_OFFLOAD,
5041                 phase=fields.NotificationPhase.END, bdms=bdms)
5042 
5043     @wrap_exception()
5044     @reverts_task_state
5045     @wrap_instance_event(prefix='compute')
5046     @wrap_instance_fault
5047     def unshelve_instance(self, context, instance, image,
5048                           filter_properties, node):
5049         """Unshelve the instance.
5050 
5051         :param context: request context
5052         :param instance: a nova.objects.instance.Instance object
5053         :param image: an image to build from.  If None we assume a
5054             volume backed instance.
5055         :param filter_properties: dict containing limits, retry info etc.
5056         :param node: target compute node
5057         """
5058         if filter_properties is None:
5059             filter_properties = {}
5060 
5061         @utils.synchronized(instance.uuid)
5062         def do_unshelve_instance():
5063             self._unshelve_instance(context, instance, image,
5064                                     filter_properties, node)
5065         do_unshelve_instance()
5066 
5067     def _unshelve_instance_key_scrub(self, instance):
5068         """Remove data from the instance that may cause side effects."""
5069         cleaned_keys = dict(
5070                 key_data=instance.key_data,
5071                 auto_disk_config=instance.auto_disk_config)
5072         instance.key_data = None
5073         instance.auto_disk_config = False
5074         return cleaned_keys
5075 
5076     def _unshelve_instance_key_restore(self, instance, keys):
5077         """Restore previously scrubbed keys before saving the instance."""
5078         instance.update(keys)
5079 
5080     def _unshelve_instance(self, context, instance, image, filter_properties,
5081                            node):
5082         LOG.info('Unshelving', instance=instance)
5083         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
5084                 context, instance.uuid)
5085 
5086         self._notify_about_instance_usage(context, instance, 'unshelve.start')
5087         compute_utils.notify_about_instance_action(context, instance,
5088                 self.host, action=fields.NotificationAction.UNSHELVE,
5089                 phase=fields.NotificationPhase.START, bdms=bdms)
5090 
5091         instance.task_state = task_states.SPAWNING
5092         instance.save()
5093 
5094         block_device_info = self._prep_block_device(context, instance, bdms)
5095         scrubbed_keys = self._unshelve_instance_key_scrub(instance)
5096 
5097         if node is None:
5098             node = self._get_nodename(instance)
5099 
5100         rt = self._get_resource_tracker()
5101         limits = filter_properties.get('limits', {})
5102 
5103         allocations = self.reportclient.get_allocations_for_consumer(
5104             context, instance.uuid)
5105 
5106         shelved_image_ref = instance.image_ref
5107         if image:
5108             instance.image_ref = image['id']
5109             image_meta = objects.ImageMeta.from_dict(image)
5110         else:
5111             image_meta = objects.ImageMeta.from_dict(
5112                 utils.get_image_from_system_metadata(
5113                     instance.system_metadata))
5114 
5115         self.network_api.setup_instance_network_on_host(context, instance,
5116                                                         self.host)
5117         network_info = self.network_api.get_instance_nw_info(context, instance)
5118         try:
5119             with rt.instance_claim(context, instance, node, limits):
5120                 self.driver.spawn(context, instance, image_meta,
5121                                   injected_files=[],
5122                                   admin_password=None,
5123                                   allocations=allocations,
5124                                   network_info=network_info,
5125                                   block_device_info=block_device_info)
5126         except Exception:
5127             with excutils.save_and_reraise_exception(logger=LOG):
5128                 LOG.exception('Instance failed to spawn',
5129                               instance=instance)
5130                 # Cleanup allocations created by the scheduler on this host
5131                 # since we failed to spawn the instance. We do this both if
5132                 # the instance claim failed with ComputeResourcesUnavailable
5133                 # or if we did claim but the spawn failed, because aborting the
5134                 # instance claim will not remove the allocations.
5135                 rt.reportclient.delete_allocation_for_instance(context,
5136                                                                instance.uuid)
5137                 # FIXME: Umm, shouldn't we be rolling back port bindings too?
5138                 self._terminate_volume_connections(context, instance, bdms)
5139                 # The reverts_task_state decorator on unshelve_instance will
5140                 # eventually save these updates.
5141                 self._nil_out_instance_obj_host_and_node(instance)
5142 
5143         if image:
5144             instance.image_ref = shelved_image_ref
5145             self._delete_snapshot_of_shelved_instance(context, instance,
5146                                                       image['id'])
5147 
5148         self._unshelve_instance_key_restore(instance, scrubbed_keys)
5149         self._update_instance_after_spawn(context, instance)
5150         # Delete system_metadata for a shelved instance
5151         compute_utils.remove_shelved_keys_from_system_metadata(instance)
5152 
5153         instance.save(expected_task_state=task_states.SPAWNING)
5154         self._update_scheduler_instance_info(context, instance)
5155         self._notify_about_instance_usage(context, instance, 'unshelve.end')
5156         compute_utils.notify_about_instance_action(context, instance,
5157                 self.host, action=fields.NotificationAction.UNSHELVE,
5158                 phase=fields.NotificationPhase.END, bdms=bdms)
5159 
5160     @messaging.expected_exceptions(NotImplementedError)
5161     @wrap_instance_fault
5162     def reset_network(self, context, instance):
5163         """Reset networking on the given instance."""
5164         LOG.debug('Reset network', instance=instance)
5165         self.driver.reset_network(instance)
5166 
5167     def _inject_network_info(self, context, instance, network_info):
5168         """Inject network info for the given instance."""
5169         LOG.debug('Inject network info', instance=instance)
5170         LOG.debug('network_info to inject: |%s|', network_info,
5171                   instance=instance)
5172 
5173         self.driver.inject_network_info(instance,
5174                                         network_info)
5175 
5176     @wrap_instance_fault
5177     def inject_network_info(self, context, instance):
5178         """Inject network info, but don't return the info."""
5179         network_info = self.network_api.get_instance_nw_info(context, instance)
5180         self._inject_network_info(context, instance, network_info)
5181 
5182     @messaging.expected_exceptions(NotImplementedError,
5183                                    exception.ConsoleNotAvailable,
5184                                    exception.InstanceNotFound)
5185     @wrap_exception()
5186     @wrap_instance_fault
5187     def get_console_output(self, context, instance, tail_length):
5188         """Send the console output for the given instance."""
5189         context = context.elevated()
5190         LOG.info("Get console output", instance=instance)
5191         output = self.driver.get_console_output(context, instance)
5192 
5193         if type(output) is six.text_type:
5194             output = six.b(output)
5195 
5196         if tail_length is not None:
5197             output = self._tail_log(output, tail_length)
5198 
5199         return output.decode('ascii', 'replace')
5200 
5201     def _tail_log(self, log, length):
5202         try:
5203             length = int(length)
5204         except ValueError:
5205             length = 0
5206 
5207         if length == 0:
5208             return b''
5209         else:
5210             return b'\n'.join(log.split(b'\n')[-int(length):])
5211 
5212     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
5213                                    exception.InstanceNotReady,
5214                                    exception.InstanceNotFound,
5215                                    exception.ConsoleTypeUnavailable,
5216                                    NotImplementedError)
5217     @wrap_exception()
5218     @wrap_instance_fault
5219     def get_vnc_console(self, context, console_type, instance):
5220         """Return connection information for a vnc console."""
5221         context = context.elevated()
5222         LOG.debug("Getting vnc console", instance=instance)
5223 
5224         if not CONF.vnc.enabled:
5225             raise exception.ConsoleTypeUnavailable(console_type=console_type)
5226 
5227         if console_type == 'novnc':
5228             # For essex, novncproxy_base_url must include the full path
5229             # including the html file (like http://myhost/vnc_auto.html)
5230             access_url_base = CONF.vnc.novncproxy_base_url
5231         elif console_type == 'xvpvnc':
5232             access_url_base = CONF.vnc.xvpvncproxy_base_url
5233         else:
5234             raise exception.ConsoleTypeInvalid(console_type=console_type)
5235 
5236         try:
5237             # Retrieve connect info from driver, and then decorate with our
5238             # access info token
5239             console = self.driver.get_vnc_console(context, instance)
5240             console_auth = objects.ConsoleAuthToken(
5241                 context=context,
5242                 console_type=console_type,
5243                 host=console.host,
5244                 port=console.port,
5245                 internal_access_path=console.internal_access_path,
5246                 instance_uuid=instance.uuid,
5247                 access_url_base=access_url_base,
5248             )
5249             console_auth.authorize(CONF.consoleauth.token_ttl)
5250             connect_info = console.get_connection_info(
5251                 console_auth.token, console_auth.access_url)
5252 
5253         except exception.InstanceNotFound:
5254             if instance.vm_state != vm_states.BUILDING:
5255                 raise
5256             raise exception.InstanceNotReady(instance_id=instance.uuid)
5257 
5258         return connect_info
5259 
5260     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
5261                                    exception.InstanceNotReady,
5262                                    exception.InstanceNotFound,
5263                                    exception.ConsoleTypeUnavailable,
5264                                    NotImplementedError)
5265     @wrap_exception()
5266     @wrap_instance_fault
5267     def get_spice_console(self, context, console_type, instance):
5268         """Return connection information for a spice console."""
5269         context = context.elevated()
5270         LOG.debug("Getting spice console", instance=instance)
5271 
5272         if not CONF.spice.enabled:
5273             raise exception.ConsoleTypeUnavailable(console_type=console_type)
5274 
5275         if console_type != 'spice-html5':
5276             raise exception.ConsoleTypeInvalid(console_type=console_type)
5277 
5278         try:
5279             # Retrieve connect info from driver, and then decorate with our
5280             # access info token
5281             console = self.driver.get_spice_console(context, instance)
5282             console_auth = objects.ConsoleAuthToken(
5283                 context=context,
5284                 console_type=console_type,
5285                 host=console.host,
5286                 port=console.port,
5287                 internal_access_path=console.internal_access_path,
5288                 instance_uuid=instance.uuid,
5289                 access_url_base=CONF.spice.html5proxy_base_url,
5290             )
5291             console_auth.authorize(CONF.consoleauth.token_ttl)
5292             connect_info = console.get_connection_info(
5293                 console_auth.token, console_auth.access_url)
5294 
5295         except exception.InstanceNotFound:
5296             if instance.vm_state != vm_states.BUILDING:
5297                 raise
5298             raise exception.InstanceNotReady(instance_id=instance.uuid)
5299 
5300         return connect_info
5301 
5302     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
5303                                    exception.InstanceNotReady,
5304                                    exception.InstanceNotFound,
5305                                    exception.ConsoleTypeUnavailable,
5306                                    NotImplementedError)
5307     @wrap_exception()
5308     @wrap_instance_fault
5309     def get_rdp_console(self, context, console_type, instance):
5310         """Return connection information for a RDP console."""
5311         context = context.elevated()
5312         LOG.debug("Getting RDP console", instance=instance)
5313 
5314         if not CONF.rdp.enabled:
5315             raise exception.ConsoleTypeUnavailable(console_type=console_type)
5316 
5317         if console_type != 'rdp-html5':
5318             raise exception.ConsoleTypeInvalid(console_type=console_type)
5319 
5320         try:
5321             # Retrieve connect info from driver, and then decorate with our
5322             # access info token
5323             console = self.driver.get_rdp_console(context, instance)
5324             console_auth = objects.ConsoleAuthToken(
5325                 context=context,
5326                 console_type=console_type,
5327                 host=console.host,
5328                 port=console.port,
5329                 internal_access_path=console.internal_access_path,
5330                 instance_uuid=instance.uuid,
5331                 access_url_base=CONF.rdp.html5_proxy_base_url,
5332             )
5333             console_auth.authorize(CONF.consoleauth.token_ttl)
5334             connect_info = console.get_connection_info(
5335                 console_auth.token, console_auth.access_url)
5336 
5337         except exception.InstanceNotFound:
5338             if instance.vm_state != vm_states.BUILDING:
5339                 raise
5340             raise exception.InstanceNotReady(instance_id=instance.uuid)
5341 
5342         return connect_info
5343 
5344     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
5345                                    exception.InstanceNotReady,
5346                                    exception.InstanceNotFound,
5347                                    exception.ConsoleTypeUnavailable,
5348                                    NotImplementedError)
5349     @wrap_exception()
5350     @wrap_instance_fault
5351     def get_mks_console(self, context, console_type, instance):
5352         """Return connection information for a MKS console."""
5353         context = context.elevated()
5354         LOG.debug("Getting MKS console", instance=instance)
5355 
5356         if not CONF.mks.enabled:
5357             raise exception.ConsoleTypeUnavailable(console_type=console_type)
5358 
5359         if console_type != 'webmks':
5360             raise exception.ConsoleTypeInvalid(console_type=console_type)
5361 
5362         try:
5363             # Retrieve connect info from driver, and then decorate with our
5364             # access info token
5365             console = self.driver.get_mks_console(context, instance)
5366             console_auth = objects.ConsoleAuthToken(
5367                 context=context,
5368                 console_type=console_type,
5369                 host=console.host,
5370                 port=console.port,
5371                 internal_access_path=console.internal_access_path,
5372                 instance_uuid=instance.uuid,
5373                 access_url_base=CONF.mks.mksproxy_base_url,
5374             )
5375             console_auth.authorize(CONF.consoleauth.token_ttl)
5376             connect_info = console.get_connection_info(
5377                 console_auth.token, console_auth.access_url)
5378 
5379         except exception.InstanceNotFound:
5380             if instance.vm_state != vm_states.BUILDING:
5381                 raise
5382             raise exception.InstanceNotReady(instance_id=instance.uuid)
5383 
5384         return connect_info
5385 
5386     @messaging.expected_exceptions(
5387         exception.ConsoleTypeInvalid,
5388         exception.InstanceNotReady,
5389         exception.InstanceNotFound,
5390         exception.ConsoleTypeUnavailable,
5391         exception.SocketPortRangeExhaustedException,
5392         exception.ImageSerialPortNumberInvalid,
5393         exception.ImageSerialPortNumberExceedFlavorValue,
5394         NotImplementedError)
5395     @wrap_exception()
5396     @wrap_instance_fault
5397     def get_serial_console(self, context, console_type, instance):
5398         """Returns connection information for a serial console."""
5399 
5400         LOG.debug("Getting serial console", instance=instance)
5401 
5402         if not CONF.serial_console.enabled:
5403             raise exception.ConsoleTypeUnavailable(console_type=console_type)
5404 
5405         context = context.elevated()
5406 
5407         try:
5408             # Retrieve connect info from driver, and then decorate with our
5409             # access info token
5410             console = self.driver.get_serial_console(context, instance)
5411             console_auth = objects.ConsoleAuthToken(
5412                 context=context,
5413                 console_type=console_type,
5414                 host=console.host,
5415                 port=console.port,
5416                 internal_access_path=console.internal_access_path,
5417                 instance_uuid=instance.uuid,
5418                 access_url_base=CONF.serial_console.base_url,
5419             )
5420             console_auth.authorize(CONF.consoleauth.token_ttl)
5421             connect_info = console.get_connection_info(
5422                 console_auth.token, console_auth.access_url)
5423 
5424         except exception.InstanceNotFound:
5425             if instance.vm_state != vm_states.BUILDING:
5426                 raise
5427             raise exception.InstanceNotReady(instance_id=instance.uuid)
5428 
5429         return connect_info
5430 
5431     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
5432                                    exception.InstanceNotReady,
5433                                    exception.InstanceNotFound)
5434     @wrap_exception()
5435     @wrap_instance_fault
5436     def validate_console_port(self, ctxt, instance, port, console_type):
5437         if console_type == "spice-html5":
5438             console_info = self.driver.get_spice_console(ctxt, instance)
5439         elif console_type == "rdp-html5":
5440             console_info = self.driver.get_rdp_console(ctxt, instance)
5441         elif console_type == "serial":
5442             console_info = self.driver.get_serial_console(ctxt, instance)
5443         elif console_type == "webmks":
5444             console_info = self.driver.get_mks_console(ctxt, instance)
5445         else:
5446             console_info = self.driver.get_vnc_console(ctxt, instance)
5447 
5448         return console_info.port == port
5449 
5450     @wrap_exception()
5451     @reverts_task_state
5452     @wrap_instance_fault
5453     def reserve_block_device_name(self, context, instance, device,
5454                                   volume_id, disk_bus, device_type, tag,
5455                                   multiattach):
5456         if (tag and not
5457                 self.driver.capabilities.get('supports_tagged_attach_volume',
5458                                              False)):
5459             raise exception.VolumeTaggedAttachNotSupported()
5460 
5461         if (multiattach and not
5462                 self.driver.capabilities.get('supports_multiattach', False)):
5463             raise exception.MultiattachNotSupportedByVirtDriver(
5464                 volume_id=volume_id)
5465 
5466         @utils.synchronized(instance.uuid)
5467         def do_reserve():
5468             bdms = (
5469                 objects.BlockDeviceMappingList.get_by_instance_uuid(
5470                     context, instance.uuid))
5471 
5472             # NOTE(ndipanov): We need to explicitly set all the fields on the
5473             #                 object so that obj_load_attr does not fail
5474             new_bdm = objects.BlockDeviceMapping(
5475                     context=context,
5476                     source_type='volume', destination_type='volume',
5477                     instance_uuid=instance.uuid, boot_index=None,
5478                     volume_id=volume_id,
5479                     device_name=device, guest_format=None,
5480                     disk_bus=disk_bus, device_type=device_type, tag=tag)
5481 
5482             new_bdm.device_name = self._get_device_name_for_instance(
5483                     instance, bdms, new_bdm)
5484 
5485             # NOTE(vish): create bdm here to avoid race condition
5486             new_bdm.create()
5487             return new_bdm
5488 
5489         return do_reserve()
5490 
5491     @wrap_exception()
5492     @wrap_instance_event(prefix='compute')
5493     @wrap_instance_fault
5494     def attach_volume(self, context, instance, bdm):
5495         """Attach a volume to an instance."""
5496         driver_bdm = driver_block_device.convert_volume(bdm)
5497 
5498         @utils.synchronized(instance.uuid)
5499         def do_attach_volume(context, instance, driver_bdm):
5500             try:
5501                 return self._attach_volume(context, instance, driver_bdm)
5502             except Exception:
5503                 with excutils.save_and_reraise_exception():
5504                     bdm.destroy()
5505 
5506         do_attach_volume(context, instance, driver_bdm)
5507 
5508     def _attach_volume(self, context, instance, bdm):
5509         context = context.elevated()
5510         LOG.info('Attaching volume %(volume_id)s to %(mountpoint)s',
5511                  {'volume_id': bdm.volume_id,
5512                   'mountpoint': bdm['mount_device']},
5513                  instance=instance)
5514         compute_utils.notify_about_volume_attach_detach(
5515             context, instance, self.host,
5516             action=fields.NotificationAction.VOLUME_ATTACH,
5517             phase=fields.NotificationPhase.START,
5518             volume_id=bdm.volume_id)
5519         try:
5520             bdm.attach(context, instance, self.volume_api, self.driver,
5521                        do_driver_attach=True)
5522         except Exception as e:
5523             with excutils.save_and_reraise_exception():
5524                 LOG.exception("Failed to attach %(volume_id)s "
5525                               "at %(mountpoint)s",
5526                               {'volume_id': bdm.volume_id,
5527                                'mountpoint': bdm['mount_device']},
5528                               instance=instance)
5529                 if bdm['attachment_id']:
5530                     self.volume_api.attachment_delete(context,
5531                                                       bdm['attachment_id'])
5532                 else:
5533                     self.volume_api.unreserve_volume(context, bdm.volume_id)
5534                 tb = traceback.format_exc()
5535                 compute_utils.notify_about_volume_attach_detach(
5536                     context, instance, self.host,
5537                     action=fields.NotificationAction.VOLUME_ATTACH,
5538                     phase=fields.NotificationPhase.ERROR,
5539                     exception=e,
5540                     volume_id=bdm.volume_id, tb=tb)
5541 
5542         info = {'volume_id': bdm.volume_id}
5543         self._notify_about_instance_usage(
5544             context, instance, "volume.attach", extra_usage_info=info)
5545         compute_utils.notify_about_volume_attach_detach(
5546             context, instance, self.host,
5547             action=fields.NotificationAction.VOLUME_ATTACH,
5548             phase=fields.NotificationPhase.END,
5549             volume_id=bdm.volume_id)
5550 
5551     def _notify_volume_usage_detach(self, context, instance, bdm):
5552         if CONF.volume_usage_poll_interval <= 0:
5553             return
5554 
5555         vol_stats = []
5556         mp = bdm.device_name
5557         # Handle bootable volumes which will not contain /dev/
5558         if '/dev/' in mp:
5559             mp = mp[5:]
5560         try:
5561             vol_stats = self.driver.block_stats(instance, mp)
5562         except NotImplementedError:
5563             return
5564 
5565         LOG.debug("Updating volume usage cache with totals", instance=instance)
5566         rd_req, rd_bytes, wr_req, wr_bytes, flush_ops = vol_stats
5567         vol_usage = objects.VolumeUsage(context)
5568         vol_usage.volume_id = bdm.volume_id
5569         vol_usage.instance_uuid = instance.uuid
5570         vol_usage.project_id = instance.project_id
5571         vol_usage.user_id = instance.user_id
5572         vol_usage.availability_zone = instance.availability_zone
5573         vol_usage.curr_reads = rd_req
5574         vol_usage.curr_read_bytes = rd_bytes
5575         vol_usage.curr_writes = wr_req
5576         vol_usage.curr_write_bytes = wr_bytes
5577         vol_usage.save(update_totals=True)
5578         self.notifier.info(context, 'volume.usage',
5579                            compute_utils.usage_volume_info(vol_usage))
5580 
5581     def _detach_volume(self, context, bdm, instance, destroy_bdm=True,
5582                        attachment_id=None):
5583         """Detach a volume from an instance.
5584 
5585         :param context: security context
5586         :param bdm: nova.objects.BlockDeviceMapping volume bdm to detach
5587         :param instance: the Instance object to detach the volume from
5588         :param destroy_bdm: if True, the corresponding BDM entry will be marked
5589                             as deleted. Disabling this is useful for operations
5590                             like rebuild, when we don't want to destroy BDM
5591         :param attachment_id: The volume attachment_id for the given instance
5592                               and volume.
5593         """
5594         volume_id = bdm.volume_id
5595         compute_utils.notify_about_volume_attach_detach(
5596             context, instance, self.host,
5597             action=fields.NotificationAction.VOLUME_DETACH,
5598             phase=fields.NotificationPhase.START,
5599             volume_id=volume_id)
5600 
5601         self._notify_volume_usage_detach(context, instance, bdm)
5602 
5603         LOG.info('Detaching volume %(volume_id)s',
5604                  {'volume_id': volume_id}, instance=instance)
5605 
5606         driver_bdm = driver_block_device.convert_volume(bdm)
5607         driver_bdm.detach(context, instance, self.volume_api, self.driver,
5608                           attachment_id=attachment_id, destroy_bdm=destroy_bdm)
5609 
5610         info = dict(volume_id=volume_id)
5611         self._notify_about_instance_usage(
5612             context, instance, "volume.detach", extra_usage_info=info)
5613         compute_utils.notify_about_volume_attach_detach(
5614             context, instance, self.host,
5615             action=fields.NotificationAction.VOLUME_DETACH,
5616             phase=fields.NotificationPhase.END,
5617             volume_id=volume_id)
5618 
5619         if 'tag' in bdm and bdm.tag:
5620             self._delete_disk_metadata(instance, bdm)
5621         if destroy_bdm:
5622             bdm.destroy()
5623 
5624     def _delete_disk_metadata(self, instance, bdm):
5625         for device in instance.device_metadata.devices:
5626             if isinstance(device, objects.DiskMetadata):
5627                 if 'serial' in device:
5628                     if device.serial == bdm.volume_id:
5629                         instance.device_metadata.devices.remove(device)
5630                         instance.save()
5631                         break
5632                 else:
5633                     # NOTE(artom) We log the entire device object because all
5634                     # fields are nullable and may not be set
5635                     LOG.warning('Unable to determine whether to clean up '
5636                                 'device metadata for disk %s', device,
5637                                 instance=instance)
5638 
5639     @wrap_exception()
5640     @wrap_instance_event(prefix='compute')
5641     @wrap_instance_fault
5642     def detach_volume(self, context, volume_id, instance, attachment_id):
5643         """Detach a volume from an instance.
5644 
5645         :param context: security context
5646         :param volume_id: the volume id
5647         :param instance: the Instance object to detach the volume from
5648         :param attachment_id: The volume attachment_id for the given instance
5649                               and volume.
5650 
5651         """
5652         bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
5653                 context, volume_id, instance.uuid)
5654         self._detach_volume(context, bdm, instance,
5655                             attachment_id=attachment_id)
5656 
5657     def _init_volume_connection(self, context, new_volume,
5658                                 old_volume_id, connector, bdm,
5659                                 new_attachment_id, mountpoint):
5660         new_volume_id = new_volume['id']
5661         if new_attachment_id is None:
5662             # We're dealing with an old-style attachment so initialize the
5663             # connection so we can get the connection_info.
5664             new_cinfo = self.volume_api.initialize_connection(context,
5665                                                               new_volume_id,
5666                                                               connector)
5667         else:
5668             # Check for multiattach on the new volume and if True, check to
5669             # see if the virt driver supports multiattach.
5670             # TODO(mriedem): This is copied from DriverVolumeBlockDevice
5671             # and should be consolidated into some common code at some point.
5672             vol_multiattach = new_volume.get('multiattach', False)
5673             virt_multiattach = self.driver.capabilities.get(
5674                 'supports_multiattach', False)
5675             if vol_multiattach and not virt_multiattach:
5676                 raise exception.MultiattachNotSupportedByVirtDriver(
5677                     volume_id=new_volume_id)
5678 
5679             # This is a new style attachment and the API created the new
5680             # volume attachment and passed the id to the compute over RPC.
5681             # At this point we need to update the new volume attachment with
5682             # the host connector, which will give us back the new attachment
5683             # connection_info.
5684             new_cinfo = self.volume_api.attachment_update(
5685                 context, new_attachment_id, connector,
5686                 mountpoint)['connection_info']
5687 
5688             if vol_multiattach:
5689                 # This will be used by the volume driver to determine the
5690                 # proper disk configuration.
5691                 new_cinfo['multiattach'] = True
5692 
5693         old_cinfo = jsonutils.loads(bdm['connection_info'])
5694         if old_cinfo and 'serial' not in old_cinfo:
5695             old_cinfo['serial'] = old_volume_id
5696         # NOTE(lyarwood): serial is not always present in the returned
5697         # connection_info so set it if it is missing as we do in
5698         # DriverVolumeBlockDevice.attach().
5699         if 'serial' not in new_cinfo:
5700             new_cinfo['serial'] = new_volume_id
5701         return (old_cinfo, new_cinfo)
5702 
5703     def _swap_volume(self, context, instance, bdm, connector,
5704                      old_volume_id, new_volume, resize_to,
5705                      new_attachment_id, is_cinder_migration):
5706         new_volume_id = new_volume['id']
5707         mountpoint = bdm['device_name']
5708         failed = False
5709         new_cinfo = None
5710         try:
5711             old_cinfo, new_cinfo = self._init_volume_connection(
5712                 context, new_volume, old_volume_id, connector,
5713                 bdm, new_attachment_id, mountpoint)
5714             # NOTE(lyarwood): The Libvirt driver, the only virt driver
5715             # currently implementing swap_volume, will modify the contents of
5716             # new_cinfo when connect_volume is called. This is then saved to
5717             # the BDM in swap_volume for future use outside of this flow.
5718             msg = ("swap_volume: Calling driver volume swap with "
5719                    "connection infos: new: %(new_cinfo)s; "
5720                    "old: %(old_cinfo)s" %
5721                    {'new_cinfo': new_cinfo, 'old_cinfo': old_cinfo})
5722             # Both new and old info might contain password
5723             LOG.debug(strutils.mask_password(msg), instance=instance)
5724 
5725             self.driver.swap_volume(context, old_cinfo, new_cinfo, instance,
5726                                     mountpoint, resize_to)
5727             if new_attachment_id:
5728                 self.volume_api.attachment_complete(context, new_attachment_id)
5729             msg = ("swap_volume: Driver volume swap returned, new "
5730                    "connection_info is now : %(new_cinfo)s" %
5731                    {'new_cinfo': new_cinfo})
5732             LOG.debug(strutils.mask_password(msg))
5733         except Exception as ex:
5734             failed = True
5735             with excutils.save_and_reraise_exception():
5736                 tb = traceback.format_exc()
5737                 compute_utils.notify_about_volume_swap(
5738                     context, instance, self.host,
5739                     fields.NotificationPhase.ERROR,
5740                     old_volume_id, new_volume_id, ex, tb)
5741                 if new_cinfo:
5742                     msg = ("Failed to swap volume %(old_volume_id)s "
5743                            "for %(new_volume_id)s")
5744                     LOG.exception(msg, {'old_volume_id': old_volume_id,
5745                                         'new_volume_id': new_volume_id},
5746                                   instance=instance)
5747                 else:
5748                     msg = ("Failed to connect to volume %(volume_id)s "
5749                            "with volume at %(mountpoint)s")
5750                     LOG.exception(msg, {'volume_id': new_volume_id,
5751                                         'mountpoint': bdm['device_name']},
5752                                   instance=instance)
5753 
5754                 # The API marked the volume as 'detaching' for the old volume
5755                 # so we need to roll that back so the volume goes back to
5756                 # 'in-use' state.
5757                 self.volume_api.roll_detaching(context, old_volume_id)
5758 
5759                 if new_attachment_id is None:
5760                     # The API reserved the new volume so it would be in
5761                     # 'attaching' status, so we need to unreserve it so it
5762                     # goes back to 'available' status.
5763                     self.volume_api.unreserve_volume(context, new_volume_id)
5764                 else:
5765                     # This is a new style attachment for the new volume, which
5766                     # was created in the API. We just need to delete it here
5767                     # to put the new volume back into 'available' status.
5768                     self.volume_api.attachment_delete(
5769                         context, new_attachment_id)
5770         finally:
5771             # TODO(mriedem): This finally block is terribly confusing and is
5772             # trying to do too much. We should consider removing the finally
5773             # block and move whatever needs to happen on success and failure
5774             # into the blocks above for clarity, even if it means a bit of
5775             # redundant code.
5776             conn_volume = new_volume_id if failed else old_volume_id
5777             if new_cinfo:
5778                 LOG.debug("swap_volume: removing Cinder connection "
5779                           "for volume %(volume)s", {'volume': conn_volume},
5780                           instance=instance)
5781                 if bdm.attachment_id is None:
5782                     # This is the pre-3.44 flow for new-style volume
5783                     # attachments so just terminate the connection.
5784                     self.volume_api.terminate_connection(context,
5785                                                          conn_volume,
5786                                                          connector)
5787                 else:
5788                     # This is a new style volume attachment. If we failed, then
5789                     # the new attachment was already deleted above in the
5790                     # exception block and we have nothing more to do here. If
5791                     # swap_volume was successful in the driver, then we need to
5792                     # "detach" the original attachment by deleting it.
5793                     if not failed:
5794                         self.volume_api.attachment_delete(
5795                             context, bdm.attachment_id)
5796 
5797             # Need to make some decisions based on whether this was
5798             # a Cinder initiated migration or not. The callback to
5799             # migration completion isn't needed in the case of a
5800             # nova initiated simple swap of two volume
5801             # "volume-update" call so skip that. The new attachment
5802             # scenarios will give us a new attachment record and
5803             # that's what we want.
5804             if bdm.attachment_id and not is_cinder_migration:
5805                 # we don't callback to cinder
5806                 comp_ret = {'save_volume_id': new_volume_id}
5807             else:
5808                 # NOTE(lyarwood): The following call to
5809                 # os-migrate-volume-completion returns a dict containing
5810                 # save_volume_id, this volume id has two possible values :
5811                 # 1. old_volume_id if we are migrating (retyping) volumes
5812                 # 2. new_volume_id if we are swapping between two existing
5813                 #    volumes
5814                 # This volume id is later used to update the volume_id and
5815                 # connection_info['serial'] of the BDM.
5816                 comp_ret = self.volume_api.migrate_volume_completion(
5817                                                           context,
5818                                                           old_volume_id,
5819                                                           new_volume_id,
5820                                                           error=failed)
5821                 LOG.debug("swap_volume: Cinder migrate_volume_completion "
5822                           "returned: %(comp_ret)s", {'comp_ret': comp_ret},
5823                           instance=instance)
5824 
5825         return (comp_ret, new_cinfo)
5826 
5827     @wrap_exception()
5828     @wrap_instance_event(prefix='compute')
5829     @wrap_instance_fault
5830     def swap_volume(self, context, old_volume_id, new_volume_id, instance,
5831                     new_attachment_id):
5832         """Swap volume for an instance."""
5833         context = context.elevated()
5834 
5835         compute_utils.notify_about_volume_swap(
5836             context, instance, self.host,
5837             fields.NotificationPhase.START,
5838             old_volume_id, new_volume_id)
5839 
5840         bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
5841                 context, old_volume_id, instance.uuid)
5842         connector = self.driver.get_volume_connector(instance)
5843 
5844         resize_to = 0
5845         old_volume = self.volume_api.get(context, old_volume_id)
5846         # Yes this is a tightly-coupled state check of what's going on inside
5847         # cinder, but we need this while we still support old (v1/v2) and
5848         # new style attachments (v3.44). Once we drop support for old style
5849         # attachments we could think about cleaning up the cinder-initiated
5850         # swap volume API flows.
5851         is_cinder_migration = (
5852             True if old_volume['status'] in ('retyping',
5853                                              'migrating') else False)
5854         old_vol_size = old_volume['size']
5855         new_volume = self.volume_api.get(context, new_volume_id)
5856         new_vol_size = new_volume['size']
5857         if new_vol_size > old_vol_size:
5858             resize_to = new_vol_size
5859 
5860         LOG.info('Swapping volume %(old_volume)s for %(new_volume)s',
5861                  {'old_volume': old_volume_id, 'new_volume': new_volume_id},
5862                  instance=instance)
5863         comp_ret, new_cinfo = self._swap_volume(context,
5864                                                 instance,
5865                                                 bdm,
5866                                                 connector,
5867                                                 old_volume_id,
5868                                                 new_volume,
5869                                                 resize_to,
5870                                                 new_attachment_id,
5871                                                 is_cinder_migration)
5872 
5873         # NOTE(lyarwood): Update the BDM with the modified new_cinfo and
5874         # correct volume_id returned by Cinder.
5875         save_volume_id = comp_ret['save_volume_id']
5876         new_cinfo['serial'] = save_volume_id
5877         values = {
5878             'connection_info': jsonutils.dumps(new_cinfo),
5879             'source_type': 'volume',
5880             'destination_type': 'volume',
5881             'snapshot_id': None,
5882             'volume_id': save_volume_id,
5883             'no_device': None}
5884 
5885         if resize_to:
5886             values['volume_size'] = resize_to
5887 
5888         if new_attachment_id is not None:
5889             # This was a volume swap for a new-style attachment so we
5890             # need to update the BDM attachment_id for the new attachment.
5891             values['attachment_id'] = new_attachment_id
5892 
5893         LOG.debug("swap_volume: Updating volume %(volume_id)s BDM record with "
5894                   "%(updates)s", {'volume_id': bdm.volume_id,
5895                                   'updates': values},
5896                   instance=instance)
5897         bdm.update(values)
5898         bdm.save()
5899 
5900         compute_utils.notify_about_volume_swap(
5901             context, instance, self.host,
5902             fields.NotificationPhase.END,
5903             old_volume_id, new_volume_id)
5904 
5905     @wrap_exception()
5906     def remove_volume_connection(self, context, volume_id, instance):
5907         """Remove the volume connection on this host
5908 
5909         Detach the volume from this instance on this host, and if this is
5910         the cinder v2 flow, call cinder to terminate the connection.
5911         """
5912         try:
5913             bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
5914                     context, volume_id, instance.uuid)
5915             driver_bdm = driver_block_device.convert_volume(bdm)
5916             driver_bdm.driver_detach(context, instance,
5917                                      self.volume_api, self.driver)
5918             if bdm.attachment_id is None:
5919                 # cinder v2 api flow
5920                 connector = self.driver.get_volume_connector(instance)
5921                 self.volume_api.terminate_connection(context, volume_id,
5922                                                      connector)
5923         except exception.NotFound:
5924             pass
5925 
5926     @wrap_exception()
5927     @wrap_instance_event(prefix='compute')
5928     @wrap_instance_fault
5929     def attach_interface(self, context, instance, network_id, port_id,
5930                          requested_ip, tag):
5931         """Use hotplug to add an network adapter to an instance."""
5932         if not self.driver.capabilities.get('supports_attach_interface',
5933                                             False):
5934             raise exception.AttachInterfaceNotSupported(
5935                 instance_uuid=instance.uuid)
5936         if (tag and not
5937             self.driver.capabilities.get('supports_tagged_attach_interface',
5938                                          False)):
5939             raise exception.NetworkInterfaceTaggedAttachNotSupported()
5940 
5941         compute_utils.notify_about_instance_action(
5942             context, instance, self.host,
5943             action=fields.NotificationAction.INTERFACE_ATTACH,
5944             phase=fields.NotificationPhase.START)
5945 
5946         bind_host_id = self.driver.network_binding_host_id(context, instance)
5947         network_info = self.network_api.allocate_port_for_instance(
5948             context, instance, port_id, network_id, requested_ip,
5949             bind_host_id=bind_host_id, tag=tag)
5950         if len(network_info) != 1:
5951             LOG.error('allocate_port_for_instance returned %(ports)s '
5952                       'ports', {'ports': len(network_info)})
5953             # TODO(elod.illes): an instance.interface_attach.error notification
5954             # should be sent here
5955             raise exception.InterfaceAttachFailed(
5956                     instance_uuid=instance.uuid)
5957         image_meta = objects.ImageMeta.from_instance(instance)
5958 
5959         try:
5960             self.driver.attach_interface(context, instance, image_meta,
5961                                          network_info[0])
5962         except exception.NovaException as ex:
5963             port_id = network_info[0].get('id')
5964             LOG.warning("attach interface failed , try to deallocate "
5965                         "port %(port_id)s, reason: %(msg)s",
5966                         {'port_id': port_id, 'msg': ex},
5967                         instance=instance)
5968             try:
5969                 self.network_api.deallocate_port_for_instance(
5970                     context, instance, port_id)
5971             except Exception:
5972                 LOG.warning("deallocate port %(port_id)s failed",
5973                             {'port_id': port_id}, instance=instance)
5974 
5975             tb = traceback.format_exc()
5976             compute_utils.notify_about_instance_action(
5977                 context, instance, self.host,
5978                 action=fields.NotificationAction.INTERFACE_ATTACH,
5979                 phase=fields.NotificationPhase.ERROR,
5980                 exception=ex, tb=tb)
5981 
5982             raise exception.InterfaceAttachFailed(
5983                 instance_uuid=instance.uuid)
5984 
5985         compute_utils.notify_about_instance_action(
5986             context, instance, self.host,
5987             action=fields.NotificationAction.INTERFACE_ATTACH,
5988             phase=fields.NotificationPhase.END)
5989 
5990         return network_info[0]
5991 
5992     @wrap_exception()
5993     @wrap_instance_event(prefix='compute')
5994     @wrap_instance_fault
5995     def detach_interface(self, context, instance, port_id):
5996         """Detach a network adapter from an instance."""
5997         network_info = instance.info_cache.network_info
5998         condemned = None
5999         for vif in network_info:
6000             if vif['id'] == port_id:
6001                 condemned = vif
6002                 break
6003         if condemned is None:
6004             raise exception.PortNotFound(_("Port %s is not "
6005                                            "attached") % port_id)
6006 
6007         compute_utils.notify_about_instance_action(
6008             context, instance, self.host,
6009             action=fields.NotificationAction.INTERFACE_DETACH,
6010             phase=fields.NotificationPhase.START)
6011 
6012         try:
6013             self.driver.detach_interface(context, instance, condemned)
6014         except exception.NovaException as ex:
6015             # If the instance was deleted before the interface was detached,
6016             # just log it at debug.
6017             log_level = (logging.DEBUG
6018                          if isinstance(ex, exception.InstanceNotFound)
6019                          else logging.WARNING)
6020             LOG.log(log_level,
6021                     "Detach interface failed, port_id=%(port_id)s, reason: "
6022                     "%(msg)s", {'port_id': port_id, 'msg': ex},
6023                     instance=instance)
6024             raise exception.InterfaceDetachFailed(instance_uuid=instance.uuid)
6025         else:
6026             try:
6027                 self.network_api.deallocate_port_for_instance(
6028                     context, instance, port_id)
6029             except Exception as ex:
6030                 with excutils.save_and_reraise_exception():
6031                     # Since this is a cast operation, log the failure for
6032                     # triage.
6033                     LOG.warning('Failed to deallocate port %(port_id)s '
6034                                 'for instance. Error: %(error)s',
6035                                 {'port_id': port_id, 'error': ex},
6036                                 instance=instance)
6037 
6038         compute_utils.notify_about_instance_action(
6039             context, instance, self.host,
6040             action=fields.NotificationAction.INTERFACE_DETACH,
6041             phase=fields.NotificationPhase.END)
6042 
6043     def _get_compute_info(self, context, host):
6044         return objects.ComputeNode.get_first_node_by_host_for_old_compat(
6045             context, host)
6046 
6047     @wrap_exception()
6048     def check_instance_shared_storage(self, ctxt, instance, data):
6049         """Check if the instance files are shared
6050 
6051         :param ctxt: security context
6052         :param instance: dict of instance data
6053         :param data: result of driver.check_instance_shared_storage_local
6054 
6055         Returns True if instance disks located on shared storage and
6056         False otherwise.
6057         """
6058         return self.driver.check_instance_shared_storage_remote(ctxt, data)
6059 
6060     @wrap_exception()
6061     @wrap_instance_event(prefix='compute')
6062     @wrap_instance_fault
6063     def check_can_live_migrate_destination(self, ctxt, instance,
6064                                            block_migration, disk_over_commit):
6065         """Check if it is possible to execute live migration.
6066 
6067         This runs checks on the destination host, and then calls
6068         back to the source host to check the results.
6069 
6070         :param context: security context
6071         :param instance: dict of instance data
6072         :param block_migration: if true, prepare for block migration
6073                                 if None, calculate it in driver
6074         :param disk_over_commit: if true, allow disk over commit
6075                                  if None, ignore disk usage checking
6076         :returns: a dict containing migration info
6077         """
6078         src_compute_info = obj_base.obj_to_primitive(
6079             self._get_compute_info(ctxt, instance.host))
6080         dst_compute_info = obj_base.obj_to_primitive(
6081             self._get_compute_info(ctxt, CONF.host))
6082         dest_check_data = self.driver.check_can_live_migrate_destination(ctxt,
6083             instance, src_compute_info, dst_compute_info,
6084             block_migration, disk_over_commit)
6085         LOG.debug('destination check data is %s', dest_check_data)
6086         try:
6087             migrate_data = self.compute_rpcapi.\
6088                                 check_can_live_migrate_source(ctxt, instance,
6089                                                               dest_check_data)
6090         finally:
6091             self.driver.cleanup_live_migration_destination_check(ctxt,
6092                     dest_check_data)
6093         return migrate_data
6094 
6095     @wrap_exception()
6096     @wrap_instance_event(prefix='compute')
6097     @wrap_instance_fault
6098     def check_can_live_migrate_source(self, ctxt, instance, dest_check_data):
6099         """Check if it is possible to execute live migration.
6100 
6101         This checks if the live migration can succeed, based on the
6102         results from check_can_live_migrate_destination.
6103 
6104         :param ctxt: security context
6105         :param instance: dict of instance data
6106         :param dest_check_data: result of check_can_live_migrate_destination
6107         :returns: a dict containing migration info
6108         """
6109         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
6110             ctxt, instance.uuid)
6111         is_volume_backed = compute_utils.is_volume_backed_instance(
6112             ctxt, instance, bdms)
6113         dest_check_data.is_volume_backed = is_volume_backed
6114         block_device_info = self._get_instance_block_device_info(
6115                             ctxt, instance, refresh_conn_info=False, bdms=bdms)
6116         result = self.driver.check_can_live_migrate_source(ctxt, instance,
6117                                                            dest_check_data,
6118                                                            block_device_info)
6119         LOG.debug('source check data is %s', result)
6120         return result
6121 
6122     @wrap_exception()
6123     @wrap_instance_event(prefix='compute')
6124     @wrap_instance_fault
6125     def pre_live_migration(self, context, instance, block_migration, disk,
6126                            migrate_data):
6127         """Preparations for live migration at dest host.
6128 
6129         :param context: security context
6130         :param instance: dict of instance data
6131         :param block_migration: if true, prepare for block migration
6132         :param disk: disk info of instance
6133         :param migrate_data: A dict or LiveMigrateData object holding data
6134                              required for live migration without shared
6135                              storage.
6136         :returns: migrate_data containing additional migration info
6137         """
6138         LOG.debug('pre_live_migration data is %s', migrate_data)
6139 
6140         migrate_data.old_vol_attachment_ids = {}
6141         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
6142             context, instance.uuid)
6143         try:
6144             connector = self.driver.get_volume_connector(instance)
6145             for bdm in bdms:
6146                 if bdm.is_volume and bdm.attachment_id is not None:
6147                     # This bdm uses the new cinder v3.44 API.
6148                     # We will create a new attachment for this
6149                     # volume on this migration destination host. The old
6150                     # attachment will be deleted on the source host
6151                     # when the migration succeeds. The old attachment_id
6152                     # is stored in dict with the key being the bdm.volume_id
6153                     # so it can be restored on rollback.
6154                     #
6155                     # Also note that attachment_update is not needed as we
6156                     # are providing the connector in the create call.
6157                     attach_ref = self.volume_api.attachment_create(
6158                         context, bdm.volume_id, bdm.instance_uuid,
6159                         connector=connector, mountpoint=bdm.device_name)
6160 
6161                     # save current attachment so we can detach it on success,
6162                     # or restore it on a rollback.
6163                     migrate_data.old_vol_attachment_ids[bdm.volume_id] = \
6164                         bdm.attachment_id
6165 
6166                     # update the bdm with the new attachment_id.
6167                     bdm.attachment_id = attach_ref['id']
6168                     bdm.save()
6169         except Exception:
6170             # If we raise, migrate_data with the updated attachment ids
6171             # will not be returned to the source host for rollback.
6172             # So we need to rollback new attachments here.
6173             with excutils.save_and_reraise_exception():
6174                 old_attachments = migrate_data.old_vol_attachment_ids
6175                 for bdm in bdms:
6176                     if (bdm.is_volume and bdm.attachment_id is not None and
6177                             bdm.volume_id in old_attachments):
6178                         self.volume_api.attachment_delete(context,
6179                                                           bdm.attachment_id)
6180                         bdm.attachment_id = old_attachments[bdm.volume_id]
6181                         bdm.save()
6182 
6183         block_device_info = self._get_instance_block_device_info(
6184                             context, instance, refresh_conn_info=True,
6185                             bdms=bdms)
6186 
6187         network_info = self.network_api.get_instance_nw_info(context, instance)
6188         self._notify_about_instance_usage(
6189                      context, instance, "live_migration.pre.start",
6190                      network_info=network_info)
6191         compute_utils.notify_about_instance_action(
6192             context, instance, self.host,
6193             action=fields.NotificationAction.LIVE_MIGRATION_PRE,
6194             phase=fields.NotificationPhase.START)
6195 
6196         # The driver pre_live_migration will plug vifs on the host.
6197         # We call plug_vifs before calling ensure_filtering_rules_for_instance,
6198         # to ensure bridge is set up.
6199         migrate_data = self.driver.pre_live_migration(context,
6200                                        instance,
6201                                        block_device_info,
6202                                        network_info,
6203                                        disk,
6204                                        migrate_data)
6205         LOG.debug('driver pre_live_migration data is %s', migrate_data)
6206         # driver.pre_live_migration is what plugs vifs on the destination host
6207         # so now we can set the wait_for_vif_plugged flag in the migrate_data
6208         # object which the source compute will use to determine if it should
6209         # wait for a 'network-vif-plugged' event from neutron before starting
6210         # the actual guest transfer in the hypervisor
6211         migrate_data.wait_for_vif_plugged = (
6212             CONF.compute.live_migration_wait_for_vif_plug)
6213 
6214         # Volume connections are complete, tell cinder that all the
6215         # attachments have completed.
6216         for bdm in bdms:
6217             if bdm.is_volume and bdm.attachment_id is not None:
6218                 self.volume_api.attachment_complete(context,
6219                                                     bdm.attachment_id)
6220 
6221         # NOTE(tr3buchet): setup networks on destination host
6222         self.network_api.setup_networks_on_host(context, instance,
6223                                                          self.host)
6224 
6225         # Creating filters to hypervisors and firewalls.
6226         # An example is that nova-instance-instance-xxx,
6227         # which is written to libvirt.xml(Check "virsh nwfilter-list")
6228         # This nwfilter is necessary on the destination host.
6229         # In addition, this method is creating filtering rule
6230         # onto destination host.
6231         self.driver.ensure_filtering_rules_for_instance(instance,
6232                                             network_info)
6233 
6234         self._notify_about_instance_usage(
6235                      context, instance, "live_migration.pre.end",
6236                      network_info=network_info)
6237         compute_utils.notify_about_instance_action(
6238             context, instance, self.host,
6239             action=fields.NotificationAction.LIVE_MIGRATION_PRE,
6240             phase=fields.NotificationPhase.END)
6241 
6242         LOG.debug('pre_live_migration result data is %s', migrate_data)
6243         return migrate_data
6244 
6245     @staticmethod
6246     def _neutron_failed_live_migration_callback(event_name, instance):
6247         msg = ('Neutron reported failure during live migration '
6248                'with %(event)s for instance %(uuid)s')
6249         msg_args = {'event': event_name, 'uuid': instance.uuid}
6250         if CONF.vif_plugging_is_fatal:
6251             raise exception.VirtualInterfacePlugException(msg % msg_args)
6252         LOG.error(msg, msg_args)
6253 
6254     @staticmethod
6255     def _get_neutron_events_for_live_migration(instance):
6256         # We don't generate events if CONF.vif_plugging_timeout=0
6257         # meaning that the operator disabled using them.
6258         if CONF.vif_plugging_timeout and utils.is_neutron():
6259             return [('network-vif-plugged', vif['id'])
6260                     for vif in instance.get_network_info()]
6261         else:
6262             return []
6263 
6264     def _cleanup_pre_live_migration(self, context, dest, instance,
6265                                     migration, migrate_data):
6266         """Helper method for when pre_live_migration fails
6267 
6268         Sets the migration status to "error" and rolls back the live migration
6269         setup on the destination host.
6270 
6271         :param context: The user request context.
6272         :type context: nova.context.RequestContext
6273         :param dest: The live migration destination hostname.
6274         :type dest: str
6275         :param instance: The instance being live migrated.
6276         :type instance: nova.objects.Instance
6277         :param migration: The migration record tracking this live migration.
6278         :type migration: nova.objects.Migration
6279         :param migrate_data: Data about the live migration, populated from
6280                              the destination host.
6281         :type migrate_data: Subclass of nova.objects.LiveMigrateData
6282         """
6283         self._set_migration_status(migration, 'error')
6284         # Make sure we set this for _rollback_live_migration()
6285         # so it can find it, as expected if it was called later
6286         migrate_data.migration = migration
6287         self._rollback_live_migration(context, instance, dest,
6288                                       migrate_data)
6289 
6290     def _do_live_migration(self, context, dest, instance, block_migration,
6291                            migration, migrate_data):
6292         # NOTE(danms): We should enhance the RT to account for migrations
6293         # and use the status field to denote when the accounting has been
6294         # done on source/destination. For now, this is just here for status
6295         # reporting
6296         self._set_migration_status(migration, 'preparing')
6297 
6298         class _BreakWaitForInstanceEvent(Exception):
6299             """Used as a signal to stop waiting for the network-vif-plugged
6300             event when we discover that
6301             [compute]/live_migration_wait_for_vif_plug is not set on the
6302             destination.
6303             """
6304             pass
6305 
6306         events = self._get_neutron_events_for_live_migration(instance)
6307         try:
6308             if ('block_migration' in migrate_data and
6309                     migrate_data.block_migration):
6310                 block_device_info = self._get_instance_block_device_info(
6311                     context, instance)
6312                 disk = self.driver.get_instance_disk_info(
6313                     instance, block_device_info=block_device_info)
6314             else:
6315                 disk = None
6316 
6317             deadline = CONF.vif_plugging_timeout
6318             error_cb = self._neutron_failed_live_migration_callback
6319             # In order to avoid a race with the vif plugging that the virt
6320             # driver does on the destination host, we register our events
6321             # to wait for before calling pre_live_migration. Then if the
6322             # dest host reports back that we shouldn't wait, we can break
6323             # out of the context manager using _BreakWaitForInstanceEvent.
6324             with self.virtapi.wait_for_instance_event(
6325                     instance, events, deadline=deadline,
6326                     error_callback=error_cb):
6327                 with timeutils.StopWatch() as timer:
6328                     migrate_data = self.compute_rpcapi.pre_live_migration(
6329                         context, instance,
6330                         block_migration, disk, dest, migrate_data)
6331                 LOG.info('Took %0.2f seconds for pre_live_migration on '
6332                          'destination host %s.',
6333                          timer.elapsed(), dest, instance=instance)
6334                 wait_for_vif_plugged = (
6335                     'wait_for_vif_plugged' in migrate_data and
6336                     migrate_data.wait_for_vif_plugged)
6337                 if events and not wait_for_vif_plugged:
6338                     raise _BreakWaitForInstanceEvent
6339         except _BreakWaitForInstanceEvent:
6340             if events:
6341                 LOG.debug('Not waiting for events after pre_live_migration: '
6342                           '%s. ', events, instance=instance)
6343             # This is a bit weird, but we need to clear sys.exc_info() so that
6344             # oslo.log formatting does not inadvertently use it later if an
6345             # error message is logged without an explicit exc_info. This is
6346             # only a problem with python 2.
6347             if six.PY2:
6348                 sys.exc_clear()
6349         except exception.VirtualInterfacePlugException:
6350             with excutils.save_and_reraise_exception():
6351                 LOG.exception('Failed waiting for network virtual interfaces '
6352                               'to be plugged on the destination host %s.',
6353                               dest, instance=instance)
6354                 self._cleanup_pre_live_migration(
6355                     context, dest, instance, migration, migrate_data)
6356         except eventlet.timeout.Timeout:
6357             msg = 'Timed out waiting for events: %s'
6358             LOG.warning(msg, events, instance=instance)
6359             if CONF.vif_plugging_is_fatal:
6360                 self._cleanup_pre_live_migration(
6361                     context, dest, instance, migration, migrate_data)
6362                 raise exception.MigrationError(reason=msg % events)
6363         except Exception:
6364             with excutils.save_and_reraise_exception():
6365                 LOG.exception('Pre live migration failed at %s',
6366                               dest, instance=instance)
6367                 self._cleanup_pre_live_migration(
6368                     context, dest, instance, migration, migrate_data)
6369 
6370         # NOTE(Kevin_Zheng): Pop the migration from the waiting queue
6371         # if it exist in the queue, then we are good to moving on, if
6372         # not, some other process must have aborted it, then we should
6373         # rollback.
6374         try:
6375             self._waiting_live_migrations.pop(instance.uuid)
6376         except KeyError:
6377             LOG.debug('Migration %s aborted by another process, rollback.',
6378                       migration.uuid, instance=instance)
6379             migrate_data.migration = migration
6380             self._rollback_live_migration(context, instance, dest,
6381                                           migrate_data, 'cancelled')
6382             self._notify_live_migrate_abort_end(context, instance)
6383             return
6384 
6385         self._set_migration_status(migration, 'running')
6386         if migrate_data:
6387             migrate_data.migration = migration
6388         LOG.debug('live_migration data is %s', migrate_data)
6389         try:
6390             self.driver.live_migration(context, instance, dest,
6391                                        self._post_live_migration,
6392                                        self._rollback_live_migration,
6393                                        block_migration, migrate_data)
6394         except Exception:
6395             LOG.exception('Live migration failed.', instance=instance)
6396             with excutils.save_and_reraise_exception():
6397                 # Put instance and migration into error state,
6398                 # as its almost certainly too late to rollback
6399                 self._set_migration_status(migration, 'error')
6400                 # first refresh instance as it may have got updated by
6401                 # post_live_migration_at_destination
6402                 instance.refresh()
6403                 self._set_instance_obj_error_state(context, instance,
6404                                                    clean_task_state=True)
6405 
6406     @wrap_exception()
6407     @wrap_instance_event(prefix='compute')
6408     @wrap_instance_fault
6409     def live_migration(self, context, dest, instance, block_migration,
6410                        migration, migrate_data):
6411         """Executing live migration.
6412 
6413         :param context: security context
6414         :param dest: destination host
6415         :param instance: a nova.objects.instance.Instance object
6416         :param block_migration: if true, prepare for block migration
6417         :param migration: an nova.objects.Migration object
6418         :param migrate_data: implementation specific params
6419 
6420         """
6421         self._set_migration_status(migration, 'queued')
6422         # NOTE(Kevin_Zheng): Submit the live_migration job to the pool and
6423         # put the returned Future object into dict mapped with migration.uuid
6424         # in order to be able to track and abort it in the future.
6425         self._waiting_live_migrations[instance.uuid] = (None, None)
6426         try:
6427             future = self._live_migration_executor.submit(
6428                 self._do_live_migration, context, dest, instance,
6429                 block_migration, migration, migrate_data)
6430             self._waiting_live_migrations[instance.uuid] = (migration, future)
6431         except RuntimeError:
6432             # ThreadPoolExecutor.submit will raise RuntimeError if the pool
6433             # is shutdown, which happens in _cleanup_live_migrations_in_pool.
6434             LOG.info('Migration %s failed to submit as the compute service '
6435                      'is shutting down.', migration.uuid, instance=instance)
6436             self._set_migration_status(migration, 'error')
6437             raise exception.LiveMigrationNotSubmitted(
6438                 migration_uuid=migration.uuid, instance_uuid=instance.uuid)
6439 
6440     @wrap_exception()
6441     @wrap_instance_event(prefix='compute')
6442     @wrap_instance_fault
6443     def live_migration_force_complete(self, context, instance):
6444         """Force live migration to complete.
6445 
6446         :param context: Security context
6447         :param instance: The instance that is being migrated
6448         """
6449 
6450         self._notify_about_instance_usage(
6451             context, instance, 'live.migration.force.complete.start')
6452         compute_utils.notify_about_instance_action(
6453             context, instance, self.host,
6454             action=fields.NotificationAction.LIVE_MIGRATION_FORCE_COMPLETE,
6455             phase=fields.NotificationPhase.START)
6456         self.driver.live_migration_force_complete(instance)
6457         self._notify_about_instance_usage(
6458             context, instance, 'live.migration.force.complete.end')
6459         compute_utils.notify_about_instance_action(
6460             context, instance, self.host,
6461             action=fields.NotificationAction.LIVE_MIGRATION_FORCE_COMPLETE,
6462             phase=fields.NotificationPhase.END)
6463 
6464     def _notify_live_migrate_abort_end(self, context, instance):
6465         self._notify_about_instance_usage(
6466             context, instance, 'live.migration.abort.end')
6467         compute_utils.notify_about_instance_action(
6468             context, instance, self.host,
6469             action=fields.NotificationAction.LIVE_MIGRATION_ABORT,
6470             phase=fields.NotificationPhase.END)
6471 
6472     @wrap_exception()
6473     @wrap_instance_event(prefix='compute')
6474     @wrap_instance_fault
6475     def live_migration_abort(self, context, instance, migration_id):
6476         """Abort an in-progress live migration.
6477 
6478         :param context: Security context
6479         :param instance: The instance that is being migrated
6480         :param migration_id: ID of in-progress live migration
6481 
6482         """
6483         self._notify_about_instance_usage(
6484             context, instance, 'live.migration.abort.start')
6485         compute_utils.notify_about_instance_action(
6486             context, instance, self.host,
6487             action=fields.NotificationAction.LIVE_MIGRATION_ABORT,
6488             phase=fields.NotificationPhase.START)
6489         # NOTE(Kevin_Zheng): Pop the migration out from the queue, this might
6490         # lead to 3 scenarios:
6491         # 1. The selected migration is still in queue, and the future.cancel()
6492         #    succeed, then the abort action is succeed, mark the migration
6493         #    status to 'cancelled'.
6494         # 2. The selected migration is still in queue, but the future.cancel()
6495         #    failed, then the _do_live_migration() has started executing, and
6496         #    the migration status is 'preparing', then we just pop it from the
6497         #    queue, and the migration process will handle it later. And the
6498         #    migration status couldn't be 'running' in this scenario because
6499         #    if _do_live_migration has started executing and we've already
6500         #    popped it from the queue and set the migration status to
6501         #    'running' at this point, popping it here will raise KeyError at
6502         #    which point we check if it's running and if so, we abort the old
6503         #    way.
6504         # 3. The selected migration is not in the queue, then the migration
6505         #    status is 'running', let the driver handle it.
6506         try:
6507             migration, future = (
6508                 self._waiting_live_migrations.pop(instance.uuid))
6509             if future and future.cancel():
6510                 # If we got here, we've successfully aborted the queued
6511                 # migration and _do_live_migration won't run so we need
6512                 # to set the migration status to cancelled and send the
6513                 # notification. If Future.cancel() fails, it means
6514                 # _do_live_migration is running and the migration status
6515                 # is preparing, and _do_live_migration() itself will attempt
6516                 # to pop the queued migration, hit a KeyError, and rollback,
6517                 # set the migration to cancelled and send the
6518                 # live.migration.abort.end notification.
6519                 self._set_migration_status(migration, 'cancelled')
6520         except KeyError:
6521             migration = objects.Migration.get_by_id(context, migration_id)
6522             if migration.status != 'running':
6523                 raise exception.InvalidMigrationState(
6524                     migration_id=migration_id, instance_uuid=instance.uuid,
6525                     state=migration.status, method='abort live migration')
6526             self.driver.live_migration_abort(instance)
6527         self._notify_live_migrate_abort_end(context, instance)
6528 
6529     def _live_migration_cleanup_flags(self, migrate_data):
6530         """Determine whether disks or instance path need to be cleaned up after
6531         live migration (at source on success, at destination on rollback)
6532 
6533         Block migration needs empty image at destination host before migration
6534         starts, so if any failure occurs, any empty images has to be deleted.
6535 
6536         Also Volume backed live migration w/o shared storage needs to delete
6537         newly created instance-xxx dir on the destination as a part of its
6538         rollback process
6539 
6540         :param migrate_data: implementation specific data
6541         :returns: (bool, bool) -- do_cleanup, destroy_disks
6542         """
6543         # NOTE(pkoniszewski): block migration specific params are set inside
6544         # migrate_data objects for drivers that expose block live migration
6545         # information (i.e. Libvirt, Xenapi and HyperV). For other drivers
6546         # cleanup is not needed.
6547         do_cleanup = False
6548         destroy_disks = False
6549         if isinstance(migrate_data, migrate_data_obj.LibvirtLiveMigrateData):
6550             # No instance booting at source host, but instance dir
6551             # must be deleted for preparing next block migration
6552             # must be deleted for preparing next live migration w/o shared
6553             # storage
6554             do_cleanup = not migrate_data.is_shared_instance_path
6555             destroy_disks = not migrate_data.is_shared_block_storage
6556         elif isinstance(migrate_data, migrate_data_obj.XenapiLiveMigrateData):
6557             do_cleanup = migrate_data.block_migration
6558             destroy_disks = migrate_data.block_migration
6559         elif isinstance(migrate_data, migrate_data_obj.HyperVLiveMigrateData):
6560             # NOTE(claudiub): We need to cleanup any zombie Planned VM.
6561             do_cleanup = True
6562             destroy_disks = not migrate_data.is_shared_instance_path
6563 
6564         return (do_cleanup, destroy_disks)
6565 
6566     @wrap_exception()
6567     @wrap_instance_fault
6568     def _post_live_migration(self, ctxt, instance,
6569                             dest, block_migration=False, migrate_data=None):
6570         """Post operations for live migration.
6571 
6572         This method is called from live_migration
6573         and mainly updating database record.
6574 
6575         :param ctxt: security context
6576         :param instance: instance dict
6577         :param dest: destination host
6578         :param block_migration: if true, prepare for block migration
6579         :param migrate_data: if not None, it is a dict which has data
6580         required for live migration without shared storage
6581 
6582         """
6583         LOG.info('_post_live_migration() is started..',
6584                  instance=instance)
6585 
6586         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
6587                 ctxt, instance.uuid)
6588 
6589         # Cleanup source host post live-migration
6590         block_device_info = self._get_instance_block_device_info(
6591                             ctxt, instance, bdms=bdms)
6592         self.driver.post_live_migration(ctxt, instance, block_device_info,
6593                                         migrate_data)
6594 
6595         # Detaching volumes.
6596         connector = self.driver.get_volume_connector(instance)
6597         for bdm in bdms:
6598             if bdm.is_volume:
6599                 if bdm.attachment_id is None:
6600                     # Prior to cinder v3.44:
6601                     # We don't want to actually mark the volume detached, or
6602                     # delete the bdm, just remove the connection from this
6603                     # host.
6604                     #
6605                     # remove the volume connection without detaching from
6606                     # hypervisor because the instance is not running anymore
6607                     # on the current host
6608                     self.volume_api.terminate_connection(ctxt, bdm.volume_id,
6609                                                          connector)
6610                 else:
6611                     # cinder v3.44 api flow - delete the old attachment
6612                     # for the source host
6613                     old_attachment_id = \
6614                         migrate_data.old_vol_attachment_ids[bdm.volume_id]
6615                     self.volume_api.attachment_delete(ctxt, old_attachment_id)
6616 
6617         # Releasing vlan.
6618         # (not necessary in current implementation?)
6619 
6620         network_info = self.network_api.get_instance_nw_info(ctxt, instance)
6621 
6622         self._notify_about_instance_usage(ctxt, instance,
6623                                           "live_migration._post.start",
6624                                           network_info=network_info)
6625         compute_utils.notify_about_instance_action(
6626             ctxt, instance, self.host,
6627             action=fields.NotificationAction.LIVE_MIGRATION_POST,
6628             phase=fields.NotificationPhase.START)
6629         # Releasing security group ingress rule.
6630         LOG.debug('Calling driver.unfilter_instance from _post_live_migration',
6631                   instance=instance)
6632         self.driver.unfilter_instance(instance,
6633                                       network_info)
6634 
6635         migration = {'source_compute': self.host,
6636                      'dest_compute': dest, }
6637         # For neutron, migrate_instance_start will activate the destination
6638         # host port bindings, if there are any created by conductor before live
6639         # migration started.
6640         self.network_api.migrate_instance_start(ctxt,
6641                                                 instance,
6642                                                 migration)
6643 
6644         destroy_vifs = False
6645         try:
6646             # It's possible that the vif type changed on the destination
6647             # host and is already bound and active, so we need to use the
6648             # stashed source vifs in migrate_data.vifs (if present) to unplug
6649             # on the source host.
6650             unplug_nw_info = network_info
6651             if migrate_data and 'vifs' in migrate_data:
6652                 nw_info = []
6653                 for migrate_vif in migrate_data.vifs:
6654                     nw_info.append(migrate_vif.source_vif)
6655                 unplug_nw_info = network_model.NetworkInfo.hydrate(nw_info)
6656                 LOG.debug('Calling driver.post_live_migration_at_source '
6657                           'with original source VIFs from migrate_data: %s',
6658                           unplug_nw_info, instance=instance)
6659             self.driver.post_live_migration_at_source(ctxt, instance,
6660                                                       unplug_nw_info)
6661         except NotImplementedError as ex:
6662             LOG.debug(ex, instance=instance)
6663             # For all hypervisors other than libvirt, there is a possibility
6664             # they are unplugging networks from source node in the cleanup
6665             # method
6666             destroy_vifs = True
6667 
6668         # NOTE(danms): Save source node before calling post method on
6669         # destination, which will update it
6670         source_node = instance.node
6671 
6672         # Define domain at destination host, without doing it,
6673         # pause/suspend/terminate do not work.
6674         post_at_dest_success = True
6675         try:
6676             self.compute_rpcapi.post_live_migration_at_destination(ctxt,
6677                     instance, block_migration, dest)
6678         except Exception as error:
6679             post_at_dest_success = False
6680             # We don't want to break _post_live_migration() if
6681             # post_live_migration_at_destination() fails as it should never
6682             # affect cleaning up source node.
6683             LOG.exception("Post live migration at destination %s failed",
6684                           dest, instance=instance, error=error)
6685 
6686         do_cleanup, destroy_disks = self._live_migration_cleanup_flags(
6687                 migrate_data)
6688 
6689         if do_cleanup:
6690             LOG.debug('Calling driver.cleanup from _post_live_migration',
6691                       instance=instance)
6692             self.driver.cleanup(ctxt, instance, unplug_nw_info,
6693                                 destroy_disks=destroy_disks,
6694                                 migrate_data=migrate_data,
6695                                 destroy_vifs=destroy_vifs)
6696 
6697         self.instance_events.clear_events_for_instance(instance)
6698 
6699         # NOTE(timello): make sure we update available resources on source
6700         # host even before next periodic task.
6701         self.update_available_resource(ctxt)
6702 
6703         self._update_scheduler_instance_info(ctxt, instance)
6704         self._notify_about_instance_usage(ctxt, instance,
6705                                           "live_migration._post.end",
6706                                           network_info=network_info)
6707         compute_utils.notify_about_instance_action(
6708             ctxt, instance, self.host,
6709             action=fields.NotificationAction.LIVE_MIGRATION_POST,
6710             phase=fields.NotificationPhase.END)
6711         if post_at_dest_success:
6712             LOG.info('Migrating instance to %s finished successfully.',
6713                      dest, instance=instance)
6714 
6715         self._clean_instance_console_tokens(ctxt, instance)
6716         if migrate_data and migrate_data.obj_attr_is_set('migration'):
6717             migrate_data.migration.status = 'completed'
6718             migrate_data.migration.save()
6719             migration = migrate_data.migration
6720             rc = self.scheduler_client.reportclient
6721             # Check to see if our migration has its own allocations
6722             allocs = rc.get_allocations_for_consumer(ctxt, migration.uuid)
6723         else:
6724             # We didn't have data on a migration, which means we can't
6725             # look up to see if we had new-style migration-based
6726             # allocations. This should really only happen in cases of
6727             # a buggy virt driver or some really old component in the
6728             # system. Log a warning so we know it happened.
6729             allocs = None
6730             LOG.warning('Live migration ended with no migrate_data '
6731                         'record. Unable to clean up migration-based '
6732                         'allocations which is almost certainly not '
6733                         'an expected situation.')
6734 
6735         if allocs:
6736             # We had a migration-based allocation that we need to handle
6737             self._delete_allocation_after_move(ctxt,
6738                                                instance,
6739                                                migrate_data.migration,
6740                                                instance.flavor,
6741                                                source_node)
6742         else:
6743             # No migration-based allocations, so do the old thing and
6744             # attempt to clean up any doubled per-instance allocation
6745             rt = self._get_resource_tracker()
6746             rt.delete_allocation_for_migrated_instance(
6747                 ctxt, instance, source_node)
6748 
6749     def _consoles_enabled(self):
6750         """Returns whether a console is enable."""
6751         return (CONF.vnc.enabled or CONF.spice.enabled or
6752                 CONF.rdp.enabled or CONF.serial_console.enabled or
6753                 CONF.mks.enabled)
6754 
6755     def _clean_instance_console_tokens(self, ctxt, instance):
6756         """Clean console tokens stored for an instance."""
6757         # If the database backend isn't in use, don't bother trying to clean
6758         # tokens. The database backend is not supported for cells v1.
6759         if not CONF.cells.enable and self._consoles_enabled():
6760             objects.ConsoleAuthToken.\
6761                 clean_console_auths_for_instance(ctxt, instance.uuid)
6762 
6763     @wrap_exception()
6764     @wrap_instance_event(prefix='compute')
6765     @wrap_instance_fault
6766     def post_live_migration_at_destination(self, context, instance,
6767                                            block_migration):
6768         """Post operations for live migration .
6769 
6770         :param context: security context
6771         :param instance: Instance dict
6772         :param block_migration: if true, prepare for block migration
6773 
6774         """
6775         LOG.info('Post operation of migration started',
6776                  instance=instance)
6777 
6778         # NOTE(tr3buchet): setup networks on destination host
6779         #                  this is called a second time because
6780         #                  multi_host does not create the bridge in
6781         #                  plug_vifs
6782         # NOTE(mriedem): This is a no-op for neutron.
6783         self.network_api.setup_networks_on_host(context, instance,
6784                                                          self.host)
6785         migration = {'source_compute': instance.host,
6786                      'dest_compute': self.host, }
6787         self.network_api.migrate_instance_finish(context,
6788                                                  instance,
6789                                                  migration)
6790 
6791         network_info = self.network_api.get_instance_nw_info(context, instance)
6792         self._notify_about_instance_usage(
6793                      context, instance, "live_migration.post.dest.start",
6794                      network_info=network_info)
6795         compute_utils.notify_about_instance_action(context, instance,
6796                 self.host,
6797                 action=fields.NotificationAction.LIVE_MIGRATION_POST_DEST,
6798                 phase=fields.NotificationPhase.START)
6799         block_device_info = self._get_instance_block_device_info(context,
6800                                                                  instance)
6801 
6802         try:
6803             self.driver.post_live_migration_at_destination(
6804                 context, instance, network_info, block_migration,
6805                 block_device_info)
6806         except Exception:
6807             with excutils.save_and_reraise_exception():
6808                 instance.vm_state = vm_states.ERROR
6809                 LOG.error('Unexpected error during post live migration at '
6810                           'destination host.', instance=instance)
6811         finally:
6812             # Restore instance state and update host
6813             current_power_state = self._get_power_state(context, instance)
6814             node_name = None
6815             prev_host = instance.host
6816             try:
6817                 compute_node = self._get_compute_info(context, self.host)
6818                 node_name = compute_node.hypervisor_hostname
6819             except exception.ComputeHostNotFound:
6820                 LOG.exception('Failed to get compute_info for %s', self.host)
6821             finally:
6822                 instance.host = self.host
6823                 instance.power_state = current_power_state
6824                 instance.task_state = None
6825                 instance.node = node_name
6826                 instance.progress = 0
6827                 instance.save(expected_task_state=task_states.MIGRATING)
6828 
6829         # NOTE(tr3buchet): tear down networks on source host (nova-net)
6830         # NOTE(mriedem): For neutron, this will delete any inactive source
6831         # host port bindings.
6832         try:
6833             self.network_api.setup_networks_on_host(context, instance,
6834                                                     prev_host, teardown=True)
6835         except exception.PortBindingDeletionFailed as e:
6836             # Removing the inactive port bindings from the source host is not
6837             # critical so just log an error but don't fail.
6838             LOG.error('Network cleanup failed for source host %s during post '
6839                       'live migration. You may need to manually clean up '
6840                       'resources in the network service. Error: %s',
6841                       prev_host, six.text_type(e))
6842         # NOTE(vish): this is necessary to update dhcp for nova-network
6843         # NOTE(mriedem): This is a no-op for neutron.
6844         self.network_api.setup_networks_on_host(context, instance, self.host)
6845         self._notify_about_instance_usage(
6846                      context, instance, "live_migration.post.dest.end",
6847                      network_info=network_info)
6848         compute_utils.notify_about_instance_action(context, instance,
6849                 self.host,
6850                 action=fields.NotificationAction.LIVE_MIGRATION_POST_DEST,
6851                 phase=fields.NotificationPhase.END)
6852 
6853     @wrap_exception()
6854     @wrap_instance_fault
6855     def _rollback_live_migration(self, context, instance,
6856                                  dest, migrate_data=None,
6857                                  migration_status='error'):
6858         """Recovers Instance/volume state from migrating -> running.
6859 
6860         :param context: security context
6861         :param instance: nova.objects.instance.Instance object
6862         :param dest:
6863             This method is called from live migration src host.
6864             This param specifies destination host.
6865         :param migrate_data:
6866             if not none, contains implementation specific data.
6867         :param migration_status:
6868             Contains the status we want to set for the migration object
6869 
6870         """
6871         if (isinstance(migrate_data, migrate_data_obj.LiveMigrateData) and
6872               migrate_data.obj_attr_is_set('migration')):
6873             migration = migrate_data.migration
6874         else:
6875             migration = None
6876 
6877         if migration:
6878             # Remove allocations created in Placement for the dest node.
6879             # If migration is None, we must be so old we don't have placement,
6880             # so no need to do something else.
6881             self._revert_allocation(context, instance, migration)
6882         else:
6883             LOG.error('Unable to revert allocations during live migration '
6884                       'rollback; compute driver did not provide migrate_data',
6885                       instance=instance)
6886 
6887         instance.task_state = None
6888         instance.progress = 0
6889         instance.save(expected_task_state=[task_states.MIGRATING])
6890 
6891         # NOTE(tr3buchet): setup networks on source host (really it's re-setup
6892         #                  for nova-network)
6893         # NOTE(mriedem): This is a no-op for neutron.
6894         self.network_api.setup_networks_on_host(context, instance, self.host)
6895 
6896         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
6897                 context, instance.uuid)
6898         for bdm in bdms:
6899             if bdm.is_volume:
6900                 # remove the connection on the destination host
6901                 self.compute_rpcapi.remove_volume_connection(
6902                         context, instance, bdm.volume_id, dest)
6903 
6904                 if bdm.attachment_id:
6905                     # 3.44 cinder api flow. Set the bdm's
6906                     # attachment_id to the old attachment of the source
6907                     # host. If old_attachments is not there, then
6908                     # there was an error before the new attachment was made.
6909                     old_attachments = migrate_data.old_vol_attachment_ids \
6910                         if 'old_vol_attachment_ids' in migrate_data else None
6911                     if old_attachments and bdm.volume_id in old_attachments:
6912                         self.volume_api.attachment_delete(context,
6913                                                           bdm.attachment_id)
6914                         bdm.attachment_id = old_attachments[bdm.volume_id]
6915                         bdm.save()
6916 
6917         self._notify_about_instance_usage(context, instance,
6918                                           "live_migration._rollback.start")
6919         compute_utils.notify_about_instance_action(context, instance,
6920                 self.host,
6921                 action=fields.NotificationAction.LIVE_MIGRATION_ROLLBACK,
6922                 phase=fields.NotificationPhase.START,
6923                 bdms=bdms)
6924 
6925         do_cleanup, destroy_disks = self._live_migration_cleanup_flags(
6926                 migrate_data)
6927 
6928         if do_cleanup:
6929             self.compute_rpcapi.rollback_live_migration_at_destination(
6930                     context, instance, dest, destroy_disks=destroy_disks,
6931                     migrate_data=migrate_data)
6932         elif utils.is_neutron():
6933             # The port binding profiles need to be cleaned up.
6934             with errors_out_migration_ctxt(migration):
6935                 try:
6936                     # This call will delete any inactive destination host
6937                     # port bindings.
6938                     self.network_api.setup_networks_on_host(
6939                         context, instance, host=dest, teardown=True)
6940                 except exception.PortBindingDeletionFailed as e:
6941                     # Removing the inactive port bindings from the destination
6942                     # host is not critical so just log an error but don't fail.
6943                     LOG.error(
6944                         'Network cleanup failed for destination host %s '
6945                         'during live migration rollback. You may need to '
6946                         'manually clean up resources in the network service. '
6947                         'Error: %s', dest, six.text_type(e))
6948                 except Exception:
6949                     with excutils.save_and_reraise_exception():
6950                         LOG.exception(
6951                             'An error occurred while cleaning up networking '
6952                             'during live migration rollback.',
6953                             instance=instance)
6954 
6955         self._notify_about_instance_usage(context, instance,
6956                                           "live_migration._rollback.end")
6957         compute_utils.notify_about_instance_action(context, instance,
6958                 self.host,
6959                 action=fields.NotificationAction.LIVE_MIGRATION_ROLLBACK,
6960                 phase=fields.NotificationPhase.END,
6961                 bdms=bdms)
6962 
6963         self._set_migration_status(migration, migration_status)
6964 
6965     @wrap_exception()
6966     @wrap_instance_event(prefix='compute')
6967     @wrap_instance_fault
6968     def rollback_live_migration_at_destination(self, context, instance,
6969                                                destroy_disks,
6970                                                migrate_data):
6971         """Cleaning up image directory that is created pre_live_migration.
6972 
6973         :param context: security context
6974         :param instance: a nova.objects.instance.Instance object sent over rpc
6975         :param destroy_disks: whether to destroy volumes or not
6976         :param migrate_data: contains migration info
6977         """
6978         network_info = self.network_api.get_instance_nw_info(context, instance)
6979         self._notify_about_instance_usage(
6980                       context, instance, "live_migration.rollback.dest.start",
6981                       network_info=network_info)
6982         compute_utils.notify_about_instance_action(
6983             context, instance, self.host,
6984             action=fields.NotificationAction.LIVE_MIGRATION_ROLLBACK_DEST,
6985             phase=fields.NotificationPhase.START)
6986         try:
6987             # NOTE(tr3buchet): tear down networks on dest host (nova-net)
6988             # NOTE(mriedem): For neutron, this call will delete any
6989             # destination host port bindings.
6990             # TODO(mriedem): We should eventually remove this call from
6991             # this method (rollback_live_migration_at_destination) since this
6992             # method is only called conditionally based on whether or not the
6993             # instance is running on shared storage. _rollback_live_migration
6994             # already calls this method for neutron if we are running on
6995             # shared storage.
6996             self.network_api.setup_networks_on_host(context, instance,
6997                                                     self.host, teardown=True)
6998         except exception.PortBindingDeletionFailed as e:
6999             # Removing the inactive port bindings from the destination
7000             # host is not critical so just log an error but don't fail.
7001             LOG.error(
7002                 'Network cleanup failed for destination host %s '
7003                 'during live migration rollback. You may need to '
7004                 'manually clean up resources in the network service. '
7005                 'Error: %s', self.host, six.text_type(e))
7006         except Exception:
7007             with excutils.save_and_reraise_exception():
7008                 # NOTE(tdurakov): even if teardown networks fails driver
7009                 # should try to rollback live migration on destination.
7010                 LOG.exception('An error occurred while deallocating network.',
7011                               instance=instance)
7012         finally:
7013             # always run this even if setup_networks_on_host fails
7014             # NOTE(vish): The mapping is passed in so the driver can disconnect
7015             #             from remote volumes if necessary
7016             block_device_info = self._get_instance_block_device_info(context,
7017                                                                      instance)
7018             self.driver.rollback_live_migration_at_destination(
7019                 context, instance, network_info, block_device_info,
7020                 destroy_disks=destroy_disks, migrate_data=migrate_data)
7021 
7022         self._notify_about_instance_usage(
7023                         context, instance, "live_migration.rollback.dest.end",
7024                         network_info=network_info)
7025         compute_utils.notify_about_instance_action(
7026             context, instance, self.host,
7027             action=fields.NotificationAction.LIVE_MIGRATION_ROLLBACK_DEST,
7028             phase=fields.NotificationPhase.END)
7029 
7030     @periodic_task.periodic_task(
7031         spacing=CONF.heal_instance_info_cache_interval)
7032     def _heal_instance_info_cache(self, context):
7033         """Called periodically.  On every call, try to update the
7034         info_cache's network information for another instance by
7035         calling to the network manager.
7036 
7037         This is implemented by keeping a cache of uuids of instances
7038         that live on this host.  On each call, we pop one off of a
7039         list, pull the DB record, and try the call to the network API.
7040         If anything errors don't fail, as it's possible the instance
7041         has been deleted, etc.
7042         """
7043         heal_interval = CONF.heal_instance_info_cache_interval
7044         if not heal_interval:
7045             return
7046 
7047         instance_uuids = getattr(self, '_instance_uuids_to_heal', [])
7048         instance = None
7049 
7050         LOG.debug('Starting heal instance info cache')
7051 
7052         if not instance_uuids:
7053             # The list of instances to heal is empty so rebuild it
7054             LOG.debug('Rebuilding the list of instances to heal')
7055             db_instances = objects.InstanceList.get_by_host(
7056                 context, self.host, expected_attrs=[], use_slave=True)
7057             for inst in db_instances:
7058                 # We don't want to refresh the cache for instances
7059                 # which are building or deleting so don't put them
7060                 # in the list. If they are building they will get
7061                 # added to the list next time we build it.
7062                 if (inst.vm_state == vm_states.BUILDING):
7063                     LOG.debug('Skipping network cache update for instance '
7064                               'because it is Building.', instance=inst)
7065                     continue
7066                 if (inst.task_state == task_states.DELETING):
7067                     LOG.debug('Skipping network cache update for instance '
7068                               'because it is being deleted.', instance=inst)
7069                     continue
7070 
7071                 if not instance:
7072                     # Save the first one we find so we don't
7073                     # have to get it again
7074                     instance = inst
7075                 else:
7076                     instance_uuids.append(inst['uuid'])
7077 
7078             self._instance_uuids_to_heal = instance_uuids
7079         else:
7080             # Find the next valid instance on the list
7081             while instance_uuids:
7082                 try:
7083                     inst = objects.Instance.get_by_uuid(
7084                             context, instance_uuids.pop(0),
7085                             expected_attrs=['system_metadata', 'info_cache',
7086                                             'flavor'],
7087                             use_slave=True)
7088                 except exception.InstanceNotFound:
7089                     # Instance is gone.  Try to grab another.
7090                     continue
7091 
7092                 # Check the instance hasn't been migrated
7093                 if inst.host != self.host:
7094                     LOG.debug('Skipping network cache update for instance '
7095                               'because it has been migrated to another '
7096                               'host.', instance=inst)
7097                 # Check the instance isn't being deleting
7098                 elif inst.task_state == task_states.DELETING:
7099                     LOG.debug('Skipping network cache update for instance '
7100                               'because it is being deleted.', instance=inst)
7101                 else:
7102                     instance = inst
7103                     break
7104 
7105         if instance:
7106             # We have an instance now to refresh
7107             try:
7108                 # Call to network API to get instance info.. this will
7109                 # force an update to the instance's info_cache
7110                 self.network_api.get_instance_nw_info(context, instance)
7111                 LOG.debug('Updated the network info_cache for instance',
7112                           instance=instance)
7113             except exception.InstanceNotFound:
7114                 # Instance is gone.
7115                 LOG.debug('Instance no longer exists. Unable to refresh',
7116                           instance=instance)
7117                 return
7118             except exception.InstanceInfoCacheNotFound:
7119                 # InstanceInfoCache is gone.
7120                 LOG.debug('InstanceInfoCache no longer exists. '
7121                           'Unable to refresh', instance=instance)
7122             except Exception:
7123                 LOG.error('An error occurred while refreshing the network '
7124                           'cache.', instance=instance, exc_info=True)
7125         else:
7126             LOG.debug("Didn't find any instances for network info cache "
7127                       "update.")
7128 
7129     @periodic_task.periodic_task
7130     def _poll_rebooting_instances(self, context):
7131         if CONF.reboot_timeout > 0:
7132             filters = {'task_state':
7133                        [task_states.REBOOTING,
7134                         task_states.REBOOT_STARTED,
7135                         task_states.REBOOT_PENDING],
7136                        'host': self.host}
7137             rebooting = objects.InstanceList.get_by_filters(
7138                 context, filters, expected_attrs=[], use_slave=True)
7139 
7140             to_poll = []
7141             for instance in rebooting:
7142                 if timeutils.is_older_than(instance.updated_at,
7143                                            CONF.reboot_timeout):
7144                     to_poll.append(instance)
7145 
7146             self.driver.poll_rebooting_instances(CONF.reboot_timeout, to_poll)
7147 
7148     @periodic_task.periodic_task
7149     def _poll_rescued_instances(self, context):
7150         if CONF.rescue_timeout > 0:
7151             filters = {'vm_state': vm_states.RESCUED,
7152                        'host': self.host}
7153             rescued_instances = objects.InstanceList.get_by_filters(
7154                 context, filters, expected_attrs=["system_metadata"],
7155                 use_slave=True)
7156 
7157             to_unrescue = []
7158             for instance in rescued_instances:
7159                 if timeutils.is_older_than(instance.launched_at,
7160                                            CONF.rescue_timeout):
7161                     to_unrescue.append(instance)
7162 
7163             for instance in to_unrescue:
7164                 self.compute_api.unrescue(context, instance)
7165 
7166     @periodic_task.periodic_task
7167     def _poll_unconfirmed_resizes(self, context):
7168         if CONF.resize_confirm_window == 0:
7169             return
7170 
7171         migrations = objects.MigrationList.get_unconfirmed_by_dest_compute(
7172                 context, CONF.resize_confirm_window, self.host,
7173                 use_slave=True)
7174 
7175         migrations_info = dict(migration_count=len(migrations),
7176                 confirm_window=CONF.resize_confirm_window)
7177 
7178         if migrations_info["migration_count"] > 0:
7179             LOG.info("Found %(migration_count)d unconfirmed migrations "
7180                      "older than %(confirm_window)d seconds",
7181                      migrations_info)
7182 
7183         def _set_migration_to_error(migration, reason, **kwargs):
7184             LOG.warning("Setting migration %(migration_id)s to error: "
7185                         "%(reason)s",
7186                         {'migration_id': migration['id'], 'reason': reason},
7187                         **kwargs)
7188             migration.status = 'error'
7189             with migration.obj_as_admin():
7190                 migration.save()
7191 
7192         for migration in migrations:
7193             instance_uuid = migration.instance_uuid
7194             LOG.info("Automatically confirming migration "
7195                      "%(migration_id)s for instance %(instance_uuid)s",
7196                      {'migration_id': migration.id,
7197                       'instance_uuid': instance_uuid})
7198             expected_attrs = ['metadata', 'system_metadata']
7199             try:
7200                 instance = objects.Instance.get_by_uuid(context,
7201                             instance_uuid, expected_attrs=expected_attrs,
7202                             use_slave=True)
7203             except exception.InstanceNotFound:
7204                 reason = (_("Instance %s not found") %
7205                           instance_uuid)
7206                 _set_migration_to_error(migration, reason)
7207                 continue
7208             if instance.vm_state == vm_states.ERROR:
7209                 reason = _("In ERROR state")
7210                 _set_migration_to_error(migration, reason,
7211                                         instance=instance)
7212                 continue
7213             # race condition: The instance in DELETING state should not be
7214             # set the migration state to error, otherwise the instance in
7215             # to be deleted which is in RESIZED state
7216             # will not be able to confirm resize
7217             if instance.task_state in [task_states.DELETING,
7218                                        task_states.SOFT_DELETING]:
7219                 msg = ("Instance being deleted or soft deleted during resize "
7220                        "confirmation. Skipping.")
7221                 LOG.debug(msg, instance=instance)
7222                 continue
7223 
7224             # race condition: This condition is hit when this method is
7225             # called between the save of the migration record with a status of
7226             # finished and the save of the instance object with a state of
7227             # RESIZED. The migration record should not be set to error.
7228             if instance.task_state == task_states.RESIZE_FINISH:
7229                 msg = ("Instance still resizing during resize "
7230                        "confirmation. Skipping.")
7231                 LOG.debug(msg, instance=instance)
7232                 continue
7233 
7234             vm_state = instance.vm_state
7235             task_state = instance.task_state
7236             if vm_state != vm_states.RESIZED or task_state is not None:
7237                 reason = (_("In states %(vm_state)s/%(task_state)s, not "
7238                            "RESIZED/None") %
7239                           {'vm_state': vm_state,
7240                            'task_state': task_state})
7241                 _set_migration_to_error(migration, reason,
7242                                         instance=instance)
7243                 continue
7244             try:
7245                 self.compute_api.confirm_resize(context, instance,
7246                                                 migration=migration)
7247             except Exception as e:
7248                 LOG.info("Error auto-confirming resize: %s. "
7249                          "Will retry later.", e, instance=instance)
7250 
7251     @periodic_task.periodic_task(spacing=CONF.shelved_poll_interval)
7252     def _poll_shelved_instances(self, context):
7253 
7254         if CONF.shelved_offload_time <= 0:
7255             return
7256 
7257         filters = {'vm_state': vm_states.SHELVED,
7258                    'task_state': None,
7259                    'host': self.host}
7260         shelved_instances = objects.InstanceList.get_by_filters(
7261             context, filters=filters, expected_attrs=['system_metadata'],
7262             use_slave=True)
7263 
7264         to_gc = []
7265         for instance in shelved_instances:
7266             sys_meta = instance.system_metadata
7267             shelved_at = timeutils.parse_strtime(sys_meta['shelved_at'])
7268             if timeutils.is_older_than(shelved_at, CONF.shelved_offload_time):
7269                 to_gc.append(instance)
7270 
7271         for instance in to_gc:
7272             try:
7273                 instance.task_state = task_states.SHELVING_OFFLOADING
7274                 instance.save(expected_task_state=(None,))
7275                 self.shelve_offload_instance(context, instance,
7276                                              clean_shutdown=False)
7277             except Exception:
7278                 LOG.exception('Periodic task failed to offload instance.',
7279                               instance=instance)
7280 
7281     @periodic_task.periodic_task
7282     def _instance_usage_audit(self, context):
7283         if not CONF.instance_usage_audit:
7284             return
7285 
7286         begin, end = utils.last_completed_audit_period()
7287         if objects.TaskLog.get(context, 'instance_usage_audit', begin, end,
7288                                self.host):
7289             return
7290 
7291         instances = objects.InstanceList.get_active_by_window_joined(
7292             context, begin, end, host=self.host,
7293             expected_attrs=['system_metadata', 'info_cache', 'metadata',
7294                             'flavor'],
7295             use_slave=True)
7296         num_instances = len(instances)
7297         errors = 0
7298         successes = 0
7299         LOG.info("Running instance usage audit for host %(host)s "
7300                  "from %(begin_time)s to %(end_time)s. "
7301                  "%(number_instances)s instances.",
7302                  {'host': self.host,
7303                   'begin_time': begin,
7304                   'end_time': end,
7305                   'number_instances': num_instances})
7306         start_time = time.time()
7307         task_log = objects.TaskLog(context)
7308         task_log.task_name = 'instance_usage_audit'
7309         task_log.period_beginning = begin
7310         task_log.period_ending = end
7311         task_log.host = self.host
7312         task_log.task_items = num_instances
7313         task_log.message = 'Instance usage audit started...'
7314         task_log.begin_task()
7315         for instance in instances:
7316             try:
7317                 compute_utils.notify_usage_exists(
7318                     self.notifier, context, instance, self.host,
7319                     ignore_missing_network_data=False)
7320                 successes += 1
7321             except Exception:
7322                 LOG.exception('Failed to generate usage '
7323                               'audit for instance '
7324                               'on host %s', self.host,
7325                               instance=instance)
7326                 errors += 1
7327         task_log.errors = errors
7328         task_log.message = (
7329             'Instance usage audit ran for host %s, %s instances in %s seconds.'
7330             % (self.host, num_instances, time.time() - start_time))
7331         task_log.end_task()
7332 
7333     @periodic_task.periodic_task(spacing=CONF.bandwidth_poll_interval)
7334     def _poll_bandwidth_usage(self, context):
7335 
7336         if not self._bw_usage_supported:
7337             return
7338 
7339         prev_time, start_time = utils.last_completed_audit_period()
7340 
7341         curr_time = time.time()
7342         if (curr_time - self._last_bw_usage_poll >
7343                 CONF.bandwidth_poll_interval):
7344             self._last_bw_usage_poll = curr_time
7345             LOG.info("Updating bandwidth usage cache")
7346             cells_update_interval = CONF.cells.bandwidth_update_interval
7347             if (cells_update_interval > 0 and
7348                    curr_time - self._last_bw_usage_cell_update >
7349                            cells_update_interval):
7350                 self._last_bw_usage_cell_update = curr_time
7351                 update_cells = True
7352             else:
7353                 update_cells = False
7354 
7355             instances = objects.InstanceList.get_by_host(context,
7356                                                               self.host,
7357                                                               use_slave=True)
7358             try:
7359                 bw_counters = self.driver.get_all_bw_counters(instances)
7360             except NotImplementedError:
7361                 # NOTE(mdragon): Not all hypervisors have bandwidth polling
7362                 # implemented yet.  If they don't it doesn't break anything,
7363                 # they just don't get the info in the usage events.
7364                 # NOTE(PhilDay): Record that its not supported so we can
7365                 # skip fast on future calls rather than waste effort getting
7366                 # the list of instances.
7367                 LOG.info("Bandwidth usage not supported by %(driver)s.",
7368                          {'driver': CONF.compute_driver})
7369                 self._bw_usage_supported = False
7370                 return
7371 
7372             refreshed = timeutils.utcnow()
7373             for bw_ctr in bw_counters:
7374                 # Allow switching of greenthreads between queries.
7375                 greenthread.sleep(0)
7376                 bw_in = 0
7377                 bw_out = 0
7378                 last_ctr_in = None
7379                 last_ctr_out = None
7380                 usage = objects.BandwidthUsage.get_by_instance_uuid_and_mac(
7381                     context, bw_ctr['uuid'], bw_ctr['mac_address'],
7382                     start_period=start_time, use_slave=True)
7383                 if usage:
7384                     bw_in = usage.bw_in
7385                     bw_out = usage.bw_out
7386                     last_ctr_in = usage.last_ctr_in
7387                     last_ctr_out = usage.last_ctr_out
7388                 else:
7389                     usage = (objects.BandwidthUsage.
7390                              get_by_instance_uuid_and_mac(
7391                         context, bw_ctr['uuid'], bw_ctr['mac_address'],
7392                         start_period=prev_time, use_slave=True))
7393                     if usage:
7394                         last_ctr_in = usage.last_ctr_in
7395                         last_ctr_out = usage.last_ctr_out
7396 
7397                 if last_ctr_in is not None:
7398                     if bw_ctr['bw_in'] < last_ctr_in:
7399                         # counter rollover
7400                         bw_in += bw_ctr['bw_in']
7401                     else:
7402                         bw_in += (bw_ctr['bw_in'] - last_ctr_in)
7403 
7404                 if last_ctr_out is not None:
7405                     if bw_ctr['bw_out'] < last_ctr_out:
7406                         # counter rollover
7407                         bw_out += bw_ctr['bw_out']
7408                     else:
7409                         bw_out += (bw_ctr['bw_out'] - last_ctr_out)
7410 
7411                 objects.BandwidthUsage(context=context).create(
7412                                               bw_ctr['uuid'],
7413                                               bw_ctr['mac_address'],
7414                                               bw_in,
7415                                               bw_out,
7416                                               bw_ctr['bw_in'],
7417                                               bw_ctr['bw_out'],
7418                                               start_period=start_time,
7419                                               last_refreshed=refreshed,
7420                                               update_cells=update_cells)
7421 
7422     def _get_host_volume_bdms(self, context, use_slave=False):
7423         """Return all block device mappings on a compute host."""
7424         compute_host_bdms = []
7425         instances = objects.InstanceList.get_by_host(context, self.host,
7426             use_slave=use_slave)
7427         for instance in instances:
7428             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
7429                     context, instance.uuid, use_slave=use_slave)
7430             instance_bdms = [bdm for bdm in bdms if bdm.is_volume]
7431             compute_host_bdms.append(dict(instance=instance,
7432                                           instance_bdms=instance_bdms))
7433 
7434         return compute_host_bdms
7435 
7436     def _update_volume_usage_cache(self, context, vol_usages):
7437         """Updates the volume usage cache table with a list of stats."""
7438         for usage in vol_usages:
7439             # Allow switching of greenthreads between queries.
7440             greenthread.sleep(0)
7441             vol_usage = objects.VolumeUsage(context)
7442             vol_usage.volume_id = usage['volume']
7443             vol_usage.instance_uuid = usage['instance'].uuid
7444             vol_usage.project_id = usage['instance'].project_id
7445             vol_usage.user_id = usage['instance'].user_id
7446             vol_usage.availability_zone = usage['instance'].availability_zone
7447             vol_usage.curr_reads = usage['rd_req']
7448             vol_usage.curr_read_bytes = usage['rd_bytes']
7449             vol_usage.curr_writes = usage['wr_req']
7450             vol_usage.curr_write_bytes = usage['wr_bytes']
7451             vol_usage.save()
7452             self.notifier.info(context, 'volume.usage',
7453                                compute_utils.usage_volume_info(vol_usage))
7454 
7455     @periodic_task.periodic_task(spacing=CONF.volume_usage_poll_interval)
7456     def _poll_volume_usage(self, context):
7457         if CONF.volume_usage_poll_interval == 0:
7458             return
7459 
7460         compute_host_bdms = self._get_host_volume_bdms(context,
7461                                                        use_slave=True)
7462         if not compute_host_bdms:
7463             return
7464 
7465         LOG.debug("Updating volume usage cache")
7466         try:
7467             vol_usages = self.driver.get_all_volume_usage(context,
7468                                                           compute_host_bdms)
7469         except NotImplementedError:
7470             return
7471 
7472         self._update_volume_usage_cache(context, vol_usages)
7473 
7474     @periodic_task.periodic_task(spacing=CONF.sync_power_state_interval,
7475                                  run_immediately=True)
7476     def _sync_power_states(self, context):
7477         """Align power states between the database and the hypervisor.
7478 
7479         To sync power state data we make a DB call to get the number of
7480         virtual machines known by the hypervisor and if the number matches the
7481         number of virtual machines known by the database, we proceed in a lazy
7482         loop, one database record at a time, checking if the hypervisor has the
7483         same power state as is in the database.
7484         """
7485         db_instances = objects.InstanceList.get_by_host(context, self.host,
7486                                                         expected_attrs=[],
7487                                                         use_slave=True)
7488 
7489         try:
7490             num_vm_instances = self.driver.get_num_instances()
7491         except exception.VirtDriverNotReady as e:
7492             # If the virt driver is not ready, like ironic-api not being up
7493             # yet in the case of ironic, just log it and exit.
7494             LOG.info('Skipping _sync_power_states periodic task due to: %s', e)
7495             return
7496 
7497         num_db_instances = len(db_instances)
7498 
7499         if num_vm_instances != num_db_instances:
7500             LOG.warning("While synchronizing instance power states, found "
7501                         "%(num_db_instances)s instances in the database "
7502                         "and %(num_vm_instances)s instances on the "
7503                         "hypervisor.",
7504                         {'num_db_instances': num_db_instances,
7505                          'num_vm_instances': num_vm_instances})
7506 
7507         def _sync(db_instance):
7508             # NOTE(melwitt): This must be synchronized as we query state from
7509             #                two separate sources, the driver and the database.
7510             #                They are set (in stop_instance) and read, in sync.
7511             @utils.synchronized(db_instance.uuid)
7512             def query_driver_power_state_and_sync():
7513                 self._query_driver_power_state_and_sync(context, db_instance)
7514 
7515             try:
7516                 query_driver_power_state_and_sync()
7517             except Exception:
7518                 LOG.exception("Periodic sync_power_state task had an "
7519                               "error while processing an instance.",
7520                               instance=db_instance)
7521 
7522             self._syncs_in_progress.pop(db_instance.uuid)
7523 
7524         for db_instance in db_instances:
7525             # process syncs asynchronously - don't want instance locking to
7526             # block entire periodic task thread
7527             uuid = db_instance.uuid
7528             if uuid in self._syncs_in_progress:
7529                 LOG.debug('Sync already in progress for %s', uuid)
7530             else:
7531                 LOG.debug('Triggering sync for uuid %s', uuid)
7532                 self._syncs_in_progress[uuid] = True
7533                 self._sync_power_pool.spawn_n(_sync, db_instance)
7534 
7535     def _query_driver_power_state_and_sync(self, context, db_instance):
7536         if db_instance.task_state is not None:
7537             LOG.info("During sync_power_state the instance has a "
7538                      "pending task (%(task)s). Skip.",
7539                      {'task': db_instance.task_state}, instance=db_instance)
7540             return
7541         # No pending tasks. Now try to figure out the real vm_power_state.
7542         try:
7543             vm_instance = self.driver.get_info(db_instance)
7544             vm_power_state = vm_instance.state
7545         except exception.InstanceNotFound:
7546             vm_power_state = power_state.NOSTATE
7547         # Note(maoy): the above get_info call might take a long time,
7548         # for example, because of a broken libvirt driver.
7549         try:
7550             self._sync_instance_power_state(context,
7551                                             db_instance,
7552                                             vm_power_state,
7553                                             use_slave=True)
7554         except exception.InstanceNotFound:
7555             # NOTE(hanlind): If the instance gets deleted during sync,
7556             # silently ignore.
7557             pass
7558 
7559     def _sync_instance_power_state(self, context, db_instance, vm_power_state,
7560                                    use_slave=False):
7561         """Align instance power state between the database and hypervisor.
7562 
7563         If the instance is not found on the hypervisor, but is in the database,
7564         then a stop() API will be called on the instance.
7565         """
7566 
7567         # We re-query the DB to get the latest instance info to minimize
7568         # (not eliminate) race condition.
7569         db_instance.refresh(use_slave=use_slave)
7570         db_power_state = db_instance.power_state
7571         vm_state = db_instance.vm_state
7572 
7573         if self.host != db_instance.host:
7574             # on the sending end of nova-compute _sync_power_state
7575             # may have yielded to the greenthread performing a live
7576             # migration; this in turn has changed the resident-host
7577             # for the VM; However, the instance is still active, it
7578             # is just in the process of migrating to another host.
7579             # This implies that the compute source must relinquish
7580             # control to the compute destination.
7581             LOG.info("During the sync_power process the "
7582                      "instance has moved from "
7583                      "host %(src)s to host %(dst)s",
7584                      {'src': db_instance.host,
7585                       'dst': self.host},
7586                      instance=db_instance)
7587             return
7588         elif db_instance.task_state is not None:
7589             # on the receiving end of nova-compute, it could happen
7590             # that the DB instance already report the new resident
7591             # but the actual VM has not showed up on the hypervisor
7592             # yet. In this case, let's allow the loop to continue
7593             # and run the state sync in a later round
7594             LOG.info("During sync_power_state the instance has a "
7595                      "pending task (%(task)s). Skip.",
7596                      {'task': db_instance.task_state},
7597                      instance=db_instance)
7598             return
7599 
7600         orig_db_power_state = db_power_state
7601         if vm_power_state != db_power_state:
7602             LOG.info('During _sync_instance_power_state the DB '
7603                      'power_state (%(db_power_state)s) does not match '
7604                      'the vm_power_state from the hypervisor '
7605                      '(%(vm_power_state)s). Updating power_state in the '
7606                      'DB to match the hypervisor.',
7607                      {'db_power_state': db_power_state,
7608                       'vm_power_state': vm_power_state},
7609                      instance=db_instance)
7610             # power_state is always updated from hypervisor to db
7611             db_instance.power_state = vm_power_state
7612             db_instance.save()
7613             db_power_state = vm_power_state
7614 
7615         # Note(maoy): Now resolve the discrepancy between vm_state and
7616         # vm_power_state. We go through all possible vm_states.
7617         if vm_state in (vm_states.BUILDING,
7618                         vm_states.RESCUED,
7619                         vm_states.RESIZED,
7620                         vm_states.SUSPENDED,
7621                         vm_states.ERROR):
7622             # TODO(maoy): we ignore these vm_state for now.
7623             pass
7624         elif vm_state == vm_states.ACTIVE:
7625             # The only rational power state should be RUNNING
7626             if vm_power_state in (power_state.SHUTDOWN,
7627                                   power_state.CRASHED):
7628                 LOG.warning("Instance shutdown by itself. Calling the "
7629                             "stop API. Current vm_state: %(vm_state)s, "
7630                             "current task_state: %(task_state)s, "
7631                             "original DB power_state: %(db_power_state)s, "
7632                             "current VM power_state: %(vm_power_state)s",
7633                             {'vm_state': vm_state,
7634                              'task_state': db_instance.task_state,
7635                              'db_power_state': orig_db_power_state,
7636                              'vm_power_state': vm_power_state},
7637                             instance=db_instance)
7638                 try:
7639                     # Note(maoy): here we call the API instead of
7640                     # brutally updating the vm_state in the database
7641                     # to allow all the hooks and checks to be performed.
7642                     if db_instance.shutdown_terminate:
7643                         self.compute_api.delete(context, db_instance)
7644                     else:
7645                         self.compute_api.stop(context, db_instance)
7646                 except Exception:
7647                     # Note(maoy): there is no need to propagate the error
7648                     # because the same power_state will be retrieved next
7649                     # time and retried.
7650                     # For example, there might be another task scheduled.
7651                     LOG.exception("error during stop() in sync_power_state.",
7652                                   instance=db_instance)
7653             elif vm_power_state == power_state.SUSPENDED:
7654                 LOG.warning("Instance is suspended unexpectedly. Calling "
7655                             "the stop API.", instance=db_instance)
7656                 try:
7657                     self.compute_api.stop(context, db_instance)
7658                 except Exception:
7659                     LOG.exception("error during stop() in sync_power_state.",
7660                                   instance=db_instance)
7661             elif vm_power_state == power_state.PAUSED:
7662                 # Note(maoy): a VM may get into the paused state not only
7663                 # because the user request via API calls, but also
7664                 # due to (temporary) external instrumentations.
7665                 # Before the virt layer can reliably report the reason,
7666                 # we simply ignore the state discrepancy. In many cases,
7667                 # the VM state will go back to running after the external
7668                 # instrumentation is done. See bug 1097806 for details.
7669                 LOG.warning("Instance is paused unexpectedly. Ignore.",
7670                             instance=db_instance)
7671             elif vm_power_state == power_state.NOSTATE:
7672                 # Occasionally, depending on the status of the hypervisor,
7673                 # which could be restarting for example, an instance may
7674                 # not be found.  Therefore just log the condition.
7675                 LOG.warning("Instance is unexpectedly not found. Ignore.",
7676                             instance=db_instance)
7677         elif vm_state == vm_states.STOPPED:
7678             if vm_power_state not in (power_state.NOSTATE,
7679                                       power_state.SHUTDOWN,
7680                                       power_state.CRASHED):
7681                 LOG.warning("Instance is not stopped. Calling "
7682                             "the stop API. Current vm_state: %(vm_state)s,"
7683                             " current task_state: %(task_state)s, "
7684                             "original DB power_state: %(db_power_state)s, "
7685                             "current VM power_state: %(vm_power_state)s",
7686                             {'vm_state': vm_state,
7687                              'task_state': db_instance.task_state,
7688                              'db_power_state': orig_db_power_state,
7689                              'vm_power_state': vm_power_state},
7690                             instance=db_instance)
7691                 try:
7692                     # NOTE(russellb) Force the stop, because normally the
7693                     # compute API would not allow an attempt to stop a stopped
7694                     # instance.
7695                     self.compute_api.force_stop(context, db_instance)
7696                 except Exception:
7697                     LOG.exception("error during stop() in sync_power_state.",
7698                                   instance=db_instance)
7699         elif vm_state == vm_states.PAUSED:
7700             if vm_power_state in (power_state.SHUTDOWN,
7701                                   power_state.CRASHED):
7702                 LOG.warning("Paused instance shutdown by itself. Calling "
7703                             "the stop API.", instance=db_instance)
7704                 try:
7705                     self.compute_api.force_stop(context, db_instance)
7706                 except Exception:
7707                     LOG.exception("error during stop() in sync_power_state.",
7708                                   instance=db_instance)
7709         elif vm_state in (vm_states.SOFT_DELETED,
7710                           vm_states.DELETED):
7711             if vm_power_state not in (power_state.NOSTATE,
7712                                       power_state.SHUTDOWN):
7713                 # Note(maoy): this should be taken care of periodically in
7714                 # _cleanup_running_deleted_instances().
7715                 LOG.warning("Instance is not (soft-)deleted.",
7716                             instance=db_instance)
7717 
7718     @periodic_task.periodic_task
7719     def _reclaim_queued_deletes(self, context):
7720         """Reclaim instances that are queued for deletion."""
7721         interval = CONF.reclaim_instance_interval
7722         if interval <= 0:
7723             LOG.debug("CONF.reclaim_instance_interval <= 0, skipping...")
7724             return
7725 
7726         filters = {'vm_state': vm_states.SOFT_DELETED,
7727                    'task_state': None,
7728                    'host': self.host}
7729         instances = objects.InstanceList.get_by_filters(
7730             context, filters,
7731             expected_attrs=objects.instance.INSTANCE_DEFAULT_FIELDS,
7732             use_slave=True)
7733         for instance in instances:
7734             if self._deleted_old_enough(instance, interval):
7735                 bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
7736                         context, instance.uuid)
7737                 LOG.info('Reclaiming deleted instance', instance=instance)
7738                 try:
7739                     self._delete_instance(context, instance, bdms)
7740                 except Exception as e:
7741                     LOG.warning("Periodic reclaim failed to delete "
7742                                 "instance: %s",
7743                                 e, instance=instance)
7744 
7745     def _get_nodename(self, instance, refresh=False):
7746         """Helper method to get the name of the first available node
7747         on this host. This method should not be used with any operations
7748         on ironic instances since it does not handle multiple nodes.
7749         """
7750         node = self.driver.get_available_nodes(refresh=refresh)[0]
7751         LOG.debug("No node specified, defaulting to %s", node,
7752                   instance=instance)
7753         return node
7754 
7755     def _update_available_resource_for_node(self, context, nodename,
7756                                             startup=False):
7757 
7758         rt = self._get_resource_tracker()
7759         try:
7760             rt.update_available_resource(context, nodename, startup=startup)
7761         except exception.ComputeHostNotFound:
7762             # NOTE(comstud): We can get to this case if a node was
7763             # marked 'deleted' in the DB and then re-added with a
7764             # different auto-increment id. The cached resource
7765             # tracker tried to update a deleted record and failed.
7766             # Don't add this resource tracker to the new dict, so
7767             # that this will resolve itself on the next run.
7768             LOG.info("Compute node '%s' not found in "
7769                      "update_available_resource.", nodename)
7770             # TODO(jaypipes): Yes, this is inefficient to throw away all of the
7771             # compute nodes to force a rebuild, but this is only temporary
7772             # until Ironic baremetal node resource providers are tracked
7773             # properly in the report client and this is a tiny edge case
7774             # anyway.
7775             self._resource_tracker = None
7776             return
7777         except exception.ReshapeFailed:
7778             # We're only supposed to get here on startup, if a reshape was
7779             # needed, was attempted, and failed. We want to kill the service.
7780             with excutils.save_and_reraise_exception():
7781                 LOG.critical("Resource provider data migration failed "
7782                              "fatally during startup for node %s.", nodename)
7783         except exception.ReshapeNeeded:
7784             # This exception should only find its way here if the virt driver's
7785             # update_provider_tree raised it incorrectly: either
7786             # a) After the resource tracker already caught it once and
7787             # reinvoked update_provider_tree with allocations. At this point
7788             # the driver is just supposed to *do* the reshape, so if it raises
7789             # ReshapeNeeded, it's a bug, and we want to kill the compute
7790             # service.
7791             # b) On periodic rather than startup (we only allow reshapes to
7792             # happen on startup). In this case we'll just make the logs red and
7793             # go again at the next periodic interval, where the same thing may
7794             # or may not happen again. Depending on the previous and intended
7795             # shape of the providers/inventories, this may not actually cause
7796             # any immediately visible symptoms (in terms of scheduling, etc.)
7797             # If this becomes a problem, we may wish to make it pop immediately
7798             # (e.g. disable the service).
7799             with excutils.save_and_reraise_exception():
7800                 LOG.exception("ReshapeNeeded exception is unexpected here!")
7801         except Exception:
7802             LOG.exception("Error updating resources for node %(node)s.",
7803                           {'node': nodename})
7804 
7805     @periodic_task.periodic_task(spacing=CONF.update_resources_interval)
7806     def update_available_resource(self, context, startup=False):
7807         """See driver.get_available_resource()
7808 
7809         Periodic process that keeps that the compute host's understanding of
7810         resource availability and usage in sync with the underlying hypervisor.
7811 
7812         :param context: security context
7813         :param startup: True if this is being called when the nova-compute
7814             service is starting, False otherwise.
7815         """
7816 
7817         compute_nodes_in_db = self._get_compute_nodes_in_db(context,
7818                                                             use_slave=True,
7819                                                             startup=startup)
7820         try:
7821             nodenames = set(self.driver.get_available_nodes())
7822         except exception.VirtDriverNotReady:
7823             LOG.warning("Virt driver is not ready.")
7824             return
7825 
7826         rt = self._get_resource_tracker()
7827         # Delete orphan compute node not reported by driver but still in db
7828         for cn in compute_nodes_in_db:
7829             if cn.hypervisor_hostname not in nodenames:
7830                 LOG.info("Deleting orphan compute node %(id)s "
7831                          "hypervisor host is %(hh)s, "
7832                          "nodes are %(nodes)s",
7833                          {'id': cn.id, 'hh': cn.hypervisor_hostname,
7834                           'nodes': nodenames})
7835                 cn.destroy()
7836                 rt.remove_node(cn.hypervisor_hostname)
7837                 # Delete the corresponding resource provider in placement,
7838                 # along with any associated allocations and inventory.
7839                 # TODO(cdent): Move use of reportclient into resource tracker.
7840                 self.scheduler_client.reportclient.delete_resource_provider(
7841                     context, cn, cascade=True)
7842 
7843         for nodename in nodenames:
7844             self._update_available_resource_for_node(context, nodename,
7845                                                      startup=startup)
7846 
7847     def _get_compute_nodes_in_db(self, context, use_slave=False,
7848                                  startup=False):
7849         try:
7850             return objects.ComputeNodeList.get_all_by_host(context, self.host,
7851                                                            use_slave=use_slave)
7852         except exception.NotFound:
7853             if startup:
7854                 LOG.warning(
7855                     "No compute node record found for host %s. If this is "
7856                     "the first time this service is starting on this "
7857                     "host, then you can ignore this warning.", self.host)
7858             else:
7859                 LOG.error("No compute node record for host %s", self.host)
7860             return []
7861 
7862     @periodic_task.periodic_task(
7863         spacing=CONF.running_deleted_instance_poll_interval)
7864     def _cleanup_instances_stuck_in_deleting_task(self, context):
7865         """Try to clean up instances that be stuck in deleting task
7866         for long time.
7867         """
7868         LOG.debug('Cleaning up instances that be stuck in deleting task')
7869         filters = {
7870             'task_state': [task_states.DELETING, task_states.SOFT_DELETING],
7871             'host': self.host,
7872         }
7873         attrs = ['info_cache', 'metadata']
7874         instances_in_deleting = objects.InstanceList.get_by_filters(
7875             context, filters, expected_attrs=attrs, use_slave=True)
7876 
7877         for instance in instances_in_deleting:
7878             LOG.info('Deleting instance that may be stuck in deleting task',
7879                      instance=instance)
7880             self._delete_instance_stuck_in_deleting_task(context, instance)
7881 
7882     @periodic_task.periodic_task(
7883         spacing=CONF.running_deleted_instance_poll_interval)
7884     def _cleanup_running_deleted_instances(self, context):
7885         """Cleanup any instances which are erroneously still running after
7886         having been deleted.
7887 
7888         Valid actions to take are:
7889 
7890             1. noop - do nothing
7891             2. log - log which instances are erroneously running
7892             3. reap - shutdown and cleanup any erroneously running instances
7893             4. shutdown - power off *and disable* any erroneously running
7894                           instances
7895 
7896         The use-case for this cleanup task is: for various reasons, it may be
7897         possible for the database to show an instance as deleted but for that
7898         instance to still be running on a host machine (see bug
7899         https://bugs.launchpad.net/nova/+bug/911366).
7900 
7901         This cleanup task is a cross-hypervisor utility for finding these
7902         zombied instances and either logging the discrepancy (likely what you
7903         should do in production), or automatically reaping the instances (more
7904         appropriate for dev environments).
7905         """
7906         action = CONF.running_deleted_instance_action
7907 
7908         if action == "noop":
7909             return
7910 
7911         # NOTE(sirp): admin contexts don't ordinarily return deleted records
7912         with utils.temporary_mutation(context, read_deleted="yes"):
7913             for instance in self._running_deleted_instances(context):
7914                 if action == "log":
7915                     LOG.warning("Detected instance with name label "
7916                                 "'%s' which is marked as "
7917                                 "DELETED but still present on host.",
7918                                 instance.name, instance=instance)
7919 
7920                 elif action == 'shutdown':
7921                     LOG.info("Powering off instance with name label "
7922                              "'%s' which is marked as "
7923                              "DELETED but still present on host.",
7924                              instance.name, instance=instance)
7925                     try:
7926                         try:
7927                             # disable starting the instance
7928                             self.driver.set_bootable(instance, False)
7929                         except NotImplementedError:
7930                             LOG.debug("set_bootable is not implemented "
7931                                       "for the current driver")
7932                         # and power it off
7933                         self.driver.power_off(instance)
7934                     except Exception:
7935                         LOG.warning("Failed to power off instance",
7936                                     instance=instance, exc_info=True)
7937 
7938                 elif action == 'reap':
7939                     LOG.info("Destroying instance with name label "
7940                              "'%s' which is marked as "
7941                              "DELETED but still present on host.",
7942                              instance.name, instance=instance)
7943                     bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
7944                         context, instance.uuid, use_slave=True)
7945                     self.instance_events.clear_events_for_instance(instance)
7946                     try:
7947                         self._shutdown_instance(context, instance, bdms,
7948                                                 notify=False)
7949                         self._cleanup_volumes(context, instance, bdms,
7950                                               detach=False)
7951                     except Exception as e:
7952                         LOG.warning("Periodic cleanup failed to delete "
7953                                     "instance: %s",
7954                                     e, instance=instance)
7955                 else:
7956                     raise Exception(_("Unrecognized value '%s'"
7957                                       " for CONF.running_deleted_"
7958                                       "instance_action") % action)
7959 
7960     def _running_deleted_instances(self, context):
7961         """Returns a list of instances nova thinks is deleted,
7962         but the hypervisor thinks is still running.
7963         """
7964         timeout = CONF.running_deleted_instance_timeout
7965         filters = {'deleted': True,
7966                    'soft_deleted': False}
7967         instances = self._get_instances_on_driver(context, filters)
7968         return [i for i in instances if self._deleted_old_enough(i, timeout)]
7969 
7970     def _deleted_old_enough(self, instance, timeout):
7971         deleted_at = instance.deleted_at
7972         if deleted_at:
7973             deleted_at = deleted_at.replace(tzinfo=None)
7974         return (not deleted_at or timeutils.is_older_than(deleted_at, timeout))
7975 
7976     @contextlib.contextmanager
7977     def _error_out_instance_on_exception(self, context, instance,
7978                                          instance_state=vm_states.ACTIVE):
7979         instance_uuid = instance.uuid
7980         try:
7981             yield
7982         except NotImplementedError as error:
7983             with excutils.save_and_reraise_exception():
7984                 LOG.info("Setting instance back to %(state)s after: "
7985                          "%(error)s",
7986                          {'state': instance_state, 'error': error},
7987                          instance_uuid=instance_uuid)
7988                 self._instance_update(context, instance,
7989                                       vm_state=instance_state,
7990                                       task_state=None)
7991         except exception.InstanceFaultRollback as error:
7992             LOG.info("Setting instance back to ACTIVE after: %s",
7993                      error, instance_uuid=instance_uuid)
7994             self._instance_update(context, instance,
7995                                   vm_state=vm_states.ACTIVE,
7996                                   task_state=None)
7997             raise error.inner_exception
7998         except Exception:
7999             LOG.exception('Setting instance vm_state to ERROR',
8000                           instance_uuid=instance_uuid)
8001             with excutils.save_and_reraise_exception():
8002                 self._set_instance_obj_error_state(context, instance)
8003 
8004     @wrap_exception()
8005     def add_aggregate_host(self, context, aggregate, host, slave_info):
8006         """Notify hypervisor of change (for hypervisor pools)."""
8007         try:
8008             self.driver.add_to_aggregate(context, aggregate, host,
8009                                          slave_info=slave_info)
8010         except NotImplementedError:
8011             LOG.debug('Hypervisor driver does not support '
8012                       'add_aggregate_host')
8013         except exception.AggregateError:
8014             with excutils.save_and_reraise_exception():
8015                 self.driver.undo_aggregate_operation(
8016                                     context,
8017                                     aggregate.delete_host,
8018                                     aggregate, host)
8019 
8020     @wrap_exception()
8021     def remove_aggregate_host(self, context, host, slave_info, aggregate):
8022         """Removes a host from a physical hypervisor pool."""
8023         try:
8024             self.driver.remove_from_aggregate(context, aggregate, host,
8025                                               slave_info=slave_info)
8026         except NotImplementedError:
8027             LOG.debug('Hypervisor driver does not support '
8028                       'remove_aggregate_host')
8029         except (exception.AggregateError,
8030                 exception.InvalidAggregateAction) as e:
8031             with excutils.save_and_reraise_exception():
8032                 self.driver.undo_aggregate_operation(
8033                                     context,
8034                                     aggregate.add_host,
8035                                     aggregate, host,
8036                                     isinstance(e, exception.AggregateError))
8037 
8038     def _process_instance_event(self, instance, event):
8039         _event = self.instance_events.pop_instance_event(instance, event)
8040         if _event:
8041             LOG.debug('Processing event %(event)s',
8042                       {'event': event.key}, instance=instance)
8043             _event.send(event)
8044         else:
8045             # If it's a network-vif-unplugged event and the instance is being
8046             # deleted then we don't need to make this a warning as it's
8047             # expected. There are other things which could trigger this like
8048             # detaching an interface, but we don't have a task state for that.
8049             if (event.name == 'network-vif-unplugged' and
8050                     instance.task_state == task_states.DELETING):
8051                 LOG.debug('Received event %s for instance which is being '
8052                           'deleted.', event.key, instance=instance)
8053             else:
8054                 LOG.warning('Received unexpected event %(event)s for '
8055                             'instance with vm_state %(vm_state)s and '
8056                             'task_state %(task_state)s.',
8057                             {'event': event.key,
8058                              'vm_state': instance.vm_state,
8059                              'task_state': instance.task_state},
8060                             instance=instance)
8061 
8062     def _process_instance_vif_deleted_event(self, context, instance,
8063                                             deleted_vif_id):
8064         # If an attached port is deleted by neutron, it needs to
8065         # be detached from the instance.
8066         # And info cache needs to be updated.
8067         network_info = instance.info_cache.network_info
8068         for index, vif in enumerate(network_info):
8069             if vif['id'] == deleted_vif_id:
8070                 LOG.info('Neutron deleted interface %(intf)s; '
8071                          'detaching it from the instance and '
8072                          'deleting it from the info cache',
8073                          {'intf': vif['id']},
8074                          instance=instance)
8075                 del network_info[index]
8076                 base_net_api.update_instance_cache_with_nw_info(
8077                                  self.network_api, context,
8078                                  instance,
8079                                  nw_info=network_info)
8080                 try:
8081                     self.driver.detach_interface(context, instance, vif)
8082                 except NotImplementedError:
8083                     # Not all virt drivers support attach/detach of interfaces
8084                     # yet (like Ironic), so just ignore this.
8085                     pass
8086                 except exception.NovaException as ex:
8087                     # If the instance was deleted before the interface was
8088                     # detached, just log it at debug.
8089                     log_level = (logging.DEBUG
8090                                  if isinstance(ex, exception.InstanceNotFound)
8091                                  else logging.WARNING)
8092                     LOG.log(log_level,
8093                             "Detach interface failed, "
8094                             "port_id=%(port_id)s, reason: %(msg)s",
8095                             {'port_id': deleted_vif_id, 'msg': ex},
8096                             instance=instance)
8097                 break
8098 
8099     @wrap_instance_event(prefix='compute')
8100     @wrap_instance_fault
8101     def extend_volume(self, context, instance, extended_volume_id):
8102 
8103         # If an attached volume is extended by cinder, it needs to
8104         # be extended by virt driver so host can detect its new size.
8105         # And bdm needs to be updated.
8106         LOG.debug('Handling volume-extended event for volume %(vol)s',
8107                   {'vol': extended_volume_id}, instance=instance)
8108 
8109         try:
8110             bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
8111                    context, extended_volume_id, instance.uuid)
8112         except exception.NotFound:
8113             LOG.warning('Extend volume failed, '
8114                         'volume %(vol)s is not attached to instance.',
8115                         {'vol': extended_volume_id},
8116                         instance=instance)
8117             return
8118 
8119         LOG.info('Cinder extended volume %(vol)s; '
8120                  'extending it to detect new size',
8121                  {'vol': extended_volume_id},
8122                  instance=instance)
8123         volume = self.volume_api.get(context, bdm.volume_id)
8124 
8125         if bdm.connection_info is None:
8126             LOG.warning('Extend volume failed, '
8127                         'attached volume %(vol)s has no connection_info',
8128                         {'vol': extended_volume_id},
8129                         instance=instance)
8130             return
8131 
8132         connection_info = jsonutils.loads(bdm.connection_info)
8133         bdm.volume_size = volume['size']
8134         bdm.save()
8135 
8136         if not self.driver.capabilities.get('supports_extend_volume', False):
8137             raise exception.ExtendVolumeNotSupported()
8138 
8139         try:
8140             self.driver.extend_volume(connection_info,
8141                                       instance)
8142         except Exception as ex:
8143             LOG.warning('Extend volume failed, '
8144                         'volume_id=%(volume_id)s, reason: %(msg)s',
8145                         {'volume_id': extended_volume_id, 'msg': ex},
8146                         instance=instance)
8147             raise
8148 
8149     @wrap_exception()
8150     def external_instance_event(self, context, instances, events):
8151         # NOTE(danms): Some event types are handled by the manager, such
8152         # as when we're asked to update the instance's info_cache. If it's
8153         # not one of those, look for some thread(s) waiting for the event and
8154         # unblock them if so.
8155         for event in events:
8156             instance = [inst for inst in instances
8157                         if inst.uuid == event.instance_uuid][0]
8158             LOG.debug('Received event %(event)s',
8159                       {'event': event.key},
8160                       instance=instance)
8161             if event.name == 'network-changed':
8162                 try:
8163                     LOG.debug('Refreshing instance network info cache due to '
8164                               'event %s.', event.key, instance=instance)
8165                     self.network_api.get_instance_nw_info(
8166                         context, instance, refresh_vif_id=event.tag)
8167                 except exception.NotFound as e:
8168                     LOG.info('Failed to process external instance event '
8169                              '%(event)s due to: %(error)s',
8170                              {'event': event.key, 'error': six.text_type(e)},
8171                              instance=instance)
8172             elif event.name == 'network-vif-deleted':
8173                 try:
8174                     self._process_instance_vif_deleted_event(context,
8175                                                              instance,
8176                                                              event.tag)
8177                 except exception.NotFound as e:
8178                     LOG.info('Failed to process external instance event '
8179                              '%(event)s due to: %(error)s',
8180                              {'event': event.key, 'error': six.text_type(e)},
8181                              instance=instance)
8182             elif event.name == 'volume-extended':
8183                 self.extend_volume(context, instance, event.tag)
8184             else:
8185                 self._process_instance_event(instance, event)
8186 
8187     @periodic_task.periodic_task(spacing=CONF.image_cache_manager_interval,
8188                                  external_process_ok=True)
8189     def _run_image_cache_manager_pass(self, context):
8190         """Run a single pass of the image cache manager."""
8191 
8192         if not self.driver.capabilities.get("has_imagecache", False):
8193             return
8194 
8195         # Determine what other nodes use this storage
8196         storage_users.register_storage_use(CONF.instances_path, CONF.host)
8197         nodes = storage_users.get_storage_users(CONF.instances_path)
8198 
8199         # Filter all_instances to only include those nodes which share this
8200         # storage path.
8201         # TODO(mikal): this should be further refactored so that the cache
8202         # cleanup code doesn't know what those instances are, just a remote
8203         # count, and then this logic should be pushed up the stack.
8204         filters = {'deleted': False,
8205                    'soft_deleted': True,
8206                    'host': nodes}
8207         filtered_instances = objects.InstanceList.get_by_filters(context,
8208                                  filters, expected_attrs=[], use_slave=True)
8209 
8210         self.driver.manage_image_cache(context, filtered_instances)
8211 
8212     @periodic_task.periodic_task(spacing=CONF.instance_delete_interval)
8213     def _run_pending_deletes(self, context):
8214         """Retry any pending instance file deletes."""
8215         LOG.debug('Cleaning up deleted instances')
8216         filters = {'deleted': True,
8217                    'soft_deleted': False,
8218                    'host': CONF.host,
8219                    'cleaned': False}
8220         attrs = ['system_metadata']
8221         with utils.temporary_mutation(context, read_deleted='yes'):
8222             instances = objects.InstanceList.get_by_filters(
8223                 context, filters, expected_attrs=attrs, use_slave=True)
8224         LOG.debug('There are %d instances to clean', len(instances))
8225 
8226         # TODO(raj_singh): Remove this if condition when min value is
8227         # introduced to "maximum_instance_delete_attempts" cfg option.
8228         if CONF.maximum_instance_delete_attempts < 1:
8229             LOG.warning('Future versions of Nova will restrict the '
8230                         '"maximum_instance_delete_attempts" config option '
8231                         'to values >=1. Update your configuration file to '
8232                         'mitigate future upgrade issues.')
8233 
8234         for instance in instances:
8235             attempts = int(instance.system_metadata.get('clean_attempts', '0'))
8236             LOG.debug('Instance has had %(attempts)s of %(max)s '
8237                       'cleanup attempts',
8238                       {'attempts': attempts,
8239                        'max': CONF.maximum_instance_delete_attempts},
8240                       instance=instance)
8241             if attempts < CONF.maximum_instance_delete_attempts:
8242                 success = self.driver.delete_instance_files(instance)
8243 
8244                 instance.system_metadata['clean_attempts'] = str(attempts + 1)
8245                 if success:
8246                     instance.cleaned = True
8247                 with utils.temporary_mutation(context, read_deleted='yes'):
8248                     instance.save()
8249 
8250     @periodic_task.periodic_task(spacing=CONF.instance_delete_interval)
8251     def _cleanup_incomplete_migrations(self, context):
8252         """Delete instance files on failed resize/revert-resize operation
8253 
8254         During resize/revert-resize operation, if that instance gets deleted
8255         in-between then instance files might remain either on source or
8256         destination compute node because of race condition.
8257         """
8258         LOG.debug('Cleaning up deleted instances with incomplete migration ')
8259         migration_filters = {'host': CONF.host,
8260                              'status': 'error'}
8261         migrations = objects.MigrationList.get_by_filters(context,
8262                                                           migration_filters)
8263 
8264         if not migrations:
8265             return
8266 
8267         inst_uuid_from_migrations = set([migration.instance_uuid for migration
8268                                          in migrations])
8269 
8270         inst_filters = {'deleted': True, 'soft_deleted': False,
8271                         'uuid': inst_uuid_from_migrations}
8272         attrs = ['info_cache', 'security_groups', 'system_metadata']
8273         with utils.temporary_mutation(context, read_deleted='yes'):
8274             instances = objects.InstanceList.get_by_filters(
8275                 context, inst_filters, expected_attrs=attrs, use_slave=True)
8276 
8277         for instance in instances:
8278             if instance.host != CONF.host:
8279                 for migration in migrations:
8280                     if instance.uuid == migration.instance_uuid:
8281                         # Delete instance files if not cleanup properly either
8282                         # from the source or destination compute nodes when
8283                         # the instance is deleted during resizing.
8284                         self.driver.delete_instance_files(instance)
8285                         try:
8286                             migration.status = 'failed'
8287                             with migration.obj_as_admin():
8288                                 migration.save()
8289                         except exception.MigrationNotFound:
8290                             LOG.warning("Migration %s is not found.",
8291                                         migration.id,
8292                                         instance=instance)
8293                         break
8294 
8295     @messaging.expected_exceptions(exception.InstanceQuiesceNotSupported,
8296                                    exception.QemuGuestAgentNotEnabled,
8297                                    exception.NovaException,
8298                                    NotImplementedError)
8299     @wrap_exception()
8300     def quiesce_instance(self, context, instance):
8301         """Quiesce an instance on this host."""
8302         context = context.elevated()
8303         image_meta = objects.ImageMeta.from_instance(instance)
8304         self.driver.quiesce(context, instance, image_meta)
8305 
8306     def _wait_for_snapshots_completion(self, context, mapping):
8307         for mapping_dict in mapping:
8308             if mapping_dict.get('source_type') == 'snapshot':
8309 
8310                 def _wait_snapshot():
8311                     snapshot = self.volume_api.get_snapshot(
8312                         context, mapping_dict['snapshot_id'])
8313                     if snapshot.get('status') != 'creating':
8314                         raise loopingcall.LoopingCallDone()
8315 
8316                 timer = loopingcall.FixedIntervalLoopingCall(_wait_snapshot)
8317                 timer.start(interval=0.5).wait()
8318 
8319     @messaging.expected_exceptions(exception.InstanceQuiesceNotSupported,
8320                                    exception.QemuGuestAgentNotEnabled,
8321                                    exception.NovaException,
8322                                    NotImplementedError)
8323     @wrap_exception()
8324     def unquiesce_instance(self, context, instance, mapping=None):
8325         """Unquiesce an instance on this host.
8326 
8327         If snapshots' image mapping is provided, it waits until snapshots are
8328         completed before unqueiscing.
8329         """
8330         context = context.elevated()
8331         if mapping:
8332             try:
8333                 self._wait_for_snapshots_completion(context, mapping)
8334             except Exception as error:
8335                 LOG.exception("Exception while waiting completion of "
8336                               "volume snapshots: %s",
8337                               error, instance=instance)
8338         image_meta = objects.ImageMeta.from_instance(instance)
8339         self.driver.unquiesce(context, instance, image_meta)
8340 
8341     @periodic_task.periodic_task(spacing=CONF.instance_delete_interval)
8342     def _cleanup_expired_console_auth_tokens(self, context):
8343         """Remove expired console auth tokens for this host.
8344 
8345         Console authorization tokens and their connection data are stored
8346         in the database when a user asks for a console connection to an
8347         instance. After a time they expire. We periodically remove any expired
8348         tokens from the database.
8349         """
8350         # If the database backend isn't in use, don't bother looking for
8351         # expired tokens. The database backend is not supported for cells v1.
8352         if not CONF.cells.enable:
8353             objects.ConsoleAuthToken.\
8354                 clean_expired_console_auths_for_host(context, self.host)
