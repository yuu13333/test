Please review the code below for security defects. You can consider defect types in terms of:
1.CWE-284 (Improper Access Control)
2.CWE-435 (Improper Interaction Between Multiple Entities)
3.CWE-664 (Improper Control of a Resource Through its Lifetime)
4.CWE-682 (Incorrect Calculation)
5.CWE-691 (Insufficient Control Flow Management)
6.CWE-693 (Protection Mechanism Failure)
7.CWE-697 (Incorrect Comparison)
8.CWE-703 (Improper Check or Handling of Exceptional Conditions)
9.CWE-707 (Improper Neutralization)
10.CWE-710 (Improper Adherence to Coding Standards)
If any are found, please describe the security defect in detail and indicate the corresponding line number of code and solution. If none are detected, states: 'No security defects are detected in the code'.

1 # Copyright 2010 United States Government as represented by the
2 # Administrator of the National Aeronautics and Space Administration.
3 # Copyright 2011 Piston Cloud Computing, Inc.
4 # Copyright 2012-2013 Red Hat, Inc.
5 # All Rights Reserved.
6 #
7 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
8 #    not use this file except in compliance with the License. You may obtain
9 #    a copy of the License at
10 #
11 #         http://www.apache.org/licenses/LICENSE-2.0
12 #
13 #    Unless required by applicable law or agreed to in writing, software
14 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
15 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
16 #    License for the specific language governing permissions and limitations
17 #    under the License.
18 
19 """Handles all requests relating to compute resources (e.g. guest VMs,
20 networking and storage of VMs, and compute hosts on which they run)."""
21 
22 import collections
23 import copy
24 import functools
25 import re
26 import string
27 
28 from castellan import key_manager
29 from oslo_log import log as logging
30 from oslo_messaging import exceptions as oslo_exceptions
31 from oslo_serialization import base64 as base64utils
32 from oslo_serialization import jsonutils
33 from oslo_utils import excutils
34 from oslo_utils import strutils
35 from oslo_utils import timeutils
36 from oslo_utils import units
37 from oslo_utils import uuidutils
38 import six
39 from six.moves import range
40 
41 from nova import availability_zones
42 from nova import block_device
43 from nova.cells import opts as cells_opts
44 from nova.compute import flavors
45 from nova.compute import instance_actions
46 from nova.compute import instance_list
47 from nova.compute import power_state
48 from nova.compute import rpcapi as compute_rpcapi
49 from nova.compute import task_states
50 from nova.compute import utils as compute_utils
51 from nova.compute.utils import wrap_instance_event
52 from nova.compute import vm_states
53 from nova import conductor
54 import nova.conf
55 from nova.consoleauth import rpcapi as consoleauth_rpcapi
56 from nova import context as nova_context
57 from nova import crypto
58 from nova.db import base
59 from nova import exception
60 from nova import exception_wrapper
61 from nova import hooks
62 from nova.i18n import _
63 from nova import image
64 from nova import network
65 from nova.network import model as network_model
66 from nova.network.security_group import openstack_driver
67 from nova.network.security_group import security_group_base
68 from nova import objects
69 from nova.objects import base as obj_base
70 from nova.objects import block_device as block_device_obj
71 from nova.objects import fields as fields_obj
72 from nova.objects import keypair as keypair_obj
73 from nova.objects import quotas as quotas_obj
74 from nova.pci import request as pci_request
75 import nova.policy
76 from nova import profiler
77 from nova import rpc
78 from nova.scheduler import client as scheduler_client
79 from nova.scheduler import utils as scheduler_utils
80 from nova import servicegroup
81 from nova import utils
82 from nova.virt import hardware
83 from nova.volume import cinder
84 
85 LOG = logging.getLogger(__name__)
86 
87 get_notifier = functools.partial(rpc.get_notifier, service='compute')
88 # NOTE(gibi): legacy notification used compute as a service but these
89 # calls still run on the client side of the compute service which is
90 # nova-api. By setting the binary to nova-api below, we can make sure
91 # that the new versioned notifications has the right publisher_id but the
92 # legacy notifications does not change.
93 wrap_exception = functools.partial(exception_wrapper.wrap_exception,
94                                    get_notifier=get_notifier,
95                                    binary='nova-api')
96 CONF = nova.conf.CONF
97 
98 RO_SECURITY_GROUPS = ['default']
99 
100 AGGREGATE_ACTION_UPDATE = 'Update'
101 AGGREGATE_ACTION_UPDATE_META = 'UpdateMeta'
102 AGGREGATE_ACTION_DELETE = 'Delete'
103 AGGREGATE_ACTION_ADD = 'Add'
104 BFV_RESERVE_MIN_COMPUTE_VERSION = 17
105 CINDER_V3_ATTACH_MIN_COMPUTE_VERSION = 24
106 
107 # FIXME(danms): Keep a global cache of the cells we find the
108 # first time we look. This needs to be refreshed on a timer or
109 # trigger.
110 CELLS = []
111 
112 
113 def check_instance_state(vm_state=None, task_state=(None,),
114                          must_have_launched=True):
115     """Decorator to check VM and/or task state before entry to API functions.
116 
117     If the instance is in the wrong state, or has not been successfully
118     started at least once the wrapper will raise an exception.
119     """
120 
121     if vm_state is not None and not isinstance(vm_state, set):
122         vm_state = set(vm_state)
123     if task_state is not None and not isinstance(task_state, set):
124         task_state = set(task_state)
125 
126     def outer(f):
127         @six.wraps(f)
128         def inner(self, context, instance, *args, **kw):
129             if vm_state is not None and instance.vm_state not in vm_state:
130                 raise exception.InstanceInvalidState(
131                     attr='vm_state',
132                     instance_uuid=instance.uuid,
133                     state=instance.vm_state,
134                     method=f.__name__)
135             if (task_state is not None and
136                     instance.task_state not in task_state):
137                 raise exception.InstanceInvalidState(
138                     attr='task_state',
139                     instance_uuid=instance.uuid,
140                     state=instance.task_state,
141                     method=f.__name__)
142             if must_have_launched and not instance.launched_at:
143                 raise exception.InstanceInvalidState(
144                     attr='launched_at',
145                     instance_uuid=instance.uuid,
146                     state=instance.launched_at,
147                     method=f.__name__)
148 
149             return f(self, context, instance, *args, **kw)
150         return inner
151     return outer
152 
153 
154 def _set_or_none(q):
155     return q if q is None or isinstance(q, set) else set(q)
156 
157 
158 def reject_instance_state(vm_state=None, task_state=None):
159     """Decorator.  Raise InstanceInvalidState if instance is in any of the
160     given states.
161     """
162 
163     vm_state = _set_or_none(vm_state)
164     task_state = _set_or_none(task_state)
165 
166     def outer(f):
167         @six.wraps(f)
168         def inner(self, context, instance, *args, **kw):
169             _InstanceInvalidState = functools.partial(
170                 exception.InstanceInvalidState,
171                 instance_uuid=instance.uuid,
172                 method=f.__name__)
173 
174             if vm_state is not None and instance.vm_state in vm_state:
175                 raise _InstanceInvalidState(
176                     attr='vm_state', state=instance.vm_state)
177 
178             if task_state is not None and instance.task_state in task_state:
179                 raise _InstanceInvalidState(
180                     attr='task_state', state=instance.task_state)
181 
182             return f(self, context, instance, *args, **kw)
183         return inner
184     return outer
185 
186 
187 def check_instance_host(function):
188     @six.wraps(function)
189     def wrapped(self, context, instance, *args, **kwargs):
190         if not instance.host:
191             raise exception.InstanceNotReady(instance_id=instance.uuid)
192         return function(self, context, instance, *args, **kwargs)
193     return wrapped
194 
195 
196 def check_instance_lock(function):
197     @six.wraps(function)
198     def inner(self, context, instance, *args, **kwargs):
199         if instance.locked and not context.is_admin:
200             raise exception.InstanceIsLocked(instance_uuid=instance.uuid)
201         return function(self, context, instance, *args, **kwargs)
202     return inner
203 
204 
205 def check_instance_cell(fn):
206     @six.wraps(fn)
207     def _wrapped(self, context, instance, *args, **kwargs):
208         self._validate_cell(instance)
209         return fn(self, context, instance, *args, **kwargs)
210     return _wrapped
211 
212 
213 def _diff_dict(orig, new):
214     """Return a dict describing how to change orig to new.  The keys
215     correspond to values that have changed; the value will be a list
216     of one or two elements.  The first element of the list will be
217     either '+' or '-', indicating whether the key was updated or
218     deleted; if the key was updated, the list will contain a second
219     element, giving the updated value.
220     """
221     # Figure out what keys went away
222     result = {k: ['-'] for k in set(orig.keys()) - set(new.keys())}
223     # Compute the updates
224     for key, value in new.items():
225         if key not in orig or value != orig[key]:
226             result[key] = ['+', value]
227     return result
228 
229 
230 def load_cells():
231     global CELLS
232     if not CELLS:
233         CELLS = objects.CellMappingList.get_all(
234             nova_context.get_admin_context())
235         LOG.debug('Found %(count)i cells: %(cells)s',
236                   dict(count=len(CELLS),
237                        cells=','.join([c.identity for c in CELLS])))
238 
239     if not CELLS:
240         LOG.error('No cells are configured, unable to continue')
241 
242 
243 @profiler.trace_cls("compute_api")
244 class API(base.Base):
245     """API for interacting with the compute manager."""
246 
247     def __init__(self, image_api=None, network_api=None, volume_api=None,
248                  security_group_api=None, **kwargs):
249         self.image_api = image_api or image.API()
250         self.network_api = network_api or network.API()
251         self.volume_api = volume_api or cinder.API()
252         self.security_group_api = (security_group_api or
253             openstack_driver.get_openstack_security_group_driver())
254         self.consoleauth_rpcapi = consoleauth_rpcapi.ConsoleAuthAPI()
255         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
256         self.compute_task_api = conductor.ComputeTaskAPI()
257         self.servicegroup_api = servicegroup.API()
258         self.notifier = rpc.get_notifier('compute', CONF.host)
259         if CONF.ephemeral_storage_encryption.enabled:
260             self.key_manager = key_manager.API()
261 
262         super(API, self).__init__(**kwargs)
263 
264     @property
265     def cell_type(self):
266         try:
267             return getattr(self, '_cell_type')
268         except AttributeError:
269             self._cell_type = cells_opts.get_cell_type()
270             return self._cell_type
271 
272     def _validate_cell(self, instance):
273         if self.cell_type != 'api':
274             return
275         cell_name = instance.cell_name
276         if not cell_name:
277             raise exception.InstanceUnknownCell(
278                     instance_uuid=instance.uuid)
279 
280     def _record_action_start(self, context, instance, action):
281         objects.InstanceAction.action_start(context, instance.uuid,
282                                             action, want_result=False)
283 
284     def _check_injected_file_quota(self, context, injected_files):
285         """Enforce quota limits on injected files.
286 
287         Raises a QuotaError if any limit is exceeded.
288         """
289         if injected_files is None:
290             return
291 
292         # Check number of files first
293         try:
294             objects.Quotas.limit_check(context,
295                                        injected_files=len(injected_files))
296         except exception.OverQuota:
297             raise exception.OnsetFileLimitExceeded()
298 
299         # OK, now count path and content lengths; we're looking for
300         # the max...
301         max_path = 0
302         max_content = 0
303         for path, content in injected_files:
304             max_path = max(max_path, len(path))
305             max_content = max(max_content, len(content))
306 
307         try:
308             objects.Quotas.limit_check(context,
309                                        injected_file_path_bytes=max_path,
310                                        injected_file_content_bytes=max_content)
311         except exception.OverQuota as exc:
312             # Favor path limit over content limit for reporting
313             # purposes
314             if 'injected_file_path_bytes' in exc.kwargs['overs']:
315                 raise exception.OnsetFilePathLimitExceeded(
316                       allowed=exc.kwargs['quotas']['injected_file_path_bytes'])
317             else:
318                 raise exception.OnsetFileContentLimitExceeded(
319                    allowed=exc.kwargs['quotas']['injected_file_content_bytes'])
320 
321     def _check_metadata_properties_quota(self, context, metadata=None):
322         """Enforce quota limits on metadata properties."""
323         if not metadata:
324             metadata = {}
325         if not isinstance(metadata, dict):
326             msg = (_("Metadata type should be dict."))
327             raise exception.InvalidMetadata(reason=msg)
328         num_metadata = len(metadata)
329         try:
330             objects.Quotas.limit_check(context, metadata_items=num_metadata)
331         except exception.OverQuota as exc:
332             quota_metadata = exc.kwargs['quotas']['metadata_items']
333             raise exception.MetadataLimitExceeded(allowed=quota_metadata)
334 
335         # Because metadata is stored in the DB, we hard-code the size limits
336         # In future, we may support more variable length strings, so we act
337         #  as if this is quota-controlled for forwards compatibility.
338         # Those are only used in V2 API, from V2.1 API, those checks are
339         # validated at API layer schema validation.
340         for k, v in metadata.items():
341             try:
342                 utils.check_string_length(v)
343                 utils.check_string_length(k, min_length=1)
344             except exception.InvalidInput as e:
345                 raise exception.InvalidMetadata(reason=e.format_message())
346 
347             if len(k) > 255:
348                 msg = _("Metadata property key greater than 255 characters")
349                 raise exception.InvalidMetadataSize(reason=msg)
350             if len(v) > 255:
351                 msg = _("Metadata property value greater than 255 characters")
352                 raise exception.InvalidMetadataSize(reason=msg)
353 
354     def _check_requested_secgroups(self, context, secgroups):
355         """Check if the security group requested exists and belongs to
356         the project.
357 
358         :param context: The nova request context.
359         :type context: nova.context.RequestContext
360         :param secgroups: list of requested security group names, or uuids in
361             the case of Neutron.
362         :type secgroups: list
363         :returns: list of requested security group names unmodified if using
364             nova-network. If using Neutron, the list returned is all uuids.
365             Note that 'default' is a special case and will be unmodified if
366             it's requested.
367         """
368         security_groups = []
369         for secgroup in secgroups:
370             # NOTE(sdague): default is handled special
371             if secgroup == "default":
372                 security_groups.append(secgroup)
373                 continue
374             secgroup_dict = self.security_group_api.get(context, secgroup)
375             if not secgroup_dict:
376                 raise exception.SecurityGroupNotFoundForProject(
377                     project_id=context.project_id, security_group_id=secgroup)
378 
379             # Check to see if it's a nova-network or neutron type.
380             if isinstance(secgroup_dict['id'], int):
381                 # This is nova-network so just return the requested name.
382                 security_groups.append(secgroup)
383             else:
384                 # The id for neutron is a uuid, so we return the id (uuid).
385                 security_groups.append(secgroup_dict['id'])
386 
387         return security_groups
388 
389     def _check_requested_networks(self, context, requested_networks,
390                                   max_count):
391         """Check if the networks requested belongs to the project
392         and the fixed IP address for each network provided is within
393         same the network block
394         """
395         if requested_networks is not None:
396             if requested_networks.no_allocate:
397                 # If the network request was specifically 'none' meaning don't
398                 # allocate any networks, we just return the number of requested
399                 # instances since quotas don't change at all.
400                 return max_count
401 
402             # NOTE(danms): Temporary transition
403             requested_networks = requested_networks.as_tuples()
404 
405         return self.network_api.validate_networks(context, requested_networks,
406                                                   max_count)
407 
408     def _handle_kernel_and_ramdisk(self, context, kernel_id, ramdisk_id,
409                                    image):
410         """Choose kernel and ramdisk appropriate for the instance.
411 
412         The kernel and ramdisk can be chosen in one of two ways:
413 
414             1. Passed in with create-instance request.
415 
416             2. Inherited from image metadata.
417 
418         If inherited from image metadata, and if that image metadata value is
419         set to 'nokernel', both kernel and ramdisk will default to None.
420         """
421         # Inherit from image if not specified
422         image_properties = image.get('properties', {})
423 
424         if kernel_id is None:
425             kernel_id = image_properties.get('kernel_id')
426 
427         if ramdisk_id is None:
428             ramdisk_id = image_properties.get('ramdisk_id')
429 
430         # Force to None if kernel_id indicates that a kernel is not to be used
431         if kernel_id == 'nokernel':
432             kernel_id = None
433             ramdisk_id = None
434 
435         # Verify kernel and ramdisk exist (fail-fast)
436         if kernel_id is not None:
437             kernel_image = self.image_api.get(context, kernel_id)
438             # kernel_id could have been a URI, not a UUID, so to keep behaviour
439             # from before, which leaked that implementation detail out to the
440             # caller, we return the image UUID of the kernel image and ramdisk
441             # image (below) and not any image URIs that might have been
442             # supplied.
443             # TODO(jaypipes): Get rid of this silliness once we move to a real
444             # Image object and hide all of that stuff within nova.image.api.
445             kernel_id = kernel_image['id']
446 
447         if ramdisk_id is not None:
448             ramdisk_image = self.image_api.get(context, ramdisk_id)
449             ramdisk_id = ramdisk_image['id']
450 
451         return kernel_id, ramdisk_id
452 
453     @staticmethod
454     def parse_availability_zone(context, availability_zone):
455         # NOTE(vish): We have a legacy hack to allow admins to specify hosts
456         #             via az using az:host:node. It might be nice to expose an
457         #             api to specify specific hosts to force onto, but for
458         #             now it just supports this legacy hack.
459         # NOTE(deva): It is also possible to specify az::node, in which case
460         #             the host manager will determine the correct host.
461         forced_host = None
462         forced_node = None
463         if availability_zone and ':' in availability_zone:
464             c = availability_zone.count(':')
465             if c == 1:
466                 availability_zone, forced_host = availability_zone.split(':')
467             elif c == 2:
468                 if '::' in availability_zone:
469                     availability_zone, forced_node = \
470                             availability_zone.split('::')
471                 else:
472                     availability_zone, forced_host, forced_node = \
473                             availability_zone.split(':')
474             else:
475                 raise exception.InvalidInput(
476                         reason="Unable to parse availability_zone")
477 
478         if not availability_zone:
479             availability_zone = CONF.default_schedule_zone
480 
481         return availability_zone, forced_host, forced_node
482 
483     def _ensure_auto_disk_config_is_valid(self, auto_disk_config_img,
484                                           auto_disk_config, image):
485         auto_disk_config_disabled = \
486                 utils.is_auto_disk_config_disabled(auto_disk_config_img)
487         if auto_disk_config_disabled and auto_disk_config:
488             raise exception.AutoDiskConfigDisabledByImage(image=image)
489 
490     def _inherit_properties_from_image(self, image, auto_disk_config):
491         image_properties = image.get('properties', {})
492         auto_disk_config_img = \
493                 utils.get_auto_disk_config_from_image_props(image_properties)
494         self._ensure_auto_disk_config_is_valid(auto_disk_config_img,
495                                                auto_disk_config,
496                                                image.get("id"))
497         if auto_disk_config is None:
498             auto_disk_config = strutils.bool_from_string(auto_disk_config_img)
499 
500         return {
501             'os_type': image_properties.get('os_type'),
502             'architecture': image_properties.get('architecture'),
503             'vm_mode': image_properties.get('vm_mode'),
504             'auto_disk_config': auto_disk_config
505         }
506 
507     def _new_instance_name_from_template(self, uuid, display_name, index):
508         params = {
509             'uuid': uuid,
510             'name': display_name,
511             'count': index + 1,
512         }
513         try:
514             new_name = (CONF.multi_instance_display_name_template %
515                         params)
516         except (KeyError, TypeError):
517             LOG.exception('Failed to set instance name using '
518                           'multi_instance_display_name_template.')
519             new_name = display_name
520         return new_name
521 
522     def _apply_instance_name_template(self, context, instance, index):
523         original_name = instance.display_name
524         new_name = self._new_instance_name_from_template(instance.uuid,
525                 instance.display_name, index)
526         instance.display_name = new_name
527         if not instance.get('hostname', None):
528             if utils.sanitize_hostname(original_name) == "":
529                 instance.hostname = self._default_host_name(instance.uuid)
530             else:
531                 instance.hostname = utils.sanitize_hostname(new_name)
532         return instance
533 
534     def _check_config_drive(self, config_drive):
535         if config_drive:
536             try:
537                 bool_val = strutils.bool_from_string(config_drive,
538                                                      strict=True)
539             except ValueError:
540                 raise exception.ConfigDriveInvalidValue(option=config_drive)
541         else:
542             bool_val = False
543         # FIXME(comstud):  Bug ID 1193438 filed for this. This looks silly,
544         # but this is because the config drive column is a String.  False
545         # is represented by using an empty string.  And for whatever
546         # reason, we rely on the DB to cast True to a String.
547         return True if bool_val else ''
548 
549     def _check_requested_image(self, context, image_id, image,
550                                instance_type, root_bdm):
551         if not image:
552             return
553 
554         if image['status'] != 'active':
555             raise exception.ImageNotActive(image_id=image_id)
556 
557         image_properties = image.get('properties', {})
558         config_drive_option = image_properties.get(
559             'img_config_drive', 'optional')
560         if config_drive_option not in ['optional', 'mandatory']:
561             raise exception.InvalidImageConfigDrive(
562                 config_drive=config_drive_option)
563 
564         if instance_type['memory_mb'] < int(image.get('min_ram') or 0):
565             raise exception.FlavorMemoryTooSmall()
566 
567         # Image min_disk is in gb, size is in bytes. For sanity, have them both
568         # in bytes.
569         image_min_disk = int(image.get('min_disk') or 0) * units.Gi
570         image_size = int(image.get('size') or 0)
571 
572         # Target disk is a volume. Don't check flavor disk size because it
573         # doesn't make sense, and check min_disk against the volume size.
574         if (root_bdm is not None and root_bdm.is_volume):
575             # There are 2 possibilities here: either the target volume already
576             # exists, or it doesn't, in which case the bdm will contain the
577             # intended volume size.
578             #
579             # Cinder does its own check against min_disk, so if the target
580             # volume already exists this has already been done and we don't
581             # need to check it again here. In this case, volume_size may not be
582             # set on the bdm.
583             #
584             # If we're going to create the volume, the bdm will contain
585             # volume_size. Therefore we should check it if it exists. This will
586             # still be checked again by cinder when the volume is created, but
587             # that will not happen until the request reaches a host. By
588             # checking it here, the user gets an immediate and useful failure
589             # indication.
590             #
591             # The third possibility is that we have failed to consider
592             # something, and there are actually more than 2 possibilities. In
593             # this case cinder will still do the check at volume creation time.
594             # The behaviour will still be correct, but the user will not get an
595             # immediate failure from the api, and will instead have to
596             # determine why the instance is in an error state with a task of
597             # block_device_mapping.
598             #
599             # We could reasonably refactor this check into _validate_bdm at
600             # some future date, as the various size logic is already split out
601             # in there.
602             dest_size = root_bdm.volume_size
603             if dest_size is not None:
604                 dest_size *= units.Gi
605 
606                 if image_min_disk > dest_size:
607                     raise exception.VolumeSmallerThanMinDisk(
608                         volume_size=dest_size, image_min_disk=image_min_disk)
609 
610         # Target disk is a local disk whose size is taken from the flavor
611         else:
612             dest_size = instance_type['root_gb'] * units.Gi
613 
614             # NOTE(johannes): root_gb is allowed to be 0 for legacy reasons
615             # since libvirt interpreted the value differently than other
616             # drivers. A value of 0 means don't check size.
617             if dest_size != 0:
618                 if image_size > dest_size:
619                     raise exception.FlavorDiskSmallerThanImage(
620                         flavor_size=dest_size, image_size=image_size)
621 
622                 if image_min_disk > dest_size:
623                     raise exception.FlavorDiskSmallerThanMinDisk(
624                         flavor_size=dest_size, image_min_disk=image_min_disk)
625 
626     def _get_image_defined_bdms(self, instance_type, image_meta,
627                                 root_device_name):
628         image_properties = image_meta.get('properties', {})
629 
630         # Get the block device mappings defined by the image.
631         image_defined_bdms = image_properties.get('block_device_mapping', [])
632         legacy_image_defined = not image_properties.get('bdm_v2', False)
633 
634         image_mapping = image_properties.get('mappings', [])
635 
636         if legacy_image_defined:
637             image_defined_bdms = block_device.from_legacy_mapping(
638                 image_defined_bdms, None, root_device_name)
639         else:
640             image_defined_bdms = list(map(block_device.BlockDeviceDict,
641                                           image_defined_bdms))
642 
643         if image_mapping:
644             image_mapping = self._prepare_image_mapping(instance_type,
645                                                         image_mapping)
646             image_defined_bdms = self._merge_bdms_lists(
647                 image_mapping, image_defined_bdms)
648 
649         return image_defined_bdms
650 
651     def _get_flavor_defined_bdms(self, instance_type, block_device_mapping):
652         flavor_defined_bdms = []
653 
654         have_ephemeral_bdms = any(filter(
655             block_device.new_format_is_ephemeral, block_device_mapping))
656         have_swap_bdms = any(filter(
657             block_device.new_format_is_swap, block_device_mapping))
658 
659         if instance_type.get('ephemeral_gb') and not have_ephemeral_bdms:
660             flavor_defined_bdms.append(
661                 block_device.create_blank_bdm(instance_type['ephemeral_gb']))
662         if instance_type.get('swap') and not have_swap_bdms:
663             flavor_defined_bdms.append(
664                 block_device.create_blank_bdm(instance_type['swap'], 'swap'))
665 
666         return flavor_defined_bdms
667 
668     def _merge_bdms_lists(self, overridable_mappings, overrider_mappings):
669         """Override any block devices from the first list by device name
670 
671         :param overridable_mappings: list which items are overridden
672         :param overrider_mappings: list which items override
673 
674         :returns: A merged list of bdms
675         """
676         device_names = set(bdm['device_name'] for bdm in overrider_mappings
677                            if bdm['device_name'])
678         return (overrider_mappings +
679                 [bdm for bdm in overridable_mappings
680                  if bdm['device_name'] not in device_names])
681 
682     def _check_and_transform_bdm(self, context, base_options, instance_type,
683                                  image_meta, min_count, max_count,
684                                  block_device_mapping, legacy_bdm):
685         # NOTE (ndipanov): Assume root dev name is 'vda' if not supplied.
686         #                  It's needed for legacy conversion to work.
687         root_device_name = (base_options.get('root_device_name') or 'vda')
688         image_ref = base_options.get('image_ref', '')
689         # If the instance is booted by image and has a volume attached,
690         # the volume cannot have the same device name as root_device_name
691         if image_ref:
692             for bdm in block_device_mapping:
693                 if (bdm.get('destination_type') == 'volume' and
694                     block_device.strip_dev(bdm.get(
695                     'device_name')) == root_device_name):
696                     msg = _('The volume cannot be assigned the same device'
697                             ' name as the root device %s') % root_device_name
698                     raise exception.InvalidRequest(msg)
699 
700         image_defined_bdms = self._get_image_defined_bdms(
701             instance_type, image_meta, root_device_name)
702         root_in_image_bdms = (
703             block_device.get_root_bdm(image_defined_bdms) is not None)
704 
705         if legacy_bdm:
706             block_device_mapping = block_device.from_legacy_mapping(
707                 block_device_mapping, image_ref, root_device_name,
708                 no_root=root_in_image_bdms)
709         elif root_in_image_bdms:
710             # NOTE (ndipanov): client will insert an image mapping into the v2
711             # block_device_mapping, but if there is a bootable device in image
712             # mappings - we need to get rid of the inserted image
713             # NOTE (gibi): another case is when a server is booted with an
714             # image to bdm mapping where the image only contains a bdm to a
715             # snapshot. In this case the other image to bdm mapping
716             # contains an unnecessary device with boot_index == 0.
717             # Also in this case the image_ref is None as we are booting from
718             # an image to volume bdm.
719             def not_image_and_root_bdm(bdm):
720                 return not (bdm.get('boot_index') == 0 and
721                             bdm.get('source_type') == 'image')
722 
723             block_device_mapping = list(
724                 filter(not_image_and_root_bdm, block_device_mapping))
725 
726         block_device_mapping = self._merge_bdms_lists(
727             image_defined_bdms, block_device_mapping)
728 
729         if min_count > 1 or max_count > 1:
730             if any(map(lambda bdm: bdm['source_type'] == 'volume',
731                        block_device_mapping)):
732                 msg = _('Cannot attach one or more volumes to multiple'
733                         ' instances')
734                 raise exception.InvalidRequest(msg)
735 
736         block_device_mapping += self._get_flavor_defined_bdms(
737             instance_type, block_device_mapping)
738 
739         return block_device_obj.block_device_make_list_from_dicts(
740                 context, block_device_mapping)
741 
742     def _get_image(self, context, image_href):
743         if not image_href:
744             return None, {}
745 
746         image = self.image_api.get(context, image_href)
747         return image['id'], image
748 
749     def _checks_for_create_and_rebuild(self, context, image_id, image,
750                                        instance_type, metadata,
751                                        files_to_inject, root_bdm):
752         self._check_metadata_properties_quota(context, metadata)
753         self._check_injected_file_quota(context, files_to_inject)
754         self._check_requested_image(context, image_id, image,
755                                     instance_type, root_bdm)
756 
757     def _validate_and_build_base_options(self, context, instance_type,
758                                          boot_meta, image_href, image_id,
759                                          kernel_id, ramdisk_id, display_name,
760                                          display_description, key_name,
761                                          key_data, security_groups,
762                                          availability_zone, user_data,
763                                          metadata, access_ip_v4, access_ip_v6,
764                                          requested_networks, config_drive,
765                                          auto_disk_config, reservation_id,
766                                          max_count):
767         """Verify all the input parameters regardless of the provisioning
768         strategy being performed.
769         """
770         if instance_type['disabled']:
771             raise exception.FlavorNotFound(flavor_id=instance_type['id'])
772 
773         if user_data:
774             try:
775                 base64utils.decode_as_bytes(user_data)
776             except TypeError:
777                 raise exception.InstanceUserDataMalformed()
778 
779         # When using Neutron, _check_requested_secgroups will translate and
780         # return any requested security group names to uuids.
781         security_groups = (
782             self._check_requested_secgroups(context, security_groups))
783 
784         # Note:  max_count is the number of instances requested by the user,
785         # max_network_count is the maximum number of instances taking into
786         # account any network quotas
787         max_network_count = self._check_requested_networks(context,
788                                      requested_networks, max_count)
789 
790         kernel_id, ramdisk_id = self._handle_kernel_and_ramdisk(
791                 context, kernel_id, ramdisk_id, boot_meta)
792 
793         config_drive = self._check_config_drive(config_drive)
794 
795         if key_data is None and key_name is not None:
796             key_pair = objects.KeyPair.get_by_name(context,
797                                                    context.user_id,
798                                                    key_name)
799             key_data = key_pair.public_key
800         else:
801             key_pair = None
802 
803         root_device_name = block_device.prepend_dev(
804                 block_device.properties_root_device_name(
805                     boot_meta.get('properties', {})))
806 
807         try:
808             image_meta = objects.ImageMeta.from_dict(boot_meta)
809         except ValueError as e:
810             # there must be invalid values in the image meta properties so
811             # consider this an invalid request
812             msg = _('Invalid image metadata. Error: %s') % six.text_type(e)
813             raise exception.InvalidRequest(msg)
814         numa_topology = hardware.numa_get_constraints(
815                 instance_type, image_meta)
816 
817         system_metadata = {}
818 
819         # PCI requests come from two sources: instance flavor and
820         # requested_networks. The first call in below returns an
821         # InstancePCIRequests object which is a list of InstancePCIRequest
822         # objects. The second call in below creates an InstancePCIRequest
823         # object for each SR-IOV port, and append it to the list in the
824         # InstancePCIRequests object
825         pci_request_info = pci_request.get_pci_requests_from_flavor(
826             instance_type)
827         self.network_api.create_pci_requests_for_sriov_ports(context,
828             pci_request_info, requested_networks)
829 
830         base_options = {
831             'reservation_id': reservation_id,
832             'image_ref': image_href,
833             'kernel_id': kernel_id or '',
834             'ramdisk_id': ramdisk_id or '',
835             'power_state': power_state.NOSTATE,
836             'vm_state': vm_states.BUILDING,
837             'config_drive': config_drive,
838             'user_id': context.user_id,
839             'project_id': context.project_id,
840             'instance_type_id': instance_type['id'],
841             'memory_mb': instance_type['memory_mb'],
842             'vcpus': instance_type['vcpus'],
843             'root_gb': instance_type['root_gb'],
844             'ephemeral_gb': instance_type['ephemeral_gb'],
845             'display_name': display_name,
846             'display_description': display_description,
847             'user_data': user_data,
848             'key_name': key_name,
849             'key_data': key_data,
850             'locked': False,
851             'metadata': metadata or {},
852             'access_ip_v4': access_ip_v4,
853             'access_ip_v6': access_ip_v6,
854             'availability_zone': availability_zone,
855             'root_device_name': root_device_name,
856             'progress': 0,
857             'pci_requests': pci_request_info,
858             'numa_topology': numa_topology,
859             'system_metadata': system_metadata}
860 
861         options_from_image = self._inherit_properties_from_image(
862                 boot_meta, auto_disk_config)
863 
864         base_options.update(options_from_image)
865 
866         # return the validated options and maximum number of instances allowed
867         # by the network quotas
868         return base_options, max_network_count, key_pair, security_groups
869 
870     def _provision_instances(self, context, instance_type, min_count,
871             max_count, base_options, boot_meta, security_groups,
872             block_device_mapping, shutdown_terminate,
873             instance_group, check_server_group_quota, filter_properties,
874             key_pair, tags):
875         # Check quotas
876         num_instances = compute_utils.check_num_instances_quota(
877                 context, instance_type, min_count, max_count)
878         security_groups = self.security_group_api.populate_security_groups(
879                 security_groups)
880         self.security_group_api.ensure_default(context)
881         LOG.debug("Going to run %s instances...", num_instances)
882         instances_to_build = []
883         try:
884             for i in range(num_instances):
885                 # Create a uuid for the instance so we can store the
886                 # RequestSpec before the instance is created.
887                 instance_uuid = uuidutils.generate_uuid()
888                 # Store the RequestSpec that will be used for scheduling.
889                 req_spec = objects.RequestSpec.from_components(context,
890                         instance_uuid, boot_meta, instance_type,
891                         base_options['numa_topology'],
892                         base_options['pci_requests'], filter_properties,
893                         instance_group, base_options['availability_zone'],
894                         security_groups=security_groups)
895                 # NOTE(danms): We need to record num_instances on the request
896                 # spec as this is how the conductor knows how many were in this
897                 # batch.
898                 req_spec.num_instances = num_instances
899                 req_spec.create()
900 
901                 # Create an instance object, but do not store in db yet.
902                 instance = objects.Instance(context=context)
903                 instance.uuid = instance_uuid
904                 instance.update(base_options)
905                 instance.keypairs = objects.KeyPairList(objects=[])
906                 if key_pair:
907                     instance.keypairs.objects.append(key_pair)
908                 instance = self.create_db_entry_for_new_instance(context,
909                         instance_type, boot_meta, instance, security_groups,
910                         block_device_mapping, num_instances, i,
911                         shutdown_terminate, create_instance=False)
912                 block_device_mapping = (
913                     self._bdm_validate_set_size_and_instance(context,
914                         instance, instance_type, block_device_mapping))
915                 instance_tags = self._transform_tags(tags, instance.uuid)
916 
917                 build_request = objects.BuildRequest(context,
918                         instance=instance, instance_uuid=instance.uuid,
919                         project_id=instance.project_id,
920                         block_device_mappings=block_device_mapping,
921                         tags=instance_tags)
922                 build_request.create()
923 
924                 # Create an instance_mapping.  The null cell_mapping indicates
925                 # that the instance doesn't yet exist in a cell, and lookups
926                 # for it need to instead look for the RequestSpec.
927                 # cell_mapping will be populated after scheduling, with a
928                 # scheduling failure using the cell_mapping for the special
929                 # cell0.
930                 inst_mapping = objects.InstanceMapping(context=context)
931                 inst_mapping.instance_uuid = instance_uuid
932                 inst_mapping.project_id = context.project_id
933                 inst_mapping.cell_mapping = None
934                 inst_mapping.create()
935 
936                 instances_to_build.append(
937                     (req_spec, build_request, inst_mapping))
938 
939                 if instance_group:
940                     if check_server_group_quota:
941                         try:
942                             objects.Quotas.check_deltas(
943                                 context, {'server_group_members': 1},
944                                 instance_group, context.user_id)
945                         except exception.OverQuota:
946                             msg = _("Quota exceeded, too many servers in "
947                                     "group")
948                             raise exception.QuotaError(msg)
949 
950                     members = objects.InstanceGroup.add_members(
951                         context, instance_group.uuid, [instance.uuid])
952 
953                     # NOTE(melwitt): We recheck the quota after creating the
954                     # object to prevent users from allocating more resources
955                     # than their allowed quota in the event of a race. This is
956                     # configurable because it can be expensive if strict quota
957                     # limits are not required in a deployment.
958                     if CONF.quota.recheck_quota and check_server_group_quota:
959                         try:
960                             objects.Quotas.check_deltas(
961                                 context, {'server_group_members': 0},
962                                 instance_group, context.user_id)
963                         except exception.OverQuota:
964                             objects.InstanceGroup._remove_members_in_db(
965                                 context, instance_group.id, [instance.uuid])
966                             msg = _("Quota exceeded, too many servers in "
967                                     "group")
968                             raise exception.QuotaError(msg)
969                     # list of members added to servers group in this iteration
970                     # is needed to check quota of server group during add next
971                     # instance
972                     instance_group.members.extend(members)
973 
974         # In the case of any exceptions, attempt DB cleanup
975         except Exception:
976             with excutils.save_and_reraise_exception():
977                 self._cleanup_build_artifacts(None, instances_to_build)
978 
979         return instances_to_build
980 
981     def _get_bdm_image_metadata(self, context, block_device_mapping,
982                                 legacy_bdm=True):
983         """If we are booting from a volume, we need to get the
984         volume details from Cinder and make sure we pass the
985         metadata back accordingly.
986         """
987         if not block_device_mapping:
988             return {}
989 
990         for bdm in block_device_mapping:
991             if (legacy_bdm and
992                     block_device.get_device_letter(
993                        bdm.get('device_name', '')) != 'a'):
994                 continue
995             elif not legacy_bdm and bdm.get('boot_index') != 0:
996                 continue
997 
998             volume_id = bdm.get('volume_id')
999             snapshot_id = bdm.get('snapshot_id')
1000             if snapshot_id:
1001                 # NOTE(alaski): A volume snapshot inherits metadata from the
1002                 # originating volume, but the API does not expose metadata
1003                 # on the snapshot itself.  So we query the volume for it below.
1004                 snapshot = self.volume_api.get_snapshot(context, snapshot_id)
1005                 volume_id = snapshot['volume_id']
1006 
1007             if bdm.get('image_id'):
1008                 try:
1009                     image_id = bdm['image_id']
1010                     image_meta = self.image_api.get(context, image_id)
1011                     return image_meta
1012                 except Exception:
1013                     raise exception.InvalidBDMImage(id=image_id)
1014             elif volume_id:
1015                 try:
1016                     volume = self.volume_api.get(context, volume_id)
1017                 except exception.CinderConnectionFailed:
1018                     raise
1019                 except Exception as exc:
1020                     raise exception.InvalidBDMVolume(id=volume_id,
1021                         reason=exc.format_message())
1022 
1023                 if not volume.get('bootable', True):
1024                     raise exception.InvalidBDMVolumeNotBootable(id=volume_id)
1025 
1026                 return utils.get_image_metadata_from_volume(volume)
1027         return {}
1028 
1029     @staticmethod
1030     def _get_requested_instance_group(context, filter_properties):
1031         if (not filter_properties or
1032                 not filter_properties.get('scheduler_hints')):
1033             return
1034 
1035         group_hint = filter_properties.get('scheduler_hints').get('group')
1036         if not group_hint:
1037             return
1038 
1039         return objects.InstanceGroup.get_by_uuid(context, group_hint)
1040 
1041     def _create_instance(self, context, instance_type,
1042                image_href, kernel_id, ramdisk_id,
1043                min_count, max_count,
1044                display_name, display_description,
1045                key_name, key_data, security_groups,
1046                availability_zone, user_data, metadata, injected_files,
1047                admin_password, access_ip_v4, access_ip_v6,
1048                requested_networks, config_drive,
1049                block_device_mapping, auto_disk_config, filter_properties,
1050                reservation_id=None, legacy_bdm=True, shutdown_terminate=False,
1051                check_server_group_quota=False, tags=None):
1052         """Verify all the input parameters regardless of the provisioning
1053         strategy being performed and schedule the instance(s) for
1054         creation.
1055         """
1056 
1057         # Normalize and setup some parameters
1058         if reservation_id is None:
1059             reservation_id = utils.generate_uid('r')
1060         security_groups = security_groups or ['default']
1061         min_count = min_count or 1
1062         max_count = max_count or min_count
1063         block_device_mapping = block_device_mapping or []
1064         tags = tags or []
1065 
1066         if image_href:
1067             image_id, boot_meta = self._get_image(context, image_href)
1068         else:
1069             image_id = None
1070             boot_meta = self._get_bdm_image_metadata(
1071                 context, block_device_mapping, legacy_bdm)
1072 
1073         self._check_auto_disk_config(image=boot_meta,
1074                                      auto_disk_config=auto_disk_config)
1075 
1076         base_options, max_net_count, key_pair, security_groups = \
1077                 self._validate_and_build_base_options(
1078                     context, instance_type, boot_meta, image_href, image_id,
1079                     kernel_id, ramdisk_id, display_name, display_description,
1080                     key_name, key_data, security_groups, availability_zone,
1081                     user_data, metadata, access_ip_v4, access_ip_v6,
1082                     requested_networks, config_drive, auto_disk_config,
1083                     reservation_id, max_count)
1084 
1085         # max_net_count is the maximum number of instances requested by the
1086         # user adjusted for any network quota constraints, including
1087         # consideration of connections to each requested network
1088         if max_net_count < min_count:
1089             raise exception.PortLimitExceeded()
1090         elif max_net_count < max_count:
1091             LOG.info("max count reduced from %(max_count)d to "
1092                      "%(max_net_count)d due to network port quota",
1093                      {'max_count': max_count,
1094                       'max_net_count': max_net_count})
1095             max_count = max_net_count
1096 
1097         block_device_mapping = self._check_and_transform_bdm(context,
1098             base_options, instance_type, boot_meta, min_count, max_count,
1099             block_device_mapping, legacy_bdm)
1100 
1101         # We can't do this check earlier because we need bdms from all sources
1102         # to have been merged in order to get the root bdm.
1103         self._checks_for_create_and_rebuild(context, image_id, boot_meta,
1104                 instance_type, metadata, injected_files,
1105                 block_device_mapping.root_bdm())
1106 
1107         instance_group = self._get_requested_instance_group(context,
1108                                    filter_properties)
1109 
1110         tags = self._create_tag_list_obj(context, tags)
1111 
1112         instances_to_build = self._provision_instances(
1113             context, instance_type, min_count, max_count, base_options,
1114             boot_meta, security_groups, block_device_mapping,
1115             shutdown_terminate, instance_group, check_server_group_quota,
1116             filter_properties, key_pair, tags)
1117 
1118         instances = []
1119         request_specs = []
1120         build_requests = []
1121         for rs, build_request, im in instances_to_build:
1122             build_requests.append(build_request)
1123             instance = build_request.get_new_instance(context)
1124             instances.append(instance)
1125             request_specs.append(rs)
1126 
1127         if CONF.cells.enable:
1128             # NOTE(danms): CellsV1 can't do the new thing, so we
1129             # do the old thing here. We can remove this path once
1130             # we stop supporting v1.
1131             for instance in instances:
1132                 instance.create()
1133             # NOTE(melwitt): We recheck the quota after creating the objects
1134             # to prevent users from allocating more resources than their
1135             # allowed quota in the event of a race. This is configurable
1136             # because it can be expensive if strict quota limits are not
1137             # required in a deployment.
1138             if CONF.quota.recheck_quota:
1139                 try:
1140                     compute_utils.check_num_instances_quota(
1141                         context, instance_type, 0, 0,
1142                         orig_num_req=len(instances))
1143                 except exception.TooManyInstances:
1144                     with excutils.save_and_reraise_exception():
1145                         # Need to clean up all the instances we created
1146                         # along with the build requests, request specs,
1147                         # and instance mappings.
1148                         self._cleanup_build_artifacts(instances,
1149                                                       instances_to_build)
1150 
1151             self.compute_task_api.build_instances(context,
1152                 instances=instances, image=boot_meta,
1153                 filter_properties=filter_properties,
1154                 admin_password=admin_password,
1155                 injected_files=injected_files,
1156                 requested_networks=requested_networks,
1157                 security_groups=security_groups,
1158                 block_device_mapping=block_device_mapping,
1159                 legacy_bdm=False)
1160         else:
1161             self.compute_task_api.schedule_and_build_instances(
1162                 context,
1163                 build_requests=build_requests,
1164                 request_spec=request_specs,
1165                 image=boot_meta,
1166                 admin_password=admin_password,
1167                 injected_files=injected_files,
1168                 requested_networks=requested_networks,
1169                 block_device_mapping=block_device_mapping,
1170                 tags=tags)
1171 
1172         return instances, reservation_id
1173 
1174     @staticmethod
1175     def _cleanup_build_artifacts(instances, instances_to_build):
1176         # instances_to_build is a list of tuples:
1177         # (RequestSpec, BuildRequest, InstanceMapping)
1178 
1179         # Be paranoid about artifacts being deleted underneath us.
1180         for instance in instances or []:
1181             try:
1182                 instance.destroy()
1183             except exception.InstanceNotFound:
1184                 pass
1185         for rs, build_request, im in instances_to_build or []:
1186             try:
1187                 rs.destroy()
1188             except exception.RequestSpecNotFound:
1189                 pass
1190             try:
1191                 build_request.destroy()
1192             except exception.BuildRequestNotFound:
1193                 pass
1194             try:
1195                 im.destroy()
1196             except exception.InstanceMappingNotFound:
1197                 pass
1198 
1199     @staticmethod
1200     def _volume_size(instance_type, bdm):
1201         size = bdm.get('volume_size')
1202         # NOTE (ndipanov): inherit flavor size only for swap and ephemeral
1203         if (size is None and bdm.get('source_type') == 'blank' and
1204                 bdm.get('destination_type') == 'local'):
1205             if bdm.get('guest_format') == 'swap':
1206                 size = instance_type.get('swap', 0)
1207             else:
1208                 size = instance_type.get('ephemeral_gb', 0)
1209         return size
1210 
1211     def _prepare_image_mapping(self, instance_type, mappings):
1212         """Extract and format blank devices from image mappings."""
1213 
1214         prepared_mappings = []
1215 
1216         for bdm in block_device.mappings_prepend_dev(mappings):
1217             LOG.debug("Image bdm %s", bdm)
1218 
1219             virtual_name = bdm['virtual']
1220             if virtual_name == 'ami' or virtual_name == 'root':
1221                 continue
1222 
1223             if not block_device.is_swap_or_ephemeral(virtual_name):
1224                 continue
1225 
1226             guest_format = bdm.get('guest_format')
1227             if virtual_name == 'swap':
1228                 guest_format = 'swap'
1229             if not guest_format:
1230                 guest_format = CONF.default_ephemeral_format
1231 
1232             values = block_device.BlockDeviceDict({
1233                 'device_name': bdm['device'],
1234                 'source_type': 'blank',
1235                 'destination_type': 'local',
1236                 'device_type': 'disk',
1237                 'guest_format': guest_format,
1238                 'delete_on_termination': True,
1239                 'boot_index': -1})
1240 
1241             values['volume_size'] = self._volume_size(
1242                 instance_type, values)
1243             if values['volume_size'] == 0:
1244                 continue
1245 
1246             prepared_mappings.append(values)
1247 
1248         return prepared_mappings
1249 
1250     def _bdm_validate_set_size_and_instance(self, context, instance,
1251                                             instance_type,
1252                                             block_device_mapping):
1253         """Ensure the bdms are valid, then set size and associate with instance
1254 
1255         Because this method can be called multiple times when more than one
1256         instance is booted in a single request it makes a copy of the bdm list.
1257         """
1258         LOG.debug("block_device_mapping %s", list(block_device_mapping),
1259                   instance_uuid=instance.uuid)
1260         self._validate_bdm(
1261             context, instance, instance_type, block_device_mapping)
1262         instance_block_device_mapping = block_device_mapping.obj_clone()
1263         for bdm in instance_block_device_mapping:
1264             bdm.volume_size = self._volume_size(instance_type, bdm)
1265             bdm.instance_uuid = instance.uuid
1266         return instance_block_device_mapping
1267 
1268     def _create_block_device_mapping(self, block_device_mapping):
1269         # Copy the block_device_mapping because this method can be called
1270         # multiple times when more than one instance is booted in a single
1271         # request. This avoids 'id' being set and triggering the object dupe
1272         # detection
1273         db_block_device_mapping = copy.deepcopy(block_device_mapping)
1274         # Create the BlockDeviceMapping objects in the db.
1275         for bdm in db_block_device_mapping:
1276             # TODO(alaski): Why is this done?
1277             if bdm.volume_size == 0:
1278                 continue
1279 
1280             bdm.update_or_create()
1281 
1282     def _validate_bdm(self, context, instance, instance_type,
1283                       block_device_mappings):
1284         def _subsequent_list(l):
1285             # Each device which is capable of being used as boot device should
1286             # be given a unique boot index, starting from 0 in ascending order.
1287             return all(el + 1 == l[i + 1] for i, el in enumerate(l[:-1]))
1288 
1289         # Make sure that the boot indexes make sense.
1290         # Setting a negative value or None indicates that the device should not
1291         # be used for booting.
1292         boot_indexes = sorted([bdm.boot_index
1293                                for bdm in block_device_mappings
1294                                if bdm.boot_index is not None
1295                                and bdm.boot_index >= 0])
1296 
1297         if 0 not in boot_indexes or not _subsequent_list(boot_indexes):
1298             # Convert the BlockDeviceMappingList to a list for repr details.
1299             LOG.debug('Invalid block device mapping boot sequence for '
1300                       'instance: %s', list(block_device_mappings),
1301                       instance=instance)
1302             raise exception.InvalidBDMBootSequence()
1303 
1304         for bdm in block_device_mappings:
1305             # NOTE(vish): For now, just make sure the volumes are accessible.
1306             # Additionally, check that the volume can be attached to this
1307             # instance.
1308             snapshot_id = bdm.snapshot_id
1309             volume_id = bdm.volume_id
1310             image_id = bdm.image_id
1311             if (image_id is not None and
1312                     image_id != instance.get('image_ref')):
1313                 try:
1314                     self._get_image(context, image_id)
1315                 except Exception:
1316                     raise exception.InvalidBDMImage(id=image_id)
1317                 if (bdm.source_type == 'image' and
1318                         bdm.destination_type == 'volume' and
1319                         not bdm.volume_size):
1320                     raise exception.InvalidBDM(message=_("Images with "
1321                         "destination_type 'volume' need to have a non-zero "
1322                         "size specified"))
1323             elif volume_id is not None:
1324                 min_compute_version = objects.Service.get_minimum_version(
1325                     context, 'nova-compute')
1326                 try:
1327                     # NOTE(ildikov): The boot from volume operation did not
1328                     # reserve the volume before Pike and as the older computes
1329                     # are running 'check_attach' which will fail if the volume
1330                     # is in 'attaching' state; if the compute service version
1331                     # is not high enough we will just perform the old check as
1332                     # opposed to reserving the volume here.
1333                     if (min_compute_version >=
1334                         BFV_RESERVE_MIN_COMPUTE_VERSION):
1335                         volume = self._check_attach_and_reserve_volume(
1336                             context, volume_id, instance, bdm)
1337                     else:
1338                         # NOTE(ildikov): This call is here only for backward
1339                         # compatibility can be removed after Ocata EOL.
1340                         volume = self._check_attach(context, volume_id,
1341                                                     instance)
1342                     bdm.volume_size = volume.get('size')
1343                 except (exception.CinderConnectionFailed,
1344                         exception.InvalidVolume):
1345                     raise
1346                 except exception.InvalidInput as exc:
1347                     raise exception.InvalidVolume(reason=exc.format_message())
1348                 except Exception as exc:
1349                     raise exception.InvalidBDMVolume(id=volume_id,
1350                         reason=exc.format_message())
1351             elif snapshot_id is not None:
1352                 try:
1353                     snap = self.volume_api.get_snapshot(context, snapshot_id)
1354                     bdm.volume_size = bdm.volume_size or snap.get('size')
1355                 except exception.CinderConnectionFailed:
1356                     raise
1357                 except Exception:
1358                     raise exception.InvalidBDMSnapshot(id=snapshot_id)
1359             elif (bdm.source_type == 'blank' and
1360                     bdm.destination_type == 'volume' and
1361                     not bdm.volume_size):
1362                 raise exception.InvalidBDM(message=_("Blank volumes "
1363                     "(source: 'blank', dest: 'volume') need to have non-zero "
1364                     "size"))
1365 
1366         ephemeral_size = sum(bdm.volume_size or instance_type['ephemeral_gb']
1367                 for bdm in block_device_mappings
1368                 if block_device.new_format_is_ephemeral(bdm))
1369         if ephemeral_size > instance_type['ephemeral_gb']:
1370             raise exception.InvalidBDMEphemeralSize()
1371 
1372         # There should be only one swap
1373         swap_list = block_device.get_bdm_swap_list(block_device_mappings)
1374         if len(swap_list) > 1:
1375             msg = _("More than one swap drive requested.")
1376             raise exception.InvalidBDMFormat(details=msg)
1377 
1378         if swap_list:
1379             swap_size = swap_list[0].volume_size or 0
1380             if swap_size > instance_type['swap']:
1381                 raise exception.InvalidBDMSwapSize()
1382 
1383         max_local = CONF.max_local_block_devices
1384         if max_local >= 0:
1385             num_local = len([bdm for bdm in block_device_mappings
1386                              if bdm.destination_type == 'local'])
1387             if num_local > max_local:
1388                 raise exception.InvalidBDMLocalsLimit()
1389 
1390     def _check_attach(self, context, volume_id, instance):
1391         # TODO(ildikov): This check_attach code is kept only for backward
1392         # compatibility and should be removed after Ocata EOL.
1393         volume = self.volume_api.get(context, volume_id)
1394         if volume['status'] != 'available':
1395             msg = _("volume '%(vol)s' status must be 'available'. Currently "
1396                     "in '%(status)s'") % {'vol': volume['id'],
1397                                           'status': volume['status']}
1398             raise exception.InvalidVolume(reason=msg)
1399         if volume['attach_status'] == 'attached':
1400             msg = _("volume %s already attached") % volume['id']
1401             raise exception.InvalidVolume(reason=msg)
1402         self.volume_api.check_availability_zone(context, volume,
1403                                                 instance=instance)
1404 
1405         return volume
1406 
1407     def _populate_instance_names(self, instance, num_instances):
1408         """Populate instance display_name and hostname."""
1409         display_name = instance.get('display_name')
1410         if instance.obj_attr_is_set('hostname'):
1411             hostname = instance.get('hostname')
1412         else:
1413             hostname = None
1414 
1415         # NOTE(mriedem): This is only here for test simplicity since a server
1416         # name is required in the REST API.
1417         if display_name is None:
1418             display_name = self._default_display_name(instance.uuid)
1419             instance.display_name = display_name
1420 
1421         if hostname is None and num_instances == 1:
1422             # NOTE(russellb) In the multi-instance case, we're going to
1423             # overwrite the display_name using the
1424             # multi_instance_display_name_template.  We need the default
1425             # display_name set so that it can be used in the template, though.
1426             # Only set the hostname here if we're only creating one instance.
1427             # Otherwise, it will be built after the template based
1428             # display_name.
1429             hostname = display_name
1430             default_hostname = self._default_host_name(instance.uuid)
1431             instance.hostname = utils.sanitize_hostname(hostname,
1432                                                         default_hostname)
1433 
1434     def _default_display_name(self, instance_uuid):
1435         return "Server %s" % instance_uuid
1436 
1437     def _default_host_name(self, instance_uuid):
1438         return "Server-%s" % instance_uuid
1439 
1440     def _populate_instance_for_create(self, context, instance, image,
1441                                       index, security_groups, instance_type,
1442                                       num_instances, shutdown_terminate):
1443         """Build the beginning of a new instance."""
1444 
1445         instance.launch_index = index
1446         instance.vm_state = vm_states.BUILDING
1447         instance.task_state = task_states.SCHEDULING
1448         info_cache = objects.InstanceInfoCache()
1449         info_cache.instance_uuid = instance.uuid
1450         info_cache.network_info = network_model.NetworkInfo()
1451         instance.info_cache = info_cache
1452         instance.flavor = instance_type
1453         instance.old_flavor = None
1454         instance.new_flavor = None
1455         if CONF.ephemeral_storage_encryption.enabled:
1456             # NOTE(kfarr): dm-crypt expects the cipher in a
1457             # hyphenated format: cipher-chainmode-ivmode
1458             # (ex: aes-xts-plain64). The algorithm needs
1459             # to be parsed out to pass to the key manager (ex: aes).
1460             cipher = CONF.ephemeral_storage_encryption.cipher
1461             algorithm = cipher.split('-')[0] if cipher else None
1462             instance.ephemeral_key_uuid = self.key_manager.create_key(
1463                 context,
1464                 algorithm=algorithm,
1465                 length=CONF.ephemeral_storage_encryption.key_size)
1466         else:
1467             instance.ephemeral_key_uuid = None
1468 
1469         # Store image properties so we can use them later
1470         # (for notifications, etc).  Only store what we can.
1471         if not instance.obj_attr_is_set('system_metadata'):
1472             instance.system_metadata = {}
1473         # Make sure we have the dict form that we need for instance_update.
1474         instance.system_metadata = utils.instance_sys_meta(instance)
1475 
1476         system_meta = utils.get_system_metadata_from_image(
1477             image, instance_type)
1478 
1479         # In case we couldn't find any suitable base_image
1480         system_meta.setdefault('image_base_image_ref', instance.image_ref)
1481 
1482         system_meta['owner_user_name'] = context.user_name
1483         system_meta['owner_project_name'] = context.project_name
1484 
1485         instance.system_metadata.update(system_meta)
1486 
1487         if CONF.use_neutron:
1488             # For Neutron we don't actually store anything in the database, we
1489             # proxy the security groups on the instance from the ports
1490             # attached to the instance.
1491             instance.security_groups = objects.SecurityGroupList()
1492         else:
1493             instance.security_groups = security_groups
1494 
1495         self._populate_instance_names(instance, num_instances)
1496         instance.shutdown_terminate = shutdown_terminate
1497         if num_instances > 1 and self.cell_type != 'api':
1498             instance = self._apply_instance_name_template(context, instance,
1499                                                           index)
1500 
1501         return instance
1502 
1503     def _create_tag_list_obj(self, context, tags):
1504         """Create TagList objects from simple string tags.
1505 
1506         :param context: security context.
1507         :param tags: simple string tags from API request.
1508         :returns: TagList object.
1509         """
1510         tag_list = [objects.Tag(context=context, tag=t) for t in tags]
1511         tag_list_obj = objects.TagList(objects=tag_list)
1512         return tag_list_obj
1513 
1514     def _transform_tags(self, tags, resource_id):
1515         """Change the resource_id of the tags according to the input param.
1516 
1517         Because this method can be called multiple times when more than one
1518         instance is booted in a single request it makes a copy of the tags
1519         list.
1520 
1521         :param tags: TagList object.
1522         :param resource_id: string.
1523         :returns: TagList object.
1524         """
1525         instance_tags = tags.obj_clone()
1526         for tag in instance_tags:
1527             tag.resource_id = resource_id
1528         return instance_tags
1529 
1530     # This method remains because cellsv1 uses it in the scheduler
1531     def create_db_entry_for_new_instance(self, context, instance_type, image,
1532             instance, security_group, block_device_mapping, num_instances,
1533             index, shutdown_terminate=False, create_instance=True):
1534         """Create an entry in the DB for this new instance,
1535         including any related table updates (such as security group,
1536         etc).
1537 
1538         This is called by the scheduler after a location for the
1539         instance has been determined.
1540 
1541         :param create_instance: Determines if the instance is created here or
1542             just populated for later creation. This is done so that this code
1543             can be shared with cellsv1 which needs the instance creation to
1544             happen here. It should be removed and this method cleaned up when
1545             cellsv1 is a distant memory.
1546         """
1547         self._populate_instance_for_create(context, instance, image, index,
1548                                            security_group, instance_type,
1549                                            num_instances, shutdown_terminate)
1550 
1551         if create_instance:
1552             instance.create()
1553 
1554         return instance
1555 
1556     def _check_multiple_instances_with_neutron_ports(self,
1557                                                      requested_networks):
1558         """Check whether multiple instances are created from port id(s)."""
1559         for requested_net in requested_networks:
1560             if requested_net.port_id:
1561                 msg = _("Unable to launch multiple instances with"
1562                         " a single configured port ID. Please launch your"
1563                         " instance one by one with different ports.")
1564                 raise exception.MultiplePortsNotApplicable(reason=msg)
1565 
1566     def _check_multiple_instances_with_specified_ip(self, requested_networks):
1567         """Check whether multiple instances are created with specified ip."""
1568 
1569         for requested_net in requested_networks:
1570             if requested_net.network_id and requested_net.address:
1571                 msg = _("max_count cannot be greater than 1 if an fixed_ip "
1572                         "is specified.")
1573                 raise exception.InvalidFixedIpAndMaxCountRequest(reason=msg)
1574 
1575     @hooks.add_hook("create_instance")
1576     def create(self, context, instance_type,
1577                image_href, kernel_id=None, ramdisk_id=None,
1578                min_count=None, max_count=None,
1579                display_name=None, display_description=None,
1580                key_name=None, key_data=None, security_groups=None,
1581                availability_zone=None, forced_host=None, forced_node=None,
1582                user_data=None, metadata=None, injected_files=None,
1583                admin_password=None, block_device_mapping=None,
1584                access_ip_v4=None, access_ip_v6=None, requested_networks=None,
1585                config_drive=None, auto_disk_config=None, scheduler_hints=None,
1586                legacy_bdm=True, shutdown_terminate=False,
1587                check_server_group_quota=False, tags=None):
1588         """Provision instances, sending instance information to the
1589         scheduler.  The scheduler will determine where the instance(s)
1590         go and will handle creating the DB entries.
1591 
1592         Returns a tuple of (instances, reservation_id)
1593         """
1594         if requested_networks and max_count is not None and max_count > 1:
1595             self._check_multiple_instances_with_specified_ip(
1596                 requested_networks)
1597             if utils.is_neutron():
1598                 self._check_multiple_instances_with_neutron_ports(
1599                     requested_networks)
1600 
1601         if availability_zone:
1602             available_zones = availability_zones.\
1603                 get_availability_zones(context.elevated(), True)
1604             if forced_host is None and availability_zone not in \
1605                     available_zones:
1606                 msg = _('The requested availability zone is not available')
1607                 raise exception.InvalidRequest(msg)
1608 
1609         filter_properties = scheduler_utils.build_filter_properties(
1610                 scheduler_hints, forced_host, forced_node, instance_type)
1611 
1612         return self._create_instance(
1613                        context, instance_type,
1614                        image_href, kernel_id, ramdisk_id,
1615                        min_count, max_count,
1616                        display_name, display_description,
1617                        key_name, key_data, security_groups,
1618                        availability_zone, user_data, metadata,
1619                        injected_files, admin_password,
1620                        access_ip_v4, access_ip_v6,
1621                        requested_networks, config_drive,
1622                        block_device_mapping, auto_disk_config,
1623                        filter_properties=filter_properties,
1624                        legacy_bdm=legacy_bdm,
1625                        shutdown_terminate=shutdown_terminate,
1626                        check_server_group_quota=check_server_group_quota,
1627                        tags=tags)
1628 
1629     def _check_auto_disk_config(self, instance=None, image=None,
1630                                 **extra_instance_updates):
1631         auto_disk_config = extra_instance_updates.get("auto_disk_config")
1632         if auto_disk_config is None:
1633             return
1634         if not image and not instance:
1635             return
1636 
1637         if image:
1638             image_props = image.get("properties", {})
1639             auto_disk_config_img = \
1640                 utils.get_auto_disk_config_from_image_props(image_props)
1641             image_ref = image.get("id")
1642         else:
1643             sys_meta = utils.instance_sys_meta(instance)
1644             image_ref = sys_meta.get('image_base_image_ref')
1645             auto_disk_config_img = \
1646                 utils.get_auto_disk_config_from_instance(sys_meta=sys_meta)
1647 
1648         self._ensure_auto_disk_config_is_valid(auto_disk_config_img,
1649                                                auto_disk_config,
1650                                                image_ref)
1651 
1652     def _lookup_instance(self, context, uuid):
1653         '''Helper method for pulling an instance object from a database.
1654 
1655         During the transition to cellsv2 there is some complexity around
1656         retrieving an instance from the database which this method hides. If
1657         there is an instance mapping then query the cell for the instance, if
1658         no mapping exists then query the configured nova database.
1659 
1660         Once we are past the point that all deployments can be assumed to be
1661         migrated to cellsv2 this method can go away.
1662         '''
1663         inst_map = None
1664         try:
1665             inst_map = objects.InstanceMapping.get_by_instance_uuid(
1666                 context, uuid)
1667         except exception.InstanceMappingNotFound:
1668             # TODO(alaski): This exception block can be removed once we're
1669             # guaranteed everyone is using cellsv2.
1670             pass
1671 
1672         if (inst_map is None or inst_map.cell_mapping is None or
1673                 CONF.cells.enable):
1674             # If inst_map is None then the deployment has not migrated to
1675             # cellsv2 yet.
1676             # If inst_map.cell_mapping is None then the instance is not in a
1677             # cell yet. Until instance creation moves to the conductor the
1678             # instance can be found in the configured database, so attempt
1679             # to look it up.
1680             # If we're on cellsv1, we can't yet short-circuit the cells
1681             # messaging path
1682             cell = None
1683             try:
1684                 instance = objects.Instance.get_by_uuid(context, uuid)
1685             except exception.InstanceNotFound:
1686                 # If we get here then the conductor is in charge of writing the
1687                 # instance to the database and hasn't done that yet. It's up to
1688                 # the caller of this method to determine what to do with that
1689                 # information.
1690                 return None, None
1691         else:
1692             cell = inst_map.cell_mapping
1693             with nova_context.target_cell(context, cell) as cctxt:
1694                 try:
1695                     instance = objects.Instance.get_by_uuid(cctxt, uuid)
1696                 except exception.InstanceNotFound:
1697                     # Since the cell_mapping exists we know the instance is in
1698                     # the cell, however InstanceNotFound means it's already
1699                     # deleted.
1700                     return None, None
1701         return cell, instance
1702 
1703     def _delete_while_booting(self, context, instance):
1704         """Handle deletion if the instance has not reached a cell yet
1705 
1706         Deletion before an instance reaches a cell needs to be handled
1707         differently. What we're attempting to do is delete the BuildRequest
1708         before the api level conductor does.  If we succeed here then the boot
1709         request stops before reaching a cell.  If not then the instance will
1710         need to be looked up in a cell db and the normal delete path taken.
1711         """
1712         deleted = self._attempt_delete_of_buildrequest(context, instance)
1713 
1714         # After service version 15 deletion of the BuildRequest will halt the
1715         # build process in the conductor. In that case run the rest of this
1716         # method and consider the instance deleted. If we have not yet reached
1717         # service version 15 then just return False so the rest of the delete
1718         # process will proceed usually.
1719         service_version = objects.Service.get_minimum_version(
1720             context, 'nova-osapi_compute')
1721         if service_version < 15:
1722             return False
1723 
1724         if deleted:
1725             # If we've reached this block the successful deletion of the
1726             # buildrequest indicates that the build process should be halted by
1727             # the conductor.
1728 
1729             # NOTE(alaski): Though the conductor halts the build process it
1730             # does not currently delete the instance record. This is
1731             # because in the near future the instance record will not be
1732             # created if the buildrequest has been deleted here. For now we
1733             # ensure the instance has been set to deleted at this point.
1734             # Yes this directly contradicts the comment earlier in this
1735             # method, but this is a temporary measure.
1736             # Look up the instance because the current instance object was
1737             # stashed on the buildrequest and therefore not complete enough
1738             # to run .destroy().
1739             try:
1740                 instance_uuid = instance.uuid
1741                 cell, instance = self._lookup_instance(context, instance_uuid)
1742                 if instance is not None:
1743                     # If instance is None it has already been deleted.
1744                     if cell:
1745                         with nova_context.target_cell(context, cell) as cctxt:
1746                             # FIXME: When the instance context is targeted,
1747                             # we can remove this
1748                             with compute_utils.notify_about_instance_delete(
1749                                     self.notifier, cctxt, instance):
1750                                 instance.destroy()
1751                     else:
1752                         instance.destroy()
1753             except exception.InstanceNotFound:
1754                 pass
1755 
1756             return True
1757         return False
1758 
1759     def _attempt_delete_of_buildrequest(self, context, instance):
1760         # If there is a BuildRequest then the instance may not have been
1761         # written to a cell db yet. Delete the BuildRequest here, which
1762         # will indicate that the Instance build should not proceed.
1763         try:
1764             build_req = objects.BuildRequest.get_by_instance_uuid(
1765                 context, instance.uuid)
1766             build_req.destroy()
1767         except exception.BuildRequestNotFound:
1768             # This means that conductor has deleted the BuildRequest so the
1769             # instance is now in a cell and the delete needs to proceed
1770             # normally.
1771             return False
1772         return True
1773 
1774     def _delete(self, context, instance, delete_type, cb, **instance_attrs):
1775         if instance.disable_terminate:
1776             LOG.info('instance termination disabled', instance=instance)
1777             return
1778 
1779         cell = None
1780         # If there is an instance.host (or the instance is shelved-offloaded),
1781         # the instance has been scheduled and sent to a cell/compute which
1782         # means it was pulled from the cell db.
1783         # Normal delete should be attempted.
1784         if not (instance.host or
1785                 instance.vm_state == vm_states.SHELVED_OFFLOADED):
1786             try:
1787                 if self._delete_while_booting(context, instance):
1788                     return
1789                 # If instance.host was not set it's possible that the Instance
1790                 # object here was pulled from a BuildRequest object and is not
1791                 # fully populated. Notably it will be missing an 'id' field
1792                 # which will prevent instance.destroy from functioning
1793                 # properly. A lookup is attempted which will either return a
1794                 # full Instance or None if not found. If not found then it's
1795                 # acceptable to skip the rest of the delete processing.
1796                 cell, instance = self._lookup_instance(context, instance.uuid)
1797                 if cell and instance:
1798                     try:
1799                         # Now destroy the instance from the cell it lives in.
1800                         with compute_utils.notify_about_instance_delete(
1801                                 self.notifier, context, instance):
1802                             instance.destroy()
1803                     except exception.InstanceNotFound:
1804                         pass
1805                     # The instance was deleted or is already gone.
1806                     return
1807                 if not instance:
1808                     # Instance is already deleted.
1809                     return
1810             except exception.ObjectActionError:
1811                 # NOTE(melwitt): This means the instance.host changed
1812                 # under us indicating the instance became scheduled
1813                 # during the destroy(). Refresh the instance from the DB and
1814                 # continue on with the delete logic for a scheduled instance.
1815                 # NOTE(danms): If instance.host is set, we should be able to
1816                 # do the following lookup. If not, there's not much we can
1817                 # do to recover.
1818                 cell, instance = self._lookup_instance(context, instance.uuid)
1819                 if not instance:
1820                     # Instance is already deleted
1821                     return
1822 
1823         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
1824                 context, instance.uuid)
1825 
1826         # At these states an instance has a snapshot associate.
1827         if instance.vm_state in (vm_states.SHELVED,
1828                                  vm_states.SHELVED_OFFLOADED):
1829             snapshot_id = instance.system_metadata.get('shelved_image_id')
1830             LOG.info("Working on deleting snapshot %s "
1831                      "from shelved instance...",
1832                      snapshot_id, instance=instance)
1833             try:
1834                 self.image_api.delete(context, snapshot_id)
1835             except (exception.ImageNotFound,
1836                     exception.ImageNotAuthorized) as exc:
1837                 LOG.warning("Failed to delete snapshot "
1838                             "from shelved instance (%s).",
1839                             exc.format_message(), instance=instance)
1840             except Exception:
1841                 LOG.exception("Something wrong happened when trying to "
1842                               "delete snapshot from shelved instance.",
1843                               instance=instance)
1844 
1845         original_task_state = instance.task_state
1846         try:
1847             # NOTE(maoy): no expected_task_state needs to be set
1848             instance.update(instance_attrs)
1849             instance.progress = 0
1850             instance.save()
1851 
1852             # NOTE(dtp): cells.enable = False means "use cells v2".
1853             # Run everywhere except v1 compute cells.
1854             if not CONF.cells.enable or self.cell_type == 'api':
1855                 self.consoleauth_rpcapi.delete_tokens_for_instance(
1856                     context, instance.uuid)
1857 
1858             if self.cell_type == 'api':
1859                 # NOTE(comstud): If we're in the API cell, we need to
1860                 # skip all remaining logic and just call the callback,
1861                 # which will cause a cast to the child cell.
1862                 cb(context, instance, bdms)
1863                 return
1864             shelved_offloaded = (instance.vm_state
1865                                  == vm_states.SHELVED_OFFLOADED)
1866             if not instance.host and not shelved_offloaded:
1867                 try:
1868                     with compute_utils.notify_about_instance_delete(
1869                             self.notifier, context, instance,
1870                             delete_type
1871                             if delete_type != 'soft_delete'
1872                             else 'delete'):
1873                         instance.destroy()
1874                     LOG.info('Instance deleted and does not have host '
1875                              'field, its vm_state is %(state)s.',
1876                              {'state': instance.vm_state},
1877                               instance=instance)
1878                     return
1879                 except exception.ObjectActionError:
1880                     instance.refresh()
1881 
1882             if instance.vm_state == vm_states.RESIZED:
1883                 self._confirm_resize_on_deleting(context, instance)
1884 
1885             is_local_delete = True
1886             try:
1887                 if not shelved_offloaded:
1888                     service = objects.Service.get_by_compute_host(
1889                         context.elevated(), instance.host)
1890                     is_local_delete = not self.servicegroup_api.service_is_up(
1891                         service)
1892                 if not is_local_delete:
1893                     if original_task_state in (task_states.DELETING,
1894                                                   task_states.SOFT_DELETING):
1895                         LOG.info('Instance is already in deleting state, '
1896                                  'ignoring this request',
1897                                  instance=instance)
1898                         return
1899                     self._record_action_start(context, instance,
1900                                               instance_actions.DELETE)
1901 
1902                     cb(context, instance, bdms)
1903             except exception.ComputeHostNotFound:
1904                 pass
1905 
1906             if is_local_delete:
1907                 # If instance is in shelved_offloaded state or compute node
1908                 # isn't up, delete instance from db and clean bdms info and
1909                 # network info
1910                 if cell is None:
1911                     # NOTE(danms): If we didn't get our cell from one of the
1912                     # paths above, look it up now.
1913                     try:
1914                         im = objects.InstanceMapping.get_by_instance_uuid(
1915                             context, instance.uuid)
1916                         cell = im.cell_mapping
1917                     except exception.InstanceMappingNotFound:
1918                         LOG.warning('During local delete, failed to find '
1919                                     'instance mapping', instance=instance)
1920                         return
1921 
1922                 LOG.debug('Doing local delete in cell %s', cell.identity,
1923                           instance=instance)
1924                 with nova_context.target_cell(context, cell) as cctxt:
1925                     self._local_delete(cctxt, instance, bdms, delete_type, cb)
1926 
1927         except exception.InstanceNotFound:
1928             # NOTE(comstud): Race condition. Instance already gone.
1929             pass
1930 
1931     def _confirm_resize_on_deleting(self, context, instance):
1932         # If in the middle of a resize, use confirm_resize to
1933         # ensure the original instance is cleaned up too
1934         migration = None
1935         for status in ('finished', 'confirming'):
1936             try:
1937                 migration = objects.Migration.get_by_instance_and_status(
1938                         context.elevated(), instance.uuid, status)
1939                 LOG.info('Found an unconfirmed migration during delete, '
1940                          'id: %(id)s, status: %(status)s',
1941                          {'id': migration.id,
1942                           'status': migration.status},
1943                          instance=instance)
1944                 break
1945             except exception.MigrationNotFoundByStatus:
1946                 pass
1947 
1948         if not migration:
1949             LOG.info('Instance may have been confirmed during delete',
1950                      instance=instance)
1951             return
1952 
1953         src_host = migration.source_compute
1954 
1955         self._record_action_start(context, instance,
1956                                   instance_actions.CONFIRM_RESIZE)
1957 
1958         self.compute_rpcapi.confirm_resize(context,
1959                 instance, migration, src_host, cast=False)
1960 
1961     def _get_stashed_volume_connector(self, bdm, instance):
1962         """Lookup a connector dict from the bdm.connection_info if set
1963 
1964         Gets the stashed connector dict out of the bdm.connection_info if set
1965         and the connector host matches the instance host.
1966 
1967         :param bdm: nova.objects.block_device.BlockDeviceMapping
1968         :param instance: nova.objects.instance.Instance
1969         :returns: volume connector dict or None
1970         """
1971         if 'connection_info' in bdm and bdm.connection_info is not None:
1972             # NOTE(mriedem): We didn't start stashing the connector in the
1973             # bdm.connection_info until Mitaka so it might not be there on old
1974             # attachments. Also, if the volume was attached when the instance
1975             # was in shelved_offloaded state and it hasn't been unshelved yet
1976             # we don't have the attachment/connection information either.
1977             connector = jsonutils.loads(bdm.connection_info).get('connector')
1978             if connector:
1979                 if connector.get('host') == instance.host:
1980                     return connector
1981                 LOG.debug('Found stashed volume connector for instance but '
1982                           'connector host %(connector_host)s does not match '
1983                           'the instance host %(instance_host)s.',
1984                           {'connector_host': connector.get('host'),
1985                            'instance_host': instance.host}, instance=instance)
1986 
1987     def _local_cleanup_bdm_volumes(self, bdms, instance, context):
1988         """The method deletes the bdm records and, if a bdm is a volume, call
1989         the terminate connection and the detach volume via the Volume API.
1990         """
1991         elevated = context.elevated()
1992         for bdm in bdms:
1993             if bdm.is_volume:
1994                 try:
1995                     if bdm.attachment_id:
1996                         self.volume_api.attachment_delete(context,
1997                                                           bdm.attachment_id)
1998                     else:
1999                         connector = self._get_stashed_volume_connector(
2000                             bdm, instance)
2001                         if connector:
2002                             self.volume_api.terminate_connection(context,
2003                                                                  bdm.volume_id,
2004                                                                  connector)
2005                         else:
2006                             LOG.debug('Unable to find connector for volume %s,'
2007                                       ' not attempting terminate_connection.',
2008                                       bdm.volume_id, instance=instance)
2009                         # Attempt to detach the volume. If there was no
2010                         # connection made in the first place this is just
2011                         # cleaning up the volume state in the Cinder DB.
2012                         self.volume_api.detach(elevated, bdm.volume_id,
2013                                                instance.uuid)
2014 
2015                     if bdm.delete_on_termination:
2016                         self.volume_api.delete(context, bdm.volume_id)
2017                 except Exception as exc:
2018                     LOG.warning("Ignoring volume cleanup failure due to %s",
2019                                 exc, instance=instance)
2020             bdm.destroy()
2021 
2022     def _local_delete(self, context, instance, bdms, delete_type, cb):
2023         if instance.vm_state == vm_states.SHELVED_OFFLOADED:
2024             LOG.info("instance is in SHELVED_OFFLOADED state, cleanup"
2025                      " the instance's info from database.",
2026                      instance=instance)
2027         else:
2028             LOG.warning("instance's host %s is down, deleting from "
2029                         "database", instance.host, instance=instance)
2030         with compute_utils.notify_about_instance_delete(
2031                 self.notifier, context, instance,
2032                 delete_type if delete_type != 'soft_delete' else 'delete'):
2033 
2034             elevated = context.elevated()
2035             if self.cell_type != 'api':
2036                 # NOTE(liusheng): In nova-network multi_host scenario,deleting
2037                 # network info of the instance may need instance['host'] as
2038                 # destination host of RPC call. If instance in
2039                 # SHELVED_OFFLOADED state, instance['host'] is None, here, use
2040                 # shelved_host as host to deallocate network info and reset
2041                 # instance['host'] after that. Here we shouldn't use
2042                 # instance.save(), because this will mislead user who may think
2043                 # the instance's host has been changed, and actually, the
2044                 # instance.host is always None.
2045                 orig_host = instance.host
2046                 try:
2047                     if instance.vm_state == vm_states.SHELVED_OFFLOADED:
2048                         sysmeta = getattr(instance,
2049                                           obj_base.get_attrname(
2050                                               'system_metadata'))
2051                         instance.host = sysmeta.get('shelved_host')
2052                     self.network_api.deallocate_for_instance(elevated,
2053                                                              instance)
2054                 finally:
2055                     instance.host = orig_host
2056 
2057             # cleanup volumes
2058             self._local_cleanup_bdm_volumes(bdms, instance, context)
2059             cb(context, instance, bdms, local=True)
2060             instance.destroy()
2061 
2062     def _do_delete(self, context, instance, bdms, local=False):
2063         if local:
2064             instance.vm_state = vm_states.DELETED
2065             instance.task_state = None
2066             instance.terminated_at = timeutils.utcnow()
2067             instance.save()
2068         else:
2069             self.compute_rpcapi.terminate_instance(context, instance, bdms,
2070                                                    delete_type='delete')
2071 
2072     def _do_force_delete(self, context, instance, bdms, local=False):
2073         if local:
2074             instance.vm_state = vm_states.DELETED
2075             instance.task_state = None
2076             instance.terminated_at = timeutils.utcnow()
2077             instance.save()
2078         else:
2079             self.compute_rpcapi.terminate_instance(context, instance, bdms,
2080                                                    delete_type='force_delete')
2081 
2082     def _do_soft_delete(self, context, instance, bdms, local=False):
2083         if local:
2084             instance.vm_state = vm_states.SOFT_DELETED
2085             instance.task_state = None
2086             instance.terminated_at = timeutils.utcnow()
2087             instance.save()
2088         else:
2089             self.compute_rpcapi.soft_delete_instance(context, instance)
2090 
2091     # NOTE(maoy): we allow delete to be called no matter what vm_state says.
2092     @check_instance_lock
2093     @check_instance_cell
2094     @check_instance_state(vm_state=None, task_state=None,
2095                           must_have_launched=True)
2096     def soft_delete(self, context, instance):
2097         """Terminate an instance."""
2098         LOG.debug('Going to try to soft delete instance',
2099                   instance=instance)
2100 
2101         self._delete(context, instance, 'soft_delete', self._do_soft_delete,
2102                      task_state=task_states.SOFT_DELETING,
2103                      deleted_at=timeutils.utcnow())
2104 
2105     def _delete_instance(self, context, instance):
2106         self._delete(context, instance, 'delete', self._do_delete,
2107                      task_state=task_states.DELETING)
2108 
2109     @check_instance_lock
2110     @check_instance_cell
2111     @check_instance_state(vm_state=None, task_state=None,
2112                           must_have_launched=False)
2113     def delete(self, context, instance):
2114         """Terminate an instance."""
2115         LOG.debug("Going to try to terminate instance", instance=instance)
2116         self._delete_instance(context, instance)
2117 
2118     @check_instance_lock
2119     @check_instance_state(vm_state=[vm_states.SOFT_DELETED])
2120     def restore(self, context, instance):
2121         """Restore a previously deleted (but not reclaimed) instance."""
2122         # Check quotas
2123         flavor = instance.get_flavor()
2124         project_id, user_id = quotas_obj.ids_from_instance(context, instance)
2125         compute_utils.check_num_instances_quota(context, flavor, 1, 1,
2126                 project_id=project_id, user_id=user_id)
2127 
2128         self._record_action_start(context, instance, instance_actions.RESTORE)
2129 
2130         if instance.host:
2131             instance.task_state = task_states.RESTORING
2132             instance.deleted_at = None
2133             instance.save(expected_task_state=[None])
2134             # TODO(melwitt): We're not rechecking for strict quota here to
2135             # guard against going over quota during a race at this time because
2136             # the resource consumption for this operation is written to the
2137             # database by compute.
2138             self.compute_rpcapi.restore_instance(context, instance)
2139         else:
2140             instance.vm_state = vm_states.ACTIVE
2141             instance.task_state = None
2142             instance.deleted_at = None
2143             instance.save(expected_task_state=[None])
2144 
2145     @check_instance_lock
2146     @check_instance_state(must_have_launched=False)
2147     def force_delete(self, context, instance):
2148         """Force delete an instance in any vm_state/task_state."""
2149         self._delete(context, instance, 'force_delete', self._do_force_delete,
2150                      task_state=task_states.DELETING)
2151 
2152     def force_stop(self, context, instance, do_cast=True, clean_shutdown=True):
2153         LOG.debug("Going to try to stop instance", instance=instance)
2154 
2155         instance.task_state = task_states.POWERING_OFF
2156         instance.progress = 0
2157         instance.save(expected_task_state=[None])
2158 
2159         self._record_action_start(context, instance, instance_actions.STOP)
2160 
2161         self.compute_rpcapi.stop_instance(context, instance, do_cast=do_cast,
2162                                           clean_shutdown=clean_shutdown)
2163 
2164     @check_instance_lock
2165     @check_instance_host
2166     @check_instance_cell
2167     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.ERROR])
2168     def stop(self, context, instance, do_cast=True, clean_shutdown=True):
2169         """Stop an instance."""
2170         self.force_stop(context, instance, do_cast, clean_shutdown)
2171 
2172     @check_instance_lock
2173     @check_instance_host
2174     @check_instance_cell
2175     @check_instance_state(vm_state=[vm_states.STOPPED])
2176     def start(self, context, instance):
2177         """Start an instance."""
2178         LOG.debug("Going to try to start instance", instance=instance)
2179 
2180         instance.task_state = task_states.POWERING_ON
2181         instance.save(expected_task_state=[None])
2182 
2183         self._record_action_start(context, instance, instance_actions.START)
2184         # TODO(yamahata): injected_files isn't supported right now.
2185         #                 It is used only for osapi. not for ec2 api.
2186         #                 availability_zone isn't used by run_instance.
2187         self.compute_rpcapi.start_instance(context, instance)
2188 
2189     @check_instance_lock
2190     @check_instance_host
2191     @check_instance_cell
2192     @check_instance_state(vm_state=vm_states.ALLOW_TRIGGER_CRASH_DUMP)
2193     def trigger_crash_dump(self, context, instance):
2194         """Trigger crash dump in an instance."""
2195         LOG.debug("Try to trigger crash dump", instance=instance)
2196 
2197         self._record_action_start(context, instance,
2198                                   instance_actions.TRIGGER_CRASH_DUMP)
2199 
2200         self.compute_rpcapi.trigger_crash_dump(context, instance)
2201 
2202     def _get_instance_map_or_none(self, context, instance_uuid):
2203         try:
2204             inst_map = objects.InstanceMapping.get_by_instance_uuid(
2205                     context, instance_uuid)
2206         except exception.InstanceMappingNotFound:
2207             # InstanceMapping should always be found generally. This exception
2208             # may be raised if a deployment has partially migrated the nova-api
2209             # services.
2210             inst_map = None
2211         return inst_map
2212 
2213     def _get_instance(self, context, instance_uuid, expected_attrs):
2214         # Before service version 15 the BuildRequest is not cleaned up during
2215         # a delete request so there is no reason to look it up here as we can't
2216         # trust that it's not referencing a deleted instance. Also even if
2217         # there is an instance mapping we don't need to honor it for older
2218         # service versions.
2219         service_version = objects.Service.get_minimum_version(
2220             context, 'nova-osapi_compute')
2221         # If we're on cellsv1, we also need to consult the top-level
2222         # merged replica instead of the cell directly, so fall through
2223         # here in that case as well.
2224         if service_version < 15 or CONF.cells.enable:
2225             return objects.Instance.get_by_uuid(context, instance_uuid,
2226                                                 expected_attrs=expected_attrs)
2227         inst_map = self._get_instance_map_or_none(context, instance_uuid)
2228         if inst_map and (inst_map.cell_mapping is not None):
2229             nova_context.set_target_cell(context, inst_map.cell_mapping)
2230             instance = objects.Instance.get_by_uuid(
2231                 context, instance_uuid, expected_attrs=expected_attrs)
2232         elif inst_map and (inst_map.cell_mapping is None):
2233             # This means the instance has not been scheduled and put in
2234             # a cell yet. For now it also may mean that the deployer
2235             # has not created their cell(s) yet.
2236             try:
2237                 build_req = objects.BuildRequest.get_by_instance_uuid(
2238                         context, instance_uuid)
2239                 instance = build_req.instance
2240             except exception.BuildRequestNotFound:
2241                 # Instance was mapped and the BuildRequest was deleted
2242                 # while fetching. Try again.
2243                 inst_map = self._get_instance_map_or_none(context,
2244                                                           instance_uuid)
2245                 if inst_map and (inst_map.cell_mapping is not None):
2246                     nova_context.set_target_cell(context,
2247                                                  inst_map.cell_mapping)
2248                     instance = objects.Instance.get_by_uuid(
2249                         context, instance_uuid,
2250                         expected_attrs=expected_attrs)
2251                 else:
2252                     raise exception.InstanceNotFound(instance_id=instance_uuid)
2253         else:
2254             raise exception.InstanceNotFound(instance_id=instance_uuid)
2255 
2256         return instance
2257 
2258     def get(self, context, instance_id, expected_attrs=None):
2259         """Get a single instance with the given instance_id."""
2260         if not expected_attrs:
2261             expected_attrs = []
2262         expected_attrs.extend(['metadata', 'system_metadata',
2263                                'security_groups', 'info_cache'])
2264         # NOTE(ameade): we still need to support integer ids for ec2
2265         try:
2266             if uuidutils.is_uuid_like(instance_id):
2267                 LOG.debug("Fetching instance by UUID",
2268                            instance_uuid=instance_id)
2269 
2270                 instance = self._get_instance(context, instance_id,
2271                                               expected_attrs)
2272             else:
2273                 LOG.debug("Failed to fetch instance by id %s", instance_id)
2274                 raise exception.InstanceNotFound(instance_id=instance_id)
2275         except exception.InvalidID:
2276             LOG.debug("Invalid instance id %s", instance_id)
2277             raise exception.InstanceNotFound(instance_id=instance_id)
2278 
2279         return instance
2280 
2281     def get_all(self, context, search_opts=None, limit=None, marker=None,
2282                 expected_attrs=None, sort_keys=None, sort_dirs=None):
2283         """Get all instances filtered by one of the given parameters.
2284 
2285         If there is no filter and the context is an admin, it will retrieve
2286         all instances in the system.
2287 
2288         Deleted instances will be returned by default, unless there is a
2289         search option that says otherwise.
2290 
2291         The results will be sorted based on the list of sort keys in the
2292         'sort_keys' parameter (first value is primary sort key, second value is
2293         secondary sort ket, etc.). For each sort key, the associated sort
2294         direction is based on the list of sort directions in the 'sort_dirs'
2295         parameter.
2296         """
2297         if search_opts is None:
2298             search_opts = {}
2299 
2300         LOG.debug("Searching by: %s", str(search_opts))
2301 
2302         # Fixups for the DB call
2303         filters = {}
2304 
2305         def _remap_flavor_filter(flavor_id):
2306             flavor = objects.Flavor.get_by_flavor_id(context, flavor_id)
2307             filters['instance_type_id'] = flavor.id
2308 
2309         def _remap_fixed_ip_filter(fixed_ip):
2310             # Turn fixed_ip into a regexp match. Since '.' matches
2311             # any character, we need to use regexp escaping for it.
2312             filters['ip'] = '^%s$' % fixed_ip.replace('.', '\\.')
2313 
2314         # search_option to filter_name mapping.
2315         filter_mapping = {
2316                 'image': 'image_ref',
2317                 'name': 'display_name',
2318                 'tenant_id': 'project_id',
2319                 'flavor': _remap_flavor_filter,
2320                 'fixed_ip': _remap_fixed_ip_filter}
2321 
2322         # copy from search_opts, doing various remappings as necessary
2323         for opt, value in search_opts.items():
2324             # Do remappings.
2325             # Values not in the filter_mapping table are copied as-is.
2326             # If remapping is None, option is not copied
2327             # If the remapping is a string, it is the filter_name to use
2328             try:
2329                 remap_object = filter_mapping[opt]
2330             except KeyError:
2331                 filters[opt] = value
2332             else:
2333                 # Remaps are strings to translate to, or functions to call
2334                 # to do the translating as defined by the table above.
2335                 if isinstance(remap_object, six.string_types):
2336                     filters[remap_object] = value
2337                 else:
2338                     try:
2339                         remap_object(value)
2340 
2341                     # We already know we can't match the filter, so
2342                     # return an empty list
2343                     except ValueError:
2344                         return objects.InstanceList()
2345 
2346         # IP address filtering cannot be applied at the DB layer, remove any DB
2347         # limit so that it can be applied after the IP filter.
2348         filter_ip = 'ip6' in filters or 'ip' in filters
2349         orig_limit = limit
2350         if filter_ip and limit:
2351             LOG.debug('Removing limit for DB query due to IP filter')
2352             limit = None
2353 
2354         # The ordering of instances will be
2355         # [sorted instances with no host] + [sorted instances with host].
2356         # This means BuildRequest and cell0 instances first, then cell
2357         # instances
2358         try:
2359             build_requests = objects.BuildRequestList.get_by_filters(
2360                 context, filters, limit=limit, marker=marker,
2361                 sort_keys=sort_keys, sort_dirs=sort_dirs)
2362             # If we found the marker in we need to set it to None
2363             # so we don't expect to find it in the cells below.
2364             marker = None
2365         except exception.MarkerNotFound:
2366             # If we didn't find the marker in the build requests then keep
2367             # looking for it in the cells.
2368             build_requests = objects.BuildRequestList()
2369         build_req_instances = objects.InstanceList(
2370             objects=[build_req.instance for build_req in build_requests])
2371         # Only subtract from limit if it is not None
2372         limit = (limit - len(build_req_instances)) if limit else limit
2373 
2374         # We could arguably avoid joining on security_groups if we're using
2375         # neutron (which is the default) but if you're using neutron then the
2376         # security_group_instance_association table should be empty anyway
2377         # and the DB should optimize out that join, making it insignificant.
2378         fields = ['metadata', 'info_cache', 'security_groups']
2379         if expected_attrs:
2380             fields.extend(expected_attrs)
2381 
2382         if CONF.cells.enable:
2383             insts = self._do_old_style_instance_list_for_poor_cellsv1_users(
2384                 context, filters, limit, marker, fields, sort_keys,
2385                 sort_dirs)
2386         else:
2387             insts = instance_list.get_instance_objects_sorted(
2388                 context, filters, limit, marker, fields, sort_keys, sort_dirs)
2389 
2390         def _get_unique_filter_method():
2391             seen_uuids = set()
2392 
2393             def _filter(instance):
2394                 if instance.uuid in seen_uuids:
2395                     return False
2396                 seen_uuids.add(instance.uuid)
2397                 return True
2398 
2399             return _filter
2400 
2401         filter_method = _get_unique_filter_method()
2402         # Only subtract from limit if it is not None
2403         limit = (limit - len(insts)) if limit else limit
2404         # TODO(alaski): Clean up the objects concatenation when List objects
2405         # support it natively.
2406         instances = objects.InstanceList(
2407             objects=list(filter(filter_method,
2408                            build_req_instances.objects +
2409                            insts.objects)))
2410 
2411         if filter_ip:
2412             instances = self._ip_filter(instances, filters, orig_limit)
2413 
2414         return instances
2415 
2416     def _do_old_style_instance_list_for_poor_cellsv1_users(self,
2417                                                            context, filters,
2418                                                            limit, marker,
2419                                                            fields,
2420                                                            sort_keys,
2421                                                            sort_dirs):
2422         try:
2423             cell0_mapping = objects.CellMapping.get_by_uuid(context,
2424                 objects.CellMapping.CELL0_UUID)
2425         except exception.CellMappingNotFound:
2426             cell0_instances = objects.InstanceList(objects=[])
2427         else:
2428             with nova_context.target_cell(context, cell0_mapping) as cctxt:
2429                 try:
2430                     cell0_instances = self._get_instances_by_filters(
2431                         cctxt, filters, limit=limit, marker=marker,
2432                         fields=fields, sort_keys=sort_keys,
2433                         sort_dirs=sort_dirs)
2434                     # If we found the marker in cell0 we need to set it to None
2435                     # so we don't expect to find it in the cells below.
2436                     marker = None
2437                 except exception.MarkerNotFound:
2438                     # We can ignore this since we need to look in the cell DB
2439                     cell0_instances = objects.InstanceList(objects=[])
2440         # Only subtract from limit if it is not None
2441         limit = (limit - len(cell0_instances)) if limit else limit
2442 
2443         # There is only planned support for a single cell here. Multiple cell
2444         # instance lists should be proxied to project Searchlight, or a similar
2445         # alternative.
2446         if limit is None or limit > 0:
2447             # NOTE(melwitt): If we're on cells v1, we need to read
2448             # instances from the top-level database because reading from
2449             # cells results in changed behavior, because of the syncing.
2450             # We can remove this path once we stop supporting cells v1.
2451             cell_instances = self._get_instances_by_filters(
2452                 context, filters, limit=limit, marker=marker,
2453                 fields=fields, sort_keys=sort_keys,
2454                 sort_dirs=sort_dirs)
2455         else:
2456             LOG.debug('Limit excludes any results from real cells')
2457             cell_instances = objects.InstanceList(objects=[])
2458 
2459         return cell0_instances + cell_instances
2460 
2461     @staticmethod
2462     def _ip_filter(inst_models, filters, limit):
2463         ipv4_f = re.compile(str(filters.get('ip')))
2464         ipv6_f = re.compile(str(filters.get('ip6')))
2465 
2466         def _match_instance(instance):
2467             nw_info = instance.get_network_info()
2468             for vif in nw_info:
2469                 for fixed_ip in vif.fixed_ips():
2470                     address = fixed_ip.get('address')
2471                     if not address:
2472                         continue
2473                     version = fixed_ip.get('version')
2474                     if ((version == 4 and ipv4_f.match(address)) or
2475                         (version == 6 and ipv6_f.match(address))):
2476                         return True
2477             return False
2478 
2479         result_objs = []
2480         for instance in inst_models:
2481             if _match_instance(instance):
2482                 result_objs.append(instance)
2483                 if limit and len(result_objs) == limit:
2484                     break
2485         return objects.InstanceList(objects=result_objs)
2486 
2487     def _get_instances_by_filters(self, context, filters,
2488                                   limit=None, marker=None, fields=None,
2489                                   sort_keys=None, sort_dirs=None):
2490         return objects.InstanceList.get_by_filters(
2491             context, filters=filters, limit=limit, marker=marker,
2492             expected_attrs=fields, sort_keys=sort_keys, sort_dirs=sort_dirs)
2493 
2494     def update_instance(self, context, instance, updates):
2495         """Updates a single Instance object with some updates dict.
2496 
2497         Returns the updated instance.
2498         """
2499 
2500         # NOTE(sbauza): Given we only persist the Instance object after we
2501         # create the BuildRequest, we are sure that if the Instance object
2502         # has an ID field set, then it was persisted in the right Cell DB.
2503         if instance.obj_attr_is_set('id'):
2504             instance.update(updates)
2505             # Instance has been scheduled and the BuildRequest has been deleted
2506             # we can directly write the update down to the right cell.
2507             inst_map = self._get_instance_map_or_none(context, instance.uuid)
2508             # If we have a cell_mapping and we're not on cells v1, then
2509             # look up the instance in the cell database
2510             if inst_map and (inst_map.cell_mapping is not None) and (
2511                     not CONF.cells.enable):
2512                 with nova_context.target_cell(context,
2513                                               inst_map.cell_mapping) as cctxt:
2514                     with instance.obj_alternate_context(cctxt):
2515                         instance.save()
2516             else:
2517                 # If inst_map.cell_mapping does not point at a cell then cell
2518                 # migration has not happened yet.
2519                 # TODO(alaski): Make this a failure case after we put in
2520                 # a block that requires migrating to cellsv2.
2521                 instance.save()
2522         else:
2523             # Instance is not yet mapped to a cell, so we need to update
2524             # BuildRequest instead
2525             # TODO(sbauza): Fix the possible race conditions where BuildRequest
2526             # could be deleted because of either a concurrent instance delete
2527             # or because the scheduler just returned a destination right
2528             # after we called the instance in the API.
2529             try:
2530                 build_req = objects.BuildRequest.get_by_instance_uuid(
2531                     context, instance.uuid)
2532                 instance = build_req.instance
2533                 instance.update(updates)
2534                 # FIXME(sbauza): Here we are updating the current
2535                 # thread-related BuildRequest object. Given that another worker
2536                 # could have looking up at that BuildRequest in the API, it
2537                 # means that it could pass it down to the conductor without
2538                 # making sure that it's not updated, we could have some race
2539                 # condition where it would missing the updated fields, but
2540                 # that's something we could discuss once the instance record
2541                 # is persisted by the conductor.
2542                 build_req.save()
2543             except exception.BuildRequestNotFound:
2544                 # Instance was mapped and the BuildRequest was deleted
2545                 # while fetching (and possibly the instance could have been
2546                 # deleted as well). We need to lookup again the Instance object
2547                 # in order to correctly update it.
2548                 # TODO(sbauza): Figure out a good way to know the expected
2549                 # attributes by checking which fields are set or not.
2550                 expected_attrs = ['flavor', 'pci_devices', 'numa_topology',
2551                                   'tags', 'metadata', 'system_metadata',
2552                                   'security_groups', 'info_cache']
2553                 inst_map = self._get_instance_map_or_none(context,
2554                                                           instance.uuid)
2555                 if inst_map and (inst_map.cell_mapping is not None):
2556                     with nova_context.target_cell(
2557                             context,
2558                             inst_map.cell_mapping) as cctxt:
2559                         instance = objects.Instance.get_by_uuid(
2560                             cctxt, instance.uuid,
2561                             expected_attrs=expected_attrs)
2562                         instance.update(updates)
2563                         instance.save()
2564                 else:
2565                     # If inst_map.cell_mapping does not point at a cell then
2566                     # cell migration has not happened yet.
2567                     # TODO(alaski): Make this a failure case after we put in
2568                     # a block that requires migrating to cellsv2.
2569                     instance = objects.Instance.get_by_uuid(
2570                         context, instance.uuid, expected_attrs=expected_attrs)
2571                     instance.update(updates)
2572                     instance.save()
2573         return instance
2574 
2575     # NOTE(melwitt): We don't check instance lock for backup because lock is
2576     #                intended to prevent accidental change/delete of instances
2577     @check_instance_cell
2578     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
2579                                     vm_states.PAUSED, vm_states.SUSPENDED])
2580     def backup(self, context, instance, name, backup_type, rotation,
2581                extra_properties=None):
2582         """Backup the given instance
2583 
2584         :param instance: nova.objects.instance.Instance object
2585         :param name: name of the backup
2586         :param backup_type: 'daily' or 'weekly'
2587         :param rotation: int representing how many backups to keep around;
2588             None if rotation shouldn't be used (as in the case of snapshots)
2589         :param extra_properties: dict of extra image properties to include
2590                                  when creating the image.
2591         :returns: A dict containing image metadata
2592         """
2593         props_copy = dict(extra_properties, backup_type=backup_type)
2594 
2595         if compute_utils.is_volume_backed_instance(context, instance):
2596             LOG.info("It's not supported to backup volume backed "
2597                      "instance.", instance=instance)
2598             raise exception.InvalidRequest(
2599                 _('Backup is not supported for volume-backed instances.'))
2600         else:
2601             image_meta = self._create_image(context, instance,
2602                                             name, 'backup',
2603                                             extra_properties=props_copy)
2604 
2605         # NOTE(comstud): Any changes to this method should also be made
2606         # to the backup_instance() method in nova/cells/messaging.py
2607 
2608         instance.task_state = task_states.IMAGE_BACKUP
2609         instance.save(expected_task_state=[None])
2610 
2611         self.compute_rpcapi.backup_instance(context, instance,
2612                                             image_meta['id'],
2613                                             backup_type,
2614                                             rotation)
2615         return image_meta
2616 
2617     # NOTE(melwitt): We don't check instance lock for snapshot because lock is
2618     #                intended to prevent accidental change/delete of instances
2619     @check_instance_cell
2620     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
2621                                     vm_states.PAUSED, vm_states.SUSPENDED])
2622     def snapshot(self, context, instance, name, extra_properties=None):
2623         """Snapshot the given instance.
2624 
2625         :param instance: nova.objects.instance.Instance object
2626         :param name: name of the snapshot
2627         :param extra_properties: dict of extra image properties to include
2628                                  when creating the image.
2629         :returns: A dict containing image metadata
2630         """
2631         image_meta = self._create_image(context, instance, name,
2632                                         'snapshot',
2633                                         extra_properties=extra_properties)
2634 
2635         # NOTE(comstud): Any changes to this method should also be made
2636         # to the snapshot_instance() method in nova/cells/messaging.py
2637         instance.task_state = task_states.IMAGE_SNAPSHOT_PENDING
2638         try:
2639             instance.save(expected_task_state=[None])
2640         except (exception.InstanceNotFound,
2641                 exception.UnexpectedDeletingTaskStateError) as ex:
2642             # Changing the instance task state to use in raising the
2643             # InstanceInvalidException below
2644             LOG.debug('Instance disappeared during snapshot.',
2645                       instance=instance)
2646             try:
2647                 image_id = image_meta['id']
2648                 self.image_api.delete(context, image_id)
2649                 LOG.info('Image %s deleted because instance '
2650                          'deleted before snapshot started.',
2651                          image_id, instance=instance)
2652             except exception.ImageNotFound:
2653                 pass
2654             except Exception as exc:
2655                 LOG.warning("Error while trying to clean up image %(img_id)s: "
2656                             "%(error_msg)s",
2657                             {"img_id": image_meta['id'],
2658                              "error_msg": six.text_type(exc)})
2659             attr = 'task_state'
2660             state = task_states.DELETING
2661             if type(ex) == exception.InstanceNotFound:
2662                 attr = 'vm_state'
2663                 state = vm_states.DELETED
2664             raise exception.InstanceInvalidState(attr=attr,
2665                                            instance_uuid=instance.uuid,
2666                                            state=state,
2667                                            method='snapshot')
2668 
2669         self.compute_rpcapi.snapshot_instance(context, instance,
2670                                               image_meta['id'])
2671 
2672         return image_meta
2673 
2674     def _create_image(self, context, instance, name, image_type,
2675                       extra_properties=None):
2676         """Create new image entry in the image service.  This new image
2677         will be reserved for the compute manager to upload a snapshot
2678         or backup.
2679 
2680         :param context: security context
2681         :param instance: nova.objects.instance.Instance object
2682         :param name: string for name of the snapshot
2683         :param image_type: snapshot | backup
2684         :param extra_properties: dict of extra image properties to include
2685 
2686         """
2687         properties = {
2688             'instance_uuid': instance.uuid,
2689             'user_id': str(context.user_id),
2690             'image_type': image_type,
2691         }
2692         properties.update(extra_properties or {})
2693 
2694         image_meta = self._initialize_instance_snapshot_metadata(
2695             instance, name, properties)
2696         # if we're making a snapshot, omit the disk and container formats,
2697         # since the image may have been converted to another format, and the
2698         # original values won't be accurate.  The driver will populate these
2699         # with the correct values later, on image upload.
2700         if image_type == 'snapshot':
2701             image_meta.pop('disk_format', None)
2702             image_meta.pop('container_format', None)
2703         return self.image_api.create(context, image_meta)
2704 
2705     def _initialize_instance_snapshot_metadata(self, instance, name,
2706                                                extra_properties=None):
2707         """Initialize new metadata for a snapshot of the given instance.
2708 
2709         :param instance: nova.objects.instance.Instance object
2710         :param name: string for name of the snapshot
2711         :param extra_properties: dict of extra metadata properties to include
2712 
2713         :returns: the new instance snapshot metadata
2714         """
2715         image_meta = utils.get_image_from_system_metadata(
2716             instance.system_metadata)
2717         image_meta.update({'name': name,
2718                            'is_public': False})
2719 
2720         # Delete properties that are non-inheritable
2721         properties = image_meta['properties']
2722         for key in CONF.non_inheritable_image_properties:
2723             properties.pop(key, None)
2724 
2725         # The properties in extra_properties have precedence
2726         properties.update(extra_properties or {})
2727 
2728         return image_meta
2729 
2730     # NOTE(melwitt): We don't check instance lock for snapshot because lock is
2731     #                intended to prevent accidental change/delete of instances
2732     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
2733                                     vm_states.SUSPENDED])
2734     def snapshot_volume_backed(self, context, instance, name,
2735                                extra_properties=None):
2736         """Snapshot the given volume-backed instance.
2737 
2738         :param instance: nova.objects.instance.Instance object
2739         :param name: name of the backup or snapshot
2740         :param extra_properties: dict of extra image properties to include
2741 
2742         :returns: the new image metadata
2743         """
2744         image_meta = self._initialize_instance_snapshot_metadata(
2745             instance, name, extra_properties)
2746         # the new image is simply a bucket of properties (particularly the
2747         # block device mapping, kernel and ramdisk IDs) with no image data,
2748         # hence the zero size
2749         image_meta['size'] = 0
2750         for attr in ('container_format', 'disk_format'):
2751             image_meta.pop(attr, None)
2752         properties = image_meta['properties']
2753         # clean properties before filling
2754         for key in ('block_device_mapping', 'bdm_v2', 'root_device_name'):
2755             properties.pop(key, None)
2756         if instance.root_device_name:
2757             properties['root_device_name'] = instance.root_device_name
2758 
2759         quiesced = False
2760         if instance.vm_state == vm_states.ACTIVE:
2761             try:
2762                 self.compute_rpcapi.quiesce_instance(context, instance)
2763                 quiesced = True
2764             except (exception.InstanceQuiesceNotSupported,
2765                     exception.QemuGuestAgentNotEnabled,
2766                     exception.NovaException, NotImplementedError) as err:
2767                 if strutils.bool_from_string(instance.system_metadata.get(
2768                         'image_os_require_quiesce')):
2769                     raise
2770                 else:
2771                     LOG.info('Skipping quiescing instance: %(reason)s.',
2772                              {'reason': err},
2773                              instance=instance)
2774 
2775         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
2776                 context, instance.uuid)
2777 
2778         mapping = []
2779         for bdm in bdms:
2780             if bdm.no_device:
2781                 continue
2782 
2783             if bdm.is_volume:
2784                 # create snapshot based on volume_id
2785                 volume = self.volume_api.get(context, bdm.volume_id)
2786                 # NOTE(yamahata): Should we wait for snapshot creation?
2787                 #                 Linux LVM snapshot creation completes in
2788                 #                 short time, it doesn't matter for now.
2789                 name = _('snapshot for %s') % image_meta['name']
2790                 LOG.debug('Creating snapshot from volume %s.', volume['id'],
2791                           instance=instance)
2792                 snapshot = self.volume_api.create_snapshot_force(
2793                     context, volume['id'], name, volume['display_description'])
2794                 mapping_dict = block_device.snapshot_from_bdm(snapshot['id'],
2795                                                               bdm)
2796                 mapping_dict = mapping_dict.get_image_mapping()
2797             else:
2798                 mapping_dict = bdm.get_image_mapping()
2799 
2800             mapping.append(mapping_dict)
2801 
2802         if quiesced:
2803             self.compute_rpcapi.unquiesce_instance(context, instance, mapping)
2804 
2805         if mapping:
2806             properties['block_device_mapping'] = mapping
2807             properties['bdm_v2'] = True
2808 
2809         return self.image_api.create(context, image_meta)
2810 
2811     @check_instance_lock
2812     def reboot(self, context, instance, reboot_type):
2813         """Reboot the given instance."""
2814         if reboot_type == 'SOFT':
2815             self._soft_reboot(context, instance)
2816         else:
2817             self._hard_reboot(context, instance)
2818 
2819     @check_instance_state(vm_state=set(vm_states.ALLOW_SOFT_REBOOT),
2820                           task_state=[None])
2821     def _soft_reboot(self, context, instance):
2822         expected_task_state = [None]
2823         instance.task_state = task_states.REBOOTING
2824         instance.save(expected_task_state=expected_task_state)
2825 
2826         self._record_action_start(context, instance, instance_actions.REBOOT)
2827 
2828         self.compute_rpcapi.reboot_instance(context, instance=instance,
2829                                             block_device_info=None,
2830                                             reboot_type='SOFT')
2831 
2832     @check_instance_state(vm_state=set(vm_states.ALLOW_HARD_REBOOT),
2833                           task_state=task_states.ALLOW_REBOOT)
2834     def _hard_reboot(self, context, instance):
2835         instance.task_state = task_states.REBOOTING_HARD
2836         expected_task_state = [None,
2837                                task_states.REBOOTING,
2838                                task_states.REBOOT_PENDING,
2839                                task_states.REBOOT_STARTED,
2840                                task_states.REBOOTING_HARD,
2841                                task_states.RESUMING,
2842                                task_states.UNPAUSING,
2843                                task_states.SUSPENDING]
2844         instance.save(expected_task_state = expected_task_state)
2845 
2846         self._record_action_start(context, instance, instance_actions.REBOOT)
2847 
2848         self.compute_rpcapi.reboot_instance(context, instance=instance,
2849                                             block_device_info=None,
2850                                             reboot_type='HARD')
2851 
2852     @check_instance_lock
2853     @check_instance_cell
2854     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
2855                                     vm_states.ERROR])
2856     def rebuild(self, context, instance, image_href, admin_password,
2857                 files_to_inject=None, **kwargs):
2858         """Rebuild the given instance with the provided attributes."""
2859         files_to_inject = files_to_inject or []
2860         metadata = kwargs.get('metadata', {})
2861         preserve_ephemeral = kwargs.get('preserve_ephemeral', False)
2862         auto_disk_config = kwargs.get('auto_disk_config')
2863 
2864         if 'key_name' in kwargs:
2865             key_name = kwargs.pop('key_name')
2866             if key_name:
2867                 # NOTE(liuyulong): we are intentionally using the user_id from
2868                 # the request context rather than the instance.user_id because
2869                 # users own keys but instances are owned by projects, and
2870                 # another user in the same project can rebuild an instance
2871                 # even if they didn't create it.
2872                 key_pair = objects.KeyPair.get_by_name(context,
2873                                                        context.user_id,
2874                                                        key_name)
2875                 instance.key_name = key_pair.name
2876                 instance.key_data = key_pair.public_key
2877                 instance.keypairs = objects.KeyPairList(objects=[key_pair])
2878             else:
2879                 instance.key_name = None
2880                 instance.key_data = None
2881                 instance.keypairs = objects.KeyPairList(objects=[])
2882 
2883         image_id, image = self._get_image(context, image_href)
2884         self._check_auto_disk_config(image=image, **kwargs)
2885 
2886         flavor = instance.get_flavor()
2887         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
2888             context, instance.uuid)
2889         root_bdm = compute_utils.get_root_bdm(context, instance, bdms)
2890 
2891         # Check to see if the image is changing and we have a volume-backed
2892         # server. The compute doesn't support changing the image in the
2893         # root disk of a volume-backed server, so we need to just fail fast.
2894         is_volume_backed = compute_utils.is_volume_backed_instance(
2895             context, instance, bdms)
2896         if is_volume_backed:
2897             # For boot from volume, instance.image_ref is empty, so we need to
2898             # query the image from the volume.
2899             if root_bdm is None:
2900                 # This shouldn't happen and is an error, we need to fail. This
2901                 # is not the users fault, it's an internal error. Without a
2902                 # root BDM we have no way of knowing the backing volume (or
2903                 # image in that volume) for this instance.
2904                 raise exception.NovaException(
2905                     _('Unable to find root block device mapping for '
2906                       'volume-backed instance.'))
2907 
2908             volume = self.volume_api.get(context, root_bdm.volume_id)
2909             volume_image_metadata = volume.get('volume_image_metadata', {})
2910             orig_image_ref = volume_image_metadata.get('image_id')
2911 
2912             if orig_image_ref != image_href:
2913                 # Leave a breadcrumb.
2914                 LOG.debug('Requested to rebuild instance with a new image %s '
2915                           'for a volume-backed server with image %s in its '
2916                           'root volume which is not supported.', image_href,
2917                           orig_image_ref, instance=instance)
2918                 msg = _('Unable to rebuild with a different image for a '
2919                         'volume-backed server.')
2920                 raise exception.ImageUnacceptable(
2921                     image_id=image_href, reason=msg)
2922         else:
2923             orig_image_ref = instance.image_ref
2924 
2925         self._checks_for_create_and_rebuild(context, image_id, image,
2926                 flavor, metadata, files_to_inject, root_bdm)
2927 
2928         kernel_id, ramdisk_id = self._handle_kernel_and_ramdisk(
2929                 context, None, None, image)
2930 
2931         def _reset_image_metadata():
2932             """Remove old image properties that we're storing as instance
2933             system metadata.  These properties start with 'image_'.
2934             Then add the properties for the new image.
2935             """
2936             # FIXME(comstud): There's a race condition here in that if
2937             # the system_metadata for this instance is updated after
2938             # we do the previous save() and before we update.. those
2939             # other updates will be lost. Since this problem exists in
2940             # a lot of other places, I think it should be addressed in
2941             # a DB layer overhaul.
2942 
2943             orig_sys_metadata = dict(instance.system_metadata)
2944             # Remove the old keys
2945             for key in list(instance.system_metadata.keys()):
2946                 if key.startswith(utils.SM_IMAGE_PROP_PREFIX):
2947                     del instance.system_metadata[key]
2948 
2949             # Add the new ones
2950             new_sys_metadata = utils.get_system_metadata_from_image(
2951                 image, flavor)
2952 
2953             instance.system_metadata.update(new_sys_metadata)
2954             instance.save()
2955             return orig_sys_metadata
2956 
2957         # Since image might have changed, we may have new values for
2958         # os_type, vm_mode, etc
2959         options_from_image = self._inherit_properties_from_image(
2960                 image, auto_disk_config)
2961         instance.update(options_from_image)
2962 
2963         instance.task_state = task_states.REBUILDING
2964         # An empty instance.image_ref is currently used as an indication
2965         # of BFV.  Preserve that over a rebuild to not break users.
2966         if not is_volume_backed:
2967             instance.image_ref = image_href
2968         instance.kernel_id = kernel_id or ""
2969         instance.ramdisk_id = ramdisk_id or ""
2970         instance.progress = 0
2971         instance.update(kwargs)
2972         instance.save(expected_task_state=[None])
2973 
2974         # On a rebuild, since we're potentially changing images, we need to
2975         # wipe out the old image properties that we're storing as instance
2976         # system metadata... and copy in the properties for the new image.
2977         orig_sys_metadata = _reset_image_metadata()
2978 
2979         self._record_action_start(context, instance, instance_actions.REBUILD)
2980 
2981         # NOTE(sbauza): The migration script we provided in Newton should make
2982         # sure that all our instances are currently migrated to have an
2983         # attached RequestSpec object but let's consider that the operator only
2984         # half migrated all their instances in the meantime.
2985         host = instance.host
2986         try:
2987             request_spec = objects.RequestSpec.get_by_instance_uuid(
2988                 context, instance.uuid)
2989             # If a new image is provided on rebuild, we will need to run
2990             # through the scheduler again, but we want the instance to be
2991             # rebuilt on the same host it's already on.
2992             if orig_image_ref != image_href:
2993                 # We have to modify the request spec that goes to the scheduler
2994                 # to contain the new image. We persist this since we've already
2995                 # changed the instance.image_ref above so we're being
2996                 # consistent.
2997                 request_spec.image = objects.ImageMeta.from_dict(image)
2998                 request_spec.save()
2999                 if 'scheduler_hints' not in request_spec:
3000                     request_spec.scheduler_hints = {}
3001                 # Nuke the id on this so we can't accidentally save
3002                 # this hint hack later
3003                 del request_spec.id
3004 
3005                 # NOTE(danms): Passing host=None tells conductor to
3006                 # call the scheduler. The _nova_check_type hint
3007                 # requires that the scheduler returns only the same
3008                 # host that we are currently on and only checks
3009                 # rebuild-related filters.
3010                 request_spec.scheduler_hints['_nova_check_type'] = ['rebuild']
3011                 request_spec.force_hosts = [instance.host]
3012                 request_spec.force_nodes = [instance.node]
3013                 host = None
3014         except exception.RequestSpecNotFound:
3015             # Some old instances can still have no RequestSpec object attached
3016             # to them, we need to support the old way
3017             request_spec = None
3018 
3019         self.compute_task_api.rebuild_instance(context, instance=instance,
3020                 new_pass=admin_password, injected_files=files_to_inject,
3021                 image_ref=image_href, orig_image_ref=orig_image_ref,
3022                 orig_sys_metadata=orig_sys_metadata, bdms=bdms,
3023                 preserve_ephemeral=preserve_ephemeral, host=host,
3024                 request_spec=request_spec,
3025                 kwargs=kwargs)
3026 
3027     @staticmethod
3028     def _check_quota_for_upsize(context, instance, current_flavor, new_flavor):
3029         project_id, user_id = quotas_obj.ids_from_instance(context,
3030                                                            instance)
3031         # Deltas will be empty if the resize is not an upsize.
3032         deltas = compute_utils.upsize_quota_delta(context, new_flavor,
3033                                                   current_flavor)
3034         if deltas:
3035             try:
3036                 res_deltas = {'cores': deltas.get('cores', 0),
3037                               'ram': deltas.get('ram', 0)}
3038                 objects.Quotas.check_deltas(context, res_deltas,
3039                                             project_id, user_id=user_id,
3040                                             check_project_id=project_id,
3041                                             check_user_id=user_id)
3042             except exception.OverQuota as exc:
3043                 quotas = exc.kwargs['quotas']
3044                 overs = exc.kwargs['overs']
3045                 usages = exc.kwargs['usages']
3046                 headroom = compute_utils.get_headroom(quotas, usages,
3047                                                       deltas)
3048                 (overs, reqs, total_alloweds,
3049                  useds) = compute_utils.get_over_quota_detail(headroom,
3050                                                               overs,
3051                                                               quotas,
3052                                                               deltas)
3053                 LOG.warning("%(overs)s quota exceeded for %(pid)s,"
3054                             " tried to resize instance.",
3055                             {'overs': overs, 'pid': context.project_id})
3056                 raise exception.TooManyInstances(overs=overs,
3057                                                  req=reqs,
3058                                                  used=useds,
3059                                                  allowed=total_alloweds)
3060 
3061     @check_instance_lock
3062     @check_instance_cell
3063     @check_instance_state(vm_state=[vm_states.RESIZED])
3064     def revert_resize(self, context, instance):
3065         """Reverts a resize, deleting the 'new' instance in the process."""
3066         elevated = context.elevated()
3067         migration = objects.Migration.get_by_instance_and_status(
3068             elevated, instance.uuid, 'finished')
3069 
3070         # If this is a resize down, a revert might go over quota.
3071         self._check_quota_for_upsize(context, instance, instance.flavor,
3072                                      instance.old_flavor)
3073 
3074         instance.task_state = task_states.RESIZE_REVERTING
3075         instance.save(expected_task_state=[None])
3076 
3077         migration.status = 'reverting'
3078         migration.save()
3079 
3080         self._record_action_start(context, instance,
3081                                   instance_actions.REVERT_RESIZE)
3082 
3083         # TODO(melwitt): We're not rechecking for strict quota here to guard
3084         # against going over quota during a race at this time because the
3085         # resource consumption for this operation is written to the database
3086         # by compute.
3087         self.compute_rpcapi.revert_resize(context, instance,
3088                                           migration,
3089                                           migration.dest_compute)
3090 
3091     @check_instance_lock
3092     @check_instance_cell
3093     @check_instance_state(vm_state=[vm_states.RESIZED])
3094     def confirm_resize(self, context, instance, migration=None):
3095         """Confirms a migration/resize and deletes the 'old' instance."""
3096         elevated = context.elevated()
3097         # NOTE(melwitt): We're not checking quota here because there isn't a
3098         # change in resource usage when confirming a resize. Resource
3099         # consumption for resizes are written to the database by compute, so
3100         # a confirm resize is just a clean up of the migration objects and a
3101         # state change in compute.
3102         if migration is None:
3103             migration = objects.Migration.get_by_instance_and_status(
3104                 elevated, instance.uuid, 'finished')
3105 
3106         migration.status = 'confirming'
3107         migration.save()
3108 
3109         self._record_action_start(context, instance,
3110                                   instance_actions.CONFIRM_RESIZE)
3111 
3112         self.compute_rpcapi.confirm_resize(context,
3113                                            instance,
3114                                            migration,
3115                                            migration.source_compute)
3116 
3117     @staticmethod
3118     def _resize_cells_support(context, instance,
3119                               current_instance_type, new_instance_type):
3120         """Special API cell logic for resize."""
3121         # NOTE(johannes/comstud): The API cell needs a local migration
3122         # record for later resize_confirm and resize_reverts.
3123         # We don't need source and/or destination
3124         # information, just the old and new flavors. Status is set to
3125         # 'finished' since nothing else will update the status along
3126         # the way.
3127         mig = objects.Migration(context=context.elevated())
3128         mig.instance_uuid = instance.uuid
3129         mig.old_instance_type_id = current_instance_type['id']
3130         mig.new_instance_type_id = new_instance_type['id']
3131         mig.status = 'finished'
3132         mig.migration_type = (
3133             mig.old_instance_type_id != mig.new_instance_type_id and
3134             'resize' or 'migration')
3135         mig.create()
3136 
3137     @check_instance_lock
3138     @check_instance_cell
3139     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED])
3140     def resize(self, context, instance, flavor_id=None, clean_shutdown=True,
3141                host_name=None, **extra_instance_updates):
3142         """Resize (ie, migrate) a running instance.
3143 
3144         If flavor_id is None, the process is considered a migration, keeping
3145         the original flavor_id. If flavor_id is not None, the instance should
3146         be migrated to a new host and resized to the new flavor_id.
3147         host_name is always None in the resize case.
3148         host_name can be set in the cold migration case only.
3149         """
3150         if host_name is not None:
3151             # Check whether host exists or not.
3152             node = objects.ComputeNode.get_first_node_by_host_for_old_compat(
3153                 context, host_name, use_slave=True)
3154 
3155             # Cannot migrate to the host where the instance exists
3156             # because it is useless.
3157             if host_name == instance.host:
3158                 raise exception.CannotMigrateToSameHost()
3159 
3160         self._check_auto_disk_config(instance, **extra_instance_updates)
3161 
3162         current_instance_type = instance.get_flavor()
3163 
3164         # If flavor_id is not provided, only migrate the instance.
3165         if not flavor_id:
3166             LOG.debug("flavor_id is None. Assuming migration.",
3167                       instance=instance)
3168             new_instance_type = current_instance_type
3169         else:
3170             new_instance_type = flavors.get_flavor_by_flavor_id(
3171                     flavor_id, read_deleted="no")
3172             if (new_instance_type.get('root_gb') == 0 and
3173                 current_instance_type.get('root_gb') != 0 and
3174                 not compute_utils.is_volume_backed_instance(context,
3175                     instance)):
3176                 reason = _('Resize to zero disk flavor is not allowed.')
3177                 raise exception.CannotResizeDisk(reason=reason)
3178 
3179         if not new_instance_type:
3180             raise exception.FlavorNotFound(flavor_id=flavor_id)
3181 
3182         current_instance_type_name = current_instance_type['name']
3183         new_instance_type_name = new_instance_type['name']
3184         LOG.debug("Old instance type %(current_instance_type_name)s, "
3185                   "new instance type %(new_instance_type_name)s",
3186                   {'current_instance_type_name': current_instance_type_name,
3187                    'new_instance_type_name': new_instance_type_name},
3188                   instance=instance)
3189 
3190         same_instance_type = (current_instance_type['id'] ==
3191                               new_instance_type['id'])
3192 
3193         # NOTE(sirp): We don't want to force a customer to change their flavor
3194         # when Ops is migrating off of a failed host.
3195         if not same_instance_type and new_instance_type.get('disabled'):
3196             raise exception.FlavorNotFound(flavor_id=flavor_id)
3197 
3198         if same_instance_type and flavor_id and self.cell_type != 'compute':
3199             raise exception.CannotResizeToSameFlavor()
3200 
3201         # ensure there is sufficient headroom for upsizes
3202         if flavor_id:
3203             self._check_quota_for_upsize(context, instance,
3204                                          current_instance_type,
3205                                          new_instance_type)
3206 
3207         instance.task_state = task_states.RESIZE_PREP
3208         instance.progress = 0
3209         instance.update(extra_instance_updates)
3210         instance.save(expected_task_state=[None])
3211 
3212         filter_properties = {'ignore_hosts': []}
3213 
3214         if not CONF.allow_resize_to_same_host:
3215             filter_properties['ignore_hosts'].append(instance.host)
3216 
3217         if self.cell_type == 'api':
3218             # Create migration record.
3219             self._resize_cells_support(context, instance,
3220                                        current_instance_type,
3221                                        new_instance_type)
3222 
3223         if not flavor_id:
3224             self._record_action_start(context, instance,
3225                                       instance_actions.MIGRATE)
3226         else:
3227             self._record_action_start(context, instance,
3228                                       instance_actions.RESIZE)
3229 
3230         # NOTE(sbauza): The migration script we provided in Newton should make
3231         # sure that all our instances are currently migrated to have an
3232         # attached RequestSpec object but let's consider that the operator only
3233         # half migrated all their instances in the meantime.
3234         try:
3235             request_spec = objects.RequestSpec.get_by_instance_uuid(
3236                 context, instance.uuid)
3237             request_spec.ignore_hosts = filter_properties['ignore_hosts']
3238         except exception.RequestSpecNotFound:
3239             # Some old instances can still have no RequestSpec object attached
3240             # to them, we need to support the old way
3241             if host_name is not None:
3242                 # If there is no request spec we cannot honor the request
3243                 # and we need to fail.
3244                 raise exception.CannotMigrateWithTargetHost()
3245             request_spec = None
3246 
3247         # TODO(melwitt): We're not rechecking for strict quota here to guard
3248         # against going over quota during a race at this time because the
3249         # resource consumption for this operation is written to the database
3250         # by compute.
3251         scheduler_hint = {'filter_properties': filter_properties}
3252 
3253         if request_spec:
3254             if host_name is None:
3255                 # If 'host_name' is not specified,
3256                 # clear the 'requested_destination' field of the RequestSpec.
3257                 request_spec.requested_destination = None
3258             else:
3259                 # Set the host and the node so that the scheduler will
3260                 # validate them.
3261                 # TODO(takashin): It will be added to check whether
3262                 # the specified host is within the same cell as
3263                 # the instance or not. If not, raise specific error message
3264                 # that is clear to the caller.
3265                 request_spec.requested_destination = objects.Destination(
3266                     host=node.host, node=node.hypervisor_hostname)
3267 
3268         self.compute_task_api.resize_instance(context, instance,
3269                 extra_instance_updates, scheduler_hint=scheduler_hint,
3270                 flavor=new_instance_type,
3271                 clean_shutdown=clean_shutdown,
3272                 request_spec=request_spec)
3273 
3274     @check_instance_lock
3275     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
3276                                     vm_states.PAUSED, vm_states.SUSPENDED])
3277     def shelve(self, context, instance, clean_shutdown=True):
3278         """Shelve an instance.
3279 
3280         Shuts down an instance and frees it up to be removed from the
3281         hypervisor.
3282         """
3283         instance.task_state = task_states.SHELVING
3284         instance.save(expected_task_state=[None])
3285 
3286         self._record_action_start(context, instance, instance_actions.SHELVE)
3287 
3288         if not compute_utils.is_volume_backed_instance(context, instance):
3289             name = '%s-shelved' % instance.display_name
3290             image_meta = self._create_image(context, instance, name,
3291                     'snapshot')
3292             image_id = image_meta['id']
3293             self.compute_rpcapi.shelve_instance(context, instance=instance,
3294                     image_id=image_id, clean_shutdown=clean_shutdown)
3295         else:
3296             self.compute_rpcapi.shelve_offload_instance(context,
3297                     instance=instance, clean_shutdown=clean_shutdown)
3298 
3299     @check_instance_lock
3300     @check_instance_state(vm_state=[vm_states.SHELVED])
3301     def shelve_offload(self, context, instance, clean_shutdown=True):
3302         """Remove a shelved instance from the hypervisor."""
3303         instance.task_state = task_states.SHELVING_OFFLOADING
3304         instance.save(expected_task_state=[None])
3305 
3306         self.compute_rpcapi.shelve_offload_instance(context, instance=instance,
3307             clean_shutdown=clean_shutdown)
3308 
3309     @check_instance_lock
3310     @check_instance_state(vm_state=[vm_states.SHELVED,
3311         vm_states.SHELVED_OFFLOADED])
3312     def unshelve(self, context, instance):
3313         """Restore a shelved instance."""
3314         instance.task_state = task_states.UNSHELVING
3315         instance.save(expected_task_state=[None])
3316 
3317         self._record_action_start(context, instance, instance_actions.UNSHELVE)
3318 
3319         try:
3320             request_spec = objects.RequestSpec.get_by_instance_uuid(
3321                 context, instance.uuid)
3322         except exception.RequestSpecNotFound:
3323             # Some old instances can still have no RequestSpec object attached
3324             # to them, we need to support the old way
3325             request_spec = None
3326         self.compute_task_api.unshelve_instance(context, instance,
3327                                                 request_spec)
3328 
3329     @check_instance_lock
3330     def add_fixed_ip(self, context, instance, network_id):
3331         """Add fixed_ip from specified network to given instance."""
3332         self.compute_rpcapi.add_fixed_ip_to_instance(context,
3333                 instance=instance, network_id=network_id)
3334 
3335     @check_instance_lock
3336     def remove_fixed_ip(self, context, instance, address):
3337         """Remove fixed_ip from specified network to given instance."""
3338         self.compute_rpcapi.remove_fixed_ip_from_instance(context,
3339                 instance=instance, address=address)
3340 
3341     @check_instance_lock
3342     @check_instance_cell
3343     @check_instance_state(vm_state=[vm_states.ACTIVE])
3344     def pause(self, context, instance):
3345         """Pause the given instance."""
3346         instance.task_state = task_states.PAUSING
3347         instance.save(expected_task_state=[None])
3348         self._record_action_start(context, instance, instance_actions.PAUSE)
3349         self.compute_rpcapi.pause_instance(context, instance)
3350 
3351     @check_instance_lock
3352     @check_instance_cell
3353     @check_instance_state(vm_state=[vm_states.PAUSED])
3354     def unpause(self, context, instance):
3355         """Unpause the given instance."""
3356         instance.task_state = task_states.UNPAUSING
3357         instance.save(expected_task_state=[None])
3358         self._record_action_start(context, instance, instance_actions.UNPAUSE)
3359         self.compute_rpcapi.unpause_instance(context, instance)
3360 
3361     @check_instance_host
3362     def get_diagnostics(self, context, instance):
3363         """Retrieve diagnostics for the given instance."""
3364         return self.compute_rpcapi.get_diagnostics(context, instance=instance)
3365 
3366     @check_instance_host
3367     def get_instance_diagnostics(self, context, instance):
3368         """Retrieve diagnostics for the given instance."""
3369         return self.compute_rpcapi.get_instance_diagnostics(context,
3370                                                             instance=instance)
3371 
3372     @check_instance_lock
3373     @check_instance_cell
3374     @check_instance_state(vm_state=[vm_states.ACTIVE])
3375     def suspend(self, context, instance):
3376         """Suspend the given instance."""
3377         instance.task_state = task_states.SUSPENDING
3378         instance.save(expected_task_state=[None])
3379         self._record_action_start(context, instance, instance_actions.SUSPEND)
3380         self.compute_rpcapi.suspend_instance(context, instance)
3381 
3382     @check_instance_lock
3383     @check_instance_cell
3384     @check_instance_state(vm_state=[vm_states.SUSPENDED])
3385     def resume(self, context, instance):
3386         """Resume the given instance."""
3387         instance.task_state = task_states.RESUMING
3388         instance.save(expected_task_state=[None])
3389         self._record_action_start(context, instance, instance_actions.RESUME)
3390         self.compute_rpcapi.resume_instance(context, instance)
3391 
3392     @check_instance_lock
3393     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
3394                                     vm_states.ERROR])
3395     def rescue(self, context, instance, rescue_password=None,
3396                rescue_image_ref=None, clean_shutdown=True):
3397         """Rescue the given instance."""
3398 
3399         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
3400                     context, instance.uuid)
3401         for bdm in bdms:
3402             if bdm.volume_id:
3403                 vol = self.volume_api.get(context, bdm.volume_id)
3404                 self.volume_api.check_attached(context, vol)
3405         if compute_utils.is_volume_backed_instance(context, instance, bdms):
3406             reason = _("Cannot rescue a volume-backed instance")
3407             raise exception.InstanceNotRescuable(instance_id=instance.uuid,
3408                                                  reason=reason)
3409 
3410         instance.task_state = task_states.RESCUING
3411         instance.save(expected_task_state=[None])
3412 
3413         self._record_action_start(context, instance, instance_actions.RESCUE)
3414 
3415         self.compute_rpcapi.rescue_instance(context, instance=instance,
3416             rescue_password=rescue_password, rescue_image_ref=rescue_image_ref,
3417             clean_shutdown=clean_shutdown)
3418 
3419     @check_instance_lock
3420     @check_instance_state(vm_state=[vm_states.RESCUED])
3421     def unrescue(self, context, instance):
3422         """Unrescue the given instance."""
3423         instance.task_state = task_states.UNRESCUING
3424         instance.save(expected_task_state=[None])
3425 
3426         self._record_action_start(context, instance, instance_actions.UNRESCUE)
3427 
3428         self.compute_rpcapi.unrescue_instance(context, instance=instance)
3429 
3430     @check_instance_lock
3431     @check_instance_cell
3432     @check_instance_state(vm_state=[vm_states.ACTIVE])
3433     def set_admin_password(self, context, instance, password=None):
3434         """Set the root/admin password for the given instance.
3435 
3436         @param context: Nova auth context.
3437         @param instance: Nova instance object.
3438         @param password: The admin password for the instance.
3439         """
3440         instance.task_state = task_states.UPDATING_PASSWORD
3441         instance.save(expected_task_state=[None])
3442 
3443         self._record_action_start(context, instance,
3444                                   instance_actions.CHANGE_PASSWORD)
3445 
3446         self.compute_rpcapi.set_admin_password(context,
3447                                                instance=instance,
3448                                                new_pass=password)
3449 
3450     @check_instance_host
3451     @reject_instance_state(
3452         task_state=[task_states.DELETING, task_states.MIGRATING])
3453     def get_vnc_console(self, context, instance, console_type):
3454         """Get a url to an instance Console."""
3455         connect_info = self.compute_rpcapi.get_vnc_console(context,
3456                 instance=instance, console_type=console_type)
3457 
3458         self.consoleauth_rpcapi.authorize_console(context,
3459                 connect_info['token'], console_type,
3460                 connect_info['host'], connect_info['port'],
3461                 connect_info['internal_access_path'], instance.uuid,
3462                 access_url=connect_info['access_url'])
3463 
3464         return {'url': connect_info['access_url']}
3465 
3466     @check_instance_host
3467     def get_vnc_connect_info(self, context, instance, console_type):
3468         """Used in a child cell to get console info."""
3469         connect_info = self.compute_rpcapi.get_vnc_console(context,
3470                 instance=instance, console_type=console_type)
3471         return connect_info
3472 
3473     @check_instance_host
3474     @reject_instance_state(
3475         task_state=[task_states.DELETING, task_states.MIGRATING])
3476     def get_spice_console(self, context, instance, console_type):
3477         """Get a url to an instance Console."""
3478         connect_info = self.compute_rpcapi.get_spice_console(context,
3479                 instance=instance, console_type=console_type)
3480         self.consoleauth_rpcapi.authorize_console(context,
3481                 connect_info['token'], console_type,
3482                 connect_info['host'], connect_info['port'],
3483                 connect_info['internal_access_path'], instance.uuid,
3484                 access_url=connect_info['access_url'])
3485 
3486         return {'url': connect_info['access_url']}
3487 
3488     @check_instance_host
3489     def get_spice_connect_info(self, context, instance, console_type):
3490         """Used in a child cell to get console info."""
3491         connect_info = self.compute_rpcapi.get_spice_console(context,
3492                 instance=instance, console_type=console_type)
3493         return connect_info
3494 
3495     @check_instance_host
3496     @reject_instance_state(
3497         task_state=[task_states.DELETING, task_states.MIGRATING])
3498     def get_rdp_console(self, context, instance, console_type):
3499         """Get a url to an instance Console."""
3500         connect_info = self.compute_rpcapi.get_rdp_console(context,
3501                 instance=instance, console_type=console_type)
3502         self.consoleauth_rpcapi.authorize_console(context,
3503                 connect_info['token'], console_type,
3504                 connect_info['host'], connect_info['port'],
3505                 connect_info['internal_access_path'], instance.uuid,
3506                 access_url=connect_info['access_url'])
3507 
3508         return {'url': connect_info['access_url']}
3509 
3510     @check_instance_host
3511     def get_rdp_connect_info(self, context, instance, console_type):
3512         """Used in a child cell to get console info."""
3513         connect_info = self.compute_rpcapi.get_rdp_console(context,
3514                 instance=instance, console_type=console_type)
3515         return connect_info
3516 
3517     @check_instance_host
3518     @reject_instance_state(
3519         task_state=[task_states.DELETING, task_states.MIGRATING])
3520     def get_serial_console(self, context, instance, console_type):
3521         """Get a url to a serial console."""
3522         connect_info = self.compute_rpcapi.get_serial_console(context,
3523                 instance=instance, console_type=console_type)
3524 
3525         self.consoleauth_rpcapi.authorize_console(context,
3526                 connect_info['token'], console_type,
3527                 connect_info['host'], connect_info['port'],
3528                 connect_info['internal_access_path'], instance.uuid,
3529                 access_url=connect_info['access_url'])
3530         return {'url': connect_info['access_url']}
3531 
3532     @check_instance_host
3533     def get_serial_console_connect_info(self, context, instance, console_type):
3534         """Used in a child cell to get serial console."""
3535         connect_info = self.compute_rpcapi.get_serial_console(context,
3536                 instance=instance, console_type=console_type)
3537         return connect_info
3538 
3539     @check_instance_host
3540     @reject_instance_state(
3541         task_state=[task_states.DELETING, task_states.MIGRATING])
3542     def get_mks_console(self, context, instance, console_type):
3543         """Get a url to a MKS console."""
3544         connect_info = self.compute_rpcapi.get_mks_console(context,
3545                 instance=instance, console_type=console_type)
3546         self.consoleauth_rpcapi.authorize_console(context,
3547                 connect_info['token'], console_type,
3548                 connect_info['host'], connect_info['port'],
3549                 connect_info['internal_access_path'], instance.uuid,
3550                 access_url=connect_info['access_url'])
3551         return {'url': connect_info['access_url']}
3552 
3553     @check_instance_host
3554     def get_console_output(self, context, instance, tail_length=None):
3555         """Get console output for an instance."""
3556         return self.compute_rpcapi.get_console_output(context,
3557                 instance=instance, tail_length=tail_length)
3558 
3559     def lock(self, context, instance):
3560         """Lock the given instance."""
3561         # Only update the lock if we are an admin (non-owner)
3562         is_owner = instance.project_id == context.project_id
3563         if instance.locked and is_owner:
3564             return
3565 
3566         context = context.elevated()
3567         self._record_action_start(context, instance,
3568                                   instance_actions.LOCK)
3569 
3570         @wrap_instance_event(prefix='api')
3571         def lock(self, context, instance):
3572             LOG.debug('Locking', instance=instance)
3573             instance.locked = True
3574             instance.locked_by = 'owner' if is_owner else 'admin'
3575             instance.save()
3576 
3577         lock(self, context, instance)
3578 
3579     def is_expected_locked_by(self, context, instance):
3580         is_owner = instance.project_id == context.project_id
3581         expect_locked_by = 'owner' if is_owner else 'admin'
3582         locked_by = instance.locked_by
3583         if locked_by and locked_by != expect_locked_by:
3584             return False
3585         return True
3586 
3587     def unlock(self, context, instance):
3588         """Unlock the given instance."""
3589         context = context.elevated()
3590         self._record_action_start(context, instance,
3591                                   instance_actions.UNLOCK)
3592 
3593         @wrap_instance_event(prefix='api')
3594         def unlock(self, context, instance):
3595             LOG.debug('Unlocking', instance=instance)
3596             instance.locked = False
3597             instance.locked_by = None
3598             instance.save()
3599 
3600         unlock(self, context, instance)
3601 
3602     @check_instance_lock
3603     @check_instance_cell
3604     def reset_network(self, context, instance):
3605         """Reset networking on the instance."""
3606         self.compute_rpcapi.reset_network(context, instance=instance)
3607 
3608     @check_instance_lock
3609     @check_instance_cell
3610     def inject_network_info(self, context, instance):
3611         """Inject network info for the instance."""
3612         self.compute_rpcapi.inject_network_info(context, instance=instance)
3613 
3614     def _create_volume_bdm(self, context, instance, device, volume_id,
3615                            disk_bus, device_type, is_local_creation=False,
3616                            tag=None):
3617         if is_local_creation:
3618             # when the creation is done locally we can't specify the device
3619             # name as we do not have a way to check that the name specified is
3620             # a valid one.
3621             # We leave the setting of that value when the actual attach
3622             # happens on the compute manager
3623             # NOTE(artom) Local attach (to a shelved-offload instance) cannot
3624             # support device tagging because we have no way to call the compute
3625             # manager to check that it supports device tagging. In fact, we
3626             # don't even know which computer manager the instance will
3627             # eventually end up on when it's unshelved.
3628             volume_bdm = objects.BlockDeviceMapping(
3629                 context=context,
3630                 source_type='volume', destination_type='volume',
3631                 instance_uuid=instance.uuid, boot_index=None,
3632                 volume_id=volume_id,
3633                 device_name=None, guest_format=None,
3634                 disk_bus=disk_bus, device_type=device_type)
3635             volume_bdm.create()
3636         else:
3637             # NOTE(vish): This is done on the compute host because we want
3638             #             to avoid a race where two devices are requested at
3639             #             the same time. When db access is removed from
3640             #             compute, the bdm will be created here and we will
3641             #             have to make sure that they are assigned atomically.
3642             volume_bdm = self.compute_rpcapi.reserve_block_device_name(
3643                 context, instance, device, volume_id, disk_bus=disk_bus,
3644                 device_type=device_type, tag=tag)
3645         return volume_bdm
3646 
3647     def _check_volume_already_attached_to_instance(self, context, instance,
3648                                                    volume_id):
3649         """Avoid attaching the same volume to the same instance twice.
3650 
3651            As the new Cinder flow (microversion 3.44) is handling the checks
3652            differently and allows to attach the same volume to the same
3653            instance twice to enable live_migrate we are checking whether the
3654            BDM already exists for this combination for the new flow and fail
3655            if it does.
3656         """
3657 
3658         try:
3659             objects.BlockDeviceMapping.get_by_volume_and_instance(
3660                 context, volume_id, instance.uuid)
3661 
3662             msg = _("volume %s already attached") % volume_id
3663             raise exception.InvalidVolume(reason=msg)
3664         except exception.VolumeBDMNotFound:
3665             pass
3666 
3667     def _check_attach_and_reserve_volume(self, context, volume_id, instance,
3668                                          bdm):
3669         volume = self.volume_api.get(context, volume_id)
3670         self.volume_api.check_availability_zone(context, volume,
3671                                                 instance=instance)
3672         if 'id' in instance:
3673             # This is a volume attach to an existing instance, so
3674             # we only care about the cell the instance is in.
3675             min_compute_version = objects.Service.get_minimum_version(
3676                 context, 'nova-compute')
3677         else:
3678             # The instance is being created and we don't know which
3679             # cell it's going to land in, so check all cells.
3680             min_compute_version = \
3681                 objects.service.get_minimum_version_all_cells(
3682                     context, ['nova-compute'])
3683         if min_compute_version >= CINDER_V3_ATTACH_MIN_COMPUTE_VERSION:
3684             # Attempt a new style volume attachment, but fallback to old-style
3685             # in case Cinder API 3.44 isn't available.
3686             try:
3687                 attachment_id = self.volume_api.attachment_create(
3688                     context, volume_id, instance.uuid)['id']
3689                 bdm.attachment_id = attachment_id
3690                 # NOTE(ildikov): In case of boot from volume the BDM at this
3691                 # point is not yet created in a cell database, so we can't
3692                 # call save().  When attaching a volume to an existing
3693                 # instance, the instance is already in a cell and the BDM has
3694                 # been created in that same cell so updating here in that case
3695                 # is "ok".
3696                 if bdm.obj_attr_is_set('id'):
3697                     bdm.save()
3698             except exception.CinderAPIVersionNotAvailable:
3699                 LOG.debug('The available Cinder microversion is not high '
3700                           'enough to create new style volume attachment.')
3701                 self.volume_api.reserve_volume(context, volume_id)
3702         else:
3703             LOG.debug('The compute service version is not high enough to '
3704                       'create a new style volume attachment.')
3705             self.volume_api.reserve_volume(context, volume_id)
3706         return volume
3707 
3708     def _attach_volume(self, context, instance, volume_id, device,
3709                        disk_bus, device_type, tag=None):
3710         """Attach an existing volume to an existing instance.
3711 
3712         This method is separated to make it possible for cells version
3713         to override it.
3714         """
3715         volume_bdm = self._create_volume_bdm(
3716             context, instance, device, volume_id, disk_bus=disk_bus,
3717             device_type=device_type, tag=tag)
3718         try:
3719             self._check_attach_and_reserve_volume(context, volume_id, instance,
3720                                                   volume_bdm)
3721             self._record_action_start(
3722                 context, instance, instance_actions.ATTACH_VOLUME)
3723             self.compute_rpcapi.attach_volume(context, instance, volume_bdm)
3724         except Exception:
3725             with excutils.save_and_reraise_exception():
3726                 volume_bdm.destroy()
3727 
3728         return volume_bdm.device_name
3729 
3730     def _attach_volume_shelved_offloaded(self, context, instance, volume_id,
3731                                          device, disk_bus, device_type):
3732         """Attach an existing volume to an instance in shelved offloaded state.
3733 
3734         Attaching a volume for an instance in shelved offloaded state requires
3735         to perform the regular check to see if we can attach and reserve the
3736         volume then we need to call the attach method on the volume API
3737         to mark the volume as 'in-use'.
3738         The instance at this stage is not managed by a compute manager
3739         therefore the actual attachment will be performed once the
3740         instance will be unshelved.
3741         """
3742         @wrap_instance_event(prefix='api')
3743         def attach_volume(self, context, v_id, instance, dev, attachment_id):
3744             if attachment_id:
3745                 # Normally we wouldn't complete an attachment without a host
3746                 # connector, but we do this to make the volume status change
3747                 # to "in-use" to maintain the API semantics with the old flow.
3748                 # When unshelving the instance, the compute service will deal
3749                 # with this disconnected attachment.
3750                 self.volume_api.attachment_complete(context, attachment_id)
3751             else:
3752                 self.volume_api.attach(context,
3753                                        v_id,
3754                                        instance.uuid,
3755                                        dev)
3756 
3757         volume_bdm = self._create_volume_bdm(
3758             context, instance, device, volume_id, disk_bus=disk_bus,
3759             device_type=device_type, is_local_creation=True)
3760         try:
3761             self._check_attach_and_reserve_volume(context, volume_id, instance,
3762                                                   volume_bdm)
3763             self._record_action_start(
3764                 context, instance,
3765                 instance_actions.ATTACH_VOLUME)
3766             attach_volume(self, context, volume_id, instance, device,
3767                           volume_bdm.attachment_id)
3768         except Exception:
3769             with excutils.save_and_reraise_exception():
3770                 volume_bdm.destroy()
3771 
3772         return volume_bdm.device_name
3773 
3774     @check_instance_lock
3775     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
3776                                     vm_states.STOPPED, vm_states.RESIZED,
3777                                     vm_states.SOFT_DELETED, vm_states.SHELVED,
3778                                     vm_states.SHELVED_OFFLOADED])
3779     def attach_volume(self, context, instance, volume_id, device=None,
3780                       disk_bus=None, device_type=None, tag=None):
3781         """Attach an existing volume to an existing instance."""
3782         # NOTE(vish): Fail fast if the device is not going to pass. This
3783         #             will need to be removed along with the test if we
3784         #             change the logic in the manager for what constitutes
3785         #             a valid device.
3786         if device and not block_device.match_device(device):
3787             raise exception.InvalidDevicePath(path=device)
3788 
3789         # Check to see if the computes in this cell can support new-style
3790         # volume attachments.
3791         min_compute_version = objects.Service.get_minimum_version(
3792             context, 'nova-compute')
3793         if min_compute_version >= CINDER_V3_ATTACH_MIN_COMPUTE_VERSION:
3794             try:
3795                 # Check to see if Cinder is new enough to create new-style
3796                 # attachments.
3797                 cinder.is_microversion_supported(context, '3.44')
3798             except exception.CinderAPIVersionNotAvailable:
3799                 pass
3800             else:
3801                 # Make sure the volume isn't already attached to this instance
3802                 # because based on the above checks, we'll use the new style
3803                 # attachment flow in _check_attach_and_reserve_volume and
3804                 # Cinder will allow multiple attachments between the same
3805                 # volume and instance but the old flow API semantics don't
3806                 # allow that so we enforce it here.
3807                 self._check_volume_already_attached_to_instance(context,
3808                                                                 instance,
3809                                                                 volume_id)
3810 
3811         is_shelved_offloaded = instance.vm_state == vm_states.SHELVED_OFFLOADED
3812         if is_shelved_offloaded:
3813             if tag:
3814                 # NOTE(artom) Local attach (to a shelved-offload instance)
3815                 # cannot support device tagging because we have no way to call
3816                 # the compute manager to check that it supports device tagging.
3817                 # In fact, we don't even know which computer manager the
3818                 # instance will eventually end up on when it's unshelved.
3819                 raise exception.VolumeTaggedAttachToShelvedNotSupported()
3820             return self._attach_volume_shelved_offloaded(context,
3821                                                          instance,
3822                                                          volume_id,
3823                                                          device,
3824                                                          disk_bus,
3825                                                          device_type)
3826 
3827         return self._attach_volume(context, instance, volume_id, device,
3828                                    disk_bus, device_type, tag=tag)
3829 
3830     def _detach_volume(self, context, instance, volume):
3831         """Detach volume from instance.
3832 
3833         This method is separated to make it easier for cells version
3834         to override.
3835         """
3836         try:
3837             self.volume_api.begin_detaching(context, volume['id'])
3838         except exception.InvalidInput as exc:
3839             raise exception.InvalidVolume(reason=exc.format_message())
3840         attachments = volume.get('attachments', {})
3841         attachment_id = None
3842         if attachments and instance.uuid in attachments:
3843             attachment_id = attachments[instance.uuid]['attachment_id']
3844         self._record_action_start(
3845             context, instance, instance_actions.DETACH_VOLUME)
3846         self.compute_rpcapi.detach_volume(context, instance=instance,
3847                 volume_id=volume['id'], attachment_id=attachment_id)
3848 
3849     def _detach_volume_shelved_offloaded(self, context, instance, volume):
3850         """Detach a volume from an instance in shelved offloaded state.
3851 
3852         If the instance is shelved offloaded we just need to cleanup volume
3853         calling the volume api detach, the volume api terminate_connection
3854         and delete the bdm record.
3855         If the volume has delete_on_termination option set then we call the
3856         volume api delete as well.
3857         """
3858         @wrap_instance_event(prefix='api')
3859         def detach_volume(self, context, instance, bdms):
3860             self._local_cleanup_bdm_volumes(bdms, instance, context)
3861 
3862         try:
3863             self.volume_api.begin_detaching(context, volume['id'])
3864         except exception.InvalidInput as exc:
3865             raise exception.InvalidVolume(reason=exc.format_message())
3866         bdms = [objects.BlockDeviceMapping.get_by_volume_id(
3867                 context, volume['id'], instance.uuid)]
3868         self._record_action_start(
3869             context, instance,
3870             instance_actions.DETACH_VOLUME)
3871         detach_volume(self, context, instance, bdms)
3872 
3873     @check_instance_lock
3874     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
3875                                     vm_states.STOPPED, vm_states.RESIZED,
3876                                     vm_states.SOFT_DELETED, vm_states.SHELVED,
3877                                     vm_states.SHELVED_OFFLOADED])
3878     def detach_volume(self, context, instance, volume):
3879         """Detach a volume from an instance."""
3880         if instance.vm_state == vm_states.SHELVED_OFFLOADED:
3881             self._detach_volume_shelved_offloaded(context, instance, volume)
3882         else:
3883             self._detach_volume(context, instance, volume)
3884 
3885     @check_instance_lock
3886     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
3887                                     vm_states.SUSPENDED, vm_states.STOPPED,
3888                                     vm_states.RESIZED, vm_states.SOFT_DELETED])
3889     def swap_volume(self, context, instance, old_volume, new_volume):
3890         """Swap volume attached to an instance."""
3891         # The caller likely got the instance from volume['attachments']
3892         # in the first place, but let's sanity check.
3893         if not old_volume.get('attachments', {}).get(instance.uuid):
3894             msg = _("Old volume is attached to a different instance.")
3895             raise exception.InvalidVolume(reason=msg)
3896         if new_volume['attach_status'] == 'attached':
3897             msg = _("New volume must be detached in order to swap.")
3898             raise exception.InvalidVolume(reason=msg)
3899         if int(new_volume['size']) < int(old_volume['size']):
3900             msg = _("New volume must be the same size or larger.")
3901             raise exception.InvalidVolume(reason=msg)
3902         self.volume_api.check_availability_zone(context, new_volume,
3903                                                 instance=instance)
3904         try:
3905             self.volume_api.begin_detaching(context, old_volume['id'])
3906         except exception.InvalidInput as exc:
3907             raise exception.InvalidVolume(reason=exc.format_message())
3908 
3909         # Get the BDM for the attached (old) volume so we can tell if it was
3910         # attached with the new-style Cinder 3.44 API.
3911         bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
3912             context, old_volume['id'], instance.uuid)
3913         new_attachment_id = None
3914         if bdm.attachment_id is None:
3915             # This is an old-style attachment so reserve the new volume before
3916             # we cast to the compute host.
3917             self.volume_api.reserve_volume(context, new_volume['id'])
3918         else:
3919             try:
3920                 self._check_volume_already_attached_to_instance(
3921                     context, instance, new_volume['id'])
3922             except exception.InvalidVolume:
3923                 with excutils.save_and_reraise_exception():
3924                     self.volume_api.roll_detaching(context, old_volume['id'])
3925 
3926             # This is a new-style attachment so for the volume that we are
3927             # going to swap to, create a new volume attachment.
3928             new_attachment_id = self.volume_api.attachment_create(
3929                 context, new_volume['id'], instance.uuid)['id']
3930 
3931         self._record_action_start(
3932             context, instance, instance_actions.SWAP_VOLUME)
3933 
3934         try:
3935             self.compute_rpcapi.swap_volume(
3936                     context, instance=instance,
3937                     old_volume_id=old_volume['id'],
3938                     new_volume_id=new_volume['id'],
3939                     new_attachment_id=new_attachment_id)
3940         except Exception:
3941             with excutils.save_and_reraise_exception():
3942                 self.volume_api.roll_detaching(context, old_volume['id'])
3943                 if new_attachment_id is None:
3944                     self.volume_api.unreserve_volume(context, new_volume['id'])
3945                 else:
3946                     self.volume_api.attachment_delete(
3947                         context, new_attachment_id)
3948 
3949     @check_instance_lock
3950     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
3951                                     vm_states.STOPPED],
3952                           task_state=[None])
3953     def attach_interface(self, context, instance, network_id, port_id,
3954                          requested_ip, tag=None):
3955         """Use hotplug to add an network adapter to an instance."""
3956         self._record_action_start(
3957             context, instance, instance_actions.ATTACH_INTERFACE)
3958         return self.compute_rpcapi.attach_interface(context,
3959             instance=instance, network_id=network_id, port_id=port_id,
3960             requested_ip=requested_ip, tag=tag)
3961 
3962     @check_instance_lock
3963     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
3964                                     vm_states.STOPPED],
3965                           task_state=[None])
3966     def detach_interface(self, context, instance, port_id):
3967         """Detach an network adapter from an instance."""
3968         self._record_action_start(
3969             context, instance, instance_actions.DETACH_INTERFACE)
3970         self.compute_rpcapi.detach_interface(context, instance=instance,
3971             port_id=port_id)
3972 
3973     def get_instance_metadata(self, context, instance):
3974         """Get all metadata associated with an instance."""
3975         return self.db.instance_metadata_get(context, instance.uuid)
3976 
3977     @check_instance_lock
3978     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
3979                                     vm_states.SUSPENDED, vm_states.STOPPED],
3980                           task_state=None)
3981     def delete_instance_metadata(self, context, instance, key):
3982         """Delete the given metadata item from an instance."""
3983         instance.delete_metadata_key(key)
3984         self.compute_rpcapi.change_instance_metadata(context,
3985                                                      instance=instance,
3986                                                      diff={key: ['-']})
3987 
3988     @check_instance_lock
3989     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
3990                                     vm_states.SUSPENDED, vm_states.STOPPED],
3991                           task_state=None)
3992     def update_instance_metadata(self, context, instance,
3993                                  metadata, delete=False):
3994         """Updates or creates instance metadata.
3995 
3996         If delete is True, metadata items that are not specified in the
3997         `metadata` argument will be deleted.
3998 
3999         """
4000         orig = dict(instance.metadata)
4001         if delete:
4002             _metadata = metadata
4003         else:
4004             _metadata = dict(instance.metadata)
4005             _metadata.update(metadata)
4006 
4007         self._check_metadata_properties_quota(context, _metadata)
4008         instance.metadata = _metadata
4009         instance.save()
4010         diff = _diff_dict(orig, instance.metadata)
4011         self.compute_rpcapi.change_instance_metadata(context,
4012                                                      instance=instance,
4013                                                      diff=diff)
4014         return _metadata
4015 
4016     @check_instance_lock
4017     @check_instance_cell
4018     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED])
4019     def live_migrate(self, context, instance, block_migration,
4020                      disk_over_commit, host_name, force=None, async=False):
4021         """Migrate a server lively to a new host."""
4022         LOG.debug("Going to try to live migrate instance to %s",
4023                   host_name or "another host", instance=instance)
4024 
4025         instance.task_state = task_states.MIGRATING
4026         instance.save(expected_task_state=[None])
4027 
4028         self._record_action_start(context, instance,
4029                                   instance_actions.LIVE_MIGRATION)
4030 
4031         self.consoleauth_rpcapi.delete_tokens_for_instance(
4032             context, instance.uuid)
4033 
4034         try:
4035             request_spec = objects.RequestSpec.get_by_instance_uuid(
4036                 context, instance.uuid)
4037         except exception.RequestSpecNotFound:
4038             # Some old instances can still have no RequestSpec object attached
4039             # to them, we need to support the old way
4040             request_spec = None
4041 
4042         # NOTE(sbauza): Force is a boolean by the new related API version
4043         if force is False and host_name:
4044             nodes = objects.ComputeNodeList.get_all_by_host(context, host_name)
4045             # Unset the host to make sure we call the scheduler
4046             # from the conductor LiveMigrationTask. Yes this is tightly-coupled
4047             # to behavior in conductor and not great.
4048             host_name = None
4049             # FIXME(sbauza): Since only Ironic driver uses more than one
4050             # compute per service but doesn't support live migrations,
4051             # let's provide the first one.
4052             target = nodes[0]
4053             if request_spec:
4054                 # TODO(sbauza): Hydrate a fake spec for old instances not yet
4055                 # having a request spec attached to them (particularly true for
4056                 # cells v1). For the moment, let's keep the same behaviour for
4057                 # all the instances but provide the destination only if a spec
4058                 # is found.
4059                 destination = objects.Destination(
4060                     host=target.host,
4061                     node=target.hypervisor_hostname
4062                 )
4063                 # This is essentially a hint to the scheduler to only consider
4064                 # the specified host but still run it through the filters.
4065                 request_spec.requested_destination = destination
4066 
4067         try:
4068             self.compute_task_api.live_migrate_instance(context, instance,
4069                 host_name, block_migration=block_migration,
4070                 disk_over_commit=disk_over_commit,
4071                 request_spec=request_spec, async=async)
4072         except oslo_exceptions.MessagingTimeout as messaging_timeout:
4073             with excutils.save_and_reraise_exception():
4074                 # NOTE(pkoniszewski): It is possible that MessagingTimeout
4075                 # occurs, but LM will still be in progress, so write
4076                 # instance fault to database
4077                 compute_utils.add_instance_fault_from_exc(context,
4078                                                           instance,
4079                                                           messaging_timeout)
4080 
4081     @check_instance_lock
4082     @check_instance_cell
4083     @check_instance_state(vm_state=[vm_states.ACTIVE],
4084                           task_state=[task_states.MIGRATING])
4085     def live_migrate_force_complete(self, context, instance, migration_id):
4086         """Force live migration to complete.
4087 
4088         :param context: Security context
4089         :param instance: The instance that is being migrated
4090         :param migration_id: ID of ongoing migration
4091 
4092         """
4093         LOG.debug("Going to try to force live migration to complete",
4094                   instance=instance)
4095 
4096         # NOTE(pkoniszewski): Get migration object to check if there is ongoing
4097         # live migration for particular instance. Also pass migration id to
4098         # compute to double check and avoid possible race condition.
4099         migration = objects.Migration.get_by_id_and_instance(
4100             context, migration_id, instance.uuid)
4101         if migration.status != 'running':
4102             raise exception.InvalidMigrationState(migration_id=migration_id,
4103                                                   instance_uuid=instance.uuid,
4104                                                   state=migration.status,
4105                                                   method='force complete')
4106 
4107         self._record_action_start(
4108             context, instance, instance_actions.LIVE_MIGRATION_FORCE_COMPLETE)
4109 
4110         self.compute_rpcapi.live_migration_force_complete(
4111             context, instance, migration)
4112 
4113     @check_instance_lock
4114     @check_instance_cell
4115     @check_instance_state(task_state=[task_states.MIGRATING])
4116     def live_migrate_abort(self, context, instance, migration_id):
4117         """Abort an in-progress live migration.
4118 
4119         :param context: Security context
4120         :param instance: The instance that is being migrated
4121         :param migration_id: ID of in-progress live migration
4122 
4123         """
4124         migration = objects.Migration.get_by_id_and_instance(context,
4125                     migration_id, instance.uuid)
4126         LOG.debug("Going to cancel live migration %s",
4127                   migration.id, instance=instance)
4128 
4129         if migration.status != 'running':
4130             raise exception.InvalidMigrationState(migration_id=migration_id,
4131                     instance_uuid=instance.uuid,
4132                     state=migration.status,
4133                     method='abort live migration')
4134         self._record_action_start(context, instance,
4135                                   instance_actions.LIVE_MIGRATION_CANCEL)
4136 
4137         self.compute_rpcapi.live_migration_abort(context,
4138                 instance, migration.id)
4139 
4140     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
4141                                     vm_states.ERROR])
4142     def evacuate(self, context, instance, host, on_shared_storage,
4143                  admin_password=None, force=None):
4144         """Running evacuate to target host.
4145 
4146         Checking vm compute host state, if the host not in expected_state,
4147         raising an exception.
4148 
4149         :param instance: The instance to evacuate
4150         :param host: Target host. if not set, the scheduler will pick up one
4151         :param on_shared_storage: True if instance files on shared storage
4152         :param admin_password: password to set on rebuilt instance
4153         :param force: Force the evacuation to the specific host target
4154 
4155         """
4156         LOG.debug('vm evacuation scheduled', instance=instance)
4157         inst_host = instance.host
4158         service = objects.Service.get_by_compute_host(context, inst_host)
4159         if self.servicegroup_api.service_is_up(service):
4160             LOG.error('Instance compute service state on %s '
4161                       'expected to be down, but it was up.', inst_host)
4162             raise exception.ComputeServiceInUse(host=inst_host)
4163 
4164         instance.task_state = task_states.REBUILDING
4165         instance.save(expected_task_state=[None])
4166         self._record_action_start(context, instance, instance_actions.EVACUATE)
4167 
4168         # NOTE(danms): Create this as a tombstone for the source compute
4169         # to find and cleanup. No need to pass it anywhere else.
4170         migration = objects.Migration(context,
4171                                       source_compute=instance.host,
4172                                       source_node=instance.node,
4173                                       instance_uuid=instance.uuid,
4174                                       status='accepted',
4175                                       migration_type='evacuation')
4176         if host:
4177             migration.dest_compute = host
4178         migration.create()
4179 
4180         compute_utils.notify_about_instance_usage(
4181             self.notifier, context, instance, "evacuate")
4182 
4183         try:
4184             request_spec = objects.RequestSpec.get_by_instance_uuid(
4185                 context, instance.uuid)
4186         except exception.RequestSpecNotFound:
4187             # Some old instances can still have no RequestSpec object attached
4188             # to them, we need to support the old way
4189             request_spec = None
4190 
4191         # NOTE(sbauza): Force is a boolean by the new related API version
4192         if force is False and host:
4193             nodes = objects.ComputeNodeList.get_all_by_host(context, host)
4194             # NOTE(sbauza): Unset the host to make sure we call the scheduler
4195             host = None
4196             # FIXME(sbauza): Since only Ironic driver uses more than one
4197             # compute per service but doesn't support evacuations,
4198             # let's provide the first one.
4199             target = nodes[0]
4200             if request_spec:
4201                 # TODO(sbauza): Hydrate a fake spec for old instances not yet
4202                 # having a request spec attached to them (particularly true for
4203                 # cells v1). For the moment, let's keep the same behaviour for
4204                 # all the instances but provide the destination only if a spec
4205                 # is found.
4206                 destination = objects.Destination(
4207                     host=target.host,
4208                     node=target.hypervisor_hostname
4209                 )
4210                 request_spec.requested_destination = destination
4211 
4212         return self.compute_task_api.rebuild_instance(context,
4213                        instance=instance,
4214                        new_pass=admin_password,
4215                        injected_files=None,
4216                        image_ref=None,
4217                        orig_image_ref=None,
4218                        orig_sys_metadata=None,
4219                        bdms=None,
4220                        recreate=True,
4221                        on_shared_storage=on_shared_storage,
4222                        host=host,
4223                        request_spec=request_spec,
4224                        )
4225 
4226     def get_migrations(self, context, filters):
4227         """Get all migrations for the given filters."""
4228         load_cells()
4229 
4230         migrations = []
4231         for cell in CELLS:
4232             if cell.uuid == objects.CellMapping.CELL0_UUID:
4233                 continue
4234             with nova_context.target_cell(context, cell) as cctxt:
4235                 migrations.extend(objects.MigrationList.get_by_filters(
4236                     cctxt, filters).objects)
4237         return objects.MigrationList(objects=migrations)
4238 
4239     def get_migrations_in_progress_by_instance(self, context, instance_uuid,
4240                                                migration_type=None):
4241         """Get all migrations of an instance in progress."""
4242         return objects.MigrationList.get_in_progress_by_instance(
4243                 context, instance_uuid, migration_type)
4244 
4245     def get_migration_by_id_and_instance(self, context,
4246                                          migration_id, instance_uuid):
4247         """Get the migration of an instance by id."""
4248         return objects.Migration.get_by_id_and_instance(
4249                 context, migration_id, instance_uuid)
4250 
4251     def _get_bdm_by_volume_id(self, context, volume_id, expected_attrs=None):
4252         """Retrieve a BDM without knowing its cell.
4253 
4254         .. note:: The context will be targeted to the cell in which the
4255             BDM is found, if any.
4256 
4257         :param context: The API request context.
4258         :param volume_id: The ID of the volume.
4259         :param expected_attrs: list of any additional attributes that should
4260             be joined when the BDM is loaded from the database.
4261         :raises: nova.exception.VolumeBDMNotFound if not found in any cell
4262         """
4263         load_cells()
4264         for cell in CELLS:
4265             nova_context.set_target_cell(context, cell)
4266             try:
4267                 return objects.BlockDeviceMapping.get_by_volume(
4268                     context, volume_id, expected_attrs=expected_attrs)
4269             except exception.NotFound:
4270                 continue
4271         raise exception.VolumeBDMNotFound(volume_id=volume_id)
4272 
4273     def volume_snapshot_create(self, context, volume_id, create_info):
4274         bdm = self._get_bdm_by_volume_id(
4275             context, volume_id, expected_attrs=['instance'])
4276 
4277         # We allow creating the snapshot in any vm_state as long as there is
4278         # no task being performed on the instance and it has a host.
4279         @check_instance_host
4280         @check_instance_state(vm_state=None)
4281         def do_volume_snapshot_create(self, context, instance):
4282             self.compute_rpcapi.volume_snapshot_create(context, instance,
4283                     volume_id, create_info)
4284             snapshot = {
4285                 'snapshot': {
4286                     'id': create_info.get('id'),
4287                     'volumeId': volume_id
4288                 }
4289             }
4290             return snapshot
4291 
4292         return do_volume_snapshot_create(self, context, bdm.instance)
4293 
4294     def volume_snapshot_delete(self, context, volume_id, snapshot_id,
4295                                delete_info):
4296         bdm = self._get_bdm_by_volume_id(
4297             context, volume_id, expected_attrs=['instance'])
4298 
4299         # We allow deleting the snapshot in any vm_state as long as there is
4300         # no task being performed on the instance and it has a host.
4301         @check_instance_host
4302         @check_instance_state(vm_state=None)
4303         def do_volume_snapshot_delete(self, context, instance):
4304             self.compute_rpcapi.volume_snapshot_delete(context, instance,
4305                     volume_id, snapshot_id, delete_info)
4306 
4307         do_volume_snapshot_delete(self, context, bdm.instance)
4308 
4309     def external_instance_event(self, api_context, instances, events):
4310         # NOTE(danms): The external API consumer just provides events,
4311         # but doesn't know where they go. We need to collate lists
4312         # by the host the affected instance is on and dispatch them
4313         # according to host
4314         instances_by_host = collections.defaultdict(list)
4315         events_by_host = collections.defaultdict(list)
4316         hosts_by_instance = collections.defaultdict(list)
4317         cell_contexts_by_host = {}
4318         for instance in instances:
4319             # instance._context is used here since it's already targeted to
4320             # the cell that the instance lives in, and we need to use that
4321             # cell context to lookup any migrations associated to the instance.
4322             for host in self._get_relevant_hosts(instance._context, instance):
4323                 # NOTE(danms): All instances on a host must have the same
4324                 # mapping, so just use that
4325                 # NOTE(mdbooth): We don't currently support migrations between
4326                 # cells, and given that the Migration record is hosted in the
4327                 # cell _get_relevant_hosts will likely have to change before we
4328                 # do. Consequently we can currently assume that the context for
4329                 # both the source and destination hosts of a migration is the
4330                 # same.
4331                 if host not in cell_contexts_by_host:
4332                     cell_contexts_by_host[host] = instance._context
4333 
4334                 instances_by_host[host].append(instance)
4335                 hosts_by_instance[instance.uuid].append(host)
4336 
4337         for event in events:
4338             if event.name == 'volume-extended':
4339                 # Volume extend is a user-initiated operation starting in the
4340                 # Block Storage service API. We record an instance action so
4341                 # the user can monitor the operation to completion.
4342                 host = hosts_by_instance[event.instance_uuid][0]
4343                 cell_context = cell_contexts_by_host[host]
4344                 objects.InstanceAction.action_start(
4345                     cell_context, event.instance_uuid,
4346                     instance_actions.EXTEND_VOLUME, want_result=False)
4347             for host in hosts_by_instance[event.instance_uuid]:
4348                 events_by_host[host].append(event)
4349 
4350         for host in instances_by_host:
4351             cell_context = cell_contexts_by_host[host]
4352 
4353             # TODO(salv-orlando): Handle exceptions raised by the rpc api layer
4354             # in order to ensure that a failure in processing events on a host
4355             # will not prevent processing events on other hosts
4356             self.compute_rpcapi.external_instance_event(
4357                 cell_context, instances_by_host[host], events_by_host[host],
4358                 host=host)
4359 
4360     def _get_relevant_hosts(self, context, instance):
4361         hosts = set()
4362         hosts.add(instance.host)
4363         if instance.migration_context is not None:
4364             migration_id = instance.migration_context.migration_id
4365             migration = objects.Migration.get_by_id(context, migration_id)
4366             hosts.add(migration.dest_compute)
4367             hosts.add(migration.source_compute)
4368             LOG.debug('Instance %(instance)s is migrating, '
4369                       'copying events to all relevant hosts: '
4370                       '%(hosts)s', {'instance': instance.uuid,
4371                                     'hosts': hosts})
4372         return hosts
4373 
4374     def get_instance_host_status(self, instance):
4375         if instance.host:
4376             try:
4377                 service = [service for service in instance.services if
4378                            service.binary == 'nova-compute'][0]
4379                 if service.forced_down:
4380                     host_status = fields_obj.HostStatus.DOWN
4381                 elif service.disabled:
4382                     host_status = fields_obj.HostStatus.MAINTENANCE
4383                 else:
4384                     alive = self.servicegroup_api.service_is_up(service)
4385                     host_status = ((alive and fields_obj.HostStatus.UP) or
4386                                    fields_obj.HostStatus.UNKNOWN)
4387             except IndexError:
4388                 host_status = fields_obj.HostStatus.NONE
4389         else:
4390             host_status = fields_obj.HostStatus.NONE
4391         return host_status
4392 
4393     def get_instances_host_statuses(self, instance_list):
4394         host_status_dict = dict()
4395         host_statuses = dict()
4396         for instance in instance_list:
4397             if instance.host:
4398                 if instance.host not in host_status_dict:
4399                     host_status = self.get_instance_host_status(instance)
4400                     host_status_dict[instance.host] = host_status
4401                 else:
4402                     host_status = host_status_dict[instance.host]
4403             else:
4404                 host_status = fields_obj.HostStatus.NONE
4405             host_statuses[instance.uuid] = host_status
4406         return host_statuses
4407 
4408 
4409 def target_host_cell(fn):
4410     """Target a host-based function to a cell.
4411 
4412     Expects to wrap a function of signature:
4413 
4414        func(self, context, host, ...)
4415     """
4416 
4417     @functools.wraps(fn)
4418     def targeted(self, context, host, *args, **kwargs):
4419         mapping = objects.HostMapping.get_by_host(context, host)
4420         nova_context.set_target_cell(context, mapping.cell_mapping)
4421         return fn(self, context, host, *args, **kwargs)
4422     return targeted
4423 
4424 
4425 def _find_service_in_cell(context, service_id=None, service_host=None):
4426     """Find a service by id or hostname by searching all cells.
4427 
4428     If one matching service is found, return it. If none or multiple
4429     are found, raise an exception.
4430 
4431     :param context: A context.RequestContext
4432     :param service_id: If not none, the DB ID of the service to find
4433     :param service_host: If not None, the hostname of the service to find
4434     :returns: An objects.Service
4435     :raises: ServiceNotUnique if multiple matching IDs are found
4436     :raises: NotFound if no matches are found
4437     :raises: NovaException if called with neither search option
4438     """
4439 
4440     load_cells()
4441     service = None
4442     found_in_cell = None
4443 
4444     is_uuid = False
4445     if service_id is not None:
4446         is_uuid = uuidutils.is_uuid_like(service_id)
4447         if is_uuid:
4448             lookup_fn = lambda c: objects.Service.get_by_uuid(c, service_id)
4449         else:
4450             lookup_fn = lambda c: objects.Service.get_by_id(c, service_id)
4451     elif service_host is not None:
4452         lookup_fn = lambda c: (
4453             objects.Service.get_by_compute_host(c, service_host))
4454     else:
4455         LOG.exception('_find_service_in_cell called with no search parameters')
4456         # This is intentionally cryptic so we don't leak implementation details
4457         # out of the API.
4458         raise exception.NovaException()
4459 
4460     for cell in CELLS:
4461         # NOTE(danms): Services can be in cell0, so don't skip it here
4462         try:
4463             with nova_context.target_cell(context, cell) as cctxt:
4464                 cell_service = lookup_fn(cctxt)
4465         except exception.NotFound:
4466             # NOTE(danms): Keep looking in other cells
4467             continue
4468         if service and cell_service:
4469             raise exception.ServiceNotUnique()
4470         service = cell_service
4471         found_in_cell = cell
4472         if service and is_uuid:
4473             break
4474 
4475     if service:
4476         # NOTE(danms): Set the cell on the context so it remains
4477         # when we return to our caller
4478         nova_context.set_target_cell(context, found_in_cell)
4479         return service
4480     else:
4481         raise exception.NotFound()
4482 
4483 
4484 class HostAPI(base.Base):
4485     """Sub-set of the Compute Manager API for managing host operations."""
4486 
4487     def __init__(self, rpcapi=None):
4488         self.rpcapi = rpcapi or compute_rpcapi.ComputeAPI()
4489         self.servicegroup_api = servicegroup.API()
4490         super(HostAPI, self).__init__()
4491 
4492     def _assert_host_exists(self, context, host_name, must_be_up=False):
4493         """Raise HostNotFound if compute host doesn't exist."""
4494         service = objects.Service.get_by_compute_host(context, host_name)
4495         if not service:
4496             raise exception.HostNotFound(host=host_name)
4497         if must_be_up and not self.servicegroup_api.service_is_up(service):
4498             raise exception.ComputeServiceUnavailable(host=host_name)
4499         return service['host']
4500 
4501     @wrap_exception()
4502     @target_host_cell
4503     def set_host_enabled(self, context, host_name, enabled):
4504         """Sets the specified host's ability to accept new instances."""
4505         host_name = self._assert_host_exists(context, host_name)
4506         payload = {'host_name': host_name, 'enabled': enabled}
4507         compute_utils.notify_about_host_update(context,
4508                                                'set_enabled.start',
4509                                                payload)
4510         result = self.rpcapi.set_host_enabled(context, enabled=enabled,
4511                 host=host_name)
4512         compute_utils.notify_about_host_update(context,
4513                                                'set_enabled.end',
4514                                                payload)
4515         return result
4516 
4517     @target_host_cell
4518     def get_host_uptime(self, context, host_name):
4519         """Returns the result of calling "uptime" on the target host."""
4520         host_name = self._assert_host_exists(context, host_name,
4521                          must_be_up=True)
4522         return self.rpcapi.get_host_uptime(context, host=host_name)
4523 
4524     @wrap_exception()
4525     @target_host_cell
4526     def host_power_action(self, context, host_name, action):
4527         """Reboots, shuts down or powers up the host."""
4528         host_name = self._assert_host_exists(context, host_name)
4529         payload = {'host_name': host_name, 'action': action}
4530         compute_utils.notify_about_host_update(context,
4531                                                'power_action.start',
4532                                                payload)
4533         result = self.rpcapi.host_power_action(context, action=action,
4534                 host=host_name)
4535         compute_utils.notify_about_host_update(context,
4536                                                'power_action.end',
4537                                                payload)
4538         return result
4539 
4540     @wrap_exception()
4541     @target_host_cell
4542     def set_host_maintenance(self, context, host_name, mode):
4543         """Start/Stop host maintenance window. On start, it triggers
4544         guest VMs evacuation.
4545         """
4546         host_name = self._assert_host_exists(context, host_name)
4547         payload = {'host_name': host_name, 'mode': mode}
4548         compute_utils.notify_about_host_update(context,
4549                                                'set_maintenance.start',
4550                                                payload)
4551         result = self.rpcapi.host_maintenance_mode(context,
4552                 host_param=host_name, mode=mode, host=host_name)
4553         compute_utils.notify_about_host_update(context,
4554                                                'set_maintenance.end',
4555                                                payload)
4556         return result
4557 
4558     def service_get_all(self, context, filters=None, set_zones=False,
4559                         all_cells=False):
4560         """Returns a list of services, optionally filtering the results.
4561 
4562         If specified, 'filters' should be a dictionary containing services
4563         attributes and matching values.  Ie, to get a list of services for
4564         the 'compute' topic, use filters={'topic': 'compute'}.
4565 
4566         If all_cells=True, then scan all cells and merge the results.
4567         """
4568         if filters is None:
4569             filters = {}
4570         disabled = filters.pop('disabled', None)
4571         if 'availability_zone' in filters:
4572             set_zones = True
4573 
4574         # NOTE(danms): Eventually this all_cells nonsense should go away
4575         # and we should always iterate over the cells. However, certain
4576         # callers need the legacy behavior for now.
4577         if all_cells:
4578             load_cells()
4579             services = []
4580             for cell in CELLS:
4581                 with nova_context.target_cell(context, cell) as cctxt:
4582                     cell_services = objects.ServiceList.get_all(
4583                         cctxt, disabled, set_zones=set_zones)
4584                 services.extend(cell_services)
4585         else:
4586             services = objects.ServiceList.get_all(context, disabled,
4587                                                    set_zones=set_zones)
4588         ret_services = []
4589         for service in services:
4590             for key, val in filters.items():
4591                 if service[key] != val:
4592                     break
4593             else:
4594                 # All filters matched.
4595                 ret_services.append(service)
4596         return ret_services
4597 
4598     def service_get_by_id(self, context, service_id):
4599         """Get service entry for the given service id or uuid."""
4600         try:
4601             return _find_service_in_cell(context, service_id=service_id)
4602         except exception.NotFound:
4603             raise exception.ServiceNotFound(service_id=service_id)
4604 
4605     @target_host_cell
4606     def service_get_by_compute_host(self, context, host_name):
4607         """Get service entry for the given compute hostname."""
4608         return objects.Service.get_by_compute_host(context, host_name)
4609 
4610     def _service_update(self, context, host_name, binary, params_to_update):
4611         """Performs the actual service update operation."""
4612         service = objects.Service.get_by_args(context, host_name, binary)
4613         service.update(params_to_update)
4614         service.save()
4615         return service
4616 
4617     @target_host_cell
4618     def service_update(self, context, host_name, binary, params_to_update):
4619         """Enable / Disable a service.
4620 
4621         For compute services, this stops new builds and migrations going to
4622         the host.
4623         """
4624         return self._service_update(context, host_name, binary,
4625                                     params_to_update)
4626 
4627     def _service_delete(self, context, service_id):
4628         """Performs the actual Service deletion operation."""
4629         try:
4630             service = _find_service_in_cell(context, service_id=service_id)
4631         except exception.NotFound:
4632             raise exception.ServiceNotFound(service_id=service_id)
4633         service.destroy()
4634 
4635     def service_delete(self, context, service_id):
4636         """Deletes the specified service found via id or uuid."""
4637         self._service_delete(context, service_id)
4638 
4639     @target_host_cell
4640     def instance_get_all_by_host(self, context, host_name):
4641         """Return all instances on the given host."""
4642         return objects.InstanceList.get_by_host(context, host_name)
4643 
4644     def task_log_get_all(self, context, task_name, period_beginning,
4645                          period_ending, host=None, state=None):
4646         """Return the task logs within a given range, optionally
4647         filtering by host and/or state.
4648         """
4649         return self.db.task_log_get_all(context, task_name,
4650                                         period_beginning,
4651                                         period_ending,
4652                                         host=host,
4653                                         state=state)
4654 
4655     def compute_node_get(self, context, compute_id):
4656         """Return compute node entry for particular integer ID or UUID."""
4657         load_cells()
4658 
4659         # NOTE(danms): Unfortunately this API exposes database identifiers
4660         # which means we really can't do something efficient here
4661         is_uuid = uuidutils.is_uuid_like(compute_id)
4662         for cell in CELLS:
4663             if cell.uuid == objects.CellMapping.CELL0_UUID:
4664                 continue
4665             with nova_context.target_cell(context, cell) as cctxt:
4666                 try:
4667                     if is_uuid:
4668                         # NOTE(mriedem): We wouldn't have to loop over cells if
4669                         # we stored the ComputeNode.uuid in the HostMapping but
4670                         # we don't have that. It could be added but would
4671                         # require an online data migration to update existing
4672                         # host mappings.
4673                         return objects.ComputeNode.get_by_uuid(cctxt,
4674                                                                compute_id)
4675                     return objects.ComputeNode.get_by_id(cctxt,
4676                                                          int(compute_id))
4677                 except exception.ComputeHostNotFound:
4678                     # NOTE(danms): Keep looking in other cells
4679                     continue
4680 
4681         raise exception.ComputeHostNotFound(host=compute_id)
4682 
4683     def compute_node_get_all(self, context, limit=None, marker=None):
4684         load_cells()
4685 
4686         computes = []
4687         uuid_marker = marker and uuidutils.is_uuid_like(marker)
4688         for cell in CELLS:
4689             if cell.uuid == objects.CellMapping.CELL0_UUID:
4690                 continue
4691             with nova_context.target_cell(context, cell) as cctxt:
4692 
4693                 # If we have a marker and it's a uuid, see if the compute node
4694                 # is in this cell.
4695                 if marker and uuid_marker:
4696                     try:
4697                         compute_marker = objects.ComputeNode.get_by_uuid(
4698                             cctxt, marker)
4699                         # we found the marker compute node, so use it's id
4700                         # for the actual marker for paging in this cell's db
4701                         marker = compute_marker.id
4702                     except exception.ComputeHostNotFound:
4703                         # The marker node isn't in this cell so keep looking.
4704                         continue
4705 
4706                 try:
4707                     cell_computes = objects.ComputeNodeList.get_by_pagination(
4708                         cctxt, limit=limit, marker=marker)
4709                 except exception.MarkerNotFound:
4710                     # NOTE(danms): Keep looking through cells
4711                     continue
4712                 computes.extend(cell_computes)
4713                 # NOTE(danms): We must have found the marker, so continue on
4714                 # without one
4715                 marker = None
4716                 if limit:
4717                     limit -= len(cell_computes)
4718                     if limit <= 0:
4719                         break
4720 
4721         if marker is not None and len(computes) == 0:
4722             # NOTE(danms): If we did not find the marker in any cell,
4723             # mimic the db_api behavior here.
4724             raise exception.MarkerNotFound(marker=marker)
4725 
4726         return objects.ComputeNodeList(objects=computes)
4727 
4728     def compute_node_search_by_hypervisor(self, context, hypervisor_match):
4729         load_cells()
4730 
4731         computes = []
4732         for cell in CELLS:
4733             if cell.uuid == objects.CellMapping.CELL0_UUID:
4734                 continue
4735             with nova_context.target_cell(context, cell) as cctxt:
4736                 cell_computes = objects.ComputeNodeList.get_by_hypervisor(
4737                     cctxt, hypervisor_match)
4738             computes.extend(cell_computes)
4739         return objects.ComputeNodeList(objects=computes)
4740 
4741     def compute_node_statistics(self, context):
4742         load_cells()
4743 
4744         cell_stats = []
4745         for cell in CELLS:
4746             if cell.uuid == objects.CellMapping.CELL0_UUID:
4747                 continue
4748             with nova_context.target_cell(context, cell) as cctxt:
4749                 cell_stats.append(self.db.compute_node_statistics(cctxt))
4750 
4751         if cell_stats:
4752             keys = cell_stats[0].keys()
4753             return {k: sum(stats[k] for stats in cell_stats)
4754                     for k in keys}
4755         else:
4756             return {}
4757 
4758 
4759 class InstanceActionAPI(base.Base):
4760     """Sub-set of the Compute Manager API for managing instance actions."""
4761 
4762     def actions_get(self, context, instance, limit=None, marker=None,
4763                     filters=None):
4764         return objects.InstanceActionList.get_by_instance_uuid(
4765             context, instance.uuid, limit, marker, filters)
4766 
4767     def action_get_by_request_id(self, context, instance, request_id):
4768         return objects.InstanceAction.get_by_request_id(
4769             context, instance.uuid, request_id)
4770 
4771     def action_events_get(self, context, instance, action_id):
4772         return objects.InstanceActionEventList.get_by_action(
4773             context, action_id)
4774 
4775 
4776 class AggregateAPI(base.Base):
4777     """Sub-set of the Compute Manager API for managing host aggregates."""
4778     def __init__(self, **kwargs):
4779         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
4780         self.scheduler_client = scheduler_client.SchedulerClient()
4781         super(AggregateAPI, self).__init__(**kwargs)
4782 
4783     @wrap_exception()
4784     def create_aggregate(self, context, aggregate_name, availability_zone):
4785         """Creates the model for the aggregate."""
4786 
4787         aggregate = objects.Aggregate(context=context)
4788         aggregate.name = aggregate_name
4789         if availability_zone:
4790             aggregate.metadata = {'availability_zone': availability_zone}
4791         aggregate.create()
4792         self.scheduler_client.update_aggregates(context, [aggregate])
4793         return aggregate
4794 
4795     def get_aggregate(self, context, aggregate_id):
4796         """Get an aggregate by id."""
4797         return objects.Aggregate.get_by_id(context, aggregate_id)
4798 
4799     def get_aggregate_list(self, context):
4800         """Get all the aggregates."""
4801         return objects.AggregateList.get_all(context)
4802 
4803     def get_aggregates_by_host(self, context, compute_host):
4804         """Get all the aggregates where the given host is presented."""
4805         return objects.AggregateList.get_by_host(context, compute_host)
4806 
4807     @wrap_exception()
4808     def update_aggregate(self, context, aggregate_id, values):
4809         """Update the properties of an aggregate."""
4810         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
4811         if 'name' in values:
4812             aggregate.name = values.pop('name')
4813             aggregate.save()
4814         self.is_safe_to_update_az(context, values, aggregate=aggregate,
4815                                   action_name=AGGREGATE_ACTION_UPDATE)
4816         if values:
4817             aggregate.update_metadata(values)
4818             aggregate.updated_at = timeutils.utcnow()
4819         self.scheduler_client.update_aggregates(context, [aggregate])
4820         # If updated values include availability_zones, then the cache
4821         # which stored availability_zones and host need to be reset
4822         if values.get('availability_zone'):
4823             availability_zones.reset_cache()
4824         return aggregate
4825 
4826     @wrap_exception()
4827     def update_aggregate_metadata(self, context, aggregate_id, metadata):
4828         """Updates the aggregate metadata."""
4829         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
4830         self.is_safe_to_update_az(context, metadata, aggregate=aggregate,
4831                                   action_name=AGGREGATE_ACTION_UPDATE_META)
4832         aggregate.update_metadata(metadata)
4833         self.scheduler_client.update_aggregates(context, [aggregate])
4834         # If updated metadata include availability_zones, then the cache
4835         # which stored availability_zones and host need to be reset
4836         if metadata and metadata.get('availability_zone'):
4837             availability_zones.reset_cache()
4838         aggregate.updated_at = timeutils.utcnow()
4839         return aggregate
4840 
4841     @wrap_exception()
4842     def delete_aggregate(self, context, aggregate_id):
4843         """Deletes the aggregate."""
4844         aggregate_payload = {'aggregate_id': aggregate_id}
4845         compute_utils.notify_about_aggregate_update(context,
4846                                                     "delete.start",
4847                                                     aggregate_payload)
4848         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
4849 
4850         compute_utils.notify_about_aggregate_action(
4851             context=context,
4852             aggregate=aggregate,
4853             action=fields_obj.NotificationAction.DELETE,
4854             phase=fields_obj.NotificationPhase.START)
4855 
4856         if len(aggregate.hosts) > 0:
4857             msg = _("Host aggregate is not empty")
4858             raise exception.InvalidAggregateActionDelete(
4859                 aggregate_id=aggregate_id, reason=msg)
4860         aggregate.destroy()
4861         self.scheduler_client.delete_aggregate(context, aggregate)
4862         compute_utils.notify_about_aggregate_update(context,
4863                                                     "delete.end",
4864                                                     aggregate_payload)
4865         compute_utils.notify_about_aggregate_action(
4866             context=context,
4867             aggregate=aggregate,
4868             action=fields_obj.NotificationAction.DELETE,
4869             phase=fields_obj.NotificationPhase.END)
4870 
4871     def is_safe_to_update_az(self, context, metadata, aggregate,
4872                              hosts=None,
4873                              action_name=AGGREGATE_ACTION_ADD):
4874         """Determine if updates alter an aggregate's availability zone.
4875 
4876             :param context: local context
4877             :param metadata: Target metadata for updating aggregate
4878             :param aggregate: Aggregate to update
4879             :param hosts: Hosts to check. If None, aggregate.hosts is used
4880             :type hosts: list
4881             :action_name: Calling method for logging purposes
4882 
4883         """
4884         if 'availability_zone' in metadata:
4885             if not metadata['availability_zone']:
4886                 msg = _("Aggregate %s does not support empty named "
4887                         "availability zone") % aggregate.name
4888                 self._raise_invalid_aggregate_exc(action_name, aggregate.id,
4889                                                   msg)
4890             _hosts = hosts or aggregate.hosts
4891             host_aggregates = objects.AggregateList.get_by_metadata_key(
4892                 context, 'availability_zone', hosts=_hosts)
4893             conflicting_azs = [
4894                 agg.availability_zone for agg in host_aggregates
4895                 if agg.availability_zone != metadata['availability_zone']
4896                 and agg.id != aggregate.id]
4897             if conflicting_azs:
4898                 msg = _("One or more hosts already in availability zone(s) "
4899                         "%s") % conflicting_azs
4900                 self._raise_invalid_aggregate_exc(action_name, aggregate.id,
4901                                                   msg)
4902 
4903     def _raise_invalid_aggregate_exc(self, action_name, aggregate_id, reason):
4904         if action_name == AGGREGATE_ACTION_ADD:
4905             raise exception.InvalidAggregateActionAdd(
4906                 aggregate_id=aggregate_id, reason=reason)
4907         elif action_name == AGGREGATE_ACTION_UPDATE:
4908             raise exception.InvalidAggregateActionUpdate(
4909                 aggregate_id=aggregate_id, reason=reason)
4910         elif action_name == AGGREGATE_ACTION_UPDATE_META:
4911             raise exception.InvalidAggregateActionUpdateMeta(
4912                 aggregate_id=aggregate_id, reason=reason)
4913         elif action_name == AGGREGATE_ACTION_DELETE:
4914             raise exception.InvalidAggregateActionDelete(
4915                 aggregate_id=aggregate_id, reason=reason)
4916 
4917         raise exception.NovaException(
4918             _("Unexpected aggregate action %s") % action_name)
4919 
4920     def _update_az_cache_for_host(self, context, host_name, aggregate_meta):
4921         # Update the availability_zone cache to avoid getting wrong
4922         # availability_zone in cache retention time when add/remove
4923         # host to/from aggregate.
4924         if aggregate_meta and aggregate_meta.get('availability_zone'):
4925             availability_zones.update_host_availability_zone_cache(context,
4926                                                                    host_name)
4927 
4928     @wrap_exception()
4929     def add_host_to_aggregate(self, context, aggregate_id, host_name):
4930         """Adds the host to an aggregate."""
4931         aggregate_payload = {'aggregate_id': aggregate_id,
4932                              'host_name': host_name}
4933         compute_utils.notify_about_aggregate_update(context,
4934                                                     "addhost.start",
4935                                                     aggregate_payload)
4936         # validates the host; HostMappingNotFound or ComputeHostNotFound
4937         # is raised if invalid
4938         try:
4939             mapping = objects.HostMapping.get_by_host(context, host_name)
4940             nova_context.set_target_cell(context, mapping.cell_mapping)
4941             objects.Service.get_by_compute_host(context, host_name)
4942         except exception.HostMappingNotFound:
4943             try:
4944                 # NOTE(danms): This targets our cell
4945                 _find_service_in_cell(context, service_host=host_name)
4946             except exception.NotFound:
4947                 raise exception.ComputeHostNotFound(host=host_name)
4948 
4949         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
4950 
4951         compute_utils.notify_about_aggregate_action(
4952             context=context,
4953             aggregate=aggregate,
4954             action=fields_obj.NotificationAction.ADD_HOST,
4955             phase=fields_obj.NotificationPhase.START)
4956 
4957         self.is_safe_to_update_az(context, aggregate.metadata,
4958                                   hosts=[host_name], aggregate=aggregate)
4959 
4960         aggregate.add_host(host_name)
4961         self.scheduler_client.update_aggregates(context, [aggregate])
4962         self._update_az_cache_for_host(context, host_name, aggregate.metadata)
4963         # NOTE(jogo): Send message to host to support resource pools
4964         self.compute_rpcapi.add_aggregate_host(context,
4965                 aggregate=aggregate, host_param=host_name, host=host_name)
4966         aggregate_payload.update({'name': aggregate.name})
4967         compute_utils.notify_about_aggregate_update(context,
4968                                                     "addhost.end",
4969                                                     aggregate_payload)
4970         compute_utils.notify_about_aggregate_action(
4971             context=context,
4972             aggregate=aggregate,
4973             action=fields_obj.NotificationAction.ADD_HOST,
4974             phase=fields_obj.NotificationPhase.END)
4975 
4976         return aggregate
4977 
4978     @wrap_exception()
4979     def remove_host_from_aggregate(self, context, aggregate_id, host_name):
4980         """Removes host from the aggregate."""
4981         aggregate_payload = {'aggregate_id': aggregate_id,
4982                              'host_name': host_name}
4983         compute_utils.notify_about_aggregate_update(context,
4984                                                     "removehost.start",
4985                                                     aggregate_payload)
4986         # validates the host; HostMappingNotFound or ComputeHostNotFound
4987         # is raised if invalid
4988         mapping = objects.HostMapping.get_by_host(context, host_name)
4989         nova_context.set_target_cell(context, mapping.cell_mapping)
4990         objects.Service.get_by_compute_host(context, host_name)
4991         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
4992 
4993         compute_utils.notify_about_aggregate_action(
4994             context=context,
4995             aggregate=aggregate,
4996             action=fields_obj.NotificationAction.REMOVE_HOST,
4997             phase=fields_obj.NotificationPhase.START)
4998 
4999         aggregate.delete_host(host_name)
5000         self.scheduler_client.update_aggregates(context, [aggregate])
5001         self._update_az_cache_for_host(context, host_name, aggregate.metadata)
5002         self.compute_rpcapi.remove_aggregate_host(context,
5003                 aggregate=aggregate, host_param=host_name, host=host_name)
5004         compute_utils.notify_about_aggregate_update(context,
5005                                                     "removehost.end",
5006                                                     aggregate_payload)
5007         compute_utils.notify_about_aggregate_action(
5008             context=context,
5009             aggregate=aggregate,
5010             action=fields_obj.NotificationAction.REMOVE_HOST,
5011             phase=fields_obj.NotificationPhase.END)
5012         return aggregate
5013 
5014 
5015 class KeypairAPI(base.Base):
5016     """Subset of the Compute Manager API for managing key pairs."""
5017 
5018     get_notifier = functools.partial(rpc.get_notifier, service='api')
5019     wrap_exception = functools.partial(exception_wrapper.wrap_exception,
5020                                        get_notifier=get_notifier,
5021                                        binary='nova-api')
5022 
5023     def _notify(self, context, event_suffix, keypair_name):
5024         payload = {
5025             'tenant_id': context.project_id,
5026             'user_id': context.user_id,
5027             'key_name': keypair_name,
5028         }
5029         notify = self.get_notifier()
5030         notify.info(context, 'keypair.%s' % event_suffix, payload)
5031 
5032     def _validate_new_key_pair(self, context, user_id, key_name, key_type):
5033         safe_chars = "_- " + string.digits + string.ascii_letters
5034         clean_value = "".join(x for x in key_name if x in safe_chars)
5035         if clean_value != key_name:
5036             raise exception.InvalidKeypair(
5037                 reason=_("Keypair name contains unsafe characters"))
5038 
5039         try:
5040             utils.check_string_length(key_name, min_length=1, max_length=255)
5041         except exception.InvalidInput:
5042             raise exception.InvalidKeypair(
5043                 reason=_('Keypair name must be string and between '
5044                          '1 and 255 characters long'))
5045         try:
5046             objects.Quotas.check_deltas(context, {'key_pairs': 1}, user_id)
5047         except exception.OverQuota:
5048             raise exception.KeypairLimitExceeded()
5049 
5050     @wrap_exception()
5051     def import_key_pair(self, context, user_id, key_name, public_key,
5052                         key_type=keypair_obj.KEYPAIR_TYPE_SSH):
5053         """Import a key pair using an existing public key."""
5054         self._validate_new_key_pair(context, user_id, key_name, key_type)
5055 
5056         self._notify(context, 'import.start', key_name)
5057 
5058         keypair = objects.KeyPair(context)
5059         keypair.user_id = user_id
5060         keypair.name = key_name
5061         keypair.type = key_type
5062         keypair.fingerprint = None
5063         keypair.public_key = public_key
5064 
5065         compute_utils.notify_about_keypair_action(
5066             context=context,
5067             keypair=keypair,
5068             action=fields_obj.NotificationAction.IMPORT,
5069             phase=fields_obj.NotificationPhase.START)
5070 
5071         fingerprint = self._generate_fingerprint(public_key, key_type)
5072 
5073         keypair.fingerprint = fingerprint
5074         keypair.create()
5075 
5076         compute_utils.notify_about_keypair_action(
5077             context=context,
5078             keypair=keypair,
5079             action=fields_obj.NotificationAction.IMPORT,
5080             phase=fields_obj.NotificationPhase.END)
5081         self._notify(context, 'import.end', key_name)
5082 
5083         return keypair
5084 
5085     @wrap_exception()
5086     def create_key_pair(self, context, user_id, key_name,
5087                         key_type=keypair_obj.KEYPAIR_TYPE_SSH):
5088         """Create a new key pair."""
5089         self._validate_new_key_pair(context, user_id, key_name, key_type)
5090 
5091         keypair = objects.KeyPair(context)
5092         keypair.user_id = user_id
5093         keypair.name = key_name
5094         keypair.type = key_type
5095         keypair.fingerprint = None
5096         keypair.public_key = None
5097 
5098         self._notify(context, 'create.start', key_name)
5099         compute_utils.notify_about_keypair_action(
5100             context=context,
5101             keypair=keypair,
5102             action=fields_obj.NotificationAction.CREATE,
5103             phase=fields_obj.NotificationPhase.START)
5104 
5105         private_key, public_key, fingerprint = self._generate_key_pair(
5106             user_id, key_type)
5107 
5108         keypair.fingerprint = fingerprint
5109         keypair.public_key = public_key
5110         keypair.create()
5111 
5112         # NOTE(melwitt): We recheck the quota after creating the object to
5113         # prevent users from allocating more resources than their allowed quota
5114         # in the event of a race. This is configurable because it can be
5115         # expensive if strict quota limits are not required in a deployment.
5116         if CONF.quota.recheck_quota:
5117             try:
5118                 objects.Quotas.check_deltas(context, {'key_pairs': 0}, user_id)
5119             except exception.OverQuota:
5120                 keypair.destroy()
5121                 raise exception.KeypairLimitExceeded()
5122 
5123         compute_utils.notify_about_keypair_action(
5124             context=context,
5125             keypair=keypair,
5126             action=fields_obj.NotificationAction.CREATE,
5127             phase=fields_obj.NotificationPhase.END)
5128 
5129         self._notify(context, 'create.end', key_name)
5130 
5131         return keypair, private_key
5132 
5133     def _generate_fingerprint(self, public_key, key_type):
5134         if key_type == keypair_obj.KEYPAIR_TYPE_SSH:
5135             return crypto.generate_fingerprint(public_key)
5136         elif key_type == keypair_obj.KEYPAIR_TYPE_X509:
5137             return crypto.generate_x509_fingerprint(public_key)
5138 
5139     def _generate_key_pair(self, user_id, key_type):
5140         if key_type == keypair_obj.KEYPAIR_TYPE_SSH:
5141             return crypto.generate_key_pair()
5142         elif key_type == keypair_obj.KEYPAIR_TYPE_X509:
5143             return crypto.generate_winrm_x509_cert(user_id)
5144 
5145     @wrap_exception()
5146     def delete_key_pair(self, context, user_id, key_name):
5147         """Delete a keypair by name."""
5148         self._notify(context, 'delete.start', key_name)
5149         keypair = self.get_key_pair(context, user_id, key_name)
5150         compute_utils.notify_about_keypair_action(
5151             context=context,
5152             keypair=keypair,
5153             action=fields_obj.NotificationAction.DELETE,
5154             phase=fields_obj.NotificationPhase.START)
5155         objects.KeyPair.destroy_by_name(context, user_id, key_name)
5156         compute_utils.notify_about_keypair_action(
5157             context=context,
5158             keypair=keypair,
5159             action=fields_obj.NotificationAction.DELETE,
5160             phase=fields_obj.NotificationPhase.END)
5161         self._notify(context, 'delete.end', key_name)
5162 
5163     def get_key_pairs(self, context, user_id, limit=None, marker=None):
5164         """List key pairs."""
5165         return objects.KeyPairList.get_by_user(
5166             context, user_id, limit=limit, marker=marker)
5167 
5168     def get_key_pair(self, context, user_id, key_name):
5169         """Get a keypair by name."""
5170         return objects.KeyPair.get_by_name(context, user_id, key_name)
5171 
5172 
5173 class SecurityGroupAPI(base.Base, security_group_base.SecurityGroupBase):
5174     """Sub-set of the Compute API related to managing security groups
5175     and security group rules
5176     """
5177 
5178     # The nova security group api does not use a uuid for the id.
5179     id_is_uuid = False
5180 
5181     def __init__(self, **kwargs):
5182         super(SecurityGroupAPI, self).__init__(**kwargs)
5183         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
5184 
5185     def validate_property(self, value, property, allowed):
5186         """Validate given security group property.
5187 
5188         :param value:          the value to validate, as a string or unicode
5189         :param property:       the property, either 'name' or 'description'
5190         :param allowed:        the range of characters allowed
5191         """
5192 
5193         try:
5194             val = value.strip()
5195         except AttributeError:
5196             msg = _("Security group %s is not a string or unicode") % property
5197             self.raise_invalid_property(msg)
5198         utils.check_string_length(val, name=property, min_length=1,
5199                                   max_length=255)
5200 
5201         if allowed and not re.match(allowed, val):
5202             # Some validation to ensure that values match API spec.
5203             # - Alphanumeric characters, spaces, dashes, and underscores.
5204             # TODO(Daviey): LP: #813685 extend beyond group_name checking, and
5205             #  probably create a param validator that can be used elsewhere.
5206             msg = (_("Value (%(value)s) for parameter Group%(property)s is "
5207                      "invalid. Content limited to '%(allowed)s'.") %
5208                    {'value': value, 'allowed': allowed,
5209                     'property': property.capitalize()})
5210             self.raise_invalid_property(msg)
5211 
5212     def ensure_default(self, context):
5213         """Ensure that a context has a security group.
5214 
5215         Creates a security group for the security context if it does not
5216         already exist.
5217 
5218         :param context: the security context
5219         """
5220         self.db.security_group_ensure_default(context)
5221 
5222     def create_security_group(self, context, name, description):
5223         try:
5224             objects.Quotas.check_deltas(context, {'security_groups': 1},
5225                                         context.project_id,
5226                                         user_id=context.user_id)
5227         except exception.OverQuota:
5228             msg = _("Quota exceeded, too many security groups.")
5229             self.raise_over_quota(msg)
5230 
5231         LOG.info("Create Security Group %s", name)
5232 
5233         self.ensure_default(context)
5234 
5235         group = {'user_id': context.user_id,
5236                  'project_id': context.project_id,
5237                  'name': name,
5238                  'description': description}
5239         try:
5240             group_ref = self.db.security_group_create(context, group)
5241         except exception.SecurityGroupExists:
5242             msg = _('Security group %s already exists') % name
5243             self.raise_group_already_exists(msg)
5244 
5245         # NOTE(melwitt): We recheck the quota after creating the object to
5246         # prevent users from allocating more resources than their allowed quota
5247         # in the event of a race. This is configurable because it can be
5248         # expensive if strict quota limits are not required in a deployment.
5249         if CONF.quota.recheck_quota:
5250             try:
5251                 objects.Quotas.check_deltas(context, {'security_groups': 0},
5252                                             context.project_id,
5253                                             user_id=context.user_id)
5254             except exception.OverQuota:
5255                 self.db.security_group_destroy(context, group_ref['id'])
5256                 msg = _("Quota exceeded, too many security groups.")
5257                 self.raise_over_quota(msg)
5258 
5259         return group_ref
5260 
5261     def update_security_group(self, context, security_group,
5262                                 name, description):
5263         if security_group['name'] in RO_SECURITY_GROUPS:
5264             msg = (_("Unable to update system group '%s'") %
5265                     security_group['name'])
5266             self.raise_invalid_group(msg)
5267 
5268         group = {'name': name,
5269                  'description': description}
5270 
5271         columns_to_join = ['rules.grantee_group']
5272         group_ref = self.db.security_group_update(context,
5273                 security_group['id'],
5274                 group,
5275                 columns_to_join=columns_to_join)
5276         return group_ref
5277 
5278     def get(self, context, name=None, id=None, map_exception=False):
5279         self.ensure_default(context)
5280         cols = ['rules']
5281         try:
5282             if name:
5283                 return self.db.security_group_get_by_name(context,
5284                                                           context.project_id,
5285                                                           name,
5286                                                           columns_to_join=cols)
5287             elif id:
5288                 return self.db.security_group_get(context, id,
5289                                                   columns_to_join=cols)
5290         except exception.NotFound as exp:
5291             if map_exception:
5292                 msg = exp.format_message()
5293                 self.raise_not_found(msg)
5294             else:
5295                 raise
5296 
5297     def list(self, context, names=None, ids=None, project=None,
5298              search_opts=None):
5299         self.ensure_default(context)
5300 
5301         groups = []
5302         if names or ids:
5303             if names:
5304                 for name in names:
5305                     groups.append(self.db.security_group_get_by_name(context,
5306                                                                      project,
5307                                                                      name))
5308             if ids:
5309                 for id in ids:
5310                     groups.append(self.db.security_group_get(context, id))
5311 
5312         elif context.is_admin:
5313             # TODO(eglynn): support a wider set of search options than just
5314             # all_tenants, at least include the standard filters defined for
5315             # the EC2 DescribeSecurityGroups API for the non-admin case also
5316             if (search_opts and 'all_tenants' in search_opts):
5317                 groups = self.db.security_group_get_all(context)
5318             else:
5319                 groups = self.db.security_group_get_by_project(context,
5320                                                                project)
5321 
5322         elif project:
5323             groups = self.db.security_group_get_by_project(context, project)
5324 
5325         return groups
5326 
5327     def destroy(self, context, security_group):
5328         if security_group['name'] in RO_SECURITY_GROUPS:
5329             msg = _("Unable to delete system group '%s'") % \
5330                     security_group['name']
5331             self.raise_invalid_group(msg)
5332 
5333         if self.db.security_group_in_use(context, security_group['id']):
5334             msg = _("Security group is still in use")
5335             self.raise_invalid_group(msg)
5336 
5337         LOG.info("Delete security group %s", security_group['name'])
5338         self.db.security_group_destroy(context, security_group['id'])
5339 
5340     def is_associated_with_server(self, security_group, instance_uuid):
5341         """Check if the security group is already associated
5342            with the instance. If Yes, return True.
5343         """
5344 
5345         if not security_group:
5346             return False
5347 
5348         instances = security_group.get('instances')
5349         if not instances:
5350             return False
5351 
5352         for inst in instances:
5353             if (instance_uuid == inst['uuid']):
5354                 return True
5355 
5356         return False
5357 
5358     def add_to_instance(self, context, instance, security_group_name):
5359         """Add security group to the instance."""
5360         security_group = self.db.security_group_get_by_name(context,
5361                 context.project_id,
5362                 security_group_name)
5363 
5364         instance_uuid = instance.uuid
5365 
5366         # check if the security group is associated with the server
5367         if self.is_associated_with_server(security_group, instance_uuid):
5368             raise exception.SecurityGroupExistsForInstance(
5369                                         security_group_id=security_group['id'],
5370                                         instance_id=instance_uuid)
5371 
5372         self.db.instance_add_security_group(context.elevated(),
5373                                             instance_uuid,
5374                                             security_group['id'])
5375         if instance.host:
5376             self.compute_rpcapi.refresh_instance_security_rules(
5377                     context, instance, instance.host)
5378 
5379     def remove_from_instance(self, context, instance, security_group_name):
5380         """Remove the security group associated with the instance."""
5381         security_group = self.db.security_group_get_by_name(context,
5382                 context.project_id,
5383                 security_group_name)
5384 
5385         instance_uuid = instance.uuid
5386 
5387         # check if the security group is associated with the server
5388         if not self.is_associated_with_server(security_group, instance_uuid):
5389             raise exception.SecurityGroupNotExistsForInstance(
5390                                     security_group_id=security_group['id'],
5391                                     instance_id=instance_uuid)
5392 
5393         self.db.instance_remove_security_group(context.elevated(),
5394                                                instance_uuid,
5395                                                security_group['id'])
5396         if instance.host:
5397             self.compute_rpcapi.refresh_instance_security_rules(
5398                     context, instance, instance.host)
5399 
5400     def get_rule(self, context, id):
5401         self.ensure_default(context)
5402         try:
5403             return self.db.security_group_rule_get(context, id)
5404         except exception.NotFound:
5405             msg = _("Rule (%s) not found") % id
5406             self.raise_not_found(msg)
5407 
5408     def add_rules(self, context, id, name, vals):
5409         """Add security group rule(s) to security group.
5410 
5411         Note: the Nova security group API doesn't support adding multiple
5412         security group rules at once but the EC2 one does. Therefore,
5413         this function is written to support both.
5414         """
5415 
5416         try:
5417             objects.Quotas.check_deltas(context,
5418                                         {'security_group_rules': len(vals)},
5419                                         id)
5420         except exception.OverQuota:
5421             msg = _("Quota exceeded, too many security group rules.")
5422             self.raise_over_quota(msg)
5423 
5424         msg = ("Security group %(name)s added %(protocol)s ingress "
5425                "(%(from_port)s:%(to_port)s)")
5426         rules = []
5427         for v in vals:
5428             rule = self.db.security_group_rule_create(context, v)
5429 
5430             # NOTE(melwitt): We recheck the quota after creating the object to
5431             # prevent users from allocating more resources than their allowed
5432             # quota in the event of a race. This is configurable because it can
5433             # be expensive if strict quota limits are not required in a
5434             # deployment.
5435             if CONF.quota.recheck_quota:
5436                 try:
5437                     objects.Quotas.check_deltas(context,
5438                                                 {'security_group_rules': 0},
5439                                                 id)
5440                 except exception.OverQuota:
5441                     self.db.security_group_rule_destroy(context, rule['id'])
5442                     msg = _("Quota exceeded, too many security group rules.")
5443                     self.raise_over_quota(msg)
5444 
5445             rules.append(rule)
5446             LOG.info(msg, {'name': name,
5447                            'protocol': rule.protocol,
5448                            'from_port': rule.from_port,
5449                            'to_port': rule.to_port})
5450 
5451         self.trigger_rules_refresh(context, id=id)
5452         return rules
5453 
5454     def remove_rules(self, context, security_group, rule_ids):
5455         msg = ("Security group %(name)s removed %(protocol)s ingress "
5456                "(%(from_port)s:%(to_port)s)")
5457         for rule_id in rule_ids:
5458             rule = self.get_rule(context, rule_id)
5459             LOG.info(msg, {'name': security_group['name'],
5460                            'protocol': rule.protocol,
5461                            'from_port': rule.from_port,
5462                            'to_port': rule.to_port})
5463 
5464             self.db.security_group_rule_destroy(context, rule_id)
5465 
5466         # NOTE(vish): we removed some rules, so refresh
5467         self.trigger_rules_refresh(context, id=security_group['id'])
5468 
5469     def remove_default_rules(self, context, rule_ids):
5470         for rule_id in rule_ids:
5471             self.db.security_group_default_rule_destroy(context, rule_id)
5472 
5473     def add_default_rules(self, context, vals):
5474         rules = [self.db.security_group_default_rule_create(context, v)
5475                  for v in vals]
5476         return rules
5477 
5478     def default_rule_exists(self, context, values):
5479         """Indicates whether the specified rule values are already
5480            defined in the default security group rules.
5481         """
5482         for rule in self.db.security_group_default_rule_list(context):
5483             keys = ('cidr', 'from_port', 'to_port', 'protocol')
5484             for key in keys:
5485                 if rule.get(key) != values.get(key):
5486                     break
5487             else:
5488                 return rule.get('id') or True
5489         return False
5490 
5491     def get_all_default_rules(self, context):
5492         try:
5493             rules = self.db.security_group_default_rule_list(context)
5494         except Exception:
5495             msg = 'cannot get default security group rules'
5496             raise exception.SecurityGroupDefaultRuleNotFound(msg)
5497 
5498         return rules
5499 
5500     def get_default_rule(self, context, id):
5501         return self.db.security_group_default_rule_get(context, id)
5502 
5503     def validate_id(self, id):
5504         try:
5505             return int(id)
5506         except ValueError:
5507             msg = _("Security group id should be integer")
5508             self.raise_invalid_property(msg)
5509 
5510     def _refresh_instance_security_rules(self, context, instances):
5511         for instance in instances:
5512             if instance.host is not None:
5513                 self.compute_rpcapi.refresh_instance_security_rules(
5514                         context, instance, instance.host)
5515 
5516     def trigger_rules_refresh(self, context, id):
5517         """Called when a rule is added to or removed from a security_group."""
5518         instances = objects.InstanceList.get_by_security_group_id(context, id)
5519         self._refresh_instance_security_rules(context, instances)
5520 
5521     def trigger_members_refresh(self, context, group_ids):
5522         """Called when a security group gains a new or loses a member.
5523 
5524         Sends an update request to each compute node for each instance for
5525         which this is relevant.
5526         """
5527         instances = objects.InstanceList.get_by_grantee_security_group_ids(
5528             context, group_ids)
5529         self._refresh_instance_security_rules(context, instances)
5530 
5531     def get_instance_security_groups(self, context, instance, detailed=False):
5532         if detailed:
5533             return self.db.security_group_get_by_instance(context,
5534                                                           instance.uuid)
5535         return [{'name': group.name} for group in instance.security_groups]
