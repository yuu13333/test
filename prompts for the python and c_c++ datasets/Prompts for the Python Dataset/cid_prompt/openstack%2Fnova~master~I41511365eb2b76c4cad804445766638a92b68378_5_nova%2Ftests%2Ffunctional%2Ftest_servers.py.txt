Please review the code below for security defects. You can consider defect types in terms of:
1.CWE-284 (Improper Access Control)
2.CWE-435 (Improper Interaction Between Multiple Entities)
3.CWE-664 (Improper Control of a Resource Through its Lifetime)
4.CWE-682 (Incorrect Calculation)
5.CWE-691 (Insufficient Control Flow Management)
6.CWE-693 (Protection Mechanism Failure)
7.CWE-697 (Incorrect Comparison)
8.CWE-703 (Improper Check or Handling of Exceptional Conditions)
9.CWE-707 (Improper Neutralization)
10.CWE-710 (Improper Adherence to Coding Standards)
If any are found, please describe the security defect in detail and indicate the corresponding line number of code and solution. If none are detected, states: 'No security defects are detected in the code'.

1 # Copyright 2011 Justin Santa Barbara
2 # All Rights Reserved.
3 #
4 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
5 #    not use this file except in compliance with the License. You may obtain
6 #    a copy of the License at
7 #
8 #         http://www.apache.org/licenses/LICENSE-2.0
9 #
10 #    Unless required by applicable law or agreed to in writing, software
11 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
12 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
13 #    License for the specific language governing permissions and limitations
14 #    under the License.
15 from __future__ import absolute_import
16 
17 import collections
18 import copy
19 import datetime
20 import time
21 import zlib
22 
23 from keystoneauth1 import adapter
24 import mock
25 import os_resource_classes as orc
26 import os_traits
27 from oslo_config import cfg
28 from oslo_log import log as logging
29 from oslo_serialization import base64
30 from oslo_serialization import jsonutils
31 from oslo_utils.fixture import uuidsentinel as uuids
32 from oslo_utils import timeutils
33 import six
34 
35 from nova.compute import api as compute_api
36 from nova.compute import instance_actions
37 from nova.compute import manager as compute_manager
38 from nova.compute import rpcapi
39 from nova.conductor import manager
40 from nova import context
41 from nova import exception
42 from nova import objects
43 from nova.objects import block_device as block_device_obj
44 from nova.scheduler import utils
45 from nova.scheduler import weights
46 from nova import test
47 from nova.tests import fixtures as nova_fixtures
48 from nova.tests.functional.api import client
49 from nova.tests.functional import integrated_helpers
50 from nova.tests.unit.api.openstack import fakes
51 from nova.tests.unit import fake_block_device
52 from nova.tests.unit import fake_notifier
53 from nova.tests.unit import fake_requests
54 import nova.tests.unit.image.fake
55 from nova.tests.unit.objects import test_instance_info_cache
56 from nova.virt import fake
57 from nova import volume
58 
59 CONF = cfg.CONF
60 
61 LOG = logging.getLogger(__name__)
62 
63 
64 class AltHostWeigher(weights.BaseHostWeigher):
65     """Used in the alternate host tests to return a pre-determined list of
66     hosts.
67     """
68     def _weigh_object(self, host_state, weight_properties):
69         """Return a defined order of hosts."""
70         weights = {"selection": 999, "alt_host1": 888, "alt_host2": 777,
71                    "alt_host3": 666, "host1": 0, "host2": 0}
72         return weights.get(host_state.host, 0)
73 
74 
75 class ServersTestBase(integrated_helpers._IntegratedTestBase):
76     api_major_version = 'v2'
77     _force_delete_parameter = 'forceDelete'
78     _image_ref_parameter = 'imageRef'
79     _flavor_ref_parameter = 'flavorRef'
80     _access_ipv4_parameter = 'accessIPv4'
81     _access_ipv6_parameter = 'accessIPv6'
82     _return_resv_id_parameter = 'return_reservation_id'
83     _min_count_parameter = 'min_count'
84 
85     USE_NEUTRON = True
86 
87     def setUp(self):
88         self.computes = {}
89         super(ServersTestBase, self).setUp()
90         self.conductor = self.start_service(
91             'conductor', manager='nova.conductor.manager.ConductorManager')
92 
93     def _wait_for_state_change(self, server, from_status):
94         for i in range(0, 50):
95             server = self.api.get_server(server['id'])
96             if server['status'] != from_status:
97                 break
98             time.sleep(.1)
99 
100         return server
101 
102     def _wait_for_deletion(self, server_id):
103         # Wait (briefly) for deletion
104         for _retries in range(50):
105             try:
106                 found_server = self.api.get_server(server_id)
107             except client.OpenStackApiNotFoundException:
108                 found_server = None
109                 LOG.debug("Got 404, proceeding")
110                 break
111 
112             LOG.debug("Found_server=%s", found_server)
113 
114             # TODO(justinsb): Mock doesn't yet do accurate state changes
115             # if found_server['status'] != 'deleting':
116             #    break
117             time.sleep(.1)
118 
119         # Should be gone
120         self.assertFalse(found_server)
121 
122     def _delete_server(self, server_id):
123         # Delete the server
124         self.api.delete_server(server_id)
125         self._wait_for_deletion(server_id)
126 
127     def _get_access_ips_params(self):
128         return {self._access_ipv4_parameter: "172.19.0.2",
129                 self._access_ipv6_parameter: "fe80::2"}
130 
131     def _verify_access_ips(self, server):
132         self.assertEqual('172.19.0.2',
133                          server[self._access_ipv4_parameter])
134         self.assertEqual('fe80::2', server[self._access_ipv6_parameter])
135 
136 
137 class ServersTest(ServersTestBase):
138 
139     def test_get_servers(self):
140         # Simple check that listing servers works.
141         servers = self.api.get_servers()
142         for server in servers:
143             LOG.debug("server: %s", server)
144 
145     def _get_node_build_failures(self):
146         ctxt = context.get_admin_context()
147         computes = objects.ComputeNodeList.get_all(ctxt)
148         return {
149             node.hypervisor_hostname: int(node.stats.get('failed_builds', 0))
150             for node in computes}
151 
152     def _run_periodics(self):
153         """Run the update_available_resource task on every compute manager
154 
155         This runs periodics on the computes in an undefined order; some child
156         class redefined this function to force a specific order.
157         """
158 
159         if self.compute.host not in self.computes:
160             self.computes[self.compute.host] = self.compute
161 
162         ctx = context.get_admin_context()
163         for compute in self.computes.values():
164             LOG.info('Running periodic for compute (%s)',
165                 compute.manager.host)
166             compute.manager.update_available_resource(ctx)
167         LOG.info('Finished with periodics')
168 
169     def test_create_server_with_error(self):
170         # Create a server which will enter error state.
171 
172         def throw_error(*args, **kwargs):
173             raise exception.BuildAbortException(reason='',
174                     instance_uuid='fake')
175 
176         self.stub_out('nova.virt.fake.FakeDriver.spawn', throw_error)
177 
178         server = self._build_minimal_create_server_request()
179         created_server = self.api.post_server({"server": server})
180         created_server_id = created_server['id']
181 
182         found_server = self.api.get_server(created_server_id)
183         self.assertEqual(created_server_id, found_server['id'])
184 
185         found_server = self._wait_for_state_change(found_server, 'BUILD')
186 
187         self.assertEqual('ERROR', found_server['status'])
188         self._delete_server(created_server_id)
189 
190         # We should have no (persisted) build failures until we update
191         # resources, after which we should have one
192         self.assertEqual([0], list(self._get_node_build_failures().values()))
193         self._run_periodics()
194         self.assertEqual([1], list(self._get_node_build_failures().values()))
195 
196     def test_create_server_with_image_type_filter(self):
197         self.flags(query_placement_for_image_type_support=True,
198                    group='scheduler')
199 
200         raw_image = '155d900f-4e14-4e4c-a73d-069cbf4541e6'
201         vhd_image = 'a440c04b-79fa-479c-bed1-0b816eaec379'
202 
203         server = self._build_minimal_create_server_request(
204             image_uuid=vhd_image)
205         server = self.api.post_server({'server': server})
206         server = self.api.get_server(server['id'])
207         errored_server = self._wait_for_state_change(server, server['status'])
208         self.assertEqual('ERROR', errored_server['status'])
209         self.assertIn('No valid host', errored_server['fault']['message'])
210 
211         server = self._build_minimal_create_server_request(
212             image_uuid=raw_image)
213         server = self.api.post_server({'server': server})
214         server = self.api.get_server(server['id'])
215         created_server = self._wait_for_state_change(server, server['status'])
216         self.assertEqual('ACTIVE', created_server['status'])
217 
218     def _test_create_server_with_error_with_retries(self):
219         # Create a server which will enter error state.
220 
221         fake.set_nodes(['host2'])
222         self.addCleanup(fake.restore_nodes)
223         self.flags(host='host2')
224         self.compute2 = self.start_service('compute', host='host2')
225         self.computes['compute2'] = self.compute2
226 
227         fails = []
228 
229         def throw_error(*args, **kwargs):
230             fails.append('one')
231             raise test.TestingException('Please retry me')
232 
233         self.stub_out('nova.virt.fake.FakeDriver.spawn', throw_error)
234 
235         server = self._build_minimal_create_server_request()
236         created_server = self.api.post_server({"server": server})
237         created_server_id = created_server['id']
238 
239         found_server = self.api.get_server(created_server_id)
240         self.assertEqual(created_server_id, found_server['id'])
241 
242         found_server = self._wait_for_state_change(found_server, 'BUILD')
243 
244         self.assertEqual('ERROR', found_server['status'])
245         self._delete_server(created_server_id)
246 
247         return len(fails)
248 
249     def test_create_server_with_error_with_retries(self):
250         self.flags(max_attempts=2, group='scheduler')
251         fails = self._test_create_server_with_error_with_retries()
252         self.assertEqual(2, fails)
253         self._run_periodics()
254         self.assertEqual(
255             [1, 1], list(self._get_node_build_failures().values()))
256 
257     def test_create_server_with_error_with_no_retries(self):
258         self.flags(max_attempts=1, group='scheduler')
259         fails = self._test_create_server_with_error_with_retries()
260         self.assertEqual(1, fails)
261         self._run_periodics()
262         self.assertEqual(
263             [0, 1], list(sorted(self._get_node_build_failures().values())))
264 
265     def test_create_and_delete_server(self):
266         # Creates and deletes a server.
267 
268         # Create server
269         # Build the server data gradually, checking errors along the way
270         server = {}
271         good_server = self._build_minimal_create_server_request()
272 
273         post = {'server': server}
274 
275         # Without an imageRef, this throws 500.
276         # TODO(justinsb): Check whatever the spec says should be thrown here
277         self.assertRaises(client.OpenStackApiException,
278                           self.api.post_server, post)
279 
280         # With an invalid imageRef, this throws 500.
281         server[self._image_ref_parameter] = self.get_invalid_image()
282         # TODO(justinsb): Check whatever the spec says should be thrown here
283         self.assertRaises(client.OpenStackApiException,
284                           self.api.post_server, post)
285 
286         # Add a valid imageRef
287         server[self._image_ref_parameter] = good_server.get(
288             self._image_ref_parameter)
289 
290         # Without flavorRef, this throws 500
291         # TODO(justinsb): Check whatever the spec says should be thrown here
292         self.assertRaises(client.OpenStackApiException,
293                           self.api.post_server, post)
294 
295         server[self._flavor_ref_parameter] = good_server.get(
296             self._flavor_ref_parameter)
297 
298         # Without a name, this throws 500
299         # TODO(justinsb): Check whatever the spec says should be thrown here
300         self.assertRaises(client.OpenStackApiException,
301                           self.api.post_server, post)
302 
303         # Set a valid server name
304         server['name'] = good_server['name']
305 
306         created_server = self.api.post_server(post)
307         LOG.debug("created_server: %s", created_server)
308         self.assertTrue(created_server['id'])
309         created_server_id = created_server['id']
310 
311         # Check it's there
312         found_server = self.api.get_server(created_server_id)
313         self.assertEqual(created_server_id, found_server['id'])
314 
315         # It should also be in the all-servers list
316         servers = self.api.get_servers()
317         server_ids = [s['id'] for s in servers]
318         self.assertIn(created_server_id, server_ids)
319 
320         found_server = self._wait_for_state_change(found_server, 'BUILD')
321         # It should be available...
322         # TODO(justinsb): Mock doesn't yet do this...
323         self.assertEqual('ACTIVE', found_server['status'])
324         servers = self.api.get_servers(detail=True)
325         for server in servers:
326             self.assertIn("image", server)
327             self.assertIn("flavor", server)
328 
329         self._delete_server(created_server_id)
330 
331     def _force_reclaim(self):
332         # Make sure that compute manager thinks the instance is
333         # old enough to be expired
334         the_past = timeutils.utcnow() + datetime.timedelta(hours=1)
335         timeutils.set_time_override(override_time=the_past)
336         self.addCleanup(timeutils.clear_time_override)
337         ctxt = context.get_admin_context()
338         self.compute._reclaim_queued_deletes(ctxt)
339 
340     def test_deferred_delete(self):
341         # Creates, deletes and waits for server to be reclaimed.
342         self.flags(reclaim_instance_interval=1)
343 
344         # Create server
345         server = self._build_minimal_create_server_request()
346 
347         created_server = self.api.post_server({'server': server})
348         LOG.debug("created_server: %s", created_server)
349         self.assertTrue(created_server['id'])
350         created_server_id = created_server['id']
351 
352         # Wait for it to finish being created
353         found_server = self._wait_for_state_change(created_server, 'BUILD')
354 
355         # It should be available...
356         self.assertEqual('ACTIVE', found_server['status'])
357 
358         # Cannot restore unless instance is deleted
359         self.assertRaises(client.OpenStackApiException,
360                           self.api.post_server_action, created_server_id,
361                           {'restore': {}})
362 
363         # Delete the server
364         self.api.delete_server(created_server_id)
365 
366         # Wait for queued deletion
367         found_server = self._wait_for_state_change(found_server, 'ACTIVE')
368         self.assertEqual('SOFT_DELETED', found_server['status'])
369 
370         self._force_reclaim()
371 
372         # Wait for real deletion
373         self._wait_for_deletion(created_server_id)
374 
375     def test_deferred_delete_restore(self):
376         # Creates, deletes and restores a server.
377         self.flags(reclaim_instance_interval=3600)
378 
379         # Create server
380         server = self._build_minimal_create_server_request()
381 
382         created_server = self.api.post_server({'server': server})
383         LOG.debug("created_server: %s", created_server)
384         self.assertTrue(created_server['id'])
385         created_server_id = created_server['id']
386 
387         # Wait for it to finish being created
388         found_server = self._wait_for_state_change(created_server, 'BUILD')
389 
390         # It should be available...
391         self.assertEqual('ACTIVE', found_server['status'])
392 
393         # Delete the server
394         self.api.delete_server(created_server_id)
395 
396         # Wait for queued deletion
397         found_server = self._wait_for_state_change(found_server, 'ACTIVE')
398         self.assertEqual('SOFT_DELETED', found_server['status'])
399 
400         # Restore server
401         self.api.post_server_action(created_server_id, {'restore': {}})
402 
403         # Wait for server to become active again
404         found_server = self._wait_for_state_change(found_server, 'DELETED')
405         self.assertEqual('ACTIVE', found_server['status'])
406 
407     def test_deferred_delete_restore_overquota(self):
408         # Test that a restore that would put the user over quota fails
409         self.flags(instances=1, group='quota')
410         # Creates, deletes and restores a server.
411         self.flags(reclaim_instance_interval=3600)
412 
413         # Create server
414         server = self._build_minimal_create_server_request()
415 
416         created_server1 = self.api.post_server({'server': server})
417         LOG.debug("created_server: %s", created_server1)
418         self.assertTrue(created_server1['id'])
419         created_server_id1 = created_server1['id']
420 
421         # Wait for it to finish being created
422         found_server1 = self._wait_for_state_change(created_server1, 'BUILD')
423 
424         # It should be available...
425         self.assertEqual('ACTIVE', found_server1['status'])
426 
427         # Delete the server
428         self.api.delete_server(created_server_id1)
429 
430         # Wait for queued deletion
431         found_server1 = self._wait_for_state_change(found_server1, 'ACTIVE')
432         self.assertEqual('SOFT_DELETED', found_server1['status'])
433 
434         # Create a second server
435         server = self._build_minimal_create_server_request()
436 
437         created_server2 = self.api.post_server({'server': server})
438         LOG.debug("created_server: %s", created_server2)
439         self.assertTrue(created_server2['id'])
440 
441         # Wait for it to finish being created
442         found_server2 = self._wait_for_state_change(created_server2, 'BUILD')
443 
444         # It should be available...
445         self.assertEqual('ACTIVE', found_server2['status'])
446 
447         # Try to restore the first server, it should fail
448         ex = self.assertRaises(client.OpenStackApiException,
449                                self.api.post_server_action,
450                                created_server_id1, {'restore': {}})
451         self.assertEqual(403, ex.response.status_code)
452         self.assertEqual('SOFT_DELETED', found_server1['status'])
453 
454     def test_deferred_delete_force(self):
455         # Creates, deletes and force deletes a server.
456         self.flags(reclaim_instance_interval=3600)
457 
458         # Create server
459         server = self._build_minimal_create_server_request()
460 
461         created_server = self.api.post_server({'server': server})
462         LOG.debug("created_server: %s", created_server)
463         self.assertTrue(created_server['id'])
464         created_server_id = created_server['id']
465 
466         # Wait for it to finish being created
467         found_server = self._wait_for_state_change(created_server, 'BUILD')
468 
469         # It should be available...
470         self.assertEqual('ACTIVE', found_server['status'])
471 
472         # Delete the server
473         self.api.delete_server(created_server_id)
474 
475         # Wait for queued deletion
476         found_server = self._wait_for_state_change(found_server, 'ACTIVE')
477         self.assertEqual('SOFT_DELETED', found_server['status'])
478 
479         # Force delete server
480         self.api.post_server_action(created_server_id,
481                                     {self._force_delete_parameter: {}})
482 
483         # Wait for real deletion
484         self._wait_for_deletion(created_server_id)
485 
486     def test_create_server_with_metadata(self):
487         # Creates a server with metadata.
488 
489         # Build the server data gradually, checking errors along the way
490         server = self._build_minimal_create_server_request()
491 
492         metadata = {}
493         for i in range(30):
494             metadata['key_%s' % i] = 'value_%s' % i
495 
496         server['metadata'] = metadata
497 
498         post = {'server': server}
499         created_server = self.api.post_server(post)
500         LOG.debug("created_server: %s", created_server)
501         self.assertTrue(created_server['id'])
502         created_server_id = created_server['id']
503 
504         found_server = self.api.get_server(created_server_id)
505         self.assertEqual(created_server_id, found_server['id'])
506         self.assertEqual(metadata, found_server.get('metadata'))
507 
508         # The server should also be in the all-servers details list
509         servers = self.api.get_servers(detail=True)
510         server_map = {server['id']: server for server in servers}
511         found_server = server_map.get(created_server_id)
512         self.assertTrue(found_server)
513         # Details do include metadata
514         self.assertEqual(metadata, found_server.get('metadata'))
515 
516         # The server should also be in the all-servers summary list
517         servers = self.api.get_servers(detail=False)
518         server_map = {server['id']: server for server in servers}
519         found_server = server_map.get(created_server_id)
520         self.assertTrue(found_server)
521         # Summary should not include metadata
522         self.assertFalse(found_server.get('metadata'))
523 
524         # Cleanup
525         self._delete_server(created_server_id)
526 
527     def test_server_metadata_actions_negative_invalid_state(self):
528         # Create server with metadata
529         server = self._build_minimal_create_server_request()
530 
531         metadata = {'key_1': 'value_1'}
532 
533         server['metadata'] = metadata
534 
535         post = {'server': server}
536         created_server = self.api.post_server(post)
537 
538         found_server = self._wait_for_state_change(created_server, 'BUILD')
539         self.assertEqual('ACTIVE', found_server['status'])
540         self.assertEqual(metadata, found_server.get('metadata'))
541         server_id = found_server['id']
542 
543         # Change status from ACTIVE to SHELVED for negative test
544         self.flags(shelved_offload_time = -1)
545         self.api.post_server_action(server_id, {'shelve': {}})
546         found_server = self._wait_for_state_change(found_server, 'ACTIVE')
547         self.assertEqual('SHELVED', found_server['status'])
548 
549         metadata = {'key_2': 'value_2'}
550 
551         # Update Metadata item in SHELVED (not ACTIVE, etc.)
552         ex = self.assertRaises(client.OpenStackApiException,
553                                self.api.post_server_metadata,
554                                server_id, metadata)
555         self.assertEqual(409, ex.response.status_code)
556         self.assertEqual('SHELVED', found_server['status'])
557 
558         # Delete Metadata item in SHELVED (not ACTIVE, etc.)
559         ex = self.assertRaises(client.OpenStackApiException,
560                                self.api.delete_server_metadata,
561                                server_id, 'key_1')
562         self.assertEqual(409, ex.response.status_code)
563         self.assertEqual('SHELVED', found_server['status'])
564 
565         # Cleanup
566         self._delete_server(server_id)
567 
568     def test_create_and_rebuild_server(self):
569         # Rebuild a server with metadata.
570 
571         # create a server with initially has no metadata
572         server = self._build_minimal_create_server_request()
573         server_post = {'server': server}
574 
575         metadata = {}
576         for i in range(30):
577             metadata['key_%s' % i] = 'value_%s' % i
578 
579         server_post['server']['metadata'] = metadata
580 
581         created_server = self.api.post_server(server_post)
582         LOG.debug("created_server: %s", created_server)
583         self.assertTrue(created_server['id'])
584         created_server_id = created_server['id']
585 
586         created_server = self._wait_for_state_change(created_server, 'BUILD')
587 
588         # rebuild the server with metadata and other server attributes
589         post = {}
590         post['rebuild'] = {
591             self._image_ref_parameter: "76fa36fc-c930-4bf3-8c8a-ea2a2420deb6",
592             "name": "blah",
593             self._access_ipv4_parameter: "172.19.0.2",
594             self._access_ipv6_parameter: "fe80::2",
595             "metadata": {'some': 'thing'},
596         }
597         post['rebuild'].update(self._get_access_ips_params())
598 
599         self.api.post_server_action(created_server_id, post)
600         LOG.debug("rebuilt server: %s", created_server)
601         self.assertTrue(created_server['id'])
602 
603         found_server = self.api.get_server(created_server_id)
604         self.assertEqual(created_server_id, found_server['id'])
605         self.assertEqual({'some': 'thing'}, found_server.get('metadata'))
606         self.assertEqual('blah', found_server.get('name'))
607         self.assertEqual(post['rebuild'][self._image_ref_parameter],
608                          found_server.get('image')['id'])
609         self._verify_access_ips(found_server)
610 
611         # rebuild the server with empty metadata and nothing else
612         post = {}
613         post['rebuild'] = {
614             self._image_ref_parameter: "76fa36fc-c930-4bf3-8c8a-ea2a2420deb6",
615             "metadata": {},
616         }
617 
618         self.api.post_server_action(created_server_id, post)
619         LOG.debug("rebuilt server: %s", created_server)
620         self.assertTrue(created_server['id'])
621 
622         found_server = self.api.get_server(created_server_id)
623         self.assertEqual(created_server_id, found_server['id'])
624         self.assertEqual({}, found_server.get('metadata'))
625         self.assertEqual('blah', found_server.get('name'))
626         self.assertEqual(post['rebuild'][self._image_ref_parameter],
627                          found_server.get('image')['id'])
628         self._verify_access_ips(found_server)
629 
630         # Cleanup
631         self._delete_server(created_server_id)
632 
633     def test_rename_server(self):
634         # Test building and renaming a server.
635 
636         # Create a server
637         server = self._build_minimal_create_server_request()
638         created_server = self.api.post_server({'server': server})
639         LOG.debug("created_server: %s", created_server)
640         server_id = created_server['id']
641         self.assertTrue(server_id)
642 
643         # Rename the server to 'new-name'
644         self.api.put_server(server_id, {'server': {'name': 'new-name'}})
645 
646         # Check the name of the server
647         created_server = self.api.get_server(server_id)
648         self.assertEqual(created_server['name'], 'new-name')
649 
650         # Cleanup
651         self._delete_server(server_id)
652 
653     def test_create_multiple_servers(self):
654         # Creates multiple servers and checks for reservation_id.
655 
656         # Create 2 servers, setting 'return_reservation_id, which should
657         # return a reservation_id
658         server = self._build_minimal_create_server_request()
659         server[self._min_count_parameter] = 2
660         server[self._return_resv_id_parameter] = True
661         post = {'server': server}
662         response = self.api.post_server(post)
663         self.assertIn('reservation_id', response)
664         reservation_id = response['reservation_id']
665         self.assertNotIn(reservation_id, ['', None])
666         # Assert that the reservation_id itself has the expected format
667         self.assertRegex(reservation_id, 'r-[0-9a-zA-Z]{8}')
668 
669         # Create 1 more server, which should not return a reservation_id
670         server = self._build_minimal_create_server_request()
671         post = {'server': server}
672         created_server = self.api.post_server(post)
673         self.assertTrue(created_server['id'])
674         created_server_id = created_server['id']
675 
676         # lookup servers created by the first request.
677         servers = self.api.get_servers(detail=True,
678                 search_opts={'reservation_id': reservation_id})
679         server_map = {server['id']: server for server in servers}
680         found_server = server_map.get(created_server_id)
681         # The server from the 2nd request should not be there.
682         self.assertIsNone(found_server)
683         # Should have found 2 servers.
684         self.assertEqual(len(server_map), 2)
685 
686         # Cleanup
687         self._delete_server(created_server_id)
688         for server_id in server_map:
689             self._delete_server(server_id)
690 
691     def test_create_server_with_injected_files(self):
692         # Creates a server with injected_files.
693         personality = []
694 
695         # Inject a text file
696         data = 'Hello, World!'
697         personality.append({
698             'path': '/helloworld.txt',
699             'contents': base64.encode_as_bytes(data),
700         })
701 
702         # Inject a binary file
703         data = zlib.compress(b'Hello, World!')
704         personality.append({
705             'path': '/helloworld.zip',
706             'contents': base64.encode_as_bytes(data),
707         })
708 
709         # Create server
710         server = self._build_minimal_create_server_request()
711         server['personality'] = personality
712 
713         post = {'server': server}
714 
715         created_server = self.api.post_server(post)
716         LOG.debug("created_server: %s", created_server)
717         self.assertTrue(created_server['id'])
718         created_server_id = created_server['id']
719 
720         # Check it's there
721         found_server = self.api.get_server(created_server_id)
722         self.assertEqual(created_server_id, found_server['id'])
723 
724         found_server = self._wait_for_state_change(found_server, 'BUILD')
725         self.assertEqual('ACTIVE', found_server['status'])
726 
727         # Cleanup
728         self._delete_server(created_server_id)
729 
730     def test_stop_start_servers_negative_invalid_state(self):
731         # Create server
732         server = self._build_minimal_create_server_request()
733         created_server = self.api.post_server({"server": server})
734         created_server_id = created_server['id']
735 
736         found_server = self._wait_for_state_change(created_server, 'BUILD')
737         self.assertEqual('ACTIVE', found_server['status'])
738 
739         # Start server in ACTIVE
740         # NOTE(mkoshiya): When os-start API runs, the server status
741         # must be SHUTOFF.
742         # By returning 409, I want to confirm that the ACTIVE server does not
743         # cause unexpected behavior.
744         post = {'os-start': {}}
745         ex = self.assertRaises(client.OpenStackApiException,
746                                self.api.post_server_action,
747                                created_server_id, post)
748         self.assertEqual(409, ex.response.status_code)
749         self.assertEqual('ACTIVE', found_server['status'])
750 
751         # Stop server
752         post = {'os-stop': {}}
753         self.api.post_server_action(created_server_id, post)
754         found_server = self._wait_for_state_change(found_server, 'ACTIVE')
755         self.assertEqual('SHUTOFF', found_server['status'])
756 
757         # Stop server in SHUTOFF
758         # NOTE(mkoshiya): When os-stop API runs, the server status
759         # must be ACTIVE or ERROR.
760         # By returning 409, I want to confirm that the SHUTOFF server does not
761         # cause unexpected behavior.
762         post = {'os-stop': {}}
763         ex = self.assertRaises(client.OpenStackApiException,
764                                self.api.post_server_action,
765                                created_server_id, post)
766         self.assertEqual(409, ex.response.status_code)
767         self.assertEqual('SHUTOFF', found_server['status'])
768 
769         # Cleanup
770         self._delete_server(created_server_id)
771 
772     def test_revert_resized_server_negative_invalid_state(self):
773         # Create server
774         server = self._build_minimal_create_server_request()
775         created_server = self.api.post_server({"server": server})
776         created_server_id = created_server['id']
777         found_server = self._wait_for_state_change(created_server, 'BUILD')
778         self.assertEqual('ACTIVE', found_server['status'])
779 
780         # Revert resized server in ACTIVE
781         # NOTE(yatsumi): When revert resized server API runs,
782         # the server status must be VERIFY_RESIZE.
783         # By returning 409, I want to confirm that the ACTIVE server does not
784         # cause unexpected behavior.
785         post = {'revertResize': {}}
786         ex = self.assertRaises(client.OpenStackApiException,
787                                self.api.post_server_action,
788                                created_server_id, post)
789         self.assertEqual(409, ex.response.status_code)
790         self.assertEqual('ACTIVE', found_server['status'])
791 
792         # Cleanup
793         self._delete_server(created_server_id)
794 
795     def test_resize_server_negative_invalid_state(self):
796         # Avoid migration
797         self.flags(allow_resize_to_same_host=True)
798 
799         # Create server
800         server = self._build_minimal_create_server_request()
801         created_server = self.api.post_server({"server": server})
802         created_server_id = created_server['id']
803         found_server = self._wait_for_state_change(created_server, 'BUILD')
804         self.assertEqual('ACTIVE', found_server['status'])
805 
806         # Resize server(flavorRef: 1 -> 2)
807         post = {'resize': {"flavorRef": "2", "OS-DCF:diskConfig": "AUTO"}}
808         self.api.post_server_action(created_server_id, post)
809         found_server = self._wait_for_state_change(found_server, 'RESIZE')
810         self.assertEqual('VERIFY_RESIZE', found_server['status'])
811 
812         # Resize server in VERIFY_RESIZE(flavorRef: 2 -> 1)
813         # NOTE(yatsumi): When resize API runs, the server status
814         # must be ACTIVE or SHUTOFF.
815         # By returning 409, I want to confirm that the VERIFY_RESIZE server
816         # does not cause unexpected behavior.
817         post = {'resize': {"flavorRef": "1", "OS-DCF:diskConfig": "AUTO"}}
818         ex = self.assertRaises(client.OpenStackApiException,
819                                self.api.post_server_action,
820                                created_server_id, post)
821         self.assertEqual(409, ex.response.status_code)
822         self.assertEqual('VERIFY_RESIZE', found_server['status'])
823 
824         # Cleanup
825         self._delete_server(created_server_id)
826 
827     def test_confirm_resized_server_negative_invalid_state(self):
828         # Create server
829         server = self._build_minimal_create_server_request()
830         created_server = self.api.post_server({"server": server})
831         created_server_id = created_server['id']
832         found_server = self._wait_for_state_change(created_server, 'BUILD')
833         self.assertEqual('ACTIVE', found_server['status'])
834 
835         # Confirm resized server in ACTIVE
836         # NOTE(yatsumi): When confirm resized server API runs,
837         # the server status must be VERIFY_RESIZE.
838         # By returning 409, I want to confirm that the ACTIVE server does not
839         # cause unexpected behavior.
840         post = {'confirmResize': {}}
841         ex = self.assertRaises(client.OpenStackApiException,
842                                self.api.post_server_action,
843                                created_server_id, post)
844         self.assertEqual(409, ex.response.status_code)
845         self.assertEqual('ACTIVE', found_server['status'])
846 
847         # Cleanup
848         self._delete_server(created_server_id)
849 
850     def test_resize_server_overquota(self):
851         self.flags(cores=1, group='quota')
852         self.flags(ram=512, group='quota')
853         # Create server with default flavor, 1 core, 512 ram
854         server = self._build_minimal_create_server_request()
855         created_server = self.api.post_server({"server": server})
856         created_server_id = created_server['id']
857 
858         found_server = self._wait_for_state_change(created_server, 'BUILD')
859         self.assertEqual('ACTIVE', found_server['status'])
860 
861         # Try to resize to flavorid 2, 1 core, 2048 ram
862         post = {'resize': {'flavorRef': '2'}}
863         ex = self.assertRaises(client.OpenStackApiException,
864                                self.api.post_server_action,
865                                created_server_id, post)
866         self.assertEqual(403, ex.response.status_code)
867 
868     def test_attach_vol_maximum_disk_devices_exceeded(self):
869         self.useFixture(nova_fixtures.CinderFixtureNewAttachFlow(self))
870 
871         server = self._build_minimal_create_server_request()
872         created_server = self.api.post_server({"server": server})
873         server_id = created_server['id']
874         self._wait_for_state_change(created_server, 'BUILD')
875 
876         volume_id = '9a695496-44aa-4404-b2cc-ccab2501f87e'
877         LOG.info('Attaching volume %s to server %s', volume_id, server_id)
878 
879         # The fake driver doesn't implement get_device_name_for_instance, so
880         # we'll just raise the exception directly here, instead of simuluating
881         # an instance with 26 disk devices already attached.
882         with mock.patch.object(self.compute.driver,
883                                'get_device_name_for_instance') as mock_get:
884             mock_get.side_effect = exception.TooManyDiskDevices(maximum=26)
885             ex = self.assertRaises(
886                 client.OpenStackApiException, self.api.post_server_volume,
887                 server_id, dict(volumeAttachment=dict(volumeId=volume_id)))
888             expected = ('The maximum allowed number of disk devices (26) to '
889                         'attach to a single instance has been exceeded.')
890             self.assertEqual(403, ex.response.status_code)
891             self.assertIn(expected, six.text_type(ex))
892 
893 
894 class ServersTestV21(ServersTest):
895     api_major_version = 'v2.1'
896 
897 
898 class ServersTestV219(ServersTestBase):
899     api_major_version = 'v2.1'
900 
901     def _create_server(self, set_desc = True, desc = None):
902         server = self._build_minimal_create_server_request()
903         if set_desc:
904             server['description'] = desc
905         post = {'server': server}
906         response = self.api.api_post('/servers', post).body
907         return (server, response['server'])
908 
909     def _update_server(self, server_id, set_desc = True, desc = None):
910         new_name = integrated_helpers.generate_random_alphanumeric(8)
911         server = {'server': {'name': new_name}}
912         if set_desc:
913             server['server']['description'] = desc
914         self.api.api_put('/servers/%s' % server_id, server)
915 
916     def _rebuild_server(self, server_id, set_desc = True, desc = None):
917         new_name = integrated_helpers.generate_random_alphanumeric(8)
918         post = {}
919         post['rebuild'] = {
920             "name": new_name,
921             self._image_ref_parameter: "76fa36fc-c930-4bf3-8c8a-ea2a2420deb6",
922             self._access_ipv4_parameter: "172.19.0.2",
923             self._access_ipv6_parameter: "fe80::2",
924             "metadata": {'some': 'thing'},
925         }
926         post['rebuild'].update(self._get_access_ips_params())
927         if set_desc:
928             post['rebuild']['description'] = desc
929         self.api.api_post('/servers/%s/action' % server_id, post)
930 
931     def _create_server_and_verify(self, set_desc = True, expected_desc = None):
932         # Creates a server with a description and verifies it is
933         # in the GET responses.
934         created_server_id = self._create_server(set_desc,
935                                                 expected_desc)[1]['id']
936         self._verify_server_description(created_server_id, expected_desc)
937         self._delete_server(created_server_id)
938 
939     def _update_server_and_verify(self, server_id, set_desc = True,
940                                   expected_desc = None):
941         # Updates a server with a description and verifies it is
942         # in the GET responses.
943         self._update_server(server_id, set_desc, expected_desc)
944         self._verify_server_description(server_id, expected_desc)
945 
946     def _rebuild_server_and_verify(self, server_id, set_desc = True,
947                                   expected_desc = None):
948         # Rebuilds a server with a description and verifies it is
949         # in the GET responses.
950         self._rebuild_server(server_id, set_desc, expected_desc)
951         self._verify_server_description(server_id, expected_desc)
952 
953     def _verify_server_description(self, server_id, expected_desc = None,
954                                    desc_in_resp = True):
955         # Calls GET on the servers and verifies that the description
956         # is set as expected in the response, or not set at all.
957         response = self.api.api_get('/servers/%s' % server_id)
958         found_server = response.body['server']
959         self.assertEqual(server_id, found_server['id'])
960         if desc_in_resp:
961             # Verify the description is set as expected (can be None)
962             self.assertEqual(expected_desc, found_server.get('description'))
963         else:
964             # Verify the description is not included in the response.
965             self.assertNotIn('description', found_server)
966 
967         servers = self.api.api_get('/servers/detail').body['servers']
968         server_map = {server['id']: server for server in servers}
969         found_server = server_map.get(server_id)
970         self.assertTrue(found_server)
971         if desc_in_resp:
972             # Verify the description is set as expected (can be None)
973             self.assertEqual(expected_desc, found_server.get('description'))
974         else:
975             # Verify the description is not included in the response.
976             self.assertNotIn('description', found_server)
977 
978     def _create_assertRaisesRegex(self, desc):
979         # Verifies that a 400 error is thrown on create server
980         with self.assertRaisesRegex(client.OpenStackApiException,
981                                     ".*Unexpected status code.*") as cm:
982             self._create_server(True, desc)
983             self.assertEqual(400, cm.exception.response.status_code)
984 
985     def _update_assertRaisesRegex(self, server_id, desc):
986         # Verifies that a 400 error is thrown on update server
987         with self.assertRaisesRegex(client.OpenStackApiException,
988                                     ".*Unexpected status code.*") as cm:
989             self._update_server(server_id, True, desc)
990             self.assertEqual(400, cm.exception.response.status_code)
991 
992     def _rebuild_assertRaisesRegex(self, server_id, desc):
993         # Verifies that a 400 error is thrown on rebuild server
994         with self.assertRaisesRegex(client.OpenStackApiException,
995                                     ".*Unexpected status code.*") as cm:
996             self._rebuild_server(server_id, True, desc)
997             self.assertEqual(400, cm.exception.response.status_code)
998 
999     def test_create_server_with_description(self):
1000         self.api.microversion = '2.19'
1001         # Create and get a server with a description
1002         self._create_server_and_verify(True, 'test description')
1003         # Create and get a server with an empty description
1004         self._create_server_and_verify(True, '')
1005         # Create and get a server with description set to None
1006         self._create_server_and_verify()
1007         # Create and get a server without setting the description
1008         self._create_server_and_verify(False)
1009 
1010     def test_update_server_with_description(self):
1011         self.api.microversion = '2.19'
1012         # Create a server with an initial description
1013         server_id = self._create_server(True, 'test desc 1')[1]['id']
1014 
1015         # Update and get the server with a description
1016         self._update_server_and_verify(server_id, True, 'updated desc')
1017         # Update and get the server name without changing the description
1018         self._update_server_and_verify(server_id, False, 'updated desc')
1019         # Update and get the server with an empty description
1020         self._update_server_and_verify(server_id, True, '')
1021         # Update and get the server by removing the description (set to None)
1022         self._update_server_and_verify(server_id)
1023         # Update and get the server with a 2nd new description
1024         self._update_server_and_verify(server_id, True, 'updated desc2')
1025 
1026         # Cleanup
1027         self._delete_server(server_id)
1028 
1029     def test_rebuild_server_with_description(self):
1030         self.api.microversion = '2.19'
1031 
1032         # Create a server with an initial description
1033         server = self._create_server(True, 'test desc 1')[1]
1034         server_id = server['id']
1035         self._wait_for_state_change(server, 'BUILD')
1036 
1037         # Rebuild and get the server with a description
1038         self._rebuild_server_and_verify(server_id, True, 'updated desc')
1039         # Rebuild and get the server name without changing the description
1040         self._rebuild_server_and_verify(server_id, False, 'updated desc')
1041         # Rebuild and get the server with an empty description
1042         self._rebuild_server_and_verify(server_id, True, '')
1043         # Rebuild and get the server by removing the description (set to None)
1044         self._rebuild_server_and_verify(server_id)
1045         # Rebuild and get the server with a 2nd new description
1046         self._rebuild_server_and_verify(server_id, True, 'updated desc2')
1047 
1048         # Cleanup
1049         self._delete_server(server_id)
1050 
1051     def test_version_compatibility(self):
1052         # Create a server with microversion v2.19 and a description.
1053         self.api.microversion = '2.19'
1054         server_id = self._create_server(True, 'test desc 1')[1]['id']
1055         # Verify that the description is not included on V2.18 GETs
1056         self.api.microversion = '2.18'
1057         self._verify_server_description(server_id, desc_in_resp = False)
1058         # Verify that updating the server with description on V2.18
1059         # results in a 400 error
1060         self._update_assertRaisesRegex(server_id, 'test update 2.18')
1061         # Verify that rebuilding the server with description on V2.18
1062         # results in a 400 error
1063         self._rebuild_assertRaisesRegex(server_id, 'test rebuild 2.18')
1064 
1065         # Cleanup
1066         self._delete_server(server_id)
1067 
1068         # Create a server on V2.18 and verify that the description
1069         # defaults to the name on a V2.19 GET
1070         server_req, response = self._create_server(False)
1071         server_id = response['id']
1072         self.api.microversion = '2.19'
1073         self._verify_server_description(server_id, server_req['name'])
1074 
1075         # Cleanup
1076         self._delete_server(server_id)
1077 
1078         # Verify that creating a server with description on V2.18
1079         # results in a 400 error
1080         self.api.microversion = '2.18'
1081         self._create_assertRaisesRegex('test create 2.18')
1082 
1083     def test_description_errors(self):
1084         self.api.microversion = '2.19'
1085         # Create servers with invalid descriptions.  These throw 400.
1086         # Invalid unicode with non-printable control char
1087         self._create_assertRaisesRegex(u'invalid\0dstring')
1088         # Description is longer than 255 chars
1089         self._create_assertRaisesRegex('x' * 256)
1090 
1091         # Update and rebuild servers with invalid descriptions.
1092         # These throw 400.
1093         server_id = self._create_server(True, "desc")[1]['id']
1094         # Invalid unicode with non-printable control char
1095         self._update_assertRaisesRegex(server_id, u'invalid\u0604string')
1096         self._rebuild_assertRaisesRegex(server_id, u'invalid\u0604string')
1097         # Description is longer than 255 chars
1098         self._update_assertRaisesRegex(server_id, 'x' * 256)
1099         self._rebuild_assertRaisesRegex(server_id, 'x' * 256)
1100 
1101 
1102 class ServerTestV220(ServersTestBase):
1103     api_major_version = 'v2.1'
1104 
1105     def setUp(self):
1106         super(ServerTestV220, self).setUp()
1107         self.api.microversion = '2.20'
1108         self.ctxt = context.get_admin_context()
1109 
1110     def _create_server(self):
1111         server = self._build_minimal_create_server_request()
1112         post = {'server': server}
1113         response = self.api.api_post('/servers', post).body
1114         return (server, response['server'])
1115 
1116     def _shelve_server(self):
1117         server = self._create_server()[1]
1118         server_id = server['id']
1119         self._wait_for_state_change(server, 'BUILD')
1120         self.api.post_server_action(server_id, {'shelve': None})
1121         return self._wait_for_state_change(server, 'ACTIVE')
1122 
1123     def _get_fake_bdms(self, ctxt):
1124         return block_device_obj.block_device_make_list(self.ctxt,
1125                     [fake_block_device.FakeDbBlockDeviceDict(
1126                     {'device_name': '/dev/vda',
1127                      'source_type': 'volume',
1128                      'destination_type': 'volume',
1129                      'volume_id': '5d721593-f033-4f6d-ab6f-b5b067e61bc4'})])
1130 
1131     def test_attach_detach_vol_to_shelved_server(self):
1132         self.flags(shelved_offload_time=-1)
1133         found_server = self._shelve_server()
1134         self.assertEqual('SHELVED', found_server['status'])
1135         server_id = found_server['id']
1136 
1137         # Test attach volume
1138         self.stub_out('nova.volume.cinder.API.get', fakes.stub_volume_get)
1139         with test.nested(mock.patch.object(volume.cinder,
1140                                        'is_microversion_supported'),
1141                          mock.patch.object(compute_api.API,
1142                                        '_check_attach_and_reserve_volume'),
1143                          mock.patch.object(rpcapi.ComputeAPI,
1144                                        'attach_volume')) as (mock_cinder_mv,
1145                                                              mock_reserve,
1146                                                              mock_attach):
1147             mock_cinder_mv.side_effect = \
1148                 exception.CinderAPIVersionNotAvailable(version='3.44')
1149             volume_attachment = {"volumeAttachment": {"volumeId":
1150                                        "5d721593-f033-4f6d-ab6f-b5b067e61bc4"}}
1151             self.api.api_post(
1152                             '/servers/%s/os-volume_attachments' % (server_id),
1153                             volume_attachment)
1154             self.assertTrue(mock_reserve.called)
1155             self.assertTrue(mock_attach.called)
1156 
1157         # Test detach volume
1158         with test.nested(mock.patch.object(volume.cinder.API,
1159                                            'begin_detaching'),
1160                          mock.patch.object(objects.BlockDeviceMappingList,
1161                                            'get_by_instance_uuid'),
1162                          mock.patch.object(rpcapi.ComputeAPI,
1163                                            'detach_volume')
1164                          ) as (mock_check, mock_get_bdms, mock_rpc):
1165 
1166             mock_get_bdms.return_value = self._get_fake_bdms(self.ctxt)
1167             attachment_id = mock_get_bdms.return_value[0]['volume_id']
1168 
1169             self.api.api_delete('/servers/%s/os-volume_attachments/%s' %
1170                             (server_id, attachment_id))
1171             self.assertTrue(mock_check.called)
1172             self.assertTrue(mock_rpc.called)
1173 
1174         self._delete_server(server_id)
1175 
1176     def test_attach_detach_vol_to_shelved_offloaded_server(self):
1177         self.flags(shelved_offload_time=0)
1178         found_server = self._shelve_server()
1179         self.assertEqual('SHELVED_OFFLOADED', found_server['status'])
1180         server_id = found_server['id']
1181 
1182         # Test attach volume
1183         self.stub_out('nova.volume.cinder.API.get', fakes.stub_volume_get)
1184         with test.nested(mock.patch.object(volume.cinder,
1185                                        'is_microversion_supported'),
1186                          mock.patch.object(compute_api.API,
1187                                        '_check_attach_and_reserve_volume'),
1188                          mock.patch.object(volume.cinder.API,
1189                                        'attach')) as (mock_cinder_mv,
1190                                                       mock_reserve, mock_vol):
1191             mock_cinder_mv.side_effect = \
1192                 exception.CinderAPIVersionNotAvailable(version='3.44')
1193             volume_attachment = {"volumeAttachment": {"volumeId":
1194                                        "5d721593-f033-4f6d-ab6f-b5b067e61bc4"}}
1195             attach_response = self.api.api_post(
1196                              '/servers/%s/os-volume_attachments' % (server_id),
1197                              volume_attachment).body['volumeAttachment']
1198             self.assertTrue(mock_reserve.called)
1199             self.assertTrue(mock_vol.called)
1200             self.assertIsNone(attach_response['device'])
1201 
1202         # Test detach volume
1203         with test.nested(mock.patch.object(volume.cinder.API,
1204                                            'begin_detaching'),
1205                          mock.patch.object(objects.BlockDeviceMappingList,
1206                                            'get_by_instance_uuid'),
1207                          mock.patch.object(compute_api.API,
1208                                            '_local_cleanup_bdm_volumes')
1209                          ) as (mock_check, mock_get_bdms, mock_clean_vols):
1210 
1211             mock_get_bdms.return_value = self._get_fake_bdms(self.ctxt)
1212             attachment_id = mock_get_bdms.return_value[0]['volume_id']
1213             self.api.api_delete('/servers/%s/os-volume_attachments/%s' %
1214                             (server_id, attachment_id))
1215             self.assertTrue(mock_check.called)
1216             self.assertTrue(mock_clean_vols.called)
1217 
1218         self._delete_server(server_id)
1219 
1220     def test_attach_detach_vol_to_shelved_offloaded_server_new_flow(self):
1221         self.flags(shelved_offload_time=0)
1222         found_server = self._shelve_server()
1223         self.assertEqual('SHELVED_OFFLOADED', found_server['status'])
1224         server_id = found_server['id']
1225         fake_bdms = self._get_fake_bdms(self.ctxt)
1226 
1227         # Test attach volume
1228         self.stub_out('nova.volume.cinder.API.get', fakes.stub_volume_get)
1229         with test.nested(mock.patch.object(volume.cinder,
1230                                        'is_microversion_supported'),
1231                          mock.patch.object(compute_api.API,
1232                             '_check_volume_already_attached_to_instance'),
1233                          mock.patch.object(volume.cinder.API,
1234                                         'check_availability_zone'),
1235                          mock.patch.object(volume.cinder.API,
1236                                         'attachment_create'),
1237                          mock.patch.object(volume.cinder.API,
1238                                         'attachment_complete')
1239                          ) as (mock_cinder_mv, mock_check_vol_attached,
1240                                mock_check_av_zone, mock_attach_create,
1241                                mock_attachment_complete):
1242             mock_attach_create.return_value = {'id': uuids.volume}
1243             volume_attachment = {"volumeAttachment": {"volumeId":
1244                                        "5d721593-f033-4f6d-ab6f-b5b067e61bc4"}}
1245             attach_response = self.api.api_post(
1246                              '/servers/%s/os-volume_attachments' % (server_id),
1247                              volume_attachment).body['volumeAttachment']
1248             self.assertTrue(mock_attach_create.called)
1249             mock_attachment_complete.assert_called_once_with(
1250                 mock.ANY, uuids.volume)
1251             self.assertIsNone(attach_response['device'])
1252 
1253         # Test detach volume
1254         with test.nested(mock.patch.object(objects.BlockDeviceMappingList,
1255                                            'get_by_instance_uuid'),
1256                          mock.patch.object(compute_api.API,
1257                                            '_local_cleanup_bdm_volumes')
1258                          ) as (mock_get_bdms, mock_clean_vols):
1259 
1260             mock_get_bdms.return_value = fake_bdms
1261             attachment_id = mock_get_bdms.return_value[0]['volume_id']
1262             self.api.api_delete('/servers/%s/os-volume_attachments/%s' %
1263                             (server_id, attachment_id))
1264             self.assertTrue(mock_clean_vols.called)
1265 
1266         self._delete_server(server_id)
1267 
1268 
1269 class ServerTestV269(ServersTestBase):
1270     api_major_version = 'v2.1'
1271     NUMBER_OF_CELLS = 3
1272 
1273     def setUp(self):
1274         super(ServerTestV269, self).setUp()
1275         self.api.microversion = '2.69'
1276 
1277         self.ctxt = context.get_admin_context()
1278         self.project_id = self.api.project_id
1279         self.cells = objects.CellMappingList.get_all(self.ctxt)
1280         self.down_cell_insts = []
1281         self.up_cell_insts = []
1282         self.down_cell_mappings = objects.CellMappingList()
1283         flavor = objects.Flavor(id=1, name='flavor1',
1284                                 memory_mb=256, vcpus=1,
1285                                 root_gb=1, ephemeral_gb=1,
1286                                 flavorid='1',
1287                                 swap=0, rxtx_factor=1.0,
1288                                 vcpu_weight=1,
1289                                 disabled=False,
1290                                 is_public=True,
1291                                 extra_specs={},
1292                                 projects=[])
1293         _info_cache = objects.InstanceInfoCache(context)
1294         objects.InstanceInfoCache._from_db_object(context, _info_cache,
1295             test_instance_info_cache.fake_info_cache)
1296         # cell1 and cell2 will be the down cells while
1297         # cell0 and cell3 will be the up cells.
1298         down_cell_names = ['cell1', 'cell2']
1299         for cell in self.cells:
1300             # create 2 instances and their mappings in all the 4 cells
1301             for i in range(2):
1302                 with context.target_cell(self.ctxt, cell) as cctxt:
1303                     inst = objects.Instance(
1304                         context=cctxt,
1305                         project_id=self.project_id,
1306                         user_id=self.project_id,
1307                         instance_type_id=flavor.id,
1308                         hostname='%s-inst%i' % (cell.name, i),
1309                         flavor=flavor,
1310                         info_cache=_info_cache,
1311                         display_name='server-test')
1312                     inst.create()
1313                 im = objects.InstanceMapping(context=self.ctxt,
1314                                              instance_uuid=inst.uuid,
1315                                              cell_mapping=cell,
1316                                              project_id=self.project_id,
1317                                              queued_for_delete=False)
1318                 im.create()
1319                 if cell.name in down_cell_names:
1320                     self.down_cell_insts.append(inst.uuid)
1321                 else:
1322                     self.up_cell_insts.append(inst.uuid)
1323             # In cell1 and cell3 add a third instance in a different project
1324             # to show the --all-tenants case.
1325             if cell.name == 'cell1' or cell.name == 'cell3':
1326                 with context.target_cell(self.ctxt, cell) as cctxt:
1327                     inst = objects.Instance(
1328                         context=cctxt,
1329                         project_id='faker',
1330                         user_id='faker',
1331                         instance_type_id=flavor.id,
1332                         hostname='%s-inst%i' % (cell.name, 3),
1333                         flavor=flavor,
1334                         info_cache=_info_cache,
1335                         display_name='server-test')
1336                     inst.create()
1337                 im = objects.InstanceMapping(context=self.ctxt,
1338                                              instance_uuid=inst.uuid,
1339                                              cell_mapping=cell,
1340                                              project_id='faker',
1341                                              queued_for_delete=False)
1342                 im.create()
1343             if cell.name in down_cell_names:
1344                 self.down_cell_mappings.objects.append(cell)
1345         self.useFixture(nova_fixtures.DownCellFixture(self.down_cell_mappings))
1346 
1347     def test_get_servers_with_down_cells(self):
1348         servers = self.api.get_servers(detail=False)
1349         # 4 servers from the up cells and 4 servers from the down cells
1350         self.assertEqual(8, len(servers))
1351         for server in servers:
1352             if 'name' not in server:
1353                 # server is in the down cell.
1354                 self.assertEqual('UNKNOWN', server['status'])
1355                 self.assertIn(server['id'], self.down_cell_insts)
1356                 self.assertIn('links', server)
1357                 # the partial construct will have only the above 3 keys
1358                 self.assertEqual(3, len(server))
1359             else:
1360                 # server in up cell
1361                 self.assertIn(server['id'], self.up_cell_insts)
1362                 # has all the keys
1363                 self.assertEqual(server['name'], 'server-test')
1364                 self.assertIn('links', server)
1365 
1366     def test_get_servers_detail_with_down_cells(self):
1367         servers = self.api.get_servers()
1368         # 4 servers from the up cells and 4 servers from the down cells
1369         self.assertEqual(8, len(servers))
1370         for server in servers:
1371             if 'user_id' not in server:
1372                 # server is in the down cell.
1373                 self.assertEqual('UNKNOWN', server['status'])
1374                 self.assertIn(server['id'], self.down_cell_insts)
1375                 # the partial construct will have only 5 keys:
1376                 # created, tenant_id, status, id and links.
1377                 self.assertEqual(5, len(server))
1378             else:
1379                 # server in up cell
1380                 self.assertIn(server['id'], self.up_cell_insts)
1381                 # has all the keys
1382                 self.assertEqual(server['user_id'], self.project_id)
1383                 self.assertIn('image', server)
1384 
1385     def test_get_servers_detail_limits_with_down_cells(self):
1386         servers = self.api.get_servers(search_opts={'limit': 5})
1387         # 4 servers from the up cells since we skip down cell
1388         # results by default for paging.
1389         self.assertEqual(4, len(servers), servers)
1390         for server in servers:
1391             # server in up cell
1392             self.assertIn(server['id'], self.up_cell_insts)
1393             # has all the keys
1394             self.assertEqual(server['user_id'], self.project_id)
1395             self.assertIn('image', server)
1396 
1397     def test_get_servers_detail_limits_with_down_cells_the_500_gift(self):
1398         self.flags(list_records_by_skipping_down_cells=False, group='api')
1399         # We get an API error with a 500 response code since the
1400         # list_records_by_skipping_down_cells config option is False.
1401         exp = self.assertRaises(client.OpenStackApiException,
1402                                 self.api.get_servers,
1403                                 search_opts={'limit': 5})
1404         self.assertEqual(500, exp.response.status_code)
1405         self.assertIn('NovaException', six.text_type(exp))
1406 
1407     def test_get_servers_detail_marker_in_down_cells(self):
1408         marker = self.down_cell_insts[2]
1409         # It will fail with a 500 if the marker is in the down cell.
1410         exp = self.assertRaises(client.OpenStackApiException,
1411                                 self.api.get_servers,
1412                                 search_opts={'marker': marker})
1413         self.assertEqual(500, exp.response.status_code)
1414         self.assertIn('oslo_db.exception.DBError', six.text_type(exp))
1415 
1416     def test_get_servers_detail_marker_sorting(self):
1417         marker = self.up_cell_insts[1]
1418         # It will give the results from the up cell if
1419         # list_records_by_skipping_down_cells config option is True.
1420         servers = self.api.get_servers(search_opts={'marker': marker,
1421                                                     'sort_key': "created_at",
1422                                                     'sort_dir': "asc"})
1423         # since there are 4 servers from the up cells, when giving the
1424         # second instance as marker, sorted by creation time in ascending
1425         # third and fourth instances will be returned.
1426         self.assertEqual(2, len(servers))
1427         for server in servers:
1428             self.assertIn(
1429                 server['id'], [self.up_cell_insts[2], self.up_cell_insts[3]])
1430 
1431     def test_get_servers_detail_non_admin_with_deleted_flag(self):
1432         # if list_records_by_skipping_down_cells config option is True
1433         # this deleted option should be ignored and the rest of the instances
1434         # from the up cells and the partial results from the down cells should
1435         # be returned.
1436         # Set the policy so we don't have permission to allow
1437         # all filters but are able to get server details.
1438         servers_rule = 'os_compute_api:servers:detail'
1439         extraspec_rule = 'os_compute_api:servers:allow_all_filters'
1440         self.policy.set_rules({
1441             extraspec_rule: 'rule:admin_api',
1442             servers_rule: '@'})
1443         servers = self.api.get_servers(search_opts={'deleted': True})
1444         # gets 4 results from up cells and 4 from down cells.
1445         self.assertEqual(8, len(servers))
1446         for server in servers:
1447             if "image" not in server:
1448                 self.assertIn(server['id'], self.down_cell_insts)
1449             else:
1450                 self.assertIn(server['id'], self.up_cell_insts)
1451 
1452     def test_get_servers_detail_filters(self):
1453         # We get the results only from the up cells, this ignoring the down
1454         # cells if list_records_by_skipping_down_cells config option is True.
1455         api_fixture = self.useFixture(nova_fixtures.OSAPIFixture(
1456             api_version='v2.1'))
1457         self.admin_api = api_fixture.admin_api
1458         self.admin_api.microversion = '2.69'
1459         servers = self.admin_api.get_servers(
1460             search_opts={'hostname': "cell3-inst0"})
1461         self.assertEqual(1, len(servers))
1462         self.assertEqual(self.up_cell_insts[2], servers[0]['id'])
1463 
1464     def test_get_servers_detail_all_tenants_with_down_cells(self):
1465         api_fixture = self.useFixture(nova_fixtures.OSAPIFixture(
1466             api_version='v2.1'))
1467         self.admin_api = api_fixture.admin_api
1468         self.admin_api.microversion = '2.69'
1469         servers = self.admin_api.get_servers(search_opts={'all_tenants': True})
1470         # 4 servers from the up cells and 4 servers from the down cells
1471         # plus the 2 instances from cell1 and cell3 which are in a different
1472         # project.
1473         self.assertEqual(10, len(servers))
1474         for server in servers:
1475             if 'user_id' not in server:
1476                 # server is in the down cell.
1477                 self.assertEqual('UNKNOWN', server['status'])
1478                 if server['tenant_id'] != 'faker':
1479                     self.assertIn(server['id'], self.down_cell_insts)
1480                 # the partial construct will have only 5 keys:
1481                 # created, tenant_id, status, id and links
1482                 self.assertEqual(5, len(server))
1483             else:
1484                 # server in up cell
1485                 if server['tenant_id'] != 'faker':
1486                     self.assertIn(server['id'], self.up_cell_insts)
1487                     self.assertEqual(server['user_id'], self.project_id)
1488                 self.assertIn('image', server)
1489 
1490 
1491 class ServerRebuildTestCase(integrated_helpers._IntegratedTestBase,
1492                             integrated_helpers.InstanceHelperMixin):
1493     api_major_version = 'v2.1'
1494     # We have to cap the microversion at 2.38 because that's the max we
1495     # can use to update image metadata via our compute images proxy API.
1496     microversion = '2.38'
1497 
1498     def _disable_compute_for(self, server):
1499         # Refresh to get its host
1500         server = self.api.get_server(server['id'])
1501         host = server['OS-EXT-SRV-ATTR:host']
1502 
1503         # Disable the service it is on
1504         self.api_fixture.admin_api.put_service('disable',
1505                                                {'host': host,
1506                                                 'binary': 'nova-compute'})
1507 
1508     def test_rebuild_with_image_novalidhost(self):
1509         """Creates a server with an image that is valid for the single compute
1510         that we have. Then rebuilds the server, passing in an image with
1511         metadata that does not fit the single compute which should result in
1512         a NoValidHost error. The ImagePropertiesFilter filter is enabled by
1513         default so that should filter out the host based on the image meta.
1514         """
1515 
1516         fake.set_nodes(['host2'])
1517         self.addCleanup(fake.restore_nodes)
1518         self.flags(host='host2')
1519         self.compute2 = self.start_service('compute', host='host2')
1520 
1521         # We hard-code from a fake image since we can't get images
1522         # via the compute /images proxy API with microversion > 2.35.
1523         original_image_ref = '155d900f-4e14-4e4c-a73d-069cbf4541e6'
1524         server_req_body = {
1525             'server': {
1526                 'imageRef': original_image_ref,
1527                 'flavorRef': '1',   # m1.tiny from DefaultFlavorsFixture,
1528                 'name': 'test_rebuild_with_image_novalidhost',
1529                 # We don't care about networking for this test. This requires
1530                 # microversion >= 2.37.
1531                 'networks': 'none'
1532             }
1533         }
1534         server = self.api.post_server(server_req_body)
1535         self._wait_for_state_change(self.api, server, 'ACTIVE')
1536 
1537         # Disable the host we're on so ComputeFilter would have ruled it out
1538         # normally
1539         self._disable_compute_for(server)
1540 
1541         # Now update the image metadata to be something that won't work with
1542         # the fake compute driver we're using since the fake driver has an
1543         # "x86_64" architecture.
1544         rebuild_image_ref = (
1545             nova.tests.unit.image.fake.AUTO_DISK_CONFIG_ENABLED_IMAGE_UUID)
1546         self.api.put_image_meta_key(
1547             rebuild_image_ref, 'hw_architecture', 'unicore32')
1548         # Now rebuild the server with that updated image and it should result
1549         # in a NoValidHost failure from the scheduler.
1550         rebuild_req_body = {
1551             'rebuild': {
1552                 'imageRef': rebuild_image_ref
1553             }
1554         }
1555         # Since we're using the CastAsCall fixture, the NoValidHost error
1556         # should actually come back to the API and result in a 500 error.
1557         # Normally the user would get a 202 response because nova-api RPC casts
1558         # to nova-conductor which RPC calls the scheduler which raises the
1559         # NoValidHost. We can mimic the end user way to figure out the failure
1560         # by looking for the failed 'rebuild' instance action event.
1561         self.api.api_post('/servers/%s/action' % server['id'],
1562                           rebuild_req_body, check_response_status=[500])
1563         # Look for the failed rebuild action.
1564         self._wait_for_action_fail_completion(
1565             server, instance_actions.REBUILD, 'rebuild_server',
1566             # Before microversion 2.51 events are only returned for instance
1567             # actions if you're an admin.
1568             self.api_fixture.admin_api)
1569         # Assert the server image_ref was rolled back on failure.
1570         server = self.api.get_server(server['id'])
1571         self.assertEqual(original_image_ref, server['image']['id'])
1572 
1573         # The server should be in ERROR state
1574         self.assertEqual('ERROR', server['status'])
1575         self.assertIn('No valid host', server['fault']['message'])
1576 
1577         # Rebuild it again with the same bad image to make sure it's rejected
1578         # again. Since we're using CastAsCall here, there is no 202 from the
1579         # API, and the exception from conductor gets passed back through the
1580         # API.
1581         ex = self.assertRaises(
1582             client.OpenStackApiException, self.api.api_post,
1583             '/servers/%s/action' % server['id'], rebuild_req_body)
1584         self.assertIn('NoValidHost', six.text_type(ex))
1585 
1586     # A rebuild to the same host should never attempt a rebuild claim.
1587     @mock.patch('nova.compute.resource_tracker.ResourceTracker.rebuild_claim',
1588                 new_callable=mock.NonCallableMock)
1589     def test_rebuild_with_new_image(self, mock_rebuild_claim):
1590         """Rebuilds a server with a different image which will run it through
1591         the scheduler to validate the image is still OK with the compute host
1592         that the instance is running on.
1593 
1594         Validates that additional resources are not allocated against the
1595         instance.host in Placement due to the rebuild on same host.
1596         """
1597         admin_api = self.api_fixture.admin_api
1598         admin_api.microversion = '2.53'
1599 
1600         def _get_provider_uuid_by_host(host):
1601             resp = admin_api.api_get(
1602                 'os-hypervisors?hypervisor_hostname_pattern=%s' % host).body
1603             return resp['hypervisors'][0]['id']
1604 
1605         def _get_provider_usages(provider_uuid):
1606             return self.placement_api.get(
1607                 '/resource_providers/%s/usages' % provider_uuid).body['usages']
1608 
1609         def _get_allocations_by_server_uuid(server_uuid):
1610             return self.placement_api.get(
1611                 '/allocations/%s' % server_uuid).body['allocations']
1612 
1613         def _set_provider_inventory(rp_uuid, resource_class, inventory):
1614             # Get the resource provider generation for the inventory update.
1615             rp = self.placement_api.get(
1616                 '/resource_providers/%s' % rp_uuid).body
1617             inventory['resource_provider_generation'] = rp['generation']
1618             return self.placement_api.put(
1619                 '/resource_providers/%s/inventories/%s' %
1620                 (rp_uuid, resource_class), inventory).body
1621 
1622         def assertFlavorMatchesAllocation(flavor, allocation):
1623             self.assertEqual(flavor['vcpus'], allocation['VCPU'])
1624             self.assertEqual(flavor['ram'], allocation['MEMORY_MB'])
1625             self.assertEqual(flavor['disk'], allocation['DISK_GB'])
1626 
1627         nodename = self.compute.manager._get_nodename(None)
1628         rp_uuid = _get_provider_uuid_by_host(nodename)
1629         # make sure we start with no usage on the compute node
1630         rp_usages = _get_provider_usages(rp_uuid)
1631         self.assertEqual({'VCPU': 0, 'MEMORY_MB': 0, 'DISK_GB': 0}, rp_usages)
1632 
1633         server_req_body = {
1634             'server': {
1635                 # We hard-code from a fake image since we can't get images
1636                 # via the compute /images proxy API with microversion > 2.35.
1637                 'imageRef': '155d900f-4e14-4e4c-a73d-069cbf4541e6',
1638                 'flavorRef': '1',   # m1.tiny from DefaultFlavorsFixture,
1639                 'name': 'test_rebuild_with_new_image',
1640                 # We don't care about networking for this test. This requires
1641                 # microversion >= 2.37.
1642                 'networks': 'none'
1643             }
1644         }
1645         server = self.api.post_server(server_req_body)
1646         self._wait_for_state_change(self.api, server, 'ACTIVE')
1647 
1648         flavor = self.api.api_get('/flavors/1').body['flavor']
1649 
1650         # make the compute node full and ensure rebuild still succeed
1651         _set_provider_inventory(rp_uuid, "VCPU", {"total": 1})
1652 
1653         # There should be usage for the server on the compute node now.
1654         rp_usages = _get_provider_usages(rp_uuid)
1655         assertFlavorMatchesAllocation(flavor, rp_usages)
1656         allocs = _get_allocations_by_server_uuid(server['id'])
1657         self.assertIn(rp_uuid, allocs)
1658         allocs = allocs[rp_uuid]['resources']
1659         assertFlavorMatchesAllocation(flavor, allocs)
1660 
1661         rebuild_image_ref = (
1662             nova.tests.unit.image.fake.AUTO_DISK_CONFIG_ENABLED_IMAGE_UUID)
1663         # Now rebuild the server with a different image.
1664         rebuild_req_body = {
1665             'rebuild': {
1666                 'imageRef': rebuild_image_ref
1667             }
1668         }
1669         self.api.api_post('/servers/%s/action' % server['id'],
1670                           rebuild_req_body)
1671         self._wait_for_server_parameter(
1672             self.api, server, {'OS-EXT-STS:task_state': None})
1673 
1674         # The usage and allocations should not have changed.
1675         rp_usages = _get_provider_usages(rp_uuid)
1676         assertFlavorMatchesAllocation(flavor, rp_usages)
1677 
1678         allocs = _get_allocations_by_server_uuid(server['id'])
1679         self.assertIn(rp_uuid, allocs)
1680         allocs = allocs[rp_uuid]['resources']
1681         assertFlavorMatchesAllocation(flavor, allocs)
1682 
1683     def test_volume_backed_rebuild_different_image(self):
1684         """Tests that trying to rebuild a volume-backed instance with a
1685         different image than what is in the root disk of the root volume
1686         will result in a 400 BadRequest error.
1687         """
1688         self.useFixture(nova_fixtures.CinderFixtureNewAttachFlow(self))
1689         # First create our server as normal.
1690         server_req_body = {
1691             # There is no imageRef because this is boot from volume.
1692             'server': {
1693                 'flavorRef': '1',  # m1.tiny from DefaultFlavorsFixture,
1694                 'name': 'test_volume_backed_rebuild_different_image',
1695                 # We don't care about networking for this test. This requires
1696                 # microversion >= 2.37.
1697                 'networks': 'none',
1698                 'block_device_mapping_v2': [{
1699                     'boot_index': 0,
1700                     'uuid':
1701                     nova_fixtures.CinderFixtureNewAttachFlow.IMAGE_BACKED_VOL,
1702                     'source_type': 'volume',
1703                     'destination_type': 'volume'
1704                 }]
1705             }
1706         }
1707         server = self.api.post_server(server_req_body)
1708         server = self._wait_for_state_change(self.api, server, 'ACTIVE')
1709         # For a volume-backed server, the image ref will be an empty string
1710         # in the server response.
1711         self.assertEqual('', server['image'])
1712 
1713         # Now rebuild the server with a different image than was used to create
1714         # our fake volume.
1715         rebuild_image_ref = (
1716             nova.tests.unit.image.fake.AUTO_DISK_CONFIG_ENABLED_IMAGE_UUID)
1717         rebuild_req_body = {
1718             'rebuild': {
1719                 'imageRef': rebuild_image_ref
1720             }
1721         }
1722         resp = self.api.api_post('/servers/%s/action' % server['id'],
1723                                  rebuild_req_body, check_response_status=[400])
1724         # Assert that we failed because of the image change and not something
1725         # else.
1726         self.assertIn('Unable to rebuild with a different image for a '
1727                       'volume-backed server', six.text_type(resp))
1728 
1729 
1730 class ProviderTreeTests(integrated_helpers.ProviderUsageBaseTestCase):
1731     compute_driver = 'fake.MediumFakeDriver'
1732 
1733     def setUp(self):
1734         super(ProviderTreeTests, self).setUp()
1735         # Before starting compute, placement has no providers registered
1736         self.assertEqual([], self._get_all_providers())
1737 
1738         # Start compute without mocking update_provider_tree. The fake driver
1739         # doesn't implement the method, so this will cause us to start with the
1740         # legacy get_available_resource()-based inventory discovery and
1741         # boostrapping of placement data.
1742         self.compute = self._start_compute(host='host1')
1743 
1744         # Mock out update_provider_tree *after* starting compute with the
1745         # (unmocked, default, unimplemented) version from the fake driver.
1746         _p = mock.patch.object(fake.MediumFakeDriver, 'update_provider_tree')
1747         self.addCleanup(_p.stop)
1748         self.mock_upt = _p.start()
1749 
1750         # The compute host should have been created in placement with
1751         # appropriate inventory and no traits
1752         rps = self._get_all_providers()
1753         self.assertEqual(1, len(rps))
1754         self.assertEqual(self.compute.host, rps[0]['name'])
1755         self.host_uuid = self._get_provider_uuid_by_host(self.compute.host)
1756         self.assertEqual({
1757             'DISK_GB': {
1758                 'total': 1028,
1759                 'allocation_ratio': 1.0,
1760                 'max_unit': 1028,
1761                 'min_unit': 1,
1762                 'reserved': 0,
1763                 'step_size': 1,
1764             },
1765             'MEMORY_MB': {
1766                 'total': 8192,
1767                 'allocation_ratio': 1.5,
1768                 'max_unit': 8192,
1769                 'min_unit': 1,
1770                 'reserved': 512,
1771                 'step_size': 1,
1772             },
1773             'VCPU': {
1774                 'total': 10,
1775                 'allocation_ratio': 16.0,
1776                 'max_unit': 10,
1777                 'min_unit': 1,
1778                 'reserved': 0,
1779                 'step_size': 1,
1780             },
1781         }, self._get_provider_inventory(self.host_uuid))
1782         self.assertItemsEqual(self.expected_fake_driver_capability_traits,
1783                               self._get_provider_traits(self.host_uuid))
1784 
1785     def _run_update_available_resource(self, startup):
1786         self.compute.rt.update_available_resource(
1787             context.get_admin_context(), self.compute.host, startup=startup)
1788 
1789     def _run_update_available_resource_and_assert_raises(
1790             self, exc=exception.ResourceProviderSyncFailed, startup=False):
1791         """Invoke ResourceTracker.update_available_resource and assert that it
1792         results in ResourceProviderSyncFailed.
1793 
1794         _run_periodicals is a little too high up in the call stack to be useful
1795         for this, because ResourceTracker.update_available_resource_for_node
1796         swallows all exceptions.
1797         """
1798         self.assertRaises(exc, self._run_update_available_resource, startup)
1799 
1800     def test_update_provider_tree_associated_info(self):
1801         """Inventory in some standard and custom resource classes.  Standard
1802         and custom traits.  Aggregates.  Custom resource class and trait get
1803         created; inventory, traits, and aggregates get set properly.
1804         """
1805         inv = {
1806             'VCPU': {
1807                 'total': 10,
1808                 'reserved': 0,
1809                 'min_unit': 1,
1810                 'max_unit': 2,
1811                 'step_size': 1,
1812                 'allocation_ratio': 10.0,
1813             },
1814             'MEMORY_MB': {
1815                 'total': 1048576,
1816                 'reserved': 2048,
1817                 'min_unit': 1024,
1818                 'max_unit': 131072,
1819                 'step_size': 1024,
1820                 'allocation_ratio': 1.0,
1821             },
1822             'CUSTOM_BANDWIDTH': {
1823                 'total': 1250000,
1824                 'reserved': 10000,
1825                 'min_unit': 5000,
1826                 'max_unit': 250000,
1827                 'step_size': 5000,
1828                 'allocation_ratio': 8.0,
1829             },
1830         }
1831         traits = set(['HW_CPU_X86_AVX', 'HW_CPU_X86_AVX2', 'CUSTOM_GOLD'])
1832         aggs = set([uuids.agg1, uuids.agg2])
1833 
1834         def update_provider_tree(prov_tree, nodename):
1835             prov_tree.update_inventory(self.compute.host, inv)
1836             prov_tree.update_traits(self.compute.host, traits)
1837             prov_tree.update_aggregates(self.compute.host, aggs)
1838         self.mock_upt.side_effect = update_provider_tree
1839 
1840         self.assertNotIn('CUSTOM_BANDWIDTH', self._get_all_resource_classes())
1841         self.assertNotIn('CUSTOM_GOLD', self._get_all_traits())
1842 
1843         self._run_periodics()
1844 
1845         self.assertIn('CUSTOM_BANDWIDTH', self._get_all_resource_classes())
1846         self.assertIn('CUSTOM_GOLD', self._get_all_traits())
1847         self.assertEqual(inv, self._get_provider_inventory(self.host_uuid))
1848         self.assertItemsEqual(
1849             traits.union(self.expected_fake_driver_capability_traits),
1850             self._get_provider_traits(self.host_uuid)
1851         )
1852         self.assertEqual(aggs,
1853                          set(self._get_provider_aggregates(self.host_uuid)))
1854 
1855     def _update_provider_tree_multiple_providers(self, startup=False,
1856                                                  do_reshape=False):
1857         """Make update_provider_tree create multiple providers, including an
1858         additional root as a sharing provider; and some descendants in the
1859         compute node's tree.
1860 
1861                    +---------------------------+   +--------------------------+
1862                    |uuid: self.host_uuid       |   |uuid: uuids.ssp           |
1863                    |name: self.compute.host    |   |name: 'ssp'               |
1864                    |inv: (per MediumFakeDriver)|   |inv: DISK_GB=500          |
1865                    |     VCPU=10               |...|traits: [MISC_SHARES_..., |
1866                    |     MEMORY_MB=8192        |   |         STORAGE_DISK_SSD]|
1867                    |     DISK_GB=1028          |   |aggs: [uuids.agg]         |
1868                    |aggs: [uuids.agg]          |   +--------------------------+
1869                    +---------------------------+
1870                          /                   \
1871              +-----------------+          +-----------------+
1872              |uuid: uuids.numa1|          |uuid: uuids.numa2|
1873              |name: 'numa1'    |          |name: 'numa2'    |
1874              |inv: VCPU=10     |          |inv: VCPU=20     |
1875              |     MEMORY_MB=1G|          |     MEMORY_MB=2G|
1876              +-----------------+          +-----------------+
1877                  /          \                    /         \
1878         +------------+  +------------+   +------------+  +------------+
1879         |uuid:       |  |uuid:       |   |uuid:       |  |uuid:       |
1880         | uuids.pf1_1|  | uuids.pf1_2|   | uuids.pf2_1|  | uuids.pf2_2|
1881         |name:       |  |name:       |   |name:       |  |name:       |
1882         | 'pf1_1'    |  | 'pf1_2'    |   | 'pf2_1'    |  | 'pf2_2'    |
1883         |inv:        |  |inv:        |   |inv:        |  |inv:        |
1884         | ..NET_VF: 2|  | ..NET_VF: 3|   | ..NET_VF: 3|  | ..NET_VF: 4|
1885         |traits:     |  |traits:     |   |traits:     |  |traits:     |
1886         | ..PHYSNET_0|  | ..PHYSNET_1|   | ..PHYSNET_0|  | ..PHYSNET_1|
1887         +------------+  +------------+   +------------+  +------------+
1888         """
1889         def update_provider_tree(prov_tree, nodename, allocations=None):
1890             if do_reshape and allocations is None:
1891                 raise exception.ReshapeNeeded()
1892 
1893             # Create a shared storage provider as a root
1894             prov_tree.new_root('ssp', uuids.ssp)
1895             prov_tree.update_traits(
1896                 'ssp', ['MISC_SHARES_VIA_AGGREGATE', 'STORAGE_DISK_SSD'])
1897             prov_tree.update_aggregates('ssp', [uuids.agg])
1898             prov_tree.update_inventory('ssp', {'DISK_GB': {'total': 500}})
1899             # Compute node is in the same aggregate
1900             prov_tree.update_aggregates(self.compute.host, [uuids.agg])
1901             # Create two NUMA nodes as children
1902             prov_tree.new_child('numa1', self.host_uuid, uuid=uuids.numa1)
1903             prov_tree.new_child('numa2', self.host_uuid, uuid=uuids.numa2)
1904             # Give the NUMA nodes the proc/mem inventory.  NUMA 2 has twice as
1905             # much as NUMA 1 (so we can validate later that everything is where
1906             # it should be).
1907             for n in (1, 2):
1908                 inv = {
1909                     'VCPU': {
1910                         'total': 10 * n,
1911                         'reserved': 0,
1912                         'min_unit': 1,
1913                         'max_unit': 2,
1914                         'step_size': 1,
1915                         'allocation_ratio': 10.0,
1916                     },
1917                     'MEMORY_MB': {
1918                          'total': 1048576 * n,
1919                          'reserved': 2048,
1920                          'min_unit': 512,
1921                          'max_unit': 131072,
1922                          'step_size': 512,
1923                          'allocation_ratio': 1.0,
1924                      },
1925                 }
1926                 prov_tree.update_inventory('numa%d' % n, inv)
1927             # Each NUMA node has two PFs providing VF inventory on one of two
1928             # networks
1929             for n in (1, 2):
1930                 for p in (1, 2):
1931                     name = 'pf%d_%d' % (n, p)
1932                     prov_tree.new_child(
1933                         name, getattr(uuids, 'numa%d' % n),
1934                         uuid=getattr(uuids, name))
1935                     trait = 'CUSTOM_PHYSNET_%d' % ((n + p) % 2)
1936                     prov_tree.update_traits(name, [trait])
1937                     inv = {
1938                         'SRIOV_NET_VF': {
1939                             'total': n + p,
1940                             'reserved': 0,
1941                             'min_unit': 1,
1942                             'max_unit': 1,
1943                             'step_size': 1,
1944                             'allocation_ratio': 1.0,
1945                         },
1946                     }
1947                     prov_tree.update_inventory(name, inv)
1948             if do_reshape:
1949                 # Clear out the compute node's inventory. Its VCPU and
1950                 # MEMORY_MB "moved" to the NUMA RPs and its DISK_GB "moved" to
1951                 # the shared storage provider.
1952                 prov_tree.update_inventory(self.host_uuid, {})
1953                 # Move all the allocations
1954                 for consumer_uuid, alloc_info in allocations.items():
1955                     allocs = alloc_info['allocations']
1956                     # All allocations should belong to the compute node.
1957                     self.assertEqual([self.host_uuid], list(allocs))
1958                     new_allocs = {}
1959                     for rc, amt in allocs[self.host_uuid]['resources'].items():
1960                         # Move VCPU to NUMA1 and MEMORY_MB to NUMA2. Bogus, but
1961                         # lets us prove stuff ends up where we tell it to go.
1962                         if rc == 'VCPU':
1963                             rp_uuid = uuids.numa1
1964                         elif rc == 'MEMORY_MB':
1965                             rp_uuid = uuids.numa2
1966                         elif rc == 'DISK_GB':
1967                             rp_uuid = uuids.ssp
1968                         else:
1969                             self.fail("Unexpected resource on compute node: "
1970                                       "%s=%d" % (rc, amt))
1971                         new_allocs[rp_uuid] = {
1972                             'resources': {rc: amt},
1973                         }
1974                     # Add a VF for the heck of it. Again bogus, but see above.
1975                     new_allocs[uuids.pf1_1] = {
1976                         'resources': {'SRIOV_NET_VF': 1}
1977                     }
1978                     # Now replace just the allocations, leaving the other stuff
1979                     # (proj/user ID and consumer generation) alone
1980                     alloc_info['allocations'] = new_allocs
1981 
1982         self.mock_upt.side_effect = update_provider_tree
1983 
1984         if startup:
1985             self.restart_compute_service(self.compute)
1986         else:
1987             self._run_update_available_resource(False)
1988 
1989         # Create a dict, keyed by provider UUID, of all the providers
1990         rps_by_uuid = {}
1991         for rp_dict in self._get_all_providers():
1992             rps_by_uuid[rp_dict['uuid']] = rp_dict
1993 
1994         # All and only the expected providers got created.
1995         all_uuids = set([self.host_uuid, uuids.ssp, uuids.numa1, uuids.numa2,
1996                          uuids.pf1_1, uuids.pf1_2, uuids.pf2_1, uuids.pf2_2])
1997         self.assertEqual(all_uuids, set(rps_by_uuid))
1998 
1999         # Validate tree roots
2000         tree_uuids = [self.host_uuid, uuids.numa1, uuids.numa2,
2001                       uuids.pf1_1, uuids.pf1_2, uuids.pf2_1, uuids.pf2_2]
2002         for tree_uuid in tree_uuids:
2003             self.assertEqual(self.host_uuid,
2004                              rps_by_uuid[tree_uuid]['root_provider_uuid'])
2005         self.assertEqual(uuids.ssp,
2006                          rps_by_uuid[uuids.ssp]['root_provider_uuid'])
2007 
2008         # SSP has the right traits
2009         self.assertEqual(
2010             set(['MISC_SHARES_VIA_AGGREGATE', 'STORAGE_DISK_SSD']),
2011             set(self._get_provider_traits(uuids.ssp)))
2012 
2013         # SSP has the right inventory
2014         self.assertEqual(
2015             500, self._get_provider_inventory(uuids.ssp)['DISK_GB']['total'])
2016 
2017         # SSP and compute are in the same aggregate
2018         agg_uuids = set([self.host_uuid, uuids.ssp])
2019         for uuid in agg_uuids:
2020             self.assertEqual(set([uuids.agg]),
2021                              set(self._get_provider_aggregates(uuid)))
2022 
2023         # The rest aren't in aggregates
2024         for uuid in (all_uuids - agg_uuids):
2025             self.assertEqual(set(), set(self._get_provider_aggregates(uuid)))
2026 
2027         # NUMAs have the right inventory and parentage
2028         for n in (1, 2):
2029             numa_uuid = getattr(uuids, 'numa%d' % n)
2030             self.assertEqual(self.host_uuid,
2031                              rps_by_uuid[numa_uuid]['parent_provider_uuid'])
2032             inv = self._get_provider_inventory(numa_uuid)
2033             self.assertEqual(10 * n, inv['VCPU']['total'])
2034             self.assertEqual(1048576 * n, inv['MEMORY_MB']['total'])
2035 
2036         # PFs have the right inventory, physnet, and parentage
2037         self.assertEqual(uuids.numa1,
2038                          rps_by_uuid[uuids.pf1_1]['parent_provider_uuid'])
2039         self.assertEqual(['CUSTOM_PHYSNET_0'],
2040                          self._get_provider_traits(uuids.pf1_1))
2041         self.assertEqual(
2042             2,
2043             self._get_provider_inventory(uuids.pf1_1)['SRIOV_NET_VF']['total'])
2044 
2045         self.assertEqual(uuids.numa1,
2046                          rps_by_uuid[uuids.pf1_2]['parent_provider_uuid'])
2047         self.assertEqual(['CUSTOM_PHYSNET_1'],
2048                          self._get_provider_traits(uuids.pf1_2))
2049         self.assertEqual(
2050             3,
2051             self._get_provider_inventory(uuids.pf1_2)['SRIOV_NET_VF']['total'])
2052 
2053         self.assertEqual(uuids.numa2,
2054                          rps_by_uuid[uuids.pf2_1]['parent_provider_uuid'])
2055         self.assertEqual(['CUSTOM_PHYSNET_1'],
2056                          self._get_provider_traits(uuids.pf2_1))
2057         self.assertEqual(
2058             3,
2059             self._get_provider_inventory(uuids.pf2_1)['SRIOV_NET_VF']['total'])
2060 
2061         self.assertEqual(uuids.numa2,
2062                          rps_by_uuid[uuids.pf2_2]['parent_provider_uuid'])
2063         self.assertEqual(['CUSTOM_PHYSNET_0'],
2064                          self._get_provider_traits(uuids.pf2_2))
2065         self.assertEqual(
2066             4,
2067             self._get_provider_inventory(uuids.pf2_2)['SRIOV_NET_VF']['total'])
2068 
2069         # Compute don't have any extra traits
2070         self.assertItemsEqual(self.expected_fake_driver_capability_traits,
2071                               self._get_provider_traits(self.host_uuid))
2072 
2073         # NUMAs don't have any traits
2074         for uuid in (uuids.numa1, uuids.numa2):
2075             self.assertEqual([], self._get_provider_traits(uuid))
2076 
2077     def test_update_provider_tree_multiple_providers(self):
2078         self._update_provider_tree_multiple_providers()
2079 
2080     def test_update_provider_tree_multiple_providers_startup(self):
2081         """The above works the same for startup when no reshape requested."""
2082         self._update_provider_tree_multiple_providers(startup=True)
2083 
2084     def test_update_provider_tree_bogus_resource_class(self):
2085         def update_provider_tree(prov_tree, nodename):
2086             prov_tree.update_inventory(self.compute.host, {'FOO': {}})
2087         self.mock_upt.side_effect = update_provider_tree
2088 
2089         rcs = self._get_all_resource_classes()
2090         self.assertIn('VCPU', rcs)
2091         self.assertNotIn('FOO', rcs)
2092 
2093         self._run_update_available_resource_and_assert_raises()
2094 
2095         rcs = self._get_all_resource_classes()
2096         self.assertIn('VCPU', rcs)
2097         self.assertNotIn('FOO', rcs)
2098 
2099     def test_update_provider_tree_bogus_trait(self):
2100         def update_provider_tree(prov_tree, nodename):
2101             prov_tree.update_traits(self.compute.host, ['FOO'])
2102         self.mock_upt.side_effect = update_provider_tree
2103 
2104         traits = self._get_all_traits()
2105         self.assertIn('HW_CPU_X86_AVX', traits)
2106         self.assertNotIn('FOO', traits)
2107 
2108         self._run_update_available_resource_and_assert_raises()
2109 
2110         traits = self._get_all_traits()
2111         self.assertIn('HW_CPU_X86_AVX', traits)
2112         self.assertNotIn('FOO', traits)
2113 
2114     def _create_instance(self, flavor):
2115         server_req = self._build_minimal_create_server_request(
2116             self.api, 'some-server', flavor_id=flavor['id'],
2117             image_uuid='155d900f-4e14-4e4c-a73d-069cbf4541e6',
2118             networks='none', az='nova:host1')
2119         inst = self.api.post_server({'server': server_req})
2120         return self._wait_for_state_change(self.admin_api, inst, 'ACTIVE')
2121 
2122     def test_reshape(self):
2123         """On startup, virt driver signals it needs to reshape, then does so.
2124 
2125         This test creates a couple of instances so there are allocations to be
2126         moved by the reshape operation. Then we do the reshape and make sure
2127         the inventories and allocations end up where they should.
2128         """
2129         # First let's create some instances so we have allocations to move.
2130         flavors = self.api.get_flavors()
2131         inst1 = self._create_instance(flavors[0])
2132         inst2 = self._create_instance(flavors[1])
2133 
2134         # Instance create calls RT._update, which calls
2135         # driver.update_provider_tree, which is currently mocked to a no-op.
2136         self.assertEqual(2, self.mock_upt.call_count)
2137         self.mock_upt.reset_mock()
2138 
2139         # Hit the reshape.
2140         self._update_provider_tree_multiple_providers(startup=True,
2141                                                       do_reshape=True)
2142 
2143         # Check the final allocations
2144         # The compute node provider should have *no* allocations.
2145         self.assertEqual(
2146             {}, self._get_allocations_by_provider_uuid(self.host_uuid))
2147         # And no inventory
2148         self.assertEqual({}, self._get_provider_inventory(self.host_uuid))
2149         # NUMA1 got all the VCPU
2150         self.assertEqual(
2151             {inst1['id']: {'resources': {'VCPU': 1}},
2152              inst2['id']: {'resources': {'VCPU': 1}}},
2153             self._get_allocations_by_provider_uuid(uuids.numa1))
2154         # NUMA2 got all the memory
2155         self.assertEqual(
2156             {inst1['id']: {'resources': {'MEMORY_MB': 512}},
2157              inst2['id']: {'resources': {'MEMORY_MB': 2048}}},
2158             self._get_allocations_by_provider_uuid(uuids.numa2))
2159         # Disk resource ended up on the shared storage provider
2160         self.assertEqual(
2161             {inst1['id']: {'resources': {'DISK_GB': 1}},
2162              inst2['id']: {'resources': {'DISK_GB': 20}}},
2163             self._get_allocations_by_provider_uuid(uuids.ssp))
2164         # We put VFs on the first PF in NUMA1
2165         self.assertEqual(
2166             {inst1['id']: {'resources': {'SRIOV_NET_VF': 1}},
2167              inst2['id']: {'resources': {'SRIOV_NET_VF': 1}}},
2168             self._get_allocations_by_provider_uuid(uuids.pf1_1))
2169         self.assertEqual(
2170             {}, self._get_allocations_by_provider_uuid(uuids.pf1_2))
2171         self.assertEqual(
2172             {}, self._get_allocations_by_provider_uuid(uuids.pf2_1))
2173         self.assertEqual(
2174             {}, self._get_allocations_by_provider_uuid(uuids.pf2_2))
2175         # This is *almost* redundant - but it makes sure the instances don't
2176         # have extra allocations from some other provider.
2177         self.assertEqual(
2178             {
2179                 uuids.numa1: {
2180                     'resources': {'VCPU': 1},
2181                     # Don't care about the generations - rely on placement db
2182                     # tests to validate that those behave properly.
2183                     'generation': mock.ANY,
2184                 },
2185                 uuids.numa2: {
2186                     'resources': {'MEMORY_MB': 512},
2187                     'generation': mock.ANY,
2188                 },
2189                 uuids.ssp: {
2190                     'resources': {'DISK_GB': 1},
2191                     'generation': mock.ANY,
2192                 },
2193                 uuids.pf1_1: {
2194                     'resources': {'SRIOV_NET_VF': 1},
2195                     'generation': mock.ANY,
2196                 },
2197             }, self._get_allocations_by_server_uuid(inst1['id']))
2198         self.assertEqual(
2199             {
2200                 uuids.numa1: {
2201                     'resources': {'VCPU': 1},
2202                     'generation': mock.ANY,
2203                 },
2204                 uuids.numa2: {
2205                     'resources': {'MEMORY_MB': 2048},
2206                     'generation': mock.ANY,
2207                 },
2208                 uuids.ssp: {
2209                     'resources': {'DISK_GB': 20},
2210                     'generation': mock.ANY,
2211                 },
2212                 uuids.pf1_1: {
2213                     'resources': {'SRIOV_NET_VF': 1},
2214                     'generation': mock.ANY,
2215                 },
2216             }, self._get_allocations_by_server_uuid(inst2['id']))
2217 
2218         # The first call raises ReshapeNeeded, resulting in the second.
2219         self.assertEqual(2, self.mock_upt.call_count)
2220         # The expected value of the allocations kwarg to update_provider_tree
2221         # for that second call:
2222         exp_allocs = {
2223             inst1['id']: {
2224                 'allocations': {
2225                     uuids.numa1: {'resources': {'VCPU': 1}},
2226                     uuids.numa2: {'resources': {'MEMORY_MB': 512}},
2227                     uuids.ssp: {'resources': {'DISK_GB': 1}},
2228                     uuids.pf1_1: {'resources': {'SRIOV_NET_VF': 1}},
2229                 },
2230                 'consumer_generation': mock.ANY,
2231                 'project_id': mock.ANY,
2232                 'user_id': mock.ANY,
2233             },
2234             inst2['id']: {
2235                 'allocations': {
2236                     uuids.numa1: {'resources': {'VCPU': 1}},
2237                     uuids.numa2: {'resources': {'MEMORY_MB': 2048}},
2238                     uuids.ssp: {'resources': {'DISK_GB': 20}},
2239                     uuids.pf1_1: {'resources': {'SRIOV_NET_VF': 1}},
2240                 },
2241                 'consumer_generation': mock.ANY,
2242                 'project_id': mock.ANY,
2243                 'user_id': mock.ANY,
2244             },
2245         }
2246         self.mock_upt.assert_has_calls([
2247             mock.call(mock.ANY, 'host1'),
2248             mock.call(mock.ANY, 'host1', allocations=exp_allocs),
2249         ])
2250 
2251 
2252 class TraitsTrackingTests(integrated_helpers.ProviderUsageBaseTestCase):
2253     compute_driver = 'fake.SmallFakeDriver'
2254 
2255     fake_caps = {
2256         'supports_attach_interface': True,
2257         'supports_device_tagging': False,
2258     }
2259 
2260     def _mock_upt(self, traits_to_add, traits_to_remove):
2261         """Set up the compute driver with a fake update_provider_tree()
2262         which injects the given traits into the provider tree
2263         """
2264         original_upt = fake.SmallFakeDriver.update_provider_tree
2265 
2266         def fake_upt(self2, ptree, nodename, allocations=None):
2267             original_upt(self2, ptree, nodename, allocations)
2268             LOG.debug("injecting traits via fake update_provider_tree(): %s",
2269                       traits_to_add)
2270             ptree.add_traits(nodename, *traits_to_add)
2271             LOG.debug("removing traits via fake update_provider_tree(): %s",
2272                       traits_to_remove)
2273             ptree.remove_traits(nodename, *traits_to_remove)
2274 
2275         self.stub_out('nova.virt.fake.FakeDriver.update_provider_tree',
2276                       fake_upt)
2277 
2278     @mock.patch.dict(fake.SmallFakeDriver.capabilities, clear=True,
2279                      values=fake_caps)
2280     def test_resource_provider_traits(self):
2281         """Test that the compute service reports traits via driver
2282         capabilities and registers them on the compute host resource
2283         provider in the placement API.
2284         """
2285         custom_trait = 'CUSTOM_FOO'
2286         ptree_traits = [custom_trait, 'HW_CPU_X86_VMX']
2287 
2288         global_traits = self._get_all_traits()
2289         self.assertNotIn(custom_trait, global_traits)
2290         self.assertIn(os_traits.COMPUTE_NET_ATTACH_INTERFACE, global_traits)
2291         self.assertIn(os_traits.COMPUTE_DEVICE_TAGGING, global_traits)
2292         self.assertEqual([], self._get_all_providers())
2293 
2294         self._mock_upt(ptree_traits, [])
2295 
2296         self.compute = self._start_compute(host='host1')
2297 
2298         rp_uuid = self._get_provider_uuid_by_host('host1')
2299         expected_traits = set(
2300             ptree_traits + [os_traits.COMPUTE_NET_ATTACH_INTERFACE]
2301         )
2302         self.assertItemsEqual(expected_traits,
2303                               self._get_provider_traits(rp_uuid))
2304         global_traits = self._get_all_traits()
2305         # CUSTOM_FOO is now a registered trait because the virt driver
2306         # reported it.
2307         self.assertIn(custom_trait, global_traits)
2308 
2309         # Now simulate user deletion of driver-provided traits from
2310         # the compute node provider.
2311         expected_traits.remove(custom_trait)
2312         expected_traits.remove(os_traits.COMPUTE_NET_ATTACH_INTERFACE)
2313         self._set_provider_traits(rp_uuid, list(expected_traits))
2314         self.assertItemsEqual(expected_traits,
2315                               self._get_provider_traits(rp_uuid))
2316 
2317         # The above trait deletions are simulations of an out-of-band
2318         # placement operation, as if the operator used the CLI.  So
2319         # now we have to "SIGHUP the compute process" to clear the
2320         # report client cache so the subsequent update picks up the
2321         # changes.
2322         self.compute.manager.reset()
2323 
2324         # Add the traits back so that the mock update_provider_tree()
2325         # can reinject them.
2326         expected_traits.update(
2327             [custom_trait, os_traits.COMPUTE_NET_ATTACH_INTERFACE])
2328 
2329         # Now when we run the periodic update task, the trait should
2330         # reappear in the provider tree and get synced back to
2331         # placement.
2332         self._run_periodics()
2333 
2334         self.assertItemsEqual(expected_traits,
2335                               self._get_provider_traits(rp_uuid))
2336         global_traits = self._get_all_traits()
2337         self.assertIn(custom_trait, global_traits)
2338         self.assertIn(os_traits.COMPUTE_NET_ATTACH_INTERFACE, global_traits)
2339 
2340     @mock.patch.dict(fake.SmallFakeDriver.capabilities, clear=True,
2341                      values=fake_caps)
2342     def test_admin_traits_preserved(self):
2343         """Test that if admin externally sets traits on the resource provider
2344         then the compute periodic doesn't remove them from placement.
2345         """
2346         admin_trait = 'CUSTOM_TRAIT_FROM_ADMIN'
2347         self._create_trait(admin_trait)
2348         global_traits = self._get_all_traits()
2349         self.assertIn(admin_trait, global_traits)
2350 
2351         self.compute = self._start_compute(host='host1')
2352         rp_uuid = self._get_provider_uuid_by_host('host1')
2353         traits = self._get_provider_traits(rp_uuid)
2354         traits.append(admin_trait)
2355         self._set_provider_traits(rp_uuid, traits)
2356         self.assertIn(admin_trait, self._get_provider_traits(rp_uuid))
2357 
2358         # SIGHUP the compute process to clear the report client
2359         # cache, so the subsequent periodic update recalculates everything.
2360         self.compute.manager.reset()
2361 
2362         self._run_periodics()
2363         self.assertIn(admin_trait, self._get_provider_traits(rp_uuid))
2364 
2365     @mock.patch.dict(fake.SmallFakeDriver.capabilities, clear=True,
2366                      values=fake_caps)
2367     def test_driver_removing_support_for_trait_via_capability(self):
2368         """Test that if a driver initially reports a trait via a supported
2369         capability, then at the next periodic update doesn't report
2370         support for it again, it gets removed from the provider in the
2371         placement service.
2372         """
2373         self.compute = self._start_compute(host='host1')
2374         rp_uuid = self._get_provider_uuid_by_host('host1')
2375         trait = os_traits.COMPUTE_NET_ATTACH_INTERFACE
2376         self.assertIn(trait, self._get_provider_traits(rp_uuid))
2377 
2378         new_caps = dict(fake.SmallFakeDriver.capabilities,
2379                         **{'supports_attach_interface': False})
2380         with mock.patch.dict(fake.SmallFakeDriver.capabilities, new_caps):
2381             self._run_periodics()
2382 
2383         self.assertNotIn(trait, self._get_provider_traits(rp_uuid))
2384 
2385     def test_driver_removing_trait_via_upt(self):
2386         """Test that if a driver reports a trait via update_provider_tree()
2387         initially, but at the next periodic update doesn't report it
2388         again, that it gets removed from placement.
2389         """
2390         custom_trait = "CUSTOM_TRAIT_FROM_DRIVER"
2391         standard_trait = os_traits.HW_CPU_X86_SGX
2392         self._mock_upt([custom_trait, standard_trait], [])
2393 
2394         self.compute = self._start_compute(host='host1')
2395         rp_uuid = self._get_provider_uuid_by_host('host1')
2396         self.assertIn(custom_trait, self._get_provider_traits(rp_uuid))
2397         self.assertIn(standard_trait, self._get_provider_traits(rp_uuid))
2398 
2399         # Now change the fake update_provider_tree() from injecting the
2400         # traits to removing them, and run the periodic update.
2401         self._mock_upt([], [custom_trait, standard_trait])
2402         self._run_periodics()
2403 
2404         self.assertNotIn(custom_trait, self._get_provider_traits(rp_uuid))
2405         self.assertNotIn(standard_trait, self._get_provider_traits(rp_uuid))
2406 
2407     @mock.patch.dict(fake.SmallFakeDriver.capabilities, clear=True,
2408                      values=fake_caps)
2409     def test_driver_removes_unsupported_trait_from_admin(self):
2410         """Test that if an admin adds a trait corresponding to a
2411         capability which is unsupported, then if the provider cache is
2412         reset, the driver will remove it during the next update.
2413         """
2414         self.compute = self._start_compute(host='host1')
2415         rp_uuid = self._get_provider_uuid_by_host('host1')
2416 
2417         traits = self._get_provider_traits(rp_uuid)
2418         trait = os_traits.COMPUTE_DEVICE_TAGGING
2419         self.assertNotIn(trait, traits)
2420 
2421         # Simulate an admin associating the trait with the host via
2422         # the placement API.
2423         traits.append(trait)
2424         self._set_provider_traits(rp_uuid, traits)
2425 
2426         # Check that worked.
2427         traits = self._get_provider_traits(rp_uuid)
2428         self.assertIn(trait, traits)
2429 
2430         # SIGHUP the compute process to clear the report client
2431         # cache, so the subsequent periodic update recalculates everything.
2432         self.compute.manager.reset()
2433 
2434         self._run_periodics()
2435         self.assertNotIn(trait, self._get_provider_traits(rp_uuid))
2436 
2437 
2438 class ServerMovingTests(integrated_helpers.ProviderUsageBaseTestCase):
2439     """Tests moving servers while checking the resource allocations and usages
2440 
2441     These tests use two compute hosts. Boot a server on one of them then try to
2442     move the server to the other. At every step resource allocation of the
2443     server and the resource usages of the computes are queried from placement
2444     API and asserted.
2445     """
2446 
2447     REQUIRES_LOCKING = True
2448     # NOTE(danms): The test defaults to using SmallFakeDriver,
2449     # which only has one vcpu, which can't take the doubled allocation
2450     # we're now giving it. So, use the bigger MediumFakeDriver here.
2451     compute_driver = 'fake.MediumFakeDriver'
2452 
2453     def setUp(self):
2454         super(ServerMovingTests, self).setUp()
2455         fake_notifier.stub_notifier(self)
2456         self.addCleanup(fake_notifier.reset)
2457 
2458         self.compute1 = self._start_compute(host='host1')
2459         self.compute2 = self._start_compute(host='host2')
2460 
2461         flavors = self.api.get_flavors()
2462         self.flavor1 = flavors[0]
2463         self.flavor2 = flavors[1]
2464         # create flavor3 which has less MEMORY_MB but more DISK_GB than flavor2
2465         flavor_body = {'flavor':
2466                            {'name': 'test_flavor3',
2467                             'ram': int(self.flavor2['ram'] / 2),
2468                             'vcpus': 1,
2469                             'disk': self.flavor2['disk'] * 2,
2470                             'id': 'a22d5517-147c-4147-a0d1-e698df5cd4e3'
2471                             }}
2472 
2473         self.flavor3 = self.api.post_flavor(flavor_body)
2474 
2475     def _other_hostname(self, host):
2476         other_host = {'host1': 'host2',
2477                       'host2': 'host1'}
2478         return other_host[host]
2479 
2480     def _run_periodics(self):
2481         # NOTE(jaypipes): We always run periodics in the same order: first on
2482         # compute1, then on compute2. However, we want to test scenarios when
2483         # the periodics run at different times during mover operations. This is
2484         # why we have the "reverse" tests which simply switch the source and
2485         # dest host while keeping the order in which we run the
2486         # periodics. This effectively allows us to test the matrix of timing
2487         # scenarios during move operations.
2488         ctx = context.get_admin_context()
2489         LOG.info('Running periodic for compute1 (%s)',
2490             self.compute1.manager.host)
2491         self.compute1.manager.update_available_resource(ctx)
2492         LOG.info('Running periodic for compute2 (%s)',
2493             self.compute2.manager.host)
2494         self.compute2.manager.update_available_resource(ctx)
2495         LOG.info('Finished with periodics')
2496 
2497     def test_resize_revert(self):
2498         self._test_resize_revert(dest_hostname='host1')
2499 
2500     def test_resize_revert_reverse(self):
2501         self._test_resize_revert(dest_hostname='host2')
2502 
2503     def test_resize_confirm(self):
2504         self._test_resize_confirm(dest_hostname='host1')
2505 
2506     def test_resize_confirm_reverse(self):
2507         self._test_resize_confirm(dest_hostname='host2')
2508 
2509     def _resize_and_check_allocations(self, server, old_flavor, new_flavor,
2510             source_rp_uuid, dest_rp_uuid):
2511         self.flags(allow_resize_to_same_host=False)
2512         resize_req = {
2513             'resize': {
2514                 'flavorRef': new_flavor['id']
2515             }
2516         }
2517         self._move_and_check_allocations(
2518             server, request=resize_req, old_flavor=old_flavor,
2519             new_flavor=new_flavor, source_rp_uuid=source_rp_uuid,
2520             dest_rp_uuid=dest_rp_uuid)
2521 
2522     def _test_resize_revert(self, dest_hostname):
2523         source_hostname = self._other_hostname(dest_hostname)
2524         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2525         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
2526 
2527         server = self._boot_and_check_allocations(self.flavor1,
2528             source_hostname)
2529 
2530         self._resize_and_check_allocations(server, self.flavor1, self.flavor2,
2531             source_rp_uuid, dest_rp_uuid)
2532 
2533         # Revert the resize and check the usages
2534         post = {'revertResize': None}
2535         self.api.post_server_action(server['id'], post)
2536         self._wait_for_state_change(self.api, server, 'ACTIVE')
2537 
2538         # Make sure the RequestSpec.flavor matches the original flavor.
2539         ctxt = context.get_admin_context()
2540         reqspec = objects.RequestSpec.get_by_instance_uuid(ctxt, server['id'])
2541         self.assertEqual(self.flavor1['id'], reqspec.flavor.flavorid)
2542 
2543         self._run_periodics()
2544 
2545         # the original host expected to have the old resource allocation
2546         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
2547 
2548         self.assertRequestMatchesUsage({'VCPU': 0,
2549                                         'MEMORY_MB': 0,
2550                                         'DISK_GB': 0}, dest_rp_uuid)
2551 
2552         # Check that the server only allocates resource from the original host
2553         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
2554                                            source_rp_uuid)
2555 
2556         self._delete_and_check_allocations(server)
2557 
2558     def _test_resize_confirm(self, dest_hostname):
2559         source_hostname = self._other_hostname(dest_hostname)
2560         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2561         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
2562 
2563         server = self._boot_and_check_allocations(self.flavor1,
2564             source_hostname)
2565 
2566         self._resize_and_check_allocations(server, self.flavor1, self.flavor2,
2567             source_rp_uuid, dest_rp_uuid)
2568 
2569         # Confirm the resize and check the usages
2570         post = {'confirmResize': None}
2571         self.api.post_server_action(
2572             server['id'], post, check_response_status=[204])
2573         self._wait_for_state_change(self.api, server, 'ACTIVE')
2574 
2575         # After confirming, we should have an allocation only on the
2576         # destination host
2577 
2578         # The target host usage should be according to the new flavor
2579         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor2)
2580         self.assertRequestMatchesUsage({'VCPU': 0,
2581                                         'MEMORY_MB': 0,
2582                                         'DISK_GB': 0}, source_rp_uuid)
2583 
2584         # and the target host allocation should be according to the new flavor
2585         self.assertFlavorMatchesAllocation(self.flavor2, server['id'],
2586                                            dest_rp_uuid)
2587 
2588         self._run_periodics()
2589 
2590         # Check we're still accurate after running the periodics
2591 
2592         # and the target host usage should be according to the new flavor
2593         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor2)
2594         self.assertRequestMatchesUsage({'VCPU': 0,
2595                                         'MEMORY_MB': 0,
2596                                         'DISK_GB': 0}, source_rp_uuid)
2597 
2598         # and the server allocates only from the target host
2599         self.assertFlavorMatchesAllocation(self.flavor2, server['id'],
2600                                            dest_rp_uuid)
2601 
2602         self._delete_and_check_allocations(server)
2603 
2604     def test_resize_revert_same_host(self):
2605         # make sure that the test only uses a single host
2606         compute2_service_id = self.admin_api.get_services(
2607             host=self.compute2.host, binary='nova-compute')[0]['id']
2608         self.admin_api.put_service(compute2_service_id, {'status': 'disabled'})
2609 
2610         hostname = self.compute1.manager.host
2611         rp_uuid = self._get_provider_uuid_by_host(hostname)
2612 
2613         server = self._boot_and_check_allocations(self.flavor2, hostname)
2614 
2615         self._resize_to_same_host_and_check_allocations(
2616             server, self.flavor2, self.flavor3, rp_uuid)
2617 
2618         # Revert the resize and check the usages
2619         post = {'revertResize': None}
2620         self.api.post_server_action(server['id'], post)
2621         self._wait_for_state_change(self.api, server, 'ACTIVE')
2622 
2623         self._run_periodics()
2624 
2625         # after revert only allocations due to the old flavor should remain
2626         self.assertFlavorMatchesUsage(rp_uuid, self.flavor2)
2627 
2628         self.assertFlavorMatchesAllocation(self.flavor2, server['id'],
2629                                            rp_uuid)
2630 
2631         self._delete_and_check_allocations(server)
2632 
2633     def test_resize_confirm_same_host(self):
2634         # make sure that the test only uses a single host
2635         compute2_service_id = self.admin_api.get_services(
2636             host=self.compute2.host, binary='nova-compute')[0]['id']
2637         self.admin_api.put_service(compute2_service_id, {'status': 'disabled'})
2638 
2639         hostname = self.compute1.manager.host
2640         rp_uuid = self._get_provider_uuid_by_host(hostname)
2641 
2642         server = self._boot_and_check_allocations(self.flavor2, hostname)
2643 
2644         self._resize_to_same_host_and_check_allocations(
2645             server, self.flavor2, self.flavor3, rp_uuid)
2646 
2647         # Confirm the resize and check the usages
2648         post = {'confirmResize': None}
2649         self.api.post_server_action(
2650             server['id'], post, check_response_status=[204])
2651         self._wait_for_state_change(self.api, server, 'ACTIVE')
2652 
2653         self._run_periodics()
2654 
2655         # after confirm only allocations due to the new flavor should remain
2656         self.assertFlavorMatchesUsage(rp_uuid, self.flavor3)
2657 
2658         self.assertFlavorMatchesAllocation(self.flavor3, server['id'],
2659                                            rp_uuid)
2660 
2661         self._delete_and_check_allocations(server)
2662 
2663     def test_resize_not_enough_resource(self):
2664         # Try to resize to a flavor that requests more VCPU than what the
2665         # compute hosts has available and expect the resize to fail
2666 
2667         flavor_body = {'flavor':
2668                            {'name': 'test_too_big_flavor',
2669                             'ram': 1024,
2670                             'vcpus': fake.MediumFakeDriver.vcpus + 1,
2671                             'disk': 20,
2672                             }}
2673 
2674         big_flavor = self.api.post_flavor(flavor_body)
2675 
2676         dest_hostname = self.compute2.host
2677         source_hostname = self._other_hostname(dest_hostname)
2678         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2679         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
2680 
2681         server = self._boot_and_check_allocations(
2682             self.flavor1, source_hostname)
2683 
2684         self.flags(allow_resize_to_same_host=False)
2685         resize_req = {
2686             'resize': {
2687                 'flavorRef': big_flavor['id']
2688             }
2689         }
2690 
2691         resp = self.api.post_server_action(
2692             server['id'], resize_req, check_response_status=[400])
2693         self.assertEqual(
2694             resp['badRequest']['message'],
2695             "No valid host was found. No valid host found for resize")
2696         server = self.admin_api.get_server(server['id'])
2697         self.assertEqual(source_hostname, server['OS-EXT-SRV-ATTR:host'])
2698 
2699         # only the source host shall have usages after the failed resize
2700         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
2701 
2702         # Check that the other provider has no usage
2703         self.assertRequestMatchesUsage(
2704             {'VCPU': 0,
2705              'MEMORY_MB': 0,
2706              'DISK_GB': 0}, dest_rp_uuid)
2707 
2708         # Check that the server only allocates resource from the host it is
2709         # booted on
2710         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
2711                                            source_rp_uuid)
2712 
2713         self._delete_and_check_allocations(server)
2714 
2715     def test_resize_delete_while_verify(self):
2716         """Test scenario where the server is deleted while in the
2717         VERIFY_RESIZE state and ensures the allocations are properly
2718         cleaned up from the source and target compute node resource providers.
2719         The _confirm_resize_on_deleting() method in the API is actually
2720         responsible for making sure the migration-based allocations get
2721         cleaned up by confirming the resize on the source host before deleting
2722         the server from the target host.
2723         """
2724         dest_hostname = 'host2'
2725         source_hostname = self._other_hostname(dest_hostname)
2726         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2727         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
2728 
2729         server = self._boot_and_check_allocations(self.flavor1,
2730                                                   source_hostname)
2731 
2732         self._resize_and_check_allocations(server, self.flavor1, self.flavor2,
2733                                            source_rp_uuid, dest_rp_uuid)
2734 
2735         self._delete_and_check_allocations(server)
2736 
2737     def _wait_for_notification_event_type(self, event_type, max_retries=50):
2738         retry_counter = 0
2739         while True:
2740             if len(fake_notifier.NOTIFICATIONS) > 0:
2741                 for notification in fake_notifier.NOTIFICATIONS:
2742                     if notification.event_type == event_type:
2743                         return
2744             if retry_counter == max_retries:
2745                 self.fail('Wait for notification event type (%s) failed'
2746                           % event_type)
2747             retry_counter += 1
2748             time.sleep(0.1)
2749 
2750     def test_evacuate_with_no_compute(self):
2751         source_hostname = self.compute1.host
2752         dest_hostname = self.compute2.host
2753         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2754         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
2755 
2756         # Disable compute service on destination host
2757         compute2_service_id = self.admin_api.get_services(
2758             host=dest_hostname, binary='nova-compute')[0]['id']
2759         self.admin_api.put_service(compute2_service_id, {'status': 'disabled'})
2760 
2761         server = self._boot_and_check_allocations(
2762             self.flavor1, source_hostname)
2763 
2764         # Force source compute down
2765         source_compute_id = self.admin_api.get_services(
2766             host=source_hostname, binary='nova-compute')[0]['id']
2767         self.compute1.stop()
2768         self.admin_api.put_service(
2769             source_compute_id, {'forced_down': 'true'})
2770 
2771         # Initialize fake_notifier
2772         fake_notifier.stub_notifier(self)
2773         fake_notifier.reset()
2774 
2775         # Initiate evacuation
2776         post = {'evacuate': {}}
2777         self.api.post_server_action(server['id'], post)
2778 
2779         # NOTE(elod.illes): Should be changed to non-polling solution when
2780         # patch https://review.openstack.org/#/c/482629/ gets merged:
2781         # fake_notifier.wait_for_versioned_notifications(
2782         #     'compute_task.rebuild_server')
2783         self._wait_for_notification_event_type('compute_task.rebuild_server')
2784 
2785         self._run_periodics()
2786 
2787         # There is no other host to evacuate to so the rebuild should put the
2788         # VM to ERROR state, but it should remain on source compute
2789         expected_params = {'OS-EXT-SRV-ATTR:host': source_hostname,
2790                            'status': 'ERROR'}
2791         server = self._wait_for_server_parameter(self.api, server,
2792                                                  expected_params)
2793 
2794         # Check migrations
2795         migrations = self.api.get_migrations()
2796         self.assertEqual(1, len(migrations))
2797         self.assertEqual('evacuation', migrations[0]['migration_type'])
2798         self.assertEqual(server['id'], migrations[0]['instance_uuid'])
2799         self.assertEqual(source_hostname, migrations[0]['source_compute'])
2800         self.assertEqual('error', migrations[0]['status'])
2801 
2802         # Restart source host
2803         self.admin_api.put_service(
2804             source_compute_id, {'forced_down': 'false'})
2805         self.compute1.start()
2806 
2807         self._run_periodics()
2808 
2809         # Check allocation and usages: should only use resources on source host
2810         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
2811 
2812         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
2813                                            source_rp_uuid)
2814         zero_usage = {'VCPU': 0, 'DISK_GB': 0, 'MEMORY_MB': 0}
2815         self.assertRequestMatchesUsage(zero_usage, dest_rp_uuid)
2816 
2817         self._delete_and_check_allocations(server)
2818 
2819     def test_migrate_no_valid_host(self):
2820         source_hostname = self.compute1.host
2821         dest_hostname = self.compute2.host
2822         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2823         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
2824 
2825         server = self._boot_and_check_allocations(
2826             self.flavor1, source_hostname)
2827 
2828         dest_compute_id = self.admin_api.get_services(
2829             host=dest_hostname, binary='nova-compute')[0]['id']
2830         self.compute2.stop()
2831         # force it down to avoid waiting for the service group to time out
2832         self.admin_api.put_service(
2833             dest_compute_id, {'forced_down': 'true'})
2834 
2835         # migrate the server
2836         post = {'migrate': None}
2837         ex = self.assertRaises(client.OpenStackApiException,
2838                                self.api.post_server_action,
2839                                server['id'], post)
2840         self.assertIn('No valid host', six.text_type(ex))
2841         expected_params = {'OS-EXT-SRV-ATTR:host': source_hostname,
2842                            'status': 'ACTIVE'}
2843         self._wait_for_server_parameter(self.api, server, expected_params)
2844 
2845         self._run_periodics()
2846 
2847         # Expect to have allocation only on source_host
2848         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
2849         zero_usage = {'VCPU': 0, 'DISK_GB': 0, 'MEMORY_MB': 0}
2850         self.assertRequestMatchesUsage(zero_usage, dest_rp_uuid)
2851 
2852         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
2853                                            source_rp_uuid)
2854 
2855         self._delete_and_check_allocations(server)
2856 
2857     def test_evacuate(self):
2858         source_hostname = self.compute1.host
2859         dest_hostname = self.compute2.host
2860         server = self._boot_and_check_allocations(
2861             self.flavor1, source_hostname)
2862 
2863         source_compute_id = self.admin_api.get_services(
2864             host=source_hostname, binary='nova-compute')[0]['id']
2865 
2866         self.compute1.stop()
2867         # force it down to avoid waiting for the service group to time out
2868         self.admin_api.put_service(
2869             source_compute_id, {'forced_down': 'true'})
2870 
2871         # evacuate the server
2872         post = {'evacuate': {}}
2873         self.api.post_server_action(
2874             server['id'], post)
2875         expected_params = {'OS-EXT-SRV-ATTR:host': dest_hostname,
2876                            'status': 'ACTIVE'}
2877         server = self._wait_for_server_parameter(self.api, server,
2878                                                  expected_params)
2879 
2880         # Expect to have allocation and usages on both computes as the
2881         # source compute is still down
2882         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2883         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
2884 
2885         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
2886 
2887         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
2888 
2889         self._check_allocation_during_evacuate(
2890             self.flavor1, server['id'], source_rp_uuid, dest_rp_uuid)
2891 
2892         # restart the source compute
2893         self.restart_compute_service(self.compute1)
2894 
2895         self.admin_api.put_service(
2896             source_compute_id, {'forced_down': 'false'})
2897 
2898         source_usages = self._get_provider_usages(source_rp_uuid)
2899         self.assertEqual({'VCPU': 0,
2900                           'MEMORY_MB': 0,
2901                           'DISK_GB': 0},
2902                          source_usages)
2903 
2904         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
2905 
2906         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
2907                                            dest_rp_uuid)
2908 
2909         self._delete_and_check_allocations(server)
2910 
2911     def test_evacuate_forced_host(self):
2912         """Evacuating a server with a forced host bypasses the scheduler
2913         which means conductor has to create the allocations against the
2914         destination node. This test recreates the scenarios and asserts
2915         the allocations on the source and destination nodes are as expected.
2916         """
2917         source_hostname = self.compute1.host
2918         dest_hostname = self.compute2.host
2919 
2920         # the ability to force evacuate a server is removed entirely in 2.68
2921         self.api.microversion = '2.67'
2922 
2923         server = self._boot_and_check_allocations(
2924             self.flavor1, source_hostname)
2925 
2926         source_compute_id = self.admin_api.get_services(
2927             host=source_hostname, binary='nova-compute')[0]['id']
2928 
2929         self.compute1.stop()
2930         # force it down to avoid waiting for the service group to time out
2931         self.admin_api.put_service(
2932             source_compute_id, {'forced_down': 'true'})
2933 
2934         # evacuate the server and force the destination host which bypasses
2935         # the scheduler
2936         post = {
2937             'evacuate': {
2938                 'host': dest_hostname,
2939                 'force': True
2940             }
2941         }
2942         self.api.post_server_action(server['id'], post)
2943         expected_params = {'OS-EXT-SRV-ATTR:host': dest_hostname,
2944                            'status': 'ACTIVE'}
2945         server = self._wait_for_server_parameter(self.api, server,
2946                                                  expected_params)
2947 
2948         # Run the periodics to show those don't modify allocations.
2949         self._run_periodics()
2950 
2951         # Expect to have allocation and usages on both computes as the
2952         # source compute is still down
2953         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2954         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
2955 
2956         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
2957 
2958         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
2959 
2960         self._check_allocation_during_evacuate(
2961             self.flavor1, server['id'], source_rp_uuid, dest_rp_uuid)
2962 
2963         # restart the source compute
2964         self.restart_compute_service(self.compute1)
2965         self.admin_api.put_service(
2966             source_compute_id, {'forced_down': 'false'})
2967 
2968         # Run the periodics again to show they don't change anything.
2969         self._run_periodics()
2970 
2971         # When the source node starts up, the instance has moved so the
2972         # ResourceTracker should cleanup allocations for the source node.
2973         source_usages = self._get_provider_usages(source_rp_uuid)
2974         self.assertEqual(
2975             {'VCPU': 0, 'MEMORY_MB': 0, 'DISK_GB': 0}, source_usages)
2976 
2977         # The usages/allocations should still exist on the destination node
2978         # after the source node starts back up.
2979         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
2980 
2981         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
2982                                            dest_rp_uuid)
2983 
2984         self._delete_and_check_allocations(server)
2985 
2986     def test_evacuate_forced_host_v268(self):
2987         """Evacuating a server with a forced host was removed in API
2988         microversion 2.68. This test ensures that the request is rejected.
2989         """
2990         source_hostname = self.compute1.host
2991         dest_hostname = self.compute2.host
2992 
2993         server = self._boot_and_check_allocations(
2994             self.flavor1, source_hostname)
2995 
2996         # evacuate the server and force the destination host which bypasses
2997         # the scheduler
2998         post = {
2999             'evacuate': {
3000                 'host': dest_hostname,
3001                 'force': True
3002             }
3003         }
3004         ex = self.assertRaises(client.OpenStackApiException,
3005                                self.api.post_server_action,
3006                                server['id'], post)
3007         self.assertIn("'force' was unexpected", six.text_type(ex))
3008 
3009     # NOTE(gibi): there is a similar test in SchedulerOnlyChecksTargetTest but
3010     # we want this test here as well because ServerMovingTest is a parent class
3011     # of multiple test classes that run this test case with different compute
3012     # node setups.
3013     def test_evacuate_host_specified_but_not_forced(self):
3014         """Evacuating a server with a host but using the scheduler to create
3015         the allocations against the destination node. This test recreates the
3016         scenarios and asserts the allocations on the source and destination
3017         nodes are as expected.
3018         """
3019         source_hostname = self.compute1.host
3020         dest_hostname = self.compute2.host
3021 
3022         server = self._boot_and_check_allocations(
3023             self.flavor1, source_hostname)
3024 
3025         source_compute_id = self.admin_api.get_services(
3026             host=source_hostname, binary='nova-compute')[0]['id']
3027 
3028         self.compute1.stop()
3029         # force it down to avoid waiting for the service group to time out
3030         self.admin_api.put_service(
3031             source_compute_id, {'forced_down': 'true'})
3032 
3033         # evacuate the server specify the target but do not force the
3034         # destination host to use the scheduler to validate the target host
3035         post = {
3036             'evacuate': {
3037                 'host': dest_hostname,
3038             }
3039         }
3040         self.api.post_server_action(server['id'], post)
3041         expected_params = {'OS-EXT-SRV-ATTR:host': dest_hostname,
3042                            'status': 'ACTIVE'}
3043         server = self._wait_for_server_parameter(self.api, server,
3044                                                  expected_params)
3045 
3046         # Run the periodics to show those don't modify allocations.
3047         self._run_periodics()
3048 
3049         # Expect to have allocation and usages on both computes as the
3050         # source compute is still down
3051         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
3052         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
3053 
3054         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
3055 
3056         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
3057 
3058         self._check_allocation_during_evacuate(
3059             self.flavor1, server['id'], source_rp_uuid, dest_rp_uuid)
3060 
3061         # restart the source compute
3062         self.restart_compute_service(self.compute1)
3063         self.admin_api.put_service(
3064             source_compute_id, {'forced_down': 'false'})
3065 
3066         # Run the periodics again to show they don't change anything.
3067         self._run_periodics()
3068 
3069         # When the source node starts up, the instance has moved so the
3070         # ResourceTracker should cleanup allocations for the source node.
3071         source_usages = self._get_provider_usages(source_rp_uuid)
3072         self.assertEqual(
3073             {'VCPU': 0, 'MEMORY_MB': 0, 'DISK_GB': 0}, source_usages)
3074 
3075         # The usages/allocations should still exist on the destination node
3076         # after the source node starts back up.
3077         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
3078 
3079         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
3080                                            dest_rp_uuid)
3081 
3082         self._delete_and_check_allocations(server)
3083 
3084     def test_evacuate_claim_on_dest_fails(self):
3085         """Tests that the allocations on the destination node are cleaned up
3086         when the rebuild move claim fails due to insufficient resources.
3087         """
3088         source_hostname = self.compute1.host
3089         dest_hostname = self.compute2.host
3090         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
3091 
3092         server = self._boot_and_check_allocations(
3093             self.flavor1, source_hostname)
3094 
3095         source_compute_id = self.admin_api.get_services(
3096             host=source_hostname, binary='nova-compute')[0]['id']
3097 
3098         self.compute1.stop()
3099         # force it down to avoid waiting for the service group to time out
3100         self.admin_api.put_service(
3101             source_compute_id, {'forced_down': 'true'})
3102 
3103         # NOTE(mriedem): This isn't great, and I'd like to fake out the driver
3104         # to make the claim fail, by doing something like returning a too high
3105         # memory_mb overhead, but the limits dict passed to the claim is empty
3106         # so the claim test is considering it as unlimited and never actually
3107         # performs a claim test. Configuring the scheduler to use the RamFilter
3108         # to get the memory_mb limit at least seems like it should work but
3109         # it doesn't appear to for some reason...
3110         def fake_move_claim(*args, **kwargs):
3111             # Assert the destination node allocation exists.
3112             self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
3113             raise exception.ComputeResourcesUnavailable(
3114                     reason='test_evacuate_claim_on_dest_fails')
3115 
3116         with mock.patch('nova.compute.claims.MoveClaim', fake_move_claim):
3117             # evacuate the server
3118             self.api.post_server_action(server['id'], {'evacuate': {}})
3119             # the migration will fail on the dest node and the instance will
3120             # go into error state
3121             server = self._wait_for_state_change(self.api, server, 'ERROR')
3122 
3123         # Run the periodics to show those don't modify allocations.
3124         self._run_periodics()
3125 
3126         # The allocation should still exist on the source node since it's
3127         # still down, and the allocation on the destination node should be
3128         # cleaned up.
3129         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
3130 
3131         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
3132 
3133         self.assertRequestMatchesUsage(
3134             {'VCPU': 0,
3135              'MEMORY_MB': 0,
3136              'DISK_GB': 0}, dest_rp_uuid)
3137 
3138         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
3139                                            source_rp_uuid)
3140 
3141         # restart the source compute
3142         self.restart_compute_service(self.compute1)
3143         self.admin_api.put_service(
3144             source_compute_id, {'forced_down': 'false'})
3145 
3146         # Run the periodics again to show they don't change anything.
3147         self._run_periodics()
3148 
3149         # The source compute shouldn't have cleaned up the allocation for
3150         # itself since the instance didn't move.
3151         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
3152 
3153         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
3154                                            source_rp_uuid)
3155 
3156     def test_evacuate_rebuild_on_dest_fails(self):
3157         """Tests that the allocations on the destination node are cleaned up
3158         automatically when the claim is made but the actual rebuild
3159         via the driver fails.
3160 
3161         """
3162         source_hostname = self.compute1.host
3163         dest_hostname = self.compute2.host
3164         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
3165 
3166         server = self._boot_and_check_allocations(
3167             self.flavor1, source_hostname)
3168 
3169         source_compute_id = self.admin_api.get_services(
3170             host=source_hostname, binary='nova-compute')[0]['id']
3171 
3172         self.compute1.stop()
3173         # force it down to avoid waiting for the service group to time out
3174         self.admin_api.put_service(
3175             source_compute_id, {'forced_down': 'true'})
3176 
3177         def fake_rebuild(*args, **kwargs):
3178             # Assert the destination node allocation exists.
3179             self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
3180             raise test.TestingException('test_evacuate_rebuild_on_dest_fails')
3181 
3182         with mock.patch.object(
3183                 self.compute2.driver, 'rebuild', fake_rebuild):
3184             # evacuate the server
3185             self.api.post_server_action(server['id'], {'evacuate': {}})
3186             # the migration will fail on the dest node and the instance will
3187             # go into error state
3188             server = self._wait_for_state_change(self.api, server, 'ERROR')
3189 
3190         # Run the periodics to show those don't modify allocations.
3191         self._run_periodics()
3192 
3193         # The allocation should still exist on the source node since it's
3194         # still down, and the allocation on the destination node should be
3195         # cleaned up.
3196         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
3197 
3198         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
3199 
3200         self.assertRequestMatchesUsage(
3201             {'VCPU': 0,
3202              'MEMORY_MB': 0,
3203              'DISK_GB': 0}, dest_rp_uuid)
3204 
3205         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
3206                                            source_rp_uuid)
3207 
3208         # restart the source compute
3209         self.restart_compute_service(self.compute1)
3210         self.admin_api.put_service(
3211             source_compute_id, {'forced_down': 'false'})
3212 
3213         # Run the periodics again to show they don't change anything.
3214         self._run_periodics()
3215 
3216         # The source compute shouldn't have cleaned up the allocation for
3217         # itself since the instance didn't move.
3218         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
3219 
3220         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
3221                                            source_rp_uuid)
3222 
3223     def _boot_then_shelve_and_check_allocations(self, hostname, rp_uuid):
3224         # avoid automatic shelve offloading
3225         self.flags(shelved_offload_time=-1)
3226         server = self._boot_and_check_allocations(
3227             self.flavor1, hostname)
3228         req = {
3229             'shelve': {}
3230         }
3231         self.api.post_server_action(server['id'], req)
3232         self._wait_for_state_change(self.api, server, 'SHELVED')
3233         # the host should maintain the existing allocation for this instance
3234         # while the instance is shelved
3235         self.assertFlavorMatchesUsage(rp_uuid, self.flavor1)
3236         # Check that the server only allocates resource from the host it is
3237         # booted on
3238         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
3239                                            rp_uuid)
3240         return server
3241 
3242     def test_shelve_unshelve(self):
3243         source_hostname = self.compute1.host
3244         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
3245         server = self._boot_then_shelve_and_check_allocations(
3246             source_hostname, source_rp_uuid)
3247 
3248         req = {
3249             'unshelve': {}
3250         }
3251         self.api.post_server_action(server['id'], req)
3252         self._wait_for_state_change(self.api, server, 'ACTIVE')
3253 
3254         # the host should have resource usage as the instance is ACTIVE
3255         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
3256 
3257         # Check that the server only allocates resource from the host it is
3258         # booted on
3259         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
3260                                            source_rp_uuid)
3261 
3262         self._delete_and_check_allocations(server)
3263 
3264     def _shelve_offload_and_check_allocations(self, server, source_rp_uuid):
3265         req = {
3266             'shelveOffload': {}
3267         }
3268         self.api.post_server_action(server['id'], req)
3269         self._wait_for_server_parameter(
3270             self.api, server, {'status': 'SHELVED_OFFLOADED',
3271                                'OS-EXT-SRV-ATTR:host': None,
3272                                'OS-EXT-AZ:availability_zone': ''})
3273         source_usages = self._get_provider_usages(source_rp_uuid)
3274         self.assertEqual({'VCPU': 0,
3275                           'MEMORY_MB': 0,
3276                           'DISK_GB': 0},
3277                          source_usages)
3278 
3279         allocations = self._get_allocations_by_server_uuid(server['id'])
3280         self.assertEqual(0, len(allocations))
3281 
3282     def test_shelve_offload_unshelve_diff_host(self):
3283         source_hostname = self.compute1.host
3284         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
3285         server = self._boot_then_shelve_and_check_allocations(
3286             source_hostname, source_rp_uuid)
3287 
3288         self._shelve_offload_and_check_allocations(server, source_rp_uuid)
3289 
3290         # unshelve after shelve offload will do scheduling. this test case
3291         # wants to test the scenario when the scheduler select a different host
3292         # to ushelve the instance. So we disable the original host.
3293         source_service_id = self.admin_api.get_services(
3294             host=source_hostname, binary='nova-compute')[0]['id']
3295         self.admin_api.put_service(source_service_id, {'status': 'disabled'})
3296 
3297         req = {
3298             'unshelve': {}
3299         }
3300         self.api.post_server_action(server['id'], req)
3301         server = self._wait_for_state_change(self.api, server, 'ACTIVE')
3302         # unshelving an offloaded instance will call the scheduler so the
3303         # instance might end up on a different host
3304         current_hostname = server['OS-EXT-SRV-ATTR:host']
3305         self.assertEqual(current_hostname, self._other_hostname(
3306             source_hostname))
3307 
3308         # the host running the instance should have resource usage
3309         current_rp_uuid = self._get_provider_uuid_by_host(current_hostname)
3310         self.assertFlavorMatchesUsage(current_rp_uuid, self.flavor1)
3311 
3312         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
3313                                            current_rp_uuid)
3314 
3315         self._delete_and_check_allocations(server)
3316 
3317     def test_shelve_offload_unshelve_same_host(self):
3318         source_hostname = self.compute1.host
3319         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
3320         server = self._boot_then_shelve_and_check_allocations(
3321             source_hostname, source_rp_uuid)
3322 
3323         self._shelve_offload_and_check_allocations(server, source_rp_uuid)
3324 
3325         # unshelve after shelve offload will do scheduling. this test case
3326         # wants to test the scenario when the scheduler select the same host
3327         # to ushelve the instance. So we disable the other host.
3328         source_service_id = self.admin_api.get_services(
3329             host=self._other_hostname(source_hostname),
3330             binary='nova-compute')[0]['id']
3331         self.admin_api.put_service(source_service_id, {'status': 'disabled'})
3332 
3333         req = {
3334             'unshelve': {}
3335         }
3336         self.api.post_server_action(server['id'], req)
3337         server = self._wait_for_state_change(self.api, server, 'ACTIVE')
3338         # unshelving an offloaded instance will call the scheduler so the
3339         # instance might end up on a different host
3340         current_hostname = server['OS-EXT-SRV-ATTR:host']
3341         self.assertEqual(current_hostname, source_hostname)
3342 
3343         # the host running the instance should have resource usage
3344         current_rp_uuid = self._get_provider_uuid_by_host(current_hostname)
3345         self.assertFlavorMatchesUsage(current_rp_uuid, self.flavor1)
3346 
3347         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
3348                                            current_rp_uuid)
3349 
3350         self._delete_and_check_allocations(server)
3351 
3352     def test_live_migrate_force(self):
3353         source_hostname = self.compute1.host
3354         dest_hostname = self.compute2.host
3355         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
3356         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
3357 
3358         # the ability to force live migrate a server is removed entirely in
3359         # 2.68
3360         self.api.microversion = '2.67'
3361 
3362         server = self._boot_and_check_allocations(
3363             self.flavor1, source_hostname)
3364 
3365         # live migrate the server and force the destination host which bypasses
3366         # the scheduler
3367         post = {
3368             'os-migrateLive': {
3369                 'host': dest_hostname,
3370                 'block_migration': True,
3371                 'force': True,
3372             }
3373         }
3374 
3375         self.api.post_server_action(server['id'], post)
3376         self._wait_for_server_parameter(self.api, server,
3377             {'OS-EXT-SRV-ATTR:host': dest_hostname,
3378              'status': 'ACTIVE'})
3379 
3380         self._run_periodics()
3381 
3382         # NOTE(danms): There should be no usage for the source
3383         self.assertRequestMatchesUsage(
3384             {'VCPU': 0,
3385              'MEMORY_MB': 0,
3386              'DISK_GB': 0}, source_rp_uuid)
3387 
3388         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
3389 
3390         # the server has an allocation on only the dest node
3391         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
3392                                            dest_rp_uuid)
3393 
3394         self._delete_and_check_allocations(server)
3395 
3396     def test_live_migrate_forced_v268(self):
3397         """Live migrating a server with a forced host was removed in API
3398         microversion 2.68. This test ensures that the request is rejected.
3399         """
3400         source_hostname = self.compute1.host
3401         dest_hostname = self.compute2.host
3402 
3403         server = self._boot_and_check_allocations(
3404             self.flavor1, source_hostname)
3405 
3406         # live migrate the server and force the destination host which bypasses
3407         # the scheduler
3408         post = {
3409             'os-migrateLive': {
3410                 'host': dest_hostname,
3411                 'block_migration': True,
3412                 'force': True,
3413             }
3414         }
3415 
3416         ex = self.assertRaises(client.OpenStackApiException,
3417                                self.api.post_server_action,
3418                                server['id'], post)
3419         self.assertIn("'force' was unexpected", six.text_type(ex))
3420 
3421     def test_live_migrate(self):
3422         source_hostname = self.compute1.host
3423         dest_hostname = self.compute2.host
3424         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
3425         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
3426 
3427         server = self._boot_and_check_allocations(
3428             self.flavor1, source_hostname)
3429         post = {
3430             'os-migrateLive': {
3431                 'host': dest_hostname,
3432                 'block_migration': True,
3433             }
3434         }
3435 
3436         self.api.post_server_action(server['id'], post)
3437         self._wait_for_server_parameter(self.api, server,
3438                                         {'OS-EXT-SRV-ATTR:host': dest_hostname,
3439                                          'status': 'ACTIVE'})
3440 
3441         self._run_periodics()
3442 
3443         # NOTE(danms): There should be no usage for the source
3444         self.assertRequestMatchesUsage(
3445             {'VCPU': 0,
3446              'MEMORY_MB': 0,
3447              'DISK_GB': 0}, source_rp_uuid)
3448 
3449         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
3450 
3451         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
3452                                            dest_rp_uuid)
3453 
3454         self._delete_and_check_allocations(server)
3455 
3456     def test_live_migrate_pre_check_fails(self):
3457         """Tests the case that the LiveMigrationTask in conductor has
3458         called the scheduler which picked a host and created allocations
3459         against it in Placement, but then when the conductor task calls
3460         check_can_live_migrate_destination on the destination compute it
3461         fails. The allocations on the destination compute node should be
3462         cleaned up before the conductor task asks the scheduler for another
3463         host to try the live migration.
3464         """
3465         self.failed_hostname = None
3466         source_hostname = self.compute1.host
3467         dest_hostname = self.compute2.host
3468         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
3469         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
3470 
3471         server = self._boot_and_check_allocations(
3472             self.flavor1, source_hostname)
3473 
3474         def fake_check_can_live_migrate_destination(
3475                 context, instance, src_compute_info, dst_compute_info,
3476                 block_migration=False, disk_over_commit=False):
3477             self.failed_hostname = dst_compute_info['host']
3478             raise exception.MigrationPreCheckError(
3479                 reason='test_live_migrate_pre_check_fails')
3480 
3481         with mock.patch('nova.virt.fake.FakeDriver.'
3482                         'check_can_live_migrate_destination',
3483                         side_effect=fake_check_can_live_migrate_destination):
3484             post = {
3485                 'os-migrateLive': {
3486                     'host': dest_hostname,
3487                     'block_migration': True,
3488                 }
3489             }
3490             self.api.post_server_action(server['id'], post)
3491             # As there are only two computes and we failed to live migrate to
3492             # the only other destination host, the LiveMigrationTask raises
3493             # MaxRetriesExceeded back to the conductor manager which handles it
3494             # generically and sets the instance back to ACTIVE status and
3495             # clears the task_state. The migration record status is set to
3496             # 'error', so that's what we need to look for to know when this
3497             # is done.
3498             migration = self._wait_for_migration_status(server, ['error'])
3499 
3500         # The source_compute should be set on the migration record, but the
3501         # destination shouldn't be as we never made it to one.
3502         self.assertEqual(source_hostname, migration['source_compute'])
3503         self.assertIsNone(migration['dest_compute'])
3504         # Make sure the destination host (the only other host) is the failed
3505         # host.
3506         self.assertEqual(dest_hostname, self.failed_hostname)
3507 
3508         # Since the instance didn't move, assert the allocations are still
3509         # on the source node.
3510         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
3511 
3512         # Assert the allocations, created by the scheduler, are cleaned up
3513         # after the migration pre-check error happens.
3514         self.assertRequestMatchesUsage(
3515             {'VCPU': 0,
3516              'MEMORY_MB': 0,
3517              'DISK_GB': 0}, dest_rp_uuid)
3518 
3519         # There should only be 1 allocation for the instance on the source node
3520         self.assertFlavorMatchesAllocation(
3521             self.flavor1, server['id'], source_rp_uuid)
3522 
3523         self._delete_and_check_allocations(server)
3524 
3525     @mock.patch('nova.virt.fake.FakeDriver.pre_live_migration')
3526     def test_live_migrate_rollback_cleans_dest_node_allocations(
3527             self, mock_pre_live_migration, force=False):
3528         """Tests the case that when live migration fails, either during the
3529         call to pre_live_migration on the destination, or during the actual
3530         live migration in the virt driver, the allocations on the destination
3531         node are rolled back since the instance is still on the source node.
3532         """
3533         source_hostname = self.compute1.host
3534         dest_hostname = self.compute2.host
3535         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
3536         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
3537 
3538         # the ability to force live migrate a server is removed entirely in
3539         # 2.68
3540         self.api.microversion = '2.67'
3541 
3542         server = self._boot_and_check_allocations(
3543             self.flavor1, source_hostname)
3544 
3545         def stub_pre_live_migration(context, instance, block_device_info,
3546                                     network_info, disk_info, migrate_data):
3547             # Make sure the source node allocations are against the migration
3548             # record and the dest node allocations are against the instance.
3549             self.assertFlavorMatchesAllocation(
3550                 self.flavor1, migrate_data.migration.uuid, source_rp_uuid)
3551 
3552             self.assertFlavorMatchesAllocation(
3553                 self.flavor1, server['id'], dest_rp_uuid)
3554             # The actual type of exception here doesn't matter. The point
3555             # is that the virt driver raised an exception from the
3556             # pre_live_migration method on the destination host.
3557             raise test.TestingException(
3558                 'test_live_migrate_rollback_cleans_dest_node_allocations')
3559 
3560         mock_pre_live_migration.side_effect = stub_pre_live_migration
3561 
3562         post = {
3563             'os-migrateLive': {
3564                 'host': dest_hostname,
3565                 'block_migration': True,
3566                 'force': force
3567             }
3568         }
3569         self.api.post_server_action(server['id'], post)
3570         # The compute manager will put the migration record into error status
3571         # when pre_live_migration fails, so wait for that to happen.
3572         migration = self._wait_for_migration_status(server, ['error'])
3573         # The _rollback_live_migration method in the compute manager will reset
3574         # the task_state on the instance, so wait for that to happen.
3575         server = self._wait_for_server_parameter(
3576             self.api, server, {'OS-EXT-STS:task_state': None})
3577 
3578         self.assertEqual(source_hostname, migration['source_compute'])
3579         self.assertEqual(dest_hostname, migration['dest_compute'])
3580 
3581         # Since the instance didn't move, assert the allocations are still
3582         # on the source node.
3583         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
3584 
3585         # Assert the allocations, created by the scheduler, are cleaned up
3586         # after the rollback happens.
3587         self.assertRequestMatchesUsage(
3588             {'VCPU': 0,
3589              'MEMORY_MB': 0,
3590              'DISK_GB': 0}, dest_rp_uuid)
3591 
3592         # There should only be 1 allocation for the instance on the source node
3593         self.assertFlavorMatchesAllocation(
3594             self.flavor1, server['id'], source_rp_uuid)
3595 
3596         self._delete_and_check_allocations(server)
3597 
3598     def test_live_migrate_rollback_cleans_dest_node_allocations_forced(self):
3599         """Tests the case that when a forced host live migration fails, either
3600         during the call to pre_live_migration on the destination, or during
3601         the actual live migration in the virt driver, the allocations on the
3602         destination node are rolled back since the instance is still on the
3603         source node.
3604         """
3605         self.test_live_migrate_rollback_cleans_dest_node_allocations(
3606             force=True)
3607 
3608     def test_rescheduling_when_migrating_instance(self):
3609         """Tests that allocations are removed from the destination node by
3610         the compute service when a cold migrate / resize fails and a reschedule
3611         request is sent back to conductor.
3612         """
3613         source_hostname = self.compute1.manager.host
3614         server = self._boot_and_check_allocations(
3615             self.flavor1, source_hostname)
3616 
3617         def fake_prep_resize(*args, **kwargs):
3618             dest_hostname = self._other_hostname(source_hostname)
3619             dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
3620             self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
3621             allocations = self._get_allocations_by_server_uuid(server['id'])
3622             self.assertIn(dest_rp_uuid, allocations)
3623 
3624             source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
3625             self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
3626             migration_uuid = self.get_migration_uuid_for_instance(server['id'])
3627             allocations = self._get_allocations_by_server_uuid(migration_uuid)
3628             self.assertIn(source_rp_uuid, allocations)
3629 
3630             raise test.TestingException('Simulated _prep_resize failure.')
3631 
3632         # Yes this isn't great in a functional test, but it's simple.
3633         self.stub_out('nova.compute.manager.ComputeManager._prep_resize',
3634                       fake_prep_resize)
3635 
3636         # Now migrate the server which is going to fail on the destination.
3637         self.api.post_server_action(server['id'], {'migrate': None})
3638 
3639         self._wait_for_action_fail_completion(
3640             server, instance_actions.MIGRATE, 'compute_prep_resize')
3641 
3642         dest_hostname = self._other_hostname(source_hostname)
3643         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
3644 
3645         # Expects no allocation records on the failed host.
3646         self.assertRequestMatchesUsage(
3647             {'VCPU': 0,
3648              'MEMORY_MB': 0,
3649              'DISK_GB': 0}, dest_rp_uuid)
3650 
3651         # Ensure the allocation records still exist on the source host.
3652         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
3653         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
3654         allocations = self._get_allocations_by_server_uuid(server['id'])
3655         self.assertIn(source_rp_uuid, allocations)
3656 
3657     def _test_resize_to_same_host_instance_fails(self, failing_method,
3658                                                  event_name):
3659         """Tests that when we resize to the same host and resize fails in
3660         the given method, we cleanup the allocations before rescheduling.
3661         """
3662         # make sure that the test only uses a single host
3663         compute2_service_id = self.admin_api.get_services(
3664             host=self.compute2.host, binary='nova-compute')[0]['id']
3665         self.admin_api.put_service(compute2_service_id, {'status': 'disabled'})
3666 
3667         hostname = self.compute1.manager.host
3668         rp_uuid = self._get_provider_uuid_by_host(hostname)
3669 
3670         server = self._boot_and_check_allocations(self.flavor1, hostname)
3671 
3672         def fake_resize_method(*args, **kwargs):
3673             # Ensure the allocations are doubled now before we fail.
3674             self.assertFlavorMatchesUsage(rp_uuid, self.flavor1, self.flavor2)
3675             raise test.TestingException('Simulated resize failure.')
3676 
3677         # Yes this isn't great in a functional test, but it's simple.
3678         self.stub_out(
3679             'nova.compute.manager.ComputeManager.%s' % failing_method,
3680             fake_resize_method)
3681 
3682         self.flags(allow_resize_to_same_host=True)
3683         resize_req = {
3684             'resize': {
3685                 'flavorRef': self.flavor2['id']
3686             }
3687         }
3688         self.api.post_server_action(server['id'], resize_req)
3689 
3690         self._wait_for_action_fail_completion(
3691             server, instance_actions.RESIZE, event_name)
3692 
3693         # Ensure the allocation records still exist on the host.
3694         source_rp_uuid = self._get_provider_uuid_by_host(hostname)
3695         # The new_flavor should have been subtracted from the doubled
3696         # allocation which just leaves us with the original flavor.
3697         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
3698 
3699     def test_resize_to_same_host_prep_resize_fails(self):
3700         self._test_resize_to_same_host_instance_fails(
3701             '_prep_resize', 'compute_prep_resize')
3702 
3703     def test_resize_instance_fails_allocation_cleanup(self):
3704         self._test_resize_to_same_host_instance_fails(
3705             '_resize_instance', 'compute_resize_instance')
3706 
3707     def test_finish_resize_fails_allocation_cleanup(self):
3708         self._test_resize_to_same_host_instance_fails(
3709             '_finish_resize', 'compute_finish_resize')
3710 
3711     def _test_resize_reschedule_uses_host_lists(self, fails, num_alts=None):
3712         """Test that when a resize attempt fails, the retry comes from the
3713         supplied host_list, and does not call the scheduler.
3714         """
3715         server_req = self._build_minimal_create_server_request(
3716                 self.api, "some-server", flavor_id=self.flavor1["id"],
3717                 image_uuid="155d900f-4e14-4e4c-a73d-069cbf4541e6",
3718                 networks='none')
3719 
3720         created_server = self.api.post_server({"server": server_req})
3721         server = self._wait_for_state_change(self.api, created_server,
3722                 "ACTIVE")
3723         inst_host = server["OS-EXT-SRV-ATTR:host"]
3724         uuid_orig = self._get_provider_uuid_by_host(inst_host)
3725 
3726         # We will need four new compute nodes to test the resize, representing
3727         # the host selected by select_destinations(), along with 3 alternates.
3728         self._start_compute(host="selection")
3729         self._start_compute(host="alt_host1")
3730         self._start_compute(host="alt_host2")
3731         self._start_compute(host="alt_host3")
3732         uuid_sel = self._get_provider_uuid_by_host("selection")
3733         uuid_alt1 = self._get_provider_uuid_by_host("alt_host1")
3734         uuid_alt2 = self._get_provider_uuid_by_host("alt_host2")
3735         uuid_alt3 = self._get_provider_uuid_by_host("alt_host3")
3736         hosts = [{"name": "selection", "uuid": uuid_sel},
3737                  {"name": "alt_host1", "uuid": uuid_alt1},
3738                  {"name": "alt_host2", "uuid": uuid_alt2},
3739                  {"name": "alt_host3", "uuid": uuid_alt3},
3740                 ]
3741 
3742         self.flags(weight_classes=[__name__ + '.AltHostWeigher'],
3743                    group='filter_scheduler')
3744         self.scheduler_service.stop()
3745         self.scheduler_service = self.start_service('scheduler')
3746 
3747         def fake_prep_resize(*args, **kwargs):
3748             if self.num_fails < fails:
3749                 self.num_fails += 1
3750                 raise Exception("fake_prep_resize")
3751             actual_prep_resize(*args, **kwargs)
3752 
3753         # Yes this isn't great in a functional test, but it's simple.
3754         actual_prep_resize = compute_manager.ComputeManager._prep_resize
3755         self.stub_out("nova.compute.manager.ComputeManager._prep_resize",
3756                       fake_prep_resize)
3757         self.num_fails = 0
3758         num_alts = 4 if num_alts is None else num_alts
3759         # Make sure we have enough retries available for the number of
3760         # requested fails.
3761         attempts = min(fails + 2, num_alts)
3762         self.flags(max_attempts=attempts, group='scheduler')
3763         server_uuid = server["id"]
3764         data = {"resize": {"flavorRef": self.flavor2["id"]}}
3765         self.api.post_server_action(server_uuid, data)
3766 
3767         if num_alts < fails:
3768             # We will run out of alternates before populate_retry will
3769             # raise a MaxRetriesExceeded exception, so the migration will
3770             # fail and the server should be in status "ERROR"
3771             server = self._wait_for_state_change(self.api, created_server,
3772                     "ERROR")
3773             # The usage should be unchanged from the original flavor
3774             self.assertFlavorMatchesUsage(uuid_orig, self.flavor1)
3775             # There should be no usages on any of the hosts
3776             target_uuids = (uuid_sel, uuid_alt1, uuid_alt2, uuid_alt3)
3777             empty_usage = {"VCPU": 0, "MEMORY_MB": 0, "DISK_GB": 0}
3778             for target_uuid in target_uuids:
3779                 usage = self._get_provider_usages(target_uuid)
3780                 self.assertEqual(empty_usage, usage)
3781         else:
3782             server = self._wait_for_state_change(self.api, created_server,
3783                     "VERIFY_RESIZE")
3784             # Verify that the selected host failed, and was rescheduled to
3785             # an alternate host.
3786             new_server_host = server.get("OS-EXT-SRV-ATTR:host")
3787             expected_host = hosts[fails]["name"]
3788             self.assertEqual(expected_host, new_server_host)
3789             uuid_dest = hosts[fails]["uuid"]
3790             # The usage should match the resized flavor
3791             self.assertFlavorMatchesUsage(uuid_dest, self.flavor2)
3792             # Verify that the other host have no allocations
3793             target_uuids = (uuid_sel, uuid_alt1, uuid_alt2, uuid_alt3)
3794             empty_usage = {"VCPU": 0, "MEMORY_MB": 0, "DISK_GB": 0}
3795             for target_uuid in target_uuids:
3796                 if target_uuid == uuid_dest:
3797                     continue
3798                 usage = self._get_provider_usages(target_uuid)
3799                 self.assertEqual(empty_usage, usage)
3800 
3801             # Verify that there is only one migration record for the instance.
3802             ctxt = context.get_admin_context()
3803             filters = {"instance_uuid": server["id"]}
3804             migrations = objects.MigrationList.get_by_filters(ctxt, filters)
3805             self.assertEqual(1, len(migrations.objects))
3806 
3807     def test_resize_reschedule_uses_host_lists_1_fail(self):
3808         self._test_resize_reschedule_uses_host_lists(fails=1)
3809 
3810     def test_resize_reschedule_uses_host_lists_3_fails(self):
3811         self._test_resize_reschedule_uses_host_lists(fails=3)
3812 
3813     def test_resize_reschedule_uses_host_lists_not_enough_alts(self):
3814         self._test_resize_reschedule_uses_host_lists(fails=3, num_alts=1)
3815 
3816     def test_migrate_confirm(self):
3817         source_hostname = self.compute1.host
3818         dest_hostname = self.compute2.host
3819         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
3820         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
3821 
3822         server = self._boot_and_check_allocations(
3823             self.flavor1, source_hostname)
3824 
3825         self._migrate_and_check_allocations(
3826             server, self.flavor1, source_rp_uuid, dest_rp_uuid)
3827 
3828         # Confirm the move and check the usages
3829         post = {'confirmResize': None}
3830         self.api.post_server_action(
3831             server['id'], post, check_response_status=[204])
3832         self._wait_for_state_change(self.api, server, 'ACTIVE')
3833 
3834         def _check_allocation():
3835             # the target host usage should be according to the flavor
3836             self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
3837             # the source host has no usage
3838             self.assertRequestMatchesUsage({'VCPU': 0,
3839                                             'MEMORY_MB': 0,
3840                                             'DISK_GB': 0}, source_rp_uuid)
3841 
3842             # and the target host allocation should be according to the flavor
3843             self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
3844                                                dest_rp_uuid)
3845 
3846         # After confirming, we should have an allocation only on the
3847         # destination host
3848         _check_allocation()
3849         self._run_periodics()
3850 
3851         # Check we're still accurate after running the periodics
3852         _check_allocation()
3853 
3854         self._delete_and_check_allocations(server)
3855 
3856     def test_migrate_revert(self):
3857         source_hostname = self.compute1.host
3858         dest_hostname = self.compute2.host
3859         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
3860         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
3861 
3862         server = self._boot_and_check_allocations(
3863             self.flavor1, source_hostname)
3864 
3865         self._migrate_and_check_allocations(
3866             server, self.flavor1, source_rp_uuid, dest_rp_uuid)
3867 
3868         # Revert the move and check the usages
3869         post = {'revertResize': None}
3870         self.api.post_server_action(server['id'], post)
3871         self._wait_for_state_change(self.api, server, 'ACTIVE')
3872 
3873         def _check_allocation():
3874             self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
3875             self.assertRequestMatchesUsage({'VCPU': 0,
3876                                             'MEMORY_MB': 0,
3877                                             'DISK_GB': 0}, dest_rp_uuid)
3878 
3879             # Check that the server only allocates resource from the original
3880             # host
3881             self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
3882                                                source_rp_uuid)
3883 
3884         # the original host expected to have the old resource allocation
3885         _check_allocation()
3886         self._run_periodics()
3887         _check_allocation()
3888 
3889         self._delete_and_check_allocations(server)
3890 
3891 
3892 class ServerLiveMigrateForceAndAbort(
3893         integrated_helpers.ProviderUsageBaseTestCase):
3894     """Test Server live migrations, which delete the migration or
3895     force_complete it, and check the allocations after the operations.
3896 
3897     The test are using fakedriver to handle the force_completion and deletion
3898     of live migration.
3899     """
3900 
3901     compute_driver = 'fake.FakeLiveMigrateDriver'
3902 
3903     def setUp(self):
3904         super(ServerLiveMigrateForceAndAbort, self).setUp()
3905 
3906         self.compute1 = self._start_compute(host='host1')
3907         self.compute2 = self._start_compute(host='host2')
3908 
3909         flavors = self.api.get_flavors()
3910         self.flavor1 = flavors[0]
3911 
3912     def test_live_migrate_force_complete(self):
3913         source_hostname = self.compute1.host
3914         dest_hostname = self.compute2.host
3915         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
3916         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
3917 
3918         server = self._boot_and_check_allocations(
3919             self.flavor1, source_hostname)
3920 
3921         post = {
3922             'os-migrateLive': {
3923                 'host': dest_hostname,
3924                 'block_migration': True,
3925             }
3926         }
3927         self.api.post_server_action(server['id'], post)
3928 
3929         migration = self._wait_for_migration_status(server, ['running'])
3930         self.api.force_complete_migration(server['id'],
3931                                           migration['id'])
3932 
3933         self._wait_for_server_parameter(self.api, server,
3934                                         {'OS-EXT-SRV-ATTR:host': dest_hostname,
3935                                          'status': 'ACTIVE'})
3936 
3937         self._run_periodics()
3938 
3939         self.assertRequestMatchesUsage(
3940             {'VCPU': 0,
3941              'MEMORY_MB': 0,
3942              'DISK_GB': 0}, source_rp_uuid)
3943 
3944         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
3945         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
3946                                            dest_rp_uuid)
3947 
3948         self._delete_and_check_allocations(server)
3949 
3950     def test_live_migrate_delete(self):
3951         source_hostname = self.compute1.host
3952         dest_hostname = self.compute2.host
3953         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
3954         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
3955 
3956         server = self._boot_and_check_allocations(
3957             self.flavor1, source_hostname)
3958 
3959         post = {
3960             'os-migrateLive': {
3961                 'host': dest_hostname,
3962                 'block_migration': True,
3963             }
3964         }
3965         self.api.post_server_action(server['id'], post)
3966 
3967         migration = self._wait_for_migration_status(server, ['running'])
3968 
3969         self.api.delete_migration(server['id'], migration['id'])
3970         self._wait_for_server_parameter(self.api, server,
3971             {'OS-EXT-SRV-ATTR:host': source_hostname,
3972              'status': 'ACTIVE'})
3973 
3974         self._run_periodics()
3975 
3976         allocations = self._get_allocations_by_server_uuid(server['id'])
3977         self.assertNotIn(dest_rp_uuid, allocations)
3978 
3979         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
3980 
3981         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
3982                                            source_rp_uuid)
3983 
3984         self.assertRequestMatchesUsage({'VCPU': 0,
3985                                         'MEMORY_MB': 0,
3986                                         'DISK_GB': 0}, dest_rp_uuid)
3987 
3988         self._delete_and_check_allocations(server)
3989 
3990 
3991 class ServerLiveMigrateForceAndAbortWithNestedResourcesRequest(
3992         ServerLiveMigrateForceAndAbort):
3993     compute_driver = 'fake.FakeLiveMigrateDriverWithNestedCustomResources'
3994 
3995     def setUp(self):
3996         super(ServerLiveMigrateForceAndAbortWithNestedResourcesRequest,
3997               self).setUp()
3998         # modify the flavor used in the test base class to require one piece of
3999         # CUSTOM_MAGIC resource as well.
4000 
4001         self.api.post_extra_spec(
4002             self.flavor1['id'], {'extra_specs': {'resources:CUSTOM_MAGIC': 1}})
4003         # save the extra_specs in the flavor stored in the test case as
4004         # well
4005         self.flavor1['extra_specs'] = {'resources:CUSTOM_MAGIC': 1}
4006 
4007 
4008 class ServerRescheduleTests(integrated_helpers.ProviderUsageBaseTestCase):
4009     """Tests server create scenarios which trigger a reschedule during
4010     a server build and validates that allocations in Placement
4011     are properly cleaned up.
4012 
4013     Uses a fake virt driver that fails the build on the first attempt.
4014     """
4015 
4016     compute_driver = 'fake.FakeRescheduleDriver'
4017 
4018     def setUp(self):
4019         super(ServerRescheduleTests, self).setUp()
4020         self.compute1 = self._start_compute(host='host1')
4021         self.compute2 = self._start_compute(host='host2')
4022 
4023         flavors = self.api.get_flavors()
4024         self.flavor1 = flavors[0]
4025 
4026     def _other_hostname(self, host):
4027         other_host = {'host1': 'host2',
4028                       'host2': 'host1'}
4029         return other_host[host]
4030 
4031     def test_rescheduling_when_booting_instance(self):
4032         """Tests that allocations, created by the scheduler, are cleaned
4033         from the source node when the build fails on that node and is
4034         rescheduled to another node.
4035         """
4036         server_req = self._build_minimal_create_server_request(
4037                 self.api, 'some-server', flavor_id=self.flavor1['id'],
4038                 image_uuid='155d900f-4e14-4e4c-a73d-069cbf4541e6',
4039                 networks='none')
4040 
4041         created_server = self.api.post_server({'server': server_req})
4042         server = self._wait_for_state_change(
4043                 self.api, created_server, 'ACTIVE')
4044         dest_hostname = server['OS-EXT-SRV-ATTR:host']
4045         failed_hostname = self._other_hostname(dest_hostname)
4046 
4047         LOG.info('failed on %s', failed_hostname)
4048         LOG.info('booting on %s', dest_hostname)
4049 
4050         failed_rp_uuid = self._get_provider_uuid_by_host(failed_hostname)
4051         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
4052 
4053         # Expects no allocation records on the failed host.
4054         self.assertRequestMatchesUsage(
4055             {'VCPU': 0,
4056              'MEMORY_MB': 0,
4057              'DISK_GB': 0}, failed_rp_uuid)
4058 
4059         # Ensure the allocation records on the destination host.
4060         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
4061 
4062     def test_allocation_fails_during_reschedule(self):
4063         """Verify that if nova fails to allocate resources during re-schedule
4064         then the server is put into ERROR state properly.
4065         """
4066 
4067         server_req = self._build_minimal_create_server_request(
4068             self.api, 'some-server', flavor_id=self.flavor1['id'],
4069             image_uuid='155d900f-4e14-4e4c-a73d-069cbf4541e6',
4070             networks='none')
4071 
4072         orig_claim = utils.claim_resources
4073         # First call is during boot, we want that to succeed normally. Then the
4074         # fake virt driver triggers a re-schedule. During that re-schedule we
4075         # simulate that the placement call fails.
4076         with mock.patch('nova.scheduler.utils.claim_resources',
4077                         side_effect=[
4078                             orig_claim,
4079                             exception.AllocationUpdateFailed(
4080                                 consumer_uuid=uuids.inst1, error='testing')]):
4081 
4082             server = self.api.post_server({'server': server_req})
4083             server = self._wait_for_state_change(
4084                 self.admin_api, server, 'ERROR')
4085 
4086         self._delete_and_check_allocations(server)
4087 
4088 
4089 class ServerRescheduleTestsWithNestedResourcesRequest(ServerRescheduleTests):
4090     compute_driver = 'fake.FakeRescheduleDriverWithNestedCustomResources'
4091 
4092     def setUp(self):
4093         super(ServerRescheduleTestsWithNestedResourcesRequest, self).setUp()
4094         # modify the flavor used in the test base class to require one piece of
4095         # CUSTOM_MAGIC resource as well.
4096 
4097         self.api.post_extra_spec(
4098             self.flavor1['id'], {'extra_specs': {'resources:CUSTOM_MAGIC': 1}})
4099         # save the extra_specs in the flavor stored in the test case as
4100         # well
4101         self.flavor1['extra_specs'] = {'resources:CUSTOM_MAGIC': 1}
4102 
4103 
4104 class ServerBuildAbortTests(integrated_helpers.ProviderUsageBaseTestCase):
4105     """Tests server create scenarios which trigger a build abort during
4106     a server build and validates that allocations in Placement
4107     are properly cleaned up.
4108 
4109     Uses a fake virt driver that aborts the build on the first attempt.
4110     """
4111 
4112     compute_driver = 'fake.FakeBuildAbortDriver'
4113 
4114     def setUp(self):
4115         super(ServerBuildAbortTests, self).setUp()
4116         # We only need one compute service/host/node for these tests.
4117         self.compute1 = self._start_compute(host='host1')
4118 
4119         flavors = self.api.get_flavors()
4120         self.flavor1 = flavors[0]
4121 
4122     def test_abort_when_booting_instance(self):
4123         """Tests that allocations, created by the scheduler, are cleaned
4124         from the source node when the build is aborted on that node.
4125         """
4126         server_req = self._build_minimal_create_server_request(
4127                 self.api, 'some-server', flavor_id=self.flavor1['id'],
4128                 image_uuid='155d900f-4e14-4e4c-a73d-069cbf4541e6',
4129                 networks='none')
4130 
4131         created_server = self.api.post_server({'server': server_req})
4132         self._wait_for_state_change(self.api, created_server, 'ERROR')
4133 
4134         failed_hostname = self.compute1.manager.host
4135 
4136         failed_rp_uuid = self._get_provider_uuid_by_host(failed_hostname)
4137         # Expects no allocation records on the failed host.
4138         self.assertRequestMatchesUsage({'VCPU': 0,
4139                                         'MEMORY_MB': 0,
4140                                         'DISK_GB': 0}, failed_rp_uuid)
4141 
4142 
4143 class ServerBuildAbortTestsWithNestedResourceRequest(ServerBuildAbortTests):
4144     compute_driver = 'fake.FakeBuildAbortDriverWithNestedCustomResources'
4145 
4146     def setUp(self):
4147         super(ServerBuildAbortTestsWithNestedResourceRequest, self).setUp()
4148         # modify the flavor used in the test base class to require one piece of
4149         # CUSTOM_MAGIC resource as well.
4150 
4151         self.api.post_extra_spec(
4152             self.flavor1['id'], {'extra_specs': {'resources:CUSTOM_MAGIC': 1}})
4153         # save the extra_specs in the flavor stored in the test case as
4154         # well
4155         self.flavor1['extra_specs'] = {'resources:CUSTOM_MAGIC': 1}
4156 
4157 
4158 class ServerUnshelveSpawnFailTests(
4159         integrated_helpers.ProviderUsageBaseTestCase):
4160     """Tests server unshelve scenarios which trigger a
4161     VirtualInterfaceCreateException during driver.spawn() and validates that
4162     allocations in Placement are properly cleaned up.
4163     """
4164 
4165     compute_driver = 'fake.FakeUnshelveSpawnFailDriver'
4166 
4167     def setUp(self):
4168         super(ServerUnshelveSpawnFailTests, self).setUp()
4169         # We only need one compute service/host/node for these tests.
4170         self.compute1 = self._start_compute('host1')
4171 
4172         flavors = self.api.get_flavors()
4173         self.flavor1 = flavors[0]
4174 
4175     def test_driver_spawn_fail_when_unshelving_instance(self):
4176         """Tests that allocations, created by the scheduler, are cleaned
4177         from the target node when the unshelve driver.spawn fails on that node.
4178         """
4179         hostname = self.compute1.manager.host
4180         rp_uuid = self._get_provider_uuid_by_host(hostname)
4181         # We start with no usages on the host.
4182         self.assertRequestMatchesUsage(
4183             {'VCPU': 0,
4184              'MEMORY_MB': 0,
4185              'DISK_GB': 0}, rp_uuid)
4186 
4187         server_req = self._build_minimal_create_server_request(
4188             self.api, 'unshelve-spawn-fail', flavor_id=self.flavor1['id'],
4189             image_uuid='155d900f-4e14-4e4c-a73d-069cbf4541e6',
4190             networks='none')
4191 
4192         server = self.api.post_server({'server': server_req})
4193         self._wait_for_state_change(self.api, server, 'ACTIVE')
4194 
4195         # assert allocations exist for the host
4196         self.assertFlavorMatchesUsage(rp_uuid, self.flavor1)
4197 
4198         # shelve offload the server
4199         self.flags(shelved_offload_time=0)
4200         self.api.post_server_action(server['id'], {'shelve': None})
4201         self._wait_for_server_parameter(
4202             self.api, server, {'status': 'SHELVED_OFFLOADED',
4203                                'OS-EXT-SRV-ATTR:host': None})
4204 
4205         # assert allocations were removed from the host
4206         self.assertRequestMatchesUsage(
4207             {'VCPU': 0,
4208              'MEMORY_MB': 0,
4209              'DISK_GB': 0}, rp_uuid)
4210 
4211         # unshelve the server, which should fail
4212         self.api.post_server_action(server['id'], {'unshelve': None})
4213         self._wait_for_action_fail_completion(
4214             server, instance_actions.UNSHELVE, 'compute_unshelve_instance')
4215 
4216         # assert allocations were removed from the host
4217         self.assertRequestMatchesUsage(
4218             {'VCPU': 0,
4219              'MEMORY_MB': 0,
4220              'DISK_GB': 0}, rp_uuid)
4221 
4222 
4223 class ServerUnshelveSpawnFailTestsWithNestedResourceRequest(
4224     ServerUnshelveSpawnFailTests):
4225     compute_driver = ('fake.'
4226                       'FakeUnshelveSpawnFailDriverWithNestedCustomResources')
4227 
4228     def setUp(self):
4229         super(ServerUnshelveSpawnFailTestsWithNestedResourceRequest,
4230               self).setUp()
4231         # modify the flavor used in the test base class to require one piece of
4232         # CUSTOM_MAGIC resource as well.
4233 
4234         self.api.post_extra_spec(
4235             self.flavor1['id'], {'extra_specs': {'resources:CUSTOM_MAGIC': 1}})
4236         # save the extra_specs in the flavor stored in the test case as
4237         # well
4238         self.flavor1['extra_specs'] = {'resources:CUSTOM_MAGIC': 1}
4239 
4240 
4241 class ServerSoftDeleteTests(integrated_helpers.ProviderUsageBaseTestCase):
4242 
4243     compute_driver = 'fake.SmallFakeDriver'
4244 
4245     def setUp(self):
4246         super(ServerSoftDeleteTests, self).setUp()
4247         # We only need one compute service/host/node for these tests.
4248         self.compute1 = self._start_compute('host1')
4249 
4250         flavors = self.api.get_flavors()
4251         self.flavor1 = flavors[0]
4252 
4253     def _soft_delete_and_check_allocation(self, server, hostname):
4254         self.api.delete_server(server['id'])
4255         server = self._wait_for_state_change(self.api, server, 'SOFT_DELETED')
4256 
4257         self._run_periodics()
4258 
4259         # in soft delete state nova should keep the resource allocation as
4260         # the instance can be restored
4261         rp_uuid = self._get_provider_uuid_by_host(hostname)
4262 
4263         self.assertFlavorMatchesUsage(rp_uuid, self.flavor1)
4264 
4265         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
4266                                            rp_uuid)
4267 
4268         # run the periodic reclaim but as time isn't advanced it should not
4269         # reclaim the instance
4270         ctxt = context.get_admin_context()
4271         self.compute1._reclaim_queued_deletes(ctxt)
4272 
4273         self._run_periodics()
4274 
4275         self.assertFlavorMatchesUsage(rp_uuid, self.flavor1)
4276 
4277         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
4278                                            rp_uuid)
4279 
4280     def test_soft_delete_then_reclaim(self):
4281         """Asserts that the automatic reclaim of soft deleted instance cleans
4282         up the allocations in placement.
4283         """
4284 
4285         # make sure that instance will go to SOFT_DELETED state instead of
4286         # deleted immediately
4287         self.flags(reclaim_instance_interval=30)
4288 
4289         hostname = self.compute1.host
4290         rp_uuid = self._get_provider_uuid_by_host(hostname)
4291 
4292         server = self._boot_and_check_allocations(self.flavor1, hostname)
4293 
4294         self._soft_delete_and_check_allocation(server, hostname)
4295 
4296         # advance the time and run periodic reclaim, instance should be deleted
4297         # and resources should be freed
4298         the_past = timeutils.utcnow() + datetime.timedelta(hours=1)
4299         timeutils.set_time_override(override_time=the_past)
4300         self.addCleanup(timeutils.clear_time_override)
4301         ctxt = context.get_admin_context()
4302         self.compute1._reclaim_queued_deletes(ctxt)
4303 
4304         # Wait for real deletion
4305         self._wait_until_deleted(server)
4306 
4307         usages = self._get_provider_usages(rp_uuid)
4308         self.assertEqual({'VCPU': 0,
4309                           'MEMORY_MB': 0,
4310                           'DISK_GB': 0}, usages)
4311         allocations = self._get_allocations_by_server_uuid(server['id'])
4312         self.assertEqual(0, len(allocations))
4313 
4314     def test_soft_delete_then_restore(self):
4315         """Asserts that restoring a soft deleted instance keeps the proper
4316         allocation in placement.
4317         """
4318 
4319         # make sure that instance will go to SOFT_DELETED state instead of
4320         # deleted immediately
4321         self.flags(reclaim_instance_interval=30)
4322 
4323         hostname = self.compute1.host
4324         rp_uuid = self._get_provider_uuid_by_host(hostname)
4325 
4326         server = self._boot_and_check_allocations(
4327             self.flavor1, hostname)
4328 
4329         self._soft_delete_and_check_allocation(server, hostname)
4330 
4331         post = {'restore': {}}
4332         self.api.post_server_action(server['id'], post)
4333         server = self._wait_for_state_change(self.api, server, 'ACTIVE')
4334 
4335         # after restore the allocations should be kept
4336         self.assertFlavorMatchesUsage(rp_uuid, self.flavor1)
4337 
4338         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
4339                                            rp_uuid)
4340 
4341         # Now we want a real delete
4342         self.flags(reclaim_instance_interval=0)
4343         self._delete_and_check_allocations(server)
4344 
4345 
4346 class ServerSoftDeleteTestsWithNestedResourceRequest(ServerSoftDeleteTests):
4347     compute_driver = 'fake.MediumFakeDriverWithNestedCustomResources'
4348 
4349     def setUp(self):
4350         super(ServerSoftDeleteTestsWithNestedResourceRequest, self).setUp()
4351         # modify the flavor used in the test base class to require one piece of
4352         # CUSTOM_MAGIC resource as well.
4353 
4354         self.api.post_extra_spec(
4355             self.flavor1['id'], {'extra_specs': {'resources:CUSTOM_MAGIC': 1}})
4356         # save the extra_specs in the flavor stored in the test case as
4357         # well
4358         self.flavor1['extra_specs'] = {'resources:CUSTOM_MAGIC': 1}
4359 
4360 
4361 class VolumeBackedServerTest(integrated_helpers.ProviderUsageBaseTestCase):
4362     """Tests for volume-backed servers."""
4363 
4364     compute_driver = 'fake.SmallFakeDriver'
4365 
4366     def setUp(self):
4367         super(VolumeBackedServerTest, self).setUp()
4368         self.compute1 = self._start_compute('host1')
4369         self.flavor_id = self._create_flavor()
4370 
4371     def _create_flavor(self):
4372         body = {
4373             'flavor': {
4374                 'id': 'vbst',
4375                 'name': 'special',
4376                 'ram': 512,
4377                 'vcpus': 1,
4378                 'disk': 10,
4379                 'OS-FLV-EXT-DATA:ephemeral': 20,
4380                 'swap': 5 * 1024,
4381                 'rxtx_factor': 1.0,
4382                 'os-flavor-access:is_public': True,
4383             },
4384         }
4385         self.admin_api.post_flavor(body)
4386         return body['flavor']['id']
4387 
4388     def _create_server(self):
4389         with nova.utils.temporary_mutation(self.api, microversion='2.35'):
4390             image_id = self.api.get_images()[0]['id']
4391         server_req = self._build_minimal_create_server_request(
4392             self.api, 'trait-based-server',
4393             image_uuid=image_id,
4394             flavor_id=self.flavor_id, networks='none')
4395         server = self.api.post_server({'server': server_req})
4396         server = self._wait_for_state_change(self.api, server, 'ACTIVE')
4397         return server
4398 
4399     def _create_volume_backed_server(self):
4400         self.useFixture(nova_fixtures.CinderFixtureNewAttachFlow(self))
4401         volume_id = nova_fixtures.CinderFixtureNewAttachFlow.IMAGE_BACKED_VOL
4402         server_req_body = {
4403             # There is no imageRef because this is boot from volume.
4404             'server': {
4405                 'flavorRef': self.flavor_id,
4406                 'name': 'test_volume_backed',
4407                 # We don't care about networking for this test. This
4408                 # requires microversion >= 2.37.
4409                 'networks': 'none',
4410                 'block_device_mapping_v2': [{
4411                     'boot_index': 0,
4412                     'uuid': volume_id,
4413                     'source_type': 'volume',
4414                     'destination_type': 'volume'
4415                 }]
4416             }
4417         }
4418         server = self.api.post_server(server_req_body)
4419         server = self._wait_for_state_change(self.api, server, 'ACTIVE')
4420         return server
4421 
4422     def test_ephemeral_has_disk_allocation(self):
4423         server = self._create_server()
4424         allocs = self._get_allocations_by_server_uuid(server['id'])
4425         resources = list(allocs.values())[0]['resources']
4426         self.assertIn('MEMORY_MB', resources)
4427         # 10gb root, 20gb ephemeral, 5gb swap
4428         expected_usage = 35
4429         self.assertEqual(expected_usage, resources['DISK_GB'])
4430         # Ensure the compute node is reporting the correct disk usage
4431         self.assertEqual(
4432             expected_usage,
4433             self.admin_api.get_hypervisor_stats()['local_gb_used'])
4434 
4435     def test_volume_backed_image_type_filter(self):
4436         # Enable the image type support filter and ensure that a
4437         # non-image-having volume-backed server can still boot
4438         self.flags(query_placement_for_image_type_support=True,
4439                    group='scheduler')
4440         server = self._create_volume_backed_server()
4441         created_server = self.api.get_server(server['id'])
4442         self.assertEqual('ACTIVE', created_server['status'])
4443 
4444     def test_volume_backed_no_disk_allocation(self):
4445         server = self._create_volume_backed_server()
4446         allocs = self._get_allocations_by_server_uuid(server['id'])
4447         resources = list(allocs.values())[0]['resources']
4448         self.assertIn('MEMORY_MB', resources)
4449         # 0gb root, 20gb ephemeral, 5gb swap
4450         expected_usage = 25
4451         self.assertEqual(expected_usage, resources['DISK_GB'])
4452         # Ensure the compute node is reporting the correct disk usage
4453         self.assertEqual(
4454             expected_usage,
4455             self.admin_api.get_hypervisor_stats()['local_gb_used'])
4456 
4457         # Now let's hack the RequestSpec.is_bfv field to mimic migrating an
4458         # old instance created before RequestSpec.is_bfv was set in the API,
4459         # move the instance and verify that the RequestSpec.is_bfv is set
4460         # and the instance still reports the same DISK_GB allocations as during
4461         # the initial create.
4462         ctxt = context.get_admin_context()
4463         reqspec = objects.RequestSpec.get_by_instance_uuid(ctxt, server['id'])
4464         # Make sure it's set.
4465         self.assertTrue(reqspec.is_bfv)
4466         del reqspec.is_bfv
4467         reqspec.save()
4468         reqspec = objects.RequestSpec.get_by_instance_uuid(ctxt, server['id'])
4469         # Make sure it's not set.
4470         self.assertNotIn('is_bfv', reqspec)
4471         # Now migrate the instance to another host and check the request spec
4472         # and allocations after the migration.
4473         self._start_compute('host2')
4474         self.admin_api.post_server_action(server['id'], {'migrate': None})
4475         # Wait for the server to complete the cold migration.
4476         server = self._wait_for_state_change(
4477             self.admin_api, server, 'VERIFY_RESIZE')
4478         self.assertEqual('host2', server['OS-EXT-SRV-ATTR:host'])
4479         # Confirm the cold migration and check usage and the request spec.
4480         self.api.post_server_action(server['id'], {'confirmResize': None})
4481         self._wait_for_state_change(self.api, server, 'ACTIVE')
4482         reqspec = objects.RequestSpec.get_by_instance_uuid(ctxt, server['id'])
4483         # Make sure it's set.
4484         self.assertTrue(reqspec.is_bfv)
4485         allocs = self._get_allocations_by_server_uuid(server['id'])
4486         resources = list(allocs.values())[0]['resources']
4487         self.assertEqual(expected_usage, resources['DISK_GB'])
4488 
4489         # Now shelve and unshelve the server to make sure root_gb DISK_GB
4490         # isn't reported for allocations after we unshelve the server.
4491         fake_notifier.stub_notifier(self)
4492         self.addCleanup(fake_notifier.reset)
4493         self.api.post_server_action(server['id'], {'shelve': None})
4494         self._wait_for_state_change(self.api, server, 'SHELVED_OFFLOADED')
4495         fake_notifier.wait_for_versioned_notifications(
4496                 'instance.shelve_offload.end')
4497         # The server should not have any allocations since it's not currently
4498         # hosted on any compute service.
4499         allocs = self._get_allocations_by_server_uuid(server['id'])
4500         self.assertDictEqual({}, allocs)
4501         # Now unshelve the server and make sure there are still no DISK_GB
4502         # allocations for the root disk.
4503         self.api.post_server_action(server['id'], {'unshelve': None})
4504         self._wait_for_state_change(self.api, server, 'ACTIVE')
4505         allocs = self._get_allocations_by_server_uuid(server['id'])
4506         resources = list(allocs.values())[0]['resources']
4507         self.assertEqual(expected_usage, resources['DISK_GB'])
4508 
4509 
4510 class TraitsBasedSchedulingTest(integrated_helpers.ProviderUsageBaseTestCase):
4511     """Tests for requesting a server with required traits in Placement"""
4512 
4513     compute_driver = 'fake.SmallFakeDriver'
4514 
4515     def setUp(self):
4516         super(TraitsBasedSchedulingTest, self).setUp()
4517         self.compute1 = self._start_compute('host1')
4518         self.compute2 = self._start_compute('host2')
4519         # Using a standard trait from the os-traits library, set a required
4520         # trait extra spec on the flavor.
4521         flavors = self.api.get_flavors()
4522         self.flavor_with_trait = flavors[0]
4523         self.admin_api.post_extra_spec(
4524             self.flavor_with_trait['id'],
4525             {'extra_specs': {'trait:HW_CPU_X86_VMX': 'required'}})
4526         self.flavor_without_trait = flavors[1]
4527         self.flavor_with_forbidden_trait = flavors[2]
4528         self.admin_api.post_extra_spec(
4529             self.flavor_with_forbidden_trait['id'],
4530             {'extra_specs': {'trait:HW_CPU_X86_SGX': 'forbidden'}})
4531 
4532         # Note that we're using v2.35 explicitly as the api returns 404
4533         # starting with 2.36
4534         with nova.utils.temporary_mutation(self.api, microversion='2.35'):
4535             images = self.api.get_images()
4536             self.image_id_with_trait = images[0]['id']
4537             self.api.api_put('/images/%s/metadata' % self.image_id_with_trait,
4538                              {'metadata': {
4539                                  'trait:HW_CPU_X86_SGX': 'required'}})
4540             self.image_id_without_trait = images[1]['id']
4541 
4542     def _create_server_with_traits(self, flavor_id, image_id):
4543         """Create a server with given flavor and image id's
4544         :param flavor_id: the flavor id
4545         :param image_id: the image id
4546         :return: create server response
4547         """
4548 
4549         server_req = self._build_minimal_create_server_request(
4550             self.api, 'trait-based-server',
4551             image_uuid=image_id,
4552             flavor_id=flavor_id, networks='none')
4553         return self.api.post_server({'server': server_req})
4554 
4555     def _create_volume_backed_server_with_traits(self, flavor_id, volume_id):
4556         """Create a server with block device mapping(volume) with the given
4557         flavor and volume id's. Either the flavor or the image backing the
4558         volume is expected to have the traits
4559         :param flavor_id: the flavor id
4560         :param volume_id: the volume id
4561         :return: create server response
4562         """
4563 
4564         server_req_body = {
4565             # There is no imageRef because this is boot from volume.
4566             'server': {
4567                 'flavorRef': flavor_id,
4568                 'name': 'test_image_trait_on_volume_backed',
4569                 # We don't care about networking for this test. This
4570                 # requires microversion >= 2.37.
4571                 'networks': 'none',
4572                 'block_device_mapping_v2': [{
4573                     'boot_index': 0,
4574                     'uuid': volume_id,
4575                     'source_type': 'volume',
4576                     'destination_type': 'volume'
4577                 }]
4578             }
4579         }
4580         server = self.api.post_server(server_req_body)
4581         return server
4582 
4583     def test_flavor_traits_based_scheduling(self):
4584         """Tests that a server create request using a required trait in the
4585         flavor ends up on the single compute node resource provider that also
4586         has that trait in Placement. That test will however pass half of the
4587         times even if the trait is not taken into consideration, so we are
4588         also disabling the compute node that has the required trait and try
4589         again, which should result in a no valid host error.
4590         """
4591 
4592         # Decorate compute1 resource provider with the required trait.
4593         rp_uuid = self._get_provider_uuid_by_host(self.compute1.host)
4594         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_VMX'])
4595 
4596         # Create server using flavor with required trait
4597         server = self._create_server_with_traits(self.flavor_with_trait['id'],
4598                                                  self.image_id_without_trait)
4599         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
4600         # Assert the server ended up on the expected compute host that has
4601         # the required trait.
4602         self.assertEqual(self.compute1.host, server['OS-EXT-SRV-ATTR:host'])
4603 
4604         # Disable the compute node that has the required trait
4605         compute1_service_id = self.admin_api.get_services(
4606             host=self.compute1.host, binary='nova-compute')[0]['id']
4607         self.admin_api.put_service(compute1_service_id, {'status': 'disabled'})
4608 
4609         # Create server using flavor with required trait
4610         server = self._create_server_with_traits(self.flavor_with_trait['id'],
4611                                                  self.image_id_without_trait)
4612 
4613         # The server should go to ERROR state because there is no valid host.
4614         server = self._wait_for_state_change(self.admin_api, server, 'ERROR')
4615         self.assertIsNone(server['OS-EXT-SRV-ATTR:host'])
4616         # Make sure the failure was due to NoValidHost by checking the fault.
4617         self.assertIn('fault', server)
4618         self.assertIn('No valid host', server['fault']['message'])
4619 
4620     def test_flavor_forbidden_traits_based_scheduling(self):
4621         """Tests that a server create request using a forbidden trait in the
4622         flavor ends up on the single compute host that doesn't have that
4623         trait in Placement. That test will however pass half of the times even
4624         if the trait is not taken into consideration, so we are also disabling
4625         the compute node that doesn't have the forbidden trait and try again,
4626         which should result in a no valid host error.
4627         """
4628 
4629         # Decorate compute1 resource provider with forbidden trait
4630         rp_uuid = self._get_provider_uuid_by_host(self.compute1.host)
4631         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_SGX'])
4632 
4633         # Create server using flavor with forbidden trait
4634         server = self._create_server_with_traits(
4635             self.flavor_with_forbidden_trait['id'],
4636             self.image_id_without_trait
4637         )
4638 
4639         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
4640 
4641         # Assert the server ended up on the expected compute host that doesn't
4642         # have the forbidden trait.
4643         self.assertEqual(self.compute2.host, server['OS-EXT-SRV-ATTR:host'])
4644 
4645         # Disable the compute node that doesn't have the forbidden trait
4646         compute2_service_id = self.admin_api.get_services(
4647             host=self.compute2.host, binary='nova-compute')[0]['id']
4648         self.admin_api.put_service(compute2_service_id, {'status': 'disabled'})
4649 
4650         # Create server using flavor with forbidden trait
4651         server = self._create_server_with_traits(
4652             self.flavor_with_forbidden_trait['id'],
4653             self.image_id_without_trait
4654         )
4655 
4656         # The server should go to ERROR state because there is no valid host.
4657         server = self._wait_for_state_change(self.admin_api, server, 'ERROR')
4658         self.assertIsNone(server['OS-EXT-SRV-ATTR:host'])
4659         # Make sure the failure was due to NoValidHost by checking the fault.
4660         self.assertIn('fault', server)
4661         self.assertIn('No valid host', server['fault']['message'])
4662 
4663     def test_image_traits_based_scheduling(self):
4664         """Tests that a server create request using a required trait on image
4665         ends up on the single compute node resource provider that also has that
4666         trait in Placement.
4667         """
4668 
4669         # Decorate compute2 resource provider with image trait.
4670         rp_uuid = self._get_provider_uuid_by_host(self.compute2.host)
4671         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_SGX'])
4672 
4673         # Create server using only image trait
4674         server = self._create_server_with_traits(
4675             self.flavor_without_trait['id'], self.image_id_with_trait)
4676         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
4677         # Assert the server ended up on the expected compute host that has
4678         # the required trait.
4679         self.assertEqual(self.compute2.host, server['OS-EXT-SRV-ATTR:host'])
4680 
4681     def test_flavor_image_traits_based_scheduling(self):
4682         """Tests that a server create request using a required trait on flavor
4683         AND a required trait on the image ends up on the single compute node
4684         resource provider that also has that trait in Placement.
4685         """
4686 
4687         # Decorate compute2 resource provider with both flavor and image trait.
4688         rp_uuid = self._get_provider_uuid_by_host(self.compute2.host)
4689         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_VMX',
4690                                             'HW_CPU_X86_SGX'])
4691 
4692         # Create server using flavor and image trait
4693         server = self._create_server_with_traits(
4694             self.flavor_with_trait['id'], self.image_id_with_trait)
4695         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
4696         # Assert the server ended up on the expected compute host that has
4697         # the required trait.
4698         self.assertEqual(self.compute2.host, server['OS-EXT-SRV-ATTR:host'])
4699 
4700     def test_image_trait_on_volume_backed_instance(self):
4701         """Tests that when trying to launch a volume-backed instance with a
4702         required trait on the image metadata contained within the volume,
4703         the instance ends up on the single compute node resource provider
4704         that also has that trait in Placement.
4705         """
4706         # Decorate compute2 resource provider with volume image metadata trait.
4707         rp_uuid = self._get_provider_uuid_by_host(self.compute2.host)
4708         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_SGX'])
4709 
4710         self.useFixture(nova_fixtures.CinderFixtureNewAttachFlow(self))
4711         # Create our server with a volume containing the image meta data with a
4712         # required trait
4713         server = self._create_volume_backed_server_with_traits(
4714             self.flavor_without_trait['id'],
4715             nova_fixtures.CinderFixtureNewAttachFlow.
4716             IMAGE_WITH_TRAITS_BACKED_VOL)
4717 
4718         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
4719         # Assert the server ended up on the expected compute host that has
4720         # the required trait.
4721         self.assertEqual(self.compute2.host, server['OS-EXT-SRV-ATTR:host'])
4722 
4723     def test_flavor_image_trait_on_volume_backed_instance(self):
4724         """Tests that when trying to launch a volume-backed instance with a
4725         required trait on flavor AND a required trait on the image metadata
4726         contained within the volume, the instance ends up on the single
4727         compute node resource provider that also has those traits in Placement.
4728         """
4729         # Decorate compute2 resource provider with volume image metadata trait.
4730         rp_uuid = self._get_provider_uuid_by_host(self.compute2.host)
4731         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_VMX',
4732                                             'HW_CPU_X86_SGX'])
4733 
4734         self.useFixture(nova_fixtures.CinderFixtureNewAttachFlow(self))
4735         # Create our server with a flavor trait and a volume containing the
4736         # image meta data with a required trait
4737         server = self._create_volume_backed_server_with_traits(
4738             self.flavor_with_trait['id'],
4739             nova_fixtures.CinderFixtureNewAttachFlow.
4740             IMAGE_WITH_TRAITS_BACKED_VOL)
4741 
4742         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
4743         # Assert the server ended up on the expected compute host that has
4744         # the required trait.
4745         self.assertEqual(self.compute2.host, server['OS-EXT-SRV-ATTR:host'])
4746 
4747     def test_flavor_traits_based_scheduling_no_valid_host(self):
4748         """Tests that a server create request using a required trait expressed
4749          in flavor fails to find a valid host since no compute node resource
4750          providers have the trait.
4751         """
4752 
4753         # Decorate compute1 resource provider with the image trait.
4754         rp_uuid = self._get_provider_uuid_by_host(self.compute1.host)
4755         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_SGX'])
4756 
4757         server = self._create_server_with_traits(self.flavor_with_trait['id'],
4758                                                  self.image_id_without_trait)
4759         # The server should go to ERROR state because there is no valid host.
4760         server = self._wait_for_state_change(self.admin_api, server, 'ERROR')
4761         self.assertIsNone(server['OS-EXT-SRV-ATTR:host'])
4762         # Make sure the failure was due to NoValidHost by checking the fault.
4763         self.assertIn('fault', server)
4764         self.assertIn('No valid host', server['fault']['message'])
4765 
4766     def test_image_traits_based_scheduling_no_valid_host(self):
4767         """Tests that a server create request using a required trait expressed
4768          in image fails to find a valid host since no compute node resource
4769          providers have the trait.
4770         """
4771 
4772         # Decorate compute1 resource provider with that flavor trait.
4773         rp_uuid = self._get_provider_uuid_by_host(self.compute1.host)
4774         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_VMX'])
4775 
4776         server = self._create_server_with_traits(
4777             self.flavor_without_trait['id'], self.image_id_with_trait)
4778         # The server should go to ERROR state because there is no valid host.
4779         server = self._wait_for_state_change(self.admin_api, server, 'ERROR')
4780         self.assertIsNone(server['OS-EXT-SRV-ATTR:host'])
4781         # Make sure the failure was due to NoValidHost by checking the fault.
4782         self.assertIn('fault', server)
4783         self.assertIn('No valid host', server['fault']['message'])
4784 
4785     def test_flavor_image_traits_based_scheduling_no_valid_host(self):
4786         """Tests that a server create request using a required trait expressed
4787          in flavor AND a required trait expressed in the image fails to find a
4788          valid host since no compute node resource providers have the trait.
4789         """
4790 
4791         server = self._create_server_with_traits(
4792             self.flavor_with_trait['id'], self.image_id_with_trait)
4793         # The server should go to ERROR state because there is no valid host.
4794         server = self._wait_for_state_change(self.admin_api, server, 'ERROR')
4795         self.assertIsNone(server['OS-EXT-SRV-ATTR:host'])
4796         # Make sure the failure was due to NoValidHost by checking the fault.
4797         self.assertIn('fault', server)
4798         self.assertIn('No valid host', server['fault']['message'])
4799 
4800     def test_image_trait_on_volume_backed_instance_no_valid_host(self):
4801         """Tests that when trying to launch a volume-backed instance with a
4802         required trait on the image metadata contained within the volume
4803         fails to find a valid host since no compute node resource providers
4804         have the trait.
4805         """
4806         self.useFixture(nova_fixtures.CinderFixtureNewAttachFlow(self))
4807         # Create our server with a volume
4808         server = self._create_volume_backed_server_with_traits(
4809             self.flavor_without_trait['id'],
4810             nova_fixtures.CinderFixtureNewAttachFlow.
4811             IMAGE_WITH_TRAITS_BACKED_VOL)
4812 
4813         # The server should go to ERROR state because there is no valid host.
4814         server = self._wait_for_state_change(self.admin_api, server, 'ERROR')
4815         self.assertIsNone(server['OS-EXT-SRV-ATTR:host'])
4816         # Make sure the failure was due to NoValidHost by checking the fault.
4817         self.assertIn('fault', server)
4818         self.assertIn('No valid host', server['fault']['message'])
4819 
4820     def test_rebuild_instance_with_image_traits(self):
4821         """Rebuilds a server with a different image which has traits
4822         associated with it and which will run it through the scheduler to
4823         validate the image is still OK with the compute host that the
4824         instance is running on.
4825          """
4826         # Decorate compute2 resource provider with both flavor and image trait.
4827         rp_uuid = self._get_provider_uuid_by_host(self.compute2.host)
4828         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_VMX',
4829                                             'HW_CPU_X86_SGX'])
4830         # make sure we start with no usage on the compute node
4831         rp_usages = self._get_provider_usages(rp_uuid)
4832         self.assertEqual({'VCPU': 0, 'MEMORY_MB': 0, 'DISK_GB': 0}, rp_usages)
4833 
4834         # create a server without traits on image and with traits on flavour
4835         server = self._create_server_with_traits(
4836             self.flavor_with_trait['id'], self.image_id_without_trait)
4837         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
4838 
4839         # make the compute node full and ensure rebuild still succeed
4840         inv = {"resource_class": "VCPU",
4841                "total": 1}
4842         self._set_inventory(rp_uuid, inv)
4843 
4844         # Now rebuild the server with a different image with traits
4845         rebuild_req_body = {
4846             'rebuild': {
4847                 'imageRef': self.image_id_with_trait
4848             }
4849         }
4850         self.api.api_post('/servers/%s/action' % server['id'],
4851                           rebuild_req_body)
4852         self._wait_for_server_parameter(
4853             self.api, server, {'OS-EXT-STS:task_state': None})
4854 
4855         allocs = self._get_allocations_by_server_uuid(server['id'])
4856         self.assertIn(rp_uuid, allocs)
4857 
4858         # Assert the server ended up on the expected compute host that has
4859         # the required trait.
4860         self.assertEqual(self.compute2.host, server['OS-EXT-SRV-ATTR:host'])
4861 
4862     def test_rebuild_instance_with_image_traits_no_host(self):
4863         """Rebuilding a server with a different image which has required
4864         traits on the image fails to valid the host that this server is
4865         currently running, cause the compute host resource provider is not
4866         associated with similar trait.
4867         """
4868         # Decorate compute2 resource provider with traits on flavor
4869         rp_uuid = self._get_provider_uuid_by_host(self.compute2.host)
4870         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_VMX'])
4871 
4872         # make sure we start with no usage on the compute node
4873         rp_usages = self._get_provider_usages(rp_uuid)
4874         self.assertEqual({'VCPU': 0, 'MEMORY_MB': 0, 'DISK_GB': 0}, rp_usages)
4875 
4876         # create a server without traits on image and with traits on flavour
4877         server = self._create_server_with_traits(
4878             self.flavor_with_trait['id'], self.image_id_without_trait)
4879         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
4880 
4881         # Now rebuild the server with a different image with traits
4882         rebuild_req_body = {
4883             'rebuild': {
4884                 'imageRef': self.image_id_with_trait
4885             }
4886         }
4887 
4888         self.api.api_post('/servers/%s/action' % server['id'],
4889                           rebuild_req_body)
4890         # Look for the failed rebuild action.
4891         self._wait_for_action_fail_completion(
4892             server, instance_actions.REBUILD, 'rebuild_server', self.admin_api)
4893         # Assert the server image_ref was rolled back on failure.
4894         server = self.api.get_server(server['id'])
4895         self.assertEqual(self.image_id_without_trait, server['image']['id'])
4896 
4897         # The server should be in ERROR state
4898         self.assertEqual('ERROR', server['status'])
4899         self.assertEqual("No valid host was found. Image traits cannot be "
4900                          "satisfied by the current resource providers. "
4901                          "Either specify a different image during rebuild "
4902                          "or create a new server with the specified image.",
4903                          server['fault']['message'])
4904 
4905     def test_rebuild_instance_with_image_traits_no_image_change(self):
4906         """Rebuilds a server with a same image which has traits
4907         associated with it and which will run it through the scheduler to
4908         validate the image is still OK with the compute host that the
4909         instance is running on.
4910          """
4911         # Decorate compute2 resource provider with both flavor and image trait.
4912         rp_uuid = self._get_provider_uuid_by_host(self.compute2.host)
4913         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_VMX',
4914                                             'HW_CPU_X86_SGX'])
4915         # make sure we start with no usage on the compute node
4916         rp_usages = self._get_provider_usages(rp_uuid)
4917         self.assertEqual({'VCPU': 0, 'MEMORY_MB': 0, 'DISK_GB': 0},
4918                          rp_usages)
4919 
4920         # create a server with traits in both image and flavour
4921         server = self._create_server_with_traits(
4922             self.flavor_with_trait['id'], self.image_id_with_trait)
4923         server = self._wait_for_state_change(self.admin_api, server,
4924                                              'ACTIVE')
4925 
4926         # Now rebuild the server with a different image with traits
4927         rebuild_req_body = {
4928             'rebuild': {
4929                 'imageRef': self.image_id_with_trait
4930             }
4931         }
4932         self.api.api_post('/servers/%s/action' % server['id'],
4933                           rebuild_req_body)
4934         self._wait_for_server_parameter(
4935             self.api, server, {'OS-EXT-STS:task_state': None})
4936 
4937         allocs = self._get_allocations_by_server_uuid(server['id'])
4938         self.assertIn(rp_uuid, allocs)
4939 
4940         # Assert the server ended up on the expected compute host that has
4941         # the required trait.
4942         self.assertEqual(self.compute2.host,
4943                          server['OS-EXT-SRV-ATTR:host'])
4944 
4945     def test_rebuild_instance_with_image_traits_and_forbidden_flavor_traits(
4946                                                                         self):
4947         """Rebuilding a server with a different image which has required
4948         traits on the image fails to validate image traits because flavor
4949         associated with the current instance has the similar trait that is
4950         forbidden
4951         """
4952         # Decorate compute2 resource provider with traits on flavor
4953         rp_uuid = self._get_provider_uuid_by_host(self.compute2.host)
4954         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_VMX'])
4955 
4956         # make sure we start with no usage on the compute node
4957         rp_usages = self._get_provider_usages(rp_uuid)
4958         self.assertEqual({'VCPU': 0, 'MEMORY_MB': 0, 'DISK_GB': 0}, rp_usages)
4959 
4960         # create a server with forbidden traits on flavor and no triats on
4961         # image
4962         server = self._create_server_with_traits(
4963             self.flavor_with_forbidden_trait['id'],
4964             self.image_id_without_trait)
4965         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
4966 
4967         # Now rebuild the server with a different image with traits
4968         rebuild_req_body = {
4969             'rebuild': {
4970                 'imageRef': self.image_id_with_trait
4971             }
4972         }
4973 
4974         self.api.api_post('/servers/%s/action' % server['id'],
4975                           rebuild_req_body)
4976         # Look for the failed rebuild action.
4977         self._wait_for_action_fail_completion(
4978             server, instance_actions.REBUILD, 'rebuild_server', self.admin_api)
4979         # Assert the server image_ref was rolled back on failure.
4980         server = self.api.get_server(server['id'])
4981         self.assertEqual(self.image_id_without_trait, server['image']['id'])
4982 
4983         # The server should be in ERROR state
4984         self.assertEqual('ERROR', server['status'])
4985         self.assertEqual("No valid host was found. Image traits are part of "
4986                          "forbidden traits in flavor associated with the "
4987                          "server. Either specify a different image during "
4988                          "rebuild or create a new server with the specified "
4989                          "image and a compatible flavor.",
4990                          server['fault']['message'])
4991 
4992 
4993 class ServerTestV256Common(ServersTestBase):
4994     api_major_version = 'v2.1'
4995     microversion = '2.56'
4996     ADMIN_API = True
4997 
4998     def _setup_compute_service(self):
4999         # Set up 3 compute services in the same cell
5000         self.addCleanup(fake.restore_nodes)
5001         for host in ('host1', 'host2', 'host3'):
5002             fake.set_nodes([host])
5003             self.start_service('compute', host=host)
5004 
5005     def _create_server(self, target_host=None):
5006         server = self._build_minimal_create_server_request(
5007             image_uuid='a2459075-d96c-40d5-893e-577ff92e721c')
5008         server.update({'networks': 'auto'})
5009         if target_host is not None:
5010             server['availability_zone'] = 'nova:%s' % target_host
5011         post = {'server': server}
5012         response = self.api.api_post('/servers', post).body
5013         return response['server']
5014 
5015     @staticmethod
5016     def _get_target_and_other_hosts(host):
5017         target_other_hosts = {'host1': ['host2', 'host3'],
5018                               'host2': ['host3', 'host1'],
5019                               'host3': ['host1', 'host2']}
5020         return target_other_hosts[host]
5021 
5022 
5023 class ServerTestV256MultiCellTestCase(ServerTestV256Common):
5024     """Negative test to ensure we fail with ComputeHostNotFound if we try to
5025     target a host in another cell from where the instance lives.
5026     """
5027     NUMBER_OF_CELLS = 2
5028 
5029     def _setup_compute_service(self):
5030         # Set up 2 compute services in different cells
5031         host_to_cell_mappings = {
5032             'host1': 'cell1',
5033             'host2': 'cell2'}
5034         self.addCleanup(fake.restore_nodes)
5035         for host in sorted(host_to_cell_mappings):
5036             fake.set_nodes([host])
5037             self.start_service('compute', host=host,
5038                                cell=host_to_cell_mappings[host])
5039 
5040     def test_migrate_server_to_host_in_different_cell(self):
5041         # We target host1 specifically so that we have a predictable target for
5042         # the cold migration in cell2.
5043         server = self._create_server(target_host='host1')
5044         server = self._wait_for_state_change(server, 'BUILD')
5045 
5046         self.assertEqual('host1', server['OS-EXT-SRV-ATTR:host'])
5047         ex = self.assertRaises(client.OpenStackApiException,
5048                                self.api.post_server_action,
5049                                server['id'],
5050                                {'migrate': {'host': 'host2'}})
5051         # When the API pulls the instance out of cell1, the context is targeted
5052         # to cell1, so when the compute API resize() method attempts to lookup
5053         # the target host in cell1, it will result in a ComputeHostNotFound
5054         # error.
5055         self.assertEqual(400, ex.response.status_code)
5056         self.assertIn('Compute host host2 could not be found',
5057                       six.text_type(ex))
5058 
5059 
5060 class ServerTestV256SingleCellMultiHostTestCase(ServerTestV256Common):
5061     """Happy path test where we create a server on one host, migrate it to
5062     another host of our choosing and ensure it lands there.
5063     """
5064     def test_migrate_server_to_host_in_same_cell(self):
5065         server = self._create_server()
5066         server = self._wait_for_state_change(server, 'BUILD')
5067         source_host = server['OS-EXT-SRV-ATTR:host']
5068         target_host = self._get_target_and_other_hosts(source_host)[0]
5069         self.api.post_server_action(server['id'],
5070                                     {'migrate': {'host': target_host}})
5071         # Assert the server is now on the target host.
5072         server = self.api.get_server(server['id'])
5073         self.assertEqual(target_host, server['OS-EXT-SRV-ATTR:host'])
5074 
5075 
5076 class ServerTestV256RescheduleTestCase(ServerTestV256Common):
5077 
5078     @mock.patch.object(compute_manager.ComputeManager, '_prep_resize',
5079                        side_effect=exception.MigrationError(
5080                            reason='Test Exception'))
5081     def test_migrate_server_not_reschedule(self, mock_prep_resize):
5082         server = self._create_server()
5083         found_server = self._wait_for_state_change(server, 'BUILD')
5084 
5085         target_host, other_host = self._get_target_and_other_hosts(
5086             found_server['OS-EXT-SRV-ATTR:host'])
5087 
5088         self.assertRaises(client.OpenStackApiException,
5089                           self.api.post_server_action,
5090                           server['id'],
5091                           {'migrate': {'host': target_host}})
5092         self.assertEqual(1, mock_prep_resize.call_count)
5093         found_server = self.api.get_server(server['id'])
5094         # Check that rescheduling is not occurred.
5095         self.assertNotEqual(other_host, found_server['OS-EXT-SRV-ATTR:host'])
5096 
5097 
5098 class ConsumerGenerationConflictTest(
5099         integrated_helpers.ProviderUsageBaseTestCase):
5100 
5101     # we need the medium driver to be able to allocate resource not just for
5102     # a single instance
5103     compute_driver = 'fake.MediumFakeDriver'
5104 
5105     def setUp(self):
5106         super(ConsumerGenerationConflictTest, self).setUp()
5107         flavors = self.api.get_flavors()
5108         self.flavor = flavors[0]
5109         self.other_flavor = flavors[1]
5110         self.compute1 = self._start_compute('compute1')
5111         self.compute2 = self._start_compute('compute2')
5112 
5113     def test_create_server_fails_as_placement_reports_consumer_conflict(self):
5114         server_req = self._build_minimal_create_server_request(
5115             self.api, 'some-server', flavor_id=self.flavor['id'],
5116             image_uuid='155d900f-4e14-4e4c-a73d-069cbf4541e6',
5117             networks='none')
5118 
5119         # We cannot pre-create a consumer with the uuid of the instance created
5120         # below as that uuid is generated. Instead we have to simulate that
5121         # Placement returns 409, consumer generation conflict for the PUT
5122         # /allocation request the scheduler does for the instance.
5123         with mock.patch('keystoneauth1.adapter.Adapter.put') as mock_put:
5124             rsp = fake_requests.FakeResponse(
5125                 409,
5126                 jsonutils.dumps(
5127                     {'errors': [
5128                         {'code': 'placement.concurrent_update',
5129                          'detail': 'consumer generation conflict'}]}))
5130             mock_put.return_value = rsp
5131 
5132             created_server = self.api.post_server({'server': server_req})
5133             server = self._wait_for_state_change(
5134                 self.admin_api, created_server, 'ERROR')
5135 
5136         # This is not a conflict that the API user can ever resolve. It is a
5137         # serious inconsistency in our database or a bug in the scheduler code
5138         # doing the claim.
5139         self.assertEqual(500, server['fault']['code'])
5140         self.assertIn('Failed to update allocations for consumer',
5141                       server['fault']['message'])
5142 
5143         allocations = self._get_allocations_by_server_uuid(server['id'])
5144         self.assertEqual(0, len(allocations))
5145 
5146         self._delete_and_check_allocations(server)
5147 
5148     def test_migrate_claim_on_dest_fails(self):
5149         source_hostname = self.compute1.host
5150         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
5151 
5152         server = self._boot_and_check_allocations(self.flavor, source_hostname)
5153 
5154         # We have to simulate that Placement returns 409, consumer generation
5155         # conflict for the PUT /allocation request the scheduler does on the
5156         # destination host for the instance.
5157         with mock.patch('keystoneauth1.adapter.Adapter.put') as mock_put:
5158             rsp = fake_requests.FakeResponse(
5159                 409,
5160                 jsonutils.dumps(
5161                     {'errors': [
5162                         {'code': 'placement.concurrent_update',
5163                          'detail': 'consumer generation conflict'}]}))
5164             mock_put.return_value = rsp
5165 
5166             request = {'migrate': None}
5167             exception = self.assertRaises(client.OpenStackApiException,
5168                                           self.api.post_server_action,
5169                                           server['id'], request)
5170 
5171         # I know that HTTP 500 is harsh code but I think this conflict case
5172         # signals either a serious db inconsistency or a bug in nova's
5173         # claim code.
5174         self.assertEqual(500, exception.response.status_code)
5175 
5176         # The migration is aborted so the instance is ACTIVE on the source
5177         # host instead of being in VERIFY_RESIZE state.
5178         server = self.api.get_server(server['id'])
5179         self.assertEqual('ACTIVE', server['status'])
5180         self.assertEqual(source_hostname, server['OS-EXT-SRV-ATTR:host'])
5181 
5182         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor)
5183 
5184         self.assertFlavorMatchesAllocation(self.flavor, server['id'],
5185                                            source_rp_uuid)
5186 
5187         self._delete_and_check_allocations(server)
5188 
5189     def test_migrate_move_allocation_fails_due_to_conflict(self):
5190         source_hostname = self.compute1.host
5191         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
5192 
5193         server = self._boot_and_check_allocations(self.flavor, source_hostname)
5194 
5195         rsp = fake_requests.FakeResponse(
5196             409,
5197             jsonutils.dumps(
5198                 {'errors': [
5199                     {'code': 'placement.concurrent_update',
5200                      'detail': 'consumer generation conflict'}]}))
5201 
5202         with mock.patch('keystoneauth1.adapter.Adapter.post',
5203                         autospec=True) as mock_post:
5204             mock_post.return_value = rsp
5205 
5206             request = {'migrate': None}
5207             exception = self.assertRaises(client.OpenStackApiException,
5208                                           self.api.post_server_action,
5209                                           server['id'], request)
5210 
5211         self.assertEqual(1, mock_post.call_count)
5212 
5213         self.assertEqual(409, exception.response.status_code)
5214         self.assertIn('Failed to move allocations', exception.response.text)
5215 
5216         migrations = self.api.get_migrations()
5217         self.assertEqual(1, len(migrations))
5218         self.assertEqual('migration', migrations[0]['migration_type'])
5219         self.assertEqual(server['id'], migrations[0]['instance_uuid'])
5220         self.assertEqual(source_hostname, migrations[0]['source_compute'])
5221         self.assertEqual('error', migrations[0]['status'])
5222 
5223         # The migration is aborted so the instance is ACTIVE on the source
5224         # host instead of being in VERIFY_RESIZE state.
5225         server = self.api.get_server(server['id'])
5226         self.assertEqual('ACTIVE', server['status'])
5227         self.assertEqual(source_hostname, server['OS-EXT-SRV-ATTR:host'])
5228 
5229         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor)
5230 
5231         self.assertFlavorMatchesAllocation(self.flavor, server['id'],
5232                                            source_rp_uuid)
5233 
5234         self._delete_and_check_allocations(server)
5235 
5236     def test_confirm_migrate_delete_alloc_on_source_fails(self):
5237         source_hostname = self.compute1.host
5238         dest_hostname = self.compute2.host
5239         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
5240         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
5241 
5242         server = self._boot_and_check_allocations(self.flavor, source_hostname)
5243         self._migrate_and_check_allocations(
5244             server, self.flavor, source_rp_uuid, dest_rp_uuid)
5245 
5246         rsp = fake_requests.FakeResponse(
5247             409,
5248             jsonutils.dumps(
5249                 {'errors': [
5250                     {'code': 'placement.concurrent_update',
5251                      'detail': 'consumer generation conflict'}]}))
5252 
5253         with mock.patch('keystoneauth1.adapter.Adapter.put',
5254                         autospec=True) as mock_put:
5255             mock_put.return_value = rsp
5256 
5257             post = {'confirmResize': None}
5258             self.api.post_server_action(
5259                 server['id'], post, check_response_status=[204])
5260             server = self._wait_for_state_change(self.api, server, 'ERROR')
5261             self.assertIn('Failed to delete allocations',
5262                           server['fault']['message'])
5263 
5264         self.assertEqual(1, mock_put.call_count)
5265 
5266         migrations = self.api.get_migrations()
5267         self.assertEqual(1, len(migrations))
5268         self.assertEqual('migration', migrations[0]['migration_type'])
5269         self.assertEqual(server['id'], migrations[0]['instance_uuid'])
5270         self.assertEqual(source_hostname, migrations[0]['source_compute'])
5271         self.assertEqual('error', migrations[0]['status'])
5272 
5273         # NOTE(gibi): Nova leaks the allocation held by the migration_uuid even
5274         # after the instance is deleted. At least nova logs a fat ERROR.
5275         self.assertIn('Deleting allocation in placement for migration %s '
5276                       'failed. The instance %s will be put to ERROR state but '
5277                       'the allocation held by the migration is leaked.' %
5278                       (migrations[0]['uuid'], server['id']),
5279                       self.stdlog.logger.output)
5280         self.api.delete_server(server['id'])
5281         self._wait_until_deleted(server)
5282         fake_notifier.wait_for_versioned_notifications('instance.delete.end')
5283 
5284         allocations = self._get_allocations_by_server_uuid(
5285             migrations[0]['uuid'])
5286         self.assertEqual(1, len(allocations))
5287 
5288     def test_revert_migrate_delete_dest_allocation_fails_due_to_conflict(self):
5289         source_hostname = self.compute1.host
5290         dest_hostname = self.compute2.host
5291         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
5292         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
5293 
5294         server = self._boot_and_check_allocations(self.flavor, source_hostname)
5295         self._migrate_and_check_allocations(
5296             server, self.flavor, source_rp_uuid, dest_rp_uuid)
5297 
5298         rsp = fake_requests.FakeResponse(
5299             409,
5300             jsonutils.dumps(
5301                 {'errors': [
5302                     {'code': 'placement.concurrent_update',
5303                      'detail': 'consumer generation conflict'}]}))
5304 
5305         with mock.patch('keystoneauth1.adapter.Adapter.post',
5306                         autospec=True) as mock_post:
5307             mock_post.return_value = rsp
5308 
5309             post = {'revertResize': None}
5310             self.api.post_server_action(server['id'], post)
5311             server = self._wait_for_state_change(self.api, server, 'ERROR')
5312 
5313         self.assertEqual(1, mock_post.call_count)
5314 
5315         migrations = self.api.get_migrations()
5316         self.assertEqual(1, len(migrations))
5317         self.assertEqual('migration', migrations[0]['migration_type'])
5318         self.assertEqual(server['id'], migrations[0]['instance_uuid'])
5319         self.assertEqual(source_hostname, migrations[0]['source_compute'])
5320         self.assertEqual('error', migrations[0]['status'])
5321 
5322         # NOTE(gibi): Nova leaks the allocation held by the migration_uuid even
5323         # after the instance is deleted. At least nova logs a fat ERROR.
5324         self.assertIn('Reverting allocation in placement for migration %s '
5325                       'failed. The instance %s will be put into ERROR state '
5326                       'but the allocation held by the migration is leaked.' %
5327                       (migrations[0]['uuid'], server['id']),
5328                       self.stdlog.logger.output)
5329         self.api.delete_server(server['id'])
5330         self._wait_until_deleted(server)
5331         fake_notifier.wait_for_versioned_notifications('instance.delete.end')
5332 
5333         allocations = self._get_allocations_by_server_uuid(
5334             migrations[0]['uuid'])
5335         self.assertEqual(1, len(allocations))
5336 
5337     def test_revert_resize_same_host_delete_dest_fails_due_to_conflict(self):
5338         # make sure that the test only uses a single host
5339         compute2_service_id = self.admin_api.get_services(
5340             host=self.compute2.host, binary='nova-compute')[0]['id']
5341         self.admin_api.put_service(compute2_service_id, {'status': 'disabled'})
5342 
5343         hostname = self.compute1.manager.host
5344         rp_uuid = self._get_provider_uuid_by_host(hostname)
5345 
5346         server = self._boot_and_check_allocations(self.flavor, hostname)
5347 
5348         self._resize_to_same_host_and_check_allocations(
5349             server, self.flavor, self.other_flavor, rp_uuid)
5350 
5351         rsp = fake_requests.FakeResponse(
5352             409,
5353             jsonutils.dumps(
5354                 {'errors': [
5355                     {'code': 'placement.concurrent_update',
5356                      'detail': 'consumer generation conflict'}]}))
5357         with mock.patch('keystoneauth1.adapter.Adapter.post',
5358                         autospec=True) as mock_post:
5359             mock_post.return_value = rsp
5360 
5361             post = {'revertResize': None}
5362             self.api.post_server_action(server['id'], post)
5363             server = self._wait_for_state_change(self.api, server, 'ERROR',)
5364 
5365         self.assertEqual(1, mock_post.call_count)
5366 
5367         migrations = self.api.get_migrations()
5368         self.assertEqual(1, len(migrations))
5369         self.assertEqual('resize', migrations[0]['migration_type'])
5370         self.assertEqual(server['id'], migrations[0]['instance_uuid'])
5371         self.assertEqual(hostname, migrations[0]['source_compute'])
5372         self.assertEqual('error', migrations[0]['status'])
5373 
5374         # NOTE(gibi): Nova leaks the allocation held by the migration_uuid even
5375         # after the instance is deleted. At least nova logs a fat ERROR.
5376         self.assertIn('Reverting allocation in placement for migration %s '
5377                       'failed. The instance %s will be put into ERROR state '
5378                       'but the allocation held by the migration is leaked.' %
5379                       (migrations[0]['uuid'], server['id']),
5380                       self.stdlog.logger.output)
5381         self.api.delete_server(server['id'])
5382         self._wait_until_deleted(server)
5383         fake_notifier.wait_for_versioned_notifications('instance.delete.end')
5384 
5385         allocations = self._get_allocations_by_server_uuid(
5386             migrations[0]['uuid'])
5387         self.assertEqual(1, len(allocations))
5388 
5389     def test_force_live_migrate_claim_on_dest_fails(self):
5390         # Normal live migrate moves source allocation from instance to
5391         # migration like a normal migrate tested above.
5392         # Normal live migrate claims on dest like a normal boot tested above.
5393         source_hostname = self.compute1.host
5394         dest_hostname = self.compute2.host
5395 
5396         # the ability to force live migrate a server is removed entirely in
5397         # 2.68
5398         self.api.microversion = '2.67'
5399 
5400         server = self._boot_and_check_allocations(
5401             self.flavor, source_hostname)
5402 
5403         rsp = fake_requests.FakeResponse(
5404             409,
5405             jsonutils.dumps(
5406                 {'errors': [
5407                     {'code': 'placement.concurrent_update',
5408                      'detail': 'consumer generation conflict'}]}))
5409         with mock.patch('keystoneauth1.adapter.Adapter.put',
5410                         autospec=True) as mock_put:
5411             mock_put.return_value = rsp
5412 
5413             post = {
5414                 'os-migrateLive': {
5415                     'host': dest_hostname,
5416                     'block_migration': True,
5417                     'force': True,
5418                 }
5419             }
5420 
5421             self.api.post_server_action(server['id'], post)
5422             server = self._wait_for_state_change(self.api, server, 'ERROR')
5423 
5424         self.assertEqual(1, mock_put.call_count)
5425 
5426         # This is not a conflict that the API user can ever resolve. It is a
5427         # serious inconsistency in our database or a bug in the scheduler code
5428         # doing the claim.
5429         self.assertEqual(500, server['fault']['code'])
5430         # The instance is in ERROR state so the allocations are in limbo but
5431         # at least we expect that when the instance is deleted the allocations
5432         # are cleaned up properly.
5433         self._delete_and_check_allocations(server)
5434 
5435     def test_live_migrate_drop_allocation_on_source_fails(self):
5436         source_hostname = self.compute1.host
5437         dest_hostname = self.compute2.host
5438         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
5439         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
5440 
5441         # the ability to force live migrate a server is removed entirely in
5442         # 2.68
5443         self.api.microversion = '2.67'
5444 
5445         server = self._boot_and_check_allocations(
5446             self.flavor, source_hostname)
5447 
5448         fake_notifier.stub_notifier(self)
5449         self.addCleanup(fake_notifier.reset)
5450 
5451         orig_put = adapter.Adapter.put
5452 
5453         rsp = fake_requests.FakeResponse(
5454             409,
5455             jsonutils.dumps(
5456                 {'errors': [
5457                     {'code': 'placement.concurrent_update',
5458                      'detail': 'consumer generation conflict'}]}))
5459 
5460         def fake_put(_self, url, *args, **kwargs):
5461             migration_uuid = self.get_migration_uuid_for_instance(server['id'])
5462             if url == '/allocations/%s' % migration_uuid:
5463                 return rsp
5464             else:
5465                 return orig_put(_self, url, *args, **kwargs)
5466 
5467         with mock.patch('keystoneauth1.adapter.Adapter.put',
5468                         autospec=True) as mock_put:
5469             mock_put.side_effect = fake_put
5470 
5471             post = {
5472                 'os-migrateLive': {
5473                     'host': dest_hostname,
5474                     'block_migration': True,
5475                     'force': True,
5476                 }
5477             }
5478 
5479             self.api.post_server_action(server['id'], post)
5480 
5481             # nova does the source host cleanup _after_ setting the migration
5482             # to completed and sending end notifications so we have to wait
5483             # here a bit.
5484             time.sleep(1)
5485 
5486             # Nova failed to clean up on the source host. This right now puts
5487             # the instance to ERROR state and fails the migration.
5488             server = self._wait_for_server_parameter(self.api, server,
5489                 {'OS-EXT-SRV-ATTR:host': dest_hostname,
5490                  'status': 'ERROR'})
5491             self._wait_for_migration_status(server, ['error'])
5492             fake_notifier.wait_for_versioned_notifications(
5493                 'instance.live_migration_post.end')
5494 
5495         # 1 claim on destination, 1 normal delete on dest that fails,
5496         self.assertEqual(2, mock_put.call_count)
5497 
5498         # As the cleanup on the source host failed Nova leaks the allocation
5499         # held by the migration.
5500         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor)
5501         migration_uuid = self.get_migration_uuid_for_instance(server['id'])
5502         self.assertFlavorMatchesAllocation(self.flavor, migration_uuid,
5503                                            source_rp_uuid)
5504 
5505         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor)
5506 
5507         self.assertFlavorMatchesAllocation(self.flavor, server['id'],
5508                                            dest_rp_uuid)
5509 
5510         # NOTE(gibi): Nova leaks the allocation held by the migration_uuid even
5511         # after the instance is deleted. At least nova logs a fat ERROR.
5512         self.assertIn('Deleting allocation in placement for migration %s '
5513                       'failed. The instance %s will be put to ERROR state but '
5514                       'the allocation held by the migration is leaked.' %
5515                       (migration_uuid, server['id']),
5516                       self.stdlog.logger.output)
5517 
5518         self.api.delete_server(server['id'])
5519         self._wait_until_deleted(server)
5520         fake_notifier.wait_for_versioned_notifications('instance.delete.end')
5521 
5522         self.assertFlavorMatchesAllocation(self.flavor, migration_uuid,
5523                                            source_rp_uuid)
5524 
5525     def _test_evacuate_fails_allocating_on_dest_host(self, force):
5526         source_hostname = self.compute1.host
5527         dest_hostname = self.compute2.host
5528 
5529         # the ability to force evacuate a server is removed entirely in 2.68
5530         self.api.microversion = '2.67'
5531 
5532         server = self._boot_and_check_allocations(
5533             self.flavor, source_hostname)
5534 
5535         source_compute_id = self.admin_api.get_services(
5536             host=source_hostname, binary='nova-compute')[0]['id']
5537 
5538         self.compute1.stop()
5539         # force it down to avoid waiting for the service group to time out
5540         self.admin_api.put_service(
5541             source_compute_id, {'forced_down': 'true'})
5542 
5543         rsp = fake_requests.FakeResponse(
5544             409,
5545             jsonutils.dumps(
5546                 {'errors': [
5547                     {'code': 'placement.concurrent_update',
5548                      'detail': 'consumer generation conflict'}]}))
5549 
5550         with mock.patch('keystoneauth1.adapter.Adapter.put',
5551                         autospec=True) as mock_put:
5552             mock_put.return_value = rsp
5553             post = {
5554                 'evacuate': {
5555                     'force': force
5556                 }
5557             }
5558             if force:
5559                 post['evacuate']['host'] = dest_hostname
5560 
5561             self.api.post_server_action(server['id'], post)
5562             server = self._wait_for_state_change(self.api, server, 'ERROR')
5563 
5564         self.assertEqual(1, mock_put.call_count)
5565 
5566         # As nova failed to allocate on the dest host we only expect allocation
5567         # on the source
5568         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
5569         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
5570 
5571         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor)
5572 
5573         self.assertRequestMatchesUsage({'VCPU': 0,
5574                                         'MEMORY_MB': 0,
5575                                         'DISK_GB': 0}, dest_rp_uuid)
5576 
5577         self.assertFlavorMatchesAllocation(self.flavor, server['id'],
5578                                            source_rp_uuid)
5579 
5580         self._delete_and_check_allocations(server)
5581 
5582     def test_force_evacuate_fails_allocating_on_dest_host(self):
5583         self._test_evacuate_fails_allocating_on_dest_host(force=True)
5584 
5585     def test_evacuate_fails_allocating_on_dest_host(self):
5586         self._test_evacuate_fails_allocating_on_dest_host(force=False)
5587 
5588     def test_server_delete_fails_due_to_conflict(self):
5589         source_hostname = self.compute1.host
5590 
5591         server = self._boot_and_check_allocations(self.flavor, source_hostname)
5592 
5593         rsp = fake_requests.FakeResponse(
5594             409, jsonutils.dumps({'text': 'consumer generation conflict'}))
5595 
5596         with mock.patch('keystoneauth1.adapter.Adapter.put',
5597                         autospec=True) as mock_put:
5598             mock_put.return_value = rsp
5599 
5600             self.api.delete_server(server['id'])
5601             server = self._wait_for_state_change(self.admin_api, server,
5602                                                  'ERROR')
5603             self.assertEqual(1, mock_put.call_count)
5604 
5605         # We still have the allocations as deletion failed
5606         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
5607         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor)
5608 
5609         self.assertFlavorMatchesAllocation(self.flavor, server['id'],
5610                                            source_rp_uuid)
5611 
5612         # retry the delete to make sure that allocations are removed this time
5613         self._delete_and_check_allocations(server)
5614 
5615     def test_server_local_delete_fails_due_to_conflict(self):
5616         source_hostname = self.compute1.host
5617 
5618         server = self._boot_and_check_allocations(self.flavor, source_hostname)
5619         source_compute_id = self.admin_api.get_services(
5620             host=self.compute1.host, binary='nova-compute')[0]['id']
5621         self.compute1.stop()
5622         self.admin_api.put_service(
5623             source_compute_id, {'forced_down': 'true'})
5624 
5625         rsp = fake_requests.FakeResponse(
5626             409, jsonutils.dumps({'text': 'consumer generation conflict'}))
5627 
5628         with mock.patch('keystoneauth1.adapter.Adapter.put',
5629                         autospec=True) as mock_put:
5630             mock_put.return_value = rsp
5631 
5632             ex = self.assertRaises(client.OpenStackApiException,
5633                                    self.api.delete_server, server['id'])
5634             self.assertEqual(409, ex.response.status_code)
5635             self.assertIn('Failed to delete allocations for consumer',
5636                           jsonutils.loads(ex.response.content)[
5637                               'conflictingRequest']['message'])
5638             self.assertEqual(1, mock_put.call_count)
5639 
5640         # We still have the allocations as deletion failed
5641         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
5642         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor)
5643 
5644         self.assertFlavorMatchesAllocation(self.flavor, server['id'],
5645                                            source_rp_uuid)
5646 
5647         # retry the delete to make sure that allocations are removed this time
5648         self._delete_and_check_allocations(server)
5649 
5650 
5651 class ServerMovingTestsWithNestedComputes(ServerMovingTests):
5652     """Runs all the server moving tests while the computes have nested trees.
5653     The servers still do not request resources from any child provider though.
5654     """
5655     compute_driver = 'fake.MediumFakeDriverWithNestedCustomResources'
5656 
5657 
5658 class ServerMovingTestsWithNestedResourceRequests(
5659     ServerMovingTestsWithNestedComputes):
5660     """Runs all the server moving tests while the computes have nested trees.
5661     The servers also request resources from child providers.
5662     """
5663 
5664     def setUp(self):
5665         super(ServerMovingTestsWithNestedResourceRequests, self).setUp()
5666         # modify the flavors used in the ServerMoving test base class to
5667         # require one piece of CUSTOM_MAGIC resource as well.
5668 
5669         for flavor in [self.flavor1, self.flavor2, self.flavor3]:
5670             self.api.post_extra_spec(
5671                 flavor['id'], {'extra_specs': {'resources:CUSTOM_MAGIC': 1}})
5672             # save the extra_specs in the flavor stored in the test case as
5673             # well
5674             flavor['extra_specs'] = {'resources:CUSTOM_MAGIC': 1}
5675 
5676     def _check_allocation_during_evacuate(
5677             self, flavor, server_uuid, source_root_rp_uuid, dest_root_rp_uuid):
5678         # NOTE(gibi): evacuate is the only case when the same consumer has
5679         # allocation from two different RP trees so we need a special check
5680         # here.
5681         allocations = self._get_allocations_by_server_uuid(server_uuid)
5682         source_rps = self._get_all_rp_uuids_in_a_tree(source_root_rp_uuid)
5683         dest_rps = self._get_all_rp_uuids_in_a_tree(dest_root_rp_uuid)
5684 
5685         self.assertEqual(set(source_rps + dest_rps), set(allocations))
5686 
5687         total_source_allocation = collections.defaultdict(int)
5688         total_dest_allocation = collections.defaultdict(int)
5689         for rp, alloc in allocations.items():
5690             for rc, value in alloc['resources'].items():
5691                 if rp in source_rps:
5692                     total_source_allocation[rc] += value
5693                 else:
5694                     total_dest_allocation[rc] += value
5695 
5696         self.assertEqual(
5697             self._resources_from_flavor(flavor), total_source_allocation)
5698         self.assertEqual(
5699             self._resources_from_flavor(flavor), total_dest_allocation)
5700 
5701     def test_live_migrate_force(self):
5702         # Nova intentionally does not support force live-migrating server
5703         # with nested allocations.
5704 
5705         source_hostname = self.compute1.host
5706         dest_hostname = self.compute2.host
5707         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
5708         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
5709 
5710         # the ability to force live migrate a server is removed entirely in
5711         # 2.68
5712         self.api.microversion = '2.67'
5713 
5714         server = self._boot_and_check_allocations(
5715             self.flavor1, source_hostname)
5716         post = {
5717             'os-migrateLive': {
5718                 'host': dest_hostname,
5719                 'block_migration': True,
5720                 'force': True,
5721             }
5722         }
5723 
5724         self.api.post_server_action(server['id'], post)
5725         self._wait_for_migration_status(server, ['error'])
5726         self._wait_for_server_parameter(self.api, server,
5727             {'OS-EXT-SRV-ATTR:host': source_hostname,
5728              'status': 'ACTIVE'})
5729         self.assertIn('Unable to move instance %s to host host2. The instance '
5730                       'has complex allocations on the source host so move '
5731                       'cannot be forced.' %
5732                       server['id'],
5733                       self.stdlog.logger.output)
5734 
5735         self._run_periodics()
5736 
5737         # NOTE(danms): There should be no usage for the dest
5738         self.assertRequestMatchesUsage(
5739             {'VCPU': 0,
5740              'MEMORY_MB': 0,
5741              'DISK_GB': 0}, dest_rp_uuid)
5742 
5743         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
5744 
5745         # the server has an allocation on only the source node
5746         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
5747                                            source_rp_uuid)
5748 
5749         self._delete_and_check_allocations(server)
5750 
5751     def test_evacuate_forced_host(self):
5752         # Nova intentionally does not support force evacuating server
5753         # with nested allocations.
5754 
5755         source_hostname = self.compute1.host
5756         dest_hostname = self.compute2.host
5757 
5758         # the ability to force evacuate a server is removed entirely in 2.68
5759         self.api.microversion = '2.67'
5760 
5761         server = self._boot_and_check_allocations(
5762             self.flavor1, source_hostname)
5763 
5764         source_compute_id = self.admin_api.get_services(
5765             host=source_hostname, binary='nova-compute')[0]['id']
5766 
5767         self.compute1.stop()
5768         # force it down to avoid waiting for the service group to time out
5769         self.admin_api.put_service(
5770             source_compute_id, {'forced_down': 'true'})
5771 
5772         # evacuate the server and force the destination host which bypasses
5773         # the scheduler
5774         post = {
5775             'evacuate': {
5776                 'host': dest_hostname,
5777                 'force': True
5778             }
5779         }
5780         self.api.post_server_action(server['id'], post)
5781         self._wait_for_migration_status(server, ['error'])
5782         expected_params = {'OS-EXT-SRV-ATTR:host': source_hostname,
5783                            'status': 'ACTIVE'}
5784         server = self._wait_for_server_parameter(self.api, server,
5785                                                  expected_params)
5786         self.assertIn('Unable to move instance %s to host host2. The instance '
5787                       'has complex allocations on the source host so move '
5788                       'cannot be forced.' %
5789                       server['id'],
5790                       self.stdlog.logger.output)
5791 
5792         # Run the periodics to show those don't modify allocations.
5793         self._run_periodics()
5794 
5795         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
5796         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
5797 
5798         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
5799 
5800         self.assertRequestMatchesUsage(
5801             {'VCPU': 0,
5802              'MEMORY_MB': 0,
5803              'DISK_GB': 0}, dest_rp_uuid)
5804 
5805         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
5806                                            source_rp_uuid)
5807 
5808         # restart the source compute
5809         self.restart_compute_service(self.compute1)
5810         self.admin_api.put_service(
5811             source_compute_id, {'forced_down': 'false'})
5812 
5813         # Run the periodics again to show they don't change anything.
5814         self._run_periodics()
5815 
5816         # When the source node starts up nothing should change as the
5817         # evacuation failed
5818         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
5819 
5820         self.assertRequestMatchesUsage(
5821             {'VCPU': 0,
5822              'MEMORY_MB': 0,
5823              'DISK_GB': 0}, dest_rp_uuid)
5824 
5825         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
5826                                            source_rp_uuid)
5827 
5828         self._delete_and_check_allocations(server)
5829 
5830 
5831 # NOTE(gibi): There is another case NestedToFlat but that leads to the same
5832 # code path that NestedToNested as in both cases the instance will have
5833 # complex allocation on the source host which is already covered in
5834 # ServerMovingTestsWithNestedResourceRequests
5835 class ServerMovingTestsFromFlatToNested(
5836         integrated_helpers.ProviderUsageBaseTestCase):
5837     """Tests trying to move servers from a compute with a flat RP tree to a
5838     compute with a nested RP tree and assert that the blind allocation copy
5839     fails cleanly.
5840     """
5841 
5842     REQUIRES_LOCKING = True
5843     compute_driver = 'fake.MediumFakeDriver'
5844 
5845     def setUp(self):
5846         super(ServerMovingTestsFromFlatToNested, self).setUp()
5847         flavors = self.api.get_flavors()
5848         self.flavor1 = flavors[0]
5849         self.api.post_extra_spec(
5850             self.flavor1['id'], {'extra_specs': {'resources:CUSTOM_MAGIC': 1}})
5851         self.flavor1['extra_specs'] = {'resources:CUSTOM_MAGIC': 1}
5852 
5853     def test_force_live_migrate_from_flat_to_nested(self):
5854         # first compute will start with the flat RP tree but we add
5855         # CUSTOM_MAGIC inventory to the root compute RP
5856         orig_update_provider_tree = fake.MediumFakeDriver.update_provider_tree
5857 
5858         # the ability to force live migrate a server is removed entirely in
5859         # 2.68
5860         self.api.microversion = '2.67'
5861 
5862         def stub_update_provider_tree(self, provider_tree, nodename,
5863                                       allocations=None):
5864             # do the regular inventory update
5865             orig_update_provider_tree(
5866                 self, provider_tree, nodename, allocations)
5867             if nodename == 'host1':
5868                 # add the extra resource
5869                 inv = provider_tree.data(nodename).inventory
5870                 inv['CUSTOM_MAGIC'] = {
5871                     'total': 10,
5872                     'reserved': 0,
5873                     'min_unit': 1,
5874                     'max_unit': 10,
5875                     'step_size': 1,
5876                     'allocation_ratio': 1,
5877                 }
5878                 provider_tree.update_inventory(nodename, inv)
5879 
5880         self.stub_out('nova.virt.fake.FakeDriver.update_provider_tree',
5881                       stub_update_provider_tree)
5882         self.compute1 = self._start_compute(host='host1')
5883         source_rp_uuid = self._get_provider_uuid_by_host('host1')
5884 
5885         server = self._boot_and_check_allocations(self.flavor1, 'host1')
5886         # start the second compute with nested RP tree
5887         self.flags(
5888             compute_driver='fake.MediumFakeDriverWithNestedCustomResources')
5889         self.compute2 = self._start_compute(host='host2')
5890 
5891         # try to force live migrate from flat to nested.
5892         post = {
5893             'os-migrateLive': {
5894                 'host': 'host2',
5895                 'block_migration': True,
5896                 'force': True,
5897             }
5898         }
5899 
5900         self.api.post_server_action(server['id'], post)
5901         # We expect that the migration will fail as force migrate tries to
5902         # blindly copy the source allocation to the destination but on the
5903         # destination there is no inventory of CUSTOM_MAGIC on the compute node
5904         # provider as that resource is reported on a child provider.
5905         self._wait_for_server_parameter(self.api, server,
5906             {'OS-EXT-SRV-ATTR:host': 'host1',
5907              'status': 'ACTIVE'})
5908 
5909         migration = self._wait_for_migration_status(server, ['error'])
5910         self.assertEqual('host1', migration['source_compute'])
5911         self.assertEqual('host2', migration['dest_compute'])
5912 
5913         # Nova fails the migration because it ties to allocation CUSTOM_MAGIC
5914         # from the dest node root RP and placement rejects the that allocation.
5915         self.assertIn("Unable to allocate inventory: Inventory for "
5916                       "'CUSTOM_MAGIC'", self.stdlog.logger.output)
5917         self.assertIn('No valid host was found. Unable to move instance %s to '
5918                       'host host2. There is not enough capacity on the host '
5919                       'for the instance.' % server['id'],
5920                       self.stdlog.logger.output)
5921 
5922         dest_rp_uuid = self._get_provider_uuid_by_host('host2')
5923 
5924         # There should be no usage for the dest
5925         self.assertRequestMatchesUsage(
5926             {'VCPU': 0,
5927              'MEMORY_MB': 0,
5928              'DISK_GB': 0}, dest_rp_uuid)
5929 
5930         # and everything stays at the source
5931         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
5932         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
5933                                            source_rp_uuid)
5934 
5935         self._delete_and_check_allocations(server)
5936 
5937     def test_force_evacuate_from_flat_to_nested(self):
5938         # first compute will start with the flat RP tree but we add
5939         # CUSTOM_MAGIC inventory to the root compute RP
5940         orig_update_provider_tree = fake.MediumFakeDriver.update_provider_tree
5941 
5942         # the ability to force evacuate a server is removed entirely in 2.68
5943         self.api.microversion = '2.67'
5944 
5945         def stub_update_provider_tree(self, provider_tree, nodename,
5946                                       allocations=None):
5947             # do the regular inventory update
5948             orig_update_provider_tree(
5949                 self, provider_tree, nodename, allocations)
5950             if nodename == 'host1':
5951                 # add the extra resource
5952                 inv = provider_tree.data(nodename).inventory
5953                 inv['CUSTOM_MAGIC'] = {
5954                     'total': 10,
5955                     'reserved': 0,
5956                     'min_unit': 1,
5957                     'max_unit': 10,
5958                     'step_size': 1,
5959                     'allocation_ratio': 1,
5960                 }
5961                 provider_tree.update_inventory(nodename, inv)
5962 
5963         self.stub_out('nova.virt.fake.FakeDriver.update_provider_tree',
5964                       stub_update_provider_tree)
5965         self.compute1 = self._start_compute(host='host1')
5966         source_rp_uuid = self._get_provider_uuid_by_host('host1')
5967 
5968         server = self._boot_and_check_allocations(self.flavor1, 'host1')
5969         # start the second compute with nested RP tree
5970         self.flags(
5971             compute_driver='fake.MediumFakeDriverWithNestedCustomResources')
5972         self.compute2 = self._start_compute(host='host2')
5973 
5974         source_compute_id = self.admin_api.get_services(
5975             host='host1', binary='nova-compute')[0]['id']
5976         self.compute1.stop()
5977         # force it down to avoid waiting for the service group to time out
5978         self.admin_api.put_service(
5979             source_compute_id, {'forced_down': 'true'})
5980 
5981         # try to force evacuate from flat to nested.
5982         post = {
5983             'evacuate': {
5984                 'host': 'host2',
5985                 'force': True,
5986             }
5987         }
5988 
5989         self.api.post_server_action(server['id'], post)
5990         # We expect that the evacuation will fail as force evacuate tries to
5991         # blindly copy the source allocation to the destination but on the
5992         # destination there is no inventory of CUSTOM_MAGIC on the compute node
5993         # provider as that resource is reported on a child provider.
5994         self._wait_for_server_parameter(self.api, server,
5995             {'OS-EXT-SRV-ATTR:host': 'host1',
5996              'status': 'ACTIVE'})
5997 
5998         migration = self._wait_for_migration_status(server, ['error'])
5999         self.assertEqual('host1', migration['source_compute'])
6000         self.assertEqual('host2', migration['dest_compute'])
6001 
6002         # Nova fails the migration because it ties to allocation CUSTOM_MAGIC
6003         # from the dest node root RP and placement rejects the that allocation.
6004         self.assertIn("Unable to allocate inventory: Inventory for "
6005                       "'CUSTOM_MAGIC'", self.stdlog.logger.output)
6006         self.assertIn('No valid host was found. Unable to move instance %s to '
6007                       'host host2. There is not enough capacity on the host '
6008                       'for the instance.' % server['id'],
6009                       self.stdlog.logger.output)
6010 
6011         dest_rp_uuid = self._get_provider_uuid_by_host('host2')
6012 
6013         # There should be no usage for the dest
6014         self.assertRequestMatchesUsage(
6015             {'VCPU': 0,
6016              'MEMORY_MB': 0,
6017              'DISK_GB': 0}, dest_rp_uuid)
6018 
6019         # and everything stays at the source
6020         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
6021         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
6022                                            source_rp_uuid)
6023 
6024         self._delete_and_check_allocations(server)
6025 
6026 
6027 class PortResourceRequestBasedSchedulingTestBase(
6028         integrated_helpers.ProviderUsageBaseTestCase):
6029 
6030     compute_driver = 'fake.FakeDriverWithPciResources'
6031 
6032     CUSTOM_VNIC_TYPE_NORMAL = 'CUSTOM_VNIC_TYPE_NORMAL'
6033     CUSTOM_VNIC_TYPE_DIRECT = 'CUSTOM_VNIC_TYPE_DIRECT'
6034     CUSTOM_PHYSNET1 = 'CUSTOM_PHYSNET1'
6035     CUSTOM_PHYSNET2 = 'CUSTOM_PHYSNET2'
6036     CUSTOM_PHYSNET3 = 'CUSTOM_PHYSNET3'
6037 
6038     def setUp(self):
6039         # enable PciPassthroughFilter to support SRIOV before the base class
6040         # starts the scheduler
6041         if 'PciPassthroughFilter' not in CONF.filter_scheduler.enabled_filters:
6042             self.flags(
6043                 enabled_filters=CONF.filter_scheduler.enabled_filters
6044                                 + ['PciPassthroughFilter'],
6045                 group='filter_scheduler')
6046 
6047         self.useFixture(
6048             fake.FakeDriverWithPciResources.
6049                 FakeDriverWithPciResourcesConfigFixture())
6050 
6051         super(PortResourceRequestBasedSchedulingTestBase, self).setUp()
6052         self.compute1 = self._start_compute('host1')
6053         self.compute1_rp_uuid = self._get_provider_uuid_by_host('host1')
6054         self.ovs_bridge_rp_per_host = {}
6055         self.flavor = self.api.get_flavors()[0]
6056         self.flavor_with_group_policy = self.api.get_flavors()[1]
6057 
6058         # Setting group policy for placement. This is mandatory when more than
6059         # one request group is included in the allocation candidate request and
6060         # we have tests with two ports both having resource request modelled as
6061         # two separate request groups.
6062         self.admin_api.post_extra_spec(
6063             self.flavor_with_group_policy['id'],
6064             {'extra_specs': {'group_policy': 'isolate'}})
6065 
6066         self._create_networking_rp_tree(self.compute1_rp_uuid)
6067 
6068         # add extra ports and the related network to the neutron fixture
6069         # specifically for these tests. It cannot be added globally in the
6070         # fixture init as it adds a second network that makes auto allocation
6071         # based test to fail due to ambiguous networks.
6072         self.neutron._ports[
6073             self.neutron.port_with_sriov_resource_request['id']] = \
6074             copy.deepcopy(self.neutron.port_with_sriov_resource_request)
6075         self.neutron._ports[self.neutron.sriov_port['id']] = \
6076             copy.deepcopy(self.neutron.sriov_port)
6077         self.neutron._networks[
6078             self.neutron.network_2['id']] = self.neutron.network_2
6079         self.neutron._subnets[
6080             self.neutron.subnet_2['id']] = self.neutron.subnet_2
6081 
6082     def _create_server(self, flavor, networks):
6083         server_req = self._build_minimal_create_server_request(
6084             self.api, 'bandwidth-aware-server',
6085             image_uuid='76fa36fc-c930-4bf3-8c8a-ea2a2420deb6',
6086             flavor_id=flavor['id'], networks=networks)
6087         return self.api.post_server({'server': server_req})
6088 
6089     def _set_provider_inventories(self, rp_uuid, inventories):
6090         rp = self.placement_api.get(
6091             '/resource_providers/%s' % rp_uuid).body
6092         inventories['resource_provider_generation'] = rp['generation']
6093         return self._update_inventory(rp_uuid, inventories)
6094 
6095     def _create_ovs_networking_rp_tree(self, compute_rp_uuid):
6096         # we need uuid sentinel for the test to make pep8 happy but we need a
6097         # unique one per compute so here is some ugliness
6098         ovs_agent_rp_uuid = getattr(uuids, compute_rp_uuid + 'ovs agent')
6099         agent_rp_req = {
6100             "name": ovs_agent_rp_uuid,
6101             "uuid": ovs_agent_rp_uuid,
6102             "parent_provider_uuid": compute_rp_uuid
6103         }
6104         self.placement_api.post('/resource_providers',
6105                                 body=agent_rp_req,
6106                                 version='1.20')
6107         ovs_bridge_rp_uuid = getattr(uuids, ovs_agent_rp_uuid + 'ovs br')
6108         ovs_bridge_req = {
6109             "name": ovs_bridge_rp_uuid,
6110             "uuid": ovs_bridge_rp_uuid,
6111             "parent_provider_uuid": ovs_agent_rp_uuid
6112         }
6113         self.placement_api.post('/resource_providers',
6114                                 body=ovs_bridge_req,
6115                                 version='1.20')
6116         self.ovs_bridge_rp_per_host[compute_rp_uuid] = ovs_bridge_rp_uuid
6117 
6118         self._set_provider_inventories(
6119             ovs_bridge_rp_uuid,
6120             {"inventories": {
6121                 orc.NET_BW_IGR_KILOBIT_PER_SEC: {"total": 10000},
6122                 orc.NET_BW_EGR_KILOBIT_PER_SEC: {"total": 10000},
6123             }})
6124 
6125         self._create_trait(self.CUSTOM_VNIC_TYPE_NORMAL)
6126         self._create_trait(self.CUSTOM_PHYSNET2)
6127 
6128         self._set_provider_traits(
6129             ovs_bridge_rp_uuid,
6130             [self.CUSTOM_VNIC_TYPE_NORMAL, self.CUSTOM_PHYSNET2])
6131 
6132     def _create_pf_device_rp(
6133             self, device_rp_uuid, parent_rp_uuid, inventories, traits,
6134             device_rp_name=None):
6135         """Create a RP in placement for a physical function network device with
6136         traits and inventories.
6137         """
6138 
6139         if not device_rp_name:
6140             device_rp_name = device_rp_uuid
6141 
6142         sriov_pf_req = {
6143             "name": device_rp_name,
6144             "uuid": device_rp_uuid,
6145             "parent_provider_uuid": parent_rp_uuid
6146         }
6147         self.placement_api.post('/resource_providers',
6148                                 body=sriov_pf_req,
6149                                 version='1.20')
6150 
6151         self._set_provider_inventories(
6152             device_rp_uuid,
6153             {"inventories": inventories})
6154 
6155         for trait in traits:
6156             self._create_trait(trait)
6157 
6158         self._set_provider_traits(
6159             device_rp_uuid,
6160             traits)
6161 
6162     def _create_sriov_networking_rp_tree(self, compute_rp_uuid):
6163         # Create a matching RP tree in placement for the PCI devices added to
6164         # the passthrough_whitelist config during setUp() and PCI devices
6165         # present in the FakeDriverWithPciResources virt driver.
6166         #
6167         # * PF1 represents the PCI device 0000:01:00, it will be mapped to
6168         # physnet1 and it will have bandwidth inventory.
6169         # * PF2 represents the PCI device 0000:02:00, it will be mapped to
6170         # physnet2 it will have bandwidth inventory.
6171         # * PF3 represents the PCI device 0000:03:00 and, it will be mapped to
6172         # physnet2 but it will not have bandwidth inventory.
6173 
6174         compute_name = compute_rp_uuid
6175         sriov_agent_rp_uuid = getattr(uuids, compute_rp_uuid + 'sriov agent')
6176         agent_rp_req = {
6177             "name": "%s:NIC Switch agent" % compute_name,
6178             "uuid": sriov_agent_rp_uuid,
6179             "parent_provider_uuid": compute_rp_uuid
6180         }
6181         self.placement_api.post('/resource_providers',
6182                                 body=agent_rp_req,
6183                                 version='1.20')
6184 
6185         self.sriov_pf1_rp_uuid = getattr(uuids, sriov_agent_rp_uuid + 'PF1')
6186         inventories = {
6187             orc.NET_BW_IGR_KILOBIT_PER_SEC: {"total": 100000},
6188             orc.NET_BW_EGR_KILOBIT_PER_SEC: {"total": 100000},
6189         }
6190         traits = [self.CUSTOM_VNIC_TYPE_DIRECT, self.CUSTOM_PHYSNET1]
6191         self._create_pf_device_rp(
6192             self.sriov_pf1_rp_uuid, sriov_agent_rp_uuid, inventories, traits,
6193             device_rp_name="%s:NIC Switch agent:ens1" % compute_name)
6194 
6195         self.sriov_pf2_rp_uuid = getattr(uuids, sriov_agent_rp_uuid + 'PF2')
6196         inventories = {
6197             orc.NET_BW_IGR_KILOBIT_PER_SEC: {"total": 100000},
6198             orc.NET_BW_EGR_KILOBIT_PER_SEC: {"total": 100000},
6199         }
6200         traits = [self.CUSTOM_VNIC_TYPE_DIRECT, self.CUSTOM_PHYSNET2]
6201         self._create_pf_device_rp(
6202             self.sriov_pf2_rp_uuid, sriov_agent_rp_uuid, inventories, traits,
6203             device_rp_name="%s:NIC Switch agent:ens2" % compute_name)
6204 
6205         self.sriov_pf3_rp_uuid = getattr(uuids, sriov_agent_rp_uuid + 'PF3')
6206         inventories = {}
6207         traits = [self.CUSTOM_VNIC_TYPE_DIRECT, self.CUSTOM_PHYSNET2]
6208         self._create_pf_device_rp(
6209             self.sriov_pf3_rp_uuid, sriov_agent_rp_uuid, inventories, traits,
6210             device_rp_name="%s:NIC Switch agent:ens3" % compute_name)
6211 
6212     def _create_networking_rp_tree(self, compute_rp_uuid):
6213         # let's simulate what the neutron would do
6214         self._create_ovs_networking_rp_tree(compute_rp_uuid)
6215         self._create_sriov_networking_rp_tree(compute_rp_uuid)
6216 
6217     def assertPortMatchesAllocation(self, port, allocations):
6218         port_request = port['resource_request']['resources']
6219         for rc, amount in allocations.items():
6220             self.assertEqual(port_request[rc], amount,
6221                              'port %s requested %d %s '
6222                              'resources but got allocation %d' %
6223                              (port['id'], port_request[rc], rc,
6224                               amount))
6225 
6226 
6227 class UnsupportedPortResourceRequestBasedSchedulingTest(
6228         PortResourceRequestBasedSchedulingTestBase):
6229     """Tests for handling servers with ports having resource requests """
6230 
6231     def _add_resource_request_to_a_bound_port(self, port_id):
6232         # NOTE(gibi): self.neutron._ports contains a copy of each neutron port
6233         # defined on class level in the fixture. So modifying what is in the
6234         # _ports list is safe as it is re-created for each Neutron fixture
6235         # instance therefore for each individual test using that fixture.
6236         bound_port = self.neutron._ports[port_id]
6237         bound_port['resource_request'] = (
6238             self.neutron.port_with_resource_request['resource_request'])
6239 
6240     def test_interface_attach_with_port_resource_request(self):
6241         # create a server
6242         server = self._create_server(
6243             flavor=self.flavor,
6244             networks=[{'port': self.neutron.port_1['id']}])
6245         self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6246 
6247         # try to add a port with resource request
6248         post = {
6249             'interfaceAttachment': {
6250                 'port_id': self.neutron.port_with_resource_request['id']
6251         }}
6252         ex = self.assertRaises(client.OpenStackApiException,
6253                                self.api.attach_interface,
6254                                server['id'], post)
6255         self.assertEqual(400, ex.response.status_code)
6256         self.assertIn('Attaching interfaces with QoS policy is '
6257                       'not supported for instance',
6258                       six.text_type(ex))
6259 
6260     @mock.patch('nova.tests.fixtures.NeutronFixture.create_port')
6261     def test_interface_attach_with_network_create_port_has_resource_request(
6262             self, mock_neutron_create_port):
6263         # create a server
6264         server = self._create_server(
6265             flavor=self.flavor,
6266             networks=[{'port': self.neutron.port_1['id']}])
6267         self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6268 
6269         # the interfaceAttach operation below will result in a new port being
6270         # created in the network that is attached. Make sure that neutron
6271         # returns a port that has resource request.
6272         mock_neutron_create_port.return_value = (
6273             {'port': copy.deepcopy(self.neutron.port_with_resource_request)})
6274 
6275         # try to attach a network
6276         post = {
6277             'interfaceAttachment': {
6278                 'net_id': self.neutron.network_1['id']
6279         }}
6280         ex = self.assertRaises(client.OpenStackApiException,
6281                                self.api.attach_interface,
6282                                server['id'], post)
6283         self.assertEqual(400, ex.response.status_code)
6284         self.assertIn('Using networks with QoS policy is not supported for '
6285                       'instance',
6286                       six.text_type(ex))
6287 
6288     @mock.patch('nova.tests.fixtures.NeutronFixture.create_port')
6289     def test_create_server_with_network_create_port_has_resource_request(
6290             self, mock_neutron_create_port):
6291         # the server create operation below will result in a new port being
6292         # created in the network. Make sure that neutron returns a port that
6293         # has resource request.
6294         mock_neutron_create_port.return_value = (
6295             {'port': copy.deepcopy(self.neutron.port_with_resource_request)})
6296 
6297         server = self._create_server(
6298             flavor=self.flavor,
6299             networks=[{'uuid': self.neutron.network_1['id']}])
6300         server = self._wait_for_state_change(self.admin_api, server, 'ERROR')
6301 
6302         self.assertEqual(500, server['fault']['code'])
6303         self.assertIn('Failed to allocate the network',
6304                       server['fault']['message'])
6305 
6306     def test_create_server_with_port_resource_request_old_microversion(self):
6307 
6308         # NOTE(gibi): 2.71 is the last microversion where nova does not support
6309         # this kind of create server
6310         self.api.microversion = '2.71'
6311         ex = self.assertRaises(
6312             client.OpenStackApiException, self._create_server,
6313             flavor=self.flavor,
6314             networks=[{'port': self.neutron.port_with_resource_request['id']}])
6315 
6316         self.assertEqual(400, ex.response.status_code)
6317         self.assertIn(
6318             "Creating servers with ports having resource requests, like a "
6319             "port with a QoS minimum bandwidth policy, is not supported "
6320             "until microversion 2.72.",
6321             six.text_type(ex))
6322 
6323     def test_resize_server_with_port_resource_request_old_microversion(self):
6324         server = self._create_server(
6325             flavor=self.flavor,
6326             networks=[{'port': self.neutron.port_1['id']}])
6327         self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6328 
6329         # We need to simulate that the above server has a port that has
6330         # resource request; we cannot boot with such a port but legacy servers
6331         # can exist with such a port.
6332         self._add_resource_request_to_a_bound_port(self.neutron.port_1['id'])
6333 
6334         resize_req = {
6335             'resize': {
6336                 'flavorRef': self.flavor['id']
6337             }
6338         }
6339         ex = self.assertRaises(
6340             client.OpenStackApiException,
6341             self.api.post_server_action, server['id'], resize_req)
6342 
6343         self.assertEqual(400, ex.response.status_code)
6344         self.assertIn(
6345             'The resize action on a server with ports having resource '
6346             'requests', six.text_type(ex))
6347 
6348     def test_migrate_server_with_port_resource_request_old_microversion(self):
6349         server = self._create_server(
6350             flavor=self.flavor,
6351             networks=[{'port': self.neutron.port_1['id']}])
6352         self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6353 
6354         # We need to simulate that the above server has a port that has
6355         # resource request; we cannot boot with such a port but legacy servers
6356         # can exist with such a port.
6357         self._add_resource_request_to_a_bound_port(self.neutron.port_1['id'])
6358 
6359         ex = self.assertRaises(
6360             client.OpenStackApiException,
6361             self.api.post_server_action, server['id'], {'migrate': None})
6362 
6363         self.assertEqual(400, ex.response.status_code)
6364         self.assertIn(
6365             'The migrate action on a server with ports having resource '
6366             'requests', six.text_type(ex))
6367 
6368     def test_live_migrate_server_with_port_resource_request_old_microversion(
6369             self):
6370         server = self._create_server(
6371             flavor=self.flavor,
6372             networks=[{'port': self.neutron.port_1['id']}])
6373         self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6374 
6375         # We need to simulate that the above server has a port that has
6376         # resource request; we cannot boot with such a port but legacy servers
6377         # can exist with such a port.
6378         self._add_resource_request_to_a_bound_port(self.neutron.port_1['id'])
6379 
6380         post = {
6381             'os-migrateLive': {
6382                 'host': None,
6383                 'block_migration': False,
6384             }
6385         }
6386         ex = self.assertRaises(
6387             client.OpenStackApiException,
6388             self.api.post_server_action, server['id'], post)
6389 
6390         self.assertEqual(400, ex.response.status_code)
6391         self.assertIn(
6392             'The os-migrateLive action on a server with ports having resource '
6393             'requests', six.text_type(ex))
6394 
6395     def test_evacuate_server_with_port_resource_request_old_microversion(
6396             self):
6397         server = self._create_server(
6398             flavor=self.flavor,
6399             networks=[{'port': self.neutron.port_1['id']}])
6400         self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6401 
6402         # We need to simulate that the above server has a port that has
6403         # resource request; we cannot boot with such a port but legacy servers
6404         # can exist with such a port.
6405         self._add_resource_request_to_a_bound_port(self.neutron.port_1['id'])
6406 
6407         ex = self.assertRaises(
6408             client.OpenStackApiException,
6409             self.api.post_server_action, server['id'], {'evacuate': {}})
6410 
6411         self.assertEqual(400, ex.response.status_code)
6412         self.assertIn(
6413             'The evacuate action on a server with ports having resource '
6414             'requests', six.text_type(ex))
6415 
6416     def test_unshelve_offloaded_server_with_port_resource_request_old_version(
6417             self):
6418         server = self._create_server(
6419             flavor=self.flavor,
6420             networks=[{'port': self.neutron.port_1['id']}])
6421         self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6422 
6423         # with default config shelve means immediate offload as well
6424         req = {
6425             'shelve': {}
6426         }
6427         self.api.post_server_action(server['id'], req)
6428         self._wait_for_server_parameter(
6429             self.api, server, {'status': 'SHELVED_OFFLOADED'})
6430 
6431         # We need to simulate that the above server has a port that has
6432         # resource request; we cannot boot with such a port but legacy servers
6433         # can exist with such a port.
6434         self._add_resource_request_to_a_bound_port(self.neutron.port_1['id'])
6435 
6436         ex = self.assertRaises(
6437             client.OpenStackApiException,
6438             self.api.post_server_action, server['id'], {'unshelve': {}})
6439 
6440         self.assertEqual(400, ex.response.status_code)
6441         self.assertIn(
6442             'The unshelve action on a server with ports having resource '
6443             'requests', six.text_type(ex))
6444 
6445     def test_unshelve_not_offloaded_server_with_port_resource_request(
6446             self):
6447         """If the server is not offloaded then unshelving does not cause a new
6448         resource allocation therefore having port resource request is
6449         irrelevant. This test asserts that such unshelve request is not
6450         rejected.
6451         """
6452         server = self._create_server(
6453             flavor=self.flavor,
6454             networks=[{'port': self.neutron.port_1['id']}])
6455         self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6456 
6457         # avoid automatic shelve offloading
6458         self.flags(shelved_offload_time=-1)
6459         req = {
6460             'shelve': {}
6461         }
6462         self.api.post_server_action(server['id'], req)
6463         self._wait_for_server_parameter(
6464             self.api, server, {'status': 'SHELVED'})
6465 
6466         # We need to simulate that the above server has a port that has
6467         # resource request; we cannot boot with such a port but legacy servers
6468         # can exist with such a port.
6469         self._add_resource_request_to_a_bound_port(self.neutron.port_1['id'])
6470 
6471         self.api.post_server_action(server['id'], {'unshelve': {}})
6472         self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6473 
6474 
6475 class PortResourceRequestBasedSchedulingTest(
6476         PortResourceRequestBasedSchedulingTestBase):
6477     """Tests creating a server with a pre-existing port that has a resource
6478     request for a QoS minimum bandwidth policy.
6479     """
6480 
6481     def test_boot_server_with_two_ports_one_having_resource_request(self):
6482         non_qos_port = self.neutron.port_1
6483         qos_port = self.neutron.port_with_resource_request
6484 
6485         server = self._create_server(
6486             flavor=self.flavor,
6487             networks=[{'port': non_qos_port['id']},
6488                       {'port': qos_port['id']}])
6489         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6490         updated_non_qos_port = self.neutron.show_port(
6491             non_qos_port['id'])['port']
6492         updated_qos_port = self.neutron.show_port(qos_port['id'])['port']
6493 
6494         allocations = self.placement_api.get(
6495             '/allocations/%s' % server['id']).body['allocations']
6496 
6497         # We expect one set of allocations for the compute resources on the
6498         # compute rp and one set for the networking resources on the ovs bridge
6499         # rp due to the qos_port resource request
6500         self.assertEqual(2, len(allocations))
6501         compute_allocations = allocations[self.compute1_rp_uuid]['resources']
6502         network_allocations = allocations[
6503             self.ovs_bridge_rp_per_host[self.compute1_rp_uuid]]['resources']
6504 
6505         self.assertEqual(self._resources_from_flavor(self.flavor),
6506                          compute_allocations)
6507         self.assertPortMatchesAllocation(qos_port, network_allocations)
6508 
6509         # We expect that only the RP uuid of the networking RP having the port
6510         # allocation is sent in the port binding for the port having resource
6511         # request
6512         qos_binding_profile = updated_qos_port['binding:profile']
6513         self.assertEqual(self.ovs_bridge_rp_per_host[self.compute1_rp_uuid],
6514                          qos_binding_profile['allocation'])
6515 
6516         # And we expect not to have any allocation set in the port binding for
6517         # the port that doesn't have resource request
6518         self.assertNotIn('binding:profile', updated_non_qos_port)
6519 
6520         self._delete_and_check_allocations(server)
6521 
6522         # assert that unbind removes the allocation from the binding of the
6523         # port that got allocation during the bind
6524         updated_qos_port = self.neutron.show_port(qos_port['id'])['port']
6525         binding_profile = updated_qos_port['binding:profile']
6526         self.assertNotIn('allocation', binding_profile)
6527 
6528     def test_one_ovs_one_sriov_port(self):
6529         ovs_port = self.neutron.port_with_resource_request
6530         sriov_port = self.neutron.port_with_sriov_resource_request
6531 
6532         server = self._create_server(flavor=self.flavor_with_group_policy,
6533                                      networks=[{'port': ovs_port['id']},
6534                                                {'port': sriov_port['id']}])
6535 
6536         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6537 
6538         ovs_port = self.neutron.show_port(ovs_port['id'])['port']
6539         sriov_port = self.neutron.show_port(sriov_port['id'])['port']
6540 
6541         allocations = self.placement_api.get(
6542             '/allocations/%s' % server['id']).body['allocations']
6543 
6544         # We expect one set of allocations for the compute resources on the
6545         # compute rp and one set for the networking resources on the ovs bridge
6546         # rp and on the sriov PF rp.
6547         self.assertEqual(3, len(allocations))
6548         compute_allocations = allocations[self.compute1_rp_uuid]['resources']
6549         ovs_allocations = allocations[
6550             self.ovs_bridge_rp_per_host[self.compute1_rp_uuid]]['resources']
6551         sriov_allocations = allocations[self.sriov_pf2_rp_uuid]['resources']
6552 
6553         self.assertEqual(
6554             self._resources_from_flavor(self.flavor_with_group_policy),
6555             compute_allocations)
6556 
6557         self.assertPortMatchesAllocation(ovs_port, ovs_allocations)
6558         self.assertPortMatchesAllocation(sriov_port, sriov_allocations)
6559 
6560         # We expect that only the RP uuid of the networking RP having the port
6561         # allocation is sent in the port binding for the port having resource
6562         # request
6563         ovs_binding = ovs_port['binding:profile']
6564         self.assertEqual(self.ovs_bridge_rp_per_host[self.compute1_rp_uuid],
6565                          ovs_binding['allocation'])
6566         sriov_binding = sriov_port['binding:profile']
6567         self.assertEqual(self.sriov_pf2_rp_uuid,
6568                          sriov_binding['allocation'])
6569 
6570     def test_interface_detach_with_port_with_bandwidth_request(self):
6571         port = self.neutron.port_with_resource_request
6572 
6573         # create a server
6574         server = self._create_server(
6575             flavor=self.flavor,
6576             networks=[{'port': port['id']}])
6577         self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6578 
6579         allocations = self.placement_api.get(
6580             '/allocations/%s' % server['id']).body['allocations']
6581         # We expect one set of allocations for the compute resources on the
6582         # compute rp and one set for the networking resources on the ovs bridge
6583         # rp due to the port resource request
6584         self.assertEqual(2, len(allocations))
6585         compute_allocations = allocations[self.compute1_rp_uuid]['resources']
6586         network_allocations = allocations[
6587             self.ovs_bridge_rp_per_host[self.compute1_rp_uuid]]['resources']
6588 
6589         self.assertEqual(self._resources_from_flavor(self.flavor),
6590                          compute_allocations)
6591         self.assertPortMatchesAllocation(port, network_allocations)
6592 
6593         # We expect that only the RP uuid of the networking RP having the port
6594         # allocation is sent in the port binding for the port having resource
6595         # request
6596         updated_port = self.neutron.show_port(port['id'])['port']
6597         binding_profile = updated_port['binding:profile']
6598         self.assertEqual(self.ovs_bridge_rp_per_host[self.compute1_rp_uuid],
6599                          binding_profile['allocation'])
6600 
6601         self.api.detach_interface(
6602             server['id'], self.neutron.port_with_resource_request['id'])
6603 
6604         fake_notifier.wait_for_versioned_notifications(
6605             'instance.interface_detach.end')
6606 
6607         updated_port = self.neutron.show_port(
6608             self.neutron.port_with_resource_request['id'])['port']
6609 
6610         allocations = self.placement_api.get(
6611             '/allocations/%s' % server['id']).body['allocations']
6612 
6613         # We expect that the port related resource allocations are removed
6614         self.assertEqual(1, len(allocations))
6615         compute_allocations = allocations[self.compute1_rp_uuid]['resources']
6616         self.assertEqual(self._resources_from_flavor(self.flavor),
6617                          compute_allocations)
6618 
6619         # We expect that the allocation is removed from the port too
6620         binding_profile = updated_port['binding:profile']
6621         self.assertNotIn('allocation', binding_profile)
6622 
6623     def test_two_sriov_ports_one_with_request_two_available_pfs(self):
6624         """Verify that the port's bandwidth allocated from the same PF as
6625         the allocated VF.
6626 
6627         One compute host:
6628         * PF1 (0000:01:00) is configured for physnet1
6629         * PF2 (0000:02:00) is configured for physnet2, with 1 VF and bandwidth
6630           inventory
6631         * PF3 (0000:03:00) is configured for physnet2, with 1 VF but without
6632           bandwidth inventory
6633 
6634         One instance will be booted with two neutron ports, both ports
6635         requested to be connected to physnet2. One port has resource request
6636         the other does not have resource request. The port having the resource
6637         request cannot be allocated to PF3 and PF1 while the other port that
6638         does not have resource request can be allocated to PF2 or PF3.
6639 
6640         For the detailed compute host config see the FakeDriverWithPciResources
6641         class. For the necessary passthrough_whitelist config see the setUp of
6642         the PortResourceRequestBasedSchedulingTestBase class.
6643         """
6644 
6645         sriov_port = self.neutron.sriov_port
6646         sriov_port_with_res_req = self.neutron.port_with_sriov_resource_request
6647         server = self._create_server(
6648             flavor=self.flavor_with_group_policy,
6649             networks=[
6650                 {'port': sriov_port_with_res_req['id']},
6651                 {'port': sriov_port['id']}])
6652 
6653         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6654 
6655         sriov_port = self.neutron.show_port(sriov_port['id'])['port']
6656         sriov_port_with_res_req = self.neutron.show_port(
6657             sriov_port_with_res_req['id'])['port']
6658 
6659         allocations = self.placement_api.get(
6660             '/allocations/%s' % server['id']).body['allocations']
6661 
6662         # We expect one set of allocations for the compute resources on the
6663         # compute rp and one set for the networking resources on the sriov PF2
6664         # rp.
6665         self.assertEqual(2, len(allocations))
6666         compute_allocations = allocations[self.compute1_rp_uuid]['resources']
6667         self.assertEqual(
6668             self._resources_from_flavor(self.flavor_with_group_policy),
6669             compute_allocations)
6670 
6671         sriov_allocations = allocations[self.sriov_pf2_rp_uuid]['resources']
6672         self.assertPortMatchesAllocation(
6673             sriov_port_with_res_req, sriov_allocations)
6674 
6675         # We expect that only the RP uuid of the networking RP having the port
6676         # allocation is sent in the port binding for the port having resource
6677         # request
6678         sriov_with_req_binding = sriov_port_with_res_req['binding:profile']
6679         self.assertEqual(
6680             self.sriov_pf2_rp_uuid, sriov_with_req_binding['allocation'])
6681         # and the port without resource request does not have allocation
6682         sriov_binding = sriov_port['binding:profile']
6683         self.assertNotIn('allocation', sriov_binding)
6684 
6685         # We expect that the selected PCI device matches with the RP from
6686         # where the bandwidth is allocated from. The bandwidth is allocated
6687         # from 0000:02:00 (PF2) so the PCI device should be a VF of that PF
6688         self.assertEqual('0000:02:00.1', sriov_with_req_binding['pci_slot'])
6689         # But also the port that has no resource request still gets a pci slot
6690         # allocated. The 0000:02:00 has no more VF available but 0000:03:00 has
6691         # one VF available and that PF is also on physnet2
6692         self.assertEqual('0000:03:00.1', sriov_binding['pci_slot'])
6693 
6694     def test_one_sriov_port_no_vf_and_bandwidth_available_on_the_same_pf(self):
6695         """Verify that if there is no PF that both provides bandwidth and VFs
6696         then the boot will fail.
6697         """
6698 
6699         # boot a server with a single sriov port that has no resource request
6700         sriov_port = self.neutron.sriov_port
6701         server = self._create_server(
6702             flavor=self.flavor_with_group_policy,
6703             networks=[{'port': sriov_port['id']}])
6704 
6705         self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6706         sriov_port = self.neutron.show_port(sriov_port['id'])['port']
6707         sriov_binding = sriov_port['binding:profile']
6708 
6709         # We expect that this consume the last available VF from the PF2
6710         self.assertEqual('0000:02:00.1', sriov_binding['pci_slot'])
6711 
6712         # Now boot a second server with a port that has resource request
6713         # At this point PF2 has available bandwidth but no available VF
6714         # and PF3 has available VF but no available bandwidth so we expect
6715         # the boot to fail.
6716 
6717         sriov_port_with_res_req = self.neutron.port_with_sriov_resource_request
6718         server = self._create_server(
6719             flavor=self.flavor_with_group_policy,
6720             networks=[{'port': sriov_port_with_res_req['id']}])
6721 
6722         # NOTE(gibi): It should be NoValidHost in an ideal world but that would
6723         # require the scheduler to detect the situation instead of the pci
6724         # claim. However that is pretty hard as the scheduler does not know
6725         # anything about allocation candidates (e.g. that the only candidate
6726         # for the port in this case is PF2) it see the whole host as a
6727         # candidate and in our host there is available VF for the request even
6728         # if that is on the wrong PF.
6729         server = self._wait_for_state_change(self.admin_api, server, 'ERROR')
6730         self.assertIn(
6731             'Exceeded maximum number of retries. Exhausted all hosts '
6732             'available for retrying build failures for instance',
6733             server['fault']['message'])
6734 
6735 
6736 class PortResourceRequestReSchedulingTest(
6737         PortResourceRequestBasedSchedulingTestBase):
6738     """Similar to PortResourceRequestBasedSchedulingTest
6739     except this test uses FakeRescheduleDriver which will test reschedules
6740     during server create work as expected, i.e. that the resource request
6741     allocations are moved from the initially selected compute to the
6742     alternative compute.
6743     """
6744 
6745     compute_driver = 'fake.FakeRescheduleDriver'
6746 
6747     def setUp(self):
6748         super(PortResourceRequestReSchedulingTest, self).setUp()
6749         self.compute2 = self._start_compute('host2')
6750         self.compute2_rp_uuid = self._get_provider_uuid_by_host('host2')
6751         self._create_networking_rp_tree(self.compute2_rp_uuid)
6752 
6753     def _create_networking_rp_tree(self, compute_rp_uuid):
6754         # let's simulate what the neutron would do
6755         self._create_ovs_networking_rp_tree(compute_rp_uuid)
6756 
6757     def test_boot_reschedule_success(self):
6758         port = self.neutron.port_with_resource_request
6759 
6760         server = self._create_server(
6761             flavor=self.flavor,
6762             networks=[{'port': port['id']}])
6763         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6764         updated_port = self.neutron.show_port(port['id'])['port']
6765 
6766         dest_hostname = server['OS-EXT-SRV-ATTR:host']
6767         dest_compute_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
6768 
6769         failed_compute_rp = (self.compute1_rp_uuid
6770                              if dest_compute_rp_uuid == self.compute2_rp_uuid
6771                              else self.compute2_rp_uuid)
6772 
6773         allocations = self.placement_api.get(
6774             '/allocations/%s' % server['id']).body['allocations']
6775 
6776         # We expect one set of allocations for the compute resources on the
6777         # compute rp and one set for the networking resources on the ovs bridge
6778         # rp
6779         self.assertEqual(2, len(allocations))
6780         compute_allocations = allocations[dest_compute_rp_uuid]['resources']
6781         network_allocations = allocations[
6782             self.ovs_bridge_rp_per_host[dest_compute_rp_uuid]]['resources']
6783 
6784         self.assertEqual(self._resources_from_flavor(self.flavor),
6785                          compute_allocations)
6786         self.assertPortMatchesAllocation(port, network_allocations)
6787 
6788         # assert that the allocations against the host where the spawn
6789         # failed are cleaned up properly
6790         self.assertEqual(
6791             {'VCPU': 0, 'MEMORY_MB': 0, 'DISK_GB': 0},
6792             self._get_provider_usages(failed_compute_rp))
6793         self.assertEqual(
6794             {'NET_BW_EGR_KILOBIT_PER_SEC': 0, 'NET_BW_IGR_KILOBIT_PER_SEC': 0},
6795             self._get_provider_usages(
6796                 self.ovs_bridge_rp_per_host[failed_compute_rp]))
6797 
6798         # We expect that only the RP uuid of the networking RP having the port
6799         # allocation is sent in the port binding
6800         binding_profile = updated_port['binding:profile']
6801         self.assertEqual(self.ovs_bridge_rp_per_host[dest_compute_rp_uuid],
6802                          binding_profile['allocation'])
6803 
6804         self._delete_and_check_allocations(server)
6805 
6806         # assert that unbind removes the allocation from the binding
6807         updated_port = self.neutron.show_port(port['id'])['port']
6808         binding_profile = updated_port['binding:profile']
6809         self.assertNotIn('allocation', binding_profile)
6810 
6811     def test_boot_reschedule_fill_provider_mapping_raises(self):
6812         """Verify that if the  _fill_provider_mapping raises during re-schedule
6813         then the instance is properly put into ERROR state.
6814         """
6815 
6816         port = self.neutron.port_with_resource_request
6817 
6818         # First call is during boot, we want that to succeed normally. Then the
6819         # fake virt driver triggers a re-schedule. During that re-schedule the
6820         # fill is called again, and we simulate that call raises.
6821         fill = manager.ComputeTaskManager._fill_provider_mapping
6822 
6823         with mock.patch(
6824                 'nova.conductor.manager.ComputeTaskManager.'
6825                 '_fill_provider_mapping',
6826                 side_effect=[
6827                     fill,
6828                     exception.ResourceProviderTraitRetrievalFailed(
6829                         uuid=uuids.rp1)],
6830                 autospec=True):
6831             server = self._create_server(
6832                 flavor=self.flavor,
6833                 networks=[{'port': port['id']}])
6834             server = self._wait_for_state_change(
6835                 self.admin_api, server, 'ERROR')
6836 
6837         self.assertIn(
6838             'Failed to get traits for resource provider',
6839             server['fault']['message'])
6840 
6841         self._delete_and_check_allocations(server)
6842 
6843         # assert that unbind removes the allocation from the binding
6844         updated_port = self.neutron.show_port(port['id'])['port']
6845         binding_profile = updated_port['binding:profile']
6846         self.assertNotIn('allocation', binding_profile)
