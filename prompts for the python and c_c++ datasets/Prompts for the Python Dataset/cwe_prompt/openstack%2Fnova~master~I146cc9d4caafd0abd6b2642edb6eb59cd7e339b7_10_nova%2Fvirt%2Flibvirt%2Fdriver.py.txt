Please review the code below for security defects using the CWE (Common Weakness Enumeration) as a reference standard. If any are found, please describe the security defect in detail and indicate the corresponding line number of code and solution. If none are detected, state: 'No security defects are detected in the code'.

1 # Copyright 2010 United States Government as represented by the
2 # Administrator of the National Aeronautics and Space Administration.
3 # All Rights Reserved.
4 # Copyright (c) 2010 Citrix Systems, Inc.
5 # Copyright (c) 2011 Piston Cloud Computing, Inc
6 # Copyright (c) 2012 University Of Minho
7 # (c) Copyright 2013 Hewlett-Packard Development Company, L.P.
8 #
9 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
10 #    not use this file except in compliance with the License. You may obtain
11 #    a copy of the License at
12 #
13 #         http://www.apache.org/licenses/LICENSE-2.0
14 #
15 #    Unless required by applicable law or agreed to in writing, software
16 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
17 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
18 #    License for the specific language governing permissions and limitations
19 #    under the License.
20 
21 """
22 A connection to a hypervisor through libvirt.
23 
24 Supports KVM, LXC, QEMU, UML, XEN and Parallels.
25 
26 """
27 
28 import binascii
29 import collections
30 from collections import deque
31 import contextlib
32 import copy
33 import errno
34 import functools
35 import glob
36 import itertools
37 import operator
38 import os
39 import pwd
40 import random
41 import shutil
42 import tempfile
43 import time
44 import uuid
45 
46 from castellan import key_manager
47 from copy import deepcopy
48 import eventlet
49 from eventlet import greenthread
50 from eventlet import tpool
51 from lxml import etree
52 from os_brick import encryptors
53 from os_brick.encryptors import luks as luks_encryptor
54 from os_brick import exception as brick_exception
55 from os_brick.initiator import connector
56 import os_resource_classes as orc
57 from oslo_concurrency import processutils
58 from oslo_log import log as logging
59 from oslo_serialization import base64
60 from oslo_serialization import jsonutils
61 from oslo_service import loopingcall
62 from oslo_utils import encodeutils
63 from oslo_utils import excutils
64 from oslo_utils import fileutils
65 from oslo_utils import importutils
66 from oslo_utils import netutils as oslo_netutils
67 from oslo_utils import strutils
68 from oslo_utils import timeutils
69 from oslo_utils import units
70 from oslo_utils import uuidutils
71 import six
72 from six.moves import range
73 
74 from nova.api.metadata import base as instance_metadata
75 from nova.api.metadata import password
76 from nova import block_device
77 from nova.compute import power_state
78 from nova.compute import task_states
79 from nova.compute import utils as compute_utils
80 from nova.compute import vm_states
81 import nova.conf
82 from nova.console import serial as serial_console
83 from nova.console import type as ctype
84 from nova import context as nova_context
85 from nova import crypto
86 from nova import exception
87 from nova.i18n import _
88 from nova import image
89 from nova.network import model as network_model
90 from nova import objects
91 from nova.objects import diagnostics as diagnostics_obj
92 from nova.objects import fields
93 from nova.pci import manager as pci_manager
94 from nova.pci import utils as pci_utils
95 import nova.privsep.libvirt
96 import nova.privsep.path
97 import nova.privsep.utils
98 from nova import utils
99 from nova import version
100 from nova.virt import block_device as driver_block_device
101 from nova.virt import configdrive
102 from nova.virt.disk import api as disk_api
103 from nova.virt.disk.vfs import guestfs
104 from nova.virt import driver
105 from nova.virt import firewall
106 from nova.virt import hardware
107 from nova.virt.image import model as imgmodel
108 from nova.virt import images
109 from nova.virt.libvirt import blockinfo
110 from nova.virt.libvirt import config as vconfig
111 from nova.virt.libvirt import designer
112 from nova.virt.libvirt import firewall as libvirt_firewall
113 from nova.virt.libvirt import guest as libvirt_guest
114 from nova.virt.libvirt import host
115 from nova.virt.libvirt import imagebackend
116 from nova.virt.libvirt import imagecache
117 from nova.virt.libvirt import instancejobtracker
118 from nova.virt.libvirt import migration as libvirt_migrate
119 from nova.virt.libvirt.storage import dmcrypt
120 from nova.virt.libvirt.storage import lvm
121 from nova.virt.libvirt.storage import rbd_utils
122 from nova.virt.libvirt import utils as libvirt_utils
123 from nova.virt.libvirt import vif as libvirt_vif
124 from nova.virt.libvirt.volume import mount
125 from nova.virt.libvirt.volume import remotefs
126 from nova.virt import netutils
127 from nova.volume import cinder
128 
129 libvirt = None
130 
131 uefi_logged = False
132 
133 LOG = logging.getLogger(__name__)
134 
135 CONF = nova.conf.CONF
136 
137 DEFAULT_FIREWALL_DRIVER = "%s.%s" % (
138     libvirt_firewall.__name__,
139     libvirt_firewall.IptablesFirewallDriver.__name__)
140 
141 DEFAULT_UEFI_LOADER_PATH = {
142     "x86_64": "/usr/share/OVMF/OVMF_CODE.fd",
143     "aarch64": "/usr/share/AAVMF/AAVMF_CODE.fd"
144 }
145 
146 MAX_CONSOLE_BYTES = 100 * units.Ki
147 
148 # The libvirt driver will prefix any disable reason codes with this string.
149 DISABLE_PREFIX = 'AUTO: '
150 # Disable reason for the service which was enabled or disabled without reason
151 DISABLE_REASON_UNDEFINED = None
152 
153 # Guest config console string
154 CONSOLE = "console=tty0 console=ttyS0 console=hvc0"
155 
156 GuestNumaConfig = collections.namedtuple(
157     'GuestNumaConfig', ['cpuset', 'cputune', 'numaconfig', 'numatune'])
158 
159 
160 class InjectionInfo(collections.namedtuple(
161         'InjectionInfo', ['network_info', 'files', 'admin_pass'])):
162     __slots__ = ()
163 
164     def __repr__(self):
165         return ('InjectionInfo(network_info=%r, files=%r, '
166                 'admin_pass=<SANITIZED>)') % (self.network_info, self.files)
167 
168 
169 libvirt_volume_drivers = [
170     'iscsi=nova.virt.libvirt.volume.iscsi.LibvirtISCSIVolumeDriver',
171     'iser=nova.virt.libvirt.volume.iser.LibvirtISERVolumeDriver',
172     'local=nova.virt.libvirt.volume.volume.LibvirtVolumeDriver',
173     'drbd=nova.virt.libvirt.volume.drbd.LibvirtDRBDVolumeDriver',
174     'fake=nova.virt.libvirt.volume.volume.LibvirtFakeVolumeDriver',
175     'rbd=nova.virt.libvirt.volume.net.LibvirtNetVolumeDriver',
176     'sheepdog=nova.virt.libvirt.volume.net.LibvirtNetVolumeDriver',
177     'nfs=nova.virt.libvirt.volume.nfs.LibvirtNFSVolumeDriver',
178     'smbfs=nova.virt.libvirt.volume.smbfs.LibvirtSMBFSVolumeDriver',
179     'aoe=nova.virt.libvirt.volume.aoe.LibvirtAOEVolumeDriver',
180     'fibre_channel='
181         'nova.virt.libvirt.volume.fibrechannel.'
182         'LibvirtFibreChannelVolumeDriver',
183     'gpfs=nova.virt.libvirt.volume.gpfs.LibvirtGPFSVolumeDriver',
184     'quobyte=nova.virt.libvirt.volume.quobyte.LibvirtQuobyteVolumeDriver',
185     'hgst=nova.virt.libvirt.volume.hgst.LibvirtHGSTVolumeDriver',
186     'scaleio=nova.virt.libvirt.volume.scaleio.LibvirtScaleIOVolumeDriver',
187     'disco=nova.virt.libvirt.volume.disco.LibvirtDISCOVolumeDriver',
188     'vzstorage='
189         'nova.virt.libvirt.volume.vzstorage.LibvirtVZStorageVolumeDriver',
190     'veritas_hyperscale='
191         'nova.virt.libvirt.volume.vrtshyperscale.'
192         'LibvirtHyperScaleVolumeDriver',
193     'storpool=nova.virt.libvirt.volume.storpool.LibvirtStorPoolVolumeDriver',
194     'nvmeof=nova.virt.libvirt.volume.nvme.LibvirtNVMEVolumeDriver',
195 ]
196 
197 
198 def patch_tpool_proxy():
199     """eventlet.tpool.Proxy doesn't work with old-style class in __str__()
200     or __repr__() calls. See bug #962840 for details.
201     We perform a monkey patch to replace those two instance methods.
202     """
203     def str_method(self):
204         return str(self._obj)
205 
206     def repr_method(self):
207         return repr(self._obj)
208 
209     tpool.Proxy.__str__ = str_method
210     tpool.Proxy.__repr__ = repr_method
211 
212 
213 patch_tpool_proxy()
214 
215 # For information about when MIN_LIBVIRT_VERSION and
216 # NEXT_MIN_LIBVIRT_VERSION can be changed, consult
217 #
218 #   https://wiki.openstack.org/wiki/LibvirtDistroSupportMatrix
219 #
220 # Currently this is effectively the min version for i686/x86_64
221 # + KVM/QEMU, as other architectures/hypervisors require newer
222 # versions. Over time, this will become a common min version
223 # for all architectures/hypervisors, as this value rises to
224 # meet them.
225 MIN_LIBVIRT_VERSION = (3, 0, 0)
226 MIN_QEMU_VERSION = (2, 8, 0)
227 # TODO(berrange): Re-evaluate this at start of each release cycle
228 # to decide if we want to plan a future min version bump.
229 # MIN_LIBVIRT_VERSION can be updated to match this after
230 # NEXT_MIN_LIBVIRT_VERSION  has been at a higher value for
231 # one cycle
232 NEXT_MIN_LIBVIRT_VERSION = (4, 0, 0)
233 NEXT_MIN_QEMU_VERSION = (2, 11, 0)
234 
235 
236 # Virtuozzo driver support
237 MIN_VIRTUOZZO_VERSION = (7, 0, 0)
238 
239 # aarch64 architecture with KVM
240 # 'chardev' support got sorted out in 3.6.0
241 MIN_LIBVIRT_KVM_AARCH64_VERSION = (3, 6, 0)
242 
243 # Names of the types that do not get compressed during migration
244 NO_COMPRESSION_TYPES = ('qcow2',)
245 
246 
247 # number of serial console limit
248 QEMU_MAX_SERIAL_PORTS = 4
249 # Qemu supports 4 serial consoles, we remove 1 because of the PTY one defined
250 ALLOWED_QEMU_SERIAL_PORTS = QEMU_MAX_SERIAL_PORTS - 1
251 
252 MIN_LIBVIRT_OTHER_ARCH = {
253     fields.Architecture.AARCH64: MIN_LIBVIRT_KVM_AARCH64_VERSION,
254 }
255 
256 # perf events support
257 MIN_LIBVIRT_PERF_VERSION = (2, 0, 0)
258 LIBVIRT_PERF_EVENT_PREFIX = 'VIR_PERF_PARAM_'
259 
260 PERF_EVENTS_CPU_FLAG_MAPPING = {'cmt': 'cmt',
261                                 'mbml': 'mbm_local',
262                                 'mbmt': 'mbm_total',
263                                }
264 
265 # Mediated devices support
266 MIN_LIBVIRT_MDEV_SUPPORT = (3, 4, 0)
267 
268 # libvirt>=3.10 is required for volume multiattach unless qemu<2.10.
269 # See https://bugzilla.redhat.com/show_bug.cgi?id=1378242
270 # for details.
271 MIN_LIBVIRT_MULTIATTACH = (3, 10, 0)
272 
273 MIN_LIBVIRT_LUKS_VERSION = (2, 2, 0)
274 MIN_QEMU_LUKS_VERSION = (2, 6, 0)
275 
276 MIN_LIBVIRT_FILE_BACKED_VERSION = (4, 0, 0)
277 MIN_QEMU_FILE_BACKED_VERSION = (2, 6, 0)
278 
279 MIN_LIBVIRT_FILE_BACKED_DISCARD_VERSION = (4, 4, 0)
280 MIN_QEMU_FILE_BACKED_DISCARD_VERSION = (2, 10, 0)
281 
282 MIN_LIBVIRT_NATIVE_TLS_VERSION = (4, 4, 0)
283 MIN_QEMU_NATIVE_TLS_VERSION = (2, 11, 0)
284 
285 # If the host has this libvirt version, then we skip the retry loop of
286 # instance destroy() call, as libvirt itself increased the wait time
287 # before the SIGKILL signal takes effect.
288 MIN_LIBVIRT_BETTER_SIGKILL_HANDLING = (4, 7, 0)
289 
290 VGPU_RESOURCE_SEMAPHORE = "vgpu_resources"
291 
292 
293 # This is used to save the pmem namespace info temporarily
294 PMEMNamespace = collections.namedtuple('PMEMNamespace',
295         ['tag', 'name', 'dev', 'size',
296          'align', 'numa_node'])
297 
298 
299 class LibvirtDriver(driver.ComputeDriver):
300     def __init__(self, virtapi, read_only=False):
301         # NOTE(aspiers) Some of these are dynamic, so putting
302         # capabilities on the instance rather than on the class.
303         # This prevents the risk of one test setting a capability
304         # which bleeds over into other tests.
305 
306         # LVM and RBD require raw images. If we are not configured to
307         # force convert images into raw format, then we _require_ raw
308         # images only.
309         raw_only = ('rbd', 'lvm')
310         requires_raw_image = (CONF.libvirt.images_type in raw_only and
311                               not CONF.force_raw_images)
312 
313         self.capabilities = {
314             "has_imagecache": True,
315             "supports_evacuate": True,
316             "supports_migrate_to_same_host": False,
317             "supports_attach_interface": True,
318             "supports_device_tagging": True,
319             "supports_tagged_attach_interface": True,
320             "supports_tagged_attach_volume": True,
321             "supports_extend_volume": True,
322             # Multiattach support is conditional on qemu and libvirt versions
323             # determined in init_host.
324             "supports_multiattach": False,
325             "supports_trusted_certs": True,
326             # Supported image types
327             "supports_image_type_aki": True,
328             "supports_image_type_ari": True,
329             "supports_image_type_ami": True,
330             # FIXME(danms): I can see a future where people might want to
331             # configure certain compute nodes to not allow giant raw images
332             # to be booted (like nodes that are across a WAN). Thus, at some
333             # point we may want to be able to _not_ expose "supports raw" on
334             # some nodes by policy. Until then, raw is always supported.
335             "supports_image_type_raw": True,
336             "supports_image_type_iso": True,
337             # NOTE(danms): Certain backends do not work with complex image
338             # formats. If we are configured for those backends, then we
339             # should not expose the corresponding support traits.
340             "supports_image_type_qcow2": not requires_raw_image,
341         }
342         super(LibvirtDriver, self).__init__(virtapi)
343 
344         global libvirt
345         if libvirt is None:
346             libvirt = importutils.import_module('libvirt')
347             libvirt_migrate.libvirt = libvirt
348 
349         self._host = host.Host(self._uri(), read_only,
350                                lifecycle_event_handler=self.emit_event,
351                                conn_event_handler=self._handle_conn_event)
352         self._initiator = None
353         self._fc_wwnns = None
354         self._fc_wwpns = None
355         self._caps = None
356         self._supported_perf_events = []
357         self.firewall_driver = firewall.load_driver(
358             DEFAULT_FIREWALL_DRIVER,
359             host=self._host)
360 
361         self.vif_driver = libvirt_vif.LibvirtGenericVIFDriver()
362 
363         # TODO(mriedem): Long-term we should load up the volume drivers on
364         # demand as needed rather than doing this on startup, as there might
365         # be unsupported volume drivers in this list based on the underlying
366         # platform.
367         self.volume_drivers = self._get_volume_drivers()
368 
369         self._disk_cachemode = None
370         self.image_cache_manager = imagecache.ImageCacheManager()
371         self.image_backend = imagebackend.Backend(CONF.use_cow_images)
372 
373         self.disk_cachemodes = {}
374 
375         self.valid_cachemodes = ["default",
376                                  "none",
377                                  "writethrough",
378                                  "writeback",
379                                  "directsync",
380                                  "unsafe",
381                                 ]
382         self._conn_supports_start_paused = CONF.libvirt.virt_type in ('kvm',
383                                                                       'qemu')
384 
385         for mode_str in CONF.libvirt.disk_cachemodes:
386             disk_type, sep, cache_mode = mode_str.partition('=')
387             if cache_mode not in self.valid_cachemodes:
388                 LOG.warning('Invalid cachemode %(cache_mode)s specified '
389                             'for disk type %(disk_type)s.',
390                             {'cache_mode': cache_mode, 'disk_type': disk_type})
391                 continue
392             self.disk_cachemodes[disk_type] = cache_mode
393 
394         self._volume_api = cinder.API()
395         self._image_api = image.API()
396 
397         # The default choice for the sysinfo_serial config option is "unique"
398         # which does not have a special function since the value is just the
399         # instance.uuid.
400         sysinfo_serial_funcs = {
401             'none': lambda: None,
402             'hardware': self._get_host_sysinfo_serial_hardware,
403             'os': self._get_host_sysinfo_serial_os,
404             'auto': self._get_host_sysinfo_serial_auto,
405         }
406 
407         self._sysinfo_serial_func = sysinfo_serial_funcs.get(
408             CONF.libvirt.sysinfo_serial)
409 
410         self.job_tracker = instancejobtracker.InstanceJobTracker()
411         self._remotefs = remotefs.RemoteFilesystem()
412 
413         self._live_migration_flags = self._block_migration_flags = 0
414         self.active_migrations = {}
415 
416         # Compute reserved hugepages from conf file at the very
417         # beginning to ensure any syntax error will be reported and
418         # avoid any re-calculation when computing resources.
419         self._reserved_hugepages = hardware.numa_get_reserved_huge_pages()
420 
421         # Copy of the compute service ProviderTree object that is updated
422         # every time update_provider_tree() is called.
423         # NOTE(sbauza): We only want a read-only cache, this attribute is not
424         # intended to be updatable directly
425         self.provider_tree = None
426 
427         # get pmem namespaces info
428         self._pmem_namespaces = self._get_pmem_namespaces()
429 
430     def _get_pmem_namespaces(self):
431         pmem_nss = []  # PMEMNamespace list
432         if CONF.libvirt.pmem_namespace_sizes:
433             pmem_nss_host = self._get_pmem_namespaces_on_host()
434             for ns_conf in CONF.libvirt.pmem_namespace_sizes:
435                 ns_name = ns_conf.split(":")[0].strip()
436                 ns_tag = ns_conf.split(":")[1].strip()
437                 if pmem_nss_host.get(ns_name):
438                     pmem_ns = pmem_nss_host[ns_name]._replace(tag=ns_tag)
439                     pmem_nss.append(pmem_ns)
440                 else:
441                     raise exception.PMEMNamespaceNotFound(namespace=ns_name)
442         return pmem_nss
443 
444     def _get_pmem_namespaces_on_host(self):
445         pmem_nss_host = {}
446         nss = jsonutils.loads(nova.privsep.libvirt.get_pmem_namespaces())
447         for ns in nss:
448             if not ns.get('name'):
449                 continue
450             pmem_nss_host[ns['name']] = \
451                 PMEMNamespace(tag=None,
452                               name=ns['name'],
453                               dev=ns['daxregion']['devices'][0]['chardev'],
454                               size=ns['size'],
455                               align=ns['daxregion']['align'],
456                               numa_node=ns['numa_node'])
457         return pmem_nss_host
458 
459     def _get_volume_drivers(self):
460         driver_registry = dict()
461 
462         for driver_str in libvirt_volume_drivers:
463             driver_type, _sep, driver = driver_str.partition('=')
464             driver_class = importutils.import_class(driver)
465             try:
466                 driver_registry[driver_type] = driver_class(self._host)
467             except brick_exception.InvalidConnectorProtocol:
468                 LOG.debug('Unable to load volume driver %s. It is not '
469                           'supported on this host.', driver)
470 
471         return driver_registry
472 
473     @property
474     def disk_cachemode(self):
475         # It can be confusing to understand the QEMU cache mode
476         # behaviour, because each cache=$MODE is a convenient shorthand
477         # to toggle _three_ cache.* booleans.  Consult the below table
478         # (quoting from the QEMU man page):
479         #
480         #              | cache.writeback | cache.direct | cache.no-flush
481         # --------------------------------------------------------------
482         # writeback    | on              | off          | off
483         # none         | on              | on           | off
484         # writethrough | off             | off          | off
485         # directsync   | off             | on           | off
486         # unsafe       | on              | off          | on
487         #
488         # Where:
489         #
490         #  - 'cache.writeback=off' means: QEMU adds an automatic fsync()
491         #    after each write request.
492         #
493         #  - 'cache.direct=on' means: Use Linux's O_DIRECT, i.e. bypass
494         #    the kernel page cache.  Caches in any other layer (disk
495         #    cache, QEMU metadata caches, etc.) can still be present.
496         #
497         #  - 'cache.no-flush=on' means: Ignore flush requests, i.e.
498         #    never call fsync(), even if the guest explicitly requested
499         #    it.
500         #
501         # Use cache mode "none" (cache.writeback=on, cache.direct=on,
502         # cache.no-flush=off) for consistent performance and
503         # migration correctness.  Some filesystems don't support
504         # O_DIRECT, though.  For those we fallback to the next
505         # reasonable option that is "writeback" (cache.writeback=on,
506         # cache.direct=off, cache.no-flush=off).
507 
508         if self._disk_cachemode is None:
509             self._disk_cachemode = "none"
510             if not nova.privsep.utils.supports_direct_io(CONF.instances_path):
511                 self._disk_cachemode = "writeback"
512         return self._disk_cachemode
513 
514     def _set_cache_mode(self, conf):
515         """Set cache mode on LibvirtConfigGuestDisk object."""
516         try:
517             source_type = conf.source_type
518             driver_cache = conf.driver_cache
519         except AttributeError:
520             return
521 
522         # Shareable disks like for a multi-attach volume need to have the
523         # driver cache disabled.
524         if getattr(conf, 'shareable', False):
525             conf.driver_cache = 'none'
526         else:
527             cache_mode = self.disk_cachemodes.get(source_type,
528                                                   driver_cache)
529             conf.driver_cache = cache_mode
530 
531     def _do_quality_warnings(self):
532         """Warn about potential configuration issues.
533 
534         This will log a warning message for things such as untested driver or
535         host arch configurations in order to indicate potential issues to
536         administrators.
537         """
538         caps = self._host.get_capabilities()
539         hostarch = caps.host.cpu.arch
540         if (CONF.libvirt.virt_type not in ('qemu', 'kvm') or
541             hostarch not in (fields.Architecture.I686,
542                              fields.Architecture.X86_64)):
543             LOG.warning('The libvirt driver is not tested on '
544                         '%(type)s/%(arch)s by the OpenStack project and '
545                         'thus its quality can not be ensured. For more '
546                         'information, see: https://docs.openstack.org/'
547                         'nova/latest/user/support-matrix.html',
548                         {'type': CONF.libvirt.virt_type, 'arch': hostarch})
549 
550         if CONF.vnc.keymap:
551             LOG.warning('The option "[vnc] keymap" has been deprecated '
552                         'in favor of configuration within the guest. '
553                         'Update nova.conf to address this change and '
554                         'refer to bug #1682020 for more information.')
555 
556         if CONF.spice.keymap:
557             LOG.warning('The option "[spice] keymap" has been deprecated '
558                         'in favor of configuration within the guest. '
559                         'Update nova.conf to address this change and '
560                         'refer to bug #1682020 for more information.')
561 
562     def _handle_conn_event(self, enabled, reason):
563         LOG.info("Connection event '%(enabled)d' reason '%(reason)s'",
564                  {'enabled': enabled, 'reason': reason})
565         self._set_host_enabled(enabled, reason)
566 
567     def init_host(self, host):
568         self._host.initialize()
569 
570         self._do_quality_warnings()
571 
572         self._parse_migration_flags()
573 
574         self._supported_perf_events = self._get_supported_perf_events()
575 
576         self._set_multiattach_support()
577 
578         self._check_file_backed_memory_support()
579 
580         if (CONF.libvirt.virt_type == 'lxc' and
581                 not (CONF.libvirt.uid_maps and CONF.libvirt.gid_maps)):
582             LOG.warning("Running libvirt-lxc without user namespaces is "
583                         "dangerous. Containers spawned by Nova will be run "
584                         "as the host's root user. It is highly suggested "
585                         "that user namespaces be used in a public or "
586                         "multi-tenant environment.")
587 
588         # Stop libguestfs using KVM unless we're also configured
589         # to use this. This solves problem where people need to
590         # stop Nova use of KVM because nested-virt is broken
591         if CONF.libvirt.virt_type != "kvm":
592             guestfs.force_tcg()
593 
594         if not self._host.has_min_version(MIN_LIBVIRT_VERSION):
595             raise exception.InternalError(
596                 _('Nova requires libvirt version %s or greater.') %
597                 libvirt_utils.version_to_string(MIN_LIBVIRT_VERSION))
598 
599         if CONF.libvirt.virt_type in ("qemu", "kvm"):
600             if self._host.has_min_version(hv_ver=MIN_QEMU_VERSION):
601                 # "qemu-img info" calls are version dependent, so we need to
602                 # store the version in the images module.
603                 images.QEMU_VERSION = self._host.get_connection().getVersion()
604             else:
605                 raise exception.InternalError(
606                     _('Nova requires QEMU version %s or greater.') %
607                     libvirt_utils.version_to_string(MIN_QEMU_VERSION))
608 
609         if CONF.libvirt.virt_type == 'parallels':
610             if not self._host.has_min_version(hv_ver=MIN_VIRTUOZZO_VERSION):
611                 raise exception.InternalError(
612                     _('Nova requires Virtuozzo version %s or greater.') %
613                     libvirt_utils.version_to_string(MIN_VIRTUOZZO_VERSION))
614 
615         # Give the cloud admin a heads up if we are intending to
616         # change the MIN_LIBVIRT_VERSION in the next release.
617         if not self._host.has_min_version(NEXT_MIN_LIBVIRT_VERSION):
618             LOG.warning('Running Nova with a libvirt version less than '
619                         '%(version)s is deprecated. The required minimum '
620                         'version of libvirt will be raised to %(version)s '
621                         'in the next release.',
622                         {'version': libvirt_utils.version_to_string(
623                             NEXT_MIN_LIBVIRT_VERSION)})
624         if (CONF.libvirt.virt_type in ("qemu", "kvm") and
625             not self._host.has_min_version(hv_ver=NEXT_MIN_QEMU_VERSION)):
626             LOG.warning('Running Nova with a QEMU version less than '
627                         '%(version)s is deprecated. The required minimum '
628                         'version of QEMU will be raised to %(version)s '
629                         'in the next release.',
630                         {'version': libvirt_utils.version_to_string(
631                             NEXT_MIN_QEMU_VERSION)})
632 
633         kvm_arch = fields.Architecture.from_host()
634         if (CONF.libvirt.virt_type in ('kvm', 'qemu') and
635             kvm_arch in MIN_LIBVIRT_OTHER_ARCH and
636                 not self._host.has_min_version(
637                     MIN_LIBVIRT_OTHER_ARCH.get(kvm_arch))):
638             raise exception.InternalError(
639                 _('Running Nova with qemu/kvm virt_type on %(arch)s '
640                   'requires libvirt version %(libvirt_ver)s or greater') %
641                 {'arch': kvm_arch,
642                  'libvirt_ver': libvirt_utils.version_to_string(
643                      MIN_LIBVIRT_OTHER_ARCH.get(kvm_arch))})
644 
645         # Allowing both "tunnelling via libvirtd" (which will be
646         # deprecated once the MIN_{LIBVIRT,QEMU}_VERSION is sufficiently
647         # new enough) and "native TLS" options at the same time is
648         # nonsensical.
649         if (CONF.libvirt.live_migration_tunnelled and
650                 CONF.libvirt.live_migration_with_native_tls):
651             msg = _("Setting both 'live_migration_tunnelled' and "
652                     "'live_migration_with_native_tls' at the same "
653                     "time is invalid. If you have the relevant "
654                     "libvirt and QEMU versions, and TLS configured "
655                     "in your environment, pick "
656                     "'live_migration_with_native_tls'.")
657             raise exception.Invalid(msg)
658 
659         # Some imagebackends are only able to import raw disk images,
660         # and will fail if given any other format. See the bug
661         # https://bugs.launchpad.net/nova/+bug/1816686 for more details.
662         if CONF.libvirt.images_type in ('rbd',):
663             if not CONF.force_raw_images:
664                 msg = _("'[DEFAULT]/force_raw_images = False' is not "
665                         "allowed with '[libvirt]/images_type = rbd'. "
666                         "Please check the two configs and if you really "
667                         "do want to use rbd as images_type, set "
668                         "force_raw_images to True.")
669                 raise exception.InvalidConfiguration(msg)
670 
671         # TODO(sbauza): Remove this code once mediated devices are persisted
672         # across reboots.
673         if self._host.has_min_version(MIN_LIBVIRT_MDEV_SUPPORT):
674             self._recreate_assigned_mediated_devices()
675 
676     @staticmethod
677     def _is_existing_mdev(uuid):
678         # FIXME(sbauza): Some kernel can have a uevent race meaning that the
679         # libvirt daemon won't know when a mediated device is created unless
680         # you restart that daemon. Until all kernels we support are not having
681         # that possible race, check the sysfs directly instead of asking the
682         # libvirt API.
683         # See https://bugzilla.redhat.com/show_bug.cgi?id=1376907 for ref.
684         return os.path.exists('/sys/bus/mdev/devices/{0}'.format(uuid))
685 
686     def _recreate_assigned_mediated_devices(self):
687         """Recreate assigned mdevs that could have disappeared if we reboot
688         the host.
689         """
690         # FIXME(sbauza): We blindly recreate mediated devices without checking
691         # which ResourceProvider was allocated for the instance so it would use
692         # another pGPU.
693         # TODO(sbauza): Pass all instances' allocations here.
694         mdevs = self._get_all_assigned_mediated_devices()
695         requested_types = self._get_supported_vgpu_types()
696         for (mdev_uuid, instance_uuid) in six.iteritems(mdevs):
697             if not self._is_existing_mdev(mdev_uuid):
698                 self._create_new_mediated_device(requested_types, mdev_uuid)
699 
700     def _set_multiattach_support(self):
701         # Check to see if multiattach is supported. Based on bugzilla
702         # https://bugzilla.redhat.com/show_bug.cgi?id=1378242 and related
703         # clones, the shareable flag on a disk device will only work with
704         # qemu<2.10 or libvirt>=3.10. So check those versions here and set
705         # the capability appropriately.
706         if (self._host.has_min_version(lv_ver=MIN_LIBVIRT_MULTIATTACH) or
707                 not self._host.has_min_version(hv_ver=(2, 10, 0))):
708             self.capabilities['supports_multiattach'] = True
709         else:
710             LOG.debug('Volume multiattach is not supported based on current '
711                       'versions of QEMU and libvirt. QEMU must be less than '
712                       '2.10 or libvirt must be greater than or equal to 3.10.')
713 
714     def _check_file_backed_memory_support(self):
715         if CONF.libvirt.file_backed_memory:
716             # file_backed_memory is only compatible with qemu/kvm virts
717             if CONF.libvirt.virt_type not in ("qemu", "kvm"):
718                 raise exception.InternalError(
719                     _('Running Nova with file_backed_memory and virt_type '
720                       '%(type)s is not supported. file_backed_memory is only '
721                       'supported with qemu and kvm types.') %
722                     {'type': CONF.libvirt.virt_type})
723 
724             # Check needed versions for file_backed_memory
725             if not self._host.has_min_version(
726                     MIN_LIBVIRT_FILE_BACKED_VERSION,
727                     MIN_QEMU_FILE_BACKED_VERSION):
728                 raise exception.InternalError(
729                     _('Running Nova with file_backed_memory requires libvirt '
730                       'version %(libvirt)s and qemu version %(qemu)s') %
731                     {'libvirt': libvirt_utils.version_to_string(
732                         MIN_LIBVIRT_FILE_BACKED_VERSION),
733                     'qemu': libvirt_utils.version_to_string(
734                         MIN_QEMU_FILE_BACKED_VERSION)})
735 
736             # file-backed memory doesn't work with memory overcommit.
737             # Block service startup if file-backed memory is enabled and
738             # ram_allocation_ratio is not 1.0
739             if CONF.ram_allocation_ratio != 1.0:
740                 raise exception.InternalError(
741                     'Running Nova with file_backed_memory requires '
742                     'ram_allocation_ratio configured to 1.0')
743 
744     def _prepare_migration_flags(self):
745         migration_flags = 0
746 
747         migration_flags |= libvirt.VIR_MIGRATE_LIVE
748 
749         # Adding p2p flag only if xen is not in use, because xen does not
750         # support p2p migrations
751         if CONF.libvirt.virt_type != 'xen':
752             migration_flags |= libvirt.VIR_MIGRATE_PEER2PEER
753 
754         # Adding VIR_MIGRATE_UNDEFINE_SOURCE because, without it, migrated
755         # instance will remain defined on the source host
756         migration_flags |= libvirt.VIR_MIGRATE_UNDEFINE_SOURCE
757 
758         # Adding VIR_MIGRATE_PERSIST_DEST to persist the VM on the
759         # destination host
760         migration_flags |= libvirt.VIR_MIGRATE_PERSIST_DEST
761 
762         live_migration_flags = block_migration_flags = migration_flags
763 
764         # Adding VIR_MIGRATE_NON_SHARED_INC, otherwise all block-migrations
765         # will be live-migrations instead
766         block_migration_flags |= libvirt.VIR_MIGRATE_NON_SHARED_INC
767 
768         return (live_migration_flags, block_migration_flags)
769 
770     # TODO(kchamart) Once the MIN_LIBVIRT_VERSION and MIN_QEMU_VERSION
771     # reach 4.4.0 and 2.11.0, which provide "native TLS" support by
772     # default, deprecate and remove the support for "tunnelled live
773     # migration" (and related config attribute), because:
774     #
775     #  (a) it cannot handle live migration of disks in a non-shared
776     #      storage setup (a.k.a. "block migration");
777     #
778     #  (b) has a huge performance overhead and latency, because it burns
779     #      more CPU and memory bandwidth due to increased number of data
780     #      copies on both source and destination hosts.
781     #
782     # Both the above limitations are addressed by the QEMU-native TLS
783     # support (`live_migration_with_native_tls`).
784     def _handle_live_migration_tunnelled(self, migration_flags):
785         if CONF.libvirt.live_migration_tunnelled:
786             migration_flags |= libvirt.VIR_MIGRATE_TUNNELLED
787         return migration_flags
788 
789     def _is_native_tls_available(self):
790         return self._host.has_min_version(MIN_LIBVIRT_NATIVE_TLS_VERSION,
791                                           MIN_QEMU_NATIVE_TLS_VERSION)
792 
793     def _handle_native_tls(self, migration_flags):
794         if (CONF.libvirt.live_migration_with_native_tls and
795                 self._is_native_tls_available()):
796             migration_flags |= libvirt.VIR_MIGRATE_TLS
797         return migration_flags
798 
799     def _is_native_luks_available(self):
800         return self._host.has_min_version(MIN_LIBVIRT_LUKS_VERSION,
801                                           MIN_QEMU_LUKS_VERSION)
802 
803     def _handle_live_migration_post_copy(self, migration_flags):
804         if CONF.libvirt.live_migration_permit_post_copy:
805             migration_flags |= libvirt.VIR_MIGRATE_POSTCOPY
806         return migration_flags
807 
808     def _handle_live_migration_auto_converge(self, migration_flags):
809         if self._is_post_copy_enabled(migration_flags):
810             LOG.info('The live_migration_permit_post_copy is set to '
811                      'True and post copy live migration is available '
812                      'so auto-converge will not be in use.')
813         elif CONF.libvirt.live_migration_permit_auto_converge:
814             migration_flags |= libvirt.VIR_MIGRATE_AUTO_CONVERGE
815         return migration_flags
816 
817     def _parse_migration_flags(self):
818         (live_migration_flags,
819             block_migration_flags) = self._prepare_migration_flags()
820 
821         live_migration_flags = self._handle_live_migration_tunnelled(
822             live_migration_flags)
823         block_migration_flags = self._handle_live_migration_tunnelled(
824             block_migration_flags)
825 
826         live_migration_flags = self._handle_native_tls(
827             live_migration_flags)
828         block_migration_flags = self._handle_native_tls(
829             block_migration_flags)
830 
831         live_migration_flags = self._handle_live_migration_post_copy(
832             live_migration_flags)
833         block_migration_flags = self._handle_live_migration_post_copy(
834             block_migration_flags)
835 
836         live_migration_flags = self._handle_live_migration_auto_converge(
837             live_migration_flags)
838         block_migration_flags = self._handle_live_migration_auto_converge(
839             block_migration_flags)
840 
841         self._live_migration_flags = live_migration_flags
842         self._block_migration_flags = block_migration_flags
843 
844     # TODO(sahid): This method is targeted for removal when the tests
845     # have been updated to avoid its use
846     #
847     # All libvirt API calls on the libvirt.Connect object should be
848     # encapsulated by methods on the nova.virt.libvirt.host.Host
849     # object, rather than directly invoking the libvirt APIs. The goal
850     # is to avoid a direct dependency on the libvirt API from the
851     # driver.py file.
852     def _get_connection(self):
853         return self._host.get_connection()
854 
855     _conn = property(_get_connection)
856 
857     @staticmethod
858     def _uri():
859         if CONF.libvirt.virt_type == 'uml':
860             uri = CONF.libvirt.connection_uri or 'uml:///system'
861         elif CONF.libvirt.virt_type == 'xen':
862             uri = CONF.libvirt.connection_uri or 'xen:///'
863         elif CONF.libvirt.virt_type == 'lxc':
864             uri = CONF.libvirt.connection_uri or 'lxc:///'
865         elif CONF.libvirt.virt_type == 'parallels':
866             uri = CONF.libvirt.connection_uri or 'parallels:///system'
867         else:
868             uri = CONF.libvirt.connection_uri or 'qemu:///system'
869         return uri
870 
871     @staticmethod
872     def _live_migration_uri(dest):
873         uris = {
874             'kvm': 'qemu+%s://%s/system',
875             'qemu': 'qemu+%s://%s/system',
876             'xen': 'xenmigr://%s/system',
877             'parallels': 'parallels+tcp://%s/system',
878         }
879         dest = oslo_netutils.escape_ipv6(dest)
880 
881         virt_type = CONF.libvirt.virt_type
882         # TODO(pkoniszewski): Remove fetching live_migration_uri in Pike
883         uri = CONF.libvirt.live_migration_uri
884         if uri:
885             return uri % dest
886 
887         uri = uris.get(virt_type)
888         if uri is None:
889             raise exception.LiveMigrationURINotAvailable(virt_type=virt_type)
890 
891         str_format = (dest,)
892         if virt_type in ('kvm', 'qemu'):
893             scheme = CONF.libvirt.live_migration_scheme or 'tcp'
894             str_format = (scheme, dest)
895         return uris.get(virt_type) % str_format
896 
897     @staticmethod
898     def _migrate_uri(dest):
899         uri = None
900         dest = oslo_netutils.escape_ipv6(dest)
901 
902         # Only QEMU live migrations supports migrate-uri parameter
903         virt_type = CONF.libvirt.virt_type
904         if virt_type in ('qemu', 'kvm'):
905             # QEMU accept two schemes: tcp and rdma.  By default
906             # libvirt build the URI using the remote hostname and the
907             # tcp schema.
908             uri = 'tcp://%s' % dest
909         # Because dest might be of type unicode, here we might return value of
910         # type unicode as well which is not acceptable by libvirt python
911         # binding when Python 2.7 is in use, so let's convert it explicitly
912         # back to string. When Python 3.x is in use, libvirt python binding
913         # accepts unicode type so it is completely fine to do a no-op str(uri)
914         # conversion which will return value of type unicode.
915         return uri and str(uri)
916 
917     def instance_exists(self, instance):
918         """Efficient override of base instance_exists method."""
919         try:
920             self._host.get_guest(instance)
921             return True
922         except (exception.InternalError, exception.InstanceNotFound):
923             return False
924 
925     def estimate_instance_overhead(self, instance_info):
926         overhead = super(LibvirtDriver, self).estimate_instance_overhead(
927             instance_info)
928         if isinstance(instance_info, objects.Flavor):
929             # A flavor object is passed during case of migrate
930             emu_policy = hardware.get_emulator_thread_policy_constraint(
931                 instance_info)
932             if emu_policy == fields.CPUEmulatorThreadsPolicy.ISOLATE:
933                 overhead['vcpus'] += 1
934         else:
935             # An instance object is passed during case of spawing or a
936             # dict is passed when computing resource for an instance
937             numa_topology = hardware.instance_topology_from_instance(
938                 instance_info)
939             if numa_topology and numa_topology.emulator_threads_isolated:
940                 overhead['vcpus'] += 1
941         return overhead
942 
943     def list_instances(self):
944         names = []
945         for guest in self._host.list_guests(only_running=False):
946             names.append(guest.name)
947 
948         return names
949 
950     def list_instance_uuids(self):
951         uuids = []
952         for guest in self._host.list_guests(only_running=False):
953             uuids.append(guest.uuid)
954 
955         return uuids
956 
957     def plug_vifs(self, instance, network_info):
958         """Plug VIFs into networks."""
959         for vif in network_info:
960             self.vif_driver.plug(instance, vif)
961 
962     def _unplug_vifs(self, instance, network_info, ignore_errors):
963         """Unplug VIFs from networks."""
964         for vif in network_info:
965             try:
966                 self.vif_driver.unplug(instance, vif)
967             except exception.NovaException:
968                 if not ignore_errors:
969                     raise
970 
971     def unplug_vifs(self, instance, network_info):
972         self._unplug_vifs(instance, network_info, False)
973 
974     def _teardown_container(self, instance):
975         inst_path = libvirt_utils.get_instance_path(instance)
976         container_dir = os.path.join(inst_path, 'rootfs')
977         rootfs_dev = instance.system_metadata.get('rootfs_device_name')
978         LOG.debug('Attempting to teardown container at path %(dir)s with '
979                   'root device: %(rootfs_dev)s',
980                   {'dir': container_dir, 'rootfs_dev': rootfs_dev},
981                   instance=instance)
982         disk_api.teardown_container(container_dir, rootfs_dev)
983 
984     def _destroy(self, instance, attempt=1):
985         try:
986             guest = self._host.get_guest(instance)
987             if CONF.serial_console.enabled:
988                 # This method is called for several events: destroy,
989                 # rebuild, hard-reboot, power-off - For all of these
990                 # events we want to release the serial ports acquired
991                 # for the guest before destroying it.
992                 serials = self._get_serial_ports_from_guest(guest)
993                 for hostname, port in serials:
994                     serial_console.release_port(host=hostname, port=port)
995         except exception.InstanceNotFound:
996             guest = None
997 
998         # If the instance is already terminated, we're still happy
999         # Otherwise, destroy it
1000         old_domid = -1
1001         if guest is not None:
1002             try:
1003                 old_domid = guest.id
1004                 guest.poweroff()
1005 
1006             except libvirt.libvirtError as e:
1007                 is_okay = False
1008                 errcode = e.get_error_code()
1009                 if errcode == libvirt.VIR_ERR_NO_DOMAIN:
1010                     # Domain already gone. This can safely be ignored.
1011                     is_okay = True
1012                 elif errcode == libvirt.VIR_ERR_OPERATION_INVALID:
1013                     # If the instance is already shut off, we get this:
1014                     # Code=55 Error=Requested operation is not valid:
1015                     # domain is not running
1016 
1017                     state = guest.get_power_state(self._host)
1018                     if state == power_state.SHUTDOWN:
1019                         is_okay = True
1020                 elif errcode == libvirt.VIR_ERR_INTERNAL_ERROR:
1021                     errmsg = e.get_error_message()
1022                     if (CONF.libvirt.virt_type == 'lxc' and
1023                         errmsg == 'internal error: '
1024                                   'Some processes refused to die'):
1025                         # Some processes in the container didn't die
1026                         # fast enough for libvirt. The container will
1027                         # eventually die. For now, move on and let
1028                         # the wait_for_destroy logic take over.
1029                         is_okay = True
1030                 elif errcode == libvirt.VIR_ERR_OPERATION_TIMEOUT:
1031                     LOG.warning("Cannot destroy instance, operation time out",
1032                                 instance=instance)
1033                     reason = _("operation time out")
1034                     raise exception.InstancePowerOffFailure(reason=reason)
1035                 elif errcode == libvirt.VIR_ERR_SYSTEM_ERROR:
1036                     if e.get_int1() == errno.EBUSY:
1037                         # NOTE(danpb): When libvirt kills a process it sends it
1038                         # SIGTERM first and waits 10 seconds. If it hasn't gone
1039                         # it sends SIGKILL and waits another 5 seconds. If it
1040                         # still hasn't gone then you get this EBUSY error.
1041                         # Usually when a QEMU process fails to go away upon
1042                         # SIGKILL it is because it is stuck in an
1043                         # uninterruptible kernel sleep waiting on I/O from
1044                         # some non-responsive server.
1045                         # Given the CPU load of the gate tests though, it is
1046                         # conceivable that the 15 second timeout is too short,
1047                         # particularly if the VM running tempest has a high
1048                         # steal time from the cloud host. ie 15 wallclock
1049                         # seconds may have passed, but the VM might have only
1050                         # have a few seconds of scheduled run time.
1051                         #
1052                         # TODO(kchamart): Once MIN_LIBVIRT_VERSION
1053                         # reaches v4.7.0, (a) rewrite the above note,
1054                         # and (b) remove the following code that retries
1055                         # _destroy() API call (which gives SIGKILL 30
1056                         # seconds to take effect) -- because from v4.7.0
1057                         # onwards, libvirt _automatically_ increases the
1058                         # timeout to 30 seconds.  This was added in the
1059                         # following libvirt commits:
1060                         #
1061                         #   - 9a4e4b942 (process: wait longer 5->30s on
1062                         #     hard shutdown)
1063                         #
1064                         #   - be2ca0444 (process: wait longer on kill
1065                         #     per assigned Hostdev)
1066                         with excutils.save_and_reraise_exception() as ctxt:
1067                             if not self._host.has_min_version(
1068                                     MIN_LIBVIRT_BETTER_SIGKILL_HANDLING):
1069                                 LOG.warning('Error from libvirt during '
1070                                             'destroy. Code=%(errcode)s '
1071                                             'Error=%(e)s; attempt '
1072                                             '%(attempt)d of 6 ',
1073                                             {'errcode': errcode, 'e': e,
1074                                              'attempt': attempt},
1075                                             instance=instance)
1076                                 # Try up to 6 times before giving up.
1077                                 if attempt < 6:
1078                                     ctxt.reraise = False
1079                                     self._destroy(instance, attempt + 1)
1080                                     return
1081 
1082                 if not is_okay:
1083                     with excutils.save_and_reraise_exception():
1084                         LOG.error('Error from libvirt during destroy. '
1085                                   'Code=%(errcode)s Error=%(e)s',
1086                                   {'errcode': errcode, 'e': e},
1087                                   instance=instance)
1088 
1089         def _wait_for_destroy(expected_domid):
1090             """Called at an interval until the VM is gone."""
1091             # NOTE(vish): If the instance disappears during the destroy
1092             #             we ignore it so the cleanup can still be
1093             #             attempted because we would prefer destroy to
1094             #             never fail.
1095             try:
1096                 dom_info = self.get_info(instance)
1097                 state = dom_info.state
1098                 new_domid = dom_info.internal_id
1099             except exception.InstanceNotFound:
1100                 LOG.debug("During wait destroy, instance disappeared.",
1101                           instance=instance)
1102                 state = power_state.SHUTDOWN
1103 
1104             if state == power_state.SHUTDOWN:
1105                 LOG.info("Instance destroyed successfully.", instance=instance)
1106                 raise loopingcall.LoopingCallDone()
1107 
1108             # NOTE(wangpan): If the instance was booted again after destroy,
1109             #                this may be an endless loop, so check the id of
1110             #                domain here, if it changed and the instance is
1111             #                still running, we should destroy it again.
1112             # see https://bugs.launchpad.net/nova/+bug/1111213 for more details
1113             if new_domid != expected_domid:
1114                 LOG.info("Instance may be started again.", instance=instance)
1115                 kwargs['is_running'] = True
1116                 raise loopingcall.LoopingCallDone()
1117 
1118         kwargs = {'is_running': False}
1119         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_destroy,
1120                                                      old_domid)
1121         timer.start(interval=0.5).wait()
1122         if kwargs['is_running']:
1123             LOG.info("Going to destroy instance again.", instance=instance)
1124             self._destroy(instance)
1125         else:
1126             # NOTE(GuanQiang): teardown container to avoid resource leak
1127             if CONF.libvirt.virt_type == 'lxc':
1128                 self._teardown_container(instance)
1129 
1130     def destroy(self, context, instance, network_info, block_device_info=None,
1131                 destroy_disks=True):
1132         self._destroy(instance)
1133         self.cleanup(context, instance, network_info, block_device_info,
1134                      destroy_disks)
1135 
1136     def _undefine_domain(self, instance):
1137         try:
1138             guest = self._host.get_guest(instance)
1139             try:
1140                 support_uefi = self._has_uefi_support()
1141                 guest.delete_configuration(support_uefi)
1142             except libvirt.libvirtError as e:
1143                 with excutils.save_and_reraise_exception() as ctxt:
1144                     errcode = e.get_error_code()
1145                     if errcode == libvirt.VIR_ERR_NO_DOMAIN:
1146                         LOG.debug("Called undefine, but domain already gone.",
1147                                   instance=instance)
1148                         ctxt.reraise = False
1149                     else:
1150                         LOG.error('Error from libvirt during undefine. '
1151                                   'Code=%(errcode)s Error=%(e)s',
1152                                   {'errcode': errcode,
1153                                    'e': encodeutils.exception_to_unicode(e)},
1154                                   instance=instance)
1155         except exception.InstanceNotFound:
1156             pass
1157 
1158     def cleanup(self, context, instance, network_info, block_device_info=None,
1159                 destroy_disks=True, migrate_data=None, destroy_vifs=True):
1160         if destroy_vifs:
1161             self._unplug_vifs(instance, network_info, True)
1162 
1163         # Continue attempting to remove firewall filters for the instance
1164         # until it's done or there is a failure to remove the filters. If
1165         # unfilter fails because the instance is not yet shutdown, try to
1166         # destroy the guest again and then retry the unfilter.
1167         while True:
1168             try:
1169                 self.unfilter_instance(instance, network_info)
1170                 break
1171             except libvirt.libvirtError as e:
1172                 try:
1173                     state = self.get_info(instance).state
1174                 except exception.InstanceNotFound:
1175                     state = power_state.SHUTDOWN
1176 
1177                 if state != power_state.SHUTDOWN:
1178                     LOG.warning("Instance may be still running, destroy "
1179                                 "it again.", instance=instance)
1180                     self._destroy(instance)
1181                 else:
1182                     errcode = e.get_error_code()
1183                     LOG.exception(_('Error from libvirt during unfilter. '
1184                                     'Code=%(errcode)s Error=%(e)s'),
1185                                   {'errcode': errcode, 'e': e},
1186                                   instance=instance)
1187                     reason = _("Error unfiltering instance.")
1188                     raise exception.InstanceTerminationFailure(reason=reason)
1189             except Exception:
1190                 raise
1191 
1192         # FIXME(wangpan): if the instance is booted again here, such as the
1193         #                 soft reboot operation boot it here, it will become
1194         #                 "running deleted", should we check and destroy it
1195         #                 at the end of this method?
1196 
1197         # NOTE(vish): we disconnect from volumes regardless
1198         block_device_mapping = driver.block_device_info_get_mapping(
1199             block_device_info)
1200         for vol in block_device_mapping:
1201             connection_info = vol['connection_info']
1202             disk_dev = vol['mount_device']
1203             if disk_dev is not None:
1204                 disk_dev = disk_dev.rpartition("/")[2]
1205             try:
1206                 self._disconnect_volume(context, connection_info, instance)
1207             except Exception as exc:
1208                 with excutils.save_and_reraise_exception() as ctxt:
1209                     if destroy_disks:
1210                         # Don't block on Volume errors if we're trying to
1211                         # delete the instance as we may be partially created
1212                         # or deleted
1213                         ctxt.reraise = False
1214                         LOG.warning(
1215                             "Ignoring Volume Error on vol %(vol_id)s "
1216                             "during delete %(exc)s",
1217                             {'vol_id': vol.get('volume_id'),
1218                              'exc': encodeutils.exception_to_unicode(exc)},
1219                             instance=instance)
1220 
1221         if destroy_disks:
1222             # NOTE(haomai): destroy volumes if needed
1223             if CONF.libvirt.images_type == 'lvm':
1224                 self._cleanup_lvm(instance, block_device_info)
1225             if CONF.libvirt.images_type == 'rbd':
1226                 self._cleanup_rbd(instance)
1227 
1228         is_shared_block_storage = False
1229         if migrate_data and 'is_shared_block_storage' in migrate_data:
1230             is_shared_block_storage = migrate_data.is_shared_block_storage
1231         # NOTE(lyarwood): The following workaround allows operators to ensure
1232         # that non-shared instance directories are removed after an evacuation
1233         # or revert resize when using the shared RBD imagebackend. This
1234         # workaround is not required when cleaning up migrations that provide
1235         # migrate_data to this method as the existing is_shared_block_storage
1236         # conditional will cause the instance directory to be removed.
1237         if ((destroy_disks or is_shared_block_storage) or
1238             (CONF.workarounds.ensure_libvirt_rbd_instance_dir_cleanup and
1239              CONF.libvirt.images_type == 'rbd')):
1240 
1241             attempts = int(instance.system_metadata.get('clean_attempts',
1242                                                         '0'))
1243             success = self.delete_instance_files(instance)
1244             # NOTE(mriedem): This is used in the _run_pending_deletes periodic
1245             # task in the compute manager. The tight coupling is not great...
1246             instance.system_metadata['clean_attempts'] = str(attempts + 1)
1247             if success:
1248                 instance.cleaned = True
1249             instance.save()
1250 
1251         self._undefine_domain(instance)
1252 
1253     def _detach_encrypted_volumes(self, instance, block_device_info):
1254         """Detaches encrypted volumes attached to instance."""
1255         disks = self._get_instance_disk_info(instance, block_device_info)
1256         encrypted_volumes = filter(dmcrypt.is_encrypted,
1257                                    [disk['path'] for disk in disks])
1258         for path in encrypted_volumes:
1259             dmcrypt.delete_volume(path)
1260 
1261     def _get_serial_ports_from_guest(self, guest, mode=None):
1262         """Returns an iterator over serial port(s) configured on guest.
1263 
1264         :param mode: Should be a value in (None, bind, connect)
1265         """
1266         xml = guest.get_xml_desc()
1267         tree = etree.fromstring(xml)
1268 
1269         # The 'serial' device is the base for x86 platforms. Other platforms
1270         # (e.g. kvm on system z = S390X) can only use 'console' devices.
1271         xpath_mode = "[@mode='%s']" % mode if mode else ""
1272         serial_tcp = "./devices/serial[@type='tcp']/source" + xpath_mode
1273         console_tcp = "./devices/console[@type='tcp']/source" + xpath_mode
1274 
1275         tcp_devices = tree.findall(serial_tcp)
1276         if len(tcp_devices) == 0:
1277             tcp_devices = tree.findall(console_tcp)
1278         for source in tcp_devices:
1279             yield (source.get("host"), int(source.get("service")))
1280 
1281     def _get_scsi_controller_max_unit(self, guest):
1282         """Returns the max disk unit used by scsi controller"""
1283         xml = guest.get_xml_desc()
1284         tree = etree.fromstring(xml)
1285         addrs = "./devices/disk[@device='disk']/address[@type='drive']"
1286 
1287         ret = []
1288         for obj in tree.findall(addrs):
1289             ret.append(int(obj.get('unit', 0)))
1290         return max(ret)
1291 
1292     @staticmethod
1293     def _get_rbd_driver():
1294         return rbd_utils.RBDDriver(
1295                 pool=CONF.libvirt.images_rbd_pool,
1296                 ceph_conf=CONF.libvirt.images_rbd_ceph_conf,
1297                 rbd_user=CONF.libvirt.rbd_user)
1298 
1299     def _cleanup_rbd(self, instance):
1300         # NOTE(nic): On revert_resize, the cleanup steps for the root
1301         # volume are handled with an "rbd snap rollback" command,
1302         # and none of this is needed (and is, in fact, harmful) so
1303         # filter out non-ephemerals from the list
1304         if instance.task_state == task_states.RESIZE_REVERTING:
1305             filter_fn = lambda disk: (disk.startswith(instance.uuid) and
1306                                       disk.endswith('disk.local'))
1307         else:
1308             filter_fn = lambda disk: disk.startswith(instance.uuid)
1309         LibvirtDriver._get_rbd_driver().cleanup_volumes(filter_fn)
1310 
1311     def _cleanup_lvm(self, instance, block_device_info):
1312         """Delete all LVM disks for given instance object."""
1313         if instance.get('ephemeral_key_uuid') is not None:
1314             self._detach_encrypted_volumes(instance, block_device_info)
1315 
1316         disks = self._lvm_disks(instance)
1317         if disks:
1318             lvm.remove_volumes(disks)
1319 
1320     def _lvm_disks(self, instance):
1321         """Returns all LVM disks for given instance object."""
1322         if CONF.libvirt.images_volume_group:
1323             vg = os.path.join('/dev', CONF.libvirt.images_volume_group)
1324             if not os.path.exists(vg):
1325                 return []
1326             pattern = '%s_' % instance.uuid
1327 
1328             def belongs_to_instance(disk):
1329                 return disk.startswith(pattern)
1330 
1331             def fullpath(name):
1332                 return os.path.join(vg, name)
1333 
1334             logical_volumes = lvm.list_volumes(vg)
1335 
1336             disks = [fullpath(disk) for disk in logical_volumes
1337                      if belongs_to_instance(disk)]
1338             return disks
1339         return []
1340 
1341     def get_volume_connector(self, instance):
1342         root_helper = utils.get_root_helper()
1343         return connector.get_connector_properties(
1344             root_helper, CONF.my_block_storage_ip,
1345             CONF.libvirt.volume_use_multipath,
1346             enforce_multipath=True,
1347             host=CONF.host)
1348 
1349     def _cleanup_resize(self, context, instance, network_info):
1350         inst_base = libvirt_utils.get_instance_path(instance)
1351         target = inst_base + '_resize'
1352 
1353         # Deletion can fail over NFS, so retry the deletion as required.
1354         # Set maximum attempt as 5, most test can remove the directory
1355         # for the second time.
1356         attempts = 0
1357         while(os.path.exists(target) and attempts < 5):
1358             shutil.rmtree(target, ignore_errors=True)
1359             if os.path.exists(target):
1360                 time.sleep(random.randint(20, 200) / 100.0)
1361             attempts += 1
1362 
1363         # NOTE(mriedem): Some image backends will recreate the instance path
1364         # and disk.info during init, and all we need the root disk for
1365         # here is removing cloned snapshots which is backend-specific, so
1366         # check that first before initializing the image backend object. If
1367         # there is ever an image type that supports clone *and* re-creates
1368         # the instance directory and disk.info on init, this condition will
1369         # need to be re-visited to make sure that backend doesn't re-create
1370         # the disk. Refer to bugs: 1666831 1728603 1769131
1371         if self.image_backend.backend(CONF.libvirt.images_type).SUPPORTS_CLONE:
1372             root_disk = self.image_backend.by_name(instance, 'disk')
1373             if root_disk.exists():
1374                 root_disk.remove_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME)
1375 
1376         if instance.host != CONF.host:
1377             self._undefine_domain(instance)
1378             self.unplug_vifs(instance, network_info)
1379             self.unfilter_instance(instance, network_info)
1380 
1381     def _get_volume_driver(self, connection_info):
1382         driver_type = connection_info.get('driver_volume_type')
1383         if driver_type not in self.volume_drivers:
1384             raise exception.VolumeDriverNotFound(driver_type=driver_type)
1385         return self.volume_drivers[driver_type]
1386 
1387     def _connect_volume(self, context, connection_info, instance,
1388                         encryption=None, allow_native_luks=True):
1389         vol_driver = self._get_volume_driver(connection_info)
1390         vol_driver.connect_volume(connection_info, instance)
1391         try:
1392             self._attach_encryptor(
1393                 context, connection_info, encryption, allow_native_luks)
1394         except Exception:
1395             # Encryption failed so rollback the volume connection.
1396             with excutils.save_and_reraise_exception(logger=LOG):
1397                 LOG.exception("Failure attaching encryptor; rolling back "
1398                               "volume connection", instance=instance)
1399                 vol_driver.disconnect_volume(connection_info, instance)
1400 
1401     def _should_disconnect_target(self, context, connection_info, instance):
1402         connection_count = 0
1403 
1404         # NOTE(jdg): Multiattach is a special case (not to be confused
1405         # with shared_targets). With multiattach we may have a single volume
1406         # attached multiple times to *this* compute node (ie Server-1 and
1407         # Server-2).  So, if we receive a call to delete the attachment for
1408         # Server-1 we need to take special care to make sure that the Volume
1409         # isn't also attached to another Server on this Node.  Otherwise we
1410         # will indiscriminantly delete the connection for all Server and that's
1411         # no good.  So check if it's attached multiple times on this node
1412         # if it is we skip the call to brick to delete the connection.
1413         if connection_info.get('multiattach', False):
1414             volume = self._volume_api.get(
1415                 context,
1416                 driver_block_device.get_volume_id(connection_info))
1417             attachments = volume.get('attachments', {})
1418             if len(attachments) > 1:
1419                 # First we get a list of all Server UUID's associated with
1420                 # this Host (Compute Node).  We're going to use this to
1421                 # determine if the Volume being detached is also in-use by
1422                 # another Server on this Host, ie just check to see if more
1423                 # than one attachment.server_id for this volume is in our
1424                 # list of Server UUID's for this Host
1425                 servers_this_host = objects.InstanceList.get_uuids_by_host(
1426                     context, instance.host)
1427 
1428                 # NOTE(jdg): nova.volume.cinder translates the
1429                 # volume['attachments'] response into a dict which includes
1430                 # the Server UUID as the key, so we're using that
1431                 # here to check against our server_this_host list
1432                 for server_id, data in attachments.items():
1433                     if server_id in servers_this_host:
1434                         connection_count += 1
1435         return (False if connection_count > 1 else True)
1436 
1437     def _disconnect_volume(self, context, connection_info, instance,
1438                            encryption=None):
1439         self._detach_encryptor(context, connection_info, encryption=encryption)
1440         if self._should_disconnect_target(context, connection_info, instance):
1441             vol_driver = self._get_volume_driver(connection_info)
1442             vol_driver.disconnect_volume(connection_info, instance)
1443         else:
1444             LOG.info("Detected multiple connections on this host for volume: "
1445                      "%s, skipping target disconnect.",
1446                      driver_block_device.get_volume_id(connection_info),
1447                      instance=instance)
1448 
1449     def _extend_volume(self, connection_info, instance, requested_size):
1450         vol_driver = self._get_volume_driver(connection_info)
1451         return vol_driver.extend_volume(connection_info, instance,
1452                                         requested_size)
1453 
1454     def _use_native_luks(self, encryption=None):
1455         """Is LUKS the required provider and native QEMU LUKS available
1456         """
1457         provider = None
1458         if encryption:
1459             provider = encryption.get('provider', None)
1460         if provider in encryptors.LEGACY_PROVIDER_CLASS_TO_FORMAT_MAP:
1461             provider = encryptors.LEGACY_PROVIDER_CLASS_TO_FORMAT_MAP[provider]
1462         return provider == encryptors.LUKS and self._is_native_luks_available()
1463 
1464     def _get_volume_config(self, connection_info, disk_info):
1465         vol_driver = self._get_volume_driver(connection_info)
1466         conf = vol_driver.get_config(connection_info, disk_info)
1467         self._set_cache_mode(conf)
1468         return conf
1469 
1470     def _get_volume_encryptor(self, connection_info, encryption):
1471         root_helper = utils.get_root_helper()
1472         return encryptors.get_volume_encryptor(root_helper=root_helper,
1473                                                keymgr=key_manager.API(CONF),
1474                                                connection_info=connection_info,
1475                                                **encryption)
1476 
1477     def _get_volume_encryption(self, context, connection_info):
1478         """Get the encryption metadata dict if it is not provided
1479         """
1480         encryption = {}
1481         volume_id = driver_block_device.get_volume_id(connection_info)
1482         if volume_id:
1483             encryption = encryptors.get_encryption_metadata(context,
1484                             self._volume_api, volume_id, connection_info)
1485         return encryption
1486 
1487     def _attach_encryptor(self, context, connection_info, encryption,
1488                           allow_native_luks):
1489         """Attach the frontend encryptor if one is required by the volume.
1490 
1491         The request context is only used when an encryption metadata dict is
1492         not provided. The encryption metadata dict being populated is then used
1493         to determine if an attempt to attach the encryptor should be made.
1494 
1495         If native LUKS decryption is enabled then create a Libvirt volume
1496         secret containing the LUKS passphrase for the volume.
1497         """
1498         if encryption is None:
1499             encryption = self._get_volume_encryption(context, connection_info)
1500 
1501         if (encryption and allow_native_luks and
1502             self._use_native_luks(encryption)):
1503             # NOTE(lyarwood): Fetch the associated key for the volume and
1504             # decode the passphrase from the key.
1505             # FIXME(lyarwood): c-vol currently creates symmetric keys for use
1506             # with volumes, leading to the binary to hex to string conversion
1507             # below.
1508             keymgr = key_manager.API(CONF)
1509             key = keymgr.get(context, encryption['encryption_key_id'])
1510             key_encoded = key.get_encoded()
1511             passphrase = binascii.hexlify(key_encoded).decode('utf-8')
1512 
1513             # NOTE(lyarwood): Retain the behaviour of the original os-brick
1514             # encryptors and format any volume that does not identify as
1515             # encrypted with LUKS.
1516             # FIXME(lyarwood): Remove this once c-vol correctly formats
1517             # encrypted volumes during their initial creation:
1518             # https://bugs.launchpad.net/cinder/+bug/1739442
1519             device_path = connection_info.get('data').get('device_path')
1520             if device_path:
1521                 root_helper = utils.get_root_helper()
1522                 if not luks_encryptor.is_luks(root_helper, device_path):
1523                     encryptor = self._get_volume_encryptor(connection_info,
1524                                                            encryption)
1525                     encryptor._format_volume(passphrase, **encryption)
1526 
1527             # NOTE(lyarwood): Store the passphrase as a libvirt secret locally
1528             # on the compute node. This secret is used later when generating
1529             # the volume config.
1530             volume_id = driver_block_device.get_volume_id(connection_info)
1531             self._host.create_secret('volume', volume_id, password=passphrase)
1532         elif encryption:
1533             encryptor = self._get_volume_encryptor(connection_info,
1534                                                    encryption)
1535             encryptor.attach_volume(context, **encryption)
1536 
1537     def _detach_encryptor(self, context, connection_info, encryption):
1538         """Detach the frontend encryptor if one is required by the volume.
1539 
1540         The request context is only used when an encryption metadata dict is
1541         not provided. The encryption metadata dict being populated is then used
1542         to determine if an attempt to detach the encryptor should be made.
1543 
1544         If native LUKS decryption is enabled then delete previously created
1545         Libvirt volume secret from the host.
1546         """
1547         volume_id = driver_block_device.get_volume_id(connection_info)
1548         if volume_id and self._host.find_secret('volume', volume_id):
1549             return self._host.delete_secret('volume', volume_id)
1550         if encryption is None:
1551             encryption = self._get_volume_encryption(context, connection_info)
1552         # NOTE(lyarwood): Handle bug #1821696 where volume secrets have been
1553         # removed manually by returning if native LUKS decryption is available
1554         # and device_path is not present in the connection_info. This avoids
1555         # VolumeEncryptionNotSupported being thrown when we incorrectly build
1556         # the encryptor below due to the secrets not being present above.
1557         if (encryption and self._use_native_luks(encryption) and
1558             not connection_info['data'].get('device_path')):
1559             return
1560         if encryption:
1561             encryptor = self._get_volume_encryptor(connection_info,
1562                                                    encryption)
1563             encryptor.detach_volume(**encryption)
1564 
1565     def _check_discard_for_attach_volume(self, conf, instance):
1566         """Perform some checks for volumes configured for discard support.
1567 
1568         If discard is configured for the volume, and the guest is using a
1569         configuration known to not work, we will log a message explaining
1570         the reason why.
1571         """
1572         if conf.driver_discard == 'unmap' and conf.target_bus == 'virtio':
1573             LOG.debug('Attempting to attach volume %(id)s with discard '
1574                       'support enabled to an instance using an '
1575                       'unsupported configuration. target_bus = '
1576                       '%(bus)s. Trim commands will not be issued to '
1577                       'the storage device.',
1578                       {'bus': conf.target_bus,
1579                        'id': conf.serial},
1580                       instance=instance)
1581 
1582     def attach_volume(self, context, connection_info, instance, mountpoint,
1583                       disk_bus=None, device_type=None, encryption=None):
1584         guest = self._host.get_guest(instance)
1585 
1586         disk_dev = mountpoint.rpartition("/")[2]
1587         bdm = {
1588             'device_name': disk_dev,
1589             'disk_bus': disk_bus,
1590             'device_type': device_type}
1591 
1592         # Note(cfb): If the volume has a custom block size, check that
1593         #            that we are using QEMU/KVM and libvirt >= 0.10.2. The
1594         #            presence of a block size is considered mandatory by
1595         #            cinder so we fail if we can't honor the request.
1596         data = {}
1597         if ('data' in connection_info):
1598             data = connection_info['data']
1599         if ('logical_block_size' in data or 'physical_block_size' in data):
1600             if ((CONF.libvirt.virt_type != "kvm" and
1601                  CONF.libvirt.virt_type != "qemu")):
1602                 msg = _("Volume sets block size, but the current "
1603                         "libvirt hypervisor '%s' does not support custom "
1604                         "block size") % CONF.libvirt.virt_type
1605                 raise exception.InvalidHypervisorType(msg)
1606 
1607         self._connect_volume(context, connection_info, instance,
1608                              encryption=encryption)
1609         disk_info = blockinfo.get_info_from_bdm(
1610             instance, CONF.libvirt.virt_type, instance.image_meta, bdm)
1611         if disk_info['bus'] == 'scsi':
1612             disk_info['unit'] = self._get_scsi_controller_max_unit(guest) + 1
1613 
1614         conf = self._get_volume_config(connection_info, disk_info)
1615 
1616         self._check_discard_for_attach_volume(conf, instance)
1617 
1618         try:
1619             state = guest.get_power_state(self._host)
1620             live = state in (power_state.RUNNING, power_state.PAUSED)
1621 
1622             guest.attach_device(conf, persistent=True, live=live)
1623             # NOTE(artom) If we're attaching with a device role tag, we need to
1624             # rebuild device_metadata. If we're attaching without a role
1625             # tag, we're rebuilding it here needlessly anyways. This isn't a
1626             # massive deal, and it helps reduce code complexity by not having
1627             # to indicate to the virt driver that the attach is tagged. The
1628             # really important optimization of not calling the database unless
1629             # device_metadata has actually changed is done for us by
1630             # instance.save().
1631             instance.device_metadata = self._build_device_metadata(
1632                 context, instance)
1633             instance.save()
1634 
1635         # TODO(lyarwood) Remove the following breadcrumb once all supported
1636         # distributions provide Libvirt 3.3.0 or earlier with
1637         # https://libvirt.org/git/?p=libvirt.git;a=commit;h=7189099 applied.
1638         except libvirt.libvirtError as ex:
1639             with excutils.save_and_reraise_exception():
1640                 if 'Incorrect number of padding bytes' in six.text_type(ex):
1641                     LOG.warning(_('Failed to attach encrypted volume due to a '
1642                                   'known Libvirt issue, see the following bug '
1643                                   'for details: '
1644                                   'https://bugzilla.redhat.com/1447297'))
1645                 else:
1646                     LOG.exception(_('Failed to attach volume at mountpoint: '
1647                                     '%s'), mountpoint, instance=instance)
1648                 self._disconnect_volume(context, connection_info, instance,
1649                                         encryption=encryption)
1650         except Exception:
1651             LOG.exception(_('Failed to attach volume at mountpoint: %s'),
1652                           mountpoint, instance=instance)
1653             with excutils.save_and_reraise_exception():
1654                 self._disconnect_volume(context, connection_info, instance,
1655                                         encryption=encryption)
1656 
1657     def _swap_volume(self, guest, disk_path, conf, resize_to):
1658         """Swap existing disk with a new block device."""
1659         dev = guest.get_block_device(disk_path)
1660 
1661         # Save a copy of the domain's persistent XML file. We'll use this
1662         # to redefine the domain if anything fails during the volume swap.
1663         xml = guest.get_xml_desc(dump_inactive=True, dump_sensitive=True)
1664 
1665         # Abort is an idempotent operation, so make sure any block
1666         # jobs which may have failed are ended.
1667         try:
1668             dev.abort_job()
1669         except Exception:
1670             pass
1671 
1672         try:
1673             # NOTE (rmk): blockRebase cannot be executed on persistent
1674             #             domains, so we need to temporarily undefine it.
1675             #             If any part of this block fails, the domain is
1676             #             re-defined regardless.
1677             if guest.has_persistent_configuration():
1678                 support_uefi = self._has_uefi_support()
1679                 guest.delete_configuration(support_uefi)
1680 
1681             try:
1682                 # Start copy with VIR_DOMAIN_BLOCK_REBASE_REUSE_EXT flag to
1683                 # allow writing to existing external volume file. Use
1684                 # VIR_DOMAIN_BLOCK_REBASE_COPY_DEV if it's a block device to
1685                 # make sure XML is generated correctly (bug 1691195)
1686                 copy_dev = conf.source_type == 'block'
1687                 dev.rebase(conf.source_path, copy=True, reuse_ext=True,
1688                            copy_dev=copy_dev)
1689                 while not dev.is_job_complete():
1690                     time.sleep(0.5)
1691 
1692                 dev.abort_job(pivot=True)
1693 
1694             except Exception as exc:
1695                 LOG.exception("Failure rebasing volume %(new_path)s on "
1696                     "%(old_path)s.", {'new_path': conf.source_path,
1697                                       'old_path': disk_path})
1698                 raise exception.VolumeRebaseFailed(reason=six.text_type(exc))
1699 
1700             if resize_to:
1701                 dev.resize(resize_to * units.Gi / units.Ki)
1702 
1703             # Make sure we will redefine the domain using the updated
1704             # configuration after the volume was swapped. The dump_inactive
1705             # keyword arg controls whether we pull the inactive (persistent)
1706             # or active (live) config from the domain. We want to pull the
1707             # live config after the volume was updated to use when we redefine
1708             # the domain.
1709             xml = guest.get_xml_desc(dump_inactive=False, dump_sensitive=True)
1710         finally:
1711             self._host.write_instance_config(xml)
1712 
1713     def swap_volume(self, context, old_connection_info,
1714                     new_connection_info, instance, mountpoint, resize_to):
1715 
1716         # NOTE(lyarwood): https://bugzilla.redhat.com/show_bug.cgi?id=760547
1717         old_encrypt = self._get_volume_encryption(context, old_connection_info)
1718         new_encrypt = self._get_volume_encryption(context, new_connection_info)
1719         if ((old_encrypt and self._use_native_luks(old_encrypt)) or
1720             (new_encrypt and self._use_native_luks(new_encrypt))):
1721             raise NotImplementedError(_("Swap volume is not supported for "
1722                 "encrypted volumes when native LUKS decryption is enabled."))
1723 
1724         guest = self._host.get_guest(instance)
1725 
1726         disk_dev = mountpoint.rpartition("/")[2]
1727         if not guest.get_disk(disk_dev):
1728             raise exception.DiskNotFound(location=disk_dev)
1729         disk_info = {
1730             'dev': disk_dev,
1731             'bus': blockinfo.get_disk_bus_for_disk_dev(
1732                 CONF.libvirt.virt_type, disk_dev),
1733             'type': 'disk',
1734             }
1735         # NOTE (lyarwood): new_connection_info will be modified by the
1736         # following _connect_volume call down into the volume drivers. The
1737         # majority of the volume drivers will add a device_path that is in turn
1738         # used by _get_volume_config to set the source_path of the
1739         # LibvirtConfigGuestDisk object it returns. We do not explicitly save
1740         # this to the BDM here as the upper compute swap_volume method will
1741         # eventually do this for us.
1742         self._connect_volume(context, new_connection_info, instance)
1743         conf = self._get_volume_config(new_connection_info, disk_info)
1744         if not conf.source_path:
1745             self._disconnect_volume(context, new_connection_info, instance)
1746             raise NotImplementedError(_("Swap only supports host devices"))
1747 
1748         try:
1749             self._swap_volume(guest, disk_dev, conf, resize_to)
1750         except exception.VolumeRebaseFailed:
1751             with excutils.save_and_reraise_exception():
1752                 self._disconnect_volume(context, new_connection_info, instance)
1753 
1754         self._disconnect_volume(context, old_connection_info, instance)
1755 
1756     def _get_existing_domain_xml(self, instance, network_info,
1757                                  block_device_info=None):
1758         try:
1759             guest = self._host.get_guest(instance)
1760             xml = guest.get_xml_desc()
1761         except exception.InstanceNotFound:
1762             disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
1763                                                 instance,
1764                                                 instance.image_meta,
1765                                                 block_device_info)
1766             xml = self._get_guest_xml(nova_context.get_admin_context(),
1767                                       instance, network_info, disk_info,
1768                                       instance.image_meta,
1769                                       block_device_info=block_device_info)
1770         return xml
1771 
1772     def detach_volume(self, context, connection_info, instance, mountpoint,
1773                       encryption=None):
1774         disk_dev = mountpoint.rpartition("/")[2]
1775         try:
1776             guest = self._host.get_guest(instance)
1777 
1778             state = guest.get_power_state(self._host)
1779             live = state in (power_state.RUNNING, power_state.PAUSED)
1780             # NOTE(lyarwood): The volume must be detached from the VM before
1781             # detaching any attached encryptors or disconnecting the underlying
1782             # volume in _disconnect_volume. Otherwise, the encryptor or volume
1783             # driver may report that the volume is still in use.
1784             wait_for_detach = guest.detach_device_with_retry(guest.get_disk,
1785                                                              disk_dev,
1786                                                              live=live)
1787             wait_for_detach()
1788 
1789         except exception.InstanceNotFound:
1790             # NOTE(zhaoqin): If the instance does not exist, _lookup_by_name()
1791             #                will throw InstanceNotFound exception. Need to
1792             #                disconnect volume under this circumstance.
1793             LOG.warning("During detach_volume, instance disappeared.",
1794                         instance=instance)
1795         except exception.DeviceNotFound:
1796             # We should still try to disconnect logical device from
1797             # host, an error might have happened during a previous
1798             # call.
1799             LOG.info("Device %s not found in instance.",
1800                      disk_dev, instance=instance)
1801         except libvirt.libvirtError as ex:
1802             # NOTE(vish): This is called to cleanup volumes after live
1803             #             migration, so we should still disconnect even if
1804             #             the instance doesn't exist here anymore.
1805             error_code = ex.get_error_code()
1806             if error_code == libvirt.VIR_ERR_NO_DOMAIN:
1807                 # NOTE(vish):
1808                 LOG.warning("During detach_volume, instance disappeared.",
1809                             instance=instance)
1810             else:
1811                 raise
1812 
1813         self._disconnect_volume(context, connection_info, instance,
1814                                 encryption=encryption)
1815 
1816     def extend_volume(self, connection_info, instance, requested_size):
1817         try:
1818             new_size = self._extend_volume(connection_info, instance,
1819                                            requested_size)
1820         except NotImplementedError:
1821             raise exception.ExtendVolumeNotSupported()
1822 
1823         # Resize the device in QEMU so its size is updated and
1824         # detected by the instance without rebooting.
1825         try:
1826             guest = self._host.get_guest(instance)
1827             state = guest.get_power_state(self._host)
1828             active_state = state in (power_state.RUNNING, power_state.PAUSED)
1829             if active_state:
1830                 if 'device_path' in connection_info['data']:
1831                     disk_path = connection_info['data']['device_path']
1832                 else:
1833                     # Some drivers (eg. net) don't put the device_path
1834                     # into the connection_info. Match disks by their serial
1835                     # number instead
1836                     volume_id = driver_block_device.get_volume_id(
1837                         connection_info)
1838                     disk = next(iter([
1839                         d for d in guest.get_all_disks()
1840                         if d.serial == volume_id
1841                     ]), None)
1842                     if not disk:
1843                         raise exception.VolumeNotFound(volume_id=volume_id)
1844                     disk_path = disk.target_dev
1845 
1846                 LOG.debug('resizing block device %(dev)s to %(size)u kb',
1847                           {'dev': disk_path, 'size': new_size})
1848                 dev = guest.get_block_device(disk_path)
1849                 dev.resize(new_size // units.Ki)
1850             else:
1851                 LOG.debug('Skipping block device resize, guest is not running',
1852                           instance=instance)
1853         except exception.InstanceNotFound:
1854             with excutils.save_and_reraise_exception():
1855                 LOG.warning('During extend_volume, instance disappeared.',
1856                             instance=instance)
1857         except libvirt.libvirtError:
1858             with excutils.save_and_reraise_exception():
1859                 LOG.exception('resizing block device failed.',
1860                               instance=instance)
1861 
1862     def attach_interface(self, context, instance, image_meta, vif):
1863         guest = self._host.get_guest(instance)
1864 
1865         self.vif_driver.plug(instance, vif)
1866         self.firewall_driver.setup_basic_filtering(instance, [vif])
1867         cfg = self.vif_driver.get_config(instance, vif, image_meta,
1868                                          instance.flavor,
1869                                          CONF.libvirt.virt_type,
1870                                          self._host)
1871         try:
1872             state = guest.get_power_state(self._host)
1873             live = state in (power_state.RUNNING, power_state.PAUSED)
1874             guest.attach_device(cfg, persistent=True, live=live)
1875         except libvirt.libvirtError:
1876             LOG.error('attaching network adapter failed.',
1877                       instance=instance, exc_info=True)
1878             self.vif_driver.unplug(instance, vif)
1879             raise exception.InterfaceAttachFailed(
1880                     instance_uuid=instance.uuid)
1881         try:
1882             # NOTE(artom) If we're attaching with a device role tag, we need to
1883             # rebuild device_metadata. If we're attaching without a role
1884             # tag, we're rebuilding it here needlessly anyways. This isn't a
1885             # massive deal, and it helps reduce code complexity by not having
1886             # to indicate to the virt driver that the attach is tagged. The
1887             # really important optimization of not calling the database unless
1888             # device_metadata has actually changed is done for us by
1889             # instance.save().
1890             instance.device_metadata = self._build_device_metadata(
1891                 context, instance)
1892             instance.save()
1893         except Exception:
1894             # NOTE(artom) If we fail here it means the interface attached
1895             # successfully but building and/or saving the device metadata
1896             # failed. Just unplugging the vif is therefore not enough cleanup,
1897             # we need to detach the interface.
1898             with excutils.save_and_reraise_exception(reraise=False):
1899                 LOG.error('Interface attached successfully but building '
1900                           'and/or saving device metadata failed.',
1901                           instance=instance, exc_info=True)
1902                 self.detach_interface(context, instance, vif)
1903                 raise exception.InterfaceAttachFailed(
1904                     instance_uuid=instance.uuid)
1905 
1906     def detach_interface(self, context, instance, vif):
1907         guest = self._host.get_guest(instance)
1908         cfg = self.vif_driver.get_config(instance, vif,
1909                                          instance.image_meta,
1910                                          instance.flavor,
1911                                          CONF.libvirt.virt_type, self._host)
1912         interface = guest.get_interface_by_cfg(cfg)
1913         try:
1914             self.vif_driver.unplug(instance, vif)
1915             # NOTE(mriedem): When deleting an instance and using Neutron,
1916             # we can be racing against Neutron deleting the port and
1917             # sending the vif-deleted event which then triggers a call to
1918             # detach the interface, so if the interface is not found then
1919             # we can just log it as a warning.
1920             if not interface:
1921                 mac = vif.get('address')
1922                 # The interface is gone so just log it as a warning.
1923                 LOG.warning('Detaching interface %(mac)s failed because '
1924                             'the device is no longer found on the guest.',
1925                             {'mac': mac}, instance=instance)
1926                 return
1927 
1928             state = guest.get_power_state(self._host)
1929             live = state in (power_state.RUNNING, power_state.PAUSED)
1930             # Now we are going to loop until the interface is detached or we
1931             # timeout.
1932             wait_for_detach = guest.detach_device_with_retry(
1933                 guest.get_interface_by_cfg, cfg, live=live,
1934                 alternative_device_name=self.vif_driver.get_vif_devname(vif))
1935             wait_for_detach()
1936         except exception.DeviceDetachFailed:
1937             # We failed to detach the device even with the retry loop, so let's
1938             # dump some debug information to the logs before raising back up.
1939             with excutils.save_and_reraise_exception():
1940                 devname = self.vif_driver.get_vif_devname(vif)
1941                 interface = guest.get_interface_by_cfg(cfg)
1942                 if interface:
1943                     LOG.warning(
1944                         'Failed to detach interface %(devname)s after '
1945                         'repeated attempts. Final interface xml:\n'
1946                         '%(interface_xml)s\nFinal guest xml:\n%(guest_xml)s',
1947                         {'devname': devname,
1948                          'interface_xml': interface.to_xml(),
1949                          'guest_xml': guest.get_xml_desc()},
1950                         instance=instance)
1951         except exception.DeviceNotFound:
1952             # The interface is gone so just log it as a warning.
1953             LOG.warning('Detaching interface %(mac)s failed because '
1954                         'the device is no longer found on the guest.',
1955                         {'mac': vif.get('address')}, instance=instance)
1956         except libvirt.libvirtError as ex:
1957             error_code = ex.get_error_code()
1958             if error_code == libvirt.VIR_ERR_NO_DOMAIN:
1959                 LOG.warning("During detach_interface, instance disappeared.",
1960                             instance=instance)
1961             else:
1962                 # NOTE(mriedem): When deleting an instance and using Neutron,
1963                 # we can be racing against Neutron deleting the port and
1964                 # sending the vif-deleted event which then triggers a call to
1965                 # detach the interface, so we might have failed because the
1966                 # network device no longer exists. Libvirt will fail with
1967                 # "operation failed: no matching network device was found"
1968                 # which unfortunately does not have a unique error code so we
1969                 # need to look up the interface by config and if it's not found
1970                 # then we can just log it as a warning rather than tracing an
1971                 # error.
1972                 mac = vif.get('address')
1973                 interface = guest.get_interface_by_cfg(cfg)
1974                 if interface:
1975                     LOG.error('detaching network adapter failed.',
1976                               instance=instance, exc_info=True)
1977                     raise exception.InterfaceDetachFailed(
1978                             instance_uuid=instance.uuid)
1979 
1980                 # The interface is gone so just log it as a warning.
1981                 LOG.warning('Detaching interface %(mac)s failed because '
1982                             'the device is no longer found on the guest.',
1983                             {'mac': mac}, instance=instance)
1984 
1985     def _create_snapshot_metadata(self, image_meta, instance,
1986                                   img_fmt, snp_name):
1987         metadata = {'status': 'active',
1988                     'name': snp_name,
1989                     'properties': {
1990                                    'kernel_id': instance.kernel_id,
1991                                    'image_location': 'snapshot',
1992                                    'image_state': 'available',
1993                                    'owner_id': instance.project_id,
1994                                    'ramdisk_id': instance.ramdisk_id,
1995                                    }
1996                     }
1997         if instance.os_type:
1998             metadata['properties']['os_type'] = instance.os_type
1999 
2000         # NOTE(vish): glance forces ami disk format to be ami
2001         if image_meta.disk_format == 'ami':
2002             metadata['disk_format'] = 'ami'
2003         else:
2004             metadata['disk_format'] = img_fmt
2005 
2006         if image_meta.obj_attr_is_set("container_format"):
2007             metadata['container_format'] = image_meta.container_format
2008         else:
2009             metadata['container_format'] = "bare"
2010 
2011         return metadata
2012 
2013     def snapshot(self, context, instance, image_id, update_task_state):
2014         """Create snapshot from a running VM instance.
2015 
2016         This command only works with qemu 0.14+
2017         """
2018         try:
2019             guest = self._host.get_guest(instance)
2020 
2021             # TODO(sahid): We are converting all calls from a
2022             # virDomain object to use nova.virt.libvirt.Guest.
2023             # We should be able to remove virt_dom at the end.
2024             virt_dom = guest._domain
2025         except exception.InstanceNotFound:
2026             raise exception.InstanceNotRunning(instance_id=instance.uuid)
2027 
2028         snapshot = self._image_api.get(context, image_id)
2029 
2030         # source_format is an on-disk format
2031         # source_type is a backend type
2032         disk_path, source_format = libvirt_utils.find_disk(guest)
2033         source_type = libvirt_utils.get_disk_type_from_path(disk_path)
2034 
2035         # We won't have source_type for raw or qcow2 disks, because we can't
2036         # determine that from the path. We should have it from the libvirt
2037         # xml, though.
2038         if source_type is None:
2039             source_type = source_format
2040         # For lxc instances we won't have it either from libvirt xml
2041         # (because we just gave libvirt the mounted filesystem), or the path,
2042         # so source_type is still going to be None. In this case,
2043         # root_disk is going to default to CONF.libvirt.images_type
2044         # below, which is still safe.
2045 
2046         image_format = CONF.libvirt.snapshot_image_format or source_type
2047 
2048         # NOTE(bfilippov): save lvm and rbd as raw
2049         if image_format == 'lvm' or image_format == 'rbd':
2050             image_format = 'raw'
2051 
2052         metadata = self._create_snapshot_metadata(instance.image_meta,
2053                                                   instance,
2054                                                   image_format,
2055                                                   snapshot['name'])
2056 
2057         snapshot_name = uuidutils.generate_uuid(dashed=False)
2058 
2059         state = guest.get_power_state(self._host)
2060 
2061         # NOTE(dgenin): Instances with LVM encrypted ephemeral storage require
2062         #               cold snapshots. Currently, checking for encryption is
2063         #               redundant because LVM supports only cold snapshots.
2064         #               It is necessary in case this situation changes in the
2065         #               future.
2066         if (self._host.has_min_version(hv_type=host.HV_DRIVER_QEMU)
2067                 and source_type not in ('lvm')
2068                 and not CONF.ephemeral_storage_encryption.enabled
2069                 and not CONF.workarounds.disable_libvirt_livesnapshot
2070                 # NOTE(rmk): We cannot perform live snapshots when a
2071                 # managedSave file is present, so we will use the cold/legacy
2072                 # method for instances which are shutdown or paused.
2073                 # NOTE(mriedem): Live snapshot doesn't work with paused
2074                 # instances on older versions of libvirt/qemu. We can likely
2075                 # remove the restriction on PAUSED once we require
2076                 # libvirt>=3.6.0 and qemu>=2.10 since that works with the
2077                 # Pike Ubuntu Cloud Archive testing in Queens.
2078                 and state not in (power_state.SHUTDOWN, power_state.PAUSED)):
2079             live_snapshot = True
2080             # Abort is an idempotent operation, so make sure any block
2081             # jobs which may have failed are ended. This operation also
2082             # confirms the running instance, as opposed to the system as a
2083             # whole, has a new enough version of the hypervisor (bug 1193146).
2084             try:
2085                 guest.get_block_device(disk_path).abort_job()
2086             except libvirt.libvirtError as ex:
2087                 error_code = ex.get_error_code()
2088                 if error_code == libvirt.VIR_ERR_CONFIG_UNSUPPORTED:
2089                     live_snapshot = False
2090                 else:
2091                     pass
2092         else:
2093             live_snapshot = False
2094 
2095         self._prepare_domain_for_snapshot(context, live_snapshot, state,
2096                                           instance)
2097 
2098         root_disk = self.image_backend.by_libvirt_path(
2099             instance, disk_path, image_type=source_type)
2100 
2101         if live_snapshot:
2102             LOG.info("Beginning live snapshot process", instance=instance)
2103         else:
2104             LOG.info("Beginning cold snapshot process", instance=instance)
2105 
2106         update_task_state(task_state=task_states.IMAGE_PENDING_UPLOAD)
2107 
2108         update_task_state(task_state=task_states.IMAGE_UPLOADING,
2109                           expected_state=task_states.IMAGE_PENDING_UPLOAD)
2110 
2111         try:
2112             metadata['location'] = root_disk.direct_snapshot(
2113                 context, snapshot_name, image_format, image_id,
2114                 instance.image_ref)
2115             self._snapshot_domain(context, live_snapshot, virt_dom, state,
2116                                   instance)
2117             self._image_api.update(context, image_id, metadata,
2118                                    purge_props=False)
2119         except (NotImplementedError, exception.ImageUnacceptable,
2120                 exception.Forbidden) as e:
2121             if type(e) != NotImplementedError:
2122                 LOG.warning('Performing standard snapshot because direct '
2123                             'snapshot failed: %(error)s',
2124                             {'error': encodeutils.exception_to_unicode(e)})
2125             failed_snap = metadata.pop('location', None)
2126             if failed_snap:
2127                 failed_snap = {'url': str(failed_snap)}
2128             root_disk.cleanup_direct_snapshot(failed_snap,
2129                                                   also_destroy_volume=True,
2130                                                   ignore_errors=True)
2131             update_task_state(task_state=task_states.IMAGE_PENDING_UPLOAD,
2132                               expected_state=task_states.IMAGE_UPLOADING)
2133 
2134             # TODO(nic): possibly abstract this out to the root_disk
2135             if source_type == 'rbd' and live_snapshot:
2136                 # Standard snapshot uses qemu-img convert from RBD which is
2137                 # not safe to run with live_snapshot.
2138                 live_snapshot = False
2139                 # Suspend the guest, so this is no longer a live snapshot
2140                 self._prepare_domain_for_snapshot(context, live_snapshot,
2141                                                   state, instance)
2142 
2143             snapshot_directory = CONF.libvirt.snapshots_directory
2144             fileutils.ensure_tree(snapshot_directory)
2145             with utils.tempdir(dir=snapshot_directory) as tmpdir:
2146                 try:
2147                     out_path = os.path.join(tmpdir, snapshot_name)
2148                     if live_snapshot:
2149                         # NOTE(xqueralt): libvirt needs o+x in the tempdir
2150                         os.chmod(tmpdir, 0o701)
2151                         self._live_snapshot(context, instance, guest,
2152                                             disk_path, out_path, source_format,
2153                                             image_format, instance.image_meta)
2154                     else:
2155                         root_disk.snapshot_extract(out_path, image_format)
2156                     LOG.info("Snapshot extracted, beginning image upload",
2157                              instance=instance)
2158                 except libvirt.libvirtError as ex:
2159                     error_code = ex.get_error_code()
2160                     if error_code == libvirt.VIR_ERR_NO_DOMAIN:
2161                         LOG.info('Instance %(instance_name)s disappeared '
2162                                  'while taking snapshot of it: [Error Code '
2163                                  '%(error_code)s] %(ex)s',
2164                                  {'instance_name': instance.name,
2165                                   'error_code': error_code,
2166                                   'ex': ex},
2167                                  instance=instance)
2168                         raise exception.InstanceNotFound(
2169                             instance_id=instance.uuid)
2170                     else:
2171                         raise
2172                 finally:
2173                     self._snapshot_domain(context, live_snapshot, virt_dom,
2174                                           state, instance)
2175 
2176                 # Upload that image to the image service
2177                 update_task_state(task_state=task_states.IMAGE_UPLOADING,
2178                         expected_state=task_states.IMAGE_PENDING_UPLOAD)
2179                 with libvirt_utils.file_open(out_path, 'rb') as image_file:
2180                     # execute operation with disk concurrency semaphore
2181                     with compute_utils.disk_ops_semaphore:
2182                         self._image_api.update(context,
2183                                                image_id,
2184                                                metadata,
2185                                                image_file)
2186         except Exception:
2187             with excutils.save_and_reraise_exception():
2188                 LOG.exception(_("Failed to snapshot image"))
2189                 failed_snap = metadata.pop('location', None)
2190                 if failed_snap:
2191                     failed_snap = {'url': str(failed_snap)}
2192                 root_disk.cleanup_direct_snapshot(
2193                         failed_snap, also_destroy_volume=True,
2194                         ignore_errors=True)
2195 
2196         LOG.info("Snapshot image upload complete", instance=instance)
2197 
2198     def _prepare_domain_for_snapshot(self, context, live_snapshot, state,
2199                                      instance):
2200         # NOTE(dkang): managedSave does not work for LXC
2201         if CONF.libvirt.virt_type != 'lxc' and not live_snapshot:
2202             if state == power_state.RUNNING or state == power_state.PAUSED:
2203                 self.suspend(context, instance)
2204 
2205     def _snapshot_domain(self, context, live_snapshot, virt_dom, state,
2206                          instance):
2207         guest = None
2208         # NOTE(dkang): because previous managedSave is not called
2209         #              for LXC, _create_domain must not be called.
2210         if CONF.libvirt.virt_type != 'lxc' and not live_snapshot:
2211             if state == power_state.RUNNING:
2212                 guest = self._create_domain(domain=virt_dom)
2213             elif state == power_state.PAUSED:
2214                 guest = self._create_domain(domain=virt_dom, pause=True)
2215 
2216             if guest is not None:
2217                 self._attach_pci_devices(
2218                     guest, pci_manager.get_instance_pci_devs(instance))
2219                 self._attach_direct_passthrough_ports(
2220                     context, instance, guest)
2221 
2222     def _can_set_admin_password(self, image_meta):
2223 
2224         if CONF.libvirt.virt_type in ('kvm', 'qemu'):
2225             if not image_meta.properties.get('hw_qemu_guest_agent', False):
2226                 raise exception.QemuGuestAgentNotEnabled()
2227         elif not CONF.libvirt.virt_type == 'parallels':
2228             raise exception.SetAdminPasswdNotSupported()
2229 
2230     # TODO(melwitt): Combine this with the similar xenapi code at some point.
2231     def _save_instance_password_if_sshkey_present(self, instance, new_pass):
2232         sshkey = instance.key_data if 'key_data' in instance else None
2233         if sshkey and sshkey.startswith("ssh-rsa"):
2234             enc = crypto.ssh_encrypt_text(sshkey, new_pass)
2235             # NOTE(melwitt): The convert_password method doesn't actually do
2236             # anything with the context argument, so we can pass None.
2237             instance.system_metadata.update(
2238                 password.convert_password(None, base64.encode_as_text(enc)))
2239             instance.save()
2240 
2241     def set_admin_password(self, instance, new_pass):
2242         self._can_set_admin_password(instance.image_meta)
2243 
2244         guest = self._host.get_guest(instance)
2245         user = instance.image_meta.properties.get("os_admin_user")
2246         if not user:
2247             if instance.os_type == "windows":
2248                 user = "Administrator"
2249             else:
2250                 user = "root"
2251         try:
2252             guest.set_user_password(user, new_pass)
2253         except libvirt.libvirtError as ex:
2254             error_code = ex.get_error_code()
2255             if error_code == libvirt.VIR_ERR_AGENT_UNRESPONSIVE:
2256                 LOG.debug('Failed to set password: QEMU agent unresponsive',
2257                           instance_uuid=instance.uuid)
2258                 raise NotImplementedError()
2259 
2260             err_msg = encodeutils.exception_to_unicode(ex)
2261             msg = (_('Error from libvirt while set password for username '
2262                      '"%(user)s": [Error Code %(error_code)s] %(ex)s')
2263                    % {'user': user, 'error_code': error_code, 'ex': err_msg})
2264             raise exception.InternalError(msg)
2265         else:
2266             # Save the password in sysmeta so it may be retrieved from the
2267             # metadata service.
2268             self._save_instance_password_if_sshkey_present(instance, new_pass)
2269 
2270     def _can_quiesce(self, instance, image_meta):
2271         if CONF.libvirt.virt_type not in ('kvm', 'qemu'):
2272             raise exception.InstanceQuiesceNotSupported(
2273                 instance_id=instance.uuid)
2274 
2275         if not image_meta.properties.get('hw_qemu_guest_agent', False):
2276             raise exception.QemuGuestAgentNotEnabled()
2277 
2278     def _requires_quiesce(self, image_meta):
2279         return image_meta.properties.get('os_require_quiesce', False)
2280 
2281     def _set_quiesced(self, context, instance, image_meta, quiesced):
2282         self._can_quiesce(instance, image_meta)
2283         try:
2284             guest = self._host.get_guest(instance)
2285             if quiesced:
2286                 guest.freeze_filesystems()
2287             else:
2288                 guest.thaw_filesystems()
2289         except libvirt.libvirtError as ex:
2290             error_code = ex.get_error_code()
2291             err_msg = encodeutils.exception_to_unicode(ex)
2292             msg = (_('Error from libvirt while quiescing %(instance_name)s: '
2293                      '[Error Code %(error_code)s] %(ex)s')
2294                    % {'instance_name': instance.name,
2295                       'error_code': error_code, 'ex': err_msg})
2296             raise exception.InternalError(msg)
2297 
2298     def quiesce(self, context, instance, image_meta):
2299         """Freeze the guest filesystems to prepare for snapshot.
2300 
2301         The qemu-guest-agent must be setup to execute fsfreeze.
2302         """
2303         self._set_quiesced(context, instance, image_meta, True)
2304 
2305     def unquiesce(self, context, instance, image_meta):
2306         """Thaw the guest filesystems after snapshot."""
2307         self._set_quiesced(context, instance, image_meta, False)
2308 
2309     def _live_snapshot(self, context, instance, guest, disk_path, out_path,
2310                        source_format, image_format, image_meta):
2311         """Snapshot an instance without downtime."""
2312         dev = guest.get_block_device(disk_path)
2313 
2314         # Save a copy of the domain's persistent XML file
2315         xml = guest.get_xml_desc(dump_inactive=True, dump_sensitive=True)
2316 
2317         # Abort is an idempotent operation, so make sure any block
2318         # jobs which may have failed are ended.
2319         try:
2320             dev.abort_job()
2321         except Exception:
2322             pass
2323 
2324         # NOTE (rmk): We are using shallow rebases as a workaround to a bug
2325         #             in QEMU 1.3. In order to do this, we need to create
2326         #             a destination image with the original backing file
2327         #             and matching size of the instance root disk.
2328         src_disk_size = libvirt_utils.get_disk_size(disk_path,
2329                                                     format=source_format)
2330         src_back_path = libvirt_utils.get_disk_backing_file(disk_path,
2331                                                         format=source_format,
2332                                                         basename=False)
2333         disk_delta = out_path + '.delta'
2334         libvirt_utils.create_cow_image(src_back_path, disk_delta,
2335                                        src_disk_size)
2336 
2337         quiesced = False
2338         try:
2339             self._set_quiesced(context, instance, image_meta, True)
2340             quiesced = True
2341         except exception.NovaException as err:
2342             if self._requires_quiesce(image_meta):
2343                 raise
2344             LOG.info('Skipping quiescing instance: %(reason)s.',
2345                      {'reason': err}, instance=instance)
2346 
2347         try:
2348             # NOTE (rmk): blockRebase cannot be executed on persistent
2349             #             domains, so we need to temporarily undefine it.
2350             #             If any part of this block fails, the domain is
2351             #             re-defined regardless.
2352             if guest.has_persistent_configuration():
2353                 support_uefi = self._has_uefi_support()
2354                 guest.delete_configuration(support_uefi)
2355 
2356             # NOTE (rmk): Establish a temporary mirror of our root disk and
2357             #             issue an abort once we have a complete copy.
2358             dev.rebase(disk_delta, copy=True, reuse_ext=True, shallow=True)
2359 
2360             while not dev.is_job_complete():
2361                 time.sleep(0.5)
2362 
2363             dev.abort_job()
2364             nova.privsep.path.chown(disk_delta, uid=os.getuid())
2365         finally:
2366             self._host.write_instance_config(xml)
2367             if quiesced:
2368                 self._set_quiesced(context, instance, image_meta, False)
2369 
2370         # Convert the delta (CoW) image with a backing file to a flat
2371         # image with no backing file.
2372         libvirt_utils.extract_snapshot(disk_delta, 'qcow2',
2373                                        out_path, image_format)
2374 
2375     def _volume_snapshot_update_status(self, context, snapshot_id, status):
2376         """Send a snapshot status update to Cinder.
2377 
2378         This method captures and logs exceptions that occur
2379         since callers cannot do anything useful with these exceptions.
2380 
2381         Operations on the Cinder side waiting for this will time out if
2382         a failure occurs sending the update.
2383 
2384         :param context: security context
2385         :param snapshot_id: id of snapshot being updated
2386         :param status: new status value
2387 
2388         """
2389 
2390         try:
2391             self._volume_api.update_snapshot_status(context,
2392                                                     snapshot_id,
2393                                                     status)
2394         except Exception:
2395             LOG.exception(_('Failed to send updated snapshot status '
2396                             'to volume service.'))
2397 
2398     def _volume_snapshot_create(self, context, instance, guest,
2399                                 volume_id, new_file):
2400         """Perform volume snapshot.
2401 
2402            :param guest: VM that volume is attached to
2403            :param volume_id: volume UUID to snapshot
2404            :param new_file: relative path to new qcow2 file present on share
2405 
2406         """
2407         xml = guest.get_xml_desc()
2408         xml_doc = etree.fromstring(xml)
2409 
2410         device_info = vconfig.LibvirtConfigGuest()
2411         device_info.parse_dom(xml_doc)
2412 
2413         disks_to_snap = []          # to be snapshotted by libvirt
2414         network_disks_to_snap = []  # network disks (netfs, etc.)
2415         disks_to_skip = []          # local disks not snapshotted
2416 
2417         for guest_disk in device_info.devices:
2418             if (guest_disk.root_name != 'disk'):
2419                 continue
2420 
2421             if (guest_disk.target_dev is None):
2422                 continue
2423 
2424             if (guest_disk.serial is None or guest_disk.serial != volume_id):
2425                 disks_to_skip.append(guest_disk.target_dev)
2426                 continue
2427 
2428             # disk is a Cinder volume with the correct volume_id
2429 
2430             disk_info = {
2431                 'dev': guest_disk.target_dev,
2432                 'serial': guest_disk.serial,
2433                 'current_file': guest_disk.source_path,
2434                 'source_protocol': guest_disk.source_protocol,
2435                 'source_name': guest_disk.source_name,
2436                 'source_hosts': guest_disk.source_hosts,
2437                 'source_ports': guest_disk.source_ports
2438             }
2439 
2440             # Determine path for new_file based on current path
2441             if disk_info['current_file'] is not None:
2442                 current_file = disk_info['current_file']
2443                 new_file_path = os.path.join(os.path.dirname(current_file),
2444                                              new_file)
2445                 disks_to_snap.append((current_file, new_file_path))
2446             # NOTE(mriedem): This used to include a check for gluster in
2447             # addition to netfs since they were added together. Support for
2448             # gluster was removed in the 16.0.0 Pike release. It is unclear,
2449             # however, if other volume drivers rely on the netfs disk source
2450             # protocol.
2451             elif disk_info['source_protocol'] == 'netfs':
2452                 network_disks_to_snap.append((disk_info, new_file))
2453 
2454         if not disks_to_snap and not network_disks_to_snap:
2455             msg = _('Found no disk to snapshot.')
2456             raise exception.InternalError(msg)
2457 
2458         snapshot = vconfig.LibvirtConfigGuestSnapshot()
2459 
2460         for current_name, new_filename in disks_to_snap:
2461             snap_disk = vconfig.LibvirtConfigGuestSnapshotDisk()
2462             snap_disk.name = current_name
2463             snap_disk.source_path = new_filename
2464             snap_disk.source_type = 'file'
2465             snap_disk.snapshot = 'external'
2466             snap_disk.driver_name = 'qcow2'
2467 
2468             snapshot.add_disk(snap_disk)
2469 
2470         for disk_info, new_filename in network_disks_to_snap:
2471             snap_disk = vconfig.LibvirtConfigGuestSnapshotDisk()
2472             snap_disk.name = disk_info['dev']
2473             snap_disk.source_type = 'network'
2474             snap_disk.source_protocol = disk_info['source_protocol']
2475             snap_disk.snapshot = 'external'
2476             snap_disk.source_path = new_filename
2477             old_dir = disk_info['source_name'].split('/')[0]
2478             snap_disk.source_name = '%s/%s' % (old_dir, new_filename)
2479             snap_disk.source_hosts = disk_info['source_hosts']
2480             snap_disk.source_ports = disk_info['source_ports']
2481 
2482             snapshot.add_disk(snap_disk)
2483 
2484         for dev in disks_to_skip:
2485             snap_disk = vconfig.LibvirtConfigGuestSnapshotDisk()
2486             snap_disk.name = dev
2487             snap_disk.snapshot = 'no'
2488 
2489             snapshot.add_disk(snap_disk)
2490 
2491         snapshot_xml = snapshot.to_xml()
2492         LOG.debug("snap xml: %s", snapshot_xml, instance=instance)
2493 
2494         image_meta = instance.image_meta
2495         try:
2496             # Check to see if we can quiesce the guest before taking the
2497             # snapshot.
2498             self._can_quiesce(instance, image_meta)
2499             try:
2500                 guest.snapshot(snapshot, no_metadata=True, disk_only=True,
2501                                reuse_ext=True, quiesce=True)
2502                 return
2503             except libvirt.libvirtError:
2504                 # If the image says that quiesce is required then we fail.
2505                 if self._requires_quiesce(image_meta):
2506                     raise
2507                 LOG.exception(_('Unable to create quiesced VM snapshot, '
2508                                 'attempting again with quiescing disabled.'),
2509                               instance=instance)
2510         except (exception.InstanceQuiesceNotSupported,
2511                 exception.QemuGuestAgentNotEnabled) as err:
2512             # If the image says that quiesce is required then we need to fail.
2513             if self._requires_quiesce(image_meta):
2514                 raise
2515             LOG.info('Skipping quiescing instance: %(reason)s.',
2516                      {'reason': err}, instance=instance)
2517 
2518         try:
2519             guest.snapshot(snapshot, no_metadata=True, disk_only=True,
2520                            reuse_ext=True, quiesce=False)
2521         except libvirt.libvirtError:
2522             LOG.exception(_('Unable to create VM snapshot, '
2523                             'failing volume_snapshot operation.'),
2524                           instance=instance)
2525 
2526             raise
2527 
2528     def _volume_refresh_connection_info(self, context, instance, volume_id):
2529         bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
2530                   context, volume_id, instance.uuid)
2531 
2532         driver_bdm = driver_block_device.convert_volume(bdm)
2533         if driver_bdm:
2534             driver_bdm.refresh_connection_info(context, instance,
2535                                                self._volume_api, self)
2536 
2537     def volume_snapshot_create(self, context, instance, volume_id,
2538                                create_info):
2539         """Create snapshots of a Cinder volume via libvirt.
2540 
2541         :param instance: VM instance object reference
2542         :param volume_id: id of volume being snapshotted
2543         :param create_info: dict of information used to create snapshots
2544                      - snapshot_id : ID of snapshot
2545                      - type : qcow2 / <other>
2546                      - new_file : qcow2 file created by Cinder which
2547                      becomes the VM's active image after
2548                      the snapshot is complete
2549         """
2550 
2551         LOG.debug("volume_snapshot_create: create_info: %(c_info)s",
2552                   {'c_info': create_info}, instance=instance)
2553 
2554         try:
2555             guest = self._host.get_guest(instance)
2556         except exception.InstanceNotFound:
2557             raise exception.InstanceNotRunning(instance_id=instance.uuid)
2558 
2559         if create_info['type'] != 'qcow2':
2560             msg = _('Unknown type: %s') % create_info['type']
2561             raise exception.InternalError(msg)
2562 
2563         snapshot_id = create_info.get('snapshot_id', None)
2564         if snapshot_id is None:
2565             msg = _('snapshot_id required in create_info')
2566             raise exception.InternalError(msg)
2567 
2568         try:
2569             self._volume_snapshot_create(context, instance, guest,
2570                                          volume_id, create_info['new_file'])
2571         except Exception:
2572             with excutils.save_and_reraise_exception():
2573                 LOG.exception(_('Error occurred during '
2574                                 'volume_snapshot_create, '
2575                                 'sending error status to Cinder.'),
2576                               instance=instance)
2577                 self._volume_snapshot_update_status(
2578                     context, snapshot_id, 'error')
2579 
2580         self._volume_snapshot_update_status(
2581             context, snapshot_id, 'creating')
2582 
2583         def _wait_for_snapshot():
2584             snapshot = self._volume_api.get_snapshot(context, snapshot_id)
2585 
2586             if snapshot.get('status') != 'creating':
2587                 self._volume_refresh_connection_info(context, instance,
2588                                                      volume_id)
2589                 raise loopingcall.LoopingCallDone()
2590 
2591         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_snapshot)
2592         timer.start(interval=0.5).wait()
2593 
2594     @staticmethod
2595     def _rebase_with_qemu_img(guest, device, active_disk_object,
2596                               rebase_base):
2597         """Rebase a device tied to a guest using qemu-img.
2598 
2599         :param guest:the Guest which owns the device being rebased
2600         :type guest: nova.virt.libvirt.guest.Guest
2601         :param device: the guest block device to rebase
2602         :type device: nova.virt.libvirt.guest.BlockDevice
2603         :param active_disk_object: the guest block device to rebase
2604         :type active_disk_object: nova.virt.libvirt.config.\
2605                                     LibvirtConfigGuestDisk
2606         :param rebase_base: the new parent in the backing chain
2607         :type rebase_base: None or string
2608         """
2609 
2610         # It's unsure how well qemu-img handles network disks for
2611         # every protocol. So let's be safe.
2612         active_protocol = active_disk_object.source_protocol
2613         if active_protocol is not None:
2614             msg = _("Something went wrong when deleting a volume snapshot: "
2615                     "rebasing a %(protocol)s network disk using qemu-img "
2616                     "has not been fully tested") % {'protocol':
2617                     active_protocol}
2618             LOG.error(msg)
2619             raise exception.InternalError(msg)
2620 
2621         if rebase_base is None:
2622             # If backing_file is specified as "" (the empty string), then
2623             # the image is rebased onto no backing file (i.e. it will exist
2624             # independently of any backing file).
2625             backing_file = ""
2626             qemu_img_extra_arg = []
2627         else:
2628             # If the rebased image is going to have a backing file then
2629             # explicitly set the backing file format to avoid any security
2630             # concerns related to file format auto detection.
2631             backing_file = rebase_base
2632             b_file_fmt = images.qemu_img_info(backing_file).file_format
2633             qemu_img_extra_arg = ['-F', b_file_fmt]
2634 
2635         qemu_img_extra_arg.append(active_disk_object.source_path)
2636         # execute operation with disk concurrency semaphore
2637         with compute_utils.disk_ops_semaphore:
2638             processutils.execute("qemu-img", "rebase", "-b", backing_file,
2639                                  *qemu_img_extra_arg)
2640 
2641     def _volume_snapshot_delete(self, context, instance, volume_id,
2642                                 snapshot_id, delete_info=None):
2643         """Note:
2644             if file being merged into == active image:
2645                 do a blockRebase (pull) operation
2646             else:
2647                 do a blockCommit operation
2648             Files must be adjacent in snap chain.
2649 
2650         :param instance: instance object reference
2651         :param volume_id: volume UUID
2652         :param snapshot_id: snapshot UUID (unused currently)
2653         :param delete_info: {
2654             'type':              'qcow2',
2655             'file_to_merge':     'a.img',
2656             'merge_target_file': 'b.img' or None (if merging file_to_merge into
2657                                                   active image)
2658           }
2659         """
2660 
2661         LOG.debug('volume_snapshot_delete: delete_info: %s', delete_info,
2662                   instance=instance)
2663 
2664         if delete_info['type'] != 'qcow2':
2665             msg = _('Unknown delete_info type %s') % delete_info['type']
2666             raise exception.InternalError(msg)
2667 
2668         try:
2669             guest = self._host.get_guest(instance)
2670         except exception.InstanceNotFound:
2671             raise exception.InstanceNotRunning(instance_id=instance.uuid)
2672 
2673         # Find dev name
2674         my_dev = None
2675         active_disk = None
2676 
2677         xml = guest.get_xml_desc()
2678         xml_doc = etree.fromstring(xml)
2679 
2680         device_info = vconfig.LibvirtConfigGuest()
2681         device_info.parse_dom(xml_doc)
2682 
2683         active_disk_object = None
2684 
2685         for guest_disk in device_info.devices:
2686             if (guest_disk.root_name != 'disk'):
2687                 continue
2688 
2689             if (guest_disk.target_dev is None or guest_disk.serial is None):
2690                 continue
2691 
2692             if guest_disk.serial == volume_id:
2693                 my_dev = guest_disk.target_dev
2694 
2695                 active_disk = guest_disk.source_path
2696                 active_protocol = guest_disk.source_protocol
2697                 active_disk_object = guest_disk
2698                 break
2699 
2700         if my_dev is None or (active_disk is None and active_protocol is None):
2701             LOG.debug('Domain XML: %s', xml, instance=instance)
2702             msg = (_('Disk with id: %s not found attached to instance.')
2703                    % volume_id)
2704             raise exception.InternalError(msg)
2705 
2706         LOG.debug("found device at %s", my_dev, instance=instance)
2707 
2708         def _get_snap_dev(filename, backing_store):
2709             if filename is None:
2710                 msg = _('filename cannot be None')
2711                 raise exception.InternalError(msg)
2712 
2713             # libgfapi delete
2714             LOG.debug("XML: %s", xml)
2715 
2716             LOG.debug("active disk object: %s", active_disk_object)
2717 
2718             # determine reference within backing store for desired image
2719             filename_to_merge = filename
2720             matched_name = None
2721             b = backing_store
2722             index = None
2723 
2724             current_filename = active_disk_object.source_name.split('/')[1]
2725             if current_filename == filename_to_merge:
2726                 return my_dev + '[0]'
2727 
2728             while b is not None:
2729                 source_filename = b.source_name.split('/')[1]
2730                 if source_filename == filename_to_merge:
2731                     LOG.debug('found match: %s', b.source_name)
2732                     matched_name = b.source_name
2733                     index = b.index
2734                     break
2735 
2736                 b = b.backing_store
2737 
2738             if matched_name is None:
2739                 msg = _('no match found for %s') % (filename_to_merge)
2740                 raise exception.InternalError(msg)
2741 
2742             LOG.debug('index of match (%s) is %s', b.source_name, index)
2743 
2744             my_snap_dev = '%s[%s]' % (my_dev, index)
2745             return my_snap_dev
2746 
2747         if delete_info['merge_target_file'] is None:
2748             # pull via blockRebase()
2749 
2750             # Merge the most recent snapshot into the active image
2751 
2752             rebase_disk = my_dev
2753             rebase_base = delete_info['file_to_merge']  # often None
2754             if (active_protocol is not None) and (rebase_base is not None):
2755                 rebase_base = _get_snap_dev(rebase_base,
2756                                             active_disk_object.backing_store)
2757 
2758             # NOTE(deepakcs): libvirt added support for _RELATIVE in v1.2.7,
2759             # and when available this flag _must_ be used to ensure backing
2760             # paths are maintained relative by qemu.
2761             #
2762             # If _RELATIVE flag not found, continue with old behaviour
2763             # (relative backing path seems to work for this case)
2764             try:
2765                 libvirt.VIR_DOMAIN_BLOCK_REBASE_RELATIVE
2766                 relative = rebase_base is not None
2767             except AttributeError:
2768                 LOG.warning(
2769                     "Relative blockrebase support was not detected. "
2770                     "Continuing with old behaviour.")
2771                 relative = False
2772 
2773             LOG.debug(
2774                 'disk: %(disk)s, base: %(base)s, '
2775                 'bw: %(bw)s, relative: %(relative)s',
2776                 {'disk': rebase_disk,
2777                  'base': rebase_base,
2778                  'bw': libvirt_guest.BlockDevice.REBASE_DEFAULT_BANDWIDTH,
2779                  'relative': str(relative)}, instance=instance)
2780 
2781             dev = guest.get_block_device(rebase_disk)
2782             if guest.is_active():
2783                 result = dev.rebase(rebase_base, relative=relative)
2784                 if result == 0:
2785                     LOG.debug('blockRebase started successfully',
2786                               instance=instance)
2787 
2788                 while not dev.is_job_complete():
2789                     LOG.debug('waiting for blockRebase job completion',
2790                               instance=instance)
2791                     time.sleep(0.5)
2792 
2793             # If the guest is not running libvirt won't do a blockRebase.
2794             # In that case, let's ask qemu-img to rebase the disk.
2795             else:
2796                 LOG.debug('Guest is not running so doing a block rebase '
2797                           'using "qemu-img rebase"', instance=instance)
2798                 self._rebase_with_qemu_img(guest, dev, active_disk_object,
2799                                            rebase_base)
2800 
2801         else:
2802             # commit with blockCommit()
2803             my_snap_base = None
2804             my_snap_top = None
2805             commit_disk = my_dev
2806 
2807             if active_protocol is not None:
2808                 my_snap_base = _get_snap_dev(delete_info['merge_target_file'],
2809                                              active_disk_object.backing_store)
2810                 my_snap_top = _get_snap_dev(delete_info['file_to_merge'],
2811                                             active_disk_object.backing_store)
2812 
2813             commit_base = my_snap_base or delete_info['merge_target_file']
2814             commit_top = my_snap_top or delete_info['file_to_merge']
2815 
2816             LOG.debug('will call blockCommit with commit_disk=%(commit_disk)s '
2817                       'commit_base=%(commit_base)s '
2818                       'commit_top=%(commit_top)s ',
2819                       {'commit_disk': commit_disk,
2820                        'commit_base': commit_base,
2821                        'commit_top': commit_top}, instance=instance)
2822 
2823             dev = guest.get_block_device(commit_disk)
2824             result = dev.commit(commit_base, commit_top, relative=True)
2825 
2826             if result == 0:
2827                 LOG.debug('blockCommit started successfully',
2828                           instance=instance)
2829 
2830             while not dev.is_job_complete():
2831                 LOG.debug('waiting for blockCommit job completion',
2832                           instance=instance)
2833                 time.sleep(0.5)
2834 
2835     def volume_snapshot_delete(self, context, instance, volume_id, snapshot_id,
2836                                delete_info):
2837         try:
2838             self._volume_snapshot_delete(context, instance, volume_id,
2839                                          snapshot_id, delete_info=delete_info)
2840         except Exception:
2841             with excutils.save_and_reraise_exception():
2842                 LOG.exception(_('Error occurred during '
2843                                 'volume_snapshot_delete, '
2844                                 'sending error status to Cinder.'),
2845                               instance=instance)
2846                 self._volume_snapshot_update_status(
2847                     context, snapshot_id, 'error_deleting')
2848 
2849         self._volume_snapshot_update_status(context, snapshot_id, 'deleting')
2850         self._volume_refresh_connection_info(context, instance, volume_id)
2851 
2852     def reboot(self, context, instance, network_info, reboot_type,
2853                block_device_info=None, bad_volumes_callback=None):
2854         """Reboot a virtual machine, given an instance reference."""
2855         if reboot_type == 'SOFT':
2856             # NOTE(vish): This will attempt to do a graceful shutdown/restart.
2857             try:
2858                 soft_reboot_success = self._soft_reboot(instance)
2859             except libvirt.libvirtError as e:
2860                 LOG.debug("Instance soft reboot failed: %s",
2861                           encodeutils.exception_to_unicode(e),
2862                           instance=instance)
2863                 soft_reboot_success = False
2864 
2865             if soft_reboot_success:
2866                 LOG.info("Instance soft rebooted successfully.",
2867                          instance=instance)
2868                 return
2869             else:
2870                 LOG.warning("Failed to soft reboot instance. "
2871                             "Trying hard reboot.",
2872                             instance=instance)
2873         return self._hard_reboot(context, instance, network_info,
2874                                  block_device_info)
2875 
2876     def _soft_reboot(self, instance):
2877         """Attempt to shutdown and restart the instance gracefully.
2878 
2879         We use shutdown and create here so we can return if the guest
2880         responded and actually rebooted. Note that this method only
2881         succeeds if the guest responds to acpi. Therefore we return
2882         success or failure so we can fall back to a hard reboot if
2883         necessary.
2884 
2885         :returns: True if the reboot succeeded
2886         """
2887         guest = self._host.get_guest(instance)
2888 
2889         state = guest.get_power_state(self._host)
2890         old_domid = guest.id
2891         # NOTE(vish): This check allows us to reboot an instance that
2892         #             is already shutdown.
2893         if state == power_state.RUNNING:
2894             guest.shutdown()
2895         # NOTE(vish): This actually could take slightly longer than the
2896         #             FLAG defines depending on how long the get_info
2897         #             call takes to return.
2898         self._prepare_pci_devices_for_use(
2899             pci_manager.get_instance_pci_devs(instance, 'all'))
2900         for x in range(CONF.libvirt.wait_soft_reboot_seconds):
2901             guest = self._host.get_guest(instance)
2902 
2903             state = guest.get_power_state(self._host)
2904             new_domid = guest.id
2905 
2906             # NOTE(ivoks): By checking domain IDs, we make sure we are
2907             #              not recreating domain that's already running.
2908             if old_domid != new_domid:
2909                 if state in [power_state.SHUTDOWN,
2910                              power_state.CRASHED]:
2911                     LOG.info("Instance shutdown successfully.",
2912                              instance=instance)
2913                     self._create_domain(domain=guest._domain)
2914                     timer = loopingcall.FixedIntervalLoopingCall(
2915                         self._wait_for_running, instance)
2916                     timer.start(interval=0.5).wait()
2917                     return True
2918                 else:
2919                     LOG.info("Instance may have been rebooted during soft "
2920                              "reboot, so return now.", instance=instance)
2921                     return True
2922             greenthread.sleep(1)
2923         return False
2924 
2925     def _hard_reboot(self, context, instance, network_info,
2926                      block_device_info=None):
2927         """Reboot a virtual machine, given an instance reference.
2928 
2929         Performs a Libvirt reset (if supported) on the domain.
2930 
2931         If Libvirt reset is unavailable this method actually destroys and
2932         re-creates the domain to ensure the reboot happens, as the guest
2933         OS cannot ignore this action.
2934         """
2935         # NOTE(sbauza): Since we undefine the guest XML when destroying, we
2936         # need to remember the existing mdevs for reusing them.
2937         mdevs = self._get_all_assigned_mediated_devices(instance)
2938         mdevs = list(mdevs.keys())
2939         # NOTE(mdbooth): In addition to performing a hard reboot of the domain,
2940         # the hard reboot operation is relied upon by operators to be an
2941         # automated attempt to fix as many things as possible about a
2942         # non-functioning instance before resorting to manual intervention.
2943         # With this goal in mind, we tear down all the aspects of an instance
2944         # we can here without losing data. This allows us to re-initialise from
2945         # scratch, and hopefully fix, most aspects of a non-functioning guest.
2946         self.destroy(context, instance, network_info, destroy_disks=False,
2947                      block_device_info=block_device_info)
2948 
2949         # Convert the system metadata to image metadata
2950         # NOTE(mdbooth): This is a workaround for stateless Nova compute
2951         #                https://bugs.launchpad.net/nova/+bug/1349978
2952         instance_dir = libvirt_utils.get_instance_path(instance)
2953         fileutils.ensure_tree(instance_dir)
2954 
2955         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
2956                                             instance,
2957                                             instance.image_meta,
2958                                             block_device_info)
2959         # NOTE(vish): This could generate the wrong device_format if we are
2960         #             using the raw backend and the images don't exist yet.
2961         #             The create_images_and_backing below doesn't properly
2962         #             regenerate raw backend images, however, so when it
2963         #             does we need to (re)generate the xml after the images
2964         #             are in place.
2965         xml = self._get_guest_xml(context, instance, network_info, disk_info,
2966                                   instance.image_meta,
2967                                   block_device_info=block_device_info,
2968                                   mdevs=mdevs)
2969 
2970         # NOTE(mdbooth): context.auth_token will not be set when we call
2971         #                _hard_reboot from resume_state_on_host_boot()
2972         if context.auth_token is not None:
2973             # NOTE (rmk): Re-populate any missing backing files.
2974             config = vconfig.LibvirtConfigGuest()
2975             config.parse_str(xml)
2976             backing_disk_info = self._get_instance_disk_info_from_config(
2977                 config, block_device_info)
2978             self._create_images_and_backing(context, instance, instance_dir,
2979                                             backing_disk_info)
2980 
2981         # Initialize all the necessary networking, block devices and
2982         # start the instance.
2983         # NOTE(melwitt): Pass vifs_already_plugged=True here even though we've
2984         # unplugged vifs earlier. The behavior of neutron plug events depends
2985         # on which vif type we're using and we are working with a stale network
2986         # info cache here, so won't rely on waiting for neutron plug events.
2987         # vifs_already_plugged=True means "do not wait for neutron plug events"
2988         self._create_domain_and_network(context, xml, instance, network_info,
2989                                         block_device_info=block_device_info,
2990                                         vifs_already_plugged=True)
2991         self._prepare_pci_devices_for_use(
2992             pci_manager.get_instance_pci_devs(instance, 'all'))
2993 
2994         def _wait_for_reboot():
2995             """Called at an interval until the VM is running again."""
2996             state = self.get_info(instance).state
2997 
2998             if state == power_state.RUNNING:
2999                 LOG.info("Instance rebooted successfully.",
3000                          instance=instance)
3001                 raise loopingcall.LoopingCallDone()
3002 
3003         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_reboot)
3004         timer.start(interval=0.5).wait()
3005 
3006     def pause(self, instance):
3007         """Pause VM instance."""
3008         self._host.get_guest(instance).pause()
3009 
3010     def unpause(self, instance):
3011         """Unpause paused VM instance."""
3012         guest = self._host.get_guest(instance)
3013         guest.resume()
3014         guest.sync_guest_time()
3015 
3016     def _clean_shutdown(self, instance, timeout, retry_interval):
3017         """Attempt to shutdown the instance gracefully.
3018 
3019         :param instance: The instance to be shutdown
3020         :param timeout: How long to wait in seconds for the instance to
3021                         shutdown
3022         :param retry_interval: How often in seconds to signal the instance
3023                                to shutdown while waiting
3024 
3025         :returns: True if the shutdown succeeded
3026         """
3027 
3028         # List of states that represent a shutdown instance
3029         SHUTDOWN_STATES = [power_state.SHUTDOWN,
3030                            power_state.CRASHED]
3031 
3032         try:
3033             guest = self._host.get_guest(instance)
3034         except exception.InstanceNotFound:
3035             # If the instance has gone then we don't need to
3036             # wait for it to shutdown
3037             return True
3038 
3039         state = guest.get_power_state(self._host)
3040         if state in SHUTDOWN_STATES:
3041             LOG.info("Instance already shutdown.", instance=instance)
3042             return True
3043 
3044         LOG.debug("Shutting down instance from state %s", state,
3045                   instance=instance)
3046         guest.shutdown()
3047         retry_countdown = retry_interval
3048 
3049         for sec in range(timeout):
3050 
3051             guest = self._host.get_guest(instance)
3052             state = guest.get_power_state(self._host)
3053 
3054             if state in SHUTDOWN_STATES:
3055                 LOG.info("Instance shutdown successfully after %d seconds.",
3056                          sec, instance=instance)
3057                 return True
3058 
3059             # Note(PhilD): We can't assume that the Guest was able to process
3060             #              any previous shutdown signal (for example it may
3061             #              have still been startingup, so within the overall
3062             #              timeout we re-trigger the shutdown every
3063             #              retry_interval
3064             if retry_countdown == 0:
3065                 retry_countdown = retry_interval
3066                 # Instance could shutdown at any time, in which case we
3067                 # will get an exception when we call shutdown
3068                 try:
3069                     LOG.debug("Instance in state %s after %d seconds - "
3070                               "resending shutdown", state, sec,
3071                               instance=instance)
3072                     guest.shutdown()
3073                 except libvirt.libvirtError:
3074                     # Assume this is because its now shutdown, so loop
3075                     # one more time to clean up.
3076                     LOG.debug("Ignoring libvirt exception from shutdown "
3077                               "request.", instance=instance)
3078                     continue
3079             else:
3080                 retry_countdown -= 1
3081 
3082             time.sleep(1)
3083 
3084         LOG.info("Instance failed to shutdown in %d seconds.",
3085                  timeout, instance=instance)
3086         return False
3087 
3088     def power_off(self, instance, timeout=0, retry_interval=0):
3089         """Power off the specified instance."""
3090         if timeout:
3091             self._clean_shutdown(instance, timeout, retry_interval)
3092         self._destroy(instance)
3093 
3094     def power_on(self, context, instance, network_info,
3095                  block_device_info=None):
3096         """Power on the specified instance."""
3097         # We use _hard_reboot here to ensure that all backing files,
3098         # network, and block device connections, etc. are established
3099         # and available before we attempt to start the instance.
3100         self._hard_reboot(context, instance, network_info, block_device_info)
3101 
3102     def trigger_crash_dump(self, instance):
3103 
3104         """Trigger crash dump by injecting an NMI to the specified instance."""
3105         try:
3106             self._host.get_guest(instance).inject_nmi()
3107         except libvirt.libvirtError as ex:
3108             error_code = ex.get_error_code()
3109 
3110             if error_code == libvirt.VIR_ERR_NO_SUPPORT:
3111                 raise exception.TriggerCrashDumpNotSupported()
3112             elif error_code == libvirt.VIR_ERR_OPERATION_INVALID:
3113                 raise exception.InstanceNotRunning(instance_id=instance.uuid)
3114 
3115             LOG.exception(_('Error from libvirt while injecting an NMI to '
3116                             '%(instance_uuid)s: '
3117                             '[Error Code %(error_code)s] %(ex)s'),
3118                           {'instance_uuid': instance.uuid,
3119                            'error_code': error_code, 'ex': ex})
3120             raise
3121 
3122     def suspend(self, context, instance):
3123         """Suspend the specified instance."""
3124         guest = self._host.get_guest(instance)
3125 
3126         self._detach_pci_devices(guest,
3127             pci_manager.get_instance_pci_devs(instance))
3128         self._detach_direct_passthrough_ports(context, instance, guest)
3129         self._detach_mediated_devices(guest)
3130         guest.save_memory_state()
3131 
3132     def resume(self, context, instance, network_info, block_device_info=None):
3133         """resume the specified instance."""
3134         xml = self._get_existing_domain_xml(instance, network_info,
3135                                             block_device_info)
3136         guest = self._create_domain_and_network(context, xml, instance,
3137                            network_info, block_device_info=block_device_info,
3138                            vifs_already_plugged=True)
3139         self._attach_pci_devices(guest,
3140             pci_manager.get_instance_pci_devs(instance))
3141         self._attach_direct_passthrough_ports(
3142             context, instance, guest, network_info)
3143         timer = loopingcall.FixedIntervalLoopingCall(self._wait_for_running,
3144                                                      instance)
3145         timer.start(interval=0.5).wait()
3146         guest.sync_guest_time()
3147 
3148     def resume_state_on_host_boot(self, context, instance, network_info,
3149                                   block_device_info=None):
3150         """resume guest state when a host is booted."""
3151         # Check if the instance is running already and avoid doing
3152         # anything if it is.
3153         try:
3154             guest = self._host.get_guest(instance)
3155             state = guest.get_power_state(self._host)
3156 
3157             ignored_states = (power_state.RUNNING,
3158                               power_state.SUSPENDED,
3159                               power_state.NOSTATE,
3160                               power_state.PAUSED)
3161 
3162             if state in ignored_states:
3163                 return
3164         except (exception.InternalError, exception.InstanceNotFound):
3165             pass
3166 
3167         # Instance is not up and could be in an unknown state.
3168         # Be as absolute as possible about getting it back into
3169         # a known and running state.
3170         self._hard_reboot(context, instance, network_info, block_device_info)
3171 
3172     def rescue(self, context, instance, network_info, image_meta,
3173                rescue_password):
3174         """Loads a VM using rescue images.
3175 
3176         A rescue is normally performed when something goes wrong with the
3177         primary images and data needs to be corrected/recovered. Rescuing
3178         should not edit or over-ride the original image, only allow for
3179         data recovery.
3180 
3181         """
3182         instance_dir = libvirt_utils.get_instance_path(instance)
3183         unrescue_xml = self._get_existing_domain_xml(instance, network_info)
3184         unrescue_xml_path = os.path.join(instance_dir, 'unrescue.xml')
3185         libvirt_utils.write_to_file(unrescue_xml_path, unrescue_xml)
3186 
3187         rescue_image_id = None
3188         if image_meta.obj_attr_is_set("id"):
3189             rescue_image_id = image_meta.id
3190 
3191         rescue_images = {
3192             'image_id': (rescue_image_id or
3193                         CONF.libvirt.rescue_image_id or instance.image_ref),
3194             'kernel_id': (CONF.libvirt.rescue_kernel_id or
3195                           instance.kernel_id),
3196             'ramdisk_id': (CONF.libvirt.rescue_ramdisk_id or
3197                            instance.ramdisk_id),
3198         }
3199         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
3200                                             instance,
3201                                             image_meta,
3202                                             rescue=True)
3203         injection_info = InjectionInfo(network_info=network_info,
3204                                        admin_pass=rescue_password,
3205                                        files=None)
3206         gen_confdrive = functools.partial(self._create_configdrive,
3207                                           context, instance, injection_info,
3208                                           rescue=True)
3209         # NOTE(sbauza): Since rescue recreates the guest XML, we need to
3210         # remember the existing mdevs for reusing them.
3211         mdevs = self._get_all_assigned_mediated_devices(instance)
3212         mdevs = list(mdevs.keys())
3213         self._create_image(context, instance, disk_info['mapping'],
3214                            injection_info=injection_info, suffix='.rescue',
3215                            disk_images=rescue_images)
3216         xml = self._get_guest_xml(context, instance, network_info, disk_info,
3217                                   image_meta, rescue=rescue_images,
3218                                   mdevs=mdevs)
3219         self._destroy(instance)
3220         self._create_domain(xml, post_xml_callback=gen_confdrive)
3221 
3222     def unrescue(self, instance, network_info):
3223         """Reboot the VM which is being rescued back into primary images.
3224         """
3225         instance_dir = libvirt_utils.get_instance_path(instance)
3226         unrescue_xml_path = os.path.join(instance_dir, 'unrescue.xml')
3227         xml = libvirt_utils.load_file(unrescue_xml_path)
3228         guest = self._host.get_guest(instance)
3229 
3230         # TODO(sahid): We are converting all calls from a
3231         # virDomain object to use nova.virt.libvirt.Guest.
3232         # We should be able to remove virt_dom at the end.
3233         virt_dom = guest._domain
3234         self._destroy(instance)
3235         self._create_domain(xml, virt_dom)
3236         os.unlink(unrescue_xml_path)
3237         rescue_files = os.path.join(instance_dir, "*.rescue")
3238         for rescue_file in glob.iglob(rescue_files):
3239             if os.path.isdir(rescue_file):
3240                 shutil.rmtree(rescue_file)
3241             else:
3242                 os.unlink(rescue_file)
3243         # cleanup rescue volume
3244         lvm.remove_volumes([lvmdisk for lvmdisk in self._lvm_disks(instance)
3245                                 if lvmdisk.endswith('.rescue')])
3246         if CONF.libvirt.images_type == 'rbd':
3247             filter_fn = lambda disk: (disk.startswith(instance.uuid) and
3248                                       disk.endswith('.rescue'))
3249             LibvirtDriver._get_rbd_driver().cleanup_volumes(filter_fn)
3250 
3251     def poll_rebooting_instances(self, timeout, instances):
3252         pass
3253 
3254     def spawn(self, context, instance, image_meta, injected_files,
3255               admin_password, allocations, network_info=None,
3256               block_device_info=None):
3257         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
3258                                             instance,
3259                                             image_meta,
3260                                             block_device_info)
3261         injection_info = InjectionInfo(network_info=network_info,
3262                                        files=injected_files,
3263                                        admin_pass=admin_password)
3264         gen_confdrive = functools.partial(self._create_configdrive,
3265                                           context, instance,
3266                                           injection_info)
3267         self._create_image(context, instance, disk_info['mapping'],
3268                            injection_info=injection_info,
3269                            block_device_info=block_device_info)
3270 
3271         # Required by Quobyte CI
3272         self._ensure_console_log_for_instance(instance)
3273 
3274         # Does the guest need to be assigned some vGPU mediated devices ?
3275         mdevs = self._allocate_mdevs(allocations)
3276 
3277         xml = self._get_guest_xml(context, instance, network_info,
3278                                   disk_info, image_meta,
3279                                   block_device_info=block_device_info,
3280                                   mdevs=mdevs)
3281         self._create_domain_and_network(
3282             context, xml, instance, network_info,
3283             block_device_info=block_device_info,
3284             post_xml_callback=gen_confdrive,
3285             destroy_disks_on_failure=True)
3286         LOG.debug("Guest created on hypervisor", instance=instance)
3287 
3288         def _wait_for_boot():
3289             """Called at an interval until the VM is running."""
3290             state = self.get_info(instance).state
3291 
3292             if state == power_state.RUNNING:
3293                 LOG.info("Instance spawned successfully.", instance=instance)
3294                 raise loopingcall.LoopingCallDone()
3295 
3296         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_boot)
3297         timer.start(interval=0.5).wait()
3298 
3299     def _get_console_output_file(self, instance, console_log):
3300         bytes_to_read = MAX_CONSOLE_BYTES
3301         log_data = b""  # The last N read bytes
3302         i = 0  # in case there is a log rotation (like "virtlogd")
3303         path = console_log
3304 
3305         while bytes_to_read > 0 and os.path.exists(path):
3306             read_log_data, remaining = nova.privsep.path.last_bytes(
3307                                         path, bytes_to_read)
3308             # We need the log file content in chronological order,
3309             # that's why we *prepend* the log data.
3310             log_data = read_log_data + log_data
3311 
3312             # Prep to read the next file in the chain
3313             bytes_to_read -= len(read_log_data)
3314             path = console_log + "." + str(i)
3315             i += 1
3316 
3317             if remaining > 0:
3318                 LOG.info('Truncated console log returned, '
3319                          '%d bytes ignored', remaining, instance=instance)
3320         return log_data
3321 
3322     def get_console_output(self, context, instance):
3323         guest = self._host.get_guest(instance)
3324 
3325         xml = guest.get_xml_desc()
3326         tree = etree.fromstring(xml)
3327 
3328         # check for different types of consoles
3329         path_sources = [
3330             ('file', "./devices/console[@type='file']/source[@path]", 'path'),
3331             ('tcp', "./devices/console[@type='tcp']/log[@file]", 'file'),
3332             ('pty', "./devices/console[@type='pty']/source[@path]", 'path')]
3333         console_type = ""
3334         console_path = ""
3335         for c_type, epath, attrib in path_sources:
3336             node = tree.find(epath)
3337             if (node is not None) and node.get(attrib):
3338                 console_type = c_type
3339                 console_path = node.get(attrib)
3340                 break
3341 
3342         # instance has no console at all
3343         if not console_path:
3344             raise exception.ConsoleNotAvailable()
3345 
3346         # instance has a console, but file doesn't exist (yet?)
3347         if not os.path.exists(console_path):
3348             LOG.info('console logfile for instance does not exist',
3349                       instance=instance)
3350             return ""
3351 
3352         # pty consoles need special handling
3353         if console_type == 'pty':
3354             console_log = self._get_console_log_path(instance)
3355             data = nova.privsep.libvirt.readpty(console_path)
3356 
3357             # NOTE(markus_z): The virt_types kvm and qemu are the only ones
3358             # which create a dedicated file device for the console logging.
3359             # Other virt_types like xen, lxc, uml, parallels depend on the
3360             # flush of that pty device into the "console.log" file to ensure
3361             # that a series of "get_console_output" calls return the complete
3362             # content even after rebooting a guest.
3363             nova.privsep.path.writefile(console_log, 'a+', data)
3364 
3365             # set console path to logfile, not to pty device
3366             console_path = console_log
3367 
3368         # return logfile content
3369         return self._get_console_output_file(instance, console_path)
3370 
3371     def get_host_ip_addr(self):
3372         ips = compute_utils.get_machine_ips()
3373         if CONF.my_ip not in ips:
3374             LOG.warning('my_ip address (%(my_ip)s) was not found on '
3375                         'any of the interfaces: %(ifaces)s',
3376                         {'my_ip': CONF.my_ip, 'ifaces': ", ".join(ips)})
3377         return CONF.my_ip
3378 
3379     def get_vnc_console(self, context, instance):
3380         def get_vnc_port_for_instance(instance_name):
3381             guest = self._host.get_guest(instance)
3382 
3383             xml = guest.get_xml_desc()
3384             xml_dom = etree.fromstring(xml)
3385 
3386             graphic = xml_dom.find("./devices/graphics[@type='vnc']")
3387             if graphic is not None:
3388                 return graphic.get('port')
3389             # NOTE(rmk): We had VNC consoles enabled but the instance in
3390             # question is not actually listening for connections.
3391             raise exception.ConsoleTypeUnavailable(console_type='vnc')
3392 
3393         port = get_vnc_port_for_instance(instance.name)
3394         host = CONF.vnc.server_proxyclient_address
3395 
3396         return ctype.ConsoleVNC(host=host, port=port)
3397 
3398     def get_spice_console(self, context, instance):
3399         def get_spice_ports_for_instance(instance_name):
3400             guest = self._host.get_guest(instance)
3401 
3402             xml = guest.get_xml_desc()
3403             xml_dom = etree.fromstring(xml)
3404 
3405             graphic = xml_dom.find("./devices/graphics[@type='spice']")
3406             if graphic is not None:
3407                 return (graphic.get('port'), graphic.get('tlsPort'))
3408             # NOTE(rmk): We had Spice consoles enabled but the instance in
3409             # question is not actually listening for connections.
3410             raise exception.ConsoleTypeUnavailable(console_type='spice')
3411 
3412         ports = get_spice_ports_for_instance(instance.name)
3413         host = CONF.spice.server_proxyclient_address
3414 
3415         return ctype.ConsoleSpice(host=host, port=ports[0], tlsPort=ports[1])
3416 
3417     def get_serial_console(self, context, instance):
3418         guest = self._host.get_guest(instance)
3419         for hostname, port in self._get_serial_ports_from_guest(
3420                 guest, mode='bind'):
3421             return ctype.ConsoleSerial(host=hostname, port=port)
3422         raise exception.ConsoleTypeUnavailable(console_type='serial')
3423 
3424     @staticmethod
3425     def _create_ephemeral(target, ephemeral_size,
3426                           fs_label, os_type, is_block_dev=False,
3427                           context=None, specified_fs=None,
3428                           vm_mode=None):
3429         if not is_block_dev:
3430             if (CONF.libvirt.virt_type == "parallels" and
3431                     vm_mode == fields.VMMode.EXE):
3432 
3433                 libvirt_utils.create_ploop_image('expanded', target,
3434                                                  '%dG' % ephemeral_size,
3435                                                  specified_fs)
3436                 return
3437             libvirt_utils.create_image('raw', target, '%dG' % ephemeral_size)
3438 
3439         # Run as root only for block devices.
3440         disk_api.mkfs(os_type, fs_label, target, run_as_root=is_block_dev,
3441                       specified_fs=specified_fs)
3442 
3443     @staticmethod
3444     def _create_swap(target, swap_mb, context=None):
3445         """Create a swap file of specified size."""
3446         libvirt_utils.create_image('raw', target, '%dM' % swap_mb)
3447         nova.privsep.fs.unprivileged_mkfs('swap', target)
3448 
3449     @staticmethod
3450     def _get_console_log_path(instance):
3451         return os.path.join(libvirt_utils.get_instance_path(instance),
3452                             'console.log')
3453 
3454     def _ensure_console_log_for_instance(self, instance):
3455         # NOTE(mdbooth): Although libvirt will create this file for us
3456         # automatically when it starts, it will initially create it with
3457         # root ownership and then chown it depending on the configuration of
3458         # the domain it is launching. Quobyte CI explicitly disables the
3459         # chown by setting dynamic_ownership=0 in libvirt's config.
3460         # Consequently when the domain starts it is unable to write to its
3461         # console.log. See bug https://bugs.launchpad.net/nova/+bug/1597644
3462         #
3463         # To work around this, we create the file manually before starting
3464         # the domain so it has the same ownership as Nova. This works
3465         # for Quobyte CI because it is also configured to run qemu as the same
3466         # user as the Nova service. Installations which don't set
3467         # dynamic_ownership=0 are not affected because libvirt will always
3468         # correctly configure permissions regardless of initial ownership.
3469         #
3470         # Setting dynamic_ownership=0 is dubious and potentially broken in
3471         # more ways than console.log (see comment #22 on the above bug), so
3472         # Future Maintainer who finds this code problematic should check to see
3473         # if we still support it.
3474         console_file = self._get_console_log_path(instance)
3475         LOG.debug('Ensure instance console log exists: %s', console_file,
3476                   instance=instance)
3477         try:
3478             libvirt_utils.file_open(console_file, 'a').close()
3479         # NOTE(sfinucan): We can safely ignore permission issues here and
3480         # assume that it is libvirt that has taken ownership of this file.
3481         except IOError as ex:
3482             if ex.errno != errno.EACCES:
3483                 raise
3484             LOG.debug('Console file already exists: %s.', console_file)
3485 
3486     @staticmethod
3487     def _get_disk_config_image_type():
3488         # TODO(mikal): there is a bug here if images_type has
3489         # changed since creation of the instance, but I am pretty
3490         # sure that this bug already exists.
3491         return 'rbd' if CONF.libvirt.images_type == 'rbd' else 'raw'
3492 
3493     @staticmethod
3494     def _is_booted_from_volume(block_device_info):
3495         """Determines whether the VM is booting from volume
3496 
3497         Determines whether the block device info indicates that the VM
3498         is booting from a volume.
3499         """
3500         block_device_mapping = driver.block_device_info_get_mapping(
3501             block_device_info)
3502         return bool(block_device.get_root_bdm(block_device_mapping))
3503 
3504     def _inject_data(self, disk, instance, injection_info):
3505         """Injects data in a disk image
3506 
3507         Helper used for injecting data in a disk image file system.
3508 
3509         :param disk: The disk we're injecting into (an Image object)
3510         :param instance: The instance we're injecting into
3511         :param injection_info: Injection info
3512         """
3513         # Handles the partition need to be used.
3514         LOG.debug('Checking root disk injection %s',
3515                   str(injection_info), instance=instance)
3516         target_partition = None
3517         if not instance.kernel_id:
3518             target_partition = CONF.libvirt.inject_partition
3519             if target_partition == 0:
3520                 target_partition = None
3521         if CONF.libvirt.virt_type == 'lxc':
3522             target_partition = None
3523 
3524         # Handles the key injection.
3525         if CONF.libvirt.inject_key and instance.get('key_data'):
3526             key = str(instance.key_data)
3527         else:
3528             key = None
3529 
3530         # Handles the admin password injection.
3531         if not CONF.libvirt.inject_password:
3532             admin_pass = None
3533         else:
3534             admin_pass = injection_info.admin_pass
3535 
3536         # Handles the network injection.
3537         net = netutils.get_injected_network_template(
3538             injection_info.network_info,
3539             libvirt_virt_type=CONF.libvirt.virt_type)
3540 
3541         # Handles the metadata injection
3542         metadata = instance.get('metadata')
3543 
3544         if any((key, net, metadata, admin_pass, injection_info.files)):
3545             LOG.debug('Injecting %s', str(injection_info),
3546                       instance=instance)
3547             img_id = instance.image_ref
3548             try:
3549                 disk_api.inject_data(disk.get_model(self._conn),
3550                                      key, net, metadata, admin_pass,
3551                                      injection_info.files,
3552                                      partition=target_partition,
3553                                      mandatory=('files',))
3554             except Exception as e:
3555                 with excutils.save_and_reraise_exception():
3556                     LOG.error('Error injecting data into image '
3557                               '%(img_id)s (%(e)s)',
3558                               {'img_id': img_id, 'e': e},
3559                               instance=instance)
3560 
3561     # NOTE(sileht): many callers of this method assume that this
3562     # method doesn't fail if an image already exists but instead
3563     # think that it will be reused (ie: (live)-migration/resize)
3564     def _create_image(self, context, instance,
3565                       disk_mapping, injection_info=None, suffix='',
3566                       disk_images=None, block_device_info=None,
3567                       fallback_from_host=None,
3568                       ignore_bdi_for_swap=False):
3569         booted_from_volume = self._is_booted_from_volume(block_device_info)
3570 
3571         def image(fname, image_type=CONF.libvirt.images_type):
3572             return self.image_backend.by_name(instance,
3573                                               fname + suffix, image_type)
3574 
3575         def raw(fname):
3576             return image(fname, image_type='raw')
3577 
3578         # ensure directories exist and are writable
3579         fileutils.ensure_tree(libvirt_utils.get_instance_path(instance))
3580 
3581         LOG.info('Creating image', instance=instance)
3582 
3583         inst_type = instance.get_flavor()
3584         swap_mb = 0
3585         if 'disk.swap' in disk_mapping:
3586             mapping = disk_mapping['disk.swap']
3587 
3588             if ignore_bdi_for_swap:
3589                 # This is a workaround to support legacy swap resizing,
3590                 # which does not touch swap size specified in bdm,
3591                 # but works with flavor specified size only.
3592                 # In this case we follow the legacy logic and ignore block
3593                 # device info completely.
3594                 # NOTE(ft): This workaround must be removed when a correct
3595                 # implementation of resize operation changing sizes in bdms is
3596                 # developed. Also at that stage we probably may get rid of
3597                 # the direct usage of flavor swap size here,
3598                 # leaving the work with bdm only.
3599                 swap_mb = inst_type['swap']
3600             else:
3601                 swap = driver.block_device_info_get_swap(block_device_info)
3602                 if driver.swap_is_usable(swap):
3603                     swap_mb = swap['swap_size']
3604                 elif (inst_type['swap'] > 0 and
3605                       not block_device.volume_in_mapping(
3606                         mapping['dev'], block_device_info)):
3607                     swap_mb = inst_type['swap']
3608 
3609             if swap_mb > 0:
3610                 if (CONF.libvirt.virt_type == "parallels" and
3611                         instance.vm_mode == fields.VMMode.EXE):
3612                     msg = _("Swap disk is not supported "
3613                             "for Virtuozzo container")
3614                     raise exception.Invalid(msg)
3615 
3616         if not disk_images:
3617             disk_images = {'image_id': instance.image_ref,
3618                            'kernel_id': instance.kernel_id,
3619                            'ramdisk_id': instance.ramdisk_id}
3620 
3621         if disk_images['kernel_id']:
3622             fname = imagecache.get_cache_fname(disk_images['kernel_id'])
3623             raw('kernel').cache(fetch_func=libvirt_utils.fetch_raw_image,
3624                                 context=context,
3625                                 filename=fname,
3626                                 image_id=disk_images['kernel_id'])
3627             if disk_images['ramdisk_id']:
3628                 fname = imagecache.get_cache_fname(disk_images['ramdisk_id'])
3629                 raw('ramdisk').cache(fetch_func=libvirt_utils.fetch_raw_image,
3630                                      context=context,
3631                                      filename=fname,
3632                                      image_id=disk_images['ramdisk_id'])
3633 
3634         if CONF.libvirt.virt_type == 'uml':
3635             # PONDERING(mikal): can I assume that root is UID zero in every
3636             # OS? Probably not.
3637             uid = pwd.getpwnam('root').pw_uid
3638             nova.privsep.path.chown(image('disk').path, uid=uid)
3639 
3640         self._create_and_inject_local_root(context, instance,
3641                                            booted_from_volume, suffix,
3642                                            disk_images, injection_info,
3643                                            fallback_from_host)
3644 
3645         # Lookup the filesystem type if required
3646         os_type_with_default = nova.privsep.fs.get_fs_type_for_os_type(
3647             instance.os_type)
3648         # Generate a file extension based on the file system
3649         # type and the mkfs commands configured if any
3650         file_extension = nova.privsep.fs.get_file_extension_for_os_type(
3651             os_type_with_default, CONF.default_ephemeral_format)
3652 
3653         vm_mode = fields.VMMode.get_from_instance(instance)
3654         ephemeral_gb = instance.flavor.ephemeral_gb
3655         if 'disk.local' in disk_mapping:
3656             disk_image = image('disk.local')
3657             fn = functools.partial(self._create_ephemeral,
3658                                    fs_label='ephemeral0',
3659                                    os_type=instance.os_type,
3660                                    is_block_dev=disk_image.is_block_dev,
3661                                    vm_mode=vm_mode)
3662             fname = "ephemeral_%s_%s" % (ephemeral_gb, file_extension)
3663             size = ephemeral_gb * units.Gi
3664             disk_image.cache(fetch_func=fn,
3665                              context=context,
3666                              filename=fname,
3667                              size=size,
3668                              ephemeral_size=ephemeral_gb)
3669 
3670         for idx, eph in enumerate(driver.block_device_info_get_ephemerals(
3671                 block_device_info)):
3672             disk_image = image(blockinfo.get_eph_disk(idx))
3673 
3674             specified_fs = eph.get('guest_format')
3675             if specified_fs and not self.is_supported_fs_format(specified_fs):
3676                 msg = _("%s format is not supported") % specified_fs
3677                 raise exception.InvalidBDMFormat(details=msg)
3678 
3679             fn = functools.partial(self._create_ephemeral,
3680                                    fs_label='ephemeral%d' % idx,
3681                                    os_type=instance.os_type,
3682                                    is_block_dev=disk_image.is_block_dev,
3683                                    vm_mode=vm_mode)
3684             size = eph['size'] * units.Gi
3685             fname = "ephemeral_%s_%s" % (eph['size'], file_extension)
3686             disk_image.cache(fetch_func=fn,
3687                              context=context,
3688                              filename=fname,
3689                              size=size,
3690                              ephemeral_size=eph['size'],
3691                              specified_fs=specified_fs)
3692 
3693         if swap_mb > 0:
3694             size = swap_mb * units.Mi
3695             image('disk.swap').cache(fetch_func=self._create_swap,
3696                                      context=context,
3697                                      filename="swap_%s" % swap_mb,
3698                                      size=size,
3699                                      swap_mb=swap_mb)
3700 
3701     def _create_and_inject_local_root(self, context, instance,
3702                                       booted_from_volume, suffix, disk_images,
3703                                       injection_info, fallback_from_host):
3704         # File injection only if needed
3705         need_inject = (not configdrive.required_by(instance) and
3706                        injection_info is not None and
3707                        CONF.libvirt.inject_partition != -2)
3708 
3709         # NOTE(ndipanov): Even if disk_mapping was passed in, which
3710         # currently happens only on rescue - we still don't want to
3711         # create a base image.
3712         if not booted_from_volume:
3713             root_fname = imagecache.get_cache_fname(disk_images['image_id'])
3714             size = instance.flavor.root_gb * units.Gi
3715 
3716             if size == 0 or suffix == '.rescue':
3717                 size = None
3718 
3719             backend = self.image_backend.by_name(instance, 'disk' + suffix,
3720                                                  CONF.libvirt.images_type)
3721             if instance.task_state == task_states.RESIZE_FINISH:
3722                 backend.create_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME)
3723             if backend.SUPPORTS_CLONE:
3724                 def clone_fallback_to_fetch(*args, **kwargs):
3725                     try:
3726                         backend.clone(context, disk_images['image_id'])
3727                     except exception.ImageUnacceptable:
3728                         libvirt_utils.fetch_image(*args, **kwargs)
3729                 fetch_func = clone_fallback_to_fetch
3730             else:
3731                 fetch_func = libvirt_utils.fetch_image
3732             self._try_fetch_image_cache(backend, fetch_func, context,
3733                                         root_fname, disk_images['image_id'],
3734                                         instance, size, fallback_from_host)
3735 
3736             if need_inject:
3737                 self._inject_data(backend, instance, injection_info)
3738 
3739         elif need_inject:
3740             LOG.warning('File injection into a boot from volume '
3741                         'instance is not supported', instance=instance)
3742 
3743     def _create_configdrive(self, context, instance, injection_info,
3744                             rescue=False):
3745         # As this method being called right after the definition of a
3746         # domain, but before its actual launch, device metadata will be built
3747         # and saved in the instance for it to be used by the config drive and
3748         # the metadata service.
3749         instance.device_metadata = self._build_device_metadata(context,
3750                                                                instance)
3751         if configdrive.required_by(instance):
3752             LOG.info('Using config drive', instance=instance)
3753 
3754             name = 'disk.config'
3755             if rescue:
3756                 name += '.rescue'
3757 
3758             config_disk = self.image_backend.by_name(
3759                 instance, name, self._get_disk_config_image_type())
3760 
3761             # Don't overwrite an existing config drive
3762             if not config_disk.exists():
3763                 extra_md = {}
3764                 if injection_info.admin_pass:
3765                     extra_md['admin_pass'] = injection_info.admin_pass
3766 
3767                 inst_md = instance_metadata.InstanceMetadata(
3768                     instance, content=injection_info.files, extra_md=extra_md,
3769                     network_info=injection_info.network_info,
3770                     request_context=context)
3771 
3772                 cdb = configdrive.ConfigDriveBuilder(instance_md=inst_md)
3773                 with cdb:
3774                     # NOTE(mdbooth): We're hardcoding here the path of the
3775                     # config disk when using the flat backend. This isn't
3776                     # good, but it's required because we need a local path we
3777                     # know we can write to in case we're subsequently
3778                     # importing into rbd. This will be cleaned up when we
3779                     # replace this with a call to create_from_func, but that
3780                     # can't happen until we've updated the backends and we
3781                     # teach them not to cache config disks. This isn't
3782                     # possible while we're still using cache() under the hood.
3783                     config_disk_local_path = os.path.join(
3784                         libvirt_utils.get_instance_path(instance), name)
3785                     LOG.info('Creating config drive at %(path)s',
3786                              {'path': config_disk_local_path},
3787                              instance=instance)
3788 
3789                     try:
3790                         cdb.make_drive(config_disk_local_path)
3791                     except processutils.ProcessExecutionError as e:
3792                         with excutils.save_and_reraise_exception():
3793                             LOG.error('Creating config drive failed with '
3794                                       'error: %s', e, instance=instance)
3795 
3796                 try:
3797                     config_disk.import_file(
3798                         instance, config_disk_local_path, name)
3799                 finally:
3800                     # NOTE(mikal): if the config drive was imported into RBD,
3801                     # then we no longer need the local copy
3802                     if CONF.libvirt.images_type == 'rbd':
3803                         LOG.info('Deleting local config drive %(path)s '
3804                                  'because it was imported into RBD.',
3805                                  {'path': config_disk_local_path},
3806                                  instance=instance)
3807                         os.unlink(config_disk_local_path)
3808 
3809     def _prepare_pci_devices_for_use(self, pci_devices):
3810         # kvm , qemu support managed mode
3811         # In managed mode, the configured device will be automatically
3812         # detached from the host OS drivers when the guest is started,
3813         # and then re-attached when the guest shuts down.
3814         if CONF.libvirt.virt_type != 'xen':
3815             # we do manual detach only for xen
3816             return
3817         try:
3818             for dev in pci_devices:
3819                 libvirt_dev_addr = dev['hypervisor_name']
3820                 libvirt_dev = \
3821                         self._host.device_lookup_by_name(libvirt_dev_addr)
3822                 # Note(yjiang5) Spelling for 'dettach' is correct, see
3823                 # http://libvirt.org/html/libvirt-libvirt.html.
3824                 libvirt_dev.dettach()
3825 
3826             # Note(yjiang5): A reset of one PCI device may impact other
3827             # devices on the same bus, thus we need two separated loops
3828             # to detach and then reset it.
3829             for dev in pci_devices:
3830                 libvirt_dev_addr = dev['hypervisor_name']
3831                 libvirt_dev = \
3832                         self._host.device_lookup_by_name(libvirt_dev_addr)
3833                 libvirt_dev.reset()
3834 
3835         except libvirt.libvirtError as exc:
3836             raise exception.PciDevicePrepareFailed(id=dev['id'],
3837                                                    instance_uuid=
3838                                                    dev['instance_uuid'],
3839                                                    reason=six.text_type(exc))
3840 
3841     def _detach_pci_devices(self, guest, pci_devs):
3842         try:
3843             for dev in pci_devs:
3844                 guest.detach_device(self._get_guest_pci_device(dev), live=True)
3845                 # after detachDeviceFlags returned, we should check the dom to
3846                 # ensure the detaching is finished
3847                 xml = guest.get_xml_desc()
3848                 xml_doc = etree.fromstring(xml)
3849                 guest_config = vconfig.LibvirtConfigGuest()
3850                 guest_config.parse_dom(xml_doc)
3851 
3852                 for hdev in [d for d in guest_config.devices
3853                     if isinstance(d, vconfig.LibvirtConfigGuestHostdevPCI)]:
3854                     hdbsf = [hdev.domain, hdev.bus, hdev.slot, hdev.function]
3855                     dbsf = pci_utils.parse_address(dev.address)
3856                     if [int(x, 16) for x in hdbsf] ==\
3857                             [int(x, 16) for x in dbsf]:
3858                         raise exception.PciDeviceDetachFailed(reason=
3859                                                               "timeout",
3860                                                               dev=dev)
3861 
3862         except libvirt.libvirtError as ex:
3863             error_code = ex.get_error_code()
3864             if error_code == libvirt.VIR_ERR_NO_DOMAIN:
3865                 LOG.warning("Instance disappeared while detaching "
3866                             "a PCI device from it.")
3867             else:
3868                 raise
3869 
3870     def _attach_pci_devices(self, guest, pci_devs):
3871         try:
3872             for dev in pci_devs:
3873                 guest.attach_device(self._get_guest_pci_device(dev))
3874 
3875         except libvirt.libvirtError:
3876             LOG.error('Attaching PCI devices %(dev)s to %(dom)s failed.',
3877                       {'dev': pci_devs, 'dom': guest.id})
3878             raise
3879 
3880     @staticmethod
3881     def _has_direct_passthrough_port(network_info):
3882         for vif in network_info:
3883             if (vif['vnic_type'] in
3884                 network_model.VNIC_TYPES_DIRECT_PASSTHROUGH):
3885                 return True
3886         return False
3887 
3888     def _attach_direct_passthrough_ports(
3889         self, context, instance, guest, network_info=None):
3890         if network_info is None:
3891             network_info = instance.info_cache.network_info
3892         if network_info is None:
3893             return
3894 
3895         if self._has_direct_passthrough_port(network_info):
3896             for vif in network_info:
3897                 if (vif['vnic_type'] in
3898                     network_model.VNIC_TYPES_DIRECT_PASSTHROUGH):
3899                     cfg = self.vif_driver.get_config(instance,
3900                                                      vif,
3901                                                      instance.image_meta,
3902                                                      instance.flavor,
3903                                                      CONF.libvirt.virt_type,
3904                                                      self._host)
3905                     LOG.debug('Attaching direct passthrough port %(port)s '
3906                               'to %(dom)s', {'port': vif, 'dom': guest.id},
3907                               instance=instance)
3908                     guest.attach_device(cfg)
3909 
3910     def _detach_direct_passthrough_ports(self, context, instance, guest):
3911         network_info = instance.info_cache.network_info
3912         if network_info is None:
3913             return
3914 
3915         if self._has_direct_passthrough_port(network_info):
3916             # In case of VNIC_TYPES_DIRECT_PASSTHROUGH ports we create
3917             # pci request per direct passthrough port. Therefore we can trust
3918             # that pci_slot value in the vif is correct.
3919             direct_passthrough_pci_addresses = [
3920                 vif['profile']['pci_slot']
3921                 for vif in network_info
3922                 if (vif['vnic_type'] in
3923                     network_model.VNIC_TYPES_DIRECT_PASSTHROUGH and
3924                     vif['profile'].get('pci_slot') is not None)
3925             ]
3926 
3927             # use detach_pci_devices to avoid failure in case of
3928             # multiple guest direct passthrough ports with the same MAC
3929             # (protection use-case, ports are on different physical
3930             # interfaces)
3931             pci_devs = pci_manager.get_instance_pci_devs(instance, 'all')
3932             direct_passthrough_pci_addresses = (
3933                 [pci_dev for pci_dev in pci_devs
3934                  if pci_dev.address in direct_passthrough_pci_addresses])
3935             self._detach_pci_devices(guest, direct_passthrough_pci_addresses)
3936 
3937     def _set_host_enabled(self, enabled,
3938                           disable_reason=DISABLE_REASON_UNDEFINED):
3939         """Enables / Disables the compute service on this host.
3940 
3941            This doesn't override non-automatic disablement with an automatic
3942            setting; thereby permitting operators to keep otherwise
3943            healthy hosts out of rotation.
3944         """
3945 
3946         status_name = {True: 'disabled',
3947                        False: 'enabled'}
3948 
3949         disable_service = not enabled
3950 
3951         ctx = nova_context.get_admin_context()
3952         try:
3953             service = objects.Service.get_by_compute_host(ctx, CONF.host)
3954 
3955             if service.disabled != disable_service:
3956                 # Note(jang): this is a quick fix to stop operator-
3957                 # disabled compute hosts from re-enabling themselves
3958                 # automatically. We prefix any automatic reason code
3959                 # with a fixed string. We only re-enable a host
3960                 # automatically if we find that string in place.
3961                 # This should probably be replaced with a separate flag.
3962                 if not service.disabled or (
3963                         service.disabled_reason and
3964                         service.disabled_reason.startswith(DISABLE_PREFIX)):
3965                     service.disabled = disable_service
3966                     service.disabled_reason = (
3967                        DISABLE_PREFIX + disable_reason
3968                        if disable_service and disable_reason else
3969                            DISABLE_REASON_UNDEFINED)
3970                     service.save()
3971                     LOG.debug('Updating compute service status to %s',
3972                               status_name[disable_service])
3973                 else:
3974                     LOG.debug('Not overriding manual compute service '
3975                               'status with: %s',
3976                               status_name[disable_service])
3977         except exception.ComputeHostNotFound:
3978             LOG.warning('Cannot update service status on host "%s" '
3979                         'since it is not registered.', CONF.host)
3980         except Exception:
3981             LOG.warning('Cannot update service status on host "%s" '
3982                         'due to an unexpected exception.', CONF.host,
3983                         exc_info=True)
3984 
3985         if enabled:
3986             mount.get_manager().host_up(self._host)
3987         else:
3988             mount.get_manager().host_down()
3989 
3990     def _get_guest_cpu_model_config(self):
3991         mode = CONF.libvirt.cpu_mode
3992         model = CONF.libvirt.cpu_model
3993         extra_flags = set([flag.lower() for flag in
3994             CONF.libvirt.cpu_model_extra_flags])
3995 
3996         if (CONF.libvirt.virt_type == "kvm" or
3997             CONF.libvirt.virt_type == "qemu"):
3998             if mode is None:
3999                 caps = self._host.get_capabilities()
4000                 # AArch64 lacks 'host-model' support because neither libvirt
4001                 # nor QEMU are able to tell what the host CPU model exactly is.
4002                 # And there is no CPU description code for ARM(64) at this
4003                 # point.
4004 
4005                 # Also worth noting: 'host-passthrough' mode will completely
4006                 # break live migration, *unless* all the Compute nodes (running
4007                 # libvirtd) have *identical* CPUs.
4008                 if caps.host.cpu.arch == fields.Architecture.AARCH64:
4009                     mode = "host-passthrough"
4010                     LOG.info('CPU mode "host-passthrough" was chosen. Live '
4011                              'migration can break unless all compute nodes '
4012                              'have identical cpus. AArch64 does not support '
4013                              'other modes.')
4014                 else:
4015                     mode = "host-model"
4016             if mode == "none":
4017                 return vconfig.LibvirtConfigGuestCPU()
4018         else:
4019             if mode is None or mode == "none":
4020                 return None
4021 
4022         if ((CONF.libvirt.virt_type != "kvm" and
4023              CONF.libvirt.virt_type != "qemu")):
4024             msg = _("Config requested an explicit CPU model, but "
4025                     "the current libvirt hypervisor '%s' does not "
4026                     "support selecting CPU models") % CONF.libvirt.virt_type
4027             raise exception.Invalid(msg)
4028 
4029         if mode == "custom" and model is None:
4030             msg = _("Config requested a custom CPU model, but no "
4031                     "model name was provided")
4032             raise exception.Invalid(msg)
4033         elif mode != "custom" and model is not None:
4034             msg = _("A CPU model name should not be set when a "
4035                     "host CPU model is requested")
4036             raise exception.Invalid(msg)
4037 
4038         LOG.debug("CPU mode '%(mode)s' model '%(model)s' was chosen, "
4039                   "with extra flags: '%(extra_flags)s'",
4040                   {'mode': mode,
4041                    'model': (model or ""),
4042                    'extra_flags': (extra_flags or "")})
4043 
4044         cpu = vconfig.LibvirtConfigGuestCPU()
4045         cpu.mode = mode
4046         cpu.model = model
4047 
4048         # NOTE (kchamart): Currently there's no existing way to ask if a
4049         # given CPU model + CPU flags combination is supported by KVM &
4050         # a specific QEMU binary.  However, libvirt runs the 'CPUID'
4051         # command upfront -- before even a Nova instance (a QEMU
4052         # process) is launched -- to construct CPU models and check
4053         # their validity; so we are good there.  In the long-term,
4054         # upstream libvirt intends to add an additional new API that can
4055         # do fine-grained validation of a certain CPU model + CPU flags
4056         # against a specific QEMU binary (the libvirt RFE bug for that:
4057         # https://bugzilla.redhat.com/show_bug.cgi?id=1559832).
4058         for flag in extra_flags:
4059             cpu.add_feature(vconfig.LibvirtConfigGuestCPUFeature(flag))
4060 
4061         return cpu
4062 
4063     def _get_guest_cpu_config(self, flavor, image_meta,
4064                               guest_cpu_numa_config, instance_numa_topology):
4065         cpu = self._get_guest_cpu_model_config()
4066 
4067         if cpu is None:
4068             return None
4069 
4070         topology = hardware.get_best_cpu_topology(
4071                 flavor, image_meta, numa_topology=instance_numa_topology)
4072 
4073         cpu.sockets = topology.sockets
4074         cpu.cores = topology.cores
4075         cpu.threads = topology.threads
4076         cpu.numa = guest_cpu_numa_config
4077 
4078         return cpu
4079 
4080     def _get_guest_disk_config(self, instance, name, disk_mapping, inst_type,
4081                                image_type=None):
4082         disk_unit = None
4083         disk = self.image_backend.by_name(instance, name, image_type)
4084         if (name == 'disk.config' and image_type == 'rbd' and
4085                 not disk.exists()):
4086             # This is likely an older config drive that has not been migrated
4087             # to rbd yet. Try to fall back on 'flat' image type.
4088             # TODO(melwitt): Add online migration of some sort so we can
4089             # remove this fall back once we know all config drives are in rbd.
4090             # NOTE(vladikr): make sure that the flat image exist, otherwise
4091             # the image will be created after the domain definition.
4092             flat_disk = self.image_backend.by_name(instance, name, 'flat')
4093             if flat_disk.exists():
4094                 disk = flat_disk
4095                 LOG.debug('Config drive not found in RBD, falling back to the '
4096                           'instance directory', instance=instance)
4097         disk_info = disk_mapping[name]
4098         if 'unit' in disk_mapping and disk_info['bus'] == 'scsi':
4099             disk_unit = disk_mapping['unit']
4100             disk_mapping['unit'] += 1  # Increments for the next disk added
4101         conf = disk.libvirt_info(disk_info, self.disk_cachemode,
4102                                  inst_type['extra_specs'],
4103                                  self._host.get_version(),
4104                                  disk_unit=disk_unit)
4105         return conf
4106 
4107     def _get_guest_fs_config(self, instance, name, image_type=None):
4108         disk = self.image_backend.by_name(instance, name, image_type)
4109         return disk.libvirt_fs_info("/", "ploop")
4110 
4111     def _get_guest_storage_config(self, context, instance, image_meta,
4112                                   disk_info,
4113                                   rescue, block_device_info,
4114                                   inst_type, os_type):
4115         devices = []
4116         disk_mapping = disk_info['mapping']
4117 
4118         block_device_mapping = driver.block_device_info_get_mapping(
4119             block_device_info)
4120         mount_rootfs = CONF.libvirt.virt_type == "lxc"
4121         scsi_controller = self._get_scsi_controller(image_meta)
4122 
4123         if scsi_controller and scsi_controller.model == 'virtio-scsi':
4124             # The virtio-scsi can handle up to 256 devices but the
4125             # optional element "address" must be defined to describe
4126             # where the device is placed on the controller (see:
4127             # LibvirtConfigGuestDeviceAddressDrive).
4128             #
4129             # Note about why it's added in disk_mapping: It's not
4130             # possible to pass an 'int' by reference in Python, so we
4131             # use disk_mapping as container to keep reference of the
4132             # unit added and be able to increment it for each disk
4133             # added.
4134             #
4135             # NOTE(jaypipes,melwitt): If this is a boot-from-volume instance,
4136             # we need to start the disk mapping unit at 1 since we set the
4137             # bootable volume's unit to 0 for the bootable volume.
4138             disk_mapping['unit'] = 0
4139             if self._is_booted_from_volume(block_device_info):
4140                 disk_mapping['unit'] = 1
4141 
4142         def _get_ephemeral_devices():
4143             eph_devices = []
4144             for idx, eph in enumerate(
4145                 driver.block_device_info_get_ephemerals(
4146                     block_device_info)):
4147                 diskeph = self._get_guest_disk_config(
4148                     instance,
4149                     blockinfo.get_eph_disk(idx),
4150                     disk_mapping, inst_type)
4151                 eph_devices.append(diskeph)
4152             return eph_devices
4153 
4154         if mount_rootfs:
4155             fs = vconfig.LibvirtConfigGuestFilesys()
4156             fs.source_type = "mount"
4157             fs.source_dir = os.path.join(
4158                 libvirt_utils.get_instance_path(instance), 'rootfs')
4159             devices.append(fs)
4160         elif (os_type == fields.VMMode.EXE and
4161               CONF.libvirt.virt_type == "parallels"):
4162             if rescue:
4163                 fsrescue = self._get_guest_fs_config(instance, "disk.rescue")
4164                 devices.append(fsrescue)
4165 
4166                 fsos = self._get_guest_fs_config(instance, "disk")
4167                 fsos.target_dir = "/mnt/rescue"
4168                 devices.append(fsos)
4169             else:
4170                 if 'disk' in disk_mapping:
4171                     fs = self._get_guest_fs_config(instance, "disk")
4172                     devices.append(fs)
4173                 devices = devices + _get_ephemeral_devices()
4174         else:
4175 
4176             if rescue:
4177                 diskrescue = self._get_guest_disk_config(instance,
4178                                                          'disk.rescue',
4179                                                          disk_mapping,
4180                                                          inst_type)
4181                 devices.append(diskrescue)
4182 
4183                 diskos = self._get_guest_disk_config(instance,
4184                                                      'disk',
4185                                                      disk_mapping,
4186                                                      inst_type)
4187                 devices.append(diskos)
4188             else:
4189                 if 'disk' in disk_mapping:
4190                     diskos = self._get_guest_disk_config(instance,
4191                                                          'disk',
4192                                                          disk_mapping,
4193                                                          inst_type)
4194                     devices.append(diskos)
4195 
4196                 if 'disk.local' in disk_mapping:
4197                     disklocal = self._get_guest_disk_config(instance,
4198                                                             'disk.local',
4199                                                             disk_mapping,
4200                                                             inst_type)
4201                     devices.append(disklocal)
4202                     instance.default_ephemeral_device = (
4203                         block_device.prepend_dev(disklocal.target_dev))
4204 
4205                 devices = devices + _get_ephemeral_devices()
4206 
4207                 if 'disk.swap' in disk_mapping:
4208                     diskswap = self._get_guest_disk_config(instance,
4209                                                            'disk.swap',
4210                                                            disk_mapping,
4211                                                            inst_type)
4212                     devices.append(diskswap)
4213                     instance.default_swap_device = (
4214                         block_device.prepend_dev(diskswap.target_dev))
4215 
4216             config_name = 'disk.config.rescue' if rescue else 'disk.config'
4217             if config_name in disk_mapping:
4218                 diskconfig = self._get_guest_disk_config(
4219                     instance, config_name, disk_mapping, inst_type,
4220                     self._get_disk_config_image_type())
4221                 devices.append(diskconfig)
4222 
4223         for vol in block_device.get_bdms_to_connect(block_device_mapping,
4224                                                    mount_rootfs):
4225             connection_info = vol['connection_info']
4226             vol_dev = block_device.prepend_dev(vol['mount_device'])
4227             info = disk_mapping[vol_dev]
4228             self._connect_volume(context, connection_info, instance)
4229             if scsi_controller and scsi_controller.model == 'virtio-scsi':
4230                 # Check if this is the bootable volume when in a
4231                 # boot-from-volume instance, and if so, ensure the unit
4232                 # attribute is 0.
4233                 if vol.get('boot_index') == 0:
4234                     info['unit'] = 0
4235                 else:
4236                     info['unit'] = disk_mapping['unit']
4237                     disk_mapping['unit'] += 1
4238             cfg = self._get_volume_config(connection_info, info)
4239             devices.append(cfg)
4240             vol['connection_info'] = connection_info
4241             vol.save()
4242 
4243         for d in devices:
4244             self._set_cache_mode(d)
4245 
4246         if scsi_controller:
4247             devices.append(scsi_controller)
4248 
4249         return devices
4250 
4251     @staticmethod
4252     def _get_scsi_controller(image_meta):
4253         """Return scsi controller or None based on image meta"""
4254         if image_meta.properties.get('hw_scsi_model'):
4255             hw_scsi_model = image_meta.properties.hw_scsi_model
4256             scsi_controller = vconfig.LibvirtConfigGuestController()
4257             scsi_controller.type = 'scsi'
4258             scsi_controller.model = hw_scsi_model
4259             scsi_controller.index = 0
4260             return scsi_controller
4261 
4262     def _get_host_sysinfo_serial_hardware(self):
4263         """Get a UUID from the host hardware
4264 
4265         Get a UUID for the host hardware reported by libvirt.
4266         This is typically from the SMBIOS data, unless it has
4267         been overridden in /etc/libvirt/libvirtd.conf
4268         """
4269         caps = self._host.get_capabilities()
4270         return caps.host.uuid
4271 
4272     def _get_host_sysinfo_serial_os(self):
4273         """Get a UUID from the host operating system
4274 
4275         Get a UUID for the host operating system. Modern Linux
4276         distros based on systemd provide a /etc/machine-id
4277         file containing a UUID. This is also provided inside
4278         systemd based containers and can be provided by other
4279         init systems too, since it is just a plain text file.
4280         """
4281         if not os.path.exists("/etc/machine-id"):
4282             msg = _("Unable to get host UUID: /etc/machine-id does not exist")
4283             raise exception.InternalError(msg)
4284 
4285         with open("/etc/machine-id") as f:
4286             # We want to have '-' in the right place
4287             # so we parse & reformat the value
4288             lines = f.read().split()
4289             if not lines:
4290                 msg = _("Unable to get host UUID: /etc/machine-id is empty")
4291                 raise exception.InternalError(msg)
4292 
4293             return str(uuid.UUID(lines[0]))
4294 
4295     def _get_host_sysinfo_serial_auto(self):
4296         if os.path.exists("/etc/machine-id"):
4297             return self._get_host_sysinfo_serial_os()
4298         else:
4299             return self._get_host_sysinfo_serial_hardware()
4300 
4301     def _get_guest_config_sysinfo(self, instance):
4302         sysinfo = vconfig.LibvirtConfigGuestSysinfo()
4303 
4304         sysinfo.system_manufacturer = version.vendor_string()
4305         sysinfo.system_product = version.product_string()
4306         sysinfo.system_version = version.version_string_with_package()
4307 
4308         if CONF.libvirt.sysinfo_serial == 'unique':
4309             sysinfo.system_serial = instance.uuid
4310         else:
4311             sysinfo.system_serial = self._sysinfo_serial_func()
4312         sysinfo.system_uuid = instance.uuid
4313 
4314         sysinfo.system_family = "Virtual Machine"
4315 
4316         return sysinfo
4317 
4318     def _get_guest_pci_device(self, pci_device):
4319 
4320         dbsf = pci_utils.parse_address(pci_device.address)
4321         dev = vconfig.LibvirtConfigGuestHostdevPCI()
4322         dev.domain, dev.bus, dev.slot, dev.function = dbsf
4323 
4324         # only kvm support managed mode
4325         if CONF.libvirt.virt_type in ('xen', 'parallels',):
4326             dev.managed = 'no'
4327         if CONF.libvirt.virt_type in ('kvm', 'qemu'):
4328             dev.managed = 'yes'
4329 
4330         return dev
4331 
4332     def _get_guest_config_meta(self, instance):
4333         """Get metadata config for guest."""
4334 
4335         meta = vconfig.LibvirtConfigGuestMetaNovaInstance()
4336         meta.package = version.version_string_with_package()
4337         meta.name = instance.display_name
4338         meta.creationTime = time.time()
4339 
4340         if instance.image_ref not in ("", None):
4341             meta.roottype = "image"
4342             meta.rootid = instance.image_ref
4343 
4344         system_meta = instance.system_metadata
4345         ometa = vconfig.LibvirtConfigGuestMetaNovaOwner()
4346         ometa.userid = instance.user_id
4347         ometa.username = system_meta.get('owner_user_name', 'N/A')
4348         ometa.projectid = instance.project_id
4349         ometa.projectname = system_meta.get('owner_project_name', 'N/A')
4350         meta.owner = ometa
4351 
4352         fmeta = vconfig.LibvirtConfigGuestMetaNovaFlavor()
4353         flavor = instance.flavor
4354         fmeta.name = flavor.name
4355         fmeta.memory = flavor.memory_mb
4356         fmeta.vcpus = flavor.vcpus
4357         fmeta.ephemeral = flavor.ephemeral_gb
4358         fmeta.disk = flavor.root_gb
4359         fmeta.swap = flavor.swap
4360 
4361         meta.flavor = fmeta
4362 
4363         return meta
4364 
4365     def _get_machine_type(self, image_meta, caps):
4366         # The guest machine type can be set as an image metadata
4367         # property, or otherwise based on architecture-specific
4368         # defaults.
4369         mach_type = None
4370 
4371         if image_meta.properties.get('hw_machine_type') is not None:
4372             mach_type = image_meta.properties.hw_machine_type
4373         else:
4374             # NOTE(kchamart): For ARMv7 and AArch64, use the 'virt'
4375             # board as the default machine type.  It is the recommended
4376             # board, which is designed to be used with virtual machines.
4377             # The 'virt' board is more flexible, supports PCI, 'virtio',
4378             # has decent RAM limits, etc.
4379             if caps.host.cpu.arch in (fields.Architecture.ARMV7,
4380                                       fields.Architecture.AARCH64):
4381                 mach_type = "virt"
4382 
4383             if caps.host.cpu.arch in (fields.Architecture.S390,
4384                                       fields.Architecture.S390X):
4385                 mach_type = 's390-ccw-virtio'
4386 
4387             # If set in the config, use that as the default.
4388             mach_type = (
4389                 libvirt_utils.get_default_machine_type(caps.host.cpu.arch)
4390                 or mach_type
4391             )
4392 
4393         return mach_type
4394 
4395     @staticmethod
4396     def _create_idmaps(klass, map_strings):
4397         idmaps = []
4398         if len(map_strings) > 5:
4399             map_strings = map_strings[0:5]
4400             LOG.warning("Too many id maps, only included first five.")
4401         for map_string in map_strings:
4402             try:
4403                 idmap = klass()
4404                 values = [int(i) for i in map_string.split(":")]
4405                 idmap.start = values[0]
4406                 idmap.target = values[1]
4407                 idmap.count = values[2]
4408                 idmaps.append(idmap)
4409             except (ValueError, IndexError):
4410                 LOG.warning("Invalid value for id mapping %s", map_string)
4411         return idmaps
4412 
4413     def _get_guest_idmaps(self):
4414         id_maps = []
4415         if CONF.libvirt.virt_type == 'lxc' and CONF.libvirt.uid_maps:
4416             uid_maps = self._create_idmaps(vconfig.LibvirtConfigGuestUIDMap,
4417                                            CONF.libvirt.uid_maps)
4418             id_maps.extend(uid_maps)
4419         if CONF.libvirt.virt_type == 'lxc' and CONF.libvirt.gid_maps:
4420             gid_maps = self._create_idmaps(vconfig.LibvirtConfigGuestGIDMap,
4421                                            CONF.libvirt.gid_maps)
4422             id_maps.extend(gid_maps)
4423         return id_maps
4424 
4425     def _update_guest_cputune(self, guest, flavor, virt_type):
4426         is_able = self._host.is_cpu_control_policy_capable()
4427 
4428         cputuning = ['shares', 'period', 'quota']
4429         wants_cputune = any([k for k in cputuning
4430             if "quota:cpu_" + k in flavor.extra_specs.keys()])
4431 
4432         if wants_cputune and not is_able:
4433             raise exception.UnsupportedHostCPUControlPolicy()
4434 
4435         if not is_able or virt_type not in ('lxc', 'kvm', 'qemu'):
4436             return
4437 
4438         if guest.cputune is None:
4439             guest.cputune = vconfig.LibvirtConfigGuestCPUTune()
4440             # Setting the default cpu.shares value to be a value
4441             # dependent on the number of vcpus
4442         guest.cputune.shares = 1024 * guest.vcpus
4443 
4444         for name in cputuning:
4445             key = "quota:cpu_" + name
4446             if key in flavor.extra_specs:
4447                 setattr(guest.cputune, name,
4448                         int(flavor.extra_specs[key]))
4449 
4450     def _get_cpu_numa_config_from_instance(self, instance_numa_topology,
4451                                            wants_hugepages):
4452         if instance_numa_topology:
4453             guest_cpu_numa = vconfig.LibvirtConfigGuestCPUNUMA()
4454             for instance_cell in instance_numa_topology.cells:
4455                 guest_cell = vconfig.LibvirtConfigGuestCPUNUMACell()
4456                 guest_cell.id = instance_cell.id
4457                 guest_cell.cpus = instance_cell.cpuset
4458                 guest_cell.memory = instance_cell.memory * units.Ki
4459 
4460                 # The vhost-user network backend requires file backed
4461                 # guest memory (ie huge pages) to be marked as shared
4462                 # access, not private, so an external process can read
4463                 # and write the pages.
4464                 #
4465                 # You can't change the shared vs private flag for an
4466                 # already running guest, and since we can't predict what
4467                 # types of NIC may be hotplugged, we have no choice but
4468                 # to unconditionally turn on the shared flag. This has
4469                 # no real negative functional effect on the guest, so
4470                 # is a reasonable approach to take
4471                 if wants_hugepages:
4472                     guest_cell.memAccess = "shared"
4473                 guest_cpu_numa.cells.append(guest_cell)
4474             return guest_cpu_numa
4475 
4476     def _wants_hugepages(self, host_topology, instance_topology):
4477         """Determine if the guest / host topology implies the
4478            use of huge pages for guest RAM backing
4479         """
4480 
4481         if host_topology is None or instance_topology is None:
4482             return False
4483 
4484         avail_pagesize = [page.size_kb
4485                           for page in host_topology.cells[0].mempages]
4486         avail_pagesize.sort()
4487         # Remove smallest page size as that's not classed as a largepage
4488         avail_pagesize = avail_pagesize[1:]
4489 
4490         # See if we have page size set
4491         for cell in instance_topology.cells:
4492             if (cell.pagesize is not None and
4493                 cell.pagesize in avail_pagesize):
4494                 return True
4495 
4496         return False
4497 
4498     def _get_cell_pairs(self, guest_cpu_numa_config, host_topology):
4499         """Returns the lists of pairs(tuple) of an instance cell and
4500         corresponding host cell:
4501             [(LibvirtConfigGuestCPUNUMACell, NUMACell), ...]
4502         """
4503         cell_pairs = []
4504         for guest_config_cell in guest_cpu_numa_config.cells:
4505             for host_cell in host_topology.cells:
4506                 if guest_config_cell.id == host_cell.id:
4507                     cell_pairs.append((guest_config_cell, host_cell))
4508         return cell_pairs
4509 
4510     def _get_pin_cpuset(self, vcpu, object_numa_cell, host_cell):
4511         """Returns the config object of LibvirtConfigGuestCPUTuneVCPUPin.
4512         Prepares vcpupin config for the guest with the following caveats:
4513 
4514             a) If there is pinning information in the cell, we pin vcpus to
4515                individual CPUs
4516             b) Otherwise we float over the whole host NUMA node
4517         """
4518         pin_cpuset = vconfig.LibvirtConfigGuestCPUTuneVCPUPin()
4519         pin_cpuset.id = vcpu
4520 
4521         if object_numa_cell.cpu_pinning:
4522             pin_cpuset.cpuset = set([object_numa_cell.cpu_pinning[vcpu]])
4523         else:
4524             pin_cpuset.cpuset = host_cell.cpuset
4525 
4526         return pin_cpuset
4527 
4528     def _get_emulatorpin_cpuset(self, vcpu, object_numa_cell, vcpus_rt,
4529                                 emulator_threads_policy, wants_realtime,
4530                                 pin_cpuset):
4531         """Returns a set of cpu_ids to add to the cpuset for emulator threads
4532            with the following caveats:
4533 
4534             a) If emulator threads policy is isolated, we pin emulator threads
4535                to one cpu we have reserved for it.
4536             b) If emulator threads policy is shared and CONF.cpu_shared_set is
4537                defined, we pin emulator threads on the set of pCPUs defined by
4538                CONF.cpu_shared_set
4539             c) Otherwise;
4540                 c1) If realtime IS NOT enabled, the emulator threads are
4541                     allowed to float cross all the pCPUs associated with
4542                     the guest vCPUs.
4543                 c2) If realtime IS enabled, at least 1 vCPU is required
4544                     to be set aside for non-realtime usage. The emulator
4545                     threads are allowed to float across the pCPUs that
4546                     are associated with the non-realtime VCPUs.
4547         """
4548         emulatorpin_cpuset = set([])
4549         shared_ids = hardware.get_cpu_shared_set()
4550 
4551         if emulator_threads_policy == fields.CPUEmulatorThreadsPolicy.ISOLATE:
4552             if object_numa_cell.cpuset_reserved:
4553                 emulatorpin_cpuset = object_numa_cell.cpuset_reserved
4554         elif ((emulator_threads_policy ==
4555               fields.CPUEmulatorThreadsPolicy.SHARE) and
4556               shared_ids):
4557             online_pcpus = self._host.get_online_cpus()
4558             cpuset = shared_ids & online_pcpus
4559             if not cpuset:
4560                 msg = (_("Invalid cpu_shared_set config, one or more of the "
4561                          "specified cpuset is not online. Online cpuset(s): "
4562                          "%(online)s, requested cpuset(s): %(req)s"),
4563                        {'online': sorted(online_pcpus),
4564                         'req': sorted(shared_ids)})
4565                 raise exception.Invalid(msg)
4566             emulatorpin_cpuset = cpuset
4567         elif not wants_realtime or vcpu not in vcpus_rt:
4568             emulatorpin_cpuset = pin_cpuset.cpuset
4569 
4570         return emulatorpin_cpuset
4571 
4572     def _get_guest_numa_config(self, instance_numa_topology, flavor,
4573                                allowed_cpus=None, image_meta=None):
4574         """Returns the config objects for the guest NUMA specs.
4575 
4576         Determines the CPUs that the guest can be pinned to if the guest
4577         specifies a cell topology and the host supports it. Constructs the
4578         libvirt XML config object representing the NUMA topology selected
4579         for the guest. Returns a tuple of:
4580 
4581             (cpu_set, guest_cpu_tune, guest_cpu_numa, guest_numa_tune)
4582 
4583         With the following caveats:
4584 
4585             a) If there is no specified guest NUMA topology, then
4586                all tuple elements except cpu_set shall be None. cpu_set
4587                will be populated with the chosen CPUs that the guest
4588                allowed CPUs fit within, which could be the supplied
4589                allowed_cpus value if the host doesn't support NUMA
4590                topologies.
4591 
4592             b) If there is a specified guest NUMA topology, then
4593                cpu_set will be None and guest_cpu_numa will be the
4594                LibvirtConfigGuestCPUNUMA object representing the guest's
4595                NUMA topology. If the host supports NUMA, then guest_cpu_tune
4596                will contain a LibvirtConfigGuestCPUTune object representing
4597                the optimized chosen cells that match the host capabilities
4598                with the instance's requested topology. If the host does
4599                not support NUMA, then guest_cpu_tune and guest_numa_tune
4600                will be None.
4601         """
4602 
4603         if (not self._has_numa_support() and
4604                 instance_numa_topology is not None):
4605             # We should not get here, since we should have avoided
4606             # reporting NUMA topology from _get_host_numa_topology
4607             # in the first place. Just in case of a scheduler
4608             # mess up though, raise an exception
4609             raise exception.NUMATopologyUnsupported()
4610 
4611         topology = self._get_host_numa_topology()
4612 
4613         # We have instance NUMA so translate it to the config class
4614         guest_cpu_numa_config = self._get_cpu_numa_config_from_instance(
4615                 instance_numa_topology,
4616                 self._wants_hugepages(topology, instance_numa_topology))
4617 
4618         if not guest_cpu_numa_config:
4619             # No NUMA topology defined for instance - let the host kernel deal
4620             # with the NUMA effects.
4621             # TODO(ndipanov): Attempt to spread the instance
4622             # across NUMA nodes and expose the topology to the
4623             # instance as an optimisation
4624             return GuestNumaConfig(allowed_cpus, None, None, None)
4625 
4626         if not topology:
4627             # No NUMA topology defined for host - This will only happen with
4628             # some libvirt versions and certain platforms.
4629             return GuestNumaConfig(allowed_cpus, None,
4630                                    guest_cpu_numa_config, None)
4631 
4632         # Now get configuration from the numa_topology
4633         # Init CPUTune configuration
4634         guest_cpu_tune = vconfig.LibvirtConfigGuestCPUTune()
4635         guest_cpu_tune.emulatorpin = (
4636             vconfig.LibvirtConfigGuestCPUTuneEmulatorPin())
4637         guest_cpu_tune.emulatorpin.cpuset = set([])
4638 
4639         # Init NUMATune configuration
4640         guest_numa_tune = vconfig.LibvirtConfigGuestNUMATune()
4641         guest_numa_tune.memory = vconfig.LibvirtConfigGuestNUMATuneMemory()
4642         guest_numa_tune.memnodes = []
4643 
4644         emulator_threads_policy = None
4645         if 'emulator_threads_policy' in instance_numa_topology:
4646             emulator_threads_policy = (
4647                 instance_numa_topology.emulator_threads_policy)
4648 
4649         # Set realtime scheduler for CPUTune
4650         vcpus_rt = set([])
4651         wants_realtime = hardware.is_realtime_enabled(flavor)
4652         if wants_realtime:
4653             vcpus_rt = hardware.vcpus_realtime_topology(flavor, image_meta)
4654             vcpusched = vconfig.LibvirtConfigGuestCPUTuneVCPUSched()
4655             designer.set_vcpu_realtime_scheduler(
4656                 vcpusched, vcpus_rt, CONF.libvirt.realtime_scheduler_priority)
4657             guest_cpu_tune.vcpusched.append(vcpusched)
4658 
4659         cell_pairs = self._get_cell_pairs(guest_cpu_numa_config, topology)
4660         for guest_node_id, (guest_config_cell, host_cell) in enumerate(
4661                 cell_pairs):
4662             # set NUMATune for the cell
4663             tnode = vconfig.LibvirtConfigGuestNUMATuneMemNode()
4664             designer.set_numa_memnode(tnode, guest_node_id, host_cell.id)
4665             guest_numa_tune.memnodes.append(tnode)
4666             guest_numa_tune.memory.nodeset.append(host_cell.id)
4667 
4668             # set CPUTune for the cell
4669             object_numa_cell = instance_numa_topology.cells[guest_node_id]
4670             for cpu in guest_config_cell.cpus:
4671                 pin_cpuset = self._get_pin_cpuset(cpu, object_numa_cell,
4672                                                   host_cell)
4673                 guest_cpu_tune.vcpupin.append(pin_cpuset)
4674 
4675                 emu_pin_cpuset = self._get_emulatorpin_cpuset(
4676                     cpu, object_numa_cell, vcpus_rt,
4677                     emulator_threads_policy, wants_realtime, pin_cpuset)
4678                 guest_cpu_tune.emulatorpin.cpuset.update(emu_pin_cpuset)
4679 
4680         # TODO(berrange) When the guest has >1 NUMA node, it will
4681         # span multiple host NUMA nodes. By pinning emulator threads
4682         # to the union of all nodes, we guarantee there will be
4683         # cross-node memory access by the emulator threads when
4684         # responding to guest I/O operations. The only way to avoid
4685         # this would be to pin emulator threads to a single node and
4686         # tell the guest OS to only do I/O from one of its virtual
4687         # NUMA nodes. This is not even remotely practical.
4688         #
4689         # The long term solution is to make use of a new QEMU feature
4690         # called "I/O Threads" which will let us configure an explicit
4691         # I/O thread for each guest vCPU or guest NUMA node. It is
4692         # still TBD how to make use of this feature though, especially
4693         # how to associate IO threads with guest devices to eliminate
4694         # cross NUMA node traffic. This is an area of investigation
4695         # for QEMU community devs.
4696 
4697         # Sort the vcpupin list per vCPU id for human-friendlier XML
4698         guest_cpu_tune.vcpupin.sort(key=operator.attrgetter("id"))
4699 
4700         # normalize cell.id
4701         for i, (cell, memnode) in enumerate(zip(guest_cpu_numa_config.cells,
4702                                                 guest_numa_tune.memnodes)):
4703             cell.id = i
4704             memnode.cellid = i
4705 
4706         return GuestNumaConfig(None, guest_cpu_tune, guest_cpu_numa_config,
4707                                guest_numa_tune)
4708 
4709     def _get_guest_os_type(self, virt_type):
4710         """Returns the guest OS type based on virt type."""
4711         if virt_type == "lxc":
4712             ret = fields.VMMode.EXE
4713         elif virt_type == "uml":
4714             ret = fields.VMMode.UML
4715         elif virt_type == "xen":
4716             ret = fields.VMMode.XEN
4717         else:
4718             ret = fields.VMMode.HVM
4719         return ret
4720 
4721     def _set_guest_for_rescue(self, rescue, guest, inst_path, virt_type,
4722                               root_device_name):
4723         if rescue.get('kernel_id'):
4724             guest.os_kernel = os.path.join(inst_path, "kernel.rescue")
4725             guest.os_cmdline = ("root=%s %s" % (root_device_name, CONSOLE))
4726             if virt_type == "qemu":
4727                 guest.os_cmdline += " no_timer_check"
4728         if rescue.get('ramdisk_id'):
4729             guest.os_initrd = os.path.join(inst_path, "ramdisk.rescue")
4730 
4731     def _set_guest_for_inst_kernel(self, instance, guest, inst_path, virt_type,
4732                                 root_device_name, image_meta):
4733         guest.os_kernel = os.path.join(inst_path, "kernel")
4734         guest.os_cmdline = ("root=%s %s" % (root_device_name, CONSOLE))
4735         if virt_type == "qemu":
4736             guest.os_cmdline += " no_timer_check"
4737         if instance.ramdisk_id:
4738             guest.os_initrd = os.path.join(inst_path, "ramdisk")
4739         # we only support os_command_line with images with an explicit
4740         # kernel set and don't want to break nova if there's an
4741         # os_command_line property without a specified kernel_id param
4742         if image_meta.properties.get("os_command_line"):
4743             guest.os_cmdline = image_meta.properties.os_command_line
4744 
4745     def _set_clock(self, guest, os_type, image_meta, virt_type):
4746         # NOTE(mikal): Microsoft Windows expects the clock to be in
4747         # "localtime". If the clock is set to UTC, then you can use a
4748         # registry key to let windows know, but Microsoft says this is
4749         # buggy in http://support.microsoft.com/kb/2687252
4750         clk = vconfig.LibvirtConfigGuestClock()
4751         if os_type == 'windows':
4752             LOG.info('Configuring timezone for windows instance to localtime')
4753             clk.offset = 'localtime'
4754         else:
4755             clk.offset = 'utc'
4756         guest.set_clock(clk)
4757 
4758         if virt_type == "kvm":
4759             self._set_kvm_timers(clk, os_type, image_meta)
4760 
4761     def _set_kvm_timers(self, clk, os_type, image_meta):
4762         # TODO(berrange) One day this should be per-guest
4763         # OS type configurable
4764         tmpit = vconfig.LibvirtConfigGuestTimer()
4765         tmpit.name = "pit"
4766         tmpit.tickpolicy = "delay"
4767 
4768         tmrtc = vconfig.LibvirtConfigGuestTimer()
4769         tmrtc.name = "rtc"
4770         tmrtc.tickpolicy = "catchup"
4771 
4772         clk.add_timer(tmpit)
4773         clk.add_timer(tmrtc)
4774 
4775         hpet = image_meta.properties.get('hw_time_hpet', False)
4776         guestarch = libvirt_utils.get_arch(image_meta)
4777         if guestarch in (fields.Architecture.I686,
4778                          fields.Architecture.X86_64):
4779             # NOTE(rfolco): HPET is a hardware timer for x86 arch.
4780             # qemu -no-hpet is not supported on non-x86 targets.
4781             tmhpet = vconfig.LibvirtConfigGuestTimer()
4782             tmhpet.name = "hpet"
4783             tmhpet.present = hpet
4784             clk.add_timer(tmhpet)
4785         else:
4786             if hpet:
4787                 LOG.warning('HPET is not turned on for non-x86 guests in image'
4788                             ' %s.', image_meta.id)
4789 
4790         # Provide Windows guests with the paravirtualized hyperv timer source.
4791         # This is the windows equiv of kvm-clock, allowing Windows
4792         # guests to accurately keep time.
4793         if os_type == 'windows':
4794             tmhyperv = vconfig.LibvirtConfigGuestTimer()
4795             tmhyperv.name = "hypervclock"
4796             tmhyperv.present = True
4797             clk.add_timer(tmhyperv)
4798 
4799     def _set_features(self, guest, os_type, caps, virt_type, image_meta,
4800             flavor):
4801         if virt_type == "xen":
4802             # PAE only makes sense in X86
4803             if caps.host.cpu.arch in (fields.Architecture.I686,
4804                                       fields.Architecture.X86_64):
4805                 guest.features.append(vconfig.LibvirtConfigGuestFeaturePAE())
4806 
4807         if (virt_type not in ("lxc", "uml", "parallels", "xen") or
4808                 (virt_type == "xen" and guest.os_type == fields.VMMode.HVM)):
4809             guest.features.append(vconfig.LibvirtConfigGuestFeatureACPI())
4810             guest.features.append(vconfig.LibvirtConfigGuestFeatureAPIC())
4811 
4812         if (virt_type in ("qemu", "kvm") and
4813                 os_type == 'windows'):
4814             hv = vconfig.LibvirtConfigGuestFeatureHyperV()
4815             hv.relaxed = True
4816 
4817             hv.spinlocks = True
4818             # Increase spinlock retries - value recommended by
4819             # KVM maintainers who certify Windows guests
4820             # with Microsoft
4821             hv.spinlock_retries = 8191
4822             hv.vapic = True
4823             guest.features.append(hv)
4824 
4825         flavor_hide_kvm = strutils.bool_from_string(
4826                 flavor.get('extra_specs', {}).get('hide_hypervisor_id'))
4827         if (virt_type in ("qemu", "kvm") and
4828                 (image_meta.properties.get('img_hide_hypervisor_id') or
4829                  flavor_hide_kvm)):
4830             guest.features.append(vconfig.LibvirtConfigGuestFeatureKvmHidden())
4831 
4832     def _check_number_of_serial_console(self, num_ports):
4833         virt_type = CONF.libvirt.virt_type
4834         if (virt_type in ("kvm", "qemu") and
4835             num_ports > ALLOWED_QEMU_SERIAL_PORTS):
4836             raise exception.SerialPortNumberLimitExceeded(
4837                 allowed=ALLOWED_QEMU_SERIAL_PORTS, virt_type=virt_type)
4838 
4839     def _add_video_driver(self, guest, image_meta, flavor):
4840         VALID_VIDEO_DEVICES = ("vga", "cirrus", "vmvga",
4841                                "xen", "qxl", "virtio")
4842         video = vconfig.LibvirtConfigGuestVideo()
4843         # NOTE(ldbragst): The following logic sets the video.type
4844         # depending on supported defaults given the architecture,
4845         # virtualization type, and features. The video.type attribute can
4846         # be overridden by the user with image_meta.properties, which
4847         # is carried out in the next if statement below this one.
4848         guestarch = libvirt_utils.get_arch(image_meta)
4849         if guest.os_type == fields.VMMode.XEN:
4850             video.type = 'xen'
4851         elif CONF.libvirt.virt_type == 'parallels':
4852             video.type = 'vga'
4853         elif guestarch in (fields.Architecture.PPC,
4854                            fields.Architecture.PPC64,
4855                            fields.Architecture.PPC64LE):
4856             # NOTE(ldbragst): PowerKVM doesn't support 'cirrus' be default
4857             # so use 'vga' instead when running on Power hardware.
4858             video.type = 'vga'
4859         elif guestarch in (fields.Architecture.AARCH64):
4860             # NOTE(kevinz): Only virtio device type is supported by AARCH64
4861             # so use 'virtio' instead when running on AArch64 hardware.
4862             video.type = 'virtio'
4863         elif CONF.spice.enabled:
4864             video.type = 'qxl'
4865         if image_meta.properties.get('hw_video_model'):
4866             video.type = image_meta.properties.hw_video_model
4867             if (video.type not in VALID_VIDEO_DEVICES):
4868                 raise exception.InvalidVideoMode(model=video.type)
4869 
4870         # Set video memory, only if the flavor's limit is set
4871         video_ram = image_meta.properties.get('hw_video_ram', 0)
4872         max_vram = int(flavor.extra_specs.get('hw_video:ram_max_mb', 0))
4873         if video_ram > max_vram:
4874             raise exception.RequestedVRamTooHigh(req_vram=video_ram,
4875                                                  max_vram=max_vram)
4876         if max_vram and video_ram:
4877             video.vram = video_ram * units.Mi / units.Ki
4878         guest.add_device(video)
4879 
4880     def _add_qga_device(self, guest, instance):
4881         qga = vconfig.LibvirtConfigGuestChannel()
4882         qga.type = "unix"
4883         qga.target_name = "org.qemu.guest_agent.0"
4884         qga.source_path = ("/var/lib/libvirt/qemu/%s.%s.sock" %
4885                           ("org.qemu.guest_agent.0", instance.name))
4886         guest.add_device(qga)
4887 
4888     def _add_rng_device(self, guest, flavor):
4889         rng_device = vconfig.LibvirtConfigGuestRng()
4890         rate_bytes = flavor.extra_specs.get('hw_rng:rate_bytes', 0)
4891         period = flavor.extra_specs.get('hw_rng:rate_period', 0)
4892         if rate_bytes:
4893             rng_device.rate_bytes = int(rate_bytes)
4894             rng_device.rate_period = int(period)
4895         rng_path = CONF.libvirt.rng_dev_path
4896         if (rng_path and not os.path.exists(rng_path)):
4897             raise exception.RngDeviceNotExist(path=rng_path)
4898         rng_device.backend = rng_path
4899         guest.add_device(rng_device)
4900 
4901     def _set_qemu_guest_agent(self, guest, flavor, instance, image_meta):
4902         # Enable qga only if the 'hw_qemu_guest_agent' is equal to yes
4903         if image_meta.properties.get('hw_qemu_guest_agent', False):
4904             LOG.debug("Qemu guest agent is enabled through image "
4905                       "metadata", instance=instance)
4906             self._add_qga_device(guest, instance)
4907         rng_is_virtio = image_meta.properties.get('hw_rng_model') == 'virtio'
4908         rng_allowed_str = flavor.extra_specs.get('hw_rng:allowed', '')
4909         rng_allowed = strutils.bool_from_string(rng_allowed_str)
4910         if rng_is_virtio and rng_allowed:
4911             self._add_rng_device(guest, flavor)
4912 
4913     def _get_guest_memory_backing_config(
4914             self, inst_topology, numatune, flavor):
4915         wantsmempages = False
4916         if inst_topology:
4917             for cell in inst_topology.cells:
4918                 if cell.pagesize:
4919                     wantsmempages = True
4920                     break
4921 
4922         wantsrealtime = hardware.is_realtime_enabled(flavor)
4923 
4924         wantsfilebacked = CONF.libvirt.file_backed_memory > 0
4925 
4926         if wantsmempages and wantsfilebacked:
4927             # Can't use file-backed memory with hugepages
4928             LOG.warning("Instance requested huge pages, but file-backed "
4929                     "memory is enabled, and incompatible with huge pages")
4930             raise exception.MemoryPagesUnsupported()
4931 
4932         membacking = None
4933         if wantsmempages:
4934             pages = self._get_memory_backing_hugepages_support(
4935                 inst_topology, numatune)
4936             if pages:
4937                 membacking = vconfig.LibvirtConfigGuestMemoryBacking()
4938                 membacking.hugepages = pages
4939         if wantsrealtime:
4940             if not membacking:
4941                 membacking = vconfig.LibvirtConfigGuestMemoryBacking()
4942             membacking.locked = True
4943             membacking.sharedpages = False
4944         if wantsfilebacked:
4945             if not membacking:
4946                 membacking = vconfig.LibvirtConfigGuestMemoryBacking()
4947             membacking.filesource = True
4948             membacking.sharedaccess = True
4949             membacking.allocateimmediate = True
4950             if self._host.has_min_version(
4951                     MIN_LIBVIRT_FILE_BACKED_DISCARD_VERSION,
4952                     MIN_QEMU_FILE_BACKED_DISCARD_VERSION):
4953                 membacking.discard = True
4954 
4955         return membacking
4956 
4957     def _get_memory_backing_hugepages_support(self, inst_topology, numatune):
4958         if not self._has_numa_support():
4959             # We should not get here, since we should have avoided
4960             # reporting NUMA topology from _get_host_numa_topology
4961             # in the first place. Just in case of a scheduler
4962             # mess up though, raise an exception
4963             raise exception.MemoryPagesUnsupported()
4964 
4965         host_topology = self._get_host_numa_topology()
4966 
4967         if host_topology is None:
4968             # As above, we should not get here but just in case...
4969             raise exception.MemoryPagesUnsupported()
4970 
4971         # Currently libvirt does not support the smallest
4972         # pagesize set as a backend memory.
4973         # https://bugzilla.redhat.com/show_bug.cgi?id=1173507
4974         avail_pagesize = [page.size_kb
4975                           for page in host_topology.cells[0].mempages]
4976         avail_pagesize.sort()
4977         smallest = avail_pagesize[0]
4978 
4979         pages = []
4980         for guest_cellid, inst_cell in enumerate(inst_topology.cells):
4981             if inst_cell.pagesize and inst_cell.pagesize > smallest:
4982                 for memnode in numatune.memnodes:
4983                     if guest_cellid == memnode.cellid:
4984                         page = (
4985                             vconfig.LibvirtConfigGuestMemoryBackingPage())
4986                         page.nodeset = [guest_cellid]
4987                         page.size_kb = inst_cell.pagesize
4988                         pages.append(page)
4989                         break  # Quit early...
4990         return pages
4991 
4992     def _get_flavor(self, ctxt, instance, flavor):
4993         if flavor is not None:
4994             return flavor
4995         return instance.flavor
4996 
4997     def _has_uefi_support(self):
4998         # This means that the host can support uefi booting for guests
4999         supported_archs = [fields.Architecture.X86_64,
5000                            fields.Architecture.AARCH64]
5001         caps = self._host.get_capabilities()
5002         return ((caps.host.cpu.arch in supported_archs) and
5003                 os.path.exists(DEFAULT_UEFI_LOADER_PATH[caps.host.cpu.arch]))
5004 
5005     def _get_supported_perf_events(self):
5006 
5007         if (len(CONF.libvirt.enabled_perf_events) == 0 or
5008              not self._host.has_min_version(MIN_LIBVIRT_PERF_VERSION)):
5009             return []
5010 
5011         supported_events = []
5012         host_cpu_info = self._get_cpu_info()
5013         for event in CONF.libvirt.enabled_perf_events:
5014             if self._supported_perf_event(event, host_cpu_info['features']):
5015                 supported_events.append(event)
5016         return supported_events
5017 
5018     def _supported_perf_event(self, event, cpu_features):
5019 
5020         libvirt_perf_event_name = LIBVIRT_PERF_EVENT_PREFIX + event.upper()
5021 
5022         if not hasattr(libvirt, libvirt_perf_event_name):
5023             LOG.warning("Libvirt doesn't support event type %s.", event)
5024             return False
5025 
5026         if event in PERF_EVENTS_CPU_FLAG_MAPPING:
5027             LOG.warning('Monitoring Intel CMT `perf` event(s) %s is '
5028                         'deprecated and will be removed in the "Stein" '
5029                         'release.  It was broken by design in the '
5030                         'Linux kernel, so support for Intel CMT was '
5031                         'removed from Linux 4.14 onwards. Therefore '
5032                         'it is recommended to not enable them.',
5033                         event)
5034             if PERF_EVENTS_CPU_FLAG_MAPPING[event] not in cpu_features:
5035                 LOG.warning("Host does not support event type %s.", event)
5036                 return False
5037         return True
5038 
5039     def _configure_guest_by_virt_type(self, guest, virt_type, caps, instance,
5040                                       image_meta, flavor, root_device_name):
5041         if virt_type == "xen":
5042             if guest.os_type == fields.VMMode.HVM:
5043                 guest.os_loader = CONF.libvirt.xen_hvmloader_path
5044             else:
5045                 guest.os_cmdline = CONSOLE
5046         elif virt_type in ("kvm", "qemu"):
5047             if caps.host.cpu.arch in (fields.Architecture.I686,
5048                                       fields.Architecture.X86_64):
5049                 guest.sysinfo = self._get_guest_config_sysinfo(instance)
5050                 guest.os_smbios = vconfig.LibvirtConfigGuestSMBIOS()
5051             hw_firmware_type = image_meta.properties.get('hw_firmware_type')
5052             if caps.host.cpu.arch == fields.Architecture.AARCH64:
5053                 if not hw_firmware_type:
5054                     hw_firmware_type = fields.FirmwareType.UEFI
5055             if hw_firmware_type == fields.FirmwareType.UEFI:
5056                 if self._has_uefi_support():
5057                     global uefi_logged
5058                     if not uefi_logged:
5059                         LOG.warning("uefi support is without some kind of "
5060                                     "functional testing and therefore "
5061                                     "considered experimental.")
5062                         uefi_logged = True
5063                     guest.os_loader = DEFAULT_UEFI_LOADER_PATH[
5064                         caps.host.cpu.arch]
5065                     guest.os_loader_type = "pflash"
5066                 else:
5067                     raise exception.UEFINotSupported()
5068             guest.os_mach_type = self._get_machine_type(image_meta, caps)
5069             if image_meta.properties.get('hw_boot_menu') is None:
5070                 guest.os_bootmenu = strutils.bool_from_string(
5071                     flavor.extra_specs.get('hw:boot_menu', 'no'))
5072             else:
5073                 guest.os_bootmenu = image_meta.properties.hw_boot_menu
5074 
5075         elif virt_type == "lxc":
5076             guest.os_init_path = "/sbin/init"
5077             guest.os_cmdline = CONSOLE
5078         elif virt_type == "uml":
5079             guest.os_kernel = "/usr/bin/linux"
5080             guest.os_root = root_device_name
5081         elif virt_type == "parallels":
5082             if guest.os_type == fields.VMMode.EXE:
5083                 guest.os_init_path = "/sbin/init"
5084 
5085     def _conf_non_lxc_uml(self, virt_type, guest, root_device_name, rescue,
5086                     instance, inst_path, image_meta, disk_info):
5087         if rescue:
5088             self._set_guest_for_rescue(rescue, guest, inst_path, virt_type,
5089                                        root_device_name)
5090         elif instance.kernel_id:
5091             self._set_guest_for_inst_kernel(instance, guest, inst_path,
5092                                             virt_type, root_device_name,
5093                                             image_meta)
5094         else:
5095             guest.os_boot_dev = blockinfo.get_boot_order(disk_info)
5096 
5097     def _create_consoles(self, virt_type, guest_cfg, instance, flavor,
5098                          image_meta):
5099         # NOTE(markus_z): Beware! Below are so many conditionals that it is
5100         # easy to lose track. Use this chart to figure out your case:
5101         #
5102         # case | is serial | is qemu | resulting
5103         #      | enabled?  | or kvm? | devices
5104         # -------------------------------------------
5105         #    1 |        no |     no  | pty*
5106         #    2 |        no |     yes | pty with logd
5107         #    3 |       yes |      no | see case 1
5108         #    4 |       yes |     yes | tcp with logd
5109         #
5110         #    * exception: `virt_type=parallels` doesn't create a device
5111         if virt_type == 'parallels':
5112             pass
5113         elif virt_type not in ("qemu", "kvm"):
5114             log_path = self._get_console_log_path(instance)
5115             self._create_pty_device(guest_cfg,
5116                                     vconfig.LibvirtConfigGuestConsole,
5117                                     log_path=log_path)
5118         elif (virt_type in ("qemu", "kvm") and
5119                   self._is_s390x_guest(image_meta)):
5120             self._create_consoles_s390x(guest_cfg, instance,
5121                                         flavor, image_meta)
5122         elif virt_type in ("qemu", "kvm"):
5123             self._create_consoles_qemu_kvm(guest_cfg, instance,
5124                                         flavor, image_meta)
5125 
5126     def _is_s390x_guest(self, image_meta):
5127         s390x_archs = (fields.Architecture.S390, fields.Architecture.S390X)
5128         return libvirt_utils.get_arch(image_meta) in s390x_archs
5129 
5130     def _create_consoles_qemu_kvm(self, guest_cfg, instance, flavor,
5131                                   image_meta):
5132         char_dev_cls = vconfig.LibvirtConfigGuestSerial
5133         log_path = self._get_console_log_path(instance)
5134         if CONF.serial_console.enabled:
5135             if not self._serial_ports_already_defined(instance):
5136                 num_ports = hardware.get_number_of_serial_ports(flavor,
5137                                                                 image_meta)
5138                 self._check_number_of_serial_console(num_ports)
5139                 self._create_serial_consoles(guest_cfg, num_ports,
5140                                              char_dev_cls, log_path)
5141         else:
5142             self._create_pty_device(guest_cfg, char_dev_cls,
5143                                     log_path=log_path)
5144 
5145     def _create_consoles_s390x(self, guest_cfg, instance, flavor, image_meta):
5146         char_dev_cls = vconfig.LibvirtConfigGuestConsole
5147         log_path = self._get_console_log_path(instance)
5148         if CONF.serial_console.enabled:
5149             if not self._serial_ports_already_defined(instance):
5150                 num_ports = hardware.get_number_of_serial_ports(flavor,
5151                                                                 image_meta)
5152                 self._create_serial_consoles(guest_cfg, num_ports,
5153                                              char_dev_cls, log_path)
5154         else:
5155             self._create_pty_device(guest_cfg, char_dev_cls,
5156                                     "sclp", log_path)
5157 
5158     def _create_pty_device(self, guest_cfg, char_dev_cls, target_type=None,
5159                            log_path=None):
5160 
5161         consolepty = char_dev_cls()
5162         consolepty.target_type = target_type
5163         consolepty.type = "pty"
5164 
5165         log = vconfig.LibvirtConfigGuestCharDeviceLog()
5166         log.file = log_path
5167         consolepty.log = log
5168 
5169         guest_cfg.add_device(consolepty)
5170 
5171     def _serial_ports_already_defined(self, instance):
5172         try:
5173             guest = self._host.get_guest(instance)
5174             if list(self._get_serial_ports_from_guest(guest)):
5175                 # Serial port are already configured for instance that
5176                 # means we are in a context of migration.
5177                 return True
5178         except exception.InstanceNotFound:
5179             LOG.debug(
5180                 "Instance does not exist yet on libvirt, we can "
5181                 "safely pass on looking for already defined serial "
5182                 "ports in its domain XML", instance=instance)
5183         return False
5184 
5185     def _create_serial_consoles(self, guest_cfg, num_ports, char_dev_cls,
5186                                 log_path):
5187         for port in six.moves.range(num_ports):
5188             console = char_dev_cls()
5189             console.port = port
5190             console.type = "tcp"
5191             console.listen_host = CONF.serial_console.proxyclient_address
5192             listen_port = serial_console.acquire_port(console.listen_host)
5193             console.listen_port = listen_port
5194             # NOTE: only the first serial console gets the boot messages,
5195             # that's why we attach the logd subdevice only to that.
5196             if port == 0:
5197                 log = vconfig.LibvirtConfigGuestCharDeviceLog()
5198                 log.file = log_path
5199                 console.log = log
5200             guest_cfg.add_device(console)
5201 
5202     def _cpu_config_to_vcpu_model(self, cpu_config, vcpu_model):
5203         """Update VirtCPUModel object according to libvirt CPU config.
5204 
5205         :param:cpu_config: vconfig.LibvirtConfigGuestCPU presenting the
5206                            instance's virtual cpu configuration.
5207         :param:vcpu_model: VirtCPUModel object. A new object will be created
5208                            if None.
5209 
5210         :return: Updated VirtCPUModel object, or None if cpu_config is None
5211 
5212         """
5213 
5214         if not cpu_config:
5215             return
5216         if not vcpu_model:
5217             vcpu_model = objects.VirtCPUModel()
5218 
5219         vcpu_model.arch = cpu_config.arch
5220         vcpu_model.vendor = cpu_config.vendor
5221         vcpu_model.model = cpu_config.model
5222         vcpu_model.mode = cpu_config.mode
5223         vcpu_model.match = cpu_config.match
5224 
5225         if cpu_config.sockets:
5226             vcpu_model.topology = objects.VirtCPUTopology(
5227                 sockets=cpu_config.sockets,
5228                 cores=cpu_config.cores,
5229                 threads=cpu_config.threads)
5230         else:
5231             vcpu_model.topology = None
5232 
5233         features = [objects.VirtCPUFeature(
5234             name=f.name,
5235             policy=f.policy) for f in cpu_config.features]
5236         vcpu_model.features = features
5237 
5238         return vcpu_model
5239 
5240     def _vcpu_model_to_cpu_config(self, vcpu_model):
5241         """Create libvirt CPU config according to VirtCPUModel object.
5242 
5243         :param:vcpu_model: VirtCPUModel object.
5244 
5245         :return: vconfig.LibvirtConfigGuestCPU.
5246 
5247         """
5248 
5249         cpu_config = vconfig.LibvirtConfigGuestCPU()
5250         cpu_config.arch = vcpu_model.arch
5251         cpu_config.model = vcpu_model.model
5252         cpu_config.mode = vcpu_model.mode
5253         cpu_config.match = vcpu_model.match
5254         cpu_config.vendor = vcpu_model.vendor
5255         if vcpu_model.topology:
5256             cpu_config.sockets = vcpu_model.topology.sockets
5257             cpu_config.cores = vcpu_model.topology.cores
5258             cpu_config.threads = vcpu_model.topology.threads
5259         if vcpu_model.features:
5260             for f in vcpu_model.features:
5261                 xf = vconfig.LibvirtConfigGuestCPUFeature()
5262                 xf.name = f.name
5263                 xf.policy = f.policy
5264                 cpu_config.features.add(xf)
5265         return cpu_config
5266 
5267     def _guest_add_pcie_root_ports(self, guest):
5268         """Add PCI Express root ports.
5269 
5270         PCI Express machine can have as many PCIe devices as it has
5271         pcie-root-port controllers (slots in virtual motherboard).
5272 
5273         If we want to have more PCIe slots for hotplug then we need to create
5274         whole PCIe structure (libvirt limitation).
5275         """
5276 
5277         pcieroot = vconfig.LibvirtConfigGuestPCIeRootController()
5278         guest.add_device(pcieroot)
5279 
5280         for x in range(0, CONF.libvirt.num_pcie_ports):
5281             pcierootport = vconfig.LibvirtConfigGuestPCIeRootPortController()
5282             guest.add_device(pcierootport)
5283 
5284     def _guest_needs_pcie(self, guest, caps):
5285         """Check for prerequisites for adding PCIe root port
5286         controllers
5287         """
5288 
5289         # TODO(kchamart) In the third 'if' conditional below, for 'x86'
5290         # arch, we're assuming: when 'os_mach_type' is 'None', you'll
5291         # have "pc" machine type.  That assumption, although it is
5292         # correct for the "forseeable future", it will be invalid when
5293         # libvirt / QEMU changes the default machine types.
5294         #
5295         # From libvirt 4.7.0 onwards (September 2018), it will ensure
5296         # that *if* 'pc' is available, it will be used as the default --
5297         # to not break existing applications.  (Refer:
5298         # https://libvirt.org/git/?p=libvirt.git;a=commit;h=26cfb1a3
5299         # --"qemu: ensure default machine types don't change if QEMU
5300         # changes").
5301         #
5302         # But even if libvirt (>=v4.7.0) handled the default case,
5303         # relying on such assumptions is not robust.  Instead we should
5304         # get the default machine type for a given architecture reliably
5305         # -- by Nova setting it explicitly (we already do it for Arm /
5306         # AArch64 & s390x).  A part of this bug is being tracked here:
5307         # https://bugs.launchpad.net/nova/+bug/1780138).
5308 
5309         # Add PCIe root port controllers for PCI Express machines
5310         # but only if their amount is configured
5311 
5312         if not CONF.libvirt.num_pcie_ports:
5313             return False
5314         if (caps.host.cpu.arch == fields.Architecture.AARCH64
5315                 and guest.os_mach_type.startswith('virt')):
5316             return True
5317         if (caps.host.cpu.arch == fields.Architecture.X86_64
5318                 and guest.os_mach_type is not None
5319                 and 'q35' in guest.os_mach_type):
5320             return True
5321         return False
5322 
5323     def _guest_add_usb_host_keyboard(self, guest):
5324         """Add USB Host controller and keyboard for graphical console use.
5325 
5326         Add USB keyboard as PS/2 support may not be present on non-x86
5327         architectures.
5328         """
5329         keyboard = vconfig.LibvirtConfigGuestInput()
5330         keyboard.type = "keyboard"
5331         keyboard.bus = "usb"
5332         guest.add_device(keyboard)
5333 
5334         usbhost = vconfig.LibvirtConfigGuestUSBHostController()
5335         usbhost.index = 0
5336         guest.add_device(usbhost)
5337 
5338     def _get_guest_config(self, instance, network_info, image_meta,
5339                           disk_info, rescue=None, block_device_info=None,
5340                           context=None, mdevs=None):
5341         """Get config data for parameters.
5342 
5343         :param rescue: optional dictionary that should contain the key
5344             'ramdisk_id' if a ramdisk is needed for the rescue image and
5345             'kernel_id' if a kernel is needed for the rescue image.
5346 
5347         :param mdevs: optional list of mediated devices to assign to the guest.
5348         """
5349         flavor = instance.flavor
5350         inst_path = libvirt_utils.get_instance_path(instance)
5351         disk_mapping = disk_info['mapping']
5352 
5353         virt_type = CONF.libvirt.virt_type
5354         guest = vconfig.LibvirtConfigGuest()
5355         guest.virt_type = virt_type
5356         guest.name = instance.name
5357         guest.uuid = instance.uuid
5358         # We are using default unit for memory: KiB
5359         guest.memory = flavor.memory_mb * units.Ki
5360         guest.vcpus = flavor.vcpus
5361         allowed_cpus = hardware.get_vcpu_pin_set()
5362 
5363         guest_numa_config = self._get_guest_numa_config(
5364             instance.numa_topology, flavor, allowed_cpus, image_meta)
5365 
5366         guest.cpuset = guest_numa_config.cpuset
5367         guest.cputune = guest_numa_config.cputune
5368         guest.numatune = guest_numa_config.numatune
5369 
5370         guest.membacking = self._get_guest_memory_backing_config(
5371             instance.numa_topology,
5372             guest_numa_config.numatune,
5373             flavor)
5374 
5375         guest.metadata.append(self._get_guest_config_meta(instance))
5376         guest.idmaps = self._get_guest_idmaps()
5377 
5378         for event in self._supported_perf_events:
5379             guest.add_perf_event(event)
5380 
5381         self._update_guest_cputune(guest, flavor, virt_type)
5382 
5383         guest.cpu = self._get_guest_cpu_config(
5384             flavor, image_meta, guest_numa_config.numaconfig,
5385             instance.numa_topology)
5386 
5387         # Notes(yjiang5): we always sync the instance's vcpu model with
5388         # the corresponding config file.
5389         instance.vcpu_model = self._cpu_config_to_vcpu_model(
5390             guest.cpu, instance.vcpu_model)
5391 
5392         if 'root' in disk_mapping:
5393             root_device_name = block_device.prepend_dev(
5394                 disk_mapping['root']['dev'])
5395         else:
5396             root_device_name = None
5397 
5398         if root_device_name:
5399             instance.root_device_name = root_device_name
5400 
5401         guest.os_type = (fields.VMMode.get_from_instance(instance) or
5402                 self._get_guest_os_type(virt_type))
5403         caps = self._host.get_capabilities()
5404 
5405         self._configure_guest_by_virt_type(guest, virt_type, caps, instance,
5406                                            image_meta, flavor,
5407                                            root_device_name)
5408         if virt_type not in ('lxc', 'uml'):
5409             self._conf_non_lxc_uml(virt_type, guest, root_device_name, rescue,
5410                     instance, inst_path, image_meta, disk_info)
5411 
5412         self._set_features(guest, instance.os_type, caps, virt_type,
5413                            image_meta, flavor)
5414         self._set_clock(guest, instance.os_type, image_meta, virt_type)
5415 
5416         storage_configs = self._get_guest_storage_config(context,
5417                 instance, image_meta, disk_info, rescue, block_device_info,
5418                 flavor, guest.os_type)
5419         for config in storage_configs:
5420             guest.add_device(config)
5421 
5422         for vif in network_info:
5423             config = self.vif_driver.get_config(
5424                 instance, vif, image_meta,
5425                 flavor, virt_type, self._host)
5426             guest.add_device(config)
5427 
5428         self._create_consoles(virt_type, guest, instance, flavor, image_meta)
5429 
5430         pointer = self._get_guest_pointer_model(guest.os_type, image_meta)
5431         if pointer:
5432             guest.add_device(pointer)
5433 
5434         self._guest_add_spice_channel(guest)
5435 
5436         if self._guest_add_video_device(guest):
5437             self._add_video_driver(guest, image_meta, flavor)
5438 
5439             # We want video == we want graphical console. Some architectures
5440             # do not have input devices attached in default configuration.
5441             # Let then add USB Host controller and USB keyboard.
5442             # x86(-64) and ppc64 have usb host controller and keyboard
5443             # s390x does not support USB
5444             if caps.host.cpu.arch == fields.Architecture.AARCH64:
5445                 self._guest_add_usb_host_keyboard(guest)
5446 
5447         # Qemu guest agent only support 'qemu' and 'kvm' hypervisor
5448         if virt_type in ('qemu', 'kvm'):
5449             self._set_qemu_guest_agent(guest, flavor, instance, image_meta)
5450 
5451         if self._guest_needs_pcie(guest, caps):
5452             self._guest_add_pcie_root_ports(guest)
5453 
5454         self._guest_add_pci_devices(guest, instance)
5455 
5456         self._guest_add_watchdog_action(guest, flavor, image_meta)
5457 
5458         self._guest_add_memory_balloon(guest)
5459 
5460         if mdevs:
5461             self._guest_add_mdevs(guest, mdevs)
5462 
5463         return guest
5464 
5465     def _guest_add_mdevs(self, guest, chosen_mdevs):
5466         for chosen_mdev in chosen_mdevs:
5467             mdev = vconfig.LibvirtConfigGuestHostdevMDEV()
5468             mdev.uuid = chosen_mdev
5469             guest.add_device(mdev)
5470 
5471     @staticmethod
5472     def _guest_add_spice_channel(guest):
5473         if (CONF.spice.enabled and CONF.spice.agent_enabled
5474                 and guest.virt_type not in ('lxc', 'uml', 'xen')):
5475             channel = vconfig.LibvirtConfigGuestChannel()
5476             channel.type = 'spicevmc'
5477             channel.target_name = "com.redhat.spice.0"
5478             guest.add_device(channel)
5479 
5480     @staticmethod
5481     def _guest_add_memory_balloon(guest):
5482         virt_type = guest.virt_type
5483         # Memory balloon device only support 'qemu/kvm' and 'xen' hypervisor
5484         if (virt_type in ('xen', 'qemu', 'kvm') and
5485                     CONF.libvirt.mem_stats_period_seconds > 0):
5486             balloon = vconfig.LibvirtConfigMemoryBalloon()
5487             if virt_type in ('qemu', 'kvm'):
5488                 balloon.model = 'virtio'
5489             else:
5490                 balloon.model = 'xen'
5491             balloon.period = CONF.libvirt.mem_stats_period_seconds
5492             guest.add_device(balloon)
5493 
5494     @staticmethod
5495     def _guest_add_watchdog_action(guest, flavor, image_meta):
5496         # image meta takes precedence over flavor extra specs; disable the
5497         # watchdog action by default
5498         watchdog_action = (flavor.extra_specs.get('hw:watchdog_action')
5499                            or 'disabled')
5500         watchdog_action = image_meta.properties.get('hw_watchdog_action',
5501                                                     watchdog_action)
5502         # NB(sross): currently only actually supported by KVM/QEmu
5503         if watchdog_action != 'disabled':
5504             if watchdog_action in fields.WatchdogAction.ALL:
5505                 bark = vconfig.LibvirtConfigGuestWatchdog()
5506                 bark.action = watchdog_action
5507                 guest.add_device(bark)
5508             else:
5509                 raise exception.InvalidWatchdogAction(action=watchdog_action)
5510 
5511     def _guest_add_pci_devices(self, guest, instance):
5512         virt_type = guest.virt_type
5513         if virt_type in ('xen', 'qemu', 'kvm'):
5514             # Get all generic PCI devices (non-SR-IOV).
5515             for pci_dev in pci_manager.get_instance_pci_devs(instance):
5516                 guest.add_device(self._get_guest_pci_device(pci_dev))
5517         else:
5518             # PCI devices is only supported for hypervisors
5519             #  'xen', 'qemu' and 'kvm'.
5520             if pci_manager.get_instance_pci_devs(instance, 'all'):
5521                 raise exception.PciDeviceUnsupportedHypervisor(type=virt_type)
5522 
5523     @staticmethod
5524     def _guest_add_video_device(guest):
5525         # NB some versions of libvirt support both SPICE and VNC
5526         # at the same time. We're not trying to second guess which
5527         # those versions are. We'll just let libvirt report the
5528         # errors appropriately if the user enables both.
5529         add_video_driver = False
5530         if CONF.vnc.enabled and guest.virt_type not in ('lxc', 'uml'):
5531             graphics = vconfig.LibvirtConfigGuestGraphics()
5532             graphics.type = "vnc"
5533             if CONF.vnc.keymap:
5534                 graphics.keymap = CONF.vnc.keymap
5535             graphics.listen = CONF.vnc.server_listen
5536             guest.add_device(graphics)
5537             add_video_driver = True
5538         if CONF.spice.enabled and guest.virt_type not in ('lxc', 'uml', 'xen'):
5539             graphics = vconfig.LibvirtConfigGuestGraphics()
5540             graphics.type = "spice"
5541             if CONF.spice.keymap:
5542                 graphics.keymap = CONF.spice.keymap
5543             graphics.listen = CONF.spice.server_listen
5544             guest.add_device(graphics)
5545             add_video_driver = True
5546         return add_video_driver
5547 
5548     def _get_guest_pointer_model(self, os_type, image_meta):
5549         pointer_model = image_meta.properties.get(
5550             'hw_pointer_model', CONF.pointer_model)
5551         if pointer_model is None and CONF.libvirt.use_usb_tablet:
5552             # TODO(sahid): We set pointer_model to keep compatibility
5553             # until the next release O*. It means operators can continue
5554             # to use the deprecated option "use_usb_tablet" or set a
5555             # specific device to use
5556             pointer_model = "usbtablet"
5557             LOG.warning('The option "use_usb_tablet" has been '
5558                         'deprecated for Newton in favor of the more '
5559                         'generic "pointer_model". Please update '
5560                         'nova.conf to address this change.')
5561 
5562         if pointer_model == "usbtablet":
5563             # We want a tablet if VNC is enabled, or SPICE is enabled and
5564             # the SPICE agent is disabled. If the SPICE agent is enabled
5565             # it provides a paravirt mouse which drastically reduces
5566             # overhead (by eliminating USB polling).
5567             if CONF.vnc.enabled or (
5568                     CONF.spice.enabled and not CONF.spice.agent_enabled):
5569                 return self._get_guest_usb_tablet(os_type)
5570             else:
5571                 if CONF.pointer_model or CONF.libvirt.use_usb_tablet:
5572                     # For backward compatibility We don't want to break
5573                     # process of booting an instance if host is configured
5574                     # to use USB tablet without VNC or SPICE and SPICE
5575                     # agent disable.
5576                     LOG.warning('USB tablet requested for guests by host '
5577                                 'configuration. In order to accept this '
5578                                 'request VNC should be enabled or SPICE '
5579                                 'and SPICE agent disabled on host.')
5580                 else:
5581                     raise exception.UnsupportedPointerModelRequested(
5582                         model="usbtablet")
5583 
5584     def _get_guest_usb_tablet(self, os_type):
5585         tablet = None
5586         if os_type == fields.VMMode.HVM:
5587             tablet = vconfig.LibvirtConfigGuestInput()
5588             tablet.type = "tablet"
5589             tablet.bus = "usb"
5590         else:
5591             if CONF.pointer_model or CONF.libvirt.use_usb_tablet:
5592                 # For backward compatibility We don't want to break
5593                 # process of booting an instance if virtual machine mode
5594                 # is not configured as HVM.
5595                 LOG.warning('USB tablet requested for guests by host '
5596                             'configuration. In order to accept this '
5597                             'request the machine mode should be '
5598                             'configured as HVM.')
5599             else:
5600                 raise exception.UnsupportedPointerModelRequested(
5601                     model="usbtablet")
5602         return tablet
5603 
5604     def _get_guest_xml(self, context, instance, network_info, disk_info,
5605                        image_meta, rescue=None,
5606                        block_device_info=None,
5607                        mdevs=None):
5608         # NOTE(danms): Stringifying a NetworkInfo will take a lock. Do
5609         # this ahead of time so that we don't acquire it while also
5610         # holding the logging lock.
5611         network_info_str = str(network_info)
5612         msg = ('Start _get_guest_xml '
5613                'network_info=%(network_info)s '
5614                'disk_info=%(disk_info)s '
5615                'image_meta=%(image_meta)s rescue=%(rescue)s '
5616                'block_device_info=%(block_device_info)s' %
5617                {'network_info': network_info_str, 'disk_info': disk_info,
5618                 'image_meta': image_meta, 'rescue': rescue,
5619                 'block_device_info': block_device_info})
5620         # NOTE(mriedem): block_device_info can contain auth_password so we
5621         # need to sanitize the password in the message.
5622         LOG.debug(strutils.mask_password(msg), instance=instance)
5623         conf = self._get_guest_config(instance, network_info, image_meta,
5624                                       disk_info, rescue, block_device_info,
5625                                       context, mdevs)
5626         xml = conf.to_xml()
5627 
5628         LOG.debug('End _get_guest_xml xml=%(xml)s',
5629                   {'xml': xml}, instance=instance)
5630         return xml
5631 
5632     def get_info(self, instance, use_cache=True):
5633         """Retrieve information from libvirt for a specific instance.
5634 
5635         If a libvirt error is encountered during lookup, we might raise a
5636         NotFound exception or Error exception depending on how severe the
5637         libvirt error is.
5638 
5639         :param instance: nova.objects.instance.Instance object
5640         :param use_cache: unused in this driver
5641         :returns: An InstanceInfo object
5642         """
5643         guest = self._host.get_guest(instance)
5644         # Kind of ugly but we need to pass host to get_info as for a
5645         # workaround, see libvirt/compat.py
5646         return guest.get_info(self._host)
5647 
5648     def _create_domain_setup_lxc(self, context, instance, image_meta,
5649                                  block_device_info):
5650         inst_path = libvirt_utils.get_instance_path(instance)
5651         block_device_mapping = driver.block_device_info_get_mapping(
5652             block_device_info)
5653         root_disk = block_device.get_root_bdm(block_device_mapping)
5654         if root_disk:
5655             self._connect_volume(context, root_disk['connection_info'],
5656                                  instance)
5657             disk_path = root_disk['connection_info']['data']['device_path']
5658 
5659             # NOTE(apmelton) - Even though the instance is being booted from a
5660             # cinder volume, it is still presented as a local block device.
5661             # LocalBlockImage is used here to indicate that the instance's
5662             # disk is backed by a local block device.
5663             image_model = imgmodel.LocalBlockImage(disk_path)
5664         else:
5665             root_disk = self.image_backend.by_name(instance, 'disk')
5666             image_model = root_disk.get_model(self._conn)
5667 
5668         container_dir = os.path.join(inst_path, 'rootfs')
5669         fileutils.ensure_tree(container_dir)
5670         rootfs_dev = disk_api.setup_container(image_model,
5671                                               container_dir=container_dir)
5672 
5673         try:
5674             # Save rootfs device to disconnect it when deleting the instance
5675             if rootfs_dev:
5676                 instance.system_metadata['rootfs_device_name'] = rootfs_dev
5677             if CONF.libvirt.uid_maps or CONF.libvirt.gid_maps:
5678                 id_maps = self._get_guest_idmaps()
5679                 libvirt_utils.chown_for_id_maps(container_dir, id_maps)
5680         except Exception:
5681             with excutils.save_and_reraise_exception():
5682                 self._create_domain_cleanup_lxc(instance)
5683 
5684     def _create_domain_cleanup_lxc(self, instance):
5685         inst_path = libvirt_utils.get_instance_path(instance)
5686         container_dir = os.path.join(inst_path, 'rootfs')
5687 
5688         try:
5689             state = self.get_info(instance).state
5690         except exception.InstanceNotFound:
5691             # The domain may not be present if the instance failed to start
5692             state = None
5693 
5694         if state == power_state.RUNNING:
5695             # NOTE(uni): Now the container is running with its own private
5696             # mount namespace and so there is no need to keep the container
5697             # rootfs mounted in the host namespace
5698             LOG.debug('Attempting to unmount container filesystem: %s',
5699                       container_dir, instance=instance)
5700             disk_api.clean_lxc_namespace(container_dir=container_dir)
5701         else:
5702             disk_api.teardown_container(container_dir=container_dir)
5703 
5704     @contextlib.contextmanager
5705     def _lxc_disk_handler(self, context, instance, image_meta,
5706                           block_device_info):
5707         """Context manager to handle the pre and post instance boot,
5708            LXC specific disk operations.
5709 
5710            An image or a volume path will be prepared and setup to be
5711            used by the container, prior to starting it.
5712            The disk will be disconnected and unmounted if a container has
5713            failed to start.
5714         """
5715 
5716         if CONF.libvirt.virt_type != 'lxc':
5717             yield
5718             return
5719 
5720         self._create_domain_setup_lxc(context, instance, image_meta,
5721                                       block_device_info)
5722 
5723         try:
5724             yield
5725         finally:
5726             self._create_domain_cleanup_lxc(instance)
5727 
5728     # TODO(sahid): Consider renaming this to _create_guest.
5729     def _create_domain(self, xml=None, domain=None,
5730                        power_on=True, pause=False, post_xml_callback=None):
5731         """Create a domain.
5732 
5733         Either domain or xml must be passed in. If both are passed, then
5734         the domain definition is overwritten from the xml.
5735 
5736         :returns guest.Guest: Guest just created
5737         """
5738         if xml:
5739             guest = libvirt_guest.Guest.create(xml, self._host)
5740             if post_xml_callback is not None:
5741                 post_xml_callback()
5742         else:
5743             guest = libvirt_guest.Guest(domain)
5744 
5745         if power_on or pause:
5746             guest.launch(pause=pause)
5747 
5748         if not utils.is_neutron():
5749             guest.enable_hairpin()
5750 
5751         return guest
5752 
5753     def _neutron_failed_callback(self, event_name, instance):
5754         LOG.error('Neutron Reported failure on event '
5755                   '%(event)s for instance %(uuid)s',
5756                   {'event': event_name, 'uuid': instance.uuid},
5757                   instance=instance)
5758         if CONF.vif_plugging_is_fatal:
5759             raise exception.VirtualInterfaceCreateException()
5760 
5761     def _get_neutron_events(self, network_info):
5762         # NOTE(danms): We need to collect any VIFs that are currently
5763         # down that we expect a down->up event for. Anything that is
5764         # already up will not undergo that transition, and for
5765         # anything that might be stale (cache-wise) assume it's
5766         # already up so we don't block on it.
5767         return [('network-vif-plugged', vif['id'])
5768                 for vif in network_info if vif.get('active', True) is False]
5769 
5770     def _cleanup_failed_start(self, context, instance, network_info,
5771                               block_device_info, guest, destroy_disks):
5772         try:
5773             if guest and guest.is_active():
5774                 guest.poweroff()
5775         finally:
5776             self.cleanup(context, instance, network_info=network_info,
5777                          block_device_info=block_device_info,
5778                          destroy_disks=destroy_disks)
5779 
5780     def _create_domain_and_network(self, context, xml, instance, network_info,
5781                                    block_device_info=None, power_on=True,
5782                                    vifs_already_plugged=False,
5783                                    post_xml_callback=None,
5784                                    destroy_disks_on_failure=False):
5785 
5786         """Do required network setup and create domain."""
5787         timeout = CONF.vif_plugging_timeout
5788         if (self._conn_supports_start_paused and
5789             utils.is_neutron() and not
5790             vifs_already_plugged and power_on and timeout):
5791             events = self._get_neutron_events(network_info)
5792         else:
5793             events = []
5794 
5795         pause = bool(events)
5796         guest = None
5797         try:
5798             with self.virtapi.wait_for_instance_event(
5799                     instance, events, deadline=timeout,
5800                     error_callback=self._neutron_failed_callback):
5801                 self.plug_vifs(instance, network_info)
5802                 self.firewall_driver.setup_basic_filtering(instance,
5803                                                            network_info)
5804                 self.firewall_driver.prepare_instance_filter(instance,
5805                                                              network_info)
5806                 with self._lxc_disk_handler(context, instance,
5807                                             instance.image_meta,
5808                                             block_device_info):
5809                     guest = self._create_domain(
5810                         xml, pause=pause, power_on=power_on,
5811                         post_xml_callback=post_xml_callback)
5812 
5813                 self.firewall_driver.apply_instance_filter(instance,
5814                                                            network_info)
5815         except exception.VirtualInterfaceCreateException:
5816             # Neutron reported failure and we didn't swallow it, so
5817             # bail here
5818             with excutils.save_and_reraise_exception():
5819                 self._cleanup_failed_start(context, instance, network_info,
5820                                            block_device_info, guest,
5821                                            destroy_disks_on_failure)
5822         except eventlet.timeout.Timeout:
5823             # We never heard from Neutron
5824             LOG.warning('Timeout waiting for %(events)s for '
5825                         'instance with vm_state %(vm_state)s and '
5826                         'task_state %(task_state)s.',
5827                         {'events': events,
5828                          'vm_state': instance.vm_state,
5829                          'task_state': instance.task_state},
5830                         instance=instance)
5831             if CONF.vif_plugging_is_fatal:
5832                 self._cleanup_failed_start(context, instance, network_info,
5833                                            block_device_info, guest,
5834                                            destroy_disks_on_failure)
5835                 raise exception.VirtualInterfaceCreateException()
5836         except Exception:
5837             # Any other error, be sure to clean up
5838             LOG.error('Failed to start libvirt guest', instance=instance)
5839             with excutils.save_and_reraise_exception():
5840                 self._cleanup_failed_start(context, instance, network_info,
5841                                            block_device_info, guest,
5842                                            destroy_disks_on_failure)
5843 
5844         # Resume only if domain has been paused
5845         if pause:
5846             guest.resume()
5847         return guest
5848 
5849     def _get_vcpu_total(self):
5850         """Get available vcpu number of physical computer.
5851 
5852         :returns: the number of cpu core instances can be used.
5853 
5854         """
5855         try:
5856             total_pcpus = self._host.get_cpu_count()
5857         except libvirt.libvirtError:
5858             LOG.warning("Cannot get the number of cpu, because this "
5859                         "function is not implemented for this platform.")
5860             return 0
5861 
5862         if not CONF.vcpu_pin_set:
5863             return total_pcpus
5864 
5865         available_ids = hardware.get_vcpu_pin_set()
5866         # We get the list of online CPUs on the host and see if the requested
5867         # set falls under these. If not, we retain the old behavior.
5868         online_pcpus = None
5869         try:
5870             online_pcpus = self._host.get_online_cpus()
5871         except libvirt.libvirtError as ex:
5872             error_code = ex.get_error_code()
5873             err_msg = encodeutils.exception_to_unicode(ex)
5874             LOG.warning(
5875                 "Couldn't retrieve the online CPUs due to a Libvirt "
5876                 "error: %(error)s with error code: %(error_code)s",
5877                 {'error': err_msg, 'error_code': error_code})
5878         if online_pcpus:
5879             if not (available_ids <= online_pcpus):
5880                 msg = (_("Invalid vcpu_pin_set config, one or more of the "
5881                          "specified cpuset is not online. Online cpuset(s): "
5882                          "%(online)s, requested cpuset(s): %(req)s"),
5883                        {'online': sorted(online_pcpus),
5884                         'req': sorted(available_ids)})
5885                 raise exception.Invalid(msg)
5886         elif sorted(available_ids)[-1] >= total_pcpus:
5887             raise exception.Invalid(_("Invalid vcpu_pin_set config, "
5888                                       "out of hypervisor cpu range."))
5889         return len(available_ids)
5890 
5891     @staticmethod
5892     def _get_local_gb_info():
5893         """Get local storage info of the compute node in GB.
5894 
5895         :returns: A dict containing:
5896              :total: How big the overall usable filesystem is (in gigabytes)
5897              :free: How much space is free (in gigabytes)
5898              :used: How much space is used (in gigabytes)
5899         """
5900 
5901         if CONF.libvirt.images_type == 'lvm':
5902             info = lvm.get_volume_group_info(
5903                                CONF.libvirt.images_volume_group)
5904         elif CONF.libvirt.images_type == 'rbd':
5905             info = LibvirtDriver._get_rbd_driver().get_pool_info()
5906         else:
5907             info = libvirt_utils.get_fs_info(CONF.instances_path)
5908 
5909         for (k, v) in info.items():
5910             info[k] = v / units.Gi
5911 
5912         return info
5913 
5914     def _get_vcpu_used(self):
5915         """Get vcpu usage number of physical computer.
5916 
5917         :returns: The total number of vcpu(s) that are currently being used.
5918 
5919         """
5920 
5921         total = 0
5922 
5923         # Not all libvirt drivers will support the get_vcpus_info()
5924         #
5925         # For example, LXC does not have a concept of vCPUs, while
5926         # QEMU (TCG) traditionally handles all vCPUs in a single
5927         # thread. So both will report an exception when the vcpus()
5928         # API call is made. In such a case we should report the
5929         # guest as having 1 vCPU, since that lets us still do
5930         # CPU over commit calculations that apply as the total
5931         # guest count scales.
5932         #
5933         # It is also possible that we might see an exception if
5934         # the guest is just in middle of shutting down. Technically
5935         # we should report 0 for vCPU usage in this case, but we
5936         # we can't reliably distinguish the vcpu not supported
5937         # case from the just shutting down case. Thus we don't know
5938         # whether to report 1 or 0 for vCPU count.
5939         #
5940         # Under-reporting vCPUs is bad because it could conceivably
5941         # let the scheduler place too many guests on the host. Over-
5942         # reporting vCPUs is not a problem as it'll auto-correct on
5943         # the next refresh of usage data.
5944         #
5945         # Thus when getting an exception we always report 1 as the
5946         # vCPU count, as the least worst value.
5947         for guest in self._host.list_guests():
5948             try:
5949                 vcpus = guest.get_vcpus_info()
5950                 total += len(list(vcpus))
5951             except libvirt.libvirtError:
5952                 total += 1
5953             # NOTE(gtt116): give other tasks a chance.
5954             greenthread.sleep(0)
5955         return total
5956 
5957     def _get_supported_vgpu_types(self):
5958         if not CONF.devices.enabled_vgpu_types:
5959             return []
5960         # TODO(sbauza): Move this check up to compute_manager.init_host
5961         if len(CONF.devices.enabled_vgpu_types) > 1:
5962             LOG.warning('libvirt only supports one GPU type per compute node,'
5963                         ' only first type will be used.')
5964         requested_types = CONF.devices.enabled_vgpu_types[:1]
5965         return requested_types
5966 
5967     def _count_mediated_devices(self, enabled_vgpu_types):
5968         """Counts the sysfs objects (handles) that represent a mediated device
5969         and filtered by $enabled_vgpu_types.
5970 
5971         Those handles can be in use by a libvirt guest or not.
5972 
5973         :param enabled_vgpu_types: list of enabled VGPU types on this host
5974         :returns: dict, keyed by parent GPU libvirt PCI device ID, of number of
5975         mdev device handles for that GPU
5976         """
5977 
5978         counts_per_parent = collections.defaultdict(int)
5979         mediated_devices = self._get_mediated_devices(types=enabled_vgpu_types)
5980         for mdev in mediated_devices:
5981             counts_per_parent[mdev['parent']] += 1
5982         return counts_per_parent
5983 
5984     def _count_mdev_capable_devices(self, enabled_vgpu_types):
5985         """Counts the mdev-capable devices on this host filtered by
5986         $enabled_vgpu_types.
5987 
5988         :param enabled_vgpu_types: list of enabled VGPU types on this host
5989         :returns: dict, keyed by device name, to an integer count of available
5990             instances of each type per device
5991         """
5992         mdev_capable_devices = self._get_mdev_capable_devices(
5993             types=enabled_vgpu_types)
5994         counts_per_dev = collections.defaultdict(int)
5995         for dev in mdev_capable_devices:
5996             # dev_id is the libvirt name for the PCI device,
5997             # eg. pci_0000_84_00_0 which matches a PCI address of 0000:84:00.0
5998             dev_name = dev['dev_id']
5999             for _type in dev['types']:
6000                 available = dev['types'][_type]['availableInstances']
6001                 # TODO(sbauza): Once we support multiple types, check which
6002                 # PCI devices are set for this type
6003                 # NOTE(sbauza): Even if we support multiple types, Nova will
6004                 # only use one per physical GPU.
6005                 counts_per_dev[dev_name] += available
6006         return counts_per_dev
6007 
6008     def _get_gpu_inventories(self):
6009         """Returns the inventories for each physical GPU for a specific type
6010         supported by the enabled_vgpu_types CONF option.
6011 
6012         :returns: dict, keyed by libvirt PCI name, of dicts like:
6013                 {'pci_0000_84_00_0':
6014                     {'total': $TOTAL,
6015                      'min_unit': 1,
6016                      'max_unit': $TOTAL,
6017                      'step_size': 1,
6018                      'reserved': 0,
6019                      'allocation_ratio': 1.0,
6020                     }
6021                 }
6022         """
6023 
6024         # Bail out early if operator doesn't care about providing vGPUs
6025         enabled_vgpu_types = self._get_supported_vgpu_types()
6026         if not enabled_vgpu_types:
6027             return {}
6028         inventories = {}
6029         count_per_parent = self._count_mediated_devices(enabled_vgpu_types)
6030         for dev_name, count in count_per_parent.items():
6031             inventories[dev_name] = {'total': count}
6032         # Filter how many available mdevs we can create for all the supported
6033         # types.
6034         count_per_dev = self._count_mdev_capable_devices(enabled_vgpu_types)
6035         # Combine the counts into the dict that we return to the caller.
6036         for dev_name, count in count_per_dev.items():
6037             inv_per_parent = inventories.setdefault(
6038                 dev_name, {'total': 0})
6039             inv_per_parent['total'] += count
6040             inv_per_parent.update({
6041                 'min_unit': 1,
6042                 'step_size': 1,
6043                 'reserved': 0,
6044                 # NOTE(sbauza): There is no sense to have a ratio but 1.0
6045                 # since we can't overallocate vGPU resources
6046                 'allocation_ratio': 1.0,
6047                 # FIXME(sbauza): Some vendors could support only one
6048                 'max_unit': inv_per_parent['total'],
6049             })
6050 
6051         return inventories
6052 
6053     def _get_instance_capabilities(self):
6054         """Get hypervisor instance capabilities
6055 
6056         Returns a list of tuples that describe instances the
6057         hypervisor is capable of hosting.  Each tuple consists
6058         of the triplet (arch, hypervisor_type, vm_mode).
6059 
6060         Supported hypervisor_type is filtered by virt_type,
6061         a parameter set by operators via `nova.conf`.
6062 
6063         :returns: List of tuples describing instance capabilities
6064         """
6065         caps = self._host.get_capabilities()
6066         instance_caps = list()
6067         for g in caps.guests:
6068             for dt in g.domtype:
6069                 if dt != CONF.libvirt.virt_type:
6070                     continue
6071                 try:
6072                     instance_cap = (
6073                         fields.Architecture.canonicalize(g.arch),
6074                         fields.HVType.canonicalize(dt),
6075                         fields.VMMode.canonicalize(g.ostype))
6076                     instance_caps.append(instance_cap)
6077                 except exception.InvalidArchitectureName:
6078                     # NOTE(danms): Libvirt is exposing a guest arch that nova
6079                     # does not even know about. Avoid aborting here and
6080                     # continue to process the rest.
6081                     pass
6082 
6083         return instance_caps
6084 
6085     def _get_cpu_info(self):
6086         """Get cpuinfo information.
6087 
6088         Obtains cpu feature from virConnect.getCapabilities.
6089 
6090         :return: see above description
6091 
6092         """
6093 
6094         caps = self._host.get_capabilities()
6095         cpu_info = dict()
6096 
6097         cpu_info['arch'] = caps.host.cpu.arch
6098         cpu_info['model'] = caps.host.cpu.model
6099         cpu_info['vendor'] = caps.host.cpu.vendor
6100 
6101         topology = dict()
6102         topology['cells'] = len(getattr(caps.host.topology, 'cells', [1]))
6103         topology['sockets'] = caps.host.cpu.sockets
6104         topology['cores'] = caps.host.cpu.cores
6105         topology['threads'] = caps.host.cpu.threads
6106         cpu_info['topology'] = topology
6107 
6108         features = set()
6109         for f in caps.host.cpu.features:
6110             features.add(f.name)
6111         cpu_info['features'] = features
6112         return cpu_info
6113 
6114     def _get_pcinet_info(self, vf_address):
6115         """Returns a dict of NET device."""
6116         devname = pci_utils.get_net_name_by_vf_pci_address(vf_address)
6117         if not devname:
6118             return
6119 
6120         virtdev = self._host.device_lookup_by_name(devname)
6121         xmlstr = virtdev.XMLDesc(0)
6122         cfgdev = vconfig.LibvirtConfigNodeDevice()
6123         cfgdev.parse_str(xmlstr)
6124         return {'name': cfgdev.name,
6125                 'capabilities': cfgdev.pci_capability.features}
6126 
6127     def _get_pcidev_info(self, devname):
6128         """Returns a dict of PCI device."""
6129 
6130         def _get_device_type(cfgdev, pci_address):
6131             """Get a PCI device's device type.
6132 
6133             An assignable PCI device can be a normal PCI device,
6134             a SR-IOV Physical Function (PF), or a SR-IOV Virtual
6135             Function (VF). Only normal PCI devices or SR-IOV VFs
6136             are assignable, while SR-IOV PFs are always owned by
6137             hypervisor.
6138             """
6139             for fun_cap in cfgdev.pci_capability.fun_capability:
6140                 if fun_cap.type == 'virt_functions':
6141                     return {
6142                         'dev_type': fields.PciDeviceType.SRIOV_PF,
6143                     }
6144                 if (fun_cap.type == 'phys_function' and
6145                     len(fun_cap.device_addrs) != 0):
6146                     phys_address = "%04x:%02x:%02x.%01x" % (
6147                         fun_cap.device_addrs[0][0],
6148                         fun_cap.device_addrs[0][1],
6149                         fun_cap.device_addrs[0][2],
6150                         fun_cap.device_addrs[0][3])
6151                     result = {
6152                         'dev_type': fields.PciDeviceType.SRIOV_VF,
6153                         'parent_addr': phys_address,
6154                     }
6155                     parent_ifname = None
6156                     try:
6157                         parent_ifname = pci_utils.get_ifname_by_pci_address(
6158                             pci_address, pf_interface=True)
6159                     except exception.PciDeviceNotFoundById:
6160                         # NOTE(sean-k-mooney): we ignore this error as it
6161                         # is expected when the virtual function is not a NIC.
6162                         pass
6163                     if parent_ifname:
6164                         result['parent_ifname'] = parent_ifname
6165                     return result
6166 
6167             return {'dev_type': fields.PciDeviceType.STANDARD}
6168 
6169         def _get_device_capabilities(device, address):
6170             """Get PCI VF device's additional capabilities.
6171 
6172             If a PCI device is a virtual function, this function reads the PCI
6173             parent's network capabilities (must be always a NIC device) and
6174             appends this information to the device's dictionary.
6175             """
6176             if device.get('dev_type') == fields.PciDeviceType.SRIOV_VF:
6177                 pcinet_info = self._get_pcinet_info(address)
6178                 if pcinet_info:
6179                     return {'capabilities':
6180                                 {'network': pcinet_info.get('capabilities')}}
6181             return {}
6182 
6183         virtdev = self._host.device_lookup_by_name(devname)
6184         xmlstr = virtdev.XMLDesc(0)
6185         cfgdev = vconfig.LibvirtConfigNodeDevice()
6186         cfgdev.parse_str(xmlstr)
6187 
6188         address = "%04x:%02x:%02x.%1x" % (
6189             cfgdev.pci_capability.domain,
6190             cfgdev.pci_capability.bus,
6191             cfgdev.pci_capability.slot,
6192             cfgdev.pci_capability.function)
6193 
6194         device = {
6195             "dev_id": cfgdev.name,
6196             "address": address,
6197             "product_id": "%04x" % cfgdev.pci_capability.product_id,
6198             "vendor_id": "%04x" % cfgdev.pci_capability.vendor_id,
6199             }
6200 
6201         device["numa_node"] = cfgdev.pci_capability.numa_node
6202 
6203         # requirement by DataBase Model
6204         device['label'] = 'label_%(vendor_id)s_%(product_id)s' % device
6205         device.update(_get_device_type(cfgdev, address))
6206         device.update(_get_device_capabilities(device, address))
6207         return device
6208 
6209     def _get_pci_passthrough_devices(self):
6210         """Get host PCI devices information.
6211 
6212         Obtains pci devices information from libvirt, and returns
6213         as a JSON string.
6214 
6215         Each device information is a dictionary, with mandatory keys
6216         of 'address', 'vendor_id', 'product_id', 'dev_type', 'dev_id',
6217         'label' and other optional device specific information.
6218 
6219         Refer to the objects/pci_device.py for more idea of these keys.
6220 
6221         :returns: a JSON string containing a list of the assignable PCI
6222                   devices information
6223         """
6224         # Bail early if we know we can't support `listDevices` to avoid
6225         # repeated warnings within a periodic task
6226         if not getattr(self, '_list_devices_supported', True):
6227             return jsonutils.dumps([])
6228 
6229         try:
6230             dev_names = self._host.list_pci_devices() or []
6231         except libvirt.libvirtError as ex:
6232             error_code = ex.get_error_code()
6233             if error_code == libvirt.VIR_ERR_NO_SUPPORT:
6234                 self._list_devices_supported = False
6235                 LOG.warning("URI %(uri)s does not support "
6236                             "listDevices: %(error)s",
6237                             {'uri': self._uri(),
6238                              'error': encodeutils.exception_to_unicode(ex)})
6239                 return jsonutils.dumps([])
6240             else:
6241                 raise
6242 
6243         pci_info = []
6244         for name in dev_names:
6245             pci_info.append(self._get_pcidev_info(name))
6246 
6247         return jsonutils.dumps(pci_info)
6248 
6249     def _get_mdev_capabilities_for_dev(self, devname, types=None):
6250         """Returns a dict of MDEV capable device with the ID as first key
6251         and then a list of supported types, each of them being a dict.
6252 
6253         :param types: Only return those specific types.
6254         """
6255         virtdev = self._host.device_lookup_by_name(devname)
6256         xmlstr = virtdev.XMLDesc(0)
6257         cfgdev = vconfig.LibvirtConfigNodeDevice()
6258         cfgdev.parse_str(xmlstr)
6259 
6260         device = {
6261             "dev_id": cfgdev.name,
6262             "types": {},
6263             "vendor_id": cfgdev.pci_capability.vendor_id,
6264         }
6265         for mdev_cap in cfgdev.pci_capability.mdev_capability:
6266             for cap in mdev_cap.mdev_types:
6267                 if not types or cap['type'] in types:
6268                     device["types"].update({cap['type']: {
6269                         'availableInstances': cap['availableInstances'],
6270                         'name': cap['name'],
6271                         'deviceAPI': cap['deviceAPI']}})
6272         return device
6273 
6274     def _get_mdev_capable_devices(self, types=None):
6275         """Get host devices supporting mdev types.
6276 
6277         Obtain devices information from libvirt and returns a list of
6278         dictionaries.
6279 
6280         :param types: Filter only devices supporting those types.
6281         """
6282         if not self._host.has_min_version(MIN_LIBVIRT_MDEV_SUPPORT):
6283             return []
6284         dev_names = self._host.list_mdev_capable_devices() or []
6285         mdev_capable_devices = []
6286         for name in dev_names:
6287             device = self._get_mdev_capabilities_for_dev(name, types)
6288             if not device["types"]:
6289                 continue
6290             mdev_capable_devices.append(device)
6291         return mdev_capable_devices
6292 
6293     def _get_mediated_device_information(self, devname):
6294         """Returns a dict of a mediated device."""
6295         virtdev = self._host.device_lookup_by_name(devname)
6296         xmlstr = virtdev.XMLDesc(0)
6297         cfgdev = vconfig.LibvirtConfigNodeDevice()
6298         cfgdev.parse_str(xmlstr)
6299 
6300         device = {
6301             "dev_id": cfgdev.name,
6302             # name is like mdev_00ead764_fdc0_46b6_8db9_2963f5c815b4
6303             "uuid": libvirt_utils.mdev_name2uuid(cfgdev.name),
6304             # the physical GPU PCI device
6305             "parent": cfgdev.parent,
6306             "type": cfgdev.mdev_information.type,
6307             "iommu_group": cfgdev.mdev_information.iommu_group,
6308         }
6309         return device
6310 
6311     def _get_mediated_devices(self, types=None):
6312         """Get host mediated devices.
6313 
6314         Obtain devices information from libvirt and returns a list of
6315         dictionaries.
6316 
6317         :param types: Filter only devices supporting those types.
6318         """
6319         if not self._host.has_min_version(MIN_LIBVIRT_MDEV_SUPPORT):
6320             return []
6321         dev_names = self._host.list_mediated_devices() or []
6322         mediated_devices = []
6323         for name in dev_names:
6324             device = self._get_mediated_device_information(name)
6325             if not types or device["type"] in types:
6326                 mediated_devices.append(device)
6327         return mediated_devices
6328 
6329     def _get_all_assigned_mediated_devices(self, instance=None):
6330         """Lookup all instances from the host and return all the mediated
6331         devices that are assigned to a guest.
6332 
6333         :param instance: Only return mediated devices for that instance.
6334 
6335         :returns: A dictionary of keys being mediated device UUIDs and their
6336                   respective values the instance UUID of the guest using it.
6337                   Returns an empty dict if an instance is provided but not
6338                   found in the hypervisor.
6339         """
6340         allocated_mdevs = {}
6341         if instance:
6342             # NOTE(sbauza): In some cases (like a migration issue), the
6343             # instance can exist in the Nova database but libvirt doesn't know
6344             # about it. For such cases, the way to fix that is to hard reboot
6345             # the instance, which will recreate the libvirt guest.
6346             # For that reason, we need to support that case by making sure
6347             # we don't raise an exception if the libvirt guest doesn't exist.
6348             try:
6349                 guest = self._host.get_guest(instance)
6350             except exception.InstanceNotFound:
6351                 # Bail out early if libvirt doesn't know about it since we
6352                 # can't know the existing mediated devices
6353                 return {}
6354             guests = [guest]
6355         else:
6356             guests = self._host.list_guests(only_running=False)
6357         for guest in guests:
6358             cfg = guest.get_config()
6359             for device in cfg.devices:
6360                 if isinstance(device, vconfig.LibvirtConfigGuestHostdevMDEV):
6361                     allocated_mdevs[device.uuid] = guest.uuid
6362         return allocated_mdevs
6363 
6364     @staticmethod
6365     def _vgpu_allocations(allocations):
6366         """Filtering only the VGPU allocations from a list of allocations.
6367 
6368         :param allocations: Information about resources allocated to the
6369                             instance via placement, of the form returned by
6370                             SchedulerReportClient.get_allocations_for_consumer.
6371         """
6372         if not allocations:
6373             # If no allocations, there is no vGPU request.
6374             return {}
6375         RC_VGPU = orc.VGPU
6376         vgpu_allocations = {}
6377         for rp in allocations:
6378             res = allocations[rp]['resources']
6379             if RC_VGPU in res and res[RC_VGPU] > 0:
6380                 vgpu_allocations[rp] = {'resources': {RC_VGPU: res[RC_VGPU]}}
6381         return vgpu_allocations
6382 
6383     def _get_existing_mdevs_not_assigned(self, requested_types=None,
6384                                          parent=None):
6385         """Returns the already created mediated devices that are not assigned
6386         to a guest yet.
6387 
6388         :param requested_types: Filter out the result for only mediated devices
6389                                 having those types.
6390         :param parent: Filter out result for only mdevs from the parent device.
6391         """
6392         allocated_mdevs = self._get_all_assigned_mediated_devices()
6393         mdevs = self._get_mediated_devices(requested_types)
6394         available_mdevs = set()
6395         for mdev in mdevs:
6396             if parent is None or mdev['parent'] == parent:
6397                 available_mdevs.add(mdev["uuid"])
6398 
6399         available_mdevs -= set(allocated_mdevs)
6400         return available_mdevs
6401 
6402     def _create_new_mediated_device(self, requested_types, uuid=None,
6403                                     parent=None):
6404         """Find a physical device that can support a new mediated device and
6405         create it.
6406 
6407         :param requested_types: Filter only capable devices supporting those
6408                                 types.
6409         :param uuid: The possible mdev UUID we want to create again
6410         :param parent: Only create a mdev for this device
6411 
6412         :returns: the newly created mdev UUID or None if not possible
6413         """
6414         # Try to see if we can still create a new mediated device
6415         devices = self._get_mdev_capable_devices(requested_types)
6416         for device in devices:
6417             # For the moment, the libvirt driver only supports one
6418             # type per host
6419             # TODO(sbauza): Once we support more than one type, make
6420             # sure we look at the flavor/trait for the asked type.
6421             asked_type = requested_types[0]
6422             if device['types'][asked_type]['availableInstances'] > 0:
6423                 # That physical GPU has enough room for a new mdev
6424                 dev_name = device['dev_id']
6425                 # the parent attribute can be None
6426                 if parent is not None and dev_name != parent:
6427                     # The device is not the one that was called, not creating
6428                     # the mdev
6429                     continue
6430                 # We need the PCI address, not the libvirt name
6431                 # The libvirt name is like 'pci_0000_84_00_0'
6432                 pci_addr = "{}:{}:{}.{}".format(*dev_name[4:].split('_'))
6433                 chosen_mdev = nova.privsep.libvirt.create_mdev(pci_addr,
6434                                                                asked_type,
6435                                                                uuid=uuid)
6436                 return chosen_mdev
6437 
6438     @utils.synchronized(VGPU_RESOURCE_SEMAPHORE)
6439     def _allocate_mdevs(self, allocations):
6440         """Returns a list of mediated device UUIDs corresponding to available
6441         resources we can assign to the guest(s) corresponding to the allocation
6442         requests passed as argument.
6443 
6444         That method can either find an existing but unassigned mediated device
6445         it can allocate, or create a new mediated device from a capable
6446         physical device if the latter has enough left capacity.
6447 
6448         :param allocations: Information about resources allocated to the
6449                             instance via placement, of the form returned by
6450                             SchedulerReportClient.get_allocations_for_consumer.
6451                             That code is supporting Placement API version 1.12
6452         """
6453         vgpu_allocations = self._vgpu_allocations(allocations)
6454         if not vgpu_allocations:
6455             return
6456         # TODO(sbauza): Once we have nested resource providers, find which one
6457         # is having the related allocation for the specific VGPU type.
6458         # For the moment, we should only have one allocation for
6459         # ResourceProvider.
6460         # TODO(sbauza): Iterate over all the allocations once we have
6461         # nested Resource Providers. For the moment, just take the first.
6462         if len(vgpu_allocations) > 1:
6463             LOG.warning('More than one allocation was passed over to libvirt '
6464                         'while at the moment libvirt only supports one. Only '
6465                         'the first allocation will be looked up.')
6466         rp_uuid, alloc = six.next(six.iteritems(vgpu_allocations))
6467         vgpus_asked = alloc['resources'][orc.VGPU]
6468 
6469         # Find if we allocated against a specific pGPU (and then the allocation
6470         # is made against a child RP) or any pGPU (in case the VGPU inventory
6471         # is still on the root RP)
6472         try:
6473             allocated_rp = self.provider_tree.data(rp_uuid)
6474         except ValueError:
6475             # The provider doesn't exist, return a better understandable
6476             # exception
6477             raise exception.ComputeResourcesUnavailable(
6478                 reason='vGPU resource is not available')
6479         # TODO(sbauza): Remove this conditional in Train once all VGPU
6480         # inventories are related to a child RP
6481         if allocated_rp.parent_uuid is None:
6482             # We are on a root RP
6483             parent_device = None
6484         else:
6485             rp_name = allocated_rp.name
6486             # There can be multiple roots, we need to find the root name
6487             # to guess the physical device name
6488             roots = self.provider_tree.roots
6489             for root in roots:
6490                 if rp_name.startswith(root.name + '_'):
6491                     # The RP name convention is :
6492                     #    root_name + '_' + parent_device
6493                     parent_device = rp_name[len(root.name) + 1:]
6494                     break
6495             else:
6496                 LOG.warning("pGPU device name %(name)s can't be guessed from "
6497                             "the ProviderTree "
6498                             "roots %(roots)s", {'name': rp_name,
6499                                                  'roots': roots})
6500                 # We f... have no idea what was the parent device
6501                 # If we can't find devices having available VGPUs, just raise
6502                 raise exception.ComputeResourcesUnavailable(
6503                     reason='vGPU resource is not available')
6504 
6505         requested_types = self._get_supported_vgpu_types()
6506         # Which mediated devices are created but not assigned to a guest ?
6507         mdevs_available = self._get_existing_mdevs_not_assigned(
6508             requested_types, parent_device)
6509 
6510         chosen_mdevs = []
6511         for c in six.moves.range(vgpus_asked):
6512             chosen_mdev = None
6513             if mdevs_available:
6514                 # Take the first available mdev
6515                 chosen_mdev = mdevs_available.pop()
6516             else:
6517                 chosen_mdev = self._create_new_mediated_device(
6518                     requested_types, parent=parent_device)
6519             if not chosen_mdev:
6520                 # If we can't find devices having available VGPUs, just raise
6521                 raise exception.ComputeResourcesUnavailable(
6522                     reason='vGPU resource is not available')
6523             else:
6524                 chosen_mdevs.append(chosen_mdev)
6525         return chosen_mdevs
6526 
6527     def _detach_mediated_devices(self, guest):
6528         mdevs = guest.get_all_devices(
6529             devtype=vconfig.LibvirtConfigGuestHostdevMDEV)
6530         for mdev_cfg in mdevs:
6531             try:
6532                 guest.detach_device(mdev_cfg, live=True)
6533             except libvirt.libvirtError as ex:
6534                 error_code = ex.get_error_code()
6535                 # NOTE(sbauza): There is a pending issue with libvirt that
6536                 # doesn't allow to hot-unplug mediated devices. Let's
6537                 # short-circuit the suspend action and set the instance back
6538                 # to ACTIVE.
6539                 # TODO(sbauza): Once libvirt supports this, amend the resume()
6540                 # operation to support reallocating mediated devices.
6541                 if error_code == libvirt.VIR_ERR_CONFIG_UNSUPPORTED:
6542                     reason = _("Suspend is not supported for instances having "
6543                                "attached vGPUs.")
6544                     raise exception.InstanceFaultRollback(
6545                         exception.InstanceSuspendFailure(reason=reason))
6546                 else:
6547                     raise
6548 
6549     def _has_numa_support(self):
6550         # This means that the host can support LibvirtConfigGuestNUMATune
6551         # and the nodeset field in LibvirtConfigGuestMemoryBackingPage
6552         caps = self._host.get_capabilities()
6553 
6554         if (caps.host.cpu.arch in (fields.Architecture.I686,
6555                                    fields.Architecture.X86_64,
6556                                    fields.Architecture.AARCH64) and
6557                 self._host.has_min_version(hv_type=host.HV_DRIVER_QEMU)):
6558             return True
6559         elif (caps.host.cpu.arch in (fields.Architecture.PPC64,
6560                                      fields.Architecture.PPC64LE)):
6561             return True
6562 
6563         return False
6564 
6565     def _get_host_numa_topology(self):
6566         if not self._has_numa_support():
6567             return
6568 
6569         caps = self._host.get_capabilities()
6570         topology = caps.host.topology
6571 
6572         if topology is None or not topology.cells:
6573             return
6574 
6575         cells = []
6576         allowed_cpus = hardware.get_vcpu_pin_set()
6577         online_cpus = self._host.get_online_cpus()
6578         if allowed_cpus:
6579             allowed_cpus &= online_cpus
6580         else:
6581             allowed_cpus = online_cpus
6582 
6583         def _get_reserved_memory_for_cell(self, cell_id, page_size):
6584             cell = self._reserved_hugepages.get(cell_id, {})
6585             return cell.get(page_size, 0)
6586 
6587         def _get_physnet_numa_affinity():
6588             affinities = {cell.id: set() for cell in topology.cells}
6589             for physnet in CONF.neutron.physnets:
6590                 # This will error out if the group is not registered, which is
6591                 # exactly what we want as that would be a bug
6592                 group = getattr(CONF, 'neutron_physnet_%s' % physnet)
6593 
6594                 if not group.numa_nodes:
6595                     msg = ("the physnet '%s' was listed in '[neutron] "
6596                            "physnets' but no corresponding "
6597                            "'[neutron_physnet_%s] numa_nodes' option was "
6598                            "defined." % (physnet, physnet))
6599                     raise exception.InvalidNetworkNUMAAffinity(reason=msg)
6600 
6601                 for node in group.numa_nodes:
6602                     if node not in affinities:
6603                         msg = ("node %d for physnet %s is not present in host "
6604                                "affinity set %r" % (node, physnet, affinities))
6605                         # The config option referenced an invalid node
6606                         raise exception.InvalidNetworkNUMAAffinity(reason=msg)
6607                     affinities[node].add(physnet)
6608 
6609             return affinities
6610 
6611         def _get_tunnel_numa_affinity():
6612             affinities = {cell.id: False for cell in topology.cells}
6613 
6614             for node in CONF.neutron_tunnel.numa_nodes:
6615                 if node not in affinities:
6616                     msg = ("node %d for tunneled networks is not present "
6617                            "in host affinity set %r" % (node, affinities))
6618                     # The config option referenced an invalid node
6619                     raise exception.InvalidNetworkNUMAAffinity(reason=msg)
6620                 affinities[node] = True
6621 
6622             return affinities
6623 
6624         physnet_affinities = _get_physnet_numa_affinity()
6625         tunnel_affinities = _get_tunnel_numa_affinity()
6626 
6627         for cell in topology.cells:
6628             cpuset = set(cpu.id for cpu in cell.cpus)
6629             siblings = sorted(map(set,
6630                                   set(tuple(cpu.siblings)
6631                                         if cpu.siblings else ()
6632                                       for cpu in cell.cpus)
6633                                   ))
6634             cpuset &= allowed_cpus
6635             siblings = [sib & allowed_cpus for sib in siblings]
6636             # Filter out empty sibling sets that may be left
6637             siblings = [sib for sib in siblings if len(sib) > 0]
6638 
6639             mempages = [
6640                 objects.NUMAPagesTopology(
6641                     size_kb=pages.size,
6642                     total=pages.total,
6643                     used=0,
6644                     reserved=_get_reserved_memory_for_cell(
6645                         self, cell.id, pages.size))
6646                 for pages in cell.mempages]
6647 
6648             network_metadata = objects.NetworkMetadata(
6649                 physnets=physnet_affinities[cell.id],
6650                 tunneled=tunnel_affinities[cell.id])
6651 
6652             cell = objects.NUMACell(id=cell.id, cpuset=cpuset,
6653                                     memory=cell.memory / units.Ki,
6654                                     cpu_usage=0, memory_usage=0,
6655                                     siblings=siblings,
6656                                     pinned_cpus=set([]),
6657                                     mempages=mempages,
6658                                     network_metadata=network_metadata)
6659             cells.append(cell)
6660 
6661         return objects.NUMATopology(cells=cells)
6662 
6663     def get_all_volume_usage(self, context, compute_host_bdms):
6664         """Return usage info for volumes attached to vms on
6665            a given host.
6666         """
6667         vol_usage = []
6668 
6669         for instance_bdms in compute_host_bdms:
6670             instance = instance_bdms['instance']
6671 
6672             for bdm in instance_bdms['instance_bdms']:
6673                 mountpoint = bdm['device_name']
6674                 if mountpoint.startswith('/dev/'):
6675                     mountpoint = mountpoint[5:]
6676                 volume_id = bdm['volume_id']
6677 
6678                 LOG.debug("Trying to get stats for the volume %s",
6679                           volume_id, instance=instance)
6680                 vol_stats = self.block_stats(instance, mountpoint)
6681 
6682                 if vol_stats:
6683                     stats = dict(volume=volume_id,
6684                                  instance=instance,
6685                                  rd_req=vol_stats[0],
6686                                  rd_bytes=vol_stats[1],
6687                                  wr_req=vol_stats[2],
6688                                  wr_bytes=vol_stats[3])
6689                     LOG.debug(
6690                         "Got volume usage stats for the volume=%(volume)s,"
6691                         " rd_req=%(rd_req)d, rd_bytes=%(rd_bytes)d, "
6692                         "wr_req=%(wr_req)d, wr_bytes=%(wr_bytes)d",
6693                         stats, instance=instance)
6694                     vol_usage.append(stats)
6695 
6696         return vol_usage
6697 
6698     def block_stats(self, instance, disk_id):
6699         """Note that this function takes an instance name."""
6700         try:
6701             guest = self._host.get_guest(instance)
6702             dev = guest.get_block_device(disk_id)
6703             return dev.blockStats()
6704         except libvirt.libvirtError as e:
6705             errcode = e.get_error_code()
6706             LOG.info('Getting block stats failed, device might have '
6707                      'been detached. Instance=%(instance_name)s '
6708                      'Disk=%(disk)s Code=%(errcode)s Error=%(e)s',
6709                      {'instance_name': instance.name, 'disk': disk_id,
6710                       'errcode': errcode, 'e': e},
6711                      instance=instance)
6712         except exception.InstanceNotFound:
6713             LOG.info('Could not find domain in libvirt for instance %s. '
6714                      'Cannot get block stats for device', instance.name,
6715                      instance=instance)
6716 
6717     def get_console_pool_info(self, console_type):
6718         # TODO(mdragon): console proxy should be implemented for libvirt,
6719         #                in case someone wants to use it with kvm or
6720         #                such. For now return fake data.
6721         return {'address': '127.0.0.1',
6722                 'username': 'fakeuser',
6723                 'password': 'fakepassword'}
6724 
6725     def refresh_security_group_rules(self, security_group_id):
6726         self.firewall_driver.refresh_security_group_rules(security_group_id)
6727 
6728     def refresh_instance_security_rules(self, instance):
6729         self.firewall_driver.refresh_instance_security_rules(instance)
6730 
6731     def update_provider_tree(self, provider_tree, nodename, allocations=None):
6732         """Update a ProviderTree object with current resource provider,
6733         inventory information and CPU traits.
6734 
6735         :param nova.compute.provider_tree.ProviderTree provider_tree:
6736             A nova.compute.provider_tree.ProviderTree object representing all
6737             the providers in the tree associated with the compute node, and any
6738             sharing providers (those with the ``MISC_SHARES_VIA_AGGREGATE``
6739             trait) associated via aggregate with any of those providers (but
6740             not *their* tree- or aggregate-associated providers), as currently
6741             known by placement.
6742         :param nodename:
6743             String name of the compute node (i.e.
6744             ComputeNode.hypervisor_hostname) for which the caller is requesting
6745             updated provider information.
6746         :param allocations:
6747             Dict of allocation data of the form:
6748               { $CONSUMER_UUID: {
6749                     # The shape of each "allocations" dict below is identical
6750                     # to the return from GET /allocations/{consumer_uuid}
6751                     "allocations": {
6752                         $RP_UUID: {
6753                             "generation": $RP_GEN,
6754                             "resources": {
6755                                 $RESOURCE_CLASS: $AMOUNT,
6756                                 ...
6757                             },
6758                         },
6759                         ...
6760                     },
6761                     "project_id": $PROJ_ID,
6762                     "user_id": $USER_ID,
6763                     "consumer_generation": $CONSUMER_GEN,
6764                 },
6765                 ...
6766               }
6767             If None, and the method determines that any inventory needs to be
6768             moved (from one provider to another and/or to a different resource
6769             class), the ReshapeNeeded exception must be raised. Otherwise, this
6770             dict must be edited in place to indicate the desired final state of
6771             allocations.
6772         :raises ReshapeNeeded: If allocations is None and any inventory needs
6773             to be moved from one provider to another and/or to a different
6774             resource class.
6775         :raises: ReshapeFailed if the requested tree reshape fails for
6776             whatever reason.
6777         """
6778         disk_gb = int(self._get_local_gb_info()['total'])
6779         memory_mb = int(self._host.get_memory_mb_total())
6780         vcpus = self._get_vcpu_total()
6781 
6782         # NOTE(yikun): If the inv record does not exists, the allocation_ratio
6783         # will use the CONF.xxx_allocation_ratio value if xxx_allocation_ratio
6784         # is set, and fallback to use the initial_xxx_allocation_ratio
6785         # otherwise.
6786         inv = provider_tree.data(nodename).inventory
6787         ratios = self._get_allocation_ratios(inv)
6788         result = {
6789             orc.VCPU: {
6790                 'total': vcpus,
6791                 'min_unit': 1,
6792                 'max_unit': vcpus,
6793                 'step_size': 1,
6794                 'allocation_ratio': ratios[orc.VCPU],
6795                 'reserved': CONF.reserved_host_cpus,
6796             },
6797             orc.MEMORY_MB: {
6798                 'total': memory_mb,
6799                 'min_unit': 1,
6800                 'max_unit': memory_mb,
6801                 'step_size': 1,
6802                 'allocation_ratio': ratios[orc.MEMORY_MB],
6803                 'reserved': CONF.reserved_host_memory_mb,
6804             },
6805         }
6806 
6807         # If a sharing DISK_GB provider exists in the provider tree, then our
6808         # storage is shared, and we should not report the DISK_GB inventory in
6809         # the compute node provider.
6810         # TODO(efried): Reinstate non-reporting of shared resource by the
6811         # compute RP once the issues from bug #1784020 have been resolved.
6812         if provider_tree.has_sharing_provider(orc.DISK_GB):
6813             LOG.debug('Ignoring sharing provider - see bug #1784020')
6814         result[orc.DISK_GB] = {
6815             'total': disk_gb,
6816             'min_unit': 1,
6817             'max_unit': disk_gb,
6818             'step_size': 1,
6819             'allocation_ratio': ratios[orc.DISK_GB],
6820             'reserved': self._get_reserved_host_disk_gb_from_config(),
6821         }
6822 
6823         # NOTE(sbauza): For the moment, the libvirt driver only supports
6824         # providing the total number of virtual GPUs for a single GPU type. If
6825         # you have multiple physical GPUs, each of them providing multiple GPU
6826         # types, only one type will be used for each of the physical GPUs.
6827         # If one of the pGPUs doesn't support this type, it won't be used.
6828         # TODO(sbauza): Use traits to make a better world.
6829         inventories_dict = self._get_gpu_inventories()
6830         if inventories_dict:
6831             self._update_provider_tree_for_vgpu(
6832                 inventories_dict, provider_tree, nodename,
6833                 allocations=allocations)
6834 
6835         provider_tree.update_inventory(nodename, result)
6836 
6837         traits = self._get_cpu_traits()
6838         if traits is not None:
6839             # _get_cpu_traits returns a dict of trait names mapped to boolean
6840             # values. Add traits equal to True to provider tree, remove
6841             # those False traits from provider tree.
6842             traits_to_add = [t for t in traits if traits[t]]
6843             traits_to_remove = set(traits) - set(traits_to_add)
6844             provider_tree.add_traits(nodename, *traits_to_add)
6845             provider_tree.remove_traits(nodename, *traits_to_remove)
6846 
6847         # Now that we updated the ProviderTree, we want to store it locally
6848         # so that spawn() or other methods can access it thru a getter
6849         self.provider_tree = copy.deepcopy(provider_tree)
6850 
6851     @staticmethod
6852     def _is_reshape_needed_vgpu_on_root(provider_tree, nodename):
6853         """Determine if root RP has VGPU inventories.
6854 
6855         Check to see if the root compute node provider in the tree for
6856         this host already has VGPU inventory because if it does, we either
6857         need to signal for a reshape (if _update_provider_tree_for_vgpu()
6858         has no allocations) or move the allocations within the ProviderTree if
6859         passed.
6860 
6861         :param provider_tree: The ProviderTree object for this host.
6862         :param nodename: The ComputeNode.hypervisor_hostname, also known as
6863             the name of the root node provider in the tree for this host.
6864         :returns: boolean, whether we have VGPU root inventory.
6865         """
6866         root_node = provider_tree.data(nodename)
6867         return orc.VGPU in root_node.inventory
6868 
6869     @staticmethod
6870     def _ensure_pgpu_providers(inventories_dict, provider_tree, nodename):
6871         """Ensures GPU inventory providers exist in the tree for $nodename.
6872 
6873         GPU providers are named $nodename_$gpu-device-id, e.g.
6874         ``somehost.foo.bar.com_pci_0000_84_00_0``.
6875 
6876         :param inventories_dict: Dictionary of inventories for VGPU class
6877             directly provided by _get_gpu_inventories() and which looks like:
6878                 {'pci_0000_84_00_0':
6879                     {'total': $TOTAL,
6880                      'min_unit': 1,
6881                      'max_unit': $MAX_UNIT, # defaults to $TOTAL
6882                      'step_size': 1,
6883                      'reserved': 0,
6884                      'allocation_ratio': 1.0,
6885                     }
6886                 }
6887         :param provider_tree: The ProviderTree to update.
6888         :param nodename: The ComputeNode.hypervisor_hostname, also known as
6889             the name of the root node provider in the tree for this host.
6890         :returns: dict, keyed by GPU device ID, to ProviderData object
6891             representing that resource provider in the tree
6892         """
6893         # Create the VGPU child providers if they do not already exist.
6894         # TODO(mriedem): For the moment, _get_supported_vgpu_types() only
6895         # returns one single type but that will be changed once we support
6896         # multiple types.
6897         # Note that we can't support multiple vgpu types until a reshape has
6898         # been performed on the vgpu resources provided by the root provider,
6899         # if any.
6900 
6901         # Dict of PGPU RPs keyed by their libvirt PCI name
6902         pgpu_rps = {}
6903         for pgpu_dev_id, inventory in inventories_dict.items():
6904             # For each physical GPU, we make sure to have a child provider
6905             pgpu_rp_name = '%s_%s' % (nodename, pgpu_dev_id)
6906             if not provider_tree.exists(pgpu_rp_name):
6907                 # This is the first time creating the child provider so add
6908                 # it to the tree under the root node provider.
6909                 provider_tree.new_child(pgpu_rp_name, nodename)
6910             # We want to idempotently return the resource providers with VGPUs
6911             pgpu_rp = provider_tree.data(pgpu_rp_name)
6912             pgpu_rps[pgpu_dev_id] = pgpu_rp
6913 
6914             # The VGPU inventory goes on a child provider of the given root
6915             # node, identified by $nodename.
6916             pgpu_inventory = {orc.VGPU: inventory}
6917             provider_tree.update_inventory(pgpu_rp_name, pgpu_inventory)
6918         return pgpu_rps
6919 
6920     @staticmethod
6921     def _assert_is_root_provider(
6922             rp_uuid, root_node, consumer_uuid, alloc_data):
6923         """Asserts during a reshape that rp_uuid is for the root node provider.
6924 
6925         When reshaping, inventory and allocations should be on the root node
6926         provider and then moved to child providers.
6927 
6928         :param rp_uuid: UUID of the provider that holds inventory/allocations.
6929         :param root_node: ProviderData object representing the root node in a
6930             provider tree.
6931         :param consumer_uuid: UUID of the consumer (instance) holding resource
6932             allocations against the given rp_uuid provider.
6933         :param alloc_data: dict of allocation data for the consumer.
6934         :raises: ReshapeFailed if rp_uuid is not the root node indicating a
6935             reshape was needed but the inventory/allocation structure is not
6936             expected.
6937         """
6938         if rp_uuid != root_node.uuid:
6939             # Something is wrong - VGPU inventory should
6940             # only be on the root node provider if we are
6941             # reshaping the tree.
6942             msg = (_('Unexpected VGPU resource allocation '
6943                      'on provider %(rp_uuid)s for consumer '
6944                      '%(consumer_uuid)s: %(alloc_data)s. '
6945                      'Expected VGPU allocation to be on root '
6946                      'compute node provider %(root_uuid)s.')
6947                    % {'rp_uuid': rp_uuid,
6948                       'consumer_uuid': consumer_uuid,
6949                       'alloc_data': alloc_data,
6950                       'root_uuid': root_node.uuid})
6951             raise exception.ReshapeFailed(error=msg)
6952 
6953     def _get_assigned_mdevs_for_reshape(
6954             self, instance_uuid, rp_uuid, alloc_data):
6955         """Gets the mediated devices assigned to the instance during a reshape.
6956 
6957         :param instance_uuid: UUID of the instance consuming VGPU resources
6958             on this host.
6959         :param rp_uuid: UUID of the resource provider with VGPU inventory being
6960             consumed by the instance.
6961         :param alloc_data: dict of allocation data for the instance consumer.
6962         :return: list of mediated device UUIDs assigned to the instance
6963         :raises: ReshapeFailed if the instance is not found in the hypervisor
6964             or no mediated devices were found to be assigned to the instance
6965             indicating VGPU allocations are out of sync with the hypervisor
6966         """
6967         # FIXME(sbauza): We don't really need an Instance
6968         # object, but given some libvirt.host logs needs
6969         # to have an instance name, just provide a fake one
6970         Instance = collections.namedtuple('Instance', ['uuid', 'name'])
6971         instance = Instance(uuid=instance_uuid, name=instance_uuid)
6972         mdevs = self._get_all_assigned_mediated_devices(instance)
6973         # _get_all_assigned_mediated_devices returns {} if the instance is
6974         # not found in the hypervisor
6975         if not mdevs:
6976             # If we found a VGPU allocation against a consumer
6977             # which is not an instance, the only left case for
6978             # Nova would be a migration but we don't support
6979             # this at the moment.
6980             msg = (_('Unexpected VGPU resource allocation on provider '
6981                      '%(rp_uuid)s for consumer %(consumer_uuid)s: '
6982                      '%(alloc_data)s. The allocation is made against a '
6983                      'non-existing instance or there are no devices assigned.')
6984                    % {'rp_uuid': rp_uuid, 'consumer_uuid': instance_uuid,
6985                       'alloc_data': alloc_data})
6986             raise exception.ReshapeFailed(error=msg)
6987         return mdevs
6988 
6989     def _count_vgpus_per_pgpu(self, mdev_uuids):
6990         """Count the number of VGPUs per physical GPU mediated device.
6991 
6992         :param mdev_uuids: List of physical GPU mediated device UUIDs.
6993         :return: dict, keyed by PGPU device ID, to count of VGPUs on that
6994             device
6995         """
6996         vgpu_count_per_pgpu = collections.defaultdict(int)
6997         for mdev_uuid in mdev_uuids:
6998             # libvirt name is like mdev_00ead764_fdc0_46b6_8db9_2963f5c815b4
6999             dev_name = libvirt_utils.mdev_uuid2name(mdev_uuid)
7000             # Count how many vGPUs are in use for this instance
7001             dev_info = self._get_mediated_device_information(dev_name)
7002             pgpu_dev_id = dev_info['parent']
7003             vgpu_count_per_pgpu[pgpu_dev_id] += 1
7004         return vgpu_count_per_pgpu
7005 
7006     @staticmethod
7007     def _check_vgpu_allocations_match_real_use(
7008             vgpu_count_per_pgpu, expected_usage, rp_uuid, consumer_uuid,
7009             alloc_data):
7010         """Checks that the number of GPU devices assigned to the consumer
7011         matches what is expected from the allocations in the placement service
7012         and logs a warning if there is a mismatch.
7013 
7014         :param vgpu_count_per_pgpu: dict, keyed by PGPU device ID, to count of
7015             VGPUs on that device where each device is assigned to the consumer
7016             (guest instance on this hypervisor)
7017         :param expected_usage: The expected usage from placement for the
7018             given resource provider and consumer
7019         :param rp_uuid: UUID of the resource provider with VGPU inventory being
7020             consumed by the instance
7021         :param consumer_uuid: UUID of the consumer (instance) holding resource
7022             allocations against the given rp_uuid provider
7023         :param alloc_data: dict of allocation data for the instance consumer
7024         """
7025         actual_usage = sum(vgpu_count_per_pgpu.values())
7026         if actual_usage != expected_usage:
7027             # Don't make it blocking, just make sure you actually correctly
7028             # allocate the existing resources
7029             LOG.warning(
7030                 'Unexpected VGPU resource allocation on provider %(rp_uuid)s '
7031                 'for consumer %(consumer_uuid)s: %(alloc_data)s. Allocations '
7032                 '(%(expected_usage)s) differ from actual use '
7033                 '(%(actual_usage)s).',
7034                 {'rp_uuid': rp_uuid, 'consumer_uuid': consumer_uuid,
7035                  'alloc_data': alloc_data, 'expected_usage': expected_usage,
7036                  'actual_usage': actual_usage})
7037 
7038     def _reshape_vgpu_allocations(
7039             self, rp_uuid, root_node, consumer_uuid, alloc_data, resources,
7040             pgpu_rps):
7041         """Update existing VGPU allocations by moving them from the root node
7042         provider to the child provider for the given VGPU provider.
7043 
7044         :param rp_uuid: UUID of the VGPU resource provider with allocations
7045             from consumer_uuid (should be the root node provider before
7046             reshaping occurs)
7047         :param root_node: ProviderData object for the root compute node
7048             resource provider in the provider tree
7049         :param consumer_uuid: UUID of the consumer (instance) with VGPU
7050             allocations against the resource provider represented by rp_uuid
7051         :param alloc_data: dict of allocation information for consumer_uuid
7052         :param resources: dict, keyed by resource class, of resources allocated
7053             to consumer_uuid from rp_uuid
7054         :param pgpu_rps: dict, keyed by GPU device ID, to ProviderData object
7055             representing that resource provider in the tree
7056         :raises: ReshapeFailed if the reshape fails for whatever reason
7057         """
7058         # We've found VGPU allocations on a provider. It should be the root
7059         # node provider.
7060         self._assert_is_root_provider(
7061             rp_uuid, root_node, consumer_uuid, alloc_data)
7062 
7063         # Find which physical GPU corresponds to this allocation.
7064         mdev_uuids = self._get_assigned_mdevs_for_reshape(
7065             consumer_uuid, rp_uuid, alloc_data)
7066 
7067         vgpu_count_per_pgpu = self._count_vgpus_per_pgpu(mdev_uuids)
7068 
7069         # We need to make sure we found all the mediated devices that
7070         # correspond to an allocation.
7071         self._check_vgpu_allocations_match_real_use(
7072             vgpu_count_per_pgpu, resources[orc.VGPU],
7073             rp_uuid, consumer_uuid, alloc_data)
7074 
7075         # Add the VGPU allocation for each VGPU provider.
7076         allocs = alloc_data['allocations']
7077         for pgpu_dev_id, pgpu_rp in pgpu_rps.items():
7078             vgpu_count = vgpu_count_per_pgpu[pgpu_dev_id]
7079             if vgpu_count:
7080                 allocs[pgpu_rp.uuid] = {
7081                     'resources': {
7082                         orc.VGPU: vgpu_count
7083                     }
7084                 }
7085         # And remove the VGPU allocation from the root node provider.
7086         del resources[orc.VGPU]
7087 
7088     def _reshape_gpu_resources(
7089             self, allocations, root_node, pgpu_rps):
7090         """Reshapes the provider tree moving VGPU inventory from root to child
7091 
7092         :param allocations:
7093             Dict of allocation data of the form:
7094               { $CONSUMER_UUID: {
7095                     # The shape of each "allocations" dict below is identical
7096                     # to the return from GET /allocations/{consumer_uuid}
7097                     "allocations": {
7098                         $RP_UUID: {
7099                             "generation": $RP_GEN,
7100                             "resources": {
7101                                 $RESOURCE_CLASS: $AMOUNT,
7102                                 ...
7103                             },
7104                         },
7105                         ...
7106                     },
7107                     "project_id": $PROJ_ID,
7108                     "user_id": $USER_ID,
7109                     "consumer_generation": $CONSUMER_GEN,
7110                 },
7111                 ...
7112               }
7113         :params root_node: The root node in the provider tree
7114         :params pgpu_rps: dict, keyed by GPU device ID, to ProviderData object
7115             representing that resource provider in the tree
7116         """
7117         LOG.info('Reshaping tree; moving VGPU allocations from root '
7118                  'provider %s to child providers %s.', root_node.uuid,
7119                  pgpu_rps.values())
7120         # For each consumer in the allocations dict, look for VGPU
7121         # allocations and move them to the VGPU provider.
7122         for consumer_uuid, alloc_data in allocations.items():
7123             # Copy and iterate over the current set of providers to avoid
7124             # modifying keys while iterating.
7125             allocs = alloc_data['allocations']
7126             for rp_uuid in list(allocs):
7127                 resources = allocs[rp_uuid]['resources']
7128                 if orc.VGPU in resources:
7129                     self._reshape_vgpu_allocations(
7130                         rp_uuid, root_node, consumer_uuid, alloc_data,
7131                         resources, pgpu_rps)
7132 
7133     def _update_provider_tree_for_vgpu(self, inventories_dict, provider_tree,
7134                                        nodename, allocations=None):
7135         """Updates the provider tree for VGPU inventory.
7136 
7137         Before Stein, VGPU inventory and allocations were on the root compute
7138         node provider in the tree. Starting in Stein, the VGPU inventory is
7139         on a child provider in the tree. As a result, this method will
7140         "reshape" the tree if necessary on first start of this compute service
7141         in Stein.
7142 
7143         :param inventories_dict: Dictionary of inventories for VGPU class
7144             directly provided by _get_gpu_inventories() and which looks like:
7145                 {'pci_0000_84_00_0':
7146                     {'total': $TOTAL,
7147                      'min_unit': 1,
7148                      'max_unit': $MAX_UNIT, # defaults to $TOTAL
7149                      'step_size': 1,
7150                      'reserved': 0,
7151                      'allocation_ratio': 1.0,
7152                     }
7153                 }
7154         :param provider_tree: The ProviderTree to update.
7155         :param nodename: The ComputeNode.hypervisor_hostname, also known as
7156             the name of the root node provider in the tree for this host.
7157         :param allocations: If not None, indicates a reshape was requested and
7158             should be performed.
7159         :raises: nova.exception.ReshapeNeeded if ``allocations`` is None and
7160             the method determines a reshape of the tree is needed, i.e. VGPU
7161             inventory and allocations must be migrated from the root node
7162             provider to a child provider of VGPU resources in the tree.
7163         :raises: nova.exception.ReshapeFailed if the requested tree reshape
7164             fails for whatever reason.
7165         """
7166         # Check to see if the root compute node provider in the tree for
7167         # this host already has VGPU inventory because if it does, and
7168         # we're not currently reshaping (allocations is None), we need
7169         # to indicate that a reshape is needed to move the VGPU inventory
7170         # onto a child provider in the tree.
7171 
7172         # Ensure GPU providers are in the ProviderTree for the given inventory.
7173         pgpu_rps = self._ensure_pgpu_providers(
7174             inventories_dict, provider_tree, nodename)
7175 
7176         if self._is_reshape_needed_vgpu_on_root(provider_tree, nodename):
7177             if allocations is None:
7178                 # We have old VGPU inventory on root RP, but we haven't yet
7179                 # allocations. That means we need to ask for a reshape.
7180                 LOG.info('Requesting provider tree reshape in order to move '
7181                          'VGPU inventory from the root compute node provider '
7182                          '%s to a child provider.', nodename)
7183                 raise exception.ReshapeNeeded()
7184             # We have allocations, that means we already asked for a reshape
7185             # and the Placement API returned us them. We now need to move
7186             # those from the root RP to the needed children RPs.
7187             root_node = provider_tree.data(nodename)
7188             # Reshape VGPU provider inventory and allocations, moving them
7189             # from the root node provider to the child providers.
7190             self._reshape_gpu_resources(allocations, root_node, pgpu_rps)
7191             # Only delete the root inventory once the reshape is done
7192             if orc.VGPU in root_node.inventory:
7193                 del root_node.inventory[orc.VGPU]
7194                 provider_tree.update_inventory(nodename, root_node.inventory)
7195 
7196     def get_available_resource(self, nodename):
7197         """Retrieve resource information.
7198 
7199         This method is called when nova-compute launches, and
7200         as part of a periodic task that records the results in the DB.
7201 
7202         :param nodename: unused in this driver
7203         :returns: dictionary containing resource info
7204         """
7205 
7206         disk_info_dict = self._get_local_gb_info()
7207         data = {}
7208 
7209         # NOTE(dprince): calling capabilities before getVersion works around
7210         # an initialization issue with some versions of Libvirt (1.0.5.5).
7211         # See: https://bugzilla.redhat.com/show_bug.cgi?id=1000116
7212         # See: https://bugs.launchpad.net/nova/+bug/1215593
7213         data["supported_instances"] = self._get_instance_capabilities()
7214 
7215         data["vcpus"] = self._get_vcpu_total()
7216         data["memory_mb"] = self._host.get_memory_mb_total()
7217         data["local_gb"] = disk_info_dict['total']
7218         data["vcpus_used"] = self._get_vcpu_used()
7219         data["memory_mb_used"] = self._host.get_memory_mb_used()
7220         data["local_gb_used"] = disk_info_dict['used']
7221         data["hypervisor_type"] = self._host.get_driver_type()
7222         data["hypervisor_version"] = self._host.get_version()
7223         data["hypervisor_hostname"] = self._host.get_hostname()
7224         # TODO(berrange): why do we bother converting the
7225         # libvirt capabilities XML into a special JSON format ?
7226         # The data format is different across all the drivers
7227         # so we could just return the raw capabilities XML
7228         # which 'compare_cpu' could use directly
7229         #
7230         # That said, arch_filter.py now seems to rely on
7231         # the libvirt drivers format which suggests this
7232         # data format needs to be standardized across drivers
7233         data["cpu_info"] = jsonutils.dumps(self._get_cpu_info())
7234 
7235         disk_free_gb = disk_info_dict['free']
7236         disk_over_committed = self._get_disk_over_committed_size_total()
7237         available_least = disk_free_gb * units.Gi - disk_over_committed
7238         data['disk_available_least'] = available_least / units.Gi
7239 
7240         data['pci_passthrough_devices'] = self._get_pci_passthrough_devices()
7241 
7242         numa_topology = self._get_host_numa_topology()
7243         if numa_topology:
7244             data['numa_topology'] = numa_topology._to_json()
7245         else:
7246             data['numa_topology'] = None
7247 
7248         return data
7249 
7250     def check_instance_shared_storage_local(self, context, instance):
7251         """Check if instance files located on shared storage.
7252 
7253         This runs check on the destination host, and then calls
7254         back to the source host to check the results.
7255 
7256         :param context: security context
7257         :param instance: nova.objects.instance.Instance object
7258         :returns:
7259          - tempfile: A dict containing the tempfile info on the destination
7260                      host
7261          - None:
7262 
7263             1. If the instance path is not existing.
7264             2. If the image backend is shared block storage type.
7265         """
7266         if self.image_backend.backend().is_shared_block_storage():
7267             return None
7268 
7269         dirpath = libvirt_utils.get_instance_path(instance)
7270 
7271         if not os.path.exists(dirpath):
7272             return None
7273 
7274         fd, tmp_file = tempfile.mkstemp(dir=dirpath)
7275         LOG.debug("Creating tmpfile %s to verify with other "
7276                   "compute node that the instance is on "
7277                   "the same shared storage.",
7278                   tmp_file, instance=instance)
7279         os.close(fd)
7280         return {"filename": tmp_file}
7281 
7282     def check_instance_shared_storage_remote(self, context, data):
7283         return os.path.exists(data['filename'])
7284 
7285     def check_instance_shared_storage_cleanup(self, context, data):
7286         fileutils.delete_if_exists(data["filename"])
7287 
7288     def check_can_live_migrate_destination(self, context, instance,
7289                                            src_compute_info, dst_compute_info,
7290                                            block_migration=False,
7291                                            disk_over_commit=False):
7292         """Check if it is possible to execute live migration.
7293 
7294         This runs checks on the destination host, and then calls
7295         back to the source host to check the results.
7296 
7297         :param context: security context
7298         :param instance: nova.db.sqlalchemy.models.Instance
7299         :param block_migration: if true, prepare for block migration
7300         :param disk_over_commit: if true, allow disk over commit
7301         :returns: a LibvirtLiveMigrateData object
7302         """
7303 
7304         # TODO(zcornelius): Remove this check in Stein, as we'll only support
7305         #                   Rocky and newer computes.
7306         # If file_backed_memory is enabled on this host, we have to make sure
7307         # the source is new enough to support it. Since the source generates
7308         # the XML for the destination, we depend on the source generating a
7309         # file-backed XML for us, so fail if it won't do that.
7310         if CONF.libvirt.file_backed_memory > 0:
7311             srv = objects.Service.get_by_compute_host(context, instance.host)
7312             if srv.version < 32:
7313                 msg = ("Cannot migrate instance %(uuid)s from node %(node)s. "
7314                        "Node %(node)s is not compatible with "
7315                        "file_backed_memory" % {"uuid": instance.uuid,
7316                                                "node": srv.host})
7317                 raise exception.MigrationPreCheckError(reason=msg)
7318 
7319         if disk_over_commit:
7320             disk_available_gb = dst_compute_info['free_disk_gb']
7321         else:
7322             disk_available_gb = dst_compute_info['disk_available_least']
7323         disk_available_mb = (
7324             (disk_available_gb * units.Ki) - CONF.reserved_host_disk_mb)
7325 
7326         # Compare CPU
7327         if not instance.vcpu_model or not instance.vcpu_model.model:
7328             source_cpu_info = src_compute_info['cpu_info']
7329             self._compare_cpu(None, source_cpu_info, instance)
7330         else:
7331             self._compare_cpu(instance.vcpu_model, None, instance)
7332 
7333         # Create file on storage, to be checked on source host
7334         filename = self._create_shared_storage_test_file(instance)
7335 
7336         data = objects.LibvirtLiveMigrateData()
7337         data.filename = filename
7338         data.image_type = CONF.libvirt.images_type
7339         data.graphics_listen_addr_vnc = CONF.vnc.server_listen
7340         data.graphics_listen_addr_spice = CONF.spice.server_listen
7341         if CONF.serial_console.enabled:
7342             data.serial_listen_addr = CONF.serial_console.proxyclient_address
7343         else:
7344             data.serial_listen_addr = None
7345         # Notes(eliqiao): block_migration and disk_over_commit are not
7346         # nullable, so just don't set them if they are None
7347         if block_migration is not None:
7348             data.block_migration = block_migration
7349         if disk_over_commit is not None:
7350             data.disk_over_commit = disk_over_commit
7351         data.disk_available_mb = disk_available_mb
7352         data.dst_wants_file_backed_memory = CONF.libvirt.file_backed_memory > 0
7353         data.file_backed_memory_discard = (CONF.libvirt.file_backed_memory and
7354             self._host.has_min_version(MIN_LIBVIRT_FILE_BACKED_DISCARD_VERSION,
7355                                        MIN_QEMU_FILE_BACKED_DISCARD_VERSION))
7356 
7357         return data
7358 
7359     def cleanup_live_migration_destination_check(self, context,
7360                                                  dest_check_data):
7361         """Do required cleanup on dest host after check_can_live_migrate calls
7362 
7363         :param context: security context
7364         """
7365         filename = dest_check_data.filename
7366         self._cleanup_shared_storage_test_file(filename)
7367 
7368     def check_can_live_migrate_source(self, context, instance,
7369                                       dest_check_data,
7370                                       block_device_info=None):
7371         """Check if it is possible to execute live migration.
7372 
7373         This checks if the live migration can succeed, based on the
7374         results from check_can_live_migrate_destination.
7375 
7376         :param context: security context
7377         :param instance: nova.db.sqlalchemy.models.Instance
7378         :param dest_check_data: result of check_can_live_migrate_destination
7379         :param block_device_info: result of _get_instance_block_device_info
7380         :returns: a LibvirtLiveMigrateData object
7381         """
7382         # Checking shared storage connectivity
7383         # if block migration, instances_path should not be on shared storage.
7384         source = CONF.host
7385 
7386         dest_check_data.is_shared_instance_path = (
7387             self._check_shared_storage_test_file(
7388                 dest_check_data.filename, instance))
7389 
7390         dest_check_data.is_shared_block_storage = (
7391             self._is_shared_block_storage(instance, dest_check_data,
7392                                           block_device_info))
7393 
7394         if 'block_migration' not in dest_check_data:
7395             dest_check_data.block_migration = (
7396                 not dest_check_data.is_on_shared_storage())
7397 
7398         if dest_check_data.block_migration:
7399             # TODO(eliqiao): Once block_migration flag is removed from the API
7400             # we can safely remove the if condition
7401             if dest_check_data.is_on_shared_storage():
7402                 reason = _("Block migration can not be used "
7403                            "with shared storage.")
7404                 raise exception.InvalidLocalStorage(reason=reason, path=source)
7405             if 'disk_over_commit' in dest_check_data:
7406                 self._assert_dest_node_has_enough_disk(context, instance,
7407                                         dest_check_data.disk_available_mb,
7408                                         dest_check_data.disk_over_commit,
7409                                         block_device_info)
7410             if block_device_info:
7411                 bdm = block_device_info.get('block_device_mapping')
7412                 # NOTE(eliqiao): Selective disk migrations are not supported
7413                 # with tunnelled block migrations so we can block them early.
7414                 if (bdm and
7415                     (self._block_migration_flags &
7416                      libvirt.VIR_MIGRATE_TUNNELLED != 0)):
7417                     msg = (_('Cannot block migrate instance %(uuid)s with'
7418                              ' mapped volumes. Selective block device'
7419                              ' migration is not supported with tunnelled'
7420                              ' block migrations.') % {'uuid': instance.uuid})
7421                     LOG.error(msg, instance=instance)
7422                     raise exception.MigrationPreCheckError(reason=msg)
7423         elif not (dest_check_data.is_shared_block_storage or
7424                   dest_check_data.is_shared_instance_path):
7425             reason = _("Shared storage live-migration requires either shared "
7426                        "storage or boot-from-volume with no local disks.")
7427             raise exception.InvalidSharedStorage(reason=reason, path=source)
7428 
7429         # NOTE(mikal): include the instance directory name here because it
7430         # doesn't yet exist on the destination but we want to force that
7431         # same name to be used
7432         instance_path = libvirt_utils.get_instance_path(instance,
7433                                                         relative=True)
7434         dest_check_data.instance_relative_path = instance_path
7435 
7436         # NOTE(lyarwood): Used to indicate to the dest that the src is capable
7437         # of wiring up the encrypted disk configuration for the domain.
7438         # Note that this does not require the QEMU and Libvirt versions to
7439         # decrypt LUKS to be installed on the source node. Only the Nova
7440         # utility code to generate the correct XML is required, so we can
7441         # default to True here for all computes >= Queens.
7442         dest_check_data.src_supports_native_luks = True
7443 
7444         return dest_check_data
7445 
7446     def _is_shared_block_storage(self, instance, dest_check_data,
7447                                  block_device_info=None):
7448         """Check if all block storage of an instance can be shared
7449         between source and destination of a live migration.
7450 
7451         Returns true if the instance is volume backed and has no local disks,
7452         or if the image backend is the same on source and destination and the
7453         backend shares block storage between compute nodes.
7454 
7455         :param instance: nova.objects.instance.Instance object
7456         :param dest_check_data: dict with boolean fields image_type,
7457                                 is_shared_instance_path, and is_volume_backed
7458         """
7459         if (dest_check_data.obj_attr_is_set('image_type') and
7460                 CONF.libvirt.images_type == dest_check_data.image_type and
7461                 self.image_backend.backend().is_shared_block_storage()):
7462             # NOTE(dgenin): currently true only for RBD image backend
7463             return True
7464 
7465         if (dest_check_data.is_shared_instance_path and
7466                 self.image_backend.backend().is_file_in_instance_path()):
7467             # NOTE(angdraug): file based image backends (Flat, Qcow2)
7468             # place block device files under the instance path
7469             return True
7470 
7471         if (dest_check_data.is_volume_backed and
7472                 not bool(self._get_instance_disk_info(instance,
7473                                                       block_device_info))):
7474             return True
7475 
7476         return False
7477 
7478     def _assert_dest_node_has_enough_disk(self, context, instance,
7479                                              available_mb, disk_over_commit,
7480                                              block_device_info):
7481         """Checks if destination has enough disk for block migration."""
7482         # Libvirt supports qcow2 disk format,which is usually compressed
7483         # on compute nodes.
7484         # Real disk image (compressed) may enlarged to "virtual disk size",
7485         # that is specified as the maximum disk size.
7486         # (See qemu-img -f path-to-disk)
7487         # Scheduler recognizes destination host still has enough disk space
7488         # if real disk size < available disk size
7489         # if disk_over_commit is True,
7490         #  otherwise virtual disk size < available disk size.
7491 
7492         available = 0
7493         if available_mb:
7494             available = available_mb * units.Mi
7495 
7496         disk_infos = self._get_instance_disk_info(instance, block_device_info)
7497 
7498         necessary = 0
7499         if disk_over_commit:
7500             for info in disk_infos:
7501                 necessary += int(info['disk_size'])
7502         else:
7503             for info in disk_infos:
7504                 necessary += int(info['virt_disk_size'])
7505 
7506         # Check that available disk > necessary disk
7507         if (available - necessary) < 0:
7508             reason = (_('Unable to migrate %(instance_uuid)s: '
7509                         'Disk of instance is too large(available'
7510                         ' on destination host:%(available)s '
7511                         '< need:%(necessary)s)') %
7512                       {'instance_uuid': instance.uuid,
7513                        'available': available,
7514                        'necessary': necessary})
7515             raise exception.MigrationPreCheckError(reason=reason)
7516 
7517     def _compare_cpu(self, guest_cpu, host_cpu_str, instance):
7518         """Check the host is compatible with the requested CPU
7519 
7520         :param guest_cpu: nova.objects.VirtCPUModel or None
7521         :param host_cpu_str: JSON from _get_cpu_info() method
7522 
7523         If the 'guest_cpu' parameter is not None, this will be
7524         validated for migration compatibility with the host.
7525         Otherwise the 'host_cpu_str' JSON string will be used for
7526         validation.
7527 
7528         :returns:
7529             None. if given cpu info is not compatible to this server,
7530             raise exception.
7531         """
7532 
7533         # NOTE(kchamart): Comparing host to guest CPU model for emulated
7534         # guests (<domain type='qemu'>) should not matter -- in this
7535         # mode (QEMU "TCG") the CPU is fully emulated in software and no
7536         # hardware acceleration, like KVM, is involved. So, skip the CPU
7537         # compatibility check for the QEMU domain type, and retain it for
7538         # KVM guests.
7539         if CONF.libvirt.virt_type not in ['kvm']:
7540             return
7541 
7542         if guest_cpu is None:
7543             info = jsonutils.loads(host_cpu_str)
7544             LOG.info('Instance launched has CPU info: %s', host_cpu_str)
7545             cpu = vconfig.LibvirtConfigCPU()
7546             cpu.arch = info['arch']
7547             cpu.model = info['model']
7548             cpu.vendor = info['vendor']
7549             cpu.sockets = info['topology']['sockets']
7550             cpu.cores = info['topology']['cores']
7551             cpu.threads = info['topology']['threads']
7552             for f in info['features']:
7553                 cpu.add_feature(vconfig.LibvirtConfigCPUFeature(f))
7554         else:
7555             cpu = self._vcpu_model_to_cpu_config(guest_cpu)
7556 
7557         u = ("http://libvirt.org/html/libvirt-libvirt-host.html#"
7558              "virCPUCompareResult")
7559         m = _("CPU doesn't have compatibility.\n\n%(ret)s\n\nRefer to %(u)s")
7560         # unknown character exists in xml, then libvirt complains
7561         try:
7562             cpu_xml = cpu.to_xml()
7563             LOG.debug("cpu compare xml: %s", cpu_xml, instance=instance)
7564             ret = self._host.compare_cpu(cpu_xml)
7565         except libvirt.libvirtError as e:
7566             error_code = e.get_error_code()
7567             if error_code == libvirt.VIR_ERR_NO_SUPPORT:
7568                 LOG.debug("URI %(uri)s does not support cpu comparison. "
7569                           "It will be proceeded though. Error: %(error)s",
7570                           {'uri': self._uri(), 'error': e})
7571                 return
7572             else:
7573                 LOG.error(m, {'ret': e, 'u': u})
7574                 raise exception.MigrationPreCheckError(
7575                     reason=m % {'ret': e, 'u': u})
7576 
7577         if ret <= 0:
7578             LOG.error(m, {'ret': ret, 'u': u})
7579             raise exception.InvalidCPUInfo(reason=m % {'ret': ret, 'u': u})
7580 
7581     def _create_shared_storage_test_file(self, instance):
7582         """Makes tmpfile under CONF.instances_path."""
7583         dirpath = CONF.instances_path
7584         fd, tmp_file = tempfile.mkstemp(dir=dirpath)
7585         LOG.debug("Creating tmpfile %s to notify to other "
7586                   "compute nodes that they should mount "
7587                   "the same storage.", tmp_file, instance=instance)
7588         os.close(fd)
7589         return os.path.basename(tmp_file)
7590 
7591     def _check_shared_storage_test_file(self, filename, instance):
7592         """Confirms existence of the tmpfile under CONF.instances_path.
7593 
7594         Cannot confirm tmpfile return False.
7595         """
7596         # NOTE(tpatzig): if instances_path is a shared volume that is
7597         # under heavy IO (many instances on many compute nodes),
7598         # then checking the existence of the testfile fails,
7599         # just because it takes longer until the client refreshes and new
7600         # content gets visible.
7601         # os.utime (like touch) on the directory forces the client to refresh.
7602         os.utime(CONF.instances_path, None)
7603 
7604         tmp_file = os.path.join(CONF.instances_path, filename)
7605         if not os.path.exists(tmp_file):
7606             exists = False
7607         else:
7608             exists = True
7609         LOG.debug('Check if temp file %s exists to indicate shared storage '
7610                   'is being used for migration. Exists? %s', tmp_file, exists,
7611                   instance=instance)
7612         return exists
7613 
7614     def _cleanup_shared_storage_test_file(self, filename):
7615         """Removes existence of the tmpfile under CONF.instances_path."""
7616         tmp_file = os.path.join(CONF.instances_path, filename)
7617         os.remove(tmp_file)
7618 
7619     def ensure_filtering_rules_for_instance(self, instance, network_info):
7620         """Ensure that an instance's filtering rules are enabled.
7621 
7622         When migrating an instance, we need the filtering rules to
7623         be configured on the destination host before starting the
7624         migration.
7625 
7626         Also, when restarting the compute service, we need to ensure
7627         that filtering rules exist for all running services.
7628         """
7629 
7630         self.firewall_driver.setup_basic_filtering(instance, network_info)
7631         self.firewall_driver.prepare_instance_filter(instance,
7632                 network_info)
7633 
7634         # nwfilters may be defined in a separate thread in the case
7635         # of libvirt non-blocking mode, so we wait for completion
7636         timeout_count = list(range(CONF.live_migration_retry_count))
7637         while timeout_count:
7638             if self.firewall_driver.instance_filter_exists(instance,
7639                                                            network_info):
7640                 break
7641             timeout_count.pop()
7642             if len(timeout_count) == 0:
7643                 msg = _('The firewall filter for %s does not exist')
7644                 raise exception.InternalError(msg % instance.name)
7645             greenthread.sleep(1)
7646 
7647     def filter_defer_apply_on(self):
7648         self.firewall_driver.filter_defer_apply_on()
7649 
7650     def filter_defer_apply_off(self):
7651         self.firewall_driver.filter_defer_apply_off()
7652 
7653     def live_migration(self, context, instance, dest,
7654                        post_method, recover_method, block_migration=False,
7655                        migrate_data=None):
7656         """Spawning live_migration operation for distributing high-load.
7657 
7658         :param context: security context
7659         :param instance:
7660             nova.db.sqlalchemy.models.Instance object
7661             instance object that is migrated.
7662         :param dest: destination host
7663         :param post_method:
7664             post operation method.
7665             expected nova.compute.manager._post_live_migration.
7666         :param recover_method:
7667             recovery method when any exception occurs.
7668             expected nova.compute.manager._rollback_live_migration.
7669         :param block_migration: if true, do block migration.
7670         :param migrate_data: a LibvirtLiveMigrateData object
7671 
7672         """
7673 
7674         # 'dest' will be substituted into 'migration_uri' so ensure
7675         # it does't contain any characters that could be used to
7676         # exploit the URI accepted by libvirt
7677         if not libvirt_utils.is_valid_hostname(dest):
7678             raise exception.InvalidHostname(hostname=dest)
7679 
7680         self._live_migration(context, instance, dest,
7681                              post_method, recover_method, block_migration,
7682                              migrate_data)
7683 
7684     def live_migration_abort(self, instance):
7685         """Aborting a running live-migration.
7686 
7687         :param instance: instance object that is in migration
7688 
7689         """
7690 
7691         guest = self._host.get_guest(instance)
7692         dom = guest._domain
7693 
7694         try:
7695             dom.abortJob()
7696         except libvirt.libvirtError as e:
7697             LOG.error("Failed to cancel migration %s",
7698                     encodeutils.exception_to_unicode(e), instance=instance)
7699             raise
7700 
7701     def _verify_serial_console_is_disabled(self):
7702         if CONF.serial_console.enabled:
7703 
7704             msg = _('Your destination node does not support'
7705                     ' retrieving listen addresses. In order'
7706                     ' for live migration to work properly you'
7707                     ' must disable serial console.')
7708             raise exception.MigrationError(reason=msg)
7709 
7710     def _detach_direct_passthrough_vifs(self, context,
7711                                         migrate_data, instance):
7712         """detaches passthrough vif to enable live migration
7713 
7714         :param context: security context
7715         :param migrate_data: a LibvirtLiveMigrateData object
7716         :param instance: instance object that is migrated.
7717         """
7718         # NOTE(sean-k-mooney): if we have vif data available we
7719         # loop over each vif and detach all direct passthrough
7720         # vifs to allow sriov live migration.
7721         direct_vnics = network_model.VNIC_TYPES_DIRECT_PASSTHROUGH
7722         vifs = [vif.source_vif for vif in migrate_data.vifs
7723                 if "source_vif" in vif and vif.source_vif]
7724         for vif in vifs:
7725             if vif['vnic_type'] in direct_vnics:
7726                 LOG.info("Detaching vif %s from instnace "
7727                          "%s for live migration", vif['id'], instance.id)
7728                 self.detach_interface(context, instance, vif)
7729 
7730     def _live_migration_operation(self, context, instance, dest,
7731                                   block_migration, migrate_data, guest,
7732                                   device_names):
7733         """Invoke the live migration operation
7734 
7735         :param context: security context
7736         :param instance:
7737             nova.db.sqlalchemy.models.Instance object
7738             instance object that is migrated.
7739         :param dest: destination host
7740         :param block_migration: if true, do block migration.
7741         :param migrate_data: a LibvirtLiveMigrateData object
7742         :param guest: the guest domain object
7743         :param device_names: list of device names that are being migrated with
7744             instance
7745 
7746         This method is intended to be run in a background thread and will
7747         block that thread until the migration is finished or failed.
7748         """
7749         try:
7750             if migrate_data.block_migration:
7751                 migration_flags = self._block_migration_flags
7752             else:
7753                 migration_flags = self._live_migration_flags
7754 
7755             serial_listen_addr = libvirt_migrate.serial_listen_addr(
7756                 migrate_data)
7757             if not serial_listen_addr:
7758                 # In this context we want to ensure that serial console is
7759                 # disabled on source node. This is because nova couldn't
7760                 # retrieve serial listen address from destination node, so we
7761                 # consider that destination node might have serial console
7762                 # disabled as well.
7763                 self._verify_serial_console_is_disabled()
7764 
7765             # NOTE(aplanas) migrate_uri will have a value only in the
7766             # case that `live_migration_inbound_addr` parameter is
7767             # set, and we propose a non tunneled migration.
7768             migrate_uri = None
7769             if ('target_connect_addr' in migrate_data and
7770                     migrate_data.target_connect_addr is not None):
7771                 dest = migrate_data.target_connect_addr
7772                 if (migration_flags &
7773                     libvirt.VIR_MIGRATE_TUNNELLED == 0):
7774                     migrate_uri = self._migrate_uri(dest)
7775 
7776             new_xml_str = None
7777             if CONF.libvirt.virt_type != "parallels":
7778                 # If the migrate_data has port binding information for the
7779                 # destination host, we need to prepare the guest vif config
7780                 # for the destination before we start migrating the guest.
7781                 get_vif_config = None
7782                 if 'vifs' in migrate_data and migrate_data.vifs:
7783                     # NOTE(mriedem): The vif kwarg must be built on the fly
7784                     # within get_updated_guest_xml based on migrate_data.vifs.
7785                     # We could stash the virt_type from the destination host
7786                     # into LibvirtLiveMigrateData but the host kwarg is a
7787                     # nova.virt.libvirt.host.Host object and is used to check
7788                     # information like libvirt version on the destination.
7789                     # If this becomes a problem, what we could do is get the
7790                     # VIF configs while on the destination host during
7791                     # pre_live_migration() and store those in the
7792                     # LibvirtLiveMigrateData object. For now we just use the
7793                     # source host information for virt_type and
7794                     # host (version) since the conductor live_migrate method
7795                     # _check_compatible_with_source_hypervisor() ensures that
7796                     # the hypervisor types and versions are compatible.
7797                     get_vif_config = functools.partial(
7798                         self.vif_driver.get_config,
7799                         instance=instance,
7800                         image_meta=instance.image_meta,
7801                         inst_type=instance.flavor,
7802                         virt_type=CONF.libvirt.virt_type,
7803                         host=self._host)
7804                     self._detach_direct_passthrough_vifs(context,
7805                         migrate_data, instance)
7806                 new_xml_str = libvirt_migrate.get_updated_guest_xml(
7807                     # TODO(sahid): It's not a really good idea to pass
7808                     # the method _get_volume_config and we should to find
7809                     # a way to avoid this in future.
7810                     guest, migrate_data, self._get_volume_config,
7811                     get_vif_config=get_vif_config)
7812 
7813             # NOTE(pkoniszewski): Because of precheck which blocks
7814             # tunnelled block live migration with mapped volumes we
7815             # can safely remove migrate_disks when tunnelling is on.
7816             # Otherwise we will block all tunnelled block migrations,
7817             # even when an instance does not have volumes mapped.
7818             # This is because selective disk migration is not
7819             # supported in tunnelled block live migration. Also we
7820             # cannot fallback to migrateToURI2 in this case because of
7821             # bug #1398999
7822             #
7823             # TODO(kchamart) Move the following bit to guest.migrate()
7824             if (migration_flags & libvirt.VIR_MIGRATE_TUNNELLED != 0):
7825                 device_names = []
7826 
7827             # TODO(sahid): This should be in
7828             # post_live_migration_at_source but no way to retrieve
7829             # ports acquired on the host for the guest at this
7830             # step. Since the domain is going to be removed from
7831             # libvird on source host after migration, we backup the
7832             # serial ports to release them if all went well.
7833             serial_ports = []
7834             if CONF.serial_console.enabled:
7835                 serial_ports = list(self._get_serial_ports_from_guest(guest))
7836 
7837             LOG.debug("About to invoke the migrate API", instance=instance)
7838             guest.migrate(self._live_migration_uri(dest),
7839                           migrate_uri=migrate_uri,
7840                           flags=migration_flags,
7841                           migrate_disks=device_names,
7842                           destination_xml=new_xml_str,
7843                           bandwidth=CONF.libvirt.live_migration_bandwidth)
7844             LOG.debug("Migrate API has completed", instance=instance)
7845 
7846             for hostname, port in serial_ports:
7847                 serial_console.release_port(host=hostname, port=port)
7848         except Exception as e:
7849             with excutils.save_and_reraise_exception():
7850                 LOG.error("Live Migration failure: %s", e, instance=instance)
7851 
7852         # If 'migrateToURI' fails we don't know what state the
7853         # VM instances on each host are in. Possibilities include
7854         #
7855         #  1. src==running, dst==none
7856         #
7857         #     Migration failed & rolled back, or never started
7858         #
7859         #  2. src==running, dst==paused
7860         #
7861         #     Migration started but is still ongoing
7862         #
7863         #  3. src==paused,  dst==paused
7864         #
7865         #     Migration data transfer completed, but switchover
7866         #     is still ongoing, or failed
7867         #
7868         #  4. src==paused,  dst==running
7869         #
7870         #     Migration data transfer completed, switchover
7871         #     happened but cleanup on source failed
7872         #
7873         #  5. src==none,    dst==running
7874         #
7875         #     Migration fully succeeded.
7876         #
7877         # Libvirt will aim to complete any migration operation
7878         # or roll it back. So even if the migrateToURI call has
7879         # returned an error, if the migration was not finished
7880         # libvirt should clean up.
7881         #
7882         # So we take the error raise here with a pinch of salt
7883         # and rely on the domain job info status to figure out
7884         # what really happened to the VM, which is a much more
7885         # reliable indicator.
7886         #
7887         # In particular we need to try very hard to ensure that
7888         # Nova does not "forget" about the guest. ie leaving it
7889         # running on a different host to the one recorded in
7890         # the database, as that would be a serious resource leak
7891 
7892         LOG.debug("Migration operation thread has finished",
7893                   instance=instance)
7894 
7895     def _live_migration_copy_disk_paths(self, context, instance, guest):
7896         '''Get list of disks to copy during migration
7897 
7898         :param context: security context
7899         :param instance: the instance being migrated
7900         :param guest: the Guest instance being migrated
7901 
7902         Get the list of disks to copy during migration.
7903 
7904         :returns: a list of local source paths and a list of device names to
7905             copy
7906         '''
7907 
7908         disk_paths = []
7909         device_names = []
7910         block_devices = []
7911 
7912         if (self._block_migration_flags &
7913                 libvirt.VIR_MIGRATE_TUNNELLED == 0):
7914             bdm_list = objects.BlockDeviceMappingList.get_by_instance_uuid(
7915                 context, instance.uuid)
7916             block_device_info = driver.get_block_device_info(instance,
7917                                                              bdm_list)
7918 
7919             block_device_mappings = driver.block_device_info_get_mapping(
7920                 block_device_info)
7921             for bdm in block_device_mappings:
7922                 device_name = str(bdm['mount_device'].rsplit('/', 1)[1])
7923                 block_devices.append(device_name)
7924 
7925         for dev in guest.get_all_disks():
7926             if dev.readonly or dev.shareable:
7927                 continue
7928             if dev.source_type not in ["file", "block"]:
7929                 continue
7930             if dev.target_dev in block_devices:
7931                 continue
7932             disk_paths.append(dev.source_path)
7933             device_names.append(dev.target_dev)
7934         return (disk_paths, device_names)
7935 
7936     def _live_migration_data_gb(self, instance, disk_paths):
7937         '''Calculate total amount of data to be transferred
7938 
7939         :param instance: the nova.objects.Instance being migrated
7940         :param disk_paths: list of disk paths that are being migrated
7941         with instance
7942 
7943         Calculates the total amount of data that needs to be
7944         transferred during the live migration. The actual
7945         amount copied will be larger than this, due to the
7946         guest OS continuing to dirty RAM while the migration
7947         is taking place. So this value represents the minimal
7948         data size possible.
7949 
7950         :returns: data size to be copied in GB
7951         '''
7952 
7953         ram_gb = instance.flavor.memory_mb * units.Mi / units.Gi
7954         if ram_gb < 2:
7955             ram_gb = 2
7956 
7957         disk_gb = 0
7958         for path in disk_paths:
7959             try:
7960                 size = os.stat(path).st_size
7961                 size_gb = (size / units.Gi)
7962                 if size_gb < 2:
7963                     size_gb = 2
7964                 disk_gb += size_gb
7965             except OSError as e:
7966                 LOG.warning("Unable to stat %(disk)s: %(ex)s",
7967                             {'disk': path, 'ex': e})
7968                 # Ignore error since we don't want to break
7969                 # the migration monitoring thread operation
7970 
7971         return ram_gb + disk_gb
7972 
7973     def _get_migration_flags(self, is_block_migration):
7974         if is_block_migration:
7975             return self._block_migration_flags
7976         return self._live_migration_flags
7977 
7978     def _live_migration_monitor(self, context, instance, guest,
7979                                 dest, post_method,
7980                                 recover_method, block_migration,
7981                                 migrate_data, finish_event,
7982                                 disk_paths):
7983         on_migration_failure = deque()
7984         data_gb = self._live_migration_data_gb(instance, disk_paths)
7985         downtime_steps = list(libvirt_migrate.downtime_steps(data_gb))
7986         migration = migrate_data.migration
7987         curdowntime = None
7988 
7989         migration_flags = self._get_migration_flags(
7990                                   migrate_data.block_migration)
7991 
7992         n = 0
7993         start = time.time()
7994         is_post_copy_enabled = self._is_post_copy_enabled(migration_flags)
7995         while True:
7996             info = guest.get_job_info()
7997 
7998             if info.type == libvirt.VIR_DOMAIN_JOB_NONE:
7999                 # Either still running, or failed or completed,
8000                 # lets untangle the mess
8001                 if not finish_event.ready():
8002                     LOG.debug("Operation thread is still running",
8003                               instance=instance)
8004                 else:
8005                     info.type = libvirt_migrate.find_job_type(guest, instance)
8006                     LOG.debug("Fixed incorrect job type to be %d",
8007                               info.type, instance=instance)
8008 
8009             if info.type == libvirt.VIR_DOMAIN_JOB_NONE:
8010                 # Migration is not yet started
8011                 LOG.debug("Migration not running yet",
8012                           instance=instance)
8013             elif info.type == libvirt.VIR_DOMAIN_JOB_UNBOUNDED:
8014                 # Migration is still running
8015                 #
8016                 # This is where we wire up calls to change live
8017                 # migration status. eg change max downtime, cancel
8018                 # the operation, change max bandwidth
8019                 libvirt_migrate.run_tasks(guest, instance,
8020                                           self.active_migrations,
8021                                           on_migration_failure,
8022                                           migration,
8023                                           is_post_copy_enabled)
8024 
8025                 now = time.time()
8026                 elapsed = now - start
8027 
8028                 completion_timeout = int(
8029                     CONF.libvirt.live_migration_completion_timeout * data_gb)
8030                 # NOTE(yikun): Check the completion timeout to determine
8031                 # should trigger the timeout action, and there are two choices
8032                 # ``abort`` (default) or ``force_complete``. If the action is
8033                 # set to ``force_complete``, the post-copy will be triggered
8034                 # if available else the VM will be suspended, otherwise the
8035                 # live migrate operation will be aborted.
8036                 if libvirt_migrate.should_trigger_timeout_action(
8037                         instance, elapsed, completion_timeout,
8038                         migration.status):
8039                     timeout_act = CONF.libvirt.live_migration_timeout_action
8040                     if timeout_act == 'force_complete':
8041                         self.live_migration_force_complete(instance)
8042                     else:
8043                         # timeout action is 'abort'
8044                         try:
8045                             guest.abort_job()
8046                         except libvirt.libvirtError as e:
8047                             LOG.warning("Failed to abort migration %s",
8048                                     encodeutils.exception_to_unicode(e),
8049                                     instance=instance)
8050                             self._clear_empty_migration(instance)
8051                             raise
8052 
8053                 curdowntime = libvirt_migrate.update_downtime(
8054                     guest, instance, curdowntime,
8055                     downtime_steps, elapsed)
8056 
8057                 # We loop every 500ms, so don't log on every
8058                 # iteration to avoid spamming logs for long
8059                 # running migrations. Just once every 5 secs
8060                 # is sufficient for developers to debug problems.
8061                 # We log once every 30 seconds at info to help
8062                 # admins see slow running migration operations
8063                 # when debug logs are off.
8064                 if (n % 10) == 0:
8065                     # Ignoring memory_processed, as due to repeated
8066                     # dirtying of data, this can be way larger than
8067                     # memory_total. Best to just look at what's
8068                     # remaining to copy and ignore what's done already
8069                     #
8070                     # TODO(berrange) perhaps we could include disk
8071                     # transfer stats in the progress too, but it
8072                     # might make memory info more obscure as large
8073                     # disk sizes might dwarf memory size
8074                     remaining = 100
8075                     if info.memory_total != 0:
8076                         remaining = round(info.memory_remaining *
8077                                           100 / info.memory_total)
8078 
8079                     libvirt_migrate.save_stats(instance, migration,
8080                                                info, remaining)
8081 
8082                     lg = LOG.debug
8083                     if (n % 60) == 0:
8084                         lg = LOG.info
8085 
8086                     lg("Migration running for %(secs)d secs, "
8087                        "memory %(remaining)d%% remaining; "
8088                        "(bytes processed=%(processed_memory)d, "
8089                        "remaining=%(remaining_memory)d, "
8090                        "total=%(total_memory)d)",
8091                        {"secs": n / 2, "remaining": remaining,
8092                         "processed_memory": info.memory_processed,
8093                         "remaining_memory": info.memory_remaining,
8094                         "total_memory": info.memory_total}, instance=instance)
8095 
8096                 n = n + 1
8097             elif info.type == libvirt.VIR_DOMAIN_JOB_COMPLETED:
8098                 # Migration is all done
8099                 LOG.info("Migration operation has completed",
8100                          instance=instance)
8101                 post_method(context, instance, dest, block_migration,
8102                             migrate_data)
8103                 break
8104             elif info.type == libvirt.VIR_DOMAIN_JOB_FAILED:
8105                 # Migration did not succeed
8106                 LOG.error("Migration operation has aborted", instance=instance)
8107                 libvirt_migrate.run_recover_tasks(self._host, guest, instance,
8108                                                   on_migration_failure)
8109                 recover_method(context, instance, dest, migrate_data)
8110                 break
8111             elif info.type == libvirt.VIR_DOMAIN_JOB_CANCELLED:
8112                 # Migration was stopped by admin
8113                 LOG.warning("Migration operation was cancelled",
8114                             instance=instance)
8115                 libvirt_migrate.run_recover_tasks(self._host, guest, instance,
8116                                                   on_migration_failure)
8117                 recover_method(context, instance, dest, migrate_data,
8118                                migration_status='cancelled')
8119                 break
8120             else:
8121                 LOG.warning("Unexpected migration job type: %d",
8122                             info.type, instance=instance)
8123 
8124             time.sleep(0.5)
8125         self._clear_empty_migration(instance)
8126 
8127     def _clear_empty_migration(self, instance):
8128         try:
8129             del self.active_migrations[instance.uuid]
8130         except KeyError:
8131             LOG.warning("There are no records in active migrations "
8132                         "for instance", instance=instance)
8133 
8134     def _live_migration(self, context, instance, dest, post_method,
8135                         recover_method, block_migration,
8136                         migrate_data):
8137         """Do live migration.
8138 
8139         :param context: security context
8140         :param instance:
8141             nova.db.sqlalchemy.models.Instance object
8142             instance object that is migrated.
8143         :param dest: destination host
8144         :param post_method:
8145             post operation method.
8146             expected nova.compute.manager._post_live_migration.
8147         :param recover_method:
8148             recovery method when any exception occurs.
8149             expected nova.compute.manager._rollback_live_migration.
8150         :param block_migration: if true, do block migration.
8151         :param migrate_data: a LibvirtLiveMigrateData object
8152 
8153         This fires off a new thread to run the blocking migration
8154         operation, and then this thread monitors the progress of
8155         migration and controls its operation
8156         """
8157 
8158         guest = self._host.get_guest(instance)
8159 
8160         disk_paths = []
8161         device_names = []
8162         if (migrate_data.block_migration and
8163                 CONF.libvirt.virt_type != "parallels"):
8164             disk_paths, device_names = self._live_migration_copy_disk_paths(
8165                 context, instance, guest)
8166 
8167         opthread = utils.spawn(self._live_migration_operation,
8168                                      context, instance, dest,
8169                                      block_migration,
8170                                      migrate_data, guest,
8171                                      device_names)
8172 
8173         finish_event = eventlet.event.Event()
8174         self.active_migrations[instance.uuid] = deque()
8175 
8176         def thread_finished(thread, event):
8177             LOG.debug("Migration operation thread notification",
8178                       instance=instance)
8179             event.send()
8180         opthread.link(thread_finished, finish_event)
8181 
8182         # Let eventlet schedule the new thread right away
8183         time.sleep(0)
8184 
8185         try:
8186             LOG.debug("Starting monitoring of live migration",
8187                       instance=instance)
8188             self._live_migration_monitor(context, instance, guest, dest,
8189                                          post_method, recover_method,
8190                                          block_migration, migrate_data,
8191                                          finish_event, disk_paths)
8192         except Exception as ex:
8193             LOG.warning("Error monitoring migration: %(ex)s",
8194                         {"ex": ex}, instance=instance, exc_info=True)
8195             raise
8196         finally:
8197             LOG.debug("Live migration monitoring is all done",
8198                       instance=instance)
8199 
8200     def _is_post_copy_enabled(self, migration_flags):
8201         return (migration_flags & libvirt.VIR_MIGRATE_POSTCOPY) != 0
8202 
8203     def live_migration_force_complete(self, instance):
8204         try:
8205             self.active_migrations[instance.uuid].append('force-complete')
8206         except KeyError:
8207             raise exception.NoActiveMigrationForInstance(
8208                 instance_id=instance.uuid)
8209 
8210     def _try_fetch_image(self, context, path, image_id, instance,
8211                          fallback_from_host=None):
8212         try:
8213             libvirt_utils.fetch_image(context, path, image_id,
8214                                       instance.trusted_certs)
8215         except exception.ImageNotFound:
8216             if not fallback_from_host:
8217                 raise
8218             LOG.debug("Image %(image_id)s doesn't exist anymore on "
8219                       "image service, attempting to copy image "
8220                       "from %(host)s",
8221                       {'image_id': image_id, 'host': fallback_from_host})
8222             libvirt_utils.copy_image(src=path, dest=path,
8223                                      host=fallback_from_host,
8224                                      receive=True)
8225 
8226     def _fetch_instance_kernel_ramdisk(self, context, instance,
8227                                        fallback_from_host=None):
8228         """Download kernel and ramdisk for instance in instance directory."""
8229         instance_dir = libvirt_utils.get_instance_path(instance)
8230         if instance.kernel_id:
8231             kernel_path = os.path.join(instance_dir, 'kernel')
8232             # NOTE(dsanders): only fetch image if it's not available at
8233             # kernel_path. This also avoids ImageNotFound exception if
8234             # the image has been deleted from glance
8235             if not os.path.exists(kernel_path):
8236                 self._try_fetch_image(context,
8237                                       kernel_path,
8238                                       instance.kernel_id,
8239                                       instance, fallback_from_host)
8240             if instance.ramdisk_id:
8241                 ramdisk_path = os.path.join(instance_dir, 'ramdisk')
8242                 # NOTE(dsanders): only fetch image if it's not available at
8243                 # ramdisk_path. This also avoids ImageNotFound exception if
8244                 # the image has been deleted from glance
8245                 if not os.path.exists(ramdisk_path):
8246                     self._try_fetch_image(context,
8247                                           ramdisk_path,
8248                                           instance.ramdisk_id,
8249                                           instance, fallback_from_host)
8250 
8251     def _reattach_instance_vifs(self, context, instance, network_info):
8252         guest = self._host.get_guest(instance)
8253         # validate that the guest has the expected number of interfaces
8254         # attached.
8255         guest_interfaces = guest.get_interfaces()
8256         # NOTE(sean-k-mooney): In general len(guest_interfaces) will
8257         # be equal to len(network_info) as interfaces will not be hot unplugged
8258         # unless they are SR-IOV direct mode interfaces. As such we do not
8259         # need an else block here as it would be a noop.
8260         if len(guest_interfaces) < len(network_info):
8261             # NOTE(sean-k-mooney): we are doing a post live migration
8262             # for a guest with sriov vif that were detached as part of
8263             # the migration. loop over the vifs and attach the missing
8264             # vif as part of the post live migration phase.
8265             direct_vnics = network_model.VNIC_TYPES_DIRECT_PASSTHROUGH
8266             for vif in network_info:
8267                 if vif['vnic_type'] in direct_vnics:
8268                     LOG.info("Attaching vif %s to instance %s",
8269                              vif['id'], instance.id)
8270                     self.attach_interface(context, instance,
8271                                           instance.image_meta, vif)
8272 
8273     def rollback_live_migration_at_source(self, context, instance,
8274                                           migrate_data):
8275         """reconnect sriov interfaces after failed live migration
8276         :param context: security context
8277         :param instance:  the instance being migrated
8278         :param migrate_date: a LibvirtLiveMigrateData object
8279         """
8280         network_info = network_model.NetworkInfo(
8281             [vif.source_vif for vif in migrate_data.vifs
8282                             if "source_vif" in vif and vif.source_vif])
8283         self._reattach_instance_vifs(context, instance, network_info)
8284 
8285     def rollback_live_migration_at_destination(self, context, instance,
8286                                                network_info,
8287                                                block_device_info,
8288                                                destroy_disks=True,
8289                                                migrate_data=None):
8290         """Clean up destination node after a failed live migration."""
8291         try:
8292             self.destroy(context, instance, network_info, block_device_info,
8293                          destroy_disks)
8294         finally:
8295             # NOTE(gcb): Failed block live migration may leave instance
8296             # directory at destination node, ensure it is always deleted.
8297             is_shared_instance_path = True
8298             if migrate_data:
8299                 is_shared_instance_path = migrate_data.is_shared_instance_path
8300                 if (migrate_data.obj_attr_is_set("serial_listen_ports")
8301                     and migrate_data.serial_listen_ports):
8302                     # Releases serial ports reserved.
8303                     for port in migrate_data.serial_listen_ports:
8304                         serial_console.release_port(
8305                             host=migrate_data.serial_listen_addr, port=port)
8306 
8307             if not is_shared_instance_path:
8308                 instance_dir = libvirt_utils.get_instance_path_at_destination(
8309                     instance, migrate_data)
8310                 if os.path.exists(instance_dir):
8311                     shutil.rmtree(instance_dir)
8312 
8313     def _pre_live_migration_plug_vifs(self, instance, network_info,
8314                                       migrate_data):
8315         if 'vifs' in migrate_data and migrate_data.vifs:
8316             LOG.debug('Plugging VIFs using destination host port bindings '
8317                       'before live migration.', instance=instance)
8318             vif_plug_nw_info = network_model.NetworkInfo([])
8319             for migrate_vif in migrate_data.vifs:
8320                 vif_plug_nw_info.append(migrate_vif.get_dest_vif())
8321         else:
8322             LOG.debug('Plugging VIFs before live migration.',
8323                       instance=instance)
8324             vif_plug_nw_info = network_info
8325         # Retry operation is necessary because continuous live migration
8326         # requests to the same host cause concurrent requests to iptables,
8327         # then it complains.
8328         max_retry = CONF.live_migration_retry_count
8329         for cnt in range(max_retry):
8330             try:
8331                 self.plug_vifs(instance, vif_plug_nw_info)
8332                 break
8333             except processutils.ProcessExecutionError:
8334                 if cnt == max_retry - 1:
8335                     raise
8336                 else:
8337                     LOG.warning('plug_vifs() failed %(cnt)d. Retry up to '
8338                                 '%(max_retry)d.',
8339                                 {'cnt': cnt, 'max_retry': max_retry},
8340                                 instance=instance)
8341                     greenthread.sleep(1)
8342 
8343     def pre_live_migration(self, context, instance, block_device_info,
8344                            network_info, disk_info, migrate_data):
8345         """Preparation live migration."""
8346         if disk_info is not None:
8347             disk_info = jsonutils.loads(disk_info)
8348 
8349         LOG.debug('migrate_data in pre_live_migration: %s', migrate_data,
8350                   instance=instance)
8351         is_shared_block_storage = migrate_data.is_shared_block_storage
8352         is_shared_instance_path = migrate_data.is_shared_instance_path
8353         is_block_migration = migrate_data.block_migration
8354 
8355         if not is_shared_instance_path:
8356             instance_dir = libvirt_utils.get_instance_path_at_destination(
8357                             instance, migrate_data)
8358 
8359             if os.path.exists(instance_dir):
8360                 raise exception.DestinationDiskExists(path=instance_dir)
8361 
8362             LOG.debug('Creating instance directory: %s', instance_dir,
8363                       instance=instance)
8364             os.mkdir(instance_dir)
8365 
8366             # Recreate the disk.info file and in doing so stop the
8367             # imagebackend from recreating it incorrectly by inspecting the
8368             # contents of each file when using the Raw backend.
8369             if disk_info:
8370                 image_disk_info = {}
8371                 for info in disk_info:
8372                     image_file = os.path.basename(info['path'])
8373                     image_path = os.path.join(instance_dir, image_file)
8374                     image_disk_info[image_path] = info['type']
8375 
8376                 LOG.debug('Creating disk.info with the contents: %s',
8377                           image_disk_info, instance=instance)
8378 
8379                 image_disk_info_path = os.path.join(instance_dir,
8380                                                     'disk.info')
8381                 libvirt_utils.write_to_file(image_disk_info_path,
8382                                             jsonutils.dumps(image_disk_info))
8383 
8384             if not is_shared_block_storage:
8385                 # Ensure images and backing files are present.
8386                 LOG.debug('Checking to make sure images and backing files are '
8387                           'present before live migration.', instance=instance)
8388                 self._create_images_and_backing(
8389                     context, instance, instance_dir, disk_info,
8390                     fallback_from_host=instance.host)
8391                 if (configdrive.required_by(instance) and
8392                         CONF.config_drive_format == 'iso9660'):
8393                     # NOTE(pkoniszewski): Due to a bug in libvirt iso config
8394                     # drive needs to be copied to destination prior to
8395                     # migration when instance path is not shared and block
8396                     # storage is not shared. Files that are already present
8397                     # on destination are excluded from a list of files that
8398                     # need to be copied to destination. If we don't do that
8399                     # live migration will fail on copying iso config drive to
8400                     # destination and writing to read-only device.
8401                     # Please see bug/1246201 for more details.
8402                     src = "%s:%s/disk.config" % (instance.host, instance_dir)
8403                     self._remotefs.copy_file(src, instance_dir)
8404 
8405             if not is_block_migration:
8406                 # NOTE(angdraug): when block storage is shared between source
8407                 # and destination and instance path isn't (e.g. volume backed
8408                 # or rbd backed instance), instance path on destination has to
8409                 # be prepared
8410 
8411                 # Required by Quobyte CI
8412                 self._ensure_console_log_for_instance(instance)
8413 
8414                 # if image has kernel and ramdisk, just download
8415                 # following normal way.
8416                 self._fetch_instance_kernel_ramdisk(context, instance)
8417 
8418         # Establishing connection to volume server.
8419         block_device_mapping = driver.block_device_info_get_mapping(
8420             block_device_info)
8421 
8422         if len(block_device_mapping):
8423             LOG.debug('Connecting volumes before live migration.',
8424                       instance=instance)
8425 
8426         for bdm in block_device_mapping:
8427             connection_info = bdm['connection_info']
8428             # NOTE(lyarwood): Handle the P to Q LM during upgrade use case
8429             # where an instance has encrypted volumes attached using the
8430             # os-brick encryptors. Do not attempt to attach the encrypted
8431             # volume using native LUKS decryption on the destionation.
8432             src_native_luks = False
8433             if migrate_data.obj_attr_is_set('src_supports_native_luks'):
8434                 src_native_luks = migrate_data.src_supports_native_luks
8435             dest_native_luks = self._is_native_luks_available()
8436             allow_native_luks = src_native_luks and dest_native_luks
8437             self._connect_volume(context, connection_info, instance,
8438                                  allow_native_luks=allow_native_luks)
8439 
8440         self._pre_live_migration_plug_vifs(
8441             instance, network_info, migrate_data)
8442 
8443         # Store server_listen and latest disk device info
8444         if not migrate_data:
8445             migrate_data = objects.LibvirtLiveMigrateData(bdms=[])
8446         else:
8447             migrate_data.bdms = []
8448         # Store live_migration_inbound_addr
8449         migrate_data.target_connect_addr = \
8450             CONF.libvirt.live_migration_inbound_addr
8451         migrate_data.supported_perf_events = self._supported_perf_events
8452 
8453         migrate_data.serial_listen_ports = []
8454         if CONF.serial_console.enabled:
8455             num_ports = hardware.get_number_of_serial_ports(
8456                 instance.flavor, instance.image_meta)
8457             for port in six.moves.range(num_ports):
8458                 migrate_data.serial_listen_ports.append(
8459                     serial_console.acquire_port(
8460                         migrate_data.serial_listen_addr))
8461 
8462         for vol in block_device_mapping:
8463             connection_info = vol['connection_info']
8464             if connection_info.get('serial'):
8465                 disk_info = blockinfo.get_info_from_bdm(
8466                     instance, CONF.libvirt.virt_type,
8467                     instance.image_meta, vol)
8468 
8469                 bdmi = objects.LibvirtLiveMigrateBDMInfo()
8470                 bdmi.serial = connection_info['serial']
8471                 bdmi.connection_info = connection_info
8472                 bdmi.bus = disk_info['bus']
8473                 bdmi.dev = disk_info['dev']
8474                 bdmi.type = disk_info['type']
8475                 bdmi.format = disk_info.get('format')
8476                 bdmi.boot_index = disk_info.get('boot_index')
8477                 volume_secret = self._host.find_secret('volume', vol.volume_id)
8478                 if volume_secret:
8479                     bdmi.encryption_secret_uuid = volume_secret.UUIDString()
8480 
8481                 migrate_data.bdms.append(bdmi)
8482 
8483         return migrate_data
8484 
8485     def _try_fetch_image_cache(self, image, fetch_func, context, filename,
8486                                image_id, instance, size,
8487                                fallback_from_host=None):
8488         try:
8489             image.cache(fetch_func=fetch_func,
8490                         context=context,
8491                         filename=filename,
8492                         image_id=image_id,
8493                         size=size,
8494                         trusted_certs=instance.trusted_certs)
8495         except exception.ImageNotFound:
8496             if not fallback_from_host:
8497                 raise
8498             LOG.debug("Image %(image_id)s doesn't exist anymore "
8499                       "on image service, attempting to copy "
8500                       "image from %(host)s",
8501                       {'image_id': image_id, 'host': fallback_from_host},
8502                       instance=instance)
8503 
8504             def copy_from_host(target):
8505                 libvirt_utils.copy_image(src=target,
8506                                          dest=target,
8507                                          host=fallback_from_host,
8508                                          receive=True)
8509             image.cache(fetch_func=copy_from_host, size=size,
8510                         filename=filename)
8511 
8512     def _create_images_and_backing(self, context, instance, instance_dir,
8513                                    disk_info, fallback_from_host=None):
8514         """:param context: security context
8515            :param instance:
8516                nova.db.sqlalchemy.models.Instance object
8517                instance object that is migrated.
8518            :param instance_dir:
8519                instance path to use, calculated externally to handle block
8520                migrating an instance with an old style instance path
8521            :param disk_info:
8522                disk info specified in _get_instance_disk_info_from_config
8523                (list of dicts)
8524            :param fallback_from_host:
8525                host where we can retrieve images if the glance images are
8526                not available.
8527 
8528         """
8529 
8530         # Virtuozzo containers don't use backing file
8531         if (CONF.libvirt.virt_type == "parallels" and
8532                 instance.vm_mode == fields.VMMode.EXE):
8533             return
8534 
8535         if not disk_info:
8536             disk_info = []
8537 
8538         for info in disk_info:
8539             base = os.path.basename(info['path'])
8540             # Get image type and create empty disk image, and
8541             # create backing file in case of qcow2.
8542             instance_disk = os.path.join(instance_dir, base)
8543             if not info['backing_file'] and not os.path.exists(instance_disk):
8544                 libvirt_utils.create_image(info['type'], instance_disk,
8545                                            info['virt_disk_size'])
8546             elif info['backing_file']:
8547                 # Creating backing file follows same way as spawning instances.
8548                 cache_name = os.path.basename(info['backing_file'])
8549 
8550                 disk = self.image_backend.by_name(instance, instance_disk,
8551                                                   CONF.libvirt.images_type)
8552                 if cache_name.startswith('ephemeral'):
8553                     # The argument 'size' is used by image.cache to
8554                     # validate disk size retrieved from cache against
8555                     # the instance disk size (should always return OK)
8556                     # and ephemeral_size is used by _create_ephemeral
8557                     # to build the image if the disk is not already
8558                     # cached.
8559                     disk.cache(
8560                         fetch_func=self._create_ephemeral,
8561                         fs_label=cache_name,
8562                         os_type=instance.os_type,
8563                         filename=cache_name,
8564                         size=info['virt_disk_size'],
8565                         ephemeral_size=info['virt_disk_size'] / units.Gi)
8566                 elif cache_name.startswith('swap'):
8567                     inst_type = instance.get_flavor()
8568                     swap_mb = inst_type.swap
8569                     disk.cache(fetch_func=self._create_swap,
8570                                 filename="swap_%s" % swap_mb,
8571                                 size=swap_mb * units.Mi,
8572                                 swap_mb=swap_mb)
8573                 else:
8574                     self._try_fetch_image_cache(disk,
8575                                                 libvirt_utils.fetch_image,
8576                                                 context, cache_name,
8577                                                 instance.image_ref,
8578                                                 instance,
8579                                                 info['virt_disk_size'],
8580                                                 fallback_from_host)
8581 
8582         # if disk has kernel and ramdisk, just download
8583         # following normal way.
8584         self._fetch_instance_kernel_ramdisk(
8585             context, instance, fallback_from_host=fallback_from_host)
8586 
8587     def post_live_migration(self, context, instance, block_device_info,
8588                             migrate_data=None):
8589         # Disconnect from volume server
8590         block_device_mapping = driver.block_device_info_get_mapping(
8591                 block_device_info)
8592         for vol in block_device_mapping:
8593             # NOTE(mdbooth): The block_device_info we were passed was
8594             # initialized with BDMs from the source host before they were
8595             # updated to point to the destination. We can safely use this to
8596             # disconnect the source without re-fetching.
8597             self._disconnect_volume(context, vol['connection_info'], instance)
8598 
8599     def post_live_migration_at_source(self, context, instance, network_info):
8600         """Unplug VIFs from networks at source.
8601 
8602         :param context: security context
8603         :param instance: instance object reference
8604         :param network_info: instance network information
8605         """
8606         self.unplug_vifs(instance, network_info)
8607 
8608     def post_live_migration_at_destination(self, context,
8609                                            instance,
8610                                            network_info,
8611                                            block_migration=False,
8612                                            block_device_info=None):
8613         """Post operation of live migration at destination host.
8614 
8615         :param context: security context
8616         :param instance:
8617             nova.db.sqlalchemy.models.Instance object
8618             instance object that is migrated.
8619         :param network_info: instance network information
8620         :param block_migration: if true, post operation of block_migration.
8621         """
8622         self._reattach_instance_vifs(context, instance, network_info)
8623 
8624     def _get_instance_disk_info_from_config(self, guest_config,
8625                                             block_device_info):
8626         """Get the non-volume disk information from the domain xml
8627 
8628         :param LibvirtConfigGuest guest_config: the libvirt domain config
8629                                                 for the instance
8630         :param dict block_device_info: block device info for BDMs
8631         :returns disk_info: list of dicts with keys:
8632 
8633           * 'type': the disk type (str)
8634           * 'path': the disk path (str)
8635           * 'virt_disk_size': the virtual disk size (int)
8636           * 'backing_file': backing file of a disk image (str)
8637           * 'disk_size': physical disk size (int)
8638           * 'over_committed_disk_size': virt_disk_size - disk_size or 0
8639         """
8640         block_device_mapping = driver.block_device_info_get_mapping(
8641             block_device_info)
8642 
8643         volume_devices = set()
8644         for vol in block_device_mapping:
8645             disk_dev = vol['mount_device'].rpartition("/")[2]
8646             volume_devices.add(disk_dev)
8647 
8648         disk_info = []
8649 
8650         if (guest_config.virt_type == 'parallels' and
8651                 guest_config.os_type == fields.VMMode.EXE):
8652             node_type = 'filesystem'
8653         else:
8654             node_type = 'disk'
8655 
8656         for device in guest_config.devices:
8657             if device.root_name != node_type:
8658                 continue
8659             disk_type = device.source_type
8660             if device.root_name == 'filesystem':
8661                 target = device.target_dir
8662                 if device.source_type == 'file':
8663                     path = device.source_file
8664                 elif device.source_type == 'block':
8665                     path = device.source_dev
8666                 else:
8667                     path = None
8668             else:
8669                 target = device.target_dev
8670                 path = device.source_path
8671 
8672             if not path:
8673                 LOG.debug('skipping disk for %s as it does not have a path',
8674                           guest_config.name)
8675                 continue
8676 
8677             if disk_type not in ['file', 'block']:
8678                 LOG.debug('skipping disk because it looks like a volume', path)
8679                 continue
8680 
8681             if target in volume_devices:
8682                 LOG.debug('skipping disk %(path)s (%(target)s) as it is a '
8683                           'volume', {'path': path, 'target': target})
8684                 continue
8685 
8686             if device.root_name == 'filesystem':
8687                 driver_type = device.driver_type
8688             else:
8689                 driver_type = device.driver_format
8690             # get the real disk size or
8691             # raise a localized error if image is unavailable
8692             if disk_type == 'file' and driver_type == 'ploop':
8693                 dk_size = 0
8694                 for dirpath, dirnames, filenames in os.walk(path):
8695                     for f in filenames:
8696                         fp = os.path.join(dirpath, f)
8697                         dk_size += os.path.getsize(fp)
8698                 qemu_img_info = disk_api.get_disk_info(path)
8699                 virt_size = qemu_img_info.virtual_size
8700                 backing_file = libvirt_utils.get_disk_backing_file(path)
8701                 over_commit_size = int(virt_size) - dk_size
8702 
8703             elif disk_type == 'file' and driver_type == 'qcow2':
8704                 qemu_img_info = disk_api.get_disk_info(path)
8705                 dk_size = qemu_img_info.disk_size
8706                 virt_size = qemu_img_info.virtual_size
8707                 backing_file = libvirt_utils.get_disk_backing_file(path)
8708                 over_commit_size = int(virt_size) - dk_size
8709 
8710             elif disk_type == 'file':
8711                 dk_size = os.stat(path).st_blocks * 512
8712                 virt_size = os.path.getsize(path)
8713                 backing_file = ""
8714                 over_commit_size = 0
8715 
8716             elif disk_type == 'block' and block_device_info:
8717                 dk_size = lvm.get_volume_size(path)
8718                 virt_size = dk_size
8719                 backing_file = ""
8720                 over_commit_size = 0
8721 
8722             else:
8723                 LOG.debug('skipping disk %(path)s (%(target)s) - unable to '
8724                           'determine if volume',
8725                           {'path': path, 'target': target})
8726                 continue
8727 
8728             disk_info.append({'type': driver_type,
8729                               'path': path,
8730                               'virt_disk_size': virt_size,
8731                               'backing_file': backing_file,
8732                               'disk_size': dk_size,
8733                               'over_committed_disk_size': over_commit_size})
8734         return disk_info
8735 
8736     def _get_instance_disk_info(self, instance, block_device_info):
8737         try:
8738             guest = self._host.get_guest(instance)
8739             config = guest.get_config()
8740         except libvirt.libvirtError as ex:
8741             error_code = ex.get_error_code()
8742             LOG.warning('Error from libvirt while getting description of '
8743                         '%(instance_name)s: [Error Code %(error_code)s] '
8744                         '%(ex)s',
8745                         {'instance_name': instance.name,
8746                          'error_code': error_code,
8747                          'ex': encodeutils.exception_to_unicode(ex)},
8748                         instance=instance)
8749             raise exception.InstanceNotFound(instance_id=instance.uuid)
8750 
8751         return self._get_instance_disk_info_from_config(config,
8752                                                         block_device_info)
8753 
8754     def get_instance_disk_info(self, instance,
8755                                block_device_info=None):
8756         return jsonutils.dumps(
8757             self._get_instance_disk_info(instance, block_device_info))
8758 
8759     def _get_disk_over_committed_size_total(self):
8760         """Return total over committed disk size for all instances."""
8761         # Disk size that all instance uses : virtual_size - disk_size
8762         disk_over_committed_size = 0
8763         instance_domains = self._host.list_instance_domains(only_running=False)
8764         if not instance_domains:
8765             return disk_over_committed_size
8766 
8767         # Get all instance uuids
8768         instance_uuids = [dom.UUIDString() for dom in instance_domains]
8769         ctx = nova_context.get_admin_context()
8770         # Get instance object list by uuid filter
8771         filters = {'uuid': instance_uuids}
8772         # NOTE(ankit): objects.InstanceList.get_by_filters method is
8773         # getting called twice one is here and another in the
8774         # _update_available_resource method of resource_tracker. Since
8775         # _update_available_resource method is synchronized, there is a
8776         # possibility the instances list retrieved here to calculate
8777         # disk_over_committed_size would differ to the list you would get
8778         # in _update_available_resource method for calculating usages based
8779         # on instance utilization.
8780         local_instance_list = objects.InstanceList.get_by_filters(
8781             ctx, filters, use_slave=True)
8782         # Convert instance list to dictionary with instance uuid as key.
8783         local_instances = {inst.uuid: inst for inst in local_instance_list}
8784 
8785         # Get bdms by instance uuids
8786         bdms = objects.BlockDeviceMappingList.bdms_by_instance_uuid(
8787             ctx, instance_uuids)
8788 
8789         for dom in instance_domains:
8790             try:
8791                 guest = libvirt_guest.Guest(dom)
8792                 config = guest.get_config()
8793 
8794                 block_device_info = None
8795                 if guest.uuid in local_instances \
8796                         and (bdms and guest.uuid in bdms):
8797                     # Get block device info for instance
8798                     block_device_info = driver.get_block_device_info(
8799                         local_instances[guest.uuid], bdms[guest.uuid])
8800 
8801                 disk_infos = self._get_instance_disk_info_from_config(
8802                     config, block_device_info)
8803                 if not disk_infos:
8804                     continue
8805 
8806                 for info in disk_infos:
8807                     disk_over_committed_size += int(
8808                         info['over_committed_disk_size'])
8809             except libvirt.libvirtError as ex:
8810                 error_code = ex.get_error_code()
8811                 LOG.warning(
8812                     'Error from libvirt while getting description of '
8813                     '%(instance_name)s: [Error Code %(error_code)s] %(ex)s',
8814                     {'instance_name': guest.name,
8815                      'error_code': error_code,
8816                      'ex': encodeutils.exception_to_unicode(ex)})
8817             except OSError as e:
8818                 if e.errno in (errno.ENOENT, errno.ESTALE):
8819                     LOG.warning('Periodic task is updating the host stat, '
8820                                 'it is trying to get disk %(i_name)s, '
8821                                 'but disk file was removed by concurrent '
8822                                 'operations such as resize.',
8823                                 {'i_name': guest.name})
8824                 elif e.errno == errno.EACCES:
8825                     LOG.warning('Periodic task is updating the host stat, '
8826                                 'it is trying to get disk %(i_name)s, '
8827                                 'but access is denied. It is most likely '
8828                                 'due to a VM that exists on the compute '
8829                                 'node but is not managed by Nova.',
8830                                 {'i_name': guest.name})
8831                 else:
8832                     raise
8833             except exception.VolumeBDMPathNotFound as e:
8834                 LOG.warning('Periodic task is updating the host stats, '
8835                             'it is trying to get disk info for %(i_name)s, '
8836                             'but the backing volume block device was removed '
8837                             'by concurrent operations such as resize. '
8838                             'Error: %(error)s',
8839                             {'i_name': guest.name, 'error': e})
8840             except exception.DiskNotFound:
8841                 with excutils.save_and_reraise_exception() as err_ctxt:
8842                     # If the instance is undergoing a task state transition,
8843                     # like moving to another host or is being deleted, we
8844                     # should ignore this instance and move on.
8845                     if guest.uuid in local_instances:
8846                         inst = local_instances[guest.uuid]
8847                         # bug 1774249 indicated when instance is in RESIZED
8848                         # state it might also can't find back disk
8849                         if (inst.task_state is not None or
8850                             inst.vm_state == vm_states.RESIZED):
8851                             LOG.info('Periodic task is updating the host '
8852                                      'stats; it is trying to get disk info '
8853                                      'for %(i_name)s, but the backing disk '
8854                                      'was removed by a concurrent operation '
8855                                      '(task_state=%(task_state)s) and '
8856                                      '(vm_state=%(vm_state)s)',
8857                                      {'i_name': guest.name,
8858                                       'task_state': inst.task_state,
8859                                       'vm_state': inst.vm_state},
8860                                      instance=inst)
8861                             err_ctxt.reraise = False
8862 
8863             # NOTE(gtt116): give other tasks a chance.
8864             greenthread.sleep(0)
8865         return disk_over_committed_size
8866 
8867     def unfilter_instance(self, instance, network_info):
8868         """See comments of same method in firewall_driver."""
8869         self.firewall_driver.unfilter_instance(instance,
8870                                                network_info=network_info)
8871 
8872     def get_available_nodes(self, refresh=False):
8873         return [self._host.get_hostname()]
8874 
8875     def get_host_cpu_stats(self):
8876         """Return the current CPU state of the host."""
8877         return self._host.get_cpu_stats()
8878 
8879     def get_host_uptime(self):
8880         """Returns the result of calling "uptime"."""
8881         out, err = processutils.execute('env', 'LANG=C', 'uptime')
8882         return out
8883 
8884     def manage_image_cache(self, context, all_instances):
8885         """Manage the local cache of images."""
8886         self.image_cache_manager.update(context, all_instances)
8887 
8888     def _cleanup_remote_migration(self, dest, inst_base, inst_base_resize,
8889                                   shared_storage=False):
8890         """Used only for cleanup in case migrate_disk_and_power_off fails."""
8891         try:
8892             if os.path.exists(inst_base_resize):
8893                 shutil.rmtree(inst_base, ignore_errors=True)
8894                 os.rename(inst_base_resize, inst_base)
8895                 if not shared_storage:
8896                     self._remotefs.remove_dir(dest, inst_base)
8897         except Exception:
8898             pass
8899 
8900     def _is_storage_shared_with(self, dest, inst_base):
8901         # NOTE (rmk): There are two methods of determining whether we are
8902         #             on the same filesystem: the source and dest IP are the
8903         #             same, or we create a file on the dest system via SSH
8904         #             and check whether the source system can also see it.
8905         # NOTE (drwahl): Actually, there is a 3rd way: if images_type is rbd,
8906         #                it will always be shared storage
8907         if CONF.libvirt.images_type == 'rbd':
8908             return True
8909         shared_storage = (dest == self.get_host_ip_addr())
8910         if not shared_storage:
8911             tmp_file = uuidutils.generate_uuid(dashed=False) + '.tmp'
8912             tmp_path = os.path.join(inst_base, tmp_file)
8913 
8914             try:
8915                 self._remotefs.create_file(dest, tmp_path)
8916                 if os.path.exists(tmp_path):
8917                     shared_storage = True
8918                     os.unlink(tmp_path)
8919                 else:
8920                     self._remotefs.remove_file(dest, tmp_path)
8921             except Exception:
8922                 pass
8923         return shared_storage
8924 
8925     def migrate_disk_and_power_off(self, context, instance, dest,
8926                                    flavor, network_info,
8927                                    block_device_info=None,
8928                                    timeout=0, retry_interval=0):
8929         LOG.debug("Starting migrate_disk_and_power_off",
8930                    instance=instance)
8931 
8932         ephemerals = driver.block_device_info_get_ephemerals(block_device_info)
8933 
8934         # get_bdm_ephemeral_disk_size() will return 0 if the new
8935         # instance's requested block device mapping contain no
8936         # ephemeral devices. However, we still want to check if
8937         # the original instance's ephemeral_gb property was set and
8938         # ensure that the new requested flavor ephemeral size is greater
8939         eph_size = (block_device.get_bdm_ephemeral_disk_size(ephemerals) or
8940                     instance.flavor.ephemeral_gb)
8941 
8942         # Checks if the migration needs a disk resize down.
8943         root_down = flavor.root_gb < instance.flavor.root_gb
8944         ephemeral_down = flavor.ephemeral_gb < eph_size
8945         booted_from_volume = self._is_booted_from_volume(block_device_info)
8946 
8947         if (root_down and not booted_from_volume) or ephemeral_down:
8948             reason = _("Unable to resize disk down.")
8949             raise exception.InstanceFaultRollback(
8950                 exception.ResizeError(reason=reason))
8951 
8952         # NOTE(dgenin): Migration is not implemented for LVM backed instances.
8953         if CONF.libvirt.images_type == 'lvm' and not booted_from_volume:
8954             reason = _("Migration is not supported for LVM backed instances")
8955             raise exception.InstanceFaultRollback(
8956                 exception.MigrationPreCheckError(reason=reason))
8957 
8958         # copy disks to destination
8959         # rename instance dir to +_resize at first for using
8960         # shared storage for instance dir (eg. NFS).
8961         inst_base = libvirt_utils.get_instance_path(instance)
8962         inst_base_resize = inst_base + "_resize"
8963         shared_storage = self._is_storage_shared_with(dest, inst_base)
8964 
8965         # try to create the directory on the remote compute node
8966         # if this fails we pass the exception up the stack so we can catch
8967         # failures here earlier
8968         if not shared_storage:
8969             try:
8970                 self._remotefs.create_dir(dest, inst_base)
8971             except processutils.ProcessExecutionError as e:
8972                 reason = _("not able to execute ssh command: %s") % e
8973                 raise exception.InstanceFaultRollback(
8974                     exception.ResizeError(reason=reason))
8975 
8976         self.power_off(instance, timeout, retry_interval)
8977 
8978         block_device_mapping = driver.block_device_info_get_mapping(
8979             block_device_info)
8980         for vol in block_device_mapping:
8981             connection_info = vol['connection_info']
8982             self._disconnect_volume(context, connection_info, instance)
8983 
8984         disk_info = self._get_instance_disk_info(instance, block_device_info)
8985 
8986         try:
8987             os.rename(inst_base, inst_base_resize)
8988             # if we are migrating the instance with shared storage then
8989             # create the directory.  If it is a remote node the directory
8990             # has already been created
8991             if shared_storage:
8992                 dest = None
8993                 fileutils.ensure_tree(inst_base)
8994 
8995             on_execute = lambda process: \
8996                 self.job_tracker.add_job(instance, process.pid)
8997             on_completion = lambda process: \
8998                 self.job_tracker.remove_job(instance, process.pid)
8999 
9000             for info in disk_info:
9001                 # assume inst_base == dirname(info['path'])
9002                 img_path = info['path']
9003                 fname = os.path.basename(img_path)
9004                 from_path = os.path.join(inst_base_resize, fname)
9005 
9006                 # We will not copy over the swap disk here, and rely on
9007                 # finish_migration to re-create it for us. This is ok because
9008                 # the OS is shut down, and as recreating a swap disk is very
9009                 # cheap it is more efficient than copying either locally or
9010                 # over the network. This also means we don't have to resize it.
9011                 if fname == 'disk.swap':
9012                     continue
9013 
9014                 compression = info['type'] not in NO_COMPRESSION_TYPES
9015                 libvirt_utils.copy_image(from_path, img_path, host=dest,
9016                                          on_execute=on_execute,
9017                                          on_completion=on_completion,
9018                                          compression=compression)
9019 
9020             # Ensure disk.info is written to the new path to avoid disks being
9021             # reinspected and potentially changing format.
9022             src_disk_info_path = os.path.join(inst_base_resize, 'disk.info')
9023             if os.path.exists(src_disk_info_path):
9024                 dst_disk_info_path = os.path.join(inst_base, 'disk.info')
9025                 libvirt_utils.copy_image(src_disk_info_path,
9026                                          dst_disk_info_path,
9027                                          host=dest, on_execute=on_execute,
9028                                          on_completion=on_completion)
9029         except Exception:
9030             with excutils.save_and_reraise_exception():
9031                 self._cleanup_remote_migration(dest, inst_base,
9032                                                inst_base_resize,
9033                                                shared_storage)
9034 
9035         return jsonutils.dumps(disk_info)
9036 
9037     def _wait_for_running(self, instance):
9038         state = self.get_info(instance).state
9039 
9040         if state == power_state.RUNNING:
9041             LOG.info("Instance running successfully.", instance=instance)
9042             raise loopingcall.LoopingCallDone()
9043 
9044     @staticmethod
9045     def _disk_raw_to_qcow2(path):
9046         """Converts a raw disk to qcow2."""
9047         path_qcow = path + '_qcow'
9048         images.convert_image(path, path_qcow, 'raw', 'qcow2')
9049         os.rename(path_qcow, path)
9050 
9051     def finish_migration(self, context, migration, instance, disk_info,
9052                          network_info, image_meta, resize_instance,
9053                          block_device_info=None, power_on=True):
9054         LOG.debug("Starting finish_migration", instance=instance)
9055 
9056         block_disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
9057                                                   instance,
9058                                                   image_meta,
9059                                                   block_device_info)
9060         # assume _create_image does nothing if a target file exists.
9061         # NOTE: This has the intended side-effect of fetching a missing
9062         # backing file.
9063         self._create_image(context, instance, block_disk_info['mapping'],
9064                            block_device_info=block_device_info,
9065                            ignore_bdi_for_swap=True,
9066                            fallback_from_host=migration.source_compute)
9067 
9068         # Required by Quobyte CI
9069         self._ensure_console_log_for_instance(instance)
9070 
9071         gen_confdrive = functools.partial(
9072             self._create_configdrive, context, instance,
9073             InjectionInfo(admin_pass=None, network_info=network_info,
9074                           files=None))
9075 
9076         # Convert raw disks to qcow2 if migrating to host which uses
9077         # qcow2 from host which uses raw.
9078         disk_info = jsonutils.loads(disk_info)
9079         for info in disk_info:
9080             path = info['path']
9081             disk_name = os.path.basename(path)
9082 
9083             # NOTE(mdbooth): The code below looks wrong, but is actually
9084             # required to prevent a security hole when migrating from a host
9085             # with use_cow_images=False to one with use_cow_images=True.
9086             # Imagebackend uses use_cow_images to select between the
9087             # atrociously-named-Raw and Qcow2 backends. The Qcow2 backend
9088             # writes to disk.info, but does not read it as it assumes qcow2.
9089             # Therefore if we don't convert raw to qcow2 here, a raw disk will
9090             # be incorrectly assumed to be qcow2, which is a severe security
9091             # flaw. The reverse is not true, because the atrociously-named-Raw
9092             # backend supports both qcow2 and raw disks, and will choose
9093             # appropriately between them as long as disk.info exists and is
9094             # correctly populated, which it is because Qcow2 writes to
9095             # disk.info.
9096             #
9097             # In general, we do not yet support format conversion during
9098             # migration. For example:
9099             #   * Converting from use_cow_images=True to use_cow_images=False
9100             #     isn't handled. This isn't a security bug, but is almost
9101             #     certainly buggy in other cases, as the 'Raw' backend doesn't
9102             #     expect a backing file.
9103             #   * Converting to/from lvm and rbd backends is not supported.
9104             #
9105             # This behaviour is inconsistent, and therefore undesirable for
9106             # users. It is tightly-coupled to implementation quirks of 2
9107             # out of 5 backends in imagebackend and defends against a severe
9108             # security flaw which is not at all obvious without deep analysis,
9109             # and is therefore undesirable to developers. We should aim to
9110             # remove it. This will not be possible, though, until we can
9111             # represent the storage layout of a specific instance
9112             # independent of the default configuration of the local compute
9113             # host.
9114 
9115             # Config disks are hard-coded to be raw even when
9116             # use_cow_images=True (see _get_disk_config_image_type),so don't
9117             # need to be converted.
9118             if (disk_name != 'disk.config' and
9119                         info['type'] == 'raw' and CONF.use_cow_images):
9120                 self._disk_raw_to_qcow2(info['path'])
9121 
9122         xml = self._get_guest_xml(context, instance, network_info,
9123                                   block_disk_info, image_meta,
9124                                   block_device_info=block_device_info)
9125         # NOTE(mriedem): vifs_already_plugged=True here, regardless of whether
9126         # or not we've migrated to another host, because we unplug VIFs locally
9127         # and the status change in the port might go undetected by the neutron
9128         # L2 agent (or neutron server) so neutron may not know that the VIF was
9129         # unplugged in the first place and never send an event.
9130         guest = self._create_domain_and_network(context, xml, instance,
9131                                         network_info,
9132                                         block_device_info=block_device_info,
9133                                         power_on=power_on,
9134                                         vifs_already_plugged=True,
9135                                         post_xml_callback=gen_confdrive)
9136         if power_on:
9137             timer = loopingcall.FixedIntervalLoopingCall(
9138                                                     self._wait_for_running,
9139                                                     instance)
9140             timer.start(interval=0.5).wait()
9141 
9142             # Sync guest time after migration.
9143             guest.sync_guest_time()
9144 
9145         LOG.debug("finish_migration finished successfully.", instance=instance)
9146 
9147     def _cleanup_failed_migration(self, inst_base):
9148         """Make sure that a failed migrate doesn't prevent us from rolling
9149         back in a revert.
9150         """
9151         try:
9152             shutil.rmtree(inst_base)
9153         except OSError as e:
9154             if e.errno != errno.ENOENT:
9155                 raise
9156 
9157     def finish_revert_migration(self, context, instance, network_info,
9158                                 block_device_info=None, power_on=True):
9159         LOG.debug("Starting finish_revert_migration",
9160                   instance=instance)
9161 
9162         inst_base = libvirt_utils.get_instance_path(instance)
9163         inst_base_resize = inst_base + "_resize"
9164 
9165         # NOTE(danms): if we're recovering from a failed migration,
9166         # make sure we don't have a left-over same-host base directory
9167         # that would conflict. Also, don't fail on the rename if the
9168         # failure happened early.
9169         if os.path.exists(inst_base_resize):
9170             self._cleanup_failed_migration(inst_base)
9171             os.rename(inst_base_resize, inst_base)
9172 
9173         root_disk = self.image_backend.by_name(instance, 'disk')
9174         # Once we rollback, the snapshot is no longer needed, so remove it
9175         if root_disk.exists():
9176             root_disk.rollback_to_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME)
9177             root_disk.remove_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME)
9178 
9179         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
9180                                             instance,
9181                                             instance.image_meta,
9182                                             block_device_info)
9183         xml = self._get_guest_xml(context, instance, network_info, disk_info,
9184                                   instance.image_meta,
9185                                   block_device_info=block_device_info)
9186         self._create_domain_and_network(context, xml, instance, network_info,
9187                                         block_device_info=block_device_info,
9188                                         power_on=power_on)
9189 
9190         if power_on:
9191             timer = loopingcall.FixedIntervalLoopingCall(
9192                                                     self._wait_for_running,
9193                                                     instance)
9194             timer.start(interval=0.5).wait()
9195 
9196         LOG.debug("finish_revert_migration finished successfully.",
9197                   instance=instance)
9198 
9199     def confirm_migration(self, context, migration, instance, network_info):
9200         """Confirms a resize, destroying the source VM."""
9201         self._cleanup_resize(context, instance, network_info)
9202 
9203     @staticmethod
9204     def _get_io_devices(xml_doc):
9205         """get the list of io devices from the xml document."""
9206         result = {"volumes": [], "ifaces": []}
9207         try:
9208             doc = etree.fromstring(xml_doc)
9209         except Exception:
9210             return result
9211         blocks = [('./devices/disk', 'volumes'),
9212             ('./devices/interface', 'ifaces')]
9213         for block, key in blocks:
9214             section = doc.findall(block)
9215             for node in section:
9216                 for child in node.getchildren():
9217                     if child.tag == 'target' and child.get('dev'):
9218                         result[key].append(child.get('dev'))
9219         return result
9220 
9221     def get_diagnostics(self, instance):
9222         guest = self._host.get_guest(instance)
9223 
9224         # TODO(sahid): We are converting all calls from a
9225         # virDomain object to use nova.virt.libvirt.Guest.
9226         # We should be able to remove domain at the end.
9227         domain = guest._domain
9228         output = {}
9229         # get cpu time, might launch an exception if the method
9230         # is not supported by the underlying hypervisor being
9231         # used by libvirt
9232         try:
9233             for vcpu in guest.get_vcpus_info():
9234                 output["cpu" + str(vcpu.id) + "_time"] = vcpu.time
9235         except libvirt.libvirtError:
9236             pass
9237         # get io status
9238         xml = guest.get_xml_desc()
9239         dom_io = LibvirtDriver._get_io_devices(xml)
9240         for guest_disk in dom_io["volumes"]:
9241             try:
9242                 # blockStats might launch an exception if the method
9243                 # is not supported by the underlying hypervisor being
9244                 # used by libvirt
9245                 stats = domain.blockStats(guest_disk)
9246                 output[guest_disk + "_read_req"] = stats[0]
9247                 output[guest_disk + "_read"] = stats[1]
9248                 output[guest_disk + "_write_req"] = stats[2]
9249                 output[guest_disk + "_write"] = stats[3]
9250                 output[guest_disk + "_errors"] = stats[4]
9251             except libvirt.libvirtError:
9252                 pass
9253         for interface in dom_io["ifaces"]:
9254             try:
9255                 # interfaceStats might launch an exception if the method
9256                 # is not supported by the underlying hypervisor being
9257                 # used by libvirt
9258                 stats = domain.interfaceStats(interface)
9259                 output[interface + "_rx"] = stats[0]
9260                 output[interface + "_rx_packets"] = stats[1]
9261                 output[interface + "_rx_errors"] = stats[2]
9262                 output[interface + "_rx_drop"] = stats[3]
9263                 output[interface + "_tx"] = stats[4]
9264                 output[interface + "_tx_packets"] = stats[5]
9265                 output[interface + "_tx_errors"] = stats[6]
9266                 output[interface + "_tx_drop"] = stats[7]
9267             except libvirt.libvirtError:
9268                 pass
9269         output["memory"] = domain.maxMemory()
9270         # memoryStats might launch an exception if the method
9271         # is not supported by the underlying hypervisor being
9272         # used by libvirt
9273         try:
9274             mem = domain.memoryStats()
9275             for key in mem.keys():
9276                 output["memory-" + key] = mem[key]
9277         except (libvirt.libvirtError, AttributeError):
9278             pass
9279         return output
9280 
9281     def get_instance_diagnostics(self, instance):
9282         guest = self._host.get_guest(instance)
9283 
9284         # TODO(sahid): We are converting all calls from a
9285         # virDomain object to use nova.virt.libvirt.Guest.
9286         # We should be able to remove domain at the end.
9287         domain = guest._domain
9288 
9289         xml = guest.get_xml_desc()
9290         xml_doc = etree.fromstring(xml)
9291 
9292         # TODO(sahid): Needs to use get_info but more changes have to
9293         # be done since a mapping STATE_MAP LIBVIRT_POWER_STATE is
9294         # needed.
9295         (state, max_mem, mem, num_cpu, cpu_time) = \
9296             guest._get_domain_info(self._host)
9297         config_drive = configdrive.required_by(instance)
9298         launched_at = timeutils.normalize_time(instance.launched_at)
9299         uptime = timeutils.delta_seconds(launched_at,
9300                                          timeutils.utcnow())
9301         diags = diagnostics_obj.Diagnostics(state=power_state.STATE_MAP[state],
9302                                         driver='libvirt',
9303                                         config_drive=config_drive,
9304                                         hypervisor=CONF.libvirt.virt_type,
9305                                         hypervisor_os='linux',
9306                                         uptime=uptime)
9307         diags.memory_details = diagnostics_obj.MemoryDiagnostics(
9308             maximum=max_mem / units.Mi,
9309             used=mem / units.Mi)
9310 
9311         # get cpu time, might launch an exception if the method
9312         # is not supported by the underlying hypervisor being
9313         # used by libvirt
9314         try:
9315             for vcpu in guest.get_vcpus_info():
9316                 diags.add_cpu(id=vcpu.id, time=vcpu.time)
9317         except libvirt.libvirtError:
9318             pass
9319         # get io status
9320         dom_io = LibvirtDriver._get_io_devices(xml)
9321         for guest_disk in dom_io["volumes"]:
9322             try:
9323                 # blockStats might launch an exception if the method
9324                 # is not supported by the underlying hypervisor being
9325                 # used by libvirt
9326                 stats = domain.blockStats(guest_disk)
9327                 diags.add_disk(read_bytes=stats[1],
9328                                read_requests=stats[0],
9329                                write_bytes=stats[3],
9330                                write_requests=stats[2],
9331                                errors_count=stats[4])
9332             except libvirt.libvirtError:
9333                 pass
9334 
9335         for interface in xml_doc.findall('./devices/interface'):
9336             mac_address = interface.find('mac').get('address')
9337             target = interface.find('./target')
9338 
9339             # add nic that has no target (therefore no stats)
9340             if target is None:
9341                 diags.add_nic(mac_address=mac_address)
9342                 continue
9343 
9344             # add nic with stats
9345             dev = target.get('dev')
9346             try:
9347                 if dev:
9348                     # interfaceStats might launch an exception if the
9349                     # method is not supported by the underlying hypervisor
9350                     # being used by libvirt
9351                     stats = domain.interfaceStats(dev)
9352                     diags.add_nic(mac_address=mac_address,
9353                                   rx_octets=stats[0],
9354                                   rx_errors=stats[2],
9355                                   rx_drop=stats[3],
9356                                   rx_packets=stats[1],
9357                                   tx_octets=stats[4],
9358                                   tx_errors=stats[6],
9359                                   tx_drop=stats[7],
9360                                   tx_packets=stats[5])
9361 
9362             except libvirt.libvirtError:
9363                 pass
9364 
9365         return diags
9366 
9367     @staticmethod
9368     def _prepare_device_bus(dev):
9369         """Determines the device bus and its hypervisor assigned address
9370         """
9371         bus = None
9372         address = (dev.device_addr.format_address() if
9373                    dev.device_addr else None)
9374         if isinstance(dev.device_addr,
9375                       vconfig.LibvirtConfigGuestDeviceAddressPCI):
9376             bus = objects.PCIDeviceBus()
9377         elif isinstance(dev, vconfig.LibvirtConfigGuestDisk):
9378             if dev.target_bus == 'scsi':
9379                 bus = objects.SCSIDeviceBus()
9380             elif dev.target_bus == 'ide':
9381                 bus = objects.IDEDeviceBus()
9382             elif dev.target_bus == 'usb':
9383                 bus = objects.USBDeviceBus()
9384         if address is not None and bus is not None:
9385             bus.address = address
9386         return bus
9387 
9388     def _build_interface_metadata(self, dev, vifs_to_expose, vlans_by_mac,
9389                                   trusted_by_mac):
9390         """Builds a metadata object for a network interface
9391 
9392         :param dev: The LibvirtConfigGuestInterface to build metadata for.
9393         :param vifs_to_expose: The list of tagged and/or vlan'ed
9394                                VirtualInterface objects.
9395         :param vlans_by_mac: A dictionary of mac address -> vlan associations.
9396         :param trusted_by_mac: A dictionary of mac address -> vf_trusted
9397                                associations.
9398         :return: A NetworkInterfaceMetadata object, or None.
9399         """
9400         vif = vifs_to_expose.get(dev.mac_addr)
9401         if not vif:
9402             LOG.debug('No VIF found with MAC %s, not building metadata',
9403                       dev.mac_addr)
9404             return None
9405         bus = self._prepare_device_bus(dev)
9406         device = objects.NetworkInterfaceMetadata(mac=vif.address)
9407         if 'tag' in vif and vif.tag:
9408             device.tags = [vif.tag]
9409         if bus:
9410             device.bus = bus
9411         vlan = vlans_by_mac.get(vif.address)
9412         if vlan:
9413             device.vlan = int(vlan)
9414         device.vf_trusted = trusted_by_mac.get(vif.address, False)
9415         return device
9416 
9417     def _build_disk_metadata(self, dev, tagged_bdms):
9418         """Builds a metadata object for a disk
9419 
9420         :param dev: The vconfig.LibvirtConfigGuestDisk to build metadata for.
9421         :param tagged_bdms: The list of tagged BlockDeviceMapping objects.
9422         :return: A DiskMetadata object, or None.
9423         """
9424         bdm = tagged_bdms.get(dev.target_dev)
9425         if not bdm:
9426             LOG.debug('No BDM found with device name %s, not building '
9427                       'metadata.', dev.target_dev)
9428             return None
9429         bus = self._prepare_device_bus(dev)
9430         device = objects.DiskMetadata(tags=[bdm.tag])
9431         # NOTE(artom) Setting the serial (which corresponds to
9432         # volume_id in BlockDeviceMapping) in DiskMetadata allows us to
9433         # find the disks's BlockDeviceMapping object when we detach the
9434         # volume and want to clean up its metadata.
9435         device.serial = bdm.volume_id
9436         if bus:
9437             device.bus = bus
9438         return device
9439 
9440     def _build_hostdev_metadata(self, dev, vifs_to_expose, vlans_by_mac):
9441         """Builds a metadata object for a hostdev. This can only be a PF, so we
9442         don't need trusted_by_mac like in _build_interface_metadata because
9443         only VFs can be trusted.
9444 
9445         :param dev: The LibvirtConfigGuestHostdevPCI to build metadata for.
9446         :param vifs_to_expose: The list of tagged and/or vlan'ed
9447                                VirtualInterface objects.
9448         :param vlans_by_mac: A dictionary of mac address -> vlan associations.
9449         :return: A NetworkInterfaceMetadata object, or None.
9450         """
9451         # Strip out the leading '0x'
9452         pci_address = pci_utils.get_pci_address(
9453             *[x[2:] for x in (dev.domain, dev.bus, dev.slot, dev.function)])
9454         try:
9455             mac = pci_utils.get_mac_by_pci_address(pci_address,
9456                                                    pf_interface=True)
9457         except exception.PciDeviceNotFoundById:
9458             LOG.debug('Not exposing metadata for not found PCI device %s',
9459                       pci_address)
9460             return None
9461 
9462         vif = vifs_to_expose.get(mac)
9463         if not vif:
9464             LOG.debug('No VIF found with MAC %s, not building metadata', mac)
9465             return None
9466 
9467         device = objects.NetworkInterfaceMetadata(mac=mac)
9468         device.bus = objects.PCIDeviceBus(address=pci_address)
9469         if 'tag' in vif and vif.tag:
9470             device.tags = [vif.tag]
9471         vlan = vlans_by_mac.get(mac)
9472         if vlan:
9473             device.vlan = int(vlan)
9474         return device
9475 
9476     def _build_device_metadata(self, context, instance):
9477         """Builds a metadata object for instance devices, that maps the user
9478            provided tag to the hypervisor assigned device address.
9479         """
9480         def _get_device_name(bdm):
9481             return block_device.strip_dev(bdm.device_name)
9482 
9483         network_info = instance.info_cache.network_info
9484         vlans_by_mac = netutils.get_cached_vifs_with_vlan(network_info)
9485         trusted_by_mac = netutils.get_cached_vifs_with_trusted(network_info)
9486         vifs = objects.VirtualInterfaceList.get_by_instance_uuid(context,
9487                                                                  instance.uuid)
9488         vifs_to_expose = {vif.address: vif for vif in vifs
9489                           if ('tag' in vif and vif.tag) or
9490                              vlans_by_mac.get(vif.address)}
9491         # TODO(mriedem): We should be able to avoid the DB query here by using
9492         # block_device_info['block_device_mapping'] which is passed into most
9493         # methods that call this function.
9494         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
9495             context, instance.uuid)
9496         tagged_bdms = {_get_device_name(bdm): bdm for bdm in bdms if bdm.tag}
9497 
9498         devices = []
9499         guest = self._host.get_guest(instance)
9500         xml = guest.get_xml_desc()
9501         xml_dom = etree.fromstring(xml)
9502         guest_config = vconfig.LibvirtConfigGuest()
9503         guest_config.parse_dom(xml_dom)
9504 
9505         for dev in guest_config.devices:
9506             device = None
9507             if isinstance(dev, vconfig.LibvirtConfigGuestInterface):
9508                 device = self._build_interface_metadata(dev, vifs_to_expose,
9509                                                         vlans_by_mac,
9510                                                         trusted_by_mac)
9511             if isinstance(dev, vconfig.LibvirtConfigGuestDisk):
9512                 device = self._build_disk_metadata(dev, tagged_bdms)
9513             if isinstance(dev, vconfig.LibvirtConfigGuestHostdevPCI):
9514                 device = self._build_hostdev_metadata(dev, vifs_to_expose,
9515                                                       vlans_by_mac)
9516             if device:
9517                 devices.append(device)
9518         if devices:
9519             dev_meta = objects.InstanceDeviceMetadata(devices=devices)
9520             return dev_meta
9521 
9522     def instance_on_disk(self, instance):
9523         # ensure directories exist and are writable
9524         instance_path = libvirt_utils.get_instance_path(instance)
9525         LOG.debug('Checking instance files accessibility %s', instance_path,
9526                   instance=instance)
9527         shared_instance_path = os.access(instance_path, os.W_OK)
9528         # NOTE(flwang): For shared block storage scenario, the file system is
9529         # not really shared by the two hosts, but the volume of evacuated
9530         # instance is reachable.
9531         shared_block_storage = (self.image_backend.backend().
9532                                 is_shared_block_storage())
9533         return shared_instance_path or shared_block_storage
9534 
9535     def inject_network_info(self, instance, nw_info):
9536         self.firewall_driver.setup_basic_filtering(instance, nw_info)
9537 
9538     def delete_instance_files(self, instance):
9539         target = libvirt_utils.get_instance_path(instance)
9540         # A resize may be in progress
9541         target_resize = target + '_resize'
9542         # Other threads may attempt to rename the path, so renaming the path
9543         # to target + '_del' (because it is atomic) and iterating through
9544         # twice in the unlikely event that a concurrent rename occurs between
9545         # the two rename attempts in this method. In general this method
9546         # should be fairly thread-safe without these additional checks, since
9547         # other operations involving renames are not permitted when the task
9548         # state is not None and the task state should be set to something
9549         # other than None by the time this method is invoked.
9550         target_del = target + '_del'
9551         for i in range(2):
9552             try:
9553                 os.rename(target, target_del)
9554                 break
9555             except Exception:
9556                 pass
9557             try:
9558                 os.rename(target_resize, target_del)
9559                 break
9560             except Exception:
9561                 pass
9562         # Either the target or target_resize path may still exist if all
9563         # rename attempts failed.
9564         remaining_path = None
9565         for p in (target, target_resize):
9566             if os.path.exists(p):
9567                 remaining_path = p
9568                 break
9569 
9570         # A previous delete attempt may have been interrupted, so target_del
9571         # may exist even if all rename attempts during the present method
9572         # invocation failed due to the absence of both target and
9573         # target_resize.
9574         if not remaining_path and os.path.exists(target_del):
9575             self.job_tracker.terminate_jobs(instance)
9576 
9577             LOG.info('Deleting instance files %s', target_del,
9578                      instance=instance)
9579             remaining_path = target_del
9580             try:
9581                 shutil.rmtree(target_del)
9582             except OSError as e:
9583                 LOG.error('Failed to cleanup directory %(target)s: %(e)s',
9584                           {'target': target_del, 'e': e}, instance=instance)
9585 
9586         # It is possible that the delete failed, if so don't mark the instance
9587         # as cleaned.
9588         if remaining_path and os.path.exists(remaining_path):
9589             LOG.info('Deletion of %s failed', remaining_path,
9590                      instance=instance)
9591             return False
9592 
9593         LOG.info('Deletion of %s complete', target_del, instance=instance)
9594         return True
9595 
9596     @property
9597     def need_legacy_block_device_info(self):
9598         return False
9599 
9600     def default_root_device_name(self, instance, image_meta, root_bdm):
9601         disk_bus = blockinfo.get_disk_bus_for_device_type(
9602             instance, CONF.libvirt.virt_type, image_meta, "disk")
9603         cdrom_bus = blockinfo.get_disk_bus_for_device_type(
9604             instance, CONF.libvirt.virt_type, image_meta, "cdrom")
9605         root_info = blockinfo.get_root_info(
9606             instance, CONF.libvirt.virt_type, image_meta,
9607             root_bdm, disk_bus, cdrom_bus)
9608         return block_device.prepend_dev(root_info['dev'])
9609 
9610     def default_device_names_for_instance(self, instance, root_device_name,
9611                                           *block_device_lists):
9612         block_device_mapping = list(itertools.chain(*block_device_lists))
9613         # NOTE(ndipanov): Null out the device names so that blockinfo code
9614         #                 will assign them
9615         for bdm in block_device_mapping:
9616             if bdm.device_name is not None:
9617                 LOG.info(
9618                     "Ignoring supplied device name: %(device_name)s. "
9619                     "Libvirt can't honour user-supplied dev names",
9620                     {'device_name': bdm.device_name}, instance=instance)
9621                 bdm.device_name = None
9622         block_device_info = driver.get_block_device_info(instance,
9623                                                          block_device_mapping)
9624 
9625         blockinfo.default_device_names(CONF.libvirt.virt_type,
9626                                        nova_context.get_admin_context(),
9627                                        instance,
9628                                        block_device_info,
9629                                        instance.image_meta)
9630 
9631     def get_device_name_for_instance(self, instance, bdms, block_device_obj):
9632         block_device_info = driver.get_block_device_info(instance, bdms)
9633         instance_info = blockinfo.get_disk_info(
9634                 CONF.libvirt.virt_type, instance,
9635                 instance.image_meta, block_device_info=block_device_info)
9636 
9637         suggested_dev_name = block_device_obj.device_name
9638         if suggested_dev_name is not None:
9639             LOG.info(
9640                 'Ignoring supplied device name: %(suggested_dev)s',
9641                 {'suggested_dev': suggested_dev_name}, instance=instance)
9642 
9643         # NOTE(ndipanov): get_info_from_bdm will generate the new device name
9644         #                 only when it's actually not set on the bd object
9645         block_device_obj.device_name = None
9646         disk_info = blockinfo.get_info_from_bdm(
9647             instance, CONF.libvirt.virt_type, instance.image_meta,
9648             block_device_obj, mapping=instance_info['mapping'])
9649         return block_device.prepend_dev(disk_info['dev'])
9650 
9651     def is_supported_fs_format(self, fs_type):
9652         return fs_type in [nova.privsep.fs.FS_FORMAT_EXT2,
9653                            nova.privsep.fs.FS_FORMAT_EXT3,
9654                            nova.privsep.fs.FS_FORMAT_EXT4,
9655                            nova.privsep.fs.FS_FORMAT_XFS]
9656 
9657     def _get_cpu_traits(self):
9658         """Get CPU traits of VMs based on guest CPU model config:
9659         1. if mode is 'host-model' or 'host-passthrough', use host's
9660         CPU features.
9661         2. if mode is None, choose a default CPU model based on CPU
9662         architecture.
9663         3. if mode is 'custom', use cpu_model to generate CPU features.
9664         The code also accounts for cpu_model_extra_flags configuration when
9665         cpu_mode is 'host-model', 'host-passthrough' or 'custom', this
9666         ensures user specified CPU feature flags to be included.
9667         :return: A dict of trait names mapped to boolean values or None.
9668         """
9669         cpu = self._get_guest_cpu_model_config()
9670         if not cpu:
9671             LOG.info('The current libvirt hypervisor %(virt_type)s '
9672                      'does not support reporting CPU traits.',
9673                      {'virt_type': CONF.libvirt.virt_type})
9674             return
9675 
9676         caps = deepcopy(self._host.get_capabilities())
9677         if cpu.mode in ('host-model', 'host-passthrough'):
9678             # Account for features in cpu_model_extra_flags conf
9679             host_features = [f.name for f in
9680                              caps.host.cpu.features | cpu.features]
9681             return libvirt_utils.cpu_features_to_traits(host_features)
9682 
9683         # Choose a default CPU model when cpu_mode is not specified
9684         if cpu.mode is None:
9685             caps.host.cpu.model = libvirt_utils.get_cpu_model_from_arch(
9686                 caps.host.cpu.arch)
9687             caps.host.cpu.features = set()
9688         else:
9689             # For custom mode, set model to guest CPU model
9690             caps.host.cpu.model = cpu.model
9691             caps.host.cpu.features = set()
9692             # Account for features in cpu_model_extra_flags conf
9693             for f in cpu.features:
9694                 caps.host.cpu.add_feature(
9695                     vconfig.LibvirtConfigCPUFeature(name=f.name))
9696 
9697         xml_str = caps.host.cpu.to_xml()
9698         features_xml = self._get_guest_baseline_cpu_features(xml_str)
9699         feature_names = []
9700         if features_xml:
9701             cpu.parse_str(features_xml)
9702             feature_names = [f.name for f in cpu.features]
9703         return libvirt_utils.cpu_features_to_traits(feature_names)
9704 
9705     def _get_guest_baseline_cpu_features(self, xml_str):
9706         """Calls libvirt's baselineCPU API to compute the biggest set of
9707         CPU features which is compatible with the given host CPU.
9708 
9709         :param xml_str: XML description of host CPU
9710         :return: An XML string of the computed CPU, or None on error
9711         """
9712         LOG.debug("Libvirt baseline CPU %s", xml_str)
9713         # TODO(lei-zh): baselineCPU is not supported on all platforms.
9714         # There is some work going on in the libvirt community to replace the
9715         # baseline call. Consider using the new apis when they are ready. See
9716         # https://www.redhat.com/archives/libvir-list/2018-May/msg01204.html.
9717         try:
9718             if hasattr(libvirt, 'VIR_CONNECT_BASELINE_CPU_EXPAND_FEATURES'):
9719                 return self._host.get_connection().baselineCPU(
9720                     [xml_str],
9721                     libvirt.VIR_CONNECT_BASELINE_CPU_EXPAND_FEATURES)
9722             else:
9723                 return self._host.get_connection().baselineCPU([xml_str])
9724         except libvirt.libvirtError as ex:
9725             with excutils.save_and_reraise_exception() as ctxt:
9726                 error_code = ex.get_error_code()
9727                 if error_code == libvirt.VIR_ERR_NO_SUPPORT:
9728                     ctxt.reraise = False
9729                     LOG.info('URI %(uri)s does not support full set'
9730                              ' of host capabilities: %(error)s',
9731                              {'uri': self._host._uri, 'error': ex})
9732                     return None
