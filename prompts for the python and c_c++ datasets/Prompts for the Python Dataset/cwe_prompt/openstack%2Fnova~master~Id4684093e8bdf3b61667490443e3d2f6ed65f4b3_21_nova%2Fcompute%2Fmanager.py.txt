Please review the code below for security defects using the CWE (Common Weakness Enumeration) as a reference standard. If any are found, please describe the security defect in detail and indicate the corresponding line number of code and solution. If none are detected, state: 'No security defects are detected in the code'.

1 # Copyright 2010 United States Government as represented by the
2 # Administrator of the National Aeronautics and Space Administration.
3 # Copyright 2011 Justin Santa Barbara
4 # All Rights Reserved.
5 #
6 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
7 #    not use this file except in compliance with the License. You may obtain
8 #    a copy of the License at
9 #
10 #         http://www.apache.org/licenses/LICENSE-2.0
11 #
12 #    Unless required by applicable law or agreed to in writing, software
13 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
14 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
15 #    License for the specific language governing permissions and limitations
16 #    under the License.
17 
18 """Handles all processes relating to instances (guest vms).
19 
20 The :py:class:`ComputeManager` class is a :py:class:`nova.manager.Manager` that
21 handles RPC calls relating to creating instances.  It is responsible for
22 building a disk image, launching it via the underlying virtualization driver,
23 responding to calls to check its state, attaching persistent storage, and
24 terminating it.
25 
26 """
27 
28 import base64
29 import binascii
30 import contextlib
31 import copy
32 import functools
33 import inspect
34 import sys
35 import time
36 import traceback
37 import typing as ty
38 
39 from cinderclient import exceptions as cinder_exception
40 from cursive import exception as cursive_exception
41 import eventlet.event
42 from eventlet import greenthread
43 import eventlet.semaphore
44 import eventlet.timeout
45 import futurist
46 from keystoneauth1 import exceptions as keystone_exception
47 import os_traits
48 from oslo_log import log as logging
49 import oslo_messaging as messaging
50 from oslo_serialization import jsonutils
51 from oslo_service import loopingcall
52 from oslo_service import periodic_task
53 from oslo_utils import excutils
54 from oslo_utils import strutils
55 from oslo_utils import timeutils
56 from oslo_utils import units
57 
58 from nova.accelerator import cyborg
59 from nova import block_device
60 from nova.compute import api as compute
61 from nova.compute import build_results
62 from nova.compute import claims
63 from nova.compute import power_state
64 from nova.compute import resource_tracker
65 from nova.compute import rpcapi as compute_rpcapi
66 from nova.compute import task_states
67 from nova.compute import utils as compute_utils
68 from nova.compute.utils import wrap_instance_event
69 from nova.compute import vm_states
70 from nova import conductor
71 import nova.conf
72 import nova.context
73 from nova import exception
74 from nova import exception_wrapper
75 from nova.i18n import _
76 from nova.image import glance
77 from nova import manager
78 from nova.network import model as network_model
79 from nova.network import neutron
80 from nova import objects
81 from nova.objects import base as obj_base
82 from nova.objects import external_event as external_event_obj
83 from nova.objects import fields
84 from nova.objects import instance as obj_instance
85 from nova.objects import migrate_data as migrate_data_obj
86 from nova.pci import request as pci_req_module
87 from nova.pci import whitelist
88 from nova import rpc
89 from nova import safe_utils
90 from nova.scheduler.client import query
91 from nova.scheduler.client import report
92 from nova.scheduler import utils as scheduler_utils
93 from nova import utils
94 from nova.virt import block_device as driver_block_device
95 from nova.virt import configdrive
96 from nova.virt import driver
97 from nova.virt import event as virtevent
98 from nova.virt import hardware
99 from nova.virt import storage_users
100 from nova.virt import virtapi
101 from nova.volume import cinder
102 
103 CONF = nova.conf.CONF
104 
105 LOG = logging.getLogger(__name__)
106 
107 get_notifier = functools.partial(rpc.get_notifier, service='compute')
108 wrap_exception = functools.partial(exception_wrapper.wrap_exception,
109                                    get_notifier=get_notifier,
110                                    binary='nova-compute')
111 
112 
113 @contextlib.contextmanager
114 def errors_out_migration_ctxt(migration):
115     """Context manager to error out migration on failure."""
116 
117     try:
118         yield
119     except Exception:
120         with excutils.save_and_reraise_exception():
121             if migration:
122                 # We may have been passed None for our migration if we're
123                 # receiving from an older client. The migration will be
124                 # errored via the legacy path.
125                 migration.status = 'error'
126                 try:
127                     migration.save()
128                 except Exception:
129                     LOG.debug(
130                         'Error setting migration status for instance %s.',
131                         migration.instance_uuid, exc_info=True)
132 
133 
134 @utils.expects_func_args('migration')
135 def errors_out_migration(function):
136     """Decorator to error out migration on failure."""
137 
138     @functools.wraps(function)
139     def decorated_function(self, context, *args, **kwargs):
140         wrapped_func = safe_utils.get_wrapped_function(function)
141         keyed_args = inspect.getcallargs(wrapped_func, self, context,
142                                          *args, **kwargs)
143         migration = keyed_args['migration']
144         with errors_out_migration_ctxt(migration):
145             return function(self, context, *args, **kwargs)
146 
147     return decorated_function
148 
149 
150 @utils.expects_func_args('instance')
151 def reverts_task_state(function):
152     """Decorator to revert task_state on failure."""
153 
154     @functools.wraps(function)
155     def decorated_function(self, context, *args, **kwargs):
156         try:
157             return function(self, context, *args, **kwargs)
158         except exception.UnexpectedTaskStateError as e:
159             # Note(maoy): unexpected task state means the current
160             # task is preempted. Do not clear task state in this
161             # case.
162             with excutils.save_and_reraise_exception():
163                 LOG.info("Task possibly preempted: %s",
164                          e.format_message())
165         except Exception:
166             with excutils.save_and_reraise_exception():
167                 wrapped_func = safe_utils.get_wrapped_function(function)
168                 keyed_args = inspect.getcallargs(wrapped_func, self, context,
169                                                  *args, **kwargs)
170                 # NOTE(mriedem): 'instance' must be in keyed_args because we
171                 # have utils.expects_func_args('instance') decorating this
172                 # method.
173                 instance = keyed_args['instance']
174                 original_task_state = instance.task_state
175                 try:
176                     self._instance_update(context, instance, task_state=None)
177                     LOG.info("Successfully reverted task state from %s on "
178                              "failure for instance.",
179                              original_task_state, instance=instance)
180                 except exception.InstanceNotFound:
181                     # We might delete an instance that failed to build shortly
182                     # after it errored out this is an expected case and we
183                     # should not trace on it.
184                     pass
185                 except Exception as e:
186                     LOG.warning("Failed to revert task state for instance. "
187                                 "Error: %s", e, instance=instance)
188 
189     return decorated_function
190 
191 
192 @utils.expects_func_args('instance')
193 def wrap_instance_fault(function):
194     """Wraps a method to catch exceptions related to instances.
195 
196     This decorator wraps a method to catch any exceptions having to do with
197     an instance that may get thrown. It then logs an instance fault in the db.
198     """
199 
200     @functools.wraps(function)
201     def decorated_function(self, context, *args, **kwargs):
202         try:
203             return function(self, context, *args, **kwargs)
204         except exception.InstanceNotFound:
205             raise
206         except Exception as e:
207             # NOTE(gtt): If argument 'instance' is in args rather than kwargs,
208             # we will get a KeyError exception which will cover up the real
209             # exception. So, we update kwargs with the values from args first.
210             # then, we can get 'instance' from kwargs easily.
211             kwargs.update(dict(zip(function.__code__.co_varnames[2:], args)))
212 
213             with excutils.save_and_reraise_exception():
214                 compute_utils.add_instance_fault_from_exc(context,
215                         kwargs['instance'], e, sys.exc_info())
216 
217     return decorated_function
218 
219 
220 @utils.expects_func_args('image_id', 'instance')
221 def delete_image_on_error(function):
222     """Used for snapshot related method to ensure the image created in
223     compute.api is deleted when an error occurs.
224     """
225 
226     @functools.wraps(function)
227     def decorated_function(self, context, image_id, instance,
228                            *args, **kwargs):
229         try:
230             return function(self, context, image_id, instance,
231                             *args, **kwargs)
232         except Exception:
233             with excutils.save_and_reraise_exception():
234                 compute_utils.delete_image(
235                     context, instance, self.image_api, image_id,
236                     log_exc_info=True)
237 
238     return decorated_function
239 
240 
241 # Each collection of events is a dict of eventlet Events keyed by a tuple of
242 # event name and associated tag
243 _InstanceEvents = ty.Dict[ty.Tuple[str, str], eventlet.event.Event]
244 
245 
246 class InstanceEvents(object):
247     def __init__(self):
248         self._events: ty.Optional[ty.Dict[str, _InstanceEvents]] = {}
249 
250     @staticmethod
251     def _lock_name(instance) -> str:
252         return '%s-%s' % (instance.uuid, 'events')
253 
254     def prepare_for_instance_event(
255         self,
256         instance: 'objects.Instance',
257         name: str,
258         tag: str,
259     ) -> eventlet.event.Event:
260         """Prepare to receive an event for an instance.
261 
262         This will register an event for the given instance that we will
263         wait on later. This should be called before initiating whatever
264         action will trigger the event. The resulting eventlet.event.Event
265         object should be wait()'d on to ensure completion.
266 
267         :param instance: the instance for which the event will be generated
268         :param name: the name of the event we're expecting
269         :param tag: the tag associated with the event we're expecting
270         :returns: an event object that should be wait()'d on
271         """
272         @utils.synchronized(self._lock_name(instance))
273         def _create_or_get_event():
274             if self._events is None:
275                 # NOTE(danms): We really should have a more specific error
276                 # here, but this is what we use for our default error case
277                 raise exception.NovaException(
278                     'In shutdown, no new events can be scheduled')
279 
280             instance_events = self._events.setdefault(instance.uuid, {})
281             return instance_events.setdefault((name, tag),
282                                               eventlet.event.Event())
283         LOG.debug('Preparing to wait for external event %(name)s-%(tag)s',
284                   {'name': name, 'tag': tag}, instance=instance)
285         return _create_or_get_event()
286 
287     def pop_instance_event(self, instance, event):
288         """Remove a pending event from the wait list.
289 
290         This will remove a pending event from the wait list so that it
291         can be used to signal the waiters to wake up.
292 
293         :param instance: the instance for which the event was generated
294         :param event: the nova.objects.external_event.InstanceExternalEvent
295                       that describes the event
296         :returns: the eventlet.event.Event object on which the waiters
297                   are blocked
298         """
299         no_events_sentinel = object()
300         no_matching_event_sentinel = object()
301 
302         @utils.synchronized(self._lock_name(instance))
303         def _pop_event():
304             if self._events is None:
305                 LOG.debug('Unexpected attempt to pop events during shutdown',
306                           instance=instance)
307                 return no_events_sentinel
308             events = self._events.get(instance.uuid)
309             if not events:
310                 return no_events_sentinel
311             _event = events.pop((event.name, event.tag), None)
312             if not events:
313                 del self._events[instance.uuid]
314             if _event is None:
315                 return no_matching_event_sentinel
316             return _event
317 
318         result = _pop_event()
319         if result is no_events_sentinel:
320             LOG.debug('No waiting events found dispatching %(event)s',
321                       {'event': event.key},
322                       instance=instance)
323             return None
324         elif result is no_matching_event_sentinel:
325             LOG.debug(
326                 'No event matching %(event)s in %(events)s',
327                 {
328                     'event': event.key,
329                     # mypy can't identify the none check in _pop_event
330                     'events': self._events.get(  # type: ignore
331                         instance.uuid, {}).keys(),
332                 },
333                 instance=instance,
334             )
335             return None
336         else:
337             return result
338 
339     def clear_events_for_instance(self, instance):
340         """Remove all pending events for an instance.
341 
342         This will remove all events currently pending for an instance
343         and return them (indexed by event name).
344 
345         :param instance: the instance for which events should be purged
346         :returns: a dictionary of {event_name: eventlet.event.Event}
347         """
348         @utils.synchronized(self._lock_name(instance))
349         def _clear_events():
350             if self._events is None:
351                 LOG.debug('Unexpected attempt to clear events during shutdown',
352                           instance=instance)
353                 return dict()
354             # NOTE(danms): We have historically returned the raw internal
355             # format here, which is {event.key: [events, ...])} so just
356             # trivially convert it here.
357             return {'%s-%s' % k: e
358                     for k, e in self._events.pop(instance.uuid, {}).items()}
359         return _clear_events()
360 
361     def cancel_all_events(self):
362         if self._events is None:
363             LOG.debug('Unexpected attempt to cancel events during shutdown.')
364             return
365         our_events = self._events
366         # NOTE(danms): Block new events
367         self._events = None
368 
369         for instance_uuid, events in our_events.items():
370             for (name, tag), eventlet_event in events.items():
371                 LOG.debug('Canceling in-flight event %(name)s-%(tag)s for '
372                           'instance %(instance_uuid)s',
373                           {'name': name,
374                            'tag': tag,
375                            'instance_uuid': instance_uuid})
376                 event = objects.InstanceExternalEvent(
377                     instance_uuid=instance_uuid,
378                     name=name, status='failed',
379                     tag=tag, data={})
380                 eventlet_event.send(event)
381 
382 
383 class ComputeVirtAPI(virtapi.VirtAPI):
384     def __init__(self, compute):
385         super(ComputeVirtAPI, self).__init__()
386         self._compute = compute
387         self.reportclient = compute.reportclient
388 
389         class ExitEarly(Exception):
390             def __init__(self, events):
391                 super(Exception, self).__init__()
392                 self.events = events
393 
394         self._exit_early_exc = ExitEarly
395 
396     def exit_wait_early(self, events):
397         """Exit a wait_for_instance_event() immediately and avoid
398         waiting for some events.
399 
400         :param: events: A list of (name, tag) tuples for events that we should
401                         skip waiting for during a wait_for_instance_event().
402         """
403         raise self._exit_early_exc(events=events)
404 
405     def _default_error_callback(self, event_name, instance):
406         raise exception.NovaException(_('Instance event failed'))
407 
408     @contextlib.contextmanager
409     def wait_for_instance_event(self, instance, event_names, deadline=300,
410                                 error_callback=None):
411         """Plan to wait for some events, run some code, then wait.
412 
413         This context manager will first create plans to wait for the
414         provided event_names, yield, and then wait for all the scheduled
415         events to complete.
416 
417         Note that this uses an eventlet.timeout.Timeout to bound the
418         operation, so callers should be prepared to catch that
419         failure and handle that situation appropriately.
420 
421         If the event is not received by the specified timeout deadline,
422         eventlet.timeout.Timeout is raised.
423 
424         If the event is received but did not have a 'completed'
425         status, a NovaException is raised.  If an error_callback is
426         provided, instead of raising an exception as detailed above
427         for the failure case, the callback will be called with the
428         event_name and instance, and can return True to continue
429         waiting for the rest of the events, False to stop processing,
430         or raise an exception which will bubble up to the waiter.
431 
432         If the inner code wishes to abort waiting for one or more
433         events because it knows some state to be finished or condition
434         to be satisfied, it can use VirtAPI.exit_wait_early() with a
435         list of event (name,tag) items to avoid waiting for those
436         events upon context exit. Note that exit_wait_early() exits
437         the context immediately and should be used to signal that all
438         work has been completed and provide the unified list of events
439         that need not be waited for. Waiting for the remaining events
440         will begin immediately upon early exit as if the context was
441         exited normally.
442 
443         :param instance: The instance for which an event is expected
444         :param event_names: A list of event names. Each element is a
445                             tuple of strings to indicate (name, tag),
446                             where name is required, but tag may be None.
447         :param deadline: Maximum number of seconds we should wait for all
448                          of the specified events to arrive.
449         :param error_callback: A function to be called if an event arrives
450 
451         """
452 
453         if error_callback is None:
454             error_callback = self._default_error_callback
455         events = {}
456         for event_name in event_names:
457             name, tag = event_name
458             event_name = objects.InstanceExternalEvent.make_key(name, tag)
459             try:
460                 events[event_name] = (
461                     self._compute.instance_events.prepare_for_instance_event(
462                         instance, name, tag))
463             except exception.NovaException:
464                 error_callback(event_name, instance)
465                 # NOTE(danms): Don't wait for any of the events. They
466                 # should all be canceled and fired immediately below,
467                 # but don't stick around if not.
468                 deadline = 0
469         try:
470             yield
471         except self._exit_early_exc as e:
472             early_events = set([objects.InstanceExternalEvent.make_key(n, t)
473                                 for n, t in e.events])
474         else:
475             early_events = set([])
476 
477         with eventlet.timeout.Timeout(deadline):
478             for event_name, event in events.items():
479                 if event_name in early_events:
480                     continue
481                 else:
482                     actual_event = event.wait()
483                     if actual_event.status == 'completed':
484                         continue
485                 # If we get here, we have an event that was not completed,
486                 # nor skipped via exit_wait_early(). Decide whether to
487                 # keep waiting by calling the error_callback() hook.
488                 decision = error_callback(event_name, instance)
489                 if decision is False:
490                     break
491 
492     def update_compute_provider_status(self, context, rp_uuid, enabled):
493         """Used to add/remove the COMPUTE_STATUS_DISABLED trait on the provider
494 
495         :param context: nova auth RequestContext
496         :param rp_uuid: UUID of a compute node resource provider in Placement
497         :param enabled: True if the node is enabled in which case the trait
498             would be removed, False if the node is disabled in which case
499             the trait would be added.
500         :raises: ResourceProviderTraitRetrievalFailed
501         :raises: ResourceProviderUpdateConflict
502         :raises: ResourceProviderUpdateFailed
503         :raises: TraitRetrievalFailed
504         :raises: keystoneauth1.exceptions.ClientException
505         """
506         trait_name = os_traits.COMPUTE_STATUS_DISABLED
507         # Get the current traits (and generation) for the provider.
508         # TODO(mriedem): Leverage the ProviderTree cache in get_provider_traits
509         trait_info = self.reportclient.get_provider_traits(context, rp_uuid)
510         # If the host is enabled, remove the trait (if set), else add
511         # the trait if it doesn't already exist.
512         original_traits = trait_info.traits
513         new_traits = None
514         if enabled and trait_name in original_traits:
515             new_traits = original_traits - {trait_name}
516             LOG.debug('Removing trait %s from compute node resource '
517                       'provider %s in placement.', trait_name, rp_uuid)
518         elif not enabled and trait_name not in original_traits:
519             new_traits = original_traits | {trait_name}
520             LOG.debug('Adding trait %s to compute node resource '
521                       'provider %s in placement.', trait_name, rp_uuid)
522 
523         if new_traits is not None:
524             self.reportclient.set_traits_for_provider(
525                 context, rp_uuid, new_traits, generation=trait_info.generation)
526 
527 
528 class ComputeManager(manager.Manager):
529     """Manages the running instances from creation to destruction."""
530 
531     target = messaging.Target(version='5.13')
532 
533     def __init__(self, compute_driver=None, *args, **kwargs):
534         """Load configuration options and connect to the hypervisor."""
535         # We want the ComputeManager, ResourceTracker and ComputeVirtAPI all
536         # using the same instance of SchedulerReportClient which has the
537         # ProviderTree cache for this compute service.
538         self.reportclient = report.SchedulerReportClient()
539         self.virtapi = ComputeVirtAPI(self)
540         self.network_api = neutron.API()
541         self.volume_api = cinder.API()
542         self.image_api = glance.API()
543         self._last_bw_usage_poll = 0.0
544         self.compute_api = compute.API()
545         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
546         self.compute_task_api = conductor.ComputeTaskAPI()
547         self.query_client = query.SchedulerQueryClient()
548         self.instance_events = InstanceEvents()
549         self._sync_power_pool = eventlet.GreenPool(
550             size=CONF.sync_power_state_pool_size)
551         self._syncs_in_progress = {}
552         self.send_instance_updates = (
553             CONF.filter_scheduler.track_instance_changes)
554         if CONF.max_concurrent_builds != 0:
555             self._build_semaphore = eventlet.semaphore.Semaphore(
556                 CONF.max_concurrent_builds)
557         else:
558             self._build_semaphore = compute_utils.UnlimitedSemaphore()
559         if CONF.max_concurrent_snapshots > 0:
560             self._snapshot_semaphore = eventlet.semaphore.Semaphore(
561                 CONF.max_concurrent_snapshots)
562         else:
563             self._snapshot_semaphore = compute_utils.UnlimitedSemaphore()
564         if CONF.max_concurrent_live_migrations > 0:
565             self._live_migration_executor = futurist.GreenThreadPoolExecutor(
566                 max_workers=CONF.max_concurrent_live_migrations)
567         else:
568             # CONF.max_concurrent_live_migrations is 0 (unlimited)
569             self._live_migration_executor = futurist.GreenThreadPoolExecutor()
570         # This is a dict, keyed by instance uuid, to a two-item tuple of
571         # migration object and Future for the queued live migration.
572         self._waiting_live_migrations = {}
573 
574         super(ComputeManager, self).__init__(service_name="compute",
575                                              *args, **kwargs)
576 
577         # NOTE(russellb) Load the driver last.  It may call back into the
578         # compute manager via the virtapi, so we want it to be fully
579         # initialized before that happens.
580         self.driver = driver.load_compute_driver(self.virtapi, compute_driver)
581         self.use_legacy_block_device_info = \
582                             self.driver.need_legacy_block_device_info
583         self.rt = resource_tracker.ResourceTracker(
584             self.host, self.driver, reportclient=self.reportclient)
585 
586     def reset(self):
587         LOG.info('Reloading compute RPC API')
588         compute_rpcapi.reset_globals()
589         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
590         self.reportclient.clear_provider_cache()
591 
592     def _update_resource_tracker(self, context, instance):
593         """Let the resource tracker know that an instance has changed state."""
594 
595         if instance.host == self.host:
596             self.rt.update_usage(context, instance, instance.node)
597 
598     def _instance_update(self, context, instance, **kwargs):
599         """Update an instance in the database using kwargs as value."""
600 
601         for k, v in kwargs.items():
602             setattr(instance, k, v)
603         instance.save()
604         self._update_resource_tracker(context, instance)
605 
606     def _nil_out_instance_obj_host_and_node(self, instance):
607         # NOTE(jwcroppe): We don't do instance.save() here for performance
608         # reasons; a call to this is expected to be immediately followed by
609         # another call that does instance.save(), thus avoiding two writes
610         # to the database layer.
611         instance.host = None
612         instance.node = None
613         # ResourceTracker._set_instance_host_and_node also sets launched_on
614         # to the same value as host and is really only ever used by legacy
615         # nova-network code, but we should also null it out to avoid confusion
616         # if there is an instance in the database with no host set but
617         # launched_on is set. Note that we do not care about using launched_on
618         # as some kind of debug helper if diagnosing a build failure, that is
619         # what instance action events are for.
620         instance.launched_on = None
621         # If the instance is not on a host, it's not in an aggregate and
622         # therefore is not in an availability zone.
623         instance.availability_zone = None
624 
625     def _set_instance_obj_error_state(self, instance, clean_task_state=False):
626         try:
627             instance.vm_state = vm_states.ERROR
628             if clean_task_state:
629                 instance.task_state = None
630             instance.save()
631         except exception.InstanceNotFound:
632             LOG.debug('Instance has been destroyed from under us while '
633                       'trying to set it to ERROR', instance=instance)
634 
635     def _get_instances_on_driver(self, context, filters=None):
636         """Return a list of instance records for the instances found
637         on the hypervisor which satisfy the specified filters. If filters=None
638         return a list of instance records for all the instances found on the
639         hypervisor.
640         """
641         if not filters:
642             filters = {}
643         try:
644             driver_uuids = self.driver.list_instance_uuids()
645             if len(driver_uuids) == 0:
646                 # Short circuit, don't waste a DB call
647                 return objects.InstanceList()
648             filters['uuid'] = driver_uuids
649             local_instances = objects.InstanceList.get_by_filters(
650                 context, filters, use_slave=True)
651             return local_instances
652         except NotImplementedError:
653             pass
654 
655         # The driver doesn't support uuids listing, so we'll have
656         # to brute force.
657         driver_instances = self.driver.list_instances()
658         # NOTE(mjozefcz): In this case we need to apply host filter.
659         # Without this all instance data would be fetched from db.
660         filters['host'] = self.host
661         instances = objects.InstanceList.get_by_filters(context, filters,
662                                                         use_slave=True)
663         name_map = {instance.name: instance for instance in instances}
664         local_instances = []
665         for driver_instance in driver_instances:
666             instance = name_map.get(driver_instance)
667             if not instance:
668                 continue
669             local_instances.append(instance)
670         return local_instances
671 
672     def _destroy_evacuated_instances(self, context, node_cache):
673         """Destroys evacuated instances.
674 
675         While nova-compute was down, the instances running on it could be
676         evacuated to another host. This method looks for evacuation migration
677         records where this is the source host and which were either started
678         (accepted), in-progress (pre-migrating) or migrated (done). From those
679         migration records, local instances reported by the hypervisor are
680         compared to the instances for the migration records and those local
681         guests are destroyed, along with instance allocation records in
682         Placement for this node.
683         Then allocations are removed from Placement for every instance that is
684         evacuated from this host regardless if the instance is reported by the
685         hypervisor or not.
686 
687         :param context: The request context
688         :param node_cache: A dict of ComputeNode objects keyed by the UUID of
689             the compute node
690         :return: A dict keyed by instance uuid mapped to Migration objects
691             for instances that were migrated away from this host
692         """
693         filters = {
694             'source_compute': self.host,
695             # NOTE(mriedem): Migration records that have been accepted are
696             # included in case the source node comes back up while instances
697             # are being evacuated to another host. We don't want the same
698             # instance being reported from multiple hosts.
699             # NOTE(lyarwood): pre-migrating is also included here as the
700             # source compute can come back online shortly after the RT
701             # claims on the destination that in-turn moves the migration to
702             # pre-migrating. If the evacuate fails on the destination host,
703             # the user can rebuild the instance (in ERROR state) on the source
704             # host.
705             'status': ['accepted', 'pre-migrating', 'done'],
706             'migration_type': fields.MigrationType.EVACUATION,
707         }
708         with utils.temporary_mutation(context, read_deleted='yes'):
709             evacuations = objects.MigrationList.get_by_filters(context,
710                                                                filters)
711         if not evacuations:
712             return {}
713         evacuations = {mig.instance_uuid: mig for mig in evacuations}
714 
715         # TODO(mriedem): We could optimize by pre-loading the joined fields
716         # we know we'll use, like info_cache and flavor.
717         local_instances = self._get_instances_on_driver(context)
718         evacuated_local_instances = {inst.uuid: inst
719                                      for inst in local_instances
720                                      if inst.uuid in evacuations}
721 
722         for instance in evacuated_local_instances.values():
723             LOG.info('Destroying instance as it has been evacuated from '
724                      'this host but still exists in the hypervisor',
725                      instance=instance)
726             try:
727                 network_info = self.network_api.get_instance_nw_info(
728                     context, instance)
729                 bdi = self._get_instance_block_device_info(context,
730                                                            instance)
731                 destroy_disks = not (self._is_instance_storage_shared(
732                     context, instance))
733             except exception.InstanceNotFound:
734                 network_info = network_model.NetworkInfo()
735                 bdi = {}
736                 LOG.info('Instance has been marked deleted already, '
737                          'removing it from the hypervisor.',
738                          instance=instance)
739                 # always destroy disks if the instance was deleted
740                 destroy_disks = True
741             self.driver.destroy(context, instance,
742                                 network_info,
743                                 bdi, destroy_disks)
744 
745         hostname_to_cn_uuid = {
746             cn.hypervisor_hostname: cn.uuid
747             for cn in node_cache.values()}
748 
749         for instance_uuid, migration in evacuations.items():
750             try:
751                 if instance_uuid in evacuated_local_instances:
752                     # Avoid the db call if we already have the instance loaded
753                     # above
754                     instance = evacuated_local_instances[instance_uuid]
755                 else:
756                     instance = objects.Instance.get_by_uuid(
757                         context, instance_uuid)
758             except exception.InstanceNotFound:
759                 # The instance already deleted so we expect that every
760                 # allocation of that instance has already been cleaned up
761                 continue
762 
763             LOG.info('Cleaning up allocations of the instance as it has been '
764                      'evacuated from this host',
765                      instance=instance)
766             if migration.source_node not in hostname_to_cn_uuid:
767                 LOG.error("Failed to clean allocation of evacuated "
768                           "instance as the source node %s is not found",
769                           migration.source_node, instance=instance)
770                 continue
771             cn_uuid = hostname_to_cn_uuid[migration.source_node]
772 
773             # If the instance was deleted in the interim, assume its
774             # allocations were properly cleaned up (either by its hosting
775             # compute service or the API).
776             if (not instance.deleted and
777                     not self.reportclient.
778                         remove_provider_tree_from_instance_allocation(
779                             context, instance.uuid, cn_uuid)):
780                 LOG.error("Failed to clean allocation of evacuated instance "
781                           "on the source node %s",
782                           cn_uuid, instance=instance)
783 
784             migration.status = 'completed'
785             migration.save()
786         return evacuations
787 
788     def _is_instance_storage_shared(self, context, instance, host=None):
789         shared_storage = True
790         data = None
791         try:
792             data = self.driver.check_instance_shared_storage_local(context,
793                                                        instance)
794             if data:
795                 shared_storage = (self.compute_rpcapi.
796                                   check_instance_shared_storage(context,
797                                   instance, data, host=host))
798         except NotImplementedError:
799             LOG.debug('Hypervisor driver does not support '
800                       'instance shared storage check, '
801                       'assuming it\'s not on shared storage',
802                       instance=instance)
803             shared_storage = False
804         except Exception:
805             LOG.exception('Failed to check if instance shared',
806                           instance=instance)
807         finally:
808             if data:
809                 self.driver.check_instance_shared_storage_cleanup(context,
810                                                                   data)
811         return shared_storage
812 
813     def _complete_partial_deletion(self, context, instance):
814         """Complete deletion for instances in DELETED status but not marked as
815         deleted in the DB
816         """
817         instance.destroy()
818         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
819                 context, instance.uuid)
820         self._complete_deletion(context,
821                                 instance)
822         self._notify_about_instance_usage(context, instance, "delete.end")
823         compute_utils.notify_about_instance_action(context, instance,
824                 self.host, action=fields.NotificationAction.DELETE,
825                 phase=fields.NotificationPhase.END, bdms=bdms)
826 
827     def _complete_deletion(self, context, instance):
828         self._update_resource_tracker(context, instance)
829 
830         self.reportclient.delete_allocation_for_instance(context,
831                                                          instance.uuid)
832 
833         self._clean_instance_console_tokens(context, instance)
834         self._delete_scheduler_instance_info(context, instance.uuid)
835 
836     def _validate_pinning_configuration(self, instances):
837         if not self.driver.capabilities.get('supports_pcpus', False):
838             return
839 
840         for instance in instances:
841             # ignore deleted instances
842             if instance.deleted:
843                 continue
844 
845             # if this is an unpinned instance and the host only has
846             # 'cpu_dedicated_set' configured, we need to tell the operator to
847             # correct their configuration
848             if not instance.numa_topology or (
849                 instance.numa_topology.cpu_policy in (
850                     None, fields.CPUAllocationPolicy.SHARED
851                 )
852             ):
853                 # we don't need to check 'vcpu_pin_set' since it can't coexist
854                 # alongside 'cpu_dedicated_set'
855                 if (CONF.compute.cpu_dedicated_set and
856                         not CONF.compute.cpu_shared_set):
857                     msg = _("This host has unpinned instances but has no CPUs "
858                             "set aside for this purpose; configure '[compute] "
859                             "cpu_shared_set' instead of, or in addition to, "
860                             "'[compute] cpu_dedicated_set'")
861                     raise exception.InvalidConfiguration(msg)
862 
863                 continue
864 
865             # ditto for pinned instances if only 'cpu_shared_set' is configured
866             if (CONF.compute.cpu_shared_set and
867                     not CONF.compute.cpu_dedicated_set and
868                     not CONF.vcpu_pin_set):
869                 msg = _("This host has pinned instances but has no CPUs "
870                         "set aside for this purpose; configure '[compute] "
871                         "cpu_dedicated_set' instead of, or in addition to, "
872                         "'[compute] cpu_shared_set'.")
873                 raise exception.InvalidConfiguration(msg)
874 
875             # if this is a mixed instance with both pinned and unpinned CPUs,
876             # the host must have both 'cpu_dedicated_set' and 'cpu_shared_set'
877             # configured. check if 'cpu_shared_set' is set.
878             if (instance.numa_topology.cpu_policy ==
879                     fields.CPUAllocationPolicy.MIXED and
880                     not CONF.compute.cpu_shared_set):
881                 msg = _("This host has mixed instance requesting both pinned "
882                         "and unpinned CPUs but hasn't set aside unpinned CPUs "
883                         "for this purpose; Configure "
884                         "'[compute] cpu_shared_set'.")
885                 raise exception.InvalidConfiguration(msg)
886 
887             # for mixed instance check if 'cpu_dedicated_set' is set.
888             if (instance.numa_topology.cpu_policy ==
889                     fields.CPUAllocationPolicy.MIXED and
890                     not CONF.compute.cpu_dedicated_set):
891                 msg = _("This host has mixed instance requesting both pinned "
892                         "and unpinned CPUs but hasn't set aside pinned CPUs "
893                         "for this purpose; Configure "
894                         "'[compute] cpu_dedicated_set'")
895                 raise exception.InvalidConfiguration(msg)
896 
897             # also check to make sure the operator hasn't accidentally
898             # dropped some cores that instances are currently using
899             available_dedicated_cpus = (hardware.get_vcpu_pin_set() or
900                                         hardware.get_cpu_dedicated_set())
901             pinned_cpus = instance.numa_topology.cpu_pinning
902             if available_dedicated_cpus and (
903                     pinned_cpus - available_dedicated_cpus):
904                 # we can't raise an exception because of bug #1289064,
905                 # which meant we didn't recalculate CPU pinning information
906                 # when we live migrated a pinned instance
907                 LOG.warning(
908                     "Instance is pinned to host CPUs %(cpus)s "
909                     "but one or more of these CPUs are not included in "
910                     "either '[compute] cpu_dedicated_set' or "
911                     "'vcpu_pin_set'; you should update these "
912                     "configuration options to include the missing CPUs "
913                     "or rebuild or cold migrate this instance.",
914                     {'cpus': list(pinned_cpus)},
915                     instance=instance)
916 
917     def _validate_vtpm_configuration(self, instances):
918         if self.driver.capabilities.get('supports_vtpm', False):
919             return
920 
921         for instance in instances:
922             if instance.deleted:
923                 continue
924 
925             # NOTE(stephenfin): We don't have an attribute on the instance to
926             # check for this, so we need to inspect the flavor/image metadata
927             if hardware.get_vtpm_constraint(
928                 instance.flavor, instance.image_meta,
929             ):
930                 msg = _(
931                     'This host has instances with the vTPM feature enabled, '
932                     'but the host is not correctly configured; enable '
933                     'vTPM support.'
934                 )
935                 raise exception.InvalidConfiguration(msg)
936 
937     def _reset_live_migration(self, context, instance):
938         migration = None
939         try:
940             migration = objects.Migration.get_by_instance_and_status(
941                                       context, instance.uuid, 'running')
942             if migration:
943                 self.live_migration_abort(context, instance, migration.id)
944         except Exception:
945             LOG.exception('Failed to abort live-migration',
946                           instance=instance)
947         finally:
948             if migration:
949                 self._set_migration_status(migration, 'error')
950             LOG.info('Instance found in migrating state during '
951                      'startup. Resetting task_state',
952                      instance=instance)
953             instance.task_state = None
954             instance.save(expected_task_state=[task_states.MIGRATING])
955 
956     def _init_instance(self, context, instance):
957         """Initialize this instance during service init."""
958 
959         # NOTE(danms): If the instance appears to not be owned by this
960         # host, it may have been evacuated away, but skipped by the
961         # evacuation cleanup code due to configuration. Thus, if that
962         # is a possibility, don't touch the instance in any way, but
963         # log the concern. This will help avoid potential issues on
964         # startup due to misconfiguration.
965         if instance.host != self.host:
966             LOG.warning('Instance %(uuid)s appears to not be owned '
967                         'by this host, but by %(host)s. Startup '
968                         'processing is being skipped.',
969                         {'uuid': instance.uuid,
970                          'host': instance.host})
971             return
972 
973         # Instances that are shut down, or in an error state can not be
974         # initialized and are not attempted to be recovered. The exception
975         # to this are instances that are in RESIZE_MIGRATING or DELETING,
976         # which are dealt with further down.
977         if (instance.vm_state == vm_states.SOFT_DELETED or
978             (instance.vm_state == vm_states.ERROR and
979             instance.task_state not in
980             (task_states.RESIZE_MIGRATING, task_states.DELETING))):
981             LOG.debug("Instance is in %s state.",
982                       instance.vm_state, instance=instance)
983             return
984 
985         if instance.vm_state == vm_states.DELETED:
986             try:
987                 self._complete_partial_deletion(context, instance)
988             except Exception:
989                 # we don't want that an exception blocks the init_host
990                 LOG.exception('Failed to complete a deletion',
991                               instance=instance)
992             return
993 
994         if (instance.vm_state == vm_states.BUILDING or
995             instance.task_state in [task_states.SCHEDULING,
996                                     task_states.BLOCK_DEVICE_MAPPING,
997                                     task_states.NETWORKING,
998                                     task_states.SPAWNING]):
999             # NOTE(dave-mcnally) compute stopped before instance was fully
1000             # spawned so set to ERROR state. This is safe to do as the state
1001             # may be set by the api but the host is not so if we get here the
1002             # instance has already been scheduled to this particular host.
1003             LOG.debug("Instance failed to spawn correctly, "
1004                       "setting to ERROR state", instance=instance)
1005             self._set_instance_obj_error_state(instance, clean_task_state=True)
1006             return
1007 
1008         if (instance.vm_state in [vm_states.ACTIVE, vm_states.STOPPED] and
1009             instance.task_state in [task_states.REBUILDING,
1010                                     task_states.REBUILD_BLOCK_DEVICE_MAPPING,
1011                                     task_states.REBUILD_SPAWNING]):
1012             # NOTE(jichenjc) compute stopped before instance was fully
1013             # spawned so set to ERROR state. This is consistent to BUILD
1014             LOG.debug("Instance failed to rebuild correctly, "
1015                       "setting to ERROR state", instance=instance)
1016             self._set_instance_obj_error_state(instance, clean_task_state=True)
1017             return
1018 
1019         if (instance.vm_state != vm_states.ERROR and
1020             instance.task_state in [task_states.IMAGE_SNAPSHOT_PENDING,
1021                                     task_states.IMAGE_PENDING_UPLOAD,
1022                                     task_states.IMAGE_UPLOADING,
1023                                     task_states.IMAGE_SNAPSHOT]):
1024             LOG.debug("Instance in transitional state %s at start-up "
1025                       "clearing task state",
1026                       instance.task_state, instance=instance)
1027             instance.task_state = None
1028             instance.save()
1029 
1030         if (instance.vm_state != vm_states.ERROR and
1031             instance.task_state in [task_states.RESIZE_PREP]):
1032             LOG.debug("Instance in transitional state %s at start-up "
1033                       "clearing task state",
1034                       instance['task_state'], instance=instance)
1035             instance.task_state = None
1036             instance.save()
1037 
1038         if instance.task_state == task_states.DELETING:
1039             try:
1040                 LOG.info('Service started deleting the instance during '
1041                          'the previous run, but did not finish. Restarting'
1042                          ' the deletion now.', instance=instance)
1043                 instance.obj_load_attr('metadata')
1044                 instance.obj_load_attr('system_metadata')
1045                 bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
1046                         context, instance.uuid)
1047                 self._delete_instance(context, instance, bdms)
1048             except Exception:
1049                 # we don't want that an exception blocks the init_host
1050                 LOG.exception('Failed to complete a deletion',
1051                               instance=instance)
1052                 self._set_instance_obj_error_state(instance)
1053             return
1054 
1055         current_power_state = self._get_power_state(instance)
1056         try_reboot, reboot_type = self._retry_reboot(
1057             instance, current_power_state)
1058 
1059         if try_reboot:
1060             LOG.debug("Instance in transitional state (%(task_state)s) at "
1061                       "start-up and power state is (%(power_state)s), "
1062                       "triggering reboot",
1063                       {'task_state': instance.task_state,
1064                        'power_state': current_power_state},
1065                       instance=instance)
1066 
1067             # NOTE(mikal): if the instance was doing a soft reboot that got as
1068             # far as shutting down the instance but not as far as starting it
1069             # again, then we've just become a hard reboot. That means the
1070             # task state for the instance needs to change so that we're in one
1071             # of the expected task states for a hard reboot.
1072             if (instance.task_state in task_states.soft_reboot_states and
1073                 reboot_type == 'HARD'):
1074                 instance.task_state = task_states.REBOOT_PENDING_HARD
1075                 instance.save()
1076 
1077             self.reboot_instance(context, instance, block_device_info=None,
1078                                  reboot_type=reboot_type)
1079             return
1080 
1081         elif (current_power_state == power_state.RUNNING and
1082               instance.task_state in [task_states.REBOOT_STARTED,
1083                                       task_states.REBOOT_STARTED_HARD,
1084                                       task_states.PAUSING,
1085                                       task_states.UNPAUSING]):
1086             LOG.warning("Instance in transitional state "
1087                         "(%(task_state)s) at start-up and power state "
1088                         "is (%(power_state)s), clearing task state",
1089                         {'task_state': instance.task_state,
1090                          'power_state': current_power_state},
1091                         instance=instance)
1092             instance.task_state = None
1093             instance.vm_state = vm_states.ACTIVE
1094             instance.save()
1095         elif (current_power_state == power_state.PAUSED and
1096               instance.task_state == task_states.UNPAUSING):
1097             LOG.warning("Instance in transitional state "
1098                         "(%(task_state)s) at start-up and power state "
1099                         "is (%(power_state)s), clearing task state "
1100                         "and unpausing the instance",
1101                         {'task_state': instance.task_state,
1102                          'power_state': current_power_state},
1103                         instance=instance)
1104             try:
1105                 self.unpause_instance(context, instance)
1106             except NotImplementedError:
1107                 # Some virt driver didn't support pause and unpause
1108                 pass
1109             except Exception:
1110                 LOG.exception('Failed to unpause instance', instance=instance)
1111             return
1112 
1113         if instance.task_state == task_states.POWERING_OFF:
1114             try:
1115                 LOG.debug("Instance in transitional state %s at start-up "
1116                           "retrying stop request",
1117                           instance.task_state, instance=instance)
1118                 self.stop_instance(context, instance, True)
1119             except Exception:
1120                 # we don't want that an exception blocks the init_host
1121                 LOG.exception('Failed to stop instance', instance=instance)
1122             return
1123 
1124         if instance.task_state == task_states.POWERING_ON:
1125             try:
1126                 LOG.debug("Instance in transitional state %s at start-up "
1127                           "retrying start request",
1128                           instance.task_state, instance=instance)
1129                 self.start_instance(context, instance)
1130             except Exception:
1131                 # we don't want that an exception blocks the init_host
1132                 LOG.exception('Failed to start instance', instance=instance)
1133             return
1134 
1135         net_info = instance.get_network_info()
1136         try:
1137             self.driver.plug_vifs(instance, net_info)
1138         except NotImplementedError as e:
1139             LOG.debug(e, instance=instance)
1140         except exception.VirtualInterfacePlugException:
1141             # NOTE(mriedem): If we get here, it could be because the vif_type
1142             # in the cache is "binding_failed" or "unbound".
1143             # The periodic task _heal_instance_info_cache checks for this
1144             # condition. It should fix this by binding the ports again when
1145             # it gets to this instance.
1146             LOG.exception('Virtual interface plugging failed for instance. '
1147                           'The port binding:host_id may need to be manually '
1148                           'updated.', instance=instance)
1149             self._set_instance_obj_error_state(instance)
1150             return
1151 
1152         if instance.task_state == task_states.RESIZE_MIGRATING:
1153             # We crashed during resize/migration, so roll back for safety
1154             try:
1155                 # NOTE(mriedem): check old_vm_state for STOPPED here, if it's
1156                 # not in system_metadata we default to True for backwards
1157                 # compatibility
1158                 power_on = (instance.system_metadata.get('old_vm_state') !=
1159                             vm_states.STOPPED)
1160 
1161                 block_dev_info = self._get_instance_block_device_info(context,
1162                                                                       instance)
1163 
1164                 migration = objects.Migration.get_by_id_and_instance(
1165                     context, instance.migration_context.migration_id,
1166                     instance.uuid)
1167                 self.driver.finish_revert_migration(context, instance,
1168                     net_info, migration, block_dev_info, power_on)
1169 
1170             except Exception:
1171                 LOG.exception('Failed to revert crashed migration',
1172                               instance=instance)
1173             finally:
1174                 LOG.info('Instance found in migrating state during '
1175                          'startup. Resetting task_state',
1176                          instance=instance)
1177                 instance.task_state = None
1178                 instance.save()
1179         if instance.task_state == task_states.MIGRATING:
1180             # Live migration did not complete, but instance is on this
1181             # host. Abort ongoing migration if still running and reset state.
1182             self._reset_live_migration(context, instance)
1183 
1184         db_state = instance.power_state
1185         drv_state = self._get_power_state(instance)
1186         expect_running = (db_state == power_state.RUNNING and
1187                           drv_state != db_state)
1188 
1189         LOG.debug('Current state is %(drv_state)s, state in DB is '
1190                   '%(db_state)s.',
1191                   {'drv_state': drv_state, 'db_state': db_state},
1192                   instance=instance)
1193 
1194         if expect_running and CONF.resume_guests_state_on_host_boot:
1195             self._resume_guests_state(context, instance, net_info)
1196 
1197     def _resume_guests_state(self, context, instance, net_info):
1198         LOG.info('Rebooting instance after nova-compute restart.',
1199                  instance=instance)
1200         block_device_info = \
1201             self._get_instance_block_device_info(context, instance)
1202 
1203         try:
1204             self.driver.resume_state_on_host_boot(
1205                 context, instance, net_info, block_device_info)
1206         except NotImplementedError:
1207             LOG.warning('Hypervisor driver does not support '
1208                         'resume guests', instance=instance)
1209         except Exception:
1210             # NOTE(vish): The instance failed to resume, so we set the
1211             #             instance to error and attempt to continue.
1212             LOG.warning('Failed to resume instance',
1213                         instance=instance)
1214             self._set_instance_obj_error_state(instance)
1215 
1216     def _retry_reboot(self, instance, current_power_state):
1217         current_task_state = instance.task_state
1218         retry_reboot = False
1219         reboot_type = compute_utils.get_reboot_type(current_task_state,
1220                                                     current_power_state)
1221 
1222         pending_soft = (
1223             current_task_state == task_states.REBOOT_PENDING and
1224             instance.vm_state in vm_states.ALLOW_SOFT_REBOOT)
1225         pending_hard = (
1226             current_task_state == task_states.REBOOT_PENDING_HARD and
1227             instance.vm_state in vm_states.ALLOW_HARD_REBOOT)
1228         started_not_running = (current_task_state in
1229                                [task_states.REBOOT_STARTED,
1230                                 task_states.REBOOT_STARTED_HARD] and
1231                                current_power_state != power_state.RUNNING)
1232 
1233         if pending_soft or pending_hard or started_not_running:
1234             retry_reboot = True
1235 
1236         return retry_reboot, reboot_type
1237 
1238     def handle_lifecycle_event(self, event):
1239         LOG.info("VM %(state)s (Lifecycle Event)",
1240                  {'state': event.get_name()},
1241                  instance_uuid=event.get_instance_uuid())
1242         context = nova.context.get_admin_context(read_deleted='yes')
1243         vm_power_state = None
1244         event_transition = event.get_transition()
1245         if event_transition == virtevent.EVENT_LIFECYCLE_STOPPED:
1246             vm_power_state = power_state.SHUTDOWN
1247         elif event_transition == virtevent.EVENT_LIFECYCLE_STARTED:
1248             vm_power_state = power_state.RUNNING
1249         elif event_transition in (
1250                 virtevent.EVENT_LIFECYCLE_PAUSED,
1251                 virtevent.EVENT_LIFECYCLE_POSTCOPY_STARTED,
1252                 virtevent.EVENT_LIFECYCLE_MIGRATION_COMPLETED):
1253             vm_power_state = power_state.PAUSED
1254         elif event_transition == virtevent.EVENT_LIFECYCLE_RESUMED:
1255             vm_power_state = power_state.RUNNING
1256         elif event_transition == virtevent.EVENT_LIFECYCLE_SUSPENDED:
1257             vm_power_state = power_state.SUSPENDED
1258         else:
1259             LOG.warning("Unexpected lifecycle event: %d", event_transition)
1260 
1261         migrate_finish_statuses = {
1262             # This happens on the source node and indicates live migration
1263             # entered post-copy mode.
1264             virtevent.EVENT_LIFECYCLE_POSTCOPY_STARTED: 'running (post-copy)',
1265             # Suspended for offline migration.
1266             virtevent.EVENT_LIFECYCLE_MIGRATION_COMPLETED: 'running'
1267         }
1268 
1269         expected_attrs = []
1270         if event_transition in migrate_finish_statuses:
1271             # Join on info_cache since that's needed in migrate_instance_start.
1272             expected_attrs.append('info_cache')
1273         instance = objects.Instance.get_by_uuid(context,
1274                                                 event.get_instance_uuid(),
1275                                                 expected_attrs=expected_attrs)
1276 
1277         # Note(lpetrut): The event may be delayed, thus not reflecting
1278         # the current instance power state. In that case, ignore the event.
1279         current_power_state = self._get_power_state(instance)
1280         if current_power_state == vm_power_state:
1281             LOG.debug('Synchronizing instance power state after lifecycle '
1282                       'event "%(event)s"; current vm_state: %(vm_state)s, '
1283                       'current task_state: %(task_state)s, current DB '
1284                       'power_state: %(db_power_state)s, VM power_state: '
1285                       '%(vm_power_state)s',
1286                       {'event': event.get_name(),
1287                        'vm_state': instance.vm_state,
1288                        'task_state': instance.task_state,
1289                        'db_power_state': instance.power_state,
1290                        'vm_power_state': vm_power_state},
1291                       instance_uuid=instance.uuid)
1292             self._sync_instance_power_state(context,
1293                                             instance,
1294                                             vm_power_state)
1295 
1296         # The following checks are for live migration. We want to activate
1297         # the port binding for the destination host before the live migration
1298         # is resumed on the destination host in order to reduce network
1299         # downtime. Otherwise the ports are bound to the destination host
1300         # in post_live_migration_at_destination.
1301         # TODO(danms): Explore options for using a different live migration
1302         # specific callback for this instead of piggy-backing on the
1303         # handle_lifecycle_event callback.
1304         if (instance.task_state == task_states.MIGRATING and
1305                 event_transition in migrate_finish_statuses):
1306             status = migrate_finish_statuses[event_transition]
1307             try:
1308                 migration = objects.Migration.get_by_instance_and_status(
1309                             context, instance.uuid, status)
1310                 LOG.debug('Binding ports to destination host: %s',
1311                           migration.dest_compute, instance=instance)
1312                 # For neutron, migrate_instance_start will activate the
1313                 # destination host port bindings, if there are any created by
1314                 # conductor before live migration started.
1315                 self.network_api.migrate_instance_start(
1316                     context, instance, migration)
1317             except exception.MigrationNotFoundByStatus:
1318                 LOG.warning("Unable to find migration record with status "
1319                             "'%s' for instance. Port binding will happen in "
1320                             "post live migration.", status, instance=instance)
1321 
1322     def handle_events(self, event):
1323         if isinstance(event, virtevent.LifecycleEvent):
1324             try:
1325                 self.handle_lifecycle_event(event)
1326             except exception.InstanceNotFound:
1327                 LOG.debug("Event %s arrived for non-existent instance. The "
1328                           "instance was probably deleted.", event)
1329         else:
1330             LOG.debug("Ignoring event %s", event)
1331 
1332     def init_virt_events(self):
1333         if CONF.workarounds.handle_virt_lifecycle_events:
1334             self.driver.register_event_listener(self.handle_events)
1335         else:
1336             # NOTE(mriedem): If the _sync_power_states periodic task is
1337             # disabled we should emit a warning in the logs.
1338             if CONF.sync_power_state_interval < 0:
1339                 LOG.warning('Instance lifecycle events from the compute '
1340                             'driver have been disabled. Note that lifecycle '
1341                             'changes to an instance outside of the compute '
1342                             'service will not be synchronized '
1343                             'automatically since the _sync_power_states '
1344                             'periodic task is also disabled.')
1345             else:
1346                 LOG.info('Instance lifecycle events from the compute '
1347                          'driver have been disabled. Note that lifecycle '
1348                          'changes to an instance outside of the compute '
1349                          'service will only be synchronized by the '
1350                          '_sync_power_states periodic task.')
1351 
1352     def _get_nodes(self, context):
1353         """Queried the ComputeNode objects from the DB that are reported by the
1354         hypervisor.
1355 
1356         :param context: the request context
1357         :return: a dict of ComputeNode objects keyed by the UUID of the given
1358             node.
1359         """
1360         nodes_by_uuid = {}
1361         try:
1362             node_names = self.driver.get_available_nodes()
1363         except exception.VirtDriverNotReady:
1364             LOG.warning(
1365                 "Virt driver is not ready. If this is the first time this "
1366                 "service is starting on this host, then you can ignore this "
1367                 "warning.")
1368             return {}
1369 
1370         for node_name in node_names:
1371             try:
1372                 node = objects.ComputeNode.get_by_host_and_nodename(
1373                     context, self.host, node_name)
1374                 nodes_by_uuid[node.uuid] = node
1375             except exception.ComputeHostNotFound:
1376                 LOG.warning(
1377                     "Compute node %s not found in the database. If this is "
1378                     "the first time this service is starting on this host, "
1379                     "then you can ignore this warning.", node_name)
1380         return nodes_by_uuid
1381 
1382     def init_host(self):
1383         """Initialization for a standalone compute service."""
1384 
1385         if CONF.pci.passthrough_whitelist:
1386             # Simply loading the PCI passthrough whitelist will do a bunch of
1387             # validation that would otherwise wait until the PciDevTracker is
1388             # constructed when updating available resources for the compute
1389             # node(s) in the resource tracker, effectively killing that task.
1390             # So load up the whitelist when starting the compute service to
1391             # flush any invalid configuration early so we can kill the service
1392             # if the configuration is wrong.
1393             whitelist.Whitelist(CONF.pci.passthrough_whitelist)
1394 
1395         nova.conf.neutron.register_dynamic_opts(CONF)
1396         # Even if only libvirt uses them, make it available for all drivers
1397         nova.conf.devices.register_dynamic_opts(CONF)
1398 
1399         # Override the number of concurrent disk operations allowed if the
1400         # user has specified a limit.
1401         if CONF.compute.max_concurrent_disk_ops != 0:
1402             compute_utils.disk_ops_semaphore = \
1403                 eventlet.semaphore.BoundedSemaphore(
1404                     CONF.compute.max_concurrent_disk_ops)
1405 
1406         self.driver.init_host(host=self.host)
1407         context = nova.context.get_admin_context()
1408         instances = objects.InstanceList.get_by_host(
1409             context, self.host,
1410             expected_attrs=['info_cache', 'metadata', 'numa_topology'])
1411 
1412         self.init_virt_events()
1413 
1414         self._validate_pinning_configuration(instances)
1415         self._validate_vtpm_configuration(instances)
1416 
1417         # NOTE(gibi): At this point the compute_nodes of the resource tracker
1418         # has not been populated yet so we cannot rely on the resource tracker
1419         # here.
1420         # NOTE(gibi): If ironic and vcenter virt driver slow start time
1421         # becomes problematic here then we should consider adding a config
1422         # option or a driver flag to tell us if we should thread
1423         # _destroy_evacuated_instances and
1424         # _error_out_instances_whose_build_was_interrupted out in the
1425         # background on startup
1426         nodes_by_uuid = self._get_nodes(context)
1427 
1428         try:
1429             # checking that instance was not already evacuated to other host
1430             evacuated_instances = self._destroy_evacuated_instances(
1431                 context, nodes_by_uuid)
1432 
1433             # Initialise instances on the host that are not evacuating
1434             for instance in instances:
1435                 if instance.uuid not in evacuated_instances:
1436                     self._init_instance(context, instance)
1437 
1438             # NOTE(gibi): collect all the instance uuids that is in some way
1439             # was already handled above. Either by init_instance or by
1440             # _destroy_evacuated_instances. This way we can limit the scope of
1441             # the _error_out_instances_whose_build_was_interrupted call to look
1442             # only for instances that have allocations on this node and not
1443             # handled by the above calls.
1444             already_handled = {instance.uuid for instance in instances}.union(
1445                 evacuated_instances)
1446             self._error_out_instances_whose_build_was_interrupted(
1447                 context, already_handled, nodes_by_uuid.keys())
1448 
1449         finally:
1450             if instances:
1451                 # We only send the instance info to the scheduler on startup
1452                 # if there is anything to send, otherwise this host might
1453                 # not be mapped yet in a cell and the scheduler may have
1454                 # issues dealing with the information. Later changes to
1455                 # instances on this host will update the scheduler, or the
1456                 # _sync_scheduler_instance_info periodic task will.
1457                 self._update_scheduler_instance_info(context, instances)
1458 
1459     def _error_out_instances_whose_build_was_interrupted(
1460             self, context, already_handled_instances, node_uuids):
1461         """If there are instances in BUILDING state that are not
1462         assigned to this host but have allocations in placement towards
1463         this compute that means the nova-compute service was
1464         restarted while those instances waited for the resource claim
1465         to finish and the _set_instance_host_and_node() to update the
1466         instance.host field. We need to push them to ERROR state here to
1467         prevent keeping them in BUILDING state forever.
1468 
1469         :param context: The request context
1470         :param already_handled_instances: The set of instance UUIDs that the
1471             host initialization process already handled in some way.
1472         :param node_uuids: The list of compute node uuids handled by this
1473             service
1474         """
1475 
1476         # Strategy:
1477         # 1) Get the allocations from placement for our compute node(s)
1478         # 2) Remove the already handled instances from the consumer list;
1479         #    they are either already initialized or need to be skipped.
1480         # 3) Check which remaining consumer is an instance in BUILDING state
1481         #    and push it to ERROR state.
1482 
1483         LOG.info(
1484             "Looking for unclaimed instances stuck in BUILDING status for "
1485             "nodes managed by this host")
1486         for cn_uuid in node_uuids:
1487             try:
1488                 f = self.reportclient.get_allocations_for_resource_provider
1489                 allocations = f(context, cn_uuid).allocations
1490             except (exception.ResourceProviderAllocationRetrievalFailed,
1491                     keystone_exception.ClientException) as e:
1492                 LOG.error(
1493                     "Could not retrieve compute node resource provider %s and "
1494                     "therefore unable to error out any instances stuck in "
1495                     "BUILDING state. Error: %s", cn_uuid, str(e))
1496                 continue
1497 
1498             not_handled_consumers = (set(allocations) -
1499                                      already_handled_instances)
1500 
1501             if not not_handled_consumers:
1502                 continue
1503 
1504             filters = {
1505                 'vm_state': vm_states.BUILDING,
1506                 'uuid': not_handled_consumers
1507             }
1508 
1509             instances = objects.InstanceList.get_by_filters(
1510                 context, filters, expected_attrs=[])
1511 
1512             for instance in instances:
1513                 LOG.debug(
1514                     "Instance spawn was interrupted before instance_claim, "
1515                     "setting instance to ERROR state", instance=instance)
1516                 self._set_instance_obj_error_state(
1517                     instance, clean_task_state=True)
1518 
1519     def cleanup_host(self):
1520         self.driver.register_event_listener(None)
1521         self.instance_events.cancel_all_events()
1522         self.driver.cleanup_host(host=self.host)
1523         self._cleanup_live_migrations_in_pool()
1524 
1525     def _cleanup_live_migrations_in_pool(self):
1526         # Shutdown the pool so we don't get new requests.
1527         self._live_migration_executor.shutdown(wait=False)
1528         # For any queued migrations, cancel the migration and update
1529         # its status.
1530         for migration, future in self._waiting_live_migrations.values():
1531             # If we got here before the Future was submitted then we need
1532             # to move on since there isn't anything we can do.
1533             if future is None:
1534                 continue
1535             if future.cancel():
1536                 self._set_migration_status(migration, 'cancelled')
1537                 LOG.info('Successfully cancelled queued live migration.',
1538                          instance_uuid=migration.instance_uuid)
1539             else:
1540                 LOG.warning('Unable to cancel live migration.',
1541                             instance_uuid=migration.instance_uuid)
1542         self._waiting_live_migrations.clear()
1543 
1544     def pre_start_hook(self):
1545         """After the service is initialized, but before we fully bring
1546         the service up by listening on RPC queues, make sure to update
1547         our available resources (and indirectly our available nodes).
1548         """
1549         self.update_available_resource(nova.context.get_admin_context(),
1550                                        startup=True)
1551 
1552     def _get_power_state(self, instance):
1553         """Retrieve the power state for the given instance."""
1554         LOG.debug('Checking state', instance=instance)
1555         try:
1556             return self.driver.get_info(instance, use_cache=False).state
1557         except exception.InstanceNotFound:
1558             return power_state.NOSTATE
1559 
1560     # TODO(stephenfin): Remove this once we bump the compute API to v6.0
1561     def get_console_topic(self, context):
1562         """Retrieves the console host for a project on this host.
1563 
1564         Currently this is just set in the flags for each compute host.
1565 
1566         """
1567         # TODO(mdragon): perhaps make this variable by console_type?
1568         return 'console.%s' % CONF.console_host
1569 
1570     # TODO(stephenfin): Remove this once we bump the compute API to v6.0
1571     @wrap_exception()
1572     def get_console_pool_info(self, context, console_type):
1573         raise NotImplementedError()
1574 
1575     # TODO(stephenfin): Remove this as it's nova-network only
1576     @wrap_exception()
1577     def refresh_instance_security_rules(self, context, instance):
1578         """Tell the virtualization driver to refresh security rules for
1579         an instance.
1580 
1581         Passes straight through to the virtualization driver.
1582 
1583         Synchronize the call because we may still be in the middle of
1584         creating the instance.
1585         """
1586         pass
1587 
1588     def _await_block_device_map_created(self, context, vol_id):
1589         # TODO(yamahata): creating volume simultaneously
1590         #                 reduces creation time?
1591         # TODO(yamahata): eliminate dumb polling
1592         start = time.time()
1593         retries = CONF.block_device_allocate_retries
1594         # (1) if the configured value is 0, one attempt should be made
1595         # (2) if the configured value is > 0, then the total number attempts
1596         #      is (retries + 1)
1597         attempts = 1
1598         if retries >= 1:
1599             attempts = retries + 1
1600         for attempt in range(1, attempts + 1):
1601             volume = self.volume_api.get(context, vol_id)
1602             volume_status = volume['status']
1603             if volume_status not in ['creating', 'downloading']:
1604                 if volume_status == 'available':
1605                     return attempt
1606                 LOG.warning("Volume id: %(vol_id)s finished being "
1607                             "created but its status is %(vol_status)s.",
1608                             {'vol_id': vol_id,
1609                              'vol_status': volume_status})
1610                 break
1611             greenthread.sleep(CONF.block_device_allocate_retries_interval)
1612         raise exception.VolumeNotCreated(volume_id=vol_id,
1613                                          seconds=int(time.time() - start),
1614                                          attempts=attempt,
1615                                          volume_status=volume_status)
1616 
1617     def _decode_files(self, injected_files):
1618         """Base64 decode the list of files to inject."""
1619         if not injected_files:
1620             return []
1621 
1622         def _decode(f):
1623             path, contents = f
1624             # Py3 raises binascii.Error instead of TypeError as in Py27
1625             try:
1626                 decoded = base64.b64decode(contents)
1627                 return path, decoded
1628             except (TypeError, binascii.Error):
1629                 raise exception.Base64Exception(path=path)
1630 
1631         return [_decode(f) for f in injected_files]
1632 
1633     def _validate_instance_group_policy(self, context, instance,
1634                                         scheduler_hints):
1635         # NOTE(russellb) Instance group policy is enforced by the scheduler.
1636         # However, there is a race condition with the enforcement of
1637         # the policy.  Since more than one instance may be scheduled at the
1638         # same time, it's possible that more than one instance with an
1639         # anti-affinity policy may end up here.  It's also possible that
1640         # multiple instances with an affinity policy could end up on different
1641         # hosts.  This is a validation step to make sure that starting the
1642         # instance here doesn't violate the policy.
1643         group_hint = scheduler_hints.get('group')
1644         if not group_hint:
1645             return
1646 
1647         # The RequestSpec stores scheduler_hints as key=list pairs so we need
1648         # to check the type on the value and pull the single entry out. The
1649         # API request schema validates that the 'group' hint is a single value.
1650         if isinstance(group_hint, list):
1651             group_hint = group_hint[0]
1652 
1653         @utils.synchronized(group_hint)
1654         def _do_validation(context, instance, group_hint):
1655             group = objects.InstanceGroup.get_by_hint(context, group_hint)
1656             if group.policy and 'anti-affinity' == group.policy:
1657                 instances_uuids = objects.InstanceList.get_uuids_by_host(
1658                     context, self.host)
1659                 ins_on_host = set(instances_uuids)
1660                 members = set(group.members)
1661                 # Determine the set of instance group members on this host
1662                 # which are not the instance in question. This is used to
1663                 # determine how many other members from the same anti-affinity
1664                 # group can be on this host.
1665                 members_on_host = ins_on_host & members - set([instance.uuid])
1666                 rules = group.rules
1667                 if rules and 'max_server_per_host' in rules:
1668                     max_server = rules['max_server_per_host']
1669                 else:
1670                     max_server = 1
1671                 if len(members_on_host) >= max_server:
1672                     msg = _("Anti-affinity instance group policy "
1673                             "was violated.")
1674                     raise exception.RescheduledException(
1675                             instance_uuid=instance.uuid,
1676                             reason=msg)
1677             elif group.policy and 'affinity' == group.policy:
1678                 group_hosts = group.get_hosts(exclude=[instance.uuid])
1679                 if group_hosts and self.host not in group_hosts:
1680                     msg = _("Affinity instance group policy was violated.")
1681                     raise exception.RescheduledException(
1682                             instance_uuid=instance.uuid,
1683                             reason=msg)
1684 
1685         if not CONF.workarounds.disable_group_policy_check_upcall:
1686             _do_validation(context, instance, group_hint)
1687 
1688     def _log_original_error(self, exc_info, instance_uuid):
1689         LOG.error('Error: %s', exc_info[1], instance_uuid=instance_uuid,
1690                   exc_info=exc_info)
1691 
1692     @periodic_task.periodic_task
1693     def _check_instance_build_time(self, context):
1694         """Ensure that instances are not stuck in build."""
1695         timeout = CONF.instance_build_timeout
1696         if timeout == 0:
1697             return
1698 
1699         filters = {'vm_state': vm_states.BUILDING,
1700                    'host': self.host}
1701 
1702         building_insts = objects.InstanceList.get_by_filters(context,
1703                            filters, expected_attrs=[], use_slave=True)
1704 
1705         for instance in building_insts:
1706             if timeutils.is_older_than(instance.created_at, timeout):
1707                 self._set_instance_obj_error_state(instance)
1708                 LOG.warning("Instance build timed out. Set to error "
1709                             "state.", instance=instance)
1710 
1711     def _check_instance_exists(self, instance):
1712         """Ensure an instance with the same name is not already present."""
1713         if self.driver.instance_exists(instance):
1714             raise exception.InstanceExists(name=instance.name)
1715 
1716     def _allocate_network_async(self, context, instance, requested_networks,
1717                                 security_groups, resource_provider_mapping):
1718         """Method used to allocate networks in the background.
1719 
1720         Broken out for testing.
1721         """
1722         # First check to see if we're specifically not supposed to allocate
1723         # networks because if so, we can exit early.
1724         if requested_networks and requested_networks.no_allocate:
1725             LOG.debug("Not allocating networking since 'none' was specified.",
1726                       instance=instance)
1727             return network_model.NetworkInfo([])
1728 
1729         LOG.debug("Allocating IP information in the background.",
1730                   instance=instance)
1731         retries = CONF.network_allocate_retries
1732         attempts = retries + 1
1733         retry_time = 1
1734         bind_host_id = self.driver.network_binding_host_id(context, instance)
1735         for attempt in range(1, attempts + 1):
1736             try:
1737                 nwinfo = self.network_api.allocate_for_instance(
1738                         context, instance,
1739                         requested_networks=requested_networks,
1740                         security_groups=security_groups,
1741                         bind_host_id=bind_host_id,
1742                         resource_provider_mapping=resource_provider_mapping)
1743                 LOG.debug('Instance network_info: |%s|', nwinfo,
1744                           instance=instance)
1745                 instance.system_metadata['network_allocated'] = 'True'
1746                 # NOTE(JoshNang) do not save the instance here, as it can cause
1747                 # races. The caller shares a reference to instance and waits
1748                 # for this async greenthread to finish before calling
1749                 # instance.save().
1750                 return nwinfo
1751             except Exception as e:
1752                 log_info = {'attempt': attempt,
1753                             'attempts': attempts}
1754                 if attempt == attempts:
1755                     LOG.exception('Instance failed network setup '
1756                                   'after %(attempts)d attempt(s)',
1757                                   log_info)
1758                     raise e
1759                 LOG.warning('Instance failed network setup '
1760                             '(attempt %(attempt)d of %(attempts)d)',
1761                             log_info, instance=instance)
1762                 time.sleep(retry_time)
1763                 retry_time *= 2
1764                 if retry_time > 30:
1765                     retry_time = 30
1766         # Not reached.
1767 
1768     def _build_networks_for_instance(self, context, instance,
1769             requested_networks, security_groups, resource_provider_mapping):
1770 
1771         # If we're here from a reschedule the network may already be allocated.
1772         if strutils.bool_from_string(
1773                 instance.system_metadata.get('network_allocated', 'False')):
1774             # NOTE(alex_xu): The network_allocated is True means the network
1775             # resource already allocated at previous scheduling, and the
1776             # network setup is cleanup at previous. After rescheduling, the
1777             # network resource need setup on the new host.
1778             self.network_api.setup_instance_network_on_host(
1779                 context, instance, instance.host)
1780             return self.network_api.get_instance_nw_info(context, instance)
1781 
1782         network_info = self._allocate_network(context, instance,
1783                 requested_networks, security_groups,
1784                 resource_provider_mapping)
1785 
1786         return network_info
1787 
1788     def _allocate_network(self, context, instance, requested_networks,
1789                           security_groups, resource_provider_mapping):
1790         """Start network allocation asynchronously.  Return an instance
1791         of NetworkInfoAsyncWrapper that can be used to retrieve the
1792         allocated networks when the operation has finished.
1793         """
1794         # NOTE(comstud): Since we're allocating networks asynchronously,
1795         # this task state has little meaning, as we won't be in this
1796         # state for very long.
1797         instance.vm_state = vm_states.BUILDING
1798         instance.task_state = task_states.NETWORKING
1799         instance.save(expected_task_state=[None])
1800 
1801         return network_model.NetworkInfoAsyncWrapper(
1802                 self._allocate_network_async, context, instance,
1803                 requested_networks, security_groups, resource_provider_mapping)
1804 
1805     def _default_root_device_name(self, instance, image_meta, root_bdm):
1806         """Gets a default root device name from the driver.
1807 
1808         :param nova.objects.Instance instance:
1809             The instance for which to get the root device name.
1810         :param nova.objects.ImageMeta image_meta:
1811             The metadata of the image of the instance.
1812         :param nova.objects.BlockDeviceMapping root_bdm:
1813             The description of the root device.
1814         :returns: str -- The default root device name.
1815         :raises: InternalError, TooManyDiskDevices
1816         """
1817         try:
1818             return self.driver.default_root_device_name(instance,
1819                                                         image_meta,
1820                                                         root_bdm)
1821         except NotImplementedError:
1822             return compute_utils.get_next_device_name(instance, [])
1823 
1824     def _default_device_names_for_instance(self, instance,
1825                                            root_device_name,
1826                                            *block_device_lists):
1827         """Default the missing device names in the BDM from the driver.
1828 
1829         :param nova.objects.Instance instance:
1830             The instance for which to get default device names.
1831         :param str root_device_name: The root device name.
1832         :param list block_device_lists: List of block device mappings.
1833         :returns: None
1834         :raises: InternalError, TooManyDiskDevices
1835         """
1836         try:
1837             self.driver.default_device_names_for_instance(instance,
1838                                                           root_device_name,
1839                                                           *block_device_lists)
1840         except NotImplementedError:
1841             compute_utils.default_device_names_for_instance(
1842                 instance, root_device_name, *block_device_lists)
1843 
1844     def _get_device_name_for_instance(self, instance, bdms, block_device_obj):
1845         """Get the next device name from the driver, based on the BDM.
1846 
1847         :param nova.objects.Instance instance:
1848             The instance whose volume is requesting a device name.
1849         :param nova.objects.BlockDeviceMappingList bdms:
1850             The block device mappings for the instance.
1851         :param nova.objects.BlockDeviceMapping block_device_obj:
1852             A block device mapping containing info about the requested block
1853             device.
1854         :returns: The next device name.
1855         :raises: InternalError, TooManyDiskDevices
1856         """
1857         # NOTE(ndipanov): Copy obj to avoid changing the original
1858         block_device_obj = block_device_obj.obj_clone()
1859         try:
1860             return self.driver.get_device_name_for_instance(
1861                 instance, bdms, block_device_obj)
1862         except NotImplementedError:
1863             return compute_utils.get_device_name_for_instance(
1864                 instance, bdms, block_device_obj.get("device_name"))
1865 
1866     def _default_block_device_names(self, instance, image_meta, block_devices):
1867         """Verify that all the devices have the device_name set. If not,
1868         provide a default name.
1869 
1870         It also ensures that there is a root_device_name and is set to the
1871         first block device in the boot sequence (boot_index=0).
1872         """
1873         root_bdm = block_device.get_root_bdm(block_devices)
1874         if not root_bdm:
1875             return
1876 
1877         # Get the root_device_name from the root BDM or the instance
1878         root_device_name = None
1879         update_root_bdm = False
1880 
1881         if root_bdm.device_name:
1882             root_device_name = root_bdm.device_name
1883             instance.root_device_name = root_device_name
1884         elif instance.root_device_name:
1885             root_device_name = instance.root_device_name
1886             root_bdm.device_name = root_device_name
1887             update_root_bdm = True
1888         else:
1889             root_device_name = self._default_root_device_name(instance,
1890                                                               image_meta,
1891                                                               root_bdm)
1892 
1893             instance.root_device_name = root_device_name
1894             root_bdm.device_name = root_device_name
1895             update_root_bdm = True
1896 
1897         if update_root_bdm:
1898             root_bdm.save()
1899 
1900         ephemerals = []
1901         swap = []
1902         block_device_mapping = []
1903 
1904         for device in block_devices:
1905             if block_device.new_format_is_ephemeral(device):
1906                 ephemerals.append(device)
1907 
1908             if block_device.new_format_is_swap(device):
1909                 swap.append(device)
1910 
1911             if driver_block_device.is_block_device_mapping(device):
1912                 block_device_mapping.append(device)
1913 
1914         self._default_device_names_for_instance(instance,
1915                                                 root_device_name,
1916                                                 ephemerals,
1917                                                 swap,
1918                                                 block_device_mapping)
1919 
1920     def _block_device_info_to_legacy(self, block_device_info):
1921         """Convert BDI to the old format for drivers that need it."""
1922 
1923         if self.use_legacy_block_device_info:
1924             ephemerals = driver_block_device.legacy_block_devices(
1925                 driver.block_device_info_get_ephemerals(block_device_info))
1926             mapping = driver_block_device.legacy_block_devices(
1927                 driver.block_device_info_get_mapping(block_device_info))
1928             swap = block_device_info['swap']
1929             if swap:
1930                 swap = swap.legacy()
1931 
1932             block_device_info.update({
1933                 'ephemerals': ephemerals,
1934                 'swap': swap,
1935                 'block_device_mapping': mapping})
1936 
1937     def _add_missing_dev_names(self, bdms, instance):
1938         for bdm in bdms:
1939             if bdm.device_name is not None:
1940                 continue
1941 
1942             device_name = self._get_device_name_for_instance(instance,
1943                                                              bdms, bdm)
1944             values = {'device_name': device_name}
1945             bdm.update(values)
1946             bdm.save()
1947 
1948     def _prep_block_device(self, context, instance, bdms):
1949         """Set up the block device for an instance with error logging."""
1950         try:
1951             self._add_missing_dev_names(bdms, instance)
1952             block_device_info = driver.get_block_device_info(instance, bdms)
1953             mapping = driver.block_device_info_get_mapping(block_device_info)
1954             driver_block_device.attach_block_devices(
1955                 mapping, context, instance, self.volume_api, self.driver,
1956                 wait_func=self._await_block_device_map_created)
1957 
1958             self._block_device_info_to_legacy(block_device_info)
1959             return block_device_info
1960 
1961         except exception.OverQuota as e:
1962             LOG.warning('Failed to create block device for instance due'
1963                         ' to exceeding volume related resource quota.'
1964                         ' Error: %s', e.message, instance=instance)
1965             raise
1966 
1967         except Exception as ex:
1968             LOG.exception('Instance failed block device setup',
1969                           instance=instance)
1970             # InvalidBDM will eventually result in a BuildAbortException when
1971             # booting from volume, and will be recorded as an instance fault.
1972             # Maintain the original exception message which most likely has
1973             # useful details which the standard InvalidBDM error message lacks.
1974             raise exception.InvalidBDM(str(ex))
1975 
1976     def _update_instance_after_spawn(self, instance,
1977                                      vm_state=vm_states.ACTIVE):
1978         instance.power_state = self._get_power_state(instance)
1979         instance.vm_state = vm_state
1980         instance.task_state = None
1981         # NOTE(sean-k-mooney): configdrive.update_instance checks
1982         # instance.launched_at to determine if it is the first or
1983         # subsequent spawn of an instance. We need to call update_instance
1984         # first before setting instance.launched_at or instance.config_drive
1985         # will never be set to true based on the value of force_config_drive.
1986         # As a result the config drive will be lost on a hard reboot of the
1987         # instance even when force_config_drive=true. see bug #1835822.
1988         configdrive.update_instance(instance)
1989         instance.launched_at = timeutils.utcnow()
1990 
1991     def _update_scheduler_instance_info(self, context, instance):
1992         """Sends an InstanceList with created or updated Instance objects to
1993         the Scheduler client.
1994 
1995         In the case of init_host, the value passed will already be an
1996         InstanceList. Other calls will send individual Instance objects that
1997         have been created or resized. In this case, we create an InstanceList
1998         object containing that Instance.
1999         """
2000         if not self.send_instance_updates:
2001             return
2002         if isinstance(instance, obj_instance.Instance):
2003             instance = objects.InstanceList(objects=[instance])
2004         context = context.elevated()
2005         self.query_client.update_instance_info(context, self.host,
2006                                                instance)
2007 
2008     def _delete_scheduler_instance_info(self, context, instance_uuid):
2009         """Sends the uuid of the deleted Instance to the Scheduler client."""
2010         if not self.send_instance_updates:
2011             return
2012         context = context.elevated()
2013         self.query_client.delete_instance_info(context, self.host,
2014                                                instance_uuid)
2015 
2016     @periodic_task.periodic_task(spacing=CONF.scheduler_instance_sync_interval)
2017     def _sync_scheduler_instance_info(self, context):
2018         if not self.send_instance_updates:
2019             return
2020         context = context.elevated()
2021         instances = objects.InstanceList.get_by_host(context, self.host,
2022                                                      expected_attrs=[],
2023                                                      use_slave=True)
2024         uuids = [instance.uuid for instance in instances]
2025         self.query_client.sync_instance_info(context, self.host, uuids)
2026 
2027     def _notify_about_instance_usage(self, context, instance, event_suffix,
2028                                      network_info=None, extra_usage_info=None,
2029                                      fault=None):
2030         compute_utils.notify_about_instance_usage(
2031             self.notifier, context, instance, event_suffix,
2032             network_info=network_info,
2033             extra_usage_info=extra_usage_info, fault=fault)
2034 
2035     def _deallocate_network(self, context, instance,
2036                             requested_networks=None):
2037         # If we were told not to allocate networks let's save ourselves
2038         # the trouble of calling the network API.
2039         if requested_networks and requested_networks.no_allocate:
2040             LOG.debug("Skipping network deallocation for instance since "
2041                       "networking was not requested.", instance=instance)
2042             return
2043 
2044         LOG.debug('Deallocating network for instance', instance=instance)
2045         with timeutils.StopWatch() as timer:
2046             self.network_api.deallocate_for_instance(
2047                 context, instance, requested_networks=requested_networks)
2048         # nova-network does an rpc call so we're OK tracking time spent here
2049         LOG.info('Took %0.2f seconds to deallocate network for instance.',
2050                  timer.elapsed(), instance=instance)
2051 
2052     def _get_instance_block_device_info(self, context, instance,
2053                                         refresh_conn_info=False,
2054                                         bdms=None):
2055         """Transform block devices to the driver block_device format."""
2056 
2057         if bdms is None:
2058             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
2059                     context, instance.uuid)
2060         block_device_info = driver.get_block_device_info(instance, bdms)
2061 
2062         if not refresh_conn_info:
2063             # if the block_device_mapping has no value in connection_info
2064             # (returned as None), don't include in the mapping
2065             block_device_info['block_device_mapping'] = [
2066                 bdm for bdm in driver.block_device_info_get_mapping(
2067                                     block_device_info)
2068                 if bdm.get('connection_info')]
2069         else:
2070             driver_block_device.refresh_conn_infos(
2071                 driver.block_device_info_get_mapping(block_device_info),
2072                 context, instance, self.volume_api, self.driver)
2073 
2074         self._block_device_info_to_legacy(block_device_info)
2075 
2076         return block_device_info
2077 
2078     def _build_failed(self, node):
2079         if CONF.compute.consecutive_build_service_disable_threshold:
2080             # NOTE(danms): Update our counter, but wait for the next
2081             # update_available_resource() periodic to flush it to the DB
2082             self.rt.build_failed(node)
2083 
2084     def _build_succeeded(self, node):
2085         self.rt.build_succeeded(node)
2086 
2087     @wrap_exception()
2088     @reverts_task_state
2089     @wrap_instance_fault
2090     def build_and_run_instance(self, context, instance, image, request_spec,
2091                      filter_properties, admin_password=None,
2092                      injected_files=None, requested_networks=None,
2093                      security_groups=None, block_device_mapping=None,
2094                      node=None, limits=None, host_list=None, accel_uuids=None):
2095 
2096         @utils.synchronized(instance.uuid)
2097         def _locked_do_build_and_run_instance(*args, **kwargs):
2098             # NOTE(danms): We grab the semaphore with the instance uuid
2099             # locked because we could wait in line to build this instance
2100             # for a while and we want to make sure that nothing else tries
2101             # to do anything with this instance while we wait.
2102             with self._build_semaphore:
2103                 try:
2104                     result = self._do_build_and_run_instance(*args, **kwargs)
2105                 except Exception:
2106                     # NOTE(mriedem): This should really only happen if
2107                     # _decode_files in _do_build_and_run_instance fails, and
2108                     # that's before a guest is spawned so it's OK to remove
2109                     # allocations for the instance for this node from Placement
2110                     # below as there is no guest consuming resources anyway.
2111                     # The _decode_files case could be handled more specifically
2112                     # but that's left for another day.
2113                     result = build_results.FAILED
2114                     raise
2115                 finally:
2116                     if result == build_results.FAILED:
2117                         # Remove the allocation records from Placement for the
2118                         # instance if the build failed. The instance.host is
2119                         # likely set to None in _do_build_and_run_instance
2120                         # which means if the user deletes the instance, it
2121                         # will be deleted in the API, not the compute service.
2122                         # Setting the instance.host to None in
2123                         # _do_build_and_run_instance means that the
2124                         # ResourceTracker will no longer consider this instance
2125                         # to be claiming resources against it, so we want to
2126                         # reflect that same thing in Placement.  No need to
2127                         # call this for a reschedule, as the allocations will
2128                         # have already been removed in
2129                         # self._do_build_and_run_instance().
2130                         self.reportclient.delete_allocation_for_instance(
2131                             context, instance.uuid)
2132 
2133                     if result in (build_results.FAILED,
2134                                   build_results.RESCHEDULED):
2135                         self._build_failed(node)
2136                     else:
2137                         self._build_succeeded(node)
2138 
2139         # NOTE(danms): We spawn here to return the RPC worker thread back to
2140         # the pool. Since what follows could take a really long time, we don't
2141         # want to tie up RPC workers.
2142         utils.spawn_n(_locked_do_build_and_run_instance,
2143                       context, instance, image, request_spec,
2144                       filter_properties, admin_password, injected_files,
2145                       requested_networks, security_groups,
2146                       block_device_mapping, node, limits, host_list,
2147                       accel_uuids)
2148 
2149     def _check_device_tagging(self, requested_networks, block_device_mapping):
2150         tagging_requested = False
2151         if requested_networks:
2152             for net in requested_networks:
2153                 if 'tag' in net and net.tag is not None:
2154                     tagging_requested = True
2155                     break
2156         if block_device_mapping and not tagging_requested:
2157             for bdm in block_device_mapping:
2158                 if 'tag' in bdm and bdm.tag is not None:
2159                     tagging_requested = True
2160                     break
2161         if (tagging_requested and
2162                 not self.driver.capabilities.get('supports_device_tagging',
2163                                                  False)):
2164             raise exception.BuildAbortException('Attempt to boot guest with '
2165                                                 'tagged devices on host that '
2166                                                 'does not support tagging.')
2167 
2168     def _check_trusted_certs(self, instance):
2169         if (instance.trusted_certs and
2170                 not self.driver.capabilities.get('supports_trusted_certs',
2171                                                  False)):
2172             raise exception.BuildAbortException(
2173                 'Trusted image certificates provided on host that does not '
2174                 'support certificate validation.')
2175 
2176     @wrap_exception()
2177     @reverts_task_state
2178     @wrap_instance_event(prefix='compute')
2179     @wrap_instance_fault
2180     def _do_build_and_run_instance(self, context, instance, image,
2181             request_spec, filter_properties, admin_password, injected_files,
2182             requested_networks, security_groups, block_device_mapping,
2183             node=None, limits=None, host_list=None, accel_uuids=None):
2184 
2185         try:
2186             LOG.debug('Starting instance...', instance=instance)
2187             instance.vm_state = vm_states.BUILDING
2188             instance.task_state = None
2189             instance.save(expected_task_state=
2190                     (task_states.SCHEDULING, None))
2191         except exception.InstanceNotFound:
2192             msg = 'Instance disappeared before build.'
2193             LOG.debug(msg, instance=instance)
2194             return build_results.FAILED
2195         except exception.UnexpectedTaskStateError as e:
2196             LOG.debug(e.format_message(), instance=instance)
2197             return build_results.FAILED
2198 
2199         # b64 decode the files to inject:
2200         decoded_files = self._decode_files(injected_files)
2201 
2202         if limits is None:
2203             limits = {}
2204 
2205         if node is None:
2206             node = self._get_nodename(instance, refresh=True)
2207 
2208         try:
2209             with timeutils.StopWatch() as timer:
2210                 self._build_and_run_instance(context, instance, image,
2211                         decoded_files, admin_password, requested_networks,
2212                         security_groups, block_device_mapping, node, limits,
2213                         filter_properties, request_spec, accel_uuids)
2214             LOG.info('Took %0.2f seconds to build instance.',
2215                      timer.elapsed(), instance=instance)
2216             return build_results.ACTIVE
2217         except exception.RescheduledException as e:
2218             retry = filter_properties.get('retry')
2219             if not retry:
2220                 # no retry information, do not reschedule.
2221                 LOG.debug("Retry info not present, will not reschedule",
2222                     instance=instance)
2223                 self._cleanup_allocated_networks(context, instance,
2224                     requested_networks)
2225                 compute_utils.add_instance_fault_from_exc(context,
2226                         instance, e, sys.exc_info(),
2227                         fault_message=e.kwargs['reason'])
2228                 self._nil_out_instance_obj_host_and_node(instance)
2229                 self._set_instance_obj_error_state(instance,
2230                                                    clean_task_state=True)
2231                 return build_results.FAILED
2232             LOG.debug(e.format_message(), instance=instance)
2233             # This will be used for logging the exception
2234             retry['exc'] = traceback.format_exception(*sys.exc_info())
2235             # This will be used for setting the instance fault message
2236             retry['exc_reason'] = e.kwargs['reason']
2237 
2238             self._cleanup_allocated_networks(context, instance,
2239                                              requested_networks)
2240 
2241             self._nil_out_instance_obj_host_and_node(instance)
2242             instance.task_state = task_states.SCHEDULING
2243             instance.save()
2244             # The instance will have already claimed resources from this host
2245             # before this build was attempted. Now that it has failed, we need
2246             # to unclaim those resources before casting to the conductor, so
2247             # that if there are alternate hosts available for a retry, it can
2248             # claim resources on that new host for the instance.
2249             self.reportclient.delete_allocation_for_instance(context,
2250                                                              instance.uuid)
2251 
2252             self.compute_task_api.build_instances(context, [instance],
2253                     image, filter_properties, admin_password,
2254                     injected_files, requested_networks, security_groups,
2255                     block_device_mapping, request_spec=request_spec,
2256                     host_lists=[host_list])
2257             return build_results.RESCHEDULED
2258         except (exception.InstanceNotFound,
2259                 exception.UnexpectedDeletingTaskStateError):
2260             msg = 'Instance disappeared during build.'
2261             LOG.debug(msg, instance=instance)
2262             self._cleanup_allocated_networks(context, instance,
2263                     requested_networks)
2264             return build_results.FAILED
2265         except Exception as e:
2266             if isinstance(e, exception.BuildAbortException):
2267                 LOG.error(e.format_message(), instance=instance)
2268             else:
2269                 # Should not reach here.
2270                 LOG.exception('Unexpected build failure, not rescheduling '
2271                               'build.', instance=instance)
2272             self._cleanup_allocated_networks(context, instance,
2273                     requested_networks)
2274             self._cleanup_volumes(context, instance,
2275                     block_device_mapping, raise_exc=False)
2276             compute_utils.add_instance_fault_from_exc(context, instance,
2277                     e, sys.exc_info())
2278             self._nil_out_instance_obj_host_and_node(instance)
2279             self._set_instance_obj_error_state(instance, clean_task_state=True)
2280             return build_results.FAILED
2281 
2282     @staticmethod
2283     def _get_scheduler_hints(filter_properties, request_spec=None):
2284         """Helper method to get scheduler hints.
2285 
2286         This method prefers to get the hints out of the request spec, but that
2287         might not be provided. Conductor will pass request_spec down to the
2288         first compute chosen for a build but older computes will not pass
2289         the request_spec to conductor's build_instances method for a
2290         a reschedule, so if we're on a host via a retry, request_spec may not
2291         be provided so we need to fallback to use the filter_properties
2292         to get scheduler hints.
2293         """
2294         hints = {}
2295         if request_spec is not None and 'scheduler_hints' in request_spec:
2296             hints = request_spec.scheduler_hints
2297         if not hints:
2298             hints = filter_properties.get('scheduler_hints') or {}
2299         return hints
2300 
2301     @staticmethod
2302     def _get_request_group_mapping(request_spec):
2303         """Return request group resource - provider mapping. This is currently
2304         used for Neutron ports that have resource request due to the port
2305         having QoS minimum bandwidth policy rule attached.
2306 
2307         :param request_spec: A RequestSpec object or None
2308         :returns: A dict keyed by RequestGroup requester_id, currently Neutron
2309         port_id, to resource provider UUID that provides resource for that
2310         RequestGroup. Or None if the request_spec was None.
2311         """
2312         if request_spec:
2313             return request_spec.get_request_group_mapping()
2314         else:
2315             return None
2316 
2317     def _build_and_run_instance(self, context, instance, image, injected_files,
2318             admin_password, requested_networks, security_groups,
2319             block_device_mapping, node, limits, filter_properties,
2320             request_spec=None, accel_uuids=None):
2321 
2322         image_name = image.get('name')
2323         self._notify_about_instance_usage(context, instance, 'create.start',
2324                 extra_usage_info={'image_name': image_name})
2325         compute_utils.notify_about_instance_create(
2326             context, instance, self.host,
2327             phase=fields.NotificationPhase.START,
2328             bdms=block_device_mapping)
2329 
2330         # NOTE(mikal): cache the keystone roles associated with the instance
2331         # at boot time for later reference
2332         instance.system_metadata.update(
2333             {'boot_roles': ','.join(context.roles)})
2334 
2335         self._check_device_tagging(requested_networks, block_device_mapping)
2336         self._check_trusted_certs(instance)
2337 
2338         provider_mapping = self._get_request_group_mapping(request_spec)
2339 
2340         if provider_mapping:
2341             try:
2342                 compute_utils\
2343                     .update_pci_request_spec_with_allocated_interface_name(
2344                         context, self.reportclient,
2345                         instance.pci_requests.requests, provider_mapping)
2346             except (exception.AmbiguousResourceProviderForPCIRequest,
2347                     exception.UnexpectedResourceProviderNameForPCIRequest
2348                     ) as e:
2349                 raise exception.BuildAbortException(
2350                     reason=str(e), instance_uuid=instance.uuid)
2351 
2352         # TODO(Luyao) cut over to get_allocs_for_consumer
2353         allocs = self.reportclient.get_allocations_for_consumer(
2354                 context, instance.uuid)
2355 
2356         try:
2357             scheduler_hints = self._get_scheduler_hints(filter_properties,
2358                                                         request_spec)
2359             with self.rt.instance_claim(context, instance, node, allocs,
2360                                         limits):
2361                 # NOTE(russellb) It's important that this validation be done
2362                 # *after* the resource tracker instance claim, as that is where
2363                 # the host is set on the instance.
2364                 self._validate_instance_group_policy(context, instance,
2365                                                      scheduler_hints)
2366                 image_meta = objects.ImageMeta.from_dict(image)
2367 
2368                 with self._build_resources(context, instance,
2369                         requested_networks, security_groups, image_meta,
2370                         block_device_mapping, provider_mapping,
2371                         accel_uuids) as resources:
2372                     instance.vm_state = vm_states.BUILDING
2373                     instance.task_state = task_states.SPAWNING
2374                     # NOTE(JoshNang) This also saves the changes to the
2375                     # instance from _allocate_network_async, as they aren't
2376                     # saved in that function to prevent races.
2377                     instance.save(expected_task_state=
2378                             task_states.BLOCK_DEVICE_MAPPING)
2379                     block_device_info = resources['block_device_info']
2380                     network_info = resources['network_info']
2381                     accel_info = resources['accel_info']
2382                     LOG.debug('Start spawning the instance on the hypervisor.',
2383                               instance=instance)
2384                     with timeutils.StopWatch() as timer:
2385                         self.driver.spawn(context, instance, image_meta,
2386                                           injected_files, admin_password,
2387                                           allocs, network_info=network_info,
2388                                           block_device_info=block_device_info,
2389                                           accel_info=accel_info)
2390                     LOG.info('Took %0.2f seconds to spawn the instance on '
2391                              'the hypervisor.', timer.elapsed(),
2392                              instance=instance)
2393         except (exception.InstanceNotFound,
2394                 exception.UnexpectedDeletingTaskStateError) as e:
2395             with excutils.save_and_reraise_exception():
2396                 self._notify_about_instance_usage(context, instance,
2397                     'create.error', fault=e)
2398                 compute_utils.notify_about_instance_create(
2399                     context, instance, self.host,
2400                     phase=fields.NotificationPhase.ERROR, exception=e,
2401                     bdms=block_device_mapping)
2402         except exception.ComputeResourcesUnavailable as e:
2403             LOG.debug(e.format_message(), instance=instance)
2404             self._notify_about_instance_usage(context, instance,
2405                     'create.error', fault=e)
2406             compute_utils.notify_about_instance_create(
2407                     context, instance, self.host,
2408                     phase=fields.NotificationPhase.ERROR, exception=e,
2409                     bdms=block_device_mapping)
2410             raise exception.RescheduledException(
2411                     instance_uuid=instance.uuid, reason=e.format_message())
2412         except exception.BuildAbortException as e:
2413             with excutils.save_and_reraise_exception():
2414                 LOG.debug(e.format_message(), instance=instance)
2415                 self._notify_about_instance_usage(context, instance,
2416                     'create.error', fault=e)
2417                 compute_utils.notify_about_instance_create(
2418                     context, instance, self.host,
2419                     phase=fields.NotificationPhase.ERROR, exception=e,
2420                     bdms=block_device_mapping)
2421         except exception.NoMoreFixedIps as e:
2422             LOG.warning('No more fixed IP to be allocated',
2423                         instance=instance)
2424             self._notify_about_instance_usage(context, instance,
2425                     'create.error', fault=e)
2426             compute_utils.notify_about_instance_create(
2427                     context, instance, self.host,
2428                     phase=fields.NotificationPhase.ERROR, exception=e,
2429                     bdms=block_device_mapping)
2430             msg = _('Failed to allocate the network(s) with error %s, '
2431                     'not rescheduling.') % e.format_message()
2432             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2433                     reason=msg)
2434         except (exception.ExternalNetworkAttachForbidden,
2435                 exception.VirtualInterfaceCreateException,
2436                 exception.VirtualInterfaceMacAddressException,
2437                 exception.FixedIpInvalidOnHost,
2438                 exception.UnableToAutoAllocateNetwork,
2439                 exception.NetworksWithQoSPolicyNotSupported) as e:
2440             LOG.exception('Failed to allocate network(s)',
2441                           instance=instance)
2442             self._notify_about_instance_usage(context, instance,
2443                     'create.error', fault=e)
2444             compute_utils.notify_about_instance_create(
2445                     context, instance, self.host,
2446                     phase=fields.NotificationPhase.ERROR, exception=e,
2447                     bdms=block_device_mapping)
2448             msg = _('Failed to allocate the network(s), not rescheduling.')
2449             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2450                     reason=msg)
2451         except (exception.FlavorDiskTooSmall,
2452                 exception.FlavorMemoryTooSmall,
2453                 exception.ImageNotActive,
2454                 exception.ImageUnacceptable,
2455                 exception.InvalidDiskInfo,
2456                 exception.InvalidDiskFormat,
2457                 cursive_exception.SignatureVerificationError,
2458                 exception.CertificateValidationFailed,
2459                 exception.VolumeEncryptionNotSupported,
2460                 exception.InvalidInput,
2461                 # TODO(mriedem): We should be validating RequestedVRamTooHigh
2462                 # in the API during server create and rebuild.
2463                 exception.RequestedVRamTooHigh) as e:
2464             self._notify_about_instance_usage(context, instance,
2465                     'create.error', fault=e)
2466             compute_utils.notify_about_instance_create(
2467                     context, instance, self.host,
2468                     phase=fields.NotificationPhase.ERROR, exception=e,
2469                     bdms=block_device_mapping)
2470             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2471                     reason=e.format_message())
2472         except Exception as e:
2473             LOG.exception('Failed to build and run instance',
2474                           instance=instance)
2475             self._notify_about_instance_usage(context, instance,
2476                     'create.error', fault=e)
2477             compute_utils.notify_about_instance_create(
2478                     context, instance, self.host,
2479                     phase=fields.NotificationPhase.ERROR, exception=e,
2480                     bdms=block_device_mapping)
2481             raise exception.RescheduledException(
2482                     instance_uuid=instance.uuid, reason=str(e))
2483 
2484         # NOTE(alaski): This is only useful during reschedules, remove it now.
2485         instance.system_metadata.pop('network_allocated', None)
2486 
2487         # If CONF.default_access_ip_network_name is set, grab the
2488         # corresponding network and set the access ip values accordingly.
2489         network_name = CONF.default_access_ip_network_name
2490         if (network_name and not instance.access_ip_v4 and
2491                 not instance.access_ip_v6):
2492             # Note that when there are multiple ips to choose from, an
2493             # arbitrary one will be chosen.
2494             for vif in network_info:
2495                 if vif['network']['label'] == network_name:
2496                     for ip in vif.fixed_ips():
2497                         if not instance.access_ip_v4 and ip['version'] == 4:
2498                             instance.access_ip_v4 = ip['address']
2499                         if not instance.access_ip_v6 and ip['version'] == 6:
2500                             instance.access_ip_v6 = ip['address']
2501                     break
2502 
2503         self._update_instance_after_spawn(instance)
2504 
2505         try:
2506             instance.save(expected_task_state=task_states.SPAWNING)
2507         except (exception.InstanceNotFound,
2508                 exception.UnexpectedDeletingTaskStateError) as e:
2509             with excutils.save_and_reraise_exception():
2510                 self._notify_about_instance_usage(context, instance,
2511                     'create.error', fault=e)
2512                 compute_utils.notify_about_instance_create(
2513                     context, instance, self.host,
2514                     phase=fields.NotificationPhase.ERROR, exception=e,
2515                     bdms=block_device_mapping)
2516 
2517         self._update_scheduler_instance_info(context, instance)
2518         self._notify_about_instance_usage(context, instance, 'create.end',
2519                 extra_usage_info={'message': _('Success')},
2520                 network_info=network_info)
2521         compute_utils.notify_about_instance_create(context, instance,
2522                 self.host, phase=fields.NotificationPhase.END,
2523                 bdms=block_device_mapping)
2524 
2525     def _build_resources_cleanup(self, instance, network_info):
2526         # Make sure the async call finishes
2527         if network_info is not None:
2528             network_info.wait(do_raise=False)
2529             self.driver.clean_networks_preparation(instance,
2530                                                    network_info)
2531         self.driver.failed_spawn_cleanup(instance)
2532 
2533     @contextlib.contextmanager
2534     def _build_resources(self, context, instance, requested_networks,
2535                          security_groups, image_meta, block_device_mapping,
2536                          resource_provider_mapping, accel_uuids):
2537         resources = {}
2538         network_info = None
2539         try:
2540             LOG.debug('Start building networks asynchronously for instance.',
2541                       instance=instance)
2542             network_info = self._build_networks_for_instance(context, instance,
2543                     requested_networks, security_groups,
2544                     resource_provider_mapping)
2545             resources['network_info'] = network_info
2546         except (exception.InstanceNotFound,
2547                 exception.UnexpectedDeletingTaskStateError):
2548             raise
2549         except exception.UnexpectedTaskStateError as e:
2550             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2551                     reason=e.format_message())
2552         except Exception:
2553             # Because this allocation is async any failures are likely to occur
2554             # when the driver accesses network_info during spawn().
2555             LOG.exception('Failed to allocate network(s)',
2556                           instance=instance)
2557             msg = _('Failed to allocate the network(s), not rescheduling.')
2558             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2559                     reason=msg)
2560 
2561         try:
2562             # Perform any driver preparation work for the driver.
2563             self.driver.prepare_for_spawn(instance)
2564 
2565             # Depending on a virt driver, some network configuration is
2566             # necessary before preparing block devices.
2567             self.driver.prepare_networks_before_block_device_mapping(
2568                 instance, network_info)
2569 
2570             # Verify that all the BDMs have a device_name set and assign a
2571             # default to the ones missing it with the help of the driver.
2572             self._default_block_device_names(instance, image_meta,
2573                                              block_device_mapping)
2574 
2575             LOG.debug('Start building block device mappings for instance.',
2576                       instance=instance)
2577             instance.vm_state = vm_states.BUILDING
2578             instance.task_state = task_states.BLOCK_DEVICE_MAPPING
2579             instance.save()
2580 
2581             block_device_info = self._prep_block_device(context, instance,
2582                     block_device_mapping)
2583             resources['block_device_info'] = block_device_info
2584         except (exception.InstanceNotFound,
2585                 exception.UnexpectedDeletingTaskStateError):
2586             with excutils.save_and_reraise_exception():
2587                 self._build_resources_cleanup(instance, network_info)
2588         except (exception.UnexpectedTaskStateError,
2589                 exception.OverQuota, exception.InvalidBDM) as e:
2590             self._build_resources_cleanup(instance, network_info)
2591             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2592                     reason=e.format_message())
2593         except Exception:
2594             LOG.exception('Failure prepping block device',
2595                           instance=instance)
2596             self._build_resources_cleanup(instance, network_info)
2597             msg = _('Failure prepping block device.')
2598             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2599                     reason=msg)
2600 
2601         arqs = []
2602         if instance.flavor.extra_specs.get('accel:device_profile'):
2603             try:
2604                 arqs = self._get_bound_arq_resources(
2605                     context, instance, accel_uuids)
2606             except (Exception, eventlet.timeout.Timeout) as exc:
2607                 LOG.exception(exc)
2608                 self._build_resources_cleanup(instance, network_info)
2609                 compute_utils.delete_arqs_if_needed(context, instance)
2610                 msg = _('Failure getting accelerator requests.')
2611                 raise exception.BuildAbortException(
2612                     reason=msg, instance_uuid=instance.uuid)
2613 
2614         resources['accel_info'] = arqs
2615         try:
2616             yield resources
2617         except Exception as exc:
2618             with excutils.save_and_reraise_exception() as ctxt:
2619                 if not isinstance(exc, (
2620                         exception.InstanceNotFound,
2621                         exception.UnexpectedDeletingTaskStateError)):
2622                     LOG.exception('Instance failed to spawn',
2623                                   instance=instance)
2624                 # Make sure the async call finishes
2625                 if network_info is not None:
2626                     network_info.wait(do_raise=False)
2627                 # if network_info is empty we're likely here because of
2628                 # network allocation failure. Since nothing can be reused on
2629                 # rescheduling it's better to deallocate network to eliminate
2630                 # the chance of orphaned ports in neutron
2631                 deallocate_networks = False if network_info else True
2632                 try:
2633                     self._shutdown_instance(context, instance,
2634                             block_device_mapping, requested_networks,
2635                             try_deallocate_networks=deallocate_networks)
2636                 except Exception as exc2:
2637                     ctxt.reraise = False
2638                     LOG.warning('Could not clean up failed build,'
2639                                 ' not rescheduling. Error: %s',
2640                                 str(exc2))
2641                     raise exception.BuildAbortException(
2642                             instance_uuid=instance.uuid,
2643                             reason=str(exc))
2644                 finally:
2645                     # Call Cyborg to delete accelerator requests
2646                     compute_utils.delete_arqs_if_needed(context, instance)
2647 
2648     def _get_bound_arq_resources(self, context, instance, arq_uuids):
2649         """Get bound accelerator requests.
2650 
2651         The ARQ binding was kicked off in the conductor as an async
2652         operation. Here we wait for the notification from Cyborg.
2653 
2654         If the notification arrived before this point, which can happen
2655         in many/most cases (see [1]), it will be lost. To handle that,
2656         we use exit_wait_early.
2657         [1] https://review.opendev.org/#/c/631244/46/nova/compute/
2658             manager.py@2627
2659 
2660         :param instance: instance object
2661         :param arq_uuids: List of accelerator request (ARQ) UUIDs.
2662         :returns: List of ARQs for which bindings have completed,
2663                   successfully or otherwise
2664         """
2665 
2666         cyclient = cyborg.get_client(context)
2667         if arq_uuids is None:
2668             arqs = cyclient.get_arqs_for_instance(instance.uuid)
2669             arq_uuids = [arq['uuid'] for arq in arqs]
2670         events = [('accelerator-request-bound', arq_uuid)
2671                   for arq_uuid in arq_uuids]
2672 
2673         timeout = CONF.arq_binding_timeout
2674         with self.virtapi.wait_for_instance_event(
2675                 instance, events, deadline=timeout):
2676             resolved_arqs = cyclient.get_arqs_for_instance(
2677                     instance.uuid, only_resolved=True)
2678             # Events for these resolved ARQs may have already arrived.
2679             # Such 'early' events need to be ignored.
2680             early_events = [('accelerator-request-bound', arq['uuid'])
2681                              for arq in resolved_arqs]
2682             if early_events:
2683                 self.virtapi.exit_wait_early(early_events)
2684 
2685         # Since a timeout in wait_for_instance_event will raise, we get
2686         # here only if all binding events have been received.
2687         resolved_uuids = [arq['uuid'] for arq in resolved_arqs]
2688         if sorted(resolved_uuids) != sorted(arq_uuids):
2689             # Query Cyborg to get all.
2690             arqs = cyclient.get_arqs_for_instance(instance.uuid)
2691         else:
2692             arqs = resolved_arqs
2693         return arqs
2694 
2695     def _cleanup_allocated_networks(self, context, instance,
2696             requested_networks):
2697         """Cleanup networks allocated for instance.
2698 
2699         :param context: nova request context
2700         :param instance: nova.objects.instance.Instance object
2701         :param requested_networks: nova.objects.NetworkRequestList
2702         """
2703         LOG.debug('Unplugging VIFs for instance', instance=instance)
2704 
2705         network_info = instance.get_network_info()
2706 
2707         # NOTE(stephenfin) to avoid nova destroying the instance without
2708         # unplugging the interface, refresh network_info if it is empty.
2709         if not network_info:
2710             try:
2711                 network_info = self.network_api.get_instance_nw_info(
2712                     context, instance,
2713                 )
2714             except Exception as exc:
2715                 LOG.warning(
2716                     'Failed to update network info cache when cleaning up '
2717                     'allocated networks. Stale VIFs may be left on this host.'
2718                     'Error: %s', str(exc)
2719                 )
2720                 return
2721 
2722         try:
2723             self.driver.unplug_vifs(instance, network_info)
2724         except NotImplementedError:
2725             # This is an optional method so ignore things if it doesn't exist
2726             LOG.debug(
2727                 'Virt driver does not provide unplug_vifs method, so it '
2728                 'is not possible determine if VIFs should be unplugged.'
2729             )
2730         except exception.NovaException as exc:
2731             # It's possible that the instance never got as far as plugging
2732             # VIFs, in which case we would see an exception which can be
2733             # mostly ignored
2734             LOG.warning(
2735                 'Cleaning up VIFs failed for instance. Error: %s',
2736                 str(exc), instance=instance,
2737             )
2738         else:
2739             LOG.debug('Unplugged VIFs for instance', instance=instance)
2740 
2741         try:
2742             self._deallocate_network(context, instance, requested_networks)
2743         except Exception:
2744             LOG.exception('Failed to deallocate networks', instance=instance)
2745             return
2746 
2747         instance.system_metadata['network_allocated'] = 'False'
2748         try:
2749             instance.save()
2750         except exception.InstanceNotFound:
2751             # NOTE(alaski): It's possible that we're cleaning up the networks
2752             # because the instance was deleted.  If that's the case then this
2753             # exception will be raised by instance.save()
2754             pass
2755 
2756     def _try_deallocate_network(self, context, instance,
2757                                 requested_networks=None):
2758 
2759         # During auto-scale cleanup, we could be deleting a large number
2760         # of servers at the same time and overloading parts of the system,
2761         # so we retry a few times in case of connection failures to the
2762         # networking service.
2763         @loopingcall.RetryDecorator(
2764             max_retry_count=3, inc_sleep_time=2, max_sleep_time=12,
2765             exceptions=(keystone_exception.connection.ConnectFailure,))
2766         def _deallocate_network_with_retries():
2767             try:
2768                 self._deallocate_network(
2769                     context, instance, requested_networks)
2770             except keystone_exception.connection.ConnectFailure as e:
2771                 # Provide a warning that something is amiss.
2772                 with excutils.save_and_reraise_exception():
2773                     LOG.warning('Failed to deallocate network for instance; '
2774                                 'retrying. Error: %s', str(e),
2775                                 instance=instance)
2776 
2777         try:
2778             # tear down allocated network structure
2779             _deallocate_network_with_retries()
2780         except Exception as ex:
2781             with excutils.save_and_reraise_exception():
2782                 LOG.error('Failed to deallocate network for instance. '
2783                           'Error: %s', ex, instance=instance)
2784                 self._set_instance_obj_error_state(instance)
2785 
2786     def _get_power_off_values(self, instance, clean_shutdown):
2787         """Get the timing configuration for powering down this instance."""
2788         if clean_shutdown:
2789             timeout = compute_utils.get_value_from_system_metadata(instance,
2790                           key='image_os_shutdown_timeout', type=int,
2791                           default=CONF.shutdown_timeout)
2792             retry_interval = CONF.compute.shutdown_retry_interval
2793         else:
2794             timeout = 0
2795             retry_interval = 0
2796 
2797         return timeout, retry_interval
2798 
2799     def _power_off_instance(self, instance, clean_shutdown=True):
2800         """Power off an instance on this host."""
2801         timeout, retry_interval = self._get_power_off_values(
2802             instance, clean_shutdown)
2803         self.driver.power_off(instance, timeout, retry_interval)
2804 
2805     def _shutdown_instance(self, context, instance,
2806                            bdms, requested_networks=None, notify=True,
2807                            try_deallocate_networks=True):
2808         """Shutdown an instance on this host.
2809 
2810         :param:context: security context
2811         :param:instance: a nova.objects.Instance object
2812         :param:bdms: the block devices for the instance to be torn
2813                      down
2814         :param:requested_networks: the networks on which the instance
2815                                    has ports
2816         :param:notify: true if a final usage notification should be
2817                        emitted
2818         :param:try_deallocate_networks: false if we should avoid
2819                                         trying to teardown networking
2820         """
2821         context = context.elevated()
2822         LOG.info('Terminating instance', instance=instance)
2823 
2824         if notify:
2825             self._notify_about_instance_usage(context, instance,
2826                                               "shutdown.start")
2827             compute_utils.notify_about_instance_action(context, instance,
2828                     self.host, action=fields.NotificationAction.SHUTDOWN,
2829                     phase=fields.NotificationPhase.START, bdms=bdms)
2830 
2831         network_info = instance.get_network_info()
2832 
2833         # NOTE(arnaudmorin) to avoid nova destroying the instance without
2834         # unplugging the interface, refresh network_info if it is empty.
2835         if not network_info:
2836             network_info = self.network_api.get_instance_nw_info(
2837                 context, instance)
2838 
2839         # NOTE(vish) get bdms before destroying the instance
2840         vol_bdms = [bdm for bdm in bdms if bdm.is_volume]
2841         block_device_info = self._get_instance_block_device_info(
2842             context, instance, bdms=bdms)
2843 
2844         # NOTE(melwitt): attempt driver destroy before releasing ip, may
2845         #                want to keep ip allocated for certain failures
2846         try:
2847             LOG.debug('Start destroying the instance on the hypervisor.',
2848                       instance=instance)
2849             with timeutils.StopWatch() as timer:
2850                 self.driver.destroy(context, instance, network_info,
2851                                     block_device_info)
2852             LOG.info('Took %0.2f seconds to destroy the instance on the '
2853                      'hypervisor.', timer.elapsed(), instance=instance)
2854         except exception.InstancePowerOffFailure:
2855             # if the instance can't power off, don't release the ip
2856             with excutils.save_and_reraise_exception():
2857                 pass
2858         except Exception:
2859             with excutils.save_and_reraise_exception():
2860                 # deallocate ip and fail without proceeding to
2861                 # volume api calls, preserving current behavior
2862                 if try_deallocate_networks:
2863                     self._try_deallocate_network(context, instance,
2864                                                  requested_networks)
2865 
2866         if try_deallocate_networks:
2867             self._try_deallocate_network(context, instance, requested_networks)
2868 
2869         timer.restart()
2870         for bdm in vol_bdms:
2871             try:
2872                 if bdm.attachment_id:
2873                     self.volume_api.attachment_delete(context,
2874                                                       bdm.attachment_id)
2875                 else:
2876                     # NOTE(vish): actual driver detach done in driver.destroy,
2877                     #             so just tell cinder that we are done with it.
2878                     connector = self.driver.get_volume_connector(instance)
2879                     self.volume_api.terminate_connection(context,
2880                                                          bdm.volume_id,
2881                                                          connector)
2882                     self.volume_api.detach(context, bdm.volume_id,
2883                                            instance.uuid)
2884 
2885             except exception.VolumeAttachmentNotFound as exc:
2886                 LOG.debug('Ignoring VolumeAttachmentNotFound: %s', exc,
2887                           instance=instance)
2888             except exception.DiskNotFound as exc:
2889                 LOG.debug('Ignoring DiskNotFound: %s', exc,
2890                           instance=instance)
2891             except exception.VolumeNotFound as exc:
2892                 LOG.debug('Ignoring VolumeNotFound: %s', exc,
2893                           instance=instance)
2894             except (cinder_exception.EndpointNotFound,
2895                     keystone_exception.EndpointNotFound) as exc:
2896                 LOG.warning('Ignoring EndpointNotFound for '
2897                             'volume %(volume_id)s: %(exc)s',
2898                             {'exc': exc, 'volume_id': bdm.volume_id},
2899                             instance=instance)
2900             except cinder_exception.ClientException as exc:
2901                 LOG.warning('Ignoring unknown cinder exception for '
2902                             'volume %(volume_id)s: %(exc)s',
2903                             {'exc': exc, 'volume_id': bdm.volume_id},
2904                             instance=instance)
2905             except Exception as exc:
2906                 LOG.warning('Ignoring unknown exception for '
2907                             'volume %(volume_id)s: %(exc)s',
2908                             {'exc': exc, 'volume_id': bdm.volume_id},
2909                             instance=instance)
2910         if vol_bdms:
2911             LOG.info('Took %(time).2f seconds to detach %(num)s volumes '
2912                      'for instance.',
2913                      {'time': timer.elapsed(), 'num': len(vol_bdms)},
2914                      instance=instance)
2915 
2916         if notify:
2917             self._notify_about_instance_usage(context, instance,
2918                                               "shutdown.end")
2919             compute_utils.notify_about_instance_action(context, instance,
2920                     self.host, action=fields.NotificationAction.SHUTDOWN,
2921                     phase=fields.NotificationPhase.END, bdms=bdms)
2922 
2923     def _cleanup_volumes(self, context, instance, bdms, raise_exc=True,
2924                          detach=True):
2925         original_exception = None
2926         for bdm in bdms:
2927             if detach and bdm.volume_id:
2928                 try:
2929                     LOG.debug("Detaching volume: %s", bdm.volume_id,
2930                               instance_uuid=instance.uuid)
2931                     destroy = bdm.delete_on_termination
2932                     self._detach_volume(context, bdm, instance,
2933                                         destroy_bdm=destroy)
2934                 except Exception as exc:
2935                     original_exception = exc
2936                     LOG.warning('Failed to detach volume: %(volume_id)s '
2937                                 'due to %(exc)s',
2938                                 {'volume_id': bdm.volume_id, 'exc': exc})
2939 
2940             if bdm.volume_id and bdm.delete_on_termination:
2941                 try:
2942                     LOG.debug("Deleting volume: %s", bdm.volume_id,
2943                               instance_uuid=instance.uuid)
2944                     self.volume_api.delete(context, bdm.volume_id)
2945                 except Exception as exc:
2946                     original_exception = exc
2947                     LOG.warning('Failed to delete volume: %(volume_id)s '
2948                                 'due to %(exc)s',
2949                                 {'volume_id': bdm.volume_id, 'exc': exc})
2950         if original_exception is not None and raise_exc:
2951             raise original_exception
2952 
2953     def _delete_instance(self, context, instance, bdms):
2954         """Delete an instance on this host.
2955 
2956         :param context: nova request context
2957         :param instance: nova.objects.instance.Instance object
2958         :param bdms: nova.objects.block_device.BlockDeviceMappingList object
2959         """
2960         events = self.instance_events.clear_events_for_instance(instance)
2961         if events:
2962             LOG.debug('Events pending at deletion: %(events)s',
2963                       {'events': ','.join(events.keys())},
2964                       instance=instance)
2965         self._notify_about_instance_usage(context, instance,
2966                                           "delete.start")
2967         compute_utils.notify_about_instance_action(context, instance,
2968                 self.host, action=fields.NotificationAction.DELETE,
2969                 phase=fields.NotificationPhase.START, bdms=bdms)
2970 
2971         self._shutdown_instance(context, instance, bdms)
2972 
2973         # NOTE(vish): We have already deleted the instance, so we have
2974         #             to ignore problems cleaning up the volumes. It
2975         #             would be nice to let the user know somehow that
2976         #             the volume deletion failed, but it is not
2977         #             acceptable to have an instance that can not be
2978         #             deleted. Perhaps this could be reworked in the
2979         #             future to set an instance fault the first time
2980         #             and to only ignore the failure if the instance
2981         #             is already in ERROR.
2982 
2983         # NOTE(ameeda): The volumes have already been detached during
2984         #               the above _shutdown_instance() call and this is
2985         #               why detach is not requested from
2986         #               _cleanup_volumes() in this case
2987 
2988         self._cleanup_volumes(context, instance, bdms,
2989                 raise_exc=False, detach=False)
2990         # Delete Cyborg ARQs if the instance has a device profile.
2991         compute_utils.delete_arqs_if_needed(context, instance)
2992         # if a delete task succeeded, always update vm state and task
2993         # state without expecting task state to be DELETING
2994         instance.vm_state = vm_states.DELETED
2995         instance.task_state = None
2996         instance.power_state = power_state.NOSTATE
2997         instance.terminated_at = timeutils.utcnow()
2998         instance.save()
2999 
3000         self._complete_deletion(context, instance)
3001         # only destroy the instance in the db if the _complete_deletion
3002         # doesn't raise and therefore allocation is successfully
3003         # deleted in placement
3004         instance.destroy()
3005 
3006         self._notify_about_instance_usage(context, instance, "delete.end")
3007         compute_utils.notify_about_instance_action(context, instance,
3008                 self.host, action=fields.NotificationAction.DELETE,
3009                 phase=fields.NotificationPhase.END, bdms=bdms)
3010 
3011     @wrap_exception()
3012     @reverts_task_state
3013     @wrap_instance_event(prefix='compute')
3014     @wrap_instance_fault
3015     def terminate_instance(self, context, instance, bdms):
3016         """Terminate an instance on this host."""
3017         @utils.synchronized(instance.uuid)
3018         def do_terminate_instance(instance, bdms):
3019             # NOTE(mriedem): If we are deleting the instance while it was
3020             # booting from volume, we could be racing with a database update of
3021             # the BDM volume_id. Since the compute API passes the BDMs over RPC
3022             # to compute here, the BDMs may be stale at this point. So check
3023             # for any volume BDMs that don't have volume_id set and if we
3024             # detect that, we need to refresh the BDM list before proceeding.
3025             # TODO(mriedem): Move this into _delete_instance and make the bdms
3026             # parameter optional.
3027             for bdm in list(bdms):
3028                 if bdm.is_volume and not bdm.volume_id:
3029                     LOG.debug('There are potentially stale BDMs during '
3030                               'delete, refreshing the BlockDeviceMappingList.',
3031                               instance=instance)
3032                     bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
3033                         context, instance.uuid)
3034                     break
3035             try:
3036                 self._delete_instance(context, instance, bdms)
3037             except exception.InstanceNotFound:
3038                 LOG.info("Instance disappeared during terminate",
3039                          instance=instance)
3040             except Exception:
3041                 # As we're trying to delete always go to Error if something
3042                 # goes wrong that _delete_instance can't handle.
3043                 with excutils.save_and_reraise_exception():
3044                     LOG.exception('Setting instance vm_state to ERROR',
3045                                   instance=instance)
3046                     self._set_instance_obj_error_state(instance)
3047 
3048         do_terminate_instance(instance, bdms)
3049 
3050     # NOTE(johannes): This is probably better named power_off_instance
3051     # so it matches the driver method, but because of other issues, we
3052     # can't use that name in grizzly.
3053     @wrap_exception()
3054     @reverts_task_state
3055     @wrap_instance_event(prefix='compute')
3056     @wrap_instance_fault
3057     def stop_instance(self, context, instance, clean_shutdown):
3058         """Stopping an instance on this host."""
3059 
3060         @utils.synchronized(instance.uuid)
3061         def do_stop_instance():
3062             current_power_state = self._get_power_state(instance)
3063             LOG.debug('Stopping instance; current vm_state: %(vm_state)s, '
3064                       'current task_state: %(task_state)s, current DB '
3065                       'power_state: %(db_power_state)s, current VM '
3066                       'power_state: %(current_power_state)s',
3067                       {'vm_state': instance.vm_state,
3068                        'task_state': instance.task_state,
3069                        'db_power_state': instance.power_state,
3070                        'current_power_state': current_power_state},
3071                       instance_uuid=instance.uuid)
3072 
3073             # NOTE(mriedem): If the instance is already powered off, we are
3074             # possibly tearing down and racing with other operations, so we can
3075             # expect the task_state to be None if something else updates the
3076             # instance and we're not locking it.
3077             expected_task_state = [task_states.POWERING_OFF]
3078             # The list of power states is from _sync_instance_power_state.
3079             if current_power_state in (power_state.NOSTATE,
3080                                        power_state.SHUTDOWN,
3081                                        power_state.CRASHED):
3082                 LOG.info('Instance is already powered off in the '
3083                          'hypervisor when stop is called.',
3084                          instance=instance)
3085                 expected_task_state.append(None)
3086 
3087             self._notify_about_instance_usage(context, instance,
3088                                               "power_off.start")
3089 
3090             compute_utils.notify_about_instance_action(context, instance,
3091                         self.host, action=fields.NotificationAction.POWER_OFF,
3092                         phase=fields.NotificationPhase.START)
3093 
3094             self._power_off_instance(instance, clean_shutdown)
3095             instance.power_state = self._get_power_state(instance)
3096             instance.vm_state = vm_states.STOPPED
3097             instance.task_state = None
3098             instance.save(expected_task_state=expected_task_state)
3099             self._notify_about_instance_usage(context, instance,
3100                                               "power_off.end")
3101 
3102             compute_utils.notify_about_instance_action(context, instance,
3103                         self.host, action=fields.NotificationAction.POWER_OFF,
3104                         phase=fields.NotificationPhase.END)
3105 
3106         do_stop_instance()
3107 
3108     def _power_on(self, context, instance):
3109         network_info = self.network_api.get_instance_nw_info(context, instance)
3110         block_device_info = self._get_instance_block_device_info(context,
3111                                                                  instance)
3112         accel_info = self._get_accel_info(context, instance)
3113         self.driver.power_on(context, instance,
3114                              network_info,
3115                              block_device_info, accel_info)
3116 
3117     def _delete_snapshot_of_shelved_instance(self, context, instance,
3118                                              snapshot_id):
3119         """Delete snapshot of shelved instance."""
3120         try:
3121             self.image_api.delete(context, snapshot_id)
3122         except (exception.ImageNotFound,
3123                 exception.ImageNotAuthorized) as exc:
3124             LOG.warning("Failed to delete snapshot "
3125                         "from shelved instance (%s).",
3126                         exc.format_message(), instance=instance)
3127         except Exception:
3128             LOG.exception("Something wrong happened when trying to "
3129                           "delete snapshot from shelved instance.",
3130                           instance=instance)
3131 
3132     # NOTE(johannes): This is probably better named power_on_instance
3133     # so it matches the driver method, but because of other issues, we
3134     # can't use that name in grizzly.
3135     @wrap_exception()
3136     @reverts_task_state
3137     @wrap_instance_event(prefix='compute')
3138     @wrap_instance_fault
3139     def start_instance(self, context, instance):
3140         """Starting an instance on this host."""
3141         self._notify_about_instance_usage(context, instance, "power_on.start")
3142         compute_utils.notify_about_instance_action(context, instance,
3143             self.host, action=fields.NotificationAction.POWER_ON,
3144             phase=fields.NotificationPhase.START)
3145         self._power_on(context, instance)
3146         instance.power_state = self._get_power_state(instance)
3147         instance.vm_state = vm_states.ACTIVE
3148         instance.task_state = None
3149 
3150         # Delete an image(VM snapshot) for a shelved instance
3151         snapshot_id = instance.system_metadata.get('shelved_image_id')
3152         if snapshot_id:
3153             self._delete_snapshot_of_shelved_instance(context, instance,
3154                                                       snapshot_id)
3155 
3156         # Delete system_metadata for a shelved instance
3157         compute_utils.remove_shelved_keys_from_system_metadata(instance)
3158 
3159         instance.save(expected_task_state=task_states.POWERING_ON)
3160         self._notify_about_instance_usage(context, instance, "power_on.end")
3161         compute_utils.notify_about_instance_action(context, instance,
3162             self.host, action=fields.NotificationAction.POWER_ON,
3163             phase=fields.NotificationPhase.END)
3164 
3165     @messaging.expected_exceptions(NotImplementedError,
3166                                    exception.TriggerCrashDumpNotSupported,
3167                                    exception.InstanceNotRunning)
3168     @wrap_exception()
3169     @wrap_instance_event(prefix='compute')
3170     @wrap_instance_fault
3171     def trigger_crash_dump(self, context, instance):
3172         """Trigger crash dump in an instance."""
3173 
3174         self._notify_about_instance_usage(context, instance,
3175                                           "trigger_crash_dump.start")
3176         compute_utils.notify_about_instance_action(context, instance,
3177                 self.host, action=fields.NotificationAction.TRIGGER_CRASH_DUMP,
3178                 phase=fields.NotificationPhase.START)
3179 
3180         # This method does not change task_state and power_state because the
3181         # effect of a trigger depends on user's configuration.
3182         self.driver.trigger_crash_dump(instance)
3183 
3184         self._notify_about_instance_usage(context, instance,
3185                                           "trigger_crash_dump.end")
3186         compute_utils.notify_about_instance_action(context, instance,
3187                 self.host, action=fields.NotificationAction.TRIGGER_CRASH_DUMP,
3188                 phase=fields.NotificationPhase.END)
3189 
3190     @wrap_exception()
3191     @reverts_task_state
3192     @wrap_instance_event(prefix='compute')
3193     @wrap_instance_fault
3194     def soft_delete_instance(self, context, instance):
3195         """Soft delete an instance on this host."""
3196         with compute_utils.notify_about_instance_delete(
3197                 self.notifier, context, instance, 'soft_delete',
3198                 source=fields.NotificationSource.COMPUTE):
3199             try:
3200                 self.driver.soft_delete(instance)
3201             except NotImplementedError:
3202                 # Fallback to just powering off the instance if the
3203                 # hypervisor doesn't implement the soft_delete method
3204                 self.driver.power_off(instance)
3205             instance.power_state = self._get_power_state(instance)
3206             instance.vm_state = vm_states.SOFT_DELETED
3207             instance.task_state = None
3208             instance.save(expected_task_state=[task_states.SOFT_DELETING])
3209 
3210     @wrap_exception()
3211     @reverts_task_state
3212     @wrap_instance_event(prefix='compute')
3213     @wrap_instance_fault
3214     def restore_instance(self, context, instance):
3215         """Restore a soft-deleted instance on this host."""
3216         self._notify_about_instance_usage(context, instance, "restore.start")
3217         compute_utils.notify_about_instance_action(context, instance,
3218             self.host, action=fields.NotificationAction.RESTORE,
3219             phase=fields.NotificationPhase.START)
3220         try:
3221             self.driver.restore(instance)
3222         except NotImplementedError:
3223             # Fallback to just powering on the instance if the hypervisor
3224             # doesn't implement the restore method
3225             self._power_on(context, instance)
3226         instance.power_state = self._get_power_state(instance)
3227         instance.vm_state = vm_states.ACTIVE
3228         instance.task_state = None
3229         instance.save(expected_task_state=task_states.RESTORING)
3230         self._notify_about_instance_usage(context, instance, "restore.end")
3231         compute_utils.notify_about_instance_action(context, instance,
3232             self.host, action=fields.NotificationAction.RESTORE,
3233             phase=fields.NotificationPhase.END)
3234 
3235     @staticmethod
3236     def _set_migration_status(migration, status):
3237         """Set the status, and guard against a None being passed in.
3238 
3239         This is useful as some of the compute RPC calls will not pass
3240         a migration object in older versions. The check can be removed when
3241         we move past 4.x major version of the RPC API.
3242         """
3243         if migration:
3244             migration.status = status
3245             migration.save()
3246 
3247     def _rebuild_default_impl(
3248             self, context, instance, image_meta, injected_files,
3249             admin_password, allocations, bdms, detach_block_devices,
3250             attach_block_devices, network_info=None, evacuate=False,
3251             block_device_info=None, preserve_ephemeral=False,
3252             accel_uuids=None):
3253         if preserve_ephemeral:
3254             # The default code path does not support preserving ephemeral
3255             # partitions.
3256             raise exception.PreserveEphemeralNotSupported()
3257 
3258         accel_info = []
3259         if evacuate:
3260             if instance.flavor.extra_specs.get('accel:device_profile'):
3261                 try:
3262                     accel_info = self._get_bound_arq_resources(
3263                         context, instance, accel_uuids or [])
3264                 except (Exception, eventlet.timeout.Timeout) as exc:
3265                     LOG.exception(exc)
3266                     self._build_resources_cleanup(instance, network_info)
3267                     msg = _('Failure getting accelerator resources.')
3268                     raise exception.BuildAbortException(
3269                         instance_uuid=instance.uuid, reason=msg)
3270             detach_block_devices(context, bdms)
3271         else:
3272             self._power_off_instance(instance, clean_shutdown=True)
3273             detach_block_devices(context, bdms)
3274             self.driver.destroy(context, instance,
3275                                 network_info=network_info,
3276                                 block_device_info=block_device_info)
3277             try:
3278                 accel_info = self._get_accel_info(context, instance)
3279             except Exception as exc:
3280                 LOG.exception(exc)
3281                 self._build_resources_cleanup(instance, network_info)
3282                 msg = _('Failure getting accelerator resources.')
3283                 raise exception.BuildAbortException(
3284                     instance_uuid=instance.uuid, reason=msg)
3285 
3286         instance.task_state = task_states.REBUILD_BLOCK_DEVICE_MAPPING
3287         instance.save(expected_task_state=[task_states.REBUILDING])
3288 
3289         new_block_device_info = attach_block_devices(context, instance, bdms)
3290 
3291         instance.task_state = task_states.REBUILD_SPAWNING
3292         instance.save(
3293             expected_task_state=[task_states.REBUILD_BLOCK_DEVICE_MAPPING])
3294 
3295         with instance.mutated_migration_context():
3296             self.driver.spawn(context, instance, image_meta, injected_files,
3297                               admin_password, allocations,
3298                               network_info=network_info,
3299                               block_device_info=new_block_device_info,
3300                               accel_info=accel_info)
3301 
3302     def _notify_instance_rebuild_error(self, context, instance, error, bdms):
3303         self._notify_about_instance_usage(context, instance,
3304                                           'rebuild.error', fault=error)
3305         compute_utils.notify_about_instance_rebuild(
3306             context, instance, self.host,
3307             phase=fields.NotificationPhase.ERROR, exception=error, bdms=bdms)
3308 
3309     @messaging.expected_exceptions(exception.PreserveEphemeralNotSupported,
3310                                    exception.BuildAbortException)
3311     @wrap_exception()
3312     @reverts_task_state
3313     @wrap_instance_event(prefix='compute')
3314     @wrap_instance_fault
3315     def rebuild_instance(self, context, instance, orig_image_ref, image_ref,
3316                          injected_files, new_pass, orig_sys_metadata,
3317                          bdms, recreate, on_shared_storage,
3318                          preserve_ephemeral, migration,
3319                          scheduled_node, limits, request_spec,
3320                          accel_uuids=None):
3321         """Destroy and re-make this instance.
3322 
3323         A 'rebuild' effectively purges all existing data from the system and
3324         remakes the VM with given 'metadata' and 'personalities'.
3325 
3326         :param context: `nova.RequestContext` object
3327         :param instance: Instance object
3328         :param orig_image_ref: Original image_ref before rebuild
3329         :param image_ref: New image_ref for rebuild
3330         :param injected_files: Files to inject
3331         :param new_pass: password to set on rebuilt instance
3332         :param orig_sys_metadata: instance system metadata from pre-rebuild
3333         :param bdms: block-device-mappings to use for rebuild
3334         :param recreate: True if the instance is being evacuated (e.g. the
3335             hypervisor it was on failed) - cleanup of old state will be
3336             skipped.
3337         :param on_shared_storage: True if instance files on shared storage.
3338                                   If not provided then information from the
3339                                   driver will be used to decide if the instance
3340                                   files are available or not on the target host
3341         :param preserve_ephemeral: True if the default ephemeral storage
3342                                    partition must be preserved on rebuild
3343         :param migration: a Migration object if one was created for this
3344                           rebuild operation (if it's a part of evacuate)
3345         :param scheduled_node: A node of the host chosen by the scheduler. If a
3346                                host was specified by the user, this will be
3347                                None
3348         :param limits: Overcommit limits set by the scheduler. If a host was
3349                        specified by the user, this will be None
3350         :param request_spec: a RequestSpec object used to schedule the instance
3351         :param accel_uuids: a list of cyborg ARQ uuids or None if the RPC API
3352                             is <=5.11
3353 
3354         """
3355         # recreate=True means the instance is being evacuated from a failed
3356         # host to a new destination host (this host). The 'recreate' variable
3357         # name is confusing, so rename it to evacuate here at the top, which
3358         # is simpler than renaming a parameter in an RPC versioned method.
3359         evacuate = recreate
3360         context = context.elevated()
3361 
3362         if evacuate:
3363             LOG.info("Evacuating instance", instance=instance)
3364         else:
3365             LOG.info("Rebuilding instance", instance=instance)
3366 
3367         if evacuate:
3368             # This is an evacuation to a new host, so we need to perform a
3369             # resource claim.
3370             rebuild_claim = self.rt.rebuild_claim
3371         else:
3372             # This is a rebuild to the same host, so we don't need to make
3373             # a claim since the instance is already on this host.
3374             rebuild_claim = claims.NopClaim
3375 
3376         if image_ref:
3377             image_meta = objects.ImageMeta.from_image_ref(
3378                 context, self.image_api, image_ref)
3379         elif evacuate:
3380             # For evacuate the API does not send down the image_ref since the
3381             # image does not change so just get it from what was stashed in
3382             # the instance system_metadata when the instance was created (or
3383             # last rebuilt). This also works for volume-backed instances.
3384             image_meta = instance.image_meta
3385         else:
3386             image_meta = objects.ImageMeta()
3387 
3388         # NOTE(mriedem): On an evacuate, we need to update
3389         # the instance's host and node properties to reflect it's
3390         # destination node for the evacuate.
3391         if not scheduled_node:
3392             if evacuate:
3393                 try:
3394                     compute_node = self._get_compute_info(context, self.host)
3395                     scheduled_node = compute_node.hypervisor_hostname
3396                 except exception.ComputeHostNotFound:
3397                     LOG.exception('Failed to get compute_info for %s',
3398                                   self.host)
3399             else:
3400                 scheduled_node = instance.node
3401 
3402         allocs = self.reportclient.get_allocations_for_consumer(
3403                     context, instance.uuid)
3404 
3405         # If the resource claim or group policy validation fails before we
3406         # do anything to the guest or its networking/volumes we want to keep
3407         # the current status rather than put the instance into ERROR status.
3408         instance_state = instance.vm_state
3409         with self._error_out_instance_on_exception(
3410                 context, instance, instance_state=instance_state):
3411             try:
3412                 self._do_rebuild_instance_with_claim(
3413                     context, instance, orig_image_ref,
3414                     image_meta, injected_files, new_pass, orig_sys_metadata,
3415                     bdms, evacuate, on_shared_storage, preserve_ephemeral,
3416                     migration, request_spec, allocs, rebuild_claim,
3417                     scheduled_node, limits, accel_uuids)
3418             except (exception.ComputeResourcesUnavailable,
3419                     exception.RescheduledException) as e:
3420                 if isinstance(e, exception.ComputeResourcesUnavailable):
3421                     LOG.debug("Could not rebuild instance on this host, not "
3422                               "enough resources available.", instance=instance)
3423                 else:
3424                     # RescheduledException is raised by the late server group
3425                     # policy check during evacuation if a parallel scheduling
3426                     # violated the policy.
3427                     # We catch the RescheduledException here but we don't have
3428                     # the plumbing to do an actual reschedule so we abort the
3429                     # operation.
3430                     LOG.debug("Could not rebuild instance on this host, "
3431                               "late server group check failed.",
3432                               instance=instance)
3433                 # NOTE(ndipanov): We just abort the build for now and leave a
3434                 # migration record for potential cleanup later
3435                 self._set_migration_status(migration, 'failed')
3436                 # Since the claim failed, we need to remove the allocation
3437                 # created against the destination node. Note that we can only
3438                 # get here when evacuating to a destination node. Rebuilding
3439                 # on the same host (not evacuate) uses the NopClaim which will
3440                 # not raise ComputeResourcesUnavailable.
3441                 self.rt.delete_allocation_for_evacuated_instance(
3442                     context, instance, scheduled_node, node_type='destination')
3443                 self._notify_instance_rebuild_error(context, instance, e, bdms)
3444                 # Wrap this in InstanceFaultRollback so that the
3445                 # _error_out_instance_on_exception context manager keeps the
3446                 # vm_state unchanged.
3447                 raise exception.InstanceFaultRollback(
3448                     inner_exception=exception.BuildAbortException(
3449                         instance_uuid=instance.uuid,
3450                         reason=e.format_message()))
3451             except (exception.InstanceNotFound,
3452                     exception.UnexpectedDeletingTaskStateError) as e:
3453                 LOG.debug('Instance was deleted while rebuilding',
3454                           instance=instance)
3455                 self._set_migration_status(migration, 'failed')
3456                 self._notify_instance_rebuild_error(context, instance, e, bdms)
3457             except Exception as e:
3458                 self._set_migration_status(migration, 'failed')
3459                 if evacuate or scheduled_node is not None:
3460                     self.rt.delete_allocation_for_evacuated_instance(
3461                         context, instance, scheduled_node,
3462                         node_type='destination')
3463                 self._notify_instance_rebuild_error(context, instance, e, bdms)
3464                 raise
3465             else:
3466                 # NOTE(gibi): Let the resource tracker set the instance
3467                 # host and drop the migration context as we need to hold the
3468                 # COMPUTE_RESOURCE_SEMAPHORE to avoid the race with
3469                 # _update_available_resources. See bug 1896463.
3470                 self.rt.finish_evacuation(instance, scheduled_node, migration)
3471 
3472     def _do_rebuild_instance_with_claim(
3473             self, context, instance, orig_image_ref, image_meta,
3474             injected_files, new_pass, orig_sys_metadata, bdms, evacuate,
3475             on_shared_storage, preserve_ephemeral, migration, request_spec,
3476             allocations, rebuild_claim, scheduled_node, limits, accel_uuids):
3477         """Helper to avoid deep nesting in the top-level method."""
3478 
3479         provider_mapping = None
3480         if evacuate:
3481             provider_mapping = self._get_request_group_mapping(request_spec)
3482 
3483             if provider_mapping:
3484                 compute_utils.\
3485                     update_pci_request_spec_with_allocated_interface_name(
3486                         context, self.reportclient,
3487                         instance.pci_requests.requests, provider_mapping)
3488 
3489         claim_context = rebuild_claim(
3490             context, instance, scheduled_node, allocations,
3491             limits=limits, image_meta=image_meta, migration=migration)
3492 
3493         with claim_context:
3494             self._do_rebuild_instance(
3495                 context, instance, orig_image_ref, image_meta, injected_files,
3496                 new_pass, orig_sys_metadata, bdms, evacuate, on_shared_storage,
3497                 preserve_ephemeral, migration, request_spec, allocations,
3498                 provider_mapping, accel_uuids)
3499 
3500     @staticmethod
3501     def _get_image_name(image_meta):
3502         if image_meta.obj_attr_is_set("name"):
3503             return image_meta.name
3504         else:
3505             return ''
3506 
3507     def _do_rebuild_instance(
3508             self, context, instance, orig_image_ref, image_meta,
3509             injected_files, new_pass, orig_sys_metadata, bdms, evacuate,
3510             on_shared_storage, preserve_ephemeral, migration, request_spec,
3511             allocations, request_group_resource_providers_mapping,
3512             accel_uuids):
3513         orig_vm_state = instance.vm_state
3514 
3515         if evacuate:
3516             if request_spec:
3517                 # NOTE(gibi): Do a late check of server group policy as
3518                 # parallel scheduling could violate such policy. This will
3519                 # cause the evacuate to fail as rebuild does not implement
3520                 # reschedule.
3521                 hints = self._get_scheduler_hints({}, request_spec)
3522                 self._validate_instance_group_policy(context, instance, hints)
3523 
3524             if not self.driver.capabilities.get("supports_evacuate", False):
3525                 raise exception.InstanceEvacuateNotSupported
3526 
3527             self._check_instance_exists(instance)
3528 
3529             if on_shared_storage is None:
3530                 LOG.debug('on_shared_storage is not provided, using driver '
3531                           'information to decide if the instance needs to '
3532                           'be evacuated')
3533                 on_shared_storage = self.driver.instance_on_disk(instance)
3534 
3535             elif (on_shared_storage !=
3536                     self.driver.instance_on_disk(instance)):
3537                 # To cover case when admin expects that instance files are
3538                 # on shared storage, but not accessible and vice versa
3539                 raise exception.InvalidSharedStorage(
3540                         _("Invalid state of instance files on shared"
3541                             " storage"))
3542 
3543             if on_shared_storage:
3544                 LOG.info('disk on shared storage, evacuating using'
3545                          ' existing disk')
3546             elif instance.image_ref:
3547                 orig_image_ref = instance.image_ref
3548                 LOG.info("disk not on shared storage, evacuating from "
3549                          "image: '%s'", str(orig_image_ref))
3550             else:
3551                 LOG.info('disk on volume, evacuating using existing '
3552                          'volume')
3553 
3554         # We check trusted certs capabilities for both evacuate (rebuild on
3555         # another host) and rebuild (rebuild on the same host) because for
3556         # evacuate we need to make sure an instance with trusted certs can
3557         # have the image verified with those certs during rebuild, and for
3558         # rebuild we could be rebuilding a server that started out with no
3559         # trusted certs on this host, and then was rebuilt with trusted certs
3560         # for a new image, in which case we need to validate that new image
3561         # with the trusted certs during the rebuild.
3562         self._check_trusted_certs(instance)
3563 
3564         # This instance.exists message should contain the original
3565         # image_ref, not the new one.  Since the DB has been updated
3566         # to point to the new one... we have to override it.
3567         orig_image_ref_url = self.image_api.generate_image_url(orig_image_ref,
3568                                                                context)
3569         extra_usage_info = {'image_ref_url': orig_image_ref_url}
3570         compute_utils.notify_usage_exists(
3571                 self.notifier, context, instance, self.host,
3572                 current_period=True, system_metadata=orig_sys_metadata,
3573                 extra_usage_info=extra_usage_info)
3574 
3575         # This message should contain the new image_ref
3576         extra_usage_info = {'image_name': self._get_image_name(image_meta)}
3577         self._notify_about_instance_usage(context, instance,
3578                 "rebuild.start", extra_usage_info=extra_usage_info)
3579         # NOTE: image_name is not included in the versioned notification
3580         # because we already provide the image_uuid in the notification
3581         # payload and the image details can be looked up via the uuid.
3582         compute_utils.notify_about_instance_rebuild(
3583             context, instance, self.host,
3584             phase=fields.NotificationPhase.START,
3585             bdms=bdms)
3586 
3587         instance.power_state = self._get_power_state(instance)
3588         instance.task_state = task_states.REBUILDING
3589         instance.save(expected_task_state=[task_states.REBUILDING])
3590 
3591         if evacuate:
3592             self.network_api.setup_networks_on_host(
3593                     context, instance, self.host)
3594             # For nova-network this is needed to move floating IPs
3595             # For neutron this updates the host in the port binding
3596             # TODO(cfriesen): this network_api call and the one above
3597             # are so similar, we should really try to unify them.
3598             self.network_api.setup_instance_network_on_host(
3599                 context, instance, self.host, migration,
3600                 provider_mappings=request_group_resource_providers_mapping)
3601             # TODO(mriedem): Consider decorating setup_instance_network_on_host
3602             # with @api.refresh_cache and then we wouldn't need this explicit
3603             # call to get_instance_nw_info.
3604             network_info = self.network_api.get_instance_nw_info(context,
3605                                                                  instance)
3606         else:
3607             network_info = instance.get_network_info()
3608 
3609         if bdms is None:
3610             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
3611                     context, instance.uuid)
3612 
3613         block_device_info = \
3614             self._get_instance_block_device_info(
3615                     context, instance, bdms=bdms)
3616 
3617         def detach_block_devices(context, bdms):
3618             for bdm in bdms:
3619                 if bdm.is_volume:
3620                     # NOTE (ildikov): Having the attachment_id set in the BDM
3621                     # means that it's the new Cinder attach/detach flow
3622                     # (available from v3.44). In that case we explicitly
3623                     # attach and detach the volumes through attachment level
3624                     # operations. In this scenario _detach_volume will delete
3625                     # the existing attachment which would make the volume
3626                     # status change to 'available' if we don't pre-create
3627                     # another empty attachment before deleting the old one.
3628                     attachment_id = None
3629                     if bdm.attachment_id:
3630                         attachment_id = self.volume_api.attachment_create(
3631                             context, bdm['volume_id'], instance.uuid)['id']
3632                     self._detach_volume(context, bdm, instance,
3633                                         destroy_bdm=False)
3634                     if attachment_id:
3635                         bdm.attachment_id = attachment_id
3636                         bdm.save()
3637 
3638         files = self._decode_files(injected_files)
3639 
3640         kwargs = dict(
3641             context=context,
3642             instance=instance,
3643             image_meta=image_meta,
3644             injected_files=files,
3645             admin_password=new_pass,
3646             allocations=allocations,
3647             bdms=bdms,
3648             detach_block_devices=detach_block_devices,
3649             attach_block_devices=self._prep_block_device,
3650             block_device_info=block_device_info,
3651             network_info=network_info,
3652             preserve_ephemeral=preserve_ephemeral,
3653             evacuate=evacuate,
3654             accel_uuids=accel_uuids)
3655         try:
3656             with instance.mutated_migration_context():
3657                 self.driver.rebuild(**kwargs)
3658         except NotImplementedError:
3659             # NOTE(rpodolyaka): driver doesn't provide specialized version
3660             # of rebuild, fall back to the default implementation
3661             self._rebuild_default_impl(**kwargs)
3662         self._update_instance_after_spawn(instance)
3663         instance.save(expected_task_state=[task_states.REBUILD_SPAWNING])
3664 
3665         if orig_vm_state == vm_states.STOPPED:
3666             LOG.info("bringing vm to original state: '%s'",
3667                      orig_vm_state, instance=instance)
3668             instance.vm_state = vm_states.ACTIVE
3669             instance.task_state = task_states.POWERING_OFF
3670             instance.progress = 0
3671             instance.save()
3672             self.stop_instance(context, instance, False)
3673         # TODO(melwitt): We should clean up instance console tokens here in the
3674         # case of evacuate. The instance is on a new host and will need to
3675         # establish a new console connection.
3676         self._update_scheduler_instance_info(context, instance)
3677         self._notify_about_instance_usage(
3678                 context, instance, "rebuild.end",
3679                 network_info=network_info,
3680                 extra_usage_info=extra_usage_info)
3681         compute_utils.notify_about_instance_rebuild(
3682             context, instance, self.host,
3683             phase=fields.NotificationPhase.END,
3684             bdms=bdms)
3685 
3686     def _handle_bad_volumes_detached(self, context, instance, bad_devices,
3687                                      block_device_info):
3688         """Handle cases where the virt-layer had to detach non-working volumes
3689         in order to complete an operation.
3690         """
3691         for bdm in block_device_info['block_device_mapping']:
3692             if bdm.get('mount_device') in bad_devices:
3693                 try:
3694                     volume_id = bdm['connection_info']['data']['volume_id']
3695                 except KeyError:
3696                     continue
3697 
3698                 # NOTE(sirp): ideally we'd just call
3699                 # `compute_api.detach_volume` here but since that hits the
3700                 # DB directly, that's off limits from within the
3701                 # compute-manager.
3702                 #
3703                 # API-detach
3704                 LOG.info("Detaching from volume api: %s", volume_id)
3705                 self.volume_api.begin_detaching(context, volume_id)
3706 
3707                 # Manager-detach
3708                 self.detach_volume(context, volume_id, instance)
3709 
3710     def _get_accel_info(self, context, instance):
3711         dp_name = instance.flavor.extra_specs.get('accel:device_profile')
3712         if dp_name:
3713             cyclient = cyborg.get_client(context)
3714             accel_info = cyclient.get_arqs_for_instance(instance.uuid)
3715         else:
3716             accel_info = []
3717         return accel_info
3718 
3719     @wrap_exception()
3720     @reverts_task_state
3721     @wrap_instance_event(prefix='compute')
3722     @wrap_instance_fault
3723     def reboot_instance(self, context, instance, block_device_info,
3724                         reboot_type):
3725         @utils.synchronized(instance.uuid)
3726         def do_reboot_instance(context, instance, block_device_info,
3727                                reboot_type):
3728             self._reboot_instance(context, instance, block_device_info,
3729                                   reboot_type)
3730         do_reboot_instance(context, instance, block_device_info, reboot_type)
3731 
3732     def _reboot_instance(self, context, instance, block_device_info,
3733                          reboot_type):
3734         """Reboot an instance on this host."""
3735         # acknowledge the request made it to the manager
3736         if reboot_type == "SOFT":
3737             instance.task_state = task_states.REBOOT_PENDING
3738             expected_states = task_states.soft_reboot_states
3739         else:
3740             instance.task_state = task_states.REBOOT_PENDING_HARD
3741             expected_states = task_states.hard_reboot_states
3742 
3743         context = context.elevated()
3744         LOG.info("Rebooting instance", instance=instance)
3745 
3746         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
3747             context, instance.uuid)
3748         block_device_info = self._get_instance_block_device_info(
3749             context, instance, bdms=bdms)
3750 
3751         network_info = self.network_api.get_instance_nw_info(context, instance)
3752 
3753         accel_info = self._get_accel_info(context, instance)
3754 
3755         self._notify_about_instance_usage(context, instance, "reboot.start")
3756         compute_utils.notify_about_instance_action(
3757             context, instance, self.host,
3758             action=fields.NotificationAction.REBOOT,
3759             phase=fields.NotificationPhase.START,
3760             bdms=bdms
3761         )
3762 
3763         instance.power_state = self._get_power_state(instance)
3764         instance.save(expected_task_state=expected_states)
3765 
3766         if instance.power_state != power_state.RUNNING:
3767             state = instance.power_state
3768             running = power_state.RUNNING
3769             LOG.warning('trying to reboot a non-running instance:'
3770                         ' (state: %(state)s expected: %(running)s)',
3771                         {'state': state, 'running': running},
3772                         instance=instance)
3773 
3774         def bad_volumes_callback(bad_devices):
3775             self._handle_bad_volumes_detached(
3776                     context, instance, bad_devices, block_device_info)
3777 
3778         try:
3779             # Don't change it out of rescue mode
3780             if instance.vm_state == vm_states.RESCUED:
3781                 new_vm_state = vm_states.RESCUED
3782             else:
3783                 new_vm_state = vm_states.ACTIVE
3784             new_power_state = None
3785             if reboot_type == "SOFT":
3786                 instance.task_state = task_states.REBOOT_STARTED
3787                 expected_state = task_states.REBOOT_PENDING
3788             else:
3789                 instance.task_state = task_states.REBOOT_STARTED_HARD
3790                 expected_state = task_states.REBOOT_PENDING_HARD
3791             instance.save(expected_task_state=expected_state)
3792             self.driver.reboot(context, instance,
3793                                network_info,
3794                                reboot_type,
3795                                block_device_info=block_device_info,
3796                                accel_info=accel_info,
3797                                bad_volumes_callback=bad_volumes_callback)
3798 
3799         except Exception as error:
3800             with excutils.save_and_reraise_exception() as ctxt:
3801                 exc_info = sys.exc_info()
3802                 # if the reboot failed but the VM is running don't
3803                 # put it into an error state
3804                 new_power_state = self._get_power_state(instance)
3805                 if new_power_state == power_state.RUNNING:
3806                     LOG.warning('Reboot failed but instance is running',
3807                                 instance=instance)
3808                     compute_utils.add_instance_fault_from_exc(context,
3809                             instance, error, exc_info)
3810                     self._notify_about_instance_usage(context, instance,
3811                             'reboot.error', fault=error)
3812                     compute_utils.notify_about_instance_action(
3813                         context, instance, self.host,
3814                         action=fields.NotificationAction.REBOOT,
3815                         phase=fields.NotificationPhase.ERROR,
3816                         exception=error, bdms=bdms
3817                     )
3818                     ctxt.reraise = False
3819                 else:
3820                     LOG.error('Cannot reboot instance: %s', error,
3821                               instance=instance)
3822                     self._set_instance_obj_error_state(instance)
3823 
3824         if not new_power_state:
3825             new_power_state = self._get_power_state(instance)
3826         try:
3827             instance.power_state = new_power_state
3828             instance.vm_state = new_vm_state
3829             instance.task_state = None
3830             instance.save()
3831         except exception.InstanceNotFound:
3832             LOG.warning("Instance disappeared during reboot",
3833                         instance=instance)
3834 
3835         self._notify_about_instance_usage(context, instance, "reboot.end")
3836         compute_utils.notify_about_instance_action(
3837             context, instance, self.host,
3838             action=fields.NotificationAction.REBOOT,
3839             phase=fields.NotificationPhase.END,
3840             bdms=bdms
3841         )
3842 
3843     @delete_image_on_error
3844     def _do_snapshot_instance(self, context, image_id, instance):
3845         self._snapshot_instance(context, image_id, instance,
3846                                 task_states.IMAGE_BACKUP)
3847 
3848     @wrap_exception()
3849     @reverts_task_state
3850     @wrap_instance_event(prefix='compute')
3851     @wrap_instance_fault
3852     def backup_instance(self, context, image_id, instance, backup_type,
3853                         rotation):
3854         """Backup an instance on this host.
3855 
3856         :param backup_type: daily | weekly
3857         :param rotation: int representing how many backups to keep around
3858         """
3859         self._do_snapshot_instance(context, image_id, instance)
3860         self._rotate_backups(context, instance, backup_type, rotation)
3861 
3862     @wrap_exception()
3863     @reverts_task_state
3864     @wrap_instance_event(prefix='compute')
3865     @wrap_instance_fault
3866     @delete_image_on_error
3867     def snapshot_instance(self, context, image_id, instance):
3868         """Snapshot an instance on this host.
3869 
3870         :param context: security context
3871         :param image_id: glance.db.sqlalchemy.models.Image.Id
3872         :param instance: a nova.objects.instance.Instance object
3873         """
3874         # NOTE(dave-mcnally) the task state will already be set by the api
3875         # but if the compute manager has crashed/been restarted prior to the
3876         # request getting here the task state may have been cleared so we set
3877         # it again and things continue normally
3878         try:
3879             instance.task_state = task_states.IMAGE_SNAPSHOT
3880             instance.save(
3881                         expected_task_state=task_states.IMAGE_SNAPSHOT_PENDING)
3882         except exception.InstanceNotFound:
3883             # possibility instance no longer exists, no point in continuing
3884             LOG.debug("Instance not found, could not set state %s "
3885                       "for instance.",
3886                       task_states.IMAGE_SNAPSHOT, instance=instance)
3887             return
3888 
3889         except exception.UnexpectedDeletingTaskStateError:
3890             LOG.debug("Instance being deleted, snapshot cannot continue",
3891                       instance=instance)
3892             return
3893 
3894         with self._snapshot_semaphore:
3895             self._snapshot_instance(context, image_id, instance,
3896                                     task_states.IMAGE_SNAPSHOT)
3897 
3898     def _snapshot_instance(self, context, image_id, instance,
3899                            expected_task_state):
3900         context = context.elevated()
3901 
3902         instance.power_state = self._get_power_state(instance)
3903         try:
3904             instance.save()
3905 
3906             LOG.info('instance snapshotting', instance=instance)
3907 
3908             if instance.power_state != power_state.RUNNING:
3909                 state = instance.power_state
3910                 running = power_state.RUNNING
3911                 LOG.warning('trying to snapshot a non-running instance: '
3912                             '(state: %(state)s expected: %(running)s)',
3913                             {'state': state, 'running': running},
3914                             instance=instance)
3915 
3916             self._notify_about_instance_usage(
3917                 context, instance, "snapshot.start")
3918             compute_utils.notify_about_instance_snapshot(context, instance,
3919                 self.host, phase=fields.NotificationPhase.START,
3920                 snapshot_image_id=image_id)
3921 
3922             def update_task_state(task_state,
3923                                   expected_state=expected_task_state):
3924                 instance.task_state = task_state
3925                 instance.save(expected_task_state=expected_state)
3926 
3927             with timeutils.StopWatch() as timer:
3928                 self.driver.snapshot(context, instance, image_id,
3929                                      update_task_state)
3930             LOG.info('Took %0.2f seconds to snapshot the instance on '
3931                      'the hypervisor.', timer.elapsed(), instance=instance)
3932 
3933             instance.task_state = None
3934             instance.save(expected_task_state=task_states.IMAGE_UPLOADING)
3935 
3936             self._notify_about_instance_usage(context, instance,
3937                                               "snapshot.end")
3938             compute_utils.notify_about_instance_snapshot(context, instance,
3939                 self.host, phase=fields.NotificationPhase.END,
3940                 snapshot_image_id=image_id)
3941         except (exception.InstanceNotFound,
3942                 exception.InstanceNotRunning,
3943                 exception.UnexpectedDeletingTaskStateError):
3944             # the instance got deleted during the snapshot
3945             # Quickly bail out of here
3946             msg = 'Instance disappeared during snapshot'
3947             LOG.debug(msg, instance=instance)
3948             try:
3949                 image = self.image_api.get(context, image_id)
3950                 if image['status'] != 'active':
3951                     self.image_api.delete(context, image_id)
3952             except exception.ImageNotFound:
3953                 LOG.debug('Image not found during clean up %s', image_id)
3954             except Exception:
3955                 LOG.warning("Error while trying to clean up image %s",
3956                             image_id, instance=instance)
3957         except exception.ImageNotFound:
3958             instance.task_state = None
3959             instance.save()
3960             LOG.warning("Image not found during snapshot", instance=instance)
3961 
3962     @messaging.expected_exceptions(NotImplementedError)
3963     @wrap_exception()
3964     def volume_snapshot_create(self, context, instance, volume_id,
3965                                create_info):
3966         try:
3967             self.driver.volume_snapshot_create(context, instance, volume_id,
3968                                                create_info)
3969         except exception.InstanceNotRunning:
3970             # Libvirt driver can raise this exception
3971             LOG.debug('Instance disappeared during volume snapshot create',
3972                       instance=instance)
3973 
3974     @messaging.expected_exceptions(NotImplementedError)
3975     @wrap_exception()
3976     def volume_snapshot_delete(self, context, instance, volume_id,
3977                                snapshot_id, delete_info):
3978         try:
3979             self.driver.volume_snapshot_delete(context, instance, volume_id,
3980                                                snapshot_id, delete_info)
3981         except exception.InstanceNotRunning:
3982             # Libvirt driver can raise this exception
3983             LOG.debug('Instance disappeared during volume snapshot delete',
3984                       instance=instance)
3985 
3986     @wrap_instance_fault
3987     def _rotate_backups(self, context, instance, backup_type, rotation):
3988         """Delete excess backups associated to an instance.
3989 
3990         Instances are allowed a fixed number of backups (the rotation number);
3991         this method deletes the oldest backups that exceed the rotation
3992         threshold.
3993 
3994         :param context: security context
3995         :param instance: Instance dict
3996         :param backup_type: a user-defined type, like "daily" or "weekly" etc.
3997         :param rotation: int representing how many backups to keep around;
3998             None if rotation shouldn't be used (as in the case of snapshots)
3999         """
4000         filters = {'property-image_type': 'backup',
4001                    'property-backup_type': backup_type,
4002                    'property-instance_uuid': instance.uuid}
4003 
4004         images = self.image_api.get_all(context, filters=filters,
4005                                         sort_key='created_at', sort_dir='desc')
4006         num_images = len(images)
4007         LOG.debug("Found %(num_images)d images (rotation: %(rotation)d)",
4008                   {'num_images': num_images, 'rotation': rotation},
4009                   instance=instance)
4010 
4011         if num_images > rotation:
4012             # NOTE(sirp): this deletes all backups that exceed the rotation
4013             # limit
4014             excess = len(images) - rotation
4015             LOG.debug("Rotating out %d backups", excess,
4016                       instance=instance)
4017             for i in range(excess):
4018                 image = images.pop()
4019                 image_id = image['id']
4020                 LOG.debug("Deleting image %s", image_id,
4021                           instance=instance)
4022                 try:
4023                     self.image_api.delete(context, image_id)
4024                 except exception.ImageNotFound:
4025                     LOG.info("Failed to find image %(image_id)s to "
4026                              "delete", {'image_id': image_id},
4027                              instance=instance)
4028                 except (exception.ImageDeleteConflict, Exception) as exc:
4029                     LOG.info("Failed to delete image %(image_id)s during "
4030                              "deleting excess backups. "
4031                              "Continuing for next image.. %(exc)s",
4032                              {'image_id': image_id, 'exc': exc},
4033                              instance=instance)
4034 
4035     @wrap_exception()
4036     @reverts_task_state
4037     @wrap_instance_event(prefix='compute')
4038     @wrap_instance_fault
4039     def set_admin_password(self, context, instance, new_pass):
4040         """Set the root/admin password for an instance on this host.
4041 
4042         This is generally only called by API password resets after an
4043         image has been built.
4044 
4045         @param context: Nova auth context.
4046         @param instance: Nova instance object.
4047         @param new_pass: The admin password for the instance.
4048         """
4049 
4050         context = context.elevated()
4051         current_power_state = self._get_power_state(instance)
4052         expected_state = power_state.RUNNING
4053 
4054         if current_power_state != expected_state:
4055             instance.task_state = None
4056             instance.save(expected_task_state=task_states.UPDATING_PASSWORD)
4057             _msg = _('instance %s is not running') % instance.uuid
4058             raise exception.InstancePasswordSetFailed(
4059                 instance=instance.uuid, reason=_msg)
4060 
4061         try:
4062             self.driver.set_admin_password(instance, new_pass)
4063             LOG.info("Admin password set", instance=instance)
4064             instance.task_state = None
4065             instance.save(
4066                 expected_task_state=task_states.UPDATING_PASSWORD)
4067         except exception.InstanceAgentNotEnabled:
4068             with excutils.save_and_reraise_exception():
4069                 LOG.debug('Guest agent is not enabled for the instance.',
4070                           instance=instance)
4071                 instance.task_state = None
4072                 instance.save(
4073                     expected_task_state=task_states.UPDATING_PASSWORD)
4074         except exception.SetAdminPasswdNotSupported:
4075             with excutils.save_and_reraise_exception():
4076                 LOG.info('set_admin_password is not supported '
4077                          'by this driver or guest instance.',
4078                          instance=instance)
4079                 instance.task_state = None
4080                 instance.save(
4081                     expected_task_state=task_states.UPDATING_PASSWORD)
4082         except NotImplementedError:
4083             LOG.warning('set_admin_password is not implemented '
4084                         'by this driver or guest instance.',
4085                         instance=instance)
4086             instance.task_state = None
4087             instance.save(
4088                 expected_task_state=task_states.UPDATING_PASSWORD)
4089             raise NotImplementedError(_('set_admin_password is not '
4090                                         'implemented by this driver or guest '
4091                                         'instance.'))
4092         except exception.UnexpectedTaskStateError:
4093             # interrupted by another (most likely delete) task
4094             # do not retry
4095             raise
4096         except Exception:
4097             # Catch all here because this could be anything.
4098             LOG.exception('set_admin_password failed', instance=instance)
4099             # We create a new exception here so that we won't
4100             # potentially reveal password information to the
4101             # API caller.  The real exception is logged above
4102             _msg = _('error setting admin password')
4103             raise exception.InstancePasswordSetFailed(
4104                 instance=instance.uuid, reason=_msg)
4105 
4106     def _get_rescue_image(self, context, instance, rescue_image_ref=None):
4107         """Determine what image should be used to boot the rescue VM."""
4108         # 1. If rescue_image_ref is passed in, use that for rescue.
4109         # 2. Else, use the base image associated with instance's current image.
4110         #       The idea here is to provide the customer with a rescue
4111         #       environment which they are familiar with.
4112         #       So, if they built their instance off of a Debian image,
4113         #       their rescue VM will also be Debian.
4114         # 3. As a last resort, use instance's current image.
4115         if not rescue_image_ref:
4116             system_meta = utils.instance_sys_meta(instance)
4117             rescue_image_ref = system_meta.get('image_base_image_ref')
4118 
4119         if not rescue_image_ref:
4120             LOG.warning('Unable to find a different image to use for '
4121                         'rescue VM, using instance\'s current image',
4122                         instance=instance)
4123             rescue_image_ref = instance.image_ref
4124 
4125         return objects.ImageMeta.from_image_ref(
4126             context, self.image_api, rescue_image_ref)
4127 
4128     @wrap_exception()
4129     @reverts_task_state
4130     @wrap_instance_event(prefix='compute')
4131     @wrap_instance_fault
4132     def rescue_instance(self, context, instance, rescue_password,
4133                         rescue_image_ref, clean_shutdown):
4134         context = context.elevated()
4135         LOG.info('Rescuing', instance=instance)
4136 
4137         admin_password = (rescue_password if rescue_password else
4138                       utils.generate_password())
4139 
4140         network_info = self.network_api.get_instance_nw_info(context, instance)
4141 
4142         rescue_image_meta = self._get_rescue_image(context, instance,
4143                                                    rescue_image_ref)
4144 
4145         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
4146                                               context, instance.uuid)
4147         block_device_info = self._get_instance_block_device_info(
4148                                 context, instance, bdms=bdms)
4149 
4150         extra_usage_info = {'rescue_image_name':
4151                             self._get_image_name(rescue_image_meta)}
4152         self._notify_about_instance_usage(context, instance,
4153                 "rescue.start", extra_usage_info=extra_usage_info,
4154                 network_info=network_info)
4155         compute_utils.notify_about_instance_rescue_action(
4156             context, instance, self.host, rescue_image_ref,
4157             phase=fields.NotificationPhase.START)
4158 
4159         try:
4160             self._power_off_instance(instance, clean_shutdown)
4161 
4162             self.driver.rescue(context, instance, network_info,
4163                                rescue_image_meta, admin_password,
4164                                block_device_info)
4165         except Exception as e:
4166             LOG.exception("Error trying to Rescue Instance",
4167                           instance=instance)
4168             self._set_instance_obj_error_state(instance)
4169             raise exception.InstanceNotRescuable(
4170                 instance_id=instance.uuid,
4171                 reason=_("Driver Error: %s") % e)
4172 
4173         compute_utils.notify_usage_exists(self.notifier, context, instance,
4174                                           self.host, current_period=True)
4175 
4176         instance.vm_state = vm_states.RESCUED
4177         instance.task_state = None
4178         instance.power_state = self._get_power_state(instance)
4179         instance.launched_at = timeutils.utcnow()
4180         instance.save(expected_task_state=task_states.RESCUING)
4181 
4182         self._notify_about_instance_usage(context, instance,
4183                 "rescue.end", extra_usage_info=extra_usage_info,
4184                 network_info=network_info)
4185         compute_utils.notify_about_instance_rescue_action(
4186             context, instance, self.host, rescue_image_ref,
4187             phase=fields.NotificationPhase.END)
4188 
4189     @wrap_exception()
4190     @reverts_task_state
4191     @wrap_instance_event(prefix='compute')
4192     @wrap_instance_fault
4193     def unrescue_instance(self, context, instance):
4194         orig_context = context
4195         context = context.elevated()
4196         LOG.info('Unrescuing', instance=instance)
4197 
4198         network_info = self.network_api.get_instance_nw_info(context, instance)
4199         self._notify_about_instance_usage(context, instance,
4200                 "unrescue.start", network_info=network_info)
4201         compute_utils.notify_about_instance_action(context, instance,
4202             self.host, action=fields.NotificationAction.UNRESCUE,
4203             phase=fields.NotificationPhase.START)
4204 
4205         with self._error_out_instance_on_exception(context, instance):
4206             self.driver.unrescue(orig_context, instance)
4207 
4208         instance.vm_state = vm_states.ACTIVE
4209         instance.task_state = None
4210         instance.power_state = self._get_power_state(instance)
4211         instance.save(expected_task_state=task_states.UNRESCUING)
4212 
4213         self._notify_about_instance_usage(context,
4214                                           instance,
4215                                           "unrescue.end",
4216                                           network_info=network_info)
4217         compute_utils.notify_about_instance_action(context, instance,
4218             self.host, action=fields.NotificationAction.UNRESCUE,
4219             phase=fields.NotificationPhase.END)
4220 
4221     # TODO(stephenfin): Remove this once we bump the compute API to v6.0
4222     @wrap_exception()
4223     @wrap_instance_fault
4224     def change_instance_metadata(self, context, diff, instance):
4225         raise NotImplementedError()
4226 
4227     @wrap_exception()
4228     @wrap_instance_event(prefix='compute')
4229     @errors_out_migration
4230     @wrap_instance_fault
4231     def confirm_resize(self, context, instance, migration):
4232         """Confirms a migration/resize and deletes the 'old' instance.
4233 
4234         This is called from the API and runs on the source host.
4235 
4236         Nothing needs to happen on the destination host at this point since
4237         the instance is already running there. This routine just cleans up the
4238         source host.
4239         """
4240         @utils.synchronized(instance.uuid)
4241         def do_confirm_resize(context, instance, migration):
4242             LOG.debug("Going to confirm migration %s", migration.id,
4243                       instance=instance)
4244 
4245             if migration.status == 'confirmed':
4246                 LOG.info("Migration %s is already confirmed",
4247                          migration.id, instance=instance)
4248                 return
4249 
4250             if migration.status not in ('finished', 'confirming'):
4251                 LOG.warning("Unexpected confirmation status '%(status)s' "
4252                             "of migration %(id)s, exit confirmation process",
4253                             {"status": migration.status, "id": migration.id},
4254                             instance=instance)
4255                 return
4256 
4257             # NOTE(wangpan): Get the instance from db, if it has been
4258             #                deleted, we do nothing and return here
4259             expected_attrs = ['metadata', 'system_metadata', 'flavor']
4260             try:
4261                 instance = objects.Instance.get_by_uuid(
4262                         context, instance.uuid,
4263                         expected_attrs=expected_attrs)
4264             except exception.InstanceNotFound:
4265                 LOG.info("Instance is not found during confirmation",
4266                          instance=instance)
4267                 return
4268 
4269             with self._error_out_instance_on_exception(context, instance):
4270                 try:
4271                     self._confirm_resize(
4272                         context, instance, migration=migration)
4273                 except Exception:
4274                     # Something failed when cleaning up the source host so
4275                     # log a traceback and leave a hint about hard rebooting
4276                     # the server to correct its state in the DB.
4277                     with excutils.save_and_reraise_exception(logger=LOG):
4278                         LOG.exception(
4279                             'Confirm resize failed on source host %s. '
4280                             'Resource allocations in the placement service '
4281                             'will be removed regardless because the instance '
4282                             'is now on the destination host %s. You can try '
4283                             'hard rebooting the instance to correct its '
4284                             'state.', self.host, migration.dest_compute,
4285                             instance=instance)
4286                 finally:
4287                     # Whether an error occurred or not, at this point the
4288                     # instance is on the dest host. Avoid leaking allocations
4289                     # in placement by deleting them here...
4290                     self._delete_allocation_after_move(
4291                         context, instance, migration)
4292                     # ...inform the scheduler about the move...
4293                     self._delete_scheduler_instance_info(
4294                         context, instance.uuid)
4295                     # ...and unset the cached flavor information (this is done
4296                     # last since the resource tracker relies on it for its
4297                     # periodic tasks)
4298                     self._delete_stashed_flavor_info(instance)
4299 
4300         do_confirm_resize(context, instance, migration)
4301 
4302     def _get_updated_nw_info_with_pci_mapping(self, nw_info, pci_mapping):
4303         # NOTE(adrianc): This method returns a copy of nw_info if modifications
4304         # are made else it returns the original nw_info.
4305         updated_nw_info = nw_info
4306         if nw_info and pci_mapping:
4307             updated_nw_info = copy.deepcopy(nw_info)
4308             for vif in updated_nw_info:
4309                 if vif['vnic_type'] in network_model.VNIC_TYPES_SRIOV:
4310                     try:
4311                         vif_pci_addr = vif['profile']['pci_slot']
4312                         new_addr = pci_mapping[vif_pci_addr].address
4313                         vif['profile']['pci_slot'] = new_addr
4314                         LOG.debug("Updating VIF's PCI address for VIF %(id)s. "
4315                                   "Original value %(orig_val)s, "
4316                                   "new value %(new_val)s",
4317                                   {'id': vif['id'],
4318                                    'orig_val': vif_pci_addr,
4319                                    'new_val': new_addr})
4320                     except (KeyError, AttributeError):
4321                         with excutils.save_and_reraise_exception():
4322                             # NOTE(adrianc): This should never happen. If we
4323                             # get here it means there is some inconsistency
4324                             # with either 'nw_info' or 'pci_mapping'.
4325                             LOG.error("Unexpected error when updating network "
4326                                       "information with PCI mapping.")
4327         return updated_nw_info
4328 
4329     def _confirm_resize(self, context, instance, migration=None):
4330         """Destroys the source instance."""
4331         self._notify_about_instance_usage(context, instance,
4332                                           "resize.confirm.start")
4333         compute_utils.notify_about_instance_action(context, instance,
4334             self.host, action=fields.NotificationAction.RESIZE_CONFIRM,
4335             phase=fields.NotificationPhase.START)
4336 
4337         # NOTE(tr3buchet): tear down networks on source host
4338         self.network_api.setup_networks_on_host(context, instance,
4339                            migration.source_compute, teardown=True)
4340 
4341         # TODO(stephenfin): These next three calls should be bundled
4342         network_info = self.network_api.get_instance_nw_info(context,
4343                                                              instance)
4344 
4345         # NOTE(adrianc): Populate old PCI device in VIF profile
4346         # to allow virt driver to properly unplug it from Hypervisor.
4347         pci_mapping = (instance.migration_context.
4348                        get_pci_mapping_for_migration(True))
4349         network_info = self._get_updated_nw_info_with_pci_mapping(
4350             network_info, pci_mapping)
4351 
4352         self.driver.confirm_migration(context, migration, instance,
4353                                       network_info)
4354 
4355         # Free up the old_flavor usage from the resource tracker for this host.
4356         self.rt.drop_move_claim_at_source(context, instance, migration)
4357 
4358         # NOTE(mriedem): The old_vm_state could be STOPPED but the user
4359         # might have manually powered up the instance to confirm the
4360         # resize/migrate, so we need to check the current power state
4361         # on the instance and set the vm_state appropriately. We default
4362         # to ACTIVE because if the power state is not SHUTDOWN, we
4363         # assume _sync_instance_power_state will clean it up.
4364         p_state = instance.power_state
4365         vm_state = None
4366         if p_state == power_state.SHUTDOWN:
4367             vm_state = vm_states.STOPPED
4368             LOG.debug("Resized/migrated instance is powered off. "
4369                       "Setting vm_state to '%s'.", vm_state,
4370                       instance=instance)
4371         else:
4372             vm_state = vm_states.ACTIVE
4373 
4374         instance.vm_state = vm_state
4375         instance.task_state = None
4376         instance.save(expected_task_state=[None, task_states.DELETING,
4377                                            task_states.SOFT_DELETING])
4378 
4379         self._notify_about_instance_usage(
4380             context, instance, "resize.confirm.end",
4381             network_info=network_info)
4382         compute_utils.notify_about_instance_action(context, instance,
4383                self.host, action=fields.NotificationAction.RESIZE_CONFIRM,
4384                phase=fields.NotificationPhase.END)
4385 
4386     def _delete_allocation_after_move(self, context, instance, migration):
4387         """Deletes resource allocations held by the migration record against
4388         the source compute node resource provider after a confirmed cold /
4389         successful live migration.
4390         """
4391         try:
4392             # NOTE(danms): We're finishing on the source node, so try
4393             # to delete the allocation based on the migration uuid
4394             self.reportclient.delete_allocation_for_instance(
4395                 context, migration.uuid, consumer_type='migration')
4396         except exception.AllocationDeleteFailed:
4397             LOG.error('Deleting allocation in placement for migration '
4398                       '%(migration_uuid)s failed. The instance '
4399                       '%(instance_uuid)s will be put to ERROR state '
4400                       'but the allocation held by the migration is '
4401                       'leaked.',
4402                       {'instance_uuid': instance.uuid,
4403                        'migration_uuid': migration.uuid})
4404             raise
4405 
4406     def _delete_stashed_flavor_info(self, instance):
4407         """Remove information about the flavor change after a resize."""
4408         instance.old_flavor = None
4409         instance.new_flavor = None
4410         instance.system_metadata.pop('old_vm_state', None)
4411         instance.save()
4412 
4413     @wrap_exception()
4414     @wrap_instance_event(prefix='compute')
4415     @errors_out_migration
4416     @wrap_instance_fault
4417     def confirm_snapshot_based_resize_at_source(
4418             self, ctxt, instance, migration):
4419         """Confirms a snapshot-based resize on the source host.
4420 
4421         Cleans the guest from the source hypervisor including disks and drops
4422         the MoveClaim which will free up "old_flavor" usage from the
4423         ResourceTracker.
4424 
4425         Deletes the allocations held by the migration consumer against the
4426         source compute node resource provider.
4427 
4428         :param ctxt: nova auth request context targeted at the source cell
4429         :param instance: Instance object being resized which should have the
4430             "old_flavor" attribute set
4431         :param migration: Migration object for the resize operation
4432         """
4433 
4434         @utils.synchronized(instance.uuid)
4435         def do_confirm():
4436             LOG.info('Confirming resize on source host.', instance=instance)
4437             with self._error_out_instance_on_exception(ctxt, instance):
4438                 # TODO(mriedem): Could probably make this try/except/finally
4439                 # a context manager to share with confirm_resize().
4440                 try:
4441                     self._confirm_snapshot_based_resize_at_source(
4442                         ctxt, instance, migration)
4443                 except Exception:
4444                     # Something failed when cleaning up the source host so
4445                     # log a traceback and leave a hint about hard rebooting
4446                     # the server to correct its state in the DB.
4447                     with excutils.save_and_reraise_exception(logger=LOG):
4448                         LOG.exception(
4449                             'Confirm resize failed on source host %s. '
4450                             'Resource allocations in the placement service '
4451                             'will be removed regardless because the instance '
4452                             'is now on the destination host %s. You can try '
4453                             'hard rebooting the instance to correct its '
4454                             'state.', self.host, migration.dest_compute,
4455                             instance=instance)
4456                 finally:
4457                     # Whether an error occurred or not, at this point the
4458                     # instance is on the dest host so to avoid leaking
4459                     # allocations in placement, delete them here.
4460                     # TODO(mriedem): Should we catch and just log
4461                     # AllocationDeleteFailed? What is the user's recourse if
4462                     # we got this far but this fails? At this point the
4463                     # instance is on the target host and the allocations
4464                     # could just be manually cleaned up by the operator.
4465                     self._delete_allocation_after_move(ctxt, instance,
4466                                                        migration)
4467         do_confirm()
4468 
4469     def _confirm_snapshot_based_resize_at_source(
4470             self, ctxt, instance, migration):
4471         """Private version of confirm_snapshot_based_resize_at_source
4472 
4473         This allows the main method to be decorated with error handlers.
4474 
4475         :param ctxt: nova auth request context targeted at the source cell
4476         :param instance: Instance object being resized which should have the
4477             "old_flavor" attribute set
4478         :param migration: Migration object for the resize operation
4479         """
4480         # Cleanup the guest from the hypervisor including local disks.
4481         network_info = self.network_api.get_instance_nw_info(ctxt, instance)
4482         LOG.debug('Cleaning up guest from source hypervisor including disks.',
4483                   instance=instance)
4484 
4485         # FIXME(mriedem): Per bug 1809095, _confirm_resize calls
4486         # _get_updated_nw_info_with_pci_mapping here prior to unplugging
4487         # VIFs on the source, but in our case we have already unplugged
4488         # VIFs during prep_snapshot_based_resize_at_source, so what do we
4489         # need to do about those kinds of ports? Do we need to wait to unplug
4490         # VIFs until confirm like normal resize?
4491 
4492         # Note that prep_snapshot_based_resize_at_source already destroyed the
4493         # guest which disconnected volumes and unplugged VIFs but did not
4494         # destroy disks in case something failed during the resize and the
4495         # instance needed to be rebooted or rebuilt on the source host. Now
4496         # that we are confirming the resize we want to cleanup the disks left
4497         # on the source host. We call cleanup() instead of destroy() to avoid
4498         # any InstanceNotFound confusion from the driver since the guest was
4499         # already destroyed on this host. block_device_info=None and
4500         # destroy_vifs=False means cleanup() will not try to disconnect volumes
4501         # or unplug VIFs.
4502         self.driver.cleanup(
4503             ctxt, instance, network_info, block_device_info=None,
4504             destroy_disks=True, destroy_vifs=False)
4505 
4506         # Delete port bindings for the source host.
4507         self._confirm_snapshot_based_resize_delete_port_bindings(
4508             ctxt, instance)
4509 
4510         # Delete volume attachments for the source host.
4511         self._delete_volume_attachments(ctxt, instance.get_bdms())
4512 
4513         # Free up the old_flavor usage from the resource tracker for this host.
4514         self.rt.drop_move_claim_at_source(ctxt, instance, migration)
4515 
4516     def _confirm_snapshot_based_resize_delete_port_bindings(
4517             self, ctxt, instance):
4518         """Delete port bindings for the source host when confirming
4519         snapshot-based resize on the source host."
4520 
4521         :param ctxt: nova auth RequestContext
4522         :param instance: Instance object that was resized/cold migrated
4523         """
4524         LOG.debug('Deleting port bindings for source host.',
4525                   instance=instance)
4526         try:
4527             self.network_api.cleanup_instance_network_on_host(
4528                 ctxt, instance, self.host)
4529         except exception.PortBindingDeletionFailed as e:
4530             # Do not let this stop us from cleaning up since the guest
4531             # is already gone.
4532             LOG.error('Failed to delete port bindings from source host. '
4533                       'Error: %s', str(e), instance=instance)
4534 
4535     def _delete_volume_attachments(self, ctxt, bdms):
4536         """Deletes volume attachment records for the given bdms.
4537 
4538         This method will log but not re-raise any exceptions if the volume
4539         attachment delete fails.
4540 
4541         :param ctxt: nova auth request context used to make
4542             DELETE /attachments/{attachment_id} requests to cinder.
4543         :param bdms: objects.BlockDeviceMappingList representing volume
4544             attachments to delete based on BlockDeviceMapping.attachment_id.
4545         """
4546         for bdm in bdms:
4547             if bdm.attachment_id:
4548                 try:
4549                     self.volume_api.attachment_delete(ctxt, bdm.attachment_id)
4550                 except Exception as e:
4551                     LOG.error('Failed to delete volume attachment with ID %s. '
4552                               'Error: %s', bdm.attachment_id, str(e),
4553                               instance_uuid=bdm.instance_uuid)
4554 
4555     @wrap_exception()
4556     @reverts_task_state
4557     @wrap_instance_event(prefix='compute')
4558     @errors_out_migration
4559     @wrap_instance_fault
4560     def revert_snapshot_based_resize_at_dest(self, ctxt, instance, migration):
4561         """Reverts a snapshot-based resize at the destination host.
4562 
4563         Cleans the guest from the destination compute service host hypervisor
4564         and related resources (ports, volumes) and frees resource usage from
4565         the compute service on that host.
4566 
4567         :param ctxt: nova auth request context targeted at the target cell
4568         :param instance: Instance object whose vm_state is "resized" and
4569             task_state is "resize_reverting".
4570         :param migration: Migration object whose status is "reverting".
4571         """
4572         # A resize revert is essentially a resize back to the old size, so we
4573         # need to send a usage event here.
4574         compute_utils.notify_usage_exists(
4575             self.notifier, ctxt, instance, self.host, current_period=True)
4576 
4577         @utils.synchronized(instance.uuid)
4578         def do_revert():
4579             LOG.info('Reverting resize on destination host.',
4580                      instance=instance)
4581             with self._error_out_instance_on_exception(ctxt, instance):
4582                 self._revert_snapshot_based_resize_at_dest(
4583                     ctxt, instance, migration)
4584         do_revert()
4585 
4586         # Broadcast to all schedulers that the instance is no longer on
4587         # this host and clear any waiting callback events. This is best effort
4588         # so if anything fails just log it.
4589         try:
4590             self._delete_scheduler_instance_info(ctxt, instance.uuid)
4591             self.instance_events.clear_events_for_instance(instance)
4592         except Exception as e:
4593             LOG.warning('revert_snapshot_based_resize_at_dest failed during '
4594                         'post-processing. Error: %s', e, instance=instance)
4595 
4596     def _revert_snapshot_based_resize_at_dest(
4597             self, ctxt, instance, migration):
4598         """Private version of revert_snapshot_based_resize_at_dest.
4599 
4600         This allows the main method to be decorated with error handlers.
4601 
4602         :param ctxt: nova auth request context targeted at the target cell
4603         :param instance: Instance object whose vm_state is "resized" and
4604             task_state is "resize_reverting".
4605         :param migration: Migration object whose status is "reverting".
4606         """
4607         # Cleanup the guest from the hypervisor including local disks.
4608         network_info = self.network_api.get_instance_nw_info(ctxt, instance)
4609         bdms = instance.get_bdms()
4610         block_device_info = self._get_instance_block_device_info(
4611             ctxt, instance, bdms=bdms)
4612         LOG.debug('Destroying guest from destination hypervisor including '
4613                   'disks.', instance=instance)
4614         self.driver.destroy(
4615             ctxt, instance, network_info, block_device_info=block_device_info)
4616 
4617         # Activate source host port bindings. We need to do this before
4618         # deleting the (active) dest host port bindings in
4619         # setup_networks_on_host otherwise the ports will be unbound and
4620         # finish on the source will fail.
4621         # migrate_instance_start uses migration.dest_compute for the port
4622         # binding host and since we want to activate the source host port
4623         # bindings, we need to temporarily mutate the migration object.
4624         with utils.temporary_mutation(
4625                 migration, dest_compute=migration.source_compute):
4626             LOG.debug('Activating port bindings for source host %s.',
4627                       migration.source_compute, instance=instance)
4628             # TODO(mriedem): https://review.opendev.org/#/c/594139/ would allow
4629             # us to remove this and make setup_networks_on_host do it.
4630             # TODO(mriedem): Should we try/except/log any errors but continue?
4631             self.network_api.migrate_instance_start(
4632                 ctxt, instance, migration)
4633 
4634         # Delete port bindings for the target host.
4635         LOG.debug('Deleting port bindings for target host %s.',
4636                   self.host, instance=instance)
4637         try:
4638             # Note that deleting the destination host port bindings does
4639             # not automatically activate the source host port bindings.
4640             self.network_api.cleanup_instance_network_on_host(
4641                 ctxt, instance, self.host)
4642         except exception.PortBindingDeletionFailed as e:
4643             # Do not let this stop us from cleaning up since the guest
4644             # is already gone.
4645             LOG.error('Failed to delete port bindings from target host. '
4646                       'Error: %s', str(e), instance=instance)
4647 
4648         # Delete any volume attachments remaining for this target host.
4649         LOG.debug('Deleting volume attachments for target host.',
4650                   instance=instance)
4651         self._delete_volume_attachments(ctxt, bdms)
4652 
4653         # Free up the new_flavor usage from the resource tracker for this host.
4654         self.rt.drop_move_claim_at_dest(ctxt, instance, migration)
4655 
4656     def _revert_instance_flavor_host_node(self, instance, migration):
4657         """Revert host, node and flavor fields after a resize-revert."""
4658         self._set_instance_info(instance, instance.old_flavor)
4659         instance.host = migration.source_compute
4660         instance.node = migration.source_node
4661         instance.save(expected_task_state=[task_states.RESIZE_REVERTING])
4662 
4663     @wrap_exception()
4664     @reverts_task_state
4665     @wrap_instance_event(prefix='compute')
4666     @errors_out_migration
4667     @wrap_instance_fault
4668     def finish_revert_snapshot_based_resize_at_source(
4669             self, ctxt, instance, migration):
4670         """Reverts a snapshot-based resize at the source host.
4671 
4672         Spawn the guest and re-connect volumes/VIFs on the source host and
4673         revert the instance to use the old_flavor for resource usage reporting.
4674 
4675         Updates allocations in the placement service to move the source node
4676         allocations, held by the migration record, to the instance and drop
4677         the allocations held by the instance on the destination node.
4678 
4679         :param ctxt: nova auth request context targeted at the target cell
4680         :param instance: Instance object whose vm_state is "resized" and
4681             task_state is "resize_reverting".
4682         :param migration: Migration object whose status is "reverting".
4683         """
4684 
4685         @utils.synchronized(instance.uuid)
4686         def do_revert():
4687             LOG.info('Reverting resize on source host.', instance=instance)
4688             with self._error_out_instance_on_exception(ctxt, instance):
4689                 self._finish_revert_snapshot_based_resize_at_source(
4690                     ctxt, instance, migration)
4691 
4692         try:
4693             do_revert()
4694         finally:
4695             self._delete_stashed_flavor_info(instance)
4696 
4697         # Broadcast to all schedulers that the instance is on this host.
4698         # This is best effort so if anything fails just log it.
4699         try:
4700             self._update_scheduler_instance_info(ctxt, instance)
4701         except Exception as e:
4702             LOG.warning('finish_revert_snapshot_based_resize_at_source failed '
4703                         'during post-processing. Error: %s', e,
4704                         instance=instance)
4705 
4706     def _finish_revert_snapshot_based_resize_at_source(
4707             self, ctxt, instance, migration):
4708         """Private version of finish_revert_snapshot_based_resize_at_source.
4709 
4710         This allows the main method to be decorated with error handlers.
4711 
4712         :param ctxt: nova auth request context targeted at the source cell
4713         :param instance: Instance object whose vm_state is "resized" and
4714             task_state is "resize_reverting".
4715         :param migration: Migration object whose status is "reverting".
4716         """
4717         # Get stashed old_vm_state information to determine if guest should
4718         # be powered on after spawn; we default to ACTIVE for backwards
4719         # compatibility if old_vm_state is not set
4720         old_vm_state = instance.system_metadata.get(
4721             'old_vm_state', vm_states.ACTIVE)
4722 
4723         # Revert the flavor and host/node fields to their previous values
4724         self._revert_instance_flavor_host_node(instance, migration)
4725 
4726         # Move the allocations against the source compute node resource
4727         # provider, held by the migration, to the instance which will drop
4728         # the destination compute node resource provider allocations held by
4729         # the instance. This puts the allocations against the source node
4730         # back to the old_flavor and owned by the instance.
4731         try:
4732             self._revert_allocation(ctxt, instance, migration)
4733         except exception.AllocationMoveFailed:
4734             # Log the error but do not re-raise because we want to continue to
4735             # process ports and volumes below.
4736             LOG.error('Reverting allocation in placement for migration '
4737                       '%(migration_uuid)s failed. You may need to manually '
4738                       'remove the allocations for the migration consumer '
4739                       'against the source node resource provider '
4740                       '%(source_provider)s and the allocations for the '
4741                       'instance consumer against the destination node '
4742                       'resource provider %(dest_provider)s and then run the '
4743                       '"nova-manage placement heal_allocations" command.',
4744                       {'instance_uuid': instance.uuid,
4745                        'migration_uuid': migration.uuid,
4746                        'source_provider': migration.source_node,
4747                        'dest_provider': migration.dest_node},
4748                       instance=instance)
4749 
4750         bdms = instance.get_bdms()
4751         # prep_snapshot_based_resize_at_source created empty volume attachments
4752         # that we need to update here to get the connection_info before calling
4753         # driver.finish_revert_migration which will connect the volumes to this
4754         # host.
4755         LOG.debug('Updating volume attachments for target host %s.',
4756                   self.host, instance=instance)
4757         # TODO(mriedem): We should probably make _update_volume_attachments
4758         # (optionally) graceful to errors so we (1) try to process all
4759         # attachments and (2) continue to process networking below.
4760         self._update_volume_attachments(ctxt, instance, bdms)
4761 
4762         LOG.debug('Updating port bindings for source host %s.',
4763                   self.host, instance=instance)
4764         # TODO(mriedem): Calculate provider mappings when we support
4765         # cross-cell resize/migrate with ports having resource requests.
4766         self._finish_revert_resize_network_migrate_finish(
4767             ctxt, instance, migration, provider_mappings=None)
4768         network_info = self.network_api.get_instance_nw_info(ctxt, instance)
4769 
4770         # Remember that prep_snapshot_based_resize_at_source destroyed the
4771         # guest but left the disks intact so we cannot call spawn() here but
4772         # finish_revert_migration should do the job.
4773         block_device_info = self._get_instance_block_device_info(
4774             ctxt, instance, bdms=bdms)
4775         power_on = old_vm_state == vm_states.ACTIVE
4776         driver_error = None
4777         try:
4778             self.driver.finish_revert_migration(
4779                 ctxt, instance, network_info, migration,
4780                 block_device_info=block_device_info, power_on=power_on)
4781         except Exception as e:
4782             driver_error = e
4783             # Leave a hint about hard rebooting the guest and reraise so the
4784             # instance is put into ERROR state.
4785             with excutils.save_and_reraise_exception(logger=LOG):
4786                 LOG.error('An error occurred during finish_revert_migration. '
4787                           'The instance may need to be hard rebooted. Error: '
4788                           '%s', driver_error, instance=instance)
4789         else:
4790             # Perform final cleanup of the instance in the database.
4791             instance.drop_migration_context()
4792             # If the original vm_state was STOPPED, set it back to STOPPED.
4793             vm_state = vm_states.ACTIVE if power_on else vm_states.STOPPED
4794             self._update_instance_after_spawn(instance, vm_state=vm_state)
4795             instance.save(expected_task_state=[task_states.RESIZE_REVERTING])
4796         finally:
4797             # Complete any volume attachments so the volumes are in-use. We
4798             # do this regardless of finish_revert_migration failing because
4799             # the instance is back on this host now and we do not want to leave
4800             # the volumes in a pending state in case the instance is hard
4801             # rebooted.
4802             LOG.debug('Completing volume attachments for instance on source '
4803                       'host.', instance=instance)
4804             with excutils.save_and_reraise_exception(
4805                     reraise=driver_error is not None, logger=LOG):
4806                 self._complete_volume_attachments(ctxt, bdms)
4807 
4808         migration.status = 'reverted'
4809         migration.save()
4810 
4811     @wrap_exception()
4812     @reverts_task_state
4813     @wrap_instance_event(prefix='compute')
4814     @errors_out_migration
4815     @wrap_instance_fault
4816     def revert_resize(self, context, instance, migration, request_spec=None):
4817         """Destroys the new instance on the destination machine.
4818 
4819         Reverts the model changes, and powers on the old instance on the
4820         source machine.
4821 
4822         """
4823         # NOTE(comstud): A revert_resize is essentially a resize back to
4824         # the old size, so we need to send a usage event here.
4825         compute_utils.notify_usage_exists(self.notifier, context, instance,
4826                                           self.host, current_period=True)
4827 
4828         with self._error_out_instance_on_exception(context, instance):
4829             # NOTE(tr3buchet): tear down networks on destination host
4830             self.network_api.setup_networks_on_host(context, instance,
4831                                                     teardown=True)
4832 
4833             self.network_api.migrate_instance_start(context,
4834                                                     instance,
4835                                                     migration)
4836 
4837             network_info = self.network_api.get_instance_nw_info(context,
4838                                                                  instance)
4839             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
4840                     context, instance.uuid)
4841             block_device_info = self._get_instance_block_device_info(
4842                                 context, instance, bdms=bdms)
4843 
4844             destroy_disks = not self._is_instance_storage_shared(
4845                 context, instance, host=migration.source_compute)
4846             self.driver.destroy(context, instance, network_info,
4847                                 block_device_info, destroy_disks)
4848 
4849             self._terminate_volume_connections(context, instance, bdms)
4850 
4851             # Free up the new_flavor usage from the resource tracker for this
4852             # host.
4853             self.rt.drop_move_claim_at_dest(context, instance, migration)
4854 
4855             # RPC cast back to the source host to finish the revert there.
4856             self.compute_rpcapi.finish_revert_resize(context, instance,
4857                     migration, migration.source_compute, request_spec)
4858 
4859     def _finish_revert_resize_network_migrate_finish(
4860             self, context, instance, migration, provider_mappings):
4861         """Causes port binding to be updated. In some Neutron or port
4862         configurations - see NetworkModel.get_bind_time_events() - we
4863         expect the vif-plugged event from Neutron immediately and wait for it.
4864         The rest of the time, the event is expected further along in the
4865         virt driver, so we don't wait here.
4866 
4867         :param context: The request context.
4868         :param instance: The instance undergoing the revert resize.
4869         :param migration: The Migration object of the resize being reverted.
4870         :param provider_mappings: a dict of list of resource provider uuids
4871             keyed by port uuid
4872         :raises: eventlet.timeout.Timeout or
4873                  exception.VirtualInterfacePlugException.
4874         """
4875         network_info = instance.get_network_info()
4876         events = []
4877         deadline = CONF.vif_plugging_timeout
4878         if deadline and network_info:
4879             events = network_info.get_bind_time_events(migration)
4880             if events:
4881                 LOG.debug('Will wait for bind-time events: %s', events)
4882         error_cb = self._neutron_failed_migration_callback
4883         try:
4884             with self.virtapi.wait_for_instance_event(instance, events,
4885                                                       deadline=deadline,
4886                                                       error_callback=error_cb):
4887                 # NOTE(hanrong): we need to change migration.dest_compute to
4888                 # source host temporarily.
4889                 # "network_api.migrate_instance_finish" will setup the network
4890                 # for the instance on the destination host. For revert resize,
4891                 # the instance will back to the source host, the setup of the
4892                 # network for instance should be on the source host. So set
4893                 # the migration.dest_compute to source host at here.
4894                 with utils.temporary_mutation(
4895                         migration, dest_compute=migration.source_compute):
4896                     self.network_api.migrate_instance_finish(
4897                         context, instance, migration, provider_mappings)
4898         except eventlet.timeout.Timeout:
4899             with excutils.save_and_reraise_exception():
4900                 LOG.error('Timeout waiting for Neutron events: %s', events,
4901                           instance=instance)
4902 
4903     @wrap_exception()
4904     @reverts_task_state
4905     @wrap_instance_event(prefix='compute')
4906     @errors_out_migration
4907     @wrap_instance_fault
4908     def finish_revert_resize(
4909             self, context, instance, migration, request_spec=None):
4910         """Finishes the second half of reverting a resize on the source host.
4911 
4912         Bring the original source instance state back (active/shutoff) and
4913         revert the resized attributes in the database.
4914 
4915         """
4916         try:
4917             self._finish_revert_resize(
4918                 context, instance, migration, request_spec)
4919         finally:
4920             self._delete_stashed_flavor_info(instance)
4921 
4922     def _finish_revert_resize(
4923         self, context, instance, migration, request_spec=None,
4924     ):
4925         """Inner version of finish_revert_resize."""
4926         with self._error_out_instance_on_exception(context, instance):
4927             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
4928                 context, instance.uuid)
4929             self._notify_about_instance_usage(
4930                     context, instance, "resize.revert.start")
4931             compute_utils.notify_about_instance_action(context, instance,
4932                 self.host, action=fields.NotificationAction.RESIZE_REVERT,
4933                     phase=fields.NotificationPhase.START, bdms=bdms)
4934 
4935             # Get stashed old_vm_state information to determine if guest should
4936             # be powered on after spawn; we default to ACTIVE for backwards
4937             # compatibility if old_vm_state is not set
4938             old_vm_state = instance.system_metadata.get(
4939                 'old_vm_state', vm_states.ACTIVE)
4940 
4941             # Revert the flavor and host/node fields to their previous values
4942             self._revert_instance_flavor_host_node(instance, migration)
4943 
4944             try:
4945                 source_allocations = self._revert_allocation(
4946                     context, instance, migration)
4947             except exception.AllocationMoveFailed:
4948                 LOG.error('Reverting allocation in placement for migration '
4949                           '%(migration_uuid)s failed. The instance '
4950                           '%(instance_uuid)s will be put into ERROR state but '
4951                           'the allocation held by the migration is leaked.',
4952                           {'instance_uuid': instance.uuid,
4953                            'migration_uuid': migration.uuid})
4954                 raise
4955 
4956             provider_mappings = self._fill_provider_mapping_based_on_allocs(
4957                 context, source_allocations, request_spec)
4958 
4959             self.network_api.setup_networks_on_host(context, instance,
4960                                                     migration.source_compute)
4961             self._finish_revert_resize_network_migrate_finish(
4962                 context, instance, migration, provider_mappings)
4963             network_info = self.network_api.get_instance_nw_info(context,
4964                                                                  instance)
4965 
4966             # revert_resize deleted any volume attachments for the instance
4967             # and created new ones to be used on this host, but we
4968             # have to update those attachments with the host connector so the
4969             # BDM.connection_info will get set in the call to
4970             # _get_instance_block_device_info below with refresh_conn_info=True
4971             # and then the volumes can be re-connected via the driver on this
4972             # host.
4973             self._update_volume_attachments(context, instance, bdms)
4974 
4975             block_device_info = self._get_instance_block_device_info(
4976                     context, instance, refresh_conn_info=True, bdms=bdms)
4977 
4978             power_on = old_vm_state != vm_states.STOPPED
4979             self.driver.finish_revert_migration(
4980                 context, instance, network_info, migration, block_device_info,
4981                 power_on)
4982 
4983             instance.drop_migration_context()
4984             instance.launched_at = timeutils.utcnow()
4985             instance.save(expected_task_state=task_states.RESIZE_REVERTING)
4986 
4987             # Complete any volume attachments so the volumes are in-use.
4988             self._complete_volume_attachments(context, bdms)
4989 
4990             # if the original vm state was STOPPED, set it back to STOPPED
4991             LOG.info("Updating instance to original state: '%s'",
4992                      old_vm_state, instance=instance)
4993             if power_on:
4994                 instance.vm_state = vm_states.ACTIVE
4995                 instance.task_state = None
4996                 instance.save()
4997             else:
4998                 instance.task_state = task_states.POWERING_OFF
4999                 instance.save()
5000                 self.stop_instance(context, instance=instance,
5001                                    clean_shutdown=True)
5002 
5003             self._notify_about_instance_usage(
5004                     context, instance, "resize.revert.end")
5005             compute_utils.notify_about_instance_action(context, instance,
5006                 self.host, action=fields.NotificationAction.RESIZE_REVERT,
5007                     phase=fields.NotificationPhase.END, bdms=bdms)
5008 
5009     def _fill_provider_mapping_based_on_allocs(
5010             self, context, allocations, request_spec):
5011         """Fills and returns the request group - resource provider mapping
5012         based on the allocation passed in.
5013 
5014         :param context: The security context
5015         :param allocation: allocation dict keyed by RP UUID.
5016         :param request_spec: The RequestSpec object associated with the
5017             operation
5018         :returns: None if the request_spec is None. Otherwise a mapping
5019             between RequestGroup requester_id, currently Neutron port_id,
5020             and a list of resource provider UUIDs providing resource for
5021             that RequestGroup.
5022         """
5023         if request_spec:
5024             # NOTE(gibi): We need to re-calculate the resource provider -
5025             # port mapping as we have to have the neutron ports allocate
5026             # from the source compute after revert.
5027             scheduler_utils.fill_provider_mapping_based_on_allocation(
5028                 context, self.reportclient, request_spec, allocations)
5029             provider_mappings = self._get_request_group_mapping(
5030                 request_spec)
5031         else:
5032             # NOTE(gibi): The compute RPC is pinned to be older than 5.2
5033             # and therefore request_spec is not sent. We cannot calculate
5034             # the provider mappings. If the instance has ports with
5035             # resource request then the port update will fail in
5036             # _update_port_binding_for_instance() called via
5037             # _finish_revert_resize_network_migrate_finish() in
5038             # finish_revert_resize.
5039             provider_mappings = None
5040         return provider_mappings
5041 
5042     def _revert_allocation(self, context, instance, migration):
5043         """Revert an allocation that is held by migration to our instance."""
5044 
5045         # Fetch the original allocation that the instance had on the source
5046         # node, which are now held by the migration
5047         orig_alloc = self.reportclient.get_allocations_for_consumer(
5048             context, migration.uuid)
5049         if not orig_alloc:
5050             LOG.error('Did not find resource allocations for migration '
5051                       '%s on source node %s. Unable to revert source node '
5052                       'allocations back to the instance.',
5053                       migration.uuid, migration.source_node, instance=instance)
5054             return False
5055 
5056         LOG.info('Swapping old allocation on %(rp_uuids)s held by migration '
5057                  '%(mig)s for instance',
5058                  {'rp_uuids': orig_alloc.keys(), 'mig': migration.uuid},
5059                  instance=instance)
5060         # FIXME(gibi): This method is flawed in that it does not handle
5061         # allocations against sharing providers in any special way. This leads
5062         # to duplicate allocations against the sharing provider during
5063         # migration.
5064         # TODO(cdent): Should we be doing anything with return values here?
5065         self.reportclient.move_allocations(context, migration.uuid,
5066                                            instance.uuid)
5067         return orig_alloc
5068 
5069     def _prep_resize(self, context, image, instance, instance_type,
5070                      filter_properties, node, migration, request_spec,
5071                      clean_shutdown=True):
5072 
5073         if not filter_properties:
5074             filter_properties = {}
5075 
5076         if not instance.host:
5077             self._set_instance_obj_error_state(instance)
5078             msg = _('Instance has no source host')
5079             raise exception.MigrationError(reason=msg)
5080 
5081         same_host = instance.host == self.host
5082         # if the flavor IDs match, it's migrate; otherwise resize
5083         if same_host and instance_type.id == instance['instance_type_id']:
5084             # check driver whether support migrate to same host
5085             if not self.driver.capabilities.get(
5086                     'supports_migrate_to_same_host', False):
5087                 # Raise InstanceFaultRollback so that the
5088                 # _error_out_instance_on_exception context manager in
5089                 # prep_resize will set the instance.vm_state properly.
5090                 raise exception.InstanceFaultRollback(
5091                     inner_exception=exception.UnableToMigrateToSelf(
5092                         instance_id=instance.uuid, host=self.host))
5093 
5094         # NOTE(danms): Stash the new instance_type to avoid having to
5095         # look it up in the database later
5096         instance.new_flavor = instance_type
5097         # NOTE(mriedem): Stash the old vm_state so we can set the
5098         # resized/reverted instance back to the same state later.
5099         vm_state = instance.vm_state
5100         LOG.debug('Stashing vm_state: %s', vm_state, instance=instance)
5101         instance.system_metadata['old_vm_state'] = vm_state
5102         instance.save()
5103 
5104         if not isinstance(request_spec, objects.RequestSpec):
5105             # Prior to compute RPC API 5.1 conductor would pass a legacy dict
5106             # version of the request spec to compute and since Stein compute
5107             # could be sending that back to conductor on reschedule, so if we
5108             # got a dict convert it to an object.
5109             # TODO(mriedem): We can drop this compat code when we only support
5110             # compute RPC API >=6.0.
5111             request_spec = objects.RequestSpec.from_primitives(
5112                 context, request_spec, filter_properties)
5113             # We don't have to set the new flavor on the request spec because
5114             # if we got here it was due to a reschedule from the compute and
5115             # the request spec would already have the new flavor in it from the
5116             # else block below.
5117 
5118         provider_mapping = self._get_request_group_mapping(request_spec)
5119 
5120         if provider_mapping:
5121             try:
5122                 compute_utils.\
5123                     update_pci_request_spec_with_allocated_interface_name(
5124                         context, self.reportclient,
5125                         instance.pci_requests.requests, provider_mapping)
5126             except (exception.AmbiguousResourceProviderForPCIRequest,
5127                     exception.UnexpectedResourceProviderNameForPCIRequest
5128                     ) as e:
5129                 raise exception.BuildAbortException(
5130                     reason=str(e), instance_uuid=instance.uuid)
5131 
5132         limits = filter_properties.get('limits', {})
5133         allocs = self.reportclient.get_allocations_for_consumer(
5134             context, instance.uuid)
5135         with self.rt.resize_claim(context, instance, instance_type, node,
5136                                   migration, allocs, image_meta=image,
5137                                   limits=limits) as claim:
5138             LOG.info('Migrating', instance=instance)
5139             # RPC cast to the source host to start the actual resize/migration.
5140             self.compute_rpcapi.resize_instance(
5141                     context, instance, claim.migration, image,
5142                     instance_type, request_spec, clean_shutdown)
5143 
5144     def _send_prep_resize_notifications(
5145             self, context, instance, phase, flavor):
5146         """Send "resize.prep.*" notifications.
5147 
5148         :param context: nova auth request context
5149         :param instance: The instance being resized
5150         :param phase: The phase of the action (NotificationPhase enum)
5151         :param flavor: The (new) flavor for the resize (same as existing
5152             instance.flavor for a cold migration)
5153         """
5154         # Only send notify_usage_exists if it's the "start" phase.
5155         if phase == fields.NotificationPhase.START:
5156             compute_utils.notify_usage_exists(
5157                 self.notifier, context, instance, self.host,
5158                 current_period=True)
5159 
5160         # Send extra usage info about the flavor if it's the "end" phase for
5161         # the legacy unversioned notification.
5162         extra_usage_info = None
5163         if phase == fields.NotificationPhase.END:
5164             extra_usage_info = dict(
5165                 new_instance_type=flavor.name,
5166                 new_instance_type_id=flavor.id)
5167         self._notify_about_instance_usage(
5168             context, instance, "resize.prep.%s" % phase,
5169             extra_usage_info=extra_usage_info)
5170 
5171         # Send the versioned notification.
5172         compute_utils.notify_about_resize_prep_instance(
5173             context, instance, self.host, phase, flavor)
5174 
5175     @wrap_exception()
5176     @reverts_task_state
5177     @wrap_instance_event(prefix='compute')
5178     @wrap_instance_fault
5179     def prep_resize(self, context, image, instance, instance_type,
5180                     request_spec, filter_properties, node,
5181                     clean_shutdown, migration, host_list):
5182         """Initiates the process of moving a running instance to another host.
5183 
5184         Possibly changes the VCPU, RAM and disk size in the process.
5185 
5186         This is initiated from conductor and runs on the destination host.
5187 
5188         The main purpose of this method is performing some checks on the
5189         destination host and making a claim for resources. If the claim fails
5190         then a reschedule to another host may be attempted which involves
5191         calling back to conductor to start the process over again.
5192         """
5193         if node is None:
5194             node = self._get_nodename(instance, refresh=True)
5195 
5196         # Pass instance_state=instance.vm_state because we can resize
5197         # a STOPPED server and we don't want to set it back to ACTIVE
5198         # in case _prep_resize fails.
5199         instance_state = instance.vm_state
5200         with self._error_out_instance_on_exception(
5201                 context, instance, instance_state=instance_state),\
5202                 errors_out_migration_ctxt(migration):
5203             self._send_prep_resize_notifications(
5204                 context, instance, fields.NotificationPhase.START,
5205                 instance_type)
5206             try:
5207                 self._prep_resize(context, image, instance,
5208                                   instance_type, filter_properties,
5209                                   node, migration, request_spec,
5210                                   clean_shutdown)
5211             except exception.BuildAbortException:
5212                 # NOTE(gibi): We failed
5213                 # update_pci_request_spec_with_allocated_interface_name so
5214                 # there is no reason to re-schedule. Just revert the allocation
5215                 # and fail the migration.
5216                 with excutils.save_and_reraise_exception():
5217                     self._revert_allocation(context, instance, migration)
5218             except Exception:
5219                 # Since we hit a failure, we're either rescheduling or dead
5220                 # and either way we need to cleanup any allocations created
5221                 # by the scheduler for the destination node.
5222                 self._revert_allocation(context, instance, migration)
5223                 # try to re-schedule the resize elsewhere:
5224                 exc_info = sys.exc_info()
5225                 self._reschedule_resize_or_reraise(context, instance,
5226                         exc_info, instance_type, request_spec,
5227                         filter_properties, host_list)
5228             finally:
5229                 self._send_prep_resize_notifications(
5230                     context, instance, fields.NotificationPhase.END,
5231                     instance_type)
5232 
5233     def _reschedule_resize_or_reraise(self, context, instance, exc_info,
5234             instance_type, request_spec, filter_properties, host_list):
5235         """Try to re-schedule the resize or re-raise the original error to
5236         error out the instance.
5237         """
5238         if not filter_properties:
5239             filter_properties = {}
5240 
5241         rescheduled = False
5242         instance_uuid = instance.uuid
5243 
5244         try:
5245             retry = filter_properties.get('retry')
5246             if retry:
5247                 LOG.debug('Rescheduling, attempt %d', retry['num_attempts'],
5248                           instance_uuid=instance_uuid)
5249 
5250                 # reset the task state
5251                 task_state = task_states.RESIZE_PREP
5252                 self._instance_update(context, instance, task_state=task_state)
5253 
5254                 if exc_info:
5255                     # stringify to avoid circular ref problem in json
5256                     # serialization
5257                     retry['exc'] = traceback.format_exception_only(
5258                         exc_info[0], exc_info[1])
5259 
5260                 scheduler_hint = {'filter_properties': filter_properties}
5261 
5262                 self.compute_task_api.resize_instance(
5263                     context, instance, scheduler_hint, instance_type,
5264                     request_spec=request_spec, host_list=host_list)
5265 
5266                 rescheduled = True
5267             else:
5268                 # no retry information, do not reschedule.
5269                 LOG.debug('Retry info not present, will not reschedule',
5270                           instance_uuid=instance_uuid)
5271                 rescheduled = False
5272         except Exception as error:
5273             rescheduled = False
5274             LOG.exception("Error trying to reschedule",
5275                           instance_uuid=instance_uuid)
5276             compute_utils.add_instance_fault_from_exc(context,
5277                     instance, error,
5278                     exc_info=sys.exc_info())
5279             self._notify_about_instance_usage(context, instance,
5280                     'resize.error', fault=error)
5281             compute_utils.notify_about_instance_action(
5282                 context, instance, self.host,
5283                 action=fields.NotificationAction.RESIZE,
5284                 phase=fields.NotificationPhase.ERROR,
5285                 exception=error,
5286             )
5287 
5288         if rescheduled:
5289             self._log_original_error(exc_info, instance_uuid)
5290             compute_utils.add_instance_fault_from_exc(context,
5291                     instance, exc_info[1], exc_info=exc_info)
5292             self._notify_about_instance_usage(context, instance,
5293                     'resize.error', fault=exc_info[1])
5294             compute_utils.notify_about_instance_action(
5295                 context, instance, self.host,
5296                 action=fields.NotificationAction.RESIZE,
5297                 phase=fields.NotificationPhase.ERROR,
5298                 exception=exc_info[1],
5299             )
5300         else:
5301             # not re-scheduling
5302             exc = exc_info[1] or exc_info[0]()
5303             if exc.__traceback__ is not exc_info[2]:
5304                 raise exc.with_traceback(exc_info[2])
5305             raise exc
5306 
5307     # TODO(stephenfin): Remove unused request_spec parameter in API v6.0
5308     @messaging.expected_exceptions(exception.MigrationPreCheckError)
5309     @wrap_exception()
5310     @wrap_instance_event(prefix='compute')
5311     @wrap_instance_fault
5312     def prep_snapshot_based_resize_at_dest(
5313             self, ctxt, instance, flavor, nodename, migration, limits,
5314             request_spec):
5315         """Performs pre-cross-cell resize resource claim on the dest host.
5316 
5317         This runs on the destination host in a cross-cell resize operation
5318         before the resize is actually started.
5319 
5320         Performs a resize_claim for resources that are not claimed in placement
5321         like PCI devices and NUMA topology.
5322 
5323         Note that this is different from same-cell prep_resize in that this:
5324 
5325         * Does not RPC cast to the source compute, that is orchestrated from
5326           conductor.
5327         * This does not reschedule on failure, conductor handles that since
5328           conductor is synchronously RPC calling this method. As such, the
5329           reverts_task_state decorator is not used on this method.
5330 
5331         :param ctxt: user auth request context
5332         :param instance: the instance being resized
5333         :param flavor: the flavor being resized to (unchanged for cold migrate)
5334         :param nodename: Name of the target compute node
5335         :param migration: nova.objects.Migration object for the operation
5336         :param limits: nova.objects.SchedulerLimits object of resource limits
5337         :param request_spec: nova.objects.RequestSpec object for the operation
5338         :returns: nova.objects.MigrationContext; the migration context created
5339             on the destination host during the resize_claim.
5340         :raises: nova.exception.MigrationPreCheckError if the pre-check
5341             validation fails for the given host selection
5342         """
5343         LOG.debug('Checking if we can cross-cell migrate instance to this '
5344                   'host (%s).', self.host, instance=instance)
5345         self._send_prep_resize_notifications(
5346             ctxt, instance, fields.NotificationPhase.START, flavor)
5347         # TODO(mriedem): update_pci_request_spec_with_allocated_interface_name
5348         # should be called here if the request spec has request group mappings,
5349         # e.g. for things like QoS ports with resource requests. Do it outside
5350         # the try/except so if it raises BuildAbortException we do not attempt
5351         # to reschedule.
5352         try:
5353             # Get the allocations within the try/except block in case we get
5354             # an error so MigrationPreCheckError is raised up.
5355             allocations = self.reportclient.get_allocs_for_consumer(
5356                 ctxt, instance.uuid)['allocations']
5357             # Claim resources on this target host using the new flavor which
5358             # will create the MigrationContext object. Note that in the future
5359             # if we want to do other validation here we should do it within
5360             # the MoveClaim context so we can drop the claim if anything fails.
5361             self.rt.resize_claim(
5362                 ctxt, instance, flavor, nodename, migration, allocations,
5363                 image_meta=instance.image_meta, limits=limits)
5364         except Exception as ex:
5365             err = str(ex)
5366             LOG.warning(
5367                 'Cross-cell resize pre-checks failed for this host (%s). '
5368                 'Cleaning up. Failure: %s', self.host, err,
5369                 instance=instance, exc_info=True)
5370             raise exception.MigrationPreCheckError(
5371                 reason=(_("Pre-checks failed on host '%(host)s'. "
5372                           "Error: %(error)s") %
5373                         {'host': self.host, 'error': err}))
5374         finally:
5375             self._send_prep_resize_notifications(
5376                 ctxt, instance, fields.NotificationPhase.END, flavor)
5377 
5378         # ResourceTracker.resize_claim() sets instance.migration_context.
5379         return instance.migration_context
5380 
5381     @messaging.expected_exceptions(exception.InstancePowerOffFailure)
5382     @wrap_exception()
5383     @reverts_task_state
5384     @wrap_instance_event(prefix='compute')
5385     @errors_out_migration
5386     @wrap_instance_fault
5387     def prep_snapshot_based_resize_at_source(
5388             self, ctxt, instance, migration, snapshot_id=None):
5389         """Prepares the instance at the source host for cross-cell resize
5390 
5391         Performs actions like powering off the guest, upload snapshot data if
5392         the instance is not volume-backed, disconnecting volumes, unplugging
5393         VIFs and activating the destination host port bindings.
5394 
5395         :param ctxt: user auth request context targeted at source cell
5396         :param instance: nova.objects.Instance; the instance being resized.
5397             The expected instance.task_state is "resize_migrating" when calling
5398             this method, and the expected task_state upon successful completion
5399             is "resize_migrated".
5400         :param migration: nova.objects.Migration object for the operation.
5401             The expected migration.status is "pre-migrating" when calling this
5402             method and the expected status upon successful completion is
5403             "post-migrating".
5404         :param snapshot_id: ID of the image snapshot to upload if not a
5405             volume-backed instance
5406         :raises: nova.exception.InstancePowerOffFailure if stopping the
5407             instance fails
5408         """
5409         LOG.info('Preparing for snapshot based resize on source host %s.',
5410                  self.host, instance=instance)
5411         # Note that if anything fails here, the migration-based allocations
5412         # created in conductor should be reverted by conductor as well,
5413         # see MigrationTask.rollback.
5414         self._prep_snapshot_based_resize_at_source(
5415             ctxt, instance, migration, snapshot_id=snapshot_id)
5416 
5417     @delete_image_on_error
5418     def _snapshot_for_resize(self, ctxt, image_id, instance):
5419         """Uploads snapshot for the instance during a snapshot-based resize
5420 
5421         If the snapshot operation fails the image will be deleted.
5422 
5423         :param ctxt: the nova auth request context for the resize operation
5424         :param image_id: the snapshot image ID
5425         :param instance: the instance to snapshot/resize
5426         """
5427         LOG.debug('Uploading snapshot data for image %s', image_id,
5428                   instance=instance)
5429         # Note that we do not track the snapshot phase task states
5430         # during resize since we do not want to reflect those into the
5431         # actual instance.task_state.
5432         update_task_state = lambda *args, **kwargs: None
5433         with timeutils.StopWatch() as timer:
5434             self.driver.snapshot(ctxt, instance, image_id, update_task_state)
5435             LOG.debug('Took %0.2f seconds to snapshot the instance on '
5436                       'the hypervisor.', timer.elapsed(), instance=instance)
5437 
5438     def _prep_snapshot_based_resize_at_source(
5439             self, ctxt, instance, migration, snapshot_id=None):
5440         """Private method for prep_snapshot_based_resize_at_source so calling
5441         code can handle errors and perform rollbacks as necessary.
5442         """
5443         # Fetch and update the instance.info_cache.
5444         network_info = self.network_api.get_instance_nw_info(ctxt, instance)
5445         # Get the BDMs attached to this instance on this source host.
5446         bdms = instance.get_bdms()
5447         # Send the resize.start notification.
5448         self._send_resize_instance_notifications(
5449             ctxt, instance, bdms, network_info, fields.NotificationPhase.START)
5450         # Update the migration status from "pre-migrating" to "migrating".
5451         migration.status = 'migrating'
5452         migration.save()
5453 
5454         # Since the instance is going to be left on the source host during the
5455         # resize, we need to power it off so we do not have the instance
5456         # potentially running in two places.
5457         LOG.debug('Stopping instance', instance=instance)
5458         try:
5459             self._power_off_instance(instance)
5460         except Exception as e:
5461             LOG.exception('Failed to power off instance.', instance=instance)
5462             raise exception.InstancePowerOffFailure(reason=str(e))
5463         instance.power_state = self._get_power_state(instance)
5464 
5465         # If a snapshot image ID was provided, we need to snapshot the guest
5466         # disk image and upload it to the image service.
5467         if snapshot_id:
5468             self._snapshot_for_resize(ctxt, snapshot_id, instance)
5469 
5470         block_device_info = self._get_instance_block_device_info(
5471             ctxt, instance, bdms=bdms)
5472 
5473         # If something fails at this point the instance must go to ERROR
5474         # status for operator intervention or to reboot/rebuild the instance.
5475         with self._error_out_instance_on_exception(
5476                 ctxt, instance, instance_state=vm_states.ERROR):
5477 
5478             # Destroy the guest on the source host which will disconnect
5479             # volumes and unplug VIFs. Note that we DO NOT destroy disks since
5480             # we want to leave those on the source host in case of a later
5481             # failure and disks are needed to recover the guest or in case the
5482             # resize is reverted.
5483             LOG.debug('Destroying guest on source host but retaining disks.',
5484                       instance=instance)
5485             self.driver.destroy(
5486                 ctxt, instance, network_info,
5487                 block_device_info=block_device_info, destroy_disks=False)
5488 
5489             # At this point the volumes are disconnected from this source host.
5490             # Delete the old volume attachment records and create new empty
5491             # ones which will be used later if the resize is reverted.
5492             LOG.debug('Deleting volume attachments for the source host.',
5493                       instance=instance)
5494             self._terminate_volume_connections(ctxt, instance, bdms)
5495 
5496             # At this point the VIFs are unplugged from this source host.
5497             # Activate the dest host port bindings created by conductor.
5498             self.network_api.migrate_instance_start(ctxt, instance, migration)
5499 
5500             # Update the migration status from "migrating" to "post-migrating".
5501             migration.status = 'post-migrating'
5502             migration.save()
5503 
5504             # At this point, the traditional resize_instance would update the
5505             # instance host/node values to point at the dest host/node because
5506             # that is where the disk is transferred during resize_instance, but
5507             # with cross-cell resize the instance is not yet at the dest host
5508             # so we do not make that update here.
5509             instance.task_state = task_states.RESIZE_MIGRATED
5510             instance.save(expected_task_state=task_states.RESIZE_MIGRATING)
5511 
5512         self._send_resize_instance_notifications(
5513             ctxt, instance, bdms, network_info,
5514             fields.NotificationPhase.END)
5515         self.instance_events.clear_events_for_instance(instance)
5516 
5517     @wrap_exception()
5518     @reverts_task_state
5519     @wrap_instance_event(prefix='compute')
5520     @wrap_instance_fault
5521     def resize_instance(self, context, instance, image,
5522                         migration, instance_type, clean_shutdown,
5523                         request_spec=None):
5524         """Starts the migration of a running instance to another host.
5525 
5526         This is initiated from the destination host's ``prep_resize`` routine
5527         and runs on the source host.
5528         """
5529         try:
5530             self._resize_instance(context, instance, image, migration,
5531                                   instance_type, clean_shutdown, request_spec)
5532         except Exception:
5533             with excutils.save_and_reraise_exception():
5534                 self._revert_allocation(context, instance, migration)
5535 
5536     def _resize_instance(self, context, instance, image,
5537                          migration, instance_type, clean_shutdown,
5538                          request_spec):
5539         # Pass instance_state=instance.vm_state because we can resize
5540         # a STOPPED server and we don't want to set it back to ACTIVE
5541         # in case migrate_disk_and_power_off raises InstanceFaultRollback.
5542         instance_state = instance.vm_state
5543         with self._error_out_instance_on_exception(
5544                 context, instance, instance_state=instance_state), \
5545              errors_out_migration_ctxt(migration):
5546             network_info = self.network_api.get_instance_nw_info(context,
5547                                                                  instance)
5548 
5549             migration.status = 'migrating'
5550             migration.save()
5551 
5552             instance.task_state = task_states.RESIZE_MIGRATING
5553             instance.save(expected_task_state=task_states.RESIZE_PREP)
5554 
5555             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
5556                     context, instance.uuid)
5557             self._send_resize_instance_notifications(
5558                 context, instance, bdms, network_info,
5559                 fields.NotificationPhase.START)
5560 
5561             block_device_info = self._get_instance_block_device_info(
5562                                 context, instance, bdms=bdms)
5563 
5564             timeout, retry_interval = self._get_power_off_values(
5565                 instance, clean_shutdown)
5566             disk_info = self.driver.migrate_disk_and_power_off(
5567                     context, instance, migration.dest_host,
5568                     instance_type, network_info,
5569                     block_device_info,
5570                     timeout, retry_interval)
5571 
5572             self._terminate_volume_connections(context, instance, bdms)
5573 
5574             self.network_api.migrate_instance_start(context,
5575                                                     instance,
5576                                                     migration)
5577 
5578             migration.status = 'post-migrating'
5579             migration.save()
5580 
5581             instance.host = migration.dest_compute
5582             instance.node = migration.dest_node
5583             instance.task_state = task_states.RESIZE_MIGRATED
5584             instance.save(expected_task_state=task_states.RESIZE_MIGRATING)
5585 
5586             # RPC cast to the destination host to finish the resize/migration.
5587             self.compute_rpcapi.finish_resize(context, instance,
5588                 migration, image, disk_info, migration.dest_compute,
5589                 request_spec)
5590 
5591         self._send_resize_instance_notifications(
5592             context, instance, bdms, network_info,
5593             fields.NotificationPhase.END)
5594         self.instance_events.clear_events_for_instance(instance)
5595 
5596     def _send_resize_instance_notifications(
5597             self, context, instance, bdms, network_info, phase):
5598         """Send "resize.(start|end)" notifications.
5599 
5600         :param context: nova auth request context
5601         :param instance: The instance being resized
5602         :param bdms: BlockDeviceMappingList for the BDMs associated with the
5603             instance
5604         :param network_info: NetworkInfo for the instance info cache of ports
5605         :param phase: The phase of the action (NotificationPhase enum, either
5606             ``start`` or ``end``)
5607         """
5608         action = fields.NotificationAction.RESIZE
5609         # Send the legacy unversioned notification.
5610         self._notify_about_instance_usage(
5611             context, instance, "%s.%s" % (action, phase),
5612             network_info=network_info)
5613         # Send the versioned notification.
5614         compute_utils.notify_about_instance_action(
5615             context, instance, self.host, action=action, phase=phase,
5616             bdms=bdms)
5617 
5618     def _terminate_volume_connections(self, context, instance, bdms):
5619         connector = None
5620         for bdm in bdms:
5621             if bdm.is_volume:
5622                 if bdm.attachment_id:
5623                     # NOTE(jdg): So here's the thing, the idea behind the new
5624                     # attach API's was to have a new code fork/path that we
5625                     # followed, we're not going to do that so we have to do
5626                     # some extra work in here to make it *behave* just like the
5627                     # old code. Cinder doesn't allow disconnect/reconnect (you
5628                     # just delete the attachment and get a new one)
5629                     # attachments in the new attach code so we have to do
5630                     # a delete and create without a connector (reserve),
5631                     # in other words, beware
5632                     attachment_id = self.volume_api.attachment_create(
5633                         context, bdm.volume_id, instance.uuid)['id']
5634                     self.volume_api.attachment_delete(context,
5635                                                       bdm.attachment_id)
5636                     bdm.attachment_id = attachment_id
5637                     bdm.save()
5638 
5639                 else:
5640                     if connector is None:
5641                         connector = self.driver.get_volume_connector(instance)
5642                     self.volume_api.terminate_connection(context,
5643                                                          bdm.volume_id,
5644                                                          connector)
5645 
5646     @staticmethod
5647     def _set_instance_info(instance, instance_type):
5648         instance.instance_type_id = instance_type.id
5649         instance.memory_mb = instance_type.memory_mb
5650         instance.vcpus = instance_type.vcpus
5651         instance.root_gb = instance_type.root_gb
5652         instance.ephemeral_gb = instance_type.ephemeral_gb
5653         instance.flavor = instance_type
5654 
5655     def _update_volume_attachments(self, context, instance, bdms):
5656         """Updates volume attachments using the virt driver host connector.
5657 
5658         :param context: nova.context.RequestContext - user request context
5659         :param instance: nova.objects.Instance
5660         :param bdms: nova.objects.BlockDeviceMappingList - the list of block
5661                      device mappings for the given instance
5662         """
5663         if bdms:
5664             connector = None
5665             for bdm in bdms:
5666                 if bdm.is_volume and bdm.attachment_id:
5667                     if connector is None:
5668                         connector = self.driver.get_volume_connector(instance)
5669                     self.volume_api.attachment_update(
5670                         context, bdm.attachment_id, connector, bdm.device_name)
5671 
5672     def _complete_volume_attachments(self, context, bdms):
5673         """Completes volume attachments for the instance
5674 
5675         :param context: nova.context.RequestContext - user request context
5676         :param bdms: nova.objects.BlockDeviceMappingList - the list of block
5677                      device mappings for the given instance
5678         """
5679         if bdms:
5680             for bdm in bdms:
5681                 if bdm.is_volume and bdm.attachment_id:
5682                     self.volume_api.attachment_complete(
5683                         context, bdm.attachment_id)
5684 
5685     def _finish_resize(self, context, instance, migration, disk_info,
5686                        image_meta, bdms, request_spec):
5687         resize_instance = False  # indicates disks have been resized
5688         old_instance_type_id = migration['old_instance_type_id']
5689         new_instance_type_id = migration['new_instance_type_id']
5690         old_flavor = instance.flavor  # the current flavor is now old
5691         # NOTE(mriedem): Get the old_vm_state so we know if we should
5692         # power on the instance. If old_vm_state is not set we need to default
5693         # to ACTIVE for backwards compatibility
5694         old_vm_state = instance.system_metadata.get('old_vm_state',
5695                                                     vm_states.ACTIVE)
5696         instance.old_flavor = old_flavor
5697 
5698         if old_instance_type_id != new_instance_type_id:
5699             new_flavor = instance.new_flavor  # this is set in _prep_resize
5700             # Set the flavor-related fields on the instance object including
5701             # making instance.flavor = new_flavor.
5702             self._set_instance_info(instance, new_flavor)
5703             for key in ('root_gb', 'swap', 'ephemeral_gb'):
5704                 if old_flavor[key] != new_flavor[key]:
5705                     resize_instance = True
5706                     break
5707         instance.apply_migration_context()
5708 
5709         # NOTE(tr3buchet): setup networks on destination host
5710         self.network_api.setup_networks_on_host(context, instance,
5711                                                 migration.dest_compute)
5712         provider_mappings = self._get_request_group_mapping(request_spec)
5713 
5714         # For neutron, migrate_instance_finish updates port bindings for this
5715         # host including any PCI devices claimed for SR-IOV ports.
5716         self.network_api.migrate_instance_finish(
5717             context, instance, migration, provider_mappings)
5718 
5719         network_info = self.network_api.get_instance_nw_info(context, instance)
5720 
5721         instance.task_state = task_states.RESIZE_FINISH
5722         instance.save(expected_task_state=task_states.RESIZE_MIGRATED)
5723 
5724         self._send_finish_resize_notifications(
5725             context, instance, bdms, network_info,
5726             fields.NotificationPhase.START)
5727 
5728         # We need to update any volume attachments using the destination
5729         # host connector so that we can update the BDM.connection_info
5730         # before calling driver.finish_migration otherwise the driver
5731         # won't know how to connect the volumes to this host.
5732         # Note that _get_instance_block_device_info with
5733         # refresh_conn_info=True will update the BDM.connection_info value
5734         # in the database so we must do this before calling that method.
5735         self._update_volume_attachments(context, instance, bdms)
5736 
5737         block_device_info = self._get_instance_block_device_info(
5738             context, instance, refresh_conn_info=True, bdms=bdms)
5739 
5740         # NOTE(mriedem): If the original vm_state was STOPPED, we don't
5741         # automatically power on the instance after it's migrated
5742         power_on = old_vm_state != vm_states.STOPPED
5743 
5744         # NOTE(sbauza): During a migration, the original allocation is against
5745         # the migration UUID while the target allocation (for the destination
5746         # node) is related to the instance UUID, so here we need to pass the
5747         # new ones.
5748         allocations = self.reportclient.get_allocs_for_consumer(
5749             context, instance.uuid)['allocations']
5750 
5751         try:
5752             self.driver.finish_migration(context, migration, instance,
5753                                          disk_info,
5754                                          network_info,
5755                                          image_meta, resize_instance,
5756                                          allocations,
5757                                          block_device_info, power_on)
5758         except Exception:
5759             # Note that we do not rollback port bindings to the source host
5760             # because resize_instance (on the source host) updated the
5761             # instance.host to point to *this* host (the destination host)
5762             # so the port bindings pointing at this host are correct even
5763             # though we failed to create the guest.
5764             with excutils.save_and_reraise_exception():
5765                 # If we failed to create the guest on this host, reset the
5766                 # instance flavor-related fields to the old flavor. An
5767                 # error handler like reverts_task_state will save the changes.
5768                 if old_instance_type_id != new_instance_type_id:
5769                     self._set_instance_info(instance, old_flavor)
5770 
5771         # Now complete any volume attachments that were previously updated.
5772         self._complete_volume_attachments(context, bdms)
5773 
5774         migration.status = 'finished'
5775         migration.save()
5776 
5777         instance.vm_state = vm_states.RESIZED
5778         instance.task_state = None
5779         instance.launched_at = timeutils.utcnow()
5780         instance.save(expected_task_state=task_states.RESIZE_FINISH)
5781 
5782         return network_info
5783 
5784     @wrap_exception()
5785     @reverts_task_state
5786     @wrap_instance_event(prefix='compute')
5787     @errors_out_migration
5788     @wrap_instance_fault
5789     def finish_resize(self, context, disk_info, image, instance,
5790                       migration, request_spec=None):
5791         """Completes the migration process.
5792 
5793         Sets up the newly transferred disk and turns on the instance at its
5794         new host machine.
5795 
5796         """
5797         try:
5798             self._finish_resize_helper(context, disk_info, image, instance,
5799                                        migration, request_spec)
5800         except Exception:
5801             with excutils.save_and_reraise_exception():
5802                 # At this point, resize_instance (which runs on the source) has
5803                 # already updated the instance host/node values to point to
5804                 # this (the dest) compute, so we need to leave the allocations
5805                 # against the dest node resource provider intact and drop the
5806                 # allocations against the source node resource provider. If the
5807                 # user tries to recover the server by hard rebooting it, it
5808                 # will happen on this host so that's where the allocations
5809                 # should go. Note that this is the same method called from
5810                 # confirm_resize to cleanup the source node allocations held
5811                 # by the migration record.
5812                 LOG.info('Deleting allocations for old flavor on source node '
5813                          '%s after finish_resize failure. You may be able to '
5814                          'recover the instance by hard rebooting it.',
5815                          migration.source_compute, instance=instance)
5816                 self._delete_allocation_after_move(
5817                     context, instance, migration)
5818 
5819     def _finish_resize_helper(self, context, disk_info, image, instance,
5820                               migration, request_spec):
5821         """Completes the migration process.
5822 
5823         The caller must revert the instance's allocations if the migration
5824         process failed.
5825         """
5826         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
5827             context, instance.uuid)
5828 
5829         with self._error_out_instance_on_exception(context, instance):
5830             image_meta = objects.ImageMeta.from_dict(image)
5831             network_info = self._finish_resize(context, instance, migration,
5832                                                disk_info, image_meta, bdms,
5833                                                request_spec)
5834 
5835         # TODO(melwitt): We should clean up instance console tokens here. The
5836         # instance is on a new host and will need to establish a new console
5837         # connection.
5838         self._update_scheduler_instance_info(context, instance)
5839         self._send_finish_resize_notifications(
5840             context, instance, bdms, network_info,
5841             fields.NotificationPhase.END)
5842 
5843     def _send_finish_resize_notifications(
5844             self, context, instance, bdms, network_info, phase):
5845         """Send notifications for the finish_resize flow.
5846 
5847         :param context: nova auth request context
5848         :param instance: The instance being resized
5849         :param bdms: BlockDeviceMappingList for the BDMs associated with the
5850             instance
5851         :param network_info: NetworkInfo for the instance info cache of ports
5852         :param phase: The phase of the action (NotificationPhase enum, either
5853             ``start`` or ``end``)
5854         """
5855         # Send the legacy unversioned notification.
5856         self._notify_about_instance_usage(
5857             context, instance, "finish_resize.%s" % phase,
5858             network_info=network_info)
5859         # Send the versioned notification.
5860         compute_utils.notify_about_instance_action(
5861             context, instance, self.host,
5862             action=fields.NotificationAction.RESIZE_FINISH, phase=phase,
5863             bdms=bdms)
5864 
5865     # TODO(stephenfin): Remove unused request_spec parameter in API v6.0
5866     @wrap_exception()
5867     @reverts_task_state
5868     @wrap_instance_event(prefix='compute')
5869     @errors_out_migration
5870     @wrap_instance_fault
5871     def finish_snapshot_based_resize_at_dest(
5872             self, ctxt, instance, migration, snapshot_id, request_spec):
5873         """Finishes the snapshot-based resize at the destination compute.
5874 
5875         Sets up block devices and networking on the destination compute and
5876         spawns the guest.
5877 
5878         :param ctxt: nova auth request context targeted at the target cell DB
5879         :param instance: The Instance object being resized with the
5880             ``migration_context`` field set. Upon successful completion of this
5881             method the vm_state should be "resized", the task_state should be
5882             None, and migration context, host/node and flavor-related fields
5883             should be set on the instance.
5884         :param migration: The Migration object for this resize operation. Upon
5885             successful completion of this method the migration status should
5886             be "finished".
5887         :param snapshot_id: ID of the image snapshot created for a
5888             non-volume-backed instance, else None.
5889         :param request_spec: nova.objects.RequestSpec object for the operation
5890         """
5891         LOG.info('Finishing snapshot based resize on destination host %s.',
5892                  self.host, instance=instance)
5893         with self._error_out_instance_on_exception(ctxt, instance):
5894             # Note that if anything fails here, the migration-based allocations
5895             # created in conductor should be reverted by conductor as well,
5896             # see MigrationTask.rollback.
5897             self._finish_snapshot_based_resize_at_dest(
5898                 ctxt, instance, migration, snapshot_id)
5899 
5900     def _finish_snapshot_based_resize_at_dest(
5901             self, ctxt, instance, migration, snapshot_id):
5902         """Private variant of finish_snapshot_based_resize_at_dest so the
5903         caller can handle reverting resource allocations on failure and perform
5904         other generic error handling.
5905         """
5906         # Figure out the image metadata to use when spawning the guest.
5907         origin_image_ref = instance.image_ref
5908         if snapshot_id:
5909             instance.image_ref = snapshot_id
5910             image_meta = objects.ImageMeta.from_image_ref(
5911                 ctxt, self.image_api, snapshot_id)
5912         else:
5913             # Just use what is already on the volume-backed instance.
5914             image_meta = instance.image_meta
5915 
5916         resize = migration.migration_type == 'resize'
5917         instance.old_flavor = instance.flavor
5918         if resize:
5919             flavor = instance.new_flavor
5920             # If we are resizing to a new flavor we need to set the
5921             # flavor-related fields on the instance.
5922             # NOTE(mriedem): This is likely where storing old/new_flavor on
5923             # the MigrationContext would make this cleaner.
5924             self._set_instance_info(instance, flavor)
5925 
5926         instance.apply_migration_context()
5927         instance.task_state = task_states.RESIZE_FINISH
5928         instance.save(expected_task_state=task_states.RESIZE_MIGRATED)
5929 
5930         # This seems a bit late to be sending the start notification but
5931         # it is what traditional resize has always done as well and it does
5932         # contain the changes to the instance with the new_flavor and
5933         # task_state.
5934         bdms = instance.get_bdms()
5935         network_info = instance.get_network_info()
5936         self._send_finish_resize_notifications(
5937             ctxt, instance, bdms, network_info,
5938             fields.NotificationPhase.START)
5939 
5940         # Setup volumes and networking and spawn the guest in the hypervisor.
5941         self._finish_snapshot_based_resize_at_dest_spawn(
5942             ctxt, instance, migration, image_meta, bdms)
5943 
5944         # If we spawned from a temporary snapshot image we can delete that now,
5945         # similar to how unshelve works.
5946         if snapshot_id:
5947             instance.image_ref = origin_image_ref
5948             compute_utils.delete_image(
5949                 ctxt, instance, self.image_api, snapshot_id)
5950 
5951         migration.status = 'finished'
5952         migration.save()
5953 
5954         self._update_instance_after_spawn(instance, vm_state=vm_states.RESIZED)
5955         # Setting the host/node values will make the ResourceTracker continue
5956         # to track usage for this instance on this host.
5957         instance.host = migration.dest_compute
5958         instance.node = migration.dest_node
5959         instance.save(expected_task_state=task_states.RESIZE_FINISH)
5960 
5961         # Broadcast to all schedulers that the instance is on this host.
5962         self._update_scheduler_instance_info(ctxt, instance)
5963         self._send_finish_resize_notifications(
5964             ctxt, instance, bdms, network_info,
5965             fields.NotificationPhase.END)
5966 
5967     def _finish_snapshot_based_resize_at_dest_spawn(
5968             self, ctxt, instance, migration, image_meta, bdms):
5969         """Sets up volumes and networking and spawns the guest on the dest host
5970 
5971         If the instance was stopped when the resize was initiated the guest
5972         will be created but remain in a shutdown power state.
5973 
5974         If the spawn fails, port bindings are rolled back to the source host
5975         and volume connections are terminated for this dest host.
5976 
5977         :param ctxt: nova auth request context
5978         :param instance: Instance object being migrated
5979         :param migration: Migration object for the operation
5980         :param image_meta: ImageMeta object used during driver.spawn
5981         :param bdms: BlockDeviceMappingList of BDMs for the instance
5982         """
5983         # Update the volume attachments using this host's connector.
5984         # That will update the BlockDeviceMapping.connection_info which
5985         # will be used to connect the volumes on this host during spawn().
5986         block_device_info = self._prep_block_device(ctxt, instance, bdms)
5987 
5988         allocations = self.reportclient.get_allocations_for_consumer(
5989             ctxt, instance.uuid)
5990 
5991         # We do not call self.network_api.setup_networks_on_host here because
5992         # for neutron that sets up the port migration profile which is only
5993         # used during live migration with DVR. Yes it is gross knowing what
5994         # that method does internally. We could change this when bug 1814837
5995         # is fixed if setup_networks_on_host is made smarter by passing the
5996         # migration record and the method checks the migration_type.
5997 
5998         # Activate the port bindings for this host.
5999         # FIXME(mriedem): We're going to have the same issue as bug 1813789
6000         # here because this will update the port bindings and send the
6001         # network-vif-plugged event and that means when driver.spawn waits for
6002         # it we might have already gotten the event and neutron won't send
6003         # another one so we could timeout.
6004         # TODO(mriedem): Calculate provider mappings when we support cross-cell
6005         # resize/migrate with ports having resource requests.
6006         self.network_api.migrate_instance_finish(
6007             ctxt, instance, migration, provider_mappings=None)
6008         network_info = self.network_api.get_instance_nw_info(ctxt, instance)
6009 
6010         # If the original vm_state was STOPPED, we do not automatically
6011         # power on the instance after it is migrated.
6012         power_on = instance.system_metadata['old_vm_state'] == vm_states.ACTIVE
6013         try:
6014             # NOTE(mriedem): If this instance uses a config drive, it will get
6015             # rebuilt here which means any personality files will be lost,
6016             # similar to unshelve. If the instance is not using a config drive
6017             # and getting metadata from the metadata API service, personality
6018             # files would be lost regardless of the move operation.
6019             self.driver.spawn(
6020                 ctxt, instance, image_meta, injected_files=[],
6021                 admin_password=None, allocations=allocations,
6022                 network_info=network_info, block_device_info=block_device_info,
6023                 power_on=power_on)
6024         except Exception:
6025             with excutils.save_and_reraise_exception(logger=LOG):
6026                 # Rollback port bindings to the source host.
6027                 try:
6028                     # This is gross but migrate_instance_start looks at the
6029                     # migration.dest_compute to determine where to activate the
6030                     # port bindings and we want the source compute port
6031                     # bindings to be re-activated. Remember at this point the
6032                     # instance.host is still pointing at the source compute.
6033                     # TODO(mriedem): Maybe we should be calling
6034                     # setup_instance_network_on_host here to deal with pci
6035                     # devices?
6036                     with utils.temporary_mutation(
6037                             migration, dest_compute=migration.source_compute):
6038                         self.network_api.migrate_instance_start(
6039                             ctxt, instance, migration)
6040                 except Exception:
6041                     LOG.exception(
6042                         'Failed to activate port bindings on the source '
6043                         'host: %s', migration.source_compute,
6044                         instance=instance)
6045 
6046                 # Rollback volume connections on this host.
6047                 for bdm in bdms:
6048                     if bdm.is_volume:
6049                         try:
6050                             self._remove_volume_connection(
6051                                 ctxt, bdm, instance, delete_attachment=True)
6052                         except Exception:
6053                             LOG.exception('Failed to remove volume connection '
6054                                           'on this host %s for volume %s.',
6055                                           self.host, bdm.volume_id,
6056                                           instance=instance)
6057 
6058     @wrap_exception()
6059     @wrap_instance_fault
6060     def add_fixed_ip_to_instance(self, context, network_id, instance):
6061         """Calls network_api to add new fixed_ip to instance
6062         then injects the new network info and resets instance networking.
6063 
6064         """
6065         self._notify_about_instance_usage(
6066                 context, instance, "create_ip.start")
6067 
6068         network_info = self.network_api.add_fixed_ip_to_instance(context,
6069                                                                  instance,
6070                                                                  network_id)
6071         self._inject_network_info(instance, network_info)
6072 
6073         # NOTE(russellb) We just want to bump updated_at.  See bug 1143466.
6074         instance.updated_at = timeutils.utcnow()
6075         instance.save()
6076 
6077         self._notify_about_instance_usage(
6078             context, instance, "create_ip.end", network_info=network_info)
6079 
6080     @wrap_exception()
6081     @wrap_instance_fault
6082     def remove_fixed_ip_from_instance(self, context, address, instance):
6083         """Calls network_api to remove existing fixed_ip from instance
6084         by injecting the altered network info and resetting
6085         instance networking.
6086         """
6087         self._notify_about_instance_usage(
6088                 context, instance, "delete_ip.start")
6089 
6090         network_info = self.network_api.remove_fixed_ip_from_instance(context,
6091                                                                       instance,
6092                                                                       address)
6093         self._inject_network_info(instance, network_info)
6094 
6095         # NOTE(russellb) We just want to bump updated_at.  See bug 1143466.
6096         instance.updated_at = timeutils.utcnow()
6097         instance.save()
6098 
6099         self._notify_about_instance_usage(
6100             context, instance, "delete_ip.end", network_info=network_info)
6101 
6102     @wrap_exception()
6103     @reverts_task_state
6104     @wrap_instance_event(prefix='compute')
6105     @wrap_instance_fault
6106     def pause_instance(self, context, instance):
6107         """Pause an instance on this host."""
6108         context = context.elevated()
6109         LOG.info('Pausing', instance=instance)
6110         self._notify_about_instance_usage(context, instance, 'pause.start')
6111         compute_utils.notify_about_instance_action(context, instance,
6112                self.host, action=fields.NotificationAction.PAUSE,
6113                phase=fields.NotificationPhase.START)
6114         self.driver.pause(instance)
6115         instance.power_state = self._get_power_state(instance)
6116         instance.vm_state = vm_states.PAUSED
6117         instance.task_state = None
6118         instance.save(expected_task_state=task_states.PAUSING)
6119         self._notify_about_instance_usage(context, instance, 'pause.end')
6120         compute_utils.notify_about_instance_action(context, instance,
6121                self.host, action=fields.NotificationAction.PAUSE,
6122                phase=fields.NotificationPhase.END)
6123 
6124     @wrap_exception()
6125     @reverts_task_state
6126     @wrap_instance_event(prefix='compute')
6127     @wrap_instance_fault
6128     def unpause_instance(self, context, instance):
6129         """Unpause a paused instance on this host."""
6130         context = context.elevated()
6131         LOG.info('Unpausing', instance=instance)
6132         self._notify_about_instance_usage(context, instance, 'unpause.start')
6133         compute_utils.notify_about_instance_action(context, instance,
6134             self.host, action=fields.NotificationAction.UNPAUSE,
6135             phase=fields.NotificationPhase.START)
6136         self.driver.unpause(instance)
6137         instance.power_state = self._get_power_state(instance)
6138         instance.vm_state = vm_states.ACTIVE
6139         instance.task_state = None
6140         instance.save(expected_task_state=task_states.UNPAUSING)
6141         self._notify_about_instance_usage(context, instance, 'unpause.end')
6142         compute_utils.notify_about_instance_action(context, instance,
6143             self.host, action=fields.NotificationAction.UNPAUSE,
6144             phase=fields.NotificationPhase.END)
6145 
6146     @wrap_exception()
6147     def host_power_action(self, context, action):
6148         """Reboots, shuts down or powers up the host."""
6149         return self.driver.host_power_action(action)
6150 
6151     @wrap_exception()
6152     def host_maintenance_mode(self, context, host, mode):
6153         """Start/Stop host maintenance window. On start, it triggers
6154         guest VMs evacuation.
6155         """
6156         return self.driver.host_maintenance_mode(host, mode)
6157 
6158     def _update_compute_provider_status(self, context, enabled):
6159         """Adds or removes the COMPUTE_STATUS_DISABLED trait for this host.
6160 
6161         For each ComputeNode managed by this service, adds or removes the
6162         COMPUTE_STATUS_DISABLED traits to/from the associated resource provider
6163         in Placement.
6164 
6165         :param context: nova auth RequestContext
6166         :param enabled: True if the node is enabled in which case the trait
6167             would be removed, False if the node is disabled in which case
6168             the trait would be added.
6169         :raises: ComputeHostNotFound if there are no compute nodes found in
6170             the ResourceTracker for this service.
6171         """
6172         # Get the compute node(s) on this host. Remember that ironic can be
6173         # managing more than one compute node.
6174         nodes = self.rt.compute_nodes.values()
6175         if not nodes:
6176             raise exception.ComputeHostNotFound(host=self.host)
6177         # For each node, we want to add (or remove) the COMPUTE_STATUS_DISABLED
6178         # trait on the related resource provider in placement so the scheduler
6179         # (pre-)filters the provider based on its status.
6180         for node in nodes:
6181             try:
6182                 self.virtapi.update_compute_provider_status(
6183                     context, node.uuid, enabled)
6184             except (exception.ResourceProviderTraitRetrievalFailed,
6185                     exception.ResourceProviderUpdateConflict,
6186                     exception.ResourceProviderUpdateFailed,
6187                     exception.TraitRetrievalFailed) as e:
6188                 # This is best effort so just log a warning and continue.
6189                 LOG.warning('An error occurred while updating '
6190                             'COMPUTE_STATUS_DISABLED trait on compute node '
6191                             'resource provider %s. The trait will be '
6192                             'synchronized when the update_available_resource '
6193                             'periodic task runs. Error: %s',
6194                             node.uuid, e.format_message())
6195             except Exception:
6196                 LOG.exception('An error occurred while updating '
6197                               'COMPUTE_STATUS_DISABLED trait on compute node '
6198                               'resource provider %s. The trait will be '
6199                               'synchronized when the '
6200                               'update_available_resource periodic task runs.',
6201                               node.uuid)
6202 
6203     @wrap_exception()
6204     def set_host_enabled(self, context, enabled):
6205         """Sets the specified host's ability to accept new instances.
6206 
6207         This method will add or remove the COMPUTE_STATUS_DISABLED trait
6208         to/from the associated compute node resource provider(s) for this
6209         compute service.
6210         """
6211         try:
6212             self._update_compute_provider_status(context, enabled)
6213         except exception.ComputeHostNotFound:
6214             LOG.warning('Unable to add/remove trait COMPUTE_STATUS_DISABLED. '
6215                         'No ComputeNode(s) found for host: %s', self.host)
6216 
6217         try:
6218             return self.driver.set_host_enabled(enabled)
6219         except NotImplementedError:
6220             # Only the xenapi driver implements set_host_enabled but we don't
6221             # want NotImplementedError to get raised back to the API. We still
6222             # need to honor the compute RPC API contract and return 'enabled'
6223             # or 'disabled' though.
6224             return 'enabled' if enabled else 'disabled'
6225 
6226     @wrap_exception()
6227     def get_host_uptime(self, context):
6228         """Returns the result of calling "uptime" on the target host."""
6229         return self.driver.get_host_uptime()
6230 
6231     @wrap_exception()
6232     @wrap_instance_fault
6233     def get_diagnostics(self, context, instance):
6234         """Retrieve diagnostics for an instance on this host."""
6235         current_power_state = self._get_power_state(instance)
6236         if current_power_state == power_state.RUNNING:
6237             LOG.info("Retrieving diagnostics", instance=instance)
6238             return self.driver.get_diagnostics(instance)
6239         else:
6240             raise exception.InstanceInvalidState(
6241                 attr='power state',
6242                 instance_uuid=instance.uuid,
6243                 state=power_state.STATE_MAP[instance.power_state],
6244                 method='get_diagnostics')
6245 
6246     @wrap_exception()
6247     @wrap_instance_fault
6248     def get_instance_diagnostics(self, context, instance):
6249         """Retrieve diagnostics for an instance on this host."""
6250         current_power_state = self._get_power_state(instance)
6251         if current_power_state == power_state.RUNNING:
6252             LOG.info("Retrieving diagnostics", instance=instance)
6253             return self.driver.get_instance_diagnostics(instance)
6254         else:
6255             raise exception.InstanceInvalidState(
6256                 attr='power state',
6257                 instance_uuid=instance.uuid,
6258                 state=power_state.STATE_MAP[instance.power_state],
6259                 method='get_diagnostics')
6260 
6261     @wrap_exception()
6262     @reverts_task_state
6263     @wrap_instance_event(prefix='compute')
6264     @wrap_instance_fault
6265     def suspend_instance(self, context, instance):
6266         """Suspend the given instance."""
6267         context = context.elevated()
6268 
6269         # Store the old state
6270         instance.system_metadata['old_vm_state'] = instance.vm_state
6271         self._notify_about_instance_usage(context, instance, 'suspend.start')
6272         compute_utils.notify_about_instance_action(context, instance,
6273                 self.host, action=fields.NotificationAction.SUSPEND,
6274                 phase=fields.NotificationPhase.START)
6275         with self._error_out_instance_on_exception(context, instance,
6276              instance_state=instance.vm_state):
6277             self.driver.suspend(context, instance)
6278         instance.power_state = self._get_power_state(instance)
6279         instance.vm_state = vm_states.SUSPENDED
6280         instance.task_state = None
6281         instance.save(expected_task_state=task_states.SUSPENDING)
6282         self._notify_about_instance_usage(context, instance, 'suspend.end')
6283         compute_utils.notify_about_instance_action(context, instance,
6284                 self.host, action=fields.NotificationAction.SUSPEND,
6285                 phase=fields.NotificationPhase.END)
6286 
6287     @wrap_exception()
6288     @reverts_task_state
6289     @wrap_instance_event(prefix='compute')
6290     @wrap_instance_fault
6291     def resume_instance(self, context, instance):
6292         """Resume the given suspended instance."""
6293         context = context.elevated()
6294         LOG.info('Resuming', instance=instance)
6295 
6296         self._notify_about_instance_usage(context, instance, 'resume.start')
6297 
6298         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
6299             context, instance.uuid)
6300         block_device_info = self._get_instance_block_device_info(
6301             context, instance, bdms=bdms)
6302 
6303         compute_utils.notify_about_instance_action(context, instance,
6304             self.host, action=fields.NotificationAction.RESUME,
6305             phase=fields.NotificationPhase.START, bdms=bdms)
6306 
6307         network_info = self.network_api.get_instance_nw_info(context, instance)
6308 
6309         with self._error_out_instance_on_exception(context, instance,
6310              instance_state=instance.vm_state):
6311             self.driver.resume(context, instance, network_info,
6312                                block_device_info)
6313 
6314         instance.power_state = self._get_power_state(instance)
6315 
6316         # We default to the ACTIVE state for backwards compatibility
6317         instance.vm_state = instance.system_metadata.pop('old_vm_state',
6318                                                          vm_states.ACTIVE)
6319 
6320         instance.task_state = None
6321         instance.save(expected_task_state=task_states.RESUMING)
6322         self._notify_about_instance_usage(context, instance, 'resume.end')
6323         compute_utils.notify_about_instance_action(context, instance,
6324             self.host, action=fields.NotificationAction.RESUME,
6325             phase=fields.NotificationPhase.END, bdms=bdms)
6326 
6327     @wrap_exception()
6328     @reverts_task_state
6329     @wrap_instance_event(prefix='compute')
6330     @wrap_instance_fault
6331     def shelve_instance(self, context, instance, image_id,
6332                         clean_shutdown, accel_uuids=None):
6333         """Shelve an instance.
6334 
6335         This should be used when you want to take a snapshot of the instance.
6336         It also adds system_metadata that can be used by a periodic task to
6337         offload the shelved instance after a period of time.
6338 
6339         :param context: request context
6340         :param instance: an Instance object
6341         :param image_id: an image id to snapshot to.
6342         :param clean_shutdown: give the GuestOS a chance to stop
6343         :param accel_uuids: the accelerators uuids for the instance
6344         """
6345 
6346         @utils.synchronized(instance.uuid)
6347         def do_shelve_instance():
6348             self._shelve_instance(context, instance, image_id, clean_shutdown,
6349                                   accel_uuids)
6350         do_shelve_instance()
6351 
6352     def _shelve_instance(self, context, instance, image_id,
6353                          clean_shutdown, accel_uuids=None):
6354         LOG.info('Shelving', instance=instance)
6355         offload = CONF.shelved_offload_time == 0
6356         if offload:
6357             # Get the BDMs early so we can pass them into versioned
6358             # notifications since _shelve_offload_instance needs the
6359             # BDMs anyway.
6360             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
6361                 context, instance.uuid)
6362         else:
6363             bdms = None
6364         compute_utils.notify_usage_exists(self.notifier, context, instance,
6365                                           self.host, current_period=True)
6366         self._notify_about_instance_usage(context, instance, 'shelve.start')
6367         compute_utils.notify_about_instance_action(context, instance,
6368                 self.host, action=fields.NotificationAction.SHELVE,
6369                 phase=fields.NotificationPhase.START, bdms=bdms)
6370 
6371         def update_task_state(task_state, expected_state=task_states.SHELVING):
6372             shelving_state_map = {
6373                     task_states.IMAGE_PENDING_UPLOAD:
6374                         task_states.SHELVING_IMAGE_PENDING_UPLOAD,
6375                     task_states.IMAGE_UPLOADING:
6376                         task_states.SHELVING_IMAGE_UPLOADING,
6377                     task_states.SHELVING: task_states.SHELVING}
6378             task_state = shelving_state_map[task_state]
6379             expected_state = shelving_state_map[expected_state]
6380             instance.task_state = task_state
6381             instance.save(expected_task_state=expected_state)
6382         # Do not attempt a clean shutdown of a paused guest since some
6383         # hypervisors will fail the clean shutdown if the guest is not
6384         # running.
6385         if instance.power_state == power_state.PAUSED:
6386             clean_shutdown = False
6387         self._power_off_instance(instance, clean_shutdown)
6388         self.driver.snapshot(context, instance, image_id, update_task_state)
6389 
6390         instance.system_metadata['shelved_at'] = timeutils.utcnow().isoformat()
6391         instance.system_metadata['shelved_image_id'] = image_id
6392         instance.system_metadata['shelved_host'] = self.host
6393         instance.vm_state = vm_states.SHELVED
6394         instance.task_state = None
6395         if offload:
6396             instance.task_state = task_states.SHELVING_OFFLOADING
6397         instance.power_state = self._get_power_state(instance)
6398         instance.save(expected_task_state=[
6399                 task_states.SHELVING,
6400                 task_states.SHELVING_IMAGE_UPLOADING])
6401 
6402         self._notify_about_instance_usage(context, instance, 'shelve.end')
6403         compute_utils.notify_about_instance_action(context, instance,
6404                 self.host, action=fields.NotificationAction.SHELVE,
6405                 phase=fields.NotificationPhase.END, bdms=bdms)
6406 
6407         if offload:
6408             self._shelve_offload_instance(
6409                 context, instance, clean_shutdown=False, bdms=bdms,
6410                 accel_uuids=accel_uuids)
6411 
6412     @wrap_exception()
6413     @reverts_task_state
6414     @wrap_instance_event(prefix='compute')
6415     @wrap_instance_fault
6416     def shelve_offload_instance(self, context, instance, clean_shutdown,
6417             accel_uuids=None):
6418         """Remove a shelved instance from the hypervisor.
6419 
6420         This frees up those resources for use by other instances, but may lead
6421         to slower unshelve times for this instance.  This method is used by
6422         volume backed instances since restoring them doesn't involve the
6423         potentially large download of an image.
6424 
6425         :param context: request context
6426         :param instance: nova.objects.instance.Instance
6427         :param clean_shutdown: give the GuestOS a chance to stop
6428         :param accel_uuids: the accelerators uuids for the instance
6429         """
6430 
6431         @utils.synchronized(instance.uuid)
6432         def do_shelve_offload_instance():
6433             self._shelve_offload_instance(context, instance, clean_shutdown,
6434                                           accel_uuids=accel_uuids)
6435         do_shelve_offload_instance()
6436 
6437     def _shelve_offload_instance(self, context, instance, clean_shutdown,
6438                                  bdms=None, accel_uuids=None):
6439         LOG.info('Shelve offloading', instance=instance)
6440         if bdms is None:
6441             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
6442                 context, instance.uuid)
6443         self._notify_about_instance_usage(context, instance,
6444                 'shelve_offload.start')
6445         compute_utils.notify_about_instance_action(context, instance,
6446                 self.host, action=fields.NotificationAction.SHELVE_OFFLOAD,
6447                 phase=fields.NotificationPhase.START, bdms=bdms)
6448 
6449         self._power_off_instance(instance, clean_shutdown)
6450         current_power_state = self._get_power_state(instance)
6451         network_info = self.network_api.get_instance_nw_info(context, instance)
6452 
6453         block_device_info = self._get_instance_block_device_info(context,
6454                                                                  instance,
6455                                                                  bdms=bdms)
6456         self.driver.destroy(context, instance, network_info,
6457                 block_device_info)
6458 
6459         # the instance is going to be removed from the host so we want to
6460         # terminate all the connections with the volume server and the host
6461         self._terminate_volume_connections(context, instance, bdms)
6462 
6463         # NOTE(brinzhang): Free up the accelerator resource occupied
6464         # in the cyborg service.
6465         if accel_uuids:
6466             cyclient = cyborg.get_client(context)
6467             cyclient.delete_arqs_for_instance(instance.uuid)
6468 
6469         # Free up the resource allocations in the placement service.
6470         # This should happen *before* the vm_state is changed to
6471         # SHELVED_OFFLOADED in case client-side code is polling the API to
6472         # schedule more instances (or unshelve) once this server is offloaded.
6473         self.rt.delete_allocation_for_shelve_offloaded_instance(context,
6474                                                                 instance)
6475 
6476         instance.power_state = current_power_state
6477         # NOTE(mriedem): The vm_state has to be set before updating the
6478         # resource tracker, see vm_states.ALLOW_RESOURCE_REMOVAL. The host/node
6479         # values cannot be nulled out until after updating the resource tracker
6480         # though.
6481         instance.vm_state = vm_states.SHELVED_OFFLOADED
6482         instance.task_state = None
6483         instance.save(expected_task_state=[task_states.SHELVING,
6484                                            task_states.SHELVING_OFFLOADING])
6485 
6486         # NOTE(ndipanov): Free resources from the resource tracker
6487         self._update_resource_tracker(context, instance)
6488 
6489         # NOTE(sfinucan): RPC calls should no longer be attempted against this
6490         # instance, so ensure any calls result in errors
6491         self._nil_out_instance_obj_host_and_node(instance)
6492         instance.save(expected_task_state=None)
6493 
6494         # TODO(melwitt): We should clean up instance console tokens here. The
6495         # instance has no host at this point and will need to establish a new
6496         # console connection in the future after it is unshelved.
6497         self._delete_scheduler_instance_info(context, instance.uuid)
6498         self._notify_about_instance_usage(context, instance,
6499                 'shelve_offload.end')
6500         compute_utils.notify_about_instance_action(context, instance,
6501                 self.host, action=fields.NotificationAction.SHELVE_OFFLOAD,
6502                 phase=fields.NotificationPhase.END, bdms=bdms)
6503 
6504     @wrap_exception()
6505     @reverts_task_state
6506     @wrap_instance_event(prefix='compute')
6507     @wrap_instance_fault
6508     def unshelve_instance(
6509             self, context, instance, image, filter_properties, node,
6510             request_spec=None, accel_uuids=None):
6511         """Unshelve the instance.
6512 
6513         :param context: request context
6514         :param instance: a nova.objects.instance.Instance object
6515         :param image: an image to build from.  If None we assume a
6516             volume backed instance.
6517         :param filter_properties: dict containing limits, retry info etc.
6518         :param node: target compute node
6519         :param request_spec: the RequestSpec object used to schedule the
6520             instance
6521         :param accel_uuids: the accelerators uuids for the instance
6522         """
6523         if filter_properties is None:
6524             filter_properties = {}
6525 
6526         @utils.synchronized(instance.uuid)
6527         def do_unshelve_instance():
6528             self._unshelve_instance(
6529                 context, instance, image, filter_properties, node,
6530                 request_spec, accel_uuids)
6531         do_unshelve_instance()
6532 
6533     def _unshelve_instance_key_scrub(self, instance):
6534         """Remove data from the instance that may cause side effects."""
6535         cleaned_keys = dict(
6536                 key_data=instance.key_data,
6537                 auto_disk_config=instance.auto_disk_config)
6538         instance.key_data = None
6539         instance.auto_disk_config = False
6540         return cleaned_keys
6541 
6542     def _unshelve_instance_key_restore(self, instance, keys):
6543         """Restore previously scrubbed keys before saving the instance."""
6544         instance.update(keys)
6545 
6546     def _unshelve_instance(self, context, instance, image, filter_properties,
6547                            node, request_spec, accel_uuids):
6548         LOG.info('Unshelving', instance=instance)
6549         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
6550                 context, instance.uuid)
6551 
6552         self._notify_about_instance_usage(context, instance, 'unshelve.start')
6553         compute_utils.notify_about_instance_action(context, instance,
6554                 self.host, action=fields.NotificationAction.UNSHELVE,
6555                 phase=fields.NotificationPhase.START, bdms=bdms)
6556 
6557         instance.task_state = task_states.SPAWNING
6558         instance.save()
6559 
6560         block_device_info = self._prep_block_device(context, instance, bdms)
6561         scrubbed_keys = self._unshelve_instance_key_scrub(instance)
6562 
6563         if node is None:
6564             node = self._get_nodename(instance)
6565 
6566         limits = filter_properties.get('limits', {})
6567 
6568         allocations = self.reportclient.get_allocations_for_consumer(
6569             context, instance.uuid)
6570 
6571         shelved_image_ref = instance.image_ref
6572         if image:
6573             instance.image_ref = image['id']
6574             image_meta = objects.ImageMeta.from_dict(image)
6575         else:
6576             image_meta = objects.ImageMeta.from_dict(
6577                 utils.get_image_from_system_metadata(
6578                     instance.system_metadata))
6579 
6580         provider_mappings = self._get_request_group_mapping(request_spec)
6581 
6582         try:
6583             if provider_mappings:
6584                 update = (
6585                     compute_utils.
6586                         update_pci_request_spec_with_allocated_interface_name)
6587                 update(
6588                     context, self.reportclient, instance.pci_requests.requests,
6589                     provider_mappings)
6590 
6591             self.network_api.setup_instance_network_on_host(
6592                 context, instance, self.host,
6593                 provider_mappings=provider_mappings)
6594             network_info = self.network_api.get_instance_nw_info(
6595                 context, instance)
6596 
6597             accel_info = []
6598             if accel_uuids:
6599                 try:
6600                     accel_info = self._get_bound_arq_resources(
6601                         context, instance, accel_uuids)
6602                 except (Exception, eventlet.timeout.Timeout) as exc:
6603                     LOG.exception('Failure getting accelerator requests '
6604                                   'with the exception: %s', exc,
6605                                   instance=instance)
6606                     self._build_resources_cleanup(instance, network_info)
6607                     raise
6608 
6609             with self.rt.instance_claim(context, instance, node, allocations,
6610                                         limits):
6611                 self.driver.spawn(context, instance, image_meta,
6612                                   injected_files=[],
6613                                   admin_password=None,
6614                                   allocations=allocations,
6615                                   network_info=network_info,
6616                                   block_device_info=block_device_info,
6617                                   accel_info=accel_info)
6618         except Exception:
6619             with excutils.save_and_reraise_exception(logger=LOG):
6620                 LOG.exception('Instance failed to spawn',
6621                               instance=instance)
6622                 # Cleanup allocations created by the scheduler on this host
6623                 # since we failed to spawn the instance. We do this both if
6624                 # the instance claim failed with ComputeResourcesUnavailable
6625                 # or if we did claim but the spawn failed, because aborting the
6626                 # instance claim will not remove the allocations.
6627                 self.reportclient.delete_allocation_for_instance(context,
6628                                                                  instance.uuid)
6629                 # FIXME: Umm, shouldn't we be rolling back port bindings too?
6630                 self._terminate_volume_connections(context, instance, bdms)
6631                 # The reverts_task_state decorator on unshelve_instance will
6632                 # eventually save these updates.
6633                 self._nil_out_instance_obj_host_and_node(instance)
6634 
6635         if image:
6636             instance.image_ref = shelved_image_ref
6637             self._delete_snapshot_of_shelved_instance(context, instance,
6638                                                       image['id'])
6639 
6640         self._unshelve_instance_key_restore(instance, scrubbed_keys)
6641         self._update_instance_after_spawn(instance)
6642         # Delete system_metadata for a shelved instance
6643         compute_utils.remove_shelved_keys_from_system_metadata(instance)
6644 
6645         instance.save(expected_task_state=task_states.SPAWNING)
6646         self._update_scheduler_instance_info(context, instance)
6647         self._notify_about_instance_usage(context, instance, 'unshelve.end')
6648         compute_utils.notify_about_instance_action(context, instance,
6649                 self.host, action=fields.NotificationAction.UNSHELVE,
6650                 phase=fields.NotificationPhase.END, bdms=bdms)
6651 
6652     # TODO(stephenfin): Remove this in RPC 6.0 since it's nova-network only
6653     @messaging.expected_exceptions(NotImplementedError)
6654     def reset_network(self, context, instance):
6655         """Reset networking on the given instance."""
6656         raise NotImplementedError()
6657 
6658     def _inject_network_info(self, instance, network_info):
6659         """Inject network info for the given instance."""
6660         LOG.debug('Inject network info', instance=instance)
6661         LOG.debug('network_info to inject: |%s|', network_info,
6662                   instance=instance)
6663 
6664         self.driver.inject_network_info(instance, network_info)
6665 
6666     @wrap_instance_fault
6667     def inject_network_info(self, context, instance):
6668         """Inject network info, but don't return the info."""
6669         network_info = self.network_api.get_instance_nw_info(context, instance)
6670         self._inject_network_info(instance, network_info)
6671 
6672     @messaging.expected_exceptions(NotImplementedError,
6673                                    exception.ConsoleNotAvailable,
6674                                    exception.InstanceNotFound)
6675     @wrap_exception()
6676     @wrap_instance_fault
6677     def get_console_output(self, context, instance, tail_length):
6678         """Send the console output for the given instance."""
6679         context = context.elevated()
6680         LOG.info("Get console output", instance=instance)
6681         output = self.driver.get_console_output(context, instance)
6682 
6683         if type(output) is str:
6684             output = output.encode("latin-1")
6685 
6686         if tail_length is not None:
6687             output = self._tail_log(output, tail_length)
6688 
6689         return output.decode('ascii', 'replace')
6690 
6691     def _tail_log(self, log, length):
6692         try:
6693             length = int(length)
6694         except ValueError:
6695             length = 0
6696 
6697         if length == 0:
6698             return b''
6699         else:
6700             return b'\n'.join(log.split(b'\n')[-int(length):])
6701 
6702     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
6703                                    exception.InstanceNotReady,
6704                                    exception.InstanceNotFound,
6705                                    exception.ConsoleTypeUnavailable,
6706                                    NotImplementedError)
6707     @wrap_exception()
6708     @wrap_instance_fault
6709     def get_vnc_console(self, context, console_type, instance):
6710         """Return connection information for a vnc console."""
6711         context = context.elevated()
6712         LOG.debug("Getting vnc console", instance=instance)
6713 
6714         if not CONF.vnc.enabled:
6715             raise exception.ConsoleTypeUnavailable(console_type=console_type)
6716 
6717         if console_type == 'novnc':
6718             # For essex, novncproxy_base_url must include the full path
6719             # including the html file (like http://myhost/vnc_auto.html)
6720             access_url_base = CONF.vnc.novncproxy_base_url
6721         else:
6722             raise exception.ConsoleTypeInvalid(console_type=console_type)
6723 
6724         try:
6725             # Retrieve connect info from driver, and then decorate with our
6726             # access info token
6727             console = self.driver.get_vnc_console(context, instance)
6728             console_auth = objects.ConsoleAuthToken(
6729                 context=context,
6730                 console_type=console_type,
6731                 host=console.host,
6732                 port=console.port,
6733                 internal_access_path=console.internal_access_path,
6734                 instance_uuid=instance.uuid,
6735                 access_url_base=access_url_base,
6736             )
6737             console_auth.authorize(CONF.consoleauth.token_ttl)
6738             connect_info = console.get_connection_info(
6739                 console_auth.token, console_auth.access_url)
6740 
6741         except exception.InstanceNotFound:
6742             if instance.vm_state != vm_states.BUILDING:
6743                 raise
6744             raise exception.InstanceNotReady(instance_id=instance.uuid)
6745 
6746         return connect_info
6747 
6748     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
6749                                    exception.InstanceNotReady,
6750                                    exception.InstanceNotFound,
6751                                    exception.ConsoleTypeUnavailable,
6752                                    NotImplementedError)
6753     @wrap_exception()
6754     @wrap_instance_fault
6755     def get_spice_console(self, context, console_type, instance):
6756         """Return connection information for a spice console."""
6757         context = context.elevated()
6758         LOG.debug("Getting spice console", instance=instance)
6759 
6760         if not CONF.spice.enabled:
6761             raise exception.ConsoleTypeUnavailable(console_type=console_type)
6762 
6763         if console_type != 'spice-html5':
6764             raise exception.ConsoleTypeInvalid(console_type=console_type)
6765 
6766         try:
6767             # Retrieve connect info from driver, and then decorate with our
6768             # access info token
6769             console = self.driver.get_spice_console(context, instance)
6770             console_auth = objects.ConsoleAuthToken(
6771                 context=context,
6772                 console_type=console_type,
6773                 host=console.host,
6774                 port=console.port,
6775                 internal_access_path=console.internal_access_path,
6776                 instance_uuid=instance.uuid,
6777                 access_url_base=CONF.spice.html5proxy_base_url,
6778             )
6779             console_auth.authorize(CONF.consoleauth.token_ttl)
6780             connect_info = console.get_connection_info(
6781                 console_auth.token, console_auth.access_url)
6782 
6783         except exception.InstanceNotFound:
6784             if instance.vm_state != vm_states.BUILDING:
6785                 raise
6786             raise exception.InstanceNotReady(instance_id=instance.uuid)
6787 
6788         return connect_info
6789 
6790     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
6791                                    exception.InstanceNotReady,
6792                                    exception.InstanceNotFound,
6793                                    exception.ConsoleTypeUnavailable,
6794                                    NotImplementedError)
6795     @wrap_exception()
6796     @wrap_instance_fault
6797     def get_rdp_console(self, context, console_type, instance):
6798         """Return connection information for a RDP console."""
6799         context = context.elevated()
6800         LOG.debug("Getting RDP console", instance=instance)
6801 
6802         if not CONF.rdp.enabled:
6803             raise exception.ConsoleTypeUnavailable(console_type=console_type)
6804 
6805         if console_type != 'rdp-html5':
6806             raise exception.ConsoleTypeInvalid(console_type=console_type)
6807 
6808         try:
6809             # Retrieve connect info from driver, and then decorate with our
6810             # access info token
6811             console = self.driver.get_rdp_console(context, instance)
6812             console_auth = objects.ConsoleAuthToken(
6813                 context=context,
6814                 console_type=console_type,
6815                 host=console.host,
6816                 port=console.port,
6817                 internal_access_path=console.internal_access_path,
6818                 instance_uuid=instance.uuid,
6819                 access_url_base=CONF.rdp.html5_proxy_base_url,
6820             )
6821             console_auth.authorize(CONF.consoleauth.token_ttl)
6822             connect_info = console.get_connection_info(
6823                 console_auth.token, console_auth.access_url)
6824 
6825         except exception.InstanceNotFound:
6826             if instance.vm_state != vm_states.BUILDING:
6827                 raise
6828             raise exception.InstanceNotReady(instance_id=instance.uuid)
6829 
6830         return connect_info
6831 
6832     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
6833                                    exception.InstanceNotReady,
6834                                    exception.InstanceNotFound,
6835                                    exception.ConsoleTypeUnavailable,
6836                                    NotImplementedError)
6837     @wrap_exception()
6838     @wrap_instance_fault
6839     def get_mks_console(self, context, console_type, instance):
6840         """Return connection information for a MKS console."""
6841         context = context.elevated()
6842         LOG.debug("Getting MKS console", instance=instance)
6843 
6844         if not CONF.mks.enabled:
6845             raise exception.ConsoleTypeUnavailable(console_type=console_type)
6846 
6847         if console_type != 'webmks':
6848             raise exception.ConsoleTypeInvalid(console_type=console_type)
6849 
6850         try:
6851             # Retrieve connect info from driver, and then decorate with our
6852             # access info token
6853             console = self.driver.get_mks_console(context, instance)
6854             console_auth = objects.ConsoleAuthToken(
6855                 context=context,
6856                 console_type=console_type,
6857                 host=console.host,
6858                 port=console.port,
6859                 internal_access_path=console.internal_access_path,
6860                 instance_uuid=instance.uuid,
6861                 access_url_base=CONF.mks.mksproxy_base_url,
6862             )
6863             console_auth.authorize(CONF.consoleauth.token_ttl)
6864             connect_info = console.get_connection_info(
6865                 console_auth.token, console_auth.access_url)
6866 
6867         except exception.InstanceNotFound:
6868             if instance.vm_state != vm_states.BUILDING:
6869                 raise
6870             raise exception.InstanceNotReady(instance_id=instance.uuid)
6871 
6872         return connect_info
6873 
6874     @messaging.expected_exceptions(
6875         exception.ConsoleTypeInvalid,
6876         exception.InstanceNotReady,
6877         exception.InstanceNotFound,
6878         exception.ConsoleTypeUnavailable,
6879         exception.SocketPortRangeExhaustedException,
6880         exception.ImageSerialPortNumberInvalid,
6881         exception.ImageSerialPortNumberExceedFlavorValue,
6882         NotImplementedError)
6883     @wrap_exception()
6884     @wrap_instance_fault
6885     def get_serial_console(self, context, console_type, instance):
6886         """Returns connection information for a serial console."""
6887 
6888         LOG.debug("Getting serial console", instance=instance)
6889 
6890         if not CONF.serial_console.enabled:
6891             raise exception.ConsoleTypeUnavailable(console_type=console_type)
6892 
6893         context = context.elevated()
6894 
6895         try:
6896             # Retrieve connect info from driver, and then decorate with our
6897             # access info token
6898             console = self.driver.get_serial_console(context, instance)
6899             console_auth = objects.ConsoleAuthToken(
6900                 context=context,
6901                 console_type=console_type,
6902                 host=console.host,
6903                 port=console.port,
6904                 internal_access_path=console.internal_access_path,
6905                 instance_uuid=instance.uuid,
6906                 access_url_base=CONF.serial_console.base_url,
6907             )
6908             console_auth.authorize(CONF.consoleauth.token_ttl)
6909             connect_info = console.get_connection_info(
6910                 console_auth.token, console_auth.access_url)
6911 
6912         except exception.InstanceNotFound:
6913             if instance.vm_state != vm_states.BUILDING:
6914                 raise
6915             raise exception.InstanceNotReady(instance_id=instance.uuid)
6916 
6917         return connect_info
6918 
6919     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
6920                                    exception.InstanceNotReady,
6921                                    exception.InstanceNotFound)
6922     @wrap_exception()
6923     @wrap_instance_fault
6924     def validate_console_port(self, ctxt, instance, port, console_type):
6925         if console_type == "spice-html5":
6926             console_info = self.driver.get_spice_console(ctxt, instance)
6927         elif console_type == "rdp-html5":
6928             console_info = self.driver.get_rdp_console(ctxt, instance)
6929         elif console_type == "serial":
6930             console_info = self.driver.get_serial_console(ctxt, instance)
6931         elif console_type == "webmks":
6932             console_info = self.driver.get_mks_console(ctxt, instance)
6933         else:
6934             console_info = self.driver.get_vnc_console(ctxt, instance)
6935 
6936         # Some drivers may return an int on console_info.port but the port
6937         # variable in this method is a string, so cast to be sure we are
6938         # comparing the correct types.
6939         return str(console_info.port) == port
6940 
6941     @wrap_exception()
6942     @reverts_task_state
6943     @wrap_instance_fault
6944     def reserve_block_device_name(self, context, instance, device,
6945                                   volume_id, disk_bus, device_type, tag,
6946                                   multiattach):
6947         if (tag and not
6948                 self.driver.capabilities.get('supports_tagged_attach_volume',
6949                                              False)):
6950             raise exception.VolumeTaggedAttachNotSupported()
6951 
6952         if (multiattach and not
6953                 self.driver.capabilities.get('supports_multiattach', False)):
6954             raise exception.MultiattachNotSupportedByVirtDriver(
6955                 volume_id=volume_id)
6956 
6957         @utils.synchronized(instance.uuid)
6958         def do_reserve():
6959             bdms = (
6960                 objects.BlockDeviceMappingList.get_by_instance_uuid(
6961                     context, instance.uuid))
6962 
6963             # NOTE(ndipanov): We need to explicitly set all the fields on the
6964             #                 object so that obj_load_attr does not fail
6965             new_bdm = objects.BlockDeviceMapping(
6966                     context=context,
6967                     source_type='volume', destination_type='volume',
6968                     instance_uuid=instance.uuid, boot_index=None,
6969                     volume_id=volume_id,
6970                     device_name=device, guest_format=None,
6971                     disk_bus=disk_bus, device_type=device_type, tag=tag)
6972 
6973             new_bdm.device_name = self._get_device_name_for_instance(
6974                     instance, bdms, new_bdm)
6975 
6976             # NOTE(vish): create bdm here to avoid race condition
6977             new_bdm.create()
6978             return new_bdm
6979 
6980         return do_reserve()
6981 
6982     @wrap_exception()
6983     @wrap_instance_event(prefix='compute')
6984     @wrap_instance_fault
6985     def attach_volume(self, context, instance, bdm):
6986         """Attach a volume to an instance."""
6987         driver_bdm = driver_block_device.convert_volume(bdm)
6988 
6989         @utils.synchronized(instance.uuid)
6990         def do_attach_volume(context, instance, driver_bdm):
6991             try:
6992                 return self._attach_volume(context, instance, driver_bdm)
6993             except Exception:
6994                 with excutils.save_and_reraise_exception():
6995                     bdm.destroy()
6996 
6997         do_attach_volume(context, instance, driver_bdm)
6998 
6999     def _attach_volume(self, context, instance, bdm):
7000         context = context.elevated()
7001         LOG.info('Attaching volume %(volume_id)s to %(mountpoint)s',
7002                  {'volume_id': bdm.volume_id,
7003                   'mountpoint': bdm['mount_device']},
7004                  instance=instance)
7005         compute_utils.notify_about_volume_attach_detach(
7006             context, instance, self.host,
7007             action=fields.NotificationAction.VOLUME_ATTACH,
7008             phase=fields.NotificationPhase.START,
7009             volume_id=bdm.volume_id)
7010         try:
7011             bdm.attach(context, instance, self.volume_api, self.driver,
7012                        do_driver_attach=True)
7013         except Exception as e:
7014             with excutils.save_and_reraise_exception():
7015                 LOG.exception("Failed to attach %(volume_id)s "
7016                               "at %(mountpoint)s",
7017                               {'volume_id': bdm.volume_id,
7018                                'mountpoint': bdm['mount_device']},
7019                               instance=instance)
7020                 if bdm['attachment_id']:
7021                     # Try to delete the attachment to make the volume
7022                     # available again. Note that DriverVolumeBlockDevice
7023                     # may have already deleted the attachment so ignore
7024                     # VolumeAttachmentNotFound.
7025                     try:
7026                         self.volume_api.attachment_delete(
7027                             context, bdm['attachment_id'])
7028                     except exception.VolumeAttachmentNotFound as exc:
7029                         LOG.debug('Ignoring VolumeAttachmentNotFound: %s',
7030                                   exc, instance=instance)
7031                 else:
7032                     self.volume_api.unreserve_volume(context, bdm.volume_id)
7033                 compute_utils.notify_about_volume_attach_detach(
7034                     context, instance, self.host,
7035                     action=fields.NotificationAction.VOLUME_ATTACH,
7036                     phase=fields.NotificationPhase.ERROR,
7037                     exception=e,
7038                     volume_id=bdm.volume_id)
7039 
7040         info = {'volume_id': bdm.volume_id}
7041         self._notify_about_instance_usage(
7042             context, instance, "volume.attach", extra_usage_info=info)
7043         compute_utils.notify_about_volume_attach_detach(
7044             context, instance, self.host,
7045             action=fields.NotificationAction.VOLUME_ATTACH,
7046             phase=fields.NotificationPhase.END,
7047             volume_id=bdm.volume_id)
7048 
7049     def _notify_volume_usage_detach(self, context, instance, bdm):
7050         if CONF.volume_usage_poll_interval <= 0:
7051             return
7052 
7053         mp = bdm.device_name
7054         # Handle bootable volumes which will not contain /dev/
7055         if '/dev/' in mp:
7056             mp = mp[5:]
7057         try:
7058             vol_stats = self.driver.block_stats(instance, mp)
7059             if vol_stats is None:
7060                 return
7061         except NotImplementedError:
7062             return
7063 
7064         LOG.debug("Updating volume usage cache with totals", instance=instance)
7065         rd_req, rd_bytes, wr_req, wr_bytes, flush_ops = vol_stats
7066         vol_usage = objects.VolumeUsage(context)
7067         vol_usage.volume_id = bdm.volume_id
7068         vol_usage.instance_uuid = instance.uuid
7069         vol_usage.project_id = instance.project_id
7070         vol_usage.user_id = instance.user_id
7071         vol_usage.availability_zone = instance.availability_zone
7072         vol_usage.curr_reads = rd_req
7073         vol_usage.curr_read_bytes = rd_bytes
7074         vol_usage.curr_writes = wr_req
7075         vol_usage.curr_write_bytes = wr_bytes
7076         vol_usage.save(update_totals=True)
7077         self.notifier.info(context, 'volume.usage', vol_usage.to_dict())
7078         compute_utils.notify_about_volume_usage(context, vol_usage, self.host)
7079 
7080     def _detach_volume(self, context, bdm, instance, destroy_bdm=True,
7081                        attachment_id=None):
7082         """Detach a volume from an instance.
7083 
7084         :param context: security context
7085         :param bdm: nova.objects.BlockDeviceMapping volume bdm to detach
7086         :param instance: the Instance object to detach the volume from
7087         :param destroy_bdm: if True, the corresponding BDM entry will be marked
7088                             as deleted. Disabling this is useful for operations
7089                             like rebuild, when we don't want to destroy BDM
7090         :param attachment_id: The volume attachment_id for the given instance
7091                               and volume.
7092         """
7093         volume_id = bdm.volume_id
7094         compute_utils.notify_about_volume_attach_detach(
7095             context, instance, self.host,
7096             action=fields.NotificationAction.VOLUME_DETACH,
7097             phase=fields.NotificationPhase.START,
7098             volume_id=volume_id)
7099 
7100         self._notify_volume_usage_detach(context, instance, bdm)
7101 
7102         LOG.info('Detaching volume %(volume_id)s',
7103                  {'volume_id': volume_id}, instance=instance)
7104 
7105         driver_bdm = driver_block_device.convert_volume(bdm)
7106         driver_bdm.detach(context, instance, self.volume_api, self.driver,
7107                           attachment_id=attachment_id, destroy_bdm=destroy_bdm)
7108 
7109         info = dict(volume_id=volume_id)
7110         self._notify_about_instance_usage(
7111             context, instance, "volume.detach", extra_usage_info=info)
7112         compute_utils.notify_about_volume_attach_detach(
7113             context, instance, self.host,
7114             action=fields.NotificationAction.VOLUME_DETACH,
7115             phase=fields.NotificationPhase.END,
7116             volume_id=volume_id)
7117 
7118         if 'tag' in bdm and bdm.tag:
7119             self._delete_disk_metadata(instance, bdm)
7120         if destroy_bdm:
7121             bdm.destroy()
7122 
7123     def _delete_disk_metadata(self, instance, bdm):
7124         for device in instance.device_metadata.devices:
7125             if isinstance(device, objects.DiskMetadata):
7126                 if 'serial' in device:
7127                     if device.serial == bdm.volume_id:
7128                         instance.device_metadata.devices.remove(device)
7129                         instance.save()
7130                         break
7131                 else:
7132                     # NOTE(artom) We log the entire device object because all
7133                     # fields are nullable and may not be set
7134                     LOG.warning('Unable to determine whether to clean up '
7135                                 'device metadata for disk %s', device,
7136                                 instance=instance)
7137 
7138     @wrap_exception()
7139     @wrap_instance_event(prefix='compute')
7140     @wrap_instance_fault
7141     def detach_volume(self, context, volume_id, instance, attachment_id):
7142         """Detach a volume from an instance.
7143 
7144         :param context: security context
7145         :param volume_id: the volume id
7146         :param instance: the Instance object to detach the volume from
7147         :param attachment_id: The volume attachment_id for the given instance
7148                               and volume.
7149 
7150         """
7151         @utils.synchronized(instance.uuid)
7152         def do_detach_volume(context, volume_id, instance, attachment_id):
7153             bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
7154                     context, volume_id, instance.uuid)
7155             self._detach_volume(context, bdm, instance,
7156                                 attachment_id=attachment_id)
7157 
7158         do_detach_volume(context, volume_id, instance, attachment_id)
7159 
7160     def _init_volume_connection(self, context, new_volume,
7161                                 old_volume_id, connector, bdm,
7162                                 new_attachment_id, mountpoint):
7163         new_volume_id = new_volume['id']
7164         if new_attachment_id is None:
7165             # We're dealing with an old-style attachment so initialize the
7166             # connection so we can get the connection_info.
7167             new_cinfo = self.volume_api.initialize_connection(context,
7168                                                               new_volume_id,
7169                                                               connector)
7170         else:
7171             # Check for multiattach on the new volume and if True, check to
7172             # see if the virt driver supports multiattach.
7173             # TODO(mriedem): This is copied from DriverVolumeBlockDevice
7174             # and should be consolidated into some common code at some point.
7175             vol_multiattach = new_volume.get('multiattach', False)
7176             virt_multiattach = self.driver.capabilities.get(
7177                 'supports_multiattach', False)
7178             if vol_multiattach and not virt_multiattach:
7179                 raise exception.MultiattachNotSupportedByVirtDriver(
7180                     volume_id=new_volume_id)
7181 
7182             # This is a new style attachment and the API created the new
7183             # volume attachment and passed the id to the compute over RPC.
7184             # At this point we need to update the new volume attachment with
7185             # the host connector, which will give us back the new attachment
7186             # connection_info.
7187             new_cinfo = self.volume_api.attachment_update(
7188                 context, new_attachment_id, connector,
7189                 mountpoint)['connection_info']
7190 
7191             if vol_multiattach:
7192                 # This will be used by the volume driver to determine the
7193                 # proper disk configuration.
7194                 new_cinfo['multiattach'] = True
7195 
7196         old_cinfo = jsonutils.loads(bdm['connection_info'])
7197         if old_cinfo and 'serial' not in old_cinfo:
7198             old_cinfo['serial'] = old_volume_id
7199         # NOTE(lyarwood): serial is not always present in the returned
7200         # connection_info so set it if it is missing as we do in
7201         # DriverVolumeBlockDevice.attach().
7202         if 'serial' not in new_cinfo:
7203             new_cinfo['serial'] = new_volume_id
7204         return (old_cinfo, new_cinfo)
7205 
7206     def _swap_volume(self, context, instance, bdm, connector,
7207                      old_volume_id, new_volume, resize_to,
7208                      new_attachment_id, is_cinder_migration):
7209         new_volume_id = new_volume['id']
7210         mountpoint = bdm['device_name']
7211         failed = False
7212         new_cinfo = None
7213         try:
7214             old_cinfo, new_cinfo = self._init_volume_connection(
7215                 context, new_volume, old_volume_id, connector,
7216                 bdm, new_attachment_id, mountpoint)
7217             # NOTE(lyarwood): The Libvirt driver, the only virt driver
7218             # currently implementing swap_volume, will modify the contents of
7219             # new_cinfo when connect_volume is called. This is then saved to
7220             # the BDM in swap_volume for future use outside of this flow.
7221             msg = ("swap_volume: Calling driver volume swap with "
7222                    "connection infos: new: %(new_cinfo)s; "
7223                    "old: %(old_cinfo)s" %
7224                    {'new_cinfo': new_cinfo, 'old_cinfo': old_cinfo})
7225             # Both new and old info might contain password
7226             LOG.debug(strutils.mask_password(msg), instance=instance)
7227 
7228             self.driver.swap_volume(context, old_cinfo, new_cinfo, instance,
7229                                     mountpoint, resize_to)
7230             if new_attachment_id:
7231                 self.volume_api.attachment_complete(context, new_attachment_id)
7232             msg = ("swap_volume: Driver volume swap returned, new "
7233                    "connection_info is now : %(new_cinfo)s" %
7234                    {'new_cinfo': new_cinfo})
7235             LOG.debug(strutils.mask_password(msg))
7236         except Exception as ex:
7237             failed = True
7238             with excutils.save_and_reraise_exception():
7239                 compute_utils.notify_about_volume_swap(
7240                     context, instance, self.host,
7241                     fields.NotificationPhase.ERROR,
7242                     old_volume_id, new_volume_id, ex)
7243                 if new_cinfo:
7244                     msg = ("Failed to swap volume %(old_volume_id)s "
7245                            "for %(new_volume_id)s")
7246                     LOG.exception(msg, {'old_volume_id': old_volume_id,
7247                                         'new_volume_id': new_volume_id},
7248                                   instance=instance)
7249                 else:
7250                     msg = ("Failed to connect to volume %(volume_id)s "
7251                            "with volume at %(mountpoint)s")
7252                     LOG.exception(msg, {'volume_id': new_volume_id,
7253                                         'mountpoint': bdm['device_name']},
7254                                   instance=instance)
7255 
7256                 # The API marked the volume as 'detaching' for the old volume
7257                 # so we need to roll that back so the volume goes back to
7258                 # 'in-use' state.
7259                 self.volume_api.roll_detaching(context, old_volume_id)
7260 
7261                 if new_attachment_id is None:
7262                     # The API reserved the new volume so it would be in
7263                     # 'attaching' status, so we need to unreserve it so it
7264                     # goes back to 'available' status.
7265                     self.volume_api.unreserve_volume(context, new_volume_id)
7266                 else:
7267                     # This is a new style attachment for the new volume, which
7268                     # was created in the API. We just need to delete it here
7269                     # to put the new volume back into 'available' status.
7270                     self.volume_api.attachment_delete(
7271                         context, new_attachment_id)
7272         finally:
7273             # TODO(mriedem): This finally block is terribly confusing and is
7274             # trying to do too much. We should consider removing the finally
7275             # block and move whatever needs to happen on success and failure
7276             # into the blocks above for clarity, even if it means a bit of
7277             # redundant code.
7278             conn_volume = new_volume_id if failed else old_volume_id
7279             if new_cinfo:
7280                 LOG.debug("swap_volume: removing Cinder connection "
7281                           "for volume %(volume)s", {'volume': conn_volume},
7282                           instance=instance)
7283                 if bdm.attachment_id is None:
7284                     # This is the pre-3.44 flow for new-style volume
7285                     # attachments so just terminate the connection.
7286                     self.volume_api.terminate_connection(context,
7287                                                          conn_volume,
7288                                                          connector)
7289                 else:
7290                     # This is a new style volume attachment. If we failed, then
7291                     # the new attachment was already deleted above in the
7292                     # exception block and we have nothing more to do here. If
7293                     # swap_volume was successful in the driver, then we need to
7294                     # "detach" the original attachment by deleting it.
7295                     if not failed:
7296                         self.volume_api.attachment_delete(
7297                             context, bdm.attachment_id)
7298 
7299             # Need to make some decisions based on whether this was
7300             # a Cinder initiated migration or not. The callback to
7301             # migration completion isn't needed in the case of a
7302             # nova initiated simple swap of two volume
7303             # "volume-update" call so skip that. The new attachment
7304             # scenarios will give us a new attachment record and
7305             # that's what we want.
7306             if bdm.attachment_id and not is_cinder_migration:
7307                 # we don't callback to cinder
7308                 comp_ret = {'save_volume_id': new_volume_id}
7309             else:
7310                 # NOTE(lyarwood): The following call to
7311                 # os-migrate-volume-completion returns a dict containing
7312                 # save_volume_id, this volume id has two possible values :
7313                 # 1. old_volume_id if we are migrating (retyping) volumes
7314                 # 2. new_volume_id if we are swapping between two existing
7315                 #    volumes
7316                 # This volume id is later used to update the volume_id and
7317                 # connection_info['serial'] of the BDM.
7318                 comp_ret = self.volume_api.migrate_volume_completion(
7319                                                           context,
7320                                                           old_volume_id,
7321                                                           new_volume_id,
7322                                                           error=failed)
7323                 LOG.debug("swap_volume: Cinder migrate_volume_completion "
7324                           "returned: %(comp_ret)s", {'comp_ret': comp_ret},
7325                           instance=instance)
7326 
7327         return (comp_ret, new_cinfo)
7328 
7329     @wrap_exception()
7330     @wrap_instance_event(prefix='compute')
7331     @wrap_instance_fault
7332     def swap_volume(self, context, old_volume_id, new_volume_id, instance,
7333                     new_attachment_id):
7334         """Replace the old volume with the new volume within the active server
7335 
7336         :param context: User request context
7337         :param old_volume_id: Original volume id
7338         :param new_volume_id: New volume id being swapped to
7339         :param instance: Instance with original_volume_id attached
7340         :param new_attachment_id: ID of the new attachment for new_volume_id
7341         """
7342         @utils.synchronized(instance.uuid)
7343         def _do_locked_swap_volume(context, old_volume_id, new_volume_id,
7344                                    instance, new_attachment_id):
7345             self._do_swap_volume(context, old_volume_id, new_volume_id,
7346                                  instance, new_attachment_id)
7347         _do_locked_swap_volume(context, old_volume_id, new_volume_id, instance,
7348                                new_attachment_id)
7349 
7350     def _do_swap_volume(self, context, old_volume_id, new_volume_id,
7351                         instance, new_attachment_id):
7352         """Replace the old volume with the new volume within the active server
7353 
7354         :param context: User request context
7355         :param old_volume_id: Original volume id
7356         :param new_volume_id: New volume id being swapped to
7357         :param instance: Instance with original_volume_id attached
7358         :param new_attachment_id: ID of the new attachment for new_volume_id
7359         """
7360         context = context.elevated()
7361         compute_utils.notify_about_volume_swap(
7362             context, instance, self.host,
7363             fields.NotificationPhase.START,
7364             old_volume_id, new_volume_id)
7365 
7366         bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
7367                 context, old_volume_id, instance.uuid)
7368         connector = self.driver.get_volume_connector(instance)
7369 
7370         resize_to = 0
7371         old_volume = self.volume_api.get(context, old_volume_id)
7372         # Yes this is a tightly-coupled state check of what's going on inside
7373         # cinder, but we need this while we still support old (v1/v2) and
7374         # new style attachments (v3.44). Once we drop support for old style
7375         # attachments we could think about cleaning up the cinder-initiated
7376         # swap volume API flows.
7377         is_cinder_migration = False
7378         if 'migration_status' in old_volume:
7379             is_cinder_migration = old_volume['migration_status'] == 'migrating'
7380         old_vol_size = old_volume['size']
7381         new_volume = self.volume_api.get(context, new_volume_id)
7382         new_vol_size = new_volume['size']
7383         if new_vol_size > old_vol_size:
7384             resize_to = new_vol_size
7385 
7386         LOG.info('Swapping volume %(old_volume)s for %(new_volume)s',
7387                  {'old_volume': old_volume_id, 'new_volume': new_volume_id},
7388                  instance=instance)
7389         comp_ret, new_cinfo = self._swap_volume(context,
7390                                                 instance,
7391                                                 bdm,
7392                                                 connector,
7393                                                 old_volume_id,
7394                                                 new_volume,
7395                                                 resize_to,
7396                                                 new_attachment_id,
7397                                                 is_cinder_migration)
7398 
7399         # NOTE(lyarwood): Update the BDM with the modified new_cinfo and
7400         # correct volume_id returned by Cinder.
7401         save_volume_id = comp_ret['save_volume_id']
7402         new_cinfo['serial'] = save_volume_id
7403         values = {
7404             'connection_info': jsonutils.dumps(new_cinfo),
7405             'source_type': 'volume',
7406             'destination_type': 'volume',
7407             'snapshot_id': None,
7408             'volume_id': save_volume_id,
7409             'no_device': None}
7410 
7411         if resize_to:
7412             values['volume_size'] = resize_to
7413 
7414         if new_attachment_id is not None:
7415             # This was a volume swap for a new-style attachment so we
7416             # need to update the BDM attachment_id for the new attachment.
7417             values['attachment_id'] = new_attachment_id
7418 
7419         LOG.debug("swap_volume: Updating volume %(volume_id)s BDM record with "
7420                   "%(updates)s", {'volume_id': bdm.volume_id,
7421                                   'updates': values},
7422                   instance=instance)
7423         bdm.update(values)
7424         bdm.save()
7425 
7426         compute_utils.notify_about_volume_swap(
7427             context, instance, self.host,
7428             fields.NotificationPhase.END,
7429             old_volume_id, new_volume_id)
7430 
7431     @wrap_exception()
7432     def remove_volume_connection(self, context, volume_id, instance):
7433         """Remove the volume connection on this host
7434 
7435         Detach the volume from this instance on this host, and if this is
7436         the cinder v2 flow, call cinder to terminate the connection.
7437         """
7438         try:
7439             # NOTE(mriedem): If the BDM was just passed directly we would not
7440             # need to do this DB query, but this is an RPC interface so
7441             # changing that requires some care.
7442             bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
7443                     context, volume_id, instance.uuid)
7444             # NOTE(mriedem): Normally we would pass delete_attachment=True to
7445             # _remove_volume_connection to delete a v3 style volume attachment,
7446             # but this method is RPC called from _rollback_live_migration which
7447             # already deletes the attachment, so because of that tight coupling
7448             # we cannot simply delete a v3 style attachment here without
7449             # needing to do some behavior modification of that
7450             # _rollback_live_migration flow which gets messy.
7451             self._remove_volume_connection(context, bdm, instance)
7452         except exception.NotFound:
7453             pass
7454 
7455     def _remove_volume_connection(self, context, bdm, instance,
7456                                   delete_attachment=False):
7457         """Remove the volume connection on this host
7458 
7459         Detach the volume from this instance on this host.
7460 
7461         :param context: nova auth request context
7462         :param bdm: BlockDeviceMapping object for a volume attached to the
7463             instance
7464         :param instance: Instance object with a volume attached represented
7465             by ``bdm``
7466         :param delete_attachment: If ``bdm.attachment_id`` is not None the
7467             attachment was made as a cinder v3 style attachment and if True,
7468             then deletes the volume attachment, otherwise just terminates
7469             the connection for a cinder legacy style connection.
7470         """
7471         driver_bdm = driver_block_device.convert_volume(bdm)
7472         driver_bdm.driver_detach(context, instance,
7473                                  self.volume_api, self.driver)
7474         if bdm.attachment_id is None:
7475             # cinder v2 api flow
7476             connector = self.driver.get_volume_connector(instance)
7477             self.volume_api.terminate_connection(context, bdm.volume_id,
7478                                                  connector)
7479         elif delete_attachment:
7480             # cinder v3 api flow
7481             self.volume_api.attachment_delete(context, bdm.attachment_id)
7482 
7483     def _deallocate_port_resource_for_instance(
7484         self,
7485         context: nova.context.RequestContext,
7486         instance: 'objects.Instance',
7487         port_id: str,
7488         port_allocation: ty.Dict[str, ty.Dict[str, ty.Dict[str, int]]],
7489     ) -> None:
7490 
7491         if not port_allocation:
7492             return
7493 
7494         try:
7495             client = self.reportclient
7496             client.remove_resources_from_instance_allocation(
7497                 context, instance.uuid, port_allocation)
7498         except Exception as ex:
7499             # We always raise here as it is not a race condition where
7500             # somebody has already deleted the port we want to cleanup.
7501             # Here we see that the port exists, the allocation exists,
7502             # but we cannot clean it up so we will actually leak
7503             # allocations.
7504             with excutils.save_and_reraise_exception():
7505                 LOG.warning(
7506                     'Failed to remove resource allocation of port %(port_id)s '
7507                     'for instance. Error: %(error)s',
7508                     {'port_id': port_id, 'error': ex},
7509                     instance=instance)
7510 
7511     def _deallocate_port_for_instance(
7512             self, context, instance, port_id, raise_on_failure=False,
7513             pci_device=None):
7514         try:
7515             result = self.network_api.deallocate_port_for_instance(
7516                 context, instance, port_id)
7517             __, port_allocation = result
7518         except Exception as ex:
7519             with excutils.save_and_reraise_exception(
7520                     reraise=raise_on_failure):
7521                 LOG.warning('Failed to deallocate port %(port_id)s '
7522                             'for instance. Error: %(error)s',
7523                             {'port_id': port_id, 'error': ex},
7524                             instance=instance)
7525         else:
7526             if pci_device:
7527                 self.rt.unclaim_pci_devices(context, pci_device, instance)
7528                 instance.remove_pci_device_and_request(pci_device)
7529 
7530             # Deallocate the resources in placement that were used by the
7531             # detached port.
7532             self._deallocate_port_resource_for_instance(
7533                 context, instance, port_id, port_allocation)
7534 
7535     def _claim_pci_device_for_interface_attach(
7536         self,
7537         context: nova.context.RequestContext,
7538         instance: 'objects.Instance',
7539         pci_reqs: 'objects.InstancePCIRequests',
7540     ) -> ty.Optional['objects.PciDevice']:
7541         """Claim PCI devices if there are PCI requests
7542 
7543         :param context: nova.context.RequestContext
7544         :param instance: the objects.Instance to where the interface is being
7545             attached
7546         :param pci_reqs: A InstancePCIRequests object describing the
7547             needed PCI devices
7548         :raises InterfaceAttachPciClaimFailed: if the PCI device claim fails
7549         :returns: An objects.PciDevice describing the claimed PCI device for
7550             the interface or None if no device is requested
7551         """
7552 
7553         if not pci_reqs.requests:
7554             return None
7555 
7556         devices = self.rt.claim_pci_devices(
7557             context, pci_reqs, instance.numa_topology)
7558 
7559         if not devices:
7560             LOG.info('Failed to claim PCI devices during interface attach '
7561                      'for PCI request %s', pci_reqs, instance=instance)
7562             raise exception.InterfaceAttachPciClaimFailed(
7563                 instance_uuid=instance.uuid)
7564 
7565         # NOTE(gibi): We assume that maximum one PCI devices is attached per
7566         # interface attach request.
7567         device = devices[0]
7568         instance.pci_devices.objects.append(device)
7569 
7570         return device
7571 
7572     def _allocate_port_resource_for_instance(
7573         self,
7574         context: nova.context.RequestContext,
7575         instance: 'objects.Instance',
7576         pci_reqs: 'objects.InstancePCIRequests',
7577         request_groups: ty.List['objects.RequestGroup'],
7578     ) -> ty.Tuple[ty.Optional[ty.Dict[str, ty.List[str]]],
7579                   ty.Optional[ty.Dict[str, ty.Dict[str, ty.Dict[str, int]]]]]:
7580         """Allocate resources for the request in placement
7581 
7582         :param context: nova.context.RequestContext
7583         :param instance: the objects.Instance to where the interface is being
7584             attached
7585         :param pci_reqs: A list of InstancePCIRequest objects describing the
7586             needed PCI devices
7587         :raises InterfaceAttachResourceAllocationFailed: if we failed to
7588             allocate resource in placement for the request
7589         :returns: A tuple of provider mappings and allocated resources or
7590             (None, None) if no resource allocation was needed for the request
7591         """
7592 
7593         if not request_groups:
7594             return None, None
7595 
7596         request_group = request_groups[0]
7597 
7598         # restrict the resource request to the current compute node. The
7599         # compute node uuid is the uuid of the root provider of the node in
7600         # placement
7601         compute_node_uuid = objects.ComputeNode.get_by_nodename(
7602             context, instance.node).uuid
7603         request_group.in_tree = compute_node_uuid
7604 
7605         # NOTE(gibi): when support is added for attaching a cyborg based
7606         # smart NIC the ResourceRequest could be extended to handle multiple
7607         # request groups.
7608         rr = scheduler_utils.ResourceRequest.from_request_group(request_group)
7609         res = self.reportclient.get_allocation_candidates(context, rr)
7610         alloc_reqs, provider_sums, version = res
7611 
7612         if not alloc_reqs:
7613             # no allocation candidates available, we run out of free resources
7614             raise exception.InterfaceAttachResourceAllocationFailed(
7615                 instance_uuid=instance.uuid)
7616 
7617         # select one of the candidates and update the instance
7618         # allocation
7619         # TODO(gibi): We could loop over all possible candidates
7620         # if the first one selected here does not work due to race or due
7621         # to not having free PCI devices. However the latter is only
7622         # detected later in the interface attach code path.
7623         alloc_req = alloc_reqs[0]
7624         resources = alloc_req['allocations']
7625         provider_mappings = alloc_req['mappings']
7626         try:
7627             self.reportclient.add_resources_to_instance_allocation(
7628                 context, instance.uuid, resources)
7629         except exception.AllocationUpdateFailed as e:
7630             # We lost a race. We could retry another candidate
7631             raise exception.InterfaceAttachResourceAllocationFailed(
7632                 instance_uuid=instance.uuid) from e
7633         except (
7634             exception.ConsumerAllocationRetrievalFailed,
7635             keystone_exception.ClientException,
7636         ) as e:
7637             # These are non-recoverable errors so we should not retry
7638             raise exception.InterfaceAttachResourceAllocationFailed(
7639                 instance_uuid=instance.uuid) from e
7640 
7641         try:
7642             update = (
7643                 compute_utils.
7644                     update_pci_request_spec_with_allocated_interface_name)
7645             update(
7646                 context, self.reportclient, pci_reqs.requests,
7647                 provider_mappings)
7648         except (
7649             exception.AmbiguousResourceProviderForPCIRequest,
7650             exception.UnexpectedResourceProviderNameForPCIRequest
7651         ):
7652             # These are programing errors. So we clean up an re-raise to let
7653             # the request fail
7654             with excutils.save_and_reraise_exception():
7655                 self.reportclient.remove_resources_from_instance_allocation(
7656                     context, instance.uuid, resources)
7657 
7658         return provider_mappings, resources
7659 
7660     # TODO(mriedem): There are likely race failures which can result in
7661     # NotFound and QuotaError exceptions getting traced as well.
7662     @messaging.expected_exceptions(
7663         # Do not log a traceback for user errors. We use Invalid generically
7664         # since this method can raise lots of different exceptions:
7665         # AttachInterfaceNotSupported
7666         # NetworkInterfaceTaggedAttachNotSupported
7667         # NetworkAmbiguous
7668         # PortNotUsable
7669         # PortInUse
7670         # PortNotUsableDNS
7671         # AttachSRIOVPortNotSupported
7672         # NetworksWithQoSPolicyNotSupported
7673         # InterfaceAttachResourceAllocationFailed
7674         exception.Invalid)
7675     @wrap_exception()
7676     @wrap_instance_event(prefix='compute')
7677     @wrap_instance_fault
7678     def attach_interface(self, context, instance, network_id, port_id,
7679                          requested_ip, tag):
7680         """Use hotplug to add an network adapter to an instance."""
7681         lockname = 'interface-%s-%s' % (instance.uuid, port_id)
7682 
7683         @utils.synchronized(lockname)
7684         def do_attach_interface(context, instance, network_id, port_id,
7685                                 requested_ip, tag):
7686             return self._attach_interface(context, instance, network_id,
7687                                 port_id, requested_ip, tag)
7688 
7689         return do_attach_interface(context, instance, network_id, port_id,
7690                                    requested_ip, tag)
7691 
7692     def _attach_interface(self, context, instance, network_id, port_id,
7693                           requested_ip, tag):
7694         if not self.driver.capabilities.get('supports_attach_interface',
7695                                             False):
7696             raise exception.AttachInterfaceNotSupported(
7697                 instance_uuid=instance.uuid)
7698         if (tag and not
7699             self.driver.capabilities.get('supports_tagged_attach_interface',
7700                                          False)):
7701             raise exception.NetworkInterfaceTaggedAttachNotSupported()
7702 
7703         compute_utils.notify_about_instance_action(
7704             context, instance, self.host,
7705             action=fields.NotificationAction.INTERFACE_ATTACH,
7706             phase=fields.NotificationPhase.START)
7707 
7708         bind_host_id = self.driver.network_binding_host_id(context, instance)
7709 
7710         requested_networks = objects.NetworkRequestList(
7711             objects=[
7712                 objects.NetworkRequest(
7713                     network_id=network_id,
7714                     port_id=port_id,
7715                     address=requested_ip,
7716                     tag=tag,
7717                 )
7718             ]
7719         )
7720 
7721         if len(requested_networks) != 1:
7722             LOG.warning(
7723                 "Interface attach only supports one interface per attach "
7724                 "request", instance=instance)
7725             raise exception.InterfaceAttachFailed(instance_uuid=instance.uuid)
7726 
7727         pci_numa_affinity_policy = hardware.get_pci_numa_policy_constraint(
7728             instance.flavor, instance.image_meta)
7729         pci_reqs = objects.InstancePCIRequests(
7730             requests=[], instance_uuid=instance.uuid)
7731         _, request_groups = self.network_api.create_resource_requests(
7732             context, requested_networks, pci_reqs,
7733             affinity_policy=pci_numa_affinity_policy)
7734 
7735         # We only support one port per attach request so we at most have one
7736         # pci request
7737         if pci_reqs.requests:
7738             pci_req = pci_reqs.requests[0]
7739             requested_networks[0].pci_request_id = pci_req.request_id
7740 
7741         result = self._allocate_port_resource_for_instance(
7742             context, instance, pci_reqs, request_groups)
7743         provider_mappings, resources = result
7744 
7745         try:
7746             pci_device = self._claim_pci_device_for_interface_attach(
7747                 context, instance, pci_reqs)
7748         except exception.InterfaceAttachPciClaimFailed:
7749             with excutils.save_and_reraise_exception():
7750                 if resources:
7751                     # TODO(gibi): Instead of giving up we could try another
7752                     # allocation candidate from _allocate_resources() if any
7753                     self._deallocate_port_resource_for_instance(
7754                         context, instance, port_id, resources)
7755 
7756         instance.pci_requests.requests.extend(pci_reqs.requests)
7757 
7758         network_info = self.network_api.allocate_for_instance(
7759             context,
7760             instance,
7761             requested_networks,
7762             bind_host_id=bind_host_id,
7763             resource_provider_mapping=provider_mappings,
7764         )
7765 
7766         if len(network_info) != 1:
7767             LOG.error('allocate_for_instance returned %(ports)s '
7768                       'ports', {'ports': len(network_info)})
7769             # TODO(elod.illes): an instance.interface_attach.error notification
7770             # should be sent here
7771             raise exception.InterfaceAttachFailed(
7772                     instance_uuid=instance.uuid)
7773         image_meta = objects.ImageMeta.from_instance(instance)
7774 
7775         try:
7776             self.driver.attach_interface(context, instance, image_meta,
7777                                          network_info[0])
7778         except exception.NovaException as ex:
7779             port_id = network_info[0].get('id')
7780             LOG.warning("attach interface failed , try to deallocate "
7781                         "port %(port_id)s, reason: %(msg)s",
7782                         {'port_id': port_id, 'msg': ex},
7783                         instance=instance)
7784             self._deallocate_port_for_instance(
7785                 context, instance, port_id, pci_device=pci_device)
7786 
7787             compute_utils.notify_about_instance_action(
7788                 context, instance, self.host,
7789                 action=fields.NotificationAction.INTERFACE_ATTACH,
7790                 phase=fields.NotificationPhase.ERROR,
7791                 exception=ex)
7792 
7793             raise exception.InterfaceAttachFailed(
7794                 instance_uuid=instance.uuid)
7795 
7796         if pci_device:
7797             # NOTE(gibi): The _claim_pci_device_for_interface_attach() call
7798             # found a pci device but it only marked the device as claimed. The
7799             # periodic update_available_resource would move the device to
7800             # allocated state. But as driver.attach_interface() has been
7801             # succeeded we now know that the interface is also allocated
7802             # (used by) to the instance. So make sure the pci tracker also
7803             # tracks this device as allocated. This way we can avoid a possible
7804             # race condition when a detach arrives for a device that is only
7805             # in claimed state.
7806             self.rt.allocate_pci_devices_for_instance(context, instance)
7807 
7808         instance.save()
7809 
7810         compute_utils.notify_about_instance_action(
7811             context, instance, self.host,
7812             action=fields.NotificationAction.INTERFACE_ATTACH,
7813             phase=fields.NotificationPhase.END)
7814 
7815         return network_info[0]
7816 
7817     @wrap_exception()
7818     @wrap_instance_event(prefix='compute')
7819     @wrap_instance_fault
7820     def detach_interface(self, context, instance, port_id):
7821         """Detach a network adapter from an instance."""
7822         lockname = 'interface-%s-%s' % (instance.uuid, port_id)
7823 
7824         @utils.synchronized(lockname)
7825         def do_detach_interface(context, instance, port_id):
7826             self._detach_interface(context, instance, port_id)
7827 
7828         do_detach_interface(context, instance, port_id)
7829 
7830     def _detach_interface(self, context, instance, port_id):
7831         # NOTE(aarents): we need to refresh info cache from DB here,
7832         # as previous detach/attach lock holder just updated it.
7833         compute_utils.refresh_info_cache_for_instance(context, instance)
7834         network_info = instance.info_cache.network_info
7835         condemned = None
7836         for vif in network_info:
7837             if vif['id'] == port_id:
7838                 condemned = vif
7839                 break
7840         if condemned is None:
7841             raise exception.PortNotFound(_("Port %s is not "
7842                                            "attached") % port_id)
7843 
7844         pci_req = pci_req_module.get_instance_pci_request_from_vif(
7845             context, instance, condemned)
7846 
7847         pci_device = None
7848         if pci_req:
7849             pci_devices = [pci_device
7850                            for pci_device in instance.pci_devices.objects
7851                            if pci_device.request_id == pci_req.request_id]
7852 
7853             if not pci_devices:
7854                 LOG.warning(
7855                     "Detach interface failed, port_id=%(port_id)s, "
7856                     "reason: PCI device not found for PCI request %(pci_req)s",
7857                     {'port_id': port_id, 'pci_req': pci_req})
7858                 raise exception.InterfaceDetachFailed(
7859                     instance_uuid=instance.uuid)
7860 
7861             pci_device = pci_devices[0]
7862 
7863         compute_utils.notify_about_instance_action(
7864             context, instance, self.host,
7865             action=fields.NotificationAction.INTERFACE_DETACH,
7866             phase=fields.NotificationPhase.START)
7867 
7868         try:
7869             self.driver.detach_interface(context, instance, condemned)
7870         except exception.NovaException as ex:
7871             # If the instance was deleted before the interface was detached,
7872             # just log it at debug.
7873             log_level = (logging.DEBUG
7874                          if isinstance(ex, exception.InstanceNotFound)
7875                          else logging.WARNING)
7876             LOG.log(log_level,
7877                     "Detach interface failed, port_id=%(port_id)s, reason: "
7878                     "%(msg)s", {'port_id': port_id, 'msg': ex},
7879                     instance=instance)
7880             raise exception.InterfaceDetachFailed(instance_uuid=instance.uuid)
7881         else:
7882             self._deallocate_port_for_instance(
7883                 context, instance, port_id, raise_on_failure=True,
7884                 pci_device=pci_device)
7885 
7886         instance.save()
7887 
7888         compute_utils.notify_about_instance_action(
7889             context, instance, self.host,
7890             action=fields.NotificationAction.INTERFACE_DETACH,
7891             phase=fields.NotificationPhase.END)
7892 
7893     def _get_compute_info(self, context, host):
7894         return objects.ComputeNode.get_first_node_by_host_for_old_compat(
7895             context, host)
7896 
7897     # TODO(stephenfin): Remove the unused instance argument in RPC version 6.0
7898     @wrap_exception()
7899     def check_instance_shared_storage(self, ctxt, instance, data):
7900         """Check if the instance files are shared
7901 
7902         :param ctxt: security context
7903         :param instance: dict of instance data
7904         :param data: result of driver.check_instance_shared_storage_local
7905 
7906         Returns True if instance disks located on shared storage and
7907         False otherwise.
7908         """
7909         return self.driver.check_instance_shared_storage_remote(ctxt, data)
7910 
7911     def _dest_can_numa_live_migrate(self, dest_check_data, migration):
7912         # TODO(artom) If we have a libvirt driver we expect it to set
7913         # dst_supports_numa_live_migration, but we have to remove it if we
7914         # did not get a migration from the conductor, indicating that it
7915         # cannot send RPC 5.3. This check can be removed in RPC 6.0.
7916         if ('dst_supports_numa_live_migration' in dest_check_data and
7917                 dest_check_data.dst_supports_numa_live_migration and
7918                 not migration):
7919             delattr(dest_check_data, 'dst_supports_numa_live_migration')
7920         return dest_check_data
7921 
7922     @wrap_exception()
7923     @wrap_instance_event(prefix='compute')
7924     @wrap_instance_fault
7925     def check_can_live_migrate_destination(self, ctxt, instance,
7926                                            block_migration, disk_over_commit,
7927                                            migration=None, limits=None):
7928         """Check if it is possible to execute live migration.
7929 
7930         This runs checks on the destination host, and then calls
7931         back to the source host to check the results.
7932 
7933         :param context: security context
7934         :param instance: dict of instance data
7935         :param block_migration: if true, prepare for block migration
7936                                 if None, calculate it in driver
7937         :param disk_over_commit: if true, allow disk over commit
7938                                  if None, ignore disk usage checking
7939         :param migration: objects.Migration object for this live migration.
7940         :param limits: objects.SchedulerLimits object for this live migration.
7941         :returns: a LiveMigrateData object (hypervisor-dependent)
7942         """
7943         src_compute_info = obj_base.obj_to_primitive(
7944             self._get_compute_info(ctxt, instance.host))
7945         dst_compute_info = obj_base.obj_to_primitive(
7946             self._get_compute_info(ctxt, self.host))
7947         dest_check_data = self.driver.check_can_live_migrate_destination(ctxt,
7948             instance, src_compute_info, dst_compute_info,
7949             block_migration, disk_over_commit)
7950         dest_check_data = self._dest_can_numa_live_migrate(dest_check_data,
7951                                                            migration)
7952         LOG.debug('destination check data is %s', dest_check_data)
7953         try:
7954             allocs = self.reportclient.get_allocations_for_consumer(
7955                 ctxt, instance.uuid)
7956             migrate_data = self.compute_rpcapi.check_can_live_migrate_source(
7957                 ctxt, instance, dest_check_data)
7958             if ('src_supports_numa_live_migration' in migrate_data and
7959                     migrate_data.src_supports_numa_live_migration):
7960                 migrate_data = self._live_migration_claim(
7961                     ctxt, instance, migrate_data, migration, limits, allocs)
7962             elif 'dst_supports_numa_live_migration' in dest_check_data:
7963                 LOG.info('Destination was ready for NUMA live migration, '
7964                          'but source is either too old, or is set to an '
7965                          'older upgrade level.', instance=instance)
7966             if self.network_api.supports_port_binding_extension(ctxt):
7967                 # Create migrate_data vifs
7968                 migrate_data.vifs = \
7969                     migrate_data_obj.\
7970                     VIFMigrateData.create_skeleton_migrate_vifs(
7971                         instance.get_network_info())
7972                 # Claim PCI devices for VIFs on destination (if needed)
7973                 port_id_to_pci = self._claim_pci_for_instance_vifs(
7974                     ctxt, instance)
7975                 # Update migrate VIFs with the newly claimed PCI devices
7976                 self._update_migrate_vifs_profile_with_pci(
7977                     migrate_data.vifs, port_id_to_pci)
7978         finally:
7979             self.driver.cleanup_live_migration_destination_check(ctxt,
7980                     dest_check_data)
7981         return migrate_data
7982 
7983     def _live_migration_claim(self, ctxt, instance, migrate_data,
7984                               migration, limits, allocs):
7985         """Runs on the destination and does a resources claim, if necessary.
7986         Currently, only NUMA live migrations require it.
7987 
7988         :param ctxt: Request context
7989         :param instance: The Instance being live migrated
7990         :param migrate_data: The MigrateData object for this live migration
7991         :param migration: The Migration object for this live migration
7992         :param limits: The SchedulerLimits object for this live migration
7993         :returns: migrate_data with dst_numa_info set if necessary
7994         """
7995         try:
7996             # NOTE(artom) We might have gotten here from _find_destination() in
7997             # the conductor live migrate task. At that point,
7998             # migration.dest_node is not set yet (nor should it be, we're still
7999             # looking for a destination, after all). Therefore, we cannot use
8000             # migration.dest_node here and must use self._get_nodename().
8001             claim = self.rt.live_migration_claim(
8002                 ctxt, instance, self._get_nodename(instance), migration,
8003                 limits, allocs)
8004             LOG.debug('Created live migration claim.', instance=instance)
8005         except exception.ComputeResourcesUnavailable as e:
8006             raise exception.MigrationPreCheckError(
8007                 reason=e.format_message())
8008         return self.driver.post_claim_migrate_data(ctxt, instance,
8009                                                    migrate_data, claim)
8010 
8011     def _source_can_numa_live_migrate(self, ctxt, dest_check_data,
8012                                       source_check_data):
8013         # TODO(artom) Our virt driver may have told us that it supports NUMA
8014         # live migration. However, the following other conditions must be met
8015         # for a NUMA live migration to happen:
8016         # 1. We got a True dst_supports_numa_live_migration in
8017         #    dest_check_data, indicating that the dest virt driver supports
8018         #    NUMA live migration and that the conductor can send RPC 5.3 and
8019         #    that the destination compute manager can receive it.
8020         # 2. Ourselves, the source, can send RPC 5.3. There's no
8021         #    sentinel/parameter for this, so we just ask our rpcapi directly.
8022         # If any of these are not met, we need to remove the
8023         # src_supports_numa_live_migration flag from source_check_data to avoid
8024         # incorrectly initiating a NUMA live migration.
8025         # All of this can be removed in RPC 6.0/objects 2.0.
8026         can_numa_live_migrate = (
8027             'dst_supports_numa_live_migration' in dest_check_data and
8028             dest_check_data.dst_supports_numa_live_migration and
8029             self.compute_rpcapi.supports_numa_live_migration(ctxt))
8030         if ('src_supports_numa_live_migration' in source_check_data and
8031                 source_check_data.src_supports_numa_live_migration and
8032                 not can_numa_live_migrate):
8033             delattr(source_check_data, 'src_supports_numa_live_migration')
8034         return source_check_data
8035 
8036     @wrap_exception()
8037     @wrap_instance_event(prefix='compute')
8038     @wrap_instance_fault
8039     def check_can_live_migrate_source(self, ctxt, instance, dest_check_data):
8040         """Check if it is possible to execute live migration.
8041 
8042         This checks if the live migration can succeed, based on the
8043         results from check_can_live_migrate_destination.
8044 
8045         :param ctxt: security context
8046         :param instance: dict of instance data
8047         :param dest_check_data: result of check_can_live_migrate_destination
8048         :returns: a LiveMigrateData object
8049         """
8050         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
8051             ctxt, instance.uuid)
8052         is_volume_backed = compute_utils.is_volume_backed_instance(
8053             ctxt, instance, bdms)
8054         dest_check_data.is_volume_backed = is_volume_backed
8055         block_device_info = self._get_instance_block_device_info(
8056                             ctxt, instance, refresh_conn_info=False, bdms=bdms)
8057         result = self.driver.check_can_live_migrate_source(ctxt, instance,
8058                                                            dest_check_data,
8059                                                            block_device_info)
8060         result = self._source_can_numa_live_migrate(ctxt, dest_check_data,
8061                                                     result)
8062         LOG.debug('source check data is %s', result)
8063         return result
8064 
8065     # TODO(mriedem): Remove the block_migration argument in v6.0 of the compute
8066     # RPC API.
8067     @wrap_exception()
8068     @wrap_instance_event(prefix='compute')
8069     @wrap_instance_fault
8070     def pre_live_migration(self, context, instance, block_migration, disk,
8071                            migrate_data):
8072         """Preparations for live migration at dest host.
8073 
8074         :param context: security context
8075         :param instance: dict of instance data
8076         :param block_migration: if true, prepare for block migration
8077         :param disk: disk info of instance
8078         :param migrate_data: A dict or LiveMigrateData object holding data
8079                              required for live migration without shared
8080                              storage.
8081         :returns: migrate_data containing additional migration info
8082         """
8083         LOG.debug('pre_live_migration data is %s', migrate_data)
8084 
8085         migrate_data.old_vol_attachment_ids = {}
8086         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
8087             context, instance.uuid)
8088         network_info = self.network_api.get_instance_nw_info(context, instance)
8089         self._notify_about_instance_usage(
8090             context, instance, "live_migration.pre.start",
8091             network_info=network_info)
8092         compute_utils.notify_about_instance_action(
8093             context, instance, self.host,
8094             action=fields.NotificationAction.LIVE_MIGRATION_PRE,
8095             phase=fields.NotificationPhase.START, bdms=bdms)
8096 
8097         connector = self.driver.get_volume_connector(instance)
8098         try:
8099             for bdm in bdms:
8100                 if bdm.is_volume and bdm.attachment_id is not None:
8101                     # This bdm uses the new cinder v3.44 API.
8102                     # We will create a new attachment for this
8103                     # volume on this migration destination host. The old
8104                     # attachment will be deleted on the source host
8105                     # when the migration succeeds. The old attachment_id
8106                     # is stored in dict with the key being the bdm.volume_id
8107                     # so it can be restored on rollback.
8108                     #
8109                     # Also note that attachment_update is not needed as we
8110                     # are providing the connector in the create call.
8111                     attach_ref = self.volume_api.attachment_create(
8112                         context, bdm.volume_id, bdm.instance_uuid,
8113                         connector=connector, mountpoint=bdm.device_name)
8114 
8115                     # save current attachment so we can detach it on success,
8116                     # or restore it on a rollback.
8117                     # NOTE(mdbooth): This data is no longer used by the source
8118                     # host since change Ibe9215c0. We can't remove it until we
8119                     # are sure the source host has been upgraded.
8120                     migrate_data.old_vol_attachment_ids[bdm.volume_id] = \
8121                         bdm.attachment_id
8122 
8123                     # update the bdm with the new attachment_id.
8124                     bdm.attachment_id = attach_ref['id']
8125                     bdm.save()
8126 
8127             block_device_info = self._get_instance_block_device_info(
8128                                 context, instance, refresh_conn_info=True,
8129                                 bdms=bdms)
8130 
8131             # The driver pre_live_migration will plug vifs on the host
8132             migrate_data = self.driver.pre_live_migration(context,
8133                                            instance,
8134                                            block_device_info,
8135                                            network_info,
8136                                            disk,
8137                                            migrate_data)
8138             LOG.debug('driver pre_live_migration data is %s', migrate_data)
8139             # driver.pre_live_migration is what plugs vifs on the destination
8140             # host so now we can set the wait_for_vif_plugged flag in the
8141             # migrate_data object which the source compute will use to
8142             # determine if it should wait for a 'network-vif-plugged' event
8143             # from neutron before starting the actual guest transfer in the
8144             # hypervisor
8145             using_multiple_port_bindings = (
8146                 'vifs' in migrate_data and migrate_data.vifs)
8147             migrate_data.wait_for_vif_plugged = (
8148                 CONF.compute.live_migration_wait_for_vif_plug and
8149                 using_multiple_port_bindings
8150             )
8151 
8152             # NOTE(tr3buchet): setup networks on destination host
8153             self.network_api.setup_networks_on_host(context, instance,
8154                                                              self.host)
8155 
8156         except Exception:
8157             # If we raise, migrate_data with the updated attachment ids
8158             # will not be returned to the source host for rollback.
8159             # So we need to rollback new attachments here.
8160             with excutils.save_and_reraise_exception():
8161                 old_attachments = migrate_data.old_vol_attachment_ids
8162                 for bdm in bdms:
8163                     if (bdm.is_volume and bdm.attachment_id is not None and
8164                             bdm.volume_id in old_attachments):
8165                         self.volume_api.attachment_delete(context,
8166                                                           bdm.attachment_id)
8167                         bdm.attachment_id = old_attachments[bdm.volume_id]
8168                         bdm.save()
8169 
8170         # Volume connections are complete, tell cinder that all the
8171         # attachments have completed.
8172         for bdm in bdms:
8173             if bdm.is_volume and bdm.attachment_id is not None:
8174                 self.volume_api.attachment_complete(context,
8175                                                     bdm.attachment_id)
8176 
8177         self._notify_about_instance_usage(
8178                      context, instance, "live_migration.pre.end",
8179                      network_info=network_info)
8180         compute_utils.notify_about_instance_action(
8181             context, instance, self.host,
8182             action=fields.NotificationAction.LIVE_MIGRATION_PRE,
8183             phase=fields.NotificationPhase.END, bdms=bdms)
8184 
8185         LOG.debug('pre_live_migration result data is %s', migrate_data)
8186         return migrate_data
8187 
8188     @staticmethod
8189     def _neutron_failed_migration_callback(event_name, instance):
8190         msg = ('Neutron reported failure during migration '
8191                'with %(event)s for instance %(uuid)s')
8192         msg_args = {'event': event_name, 'uuid': instance.uuid}
8193         if CONF.vif_plugging_is_fatal:
8194             raise exception.VirtualInterfacePlugException(msg % msg_args)
8195         LOG.error(msg, msg_args)
8196 
8197     @staticmethod
8198     def _get_neutron_events_for_live_migration(instance, migration):
8199         # We don't generate events if CONF.vif_plugging_timeout=0
8200         # meaning that the operator disabled using them.
8201         if CONF.vif_plugging_timeout:
8202             return (instance.get_network_info()
8203                     .get_live_migration_plug_time_events())
8204         else:
8205             return []
8206 
8207     def _cleanup_pre_live_migration(self, context, dest, instance,
8208                                     migration, migrate_data, source_bdms):
8209         """Helper method for when pre_live_migration fails
8210 
8211         Sets the migration status to "error" and rolls back the live migration
8212         setup on the destination host.
8213 
8214         :param context: The user request context.
8215         :type context: nova.context.RequestContext
8216         :param dest: The live migration destination hostname.
8217         :type dest: str
8218         :param instance: The instance being live migrated.
8219         :type instance: nova.objects.Instance
8220         :param migration: The migration record tracking this live migration.
8221         :type migration: nova.objects.Migration
8222         :param migrate_data: Data about the live migration, populated from
8223                              the destination host.
8224         :type migrate_data: Subclass of nova.objects.LiveMigrateData
8225         :param source_bdms: BDMs prior to modification by the destination
8226                             compute host. Set by _do_live_migration and not
8227                             part of the callback interface, so this is never
8228                             None
8229         """
8230         self._set_migration_status(migration, 'error')
8231         # Make sure we set this for _rollback_live_migration()
8232         # so it can find it, as expected if it was called later
8233         migrate_data.migration = migration
8234         self._rollback_live_migration(context, instance, dest,
8235                                       migrate_data=migrate_data,
8236                                       source_bdms=source_bdms)
8237 
8238     def _do_pre_live_migration_from_source(self, context, dest, instance,
8239                                            block_migration, migration,
8240                                            migrate_data, source_bdms):
8241         """Prepares for pre-live-migration on the source host and calls dest
8242 
8243         Will setup a callback networking event handler (if configured) and
8244         then call the dest host's pre_live_migration method to prepare the
8245         dest host for live migration (plugs vifs, connect volumes, etc).
8246 
8247         _rollback_live_migration (on the source) will be called if
8248         pre_live_migration (on the dest) fails.
8249 
8250         :param context: nova auth request context for this operation
8251         :param dest: name of the destination compute service host
8252         :param instance: Instance object being live migrated
8253         :param block_migration: If true, prepare for block migration.
8254         :param migration: Migration object tracking this operation
8255         :param migrate_data: MigrateData object for this operation populated
8256             by the destination host compute driver as part of the
8257             check_can_live_migrate_destination call.
8258         :param source_bdms: BlockDeviceMappingList of BDMs currently attached
8259             to the instance from the source host.
8260         :returns: MigrateData object which is a modified version of the
8261             ``migrate_data`` argument from the compute driver on the dest
8262             host during the ``pre_live_migration`` call.
8263         :raises: MigrationError if waiting for the network-vif-plugged event
8264             timed out and is fatal.
8265         """
8266         class _BreakWaitForInstanceEvent(Exception):
8267             """Used as a signal to stop waiting for the network-vif-plugged
8268             event when we discover that
8269             [compute]/live_migration_wait_for_vif_plug is not set on the
8270             destination.
8271             """
8272             pass
8273 
8274         events = self._get_neutron_events_for_live_migration(
8275             instance, migration)
8276         try:
8277             if ('block_migration' in migrate_data and
8278                     migrate_data.block_migration):
8279                 block_device_info = self._get_instance_block_device_info(
8280                     context, instance, bdms=source_bdms)
8281                 disk = self.driver.get_instance_disk_info(
8282                     instance, block_device_info=block_device_info)
8283             else:
8284                 disk = None
8285 
8286             deadline = CONF.vif_plugging_timeout
8287             error_cb = self._neutron_failed_migration_callback
8288             # In order to avoid a race with the vif plugging that the virt
8289             # driver does on the destination host, we register our events
8290             # to wait for before calling pre_live_migration. Then if the
8291             # dest host reports back that we shouldn't wait, we can break
8292             # out of the context manager using _BreakWaitForInstanceEvent.
8293             with self.virtapi.wait_for_instance_event(
8294                     instance, events, deadline=deadline,
8295                     error_callback=error_cb):
8296                 with timeutils.StopWatch() as timer:
8297                     # TODO(mriedem): The "block_migration" parameter passed
8298                     # here is not actually used in pre_live_migration but it
8299                     # is not optional in the RPC interface either.
8300                     migrate_data = self.compute_rpcapi.pre_live_migration(
8301                         context, instance,
8302                         block_migration, disk, dest, migrate_data)
8303                 LOG.info('Took %0.2f seconds for pre_live_migration on '
8304                          'destination host %s.',
8305                          timer.elapsed(), dest, instance=instance)
8306                 wait_for_vif_plugged = (
8307                     'wait_for_vif_plugged' in migrate_data and
8308                     migrate_data.wait_for_vif_plugged)
8309                 if events and not wait_for_vif_plugged:
8310                     raise _BreakWaitForInstanceEvent
8311         except _BreakWaitForInstanceEvent:
8312             if events:
8313                 LOG.debug('Not waiting for events after pre_live_migration: '
8314                           '%s. ', events, instance=instance)
8315         except exception.VirtualInterfacePlugException:
8316             with excutils.save_and_reraise_exception():
8317                 LOG.exception('Failed waiting for network virtual interfaces '
8318                               'to be plugged on the destination host %s.',
8319                               dest, instance=instance)
8320                 self._cleanup_pre_live_migration(
8321                     context, dest, instance, migration, migrate_data,
8322                     source_bdms)
8323         except eventlet.timeout.Timeout:
8324             # We only get here if wait_for_vif_plugged is True which means
8325             # live_migration_wait_for_vif_plug=True on the destination host.
8326             msg = (
8327                 'Timed out waiting for events: %(events)s. If these timeouts '
8328                 'are a persistent issue it could mean the networking backend '
8329                 'on host %(dest)s does not support sending these events '
8330                 'unless there are port binding host changes which does not '
8331                 'happen at this point in the live migration process. You may '
8332                 'need to disable the live_migration_wait_for_vif_plug option '
8333                 'on host %(dest)s.')
8334             subs = {'events': events, 'dest': dest}
8335             LOG.warning(msg, subs, instance=instance)
8336             if CONF.vif_plugging_is_fatal:
8337                 self._cleanup_pre_live_migration(
8338                     context, dest, instance, migration, migrate_data,
8339                     source_bdms)
8340                 raise exception.MigrationError(reason=msg % subs)
8341         except Exception:
8342             with excutils.save_and_reraise_exception():
8343                 LOG.exception('Pre live migration failed at %s',
8344                               dest, instance=instance)
8345                 self._cleanup_pre_live_migration(
8346                     context, dest, instance, migration, migrate_data,
8347                     source_bdms)
8348         return migrate_data
8349 
8350     def _do_live_migration(self, context, dest, instance, block_migration,
8351                            migration, migrate_data):
8352         # NOTE(danms): We should enhance the RT to account for migrations
8353         # and use the status field to denote when the accounting has been
8354         # done on source/destination. For now, this is just here for status
8355         # reporting
8356         self._set_migration_status(migration, 'preparing')
8357         source_bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
8358                 context, instance.uuid)
8359 
8360         migrate_data = self._do_pre_live_migration_from_source(
8361             context, dest, instance, block_migration, migration, migrate_data,
8362             source_bdms)
8363 
8364         # Set migrate_data.migration because that is how _post_live_migration
8365         # and _rollback_live_migration get the migration object for cleanup.
8366         # Yes this is gross but changing the _post_live_migration and
8367         # _rollback_live_migration interfaces would also mean changing how the
8368         # virt drivers call them from the driver.live_migration method, i.e.
8369         # we would have to pass the migration object through the driver (or
8370         # consider using a partial but some do not like that pattern).
8371         migrate_data.migration = migration
8372 
8373         # NOTE(Kevin_Zheng): Pop the migration from the waiting queue
8374         # if it exist in the queue, then we are good to moving on, if
8375         # not, some other process must have aborted it, then we should
8376         # rollback.
8377         try:
8378             self._waiting_live_migrations.pop(instance.uuid)
8379         except KeyError:
8380             LOG.debug('Migration %s aborted by another process, rollback.',
8381                       migration.uuid, instance=instance)
8382             self._rollback_live_migration(context, instance, dest,
8383                                           migrate_data, 'cancelled',
8384                                           source_bdms=source_bdms)
8385             self._notify_live_migrate_abort_end(context, instance)
8386             return
8387 
8388         self._set_migration_status(migration, 'running')
8389 
8390         # NOTE(mdbooth): pre_live_migration will update connection_info and
8391         # attachment_id on all volume BDMS to reflect the new destination
8392         # host attachment. We fetch BDMs before that to retain connection_info
8393         # and attachment_id relating to the source host for post migration
8394         # cleanup.
8395         post_live_migration = functools.partial(self._post_live_migration,
8396                                                 source_bdms=source_bdms)
8397         rollback_live_migration = functools.partial(
8398             self._rollback_live_migration, source_bdms=source_bdms)
8399 
8400         LOG.debug('live_migration data is %s', migrate_data)
8401         try:
8402             self.driver.live_migration(context, instance, dest,
8403                                        post_live_migration,
8404                                        rollback_live_migration,
8405                                        block_migration, migrate_data)
8406         except Exception:
8407             LOG.exception('Live migration failed.', instance=instance)
8408             with excutils.save_and_reraise_exception():
8409                 # Put instance and migration into error state,
8410                 # as its almost certainly too late to rollback
8411                 self._set_migration_status(migration, 'error')
8412                 # first refresh instance as it may have got updated by
8413                 # post_live_migration_at_destination
8414                 instance.refresh()
8415                 self._set_instance_obj_error_state(instance,
8416                                                    clean_task_state=True)
8417 
8418     @wrap_exception()
8419     @wrap_instance_event(prefix='compute')
8420     @errors_out_migration
8421     @wrap_instance_fault
8422     def live_migration(self, context, dest, instance, block_migration,
8423                        migration, migrate_data):
8424         """Executing live migration.
8425 
8426         :param context: security context
8427         :param dest: destination host
8428         :param instance: a nova.objects.instance.Instance object
8429         :param block_migration: if true, prepare for block migration
8430         :param migration: an nova.objects.Migration object
8431         :param migrate_data: implementation specific params
8432 
8433         """
8434         self._set_migration_status(migration, 'queued')
8435         # NOTE(Kevin_Zheng): Submit the live_migration job to the pool and
8436         # put the returned Future object into dict mapped with migration.uuid
8437         # in order to be able to track and abort it in the future.
8438         self._waiting_live_migrations[instance.uuid] = (None, None)
8439         try:
8440             future = self._live_migration_executor.submit(
8441                 self._do_live_migration, context, dest, instance,
8442                 block_migration, migration, migrate_data)
8443             self._waiting_live_migrations[instance.uuid] = (migration, future)
8444         except RuntimeError:
8445             # GreenThreadPoolExecutor.submit will raise RuntimeError if the
8446             # pool is shutdown, which happens in
8447             # _cleanup_live_migrations_in_pool.
8448             LOG.info('Migration %s failed to submit as the compute service '
8449                      'is shutting down.', migration.uuid, instance=instance)
8450             raise exception.LiveMigrationNotSubmitted(
8451                 migration_uuid=migration.uuid, instance_uuid=instance.uuid)
8452 
8453     @wrap_exception()
8454     @wrap_instance_event(prefix='compute')
8455     @wrap_instance_fault
8456     def live_migration_force_complete(self, context, instance):
8457         """Force live migration to complete.
8458 
8459         :param context: Security context
8460         :param instance: The instance that is being migrated
8461         """
8462 
8463         self._notify_about_instance_usage(
8464             context, instance, 'live.migration.force.complete.start')
8465         compute_utils.notify_about_instance_action(
8466             context, instance, self.host,
8467             action=fields.NotificationAction.LIVE_MIGRATION_FORCE_COMPLETE,
8468             phase=fields.NotificationPhase.START)
8469         self.driver.live_migration_force_complete(instance)
8470         self._notify_about_instance_usage(
8471             context, instance, 'live.migration.force.complete.end')
8472         compute_utils.notify_about_instance_action(
8473             context, instance, self.host,
8474             action=fields.NotificationAction.LIVE_MIGRATION_FORCE_COMPLETE,
8475             phase=fields.NotificationPhase.END)
8476 
8477     def _notify_live_migrate_abort_end(self, context, instance):
8478         self._notify_about_instance_usage(
8479             context, instance, 'live.migration.abort.end')
8480         compute_utils.notify_about_instance_action(
8481             context, instance, self.host,
8482             action=fields.NotificationAction.LIVE_MIGRATION_ABORT,
8483             phase=fields.NotificationPhase.END)
8484 
8485     @wrap_exception()
8486     @wrap_instance_event(prefix='compute')
8487     @wrap_instance_fault
8488     def live_migration_abort(self, context, instance, migration_id):
8489         """Abort an in-progress live migration.
8490 
8491         :param context: Security context
8492         :param instance: The instance that is being migrated
8493         :param migration_id: ID of in-progress live migration
8494 
8495         """
8496         self._notify_about_instance_usage(
8497             context, instance, 'live.migration.abort.start')
8498         compute_utils.notify_about_instance_action(
8499             context, instance, self.host,
8500             action=fields.NotificationAction.LIVE_MIGRATION_ABORT,
8501             phase=fields.NotificationPhase.START)
8502         # NOTE(Kevin_Zheng): Pop the migration out from the queue, this might
8503         # lead to 3 scenarios:
8504         # 1. The selected migration is still in queue, and the future.cancel()
8505         #    succeed, then the abort action is succeed, mark the migration
8506         #    status to 'cancelled'.
8507         # 2. The selected migration is still in queue, but the future.cancel()
8508         #    failed, then the _do_live_migration() has started executing, and
8509         #    the migration status is 'preparing', then we just pop it from the
8510         #    queue, and the migration process will handle it later. And the
8511         #    migration status couldn't be 'running' in this scenario because
8512         #    if _do_live_migration has started executing and we've already
8513         #    popped it from the queue and set the migration status to
8514         #    'running' at this point, popping it here will raise KeyError at
8515         #    which point we check if it's running and if so, we abort the old
8516         #    way.
8517         # 3. The selected migration is not in the queue, then the migration
8518         #    status is 'running', let the driver handle it.
8519         try:
8520             migration, future = (
8521                 self._waiting_live_migrations.pop(instance.uuid))
8522             if future and future.cancel():
8523                 # If we got here, we've successfully aborted the queued
8524                 # migration and _do_live_migration won't run so we need
8525                 # to set the migration status to cancelled and send the
8526                 # notification. If Future.cancel() fails, it means
8527                 # _do_live_migration is running and the migration status
8528                 # is preparing, and _do_live_migration() itself will attempt
8529                 # to pop the queued migration, hit a KeyError, and rollback,
8530                 # set the migration to cancelled and send the
8531                 # live.migration.abort.end notification.
8532                 self._set_migration_status(migration, 'cancelled')
8533         except KeyError:
8534             migration = objects.Migration.get_by_id(context, migration_id)
8535             if migration.status != 'running':
8536                 raise exception.InvalidMigrationState(
8537                     migration_id=migration_id, instance_uuid=instance.uuid,
8538                     state=migration.status, method='abort live migration')
8539             self.driver.live_migration_abort(instance)
8540         self._notify_live_migrate_abort_end(context, instance)
8541 
8542     def _live_migration_cleanup_flags(self, migrate_data, migr_ctxt=None):
8543         """Determine whether disks, instance path or other resources
8544         need to be cleaned up after live migration (at source on success,
8545         at destination on rollback)
8546 
8547         Block migration needs empty image at destination host before migration
8548         starts, so if any failure occurs, any empty images has to be deleted.
8549 
8550         Also Volume backed live migration w/o shared storage needs to delete
8551         newly created instance-xxx dir on the destination as a part of its
8552         rollback process
8553 
8554         There may be other resources which need cleanup; currently this is
8555         limited to vPMEM devices with the libvirt driver.
8556 
8557         :param migrate_data: implementation specific data
8558         :param migr_ctxt: specific resources stored in migration_context
8559         :returns: (bool, bool) -- do_cleanup, destroy_disks
8560         """
8561         # NOTE(pkoniszewski): block migration specific params are set inside
8562         # migrate_data objects for drivers that expose block live migration
8563         # information (i.e. Libvirt, HyperV). For other drivers cleanup is not
8564         # needed.
8565         do_cleanup = False
8566         destroy_disks = False
8567         if isinstance(migrate_data, migrate_data_obj.LibvirtLiveMigrateData):
8568             has_vpmem = False
8569             if migr_ctxt and migr_ctxt.old_resources:
8570                 for resource in migr_ctxt.old_resources:
8571                     if ('metadata' in resource and
8572                         isinstance(resource.metadata,
8573                                    objects.LibvirtVPMEMDevice)):
8574                         has_vpmem = True
8575                         break
8576             # No instance booting at source host, but instance dir
8577             # must be deleted for preparing next block migration
8578             # must be deleted for preparing next live migration w/o shared
8579             # storage
8580             # vpmem must be cleanped
8581             do_cleanup = not migrate_data.is_shared_instance_path or has_vpmem
8582             destroy_disks = not migrate_data.is_shared_block_storage
8583         elif isinstance(migrate_data, migrate_data_obj.HyperVLiveMigrateData):
8584             # NOTE(claudiub): We need to cleanup any zombie Planned VM.
8585             do_cleanup = True
8586             destroy_disks = not migrate_data.is_shared_instance_path
8587 
8588         return (do_cleanup, destroy_disks)
8589 
8590     def _post_live_migration_remove_source_vol_connections(
8591             self, context, instance, source_bdms):
8592         """Disconnect volume connections from the source host during
8593         _post_live_migration.
8594 
8595         :param context: nova auth RequestContext
8596         :param instance: Instance object being live migrated
8597         :param source_bdms: BlockDeviceMappingList representing the attached
8598             volumes with connection_info set for the source host
8599         """
8600         # Detaching volumes.
8601         connector = self.driver.get_volume_connector(instance)
8602         for bdm in source_bdms:
8603             if bdm.is_volume:
8604                 # Detaching volumes is a call to an external API that can fail.
8605                 # If it does, we need to handle it gracefully so that the call
8606                 # to post_live_migration_at_destination - where we set instance
8607                 # host and task state - still happens. We need to rethink the
8608                 # current approach of setting instance host and task state
8609                 # AFTER a whole bunch of things that could fail in unhandled
8610                 # ways, but that is left as a TODO(artom).
8611                 try:
8612                     if bdm.attachment_id is None:
8613                         # Prior to cinder v3.44:
8614                         # We don't want to actually mark the volume detached,
8615                         # or delete the bdm, just remove the connection from
8616                         # this host.
8617                         #
8618                         # remove the volume connection without detaching from
8619                         # hypervisor because the instance is not running
8620                         # anymore on the current host
8621                         self.volume_api.terminate_connection(context,
8622                                                              bdm.volume_id,
8623                                                              connector)
8624                     else:
8625                         # cinder v3.44 api flow - delete the old attachment
8626                         # for the source host
8627                         self.volume_api.attachment_delete(context,
8628                                                           bdm.attachment_id)
8629 
8630                 except Exception as e:
8631                     if bdm.attachment_id is None:
8632                         LOG.error('Connection for volume %s not terminated on '
8633                                   'source host %s during post_live_migration: '
8634                                   '%s', bdm.volume_id, self.host,
8635                                   str(e), instance=instance)
8636                     else:
8637                         LOG.error('Volume attachment %s not deleted on source '
8638                                   'host %s during post_live_migration: %s',
8639                                   bdm.attachment_id, self.host,
8640                                   str(e), instance=instance)
8641 
8642     @wrap_exception()
8643     @wrap_instance_fault
8644     def _post_live_migration(self, ctxt, instance, dest,
8645                              block_migration=False, migrate_data=None,
8646                              source_bdms=None):
8647         """Post operations for live migration.
8648 
8649         This method is called from live_migration
8650         and mainly updating database record.
8651 
8652         :param ctxt: security context
8653         :param instance: instance dict
8654         :param dest: destination host
8655         :param block_migration: if true, prepare for block migration
8656         :param migrate_data: if not None, it is a dict which has data
8657         :param source_bdms: BDMs prior to modification by the destination
8658                             compute host. Set by _do_live_migration and not
8659                             part of the callback interface, so this is never
8660                             None
8661         required for live migration without shared storage
8662 
8663         """
8664         LOG.info('_post_live_migration() is started..',
8665                  instance=instance)
8666 
8667         # Cleanup source host post live-migration
8668         block_device_info = self._get_instance_block_device_info(
8669                             ctxt, instance, bdms=source_bdms)
8670         self.driver.post_live_migration(ctxt, instance, block_device_info,
8671                                         migrate_data)
8672 
8673         # Disconnect volumes from this (the source) host.
8674         self._post_live_migration_remove_source_vol_connections(
8675             ctxt, instance, source_bdms)
8676 
8677         # NOTE(artom) At this point in time we have not bound the ports to the
8678         # destination host yet (this happens in migrate_instance_start()
8679         # below). Therefore, the "old" source network info that's still in the
8680         # instance info cache is safe to use here, since it'll be used below
8681         # during driver.post_live_migration_at_source() to unplug the VIFs on
8682         # the source.
8683         network_info = instance.get_network_info()
8684 
8685         self._notify_about_instance_usage(ctxt, instance,
8686                                           "live_migration._post.start",
8687                                           network_info=network_info)
8688         compute_utils.notify_about_instance_action(
8689             ctxt, instance, self.host,
8690             action=fields.NotificationAction.LIVE_MIGRATION_POST,
8691             phase=fields.NotificationPhase.START)
8692 
8693         migration = {'source_compute': self.host,
8694                      'dest_compute': dest, }
8695         # For neutron, migrate_instance_start will activate the destination
8696         # host port bindings, if there are any created by conductor before live
8697         # migration started.
8698         self.network_api.migrate_instance_start(ctxt,
8699                                                 instance,
8700                                                 migration)
8701 
8702         destroy_vifs = False
8703         try:
8704             # It's possible that the vif type changed on the destination
8705             # host and is already bound and active, so we need to use the
8706             # stashed source vifs in migrate_data.vifs (if present) to unplug
8707             # on the source host.
8708             unplug_nw_info = network_info
8709             if migrate_data and 'vifs' in migrate_data:
8710                 nw_info = []
8711                 for migrate_vif in migrate_data.vifs:
8712                     nw_info.append(migrate_vif.source_vif)
8713                 unplug_nw_info = network_model.NetworkInfo.hydrate(nw_info)
8714                 LOG.debug('Calling driver.post_live_migration_at_source '
8715                           'with original source VIFs from migrate_data: %s',
8716                           unplug_nw_info, instance=instance)
8717             self.driver.post_live_migration_at_source(ctxt, instance,
8718                                                       unplug_nw_info)
8719         except NotImplementedError as ex:
8720             LOG.debug(ex, instance=instance)
8721             # For all hypervisors other than libvirt, there is a possibility
8722             # they are unplugging networks from source node in the cleanup
8723             # method
8724             destroy_vifs = True
8725 
8726         # Free instance allocations on source before claims are allocated on
8727         # destination node
8728         self.rt.free_pci_device_allocations_for_instance(ctxt, instance)
8729         # NOTE(danms): Save source node before calling post method on
8730         # destination, which will update it
8731         source_node = instance.node
8732 
8733         do_cleanup, destroy_disks = self._live_migration_cleanup_flags(
8734             migrate_data, migr_ctxt=instance.migration_context)
8735 
8736         if do_cleanup:
8737             LOG.debug('Calling driver.cleanup from _post_live_migration',
8738                       instance=instance)
8739             self.driver.cleanup(ctxt, instance, unplug_nw_info,
8740                                 destroy_disks=destroy_disks,
8741                                 migrate_data=migrate_data,
8742                                 destroy_vifs=destroy_vifs)
8743 
8744         # Define domain at destination host, without doing it,
8745         # pause/suspend/terminate do not work.
8746         post_at_dest_success = True
8747         try:
8748             self.compute_rpcapi.post_live_migration_at_destination(ctxt,
8749                     instance, block_migration, dest)
8750         except Exception as error:
8751             post_at_dest_success = False
8752             # We don't want to break _post_live_migration() if
8753             # post_live_migration_at_destination() fails as it should never
8754             # affect cleaning up source node.
8755             LOG.exception("Post live migration at destination %s failed",
8756                           dest, instance=instance, error=error)
8757 
8758         self.instance_events.clear_events_for_instance(instance)
8759 
8760         # NOTE(timello): make sure we update available resources on source
8761         # host even before next periodic task.
8762         self.update_available_resource(ctxt)
8763 
8764         self._update_scheduler_instance_info(ctxt, instance)
8765         self._notify_about_instance_usage(ctxt, instance,
8766                                           "live_migration._post.end",
8767                                           network_info=network_info)
8768         compute_utils.notify_about_instance_action(
8769             ctxt, instance, self.host,
8770             action=fields.NotificationAction.LIVE_MIGRATION_POST,
8771             phase=fields.NotificationPhase.END)
8772         if post_at_dest_success:
8773             LOG.info('Migrating instance to %s finished successfully.',
8774                      dest, instance=instance)
8775 
8776         self._clean_instance_console_tokens(ctxt, instance)
8777         if migrate_data and migrate_data.obj_attr_is_set('migration'):
8778             migrate_data.migration.status = 'completed'
8779             migrate_data.migration.save()
8780             self._delete_allocation_after_move(ctxt,
8781                                                instance,
8782                                                migrate_data.migration)
8783         else:
8784             # We didn't have data on a migration, which means we can't
8785             # look up to see if we had new-style migration-based
8786             # allocations. This should really only happen in cases of
8787             # a buggy virt driver. Log a warning so we know it happened.
8788             LOG.warning('Live migration ended with no migrate_data '
8789                         'record. Unable to clean up migration-based '
8790                         'allocations for node %s which is almost certainly '
8791                         'not an expected situation.', source_node,
8792                         instance=instance)
8793 
8794     def _consoles_enabled(self):
8795         """Returns whether a console is enable."""
8796         return (CONF.vnc.enabled or CONF.spice.enabled or
8797                 CONF.rdp.enabled or CONF.serial_console.enabled or
8798                 CONF.mks.enabled)
8799 
8800     def _clean_instance_console_tokens(self, ctxt, instance):
8801         """Clean console tokens stored for an instance."""
8802         # If the database backend isn't in use, don't bother trying to clean
8803         # tokens.
8804         if self._consoles_enabled():
8805             objects.ConsoleAuthToken.\
8806                 clean_console_auths_for_instance(ctxt, instance.uuid)
8807 
8808     @wrap_exception()
8809     @wrap_instance_event(prefix='compute')
8810     @wrap_instance_fault
8811     def post_live_migration_at_destination(self, context, instance,
8812                                            block_migration):
8813         """Post operations for live migration .
8814 
8815         :param context: security context
8816         :param instance: Instance dict
8817         :param block_migration: if true, prepare for block migration
8818 
8819         """
8820         LOG.info('Post operation of migration started',
8821                  instance=instance)
8822 
8823         # NOTE(tr3buchet): setup networks on destination host
8824         #                  this is called a second time because
8825         #                  multi_host does not create the bridge in
8826         #                  plug_vifs
8827         # NOTE(mriedem): This is a no-op for neutron.
8828         self.network_api.setup_networks_on_host(context, instance,
8829                                                          self.host)
8830         migration = objects.Migration(
8831             source_compute=instance.host,
8832             dest_compute=self.host,
8833             migration_type=fields.MigrationType.LIVE_MIGRATION)
8834         self.network_api.migrate_instance_finish(
8835             context, instance, migration, provider_mappings=None)
8836 
8837         network_info = self.network_api.get_instance_nw_info(context, instance)
8838         self._notify_about_instance_usage(
8839                      context, instance, "live_migration.post.dest.start",
8840                      network_info=network_info)
8841         compute_utils.notify_about_instance_action(context, instance,
8842                 self.host,
8843                 action=fields.NotificationAction.LIVE_MIGRATION_POST_DEST,
8844                 phase=fields.NotificationPhase.START)
8845         block_device_info = self._get_instance_block_device_info(context,
8846                                                                  instance)
8847         # Allocate the claimed PCI resources at destination.
8848         self.rt.allocate_pci_devices_for_instance(context, instance)
8849 
8850         try:
8851             self.driver.post_live_migration_at_destination(
8852                 context, instance, network_info, block_migration,
8853                 block_device_info)
8854         except Exception:
8855             with excutils.save_and_reraise_exception():
8856                 instance.vm_state = vm_states.ERROR
8857                 LOG.error('Unexpected error during post live migration at '
8858                           'destination host.', instance=instance)
8859         finally:
8860             # Restore instance state and update host
8861             current_power_state = self._get_power_state(instance)
8862             node_name = None
8863             prev_host = instance.host
8864             try:
8865                 compute_node = self._get_compute_info(context, self.host)
8866                 node_name = compute_node.hypervisor_hostname
8867             except exception.ComputeHostNotFound:
8868                 LOG.exception('Failed to get compute_info for %s', self.host)
8869             finally:
8870                 # NOTE(artom) We need to apply the migration context here
8871                 # regardless of whether the driver's
8872                 # post_live_migration_at_destination succeeded or not: the
8873                 # instance is on the destination, potentially with a new NUMA
8874                 # topology and resource usage. We need to persist that.
8875                 # NOTE(artom) Apply followed by drop looks weird, but apply
8876                 # just saves the new fields while drop actually removes the
8877                 # migration context from the instance.
8878                 instance.apply_migration_context()
8879                 instance.drop_migration_context()
8880                 instance.host = self.host
8881                 instance.power_state = current_power_state
8882                 instance.task_state = None
8883                 instance.node = node_name
8884                 instance.progress = 0
8885                 instance.save(expected_task_state=task_states.MIGRATING)
8886 
8887         # NOTE(tr3buchet): tear down networks on source host (nova-net)
8888         # NOTE(mriedem): For neutron, this will delete any inactive source
8889         # host port bindings.
8890         try:
8891             self.network_api.setup_networks_on_host(context, instance,
8892                                                     prev_host, teardown=True)
8893         except exception.PortBindingDeletionFailed as e:
8894             # Removing the inactive port bindings from the source host is not
8895             # critical so just log an error but don't fail.
8896             LOG.error('Network cleanup failed for source host %s during post '
8897                       'live migration. You may need to manually clean up '
8898                       'resources in the network service. Error: %s',
8899                       prev_host, str(e))
8900         # NOTE(vish): this is necessary to update dhcp for nova-network
8901         # NOTE(mriedem): This is a no-op for neutron.
8902         self.network_api.setup_networks_on_host(context, instance, self.host)
8903         self._notify_about_instance_usage(
8904                      context, instance, "live_migration.post.dest.end",
8905                      network_info=network_info)
8906         compute_utils.notify_about_instance_action(context, instance,
8907                 self.host,
8908                 action=fields.NotificationAction.LIVE_MIGRATION_POST_DEST,
8909                 phase=fields.NotificationPhase.END)
8910 
8911     def _remove_remote_volume_connections(self, context, dest, bdms, instance):
8912         """Rollback remote volume connections on the dest"""
8913         for bdm in bdms:
8914             try:
8915                 # remove the connection on the destination host
8916                 # NOTE(lyarwood): This actually calls the cinderv2
8917                 # os-terminate_connection API if required.
8918                 self.compute_rpcapi.remove_volume_connection(
8919                         context, instance, bdm.volume_id, dest)
8920             except Exception:
8921                 LOG.warning("Ignoring exception while attempting "
8922                             "to rollback volume connections for "
8923                             "volume %s on host %s.", bdm.volume_id,
8924                             dest, instance=instance)
8925 
8926     def _rollback_volume_bdms(self, context, bdms, original_bdms, instance):
8927         """Rollback the connection_info and attachment_id for each bdm"""
8928         original_bdms_by_volid = {bdm.volume_id: bdm for bdm in original_bdms
8929                                   if bdm.is_volume}
8930         for bdm in bdms:
8931             try:
8932                 original_bdm = original_bdms_by_volid[bdm.volume_id]
8933                 # NOTE(lyarwood): Only delete the referenced attachment if it
8934                 # is different to the original in order to avoid accidentally
8935                 # removing the source host volume attachment after it has
8936                 # already been rolled back by a failure in pre_live_migration.
8937                 if (bdm.attachment_id and original_bdm.attachment_id and
8938                     bdm.attachment_id != original_bdm.attachment_id):
8939                     # NOTE(lyarwood): 3.44 cinder api flow. Delete the
8940                     # attachment used by the bdm and reset it to that of
8941                     # the original bdm.
8942                     self.volume_api.attachment_delete(context,
8943                                                       bdm.attachment_id)
8944                     bdm.attachment_id = original_bdm.attachment_id
8945                 # NOTE(lyarwood): Reset the connection_info to the original
8946                 bdm.connection_info = original_bdm.connection_info
8947                 bdm.save()
8948             except cinder_exception.ClientException:
8949                 LOG.warning("Ignoring cinderclient exception when "
8950                             "attempting to delete attachment %s for volume "
8951                             "%s while rolling back volume bdms.",
8952                             bdm.attachment_id, bdm.volume_id,
8953                             instance=instance)
8954             except Exception:
8955                 with excutils.save_and_reraise_exception():
8956                     LOG.exception("Exception while attempting to rollback "
8957                                   "BDM for volume %s.", bdm.volume_id,
8958                                   instance=instance)
8959 
8960     @wrap_exception()
8961     @wrap_instance_fault
8962     def _rollback_live_migration(self, context, instance,
8963                                  dest, migrate_data=None,
8964                                  migration_status='failed',
8965                                  source_bdms=None):
8966         """Recovers Instance/volume state from migrating -> running.
8967 
8968         :param context: security context
8969         :param instance: nova.objects.instance.Instance object
8970         :param dest:
8971             This method is called from live migration src host.
8972             This param specifies destination host.
8973         :param migrate_data:
8974             if not none, contains implementation specific data.
8975         :param migration_status:
8976             Contains the status we want to set for the migration object
8977         :param source_bdms: BDMs prior to modification by the destination
8978                             compute host. Set by _do_live_migration and not
8979                             part of the callback interface, so this is never
8980                             None
8981 
8982         """
8983         # NOTE(gibi): We need to refresh pci_requests of the instance as it
8984         # might be changed by the conductor during scheduling based on the
8985         # selected destination host. If the instance has SRIOV ports with
8986         # resource request then the LiveMigrationTask._find_destination call
8987         # updated the instance.pci_requests.requests[].spec with the SRIOV PF
8988         # device name to be used on the destination host. As the migration is
8989         # rolling back to the source host now we don't want to persist the
8990         # destination host related changes in the DB.
8991         instance.pci_requests = \
8992             objects.InstancePCIRequests.get_by_instance_uuid(
8993                 context, instance.uuid)
8994 
8995         if (isinstance(migrate_data, migrate_data_obj.LiveMigrateData) and
8996               migrate_data.obj_attr_is_set('migration')):
8997             migration = migrate_data.migration
8998         else:
8999             migration = None
9000 
9001         if migration:
9002             # Remove allocations created in Placement for the dest node.
9003             # If migration is None, the virt driver didn't pass it which is
9004             # a bug.
9005             self._revert_allocation(context, instance, migration)
9006         else:
9007             LOG.error('Unable to revert allocations during live migration '
9008                       'rollback; compute driver did not provide migrate_data',
9009                       instance=instance)
9010 
9011         # NOTE(tr3buchet): setup networks on source host (really it's re-setup
9012         #                  for nova-network)
9013         # NOTE(mriedem): This is a no-op for neutron.
9014         self.network_api.setup_networks_on_host(context, instance, self.host)
9015         self.driver.rollback_live_migration_at_source(context, instance,
9016                                                       migrate_data)
9017 
9018         # NOTE(lyarwood): Fetch the current list of BDMs, disconnect any
9019         # connected volumes from the dest and delete any volume attachments
9020         # used by the destination host before rolling back to the original
9021         # still valid source host volume attachments.
9022         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
9023                 context, instance.uuid)
9024         # TODO(lyarwood): Turn the following into a lookup method within
9025         # BlockDeviceMappingList.
9026         vol_bdms = [bdm for bdm in bdms if bdm.is_volume]
9027         self._remove_remote_volume_connections(context, dest, vol_bdms,
9028                                                instance)
9029         self._rollback_volume_bdms(context, vol_bdms, source_bdms, instance)
9030 
9031         self._notify_about_instance_usage(context, instance,
9032                                           "live_migration._rollback.start")
9033         compute_utils.notify_about_instance_action(context, instance,
9034                 self.host,
9035                 action=fields.NotificationAction.LIVE_MIGRATION_ROLLBACK,
9036                 phase=fields.NotificationPhase.START,
9037                 bdms=bdms)
9038 
9039         do_cleanup, destroy_disks = self._live_migration_cleanup_flags(
9040                 migrate_data, migr_ctxt=instance.migration_context)
9041 
9042         if do_cleanup:
9043             self.compute_rpcapi.rollback_live_migration_at_destination(
9044                     context, instance, dest, destroy_disks=destroy_disks,
9045                     migrate_data=migrate_data)
9046         else:
9047             # The port binding profiles need to be cleaned up.
9048             with errors_out_migration_ctxt(migration):
9049                 try:
9050                     # This call will delete any inactive destination host
9051                     # port bindings.
9052                     self.network_api.setup_networks_on_host(
9053                         context, instance, host=dest, teardown=True)
9054                 except exception.PortBindingDeletionFailed as e:
9055                     # Removing the inactive port bindings from the destination
9056                     # host is not critical so just log an error but don't fail.
9057                     LOG.error(
9058                         'Network cleanup failed for destination host %s '
9059                         'during live migration rollback. You may need to '
9060                         'manually clean up resources in the network service. '
9061                         'Error: %s', dest, str(e))
9062                 except Exception:
9063                     with excutils.save_and_reraise_exception():
9064                         LOG.exception(
9065                             'An error occurred while cleaning up networking '
9066                             'during live migration rollback.',
9067                             instance=instance)
9068 
9069         # NOTE(luyao): We drop move_claim and migration_context after cleanup
9070         # is complete, to ensure the specific resources claimed on destination
9071         # are released safely.
9072         # TODO(artom) drop_move_claim_at_destination() is new in RPC 5.3, only
9073         # call it if we performed a NUMA-aware live migration (which implies us
9074         # being able to send RPC 5.3). To check this, we can use the
9075         # src_supports_numa_live_migration flag, as it will be set if and only
9076         # if:
9077         # - dst_supports_numa_live_migration made its way to the source
9078         #   (meaning both dest and source are new and conductor can speak
9079         #   RPC 5.3)
9080         # - src_supports_numa_live_migration was set by the source driver and
9081         #   passed the send-RPC-5.3 check.
9082         # This check can be removed in RPC 6.0.
9083         if ('src_supports_numa_live_migration' in migrate_data and
9084                 migrate_data.src_supports_numa_live_migration):
9085             LOG.debug('Calling destination to drop move claim.',
9086                       instance=instance)
9087             self.compute_rpcapi.drop_move_claim_at_destination(context,
9088                                                                instance, dest)
9089 
9090         # NOTE(luyao): We only update instance info after rollback operations
9091         # are complete
9092         instance.task_state = None
9093         instance.progress = 0
9094         instance.drop_migration_context()
9095         instance.save(expected_task_state=[task_states.MIGRATING])
9096 
9097         self._notify_about_instance_usage(context, instance,
9098                                           "live_migration._rollback.end")
9099         compute_utils.notify_about_instance_action(context, instance,
9100                 self.host,
9101                 action=fields.NotificationAction.LIVE_MIGRATION_ROLLBACK,
9102                 phase=fields.NotificationPhase.END,
9103                 bdms=bdms)
9104 
9105         # NOTE(luyao): we have cleanup everything and get instance
9106         # back to normal status, now set migration status to 'failed'
9107         self._set_migration_status(migration, migration_status)
9108 
9109     @wrap_exception()
9110     @wrap_instance_fault
9111     def drop_move_claim_at_destination(self, context, instance):
9112         """Called by the source of a live migration during rollback to ask the
9113         destination to drop the MoveClaim object that was created for the live
9114         migration on the destination.
9115         """
9116         nodename = self._get_nodename(instance)
9117         LOG.debug('Dropping live migration resource claim on destination '
9118                   'node %s', nodename, instance=instance)
9119         self.rt.drop_move_claim(
9120             context, instance, nodename, instance_type=instance.flavor)
9121 
9122     @wrap_exception()
9123     @wrap_instance_event(prefix='compute')
9124     @wrap_instance_fault
9125     def rollback_live_migration_at_destination(self, context, instance,
9126                                                destroy_disks,
9127                                                migrate_data):
9128         """Cleaning up image directory that is created pre_live_migration.
9129 
9130         :param context: security context
9131         :param instance: a nova.objects.instance.Instance object sent over rpc
9132         :param destroy_disks: whether to destroy volumes or not
9133         :param migrate_data: contains migration info
9134         """
9135         network_info = self.network_api.get_instance_nw_info(context, instance)
9136         self._notify_about_instance_usage(
9137                       context, instance, "live_migration.rollback.dest.start",
9138                       network_info=network_info)
9139         compute_utils.notify_about_instance_action(
9140             context, instance, self.host,
9141             action=fields.NotificationAction.LIVE_MIGRATION_ROLLBACK_DEST,
9142             phase=fields.NotificationPhase.START)
9143         try:
9144             # NOTE(tr3buchet): tear down networks on dest host (nova-net)
9145             # NOTE(mriedem): For neutron, this call will delete any
9146             # destination host port bindings.
9147             # TODO(mriedem): We should eventually remove this call from
9148             # this method (rollback_live_migration_at_destination) since this
9149             # method is only called conditionally based on whether or not the
9150             # instance is running on shared storage. _rollback_live_migration
9151             # already calls this method for neutron if we are running on
9152             # shared storage.
9153             self.network_api.setup_networks_on_host(context, instance,
9154                                                     self.host, teardown=True)
9155         except exception.PortBindingDeletionFailed as e:
9156             # Removing the inactive port bindings from the destination
9157             # host is not critical so just log an error but don't fail.
9158             LOG.error(
9159                 'Network cleanup failed for destination host %s '
9160                 'during live migration rollback. You may need to '
9161                 'manually clean up resources in the network service. '
9162                 'Error: %s', self.host, str(e))
9163         except Exception:
9164             with excutils.save_and_reraise_exception():
9165                 # NOTE(tdurakov): even if teardown networks fails driver
9166                 # should try to rollback live migration on destination.
9167                 LOG.exception('An error occurred while deallocating network.',
9168                               instance=instance)
9169         finally:
9170             # always run this even if setup_networks_on_host fails
9171             # NOTE(vish): The mapping is passed in so the driver can disconnect
9172             #             from remote volumes if necessary
9173             block_device_info = self._get_instance_block_device_info(context,
9174                                                                      instance)
9175             # free any instance PCI claims done on destination during
9176             # check_can_live_migrate_destination()
9177             self.rt.free_pci_device_claims_for_instance(context, instance)
9178 
9179             # NOTE(luyao): Apply migration_context temporarily since it's
9180             # on destination host, we rely on instance object to cleanup
9181             # specific resources like vpmem
9182             with instance.mutated_migration_context():
9183                 self.driver.rollback_live_migration_at_destination(
9184                     context, instance, network_info, block_device_info,
9185                     destroy_disks=destroy_disks, migrate_data=migrate_data)
9186 
9187         self._notify_about_instance_usage(
9188                         context, instance, "live_migration.rollback.dest.end",
9189                         network_info=network_info)
9190         compute_utils.notify_about_instance_action(
9191             context, instance, self.host,
9192             action=fields.NotificationAction.LIVE_MIGRATION_ROLLBACK_DEST,
9193             phase=fields.NotificationPhase.END)
9194 
9195     def _require_nw_info_update(self, context, instance):
9196         """Detect whether there is a mismatch in binding:host_id, or
9197         binding_failed or unbound binding:vif_type for any of the instances
9198         ports.
9199         """
9200         # Only update port bindings if compute manager does manage port
9201         # bindings instead of the compute driver. For example IronicDriver
9202         # manages the port binding for baremetal instance ports, hence,
9203         # external intervention with the binding is not desired.
9204         if self.driver.manages_network_binding_host_id():
9205             return False
9206 
9207         search_opts = {'device_id': instance.uuid,
9208                        'fields': ['binding:host_id', 'binding:vif_type']}
9209         ports = self.network_api.list_ports(context, **search_opts)
9210         for p in ports['ports']:
9211             if p.get('binding:host_id') != self.host:
9212                 return True
9213             vif_type = p.get('binding:vif_type')
9214             if (vif_type == network_model.VIF_TYPE_UNBOUND or
9215                     vif_type == network_model.VIF_TYPE_BINDING_FAILED):
9216                 return True
9217         return False
9218 
9219     @periodic_task.periodic_task(
9220         spacing=CONF.heal_instance_info_cache_interval)
9221     def _heal_instance_info_cache(self, context):
9222         """Called periodically.  On every call, try to update the
9223         info_cache's network information for another instance by
9224         calling to the network manager.
9225 
9226         This is implemented by keeping a cache of uuids of instances
9227         that live on this host.  On each call, we pop one off of a
9228         list, pull the DB record, and try the call to the network API.
9229         If anything errors don't fail, as it's possible the instance
9230         has been deleted, etc.
9231         """
9232         heal_interval = CONF.heal_instance_info_cache_interval
9233         if not heal_interval:
9234             return
9235 
9236         instance_uuids = getattr(self, '_instance_uuids_to_heal', [])
9237         instance = None
9238 
9239         LOG.debug('Starting heal instance info cache')
9240 
9241         if not instance_uuids:
9242             # The list of instances to heal is empty so rebuild it
9243             LOG.debug('Rebuilding the list of instances to heal')
9244             db_instances = objects.InstanceList.get_by_host(
9245                 context, self.host, expected_attrs=[], use_slave=True)
9246             for inst in db_instances:
9247                 # We don't want to refresh the cache for instances
9248                 # which are building or deleting so don't put them
9249                 # in the list. If they are building they will get
9250                 # added to the list next time we build it.
9251                 if (inst.vm_state == vm_states.BUILDING):
9252                     LOG.debug('Skipping network cache update for instance '
9253                               'because it is Building.', instance=inst)
9254                     continue
9255                 if (inst.task_state == task_states.DELETING):
9256                     LOG.debug('Skipping network cache update for instance '
9257                               'because it is being deleted.', instance=inst)
9258                     continue
9259 
9260                 if not instance:
9261                     # Save the first one we find so we don't
9262                     # have to get it again
9263                     instance = inst
9264                 else:
9265                     instance_uuids.append(inst['uuid'])
9266 
9267             self._instance_uuids_to_heal = instance_uuids
9268         else:
9269             # Find the next valid instance on the list
9270             while instance_uuids:
9271                 try:
9272                     inst = objects.Instance.get_by_uuid(
9273                             context, instance_uuids.pop(0),
9274                             expected_attrs=['system_metadata', 'info_cache',
9275                                             'flavor'],
9276                             use_slave=True)
9277                 except exception.InstanceNotFound:
9278                     # Instance is gone.  Try to grab another.
9279                     continue
9280 
9281                 # Check the instance hasn't been migrated
9282                 if inst.host != self.host:
9283                     LOG.debug('Skipping network cache update for instance '
9284                               'because it has been migrated to another '
9285                               'host.', instance=inst)
9286                 # Check the instance isn't being deleting
9287                 elif inst.task_state == task_states.DELETING:
9288                     LOG.debug('Skipping network cache update for instance '
9289                               'because it is being deleted.', instance=inst)
9290                 else:
9291                     instance = inst
9292                     break
9293 
9294         if instance:
9295             # We have an instance now to refresh
9296             try:
9297                 # Fix potential mismatch in port binding if evacuation failed
9298                 # after reassigning the port binding to the dest host but
9299                 # before the instance host is changed.
9300                 # Do this only when instance has no pending task.
9301                 if instance.task_state is None and \
9302                         self._require_nw_info_update(context, instance):
9303                     LOG.info("Updating ports in neutron", instance=instance)
9304                     self.network_api.setup_instance_network_on_host(
9305                         context, instance, self.host)
9306                 # Call to network API to get instance info.. this will
9307                 # force an update to the instance's info_cache
9308                 self.network_api.get_instance_nw_info(
9309                     context, instance, force_refresh=True)
9310                 LOG.debug('Updated the network info_cache for instance',
9311                           instance=instance)
9312             except exception.InstanceNotFound:
9313                 # Instance is gone.
9314                 LOG.debug('Instance no longer exists. Unable to refresh',
9315                           instance=instance)
9316                 return
9317             except exception.InstanceInfoCacheNotFound:
9318                 # InstanceInfoCache is gone.
9319                 LOG.debug('InstanceInfoCache no longer exists. '
9320                           'Unable to refresh', instance=instance)
9321             except Exception:
9322                 LOG.error('An error occurred while refreshing the network '
9323                           'cache.', instance=instance, exc_info=True)
9324         else:
9325             LOG.debug("Didn't find any instances for network info cache "
9326                       "update.")
9327 
9328     @periodic_task.periodic_task
9329     def _poll_rebooting_instances(self, context):
9330         if CONF.reboot_timeout > 0:
9331             filters = {'task_state':
9332                        [task_states.REBOOTING,
9333                         task_states.REBOOT_STARTED,
9334                         task_states.REBOOT_PENDING],
9335                        'host': self.host}
9336             rebooting = objects.InstanceList.get_by_filters(
9337                 context, filters, expected_attrs=[], use_slave=True)
9338 
9339             to_poll = []
9340             for instance in rebooting:
9341                 if timeutils.is_older_than(instance.updated_at,
9342                                            CONF.reboot_timeout):
9343                     to_poll.append(instance)
9344 
9345             self.driver.poll_rebooting_instances(CONF.reboot_timeout, to_poll)
9346 
9347     @periodic_task.periodic_task
9348     def _poll_rescued_instances(self, context):
9349         if CONF.rescue_timeout > 0:
9350             filters = {'vm_state': vm_states.RESCUED,
9351                        'host': self.host}
9352             rescued_instances = objects.InstanceList.get_by_filters(
9353                 context, filters, expected_attrs=["system_metadata"],
9354                 use_slave=True)
9355 
9356             to_unrescue = []
9357             for instance in rescued_instances:
9358                 if timeutils.is_older_than(instance.launched_at,
9359                                            CONF.rescue_timeout):
9360                     to_unrescue.append(instance)
9361 
9362             for instance in to_unrescue:
9363                 self.compute_api.unrescue(context, instance)
9364 
9365     @periodic_task.periodic_task
9366     def _poll_unconfirmed_resizes(self, context):
9367         if CONF.resize_confirm_window == 0:
9368             return
9369 
9370         migrations = objects.MigrationList.get_unconfirmed_by_dest_compute(
9371                 context, CONF.resize_confirm_window, self.host,
9372                 use_slave=True)
9373 
9374         migrations_info = dict(migration_count=len(migrations),
9375                 confirm_window=CONF.resize_confirm_window)
9376 
9377         if migrations_info["migration_count"] > 0:
9378             LOG.info("Found %(migration_count)d unconfirmed migrations "
9379                      "older than %(confirm_window)d seconds",
9380                      migrations_info)
9381 
9382         def _set_migration_to_error(migration, reason, **kwargs):
9383             LOG.warning("Setting migration %(migration_id)s to error: "
9384                         "%(reason)s",
9385                         {'migration_id': migration['id'], 'reason': reason},
9386                         **kwargs)
9387             migration.status = 'error'
9388             migration.save()
9389 
9390         for migration in migrations:
9391             instance_uuid = migration.instance_uuid
9392             LOG.info("Automatically confirming migration "
9393                      "%(migration_id)s for instance %(instance_uuid)s",
9394                      {'migration_id': migration.id,
9395                       'instance_uuid': instance_uuid})
9396             expected_attrs = ['metadata', 'system_metadata']
9397             try:
9398                 instance = objects.Instance.get_by_uuid(context,
9399                             instance_uuid, expected_attrs=expected_attrs,
9400                             use_slave=True)
9401             except exception.InstanceNotFound:
9402                 reason = (_("Instance %s not found") %
9403                           instance_uuid)
9404                 _set_migration_to_error(migration, reason)
9405                 continue
9406             if instance.vm_state == vm_states.ERROR:
9407                 reason = _("In ERROR state")
9408                 _set_migration_to_error(migration, reason,
9409                                         instance=instance)
9410                 continue
9411             # race condition: The instance in DELETING state should not be
9412             # set the migration state to error, otherwise the instance in
9413             # to be deleted which is in RESIZED state
9414             # will not be able to confirm resize
9415             if instance.task_state in [task_states.DELETING,
9416                                        task_states.SOFT_DELETING]:
9417                 msg = ("Instance being deleted or soft deleted during resize "
9418                        "confirmation. Skipping.")
9419                 LOG.debug(msg, instance=instance)
9420                 continue
9421 
9422             # race condition: This condition is hit when this method is
9423             # called between the save of the migration record with a status of
9424             # finished and the save of the instance object with a state of
9425             # RESIZED. The migration record should not be set to error.
9426             if instance.task_state == task_states.RESIZE_FINISH:
9427                 msg = ("Instance still resizing during resize "
9428                        "confirmation. Skipping.")
9429                 LOG.debug(msg, instance=instance)
9430                 continue
9431 
9432             vm_state = instance.vm_state
9433             task_state = instance.task_state
9434             if vm_state != vm_states.RESIZED or task_state is not None:
9435                 reason = (_("In states %(vm_state)s/%(task_state)s, not "
9436                            "RESIZED/None") %
9437                           {'vm_state': vm_state,
9438                            'task_state': task_state})
9439                 _set_migration_to_error(migration, reason,
9440                                         instance=instance)
9441                 continue
9442             try:
9443                 self.compute_api.confirm_resize(context, instance,
9444                                                 migration=migration)
9445             except Exception as e:
9446                 LOG.info("Error auto-confirming resize: %s. "
9447                          "Will retry later.", e, instance=instance)
9448 
9449     @periodic_task.periodic_task(spacing=CONF.shelved_poll_interval)
9450     def _poll_shelved_instances(self, context):
9451 
9452         if CONF.shelved_offload_time <= 0:
9453             return
9454 
9455         filters = {'vm_state': vm_states.SHELVED,
9456                    'task_state': None,
9457                    'host': self.host}
9458         shelved_instances = objects.InstanceList.get_by_filters(
9459             context, filters=filters, expected_attrs=['system_metadata'],
9460             use_slave=True)
9461 
9462         to_gc = []
9463         for instance in shelved_instances:
9464             sys_meta = instance.system_metadata
9465             shelved_at = timeutils.parse_strtime(sys_meta['shelved_at'])
9466             if timeutils.is_older_than(shelved_at, CONF.shelved_offload_time):
9467                 to_gc.append(instance)
9468 
9469         for instance in to_gc:
9470             try:
9471                 instance.task_state = task_states.SHELVING_OFFLOADING
9472                 instance.save(expected_task_state=(None,))
9473                 self.shelve_offload_instance(context, instance,
9474                                              clean_shutdown=False)
9475             except Exception:
9476                 LOG.exception('Periodic task failed to offload instance.',
9477                               instance=instance)
9478 
9479     @periodic_task.periodic_task
9480     def _instance_usage_audit(self, context):
9481         if not CONF.instance_usage_audit:
9482             return
9483 
9484         begin, end = utils.last_completed_audit_period()
9485         if objects.TaskLog.get(context, 'instance_usage_audit', begin, end,
9486                                self.host):
9487             return
9488 
9489         instances = objects.InstanceList.get_active_by_window_joined(
9490             context, begin, end, host=self.host,
9491             expected_attrs=['system_metadata', 'info_cache', 'metadata',
9492                             'flavor'],
9493             use_slave=True)
9494         num_instances = len(instances)
9495         errors = 0
9496         successes = 0
9497         LOG.info("Running instance usage audit for host %(host)s "
9498                  "from %(begin_time)s to %(end_time)s. "
9499                  "%(number_instances)s instances.",
9500                  {'host': self.host,
9501                   'begin_time': begin,
9502                   'end_time': end,
9503                   'number_instances': num_instances})
9504         start_time = time.time()
9505         task_log = objects.TaskLog(context)
9506         task_log.task_name = 'instance_usage_audit'
9507         task_log.period_beginning = begin
9508         task_log.period_ending = end
9509         task_log.host = self.host
9510         task_log.task_items = num_instances
9511         task_log.message = 'Instance usage audit started...'
9512         task_log.begin_task()
9513         for instance in instances:
9514             try:
9515                 compute_utils.notify_usage_exists(
9516                     self.notifier, context, instance, self.host,
9517                     ignore_missing_network_data=False)
9518                 successes += 1
9519             except Exception:
9520                 LOG.exception('Failed to generate usage '
9521                               'audit for instance '
9522                               'on host %s', self.host,
9523                               instance=instance)
9524                 errors += 1
9525         task_log.errors = errors
9526         task_log.message = (
9527             'Instance usage audit ran for host %s, %s instances in %s seconds.'
9528             % (self.host, num_instances, time.time() - start_time))
9529         task_log.end_task()
9530 
9531     def _get_host_volume_bdms(self, context, use_slave=False):
9532         """Return all block device mappings on a compute host."""
9533         compute_host_bdms = []
9534         instances = objects.InstanceList.get_by_host(context, self.host,
9535             use_slave=use_slave)
9536         for instance in instances:
9537             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
9538                     context, instance.uuid, use_slave=use_slave)
9539             instance_bdms = [bdm for bdm in bdms if bdm.is_volume]
9540             compute_host_bdms.append(dict(instance=instance,
9541                                           instance_bdms=instance_bdms))
9542 
9543         return compute_host_bdms
9544 
9545     def _update_volume_usage_cache(self, context, vol_usages):
9546         """Updates the volume usage cache table with a list of stats."""
9547         for usage in vol_usages:
9548             # Allow switching of greenthreads between queries.
9549             greenthread.sleep(0)
9550             vol_usage = objects.VolumeUsage(context)
9551             vol_usage.volume_id = usage['volume']
9552             vol_usage.instance_uuid = usage['instance'].uuid
9553             vol_usage.project_id = usage['instance'].project_id
9554             vol_usage.user_id = usage['instance'].user_id
9555             vol_usage.availability_zone = usage['instance'].availability_zone
9556             vol_usage.curr_reads = usage['rd_req']
9557             vol_usage.curr_read_bytes = usage['rd_bytes']
9558             vol_usage.curr_writes = usage['wr_req']
9559             vol_usage.curr_write_bytes = usage['wr_bytes']
9560             vol_usage.save()
9561             self.notifier.info(context, 'volume.usage', vol_usage.to_dict())
9562             compute_utils.notify_about_volume_usage(context, vol_usage,
9563                                                     self.host)
9564 
9565     @periodic_task.periodic_task(spacing=CONF.volume_usage_poll_interval)
9566     def _poll_volume_usage(self, context):
9567         if CONF.volume_usage_poll_interval == 0:
9568             return
9569 
9570         compute_host_bdms = self._get_host_volume_bdms(context,
9571                                                        use_slave=True)
9572         if not compute_host_bdms:
9573             return
9574 
9575         LOG.debug("Updating volume usage cache")
9576         try:
9577             vol_usages = self.driver.get_all_volume_usage(context,
9578                                                           compute_host_bdms)
9579         except NotImplementedError:
9580             return
9581 
9582         self._update_volume_usage_cache(context, vol_usages)
9583 
9584     @periodic_task.periodic_task(spacing=CONF.sync_power_state_interval,
9585                                  run_immediately=True)
9586     def _sync_power_states(self, context):
9587         """Align power states between the database and the hypervisor.
9588 
9589         To sync power state data we make a DB call to get the number of
9590         virtual machines known by the hypervisor and if the number matches the
9591         number of virtual machines known by the database, we proceed in a lazy
9592         loop, one database record at a time, checking if the hypervisor has the
9593         same power state as is in the database.
9594         """
9595         db_instances = objects.InstanceList.get_by_host(context, self.host,
9596                                                         expected_attrs=[],
9597                                                         use_slave=True)
9598 
9599         try:
9600             num_vm_instances = self.driver.get_num_instances()
9601         except exception.VirtDriverNotReady as e:
9602             # If the virt driver is not ready, like ironic-api not being up
9603             # yet in the case of ironic, just log it and exit.
9604             LOG.info('Skipping _sync_power_states periodic task due to: %s', e)
9605             return
9606 
9607         num_db_instances = len(db_instances)
9608 
9609         if num_vm_instances != num_db_instances:
9610             LOG.warning("While synchronizing instance power states, found "
9611                         "%(num_db_instances)s instances in the database "
9612                         "and %(num_vm_instances)s instances on the "
9613                         "hypervisor.",
9614                         {'num_db_instances': num_db_instances,
9615                          'num_vm_instances': num_vm_instances})
9616 
9617         def _sync(db_instance):
9618             # NOTE(melwitt): This must be synchronized as we query state from
9619             #                two separate sources, the driver and the database.
9620             #                They are set (in stop_instance) and read, in sync.
9621             @utils.synchronized(db_instance.uuid)
9622             def query_driver_power_state_and_sync():
9623                 self._query_driver_power_state_and_sync(context, db_instance)
9624 
9625             try:
9626                 query_driver_power_state_and_sync()
9627             except Exception:
9628                 LOG.exception("Periodic sync_power_state task had an "
9629                               "error while processing an instance.",
9630                               instance=db_instance)
9631 
9632             self._syncs_in_progress.pop(db_instance.uuid)
9633 
9634         for db_instance in db_instances:
9635             # process syncs asynchronously - don't want instance locking to
9636             # block entire periodic task thread
9637             uuid = db_instance.uuid
9638             if uuid in self._syncs_in_progress:
9639                 LOG.debug('Sync already in progress for %s', uuid)
9640             else:
9641                 LOG.debug('Triggering sync for uuid %s', uuid)
9642                 self._syncs_in_progress[uuid] = True
9643                 self._sync_power_pool.spawn_n(_sync, db_instance)
9644 
9645     def _query_driver_power_state_and_sync(self, context, db_instance):
9646         if db_instance.task_state is not None:
9647             LOG.info("During sync_power_state the instance has a "
9648                      "pending task (%(task)s). Skip.",
9649                      {'task': db_instance.task_state}, instance=db_instance)
9650             return
9651         # No pending tasks. Now try to figure out the real vm_power_state.
9652         try:
9653             vm_instance = self.driver.get_info(db_instance)
9654             vm_power_state = vm_instance.state
9655         except exception.InstanceNotFound:
9656             vm_power_state = power_state.NOSTATE
9657         # Note(maoy): the above get_info call might take a long time,
9658         # for example, because of a broken libvirt driver.
9659         try:
9660             self._sync_instance_power_state(context,
9661                                             db_instance,
9662                                             vm_power_state,
9663                                             use_slave=True)
9664         except exception.InstanceNotFound:
9665             # NOTE(hanlind): If the instance gets deleted during sync,
9666             # silently ignore.
9667             pass
9668 
9669     def _stop_unexpected_shutdown_instance(self, context, vm_state,
9670                                            db_instance, orig_db_power_state):
9671         # this is an exceptional case; make sure our data is up
9672         # to date before slamming through a power off
9673         vm_instance = self.driver.get_info(db_instance,
9674                                            use_cache=False)
9675         vm_power_state = vm_instance.state
9676 
9677         # if it still looks off, go ahead and call stop()
9678         if vm_power_state in (power_state.SHUTDOWN,
9679                               power_state.CRASHED):
9680 
9681             LOG.warning("Instance shutdown by itself. Calling the "
9682                         "stop API. Current vm_state: %(vm_state)s, "
9683                         "current task_state: %(task_state)s, "
9684                         "original DB power_state: %(db_power_state)s, "
9685                         "current VM power_state: %(vm_power_state)s",
9686                         {'vm_state': vm_state,
9687                          'task_state': db_instance.task_state,
9688                          'db_power_state': orig_db_power_state,
9689                          'vm_power_state': vm_power_state},
9690                         instance=db_instance)
9691             try:
9692                 # Note(maoy): here we call the API instead of
9693                 # brutally updating the vm_state in the database
9694                 # to allow all the hooks and checks to be performed.
9695                 if db_instance.shutdown_terminate:
9696                     self.compute_api.delete(context, db_instance)
9697                 else:
9698                     self.compute_api.stop(context, db_instance)
9699             except Exception:
9700                 # Note(maoy): there is no need to propagate the error
9701                 # because the same power_state will be retrieved next
9702                 # time and retried.
9703                 # For example, there might be another task scheduled.
9704                 LOG.exception("error during stop() in sync_power_state.",
9705                               instance=db_instance)
9706 
9707     def _sync_instance_power_state(self, context, db_instance, vm_power_state,
9708                                    use_slave=False):
9709         """Align instance power state between the database and hypervisor.
9710 
9711         If the instance is not found on the hypervisor, but is in the database,
9712         then a stop() API will be called on the instance.
9713         """
9714 
9715         # We re-query the DB to get the latest instance info to minimize
9716         # (not eliminate) race condition.
9717         db_instance.refresh(use_slave=use_slave)
9718         db_power_state = db_instance.power_state
9719         vm_state = db_instance.vm_state
9720 
9721         if self.host != db_instance.host:
9722             # on the sending end of nova-compute _sync_power_state
9723             # may have yielded to the greenthread performing a live
9724             # migration; this in turn has changed the resident-host
9725             # for the VM; However, the instance is still active, it
9726             # is just in the process of migrating to another host.
9727             # This implies that the compute source must relinquish
9728             # control to the compute destination.
9729             LOG.info("During the sync_power process the "
9730                      "instance has moved from "
9731                      "host %(src)s to host %(dst)s",
9732                      {'src': db_instance.host,
9733                       'dst': self.host},
9734                      instance=db_instance)
9735             return
9736         elif db_instance.task_state is not None:
9737             # on the receiving end of nova-compute, it could happen
9738             # that the DB instance already report the new resident
9739             # but the actual VM has not showed up on the hypervisor
9740             # yet. In this case, let's allow the loop to continue
9741             # and run the state sync in a later round
9742             LOG.info("During sync_power_state the instance has a "
9743                      "pending task (%(task)s). Skip.",
9744                      {'task': db_instance.task_state},
9745                      instance=db_instance)
9746             return
9747 
9748         orig_db_power_state = db_power_state
9749         if vm_power_state != db_power_state:
9750             LOG.info('During _sync_instance_power_state the DB '
9751                      'power_state (%(db_power_state)s) does not match '
9752                      'the vm_power_state from the hypervisor '
9753                      '(%(vm_power_state)s). Updating power_state in the '
9754                      'DB to match the hypervisor.',
9755                      {'db_power_state': db_power_state,
9756                       'vm_power_state': vm_power_state},
9757                      instance=db_instance)
9758             # power_state is always updated from hypervisor to db
9759             db_instance.power_state = vm_power_state
9760             db_instance.save()
9761             db_power_state = vm_power_state
9762 
9763         # Note(maoy): Now resolve the discrepancy between vm_state and
9764         # vm_power_state. We go through all possible vm_states.
9765         if vm_state in (vm_states.BUILDING,
9766                         vm_states.RESCUED,
9767                         vm_states.RESIZED,
9768                         vm_states.SUSPENDED,
9769                         vm_states.ERROR):
9770             # TODO(maoy): we ignore these vm_state for now.
9771             pass
9772         elif vm_state == vm_states.ACTIVE:
9773             # The only rational power state should be RUNNING
9774             if vm_power_state in (power_state.SHUTDOWN,
9775                                   power_state.CRASHED):
9776                 self._stop_unexpected_shutdown_instance(
9777                     context, vm_state, db_instance, orig_db_power_state)
9778             elif vm_power_state == power_state.SUSPENDED:
9779                 LOG.warning("Instance is suspended unexpectedly. Calling "
9780                             "the stop API.", instance=db_instance)
9781                 try:
9782                     self.compute_api.stop(context, db_instance)
9783                 except Exception:
9784                     LOG.exception("error during stop() in sync_power_state.",
9785                                   instance=db_instance)
9786             elif vm_power_state == power_state.PAUSED:
9787                 # Note(maoy): a VM may get into the paused state not only
9788                 # because the user request via API calls, but also
9789                 # due to (temporary) external instrumentations.
9790                 # Before the virt layer can reliably report the reason,
9791                 # we simply ignore the state discrepancy. In many cases,
9792                 # the VM state will go back to running after the external
9793                 # instrumentation is done. See bug 1097806 for details.
9794                 LOG.warning("Instance is paused unexpectedly. Ignore.",
9795                             instance=db_instance)
9796             elif vm_power_state == power_state.NOSTATE:
9797                 # Occasionally, depending on the status of the hypervisor,
9798                 # which could be restarting for example, an instance may
9799                 # not be found.  Therefore just log the condition.
9800                 LOG.warning("Instance is unexpectedly not found. Ignore.",
9801                             instance=db_instance)
9802         elif vm_state == vm_states.STOPPED:
9803             if vm_power_state not in (power_state.NOSTATE,
9804                                       power_state.SHUTDOWN,
9805                                       power_state.CRASHED):
9806                 LOG.warning("Instance is not stopped. Calling "
9807                             "the stop API. Current vm_state: %(vm_state)s,"
9808                             " current task_state: %(task_state)s, "
9809                             "original DB power_state: %(db_power_state)s, "
9810                             "current VM power_state: %(vm_power_state)s",
9811                             {'vm_state': vm_state,
9812                              'task_state': db_instance.task_state,
9813                              'db_power_state': orig_db_power_state,
9814                              'vm_power_state': vm_power_state},
9815                             instance=db_instance)
9816                 try:
9817                     # NOTE(russellb) Force the stop, because normally the
9818                     # compute API would not allow an attempt to stop a stopped
9819                     # instance.
9820                     self.compute_api.force_stop(context, db_instance)
9821                 except Exception:
9822                     LOG.exception("error during stop() in sync_power_state.",
9823                                   instance=db_instance)
9824         elif vm_state == vm_states.PAUSED:
9825             if vm_power_state in (power_state.SHUTDOWN,
9826                                   power_state.CRASHED):
9827                 LOG.warning("Paused instance shutdown by itself. Calling "
9828                             "the stop API.", instance=db_instance)
9829                 try:
9830                     self.compute_api.force_stop(context, db_instance)
9831                 except Exception:
9832                     LOG.exception("error during stop() in sync_power_state.",
9833                                   instance=db_instance)
9834         elif vm_state in (vm_states.SOFT_DELETED,
9835                           vm_states.DELETED):
9836             if vm_power_state not in (power_state.NOSTATE,
9837                                       power_state.SHUTDOWN):
9838                 # Note(maoy): this should be taken care of periodically in
9839                 # _cleanup_running_deleted_instances().
9840                 LOG.warning("Instance is not (soft-)deleted.",
9841                             instance=db_instance)
9842 
9843     @periodic_task.periodic_task
9844     def _reclaim_queued_deletes(self, context):
9845         """Reclaim instances that are queued for deletion."""
9846         interval = CONF.reclaim_instance_interval
9847         if interval <= 0:
9848             LOG.debug("CONF.reclaim_instance_interval <= 0, skipping...")
9849             return
9850 
9851         filters = {'vm_state': vm_states.SOFT_DELETED,
9852                    'task_state': None,
9853                    'host': self.host}
9854         instances = objects.InstanceList.get_by_filters(
9855             context, filters,
9856             expected_attrs=objects.instance.INSTANCE_DEFAULT_FIELDS,
9857             use_slave=True)
9858         for instance in instances:
9859             if self._deleted_old_enough(instance, interval):
9860                 bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
9861                         context, instance.uuid)
9862                 LOG.info('Reclaiming deleted instance', instance=instance)
9863                 try:
9864                     self._delete_instance(context, instance, bdms)
9865                 except Exception as e:
9866                     LOG.warning("Periodic reclaim failed to delete "
9867                                 "instance: %s",
9868                                 e, instance=instance)
9869 
9870     def _get_nodename(self, instance, refresh=False):
9871         """Helper method to get the name of the first available node
9872         on this host. This method should not be used with any operations
9873         on ironic instances since it does not handle multiple nodes.
9874         """
9875         node = self.driver.get_available_nodes(refresh=refresh)[0]
9876         LOG.debug("No node specified, defaulting to %s", node,
9877                   instance=instance)
9878         return node
9879 
9880     def _update_available_resource_for_node(self, context, nodename,
9881                                             startup=False):
9882 
9883         try:
9884             self.rt.update_available_resource(context, nodename,
9885                                               startup=startup)
9886         except exception.ComputeHostNotFound:
9887             LOG.warning("Compute node '%s' not found in "
9888                         "update_available_resource.", nodename)
9889         except exception.ReshapeFailed:
9890             # We're only supposed to get here on startup, if a reshape was
9891             # needed, was attempted, and failed. We want to kill the service.
9892             with excutils.save_and_reraise_exception():
9893                 LOG.critical("Resource provider data migration failed "
9894                              "fatally during startup for node %s.", nodename)
9895         except exception.ReshapeNeeded:
9896             # This exception should only find its way here if the virt driver's
9897             # update_provider_tree raised it incorrectly: either
9898             # a) After the resource tracker already caught it once and
9899             # reinvoked update_provider_tree with allocations. At this point
9900             # the driver is just supposed to *do* the reshape, so if it raises
9901             # ReshapeNeeded, it's a bug, and we want to kill the compute
9902             # service.
9903             # b) On periodic rather than startup (we only allow reshapes to
9904             # happen on startup). In this case we'll just make the logs red and
9905             # go again at the next periodic interval, where the same thing may
9906             # or may not happen again. Depending on the previous and intended
9907             # shape of the providers/inventories, this may not actually cause
9908             # any immediately visible symptoms (in terms of scheduling, etc.)
9909             # If this becomes a problem, we may wish to make it pop immediately
9910             # (e.g. disable the service).
9911             with excutils.save_and_reraise_exception():
9912                 LOG.exception("ReshapeNeeded exception is unexpected here!")
9913         except Exception:
9914             LOG.exception("Error updating resources for node %(node)s.",
9915                           {'node': nodename})
9916 
9917     @periodic_task.periodic_task(spacing=CONF.update_resources_interval)
9918     def update_available_resource(self, context, startup=False):
9919         """See driver.get_available_resource()
9920 
9921         Periodic process that keeps that the compute host's understanding of
9922         resource availability and usage in sync with the underlying hypervisor.
9923 
9924         :param context: security context
9925         :param startup: True if this is being called when the nova-compute
9926             service is starting, False otherwise.
9927         """
9928         try:
9929             nodenames = set(self.driver.get_available_nodes())
9930         except exception.VirtDriverNotReady:
9931             LOG.warning("Virt driver is not ready.")
9932             return
9933 
9934         compute_nodes_in_db = self._get_compute_nodes_in_db(context,
9935                                                             nodenames,
9936                                                             use_slave=True,
9937                                                             startup=startup)
9938 
9939         # Delete orphan compute node not reported by driver but still in db
9940         for cn in compute_nodes_in_db:
9941             if cn.hypervisor_hostname not in nodenames:
9942                 LOG.info("Deleting orphan compute node %(id)s "
9943                          "hypervisor host is %(hh)s, "
9944                          "nodes are %(nodes)s",
9945                          {'id': cn.id, 'hh': cn.hypervisor_hostname,
9946                           'nodes': nodenames})
9947                 cn.destroy()
9948                 self.rt.remove_node(cn.hypervisor_hostname)
9949                 # Delete the corresponding resource provider in placement,
9950                 # along with any associated allocations.
9951                 try:
9952                     self.reportclient.delete_resource_provider(context, cn,
9953                                                                cascade=True)
9954                 except keystone_exception.ClientException as e:
9955                     LOG.error(
9956                         "Failed to delete compute node resource provider "
9957                         "for compute node %s: %s", cn.uuid, str(e))
9958 
9959         for nodename in nodenames:
9960             self._update_available_resource_for_node(context, nodename,
9961                                                      startup=startup)
9962 
9963     def _get_compute_nodes_in_db(self, context, nodenames, use_slave=False,
9964                                  startup=False):
9965         try:
9966             return objects.ComputeNodeList.get_all_by_host(context, self.host,
9967                                                            use_slave=use_slave)
9968         except exception.NotFound:
9969             # If the driver is not reporting any nodenames we should not
9970             # expect there to be compute nodes so we just return in that case.
9971             # For example, this could be an ironic compute and it is not
9972             # managing any nodes yet.
9973             if nodenames:
9974                 if startup:
9975                     LOG.warning(
9976                         "No compute node record found for host %s. If this is "
9977                         "the first time this service is starting on this "
9978                         "host, then you can ignore this warning.", self.host)
9979                 else:
9980                     LOG.error("No compute node record for host %s", self.host)
9981             return []
9982 
9983     @periodic_task.periodic_task(
9984         spacing=CONF.running_deleted_instance_poll_interval,
9985         run_immediately=True)
9986     def _cleanup_running_deleted_instances(self, context):
9987         """Cleanup any instances which are erroneously still running after
9988         having been deleted.
9989 
9990         Valid actions to take are:
9991 
9992             1. noop - do nothing
9993             2. log - log which instances are erroneously running
9994             3. reap - shutdown and cleanup any erroneously running instances
9995             4. shutdown - power off *and disable* any erroneously running
9996                           instances
9997 
9998         The use-case for this cleanup task is: for various reasons, it may be
9999         possible for the database to show an instance as deleted but for that
10000         instance to still be running on a host machine (see bug
10001         https://bugs.launchpad.net/nova/+bug/911366).
10002 
10003         This cleanup task is a cross-hypervisor utility for finding these
10004         zombied instances and either logging the discrepancy (likely what you
10005         should do in production), or automatically reaping the instances (more
10006         appropriate for dev environments).
10007         """
10008         action = CONF.running_deleted_instance_action
10009 
10010         if action == "noop":
10011             return
10012 
10013         # NOTE(sirp): admin contexts don't ordinarily return deleted records
10014         with utils.temporary_mutation(context, read_deleted="yes"):
10015 
10016             try:
10017                 instances = self._running_deleted_instances(context)
10018             except exception.VirtDriverNotReady:
10019                 # Since this task runs immediately on startup, if the
10020                 # hypervisor is not yet ready handle it gracefully.
10021                 LOG.debug('Unable to check for running deleted instances '
10022                           'at this time since the hypervisor is not ready.')
10023                 return
10024 
10025             for instance in instances:
10026                 if action == "log":
10027                     LOG.warning("Detected instance with name label "
10028                                 "'%s' which is marked as "
10029                                 "DELETED but still present on host.",
10030                                 instance.name, instance=instance)
10031 
10032                 elif action == 'shutdown':
10033                     LOG.info("Powering off instance with name label "
10034                              "'%s' which is marked as "
10035                              "DELETED but still present on host.",
10036                              instance.name, instance=instance)
10037                     try:
10038                         self.driver.power_off(instance)
10039                     except Exception:
10040                         LOG.warning("Failed to power off instance",
10041                                     instance=instance, exc_info=True)
10042 
10043                 elif action == 'reap':
10044                     LOG.info("Destroying instance with name label "
10045                              "'%s' which is marked as "
10046                              "DELETED but still present on host.",
10047                              instance.name, instance=instance)
10048                     bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
10049                         context, instance.uuid, use_slave=True)
10050                     self.instance_events.clear_events_for_instance(instance)
10051                     try:
10052                         self._shutdown_instance(context, instance, bdms,
10053                                                 notify=False)
10054                         self._cleanup_volumes(context, instance, bdms,
10055                                               detach=False)
10056                     except Exception as e:
10057                         LOG.warning("Periodic cleanup failed to delete "
10058                                     "instance: %s",
10059                                     e, instance=instance)
10060                 else:
10061                     raise Exception(_("Unrecognized value '%s'"
10062                                       " for CONF.running_deleted_"
10063                                       "instance_action") % action)
10064 
10065     def _running_deleted_instances(self, context):
10066         """Returns a list of instances nova thinks is deleted,
10067         but the hypervisor thinks is still running.
10068         """
10069         timeout = CONF.running_deleted_instance_timeout
10070         filters = {'deleted': True,
10071                    'soft_deleted': False}
10072         instances = self._get_instances_on_driver(context, filters)
10073         return [i for i in instances if self._deleted_old_enough(i, timeout)]
10074 
10075     def _deleted_old_enough(self, instance, timeout):
10076         deleted_at = instance.deleted_at
10077         if deleted_at:
10078             deleted_at = deleted_at.replace(tzinfo=None)
10079         return (not deleted_at or timeutils.is_older_than(deleted_at, timeout))
10080 
10081     @contextlib.contextmanager
10082     def _error_out_instance_on_exception(self, context, instance,
10083                                          instance_state=vm_states.ACTIVE):
10084         """Context manager to set instance.vm_state after some operation raises
10085 
10086         Used to handle NotImplementedError and InstanceFaultRollback errors
10087         and reset the instance vm_state and task_state. The vm_state is set
10088         to the $instance_state parameter and task_state is set to None.
10089         For all other types of exceptions, the vm_state is set to ERROR and
10090         the task_state is left unchanged (although most callers will have the
10091         @reverts_task_state decorator which will set the task_state to None).
10092 
10093         Re-raises the original exception *except* in the case of
10094         InstanceFaultRollback in which case the wrapped `inner_exception` is
10095         re-raised.
10096 
10097         :param context: The nova auth request context for the operation.
10098         :param instance: The instance to update. The vm_state will be set by
10099             this context manager when an exception is raised.
10100         :param instance_state: For NotImplementedError and
10101             InstanceFaultRollback this is the vm_state to set the instance to
10102             when handling one of those types of exceptions. By default the
10103             instance will be set to ACTIVE, but the caller should control this
10104             in case there have been no changes to the running state of the
10105             instance. For example, resizing a stopped server where prep_resize
10106             fails early and does not change the power state of the guest should
10107             not set the instance status to ACTIVE but remain STOPPED.
10108             This parameter is ignored for all other types of exceptions and the
10109             instance vm_state is set to ERROR.
10110         """
10111         # NOTE(mriedem): Why doesn't this method just save off the
10112         # original instance.vm_state here rather than use a parameter? Or use
10113         # instance_state=None as an override but default to the current
10114         # vm_state when rolling back.
10115         instance_uuid = instance.uuid
10116         try:
10117             yield
10118         except (NotImplementedError, exception.InstanceFaultRollback) as error:
10119             # Use reraise=False to determine if we want to raise the original
10120             # exception or something else.
10121             with excutils.save_and_reraise_exception(reraise=False) as ctxt:
10122                 LOG.info("Setting instance back to %(state)s after: %(error)s",
10123                          {'state': instance_state, 'error': error},
10124                          instance_uuid=instance_uuid)
10125                 self._instance_update(context, instance,
10126                                       vm_state=instance_state,
10127                                       task_state=None)
10128                 if isinstance(error, exception.InstanceFaultRollback):
10129                     # Raise the wrapped exception.
10130                     raise error.inner_exception
10131                 # Else re-raise the NotImplementedError.
10132                 ctxt.reraise = True
10133         except Exception:
10134             LOG.exception('Setting instance vm_state to ERROR',
10135                           instance_uuid=instance_uuid)
10136             with excutils.save_and_reraise_exception():
10137                 # NOTE(mriedem): Why don't we pass clean_task_state=True here?
10138                 self._set_instance_obj_error_state(instance)
10139 
10140     # TODO(stephenfin): Remove this once we bump the compute API to v6.0
10141     @wrap_exception()
10142     def add_aggregate_host(self, context, aggregate, host, slave_info):
10143         """(REMOVED) Notify hypervisor of change (for hypervisor pools)."""
10144         raise NotImplementedError()
10145 
10146     # TODO(stephenfin): Remove this once we bump the compute API to v6.0
10147     @wrap_exception()
10148     def remove_aggregate_host(self, context, host, slave_info, aggregate):
10149         """(REMOVED) Removes a host from a physical hypervisor pool."""
10150         raise NotImplementedError()
10151 
10152     def _process_instance_event(self, instance, event):
10153         _event = self.instance_events.pop_instance_event(instance, event)
10154         if _event:
10155             LOG.debug('Processing event %(event)s',
10156                       {'event': event.key}, instance=instance)
10157             _event.send(event)
10158         else:
10159             # If it's a network-vif-unplugged event and the instance is being
10160             # deleted or live migrated then we don't need to make this a
10161             # warning as it's expected. There are other expected things which
10162             # could trigger this event like detaching an interface, but we
10163             # don't have a task state for that.
10164             # TODO(mriedem): We have other move operations and things like
10165             # hard reboot (probably rebuild as well) which trigger this event
10166             # but nothing listens for network-vif-unplugged. We should either
10167             # handle those other known cases or consider just not logging a
10168             # warning if we get this event and the instance is undergoing some
10169             # task state transition.
10170             if (event.name == 'network-vif-unplugged' and
10171                     instance.task_state in (
10172                         task_states.DELETING, task_states.MIGRATING)):
10173                 LOG.debug('Received event %s for instance with task_state %s.',
10174                           event.key, instance.task_state, instance=instance)
10175             else:
10176                 LOG.warning('Received unexpected event %(event)s for '
10177                             'instance with vm_state %(vm_state)s and '
10178                             'task_state %(task_state)s.',
10179                             {'event': event.key,
10180                              'vm_state': instance.vm_state,
10181                              'task_state': instance.task_state},
10182                             instance=instance)
10183 
10184     def _process_instance_vif_deleted_event(self, context, instance,
10185                                             deleted_vif_id):
10186         # If an attached port is deleted by neutron, it needs to
10187         # be detached from the instance.
10188         # And info cache needs to be updated.
10189         network_info = instance.info_cache.network_info
10190         for index, vif in enumerate(network_info):
10191             if vif['id'] == deleted_vif_id:
10192                 LOG.info('Neutron deleted interface %(intf)s; '
10193                          'detaching it from the instance and '
10194                          'deleting it from the info cache',
10195                          {'intf': vif['id']},
10196                          instance=instance)
10197                 profile = vif.get('profile', {}) or {}  # profile can be None
10198                 if profile.get('allocation'):
10199                     LOG.error(
10200                         'The bound port %(port_id)s is deleted in Neutron but '
10201                         'the resource allocation on the resource provider '
10202                         '%(rp_uuid)s is leaked until the server '
10203                         '%(server_uuid)s is deleted.',
10204                         {'port_id': vif['id'],
10205                          'rp_uuid': vif['profile']['allocation'],
10206                          'server_uuid': instance.uuid})
10207 
10208                 del network_info[index]
10209                 neutron.update_instance_cache_with_nw_info(
10210                     self.network_api, context, instance, nw_info=network_info)
10211                 try:
10212                     self.driver.detach_interface(context, instance, vif)
10213                 except NotImplementedError:
10214                     # Not all virt drivers support attach/detach of interfaces
10215                     # yet (like Ironic), so just ignore this.
10216                     pass
10217                 except exception.NovaException as ex:
10218                     # If the instance was deleted before the interface was
10219                     # detached, just log it at debug.
10220                     log_level = (logging.DEBUG
10221                                  if isinstance(ex, exception.InstanceNotFound)
10222                                  else logging.WARNING)
10223                     LOG.log(log_level,
10224                             "Detach interface failed, "
10225                             "port_id=%(port_id)s, reason: %(msg)s",
10226                             {'port_id': deleted_vif_id, 'msg': ex},
10227                             instance=instance)
10228                 break
10229 
10230     @wrap_instance_event(prefix='compute')
10231     @wrap_instance_fault
10232     def extend_volume(self, context, instance, extended_volume_id):
10233 
10234         # If an attached volume is extended by cinder, it needs to
10235         # be extended by virt driver so host can detect its new size.
10236         # And bdm needs to be updated.
10237         LOG.debug('Handling volume-extended event for volume %(vol)s',
10238                   {'vol': extended_volume_id}, instance=instance)
10239 
10240         try:
10241             bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
10242                    context, extended_volume_id, instance.uuid)
10243         except exception.NotFound:
10244             LOG.warning('Extend volume failed, '
10245                         'volume %(vol)s is not attached to instance.',
10246                         {'vol': extended_volume_id},
10247                         instance=instance)
10248             return
10249 
10250         LOG.info('Cinder extended volume %(vol)s; '
10251                  'extending it to detect new size',
10252                  {'vol': extended_volume_id},
10253                  instance=instance)
10254         volume = self.volume_api.get(context, bdm.volume_id)
10255 
10256         if bdm.connection_info is None:
10257             LOG.warning('Extend volume failed, '
10258                         'attached volume %(vol)s has no connection_info',
10259                         {'vol': extended_volume_id},
10260                         instance=instance)
10261             return
10262 
10263         connection_info = jsonutils.loads(bdm.connection_info)
10264         bdm.volume_size = volume['size']
10265         bdm.save()
10266 
10267         if not self.driver.capabilities.get('supports_extend_volume', False):
10268             raise exception.ExtendVolumeNotSupported()
10269 
10270         try:
10271             self.driver.extend_volume(context, connection_info, instance,
10272                                       bdm.volume_size * units.Gi)
10273         except Exception as ex:
10274             LOG.warning('Extend volume failed, '
10275                         'volume_id=%(volume_id)s, reason: %(msg)s',
10276                         {'volume_id': extended_volume_id, 'msg': ex},
10277                         instance=instance)
10278             raise
10279 
10280     @staticmethod
10281     def _is_state_valid_for_power_update_event(instance, target_power_state):
10282         """Check if the current state of the instance allows it to be
10283         a candidate for the power-update event.
10284 
10285         :param instance: The nova instance object.
10286         :param target_power_state: The desired target power state; this should
10287                                    either be "POWER_ON" or "POWER_OFF".
10288         :returns Boolean: True if the instance can be subjected to the
10289                           power-update event.
10290         """
10291         if ((target_power_state == external_event_obj.POWER_ON and
10292                 instance.task_state is None and
10293                 instance.vm_state == vm_states.STOPPED and
10294                 instance.power_state == power_state.SHUTDOWN) or
10295             (target_power_state == external_event_obj.POWER_OFF and
10296                 instance.task_state is None and
10297                 instance.vm_state == vm_states.ACTIVE and
10298                 instance.power_state == power_state.RUNNING)):
10299             return True
10300         return False
10301 
10302     @wrap_exception()
10303     @reverts_task_state
10304     @wrap_instance_event(prefix='compute')
10305     @wrap_instance_fault
10306     def power_update(self, context, instance, target_power_state):
10307         """Power update of an instance prompted by an external event.
10308         :param context: The API request context.
10309         :param instance: The nova instance object.
10310         :param target_power_state: The desired target power state;
10311                                    this should either be "POWER_ON" or
10312                                    "POWER_OFF".
10313         """
10314 
10315         @utils.synchronized(instance.uuid)
10316         def do_power_update():
10317             LOG.debug('Handling power-update event with target_power_state %s '
10318                       'for instance', target_power_state, instance=instance)
10319             if not self._is_state_valid_for_power_update_event(
10320                     instance, target_power_state):
10321                 pow_state = fields.InstancePowerState.from_index(
10322                     instance.power_state)
10323                 LOG.info('The power-update %(tag)s event for instance '
10324                          '%(uuid)s is a no-op since the instance is in '
10325                          'vm_state %(vm_state)s, task_state '
10326                          '%(task_state)s and power_state '
10327                          '%(power_state)s.',
10328                          {'tag': target_power_state, 'uuid': instance.uuid,
10329                          'vm_state': instance.vm_state,
10330                          'task_state': instance.task_state,
10331                          'power_state': pow_state})
10332                 return
10333             LOG.debug("Trying to %s instance",
10334                       target_power_state, instance=instance)
10335             if target_power_state == external_event_obj.POWER_ON:
10336                 action = fields.NotificationAction.POWER_ON
10337                 notification_name = "power_on."
10338                 instance.task_state = task_states.POWERING_ON
10339             else:
10340                 # It's POWER_OFF
10341                 action = fields.NotificationAction.POWER_OFF
10342                 notification_name = "power_off."
10343                 instance.task_state = task_states.POWERING_OFF
10344                 instance.progress = 0
10345 
10346             try:
10347                 # Note that the task_state is set here rather than the API
10348                 # because this is a best effort operation and deferring
10349                 # updating the task_state until we get to the compute service
10350                 # avoids error handling in the API and needing to account for
10351                 # older compute services during rolling upgrades from Stein.
10352                 # If we lose a race, UnexpectedTaskStateError is handled
10353                 # below.
10354                 instance.save(expected_task_state=[None])
10355                 self._notify_about_instance_usage(context, instance,
10356                                                   notification_name + "start")
10357                 compute_utils.notify_about_instance_action(context, instance,
10358                     self.host, action=action,
10359                     phase=fields.NotificationPhase.START)
10360                 # UnexpectedTaskStateError raised from the driver will be
10361                 # handled below and not result in a fault, error notification
10362                 # or failure of the instance action. Other driver errors like
10363                 # NotImplementedError will be record a fault, send an error
10364                 # notification and mark the instance action as failed.
10365                 self.driver.power_update_event(instance, target_power_state)
10366                 self._notify_about_instance_usage(context, instance,
10367                                                   notification_name + "end")
10368                 compute_utils.notify_about_instance_action(context, instance,
10369                     self.host, action=action,
10370                     phase=fields.NotificationPhase.END)
10371             except exception.UnexpectedTaskStateError as e:
10372                 # Handling the power-update event is best effort and if we lost
10373                 # a race with some other action happening to the instance we
10374                 # just log it and return rather than fail the action.
10375                 LOG.info("The power-update event was possibly preempted: %s ",
10376                          e.format_message(), instance=instance)
10377                 return
10378         do_power_update()
10379 
10380     @wrap_exception()
10381     def external_instance_event(self, context, instances, events):
10382         # NOTE(danms): Some event types are handled by the manager, such
10383         # as when we're asked to update the instance's info_cache. If it's
10384         # not one of those, look for some thread(s) waiting for the event and
10385         # unblock them if so.
10386         for event in events:
10387             instance = [inst for inst in instances
10388                         if inst.uuid == event.instance_uuid][0]
10389             LOG.debug('Received event %(event)s',
10390                       {'event': event.key},
10391                       instance=instance)
10392             if event.name == 'network-changed':
10393                 try:
10394                     LOG.debug('Refreshing instance network info cache due to '
10395                               'event %s.', event.key, instance=instance)
10396                     self.network_api.get_instance_nw_info(
10397                         context, instance, refresh_vif_id=event.tag)
10398                 except exception.NotFound as e:
10399                     LOG.info('Failed to process external instance event '
10400                              '%(event)s due to: %(error)s',
10401                              {'event': event.key, 'error': str(e)},
10402                              instance=instance)
10403             elif event.name == 'network-vif-deleted':
10404                 try:
10405                     self._process_instance_vif_deleted_event(context,
10406                                                              instance,
10407                                                              event.tag)
10408                 except exception.NotFound as e:
10409                     LOG.info('Failed to process external instance event '
10410                              '%(event)s due to: %(error)s',
10411                              {'event': event.key, 'error': str(e)},
10412                              instance=instance)
10413             elif event.name == 'volume-extended':
10414                 self.extend_volume(context, instance, event.tag)
10415             elif event.name == 'power-update':
10416                 self.power_update(context, instance, event.tag)
10417             else:
10418                 self._process_instance_event(instance, event)
10419 
10420     @periodic_task.periodic_task(spacing=CONF.image_cache.manager_interval,
10421                                  external_process_ok=True)
10422     def _run_image_cache_manager_pass(self, context):
10423         """Run a single pass of the image cache manager."""
10424 
10425         if not self.driver.capabilities.get("has_imagecache", False):
10426             return
10427 
10428         # Determine what other nodes use this storage
10429         storage_users.register_storage_use(CONF.instances_path, CONF.host)
10430         nodes = storage_users.get_storage_users(CONF.instances_path)
10431 
10432         # Filter all_instances to only include those nodes which share this
10433         # storage path.
10434         # TODO(mikal): this should be further refactored so that the cache
10435         # cleanup code doesn't know what those instances are, just a remote
10436         # count, and then this logic should be pushed up the stack.
10437         filters = {'deleted': False,
10438                    'soft_deleted': True,
10439                    'host': nodes}
10440         filtered_instances = objects.InstanceList.get_by_filters(context,
10441                                  filters, expected_attrs=[], use_slave=True)
10442 
10443         self.driver.manage_image_cache(context, filtered_instances)
10444 
10445     def cache_images(self, context, image_ids):
10446         """Ask the virt driver to pre-cache a set of base images.
10447 
10448         :param context: The RequestContext
10449         :param image_ids: The image IDs to be cached
10450         :return: A dict, keyed by image-id where the values are one of:
10451                  'cached' if the image was downloaded,
10452                  'existing' if the image was already in the cache,
10453                  'unsupported' if the virt driver does not support caching,
10454                  'error' if the virt driver raised an exception.
10455         """
10456 
10457         results = {}
10458 
10459         LOG.info('Caching %i image(s) by request', len(image_ids))
10460         for image_id in image_ids:
10461             try:
10462                 cached = self.driver.cache_image(context, image_id)
10463                 if cached:
10464                     results[image_id] = 'cached'
10465                 else:
10466                     results[image_id] = 'existing'
10467             except NotImplementedError:
10468                 LOG.warning('Virt driver does not support image pre-caching;'
10469                             ' ignoring request')
10470                 # NOTE(danms): Yes, technically we could short-circuit here to
10471                 # avoid trying the rest of the images, but it's very cheap to
10472                 # just keep hitting the NotImplementedError to keep the logic
10473                 # clean.
10474                 results[image_id] = 'unsupported'
10475             except Exception as e:
10476                 results[image_id] = 'error'
10477                 LOG.error('Failed to cache image %(image_id)s: %(err)s',
10478                           {'image_id': image_id,
10479                            'err': e})
10480 
10481         return results
10482 
10483     @periodic_task.periodic_task(spacing=CONF.instance_delete_interval)
10484     def _run_pending_deletes(self, context):
10485         """Retry any pending instance file deletes."""
10486         LOG.debug('Cleaning up deleted instances')
10487         filters = {'deleted': True,
10488                    'soft_deleted': False,
10489                    'host': CONF.host,
10490                    'cleaned': False}
10491         attrs = ['system_metadata']
10492         with utils.temporary_mutation(context, read_deleted='yes'):
10493             instances = objects.InstanceList.get_by_filters(
10494                 context, filters, expected_attrs=attrs, use_slave=True)
10495         LOG.debug('There are %d instances to clean', len(instances))
10496 
10497         for instance in instances:
10498             attempts = int(instance.system_metadata.get('clean_attempts', '0'))
10499             LOG.debug('Instance has had %(attempts)s of %(max)s '
10500                       'cleanup attempts',
10501                       {'attempts': attempts,
10502                        'max': CONF.maximum_instance_delete_attempts},
10503                       instance=instance)
10504             if attempts < CONF.maximum_instance_delete_attempts:
10505                 success = self.driver.delete_instance_files(instance)
10506 
10507                 instance.system_metadata['clean_attempts'] = str(attempts + 1)
10508                 if success:
10509                     instance.cleaned = True
10510                 with utils.temporary_mutation(context, read_deleted='yes'):
10511                     instance.save()
10512 
10513     @periodic_task.periodic_task(spacing=CONF.instance_delete_interval)
10514     def _cleanup_incomplete_migrations(self, context):
10515         """Cleanup on failed resize/revert-resize operation and
10516         failed rollback live migration operation.
10517 
10518         During resize/revert-resize operation, or after a failed rollback
10519         live migration operation, if that instance gets deleted then instance
10520         files might remain either on source or destination compute node and
10521         other specific resources might not be cleaned up because of the race
10522         condition.
10523         """
10524         LOG.debug('Cleaning up deleted instances with incomplete migration ')
10525         migration_filters = {'host': CONF.host,
10526                              'status': 'error'}
10527         migrations = objects.MigrationList.get_by_filters(context,
10528                                                           migration_filters)
10529 
10530         if not migrations:
10531             return
10532 
10533         inst_uuid_from_migrations = set([migration.instance_uuid for migration
10534                                          in migrations])
10535 
10536         inst_filters = {'deleted': True, 'soft_deleted': False,
10537                         'uuid': inst_uuid_from_migrations}
10538         attrs = ['info_cache', 'security_groups', 'system_metadata']
10539         with utils.temporary_mutation(context, read_deleted='yes'):
10540             instances = objects.InstanceList.get_by_filters(
10541                 context, inst_filters, expected_attrs=attrs, use_slave=True)
10542 
10543         for instance in instances:
10544             if instance.host == CONF.host:
10545                 continue
10546             for migration in migrations:
10547                 if instance.uuid != migration.instance_uuid:
10548                     continue
10549                 self.driver.delete_instance_files(instance)
10550                 # we are not sure whether the migration_context is applied
10551                 # during incompleted migrating, we need to apply/revert
10552                 # migration_context to get instance object content matching
10553                 # current host.
10554                 revert = (True if migration.source_compute == CONF.host
10555                           else False)
10556                 with instance.mutated_migration_context(revert=revert):
10557                     self.driver.cleanup_lingering_instance_resources(instance)
10558 
10559                 try:
10560                     migration.status = 'failed'
10561                     migration.save()
10562                 except exception.MigrationNotFound:
10563                     LOG.warning("Migration %s is not found.",
10564                                 migration.id,
10565                                 instance=instance)
10566                 break
10567 
10568     @messaging.expected_exceptions(exception.InstanceQuiesceNotSupported,
10569                                    exception.QemuGuestAgentNotEnabled,
10570                                    exception.NovaException,
10571                                    NotImplementedError)
10572     @wrap_exception()
10573     def quiesce_instance(self, context, instance):
10574         """Quiesce an instance on this host."""
10575         context = context.elevated()
10576         image_meta = objects.ImageMeta.from_instance(instance)
10577         self.driver.quiesce(context, instance, image_meta)
10578 
10579     def _wait_for_snapshots_completion(self, context, mapping):
10580         for mapping_dict in mapping:
10581             if mapping_dict.get('source_type') == 'snapshot':
10582 
10583                 def _wait_snapshot():
10584                     snapshot = self.volume_api.get_snapshot(
10585                         context, mapping_dict['snapshot_id'])
10586                     if snapshot.get('status') != 'creating':
10587                         raise loopingcall.LoopingCallDone()
10588 
10589                 timer = loopingcall.FixedIntervalLoopingCall(_wait_snapshot)
10590                 timer.start(interval=0.5).wait()
10591 
10592     @messaging.expected_exceptions(exception.InstanceQuiesceNotSupported,
10593                                    exception.QemuGuestAgentNotEnabled,
10594                                    exception.NovaException,
10595                                    NotImplementedError)
10596     @wrap_exception()
10597     def unquiesce_instance(self, context, instance, mapping=None):
10598         """Unquiesce an instance on this host.
10599 
10600         If snapshots' image mapping is provided, it waits until snapshots are
10601         completed before unqueiscing.
10602         """
10603         context = context.elevated()
10604         if mapping:
10605             try:
10606                 self._wait_for_snapshots_completion(context, mapping)
10607             except Exception as error:
10608                 LOG.exception("Exception while waiting completion of "
10609                               "volume snapshots: %s",
10610                               error, instance=instance)
10611         image_meta = objects.ImageMeta.from_instance(instance)
10612         self.driver.unquiesce(context, instance, image_meta)
10613 
10614     @periodic_task.periodic_task(spacing=CONF.instance_delete_interval)
10615     def _cleanup_expired_console_auth_tokens(self, context):
10616         """Remove all expired console auth tokens.
10617 
10618         Console authorization tokens and their connection data are stored
10619         in the database when a user asks for a console connection to an
10620         instance. After a time they expire. We periodically remove any expired
10621         tokens from the database.
10622         """
10623         objects.ConsoleAuthToken.clean_expired_console_auths(context)
10624 
10625     def _claim_pci_for_instance_vifs(self, ctxt, instance):
10626         """Claim PCI devices for the instance's VIFs on the compute node
10627 
10628         :param ctxt: Context
10629         :param instance: Instance object
10630         :return: <port ID: PciDevice> mapping for the VIFs that yielded a
10631                 PCI claim on the compute node
10632         """
10633         pci_req_id_to_port_id = {}
10634         pci_reqs = []
10635         port_id_to_pci_dev = {}
10636 
10637         for vif in instance.get_network_info():
10638             pci_req = pci_req_module.get_instance_pci_request_from_vif(
10639                 ctxt,
10640                 instance,
10641                 vif)
10642             if pci_req:
10643                 pci_req_id_to_port_id[pci_req.request_id] = vif['id']
10644                 pci_reqs.append(pci_req)
10645 
10646         if pci_reqs:
10647             # Create PCI requests and claim against PCI resource tracker
10648             # NOTE(adrianc): We claim against the same requests as on the
10649             # source node.
10650             vif_pci_requests = objects.InstancePCIRequests(
10651                 requests=pci_reqs,
10652                 instance_uuid=instance.uuid)
10653 
10654             claimed_pci_devices_objs = self.rt.claim_pci_devices(
10655                 ctxt,
10656                 vif_pci_requests)
10657 
10658             # Update VIFMigrateData profile with the newly claimed PCI
10659             # device
10660             for pci_dev in claimed_pci_devices_objs:
10661                 LOG.debug("PCI device: %s Claimed on destination node",
10662                           pci_dev.address)
10663                 port_id = pci_req_id_to_port_id[pci_dev.request_id]
10664                 port_id_to_pci_dev[port_id] = pci_dev
10665 
10666         return port_id_to_pci_dev
10667 
10668     def _update_migrate_vifs_profile_with_pci(self,
10669                                               migrate_vifs,
10670                                               port_id_to_pci_dev):
10671         """Update migrate vifs profile with the claimed PCI devices
10672 
10673         :param migrate_vifs: list of VIFMigrateData objects
10674         :param port_id_to_pci_dev: a <port_id: PciDevice> mapping
10675         :return: None.
10676         """
10677         for mig_vif in migrate_vifs:
10678             port_id = mig_vif.port_id
10679             if port_id not in port_id_to_pci_dev:
10680                 continue
10681 
10682             pci_dev = port_id_to_pci_dev[port_id]
10683             profile = copy.deepcopy(mig_vif.source_vif['profile'])
10684             profile['pci_slot'] = pci_dev.address
10685             profile['pci_vendor_info'] = ':'.join([pci_dev.vendor_id,
10686                                                    pci_dev.product_id])
10687             mig_vif.profile = profile
10688             LOG.debug("Updating migrate VIF profile for port %(port_id)s:"
10689                       "%(profile)s", {'port_id': port_id,
10690                                       'profile': profile})
