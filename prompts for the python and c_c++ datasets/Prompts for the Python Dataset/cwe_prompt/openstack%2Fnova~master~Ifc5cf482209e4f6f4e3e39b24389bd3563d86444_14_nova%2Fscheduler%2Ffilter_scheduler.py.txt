Please review the code below for security defects using the CWE (Common Weakness Enumeration) as a reference standard. If any are found, please describe the security defect in detail and indicate the corresponding line number of code and solution. If none are detected, state: 'No security defects are detected in the code'.

1 # Copyright (c) 2011 OpenStack Foundation
2 # All Rights Reserved.
3 #
4 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
5 #    not use this file except in compliance with the License. You may obtain
6 #    a copy of the License at
7 #
8 #         http://www.apache.org/licenses/LICENSE-2.0
9 #
10 #    Unless required by applicable law or agreed to in writing, software
11 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
12 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
13 #    License for the specific language governing permissions and limitations
14 #    under the License.
15 
16 """
17 The FilterScheduler is for creating instances locally.
18 You can customize this scheduler by specifying your own Host Filters and
19 Weighing Functions.
20 """
21 
22 import random
23 
24 from oslo_log import log as logging
25 from six.moves import range
26 
27 import nova.conf
28 from nova import exception
29 from nova.i18n import _
30 from nova import rpc
31 from nova.scheduler import client
32 from nova.scheduler import driver
33 
34 CONF = nova.conf.CONF
35 LOG = logging.getLogger(__name__)
36 
37 
38 class FilterScheduler(driver.Scheduler):
39     """Scheduler that can be used for filtering and weighing."""
40     def __init__(self, *args, **kwargs):
41         super(FilterScheduler, self).__init__(*args, **kwargs)
42         self.notifier = rpc.get_notifier('scheduler')
43         scheduler_client = client.SchedulerClient()
44         self.placement_client = scheduler_client.reportclient
45 
46     def select_destinations(self, context, spec_obj, instance_uuids,
47             alloc_reqs_by_rp_uuid, provider_summaries):
48         """Returns a sorted list of HostState objects that satisfy the
49         supplied request_spec.
50 
51         These hosts will have already had their resources claimed in Placement.
52 
53         :param context: The RequestContext object
54         :param spec_obj: The RequestSpec object
55         :param instance_uuids: List of UUIDs, one for each value of the spec
56                                object's num_instances attribute
57         :param alloc_reqs_by_rp_uuid: Optional dict, keyed by resource provider
58                                       UUID, of the allocation requests that may
59                                       be used to claim resources against
60                                       matched hosts. If None, indicates either
61                                       the placement API wasn't reachable or
62                                       that there were no allocation requests
63                                       returned by the placement API. If the
64                                       latter, the provider_summaries will be an
65                                       empty dict, not None.
66         :param provider_summaries: Optional dict, keyed by resource provider
67                                    UUID, of information that will be used by
68                                    the filters/weighers in selecting matching
69                                    hosts for a request. If None, indicates that
70                                    the scheduler driver should grab all compute
71                                    node information locally and that the
72                                    Placement API is not used. If an empty dict,
73                                    indicates the Placement API returned no
74                                    potential matches for the requested
75                                    resources.
76         """
77         self.notifier.info(
78             context, 'scheduler.select_destinations.start',
79             dict(request_spec=spec_obj.to_legacy_request_spec_dict()))
80 
81         num_instances = spec_obj.num_instances
82         selected_hosts = self._schedule(context, spec_obj, instance_uuids,
83             alloc_reqs_by_rp_uuid, provider_summaries)
84 
85         # Couldn't fulfill the request_spec
86         if len(selected_hosts) < num_instances:
87             # NOTE(Rui Chen): If multiple creates failed, set the updated time
88             # of selected HostState to None so that these HostStates are
89             # refreshed according to database in next schedule, and release
90             # the resource consumed by instance in the process of selecting
91             # host.
92             for host in selected_hosts:
93                 host.updated = None
94 
95             # Log the details but don't put those into the reason since
96             # we don't want to give away too much information about our
97             # actual environment.
98             LOG.debug('There are %(hosts)d hosts available but '
99                       '%(num_instances)d instances requested to build.',
100                       {'hosts': len(selected_hosts),
101                        'num_instances': num_instances})
102 
103             reason = _('There are not enough hosts available.')
104             raise exception.NoValidHost(reason=reason)
105 
106         self.notifier.info(
107             context, 'scheduler.select_destinations.end',
108             dict(request_spec=spec_obj.to_legacy_request_spec_dict()))
109         return selected_hosts
110 
111     def _schedule(self, context, spec_obj, instance_uuids,
112             alloc_reqs_by_rp_uuid, provider_summaries):
113         """Returns a list of hosts that meet the required specs, ordered by
114         their fitness.
115 
116         These hosts will have already had their resources claimed in Placement.
117 
118         :param context: The RequestContext object
119         :param spec_obj: The RequestSpec object
120         :param instance_uuids: List of UUIDs, one for each value of the spec
121                                object's num_instances attribute
122         :param alloc_reqs_by_rp_uuid: Optional dict, keyed by resource provider
123                                       UUID, of the allocation requests that may
124                                       be used to claim resources against
125                                       matched hosts. If None, indicates either
126                                       the placement API wasn't reachable or
127                                       that there were no allocation requests
128                                       returned by the placement API. If the
129                                       latter, the provider_summaries will be an
130                                       empty dict, not None.
131         :param provider_summaries: Optional dict, keyed by resource provider
132                                    UUID, of information that will be used by
133                                    the filters/weighers in selecting matching
134                                    hosts for a request. If None, indicates that
135                                    the scheduler driver should grab all compute
136                                    node information locally and that the
137                                    Placement API is not used. If an empty dict,
138                                    indicates the Placement API returned no
139                                    potential matches for the requested
140                                    resources.
141         """
142         elevated = context.elevated()
143 
144         # Find our local list of acceptable hosts by repeatedly
145         # filtering and weighing our options. Each time we choose a
146         # host, we virtually consume resources on it so subsequent
147         # selections can adjust accordingly.
148 
149         # Note: remember, we are using an iterator here. So only
150         # traverse this list once. This can bite you if the hosts
151         # are being scanned in a filter or weighing function.
152         hosts = self._get_all_host_states(elevated, spec_obj,
153             provider_summaries)
154 
155         # A list of the instance UUIDs that were successfully claimed against
156         # in the placement API. If we are not able to successfully claim for
157         # all involved instances, we use this list to remove those allocations
158         # before returning
159         claimed_instance_uuids = []
160 
161         selected_hosts = []
162         num_instances = spec_obj.num_instances
163         for num in range(num_instances):
164             hosts = self._get_sorted_hosts(spec_obj, hosts, num)
165             if not hosts:
166                 # NOTE(jaypipes): If we get here, that means not all instances
167                 # in instance_uuids were able to be matched to a selected host.
168                 # So, let's clean up any already-claimed allocations here
169                 # before breaking and returning
170                 self._cleanup_allocations(claimed_instance_uuids)
171                 break
172 
173             if (instance_uuids is None or
174                     not self.USES_ALLOCATION_CANDIDATES or
175                     alloc_reqs_by_rp_uuid is None):
176                 # Unfortunately, we still need to deal with older conductors
177                 # that may not be passing in a list of instance_uuids. In those
178                 # cases, obviously we can't claim resources because we don't
179                 # have instance UUIDs to claim with, so we just grab the first
180                 # host in the list of sorted hosts. In addition to older
181                 # conductors, we need to support the caching scheduler, which
182                 # doesn't use the placement API (and has
183                 # USES_ALLOCATION_CANDIDATE = False) and therefore we skip all
184                 # the claiming logic for that scheduler driver. Finally, if
185                 # there was a problem communicating with the placement API,
186                 # alloc_reqs_by_rp_uuid will be None, so we skip claiming in
187                 # that case as well
188                 claimed_host = hosts[0]
189             else:
190                 instance_uuid = instance_uuids[num]
191 
192                 # Attempt to claim the resources against one or more resource
193                 # providers, looping over the sorted list of possible hosts
194                 # looking for an allocation request that contains that host's
195                 # resource provider UUID
196                 claimed_host = None
197                 for host in hosts:
198                     cn_uuid = host.uuid
199                     if cn_uuid not in alloc_reqs_by_rp_uuid:
200                         LOG.debug("Found host state %s that wasn't in "
201                                   "allocation requests. Skipping.", cn_uuid)
202                         continue
203 
204                     alloc_reqs = alloc_reqs_by_rp_uuid[cn_uuid]
205                     if self._claim_resources(elevated, spec_obj, instance_uuid,
206                             alloc_reqs):
207                         claimed_host = host
208                         break
209 
210                 if claimed_host is None:
211                     # We weren't able to claim resources in the placement API
212                     # for any of the sorted hosts identified. So, clean up any
213                     # successfully-claimed resources for prior instances in
214                     # this request and return an empty list which will cause
215                     # select_destinations() to raise NoValidHost
216                     LOG.debug("Unable to successfully claim against any host.")
217                     self._cleanup_allocations(claimed_instance_uuids)
218                     return []
219 
220                 claimed_instance_uuids.append(instance_uuid)
221 
222             LOG.debug("Selected host: %(host)s", {'host': claimed_host})
223             selected_hosts.append(claimed_host)
224 
225             # Now consume the resources so the filter/weights will change for
226             # the next instance.
227             claimed_host.consume_from_request(spec_obj)
228             if spec_obj.instance_group is not None:
229                 spec_obj.instance_group.hosts.append(claimed_host.host)
230                 # hosts has to be not part of the updates when saving
231                 spec_obj.instance_group.obj_reset_changes(['hosts'])
232         return selected_hosts
233 
234     def _cleanup_allocations(self, instance_uuids):
235         """Removes allocations for the supplied instance UUIDs."""
236         if not instance_uuids:
237             return
238         LOG.debug("Cleaning up allocations for %s", instance_uuids)
239         for uuid in instance_uuids:
240             self.placement_client.delete_allocation_for_instance(uuid)
241 
242     def _claim_resources(self, ctx, spec_obj, instance_uuid, alloc_reqs):
243         """Given an instance UUID (representing the consumer of resources), the
244         HostState object for the host that was chosen for the instance, and a
245         list of allocation request JSON objects, attempt to claim resources for
246         the instance in the placement API. Returns True if the claim process
247         was successful, False otherwise.
248 
249         :param ctx: The RequestContext object
250         :param spec_obj: The RequestSpec object
251         :param instance_uuid: The UUID of the consuming instance
252         :param cn_uuid: UUID of the host to allocate against
253         :param alloc_reqs: A list of allocation request JSON objects that
254                            allocate against (at least) the compute host
255                            selected by the _schedule() method. These allocation
256                            requests were constructed from a call to the GET
257                            /allocation_candidates placement API call.  Each
258                            allocation_request satisfies the original request
259                            for resources and can be supplied as-is (along with
260                            the project and user ID to the placement API's
261                            PUT /allocations/{consumer_uuid} call to claim
262                            resources for the instance
263         """
264         LOG.debug("Attempting to claim resources in the placement API for "
265                   "instance %s", instance_uuid)
266 
267         project_id = spec_obj.project_id
268 
269         # NOTE(jaypipes): So, the RequestSpec doesn't store the user_id,
270         # only the project_id, so we need to grab the user information from
271         # the context. Perhaps we should consider putting the user ID in
272         # the spec object?
273         user_id = ctx.user_id
274 
275         # TODO(jaypipes): Loop through all allocation requests instead of just
276         # trying the first one. For now, since we'll likely want to order the
277         # allocation requests in the future based on information in the
278         # provider summaries, we'll just try to claim resources using the first
279         # allocation request
280         alloc_req = alloc_reqs[0]
281 
282         claimed = self.placement_client.claim_resources(instance_uuid,
283             alloc_req, project_id, user_id)
284 
285         if not claimed:
286             return False
287 
288         LOG.debug("Successfully claimed resources for instance %s using "
289                   "allocation request %s", instance_uuid, alloc_req)
290 
291         return True
292 
293     def _get_sorted_hosts(self, spec_obj, host_states, index):
294         """Returns a list of HostState objects that match the required
295         scheduling constraints for the request spec object and have been sorted
296         according to the weighers.
297         """
298         filtered_hosts = self.host_manager.get_filtered_hosts(host_states,
299             spec_obj, index)
300 
301         LOG.debug("Filtered %(hosts)s", {'hosts': filtered_hosts})
302 
303         if not filtered_hosts:
304             return []
305 
306         weighed_hosts = self.host_manager.get_weighed_hosts(filtered_hosts,
307             spec_obj)
308         # Strip off the WeighedHost wrapper class...
309         weighed_hosts = [h.obj for h in weighed_hosts]
310 
311         LOG.debug("Weighed %(hosts)s", {'hosts': weighed_hosts})
312 
313         # We randomize the first element in the returned list to alleviate
314         # congestion where the same host is consistently selected among
315         # numerous potential hosts for similar request specs.
316         host_subset_size = CONF.filter_scheduler.host_subset_size
317         if host_subset_size < len(weighed_hosts):
318             weighed_subset = weighed_hosts[0:host_subset_size]
319         else:
320             weighed_subset = weighed_hosts
321         chosen_host = random.choice(weighed_subset)
322         weighed_hosts.remove(chosen_host)
323         return [chosen_host] + weighed_hosts
324 
325     def _get_all_host_states(self, context, spec_obj, provider_summaries):
326         """Template method, so a subclass can implement caching."""
327         # NOTE(jaypipes): None is treated differently from an empty dict. We
328         # pass None when we want to grab all compute nodes (for instance, when
329         # using the caching scheduler. We pass an empty dict when the Placement
330         # API found no providers that match the requested constraints.
331         compute_uuids = None
332         if provider_summaries is not None:
333             compute_uuids = list(provider_summaries.keys())
334         return self.host_manager.get_host_states_by_uuids(context,
335                                                           compute_uuids,
336                                                           spec_obj)
