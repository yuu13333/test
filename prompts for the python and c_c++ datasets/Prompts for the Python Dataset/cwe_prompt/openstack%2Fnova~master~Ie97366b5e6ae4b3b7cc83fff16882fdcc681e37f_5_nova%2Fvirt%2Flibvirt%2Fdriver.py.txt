Please review the code below for security defects using the CWE (Common Weakness Enumeration) as a reference standard. If any are found, please describe the security defect in detail and indicate the corresponding line number of code and solution. If none are detected, state: 'No security defects are detected in the code'.

1 # Copyright 2010 United States Government as represented by the
2 # Administrator of the National Aeronautics and Space Administration.
3 # All Rights Reserved.
4 # Copyright (c) 2010 Citrix Systems, Inc.
5 # Copyright (c) 2011 Piston Cloud Computing, Inc
6 # Copyright (c) 2012 University Of Minho
7 # (c) Copyright 2013 Hewlett-Packard Development Company, L.P.
8 #
9 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
10 #    not use this file except in compliance with the License. You may obtain
11 #    a copy of the License at
12 #
13 #         http://www.apache.org/licenses/LICENSE-2.0
14 #
15 #    Unless required by applicable law or agreed to in writing, software
16 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
17 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
18 #    License for the specific language governing permissions and limitations
19 #    under the License.
20 
21 """
22 A connection to a hypervisor through libvirt.
23 
24 Supports KVM, LXC, QEMU, UML, XEN and Parallels.
25 
26 """
27 
28 import binascii
29 import collections
30 from collections import deque
31 import contextlib
32 import errno
33 import functools
34 import glob
35 import itertools
36 import operator
37 import os
38 import pwd
39 import random
40 import shutil
41 import tempfile
42 import time
43 import uuid
44 
45 from castellan import key_manager
46 import eventlet
47 from eventlet import greenthread
48 from eventlet import tpool
49 from lxml import etree
50 from os_brick import encryptors
51 from os_brick.encryptors import luks as luks_encryptor
52 from os_brick import exception as brick_exception
53 from os_brick.initiator import connector
54 from oslo_concurrency import processutils
55 from oslo_log import log as logging
56 from oslo_serialization import base64
57 from oslo_serialization import jsonutils
58 from oslo_service import loopingcall
59 from oslo_utils import encodeutils
60 from oslo_utils import excutils
61 from oslo_utils import fileutils
62 from oslo_utils import importutils
63 from oslo_utils import strutils
64 from oslo_utils import timeutils
65 from oslo_utils import units
66 from oslo_utils import uuidutils
67 import six
68 from six.moves import range
69 
70 from nova.api.metadata import base as instance_metadata
71 from nova.api.metadata import password
72 from nova import block_device
73 from nova.compute import power_state
74 from nova.compute import task_states
75 from nova.compute import utils as compute_utils
76 import nova.conf
77 from nova.console import serial as serial_console
78 from nova.console import type as ctype
79 from nova import context as nova_context
80 from nova import crypto
81 from nova import exception
82 from nova.i18n import _
83 from nova import image
84 from nova.network import model as network_model
85 from nova import objects
86 from nova.objects import diagnostics as diagnostics_obj
87 from nova.objects import fields
88 from nova.objects import migrate_data as migrate_data_obj
89 from nova.pci import manager as pci_manager
90 from nova.pci import utils as pci_utils
91 import nova.privsep.libvirt
92 import nova.privsep.path
93 import nova.privsep.utils
94 from nova import rc_fields
95 from nova import utils
96 from nova import version
97 from nova.virt import block_device as driver_block_device
98 from nova.virt import configdrive
99 from nova.virt.disk import api as disk_api
100 from nova.virt.disk.vfs import guestfs
101 from nova.virt import driver
102 from nova.virt import firewall
103 from nova.virt import hardware
104 from nova.virt.image import model as imgmodel
105 from nova.virt import images
106 from nova.virt.libvirt import blockinfo
107 from nova.virt.libvirt import config as vconfig
108 from nova.virt.libvirt import designer
109 from nova.virt.libvirt import firewall as libvirt_firewall
110 from nova.virt.libvirt import guest as libvirt_guest
111 from nova.virt.libvirt import host
112 from nova.virt.libvirt import imagebackend
113 from nova.virt.libvirt import imagecache
114 from nova.virt.libvirt import instancejobtracker
115 from nova.virt.libvirt import migration as libvirt_migrate
116 from nova.virt.libvirt.storage import dmcrypt
117 from nova.virt.libvirt.storage import lvm
118 from nova.virt.libvirt.storage import rbd_utils
119 from nova.virt.libvirt import utils as libvirt_utils
120 from nova.virt.libvirt import vif as libvirt_vif
121 from nova.virt.libvirt.volume import mount
122 from nova.virt.libvirt.volume import remotefs
123 from nova.virt import netutils
124 from nova.volume import cinder
125 
126 libvirt = None
127 
128 uefi_logged = False
129 
130 LOG = logging.getLogger(__name__)
131 
132 CONF = nova.conf.CONF
133 
134 DEFAULT_FIREWALL_DRIVER = "%s.%s" % (
135     libvirt_firewall.__name__,
136     libvirt_firewall.IptablesFirewallDriver.__name__)
137 
138 DEFAULT_UEFI_LOADER_PATH = {
139     "x86_64": "/usr/share/OVMF/OVMF_CODE.fd",
140     "aarch64": "/usr/share/AAVMF/AAVMF_CODE.fd"
141 }
142 
143 MAX_CONSOLE_BYTES = 100 * units.Ki
144 
145 # The libvirt driver will prefix any disable reason codes with this string.
146 DISABLE_PREFIX = 'AUTO: '
147 # Disable reason for the service which was enabled or disabled without reason
148 DISABLE_REASON_UNDEFINED = None
149 
150 # Guest config console string
151 CONSOLE = "console=tty0 console=ttyS0 console=hvc0"
152 
153 GuestNumaConfig = collections.namedtuple(
154     'GuestNumaConfig', ['cpuset', 'cputune', 'numaconfig', 'numatune'])
155 
156 
157 class InjectionInfo(collections.namedtuple(
158         'InjectionInfo', ['network_info', 'files', 'admin_pass'])):
159     __slots__ = ()
160 
161     def __repr__(self):
162         return ('InjectionInfo(network_info=%r, files=%r, '
163                 'admin_pass=<SANITIZED>)') % (self.network_info, self.files)
164 
165 libvirt_volume_drivers = [
166     'iscsi=nova.virt.libvirt.volume.iscsi.LibvirtISCSIVolumeDriver',
167     'iser=nova.virt.libvirt.volume.iser.LibvirtISERVolumeDriver',
168     'local=nova.virt.libvirt.volume.volume.LibvirtVolumeDriver',
169     'drbd=nova.virt.libvirt.volume.drbd.LibvirtDRBDVolumeDriver',
170     'fake=nova.virt.libvirt.volume.volume.LibvirtFakeVolumeDriver',
171     'rbd=nova.virt.libvirt.volume.net.LibvirtNetVolumeDriver',
172     'sheepdog=nova.virt.libvirt.volume.net.LibvirtNetVolumeDriver',
173     'nfs=nova.virt.libvirt.volume.nfs.LibvirtNFSVolumeDriver',
174     'smbfs=nova.virt.libvirt.volume.smbfs.LibvirtSMBFSVolumeDriver',
175     'aoe=nova.virt.libvirt.volume.aoe.LibvirtAOEVolumeDriver',
176     'fibre_channel='
177         'nova.virt.libvirt.volume.fibrechannel.'
178         'LibvirtFibreChannelVolumeDriver',
179     'gpfs=nova.virt.libvirt.volume.gpfs.LibvirtGPFSVolumeDriver',
180     'quobyte=nova.virt.libvirt.volume.quobyte.LibvirtQuobyteVolumeDriver',
181     'hgst=nova.virt.libvirt.volume.hgst.LibvirtHGSTVolumeDriver',
182     'scaleio=nova.virt.libvirt.volume.scaleio.LibvirtScaleIOVolumeDriver',
183     'disco=nova.virt.libvirt.volume.disco.LibvirtDISCOVolumeDriver',
184     'vzstorage='
185         'nova.virt.libvirt.volume.vzstorage.LibvirtVZStorageVolumeDriver',
186     'veritas_hyperscale='
187         'nova.virt.libvirt.volume.vrtshyperscale.'
188         'LibvirtHyperScaleVolumeDriver',
189     'storpool=nova.virt.libvirt.volume.storpool.LibvirtStorPoolVolumeDriver',
190     'nvmeof=nova.virt.libvirt.volume.nvme.LibvirtNVMEVolumeDriver',
191 ]
192 
193 
194 def patch_tpool_proxy():
195     """eventlet.tpool.Proxy doesn't work with old-style class in __str__()
196     or __repr__() calls. See bug #962840 for details.
197     We perform a monkey patch to replace those two instance methods.
198     """
199     def str_method(self):
200         return str(self._obj)
201 
202     def repr_method(self):
203         return repr(self._obj)
204 
205     tpool.Proxy.__str__ = str_method
206     tpool.Proxy.__repr__ = repr_method
207 
208 
209 patch_tpool_proxy()
210 
211 # For information about when MIN_LIBVIRT_VERSION and
212 # NEXT_MIN_LIBVIRT_VERSION can be changed, consult
213 #
214 #   https://wiki.openstack.org/wiki/LibvirtDistroSupportMatrix
215 #
216 # Currently this is effectively the min version for i686/x86_64
217 # + KVM/QEMU, as other architectures/hypervisors require newer
218 # versions. Over time, this will become a common min version
219 # for all architectures/hypervisors, as this value rises to
220 # meet them.
221 MIN_LIBVIRT_VERSION = (1, 3, 1)
222 MIN_QEMU_VERSION = (2, 5, 0)
223 # TODO(berrange): Re-evaluate this at start of each release cycle
224 # to decide if we want to plan a future min version bump.
225 # MIN_LIBVIRT_VERSION can be updated to match this after
226 # NEXT_MIN_LIBVIRT_VERSION  has been at a higher value for
227 # one cycle
228 NEXT_MIN_LIBVIRT_VERSION = (3, 0, 0)
229 NEXT_MIN_QEMU_VERSION = (2, 8, 0)
230 
231 
232 # Virtuozzo driver support
233 MIN_VIRTUOZZO_VERSION = (7, 0, 0)
234 
235 # Ability to set the user guest password with parallels
236 MIN_LIBVIRT_PARALLELS_SET_ADMIN_PASSWD = (2, 0, 0)
237 
238 # libvirt < 1.3 reported virt_functions capability
239 # only when VFs are enabled.
240 # libvirt 1.3 fix f391889f4e942e22b9ef8ecca492de05106ce41e
241 MIN_LIBVIRT_PF_WITH_NO_VFS_CAP_VERSION = (1, 3, 0)
242 
243 # Use the "logd" backend for handling stdout/stderr from QEMU processes.
244 MIN_LIBVIRT_VIRTLOGD = (1, 3, 3)
245 MIN_QEMU_VIRTLOGD = (2, 7, 0)
246 
247 
248 # aarch64 architecture with KVM
249 # 'chardev' support got sorted out in 3.6.0
250 MIN_LIBVIRT_KVM_AARCH64_VERSION = (3, 6, 0)
251 
252 # Names of the types that do not get compressed during migration
253 NO_COMPRESSION_TYPES = ('qcow2',)
254 
255 
256 # number of serial console limit
257 QEMU_MAX_SERIAL_PORTS = 4
258 # Qemu supports 4 serial consoles, we remove 1 because of the PTY one defined
259 ALLOWED_QEMU_SERIAL_PORTS = QEMU_MAX_SERIAL_PORTS - 1
260 
261 # libvirt postcopy support
262 MIN_LIBVIRT_POSTCOPY_VERSION = (1, 3, 3)
263 
264 MIN_LIBVIRT_OTHER_ARCH = {
265     fields.Architecture.AARCH64: MIN_LIBVIRT_KVM_AARCH64_VERSION,
266 }
267 
268 # perf events support
269 MIN_LIBVIRT_PERF_VERSION = (2, 0, 0)
270 LIBVIRT_PERF_EVENT_PREFIX = 'VIR_PERF_PARAM_'
271 
272 PERF_EVENTS_CPU_FLAG_MAPPING = {'cmt': 'cmt',
273                                 'mbml': 'mbm_local',
274                                 'mbmt': 'mbm_total',
275                                }
276 
277 # Mediated devices support
278 MIN_LIBVIRT_MDEV_SUPPORT = (3, 4, 0)
279 
280 # libvirt>=3.10 is required for volume multiattach if qemu<2.10.
281 # See https://bugzilla.redhat.com/show_bug.cgi?id=1378242
282 # for details.
283 MIN_LIBVIRT_MULTIATTACH = (3, 10, 0)
284 
285 MIN_LIBVIRT_LUKS_VERSION = (2, 2, 0)
286 MIN_QEMU_LUKS_VERSION = (2, 6, 0)
287 
288 
289 VGPU_RESOURCE_SEMAPHORE = "vgpu_resources"
290 
291 MIN_MIGRATION_SPEED_BW = 1  # 1 MiB/s
292 
293 
294 class LibvirtDriver(driver.ComputeDriver):
295     capabilities = {
296         "has_imagecache": True,
297         "supports_recreate": True,
298         "supports_migrate_to_same_host": False,
299         "supports_attach_interface": True,
300         "supports_device_tagging": True,
301         "supports_tagged_attach_interface": True,
302         "supports_tagged_attach_volume": True,
303         "supports_extend_volume": True,
304         # Multiattach support is conditional on qemu and libvirt versions
305         # determined in init_host.
306         "supports_multiattach": False,
307         "supports_trusted_certs": True,
308     }
309 
310     def __init__(self, virtapi, read_only=False):
311         super(LibvirtDriver, self).__init__(virtapi)
312 
313         global libvirt
314         if libvirt is None:
315             libvirt = importutils.import_module('libvirt')
316             libvirt_migrate.libvirt = libvirt
317 
318         self._host = host.Host(self._uri(), read_only,
319                                lifecycle_event_handler=self.emit_event,
320                                conn_event_handler=self._handle_conn_event)
321         self._initiator = None
322         self._fc_wwnns = None
323         self._fc_wwpns = None
324         self._caps = None
325         self._supported_perf_events = []
326         self.firewall_driver = firewall.load_driver(
327             DEFAULT_FIREWALL_DRIVER,
328             host=self._host)
329 
330         self.vif_driver = libvirt_vif.LibvirtGenericVIFDriver()
331 
332         # TODO(mriedem): Long-term we should load up the volume drivers on
333         # demand as needed rather than doing this on startup, as there might
334         # be unsupported volume drivers in this list based on the underlying
335         # platform.
336         self.volume_drivers = self._get_volume_drivers()
337 
338         self._disk_cachemode = None
339         self.image_cache_manager = imagecache.ImageCacheManager()
340         self.image_backend = imagebackend.Backend(CONF.use_cow_images)
341 
342         self.disk_cachemodes = {}
343 
344         self.valid_cachemodes = ["default",
345                                  "none",
346                                  "writethrough",
347                                  "writeback",
348                                  "directsync",
349                                  "unsafe",
350                                 ]
351         self._conn_supports_start_paused = CONF.libvirt.virt_type in ('kvm',
352                                                                       'qemu')
353 
354         for mode_str in CONF.libvirt.disk_cachemodes:
355             disk_type, sep, cache_mode = mode_str.partition('=')
356             if cache_mode not in self.valid_cachemodes:
357                 LOG.warning('Invalid cachemode %(cache_mode)s specified '
358                             'for disk type %(disk_type)s.',
359                             {'cache_mode': cache_mode, 'disk_type': disk_type})
360                 continue
361             self.disk_cachemodes[disk_type] = cache_mode
362 
363         self._volume_api = cinder.API()
364         self._image_api = image.API()
365 
366         sysinfo_serial_funcs = {
367             'none': lambda: None,
368             'hardware': self._get_host_sysinfo_serial_hardware,
369             'os': self._get_host_sysinfo_serial_os,
370             'auto': self._get_host_sysinfo_serial_auto,
371         }
372 
373         self._sysinfo_serial_func = sysinfo_serial_funcs.get(
374             CONF.libvirt.sysinfo_serial)
375 
376         self.job_tracker = instancejobtracker.InstanceJobTracker()
377         self._remotefs = remotefs.RemoteFilesystem()
378 
379         self._live_migration_flags = self._block_migration_flags = 0
380         self.active_migrations = {}
381 
382         # Compute reserved hugepages from conf file at the very
383         # beginning to ensure any syntax error will be reported and
384         # avoid any re-calculation when computing resources.
385         self._reserved_hugepages = hardware.numa_get_reserved_huge_pages()
386 
387     def _get_volume_drivers(self):
388         driver_registry = dict()
389 
390         for driver_str in libvirt_volume_drivers:
391             driver_type, _sep, driver = driver_str.partition('=')
392             driver_class = importutils.import_class(driver)
393             try:
394                 driver_registry[driver_type] = driver_class(self._host)
395             except brick_exception.InvalidConnectorProtocol:
396                 LOG.debug('Unable to load volume driver %s. It is not '
397                           'supported on this host.', driver)
398 
399         return driver_registry
400 
401     @property
402     def disk_cachemode(self):
403         if self._disk_cachemode is None:
404             # We prefer 'none' for consistent performance, host crash
405             # safety & migration correctness by avoiding host page cache.
406             # Some filesystems don't support O_DIRECT though. For those we
407             # fallback to 'writethrough' which gives host crash safety, and
408             # is safe for migration provided the filesystem is cache coherent
409             # (cluster filesystems typically are, but things like NFS are not).
410             self._disk_cachemode = "none"
411             if not nova.privsep.utils.supports_direct_io(CONF.instances_path):
412                 self._disk_cachemode = "writethrough"
413         return self._disk_cachemode
414 
415     def _set_cache_mode(self, conf):
416         """Set cache mode on LibvirtConfigGuestDisk object."""
417         try:
418             source_type = conf.source_type
419             driver_cache = conf.driver_cache
420         except AttributeError:
421             return
422 
423         # Shareable disks like for a multi-attach volume need to have the
424         # driver cache disabled.
425         if getattr(conf, 'shareable', False):
426             conf.driver_cache = 'none'
427         else:
428             cache_mode = self.disk_cachemodes.get(source_type,
429                                                   driver_cache)
430             conf.driver_cache = cache_mode
431 
432     def _do_quality_warnings(self):
433         """Warn about potential configuration issues.
434 
435         This will log a warning message for things such as untested driver or
436         host arch configurations in order to indicate potential issues to
437         administrators.
438         """
439         caps = self._host.get_capabilities()
440         hostarch = caps.host.cpu.arch
441         if (CONF.libvirt.virt_type not in ('qemu', 'kvm') or
442             hostarch not in (fields.Architecture.I686,
443                              fields.Architecture.X86_64)):
444             LOG.warning('The libvirt driver is not tested on '
445                         '%(type)s/%(arch)s by the OpenStack project and '
446                         'thus its quality can not be ensured. For more '
447                         'information, see: https://docs.openstack.org/'
448                         'nova/latest/user/support-matrix.html',
449                         {'type': CONF.libvirt.virt_type, 'arch': hostarch})
450 
451         if CONF.vnc.keymap:
452             LOG.warning('The option "[vnc] keymap" has been deprecated '
453                         'in favor of configuration within the guest. '
454                         'Update nova.conf to address this change and '
455                         'refer to bug #1682020 for more information.')
456 
457         if CONF.spice.keymap:
458             LOG.warning('The option "[spice] keymap" has been deprecated '
459                         'in favor of configuration within the guest. '
460                         'Update nova.conf to address this change and '
461                         'refer to bug #1682020 for more information.')
462 
463     def _handle_conn_event(self, enabled, reason):
464         LOG.info("Connection event '%(enabled)d' reason '%(reason)s'",
465                  {'enabled': enabled, 'reason': reason})
466         self._set_host_enabled(enabled, reason)
467 
468     def init_host(self, host):
469         self._host.initialize()
470 
471         self._do_quality_warnings()
472 
473         self._parse_migration_flags()
474 
475         self._supported_perf_events = self._get_supported_perf_events()
476 
477         self._set_multiattach_support()
478 
479         if (CONF.libvirt.virt_type == 'lxc' and
480                 not (CONF.libvirt.uid_maps and CONF.libvirt.gid_maps)):
481             LOG.warning("Running libvirt-lxc without user namespaces is "
482                         "dangerous. Containers spawned by Nova will be run "
483                         "as the host's root user. It is highly suggested "
484                         "that user namespaces be used in a public or "
485                         "multi-tenant environment.")
486 
487         # Stop libguestfs using KVM unless we're also configured
488         # to use this. This solves problem where people need to
489         # stop Nova use of KVM because nested-virt is broken
490         if CONF.libvirt.virt_type != "kvm":
491             guestfs.force_tcg()
492 
493         if not self._host.has_min_version(MIN_LIBVIRT_VERSION):
494             raise exception.InternalError(
495                 _('Nova requires libvirt version %s or greater.') %
496                 libvirt_utils.version_to_string(MIN_LIBVIRT_VERSION))
497 
498         if CONF.libvirt.virt_type in ("qemu", "kvm"):
499             if self._host.has_min_version(hv_ver=MIN_QEMU_VERSION):
500                 # "qemu-img info" calls are version dependent, so we need to
501                 # store the version in the images module.
502                 images.QEMU_VERSION = self._host.get_connection().getVersion()
503             else:
504                 raise exception.InternalError(
505                     _('Nova requires QEMU version %s or greater.') %
506                     libvirt_utils.version_to_string(MIN_QEMU_VERSION))
507 
508         if CONF.libvirt.virt_type == 'parallels':
509             if not self._host.has_min_version(hv_ver=MIN_VIRTUOZZO_VERSION):
510                 raise exception.InternalError(
511                     _('Nova requires Virtuozzo version %s or greater.') %
512                     libvirt_utils.version_to_string(MIN_VIRTUOZZO_VERSION))
513 
514         # Give the cloud admin a heads up if we are intending to
515         # change the MIN_LIBVIRT_VERSION in the next release.
516         if not self._host.has_min_version(NEXT_MIN_LIBVIRT_VERSION):
517             LOG.warning('Running Nova with a libvirt version less than '
518                         '%(version)s is deprecated. The required minimum '
519                         'version of libvirt will be raised to %(version)s '
520                         'in the next release.',
521                         {'version': libvirt_utils.version_to_string(
522                             NEXT_MIN_LIBVIRT_VERSION)})
523         if (CONF.libvirt.virt_type in ("qemu", "kvm") and
524             not self._host.has_min_version(hv_ver=NEXT_MIN_QEMU_VERSION)):
525             LOG.warning('Running Nova with a QEMU version less than '
526                         '%(version)s is deprecated. The required minimum '
527                         'version of QEMU will be raised to %(version)s '
528                         'in the next release.',
529                         {'version': libvirt_utils.version_to_string(
530                             NEXT_MIN_QEMU_VERSION)})
531 
532         kvm_arch = fields.Architecture.from_host()
533         if (CONF.libvirt.virt_type in ('kvm', 'qemu') and
534             kvm_arch in MIN_LIBVIRT_OTHER_ARCH and
535                 not self._host.has_min_version(
536                     MIN_LIBVIRT_OTHER_ARCH.get(kvm_arch))):
537             raise exception.InternalError(
538                 _('Running Nova with qemu/kvm virt_type on %(arch)s '
539                   'requires libvirt version %(libvirt_ver)s or greater') %
540                 {'arch': kvm_arch,
541                  'libvirt_ver': libvirt_utils.version_to_string(
542                      MIN_LIBVIRT_OTHER_ARCH.get(kvm_arch))})
543 
544         # TODO(sbauza): Remove this code once mediated devices are persisted
545         # across reboots.
546         if self._host.has_min_version(MIN_LIBVIRT_MDEV_SUPPORT):
547             self._recreate_assigned_mediated_devices()
548 
549     @staticmethod
550     def _is_existing_mdev(uuid):
551         # FIXME(sbauza): Some kernel can have a uevent race meaning that the
552         # libvirt daemon won't know when a mediated device is created unless
553         # you restart that daemon. Until all kernels we support are not having
554         # that possible race, check the sysfs directly instead of asking the
555         # libvirt API.
556         # See https://bugzilla.redhat.com/show_bug.cgi?id=1376907 for ref.
557         return os.path.exists('/sys/bus/mdev/devices/{0}'.format(uuid))
558 
559     def _recreate_assigned_mediated_devices(self):
560         """Recreate assigned mdevs that could have disappeared if we reboot
561         the host.
562         """
563         mdevs = self._get_all_assigned_mediated_devices()
564         requested_types = self._get_supported_vgpu_types()
565         for (mdev_uuid, instance_uuid) in six.iteritems(mdevs):
566             if not self._is_existing_mdev(mdev_uuid):
567                 self._create_new_mediated_device(requested_types, mdev_uuid)
568 
569     def _set_multiattach_support(self):
570         # Check to see if multiattach is supported. Based on bugzilla
571         # https://bugzilla.redhat.com/show_bug.cgi?id=1378242 and related
572         # clones, the shareable flag on a disk device will only work with
573         # qemu<2.10 or libvirt>=3.10. So check those versions here and set
574         # the capability appropriately.
575         if (self._host.has_min_version(lv_ver=MIN_LIBVIRT_MULTIATTACH) or
576                 not self._host.has_min_version(hv_ver=(2, 10, 0))):
577             self.capabilities['supports_multiattach'] = True
578         else:
579             LOG.debug('Volume multiattach is not supported based on current '
580                       'versions of QEMU and libvirt. QEMU must be less than '
581                       '2.10 or libvirt must be greater than or equal to 3.10.')
582 
583     def _prepare_migration_flags(self):
584         migration_flags = 0
585 
586         migration_flags |= libvirt.VIR_MIGRATE_LIVE
587 
588         # Adding p2p flag only if xen is not in use, because xen does not
589         # support p2p migrations
590         if CONF.libvirt.virt_type != 'xen':
591             migration_flags |= libvirt.VIR_MIGRATE_PEER2PEER
592 
593         # Adding VIR_MIGRATE_UNDEFINE_SOURCE because, without it, migrated
594         # instance will remain defined on the source host
595         migration_flags |= libvirt.VIR_MIGRATE_UNDEFINE_SOURCE
596 
597         # Adding VIR_MIGRATE_PERSIST_DEST to persist the VM on the
598         # destination host
599         migration_flags |= libvirt.VIR_MIGRATE_PERSIST_DEST
600 
601         live_migration_flags = block_migration_flags = migration_flags
602 
603         # Adding VIR_MIGRATE_NON_SHARED_INC, otherwise all block-migrations
604         # will be live-migrations instead
605         block_migration_flags |= libvirt.VIR_MIGRATE_NON_SHARED_INC
606 
607         return (live_migration_flags, block_migration_flags)
608 
609     def _handle_live_migration_tunnelled(self, migration_flags):
610         if (CONF.libvirt.live_migration_tunnelled is None or
611                 CONF.libvirt.live_migration_tunnelled):
612             migration_flags |= libvirt.VIR_MIGRATE_TUNNELLED
613         return migration_flags
614 
615     def _is_post_copy_available(self):
616         return self._host.has_min_version(lv_ver=MIN_LIBVIRT_POSTCOPY_VERSION)
617 
618     def _is_virtlogd_available(self):
619         return self._host.has_min_version(MIN_LIBVIRT_VIRTLOGD,
620                                           MIN_QEMU_VIRTLOGD)
621 
622     def _is_native_luks_available(self):
623         return self._host.has_min_version(MIN_LIBVIRT_LUKS_VERSION,
624                                           MIN_QEMU_LUKS_VERSION)
625 
626     def _handle_live_migration_post_copy(self, migration_flags):
627         if CONF.libvirt.live_migration_permit_post_copy:
628             if self._is_post_copy_available():
629                 migration_flags |= libvirt.VIR_MIGRATE_POSTCOPY
630             else:
631                 LOG.info('The live_migration_permit_post_copy is set '
632                          'to True, but it is not supported.')
633         return migration_flags
634 
635     def _handle_live_migration_auto_converge(self, migration_flags):
636         if (self._is_post_copy_available() and
637                 (migration_flags & libvirt.VIR_MIGRATE_POSTCOPY) != 0):
638             LOG.info('The live_migration_permit_post_copy is set to '
639                      'True and post copy live migration is available '
640                      'so auto-converge will not be in use.')
641         elif CONF.libvirt.live_migration_permit_auto_converge:
642             migration_flags |= libvirt.VIR_MIGRATE_AUTO_CONVERGE
643         return migration_flags
644 
645     def _parse_migration_flags(self):
646         (live_migration_flags,
647             block_migration_flags) = self._prepare_migration_flags()
648 
649         live_migration_flags = self._handle_live_migration_tunnelled(
650             live_migration_flags)
651         block_migration_flags = self._handle_live_migration_tunnelled(
652             block_migration_flags)
653 
654         live_migration_flags = self._handle_live_migration_post_copy(
655             live_migration_flags)
656         block_migration_flags = self._handle_live_migration_post_copy(
657             block_migration_flags)
658 
659         live_migration_flags = self._handle_live_migration_auto_converge(
660             live_migration_flags)
661         block_migration_flags = self._handle_live_migration_auto_converge(
662             block_migration_flags)
663 
664         self._live_migration_flags = live_migration_flags
665         self._block_migration_flags = block_migration_flags
666 
667     # TODO(sahid): This method is targeted for removal when the tests
668     # have been updated to avoid its use
669     #
670     # All libvirt API calls on the libvirt.Connect object should be
671     # encapsulated by methods on the nova.virt.libvirt.host.Host
672     # object, rather than directly invoking the libvirt APIs. The goal
673     # is to avoid a direct dependency on the libvirt API from the
674     # driver.py file.
675     def _get_connection(self):
676         return self._host.get_connection()
677 
678     _conn = property(_get_connection)
679 
680     @staticmethod
681     def _uri():
682         if CONF.libvirt.virt_type == 'uml':
683             uri = CONF.libvirt.connection_uri or 'uml:///system'
684         elif CONF.libvirt.virt_type == 'xen':
685             uri = CONF.libvirt.connection_uri or 'xen:///'
686         elif CONF.libvirt.virt_type == 'lxc':
687             uri = CONF.libvirt.connection_uri or 'lxc:///'
688         elif CONF.libvirt.virt_type == 'parallels':
689             uri = CONF.libvirt.connection_uri or 'parallels:///system'
690         else:
691             uri = CONF.libvirt.connection_uri or 'qemu:///system'
692         return uri
693 
694     @staticmethod
695     def _live_migration_uri(dest):
696         uris = {
697             'kvm': 'qemu+%s://%s/system',
698             'qemu': 'qemu+%s://%s/system',
699             'xen': 'xenmigr://%s/system',
700             'parallels': 'parallels+tcp://%s/system',
701         }
702         virt_type = CONF.libvirt.virt_type
703         # TODO(pkoniszewski): Remove fetching live_migration_uri in Pike
704         uri = CONF.libvirt.live_migration_uri
705         if uri:
706             return uri % dest
707 
708         uri = uris.get(virt_type)
709         if uri is None:
710             raise exception.LiveMigrationURINotAvailable(virt_type=virt_type)
711 
712         str_format = (dest,)
713         if virt_type in ('kvm', 'qemu'):
714             scheme = CONF.libvirt.live_migration_scheme or 'tcp'
715             str_format = (scheme, dest)
716         return uris.get(virt_type) % str_format
717 
718     @staticmethod
719     def _migrate_uri(dest):
720         uri = None
721         # Only QEMU live migrations supports migrate-uri parameter
722         virt_type = CONF.libvirt.virt_type
723         if virt_type in ('qemu', 'kvm'):
724             # QEMU accept two schemes: tcp and rdma.  By default
725             # libvirt build the URI using the remote hostname and the
726             # tcp schema.
727             uri = 'tcp://%s' % dest
728         # Because dest might be of type unicode, here we might return value of
729         # type unicode as well which is not acceptable by libvirt python
730         # binding when Python 2.7 is in use, so let's convert it explicitly
731         # back to string. When Python 3.x is in use, libvirt python binding
732         # accepts unicode type so it is completely fine to do a no-op str(uri)
733         # conversion which will return value of type unicode.
734         return uri and str(uri)
735 
736     def instance_exists(self, instance):
737         """Efficient override of base instance_exists method."""
738         try:
739             self._host.get_guest(instance)
740             return True
741         except (exception.InternalError, exception.InstanceNotFound):
742             return False
743 
744     def estimate_instance_overhead(self, instance_info):
745         overhead = super(LibvirtDriver, self).estimate_instance_overhead(
746             instance_info)
747         if isinstance(instance_info, objects.Flavor):
748             # A flavor object is passed during case of migrate
749             emu_policy = hardware.get_emulator_thread_policy_constraint(
750                 instance_info)
751             if emu_policy == fields.CPUEmulatorThreadsPolicy.ISOLATE:
752                 overhead['vcpus'] += 1
753         else:
754             # An instance object is passed during case of spawing or a
755             # dict is passed when computing resource for an instance
756             numa_topology = hardware.instance_topology_from_instance(
757                 instance_info)
758             if numa_topology and numa_topology.emulator_threads_isolated:
759                 overhead['vcpus'] += 1
760         return overhead
761 
762     def list_instances(self):
763         names = []
764         for guest in self._host.list_guests(only_running=False):
765             names.append(guest.name)
766 
767         return names
768 
769     def list_instance_uuids(self):
770         uuids = []
771         for guest in self._host.list_guests(only_running=False):
772             uuids.append(guest.uuid)
773 
774         return uuids
775 
776     def plug_vifs(self, instance, network_info):
777         """Plug VIFs into networks."""
778         for vif in network_info:
779             self.vif_driver.plug(instance, vif)
780 
781     def _unplug_vifs(self, instance, network_info, ignore_errors):
782         """Unplug VIFs from networks."""
783         for vif in network_info:
784             try:
785                 self.vif_driver.unplug(instance, vif)
786             except exception.NovaException:
787                 if not ignore_errors:
788                     raise
789 
790     def unplug_vifs(self, instance, network_info):
791         self._unplug_vifs(instance, network_info, False)
792 
793     def _teardown_container(self, instance):
794         inst_path = libvirt_utils.get_instance_path(instance)
795         container_dir = os.path.join(inst_path, 'rootfs')
796         rootfs_dev = instance.system_metadata.get('rootfs_device_name')
797         LOG.debug('Attempting to teardown container at path %(dir)s with '
798                   'root device: %(rootfs_dev)s',
799                   {'dir': container_dir, 'rootfs_dev': rootfs_dev},
800                   instance=instance)
801         disk_api.teardown_container(container_dir, rootfs_dev)
802 
803     def _destroy(self, instance, attempt=1):
804         try:
805             guest = self._host.get_guest(instance)
806             if CONF.serial_console.enabled:
807                 # This method is called for several events: destroy,
808                 # rebuild, hard-reboot, power-off - For all of these
809                 # events we want to release the serial ports acquired
810                 # for the guest before destroying it.
811                 serials = self._get_serial_ports_from_guest(guest)
812                 for hostname, port in serials:
813                     serial_console.release_port(host=hostname, port=port)
814         except exception.InstanceNotFound:
815             guest = None
816 
817         # If the instance is already terminated, we're still happy
818         # Otherwise, destroy it
819         old_domid = -1
820         if guest is not None:
821             try:
822                 old_domid = guest.id
823                 guest.poweroff()
824 
825             except libvirt.libvirtError as e:
826                 is_okay = False
827                 errcode = e.get_error_code()
828                 if errcode == libvirt.VIR_ERR_NO_DOMAIN:
829                     # Domain already gone. This can safely be ignored.
830                     is_okay = True
831                 elif errcode == libvirt.VIR_ERR_OPERATION_INVALID:
832                     # If the instance is already shut off, we get this:
833                     # Code=55 Error=Requested operation is not valid:
834                     # domain is not running
835 
836                     state = guest.get_power_state(self._host)
837                     if state == power_state.SHUTDOWN:
838                         is_okay = True
839                 elif errcode == libvirt.VIR_ERR_INTERNAL_ERROR:
840                     errmsg = e.get_error_message()
841                     if (CONF.libvirt.virt_type == 'lxc' and
842                         errmsg == 'internal error: '
843                                   'Some processes refused to die'):
844                         # Some processes in the container didn't die
845                         # fast enough for libvirt. The container will
846                         # eventually die. For now, move on and let
847                         # the wait_for_destroy logic take over.
848                         is_okay = True
849                 elif errcode == libvirt.VIR_ERR_OPERATION_TIMEOUT:
850                     LOG.warning("Cannot destroy instance, operation time out",
851                                 instance=instance)
852                     reason = _("operation time out")
853                     raise exception.InstancePowerOffFailure(reason=reason)
854                 elif errcode == libvirt.VIR_ERR_SYSTEM_ERROR:
855                     if e.get_int1() == errno.EBUSY:
856                         # NOTE(danpb): When libvirt kills a process it sends it
857                         # SIGTERM first and waits 10 seconds. If it hasn't gone
858                         # it sends SIGKILL and waits another 5 seconds. If it
859                         # still hasn't gone then you get this EBUSY error.
860                         # Usually when a QEMU process fails to go away upon
861                         # SIGKILL it is because it is stuck in an
862                         # uninterruptible kernel sleep waiting on I/O from
863                         # some non-responsive server.
864                         # Given the CPU load of the gate tests though, it is
865                         # conceivable that the 15 second timeout is too short,
866                         # particularly if the VM running tempest has a high
867                         # steal time from the cloud host. ie 15 wallclock
868                         # seconds may have passed, but the VM might have only
869                         # have a few seconds of scheduled run time.
870                         LOG.warning('Error from libvirt during destroy. '
871                                     'Code=%(errcode)s Error=%(e)s; '
872                                     'attempt %(attempt)d of 3',
873                                     {'errcode': errcode, 'e': e,
874                                      'attempt': attempt},
875                                     instance=instance)
876                         with excutils.save_and_reraise_exception() as ctxt:
877                             # Try up to 3 times before giving up.
878                             if attempt < 3:
879                                 ctxt.reraise = False
880                                 self._destroy(instance, attempt + 1)
881                                 return
882 
883                 if not is_okay:
884                     with excutils.save_and_reraise_exception():
885                         LOG.error('Error from libvirt during destroy. '
886                                   'Code=%(errcode)s Error=%(e)s',
887                                   {'errcode': errcode, 'e': e},
888                                   instance=instance)
889 
890         def _wait_for_destroy(expected_domid):
891             """Called at an interval until the VM is gone."""
892             # NOTE(vish): If the instance disappears during the destroy
893             #             we ignore it so the cleanup can still be
894             #             attempted because we would prefer destroy to
895             #             never fail.
896             try:
897                 dom_info = self.get_info(instance)
898                 state = dom_info.state
899                 new_domid = dom_info.internal_id
900             except exception.InstanceNotFound:
901                 LOG.debug("During wait destroy, instance disappeared.",
902                           instance=instance)
903                 state = power_state.SHUTDOWN
904 
905             if state == power_state.SHUTDOWN:
906                 LOG.info("Instance destroyed successfully.", instance=instance)
907                 raise loopingcall.LoopingCallDone()
908 
909             # NOTE(wangpan): If the instance was booted again after destroy,
910             #                this may be an endless loop, so check the id of
911             #                domain here, if it changed and the instance is
912             #                still running, we should destroy it again.
913             # see https://bugs.launchpad.net/nova/+bug/1111213 for more details
914             if new_domid != expected_domid:
915                 LOG.info("Instance may be started again.", instance=instance)
916                 kwargs['is_running'] = True
917                 raise loopingcall.LoopingCallDone()
918 
919         kwargs = {'is_running': False}
920         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_destroy,
921                                                      old_domid)
922         timer.start(interval=0.5).wait()
923         if kwargs['is_running']:
924             LOG.info("Going to destroy instance again.", instance=instance)
925             self._destroy(instance)
926         else:
927             # NOTE(GuanQiang): teardown container to avoid resource leak
928             if CONF.libvirt.virt_type == 'lxc':
929                 self._teardown_container(instance)
930 
931     def destroy(self, context, instance, network_info, block_device_info=None,
932                 destroy_disks=True):
933         self._destroy(instance)
934         self.cleanup(context, instance, network_info, block_device_info,
935                      destroy_disks)
936 
937     def _undefine_domain(self, instance):
938         try:
939             guest = self._host.get_guest(instance)
940             try:
941                 support_uefi = self._has_uefi_support()
942                 guest.delete_configuration(support_uefi)
943             except libvirt.libvirtError as e:
944                 with excutils.save_and_reraise_exception() as ctxt:
945                     errcode = e.get_error_code()
946                     if errcode == libvirt.VIR_ERR_NO_DOMAIN:
947                         LOG.debug("Called undefine, but domain already gone.",
948                                   instance=instance)
949                         ctxt.reraise = False
950                     else:
951                         LOG.error('Error from libvirt during undefine. '
952                                   'Code=%(errcode)s Error=%(e)s',
953                                   {'errcode': errcode,
954                                    'e': encodeutils.exception_to_unicode(e)},
955                                   instance=instance)
956         except exception.InstanceNotFound:
957             pass
958 
959     def cleanup(self, context, instance, network_info, block_device_info=None,
960                 destroy_disks=True, migrate_data=None, destroy_vifs=True):
961         if destroy_vifs:
962             self._unplug_vifs(instance, network_info, True)
963 
964         # Continue attempting to remove firewall filters for the instance
965         # until it's done or there is a failure to remove the filters. If
966         # unfilter fails because the instance is not yet shutdown, try to
967         # destroy the guest again and then retry the unfilter.
968         while True:
969             try:
970                 self.unfilter_instance(instance, network_info)
971                 break
972             except libvirt.libvirtError as e:
973                 try:
974                     state = self.get_info(instance).state
975                 except exception.InstanceNotFound:
976                     state = power_state.SHUTDOWN
977 
978                 if state != power_state.SHUTDOWN:
979                     LOG.warning("Instance may be still running, destroy "
980                                 "it again.", instance=instance)
981                     self._destroy(instance)
982                 else:
983                     errcode = e.get_error_code()
984                     LOG.exception(_('Error from libvirt during unfilter. '
985                                     'Code=%(errcode)s Error=%(e)s'),
986                                   {'errcode': errcode, 'e': e},
987                                   instance=instance)
988                     reason = _("Error unfiltering instance.")
989                     raise exception.InstanceTerminationFailure(reason=reason)
990             except Exception:
991                 raise
992 
993         # FIXME(wangpan): if the instance is booted again here, such as the
994         #                 soft reboot operation boot it here, it will become
995         #                 "running deleted", should we check and destroy it
996         #                 at the end of this method?
997 
998         # NOTE(vish): we disconnect from volumes regardless
999         block_device_mapping = driver.block_device_info_get_mapping(
1000             block_device_info)
1001         for vol in block_device_mapping:
1002             connection_info = vol['connection_info']
1003             disk_dev = vol['mount_device']
1004             if disk_dev is not None:
1005                 disk_dev = disk_dev.rpartition("/")[2]
1006             try:
1007                 self._disconnect_volume(context, connection_info, instance)
1008             except Exception as exc:
1009                 with excutils.save_and_reraise_exception() as ctxt:
1010                     if destroy_disks:
1011                         # Don't block on Volume errors if we're trying to
1012                         # delete the instance as we may be partially created
1013                         # or deleted
1014                         ctxt.reraise = False
1015                         LOG.warning(
1016                             "Ignoring Volume Error on vol %(vol_id)s "
1017                             "during delete %(exc)s",
1018                             {'vol_id': vol.get('volume_id'),
1019                              'exc': encodeutils.exception_to_unicode(exc)},
1020                             instance=instance)
1021 
1022         if destroy_disks:
1023             # NOTE(haomai): destroy volumes if needed
1024             if CONF.libvirt.images_type == 'lvm':
1025                 self._cleanup_lvm(instance, block_device_info)
1026             if CONF.libvirt.images_type == 'rbd':
1027                 self._cleanup_rbd(instance)
1028 
1029         is_shared_block_storage = False
1030         if migrate_data and 'is_shared_block_storage' in migrate_data:
1031             is_shared_block_storage = migrate_data.is_shared_block_storage
1032         if destroy_disks or is_shared_block_storage:
1033             attempts = int(instance.system_metadata.get('clean_attempts',
1034                                                         '0'))
1035             success = self.delete_instance_files(instance)
1036             # NOTE(mriedem): This is used in the _run_pending_deletes periodic
1037             # task in the compute manager. The tight coupling is not great...
1038             instance.system_metadata['clean_attempts'] = str(attempts + 1)
1039             if success:
1040                 instance.cleaned = True
1041             instance.save()
1042 
1043         self._undefine_domain(instance)
1044 
1045     def _detach_encrypted_volumes(self, instance, block_device_info):
1046         """Detaches encrypted volumes attached to instance."""
1047         disks = self._get_instance_disk_info(instance, block_device_info)
1048         encrypted_volumes = filter(dmcrypt.is_encrypted,
1049                                    [disk['path'] for disk in disks])
1050         for path in encrypted_volumes:
1051             dmcrypt.delete_volume(path)
1052 
1053     def _get_serial_ports_from_guest(self, guest, mode=None):
1054         """Returns an iterator over serial port(s) configured on guest.
1055 
1056         :param mode: Should be a value in (None, bind, connect)
1057         """
1058         xml = guest.get_xml_desc()
1059         tree = etree.fromstring(xml)
1060 
1061         # The 'serial' device is the base for x86 platforms. Other platforms
1062         # (e.g. kvm on system z = S390X) can only use 'console' devices.
1063         xpath_mode = "[@mode='%s']" % mode if mode else ""
1064         serial_tcp = "./devices/serial[@type='tcp']/source" + xpath_mode
1065         console_tcp = "./devices/console[@type='tcp']/source" + xpath_mode
1066 
1067         tcp_devices = tree.findall(serial_tcp)
1068         if len(tcp_devices) == 0:
1069             tcp_devices = tree.findall(console_tcp)
1070         for source in tcp_devices:
1071             yield (source.get("host"), int(source.get("service")))
1072 
1073     def _get_scsi_controller_max_unit(self, guest):
1074         """Returns the max disk unit used by scsi controller"""
1075         xml = guest.get_xml_desc()
1076         tree = etree.fromstring(xml)
1077         addrs = "./devices/disk[@device='disk']/address[@type='drive']"
1078 
1079         ret = []
1080         for obj in tree.findall(addrs):
1081             ret.append(int(obj.get('unit', 0)))
1082         return max(ret)
1083 
1084     @staticmethod
1085     def _get_rbd_driver():
1086         return rbd_utils.RBDDriver(
1087                 pool=CONF.libvirt.images_rbd_pool,
1088                 ceph_conf=CONF.libvirt.images_rbd_ceph_conf,
1089                 rbd_user=CONF.libvirt.rbd_user)
1090 
1091     def _cleanup_rbd(self, instance):
1092         # NOTE(nic): On revert_resize, the cleanup steps for the root
1093         # volume are handled with an "rbd snap rollback" command,
1094         # and none of this is needed (and is, in fact, harmful) so
1095         # filter out non-ephemerals from the list
1096         if instance.task_state == task_states.RESIZE_REVERTING:
1097             filter_fn = lambda disk: (disk.startswith(instance.uuid) and
1098                                       disk.endswith('disk.local'))
1099         else:
1100             filter_fn = lambda disk: disk.startswith(instance.uuid)
1101         LibvirtDriver._get_rbd_driver().cleanup_volumes(filter_fn)
1102 
1103     def _cleanup_lvm(self, instance, block_device_info):
1104         """Delete all LVM disks for given instance object."""
1105         if instance.get('ephemeral_key_uuid') is not None:
1106             self._detach_encrypted_volumes(instance, block_device_info)
1107 
1108         disks = self._lvm_disks(instance)
1109         if disks:
1110             lvm.remove_volumes(disks)
1111 
1112     def _lvm_disks(self, instance):
1113         """Returns all LVM disks for given instance object."""
1114         if CONF.libvirt.images_volume_group:
1115             vg = os.path.join('/dev', CONF.libvirt.images_volume_group)
1116             if not os.path.exists(vg):
1117                 return []
1118             pattern = '%s_' % instance.uuid
1119 
1120             def belongs_to_instance(disk):
1121                 return disk.startswith(pattern)
1122 
1123             def fullpath(name):
1124                 return os.path.join(vg, name)
1125 
1126             logical_volumes = lvm.list_volumes(vg)
1127 
1128             disks = [fullpath(disk) for disk in logical_volumes
1129                      if belongs_to_instance(disk)]
1130             return disks
1131         return []
1132 
1133     def get_volume_connector(self, instance):
1134         root_helper = utils.get_root_helper()
1135         return connector.get_connector_properties(
1136             root_helper, CONF.my_block_storage_ip,
1137             CONF.libvirt.volume_use_multipath,
1138             enforce_multipath=True,
1139             host=CONF.host)
1140 
1141     def _cleanup_resize(self, context, instance, network_info):
1142         inst_base = libvirt_utils.get_instance_path(instance)
1143         target = inst_base + '_resize'
1144 
1145         # Deletion can fail over NFS, so retry the deletion as required.
1146         # Set maximum attempt as 5, most test can remove the directory
1147         # for the second time.
1148         attempts = 0
1149         while(os.path.exists(target) and attempts < 5):
1150             shutil.rmtree(target, ignore_errors=True)
1151             if os.path.exists(target):
1152                 time.sleep(random.randint(20, 200) / 100.0)
1153             attempts += 1
1154 
1155         # NOTE(mriedem): Some image backends will recreate the instance path
1156         # and disk.info during init, and all we need the root disk for
1157         # here is removing cloned snapshots which is backend-specific, so
1158         # check that first before initializing the image backend object. If
1159         # there is ever an image type that supports clone *and* re-creates
1160         # the instance directory and disk.info on init, this condition will
1161         # need to be re-visited to make sure that backend doesn't re-create
1162         # the disk. Refer to bugs: 1666831 1728603 1769131
1163         if self.image_backend.backend(CONF.libvirt.images_type).SUPPORTS_CLONE:
1164             root_disk = self.image_backend.by_name(instance, 'disk')
1165             # TODO(nic): Set ignore_errors=False in a future release.
1166             # It is set to True here to avoid any upgrade issues surrounding
1167             # instances being in pending resize state when the software is
1168             # updated; in that case there will be no snapshot to remove.
1169             # Once it can be reasonably assumed that no such instances exist
1170             # in the wild anymore, it should be set back to False
1171             # (the default) so it will throw errors, like it should.
1172             if root_disk.exists():
1173                 root_disk.remove_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME,
1174                                       ignore_errors=True)
1175 
1176         if instance.host != CONF.host:
1177             self._undefine_domain(instance)
1178             self.unplug_vifs(instance, network_info)
1179             self.unfilter_instance(instance, network_info)
1180 
1181     def _get_volume_driver(self, connection_info):
1182         driver_type = connection_info.get('driver_volume_type')
1183         if driver_type not in self.volume_drivers:
1184             raise exception.VolumeDriverNotFound(driver_type=driver_type)
1185         return self.volume_drivers[driver_type]
1186 
1187     def _connect_volume(self, context, connection_info, instance,
1188                         encryption=None, allow_native_luks=True):
1189         vol_driver = self._get_volume_driver(connection_info)
1190         vol_driver.connect_volume(connection_info, instance)
1191         self._attach_encryptor(context, connection_info, encryption,
1192                                allow_native_luks)
1193 
1194     def _should_disconnect_target(self, context, connection_info, instance):
1195         connection_count = 0
1196 
1197         # NOTE(jdg): Multiattach is a special case (not to be confused
1198         # with shared_targets). With multiattach we may have a single volume
1199         # attached multiple times to *this* compute node (ie Server-1 and
1200         # Server-2).  So, if we receive a call to delete the attachment for
1201         # Server-1 we need to take special care to make sure that the Volume
1202         # isn't also attached to another Server on this Node.  Otherwise we
1203         # will indiscriminantly delete the connection for all Server and that's
1204         # no good.  So check if it's attached multiple times on this node
1205         # if it is we skip the call to brick to delete the connection.
1206         if connection_info.get('multiattach', False):
1207             volume = self._volume_api.get(
1208                 context,
1209                 driver_block_device.get_volume_id(connection_info))
1210             attachments = volume.get('attachments', {})
1211             if len(attachments) > 1:
1212                 # First we get a list of all Server UUID's associated with
1213                 # this Host (Compute Node).  We're going to use this to
1214                 # determine if the Volume being detached is also in-use by
1215                 # another Server on this Host, ie just check to see if more
1216                 # than one attachment.server_id for this volume is in our
1217                 # list of Server UUID's for this Host
1218                 servers_this_host = objects.InstanceList.get_uuids_by_host(
1219                     context, instance.host)
1220 
1221                 # NOTE(jdg): nova.volume.cinder translates the
1222                 # volume['attachments'] response into a dict which includes
1223                 # the Server UUID as the key, so we're using that
1224                 # here to check against our server_this_host list
1225                 for server_id, data in attachments.items():
1226                     if server_id in servers_this_host:
1227                         connection_count += 1
1228         return (False if connection_count > 1 else True)
1229 
1230     def _disconnect_volume(self, context, connection_info, instance,
1231                            encryption=None):
1232         self._detach_encryptor(context, connection_info, encryption=encryption)
1233         if self._should_disconnect_target(context, connection_info, instance):
1234             vol_driver = self._get_volume_driver(connection_info)
1235             vol_driver.disconnect_volume(connection_info, instance)
1236         else:
1237             LOG.info("Detected multiple connections on this host for volume: "
1238                      "%s, skipping target disconnect.",
1239                      driver_block_device.get_volume_id(connection_info),
1240                      instance=instance)
1241 
1242     def _extend_volume(self, connection_info, instance):
1243         vol_driver = self._get_volume_driver(connection_info)
1244         return vol_driver.extend_volume(connection_info, instance)
1245 
1246     def _use_native_luks(self, encryption=None):
1247         """Is LUKS the required provider and native QEMU LUKS available
1248         """
1249         provider = None
1250         if encryption:
1251             provider = encryption.get('provider', None)
1252         if provider in encryptors.LEGACY_PROVIDER_CLASS_TO_FORMAT_MAP:
1253             provider = encryptors.LEGACY_PROVIDER_CLASS_TO_FORMAT_MAP[provider]
1254         return provider == encryptors.LUKS and self._is_native_luks_available()
1255 
1256     def _get_volume_config(self, connection_info, disk_info):
1257         vol_driver = self._get_volume_driver(connection_info)
1258         conf = vol_driver.get_config(connection_info, disk_info)
1259         self._set_cache_mode(conf)
1260         return conf
1261 
1262     def _get_volume_encryptor(self, connection_info, encryption):
1263         root_helper = utils.get_root_helper()
1264         return encryptors.get_volume_encryptor(root_helper=root_helper,
1265                                                keymgr=key_manager.API(CONF),
1266                                                connection_info=connection_info,
1267                                                **encryption)
1268 
1269     def _get_volume_encryption(self, context, connection_info):
1270         """Get the encryption metadata dict if it is not provided
1271         """
1272         encryption = {}
1273         volume_id = driver_block_device.get_volume_id(connection_info)
1274         if volume_id:
1275             encryption = encryptors.get_encryption_metadata(context,
1276                             self._volume_api, volume_id, connection_info)
1277         return encryption
1278 
1279     def _attach_encryptor(self, context, connection_info, encryption,
1280                           allow_native_luks):
1281         """Attach the frontend encryptor if one is required by the volume.
1282 
1283         The request context is only used when an encryption metadata dict is
1284         not provided. The encryption metadata dict being populated is then used
1285         to determine if an attempt to attach the encryptor should be made.
1286 
1287         If native LUKS decryption is enabled then create a Libvirt volume
1288         secret containing the LUKS passphrase for the volume.
1289         """
1290         if encryption is None:
1291             encryption = self._get_volume_encryption(context, connection_info)
1292 
1293         if (encryption and allow_native_luks and
1294             self._use_native_luks(encryption)):
1295             # NOTE(lyarwood): Fetch the associated key for the volume and
1296             # decode the passphrase from the key.
1297             # FIXME(lyarwood): c-vol currently creates symmetric keys for use
1298             # with volumes, leading to the binary to hex to string conversion
1299             # below.
1300             keymgr = key_manager.API(CONF)
1301             key = keymgr.get(context, encryption['encryption_key_id'])
1302             key_encoded = key.get_encoded()
1303             passphrase = binascii.hexlify(key_encoded).decode('utf-8')
1304 
1305             # NOTE(lyarwood): Retain the behaviour of the original os-brick
1306             # encryptors and format any volume that does not identify as
1307             # encrypted with LUKS.
1308             # FIXME(lyarwood): Remove this once c-vol correctly formats
1309             # encrypted volumes during their initial creation:
1310             # https://bugs.launchpad.net/cinder/+bug/1739442
1311             device_path = connection_info.get('data').get('device_path')
1312             if device_path:
1313                 root_helper = utils.get_root_helper()
1314                 if not luks_encryptor.is_luks(root_helper, device_path):
1315                     encryptor = self._get_volume_encryptor(connection_info,
1316                                                            encryption)
1317                     encryptor._format_volume(passphrase, **encryption)
1318 
1319             # NOTE(lyarwood): Store the passphrase as a libvirt secret locally
1320             # on the compute node. This secret is used later when generating
1321             # the volume config.
1322             volume_id = driver_block_device.get_volume_id(connection_info)
1323             self._host.create_secret('volume', volume_id, password=passphrase)
1324         elif encryption:
1325             encryptor = self._get_volume_encryptor(connection_info,
1326                                                    encryption)
1327             encryptor.attach_volume(context, **encryption)
1328 
1329     def _detach_encryptor(self, context, connection_info, encryption):
1330         """Detach the frontend encryptor if one is required by the volume.
1331 
1332         The request context is only used when an encryption metadata dict is
1333         not provided. The encryption metadata dict being populated is then used
1334         to determine if an attempt to detach the encryptor should be made.
1335 
1336         If native LUKS decryption is enabled then delete previously created
1337         Libvirt volume secret from the host.
1338         """
1339         volume_id = driver_block_device.get_volume_id(connection_info)
1340         if volume_id and self._host.find_secret('volume', volume_id):
1341             return self._host.delete_secret('volume', volume_id)
1342         if encryption is None:
1343             encryption = self._get_volume_encryption(context, connection_info)
1344         if encryption:
1345             encryptor = self._get_volume_encryptor(connection_info,
1346                                                    encryption)
1347             encryptor.detach_volume(**encryption)
1348 
1349     def _check_discard_for_attach_volume(self, conf, instance):
1350         """Perform some checks for volumes configured for discard support.
1351 
1352         If discard is configured for the volume, and the guest is using a
1353         configuration known to not work, we will log a message explaining
1354         the reason why.
1355         """
1356         if conf.driver_discard == 'unmap' and conf.target_bus == 'virtio':
1357             LOG.debug('Attempting to attach volume %(id)s with discard '
1358                       'support enabled to an instance using an '
1359                       'unsupported configuration. target_bus = '
1360                       '%(bus)s. Trim commands will not be issued to '
1361                       'the storage device.',
1362                       {'bus': conf.target_bus,
1363                        'id': conf.serial},
1364                       instance=instance)
1365 
1366     def attach_volume(self, context, connection_info, instance, mountpoint,
1367                       disk_bus=None, device_type=None, encryption=None):
1368         guest = self._host.get_guest(instance)
1369 
1370         disk_dev = mountpoint.rpartition("/")[2]
1371         bdm = {
1372             'device_name': disk_dev,
1373             'disk_bus': disk_bus,
1374             'device_type': device_type}
1375 
1376         # Note(cfb): If the volume has a custom block size, check that
1377         #            that we are using QEMU/KVM and libvirt >= 0.10.2. The
1378         #            presence of a block size is considered mandatory by
1379         #            cinder so we fail if we can't honor the request.
1380         data = {}
1381         if ('data' in connection_info):
1382             data = connection_info['data']
1383         if ('logical_block_size' in data or 'physical_block_size' in data):
1384             if ((CONF.libvirt.virt_type != "kvm" and
1385                  CONF.libvirt.virt_type != "qemu")):
1386                 msg = _("Volume sets block size, but the current "
1387                         "libvirt hypervisor '%s' does not support custom "
1388                         "block size") % CONF.libvirt.virt_type
1389                 raise exception.InvalidHypervisorType(msg)
1390 
1391         self._connect_volume(context, connection_info, instance,
1392                              encryption=encryption)
1393         disk_info = blockinfo.get_info_from_bdm(
1394             instance, CONF.libvirt.virt_type, instance.image_meta, bdm)
1395         if disk_info['bus'] == 'scsi':
1396             disk_info['unit'] = self._get_scsi_controller_max_unit(guest) + 1
1397 
1398         conf = self._get_volume_config(connection_info, disk_info)
1399 
1400         self._check_discard_for_attach_volume(conf, instance)
1401 
1402         try:
1403             state = guest.get_power_state(self._host)
1404             live = state in (power_state.RUNNING, power_state.PAUSED)
1405 
1406             guest.attach_device(conf, persistent=True, live=live)
1407             # NOTE(artom) If we're attaching with a device role tag, we need to
1408             # rebuild device_metadata. If we're attaching without a role
1409             # tag, we're rebuilding it here needlessly anyways. This isn't a
1410             # massive deal, and it helps reduce code complexity by not having
1411             # to indicate to the virt driver that the attach is tagged. The
1412             # really important optimization of not calling the database unless
1413             # device_metadata has actually changed is done for us by
1414             # instance.save().
1415             instance.device_metadata = self._build_device_metadata(
1416                 context, instance)
1417             instance.save()
1418         except Exception:
1419             LOG.exception(_('Failed to attach volume at mountpoint: %s'),
1420                           mountpoint, instance=instance)
1421             with excutils.save_and_reraise_exception():
1422                 self._disconnect_volume(context, connection_info, instance,
1423                                         encryption=encryption)
1424 
1425     def _swap_volume(self, guest, disk_path, conf, resize_to):
1426         """Swap existing disk with a new block device."""
1427         dev = guest.get_block_device(disk_path)
1428 
1429         # Save a copy of the domain's persistent XML file. We'll use this
1430         # to redefine the domain if anything fails during the volume swap.
1431         xml = guest.get_xml_desc(dump_inactive=True, dump_sensitive=True)
1432 
1433         # Abort is an idempotent operation, so make sure any block
1434         # jobs which may have failed are ended.
1435         try:
1436             dev.abort_job()
1437         except Exception:
1438             pass
1439 
1440         try:
1441             # NOTE (rmk): blockRebase cannot be executed on persistent
1442             #             domains, so we need to temporarily undefine it.
1443             #             If any part of this block fails, the domain is
1444             #             re-defined regardless.
1445             if guest.has_persistent_configuration():
1446                 support_uefi = self._has_uefi_support()
1447                 guest.delete_configuration(support_uefi)
1448 
1449             try:
1450                 # Start copy with VIR_DOMAIN_BLOCK_REBASE_REUSE_EXT flag to
1451                 # allow writing to existing external volume file. Use
1452                 # VIR_DOMAIN_BLOCK_REBASE_COPY_DEV if it's a block device to
1453                 # make sure XML is generated correctly (bug 1691195)
1454                 copy_dev = conf.source_type == 'block'
1455                 dev.rebase(conf.source_path, copy=True, reuse_ext=True,
1456                            copy_dev=copy_dev)
1457                 while not dev.is_job_complete():
1458                     time.sleep(0.5)
1459 
1460                 dev.abort_job(pivot=True)
1461 
1462             except Exception as exc:
1463                 LOG.exception("Failure rebasing volume %(new_path)s on "
1464                     "%(old_path)s.", {'new_path': conf.source_path,
1465                                       'old_path': disk_path})
1466                 raise exception.VolumeRebaseFailed(reason=six.text_type(exc))
1467 
1468             if resize_to:
1469                 dev.resize(resize_to * units.Gi / units.Ki)
1470 
1471             # Make sure we will redefine the domain using the updated
1472             # configuration after the volume was swapped. The dump_inactive
1473             # keyword arg controls whether we pull the inactive (persistent)
1474             # or active (live) config from the domain. We want to pull the
1475             # live config after the volume was updated to use when we redefine
1476             # the domain.
1477             xml = guest.get_xml_desc(dump_inactive=False, dump_sensitive=True)
1478         finally:
1479             self._host.write_instance_config(xml)
1480 
1481     def swap_volume(self, context, old_connection_info,
1482                     new_connection_info, instance, mountpoint, resize_to):
1483 
1484         # NOTE(lyarwood): https://bugzilla.redhat.com/show_bug.cgi?id=760547
1485         old_encrypt = self._get_volume_encryption(context, old_connection_info)
1486         new_encrypt = self._get_volume_encryption(context, new_connection_info)
1487         if ((old_encrypt and self._use_native_luks(old_encrypt)) or
1488             (new_encrypt and self._use_native_luks(new_encrypt))):
1489             raise NotImplementedError(_("Swap volume is not supported for "
1490                 "encrypted volumes when native LUKS decryption is enabled."))
1491 
1492         guest = self._host.get_guest(instance)
1493 
1494         disk_dev = mountpoint.rpartition("/")[2]
1495         if not guest.get_disk(disk_dev):
1496             raise exception.DiskNotFound(location=disk_dev)
1497         disk_info = {
1498             'dev': disk_dev,
1499             'bus': blockinfo.get_disk_bus_for_disk_dev(
1500                 CONF.libvirt.virt_type, disk_dev),
1501             'type': 'disk',
1502             }
1503         # NOTE (lyarwood): new_connection_info will be modified by the
1504         # following _connect_volume call down into the volume drivers. The
1505         # majority of the volume drivers will add a device_path that is in turn
1506         # used by _get_volume_config to set the source_path of the
1507         # LibvirtConfigGuestDisk object it returns. We do not explicitly save
1508         # this to the BDM here as the upper compute swap_volume method will
1509         # eventually do this for us.
1510         self._connect_volume(context, new_connection_info, instance)
1511         conf = self._get_volume_config(new_connection_info, disk_info)
1512         if not conf.source_path:
1513             self._disconnect_volume(context, new_connection_info, instance)
1514             raise NotImplementedError(_("Swap only supports host devices"))
1515 
1516         try:
1517             self._swap_volume(guest, disk_dev, conf, resize_to)
1518         except exception.VolumeRebaseFailed:
1519             with excutils.save_and_reraise_exception():
1520                 self._disconnect_volume(context, new_connection_info, instance)
1521 
1522         self._disconnect_volume(context, old_connection_info, instance)
1523 
1524     def _get_existing_domain_xml(self, instance, network_info,
1525                                  block_device_info=None):
1526         try:
1527             guest = self._host.get_guest(instance)
1528             xml = guest.get_xml_desc()
1529         except exception.InstanceNotFound:
1530             disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
1531                                                 instance,
1532                                                 instance.image_meta,
1533                                                 block_device_info)
1534             xml = self._get_guest_xml(nova_context.get_admin_context(),
1535                                       instance, network_info, disk_info,
1536                                       instance.image_meta,
1537                                       block_device_info=block_device_info)
1538         return xml
1539 
1540     def detach_volume(self, context, connection_info, instance, mountpoint,
1541                       encryption=None):
1542         disk_dev = mountpoint.rpartition("/")[2]
1543         try:
1544             guest = self._host.get_guest(instance)
1545 
1546             state = guest.get_power_state(self._host)
1547             live = state in (power_state.RUNNING, power_state.PAUSED)
1548             # NOTE(lyarwood): The volume must be detached from the VM before
1549             # detaching any attached encryptors or disconnecting the underlying
1550             # volume in _disconnect_volume. Otherwise, the encryptor or volume
1551             # driver may report that the volume is still in use.
1552             wait_for_detach = guest.detach_device_with_retry(guest.get_disk,
1553                                                              disk_dev,
1554                                                              live=live)
1555             wait_for_detach()
1556 
1557         except exception.InstanceNotFound:
1558             # NOTE(zhaoqin): If the instance does not exist, _lookup_by_name()
1559             #                will throw InstanceNotFound exception. Need to
1560             #                disconnect volume under this circumstance.
1561             LOG.warning("During detach_volume, instance disappeared.",
1562                         instance=instance)
1563         except exception.DeviceNotFound:
1564             # We should still try to disconnect logical device from
1565             # host, an error might have happened during a previous
1566             # call.
1567             LOG.info("Device %s not found in instance.",
1568                      disk_dev, instance=instance)
1569         except libvirt.libvirtError as ex:
1570             # NOTE(vish): This is called to cleanup volumes after live
1571             #             migration, so we should still disconnect even if
1572             #             the instance doesn't exist here anymore.
1573             error_code = ex.get_error_code()
1574             if error_code == libvirt.VIR_ERR_NO_DOMAIN:
1575                 # NOTE(vish):
1576                 LOG.warning("During detach_volume, instance disappeared.",
1577                             instance=instance)
1578             else:
1579                 raise
1580 
1581         self._disconnect_volume(context, connection_info, instance,
1582                                 encryption=encryption)
1583 
1584     def extend_volume(self, connection_info, instance):
1585         try:
1586             new_size = self._extend_volume(connection_info, instance)
1587         except NotImplementedError:
1588             raise exception.ExtendVolumeNotSupported()
1589 
1590         # Resize the device in QEMU so its size is updated and
1591         # detected by the instance without rebooting.
1592         try:
1593             guest = self._host.get_guest(instance)
1594             state = guest.get_power_state(self._host)
1595             active_state = state in (power_state.RUNNING, power_state.PAUSED)
1596             if active_state:
1597                 disk_path = connection_info['data']['device_path']
1598                 LOG.debug('resizing block device %(dev)s to %(size)u kb',
1599                           {'dev': disk_path, 'size': new_size})
1600                 dev = guest.get_block_device(disk_path)
1601                 dev.resize(new_size // units.Ki)
1602             else:
1603                 LOG.debug('Skipping block device resize, guest is not running',
1604                           instance=instance)
1605         except exception.InstanceNotFound:
1606             with excutils.save_and_reraise_exception():
1607                 LOG.warning('During extend_volume, instance disappeared.',
1608                             instance=instance)
1609         except libvirt.libvirtError:
1610             with excutils.save_and_reraise_exception():
1611                 LOG.exception('resizing block device failed.',
1612                               instance=instance)
1613 
1614     def attach_interface(self, context, instance, image_meta, vif):
1615         guest = self._host.get_guest(instance)
1616 
1617         self.vif_driver.plug(instance, vif)
1618         self.firewall_driver.setup_basic_filtering(instance, [vif])
1619         cfg = self.vif_driver.get_config(instance, vif, image_meta,
1620                                          instance.flavor,
1621                                          CONF.libvirt.virt_type,
1622                                          self._host)
1623         try:
1624             state = guest.get_power_state(self._host)
1625             live = state in (power_state.RUNNING, power_state.PAUSED)
1626             guest.attach_device(cfg, persistent=True, live=live)
1627         except libvirt.libvirtError:
1628             LOG.error('attaching network adapter failed.',
1629                       instance=instance, exc_info=True)
1630             self.vif_driver.unplug(instance, vif)
1631             raise exception.InterfaceAttachFailed(
1632                     instance_uuid=instance.uuid)
1633         try:
1634             # NOTE(artom) If we're attaching with a device role tag, we need to
1635             # rebuild device_metadata. If we're attaching without a role
1636             # tag, we're rebuilding it here needlessly anyways. This isn't a
1637             # massive deal, and it helps reduce code complexity by not having
1638             # to indicate to the virt driver that the attach is tagged. The
1639             # really important optimization of not calling the database unless
1640             # device_metadata has actually changed is done for us by
1641             # instance.save().
1642             instance.device_metadata = self._build_device_metadata(
1643                 context, instance)
1644             instance.save()
1645         except Exception:
1646             # NOTE(artom) If we fail here it means the interface attached
1647             # successfully but building and/or saving the device metadata
1648             # failed. Just unplugging the vif is therefore not enough cleanup,
1649             # we need to detach the interface.
1650             with excutils.save_and_reraise_exception(reraise=False):
1651                 LOG.error('Interface attached successfully but building '
1652                           'and/or saving device metadata failed.',
1653                           instance=instance, exc_info=True)
1654                 self.detach_interface(context, instance, vif)
1655                 raise exception.InterfaceAttachFailed(
1656                     instance_uuid=instance.uuid)
1657 
1658     def detach_interface(self, context, instance, vif):
1659         guest = self._host.get_guest(instance)
1660         cfg = self.vif_driver.get_config(instance, vif,
1661                                          instance.image_meta,
1662                                          instance.flavor,
1663                                          CONF.libvirt.virt_type, self._host)
1664         interface = guest.get_interface_by_cfg(cfg)
1665         try:
1666             self.vif_driver.unplug(instance, vif)
1667             # NOTE(mriedem): When deleting an instance and using Neutron,
1668             # we can be racing against Neutron deleting the port and
1669             # sending the vif-deleted event which then triggers a call to
1670             # detach the interface, so if the interface is not found then
1671             # we can just log it as a warning.
1672             if not interface:
1673                 mac = vif.get('address')
1674                 # The interface is gone so just log it as a warning.
1675                 LOG.warning('Detaching interface %(mac)s failed because '
1676                             'the device is no longer found on the guest.',
1677                             {'mac': mac}, instance=instance)
1678                 return
1679 
1680             state = guest.get_power_state(self._host)
1681             live = state in (power_state.RUNNING, power_state.PAUSED)
1682             # Now we are going to loop until the interface is detached or we
1683             # timeout.
1684             wait_for_detach = guest.detach_device_with_retry(
1685                 guest.get_interface_by_cfg, cfg, live=live,
1686                 alternative_device_name=self.vif_driver.get_vif_devname(vif))
1687             wait_for_detach()
1688         except exception.DeviceDetachFailed:
1689             # We failed to detach the device even with the retry loop, so let's
1690             # dump some debug information to the logs before raising back up.
1691             with excutils.save_and_reraise_exception():
1692                 devname = self.vif_driver.get_vif_devname(vif)
1693                 interface = guest.get_interface_by_cfg(cfg)
1694                 if interface:
1695                     LOG.warning(
1696                         'Failed to detach interface %(devname)s after '
1697                         'repeated attempts. Final interface xml:\n'
1698                         '%(interface_xml)s\nFinal guest xml:\n%(guest_xml)s',
1699                         {'devname': devname,
1700                          'interface_xml': interface.to_xml(),
1701                          'guest_xml': guest.get_xml_desc()},
1702                         instance=instance)
1703         except exception.DeviceNotFound:
1704             # The interface is gone so just log it as a warning.
1705             LOG.warning('Detaching interface %(mac)s failed because '
1706                         'the device is no longer found on the guest.',
1707                         {'mac': vif.get('address')}, instance=instance)
1708         except libvirt.libvirtError as ex:
1709             error_code = ex.get_error_code()
1710             if error_code == libvirt.VIR_ERR_NO_DOMAIN:
1711                 LOG.warning("During detach_interface, instance disappeared.",
1712                             instance=instance)
1713             else:
1714                 # NOTE(mriedem): When deleting an instance and using Neutron,
1715                 # we can be racing against Neutron deleting the port and
1716                 # sending the vif-deleted event which then triggers a call to
1717                 # detach the interface, so we might have failed because the
1718                 # network device no longer exists. Libvirt will fail with
1719                 # "operation failed: no matching network device was found"
1720                 # which unfortunately does not have a unique error code so we
1721                 # need to look up the interface by config and if it's not found
1722                 # then we can just log it as a warning rather than tracing an
1723                 # error.
1724                 mac = vif.get('address')
1725                 interface = guest.get_interface_by_cfg(cfg)
1726                 if interface:
1727                     LOG.error('detaching network adapter failed.',
1728                               instance=instance, exc_info=True)
1729                     raise exception.InterfaceDetachFailed(
1730                             instance_uuid=instance.uuid)
1731 
1732                 # The interface is gone so just log it as a warning.
1733                 LOG.warning('Detaching interface %(mac)s failed because '
1734                             'the device is no longer found on the guest.',
1735                             {'mac': mac}, instance=instance)
1736 
1737     def _create_snapshot_metadata(self, image_meta, instance,
1738                                   img_fmt, snp_name):
1739         metadata = {'is_public': False,
1740                     'status': 'active',
1741                     'name': snp_name,
1742                     'properties': {
1743                                    'kernel_id': instance.kernel_id,
1744                                    'image_location': 'snapshot',
1745                                    'image_state': 'available',
1746                                    'owner_id': instance.project_id,
1747                                    'ramdisk_id': instance.ramdisk_id,
1748                                    }
1749                     }
1750         if instance.os_type:
1751             metadata['properties']['os_type'] = instance.os_type
1752 
1753         # NOTE(vish): glance forces ami disk format to be ami
1754         if image_meta.disk_format == 'ami':
1755             metadata['disk_format'] = 'ami'
1756         else:
1757             metadata['disk_format'] = img_fmt
1758 
1759         if image_meta.obj_attr_is_set("container_format"):
1760             metadata['container_format'] = image_meta.container_format
1761         else:
1762             metadata['container_format'] = "bare"
1763 
1764         return metadata
1765 
1766     def snapshot(self, context, instance, image_id, update_task_state):
1767         """Create snapshot from a running VM instance.
1768 
1769         This command only works with qemu 0.14+
1770         """
1771         try:
1772             guest = self._host.get_guest(instance)
1773 
1774             # TODO(sahid): We are converting all calls from a
1775             # virDomain object to use nova.virt.libvirt.Guest.
1776             # We should be able to remove virt_dom at the end.
1777             virt_dom = guest._domain
1778         except exception.InstanceNotFound:
1779             raise exception.InstanceNotRunning(instance_id=instance.uuid)
1780 
1781         snapshot = self._image_api.get(context, image_id)
1782 
1783         # source_format is an on-disk format
1784         # source_type is a backend type
1785         disk_path, source_format = libvirt_utils.find_disk(guest)
1786         source_type = libvirt_utils.get_disk_type_from_path(disk_path)
1787 
1788         # We won't have source_type for raw or qcow2 disks, because we can't
1789         # determine that from the path. We should have it from the libvirt
1790         # xml, though.
1791         if source_type is None:
1792             source_type = source_format
1793         # For lxc instances we won't have it either from libvirt xml
1794         # (because we just gave libvirt the mounted filesystem), or the path,
1795         # so source_type is still going to be None. In this case,
1796         # root_disk is going to default to CONF.libvirt.images_type
1797         # below, which is still safe.
1798 
1799         image_format = CONF.libvirt.snapshot_image_format or source_type
1800 
1801         # NOTE(bfilippov): save lvm and rbd as raw
1802         if image_format == 'lvm' or image_format == 'rbd':
1803             image_format = 'raw'
1804 
1805         metadata = self._create_snapshot_metadata(instance.image_meta,
1806                                                   instance,
1807                                                   image_format,
1808                                                   snapshot['name'])
1809 
1810         snapshot_name = uuidutils.generate_uuid(dashed=False)
1811 
1812         state = guest.get_power_state(self._host)
1813 
1814         # NOTE(dgenin): Instances with LVM encrypted ephemeral storage require
1815         #               cold snapshots. Currently, checking for encryption is
1816         #               redundant because LVM supports only cold snapshots.
1817         #               It is necessary in case this situation changes in the
1818         #               future.
1819         if (self._host.has_min_version(hv_type=host.HV_DRIVER_QEMU)
1820                 and source_type not in ('lvm')
1821                 and not CONF.ephemeral_storage_encryption.enabled
1822                 and not CONF.workarounds.disable_libvirt_livesnapshot
1823                 # NOTE(rmk): We cannot perform live snapshots when a
1824                 # managedSave file is present, so we will use the cold/legacy
1825                 # method for instances which are shutdown or paused.
1826                 # NOTE(mriedem): Live snapshot doesn't work with paused
1827                 # instances on older versions of libvirt/qemu. We can likely
1828                 # remove the restriction on PAUSED once we require
1829                 # libvirt>=3.6.0 and qemu>=2.10 since that works with the
1830                 # Pike Ubuntu Cloud Archive testing in Queens.
1831                 and state not in (power_state.SHUTDOWN, power_state.PAUSED)):
1832             live_snapshot = True
1833             # Abort is an idempotent operation, so make sure any block
1834             # jobs which may have failed are ended. This operation also
1835             # confirms the running instance, as opposed to the system as a
1836             # whole, has a new enough version of the hypervisor (bug 1193146).
1837             try:
1838                 guest.get_block_device(disk_path).abort_job()
1839             except libvirt.libvirtError as ex:
1840                 error_code = ex.get_error_code()
1841                 if error_code == libvirt.VIR_ERR_CONFIG_UNSUPPORTED:
1842                     live_snapshot = False
1843                 else:
1844                     pass
1845         else:
1846             live_snapshot = False
1847 
1848         self._prepare_domain_for_snapshot(context, live_snapshot, state,
1849                                           instance)
1850 
1851         root_disk = self.image_backend.by_libvirt_path(
1852             instance, disk_path, image_type=source_type)
1853 
1854         if live_snapshot:
1855             LOG.info("Beginning live snapshot process", instance=instance)
1856         else:
1857             LOG.info("Beginning cold snapshot process", instance=instance)
1858 
1859         update_task_state(task_state=task_states.IMAGE_PENDING_UPLOAD)
1860 
1861         update_task_state(task_state=task_states.IMAGE_UPLOADING,
1862                           expected_state=task_states.IMAGE_PENDING_UPLOAD)
1863 
1864         try:
1865             metadata['location'] = root_disk.direct_snapshot(
1866                 context, snapshot_name, image_format, image_id,
1867                 instance.image_ref)
1868             self._snapshot_domain(context, live_snapshot, virt_dom, state,
1869                                   instance)
1870             self._image_api.update(context, image_id, metadata,
1871                                    purge_props=False)
1872         except (NotImplementedError, exception.ImageUnacceptable,
1873                 exception.Forbidden) as e:
1874             if type(e) != NotImplementedError:
1875                 LOG.warning('Performing standard snapshot because direct '
1876                             'snapshot failed: %(error)s',
1877                             {'error': encodeutils.exception_to_unicode(e)})
1878             failed_snap = metadata.pop('location', None)
1879             if failed_snap:
1880                 failed_snap = {'url': str(failed_snap)}
1881             root_disk.cleanup_direct_snapshot(failed_snap,
1882                                                   also_destroy_volume=True,
1883                                                   ignore_errors=True)
1884             update_task_state(task_state=task_states.IMAGE_PENDING_UPLOAD,
1885                               expected_state=task_states.IMAGE_UPLOADING)
1886 
1887             # TODO(nic): possibly abstract this out to the root_disk
1888             if source_type == 'rbd' and live_snapshot:
1889                 # Standard snapshot uses qemu-img convert from RBD which is
1890                 # not safe to run with live_snapshot.
1891                 live_snapshot = False
1892                 # Suspend the guest, so this is no longer a live snapshot
1893                 self._prepare_domain_for_snapshot(context, live_snapshot,
1894                                                   state, instance)
1895 
1896             snapshot_directory = CONF.libvirt.snapshots_directory
1897             fileutils.ensure_tree(snapshot_directory)
1898             with utils.tempdir(dir=snapshot_directory) as tmpdir:
1899                 try:
1900                     out_path = os.path.join(tmpdir, snapshot_name)
1901                     if live_snapshot:
1902                         # NOTE(xqueralt): libvirt needs o+x in the tempdir
1903                         os.chmod(tmpdir, 0o701)
1904                         self._live_snapshot(context, instance, guest,
1905                                             disk_path, out_path, source_format,
1906                                             image_format, instance.image_meta)
1907                     else:
1908                         root_disk.snapshot_extract(out_path, image_format)
1909                     LOG.info("Snapshot extracted, beginning image upload",
1910                              instance=instance)
1911                 except libvirt.libvirtError as ex:
1912                     error_code = ex.get_error_code()
1913                     if error_code == libvirt.VIR_ERR_NO_DOMAIN:
1914                         LOG.info('Instance %(instance_name)s disappeared '
1915                                  'while taking snapshot of it: [Error Code '
1916                                  '%(error_code)s] %(ex)s',
1917                                  {'instance_name': instance.name,
1918                                   'error_code': error_code,
1919                                   'ex': ex},
1920                                  instance=instance)
1921                         raise exception.InstanceNotFound(
1922                             instance_id=instance.uuid)
1923                     else:
1924                         raise
1925                 finally:
1926                     self._snapshot_domain(context, live_snapshot, virt_dom,
1927                                           state, instance)
1928 
1929                 # Upload that image to the image service
1930                 update_task_state(task_state=task_states.IMAGE_UPLOADING,
1931                         expected_state=task_states.IMAGE_PENDING_UPLOAD)
1932                 with libvirt_utils.file_open(out_path, 'rb') as image_file:
1933                     self._image_api.update(context,
1934                                            image_id,
1935                                            metadata,
1936                                            image_file)
1937         except Exception:
1938             with excutils.save_and_reraise_exception():
1939                 LOG.exception(_("Failed to snapshot image"))
1940                 failed_snap = metadata.pop('location', None)
1941                 if failed_snap:
1942                     failed_snap = {'url': str(failed_snap)}
1943                 root_disk.cleanup_direct_snapshot(
1944                         failed_snap, also_destroy_volume=True,
1945                         ignore_errors=True)
1946 
1947         LOG.info("Snapshot image upload complete", instance=instance)
1948 
1949     def _prepare_domain_for_snapshot(self, context, live_snapshot, state,
1950                                      instance):
1951         # NOTE(dkang): managedSave does not work for LXC
1952         if CONF.libvirt.virt_type != 'lxc' and not live_snapshot:
1953             if state == power_state.RUNNING or state == power_state.PAUSED:
1954                 self.suspend(context, instance)
1955 
1956     def _snapshot_domain(self, context, live_snapshot, virt_dom, state,
1957                          instance):
1958         guest = None
1959         # NOTE(dkang): because previous managedSave is not called
1960         #              for LXC, _create_domain must not be called.
1961         if CONF.libvirt.virt_type != 'lxc' and not live_snapshot:
1962             if state == power_state.RUNNING:
1963                 guest = self._create_domain(domain=virt_dom)
1964             elif state == power_state.PAUSED:
1965                 guest = self._create_domain(domain=virt_dom, pause=True)
1966 
1967             if guest is not None:
1968                 self._attach_pci_devices(
1969                     guest, pci_manager.get_instance_pci_devs(instance))
1970                 self._attach_direct_passthrough_ports(
1971                     context, instance, guest)
1972 
1973     def _can_set_admin_password(self, image_meta):
1974 
1975         if CONF.libvirt.virt_type == 'parallels':
1976             if not self._host.has_min_version(
1977                    MIN_LIBVIRT_PARALLELS_SET_ADMIN_PASSWD):
1978                 raise exception.SetAdminPasswdNotSupported()
1979         elif CONF.libvirt.virt_type in ('kvm', 'qemu'):
1980             if not image_meta.properties.get('hw_qemu_guest_agent', False):
1981                 raise exception.QemuGuestAgentNotEnabled()
1982         else:
1983             raise exception.SetAdminPasswdNotSupported()
1984 
1985     # TODO(melwitt): Combine this with the similar xenapi code at some point.
1986     def _save_instance_password_if_sshkey_present(self, instance, new_pass):
1987         sshkey = instance.key_data if 'key_data' in instance else None
1988         if sshkey and sshkey.startswith("ssh-rsa"):
1989             enc = crypto.ssh_encrypt_text(sshkey, new_pass)
1990             # NOTE(melwitt): The convert_password method doesn't actually do
1991             # anything with the context argument, so we can pass None.
1992             instance.system_metadata.update(
1993                 password.convert_password(None, base64.encode_as_text(enc)))
1994             instance.save()
1995 
1996     def set_admin_password(self, instance, new_pass):
1997         self._can_set_admin_password(instance.image_meta)
1998 
1999         guest = self._host.get_guest(instance)
2000         user = instance.image_meta.properties.get("os_admin_user")
2001         if not user:
2002             if instance.os_type == "windows":
2003                 user = "Administrator"
2004             else:
2005                 user = "root"
2006         try:
2007             guest.set_user_password(user, new_pass)
2008         except libvirt.libvirtError as ex:
2009             error_code = ex.get_error_code()
2010             if error_code == libvirt.VIR_ERR_AGENT_UNRESPONSIVE:
2011                 LOG.debug('Failed to set password: QEMU agent unresponsive',
2012                           instance_uuid=instance.uuid)
2013                 raise NotImplementedError()
2014 
2015             err_msg = encodeutils.exception_to_unicode(ex)
2016             msg = (_('Error from libvirt while set password for username '
2017                      '"%(user)s": [Error Code %(error_code)s] %(ex)s')
2018                    % {'user': user, 'error_code': error_code, 'ex': err_msg})
2019             raise exception.InternalError(msg)
2020         else:
2021             # Save the password in sysmeta so it may be retrieved from the
2022             # metadata service.
2023             self._save_instance_password_if_sshkey_present(instance, new_pass)
2024 
2025     def _can_quiesce(self, instance, image_meta):
2026         if CONF.libvirt.virt_type not in ('kvm', 'qemu'):
2027             raise exception.InstanceQuiesceNotSupported(
2028                 instance_id=instance.uuid)
2029 
2030         if not image_meta.properties.get('hw_qemu_guest_agent', False):
2031             raise exception.QemuGuestAgentNotEnabled()
2032 
2033     def _requires_quiesce(self, image_meta):
2034         return image_meta.properties.get('os_require_quiesce', False)
2035 
2036     def _set_quiesced(self, context, instance, image_meta, quiesced):
2037         self._can_quiesce(instance, image_meta)
2038         try:
2039             guest = self._host.get_guest(instance)
2040             if quiesced:
2041                 guest.freeze_filesystems()
2042             else:
2043                 guest.thaw_filesystems()
2044         except libvirt.libvirtError as ex:
2045             error_code = ex.get_error_code()
2046             err_msg = encodeutils.exception_to_unicode(ex)
2047             msg = (_('Error from libvirt while quiescing %(instance_name)s: '
2048                      '[Error Code %(error_code)s] %(ex)s')
2049                    % {'instance_name': instance.name,
2050                       'error_code': error_code, 'ex': err_msg})
2051             raise exception.InternalError(msg)
2052 
2053     def quiesce(self, context, instance, image_meta):
2054         """Freeze the guest filesystems to prepare for snapshot.
2055 
2056         The qemu-guest-agent must be setup to execute fsfreeze.
2057         """
2058         self._set_quiesced(context, instance, image_meta, True)
2059 
2060     def unquiesce(self, context, instance, image_meta):
2061         """Thaw the guest filesystems after snapshot."""
2062         self._set_quiesced(context, instance, image_meta, False)
2063 
2064     def _live_snapshot(self, context, instance, guest, disk_path, out_path,
2065                        source_format, image_format, image_meta):
2066         """Snapshot an instance without downtime."""
2067         dev = guest.get_block_device(disk_path)
2068 
2069         # Save a copy of the domain's persistent XML file
2070         xml = guest.get_xml_desc(dump_inactive=True, dump_sensitive=True)
2071 
2072         # Abort is an idempotent operation, so make sure any block
2073         # jobs which may have failed are ended.
2074         try:
2075             dev.abort_job()
2076         except Exception:
2077             pass
2078 
2079         # NOTE (rmk): We are using shallow rebases as a workaround to a bug
2080         #             in QEMU 1.3. In order to do this, we need to create
2081         #             a destination image with the original backing file
2082         #             and matching size of the instance root disk.
2083         src_disk_size = libvirt_utils.get_disk_size(disk_path,
2084                                                     format=source_format)
2085         src_back_path = libvirt_utils.get_disk_backing_file(disk_path,
2086                                                         format=source_format,
2087                                                         basename=False)
2088         disk_delta = out_path + '.delta'
2089         libvirt_utils.create_cow_image(src_back_path, disk_delta,
2090                                        src_disk_size)
2091 
2092         quiesced = False
2093         try:
2094             self._set_quiesced(context, instance, image_meta, True)
2095             quiesced = True
2096         except exception.NovaException as err:
2097             if self._requires_quiesce(image_meta):
2098                 raise
2099             LOG.info('Skipping quiescing instance: %(reason)s.',
2100                      {'reason': err}, instance=instance)
2101 
2102         try:
2103             # NOTE (rmk): blockRebase cannot be executed on persistent
2104             #             domains, so we need to temporarily undefine it.
2105             #             If any part of this block fails, the domain is
2106             #             re-defined regardless.
2107             if guest.has_persistent_configuration():
2108                 support_uefi = self._has_uefi_support()
2109                 guest.delete_configuration(support_uefi)
2110 
2111             # NOTE (rmk): Establish a temporary mirror of our root disk and
2112             #             issue an abort once we have a complete copy.
2113             dev.rebase(disk_delta, copy=True, reuse_ext=True, shallow=True)
2114 
2115             while not dev.is_job_complete():
2116                 time.sleep(0.5)
2117 
2118             dev.abort_job()
2119             nova.privsep.path.chown(disk_delta, uid=os.getuid())
2120         finally:
2121             self._host.write_instance_config(xml)
2122             if quiesced:
2123                 self._set_quiesced(context, instance, image_meta, False)
2124 
2125         # Convert the delta (CoW) image with a backing file to a flat
2126         # image with no backing file.
2127         libvirt_utils.extract_snapshot(disk_delta, 'qcow2',
2128                                        out_path, image_format)
2129 
2130     def _volume_snapshot_update_status(self, context, snapshot_id, status):
2131         """Send a snapshot status update to Cinder.
2132 
2133         This method captures and logs exceptions that occur
2134         since callers cannot do anything useful with these exceptions.
2135 
2136         Operations on the Cinder side waiting for this will time out if
2137         a failure occurs sending the update.
2138 
2139         :param context: security context
2140         :param snapshot_id: id of snapshot being updated
2141         :param status: new status value
2142 
2143         """
2144 
2145         try:
2146             self._volume_api.update_snapshot_status(context,
2147                                                     snapshot_id,
2148                                                     status)
2149         except Exception:
2150             LOG.exception(_('Failed to send updated snapshot status '
2151                             'to volume service.'))
2152 
2153     def _volume_snapshot_create(self, context, instance, guest,
2154                                 volume_id, new_file):
2155         """Perform volume snapshot.
2156 
2157            :param guest: VM that volume is attached to
2158            :param volume_id: volume UUID to snapshot
2159            :param new_file: relative path to new qcow2 file present on share
2160 
2161         """
2162         xml = guest.get_xml_desc()
2163         xml_doc = etree.fromstring(xml)
2164 
2165         device_info = vconfig.LibvirtConfigGuest()
2166         device_info.parse_dom(xml_doc)
2167 
2168         disks_to_snap = []          # to be snapshotted by libvirt
2169         network_disks_to_snap = []  # network disks (netfs, etc.)
2170         disks_to_skip = []          # local disks not snapshotted
2171 
2172         for guest_disk in device_info.devices:
2173             if (guest_disk.root_name != 'disk'):
2174                 continue
2175 
2176             if (guest_disk.target_dev is None):
2177                 continue
2178 
2179             if (guest_disk.serial is None or guest_disk.serial != volume_id):
2180                 disks_to_skip.append(guest_disk.target_dev)
2181                 continue
2182 
2183             # disk is a Cinder volume with the correct volume_id
2184 
2185             disk_info = {
2186                 'dev': guest_disk.target_dev,
2187                 'serial': guest_disk.serial,
2188                 'current_file': guest_disk.source_path,
2189                 'source_protocol': guest_disk.source_protocol,
2190                 'source_name': guest_disk.source_name,
2191                 'source_hosts': guest_disk.source_hosts,
2192                 'source_ports': guest_disk.source_ports
2193             }
2194 
2195             # Determine path for new_file based on current path
2196             if disk_info['current_file'] is not None:
2197                 current_file = disk_info['current_file']
2198                 new_file_path = os.path.join(os.path.dirname(current_file),
2199                                              new_file)
2200                 disks_to_snap.append((current_file, new_file_path))
2201             # NOTE(mriedem): This used to include a check for gluster in
2202             # addition to netfs since they were added together. Support for
2203             # gluster was removed in the 16.0.0 Pike release. It is unclear,
2204             # however, if other volume drivers rely on the netfs disk source
2205             # protocol.
2206             elif disk_info['source_protocol'] == 'netfs':
2207                 network_disks_to_snap.append((disk_info, new_file))
2208 
2209         if not disks_to_snap and not network_disks_to_snap:
2210             msg = _('Found no disk to snapshot.')
2211             raise exception.InternalError(msg)
2212 
2213         snapshot = vconfig.LibvirtConfigGuestSnapshot()
2214 
2215         for current_name, new_filename in disks_to_snap:
2216             snap_disk = vconfig.LibvirtConfigGuestSnapshotDisk()
2217             snap_disk.name = current_name
2218             snap_disk.source_path = new_filename
2219             snap_disk.source_type = 'file'
2220             snap_disk.snapshot = 'external'
2221             snap_disk.driver_name = 'qcow2'
2222 
2223             snapshot.add_disk(snap_disk)
2224 
2225         for disk_info, new_filename in network_disks_to_snap:
2226             snap_disk = vconfig.LibvirtConfigGuestSnapshotDisk()
2227             snap_disk.name = disk_info['dev']
2228             snap_disk.source_type = 'network'
2229             snap_disk.source_protocol = disk_info['source_protocol']
2230             snap_disk.snapshot = 'external'
2231             snap_disk.source_path = new_filename
2232             old_dir = disk_info['source_name'].split('/')[0]
2233             snap_disk.source_name = '%s/%s' % (old_dir, new_filename)
2234             snap_disk.source_hosts = disk_info['source_hosts']
2235             snap_disk.source_ports = disk_info['source_ports']
2236 
2237             snapshot.add_disk(snap_disk)
2238 
2239         for dev in disks_to_skip:
2240             snap_disk = vconfig.LibvirtConfigGuestSnapshotDisk()
2241             snap_disk.name = dev
2242             snap_disk.snapshot = 'no'
2243 
2244             snapshot.add_disk(snap_disk)
2245 
2246         snapshot_xml = snapshot.to_xml()
2247         LOG.debug("snap xml: %s", snapshot_xml, instance=instance)
2248 
2249         image_meta = instance.image_meta
2250         try:
2251             # Check to see if we can quiesce the guest before taking the
2252             # snapshot.
2253             self._can_quiesce(instance, image_meta)
2254             try:
2255                 guest.snapshot(snapshot, no_metadata=True, disk_only=True,
2256                                reuse_ext=True, quiesce=True)
2257                 return
2258             except libvirt.libvirtError:
2259                 # If the image says that quiesce is required then we fail.
2260                 if self._requires_quiesce(image_meta):
2261                     raise
2262                 LOG.exception(_('Unable to create quiesced VM snapshot, '
2263                                 'attempting again with quiescing disabled.'),
2264                               instance=instance)
2265         except (exception.InstanceQuiesceNotSupported,
2266                 exception.QemuGuestAgentNotEnabled) as err:
2267             # If the image says that quiesce is required then we need to fail.
2268             if self._requires_quiesce(image_meta):
2269                 raise
2270             LOG.info('Skipping quiescing instance: %(reason)s.',
2271                      {'reason': err}, instance=instance)
2272 
2273         try:
2274             guest.snapshot(snapshot, no_metadata=True, disk_only=True,
2275                            reuse_ext=True, quiesce=False)
2276         except libvirt.libvirtError:
2277             LOG.exception(_('Unable to create VM snapshot, '
2278                             'failing volume_snapshot operation.'),
2279                           instance=instance)
2280 
2281             raise
2282 
2283     def _volume_refresh_connection_info(self, context, instance, volume_id):
2284         bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
2285                   context, volume_id, instance.uuid)
2286 
2287         driver_bdm = driver_block_device.convert_volume(bdm)
2288         if driver_bdm:
2289             driver_bdm.refresh_connection_info(context, instance,
2290                                                self._volume_api, self)
2291 
2292     def volume_snapshot_create(self, context, instance, volume_id,
2293                                create_info):
2294         """Create snapshots of a Cinder volume via libvirt.
2295 
2296         :param instance: VM instance object reference
2297         :param volume_id: id of volume being snapshotted
2298         :param create_info: dict of information used to create snapshots
2299                      - snapshot_id : ID of snapshot
2300                      - type : qcow2 / <other>
2301                      - new_file : qcow2 file created by Cinder which
2302                      becomes the VM's active image after
2303                      the snapshot is complete
2304         """
2305 
2306         LOG.debug("volume_snapshot_create: create_info: %(c_info)s",
2307                   {'c_info': create_info}, instance=instance)
2308 
2309         try:
2310             guest = self._host.get_guest(instance)
2311         except exception.InstanceNotFound:
2312             raise exception.InstanceNotRunning(instance_id=instance.uuid)
2313 
2314         if create_info['type'] != 'qcow2':
2315             msg = _('Unknown type: %s') % create_info['type']
2316             raise exception.InternalError(msg)
2317 
2318         snapshot_id = create_info.get('snapshot_id', None)
2319         if snapshot_id is None:
2320             msg = _('snapshot_id required in create_info')
2321             raise exception.InternalError(msg)
2322 
2323         try:
2324             self._volume_snapshot_create(context, instance, guest,
2325                                          volume_id, create_info['new_file'])
2326         except Exception:
2327             with excutils.save_and_reraise_exception():
2328                 LOG.exception(_('Error occurred during '
2329                                 'volume_snapshot_create, '
2330                                 'sending error status to Cinder.'),
2331                               instance=instance)
2332                 self._volume_snapshot_update_status(
2333                     context, snapshot_id, 'error')
2334 
2335         self._volume_snapshot_update_status(
2336             context, snapshot_id, 'creating')
2337 
2338         def _wait_for_snapshot():
2339             snapshot = self._volume_api.get_snapshot(context, snapshot_id)
2340 
2341             if snapshot.get('status') != 'creating':
2342                 self._volume_refresh_connection_info(context, instance,
2343                                                      volume_id)
2344                 raise loopingcall.LoopingCallDone()
2345 
2346         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_snapshot)
2347         timer.start(interval=0.5).wait()
2348 
2349     @staticmethod
2350     def _rebase_with_qemu_img(guest, device, active_disk_object,
2351                               rebase_base):
2352         """Rebase a device tied to a guest using qemu-img.
2353 
2354         :param guest:the Guest which owns the device being rebased
2355         :type guest: nova.virt.libvirt.guest.Guest
2356         :param device: the guest block device to rebase
2357         :type device: nova.virt.libvirt.guest.BlockDevice
2358         :param active_disk_object: the guest block device to rebase
2359         :type active_disk_object: nova.virt.libvirt.config.\
2360                                     LibvirtConfigGuestDisk
2361         :param rebase_base: the new parent in the backing chain
2362         :type rebase_base: None or string
2363         """
2364 
2365         # It's unsure how well qemu-img handles network disks for
2366         # every protocol. So let's be safe.
2367         active_protocol = active_disk_object.source_protocol
2368         if active_protocol is not None:
2369             msg = _("Something went wrong when deleting a volume snapshot: "
2370                     "rebasing a %(protocol)s network disk using qemu-img "
2371                     "has not been fully tested") % {'protocol':
2372                     active_protocol}
2373             LOG.error(msg)
2374             raise exception.InternalError(msg)
2375 
2376         if rebase_base is None:
2377             # If backing_file is specified as "" (the empty string), then
2378             # the image is rebased onto no backing file (i.e. it will exist
2379             # independently of any backing file).
2380             backing_file = ""
2381             qemu_img_extra_arg = []
2382         else:
2383             # If the rebased image is going to have a backing file then
2384             # explicitly set the backing file format to avoid any security
2385             # concerns related to file format auto detection.
2386             backing_file = rebase_base
2387             b_file_fmt = images.qemu_img_info(backing_file).file_format
2388             qemu_img_extra_arg = ['-F', b_file_fmt]
2389 
2390         qemu_img_extra_arg.append(active_disk_object.source_path)
2391         utils.execute("qemu-img", "rebase", "-b", backing_file,
2392                       *qemu_img_extra_arg)
2393 
2394     def _volume_snapshot_delete(self, context, instance, volume_id,
2395                                 snapshot_id, delete_info=None):
2396         """Note:
2397             if file being merged into == active image:
2398                 do a blockRebase (pull) operation
2399             else:
2400                 do a blockCommit operation
2401             Files must be adjacent in snap chain.
2402 
2403         :param instance: instance object reference
2404         :param volume_id: volume UUID
2405         :param snapshot_id: snapshot UUID (unused currently)
2406         :param delete_info: {
2407             'type':              'qcow2',
2408             'file_to_merge':     'a.img',
2409             'merge_target_file': 'b.img' or None (if merging file_to_merge into
2410                                                   active image)
2411           }
2412         """
2413 
2414         LOG.debug('volume_snapshot_delete: delete_info: %s', delete_info,
2415                   instance=instance)
2416 
2417         if delete_info['type'] != 'qcow2':
2418             msg = _('Unknown delete_info type %s') % delete_info['type']
2419             raise exception.InternalError(msg)
2420 
2421         try:
2422             guest = self._host.get_guest(instance)
2423         except exception.InstanceNotFound:
2424             raise exception.InstanceNotRunning(instance_id=instance.uuid)
2425 
2426         # Find dev name
2427         my_dev = None
2428         active_disk = None
2429 
2430         xml = guest.get_xml_desc()
2431         xml_doc = etree.fromstring(xml)
2432 
2433         device_info = vconfig.LibvirtConfigGuest()
2434         device_info.parse_dom(xml_doc)
2435 
2436         active_disk_object = None
2437 
2438         for guest_disk in device_info.devices:
2439             if (guest_disk.root_name != 'disk'):
2440                 continue
2441 
2442             if (guest_disk.target_dev is None or guest_disk.serial is None):
2443                 continue
2444 
2445             if guest_disk.serial == volume_id:
2446                 my_dev = guest_disk.target_dev
2447 
2448                 active_disk = guest_disk.source_path
2449                 active_protocol = guest_disk.source_protocol
2450                 active_disk_object = guest_disk
2451                 break
2452 
2453         if my_dev is None or (active_disk is None and active_protocol is None):
2454             LOG.debug('Domain XML: %s', xml, instance=instance)
2455             msg = (_('Disk with id: %s not found attached to instance.')
2456                    % volume_id)
2457             raise exception.InternalError(msg)
2458 
2459         LOG.debug("found device at %s", my_dev, instance=instance)
2460 
2461         def _get_snap_dev(filename, backing_store):
2462             if filename is None:
2463                 msg = _('filename cannot be None')
2464                 raise exception.InternalError(msg)
2465 
2466             # libgfapi delete
2467             LOG.debug("XML: %s", xml)
2468 
2469             LOG.debug("active disk object: %s", active_disk_object)
2470 
2471             # determine reference within backing store for desired image
2472             filename_to_merge = filename
2473             matched_name = None
2474             b = backing_store
2475             index = None
2476 
2477             current_filename = active_disk_object.source_name.split('/')[1]
2478             if current_filename == filename_to_merge:
2479                 return my_dev + '[0]'
2480 
2481             while b is not None:
2482                 source_filename = b.source_name.split('/')[1]
2483                 if source_filename == filename_to_merge:
2484                     LOG.debug('found match: %s', b.source_name)
2485                     matched_name = b.source_name
2486                     index = b.index
2487                     break
2488 
2489                 b = b.backing_store
2490 
2491             if matched_name is None:
2492                 msg = _('no match found for %s') % (filename_to_merge)
2493                 raise exception.InternalError(msg)
2494 
2495             LOG.debug('index of match (%s) is %s', b.source_name, index)
2496 
2497             my_snap_dev = '%s[%s]' % (my_dev, index)
2498             return my_snap_dev
2499 
2500         if delete_info['merge_target_file'] is None:
2501             # pull via blockRebase()
2502 
2503             # Merge the most recent snapshot into the active image
2504 
2505             rebase_disk = my_dev
2506             rebase_base = delete_info['file_to_merge']  # often None
2507             if (active_protocol is not None) and (rebase_base is not None):
2508                 rebase_base = _get_snap_dev(rebase_base,
2509                                             active_disk_object.backing_store)
2510 
2511             # NOTE(deepakcs): libvirt added support for _RELATIVE in v1.2.7,
2512             # and when available this flag _must_ be used to ensure backing
2513             # paths are maintained relative by qemu.
2514             #
2515             # If _RELATIVE flag not found, continue with old behaviour
2516             # (relative backing path seems to work for this case)
2517             try:
2518                 libvirt.VIR_DOMAIN_BLOCK_REBASE_RELATIVE
2519                 relative = rebase_base is not None
2520             except AttributeError:
2521                 LOG.warning(
2522                     "Relative blockrebase support was not detected. "
2523                     "Continuing with old behaviour.")
2524                 relative = False
2525 
2526             LOG.debug(
2527                 'disk: %(disk)s, base: %(base)s, '
2528                 'bw: %(bw)s, relative: %(relative)s',
2529                 {'disk': rebase_disk,
2530                  'base': rebase_base,
2531                  'bw': libvirt_guest.BlockDevice.REBASE_DEFAULT_BANDWIDTH,
2532                  'relative': str(relative)}, instance=instance)
2533 
2534             dev = guest.get_block_device(rebase_disk)
2535             if guest.is_active():
2536                 result = dev.rebase(rebase_base, relative=relative)
2537                 if result == 0:
2538                     LOG.debug('blockRebase started successfully',
2539                               instance=instance)
2540 
2541                 while not dev.is_job_complete():
2542                     LOG.debug('waiting for blockRebase job completion',
2543                               instance=instance)
2544                     time.sleep(0.5)
2545 
2546             # If the guest is not running libvirt won't do a blockRebase.
2547             # In that case, let's ask qemu-img to rebase the disk.
2548             else:
2549                 LOG.debug('Guest is not running so doing a block rebase '
2550                           'using "qemu-img rebase"', instance=instance)
2551                 self._rebase_with_qemu_img(guest, dev, active_disk_object,
2552                                            rebase_base)
2553 
2554         else:
2555             # commit with blockCommit()
2556             my_snap_base = None
2557             my_snap_top = None
2558             commit_disk = my_dev
2559 
2560             if active_protocol is not None:
2561                 my_snap_base = _get_snap_dev(delete_info['merge_target_file'],
2562                                              active_disk_object.backing_store)
2563                 my_snap_top = _get_snap_dev(delete_info['file_to_merge'],
2564                                             active_disk_object.backing_store)
2565 
2566             commit_base = my_snap_base or delete_info['merge_target_file']
2567             commit_top = my_snap_top or delete_info['file_to_merge']
2568 
2569             LOG.debug('will call blockCommit with commit_disk=%(commit_disk)s '
2570                       'commit_base=%(commit_base)s '
2571                       'commit_top=%(commit_top)s ',
2572                       {'commit_disk': commit_disk,
2573                        'commit_base': commit_base,
2574                        'commit_top': commit_top}, instance=instance)
2575 
2576             dev = guest.get_block_device(commit_disk)
2577             result = dev.commit(commit_base, commit_top, relative=True)
2578 
2579             if result == 0:
2580                 LOG.debug('blockCommit started successfully',
2581                           instance=instance)
2582 
2583             while not dev.is_job_complete():
2584                 LOG.debug('waiting for blockCommit job completion',
2585                           instance=instance)
2586                 time.sleep(0.5)
2587 
2588     def volume_snapshot_delete(self, context, instance, volume_id, snapshot_id,
2589                                delete_info):
2590         try:
2591             self._volume_snapshot_delete(context, instance, volume_id,
2592                                          snapshot_id, delete_info=delete_info)
2593         except Exception:
2594             with excutils.save_and_reraise_exception():
2595                 LOG.exception(_('Error occurred during '
2596                                 'volume_snapshot_delete, '
2597                                 'sending error status to Cinder.'),
2598                               instance=instance)
2599                 self._volume_snapshot_update_status(
2600                     context, snapshot_id, 'error_deleting')
2601 
2602         self._volume_snapshot_update_status(context, snapshot_id, 'deleting')
2603         self._volume_refresh_connection_info(context, instance, volume_id)
2604 
2605     def reboot(self, context, instance, network_info, reboot_type,
2606                block_device_info=None, bad_volumes_callback=None):
2607         """Reboot a virtual machine, given an instance reference."""
2608         if reboot_type == 'SOFT':
2609             # NOTE(vish): This will attempt to do a graceful shutdown/restart.
2610             try:
2611                 soft_reboot_success = self._soft_reboot(instance)
2612             except libvirt.libvirtError as e:
2613                 LOG.debug("Instance soft reboot failed: %s",
2614                           encodeutils.exception_to_unicode(e),
2615                           instance=instance)
2616                 soft_reboot_success = False
2617 
2618             if soft_reboot_success:
2619                 LOG.info("Instance soft rebooted successfully.",
2620                          instance=instance)
2621                 return
2622             else:
2623                 LOG.warning("Failed to soft reboot instance. "
2624                             "Trying hard reboot.",
2625                             instance=instance)
2626         return self._hard_reboot(context, instance, network_info,
2627                                  block_device_info)
2628 
2629     def _soft_reboot(self, instance):
2630         """Attempt to shutdown and restart the instance gracefully.
2631 
2632         We use shutdown and create here so we can return if the guest
2633         responded and actually rebooted. Note that this method only
2634         succeeds if the guest responds to acpi. Therefore we return
2635         success or failure so we can fall back to a hard reboot if
2636         necessary.
2637 
2638         :returns: True if the reboot succeeded
2639         """
2640         guest = self._host.get_guest(instance)
2641 
2642         state = guest.get_power_state(self._host)
2643         old_domid = guest.id
2644         # NOTE(vish): This check allows us to reboot an instance that
2645         #             is already shutdown.
2646         if state == power_state.RUNNING:
2647             guest.shutdown()
2648         # NOTE(vish): This actually could take slightly longer than the
2649         #             FLAG defines depending on how long the get_info
2650         #             call takes to return.
2651         self._prepare_pci_devices_for_use(
2652             pci_manager.get_instance_pci_devs(instance, 'all'))
2653         for x in range(CONF.libvirt.wait_soft_reboot_seconds):
2654             guest = self._host.get_guest(instance)
2655 
2656             state = guest.get_power_state(self._host)
2657             new_domid = guest.id
2658 
2659             # NOTE(ivoks): By checking domain IDs, we make sure we are
2660             #              not recreating domain that's already running.
2661             if old_domid != new_domid:
2662                 if state in [power_state.SHUTDOWN,
2663                              power_state.CRASHED]:
2664                     LOG.info("Instance shutdown successfully.",
2665                              instance=instance)
2666                     self._create_domain(domain=guest._domain)
2667                     timer = loopingcall.FixedIntervalLoopingCall(
2668                         self._wait_for_running, instance)
2669                     timer.start(interval=0.5).wait()
2670                     return True
2671                 else:
2672                     LOG.info("Instance may have been rebooted during soft "
2673                              "reboot, so return now.", instance=instance)
2674                     return True
2675             greenthread.sleep(1)
2676         return False
2677 
2678     def _hard_reboot(self, context, instance, network_info,
2679                      block_device_info=None):
2680         """Reboot a virtual machine, given an instance reference.
2681 
2682         Performs a Libvirt reset (if supported) on the domain.
2683 
2684         If Libvirt reset is unavailable this method actually destroys and
2685         re-creates the domain to ensure the reboot happens, as the guest
2686         OS cannot ignore this action.
2687         """
2688         # NOTE(sbauza): Since we undefine the guest XML when destroying, we
2689         # need to remember the existing mdevs for reusing them.
2690         mdevs = self._get_all_assigned_mediated_devices(instance)
2691         mdevs = list(mdevs.keys())
2692         # NOTE(mdbooth): In addition to performing a hard reboot of the domain,
2693         # the hard reboot operation is relied upon by operators to be an
2694         # automated attempt to fix as many things as possible about a
2695         # non-functioning instance before resorting to manual intervention.
2696         # With this goal in mind, we tear down all the aspects of an instance
2697         # we can here without losing data. This allows us to re-initialise from
2698         # scratch, and hopefully fix, most aspects of a non-functioning guest.
2699         self.destroy(context, instance, network_info, destroy_disks=False,
2700                      block_device_info=block_device_info)
2701 
2702         # Convert the system metadata to image metadata
2703         # NOTE(mdbooth): This is a workaround for stateless Nova compute
2704         #                https://bugs.launchpad.net/nova/+bug/1349978
2705         instance_dir = libvirt_utils.get_instance_path(instance)
2706         fileutils.ensure_tree(instance_dir)
2707 
2708         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
2709                                             instance,
2710                                             instance.image_meta,
2711                                             block_device_info)
2712         # NOTE(vish): This could generate the wrong device_format if we are
2713         #             using the raw backend and the images don't exist yet.
2714         #             The create_images_and_backing below doesn't properly
2715         #             regenerate raw backend images, however, so when it
2716         #             does we need to (re)generate the xml after the images
2717         #             are in place.
2718         xml = self._get_guest_xml(context, instance, network_info, disk_info,
2719                                   instance.image_meta,
2720                                   block_device_info=block_device_info,
2721                                   mdevs=mdevs)
2722 
2723         # NOTE(mdbooth): context.auth_token will not be set when we call
2724         #                _hard_reboot from resume_state_on_host_boot()
2725         if context.auth_token is not None:
2726             # NOTE (rmk): Re-populate any missing backing files.
2727             config = vconfig.LibvirtConfigGuest()
2728             config.parse_str(xml)
2729             backing_disk_info = self._get_instance_disk_info_from_config(
2730                 config, block_device_info)
2731             self._create_images_and_backing(context, instance, instance_dir,
2732                                             backing_disk_info)
2733 
2734         # Initialize all the necessary networking, block devices and
2735         # start the instance.
2736         # NOTE(melwitt): Pass vifs_already_plugged=True here even though we've
2737         # unplugged vifs earlier. The behavior of neutron plug events depends
2738         # on which vif type we're using and we are working with a stale network
2739         # info cache here, so won't rely on waiting for neutron plug events.
2740         # vifs_already_plugged=True means "do not wait for neutron plug events"
2741         self._create_domain_and_network(context, xml, instance, network_info,
2742                                         block_device_info=block_device_info,
2743                                         vifs_already_plugged=True)
2744         self._prepare_pci_devices_for_use(
2745             pci_manager.get_instance_pci_devs(instance, 'all'))
2746 
2747         def _wait_for_reboot():
2748             """Called at an interval until the VM is running again."""
2749             state = self.get_info(instance).state
2750 
2751             if state == power_state.RUNNING:
2752                 LOG.info("Instance rebooted successfully.",
2753                          instance=instance)
2754                 raise loopingcall.LoopingCallDone()
2755 
2756         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_reboot)
2757         timer.start(interval=0.5).wait()
2758 
2759     def pause(self, instance):
2760         """Pause VM instance."""
2761         self._host.get_guest(instance).pause()
2762 
2763     def unpause(self, instance):
2764         """Unpause paused VM instance."""
2765         guest = self._host.get_guest(instance)
2766         guest.resume()
2767         guest.sync_guest_time()
2768 
2769     def _clean_shutdown(self, instance, timeout, retry_interval):
2770         """Attempt to shutdown the instance gracefully.
2771 
2772         :param instance: The instance to be shutdown
2773         :param timeout: How long to wait in seconds for the instance to
2774                         shutdown
2775         :param retry_interval: How often in seconds to signal the instance
2776                                to shutdown while waiting
2777 
2778         :returns: True if the shutdown succeeded
2779         """
2780 
2781         # List of states that represent a shutdown instance
2782         SHUTDOWN_STATES = [power_state.SHUTDOWN,
2783                            power_state.CRASHED]
2784 
2785         try:
2786             guest = self._host.get_guest(instance)
2787         except exception.InstanceNotFound:
2788             # If the instance has gone then we don't need to
2789             # wait for it to shutdown
2790             return True
2791 
2792         state = guest.get_power_state(self._host)
2793         if state in SHUTDOWN_STATES:
2794             LOG.info("Instance already shutdown.", instance=instance)
2795             return True
2796 
2797         LOG.debug("Shutting down instance from state %s", state,
2798                   instance=instance)
2799         guest.shutdown()
2800         retry_countdown = retry_interval
2801 
2802         for sec in range(timeout):
2803 
2804             guest = self._host.get_guest(instance)
2805             state = guest.get_power_state(self._host)
2806 
2807             if state in SHUTDOWN_STATES:
2808                 LOG.info("Instance shutdown successfully after %d seconds.",
2809                          sec, instance=instance)
2810                 return True
2811 
2812             # Note(PhilD): We can't assume that the Guest was able to process
2813             #              any previous shutdown signal (for example it may
2814             #              have still been startingup, so within the overall
2815             #              timeout we re-trigger the shutdown every
2816             #              retry_interval
2817             if retry_countdown == 0:
2818                 retry_countdown = retry_interval
2819                 # Instance could shutdown at any time, in which case we
2820                 # will get an exception when we call shutdown
2821                 try:
2822                     LOG.debug("Instance in state %s after %d seconds - "
2823                               "resending shutdown", state, sec,
2824                               instance=instance)
2825                     guest.shutdown()
2826                 except libvirt.libvirtError:
2827                     # Assume this is because its now shutdown, so loop
2828                     # one more time to clean up.
2829                     LOG.debug("Ignoring libvirt exception from shutdown "
2830                               "request.", instance=instance)
2831                     continue
2832             else:
2833                 retry_countdown -= 1
2834 
2835             time.sleep(1)
2836 
2837         LOG.info("Instance failed to shutdown in %d seconds.",
2838                  timeout, instance=instance)
2839         return False
2840 
2841     def power_off(self, instance, timeout=0, retry_interval=0):
2842         """Power off the specified instance."""
2843         if timeout:
2844             self._clean_shutdown(instance, timeout, retry_interval)
2845         self._destroy(instance)
2846 
2847     def power_on(self, context, instance, network_info,
2848                  block_device_info=None):
2849         """Power on the specified instance."""
2850         # We use _hard_reboot here to ensure that all backing files,
2851         # network, and block device connections, etc. are established
2852         # and available before we attempt to start the instance.
2853         self._hard_reboot(context, instance, network_info, block_device_info)
2854 
2855     def trigger_crash_dump(self, instance):
2856 
2857         """Trigger crash dump by injecting an NMI to the specified instance."""
2858         try:
2859             self._host.get_guest(instance).inject_nmi()
2860         except libvirt.libvirtError as ex:
2861             error_code = ex.get_error_code()
2862 
2863             if error_code == libvirt.VIR_ERR_NO_SUPPORT:
2864                 raise exception.TriggerCrashDumpNotSupported()
2865             elif error_code == libvirt.VIR_ERR_OPERATION_INVALID:
2866                 raise exception.InstanceNotRunning(instance_id=instance.uuid)
2867 
2868             LOG.exception(_('Error from libvirt while injecting an NMI to '
2869                             '%(instance_uuid)s: '
2870                             '[Error Code %(error_code)s] %(ex)s'),
2871                           {'instance_uuid': instance.uuid,
2872                            'error_code': error_code, 'ex': ex})
2873             raise
2874 
2875     def suspend(self, context, instance):
2876         """Suspend the specified instance."""
2877         guest = self._host.get_guest(instance)
2878 
2879         self._detach_pci_devices(guest,
2880             pci_manager.get_instance_pci_devs(instance))
2881         self._detach_direct_passthrough_ports(context, instance, guest)
2882         self._detach_mediated_devices(guest)
2883         guest.save_memory_state()
2884 
2885     def resume(self, context, instance, network_info, block_device_info=None):
2886         """resume the specified instance."""
2887         xml = self._get_existing_domain_xml(instance, network_info,
2888                                             block_device_info)
2889         guest = self._create_domain_and_network(context, xml, instance,
2890                            network_info, block_device_info=block_device_info,
2891                            vifs_already_plugged=True)
2892         self._attach_pci_devices(guest,
2893             pci_manager.get_instance_pci_devs(instance))
2894         self._attach_direct_passthrough_ports(
2895             context, instance, guest, network_info)
2896         timer = loopingcall.FixedIntervalLoopingCall(self._wait_for_running,
2897                                                      instance)
2898         timer.start(interval=0.5).wait()
2899         guest.sync_guest_time()
2900 
2901     def resume_state_on_host_boot(self, context, instance, network_info,
2902                                   block_device_info=None):
2903         """resume guest state when a host is booted."""
2904         # Check if the instance is running already and avoid doing
2905         # anything if it is.
2906         try:
2907             guest = self._host.get_guest(instance)
2908             state = guest.get_power_state(self._host)
2909 
2910             ignored_states = (power_state.RUNNING,
2911                               power_state.SUSPENDED,
2912                               power_state.NOSTATE,
2913                               power_state.PAUSED)
2914 
2915             if state in ignored_states:
2916                 return
2917         except (exception.InternalError, exception.InstanceNotFound):
2918             pass
2919 
2920         # Instance is not up and could be in an unknown state.
2921         # Be as absolute as possible about getting it back into
2922         # a known and running state.
2923         self._hard_reboot(context, instance, network_info, block_device_info)
2924 
2925     def rescue(self, context, instance, network_info, image_meta,
2926                rescue_password):
2927         """Loads a VM using rescue images.
2928 
2929         A rescue is normally performed when something goes wrong with the
2930         primary images and data needs to be corrected/recovered. Rescuing
2931         should not edit or over-ride the original image, only allow for
2932         data recovery.
2933 
2934         """
2935         instance_dir = libvirt_utils.get_instance_path(instance)
2936         unrescue_xml = self._get_existing_domain_xml(instance, network_info)
2937         unrescue_xml_path = os.path.join(instance_dir, 'unrescue.xml')
2938         libvirt_utils.write_to_file(unrescue_xml_path, unrescue_xml)
2939 
2940         rescue_image_id = None
2941         if image_meta.obj_attr_is_set("id"):
2942             rescue_image_id = image_meta.id
2943 
2944         rescue_images = {
2945             'image_id': (rescue_image_id or
2946                         CONF.libvirt.rescue_image_id or instance.image_ref),
2947             'kernel_id': (CONF.libvirt.rescue_kernel_id or
2948                           instance.kernel_id),
2949             'ramdisk_id': (CONF.libvirt.rescue_ramdisk_id or
2950                            instance.ramdisk_id),
2951         }
2952         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
2953                                             instance,
2954                                             image_meta,
2955                                             rescue=True)
2956         injection_info = InjectionInfo(network_info=network_info,
2957                                        admin_pass=rescue_password,
2958                                        files=None)
2959         gen_confdrive = functools.partial(self._create_configdrive,
2960                                           context, instance, injection_info,
2961                                           rescue=True)
2962         self._create_image(context, instance, disk_info['mapping'],
2963                            injection_info=injection_info, suffix='.rescue',
2964                            disk_images=rescue_images)
2965         xml = self._get_guest_xml(context, instance, network_info, disk_info,
2966                                   image_meta, rescue=rescue_images)
2967         self._destroy(instance)
2968         self._create_domain(xml, post_xml_callback=gen_confdrive)
2969 
2970     def unrescue(self, instance, network_info):
2971         """Reboot the VM which is being rescued back into primary images.
2972         """
2973         instance_dir = libvirt_utils.get_instance_path(instance)
2974         unrescue_xml_path = os.path.join(instance_dir, 'unrescue.xml')
2975         xml = libvirt_utils.load_file(unrescue_xml_path)
2976         guest = self._host.get_guest(instance)
2977 
2978         # TODO(sahid): We are converting all calls from a
2979         # virDomain object to use nova.virt.libvirt.Guest.
2980         # We should be able to remove virt_dom at the end.
2981         virt_dom = guest._domain
2982         self._destroy(instance)
2983         self._create_domain(xml, virt_dom)
2984         os.unlink(unrescue_xml_path)
2985         rescue_files = os.path.join(instance_dir, "*.rescue")
2986         for rescue_file in glob.iglob(rescue_files):
2987             if os.path.isdir(rescue_file):
2988                 shutil.rmtree(rescue_file)
2989             else:
2990                 os.unlink(rescue_file)
2991         # cleanup rescue volume
2992         lvm.remove_volumes([lvmdisk for lvmdisk in self._lvm_disks(instance)
2993                                 if lvmdisk.endswith('.rescue')])
2994         if CONF.libvirt.images_type == 'rbd':
2995             filter_fn = lambda disk: (disk.startswith(instance.uuid) and
2996                                       disk.endswith('.rescue'))
2997             LibvirtDriver._get_rbd_driver().cleanup_volumes(filter_fn)
2998 
2999     def poll_rebooting_instances(self, timeout, instances):
3000         pass
3001 
3002     # NOTE(ilyaalekseyev): Implementation like in multinics
3003     # for xenapi(tr3buchet)
3004     def spawn(self, context, instance, image_meta, injected_files,
3005               admin_password, allocations, network_info=None,
3006               block_device_info=None):
3007         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
3008                                             instance,
3009                                             image_meta,
3010                                             block_device_info)
3011         injection_info = InjectionInfo(network_info=network_info,
3012                                        files=injected_files,
3013                                        admin_pass=admin_password)
3014         gen_confdrive = functools.partial(self._create_configdrive,
3015                                           context, instance,
3016                                           injection_info)
3017         self._create_image(context, instance, disk_info['mapping'],
3018                            injection_info=injection_info,
3019                            block_device_info=block_device_info)
3020 
3021         # Required by Quobyte CI
3022         self._ensure_console_log_for_instance(instance)
3023 
3024         # Does the guest need to be assigned some vGPU mediated devices ?
3025         mdevs = self._allocate_mdevs(allocations)
3026 
3027         xml = self._get_guest_xml(context, instance, network_info,
3028                                   disk_info, image_meta,
3029                                   block_device_info=block_device_info,
3030                                   mdevs=mdevs)
3031         self._create_domain_and_network(
3032             context, xml, instance, network_info,
3033             block_device_info=block_device_info,
3034             post_xml_callback=gen_confdrive,
3035             destroy_disks_on_failure=True)
3036         LOG.debug("Guest created on hypervisor", instance=instance)
3037 
3038         def _wait_for_boot():
3039             """Called at an interval until the VM is running."""
3040             state = self.get_info(instance).state
3041 
3042             if state == power_state.RUNNING:
3043                 LOG.info("Instance spawned successfully.", instance=instance)
3044                 raise loopingcall.LoopingCallDone()
3045 
3046         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_boot)
3047         timer.start(interval=0.5).wait()
3048 
3049     def _get_console_output_file(self, instance, console_log):
3050         bytes_to_read = MAX_CONSOLE_BYTES
3051         log_data = b""  # The last N read bytes
3052         i = 0  # in case there is a log rotation (like "virtlogd")
3053         path = console_log
3054 
3055         while bytes_to_read > 0 and os.path.exists(path):
3056             read_log_data, remaining = nova.privsep.path.last_bytes(
3057                                         path, bytes_to_read)
3058             # We need the log file content in chronological order,
3059             # that's why we *prepend* the log data.
3060             log_data = read_log_data + log_data
3061 
3062             # Prep to read the next file in the chain
3063             bytes_to_read -= len(read_log_data)
3064             path = console_log + "." + str(i)
3065             i += 1
3066 
3067             if remaining > 0:
3068                 LOG.info('Truncated console log returned, '
3069                          '%d bytes ignored', remaining, instance=instance)
3070         return log_data
3071 
3072     def get_console_output(self, context, instance):
3073         guest = self._host.get_guest(instance)
3074 
3075         xml = guest.get_xml_desc()
3076         tree = etree.fromstring(xml)
3077 
3078         # If the guest has a console logging to a file prefer to use that
3079         file_consoles = tree.findall("./devices/console[@type='file']")
3080         if file_consoles:
3081             for file_console in file_consoles:
3082                 source_node = file_console.find('./source')
3083                 if source_node is None:
3084                     continue
3085                 path = source_node.get("path")
3086                 if not path:
3087                     continue
3088 
3089                 if not os.path.exists(path):
3090                     LOG.info('Instance is configured with a file console, '
3091                              'but the backing file is not (yet?) present',
3092                              instance=instance)
3093                     return ""
3094 
3095                 return self._get_console_output_file(instance, path)
3096 
3097         # Try 'pty' types
3098         pty_consoles = tree.findall("./devices/console[@type='pty']")
3099         if pty_consoles:
3100             for pty_console in pty_consoles:
3101                 source_node = pty_console.find('./source')
3102                 if source_node is None:
3103                     continue
3104                 pty = source_node.get("path")
3105                 if not pty:
3106                     continue
3107                 break
3108             else:
3109                 raise exception.ConsoleNotAvailable()
3110         else:
3111             raise exception.ConsoleNotAvailable()
3112 
3113         console_log = self._get_console_log_path(instance)
3114         data = nova.privsep.libvirt.readpty(pty)
3115 
3116         # NOTE(markus_z): The virt_types kvm and qemu are the only ones
3117         # which create a dedicated file device for the console logging.
3118         # Other virt_types like xen, lxc, uml, parallels depend on the
3119         # flush of that pty device into the "console.log" file to ensure
3120         # that a series of "get_console_output" calls return the complete
3121         # content even after rebooting a guest.
3122         nova.privsep.path.writefile(console_log, 'a+', data)
3123         return self._get_console_output_file(instance, console_log)
3124 
3125     def get_host_ip_addr(self):
3126         ips = compute_utils.get_machine_ips()
3127         if CONF.my_ip not in ips:
3128             LOG.warning('my_ip address (%(my_ip)s) was not found on '
3129                         'any of the interfaces: %(ifaces)s',
3130                         {'my_ip': CONF.my_ip, 'ifaces': ", ".join(ips)})
3131         return CONF.my_ip
3132 
3133     def get_vnc_console(self, context, instance):
3134         def get_vnc_port_for_instance(instance_name):
3135             guest = self._host.get_guest(instance)
3136 
3137             xml = guest.get_xml_desc()
3138             xml_dom = etree.fromstring(xml)
3139 
3140             graphic = xml_dom.find("./devices/graphics[@type='vnc']")
3141             if graphic is not None:
3142                 return graphic.get('port')
3143             # NOTE(rmk): We had VNC consoles enabled but the instance in
3144             # question is not actually listening for connections.
3145             raise exception.ConsoleTypeUnavailable(console_type='vnc')
3146 
3147         port = get_vnc_port_for_instance(instance.name)
3148         host = CONF.vnc.server_proxyclient_address
3149 
3150         return ctype.ConsoleVNC(host=host, port=port)
3151 
3152     def get_spice_console(self, context, instance):
3153         def get_spice_ports_for_instance(instance_name):
3154             guest = self._host.get_guest(instance)
3155 
3156             xml = guest.get_xml_desc()
3157             xml_dom = etree.fromstring(xml)
3158 
3159             graphic = xml_dom.find("./devices/graphics[@type='spice']")
3160             if graphic is not None:
3161                 return (graphic.get('port'), graphic.get('tlsPort'))
3162             # NOTE(rmk): We had Spice consoles enabled but the instance in
3163             # question is not actually listening for connections.
3164             raise exception.ConsoleTypeUnavailable(console_type='spice')
3165 
3166         ports = get_spice_ports_for_instance(instance.name)
3167         host = CONF.spice.server_proxyclient_address
3168 
3169         return ctype.ConsoleSpice(host=host, port=ports[0], tlsPort=ports[1])
3170 
3171     def get_serial_console(self, context, instance):
3172         guest = self._host.get_guest(instance)
3173         for hostname, port in self._get_serial_ports_from_guest(
3174                 guest, mode='bind'):
3175             return ctype.ConsoleSerial(host=hostname, port=port)
3176         raise exception.ConsoleTypeUnavailable(console_type='serial')
3177 
3178     @staticmethod
3179     def _create_ephemeral(target, ephemeral_size,
3180                           fs_label, os_type, is_block_dev=False,
3181                           context=None, specified_fs=None,
3182                           vm_mode=None):
3183         if not is_block_dev:
3184             if (CONF.libvirt.virt_type == "parallels" and
3185                     vm_mode == fields.VMMode.EXE):
3186 
3187                 libvirt_utils.create_ploop_image('expanded', target,
3188                                                  '%dG' % ephemeral_size,
3189                                                  specified_fs)
3190                 return
3191             libvirt_utils.create_image('raw', target, '%dG' % ephemeral_size)
3192 
3193         # Run as root only for block devices.
3194         disk_api.mkfs(os_type, fs_label, target, run_as_root=is_block_dev,
3195                       specified_fs=specified_fs)
3196 
3197     @staticmethod
3198     def _create_swap(target, swap_mb, context=None):
3199         """Create a swap file of specified size."""
3200         libvirt_utils.create_image('raw', target, '%dM' % swap_mb)
3201         nova.privsep.fs.unprivileged_mkfs('swap', target)
3202 
3203     @staticmethod
3204     def _get_console_log_path(instance):
3205         return os.path.join(libvirt_utils.get_instance_path(instance),
3206                             'console.log')
3207 
3208     def _ensure_console_log_for_instance(self, instance):
3209         # NOTE(mdbooth): Although libvirt will create this file for us
3210         # automatically when it starts, it will initially create it with
3211         # root ownership and then chown it depending on the configuration of
3212         # the domain it is launching. Quobyte CI explicitly disables the
3213         # chown by setting dynamic_ownership=0 in libvirt's config.
3214         # Consequently when the domain starts it is unable to write to its
3215         # console.log. See bug https://bugs.launchpad.net/nova/+bug/1597644
3216         #
3217         # To work around this, we create the file manually before starting
3218         # the domain so it has the same ownership as Nova. This works
3219         # for Quobyte CI because it is also configured to run qemu as the same
3220         # user as the Nova service. Installations which don't set
3221         # dynamic_ownership=0 are not affected because libvirt will always
3222         # correctly configure permissions regardless of initial ownership.
3223         #
3224         # Setting dynamic_ownership=0 is dubious and potentially broken in
3225         # more ways than console.log (see comment #22 on the above bug), so
3226         # Future Maintainer who finds this code problematic should check to see
3227         # if we still support it.
3228         console_file = self._get_console_log_path(instance)
3229         LOG.debug('Ensure instance console log exists: %s', console_file,
3230                   instance=instance)
3231         try:
3232             libvirt_utils.file_open(console_file, 'a').close()
3233         # NOTE(sfinucan): We can safely ignore permission issues here and
3234         # assume that it is libvirt that has taken ownership of this file.
3235         except IOError as ex:
3236             if ex.errno != errno.EACCES:
3237                 raise
3238             LOG.debug('Console file already exists: %s.', console_file)
3239 
3240     @staticmethod
3241     def _get_disk_config_image_type():
3242         # TODO(mikal): there is a bug here if images_type has
3243         # changed since creation of the instance, but I am pretty
3244         # sure that this bug already exists.
3245         return 'rbd' if CONF.libvirt.images_type == 'rbd' else 'raw'
3246 
3247     @staticmethod
3248     def _is_booted_from_volume(block_device_info):
3249         """Determines whether the VM is booting from volume
3250 
3251         Determines whether the block device info indicates that the VM
3252         is booting from a volume.
3253         """
3254         block_device_mapping = driver.block_device_info_get_mapping(
3255             block_device_info)
3256         return bool(block_device.get_root_bdm(block_device_mapping))
3257 
3258     def _inject_data(self, disk, instance, injection_info):
3259         """Injects data in a disk image
3260 
3261         Helper used for injecting data in a disk image file system.
3262 
3263         :param disk: The disk we're injecting into (an Image object)
3264         :param instance: The instance we're injecting into
3265         :param injection_info: Injection info
3266         """
3267         # Handles the partition need to be used.
3268         LOG.debug('Checking root disk injection %s',
3269                   str(injection_info), instance=instance)
3270         target_partition = None
3271         if not instance.kernel_id:
3272             target_partition = CONF.libvirt.inject_partition
3273             if target_partition == 0:
3274                 target_partition = None
3275         if CONF.libvirt.virt_type == 'lxc':
3276             target_partition = None
3277 
3278         # Handles the key injection.
3279         if CONF.libvirt.inject_key and instance.get('key_data'):
3280             key = str(instance.key_data)
3281         else:
3282             key = None
3283 
3284         # Handles the admin password injection.
3285         if not CONF.libvirt.inject_password:
3286             admin_pass = None
3287         else:
3288             admin_pass = injection_info.admin_pass
3289 
3290         # Handles the network injection.
3291         net = netutils.get_injected_network_template(
3292             injection_info.network_info,
3293             libvirt_virt_type=CONF.libvirt.virt_type)
3294 
3295         # Handles the metadata injection
3296         metadata = instance.get('metadata')
3297 
3298         if any((key, net, metadata, admin_pass, injection_info.files)):
3299             LOG.debug('Injecting %s', str(injection_info),
3300                       instance=instance)
3301             img_id = instance.image_ref
3302             try:
3303                 disk_api.inject_data(disk.get_model(self._conn),
3304                                      key, net, metadata, admin_pass,
3305                                      injection_info.files,
3306                                      partition=target_partition,
3307                                      mandatory=('files',))
3308             except Exception as e:
3309                 with excutils.save_and_reraise_exception():
3310                     LOG.error('Error injecting data into image '
3311                               '%(img_id)s (%(e)s)',
3312                               {'img_id': img_id, 'e': e},
3313                               instance=instance)
3314 
3315     # NOTE(sileht): many callers of this method assume that this
3316     # method doesn't fail if an image already exists but instead
3317     # think that it will be reused (ie: (live)-migration/resize)
3318     def _create_image(self, context, instance,
3319                       disk_mapping, injection_info=None, suffix='',
3320                       disk_images=None, block_device_info=None,
3321                       fallback_from_host=None,
3322                       ignore_bdi_for_swap=False):
3323         booted_from_volume = self._is_booted_from_volume(block_device_info)
3324 
3325         def image(fname, image_type=CONF.libvirt.images_type):
3326             return self.image_backend.by_name(instance,
3327                                               fname + suffix, image_type)
3328 
3329         def raw(fname):
3330             return image(fname, image_type='raw')
3331 
3332         # ensure directories exist and are writable
3333         fileutils.ensure_tree(libvirt_utils.get_instance_path(instance))
3334 
3335         LOG.info('Creating image', instance=instance)
3336 
3337         inst_type = instance.get_flavor()
3338         swap_mb = 0
3339         if 'disk.swap' in disk_mapping:
3340             mapping = disk_mapping['disk.swap']
3341 
3342             if ignore_bdi_for_swap:
3343                 # This is a workaround to support legacy swap resizing,
3344                 # which does not touch swap size specified in bdm,
3345                 # but works with flavor specified size only.
3346                 # In this case we follow the legacy logic and ignore block
3347                 # device info completely.
3348                 # NOTE(ft): This workaround must be removed when a correct
3349                 # implementation of resize operation changing sizes in bdms is
3350                 # developed. Also at that stage we probably may get rid of
3351                 # the direct usage of flavor swap size here,
3352                 # leaving the work with bdm only.
3353                 swap_mb = inst_type['swap']
3354             else:
3355                 swap = driver.block_device_info_get_swap(block_device_info)
3356                 if driver.swap_is_usable(swap):
3357                     swap_mb = swap['swap_size']
3358                 elif (inst_type['swap'] > 0 and
3359                       not block_device.volume_in_mapping(
3360                         mapping['dev'], block_device_info)):
3361                     swap_mb = inst_type['swap']
3362 
3363             if swap_mb > 0:
3364                 if (CONF.libvirt.virt_type == "parallels" and
3365                         instance.vm_mode == fields.VMMode.EXE):
3366                     msg = _("Swap disk is not supported "
3367                             "for Virtuozzo container")
3368                     raise exception.Invalid(msg)
3369 
3370         if not disk_images:
3371             disk_images = {'image_id': instance.image_ref,
3372                            'kernel_id': instance.kernel_id,
3373                            'ramdisk_id': instance.ramdisk_id}
3374 
3375         if disk_images['kernel_id']:
3376             fname = imagecache.get_cache_fname(disk_images['kernel_id'])
3377             raw('kernel').cache(fetch_func=libvirt_utils.fetch_raw_image,
3378                                 context=context,
3379                                 filename=fname,
3380                                 image_id=disk_images['kernel_id'])
3381             if disk_images['ramdisk_id']:
3382                 fname = imagecache.get_cache_fname(disk_images['ramdisk_id'])
3383                 raw('ramdisk').cache(fetch_func=libvirt_utils.fetch_raw_image,
3384                                      context=context,
3385                                      filename=fname,
3386                                      image_id=disk_images['ramdisk_id'])
3387 
3388         if CONF.libvirt.virt_type == 'uml':
3389             # PONDERING(mikal): can I assume that root is UID zero in every
3390             # OS? Probably not.
3391             uid = pwd.getpwnam('root').pw_uid
3392             nova.privsep.path.chown(image('disk').path, uid=uid)
3393 
3394         self._create_and_inject_local_root(context, instance,
3395                                            booted_from_volume, suffix,
3396                                            disk_images, injection_info,
3397                                            fallback_from_host)
3398 
3399         # Lookup the filesystem type if required
3400         os_type_with_default = nova.privsep.fs.get_fs_type_for_os_type(
3401             instance.os_type)
3402         # Generate a file extension based on the file system
3403         # type and the mkfs commands configured if any
3404         file_extension = nova.privsep.fs.get_file_extension_for_os_type(
3405             os_type_with_default, CONF.default_ephemeral_format)
3406 
3407         vm_mode = fields.VMMode.get_from_instance(instance)
3408         ephemeral_gb = instance.flavor.ephemeral_gb
3409         if 'disk.local' in disk_mapping:
3410             disk_image = image('disk.local')
3411             fn = functools.partial(self._create_ephemeral,
3412                                    fs_label='ephemeral0',
3413                                    os_type=instance.os_type,
3414                                    is_block_dev=disk_image.is_block_dev,
3415                                    vm_mode=vm_mode)
3416             fname = "ephemeral_%s_%s" % (ephemeral_gb, file_extension)
3417             size = ephemeral_gb * units.Gi
3418             disk_image.cache(fetch_func=fn,
3419                              context=context,
3420                              filename=fname,
3421                              size=size,
3422                              ephemeral_size=ephemeral_gb)
3423 
3424         for idx, eph in enumerate(driver.block_device_info_get_ephemerals(
3425                 block_device_info)):
3426             disk_image = image(blockinfo.get_eph_disk(idx))
3427 
3428             specified_fs = eph.get('guest_format')
3429             if specified_fs and not self.is_supported_fs_format(specified_fs):
3430                 msg = _("%s format is not supported") % specified_fs
3431                 raise exception.InvalidBDMFormat(details=msg)
3432 
3433             fn = functools.partial(self._create_ephemeral,
3434                                    fs_label='ephemeral%d' % idx,
3435                                    os_type=instance.os_type,
3436                                    is_block_dev=disk_image.is_block_dev,
3437                                    vm_mode=vm_mode)
3438             size = eph['size'] * units.Gi
3439             fname = "ephemeral_%s_%s" % (eph['size'], file_extension)
3440             disk_image.cache(fetch_func=fn,
3441                              context=context,
3442                              filename=fname,
3443                              size=size,
3444                              ephemeral_size=eph['size'],
3445                              specified_fs=specified_fs)
3446 
3447         if swap_mb > 0:
3448             size = swap_mb * units.Mi
3449             image('disk.swap').cache(fetch_func=self._create_swap,
3450                                      context=context,
3451                                      filename="swap_%s" % swap_mb,
3452                                      size=size,
3453                                      swap_mb=swap_mb)
3454 
3455     def _create_and_inject_local_root(self, context, instance,
3456                                       booted_from_volume, suffix, disk_images,
3457                                       injection_info, fallback_from_host):
3458         # File injection only if needed
3459         need_inject = (not configdrive.required_by(instance) and
3460                        injection_info is not None and
3461                        CONF.libvirt.inject_partition != -2)
3462 
3463         # NOTE(ndipanov): Even if disk_mapping was passed in, which
3464         # currently happens only on rescue - we still don't want to
3465         # create a base image.
3466         if not booted_from_volume:
3467             root_fname = imagecache.get_cache_fname(disk_images['image_id'])
3468             size = instance.flavor.root_gb * units.Gi
3469 
3470             if size == 0 or suffix == '.rescue':
3471                 size = None
3472 
3473             backend = self.image_backend.by_name(instance, 'disk' + suffix,
3474                                                  CONF.libvirt.images_type)
3475             if instance.task_state == task_states.RESIZE_FINISH:
3476                 backend.create_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME)
3477             if backend.SUPPORTS_CLONE:
3478                 def clone_fallback_to_fetch(*args, **kwargs):
3479                     try:
3480                         backend.clone(context, disk_images['image_id'])
3481                     except exception.ImageUnacceptable:
3482                         libvirt_utils.fetch_image(*args, **kwargs)
3483                 fetch_func = clone_fallback_to_fetch
3484             else:
3485                 fetch_func = libvirt_utils.fetch_image
3486             self._try_fetch_image_cache(backend, fetch_func, context,
3487                                         root_fname, disk_images['image_id'],
3488                                         instance, size, fallback_from_host)
3489 
3490             if need_inject:
3491                 self._inject_data(backend, instance, injection_info)
3492 
3493         elif need_inject:
3494             LOG.warning('File injection into a boot from volume '
3495                         'instance is not supported', instance=instance)
3496 
3497     def _create_configdrive(self, context, instance, injection_info,
3498                             rescue=False):
3499         # As this method being called right after the definition of a
3500         # domain, but before its actual launch, device metadata will be built
3501         # and saved in the instance for it to be used by the config drive and
3502         # the metadata service.
3503         instance.device_metadata = self._build_device_metadata(context,
3504                                                                instance)
3505         if configdrive.required_by(instance):
3506             LOG.info('Using config drive', instance=instance)
3507 
3508             name = 'disk.config'
3509             if rescue:
3510                 name += '.rescue'
3511 
3512             config_disk = self.image_backend.by_name(
3513                 instance, name, self._get_disk_config_image_type())
3514 
3515             # Don't overwrite an existing config drive
3516             if not config_disk.exists():
3517                 extra_md = {}
3518                 if injection_info.admin_pass:
3519                     extra_md['admin_pass'] = injection_info.admin_pass
3520 
3521                 inst_md = instance_metadata.InstanceMetadata(
3522                     instance, content=injection_info.files, extra_md=extra_md,
3523                     network_info=injection_info.network_info,
3524                     request_context=context)
3525 
3526                 cdb = configdrive.ConfigDriveBuilder(instance_md=inst_md)
3527                 with cdb:
3528                     # NOTE(mdbooth): We're hardcoding here the path of the
3529                     # config disk when using the flat backend. This isn't
3530                     # good, but it's required because we need a local path we
3531                     # know we can write to in case we're subsequently
3532                     # importing into rbd. This will be cleaned up when we
3533                     # replace this with a call to create_from_func, but that
3534                     # can't happen until we've updated the backends and we
3535                     # teach them not to cache config disks. This isn't
3536                     # possible while we're still using cache() under the hood.
3537                     config_disk_local_path = os.path.join(
3538                         libvirt_utils.get_instance_path(instance), name)
3539                     LOG.info('Creating config drive at %(path)s',
3540                              {'path': config_disk_local_path},
3541                              instance=instance)
3542 
3543                     try:
3544                         cdb.make_drive(config_disk_local_path)
3545                     except processutils.ProcessExecutionError as e:
3546                         with excutils.save_and_reraise_exception():
3547                             LOG.error('Creating config drive failed with '
3548                                       'error: %s', e, instance=instance)
3549 
3550                 try:
3551                     config_disk.import_file(
3552                         instance, config_disk_local_path, name)
3553                 finally:
3554                     # NOTE(mikal): if the config drive was imported into RBD,
3555                     # then we no longer need the local copy
3556                     if CONF.libvirt.images_type == 'rbd':
3557                         LOG.info('Deleting local config drive %(path)s '
3558                                  'because it was imported into RBD.',
3559                                  {'path': config_disk_local_path},
3560                                  instance=instance)
3561                         os.unlink(config_disk_local_path)
3562 
3563     def _prepare_pci_devices_for_use(self, pci_devices):
3564         # kvm , qemu support managed mode
3565         # In managed mode, the configured device will be automatically
3566         # detached from the host OS drivers when the guest is started,
3567         # and then re-attached when the guest shuts down.
3568         if CONF.libvirt.virt_type != 'xen':
3569             # we do manual detach only for xen
3570             return
3571         try:
3572             for dev in pci_devices:
3573                 libvirt_dev_addr = dev['hypervisor_name']
3574                 libvirt_dev = \
3575                         self._host.device_lookup_by_name(libvirt_dev_addr)
3576                 # Note(yjiang5) Spelling for 'dettach' is correct, see
3577                 # http://libvirt.org/html/libvirt-libvirt.html.
3578                 libvirt_dev.dettach()
3579 
3580             # Note(yjiang5): A reset of one PCI device may impact other
3581             # devices on the same bus, thus we need two separated loops
3582             # to detach and then reset it.
3583             for dev in pci_devices:
3584                 libvirt_dev_addr = dev['hypervisor_name']
3585                 libvirt_dev = \
3586                         self._host.device_lookup_by_name(libvirt_dev_addr)
3587                 libvirt_dev.reset()
3588 
3589         except libvirt.libvirtError as exc:
3590             raise exception.PciDevicePrepareFailed(id=dev['id'],
3591                                                    instance_uuid=
3592                                                    dev['instance_uuid'],
3593                                                    reason=six.text_type(exc))
3594 
3595     def _detach_pci_devices(self, guest, pci_devs):
3596         try:
3597             for dev in pci_devs:
3598                 guest.detach_device(self._get_guest_pci_device(dev), live=True)
3599                 # after detachDeviceFlags returned, we should check the dom to
3600                 # ensure the detaching is finished
3601                 xml = guest.get_xml_desc()
3602                 xml_doc = etree.fromstring(xml)
3603                 guest_config = vconfig.LibvirtConfigGuest()
3604                 guest_config.parse_dom(xml_doc)
3605 
3606                 for hdev in [d for d in guest_config.devices
3607                     if isinstance(d, vconfig.LibvirtConfigGuestHostdevPCI)]:
3608                     hdbsf = [hdev.domain, hdev.bus, hdev.slot, hdev.function]
3609                     dbsf = pci_utils.parse_address(dev.address)
3610                     if [int(x, 16) for x in hdbsf] ==\
3611                             [int(x, 16) for x in dbsf]:
3612                         raise exception.PciDeviceDetachFailed(reason=
3613                                                               "timeout",
3614                                                               dev=dev)
3615 
3616         except libvirt.libvirtError as ex:
3617             error_code = ex.get_error_code()
3618             if error_code == libvirt.VIR_ERR_NO_DOMAIN:
3619                 LOG.warning("Instance disappeared while detaching "
3620                             "a PCI device from it.")
3621             else:
3622                 raise
3623 
3624     def _attach_pci_devices(self, guest, pci_devs):
3625         try:
3626             for dev in pci_devs:
3627                 guest.attach_device(self._get_guest_pci_device(dev))
3628 
3629         except libvirt.libvirtError:
3630             LOG.error('Attaching PCI devices %(dev)s to %(dom)s failed.',
3631                       {'dev': pci_devs, 'dom': guest.id})
3632             raise
3633 
3634     @staticmethod
3635     def _has_direct_passthrough_port(network_info):
3636         for vif in network_info:
3637             if (vif['vnic_type'] in
3638                 network_model.VNIC_TYPES_DIRECT_PASSTHROUGH):
3639                 return True
3640         return False
3641 
3642     def _attach_direct_passthrough_ports(
3643         self, context, instance, guest, network_info=None):
3644         if network_info is None:
3645             network_info = instance.info_cache.network_info
3646         if network_info is None:
3647             return
3648 
3649         if self._has_direct_passthrough_port(network_info):
3650             for vif in network_info:
3651                 if (vif['vnic_type'] in
3652                     network_model.VNIC_TYPES_DIRECT_PASSTHROUGH):
3653                     cfg = self.vif_driver.get_config(instance,
3654                                                      vif,
3655                                                      instance.image_meta,
3656                                                      instance.flavor,
3657                                                      CONF.libvirt.virt_type,
3658                                                      self._host)
3659                     LOG.debug('Attaching direct passthrough port %(port)s '
3660                               'to %(dom)s', {'port': vif, 'dom': guest.id},
3661                               instance=instance)
3662                     guest.attach_device(cfg)
3663 
3664     def _detach_direct_passthrough_ports(self, context, instance, guest):
3665         network_info = instance.info_cache.network_info
3666         if network_info is None:
3667             return
3668 
3669         if self._has_direct_passthrough_port(network_info):
3670             # In case of VNIC_TYPES_DIRECT_PASSTHROUGH ports we create
3671             # pci request per direct passthrough port. Therefore we can trust
3672             # that pci_slot value in the vif is correct.
3673             direct_passthrough_pci_addresses = [
3674                 vif['profile']['pci_slot']
3675                 for vif in network_info
3676                 if (vif['vnic_type'] in
3677                     network_model.VNIC_TYPES_DIRECT_PASSTHROUGH and
3678                     vif['profile'].get('pci_slot') is not None)
3679             ]
3680 
3681             # use detach_pci_devices to avoid failure in case of
3682             # multiple guest direct passthrough ports with the same MAC
3683             # (protection use-case, ports are on different physical
3684             # interfaces)
3685             pci_devs = pci_manager.get_instance_pci_devs(instance, 'all')
3686             direct_passthrough_pci_addresses = (
3687                 [pci_dev for pci_dev in pci_devs
3688                  if pci_dev.address in direct_passthrough_pci_addresses])
3689             self._detach_pci_devices(guest, direct_passthrough_pci_addresses)
3690 
3691     def _set_host_enabled(self, enabled,
3692                           disable_reason=DISABLE_REASON_UNDEFINED):
3693         """Enables / Disables the compute service on this host.
3694 
3695            This doesn't override non-automatic disablement with an automatic
3696            setting; thereby permitting operators to keep otherwise
3697            healthy hosts out of rotation.
3698         """
3699 
3700         status_name = {True: 'disabled',
3701                        False: 'enabled'}
3702 
3703         disable_service = not enabled
3704 
3705         ctx = nova_context.get_admin_context()
3706         try:
3707             service = objects.Service.get_by_compute_host(ctx, CONF.host)
3708 
3709             if service.disabled != disable_service:
3710                 # Note(jang): this is a quick fix to stop operator-
3711                 # disabled compute hosts from re-enabling themselves
3712                 # automatically. We prefix any automatic reason code
3713                 # with a fixed string. We only re-enable a host
3714                 # automatically if we find that string in place.
3715                 # This should probably be replaced with a separate flag.
3716                 if not service.disabled or (
3717                         service.disabled_reason and
3718                         service.disabled_reason.startswith(DISABLE_PREFIX)):
3719                     service.disabled = disable_service
3720                     service.disabled_reason = (
3721                        DISABLE_PREFIX + disable_reason
3722                        if disable_service and disable_reason else
3723                            DISABLE_REASON_UNDEFINED)
3724                     service.save()
3725                     LOG.debug('Updating compute service status to %s',
3726                               status_name[disable_service])
3727                 else:
3728                     LOG.debug('Not overriding manual compute service '
3729                               'status with: %s',
3730                               status_name[disable_service])
3731         except exception.ComputeHostNotFound:
3732             LOG.warning('Cannot update service status on host "%s" '
3733                         'since it is not registered.', CONF.host)
3734         except Exception:
3735             LOG.warning('Cannot update service status on host "%s" '
3736                         'due to an unexpected exception.', CONF.host,
3737                         exc_info=True)
3738 
3739         if enabled:
3740             mount.get_manager().host_up(self._host)
3741         else:
3742             mount.get_manager().host_down()
3743 
3744     def _get_guest_cpu_model_config(self):
3745         mode = CONF.libvirt.cpu_mode
3746         model = CONF.libvirt.cpu_model
3747         extra_flags = set([flag.lower() for flag in
3748             CONF.libvirt.cpu_model_extra_flags])
3749 
3750         if (CONF.libvirt.virt_type == "kvm" or
3751             CONF.libvirt.virt_type == "qemu"):
3752             if mode is None:
3753                 caps = self._host.get_capabilities()
3754                 # AArch64 lacks 'host-model' support because neither libvirt
3755                 # nor QEMU are able to tell what the host CPU model exactly is.
3756                 # And there is no CPU description code for ARM(64) at this
3757                 # point.
3758 
3759                 # Also worth noting: 'host-passthrough' mode will completely
3760                 # break live migration, *unless* all the Compute nodes (running
3761                 # libvirtd) have *identical* CPUs.
3762                 if caps.host.cpu.arch == fields.Architecture.AARCH64:
3763                     mode = "host-passthrough"
3764                     LOG.info('CPU mode "host-passthrough" was chosen. Live '
3765                              'migration can break unless all compute nodes '
3766                              'have identical cpus. AArch64 does not support '
3767                              'other modes.')
3768                 else:
3769                     mode = "host-model"
3770             if mode == "none":
3771                 return vconfig.LibvirtConfigGuestCPU()
3772         else:
3773             if mode is None or mode == "none":
3774                 return None
3775 
3776         if ((CONF.libvirt.virt_type != "kvm" and
3777              CONF.libvirt.virt_type != "qemu")):
3778             msg = _("Config requested an explicit CPU model, but "
3779                     "the current libvirt hypervisor '%s' does not "
3780                     "support selecting CPU models") % CONF.libvirt.virt_type
3781             raise exception.Invalid(msg)
3782 
3783         if mode == "custom" and model is None:
3784             msg = _("Config requested a custom CPU model, but no "
3785                     "model name was provided")
3786             raise exception.Invalid(msg)
3787         elif mode != "custom" and model is not None:
3788             msg = _("A CPU model name should not be set when a "
3789                     "host CPU model is requested")
3790             raise exception.Invalid(msg)
3791 
3792         LOG.debug("CPU mode '%(mode)s' model '%(model)s' was chosen, "
3793                   "with extra flags: '%(extra_flags)s'",
3794                   {'mode': mode,
3795                    'model': (model or ""),
3796                    'extra_flags': (extra_flags or "")})
3797 
3798         cpu = vconfig.LibvirtConfigGuestCPU()
3799         cpu.mode = mode
3800         cpu.model = model
3801 
3802         # NOTE (kchamart): Currently there's no existing way to ask if a
3803         # given CPU model + CPU flags combination is supported by KVM &
3804         # a specific QEMU binary.  However, libvirt runs the 'CPUID'
3805         # command upfront -- before even a Nova instance (a QEMU
3806         # process) is launched -- to construct CPU models and check
3807         # their validity; so we are good there.  In the long-term,
3808         # upstream libvirt intends to add an additional new API that can
3809         # do fine-grained validation of a certain CPU model + CPU flags
3810         # against a specific QEMU binary (the libvirt RFE bug for that:
3811         # https://bugzilla.redhat.com/show_bug.cgi?id=1559832).
3812         for flag in extra_flags:
3813             cpu.add_feature(vconfig.LibvirtConfigGuestCPUFeature(flag))
3814 
3815         return cpu
3816 
3817     def _get_guest_cpu_config(self, flavor, image_meta,
3818                               guest_cpu_numa_config, instance_numa_topology):
3819         cpu = self._get_guest_cpu_model_config()
3820 
3821         if cpu is None:
3822             return None
3823 
3824         topology = hardware.get_best_cpu_topology(
3825                 flavor, image_meta, numa_topology=instance_numa_topology)
3826 
3827         cpu.sockets = topology.sockets
3828         cpu.cores = topology.cores
3829         cpu.threads = topology.threads
3830         cpu.numa = guest_cpu_numa_config
3831 
3832         return cpu
3833 
3834     def _get_guest_disk_config(self, instance, name, disk_mapping, inst_type,
3835                                image_type=None):
3836         disk_unit = None
3837         disk = self.image_backend.by_name(instance, name, image_type)
3838         if (name == 'disk.config' and image_type == 'rbd' and
3839                 not disk.exists()):
3840             # This is likely an older config drive that has not been migrated
3841             # to rbd yet. Try to fall back on 'flat' image type.
3842             # TODO(melwitt): Add online migration of some sort so we can
3843             # remove this fall back once we know all config drives are in rbd.
3844             # NOTE(vladikr): make sure that the flat image exist, otherwise
3845             # the image will be created after the domain definition.
3846             flat_disk = self.image_backend.by_name(instance, name, 'flat')
3847             if flat_disk.exists():
3848                 disk = flat_disk
3849                 LOG.debug('Config drive not found in RBD, falling back to the '
3850                           'instance directory', instance=instance)
3851         disk_info = disk_mapping[name]
3852         if 'unit' in disk_mapping and disk_info['bus'] == 'scsi':
3853             disk_unit = disk_mapping['unit']
3854             disk_mapping['unit'] += 1  # Increments for the next disk added
3855         conf = disk.libvirt_info(disk_info['bus'],
3856                                  disk_info['dev'],
3857                                  disk_info['type'],
3858                                  self.disk_cachemode,
3859                                  inst_type['extra_specs'],
3860                                  self._host.get_version(),
3861                                  disk_unit=disk_unit)
3862         return conf
3863 
3864     def _get_guest_fs_config(self, instance, name, image_type=None):
3865         disk = self.image_backend.by_name(instance, name, image_type)
3866         return disk.libvirt_fs_info("/", "ploop")
3867 
3868     def _get_guest_storage_config(self, context, instance, image_meta,
3869                                   disk_info,
3870                                   rescue, block_device_info,
3871                                   inst_type, os_type):
3872         devices = []
3873         disk_mapping = disk_info['mapping']
3874 
3875         block_device_mapping = driver.block_device_info_get_mapping(
3876             block_device_info)
3877         mount_rootfs = CONF.libvirt.virt_type == "lxc"
3878         scsi_controller = self._get_scsi_controller(image_meta)
3879 
3880         if scsi_controller and scsi_controller.model == 'virtio-scsi':
3881             # The virtio-scsi can handle up to 256 devices but the
3882             # optional element "address" must be defined to describe
3883             # where the device is placed on the controller (see:
3884             # LibvirtConfigGuestDeviceAddressDrive).
3885             #
3886             # Note about why it's added in disk_mapping: It's not
3887             # possible to pass an 'int' by reference in Python, so we
3888             # use disk_mapping as container to keep reference of the
3889             # unit added and be able to increment it for each disk
3890             # added.
3891             #
3892             # NOTE(jaypipes,melwitt): If this is a boot-from-volume instance,
3893             # we need to start the disk mapping unit at 1 since we set the
3894             # bootable volume's unit to 0 for the bootable volume.
3895             disk_mapping['unit'] = 0
3896             if self._is_booted_from_volume(block_device_info):
3897                 disk_mapping['unit'] = 1
3898 
3899         def _get_ephemeral_devices():
3900             eph_devices = []
3901             for idx, eph in enumerate(
3902                 driver.block_device_info_get_ephemerals(
3903                     block_device_info)):
3904                 diskeph = self._get_guest_disk_config(
3905                     instance,
3906                     blockinfo.get_eph_disk(idx),
3907                     disk_mapping, inst_type)
3908                 eph_devices.append(diskeph)
3909             return eph_devices
3910 
3911         if mount_rootfs:
3912             fs = vconfig.LibvirtConfigGuestFilesys()
3913             fs.source_type = "mount"
3914             fs.source_dir = os.path.join(
3915                 libvirt_utils.get_instance_path(instance), 'rootfs')
3916             devices.append(fs)
3917         elif (os_type == fields.VMMode.EXE and
3918               CONF.libvirt.virt_type == "parallels"):
3919             if rescue:
3920                 fsrescue = self._get_guest_fs_config(instance, "disk.rescue")
3921                 devices.append(fsrescue)
3922 
3923                 fsos = self._get_guest_fs_config(instance, "disk")
3924                 fsos.target_dir = "/mnt/rescue"
3925                 devices.append(fsos)
3926             else:
3927                 if 'disk' in disk_mapping:
3928                     fs = self._get_guest_fs_config(instance, "disk")
3929                     devices.append(fs)
3930                 devices = devices + _get_ephemeral_devices()
3931         else:
3932 
3933             if rescue:
3934                 diskrescue = self._get_guest_disk_config(instance,
3935                                                          'disk.rescue',
3936                                                          disk_mapping,
3937                                                          inst_type)
3938                 devices.append(diskrescue)
3939 
3940                 diskos = self._get_guest_disk_config(instance,
3941                                                      'disk',
3942                                                      disk_mapping,
3943                                                      inst_type)
3944                 devices.append(diskos)
3945             else:
3946                 if 'disk' in disk_mapping:
3947                     diskos = self._get_guest_disk_config(instance,
3948                                                          'disk',
3949                                                          disk_mapping,
3950                                                          inst_type)
3951                     devices.append(diskos)
3952 
3953                 if 'disk.local' in disk_mapping:
3954                     disklocal = self._get_guest_disk_config(instance,
3955                                                             'disk.local',
3956                                                             disk_mapping,
3957                                                             inst_type)
3958                     devices.append(disklocal)
3959                     instance.default_ephemeral_device = (
3960                         block_device.prepend_dev(disklocal.target_dev))
3961 
3962                 devices = devices + _get_ephemeral_devices()
3963 
3964                 if 'disk.swap' in disk_mapping:
3965                     diskswap = self._get_guest_disk_config(instance,
3966                                                            'disk.swap',
3967                                                            disk_mapping,
3968                                                            inst_type)
3969                     devices.append(diskswap)
3970                     instance.default_swap_device = (
3971                         block_device.prepend_dev(diskswap.target_dev))
3972 
3973             config_name = 'disk.config.rescue' if rescue else 'disk.config'
3974             if config_name in disk_mapping:
3975                 diskconfig = self._get_guest_disk_config(
3976                     instance, config_name, disk_mapping, inst_type,
3977                     self._get_disk_config_image_type())
3978                 devices.append(diskconfig)
3979 
3980         for vol in block_device.get_bdms_to_connect(block_device_mapping,
3981                                                    mount_rootfs):
3982             connection_info = vol['connection_info']
3983             vol_dev = block_device.prepend_dev(vol['mount_device'])
3984             info = disk_mapping[vol_dev]
3985             self._connect_volume(context, connection_info, instance)
3986             if scsi_controller and scsi_controller.model == 'virtio-scsi':
3987                 # Check if this is the bootable volume when in a
3988                 # boot-from-volume instance, and if so, ensure the unit
3989                 # attribute is 0.
3990                 if vol.get('boot_index') == 0:
3991                     info['unit'] = 0
3992                 else:
3993                     info['unit'] = disk_mapping['unit']
3994                     disk_mapping['unit'] += 1
3995             cfg = self._get_volume_config(connection_info, info)
3996             devices.append(cfg)
3997             vol['connection_info'] = connection_info
3998             vol.save()
3999 
4000         for d in devices:
4001             self._set_cache_mode(d)
4002 
4003         if scsi_controller:
4004             devices.append(scsi_controller)
4005 
4006         return devices
4007 
4008     @staticmethod
4009     def _get_scsi_controller(image_meta):
4010         """Return scsi controller or None based on image meta"""
4011         if image_meta.properties.get('hw_scsi_model'):
4012             hw_scsi_model = image_meta.properties.hw_scsi_model
4013             scsi_controller = vconfig.LibvirtConfigGuestController()
4014             scsi_controller.type = 'scsi'
4015             scsi_controller.model = hw_scsi_model
4016             scsi_controller.index = 0
4017             return scsi_controller
4018 
4019     def _get_host_sysinfo_serial_hardware(self):
4020         """Get a UUID from the host hardware
4021 
4022         Get a UUID for the host hardware reported by libvirt.
4023         This is typically from the SMBIOS data, unless it has
4024         been overridden in /etc/libvirt/libvirtd.conf
4025         """
4026         caps = self._host.get_capabilities()
4027         return caps.host.uuid
4028 
4029     def _get_host_sysinfo_serial_os(self):
4030         """Get a UUID from the host operating system
4031 
4032         Get a UUID for the host operating system. Modern Linux
4033         distros based on systemd provide a /etc/machine-id
4034         file containing a UUID. This is also provided inside
4035         systemd based containers and can be provided by other
4036         init systems too, since it is just a plain text file.
4037         """
4038         if not os.path.exists("/etc/machine-id"):
4039             msg = _("Unable to get host UUID: /etc/machine-id does not exist")
4040             raise exception.InternalError(msg)
4041 
4042         with open("/etc/machine-id") as f:
4043             # We want to have '-' in the right place
4044             # so we parse & reformat the value
4045             lines = f.read().split()
4046             if not lines:
4047                 msg = _("Unable to get host UUID: /etc/machine-id is empty")
4048                 raise exception.InternalError(msg)
4049 
4050             return str(uuid.UUID(lines[0]))
4051 
4052     def _get_host_sysinfo_serial_auto(self):
4053         if os.path.exists("/etc/machine-id"):
4054             return self._get_host_sysinfo_serial_os()
4055         else:
4056             return self._get_host_sysinfo_serial_hardware()
4057 
4058     def _get_guest_config_sysinfo(self, instance):
4059         sysinfo = vconfig.LibvirtConfigGuestSysinfo()
4060 
4061         sysinfo.system_manufacturer = version.vendor_string()
4062         sysinfo.system_product = version.product_string()
4063         sysinfo.system_version = version.version_string_with_package()
4064 
4065         sysinfo.system_serial = self._sysinfo_serial_func()
4066         sysinfo.system_uuid = instance.uuid
4067 
4068         sysinfo.system_family = "Virtual Machine"
4069 
4070         return sysinfo
4071 
4072     def _get_guest_pci_device(self, pci_device):
4073 
4074         dbsf = pci_utils.parse_address(pci_device.address)
4075         dev = vconfig.LibvirtConfigGuestHostdevPCI()
4076         dev.domain, dev.bus, dev.slot, dev.function = dbsf
4077 
4078         # only kvm support managed mode
4079         if CONF.libvirt.virt_type in ('xen', 'parallels',):
4080             dev.managed = 'no'
4081         if CONF.libvirt.virt_type in ('kvm', 'qemu'):
4082             dev.managed = 'yes'
4083 
4084         return dev
4085 
4086     def _get_guest_config_meta(self, instance):
4087         """Get metadata config for guest."""
4088 
4089         meta = vconfig.LibvirtConfigGuestMetaNovaInstance()
4090         meta.package = version.version_string_with_package()
4091         meta.name = instance.display_name
4092         meta.creationTime = time.time()
4093 
4094         if instance.image_ref not in ("", None):
4095             meta.roottype = "image"
4096             meta.rootid = instance.image_ref
4097 
4098         system_meta = instance.system_metadata
4099         ometa = vconfig.LibvirtConfigGuestMetaNovaOwner()
4100         ometa.userid = instance.user_id
4101         ometa.username = system_meta.get('owner_user_name', 'N/A')
4102         ometa.projectid = instance.project_id
4103         ometa.projectname = system_meta.get('owner_project_name', 'N/A')
4104         meta.owner = ometa
4105 
4106         fmeta = vconfig.LibvirtConfigGuestMetaNovaFlavor()
4107         flavor = instance.flavor
4108         fmeta.name = flavor.name
4109         fmeta.memory = flavor.memory_mb
4110         fmeta.vcpus = flavor.vcpus
4111         fmeta.ephemeral = flavor.ephemeral_gb
4112         fmeta.disk = flavor.root_gb
4113         fmeta.swap = flavor.swap
4114 
4115         meta.flavor = fmeta
4116 
4117         return meta
4118 
4119     def _machine_type_mappings(self):
4120         mappings = {}
4121         for mapping in CONF.libvirt.hw_machine_type:
4122             host_arch, _, machine_type = mapping.partition('=')
4123             mappings[host_arch] = machine_type
4124         return mappings
4125 
4126     def _get_machine_type(self, image_meta, caps):
4127         # The underlying machine type can be set as an image attribute,
4128         # or otherwise based on some architecture specific defaults
4129 
4130         mach_type = None
4131 
4132         if image_meta.properties.get('hw_machine_type') is not None:
4133             mach_type = image_meta.properties.hw_machine_type
4134         else:
4135             # For ARM systems we will default to vexpress-a15 for armv7
4136             # and virt for aarch64
4137             if caps.host.cpu.arch == fields.Architecture.ARMV7:
4138                 mach_type = "vexpress-a15"
4139 
4140             if caps.host.cpu.arch == fields.Architecture.AARCH64:
4141                 mach_type = "virt"
4142 
4143             if caps.host.cpu.arch in (fields.Architecture.S390,
4144                                       fields.Architecture.S390X):
4145                 mach_type = 's390-ccw-virtio'
4146 
4147             # If set in the config, use that as the default.
4148             if CONF.libvirt.hw_machine_type:
4149                 mappings = self._machine_type_mappings()
4150                 mach_type = mappings.get(caps.host.cpu.arch)
4151 
4152         return mach_type
4153 
4154     @staticmethod
4155     def _create_idmaps(klass, map_strings):
4156         idmaps = []
4157         if len(map_strings) > 5:
4158             map_strings = map_strings[0:5]
4159             LOG.warning("Too many id maps, only included first five.")
4160         for map_string in map_strings:
4161             try:
4162                 idmap = klass()
4163                 values = [int(i) for i in map_string.split(":")]
4164                 idmap.start = values[0]
4165                 idmap.target = values[1]
4166                 idmap.count = values[2]
4167                 idmaps.append(idmap)
4168             except (ValueError, IndexError):
4169                 LOG.warning("Invalid value for id mapping %s", map_string)
4170         return idmaps
4171 
4172     def _get_guest_idmaps(self):
4173         id_maps = []
4174         if CONF.libvirt.virt_type == 'lxc' and CONF.libvirt.uid_maps:
4175             uid_maps = self._create_idmaps(vconfig.LibvirtConfigGuestUIDMap,
4176                                            CONF.libvirt.uid_maps)
4177             id_maps.extend(uid_maps)
4178         if CONF.libvirt.virt_type == 'lxc' and CONF.libvirt.gid_maps:
4179             gid_maps = self._create_idmaps(vconfig.LibvirtConfigGuestGIDMap,
4180                                            CONF.libvirt.gid_maps)
4181             id_maps.extend(gid_maps)
4182         return id_maps
4183 
4184     def _update_guest_cputune(self, guest, flavor, virt_type):
4185         is_able = self._host.is_cpu_control_policy_capable()
4186 
4187         cputuning = ['shares', 'period', 'quota']
4188         wants_cputune = any([k for k in cputuning
4189             if "quota:cpu_" + k in flavor.extra_specs.keys()])
4190 
4191         if wants_cputune and not is_able:
4192             raise exception.UnsupportedHostCPUControlPolicy()
4193 
4194         if not is_able or virt_type not in ('lxc', 'kvm', 'qemu'):
4195             return
4196 
4197         if guest.cputune is None:
4198             guest.cputune = vconfig.LibvirtConfigGuestCPUTune()
4199             # Setting the default cpu.shares value to be a value
4200             # dependent on the number of vcpus
4201         guest.cputune.shares = 1024 * guest.vcpus
4202 
4203         for name in cputuning:
4204             key = "quota:cpu_" + name
4205             if key in flavor.extra_specs:
4206                 setattr(guest.cputune, name,
4207                         int(flavor.extra_specs[key]))
4208 
4209     def _get_cpu_numa_config_from_instance(self, instance_numa_topology,
4210                                            wants_hugepages):
4211         if instance_numa_topology:
4212             guest_cpu_numa = vconfig.LibvirtConfigGuestCPUNUMA()
4213             for instance_cell in instance_numa_topology.cells:
4214                 guest_cell = vconfig.LibvirtConfigGuestCPUNUMACell()
4215                 guest_cell.id = instance_cell.id
4216                 guest_cell.cpus = instance_cell.cpuset
4217                 guest_cell.memory = instance_cell.memory * units.Ki
4218 
4219                 # The vhost-user network backend requires file backed
4220                 # guest memory (ie huge pages) to be marked as shared
4221                 # access, not private, so an external process can read
4222                 # and write the pages.
4223                 #
4224                 # You can't change the shared vs private flag for an
4225                 # already running guest, and since we can't predict what
4226                 # types of NIC may be hotplugged, we have no choice but
4227                 # to unconditionally turn on the shared flag. This has
4228                 # no real negative functional effect on the guest, so
4229                 # is a reasonable approach to take
4230                 if wants_hugepages:
4231                     guest_cell.memAccess = "shared"
4232                 guest_cpu_numa.cells.append(guest_cell)
4233             return guest_cpu_numa
4234 
4235     def _wants_hugepages(self, host_topology, instance_topology):
4236         """Determine if the guest / host topology implies the
4237            use of huge pages for guest RAM backing
4238         """
4239 
4240         if host_topology is None or instance_topology is None:
4241             return False
4242 
4243         avail_pagesize = [page.size_kb
4244                           for page in host_topology.cells[0].mempages]
4245         avail_pagesize.sort()
4246         # Remove smallest page size as that's not classed as a largepage
4247         avail_pagesize = avail_pagesize[1:]
4248 
4249         # See if we have page size set
4250         for cell in instance_topology.cells:
4251             if (cell.pagesize is not None and
4252                 cell.pagesize in avail_pagesize):
4253                 return True
4254 
4255         return False
4256 
4257     def _get_cell_pairs(self, guest_cpu_numa_config, host_topology):
4258         """Returns the lists of pairs(tuple) of an instance cell and
4259         corresponding host cell:
4260             [(LibvirtConfigGuestCPUNUMACell, NUMACell), ...]
4261         """
4262         cell_pairs = []
4263         for guest_config_cell in guest_cpu_numa_config.cells:
4264             for host_cell in host_topology.cells:
4265                 if guest_config_cell.id == host_cell.id:
4266                     cell_pairs.append((guest_config_cell, host_cell))
4267         return cell_pairs
4268 
4269     def _get_pin_cpuset(self, vcpu, object_numa_cell, host_cell):
4270         """Returns the config object of LibvirtConfigGuestCPUTuneVCPUPin.
4271         Prepares vcpupin config for the guest with the following caveats:
4272 
4273             a) If there is pinning information in the cell, we pin vcpus to
4274                individual CPUs
4275             b) Otherwise we float over the whole host NUMA node
4276         """
4277         pin_cpuset = vconfig.LibvirtConfigGuestCPUTuneVCPUPin()
4278         pin_cpuset.id = vcpu
4279 
4280         if object_numa_cell.cpu_pinning:
4281             pin_cpuset.cpuset = set([object_numa_cell.cpu_pinning[vcpu]])
4282         else:
4283             pin_cpuset.cpuset = host_cell.cpuset
4284 
4285         return pin_cpuset
4286 
4287     def _get_emulatorpin_cpuset(self, vcpu, object_numa_cell, vcpus_rt,
4288                                 emulator_threads_policy, wants_realtime,
4289                                 pin_cpuset):
4290         """Returns a set of cpu_ids to add to the cpuset for emulator threads
4291            with the following caveats:
4292 
4293             a) If emulator threads policy is isolated, we pin emulator threads
4294                to one cpu we have reserved for it.
4295             b) If emulator threads policy is shared and CONF.cpu_shared_set is
4296                defined, we pin emulator threads on the set of pCPUs defined by
4297                CONF.cpu_shared_set
4298             c) Otherwise;
4299                 c1) If realtime IS NOT enabled, the emulator threads are
4300                     allowed to float cross all the pCPUs associated with
4301                     the guest vCPUs.
4302                 c2) If realtime IS enabled, at least 1 vCPU is required
4303                     to be set aside for non-realtime usage. The emulator
4304                     threads are allowed to float across the pCPUs that
4305                     are associated with the non-realtime VCPUs.
4306         """
4307         emulatorpin_cpuset = set([])
4308         shared_ids = hardware.get_cpu_shared_set()
4309 
4310         if emulator_threads_policy == fields.CPUEmulatorThreadsPolicy.ISOLATE:
4311             if object_numa_cell.cpuset_reserved:
4312                 emulatorpin_cpuset = object_numa_cell.cpuset_reserved
4313         elif ((emulator_threads_policy ==
4314               fields.CPUEmulatorThreadsPolicy.SHARE) and
4315               shared_ids):
4316             online_pcpus = self._host.get_online_cpus()
4317             cpuset = shared_ids & online_pcpus
4318             if not cpuset:
4319                 msg = (_("Invalid cpu_shared_set config, one or more of the "
4320                          "specified cpuset is not online. Online cpuset(s): "
4321                          "%(online)s, requested cpuset(s): %(req)s"),
4322                        {'online': sorted(online_pcpus),
4323                         'req': sorted(shared_ids)})
4324                 raise exception.Invalid(msg)
4325             emulatorpin_cpuset = cpuset
4326         elif not wants_realtime or vcpu not in vcpus_rt:
4327             emulatorpin_cpuset = pin_cpuset.cpuset
4328 
4329         return emulatorpin_cpuset
4330 
4331     def _get_guest_numa_config(self, instance_numa_topology, flavor,
4332                                allowed_cpus=None, image_meta=None):
4333         """Returns the config objects for the guest NUMA specs.
4334 
4335         Determines the CPUs that the guest can be pinned to if the guest
4336         specifies a cell topology and the host supports it. Constructs the
4337         libvirt XML config object representing the NUMA topology selected
4338         for the guest. Returns a tuple of:
4339 
4340             (cpu_set, guest_cpu_tune, guest_cpu_numa, guest_numa_tune)
4341 
4342         With the following caveats:
4343 
4344             a) If there is no specified guest NUMA topology, then
4345                all tuple elements except cpu_set shall be None. cpu_set
4346                will be populated with the chosen CPUs that the guest
4347                allowed CPUs fit within, which could be the supplied
4348                allowed_cpus value if the host doesn't support NUMA
4349                topologies.
4350 
4351             b) If there is a specified guest NUMA topology, then
4352                cpu_set will be None and guest_cpu_numa will be the
4353                LibvirtConfigGuestCPUNUMA object representing the guest's
4354                NUMA topology. If the host supports NUMA, then guest_cpu_tune
4355                will contain a LibvirtConfigGuestCPUTune object representing
4356                the optimized chosen cells that match the host capabilities
4357                with the instance's requested topology. If the host does
4358                not support NUMA, then guest_cpu_tune and guest_numa_tune
4359                will be None.
4360         """
4361 
4362         if (not self._has_numa_support() and
4363                 instance_numa_topology is not None):
4364             # We should not get here, since we should have avoided
4365             # reporting NUMA topology from _get_host_numa_topology
4366             # in the first place. Just in case of a scheduler
4367             # mess up though, raise an exception
4368             raise exception.NUMATopologyUnsupported()
4369 
4370         topology = self._get_host_numa_topology()
4371 
4372         # We have instance NUMA so translate it to the config class
4373         guest_cpu_numa_config = self._get_cpu_numa_config_from_instance(
4374                 instance_numa_topology,
4375                 self._wants_hugepages(topology, instance_numa_topology))
4376 
4377         if not guest_cpu_numa_config:
4378             # No NUMA topology defined for instance - let the host kernel deal
4379             # with the NUMA effects.
4380             # TODO(ndipanov): Attempt to spread the instance
4381             # across NUMA nodes and expose the topology to the
4382             # instance as an optimisation
4383             return GuestNumaConfig(allowed_cpus, None, None, None)
4384 
4385         if not topology:
4386             # No NUMA topology defined for host - This will only happen with
4387             # some libvirt versions and certain platforms.
4388             return GuestNumaConfig(allowed_cpus, None,
4389                                    guest_cpu_numa_config, None)
4390 
4391         # Now get configuration from the numa_topology
4392         # Init CPUTune configuration
4393         guest_cpu_tune = vconfig.LibvirtConfigGuestCPUTune()
4394         guest_cpu_tune.emulatorpin = (
4395             vconfig.LibvirtConfigGuestCPUTuneEmulatorPin())
4396         guest_cpu_tune.emulatorpin.cpuset = set([])
4397 
4398         # Init NUMATune configuration
4399         guest_numa_tune = vconfig.LibvirtConfigGuestNUMATune()
4400         guest_numa_tune.memory = vconfig.LibvirtConfigGuestNUMATuneMemory()
4401         guest_numa_tune.memnodes = []
4402 
4403         emulator_threads_policy = None
4404         if 'emulator_threads_policy' in instance_numa_topology:
4405             emulator_threads_policy = (
4406                 instance_numa_topology.emulator_threads_policy)
4407 
4408         # Set realtime scheduler for CPUTune
4409         vcpus_rt = set([])
4410         wants_realtime = hardware.is_realtime_enabled(flavor)
4411         if wants_realtime:
4412             vcpus_rt = hardware.vcpus_realtime_topology(flavor, image_meta)
4413             vcpusched = vconfig.LibvirtConfigGuestCPUTuneVCPUSched()
4414             designer.set_vcpu_realtime_scheduler(
4415                 vcpusched, vcpus_rt, CONF.libvirt.realtime_scheduler_priority)
4416             guest_cpu_tune.vcpusched.append(vcpusched)
4417 
4418         cell_pairs = self._get_cell_pairs(guest_cpu_numa_config, topology)
4419         for guest_node_id, (guest_config_cell, host_cell) in enumerate(
4420                 cell_pairs):
4421             # set NUMATune for the cell
4422             tnode = vconfig.LibvirtConfigGuestNUMATuneMemNode()
4423             designer.set_numa_memnode(tnode, guest_node_id, host_cell.id)
4424             guest_numa_tune.memnodes.append(tnode)
4425             guest_numa_tune.memory.nodeset.append(host_cell.id)
4426 
4427             # set CPUTune for the cell
4428             object_numa_cell = instance_numa_topology.cells[guest_node_id]
4429             for cpu in guest_config_cell.cpus:
4430                 pin_cpuset = self._get_pin_cpuset(cpu, object_numa_cell,
4431                                                   host_cell)
4432                 guest_cpu_tune.vcpupin.append(pin_cpuset)
4433 
4434                 emu_pin_cpuset = self._get_emulatorpin_cpuset(
4435                     cpu, object_numa_cell, vcpus_rt,
4436                     emulator_threads_policy, wants_realtime, pin_cpuset)
4437                 guest_cpu_tune.emulatorpin.cpuset.update(emu_pin_cpuset)
4438 
4439         # TODO(berrange) When the guest has >1 NUMA node, it will
4440         # span multiple host NUMA nodes. By pinning emulator threads
4441         # to the union of all nodes, we guarantee there will be
4442         # cross-node memory access by the emulator threads when
4443         # responding to guest I/O operations. The only way to avoid
4444         # this would be to pin emulator threads to a single node and
4445         # tell the guest OS to only do I/O from one of its virtual
4446         # NUMA nodes. This is not even remotely practical.
4447         #
4448         # The long term solution is to make use of a new QEMU feature
4449         # called "I/O Threads" which will let us configure an explicit
4450         # I/O thread for each guest vCPU or guest NUMA node. It is
4451         # still TBD how to make use of this feature though, especially
4452         # how to associate IO threads with guest devices to eliminate
4453         # cross NUMA node traffic. This is an area of investigation
4454         # for QEMU community devs.
4455 
4456         # Sort the vcpupin list per vCPU id for human-friendlier XML
4457         guest_cpu_tune.vcpupin.sort(key=operator.attrgetter("id"))
4458 
4459         # normalize cell.id
4460         for i, (cell, memnode) in enumerate(zip(guest_cpu_numa_config.cells,
4461                                                 guest_numa_tune.memnodes)):
4462             cell.id = i
4463             memnode.cellid = i
4464 
4465         return GuestNumaConfig(None, guest_cpu_tune, guest_cpu_numa_config,
4466                                guest_numa_tune)
4467 
4468     def _get_guest_os_type(self, virt_type):
4469         """Returns the guest OS type based on virt type."""
4470         if virt_type == "lxc":
4471             ret = fields.VMMode.EXE
4472         elif virt_type == "uml":
4473             ret = fields.VMMode.UML
4474         elif virt_type == "xen":
4475             ret = fields.VMMode.XEN
4476         else:
4477             ret = fields.VMMode.HVM
4478         return ret
4479 
4480     def _set_guest_for_rescue(self, rescue, guest, inst_path, virt_type,
4481                               root_device_name):
4482         if rescue.get('kernel_id'):
4483             guest.os_kernel = os.path.join(inst_path, "kernel.rescue")
4484             guest.os_cmdline = ("root=%s %s" % (root_device_name, CONSOLE))
4485             if virt_type == "qemu":
4486                 guest.os_cmdline += " no_timer_check"
4487         if rescue.get('ramdisk_id'):
4488             guest.os_initrd = os.path.join(inst_path, "ramdisk.rescue")
4489 
4490     def _set_guest_for_inst_kernel(self, instance, guest, inst_path, virt_type,
4491                                 root_device_name, image_meta):
4492         guest.os_kernel = os.path.join(inst_path, "kernel")
4493         guest.os_cmdline = ("root=%s %s" % (root_device_name, CONSOLE))
4494         if virt_type == "qemu":
4495             guest.os_cmdline += " no_timer_check"
4496         if instance.ramdisk_id:
4497             guest.os_initrd = os.path.join(inst_path, "ramdisk")
4498         # we only support os_command_line with images with an explicit
4499         # kernel set and don't want to break nova if there's an
4500         # os_command_line property without a specified kernel_id param
4501         if image_meta.properties.get("os_command_line"):
4502             guest.os_cmdline = image_meta.properties.os_command_line
4503 
4504     def _set_clock(self, guest, os_type, image_meta, virt_type):
4505         # NOTE(mikal): Microsoft Windows expects the clock to be in
4506         # "localtime". If the clock is set to UTC, then you can use a
4507         # registry key to let windows know, but Microsoft says this is
4508         # buggy in http://support.microsoft.com/kb/2687252
4509         clk = vconfig.LibvirtConfigGuestClock()
4510         if os_type == 'windows':
4511             LOG.info('Configuring timezone for windows instance to localtime')
4512             clk.offset = 'localtime'
4513         else:
4514             clk.offset = 'utc'
4515         guest.set_clock(clk)
4516 
4517         if virt_type == "kvm":
4518             self._set_kvm_timers(clk, os_type, image_meta)
4519 
4520     def _set_kvm_timers(self, clk, os_type, image_meta):
4521         # TODO(berrange) One day this should be per-guest
4522         # OS type configurable
4523         tmpit = vconfig.LibvirtConfigGuestTimer()
4524         tmpit.name = "pit"
4525         tmpit.tickpolicy = "delay"
4526 
4527         tmrtc = vconfig.LibvirtConfigGuestTimer()
4528         tmrtc.name = "rtc"
4529         tmrtc.tickpolicy = "catchup"
4530 
4531         clk.add_timer(tmpit)
4532         clk.add_timer(tmrtc)
4533 
4534         guestarch = libvirt_utils.get_arch(image_meta)
4535         if guestarch in (fields.Architecture.I686,
4536                          fields.Architecture.X86_64):
4537             # NOTE(rfolco): HPET is a hardware timer for x86 arch.
4538             # qemu -no-hpet is not supported on non-x86 targets.
4539             tmhpet = vconfig.LibvirtConfigGuestTimer()
4540             tmhpet.name = "hpet"
4541             tmhpet.present = False
4542             clk.add_timer(tmhpet)
4543 
4544         # Provide Windows guests with the paravirtualized hyperv timer source.
4545         # This is the windows equiv of kvm-clock, allowing Windows
4546         # guests to accurately keep time.
4547         if os_type == 'windows':
4548             tmhyperv = vconfig.LibvirtConfigGuestTimer()
4549             tmhyperv.name = "hypervclock"
4550             tmhyperv.present = True
4551             clk.add_timer(tmhyperv)
4552 
4553     def _set_features(self, guest, os_type, caps, virt_type, image_meta,
4554             flavor):
4555         if virt_type == "xen":
4556             # PAE only makes sense in X86
4557             if caps.host.cpu.arch in (fields.Architecture.I686,
4558                                       fields.Architecture.X86_64):
4559                 guest.features.append(vconfig.LibvirtConfigGuestFeaturePAE())
4560 
4561         if (virt_type not in ("lxc", "uml", "parallels", "xen") or
4562                 (virt_type == "xen" and guest.os_type == fields.VMMode.HVM)):
4563             guest.features.append(vconfig.LibvirtConfigGuestFeatureACPI())
4564             guest.features.append(vconfig.LibvirtConfigGuestFeatureAPIC())
4565 
4566         if (virt_type in ("qemu", "kvm") and
4567                 os_type == 'windows'):
4568             hv = vconfig.LibvirtConfigGuestFeatureHyperV()
4569             hv.relaxed = True
4570 
4571             hv.spinlocks = True
4572             # Increase spinlock retries - value recommended by
4573             # KVM maintainers who certify Windows guests
4574             # with Microsoft
4575             hv.spinlock_retries = 8191
4576             hv.vapic = True
4577             guest.features.append(hv)
4578 
4579         flavor_hide_kvm = strutils.bool_from_string(
4580                 flavor.get('extra_specs', {}).get('hide_hypervisor_id'))
4581         if (virt_type in ("qemu", "kvm") and
4582                 (image_meta.properties.get('img_hide_hypervisor_id') or
4583                  flavor_hide_kvm)):
4584             guest.features.append(vconfig.LibvirtConfigGuestFeatureKvmHidden())
4585 
4586     def _check_number_of_serial_console(self, num_ports):
4587         virt_type = CONF.libvirt.virt_type
4588         if (virt_type in ("kvm", "qemu") and
4589             num_ports > ALLOWED_QEMU_SERIAL_PORTS):
4590             raise exception.SerialPortNumberLimitExceeded(
4591                 allowed=ALLOWED_QEMU_SERIAL_PORTS, virt_type=virt_type)
4592 
4593     def _add_video_driver(self, guest, image_meta, flavor):
4594         VALID_VIDEO_DEVICES = ("vga", "cirrus", "vmvga",
4595                                "xen", "qxl", "virtio")
4596         video = vconfig.LibvirtConfigGuestVideo()
4597         # NOTE(ldbragst): The following logic sets the video.type
4598         # depending on supported defaults given the architecture,
4599         # virtualization type, and features. The video.type attribute can
4600         # be overridden by the user with image_meta.properties, which
4601         # is carried out in the next if statement below this one.
4602         guestarch = libvirt_utils.get_arch(image_meta)
4603         if guest.os_type == fields.VMMode.XEN:
4604             video.type = 'xen'
4605         elif CONF.libvirt.virt_type == 'parallels':
4606             video.type = 'vga'
4607         elif guestarch in (fields.Architecture.PPC,
4608                            fields.Architecture.PPC64,
4609                            fields.Architecture.PPC64LE):
4610             # NOTE(ldbragst): PowerKVM doesn't support 'cirrus' be default
4611             # so use 'vga' instead when running on Power hardware.
4612             video.type = 'vga'
4613         elif guestarch in (fields.Architecture.AARCH64):
4614             # NOTE(kevinz): Only virtio device type is supported by AARCH64
4615             # so use 'virtio' instead when running on AArch64 hardware.
4616             video.type = 'virtio'
4617         elif CONF.spice.enabled:
4618             video.type = 'qxl'
4619         if image_meta.properties.get('hw_video_model'):
4620             video.type = image_meta.properties.hw_video_model
4621             if (video.type not in VALID_VIDEO_DEVICES):
4622                 raise exception.InvalidVideoMode(model=video.type)
4623 
4624         # Set video memory, only if the flavor's limit is set
4625         video_ram = image_meta.properties.get('hw_video_ram', 0)
4626         max_vram = int(flavor.extra_specs.get('hw_video:ram_max_mb', 0))
4627         if video_ram > max_vram:
4628             raise exception.RequestedVRamTooHigh(req_vram=video_ram,
4629                                                  max_vram=max_vram)
4630         if max_vram and video_ram:
4631             video.vram = video_ram * units.Mi / units.Ki
4632         guest.add_device(video)
4633 
4634     def _add_qga_device(self, guest, instance):
4635         qga = vconfig.LibvirtConfigGuestChannel()
4636         qga.type = "unix"
4637         qga.target_name = "org.qemu.guest_agent.0"
4638         qga.source_path = ("/var/lib/libvirt/qemu/%s.%s.sock" %
4639                           ("org.qemu.guest_agent.0", instance.name))
4640         guest.add_device(qga)
4641 
4642     def _add_rng_device(self, guest, flavor):
4643         rng_device = vconfig.LibvirtConfigGuestRng()
4644         rate_bytes = flavor.extra_specs.get('hw_rng:rate_bytes', 0)
4645         period = flavor.extra_specs.get('hw_rng:rate_period', 0)
4646         if rate_bytes:
4647             rng_device.rate_bytes = int(rate_bytes)
4648             rng_device.rate_period = int(period)
4649         rng_path = CONF.libvirt.rng_dev_path
4650         if (rng_path and not os.path.exists(rng_path)):
4651             raise exception.RngDeviceNotExist(path=rng_path)
4652         rng_device.backend = rng_path
4653         guest.add_device(rng_device)
4654 
4655     def _set_qemu_guest_agent(self, guest, flavor, instance, image_meta):
4656         # Enable qga only if the 'hw_qemu_guest_agent' is equal to yes
4657         if image_meta.properties.get('hw_qemu_guest_agent', False):
4658             LOG.debug("Qemu guest agent is enabled through image "
4659                       "metadata", instance=instance)
4660             self._add_qga_device(guest, instance)
4661         rng_is_virtio = image_meta.properties.get('hw_rng_model') == 'virtio'
4662         rng_allowed_str = flavor.extra_specs.get('hw_rng:allowed', '')
4663         rng_allowed = strutils.bool_from_string(rng_allowed_str)
4664         if rng_is_virtio and rng_allowed:
4665             self._add_rng_device(guest, flavor)
4666 
4667     def _get_guest_memory_backing_config(
4668             self, inst_topology, numatune, flavor):
4669         wantsmempages = False
4670         if inst_topology:
4671             for cell in inst_topology.cells:
4672                 if cell.pagesize:
4673                     wantsmempages = True
4674                     break
4675 
4676         wantsrealtime = hardware.is_realtime_enabled(flavor)
4677 
4678         membacking = None
4679         if wantsmempages:
4680             pages = self._get_memory_backing_hugepages_support(
4681                 inst_topology, numatune)
4682             if pages:
4683                 membacking = vconfig.LibvirtConfigGuestMemoryBacking()
4684                 membacking.hugepages = pages
4685         if wantsrealtime:
4686             if not membacking:
4687                 membacking = vconfig.LibvirtConfigGuestMemoryBacking()
4688             membacking.locked = True
4689             membacking.sharedpages = False
4690 
4691         return membacking
4692 
4693     def _get_memory_backing_hugepages_support(self, inst_topology, numatune):
4694         if not self._has_numa_support():
4695             # We should not get here, since we should have avoided
4696             # reporting NUMA topology from _get_host_numa_topology
4697             # in the first place. Just in case of a scheduler
4698             # mess up though, raise an exception
4699             raise exception.MemoryPagesUnsupported()
4700 
4701         host_topology = self._get_host_numa_topology()
4702 
4703         if host_topology is None:
4704             # As above, we should not get here but just in case...
4705             raise exception.MemoryPagesUnsupported()
4706 
4707         # Currently libvirt does not support the smallest
4708         # pagesize set as a backend memory.
4709         # https://bugzilla.redhat.com/show_bug.cgi?id=1173507
4710         avail_pagesize = [page.size_kb
4711                           for page in host_topology.cells[0].mempages]
4712         avail_pagesize.sort()
4713         smallest = avail_pagesize[0]
4714 
4715         pages = []
4716         for guest_cellid, inst_cell in enumerate(inst_topology.cells):
4717             if inst_cell.pagesize and inst_cell.pagesize > smallest:
4718                 for memnode in numatune.memnodes:
4719                     if guest_cellid == memnode.cellid:
4720                         page = (
4721                             vconfig.LibvirtConfigGuestMemoryBackingPage())
4722                         page.nodeset = [guest_cellid]
4723                         page.size_kb = inst_cell.pagesize
4724                         pages.append(page)
4725                         break  # Quit early...
4726         return pages
4727 
4728     def _get_flavor(self, ctxt, instance, flavor):
4729         if flavor is not None:
4730             return flavor
4731         return instance.flavor
4732 
4733     def _has_uefi_support(self):
4734         # This means that the host can support uefi booting for guests
4735         supported_archs = [fields.Architecture.X86_64,
4736                            fields.Architecture.AARCH64]
4737         caps = self._host.get_capabilities()
4738         return ((caps.host.cpu.arch in supported_archs) and
4739                 os.path.exists(DEFAULT_UEFI_LOADER_PATH[caps.host.cpu.arch]))
4740 
4741     def _get_supported_perf_events(self):
4742 
4743         if (len(CONF.libvirt.enabled_perf_events) == 0 or
4744              not self._host.has_min_version(MIN_LIBVIRT_PERF_VERSION)):
4745             return []
4746 
4747         supported_events = []
4748         host_cpu_info = self._get_cpu_info()
4749         for event in CONF.libvirt.enabled_perf_events:
4750             if self._supported_perf_event(event, host_cpu_info['features']):
4751                 supported_events.append(event)
4752         return supported_events
4753 
4754     def _supported_perf_event(self, event, cpu_features):
4755 
4756         libvirt_perf_event_name = LIBVIRT_PERF_EVENT_PREFIX + event.upper()
4757 
4758         if not hasattr(libvirt, libvirt_perf_event_name):
4759             LOG.warning("Libvirt doesn't support event type %s.", event)
4760             return False
4761 
4762         if event in PERF_EVENTS_CPU_FLAG_MAPPING:
4763             LOG.warning('Monitoring Intel CMT `perf` event(s) %s is '
4764                         'deprecated and will be removed in the "Stein" '
4765                         'release.  It was broken by design in the '
4766                         'Linux kernel, so support for Intel CMT was '
4767                         'removed from Linux 4.14 onwards. Therefore '
4768                         'it is recommended to not enable them.',
4769                         event)
4770             if PERF_EVENTS_CPU_FLAG_MAPPING[event] not in cpu_features:
4771                 LOG.warning("Host does not support event type %s.", event)
4772                 return False
4773         return True
4774 
4775     def _configure_guest_by_virt_type(self, guest, virt_type, caps, instance,
4776                                       image_meta, flavor, root_device_name):
4777         if virt_type == "xen":
4778             if guest.os_type == fields.VMMode.HVM:
4779                 guest.os_loader = CONF.libvirt.xen_hvmloader_path
4780             else:
4781                 guest.os_cmdline = CONSOLE
4782         elif virt_type in ("kvm", "qemu"):
4783             if caps.host.cpu.arch in (fields.Architecture.I686,
4784                                       fields.Architecture.X86_64):
4785                 guest.sysinfo = self._get_guest_config_sysinfo(instance)
4786                 guest.os_smbios = vconfig.LibvirtConfigGuestSMBIOS()
4787             hw_firmware_type = image_meta.properties.get('hw_firmware_type')
4788             if caps.host.cpu.arch == fields.Architecture.AARCH64:
4789                 if not hw_firmware_type:
4790                     hw_firmware_type = fields.FirmwareType.UEFI
4791             if hw_firmware_type == fields.FirmwareType.UEFI:
4792                 if self._has_uefi_support():
4793                     global uefi_logged
4794                     if not uefi_logged:
4795                         LOG.warning("uefi support is without some kind of "
4796                                     "functional testing and therefore "
4797                                     "considered experimental.")
4798                         uefi_logged = True
4799                     guest.os_loader = DEFAULT_UEFI_LOADER_PATH[
4800                         caps.host.cpu.arch]
4801                     guest.os_loader_type = "pflash"
4802                 else:
4803                     raise exception.UEFINotSupported()
4804             guest.os_mach_type = self._get_machine_type(image_meta, caps)
4805             if image_meta.properties.get('hw_boot_menu') is None:
4806                 guest.os_bootmenu = strutils.bool_from_string(
4807                     flavor.extra_specs.get('hw:boot_menu', 'no'))
4808             else:
4809                 guest.os_bootmenu = image_meta.properties.hw_boot_menu
4810 
4811         elif virt_type == "lxc":
4812             guest.os_init_path = "/sbin/init"
4813             guest.os_cmdline = CONSOLE
4814         elif virt_type == "uml":
4815             guest.os_kernel = "/usr/bin/linux"
4816             guest.os_root = root_device_name
4817         elif virt_type == "parallels":
4818             if guest.os_type == fields.VMMode.EXE:
4819                 guest.os_init_path = "/sbin/init"
4820 
4821     def _conf_non_lxc_uml(self, virt_type, guest, root_device_name, rescue,
4822                     instance, inst_path, image_meta, disk_info):
4823         if rescue:
4824             self._set_guest_for_rescue(rescue, guest, inst_path, virt_type,
4825                                        root_device_name)
4826         elif instance.kernel_id:
4827             self._set_guest_for_inst_kernel(instance, guest, inst_path,
4828                                             virt_type, root_device_name,
4829                                             image_meta)
4830         else:
4831             guest.os_boot_dev = blockinfo.get_boot_order(disk_info)
4832 
4833     def _create_consoles(self, virt_type, guest_cfg, instance, flavor,
4834                          image_meta):
4835         # NOTE(markus_z): Beware! Below are so many conditionals that it is
4836         # easy to lose track. Use this chart to figure out your case:
4837         #
4838         # case | is serial | has       | is qemu | resulting
4839         #      | enabled?  | virtlogd? | or kvm? | devices
4840         # --------------------------------------------------
4841         #    1 |        no |        no |     no  | pty*
4842         #    2 |        no |        no |     yes | file + pty
4843         #    3 |        no |       yes |      no | see case 1
4844         #    4 |        no |       yes |     yes | pty with logd
4845         #    5 |       yes |        no |      no | see case 1
4846         #    6 |       yes |        no |     yes | tcp + pty
4847         #    7 |       yes |       yes |      no | see case 1
4848         #    8 |       yes |       yes |     yes | tcp with logd
4849         #    * exception: virt_type "parallels" doesn't create a device
4850         if virt_type == 'parallels':
4851             pass
4852         elif virt_type not in ("qemu", "kvm"):
4853             log_path = self._get_console_log_path(instance)
4854             self._create_pty_device(guest_cfg,
4855                                     vconfig.LibvirtConfigGuestConsole,
4856                                     log_path=log_path)
4857         elif (virt_type in ("qemu", "kvm") and
4858                   self._is_s390x_guest(image_meta)):
4859             self._create_consoles_s390x(guest_cfg, instance,
4860                                         flavor, image_meta)
4861         elif virt_type in ("qemu", "kvm"):
4862             self._create_consoles_qemu_kvm(guest_cfg, instance,
4863                                         flavor, image_meta)
4864 
4865     def _is_s390x_guest(self, image_meta):
4866         s390x_archs = (fields.Architecture.S390, fields.Architecture.S390X)
4867         return libvirt_utils.get_arch(image_meta) in s390x_archs
4868 
4869     def _create_consoles_qemu_kvm(self, guest_cfg, instance, flavor,
4870                                   image_meta):
4871         char_dev_cls = vconfig.LibvirtConfigGuestSerial
4872         log_path = self._get_console_log_path(instance)
4873         if CONF.serial_console.enabled:
4874             if not self._serial_ports_already_defined(instance):
4875                 num_ports = hardware.get_number_of_serial_ports(flavor,
4876                                                                 image_meta)
4877                 self._check_number_of_serial_console(num_ports)
4878                 self._create_serial_consoles(guest_cfg, num_ports,
4879                                              char_dev_cls, log_path)
4880         else:
4881             self._create_file_device(guest_cfg, instance, char_dev_cls)
4882         self._create_pty_device(guest_cfg, char_dev_cls, log_path=log_path)
4883 
4884     def _create_consoles_s390x(self, guest_cfg, instance, flavor, image_meta):
4885         char_dev_cls = vconfig.LibvirtConfigGuestConsole
4886         log_path = self._get_console_log_path(instance)
4887         if CONF.serial_console.enabled:
4888             if not self._serial_ports_already_defined(instance):
4889                 num_ports = hardware.get_number_of_serial_ports(flavor,
4890                                                                 image_meta)
4891                 self._create_serial_consoles(guest_cfg, num_ports,
4892                                              char_dev_cls, log_path)
4893         else:
4894             self._create_file_device(guest_cfg, instance, char_dev_cls,
4895                                      "sclplm")
4896         self._create_pty_device(guest_cfg, char_dev_cls, "sclp", log_path)
4897 
4898     def _create_pty_device(self, guest_cfg, char_dev_cls, target_type=None,
4899                            log_path=None):
4900         def _create_base_dev():
4901             consolepty = char_dev_cls()
4902             consolepty.target_type = target_type
4903             consolepty.type = "pty"
4904             return consolepty
4905 
4906         def _create_logd_dev():
4907             consolepty = _create_base_dev()
4908             log = vconfig.LibvirtConfigGuestCharDeviceLog()
4909             log.file = log_path
4910             consolepty.log = log
4911             return consolepty
4912 
4913         if CONF.serial_console.enabled:
4914             if self._is_virtlogd_available():
4915                 return
4916             else:
4917                 # NOTE(markus_z): You may wonder why this is necessary and
4918                 # so do I. I'm certain that this is *not* needed in any
4919                 # real use case. It is, however, useful if you want to
4920                 # pypass the Nova API and use "virsh console <guest>" on
4921                 # an hypervisor, as this CLI command doesn't work with TCP
4922                 # devices (like the serial console is).
4923                 #     https://bugzilla.redhat.com/show_bug.cgi?id=781467
4924                 # Pypassing the Nova API however is a thing we don't want.
4925                 # Future changes should remove this and fix the unit tests
4926                 # which ask for the existence.
4927                 guest_cfg.add_device(_create_base_dev())
4928         else:
4929             if self._is_virtlogd_available():
4930                 guest_cfg.add_device(_create_logd_dev())
4931             else:
4932                 guest_cfg.add_device(_create_base_dev())
4933 
4934     def _create_file_device(self, guest_cfg, instance, char_dev_cls,
4935                             target_type=None):
4936         if self._is_virtlogd_available():
4937             return
4938 
4939         consolelog = char_dev_cls()
4940         consolelog.target_type = target_type
4941         consolelog.type = "file"
4942         consolelog.source_path = self._get_console_log_path(instance)
4943         guest_cfg.add_device(consolelog)
4944 
4945     def _serial_ports_already_defined(self, instance):
4946         try:
4947             guest = self._host.get_guest(instance)
4948             if list(self._get_serial_ports_from_guest(guest)):
4949                 # Serial port are already configured for instance that
4950                 # means we are in a context of migration.
4951                 return True
4952         except exception.InstanceNotFound:
4953             LOG.debug(
4954                 "Instance does not exist yet on libvirt, we can "
4955                 "safely pass on looking for already defined serial "
4956                 "ports in its domain XML", instance=instance)
4957         return False
4958 
4959     def _create_serial_consoles(self, guest_cfg, num_ports, char_dev_cls,
4960                                 log_path):
4961         for port in six.moves.range(num_ports):
4962             console = char_dev_cls()
4963             console.port = port
4964             console.type = "tcp"
4965             console.listen_host = CONF.serial_console.proxyclient_address
4966             listen_port = serial_console.acquire_port(console.listen_host)
4967             console.listen_port = listen_port
4968             # NOTE: only the first serial console gets the boot messages,
4969             # that's why we attach the logd subdevice only to that.
4970             if port == 0 and self._is_virtlogd_available():
4971                 log = vconfig.LibvirtConfigGuestCharDeviceLog()
4972                 log.file = log_path
4973                 console.log = log
4974             guest_cfg.add_device(console)
4975 
4976     def _cpu_config_to_vcpu_model(self, cpu_config, vcpu_model):
4977         """Update VirtCPUModel object according to libvirt CPU config.
4978 
4979         :param:cpu_config: vconfig.LibvirtConfigGuestCPU presenting the
4980                            instance's virtual cpu configuration.
4981         :param:vcpu_model: VirtCPUModel object. A new object will be created
4982                            if None.
4983 
4984         :return: Updated VirtCPUModel object, or None if cpu_config is None
4985 
4986         """
4987 
4988         if not cpu_config:
4989             return
4990         if not vcpu_model:
4991             vcpu_model = objects.VirtCPUModel()
4992 
4993         vcpu_model.arch = cpu_config.arch
4994         vcpu_model.vendor = cpu_config.vendor
4995         vcpu_model.model = cpu_config.model
4996         vcpu_model.mode = cpu_config.mode
4997         vcpu_model.match = cpu_config.match
4998 
4999         if cpu_config.sockets:
5000             vcpu_model.topology = objects.VirtCPUTopology(
5001                 sockets=cpu_config.sockets,
5002                 cores=cpu_config.cores,
5003                 threads=cpu_config.threads)
5004         else:
5005             vcpu_model.topology = None
5006 
5007         features = [objects.VirtCPUFeature(
5008             name=f.name,
5009             policy=f.policy) for f in cpu_config.features]
5010         vcpu_model.features = features
5011 
5012         return vcpu_model
5013 
5014     def _vcpu_model_to_cpu_config(self, vcpu_model):
5015         """Create libvirt CPU config according to VirtCPUModel object.
5016 
5017         :param:vcpu_model: VirtCPUModel object.
5018 
5019         :return: vconfig.LibvirtConfigGuestCPU.
5020 
5021         """
5022 
5023         cpu_config = vconfig.LibvirtConfigGuestCPU()
5024         cpu_config.arch = vcpu_model.arch
5025         cpu_config.model = vcpu_model.model
5026         cpu_config.mode = vcpu_model.mode
5027         cpu_config.match = vcpu_model.match
5028         cpu_config.vendor = vcpu_model.vendor
5029         if vcpu_model.topology:
5030             cpu_config.sockets = vcpu_model.topology.sockets
5031             cpu_config.cores = vcpu_model.topology.cores
5032             cpu_config.threads = vcpu_model.topology.threads
5033         if vcpu_model.features:
5034             for f in vcpu_model.features:
5035                 xf = vconfig.LibvirtConfigGuestCPUFeature()
5036                 xf.name = f.name
5037                 xf.policy = f.policy
5038                 cpu_config.features.add(xf)
5039         return cpu_config
5040 
5041     def _guest_add_pcie_root_ports(self, guest):
5042         """Add PCI Express root ports.
5043 
5044         PCI Express machine can have as many PCIe devices as it has
5045         pcie-root-port controllers (slots in virtual motherboard).
5046 
5047         If we want to have more PCIe slots for hotplug then we need to create
5048         whole PCIe structure (libvirt limitation).
5049         """
5050 
5051         pcieroot = vconfig.LibvirtConfigGuestPCIeRootController()
5052         guest.add_device(pcieroot)
5053 
5054         for x in range(0, CONF.libvirt.num_pcie_ports):
5055             pcierootport = vconfig.LibvirtConfigGuestPCIeRootPortController()
5056             guest.add_device(pcierootport)
5057 
5058     def _guest_add_usb_host_keyboard(self, guest):
5059         """Add USB Host controller and keyboard for graphical console use.
5060 
5061         Add USB keyboard as PS/2 support may not be present on non-x86
5062         architectures.
5063         """
5064         keyboard = vconfig.LibvirtConfigGuestInput()
5065         keyboard.type = "keyboard"
5066         keyboard.bus = "usb"
5067         guest.add_device(keyboard)
5068 
5069         usbhost = vconfig.LibvirtConfigGuestUSBHostController()
5070         usbhost.index = 0
5071         guest.add_device(usbhost)
5072 
5073     def _get_guest_config(self, instance, network_info, image_meta,
5074                           disk_info, rescue=None, block_device_info=None,
5075                           context=None, mdevs=None):
5076         """Get config data for parameters.
5077 
5078         :param rescue: optional dictionary that should contain the key
5079             'ramdisk_id' if a ramdisk is needed for the rescue image and
5080             'kernel_id' if a kernel is needed for the rescue image.
5081 
5082         :param mdevs: optional list of mediated devices to assign to the guest.
5083         """
5084         flavor = instance.flavor
5085         inst_path = libvirt_utils.get_instance_path(instance)
5086         disk_mapping = disk_info['mapping']
5087 
5088         virt_type = CONF.libvirt.virt_type
5089         guest = vconfig.LibvirtConfigGuest()
5090         guest.virt_type = virt_type
5091         guest.name = instance.name
5092         guest.uuid = instance.uuid
5093         # We are using default unit for memory: KiB
5094         guest.memory = flavor.memory_mb * units.Ki
5095         guest.vcpus = flavor.vcpus
5096         allowed_cpus = hardware.get_vcpu_pin_set()
5097 
5098         guest_numa_config = self._get_guest_numa_config(
5099             instance.numa_topology, flavor, allowed_cpus, image_meta)
5100 
5101         guest.cpuset = guest_numa_config.cpuset
5102         guest.cputune = guest_numa_config.cputune
5103         guest.numatune = guest_numa_config.numatune
5104 
5105         guest.membacking = self._get_guest_memory_backing_config(
5106             instance.numa_topology,
5107             guest_numa_config.numatune,
5108             flavor)
5109 
5110         guest.metadata.append(self._get_guest_config_meta(instance))
5111         guest.idmaps = self._get_guest_idmaps()
5112 
5113         for event in self._supported_perf_events:
5114             guest.add_perf_event(event)
5115 
5116         self._update_guest_cputune(guest, flavor, virt_type)
5117 
5118         guest.cpu = self._get_guest_cpu_config(
5119             flavor, image_meta, guest_numa_config.numaconfig,
5120             instance.numa_topology)
5121 
5122         # Notes(yjiang5): we always sync the instance's vcpu model with
5123         # the corresponding config file.
5124         instance.vcpu_model = self._cpu_config_to_vcpu_model(
5125             guest.cpu, instance.vcpu_model)
5126 
5127         if 'root' in disk_mapping:
5128             root_device_name = block_device.prepend_dev(
5129                 disk_mapping['root']['dev'])
5130         else:
5131             root_device_name = None
5132 
5133         if root_device_name:
5134             instance.root_device_name = root_device_name
5135 
5136         guest.os_type = (fields.VMMode.get_from_instance(instance) or
5137                 self._get_guest_os_type(virt_type))
5138         caps = self._host.get_capabilities()
5139 
5140         self._configure_guest_by_virt_type(guest, virt_type, caps, instance,
5141                                            image_meta, flavor,
5142                                            root_device_name)
5143         if virt_type not in ('lxc', 'uml'):
5144             self._conf_non_lxc_uml(virt_type, guest, root_device_name, rescue,
5145                     instance, inst_path, image_meta, disk_info)
5146 
5147         self._set_features(guest, instance.os_type, caps, virt_type,
5148                            image_meta, flavor)
5149         self._set_clock(guest, instance.os_type, image_meta, virt_type)
5150 
5151         storage_configs = self._get_guest_storage_config(context,
5152                 instance, image_meta, disk_info, rescue, block_device_info,
5153                 flavor, guest.os_type)
5154         for config in storage_configs:
5155             guest.add_device(config)
5156 
5157         for vif in network_info:
5158             config = self.vif_driver.get_config(
5159                 instance, vif, image_meta,
5160                 flavor, virt_type, self._host)
5161             guest.add_device(config)
5162 
5163         self._create_consoles(virt_type, guest, instance, flavor, image_meta)
5164 
5165         pointer = self._get_guest_pointer_model(guest.os_type, image_meta)
5166         if pointer:
5167             guest.add_device(pointer)
5168 
5169         self._guest_add_spice_channel(guest)
5170 
5171         if self._guest_add_video_device(guest):
5172             self._add_video_driver(guest, image_meta, flavor)
5173 
5174             # We want video == we want graphical console. Some architectures
5175             # do not have input devices attached in default configuration.
5176             # Let then add USB Host controller and USB keyboard.
5177             # x86(-64) and ppc64 have usb host controller and keyboard
5178             # s390x does not support USB
5179             if caps.host.cpu.arch == fields.Architecture.AARCH64:
5180                 self._guest_add_usb_host_keyboard(guest)
5181 
5182         # Qemu guest agent only support 'qemu' and 'kvm' hypervisor
5183         if virt_type in ('qemu', 'kvm'):
5184             self._set_qemu_guest_agent(guest, flavor, instance, image_meta)
5185 
5186         # Add PCIe root port controllers for PCI Express machines
5187         # but only if their amount is configured
5188         if (CONF.libvirt.num_pcie_ports and
5189                 ((caps.host.cpu.arch == fields.Architecture.AARCH64 and
5190                 guest.os_mach_type.startswith('virt')) or
5191                 (caps.host.cpu.arch == fields.Architecture.X86_64 and
5192                 guest.os_mach_type is not None and
5193                 'q35' in guest.os_mach_type))):
5194             self._guest_add_pcie_root_ports(guest)
5195 
5196         self._guest_add_pci_devices(guest, instance)
5197 
5198         self._guest_add_watchdog_action(guest, flavor, image_meta)
5199 
5200         self._guest_add_memory_balloon(guest)
5201 
5202         if mdevs:
5203             self._guest_add_mdevs(guest, mdevs)
5204 
5205         return guest
5206 
5207     def _guest_add_mdevs(self, guest, chosen_mdevs):
5208         for chosen_mdev in chosen_mdevs:
5209             mdev = vconfig.LibvirtConfigGuestHostdevMDEV()
5210             mdev.uuid = chosen_mdev
5211             guest.add_device(mdev)
5212 
5213     @staticmethod
5214     def _guest_add_spice_channel(guest):
5215         if (CONF.spice.enabled and CONF.spice.agent_enabled
5216                 and guest.virt_type not in ('lxc', 'uml', 'xen')):
5217             channel = vconfig.LibvirtConfigGuestChannel()
5218             channel.type = 'spicevmc'
5219             channel.target_name = "com.redhat.spice.0"
5220             guest.add_device(channel)
5221 
5222     @staticmethod
5223     def _guest_add_memory_balloon(guest):
5224         virt_type = guest.virt_type
5225         # Memory balloon device only support 'qemu/kvm' and 'xen' hypervisor
5226         if (virt_type in ('xen', 'qemu', 'kvm') and
5227                     CONF.libvirt.mem_stats_period_seconds > 0):
5228             balloon = vconfig.LibvirtConfigMemoryBalloon()
5229             if virt_type in ('qemu', 'kvm'):
5230                 balloon.model = 'virtio'
5231             else:
5232                 balloon.model = 'xen'
5233             balloon.period = CONF.libvirt.mem_stats_period_seconds
5234             guest.add_device(balloon)
5235 
5236     @staticmethod
5237     def _guest_add_watchdog_action(guest, flavor, image_meta):
5238         # image meta takes precedence over flavor extra specs; disable the
5239         # watchdog action by default
5240         watchdog_action = (flavor.extra_specs.get('hw:watchdog_action')
5241                            or 'disabled')
5242         watchdog_action = image_meta.properties.get('hw_watchdog_action',
5243                                                     watchdog_action)
5244         # NB(sross): currently only actually supported by KVM/QEmu
5245         if watchdog_action != 'disabled':
5246             if watchdog_action in fields.WatchdogAction.ALL:
5247                 bark = vconfig.LibvirtConfigGuestWatchdog()
5248                 bark.action = watchdog_action
5249                 guest.add_device(bark)
5250             else:
5251                 raise exception.InvalidWatchdogAction(action=watchdog_action)
5252 
5253     def _guest_add_pci_devices(self, guest, instance):
5254         virt_type = guest.virt_type
5255         if virt_type in ('xen', 'qemu', 'kvm'):
5256             # Get all generic PCI devices (non-SR-IOV).
5257             for pci_dev in pci_manager.get_instance_pci_devs(instance):
5258                 guest.add_device(self._get_guest_pci_device(pci_dev))
5259         else:
5260             # PCI devices is only supported for hypervisors
5261             #  'xen', 'qemu' and 'kvm'.
5262             if pci_manager.get_instance_pci_devs(instance, 'all'):
5263                 raise exception.PciDeviceUnsupportedHypervisor(type=virt_type)
5264 
5265     @staticmethod
5266     def _guest_add_video_device(guest):
5267         # NB some versions of libvirt support both SPICE and VNC
5268         # at the same time. We're not trying to second guess which
5269         # those versions are. We'll just let libvirt report the
5270         # errors appropriately if the user enables both.
5271         add_video_driver = False
5272         if CONF.vnc.enabled and guest.virt_type not in ('lxc', 'uml'):
5273             graphics = vconfig.LibvirtConfigGuestGraphics()
5274             graphics.type = "vnc"
5275             if CONF.vnc.keymap:
5276                 graphics.keymap = CONF.vnc.keymap
5277             graphics.listen = CONF.vnc.server_listen
5278             guest.add_device(graphics)
5279             add_video_driver = True
5280         if CONF.spice.enabled and guest.virt_type not in ('lxc', 'uml', 'xen'):
5281             graphics = vconfig.LibvirtConfigGuestGraphics()
5282             graphics.type = "spice"
5283             if CONF.spice.keymap:
5284                 graphics.keymap = CONF.spice.keymap
5285             graphics.listen = CONF.spice.server_listen
5286             guest.add_device(graphics)
5287             add_video_driver = True
5288         return add_video_driver
5289 
5290     def _get_guest_pointer_model(self, os_type, image_meta):
5291         pointer_model = image_meta.properties.get(
5292             'hw_pointer_model', CONF.pointer_model)
5293         if pointer_model is None and CONF.libvirt.use_usb_tablet:
5294             # TODO(sahid): We set pointer_model to keep compatibility
5295             # until the next release O*. It means operators can continue
5296             # to use the deprecated option "use_usb_tablet" or set a
5297             # specific device to use
5298             pointer_model = "usbtablet"
5299             LOG.warning('The option "use_usb_tablet" has been '
5300                         'deprecated for Newton in favor of the more '
5301                         'generic "pointer_model". Please update '
5302                         'nova.conf to address this change.')
5303 
5304         if pointer_model == "usbtablet":
5305             # We want a tablet if VNC is enabled, or SPICE is enabled and
5306             # the SPICE agent is disabled. If the SPICE agent is enabled
5307             # it provides a paravirt mouse which drastically reduces
5308             # overhead (by eliminating USB polling).
5309             if CONF.vnc.enabled or (
5310                     CONF.spice.enabled and not CONF.spice.agent_enabled):
5311                 return self._get_guest_usb_tablet(os_type)
5312             else:
5313                 if CONF.pointer_model or CONF.libvirt.use_usb_tablet:
5314                     # For backward compatibility We don't want to break
5315                     # process of booting an instance if host is configured
5316                     # to use USB tablet without VNC or SPICE and SPICE
5317                     # agent disable.
5318                     LOG.warning('USB tablet requested for guests by host '
5319                                 'configuration. In order to accept this '
5320                                 'request VNC should be enabled or SPICE '
5321                                 'and SPICE agent disabled on host.')
5322                 else:
5323                     raise exception.UnsupportedPointerModelRequested(
5324                         model="usbtablet")
5325 
5326     def _get_guest_usb_tablet(self, os_type):
5327         tablet = None
5328         if os_type == fields.VMMode.HVM:
5329             tablet = vconfig.LibvirtConfigGuestInput()
5330             tablet.type = "tablet"
5331             tablet.bus = "usb"
5332         else:
5333             if CONF.pointer_model or CONF.libvirt.use_usb_tablet:
5334                 # For backward compatibility We don't want to break
5335                 # process of booting an instance if virtual machine mode
5336                 # is not configured as HVM.
5337                 LOG.warning('USB tablet requested for guests by host '
5338                             'configuration. In order to accept this '
5339                             'request the machine mode should be '
5340                             'configured as HVM.')
5341             else:
5342                 raise exception.UnsupportedPointerModelRequested(
5343                     model="usbtablet")
5344         return tablet
5345 
5346     def _get_guest_xml(self, context, instance, network_info, disk_info,
5347                        image_meta, rescue=None,
5348                        block_device_info=None,
5349                        mdevs=None):
5350         # NOTE(danms): Stringifying a NetworkInfo will take a lock. Do
5351         # this ahead of time so that we don't acquire it while also
5352         # holding the logging lock.
5353         network_info_str = str(network_info)
5354         msg = ('Start _get_guest_xml '
5355                'network_info=%(network_info)s '
5356                'disk_info=%(disk_info)s '
5357                'image_meta=%(image_meta)s rescue=%(rescue)s '
5358                'block_device_info=%(block_device_info)s' %
5359                {'network_info': network_info_str, 'disk_info': disk_info,
5360                 'image_meta': image_meta, 'rescue': rescue,
5361                 'block_device_info': block_device_info})
5362         # NOTE(mriedem): block_device_info can contain auth_password so we
5363         # need to sanitize the password in the message.
5364         LOG.debug(strutils.mask_password(msg), instance=instance)
5365         conf = self._get_guest_config(instance, network_info, image_meta,
5366                                       disk_info, rescue, block_device_info,
5367                                       context, mdevs)
5368         xml = conf.to_xml()
5369 
5370         LOG.debug('End _get_guest_xml xml=%(xml)s',
5371                   {'xml': xml}, instance=instance)
5372         return xml
5373 
5374     def get_info(self, instance):
5375         """Retrieve information from libvirt for a specific instance.
5376 
5377         If a libvirt error is encountered during lookup, we might raise a
5378         NotFound exception or Error exception depending on how severe the
5379         libvirt error is.
5380 
5381         :param instance: nova.objects.instance.Instance object
5382         :returns: An InstanceInfo object
5383         """
5384         guest = self._host.get_guest(instance)
5385         # Kind of ugly but we need to pass host to get_info as for a
5386         # workaround, see libvirt/compat.py
5387         return guest.get_info(self._host)
5388 
5389     def _create_domain_setup_lxc(self, context, instance, image_meta,
5390                                  block_device_info):
5391         inst_path = libvirt_utils.get_instance_path(instance)
5392         block_device_mapping = driver.block_device_info_get_mapping(
5393             block_device_info)
5394         root_disk = block_device.get_root_bdm(block_device_mapping)
5395         if root_disk:
5396             self._connect_volume(context, root_disk['connection_info'],
5397                                  instance)
5398             disk_path = root_disk['connection_info']['data']['device_path']
5399 
5400             # NOTE(apmelton) - Even though the instance is being booted from a
5401             # cinder volume, it is still presented as a local block device.
5402             # LocalBlockImage is used here to indicate that the instance's
5403             # disk is backed by a local block device.
5404             image_model = imgmodel.LocalBlockImage(disk_path)
5405         else:
5406             root_disk = self.image_backend.by_name(instance, 'disk')
5407             image_model = root_disk.get_model(self._conn)
5408 
5409         container_dir = os.path.join(inst_path, 'rootfs')
5410         fileutils.ensure_tree(container_dir)
5411         rootfs_dev = disk_api.setup_container(image_model,
5412                                               container_dir=container_dir)
5413 
5414         try:
5415             # Save rootfs device to disconnect it when deleting the instance
5416             if rootfs_dev:
5417                 instance.system_metadata['rootfs_device_name'] = rootfs_dev
5418             if CONF.libvirt.uid_maps or CONF.libvirt.gid_maps:
5419                 id_maps = self._get_guest_idmaps()
5420                 libvirt_utils.chown_for_id_maps(container_dir, id_maps)
5421         except Exception:
5422             with excutils.save_and_reraise_exception():
5423                 self._create_domain_cleanup_lxc(instance)
5424 
5425     def _create_domain_cleanup_lxc(self, instance):
5426         inst_path = libvirt_utils.get_instance_path(instance)
5427         container_dir = os.path.join(inst_path, 'rootfs')
5428 
5429         try:
5430             state = self.get_info(instance).state
5431         except exception.InstanceNotFound:
5432             # The domain may not be present if the instance failed to start
5433             state = None
5434 
5435         if state == power_state.RUNNING:
5436             # NOTE(uni): Now the container is running with its own private
5437             # mount namespace and so there is no need to keep the container
5438             # rootfs mounted in the host namespace
5439             LOG.debug('Attempting to unmount container filesystem: %s',
5440                       container_dir, instance=instance)
5441             disk_api.clean_lxc_namespace(container_dir=container_dir)
5442         else:
5443             disk_api.teardown_container(container_dir=container_dir)
5444 
5445     @contextlib.contextmanager
5446     def _lxc_disk_handler(self, context, instance, image_meta,
5447                           block_device_info):
5448         """Context manager to handle the pre and post instance boot,
5449            LXC specific disk operations.
5450 
5451            An image or a volume path will be prepared and setup to be
5452            used by the container, prior to starting it.
5453            The disk will be disconnected and unmounted if a container has
5454            failed to start.
5455         """
5456 
5457         if CONF.libvirt.virt_type != 'lxc':
5458             yield
5459             return
5460 
5461         self._create_domain_setup_lxc(context, instance, image_meta,
5462                                       block_device_info)
5463 
5464         try:
5465             yield
5466         finally:
5467             self._create_domain_cleanup_lxc(instance)
5468 
5469     # TODO(sahid): Consider renaming this to _create_guest.
5470     def _create_domain(self, xml=None, domain=None,
5471                        power_on=True, pause=False, post_xml_callback=None):
5472         """Create a domain.
5473 
5474         Either domain or xml must be passed in. If both are passed, then
5475         the domain definition is overwritten from the xml.
5476 
5477         :returns guest.Guest: Guest just created
5478         """
5479         if xml:
5480             guest = libvirt_guest.Guest.create(xml, self._host)
5481             if post_xml_callback is not None:
5482                 post_xml_callback()
5483         else:
5484             guest = libvirt_guest.Guest(domain)
5485 
5486         if power_on or pause:
5487             guest.launch(pause=pause)
5488 
5489         if not utils.is_neutron():
5490             guest.enable_hairpin()
5491 
5492         return guest
5493 
5494     def _neutron_failed_callback(self, event_name, instance):
5495         LOG.error('Neutron Reported failure on event '
5496                   '%(event)s for instance %(uuid)s',
5497                   {'event': event_name, 'uuid': instance.uuid},
5498                   instance=instance)
5499         if CONF.vif_plugging_is_fatal:
5500             raise exception.VirtualInterfaceCreateException()
5501 
5502     def _neutron_failed_live_migration_callback(self, event_name, instance):
5503         msg = ('Neutron reported failure during live migration '
5504                'with %(event)s for instance %(uuid)s' %
5505                {'event': event_name, 'uuid': instance.uuid})
5506         raise exception.MigrationError(reason=msg)
5507 
5508     def _get_neutron_events(self, network_info):
5509         # NOTE(danms): We need to collect any VIFs that are currently
5510         # down that we expect a down->up event for. Anything that is
5511         # already up will not undergo that transition, and for
5512         # anything that might be stale (cache-wise) assume it's
5513         # already up so we don't block on it.
5514         return [('network-vif-plugged', vif['id'])
5515                 for vif in network_info if vif.get('active', True) is False]
5516 
5517     def _get_neutron_events_for_live_migration(self, network_info):
5518         # Neutron should send events to Nova indicating that the VIFs
5519         # are successfully plugged on destination host.
5520 
5521         # TODO(sahid): Currently we only use the mechanism of waiting
5522         # for neutron events during live-migration for linux-bridge.
5523         return [('network-vif-plugged', vif['id'])
5524                 for vif in network_info if (
5525                         vif.get('type') == network_model.VIF_TYPE_BRIDGE)]
5526 
5527     def _cleanup_failed_start(self, context, instance, network_info,
5528                               block_device_info, guest, destroy_disks):
5529         try:
5530             if guest and guest.is_active():
5531                 guest.poweroff()
5532         finally:
5533             self.cleanup(context, instance, network_info=network_info,
5534                          block_device_info=block_device_info,
5535                          destroy_disks=destroy_disks)
5536 
5537     def _create_domain_and_network(self, context, xml, instance, network_info,
5538                                    block_device_info=None, power_on=True,
5539                                    vifs_already_plugged=False,
5540                                    post_xml_callback=None,
5541                                    destroy_disks_on_failure=False):
5542 
5543         """Do required network setup and create domain."""
5544         timeout = CONF.vif_plugging_timeout
5545         if (self._conn_supports_start_paused and
5546             utils.is_neutron() and not
5547             vifs_already_plugged and power_on and timeout):
5548             events = self._get_neutron_events(network_info)
5549         else:
5550             events = []
5551 
5552         pause = bool(events)
5553         guest = None
5554         try:
5555             with self.virtapi.wait_for_instance_event(
5556                     instance, events, deadline=timeout,
5557                     error_callback=self._neutron_failed_callback):
5558                 self.plug_vifs(instance, network_info)
5559                 self.firewall_driver.setup_basic_filtering(instance,
5560                                                            network_info)
5561                 self.firewall_driver.prepare_instance_filter(instance,
5562                                                              network_info)
5563                 with self._lxc_disk_handler(context, instance,
5564                                             instance.image_meta,
5565                                             block_device_info):
5566                     guest = self._create_domain(
5567                         xml, pause=pause, power_on=power_on,
5568                         post_xml_callback=post_xml_callback)
5569 
5570                 self.firewall_driver.apply_instance_filter(instance,
5571                                                            network_info)
5572         except exception.VirtualInterfaceCreateException:
5573             # Neutron reported failure and we didn't swallow it, so
5574             # bail here
5575             with excutils.save_and_reraise_exception():
5576                 self._cleanup_failed_start(context, instance, network_info,
5577                                            block_device_info, guest,
5578                                            destroy_disks_on_failure)
5579         except eventlet.timeout.Timeout:
5580             # We never heard from Neutron
5581             LOG.warning('Timeout waiting for %(events)s for '
5582                         'instance with vm_state %(vm_state)s and '
5583                         'task_state %(task_state)s.',
5584                         {'events': events,
5585                          'vm_state': instance.vm_state,
5586                          'task_state': instance.task_state},
5587                         instance=instance)
5588             if CONF.vif_plugging_is_fatal:
5589                 self._cleanup_failed_start(context, instance, network_info,
5590                                            block_device_info, guest,
5591                                            destroy_disks_on_failure)
5592                 raise exception.VirtualInterfaceCreateException()
5593         except Exception:
5594             # Any other error, be sure to clean up
5595             LOG.error('Failed to start libvirt guest', instance=instance)
5596             with excutils.save_and_reraise_exception():
5597                 self._cleanup_failed_start(context, instance, network_info,
5598                                            block_device_info, guest,
5599                                            destroy_disks_on_failure)
5600 
5601         # Resume only if domain has been paused
5602         if pause:
5603             guest.resume()
5604         return guest
5605 
5606     def _get_vcpu_total(self):
5607         """Get available vcpu number of physical computer.
5608 
5609         :returns: the number of cpu core instances can be used.
5610 
5611         """
5612         try:
5613             total_pcpus = self._host.get_cpu_count()
5614         except libvirt.libvirtError:
5615             LOG.warning("Cannot get the number of cpu, because this "
5616                         "function is not implemented for this platform. ")
5617             return 0
5618 
5619         if not CONF.vcpu_pin_set:
5620             return total_pcpus
5621 
5622         available_ids = hardware.get_vcpu_pin_set()
5623         # We get the list of online CPUs on the host and see if the requested
5624         # set falls under these. If not, we retain the old behavior.
5625         online_pcpus = None
5626         try:
5627             online_pcpus = self._host.get_online_cpus()
5628         except libvirt.libvirtError as ex:
5629             error_code = ex.get_error_code()
5630             err_msg = encodeutils.exception_to_unicode(ex)
5631             LOG.warning(
5632                 "Couldn't retrieve the online CPUs due to a Libvirt "
5633                 "error: %(error)s with error code: %(error_code)s",
5634                 {'error': err_msg, 'error_code': error_code})
5635         if online_pcpus:
5636             if not (available_ids <= online_pcpus):
5637                 msg = (_("Invalid vcpu_pin_set config, one or more of the "
5638                          "specified cpuset is not online. Online cpuset(s): "
5639                          "%(online)s, requested cpuset(s): %(req)s"),
5640                        {'online': sorted(online_pcpus),
5641                         'req': sorted(available_ids)})
5642                 raise exception.Invalid(msg)
5643         elif sorted(available_ids)[-1] >= total_pcpus:
5644             raise exception.Invalid(_("Invalid vcpu_pin_set config, "
5645                                       "out of hypervisor cpu range."))
5646         return len(available_ids)
5647 
5648     @staticmethod
5649     def _get_local_gb_info():
5650         """Get local storage info of the compute node in GB.
5651 
5652         :returns: A dict containing:
5653              :total: How big the overall usable filesystem is (in gigabytes)
5654              :free: How much space is free (in gigabytes)
5655              :used: How much space is used (in gigabytes)
5656         """
5657 
5658         if CONF.libvirt.images_type == 'lvm':
5659             info = lvm.get_volume_group_info(
5660                                CONF.libvirt.images_volume_group)
5661         elif CONF.libvirt.images_type == 'rbd':
5662             info = LibvirtDriver._get_rbd_driver().get_pool_info()
5663         else:
5664             info = libvirt_utils.get_fs_info(CONF.instances_path)
5665 
5666         for (k, v) in info.items():
5667             info[k] = v / units.Gi
5668 
5669         return info
5670 
5671     def _get_vcpu_used(self):
5672         """Get vcpu usage number of physical computer.
5673 
5674         :returns: The total number of vcpu(s) that are currently being used.
5675 
5676         """
5677 
5678         total = 0
5679 
5680         # Not all libvirt drivers will support the get_vcpus_info()
5681         #
5682         # For example, LXC does not have a concept of vCPUs, while
5683         # QEMU (TCG) traditionally handles all vCPUs in a single
5684         # thread. So both will report an exception when the vcpus()
5685         # API call is made. In such a case we should report the
5686         # guest as having 1 vCPU, since that lets us still do
5687         # CPU over commit calculations that apply as the total
5688         # guest count scales.
5689         #
5690         # It is also possible that we might see an exception if
5691         # the guest is just in middle of shutting down. Technically
5692         # we should report 0 for vCPU usage in this case, but we
5693         # we can't reliably distinguish the vcpu not supported
5694         # case from the just shutting down case. Thus we don't know
5695         # whether to report 1 or 0 for vCPU count.
5696         #
5697         # Under-reporting vCPUs is bad because it could conceivably
5698         # let the scheduler place too many guests on the host. Over-
5699         # reporting vCPUs is not a problem as it'll auto-correct on
5700         # the next refresh of usage data.
5701         #
5702         # Thus when getting an exception we always report 1 as the
5703         # vCPU count, as the least worst value.
5704         for guest in self._host.list_guests():
5705             try:
5706                 vcpus = guest.get_vcpus_info()
5707                 total += len(list(vcpus))
5708             except libvirt.libvirtError:
5709                 total += 1
5710             # NOTE(gtt116): give other tasks a chance.
5711             greenthread.sleep(0)
5712         return total
5713 
5714     def _get_supported_vgpu_types(self):
5715         if not CONF.devices.enabled_vgpu_types:
5716             return []
5717         # TODO(sbauza): Move this check up to compute_manager.init_host
5718         if len(CONF.devices.enabled_vgpu_types) > 1:
5719             LOG.warning('libvirt only supports one GPU type per compute node,'
5720                         ' only first type will be used.')
5721         requested_types = CONF.devices.enabled_vgpu_types[:1]
5722         return requested_types
5723 
5724     def _get_vgpu_total(self):
5725         """Returns the number of total available vGPUs for any GPU type that is
5726         enabled with the enabled_vgpu_types CONF option.
5727         """
5728         requested_types = self._get_supported_vgpu_types()
5729         # Bail out early if operator doesn't care about providing vGPUs
5730         if not requested_types:
5731             return 0
5732         # Filter how many available mdevs we can create for all the supported
5733         # types.
5734         mdev_capable_devices = self._get_mdev_capable_devices(requested_types)
5735         vgpus = 0
5736         for dev in mdev_capable_devices:
5737             for _type in dev['types']:
5738                 vgpus += dev['types'][_type]['availableInstances']
5739         # Count the already created (but possibly not assigned to a guest)
5740         # mdevs for all the supported types
5741         mediated_devices = self._get_mediated_devices(requested_types)
5742         vgpus += len(mediated_devices)
5743         return vgpus
5744 
5745     def _get_instance_capabilities(self):
5746         """Get hypervisor instance capabilities
5747 
5748         Returns a list of tuples that describe instances the
5749         hypervisor is capable of hosting.  Each tuple consists
5750         of the triplet (arch, hypervisor_type, vm_mode).
5751 
5752         Supported hypervisor_type is filtered by virt_type,
5753         a parameter set by operators via `nova.conf`.
5754 
5755         :returns: List of tuples describing instance capabilities
5756         """
5757         caps = self._host.get_capabilities()
5758         instance_caps = list()
5759         for g in caps.guests:
5760             for dt in g.domtype:
5761                 if dt != CONF.libvirt.virt_type:
5762                     continue
5763                 instance_cap = (
5764                     fields.Architecture.canonicalize(g.arch),
5765                     fields.HVType.canonicalize(dt),
5766                     fields.VMMode.canonicalize(g.ostype))
5767                 instance_caps.append(instance_cap)
5768 
5769         return instance_caps
5770 
5771     def _get_cpu_info(self):
5772         """Get cpuinfo information.
5773 
5774         Obtains cpu feature from virConnect.getCapabilities.
5775 
5776         :return: see above description
5777 
5778         """
5779 
5780         caps = self._host.get_capabilities()
5781         cpu_info = dict()
5782 
5783         cpu_info['arch'] = caps.host.cpu.arch
5784         cpu_info['model'] = caps.host.cpu.model
5785         cpu_info['vendor'] = caps.host.cpu.vendor
5786 
5787         topology = dict()
5788         topology['cells'] = len(getattr(caps.host.topology, 'cells', [1]))
5789         topology['sockets'] = caps.host.cpu.sockets
5790         topology['cores'] = caps.host.cpu.cores
5791         topology['threads'] = caps.host.cpu.threads
5792         cpu_info['topology'] = topology
5793 
5794         features = set()
5795         for f in caps.host.cpu.features:
5796             features.add(f.name)
5797         cpu_info['features'] = features
5798         return cpu_info
5799 
5800     def _get_pcinet_info(self, vf_address):
5801         """Returns a dict of NET device."""
5802         devname = pci_utils.get_net_name_by_vf_pci_address(vf_address)
5803         if not devname:
5804             return
5805 
5806         virtdev = self._host.device_lookup_by_name(devname)
5807         xmlstr = virtdev.XMLDesc(0)
5808         cfgdev = vconfig.LibvirtConfigNodeDevice()
5809         cfgdev.parse_str(xmlstr)
5810         return {'name': cfgdev.name,
5811                 'capabilities': cfgdev.pci_capability.features}
5812 
5813     def _get_pcidev_info(self, devname):
5814         """Returns a dict of PCI device."""
5815 
5816         def _get_device_type(cfgdev, pci_address):
5817             """Get a PCI device's device type.
5818 
5819             An assignable PCI device can be a normal PCI device,
5820             a SR-IOV Physical Function (PF), or a SR-IOV Virtual
5821             Function (VF). Only normal PCI devices or SR-IOV VFs
5822             are assignable, while SR-IOV PFs are always owned by
5823             hypervisor.
5824             """
5825             for fun_cap in cfgdev.pci_capability.fun_capability:
5826                 if fun_cap.type == 'virt_functions':
5827                     return {
5828                         'dev_type': fields.PciDeviceType.SRIOV_PF,
5829                     }
5830                 if (fun_cap.type == 'phys_function' and
5831                     len(fun_cap.device_addrs) != 0):
5832                     phys_address = "%04x:%02x:%02x.%01x" % (
5833                         fun_cap.device_addrs[0][0],
5834                         fun_cap.device_addrs[0][1],
5835                         fun_cap.device_addrs[0][2],
5836                         fun_cap.device_addrs[0][3])
5837                     return {
5838                         'dev_type': fields.PciDeviceType.SRIOV_VF,
5839                         'parent_addr': phys_address,
5840                     }
5841 
5842             # Note(moshele): libvirt < 1.3 reported virt_functions capability
5843             # only when VFs are enabled. The check below is a workaround
5844             # to get the correct report regardless of whether or not any
5845             # VFs are enabled for the device.
5846             if not self._host.has_min_version(
5847                 MIN_LIBVIRT_PF_WITH_NO_VFS_CAP_VERSION):
5848                 is_physical_function = pci_utils.is_physical_function(
5849                     *pci_utils.get_pci_address_fields(pci_address))
5850                 if is_physical_function:
5851                     return {'dev_type': fields.PciDeviceType.SRIOV_PF}
5852 
5853             return {'dev_type': fields.PciDeviceType.STANDARD}
5854 
5855         def _get_device_capabilities(device, address):
5856             """Get PCI VF device's additional capabilities.
5857 
5858             If a PCI device is a virtual function, this function reads the PCI
5859             parent's network capabilities (must be always a NIC device) and
5860             appends this information to the device's dictionary.
5861             """
5862             if device.get('dev_type') == fields.PciDeviceType.SRIOV_VF:
5863                 pcinet_info = self._get_pcinet_info(address)
5864                 if pcinet_info:
5865                     return {'capabilities':
5866                                 {'network': pcinet_info.get('capabilities')}}
5867             return {}
5868 
5869         virtdev = self._host.device_lookup_by_name(devname)
5870         xmlstr = virtdev.XMLDesc(0)
5871         cfgdev = vconfig.LibvirtConfigNodeDevice()
5872         cfgdev.parse_str(xmlstr)
5873 
5874         address = "%04x:%02x:%02x.%1x" % (
5875             cfgdev.pci_capability.domain,
5876             cfgdev.pci_capability.bus,
5877             cfgdev.pci_capability.slot,
5878             cfgdev.pci_capability.function)
5879 
5880         device = {
5881             "dev_id": cfgdev.name,
5882             "address": address,
5883             "product_id": "%04x" % cfgdev.pci_capability.product_id,
5884             "vendor_id": "%04x" % cfgdev.pci_capability.vendor_id,
5885             }
5886 
5887         device["numa_node"] = cfgdev.pci_capability.numa_node
5888 
5889         # requirement by DataBase Model
5890         device['label'] = 'label_%(vendor_id)s_%(product_id)s' % device
5891         device.update(_get_device_type(cfgdev, address))
5892         device.update(_get_device_capabilities(device, address))
5893         return device
5894 
5895     def _get_pci_passthrough_devices(self):
5896         """Get host PCI devices information.
5897 
5898         Obtains pci devices information from libvirt, and returns
5899         as a JSON string.
5900 
5901         Each device information is a dictionary, with mandatory keys
5902         of 'address', 'vendor_id', 'product_id', 'dev_type', 'dev_id',
5903         'label' and other optional device specific information.
5904 
5905         Refer to the objects/pci_device.py for more idea of these keys.
5906 
5907         :returns: a JSON string containing a list of the assignable PCI
5908                   devices information
5909         """
5910         # Bail early if we know we can't support `listDevices` to avoid
5911         # repeated warnings within a periodic task
5912         if not getattr(self, '_list_devices_supported', True):
5913             return jsonutils.dumps([])
5914 
5915         try:
5916             dev_names = self._host.list_pci_devices() or []
5917         except libvirt.libvirtError as ex:
5918             error_code = ex.get_error_code()
5919             if error_code == libvirt.VIR_ERR_NO_SUPPORT:
5920                 self._list_devices_supported = False
5921                 LOG.warning("URI %(uri)s does not support "
5922                             "listDevices: %(error)s",
5923                             {'uri': self._uri(),
5924                              'error': encodeutils.exception_to_unicode(ex)})
5925                 return jsonutils.dumps([])
5926             else:
5927                 raise
5928 
5929         pci_info = []
5930         for name in dev_names:
5931             pci_info.append(self._get_pcidev_info(name))
5932 
5933         return jsonutils.dumps(pci_info)
5934 
5935     def _get_mdev_capabilities_for_dev(self, devname, types=None):
5936         """Returns a dict of MDEV capable device with the ID as first key
5937         and then a list of supported types, each of them being a dict.
5938 
5939         :param types: Only return those specific types.
5940         """
5941         virtdev = self._host.device_lookup_by_name(devname)
5942         xmlstr = virtdev.XMLDesc(0)
5943         cfgdev = vconfig.LibvirtConfigNodeDevice()
5944         cfgdev.parse_str(xmlstr)
5945 
5946         device = {
5947             "dev_id": cfgdev.name,
5948             "types": {},
5949         }
5950         for mdev_cap in cfgdev.pci_capability.mdev_capability:
5951             for cap in mdev_cap.mdev_types:
5952                 if not types or cap['type'] in types:
5953                     device["types"].update({cap['type']: {
5954                         'availableInstances': cap['availableInstances'],
5955                         'name': cap['name'],
5956                         'deviceAPI': cap['deviceAPI']}})
5957         return device
5958 
5959     def _get_mdev_capable_devices(self, types=None):
5960         """Get host devices supporting mdev types.
5961 
5962         Obtain devices information from libvirt and returns a list of
5963         dictionaries.
5964 
5965         :param types: Filter only devices supporting those types.
5966         """
5967         if not self._host.has_min_version(MIN_LIBVIRT_MDEV_SUPPORT):
5968             return []
5969         dev_names = self._host.list_mdev_capable_devices() or []
5970         mdev_capable_devices = []
5971         for name in dev_names:
5972             device = self._get_mdev_capabilities_for_dev(name, types)
5973             if not device["types"]:
5974                 continue
5975             mdev_capable_devices.append(device)
5976         return mdev_capable_devices
5977 
5978     def _get_mediated_device_information(self, devname):
5979         """Returns a dict of a mediated device."""
5980         virtdev = self._host.device_lookup_by_name(devname)
5981         xmlstr = virtdev.XMLDesc(0)
5982         cfgdev = vconfig.LibvirtConfigNodeDevice()
5983         cfgdev.parse_str(xmlstr)
5984 
5985         device = {
5986             "dev_id": cfgdev.name,
5987             # name is like mdev_00ead764_fdc0_46b6_8db9_2963f5c815b4
5988             "uuid": str(uuid.UUID(cfgdev.name[5:].replace('_', '-'))),
5989             "type": cfgdev.mdev_information.type,
5990             "iommu_group": cfgdev.mdev_information.iommu_group,
5991         }
5992         return device
5993 
5994     def _get_mediated_devices(self, types=None):
5995         """Get host mediated devices.
5996 
5997         Obtain devices information from libvirt and returns a list of
5998         dictionaries.
5999 
6000         :param types: Filter only devices supporting those types.
6001         """
6002         if not self._host.has_min_version(MIN_LIBVIRT_MDEV_SUPPORT):
6003             return []
6004         dev_names = self._host.list_mediated_devices() or []
6005         mediated_devices = []
6006         for name in dev_names:
6007             device = self._get_mediated_device_information(name)
6008             if not types or device["type"] in types:
6009                 mediated_devices.append(device)
6010         return mediated_devices
6011 
6012     def _get_all_assigned_mediated_devices(self, instance=None):
6013         """Lookup all instances from the host and return all the mediated
6014         devices that are assigned to a guest.
6015 
6016         :param instance: Only return mediated devices for that instance.
6017 
6018         :returns: A dictionary of keys being mediated device UUIDs and their
6019                   respective values the instance UUID of the guest using it.
6020         """
6021         allocated_mdevs = {}
6022         if instance:
6023             # NOTE(sbauza): In some cases (like a migration issue), the
6024             # instance can exist in the Nova database but libvirt doesn't know
6025             # about it. For such cases, the way to fix that is to hard reboot
6026             # the instance, which will recreate the libvirt guest.
6027             # For that reason, we need to support that case by making sure
6028             # we don't raise an exception if the libvirt guest doesn't exist.
6029             try:
6030                 guest = self._host.get_guest(instance)
6031             except exception.InstanceNotFound:
6032                 # Bail out early if libvirt doesn't know about it since we
6033                 # can't know the existing mediated devices
6034                 return {}
6035             guests = [guest]
6036         else:
6037             guests = self._host.list_guests(only_running=False)
6038         for guest in guests:
6039             cfg = guest.get_config()
6040             for device in cfg.devices:
6041                 if isinstance(device, vconfig.LibvirtConfigGuestHostdevMDEV):
6042                     allocated_mdevs[device.uuid] = guest.uuid
6043         return allocated_mdevs
6044 
6045     @staticmethod
6046     def _vgpu_allocations(allocations):
6047         """Filtering only the VGPU allocations from a list of allocations.
6048 
6049         :param allocations: Information about resources allocated to the
6050                             instance via placement, of the form returned by
6051                             SchedulerReportClient.get_allocations_for_consumer.
6052         """
6053         if not allocations:
6054             # If no allocations, there is no vGPU request.
6055             return {}
6056         RC_VGPU = rc_fields.ResourceClass.VGPU
6057         vgpu_allocations = {}
6058         for rp in allocations:
6059             res = allocations[rp]['resources']
6060             if RC_VGPU in res and res[RC_VGPU] > 0:
6061                 vgpu_allocations[rp] = {'resources': {RC_VGPU: res[RC_VGPU]}}
6062         return vgpu_allocations
6063 
6064     def _get_existing_mdevs_not_assigned(self, requested_types=None):
6065         """Returns the already created mediated devices that are not assigned
6066         to a guest yet.
6067 
6068         :param requested_types: Filter out the result for only mediated devices
6069                                 having those types.
6070         """
6071         allocated_mdevs = self._get_all_assigned_mediated_devices()
6072         mdevs = self._get_mediated_devices(requested_types)
6073         available_mdevs = set([mdev["uuid"]
6074                                for mdev in mdevs]) - set(allocated_mdevs)
6075         return available_mdevs
6076 
6077     def _create_new_mediated_device(self, requested_types, uuid=None):
6078         """Find a physical device that can support a new mediated device and
6079         create it.
6080 
6081         :param requested_types: Filter only capable devices supporting those
6082                                 types.
6083         :param uuid: The possible mdev UUID we want to create again
6084 
6085         :returns: the newly created mdev UUID or None if not possible
6086         """
6087         # Try to see if we can still create a new mediated device
6088         devices = self._get_mdev_capable_devices(requested_types)
6089         for device in devices:
6090             # For the moment, the libvirt driver only supports one
6091             # type per host
6092             # TODO(sbauza): Once we support more than one type, make
6093             # sure we look at the flavor/trait for the asked type.
6094             asked_type = requested_types[0]
6095             if device['types'][asked_type]['availableInstances'] > 0:
6096                 # That physical GPU has enough room for a new mdev
6097                 dev_name = device['dev_id']
6098                 # We need the PCI address, not the libvirt name
6099                 # The libvirt name is like 'pci_0000_84_00_0'
6100                 pci_addr = "{}:{}:{}.{}".format(*dev_name[4:].split('_'))
6101                 chosen_mdev = nova.privsep.libvirt.create_mdev(pci_addr,
6102                                                                asked_type,
6103                                                                uuid=uuid)
6104                 return chosen_mdev
6105 
6106     @utils.synchronized(VGPU_RESOURCE_SEMAPHORE)
6107     def _allocate_mdevs(self, allocations):
6108         """Returns a list of mediated device UUIDs corresponding to available
6109         resources we can assign to the guest(s) corresponding to the allocation
6110         requests passed as argument.
6111 
6112         That method can either find an existing but unassigned mediated device
6113         it can allocate, or create a new mediated device from a capable
6114         physical device if the latter has enough left capacity.
6115 
6116         :param allocations: Information about resources allocated to the
6117                             instance via placement, of the form returned by
6118                             SchedulerReportClient.get_allocations_for_consumer.
6119                             That code is supporting Placement API version 1.12
6120         """
6121         vgpu_allocations = self._vgpu_allocations(allocations)
6122         if not vgpu_allocations:
6123             return
6124         # TODO(sbauza): Once we have nested resource providers, find which one
6125         # is having the related allocation for the specific VGPU type.
6126         # For the moment, we should only have one allocation for
6127         # ResourceProvider.
6128         # TODO(sbauza): Iterate over all the allocations once we have
6129         # nested Resource Providers. For the moment, just take the first.
6130         if len(vgpu_allocations) > 1:
6131             LOG.warning('More than one allocation was passed over to libvirt '
6132                         'while at the moment libvirt only supports one. Only '
6133                         'the first allocation will be looked up.')
6134         alloc = six.next(six.itervalues(vgpu_allocations))
6135         vgpus_asked = alloc['resources'][rc_fields.ResourceClass.VGPU]
6136 
6137         requested_types = self._get_supported_vgpu_types()
6138         # Which mediated devices are created but not assigned to a guest ?
6139         mdevs_available = self._get_existing_mdevs_not_assigned(
6140             requested_types)
6141 
6142         chosen_mdevs = []
6143         for c in six.moves.range(vgpus_asked):
6144             chosen_mdev = None
6145             if mdevs_available:
6146                 # Take the first available mdev
6147                 chosen_mdev = mdevs_available.pop()
6148             else:
6149                 chosen_mdev = self._create_new_mediated_device(requested_types)
6150             if not chosen_mdev:
6151                 # If we can't find devices having available VGPUs, just raise
6152                 raise exception.ComputeResourcesUnavailable(
6153                     reason='vGPU resource is not available')
6154             else:
6155                 chosen_mdevs.append(chosen_mdev)
6156         return chosen_mdevs
6157 
6158     def _detach_mediated_devices(self, guest):
6159         mdevs = guest.get_all_devices(
6160             devtype=vconfig.LibvirtConfigGuestHostdevMDEV)
6161         for mdev_cfg in mdevs:
6162             try:
6163                 guest.detach_device(mdev_cfg, live=True)
6164             except libvirt.libvirtError as ex:
6165                 error_code = ex.get_error_code()
6166                 # NOTE(sbauza): There is a pending issue with libvirt that
6167                 # doesn't allow to hot-unplug mediated devices. Let's
6168                 # short-circuit the suspend action and set the instance back
6169                 # to ACTIVE.
6170                 # TODO(sbauza): Once libvirt supports this, amend the resume()
6171                 # operation to support reallocating mediated devices.
6172                 if error_code == libvirt.VIR_ERR_CONFIG_UNSUPPORTED:
6173                     reason = _("Suspend is not supported for instances having "
6174                                "attached vGPUs.")
6175                     raise exception.InstanceFaultRollback(
6176                         exception.InstanceSuspendFailure(reason=reason))
6177                 else:
6178                     raise
6179 
6180     def _has_numa_support(self):
6181         # This means that the host can support LibvirtConfigGuestNUMATune
6182         # and the nodeset field in LibvirtConfigGuestMemoryBackingPage
6183         caps = self._host.get_capabilities()
6184 
6185         if (caps.host.cpu.arch in (fields.Architecture.I686,
6186                                    fields.Architecture.X86_64,
6187                                    fields.Architecture.AARCH64) and
6188                 self._host.has_min_version(hv_type=host.HV_DRIVER_QEMU)):
6189             return True
6190         elif (caps.host.cpu.arch in (fields.Architecture.PPC64,
6191                                      fields.Architecture.PPC64LE)):
6192             return True
6193 
6194         return False
6195 
6196     def _get_host_numa_topology(self):
6197         if not self._has_numa_support():
6198             return
6199 
6200         caps = self._host.get_capabilities()
6201         topology = caps.host.topology
6202 
6203         if topology is None or not topology.cells:
6204             return
6205 
6206         cells = []
6207         allowed_cpus = hardware.get_vcpu_pin_set()
6208         online_cpus = self._host.get_online_cpus()
6209         if allowed_cpus:
6210             allowed_cpus &= online_cpus
6211         else:
6212             allowed_cpus = online_cpus
6213 
6214         def _get_reserved_memory_for_cell(self, cell_id, page_size):
6215             cell = self._reserved_hugepages.get(cell_id, {})
6216             return cell.get(page_size, 0)
6217 
6218         for cell in topology.cells:
6219             cpuset = set(cpu.id for cpu in cell.cpus)
6220             siblings = sorted(map(set,
6221                                   set(tuple(cpu.siblings)
6222                                         if cpu.siblings else ()
6223                                       for cpu in cell.cpus)
6224                                   ))
6225             cpuset &= allowed_cpus
6226             siblings = [sib & allowed_cpus for sib in siblings]
6227             # Filter out empty sibling sets that may be left
6228             siblings = [sib for sib in siblings if len(sib) > 0]
6229 
6230             mempages = [
6231                 objects.NUMAPagesTopology(
6232                     size_kb=pages.size,
6233                     total=pages.total,
6234                     used=0,
6235                     reserved=_get_reserved_memory_for_cell(
6236                         self, cell.id, pages.size))
6237                 for pages in cell.mempages]
6238 
6239             cell = objects.NUMACell(id=cell.id, cpuset=cpuset,
6240                                     memory=cell.memory / units.Ki,
6241                                     cpu_usage=0, memory_usage=0,
6242                                     siblings=siblings,
6243                                     pinned_cpus=set([]),
6244                                     mempages=mempages)
6245             cells.append(cell)
6246 
6247         return objects.NUMATopology(cells=cells)
6248 
6249     def get_all_volume_usage(self, context, compute_host_bdms):
6250         """Return usage info for volumes attached to vms on
6251            a given host.
6252         """
6253         vol_usage = []
6254 
6255         for instance_bdms in compute_host_bdms:
6256             instance = instance_bdms['instance']
6257 
6258             for bdm in instance_bdms['instance_bdms']:
6259                 mountpoint = bdm['device_name']
6260                 if mountpoint.startswith('/dev/'):
6261                     mountpoint = mountpoint[5:]
6262                 volume_id = bdm['volume_id']
6263 
6264                 LOG.debug("Trying to get stats for the volume %s",
6265                           volume_id, instance=instance)
6266                 vol_stats = self.block_stats(instance, mountpoint)
6267 
6268                 if vol_stats:
6269                     stats = dict(volume=volume_id,
6270                                  instance=instance,
6271                                  rd_req=vol_stats[0],
6272                                  rd_bytes=vol_stats[1],
6273                                  wr_req=vol_stats[2],
6274                                  wr_bytes=vol_stats[3])
6275                     LOG.debug(
6276                         "Got volume usage stats for the volume=%(volume)s,"
6277                         " rd_req=%(rd_req)d, rd_bytes=%(rd_bytes)d, "
6278                         "wr_req=%(wr_req)d, wr_bytes=%(wr_bytes)d",
6279                         stats, instance=instance)
6280                     vol_usage.append(stats)
6281 
6282         return vol_usage
6283 
6284     def block_stats(self, instance, disk_id):
6285         """Note that this function takes an instance name."""
6286         try:
6287             guest = self._host.get_guest(instance)
6288 
6289             # TODO(sahid): We are converting all calls from a
6290             # virDomain object to use nova.virt.libvirt.Guest.
6291             # We should be able to remove domain at the end.
6292             domain = guest._domain
6293             return domain.blockStats(disk_id)
6294         except libvirt.libvirtError as e:
6295             errcode = e.get_error_code()
6296             LOG.info('Getting block stats failed, device might have '
6297                      'been detached. Instance=%(instance_name)s '
6298                      'Disk=%(disk)s Code=%(errcode)s Error=%(e)s',
6299                      {'instance_name': instance.name, 'disk': disk_id,
6300                       'errcode': errcode, 'e': e},
6301                      instance=instance)
6302         except exception.InstanceNotFound:
6303             LOG.info('Could not find domain in libvirt for instance %s. '
6304                      'Cannot get block stats for device', instance.name,
6305                      instance=instance)
6306 
6307     def get_console_pool_info(self, console_type):
6308         # TODO(mdragon): console proxy should be implemented for libvirt,
6309         #                in case someone wants to use it with kvm or
6310         #                such. For now return fake data.
6311         return {'address': '127.0.0.1',
6312                 'username': 'fakeuser',
6313                 'password': 'fakepassword'}
6314 
6315     def refresh_security_group_rules(self, security_group_id):
6316         self.firewall_driver.refresh_security_group_rules(security_group_id)
6317 
6318     def refresh_instance_security_rules(self, instance):
6319         self.firewall_driver.refresh_instance_security_rules(instance)
6320 
6321     def update_provider_tree(self, provider_tree, nodename):
6322         """Update a ProviderTree object with current resource provider and
6323         inventory information.
6324 
6325         :param nova.compute.provider_tree.ProviderTree provider_tree:
6326             A nova.compute.provider_tree.ProviderTree object representing all
6327             the providers in the tree associated with the compute node, and any
6328             sharing providers (those with the ``MISC_SHARES_VIA_AGGREGATE``
6329             trait) associated via aggregate with any of those providers (but
6330             not *their* tree- or aggregate-associated providers), as currently
6331             known by placement.
6332 
6333         :param nodename:
6334             String name of the compute node (i.e.
6335             ComputeNode.hypervisor_hostname) for which the caller is requesting
6336             updated provider information.
6337         """
6338         disk_gb = int(self._get_local_gb_info()['total'])
6339         memory_mb = int(self._host.get_memory_mb_total())
6340         vcpus = self._get_vcpu_total()
6341 
6342         # NOTE(sbauza): For the moment, the libvirt driver only supports
6343         # providing the total number of virtual GPUs for a single GPU type. If
6344         # you have multiple physical GPUs, each of them providing multiple GPU
6345         # types, libvirt will return the total sum of virtual GPUs
6346         # corresponding to the single type passed in enabled_vgpu_types
6347         # configuration option. Eg. if you have 2 pGPUs supporting 'nvidia-35',
6348         # each of them having 16 available instances, the total here will be
6349         # 32.
6350         # If one of the 2 pGPUs doesn't support 'nvidia-35', it won't be used.
6351         # TODO(sbauza): Use traits to make a better world.
6352         vgpus = self._get_vgpu_total()
6353 
6354         # NOTE(jaypipes): We leave some fields like allocation_ratio and
6355         # reserved out of the returned dicts here because, for now at least,
6356         # the RT injects those values into the inventory dict based on the
6357         # compute_nodes record values.
6358         result = {
6359             rc_fields.ResourceClass.VCPU: {
6360                 'total': vcpus,
6361                 'min_unit': 1,
6362                 'max_unit': vcpus,
6363                 'step_size': 1,
6364             },
6365             rc_fields.ResourceClass.MEMORY_MB: {
6366                 'total': memory_mb,
6367                 'min_unit': 1,
6368                 'max_unit': memory_mb,
6369                 'step_size': 1,
6370             },
6371         }
6372 
6373         # If a sharing DISK_GB provider exists in the provider tree, then our
6374         # storage is shared, and we should not report the DISK_GB inventory in
6375         # the compute node provider.
6376         if not provider_tree.has_sharing_provider(
6377                 rc_fields.ResourceClass.DISK_GB):
6378             result[rc_fields.ResourceClass.DISK_GB] = {
6379                 'total': disk_gb,
6380                 'min_unit': 1,
6381                 'max_unit': disk_gb,
6382                 'step_size': 1,
6383             }
6384 
6385         if vgpus > 0:
6386             # Only provide VGPU resource classes if the driver supports it.
6387             result[rc_fields.ResourceClass.VGPU] = {
6388                 'total': vgpus,
6389                 'min_unit': 1,
6390                 'max_unit': vgpus,
6391                 'step_size': 1,
6392                 }
6393 
6394         provider_tree.update_inventory(nodename, result)
6395 
6396     def get_available_resource(self, nodename):
6397         """Retrieve resource information.
6398 
6399         This method is called when nova-compute launches, and
6400         as part of a periodic task that records the results in the DB.
6401 
6402         :param nodename: unused in this driver
6403         :returns: dictionary containing resource info
6404         """
6405 
6406         disk_info_dict = self._get_local_gb_info()
6407         data = {}
6408 
6409         # NOTE(dprince): calling capabilities before getVersion works around
6410         # an initialization issue with some versions of Libvirt (1.0.5.5).
6411         # See: https://bugzilla.redhat.com/show_bug.cgi?id=1000116
6412         # See: https://bugs.launchpad.net/nova/+bug/1215593
6413         data["supported_instances"] = self._get_instance_capabilities()
6414 
6415         data["vcpus"] = self._get_vcpu_total()
6416         data["memory_mb"] = self._host.get_memory_mb_total()
6417         data["local_gb"] = disk_info_dict['total']
6418         data["vcpus_used"] = self._get_vcpu_used()
6419         data["memory_mb_used"] = self._host.get_memory_mb_used()
6420         data["local_gb_used"] = disk_info_dict['used']
6421         data["hypervisor_type"] = self._host.get_driver_type()
6422         data["hypervisor_version"] = self._host.get_version()
6423         data["hypervisor_hostname"] = self._host.get_hostname()
6424         # TODO(berrange): why do we bother converting the
6425         # libvirt capabilities XML into a special JSON format ?
6426         # The data format is different across all the drivers
6427         # so we could just return the raw capabilities XML
6428         # which 'compare_cpu' could use directly
6429         #
6430         # That said, arch_filter.py now seems to rely on
6431         # the libvirt drivers format which suggests this
6432         # data format needs to be standardized across drivers
6433         data["cpu_info"] = jsonutils.dumps(self._get_cpu_info())
6434 
6435         disk_free_gb = disk_info_dict['free']
6436         disk_over_committed = self._get_disk_over_committed_size_total()
6437         available_least = disk_free_gb * units.Gi - disk_over_committed
6438         data['disk_available_least'] = available_least / units.Gi
6439 
6440         data['pci_passthrough_devices'] = \
6441             self._get_pci_passthrough_devices()
6442 
6443         numa_topology = self._get_host_numa_topology()
6444         if numa_topology:
6445             data['numa_topology'] = numa_topology._to_json()
6446         else:
6447             data['numa_topology'] = None
6448 
6449         return data
6450 
6451     def check_instance_shared_storage_local(self, context, instance):
6452         """Check if instance files located on shared storage.
6453 
6454         This runs check on the destination host, and then calls
6455         back to the source host to check the results.
6456 
6457         :param context: security context
6458         :param instance: nova.objects.instance.Instance object
6459         :returns:
6460          - tempfile: A dict containing the tempfile info on the destination
6461                      host
6462          - None:
6463 
6464             1. If the instance path is not existing.
6465             2. If the image backend is shared block storage type.
6466         """
6467         if self.image_backend.backend().is_shared_block_storage():
6468             return None
6469 
6470         dirpath = libvirt_utils.get_instance_path(instance)
6471 
6472         if not os.path.exists(dirpath):
6473             return None
6474 
6475         fd, tmp_file = tempfile.mkstemp(dir=dirpath)
6476         LOG.debug("Creating tmpfile %s to verify with other "
6477                   "compute node that the instance is on "
6478                   "the same shared storage.",
6479                   tmp_file, instance=instance)
6480         os.close(fd)
6481         return {"filename": tmp_file}
6482 
6483     def check_instance_shared_storage_remote(self, context, data):
6484         return os.path.exists(data['filename'])
6485 
6486     def check_instance_shared_storage_cleanup(self, context, data):
6487         fileutils.delete_if_exists(data["filename"])
6488 
6489     def check_can_live_migrate_destination(self, context, instance,
6490                                            src_compute_info, dst_compute_info,
6491                                            block_migration=False,
6492                                            disk_over_commit=False):
6493         """Check if it is possible to execute live migration.
6494 
6495         This runs checks on the destination host, and then calls
6496         back to the source host to check the results.
6497 
6498         :param context: security context
6499         :param instance: nova.db.sqlalchemy.models.Instance
6500         :param block_migration: if true, prepare for block migration
6501         :param disk_over_commit: if true, allow disk over commit
6502         :returns: a LibvirtLiveMigrateData object
6503         """
6504         if disk_over_commit:
6505             disk_available_gb = dst_compute_info['local_gb']
6506         else:
6507             disk_available_gb = dst_compute_info['disk_available_least']
6508         disk_available_mb = (
6509             (disk_available_gb * units.Ki) - CONF.reserved_host_disk_mb)
6510 
6511         # Compare CPU
6512         if not instance.vcpu_model or not instance.vcpu_model.model:
6513             source_cpu_info = src_compute_info['cpu_info']
6514             self._compare_cpu(None, source_cpu_info, instance)
6515         else:
6516             self._compare_cpu(instance.vcpu_model, None, instance)
6517 
6518         # Create file on storage, to be checked on source host
6519         filename = self._create_shared_storage_test_file(instance)
6520 
6521         data = objects.LibvirtLiveMigrateData()
6522         data.filename = filename
6523         data.image_type = CONF.libvirt.images_type
6524         data.graphics_listen_addr_vnc = CONF.vnc.server_listen
6525         data.graphics_listen_addr_spice = CONF.spice.server_listen
6526         if CONF.serial_console.enabled:
6527             data.serial_listen_addr = CONF.serial_console.proxyclient_address
6528         else:
6529             data.serial_listen_addr = None
6530         # Notes(eliqiao): block_migration and disk_over_commit are not
6531         # nullable, so just don't set them if they are None
6532         if block_migration is not None:
6533             data.block_migration = block_migration
6534         if disk_over_commit is not None:
6535             data.disk_over_commit = disk_over_commit
6536         data.disk_available_mb = disk_available_mb
6537 
6538         # Update instance's NUMA topology, if it has one, to fit on this
6539         # host's NUMA topology. Raise if we fail.
6540         if instance.numa_topology:
6541             new_topology = hardware.numa_fit_instance_to_host(
6542                 self._get_host_numa_topology(), instance.numa_topology)
6543             if new_topology:
6544                 data.instance_numa_topology = new_topology
6545             else:
6546                 msg = _('Unable to fit instance NUMA topology to our '
6547                         'host topology')
6548                 LOG.error(msg, instance=instance)
6549                 raise exception.MigrationPreCheckError(reason=msg)
6550 
6551         return data
6552 
6553     def cleanup_live_migration_destination_check(self, context,
6554                                                  dest_check_data):
6555         """Do required cleanup on dest host after check_can_live_migrate calls
6556 
6557         :param context: security context
6558         """
6559         filename = dest_check_data.filename
6560         self._cleanup_shared_storage_test_file(filename)
6561 
6562     def check_can_live_migrate_source(self, context, instance,
6563                                       dest_check_data,
6564                                       block_device_info=None):
6565         """Check if it is possible to execute live migration.
6566 
6567         This checks if the live migration can succeed, based on the
6568         results from check_can_live_migrate_destination.
6569 
6570         :param context: security context
6571         :param instance: nova.db.sqlalchemy.models.Instance
6572         :param dest_check_data: result of check_can_live_migrate_destination
6573         :param block_device_info: result of _get_instance_block_device_info
6574         :returns: a LibvirtLiveMigrateData object
6575         """
6576         if not isinstance(dest_check_data, migrate_data_obj.LiveMigrateData):
6577             md_obj = objects.LibvirtLiveMigrateData()
6578             md_obj.from_legacy_dict(dest_check_data)
6579             dest_check_data = md_obj
6580 
6581         # Checking shared storage connectivity
6582         # if block migration, instances_path should not be on shared storage.
6583         source = CONF.host
6584 
6585         dest_check_data.is_shared_instance_path = (
6586             self._check_shared_storage_test_file(
6587                 dest_check_data.filename, instance))
6588 
6589         dest_check_data.is_shared_block_storage = (
6590             self._is_shared_block_storage(instance, dest_check_data,
6591                                           block_device_info))
6592 
6593         if 'block_migration' not in dest_check_data:
6594             dest_check_data.block_migration = (
6595                 not dest_check_data.is_on_shared_storage())
6596 
6597         if dest_check_data.block_migration:
6598             # TODO(eliqiao): Once block_migration flag is removed from the API
6599             # we can safely remove the if condition
6600             if dest_check_data.is_on_shared_storage():
6601                 reason = _("Block migration can not be used "
6602                            "with shared storage.")
6603                 raise exception.InvalidLocalStorage(reason=reason, path=source)
6604             if 'disk_over_commit' in dest_check_data:
6605                 self._assert_dest_node_has_enough_disk(context, instance,
6606                                         dest_check_data.disk_available_mb,
6607                                         dest_check_data.disk_over_commit,
6608                                         block_device_info)
6609             if block_device_info:
6610                 bdm = block_device_info.get('block_device_mapping')
6611                 # NOTE(eliqiao): Selective disk migrations are not supported
6612                 # with tunnelled block migrations so we can block them early.
6613                 if (bdm and
6614                     (self._block_migration_flags &
6615                      libvirt.VIR_MIGRATE_TUNNELLED != 0)):
6616                     msg = (_('Cannot block migrate instance %(uuid)s with'
6617                              ' mapped volumes. Selective block device'
6618                              ' migration is not supported with tunnelled'
6619                              ' block migrations.') % {'uuid': instance.uuid})
6620                     LOG.error(msg, instance=instance)
6621                     raise exception.MigrationPreCheckError(reason=msg)
6622         elif not (dest_check_data.is_shared_block_storage or
6623                   dest_check_data.is_shared_instance_path):
6624             reason = _("Shared storage live-migration requires either shared "
6625                        "storage or boot-from-volume with no local disks.")
6626             raise exception.InvalidSharedStorage(reason=reason, path=source)
6627 
6628         # NOTE(mikal): include the instance directory name here because it
6629         # doesn't yet exist on the destination but we want to force that
6630         # same name to be used
6631         instance_path = libvirt_utils.get_instance_path(instance,
6632                                                         relative=True)
6633         dest_check_data.instance_relative_path = instance_path
6634 
6635         # NOTE(lyarwood): Used to indicate to the dest that the src is capable
6636         # of wiring up the encrypted disk configuration for the domain.
6637         # Note that this does not require the QEMU and Libvirt versions to
6638         # decrypt LUKS to be installed on the source node. Only the Nova
6639         # utility code to generate the correct XML is required, so we can
6640         # default to True here for all computes >= Queens.
6641         dest_check_data.src_supports_native_luks = True
6642 
6643         return dest_check_data
6644 
6645     def _is_shared_block_storage(self, instance, dest_check_data,
6646                                  block_device_info=None):
6647         """Check if all block storage of an instance can be shared
6648         between source and destination of a live migration.
6649 
6650         Returns true if the instance is volume backed and has no local disks,
6651         or if the image backend is the same on source and destination and the
6652         backend shares block storage between compute nodes.
6653 
6654         :param instance: nova.objects.instance.Instance object
6655         :param dest_check_data: dict with boolean fields image_type,
6656                                 is_shared_instance_path, and is_volume_backed
6657         """
6658         if (dest_check_data.obj_attr_is_set('image_type') and
6659                 CONF.libvirt.images_type == dest_check_data.image_type and
6660                 self.image_backend.backend().is_shared_block_storage()):
6661             # NOTE(dgenin): currently true only for RBD image backend
6662             return True
6663 
6664         if (dest_check_data.is_shared_instance_path and
6665                 self.image_backend.backend().is_file_in_instance_path()):
6666             # NOTE(angdraug): file based image backends (Flat, Qcow2)
6667             # place block device files under the instance path
6668             return True
6669 
6670         if (dest_check_data.is_volume_backed and
6671                 not bool(self._get_instance_disk_info(instance,
6672                                                       block_device_info))):
6673             return True
6674 
6675         return False
6676 
6677     def _assert_dest_node_has_enough_disk(self, context, instance,
6678                                              available_mb, disk_over_commit,
6679                                              block_device_info):
6680         """Checks if destination has enough disk for block migration."""
6681         # Libvirt supports qcow2 disk format,which is usually compressed
6682         # on compute nodes.
6683         # Real disk image (compressed) may enlarged to "virtual disk size",
6684         # that is specified as the maximum disk size.
6685         # (See qemu-img -f path-to-disk)
6686         # Scheduler recognizes destination host still has enough disk space
6687         # if real disk size < available disk size
6688         # if disk_over_commit is True,
6689         #  otherwise virtual disk size < available disk size.
6690 
6691         available = 0
6692         if available_mb:
6693             available = available_mb * units.Mi
6694 
6695         disk_infos = self._get_instance_disk_info(instance, block_device_info)
6696 
6697         necessary = 0
6698         if disk_over_commit:
6699             for info in disk_infos:
6700                 necessary += int(info['disk_size'])
6701         else:
6702             for info in disk_infos:
6703                 necessary += int(info['virt_disk_size'])
6704 
6705         # Check that available disk > necessary disk
6706         if (available - necessary) < 0:
6707             reason = (_('Unable to migrate %(instance_uuid)s: '
6708                         'Disk of instance is too large(available'
6709                         ' on destination host:%(available)s '
6710                         '< need:%(necessary)s)') %
6711                       {'instance_uuid': instance.uuid,
6712                        'available': available,
6713                        'necessary': necessary})
6714             raise exception.MigrationPreCheckError(reason=reason)
6715 
6716     def _compare_cpu(self, guest_cpu, host_cpu_str, instance):
6717         """Check the host is compatible with the requested CPU
6718 
6719         :param guest_cpu: nova.objects.VirtCPUModel or None
6720         :param host_cpu_str: JSON from _get_cpu_info() method
6721 
6722         If the 'guest_cpu' parameter is not None, this will be
6723         validated for migration compatibility with the host.
6724         Otherwise the 'host_cpu_str' JSON string will be used for
6725         validation.
6726 
6727         :returns:
6728             None. if given cpu info is not compatible to this server,
6729             raise exception.
6730         """
6731 
6732         # NOTE(kchamart): Comparing host to guest CPU model for emulated
6733         # guests (<domain type='qemu'>) should not matter -- in this
6734         # mode (QEMU "TCG") the CPU is fully emulated in software and no
6735         # hardware acceleration, like KVM, is involved. So, skip the CPU
6736         # compatibility check for the QEMU domain type, and retain it for
6737         # KVM guests.
6738         if CONF.libvirt.virt_type not in ['kvm']:
6739             return
6740 
6741         if guest_cpu is None:
6742             info = jsonutils.loads(host_cpu_str)
6743             LOG.info('Instance launched has CPU info: %s', host_cpu_str)
6744             cpu = vconfig.LibvirtConfigCPU()
6745             cpu.arch = info['arch']
6746             cpu.model = info['model']
6747             cpu.vendor = info['vendor']
6748             cpu.sockets = info['topology']['sockets']
6749             cpu.cores = info['topology']['cores']
6750             cpu.threads = info['topology']['threads']
6751             for f in info['features']:
6752                 cpu.add_feature(vconfig.LibvirtConfigCPUFeature(f))
6753         else:
6754             cpu = self._vcpu_model_to_cpu_config(guest_cpu)
6755 
6756         u = ("http://libvirt.org/html/libvirt-libvirt-host.html#"
6757              "virCPUCompareResult")
6758         m = _("CPU doesn't have compatibility.\n\n%(ret)s\n\nRefer to %(u)s")
6759         # unknown character exists in xml, then libvirt complains
6760         try:
6761             cpu_xml = cpu.to_xml()
6762             LOG.debug("cpu compare xml: %s", cpu_xml, instance=instance)
6763             ret = self._host.compare_cpu(cpu_xml)
6764         except libvirt.libvirtError as e:
6765             error_code = e.get_error_code()
6766             if error_code == libvirt.VIR_ERR_NO_SUPPORT:
6767                 LOG.debug("URI %(uri)s does not support cpu comparison. "
6768                           "It will be proceeded though. Error: %(error)s",
6769                           {'uri': self._uri(), 'error': e})
6770                 return
6771             else:
6772                 LOG.error(m, {'ret': e, 'u': u})
6773                 raise exception.MigrationPreCheckError(
6774                     reason=m % {'ret': e, 'u': u})
6775 
6776         if ret <= 0:
6777             LOG.error(m, {'ret': ret, 'u': u})
6778             raise exception.InvalidCPUInfo(reason=m % {'ret': ret, 'u': u})
6779 
6780     def _create_shared_storage_test_file(self, instance):
6781         """Makes tmpfile under CONF.instances_path."""
6782         dirpath = CONF.instances_path
6783         fd, tmp_file = tempfile.mkstemp(dir=dirpath)
6784         LOG.debug("Creating tmpfile %s to notify to other "
6785                   "compute nodes that they should mount "
6786                   "the same storage.", tmp_file, instance=instance)
6787         os.close(fd)
6788         return os.path.basename(tmp_file)
6789 
6790     def _check_shared_storage_test_file(self, filename, instance):
6791         """Confirms existence of the tmpfile under CONF.instances_path.
6792 
6793         Cannot confirm tmpfile return False.
6794         """
6795         # NOTE(tpatzig): if instances_path is a shared volume that is
6796         # under heavy IO (many instances on many compute nodes),
6797         # then checking the existence of the testfile fails,
6798         # just because it takes longer until the client refreshes and new
6799         # content gets visible.
6800         # os.utime (like touch) on the directory forces the client to refresh.
6801         os.utime(CONF.instances_path, None)
6802 
6803         tmp_file = os.path.join(CONF.instances_path, filename)
6804         if not os.path.exists(tmp_file):
6805             exists = False
6806         else:
6807             exists = True
6808         LOG.debug('Check if temp file %s exists to indicate shared storage '
6809                   'is being used for migration. Exists? %s', tmp_file, exists,
6810                   instance=instance)
6811         return exists
6812 
6813     def _cleanup_shared_storage_test_file(self, filename):
6814         """Removes existence of the tmpfile under CONF.instances_path."""
6815         tmp_file = os.path.join(CONF.instances_path, filename)
6816         os.remove(tmp_file)
6817 
6818     def ensure_filtering_rules_for_instance(self, instance, network_info):
6819         """Ensure that an instance's filtering rules are enabled.
6820 
6821         When migrating an instance, we need the filtering rules to
6822         be configured on the destination host before starting the
6823         migration.
6824 
6825         Also, when restarting the compute service, we need to ensure
6826         that filtering rules exist for all running services.
6827         """
6828 
6829         self.firewall_driver.setup_basic_filtering(instance, network_info)
6830         self.firewall_driver.prepare_instance_filter(instance,
6831                 network_info)
6832 
6833         # nwfilters may be defined in a separate thread in the case
6834         # of libvirt non-blocking mode, so we wait for completion
6835         timeout_count = list(range(CONF.live_migration_retry_count))
6836         while timeout_count:
6837             if self.firewall_driver.instance_filter_exists(instance,
6838                                                            network_info):
6839                 break
6840             timeout_count.pop()
6841             if len(timeout_count) == 0:
6842                 msg = _('The firewall filter for %s does not exist')
6843                 raise exception.InternalError(msg % instance.name)
6844             greenthread.sleep(1)
6845 
6846     def filter_defer_apply_on(self):
6847         self.firewall_driver.filter_defer_apply_on()
6848 
6849     def filter_defer_apply_off(self):
6850         self.firewall_driver.filter_defer_apply_off()
6851 
6852     def live_migration(self, context, instance, dest,
6853                        post_method, recover_method, block_migration=False,
6854                        migrate_data=None):
6855         """Spawning live_migration operation for distributing high-load.
6856 
6857         :param context: security context
6858         :param instance:
6859             nova.db.sqlalchemy.models.Instance object
6860             instance object that is migrated.
6861         :param dest: destination host
6862         :param post_method:
6863             post operation method.
6864             expected nova.compute.manager._post_live_migration.
6865         :param recover_method:
6866             recovery method when any exception occurs.
6867             expected nova.compute.manager._rollback_live_migration.
6868         :param block_migration: if true, do block migration.
6869         :param migrate_data: a LibvirtLiveMigrateData object
6870 
6871         """
6872 
6873         # 'dest' will be substituted into 'migration_uri' so ensure
6874         # it does't contain any characters that could be used to
6875         # exploit the URI accepted by libivrt
6876         if not libvirt_utils.is_valid_hostname(dest):
6877             raise exception.InvalidHostname(hostname=dest)
6878 
6879         self._live_migration(context, instance, dest,
6880                              post_method, recover_method, block_migration,
6881                              migrate_data)
6882 
6883     def live_migration_abort(self, instance):
6884         """Aborting a running live-migration.
6885 
6886         :param instance: instance object that is in migration
6887 
6888         """
6889 
6890         guest = self._host.get_guest(instance)
6891         dom = guest._domain
6892 
6893         try:
6894             dom.abortJob()
6895         except libvirt.libvirtError as e:
6896             LOG.error("Failed to cancel migration %s",
6897                     encodeutils.exception_to_unicode(e), instance=instance)
6898             raise
6899 
6900     def _verify_serial_console_is_disabled(self):
6901         if CONF.serial_console.enabled:
6902 
6903             msg = _('Your destination node does not support'
6904                     ' retrieving listen addresses.  In order'
6905                     ' for live migration to work properly you'
6906                     ' must disable serial console.')
6907             raise exception.MigrationError(reason=msg)
6908 
6909     def _live_migration_operation(self, context, instance, dest,
6910                                   block_migration, migrate_data, guest,
6911                                   device_names, bandwidth):
6912         """Invoke the live migration operation
6913 
6914         :param context: security context
6915         :param instance:
6916             nova.db.sqlalchemy.models.Instance object
6917             instance object that is migrated.
6918         :param dest: destination host
6919         :param block_migration: if true, do block migration.
6920         :param migrate_data: a LibvirtLiveMigrateData object
6921         :param guest: the guest domain object
6922         :param device_names: list of device names that are being migrated with
6923             instance
6924         :param bandwidth: MiB/s of bandwidth allowed for the migration at start
6925 
6926         This method is intended to be run in a background thread and will
6927         block that thread until the migration is finished or failed.
6928         """
6929         try:
6930             if migrate_data.block_migration:
6931                 migration_flags = self._block_migration_flags
6932             else:
6933                 migration_flags = self._live_migration_flags
6934 
6935             serial_listen_addr = libvirt_migrate.serial_listen_addr(
6936                 migrate_data)
6937             if not serial_listen_addr:
6938                 # In this context we want to ensure that serial console is
6939                 # disabled on source node. This is because nova couldn't
6940                 # retrieve serial listen address from destination node, so we
6941                 # consider that destination node might have serial console
6942                 # disabled as well.
6943                 self._verify_serial_console_is_disabled()
6944 
6945             # NOTE(aplanas) migrate_uri will have a value only in the
6946             # case that `live_migration_inbound_addr` parameter is
6947             # set, and we propose a non tunneled migration.
6948             migrate_uri = None
6949             if ('target_connect_addr' in migrate_data and
6950                     migrate_data.target_connect_addr is not None):
6951                 dest = migrate_data.target_connect_addr
6952                 if (migration_flags &
6953                     libvirt.VIR_MIGRATE_TUNNELLED == 0):
6954                     migrate_uri = self._migrate_uri(dest)
6955 
6956             new_xml_str = None
6957             if CONF.libvirt.virt_type != "parallels":
6958                 new_xml_str = libvirt_migrate.get_updated_guest_xml(
6959                     # TODO(sahid): It's not a really good idea to pass
6960                     # the method _get_volume_config and we should to find
6961                     # a way to avoid this in future.
6962                     guest, migrate_data, self._get_volume_config)
6963             params = {
6964                'destination_xml': new_xml_str,
6965                'migrate_disks': device_names,
6966             }
6967             # NOTE(pkoniszewski): Because of precheck which blocks
6968             # tunnelled block live migration with mapped volumes we
6969             # can safely remove migrate_disks when tunnelling is on.
6970             # Otherwise we will block all tunnelled block migrations,
6971             # even when an instance does not have volumes mapped.
6972             # This is because selective disk migration is not
6973             # supported in tunnelled block live migration. Also we
6974             # cannot fallback to migrateToURI2 in this case because of
6975             # bug #1398999
6976             if (migration_flags & libvirt.VIR_MIGRATE_TUNNELLED != 0):
6977                 params.pop('migrate_disks')
6978 
6979             # TODO(sahid): This should be in
6980             # post_live_migration_at_source but no way to retrieve
6981             # ports acquired on the host for the guest at this
6982             # step. Since the domain is going to be removed from
6983             # libvird on source host after migration, we backup the
6984             # serial ports to release them if all went well.
6985             serial_ports = []
6986             if CONF.serial_console.enabled:
6987                 serial_ports = list(self._get_serial_ports_from_guest(guest))
6988 
6989             LOG.debug("About to invoke the migrate API", instance=instance)
6990             guest.migrate(self._live_migration_uri(dest),
6991                           migrate_uri=migrate_uri,
6992                           flags=migration_flags,
6993                           params=params,
6994                           domain_xml=new_xml_str,
6995                           bandwidth=bandwidth)
6996             LOG.debug("Migrate API has completed", instance=instance)
6997 
6998             for hostname, port in serial_ports:
6999                 serial_console.release_port(host=hostname, port=port)
7000         except Exception as e:
7001             with excutils.save_and_reraise_exception():
7002                 LOG.error("Live Migration failure: %s", e, instance=instance)
7003 
7004         # If 'migrateToURI' fails we don't know what state the
7005         # VM instances on each host are in. Possibilities include
7006         #
7007         #  1. src==running, dst==none
7008         #
7009         #     Migration failed & rolled back, or never started
7010         #
7011         #  2. src==running, dst==paused
7012         #
7013         #     Migration started but is still ongoing
7014         #
7015         #  3. src==paused,  dst==paused
7016         #
7017         #     Migration data transfer completed, but switchover
7018         #     is still ongoing, or failed
7019         #
7020         #  4. src==paused,  dst==running
7021         #
7022         #     Migration data transfer completed, switchover
7023         #     happened but cleanup on source failed
7024         #
7025         #  5. src==none,    dst==running
7026         #
7027         #     Migration fully succeeded.
7028         #
7029         # Libvirt will aim to complete any migration operation
7030         # or roll it back. So even if the migrateToURI call has
7031         # returned an error, if the migration was not finished
7032         # libvirt should clean up.
7033         #
7034         # So we take the error raise here with a pinch of salt
7035         # and rely on the domain job info status to figure out
7036         # what really happened to the VM, which is a much more
7037         # reliable indicator.
7038         #
7039         # In particular we need to try very hard to ensure that
7040         # Nova does not "forget" about the guest. ie leaving it
7041         # running on a different host to the one recorded in
7042         # the database, as that would be a serious resource leak
7043 
7044         LOG.debug("Migration operation thread has finished",
7045                   instance=instance)
7046 
7047     def _live_migration_copy_disk_paths(self, context, instance, guest):
7048         '''Get list of disks to copy during migration
7049 
7050         :param context: security context
7051         :param instance: the instance being migrated
7052         :param guest: the Guest instance being migrated
7053 
7054         Get the list of disks to copy during migration.
7055 
7056         :returns: a list of local source paths and a list of device names to
7057             copy
7058         '''
7059 
7060         disk_paths = []
7061         device_names = []
7062         block_devices = []
7063 
7064         if (self._block_migration_flags &
7065                 libvirt.VIR_MIGRATE_TUNNELLED == 0):
7066             bdm_list = objects.BlockDeviceMappingList.get_by_instance_uuid(
7067                 context, instance.uuid)
7068             block_device_info = driver.get_block_device_info(instance,
7069                                                              bdm_list)
7070 
7071             block_device_mappings = driver.block_device_info_get_mapping(
7072                 block_device_info)
7073             for bdm in block_device_mappings:
7074                 device_name = str(bdm['mount_device'].rsplit('/', 1)[1])
7075                 block_devices.append(device_name)
7076 
7077         for dev in guest.get_all_disks():
7078             if dev.readonly or dev.shareable:
7079                 continue
7080             if dev.source_type not in ["file", "block"]:
7081                 continue
7082             if dev.target_dev in block_devices:
7083                 continue
7084             disk_paths.append(dev.source_path)
7085             device_names.append(dev.target_dev)
7086         return (disk_paths, device_names)
7087 
7088     def _live_migration_data_gb(self, instance, disk_paths):
7089         '''Calculate total amount of data to be transferred
7090 
7091         :param instance: the nova.objects.Instance being migrated
7092         :param disk_paths: list of disk paths that are being migrated
7093         with instance
7094 
7095         Calculates the total amount of data that needs to be
7096         transferred during the live migration. The actual
7097         amount copied will be larger than this, due to the
7098         guest OS continuing to dirty RAM while the migration
7099         is taking place. So this value represents the minimal
7100         data size possible.
7101 
7102         :returns: data size to be copied in GB
7103         '''
7104 
7105         ram_gb = instance.flavor.memory_mb * units.Mi / units.Gi
7106         if ram_gb < 2:
7107             ram_gb = 2
7108 
7109         disk_gb = 0
7110         for path in disk_paths:
7111             try:
7112                 size = os.stat(path).st_size
7113                 size_gb = (size / units.Gi)
7114                 if size_gb < 2:
7115                     size_gb = 2
7116                 disk_gb += size_gb
7117             except OSError as e:
7118                 LOG.warning("Unable to stat %(disk)s: %(ex)s",
7119                             {'disk': path, 'ex': e})
7120                 # Ignore error since we don't want to break
7121                 # the migration monitoring thread operation
7122 
7123         return ram_gb + disk_gb
7124 
7125     def _get_migration_flags(self, is_block_migration):
7126         if is_block_migration:
7127             return self._block_migration_flags
7128         return self._live_migration_flags
7129 
7130     def _live_migration_monitor(self, context, instance, guest,
7131                                 dest, post_method,
7132                                 recover_method, block_migration,
7133                                 migrate_data, finish_event,
7134                                 disk_paths):
7135         on_migration_failure = deque()
7136         data_gb = self._live_migration_data_gb(instance, disk_paths)
7137         downtime_steps = list(libvirt_migrate.downtime_steps(data_gb))
7138         migration = migrate_data.migration
7139         curdowntime = None
7140 
7141         migration_flags = self._get_migration_flags(
7142                                   migrate_data.block_migration)
7143 
7144         n = 0
7145         start = time.time()
7146         progress_time = start
7147         progress_watermark = None
7148         previous_data_remaining = -1
7149         is_post_copy_enabled = self._is_post_copy_enabled(migration_flags)
7150         while True:
7151             info = guest.get_job_info()
7152 
7153             if info.type == libvirt.VIR_DOMAIN_JOB_NONE:
7154                 # Either still running, or failed or completed,
7155                 # lets untangle the mess
7156                 if not finish_event.ready():
7157                     LOG.debug("Operation thread is still running",
7158                               instance=instance)
7159                 else:
7160                     info.type = libvirt_migrate.find_job_type(guest, instance)
7161                     LOG.debug("Fixed incorrect job type to be %d",
7162                               info.type, instance=instance)
7163 
7164             if info.type == libvirt.VIR_DOMAIN_JOB_NONE:
7165                 # Migration is not yet started
7166                 LOG.debug("Migration not running yet",
7167                           instance=instance)
7168             elif info.type == libvirt.VIR_DOMAIN_JOB_UNBOUNDED:
7169                 # Migration is still running
7170                 #
7171                 # This is where we wire up calls to change live
7172                 # migration status. eg change max downtime, cancel
7173                 # the operation, change max bandwidth
7174                 libvirt_migrate.run_tasks(guest, instance,
7175                                           self.active_migrations,
7176                                           on_migration_failure,
7177                                           migration,
7178                                           is_post_copy_enabled)
7179 
7180                 now = time.time()
7181                 elapsed = now - start
7182 
7183                 if ((progress_watermark is None) or
7184                     (progress_watermark == 0) or
7185                     (progress_watermark > info.data_remaining)):
7186                     progress_watermark = info.data_remaining
7187                     progress_time = now
7188 
7189                 progress_timeout = CONF.libvirt.live_migration_progress_timeout
7190                 completion_timeout = int(
7191                     CONF.libvirt.live_migration_completion_timeout * data_gb)
7192                 if libvirt_migrate.should_abort(instance, now, progress_time,
7193                                                 progress_timeout, elapsed,
7194                                                 completion_timeout,
7195                                                 migration.status):
7196                     try:
7197                         guest.abort_job()
7198                     except libvirt.libvirtError as e:
7199                         LOG.warning("Failed to abort migration %s",
7200                                 encodeutils.exception_to_unicode(e),
7201                                 instance=instance)
7202                         self._clear_empty_migration(instance)
7203                         raise
7204 
7205                 if (is_post_copy_enabled and
7206                     libvirt_migrate.should_switch_to_postcopy(
7207                     info.memory_iteration, info.data_remaining,
7208                     previous_data_remaining, migration.status)):
7209                     libvirt_migrate.trigger_postcopy_switch(guest,
7210                                                             instance,
7211                                                             migration)
7212                 previous_data_remaining = info.data_remaining
7213 
7214                 curdowntime = libvirt_migrate.update_downtime(
7215                     guest, instance, curdowntime,
7216                     downtime_steps, elapsed)
7217 
7218                 # We loop every 500ms, so don't log on every
7219                 # iteration to avoid spamming logs for long
7220                 # running migrations. Just once every 5 secs
7221                 # is sufficient for developers to debug problems.
7222                 # We log once every 30 seconds at info to help
7223                 # admins see slow running migration operations
7224                 # when debug logs are off.
7225                 if (n % 10) == 0:
7226                     # Ignoring memory_processed, as due to repeated
7227                     # dirtying of data, this can be way larger than
7228                     # memory_total. Best to just look at what's
7229                     # remaining to copy and ignore what's done already
7230                     #
7231                     # TODO(berrange) perhaps we could include disk
7232                     # transfer stats in the progress too, but it
7233                     # might make memory info more obscure as large
7234                     # disk sizes might dwarf memory size
7235                     remaining = 100
7236                     if info.memory_total != 0:
7237                         remaining = round(info.memory_remaining *
7238                                           100 / info.memory_total)
7239 
7240                     libvirt_migrate.save_stats(instance, migration,
7241                                                info, remaining)
7242 
7243                     lg = LOG.debug
7244                     if (n % 60) == 0:
7245                         lg = LOG.info
7246 
7247                     lg("Migration running for %(secs)d secs, "
7248                        "memory %(remaining)d%% remaining; "
7249                        "(bytes processed=%(processed_memory)d, "
7250                        "remaining=%(remaining_memory)d, "
7251                        "total=%(total_memory)d)",
7252                        {"secs": n / 2, "remaining": remaining,
7253                         "processed_memory": info.memory_processed,
7254                         "remaining_memory": info.memory_remaining,
7255                         "total_memory": info.memory_total}, instance=instance)
7256                     if info.data_remaining > progress_watermark:
7257                         lg("Data remaining %(remaining)d bytes, "
7258                            "low watermark %(watermark)d bytes "
7259                            "%(last)d seconds ago",
7260                            {"remaining": info.data_remaining,
7261                             "watermark": progress_watermark,
7262                             "last": (now - progress_time)}, instance=instance)
7263 
7264                 n = n + 1
7265             elif info.type == libvirt.VIR_DOMAIN_JOB_COMPLETED:
7266                 # Migration is all done
7267                 LOG.info("Migration operation has completed",
7268                          instance=instance)
7269                 post_method(context, instance, dest, block_migration,
7270                             migrate_data)
7271                 break
7272             elif info.type == libvirt.VIR_DOMAIN_JOB_FAILED:
7273                 # Migration did not succeed
7274                 LOG.error("Migration operation has aborted", instance=instance)
7275                 libvirt_migrate.run_recover_tasks(self._host, guest, instance,
7276                                                   on_migration_failure)
7277                 recover_method(context, instance, dest, migrate_data)
7278                 break
7279             elif info.type == libvirt.VIR_DOMAIN_JOB_CANCELLED:
7280                 # Migration was stopped by admin
7281                 LOG.warning("Migration operation was cancelled",
7282                             instance=instance)
7283                 libvirt_migrate.run_recover_tasks(self._host, guest, instance,
7284                                                   on_migration_failure)
7285                 recover_method(context, instance, dest, migrate_data,
7286                                migration_status='cancelled')
7287                 break
7288             else:
7289                 LOG.warning("Unexpected migration job type: %d",
7290                             info.type, instance=instance)
7291 
7292             time.sleep(0.5)
7293         self._clear_empty_migration(instance)
7294 
7295     def _clear_empty_migration(self, instance):
7296         try:
7297             del self.active_migrations[instance.uuid]
7298         except KeyError:
7299             LOG.warning("There are no records in active migrations "
7300                         "for instance", instance=instance)
7301 
7302     def _live_migration(self, context, instance, dest, post_method,
7303                         recover_method, block_migration,
7304                         migrate_data):
7305         """Do live migration.
7306 
7307         :param context: security context
7308         :param instance:
7309             nova.db.sqlalchemy.models.Instance object
7310             instance object that is migrated.
7311         :param dest: destination host
7312         :param post_method:
7313             post operation method.
7314             expected nova.compute.manager._post_live_migration.
7315         :param recover_method:
7316             recovery method when any exception occurs.
7317             expected nova.compute.manager._rollback_live_migration.
7318         :param block_migration: if true, do block migration.
7319         :param migrate_data: a LibvirtLiveMigrateData object
7320 
7321         This fires off a new thread to run the blocking migration
7322         operation, and then this thread monitors the progress of
7323         migration and controls its operation
7324         """
7325 
7326         guest = self._host.get_guest(instance)
7327 
7328         disk_paths = []
7329         device_names = []
7330         if (migrate_data.block_migration and
7331                 CONF.libvirt.virt_type != "parallels"):
7332             disk_paths, device_names = self._live_migration_copy_disk_paths(
7333                 context, instance, guest)
7334 
7335         deadline = CONF.vif_plugging_timeout
7336         if utils.is_neutron() and deadline:
7337             # We don't generate events if CONF.vif_plugging_timeout=0
7338             # meaning that the operator disabled using them.
7339 
7340             # In case of Linux Bridge, the agent is waiting for new
7341             # TAP devices on destination node. They are going to be
7342             # created by libvirt at the very beginning of the
7343             # live-migration process. Then receiving the events from
7344             # Neutron will ensure that everything is configured
7345             # correctly.
7346             events = self._get_neutron_events_for_live_migration(
7347                 instance.get_network_info())
7348         else:
7349             # TODO(sahid): This 'is_neutron()' condition should be
7350             # removed when nova-network will be erased from the tree
7351             # (Rocky).
7352             events = []
7353 
7354         if events:
7355             # We start migration with the minimum bandwidth
7356             # speed. Depending on the VIF type (see:
7357             # _get_neutron_events_for_live_migration) we will wait for
7358             # Neutron to send events that confirm network is setup or
7359             # directly configure QEMU to use the maximun BW allowed.
7360             bandwidth = MIN_MIGRATION_SPEED_BW
7361         else:
7362             bandwidth = CONF.libvirt.live_migration_bandwidth
7363 
7364         try:
7365             error_cb = self._neutron_failed_live_migration_callback
7366             with self.virtapi.wait_for_instance_event(instance, events,
7367                                                       deadline=deadline,
7368                                                       error_callback=error_cb):
7369                 opthread = utils.spawn(self._live_migration_operation,
7370                                        context, instance, dest,
7371                                        block_migration,
7372                                        migrate_data, guest,
7373                                        device_names, bandwidth)
7374         except eventlet.timeout.Timeout:
7375             msg = ('Timeout waiting for VIF plugging events, '
7376                    'canceling migration')
7377             raise exception.MigrationError(reason=msg)
7378         else:
7379             if utils.is_neutron() and events:
7380                 LOG.debug('VIF events received, continuing migration '
7381                           'with max bandwidth configured: %d',
7382                           CONF.libvirt.live_migration_bandwidth,
7383                           instance=instance)
7384                 # Configure QEMU to use the maximum bandwidth allowed.
7385                 guest.migrate_configure_max_speed(
7386                     CONF.libvirt.live_migration_bandwidth)
7387 
7388         finish_event = eventlet.event.Event()
7389         self.active_migrations[instance.uuid] = deque()
7390 
7391         def thread_finished(thread, event):
7392             LOG.debug("Migration operation thread notification",
7393                       instance=instance)
7394             event.send()
7395         opthread.link(thread_finished, finish_event)
7396 
7397         # Let eventlet schedule the new thread right away
7398         time.sleep(0)
7399 
7400         try:
7401             LOG.debug("Starting monitoring of live migration",
7402                       instance=instance)
7403             self._live_migration_monitor(context, instance, guest, dest,
7404                                          post_method, recover_method,
7405                                          block_migration, migrate_data,
7406                                          finish_event, disk_paths)
7407         except Exception as ex:
7408             LOG.warning("Error monitoring migration: %(ex)s",
7409                         {"ex": ex}, instance=instance, exc_info=True)
7410             raise
7411         finally:
7412             LOG.debug("Live migration monitoring is all done",
7413                       instance=instance)
7414 
7415     def _is_post_copy_enabled(self, migration_flags):
7416         if self._is_post_copy_available():
7417             if (migration_flags & libvirt.VIR_MIGRATE_POSTCOPY) != 0:
7418                 return True
7419         return False
7420 
7421     def live_migration_force_complete(self, instance):
7422         try:
7423             self.active_migrations[instance.uuid].append('force-complete')
7424         except KeyError:
7425             raise exception.NoActiveMigrationForInstance(
7426                 instance_id=instance.uuid)
7427 
7428     def _try_fetch_image(self, context, path, image_id, instance,
7429                          fallback_from_host=None):
7430         try:
7431             libvirt_utils.fetch_image(context, path, image_id,
7432                                       instance.trusted_certs)
7433         except exception.ImageNotFound:
7434             if not fallback_from_host:
7435                 raise
7436             LOG.debug("Image %(image_id)s doesn't exist anymore on "
7437                       "image service, attempting to copy image "
7438                       "from %(host)s",
7439                       {'image_id': image_id, 'host': fallback_from_host})
7440             libvirt_utils.copy_image(src=path, dest=path,
7441                                      host=fallback_from_host,
7442                                      receive=True)
7443 
7444     def _fetch_instance_kernel_ramdisk(self, context, instance,
7445                                        fallback_from_host=None):
7446         """Download kernel and ramdisk for instance in instance directory."""
7447         instance_dir = libvirt_utils.get_instance_path(instance)
7448         if instance.kernel_id:
7449             kernel_path = os.path.join(instance_dir, 'kernel')
7450             # NOTE(dsanders): only fetch image if it's not available at
7451             # kernel_path. This also avoids ImageNotFound exception if
7452             # the image has been deleted from glance
7453             if not os.path.exists(kernel_path):
7454                 self._try_fetch_image(context,
7455                                       kernel_path,
7456                                       instance.kernel_id,
7457                                       instance, fallback_from_host)
7458             if instance.ramdisk_id:
7459                 ramdisk_path = os.path.join(instance_dir, 'ramdisk')
7460                 # NOTE(dsanders): only fetch image if it's not available at
7461                 # ramdisk_path. This also avoids ImageNotFound exception if
7462                 # the image has been deleted from glance
7463                 if not os.path.exists(ramdisk_path):
7464                     self._try_fetch_image(context,
7465                                           ramdisk_path,
7466                                           instance.ramdisk_id,
7467                                           instance, fallback_from_host)
7468 
7469     def rollback_live_migration_at_destination(self, context, instance,
7470                                                network_info,
7471                                                block_device_info,
7472                                                destroy_disks=True,
7473                                                migrate_data=None):
7474         """Clean up destination node after a failed live migration."""
7475         try:
7476             self.destroy(context, instance, network_info, block_device_info,
7477                          destroy_disks)
7478         finally:
7479             # NOTE(gcb): Failed block live migration may leave instance
7480             # directory at destination node, ensure it is always deleted.
7481             is_shared_instance_path = True
7482             if migrate_data:
7483                 is_shared_instance_path = migrate_data.is_shared_instance_path
7484                 if (migrate_data.obj_attr_is_set("serial_listen_ports")
7485                     and migrate_data.serial_listen_ports):
7486                     # Releases serial ports reserved.
7487                     for port in migrate_data.serial_listen_ports:
7488                         serial_console.release_port(
7489                             host=migrate_data.serial_listen_addr, port=port)
7490 
7491             if not is_shared_instance_path:
7492                 instance_dir = libvirt_utils.get_instance_path_at_destination(
7493                     instance, migrate_data)
7494                 if os.path.exists(instance_dir):
7495                     shutil.rmtree(instance_dir)
7496 
7497     def pre_live_migration(self, context, instance, block_device_info,
7498                            network_info, disk_info, migrate_data):
7499         """Preparation live migration."""
7500         if disk_info is not None:
7501             disk_info = jsonutils.loads(disk_info)
7502 
7503         LOG.debug('migrate_data in pre_live_migration: %s', migrate_data,
7504                   instance=instance)
7505         is_shared_block_storage = migrate_data.is_shared_block_storage
7506         is_shared_instance_path = migrate_data.is_shared_instance_path
7507         is_block_migration = migrate_data.block_migration
7508 
7509         if not is_shared_instance_path:
7510             instance_dir = libvirt_utils.get_instance_path_at_destination(
7511                             instance, migrate_data)
7512 
7513             if os.path.exists(instance_dir):
7514                 raise exception.DestinationDiskExists(path=instance_dir)
7515 
7516             LOG.debug('Creating instance directory: %s', instance_dir,
7517                       instance=instance)
7518             os.mkdir(instance_dir)
7519 
7520             # Recreate the disk.info file and in doing so stop the
7521             # imagebackend from recreating it incorrectly by inspecting the
7522             # contents of each file when using the Raw backend.
7523             if disk_info:
7524                 image_disk_info = {}
7525                 for info in disk_info:
7526                     image_file = os.path.basename(info['path'])
7527                     image_path = os.path.join(instance_dir, image_file)
7528                     image_disk_info[image_path] = info['type']
7529 
7530                 LOG.debug('Creating disk.info with the contents: %s',
7531                           image_disk_info, instance=instance)
7532 
7533                 image_disk_info_path = os.path.join(instance_dir,
7534                                                     'disk.info')
7535                 libvirt_utils.write_to_file(image_disk_info_path,
7536                                             jsonutils.dumps(image_disk_info))
7537 
7538             if not is_shared_block_storage:
7539                 # Ensure images and backing files are present.
7540                 LOG.debug('Checking to make sure images and backing files are '
7541                           'present before live migration.', instance=instance)
7542                 self._create_images_and_backing(
7543                     context, instance, instance_dir, disk_info,
7544                     fallback_from_host=instance.host)
7545                 if (configdrive.required_by(instance) and
7546                         CONF.config_drive_format == 'iso9660'):
7547                     # NOTE(pkoniszewski): Due to a bug in libvirt iso config
7548                     # drive needs to be copied to destination prior to
7549                     # migration when instance path is not shared and block
7550                     # storage is not shared. Files that are already present
7551                     # on destination are excluded from a list of files that
7552                     # need to be copied to destination. If we don't do that
7553                     # live migration will fail on copying iso config drive to
7554                     # destination and writing to read-only device.
7555                     # Please see bug/1246201 for more details.
7556                     src = "%s:%s/disk.config" % (instance.host, instance_dir)
7557                     self._remotefs.copy_file(src, instance_dir)
7558 
7559             if not is_block_migration:
7560                 # NOTE(angdraug): when block storage is shared between source
7561                 # and destination and instance path isn't (e.g. volume backed
7562                 # or rbd backed instance), instance path on destination has to
7563                 # be prepared
7564 
7565                 # Required by Quobyte CI
7566                 self._ensure_console_log_for_instance(instance)
7567 
7568                 # if image has kernel and ramdisk, just download
7569                 # following normal way.
7570                 self._fetch_instance_kernel_ramdisk(context, instance)
7571 
7572         # Establishing connection to volume server.
7573         block_device_mapping = driver.block_device_info_get_mapping(
7574             block_device_info)
7575 
7576         if len(block_device_mapping):
7577             LOG.debug('Connecting volumes before live migration.',
7578                       instance=instance)
7579 
7580         for bdm in block_device_mapping:
7581             connection_info = bdm['connection_info']
7582             # NOTE(lyarwood): Handle the P to Q LM during upgrade use case
7583             # where an instance has encrypted volumes attached using the
7584             # os-brick encryptors. Do not attempt to attach the encrypted
7585             # volume using native LUKS decryption on the destionation.
7586             src_native_luks = False
7587             if migrate_data.obj_attr_is_set('src_supports_native_luks'):
7588                 src_native_luks = migrate_data.src_supports_native_luks
7589             dest_native_luks = self._is_native_luks_available()
7590             allow_native_luks = src_native_luks and dest_native_luks
7591             self._connect_volume(context, connection_info, instance,
7592                                  allow_native_luks=allow_native_luks)
7593 
7594         # We call plug_vifs before the compute manager calls
7595         # ensure_filtering_rules_for_instance, to ensure bridge is set up
7596         # Retry operation is necessary because continuously request comes,
7597         # concurrent request occurs to iptables, then it complains.
7598         LOG.debug('Plugging VIFs before live migration.', instance=instance)
7599         max_retry = CONF.live_migration_retry_count
7600         for cnt in range(max_retry):
7601             try:
7602                 self.plug_vifs(instance, network_info)
7603                 break
7604             except processutils.ProcessExecutionError:
7605                 if cnt == max_retry - 1:
7606                     raise
7607                 else:
7608                     LOG.warning('plug_vifs() failed %(cnt)d. Retry up to '
7609                                 '%(max_retry)d.',
7610                                 {'cnt': cnt, 'max_retry': max_retry},
7611                                 instance=instance)
7612                     greenthread.sleep(1)
7613 
7614         # Store server_listen and latest disk device info
7615         if not migrate_data:
7616             migrate_data = objects.LibvirtLiveMigrateData(bdms=[])
7617         else:
7618             migrate_data.bdms = []
7619         # Store live_migration_inbound_addr
7620         migrate_data.target_connect_addr = \
7621             CONF.libvirt.live_migration_inbound_addr
7622         migrate_data.supported_perf_events = self._supported_perf_events
7623 
7624         migrate_data.serial_listen_ports = []
7625         if CONF.serial_console.enabled:
7626             num_ports = hardware.get_number_of_serial_ports(
7627                 instance.flavor, instance.image_meta)
7628             for port in six.moves.range(num_ports):
7629                 migrate_data.serial_listen_ports.append(
7630                     serial_console.acquire_port(
7631                         migrate_data.serial_listen_addr))
7632 
7633         for vol in block_device_mapping:
7634             connection_info = vol['connection_info']
7635             if connection_info.get('serial'):
7636                 disk_info = blockinfo.get_info_from_bdm(
7637                     instance, CONF.libvirt.virt_type,
7638                     instance.image_meta, vol)
7639 
7640                 bdmi = objects.LibvirtLiveMigrateBDMInfo()
7641                 bdmi.serial = connection_info['serial']
7642                 bdmi.connection_info = connection_info
7643                 bdmi.bus = disk_info['bus']
7644                 bdmi.dev = disk_info['dev']
7645                 bdmi.type = disk_info['type']
7646                 bdmi.format = disk_info.get('format')
7647                 bdmi.boot_index = disk_info.get('boot_index')
7648                 volume_secret = self._host.find_secret('volume', vol.volume_id)
7649                 if volume_secret:
7650                     bdmi.encryption_secret_uuid = volume_secret.UUIDString()
7651 
7652                 migrate_data.bdms.append(bdmi)
7653 
7654         return migrate_data
7655 
7656     def _try_fetch_image_cache(self, image, fetch_func, context, filename,
7657                                image_id, instance, size,
7658                                fallback_from_host=None):
7659         try:
7660             image.cache(fetch_func=fetch_func,
7661                         context=context,
7662                         filename=filename,
7663                         image_id=image_id,
7664                         size=size,
7665                         trusted_certs=instance.trusted_certs)
7666         except exception.ImageNotFound:
7667             if not fallback_from_host:
7668                 raise
7669             LOG.debug("Image %(image_id)s doesn't exist anymore "
7670                       "on image service, attempting to copy "
7671                       "image from %(host)s",
7672                       {'image_id': image_id, 'host': fallback_from_host},
7673                       instance=instance)
7674 
7675             def copy_from_host(target):
7676                 libvirt_utils.copy_image(src=target,
7677                                          dest=target,
7678                                          host=fallback_from_host,
7679                                          receive=True)
7680             image.cache(fetch_func=copy_from_host,
7681                         filename=filename)
7682 
7683     def _create_images_and_backing(self, context, instance, instance_dir,
7684                                    disk_info, fallback_from_host=None):
7685         """:param context: security context
7686            :param instance:
7687                nova.db.sqlalchemy.models.Instance object
7688                instance object that is migrated.
7689            :param instance_dir:
7690                instance path to use, calculated externally to handle block
7691                migrating an instance with an old style instance path
7692            :param disk_info:
7693                disk info specified in _get_instance_disk_info_from_config
7694                (list of dicts)
7695            :param fallback_from_host:
7696                host where we can retrieve images if the glance images are
7697                not available.
7698 
7699         """
7700 
7701         # Virtuozzo containers don't use backing file
7702         if (CONF.libvirt.virt_type == "parallels" and
7703                 instance.vm_mode == fields.VMMode.EXE):
7704             return
7705 
7706         if not disk_info:
7707             disk_info = []
7708 
7709         for info in disk_info:
7710             base = os.path.basename(info['path'])
7711             # Get image type and create empty disk image, and
7712             # create backing file in case of qcow2.
7713             instance_disk = os.path.join(instance_dir, base)
7714             if not info['backing_file'] and not os.path.exists(instance_disk):
7715                 libvirt_utils.create_image(info['type'], instance_disk,
7716                                            info['virt_disk_size'])
7717             elif info['backing_file']:
7718                 # Creating backing file follows same way as spawning instances.
7719                 cache_name = os.path.basename(info['backing_file'])
7720 
7721                 disk = self.image_backend.by_name(instance, instance_disk,
7722                                                   CONF.libvirt.images_type)
7723                 if cache_name.startswith('ephemeral'):
7724                     # The argument 'size' is used by image.cache to
7725                     # validate disk size retrieved from cache against
7726                     # the instance disk size (should always return OK)
7727                     # and ephemeral_size is used by _create_ephemeral
7728                     # to build the image if the disk is not already
7729                     # cached.
7730                     disk.cache(
7731                         fetch_func=self._create_ephemeral,
7732                         fs_label=cache_name,
7733                         os_type=instance.os_type,
7734                         filename=cache_name,
7735                         size=info['virt_disk_size'],
7736                         ephemeral_size=info['virt_disk_size'] / units.Gi)
7737                 elif cache_name.startswith('swap'):
7738                     inst_type = instance.get_flavor()
7739                     swap_mb = inst_type.swap
7740                     disk.cache(fetch_func=self._create_swap,
7741                                 filename="swap_%s" % swap_mb,
7742                                 size=swap_mb * units.Mi,
7743                                 swap_mb=swap_mb)
7744                 else:
7745                     self._try_fetch_image_cache(disk,
7746                                                 libvirt_utils.fetch_image,
7747                                                 context, cache_name,
7748                                                 instance.image_ref,
7749                                                 instance,
7750                                                 info['virt_disk_size'],
7751                                                 fallback_from_host)
7752 
7753         # if disk has kernel and ramdisk, just download
7754         # following normal way.
7755         self._fetch_instance_kernel_ramdisk(
7756             context, instance, fallback_from_host=fallback_from_host)
7757 
7758     def post_live_migration(self, context, instance, block_device_info,
7759                             migrate_data=None):
7760         # Disconnect from volume server
7761         block_device_mapping = driver.block_device_info_get_mapping(
7762                 block_device_info)
7763         volume_api = self._volume_api
7764         for vol in block_device_mapping:
7765             volume_id = vol['connection_info']['serial']
7766             if vol['attachment_id'] is None:
7767                 # Cinder v2 api flow: Retrieve connection info from Cinder's
7768                 # initialize_connection API. The info returned will be
7769                 # accurate for the source server.
7770                 connector = self.get_volume_connector(instance)
7771                 connection_info = volume_api.initialize_connection(
7772                     context, volume_id, connector)
7773             else:
7774                 # cinder v3.44 api flow: Retrieve the connection_info for
7775                 # the old attachment from cinder.
7776                 old_attachment_id = \
7777                     migrate_data.old_vol_attachment_ids[volume_id]
7778                 old_attachment = volume_api.attachment_get(
7779                     context, old_attachment_id)
7780                 connection_info = old_attachment['connection_info']
7781 
7782             # TODO(leeantho) The following multipath_id logic is temporary
7783             # and will be removed in the future once os-brick is updated
7784             # to handle multipath for drivers in a more efficient way.
7785             # For now this logic is needed to ensure the connection info
7786             # data is correct.
7787 
7788             # Pull out multipath_id from the bdm information. The
7789             # multipath_id can be placed into the connection info
7790             # because it is based off of the volume and will be the
7791             # same on the source and destination hosts.
7792             if 'multipath_id' in vol['connection_info']['data']:
7793                 multipath_id = vol['connection_info']['data']['multipath_id']
7794                 connection_info['data']['multipath_id'] = multipath_id
7795 
7796             self._disconnect_volume(context, connection_info, instance)
7797 
7798     def post_live_migration_at_source(self, context, instance, network_info):
7799         """Unplug VIFs from networks at source.
7800 
7801         :param context: security context
7802         :param instance: instance object reference
7803         :param network_info: instance network information
7804         """
7805         self.unplug_vifs(instance, network_info)
7806 
7807     def post_live_migration_at_destination(self, context,
7808                                            instance,
7809                                            network_info,
7810                                            block_migration=False,
7811                                            block_device_info=None):
7812         """Post operation of live migration at destination host.
7813 
7814         :param context: security context
7815         :param instance:
7816             nova.db.sqlalchemy.models.Instance object
7817             instance object that is migrated.
7818         :param network_info: instance network information
7819         :param block_migration: if true, post operation of block_migration.
7820         """
7821         # The source node set the VIR_MIGRATE_PERSIST_DEST flag when live
7822         # migrating so the guest xml should already be persisted on the
7823         # destination host, so just perform a sanity check to make sure it
7824         # made it as expected.
7825         self._host.get_guest(instance)
7826 
7827     def _get_instance_disk_info_from_config(self, guest_config,
7828                                             block_device_info):
7829         """Get the non-volume disk information from the domain xml
7830 
7831         :param LibvirtConfigGuest guest_config: the libvirt domain config
7832                                                 for the instance
7833         :param dict block_device_info: block device info for BDMs
7834         :returns disk_info: list of dicts with keys:
7835 
7836           * 'type': the disk type (str)
7837           * 'path': the disk path (str)
7838           * 'virt_disk_size': the virtual disk size (int)
7839           * 'backing_file': backing file of a disk image (str)
7840           * 'disk_size': physical disk size (int)
7841           * 'over_committed_disk_size': virt_disk_size - disk_size or 0
7842         """
7843         block_device_mapping = driver.block_device_info_get_mapping(
7844             block_device_info)
7845 
7846         volume_devices = set()
7847         for vol in block_device_mapping:
7848             disk_dev = vol['mount_device'].rpartition("/")[2]
7849             volume_devices.add(disk_dev)
7850 
7851         disk_info = []
7852 
7853         if (guest_config.virt_type == 'parallels' and
7854                 guest_config.os_type == fields.VMMode.EXE):
7855             node_type = 'filesystem'
7856         else:
7857             node_type = 'disk'
7858 
7859         for device in guest_config.devices:
7860             if device.root_name != node_type:
7861                 continue
7862             disk_type = device.source_type
7863             if device.root_name == 'filesystem':
7864                 target = device.target_dir
7865                 if device.source_type == 'file':
7866                     path = device.source_file
7867                 elif device.source_type == 'block':
7868                     path = device.source_dev
7869                 else:
7870                     path = None
7871             else:
7872                 target = device.target_dev
7873                 path = device.source_path
7874 
7875             if not path:
7876                 LOG.debug('skipping disk for %s as it does not have a path',
7877                           guest_config.name)
7878                 continue
7879 
7880             if disk_type not in ['file', 'block']:
7881                 LOG.debug('skipping disk because it looks like a volume', path)
7882                 continue
7883 
7884             if target in volume_devices:
7885                 LOG.debug('skipping disk %(path)s (%(target)s) as it is a '
7886                           'volume', {'path': path, 'target': target})
7887                 continue
7888 
7889             if device.root_name == 'filesystem':
7890                 driver_type = device.driver_type
7891             else:
7892                 driver_type = device.driver_format
7893             # get the real disk size or
7894             # raise a localized error if image is unavailable
7895             if disk_type == 'file':
7896                 if driver_type == 'ploop':
7897                     dk_size = 0
7898                     for dirpath, dirnames, filenames in os.walk(path):
7899                         for f in filenames:
7900                             fp = os.path.join(dirpath, f)
7901                             dk_size += os.path.getsize(fp)
7902                 else:
7903                     dk_size = disk_api.get_allocated_disk_size(path)
7904 
7905                 # NOTE(lyarwood): Fetch the virtual size for all file disks.
7906                 virt_size = disk_api.get_disk_size(path)
7907 
7908             elif disk_type == 'block' and block_device_info:
7909                 # FIXME(lyarwood): There's no reason to use a separate call
7910                 # here, once disk_api uses privsep this should be removed along
7911                 # with the surrounding conditionals to simplify this mess.
7912                 dk_size = lvm.get_volume_size(path)
7913                 # NOTE(lyarwood): As above, we should be using disk_api to
7914                 # fetch the virt-size but can't as it currently runs qemu-img
7915                 # as an unprivileged user, causing a failure for block devices.
7916                 virt_size = dk_size
7917             else:
7918                 LOG.debug('skipping disk %(path)s (%(target)s) - unable to '
7919                           'determine if volume',
7920                           {'path': path, 'target': target})
7921                 continue
7922 
7923             if driver_type in ("qcow2", "ploop"):
7924                 backing_file = libvirt_utils.get_disk_backing_file(path)
7925                 over_commit_size = int(virt_size) - dk_size
7926             else:
7927                 backing_file = ""
7928                 over_commit_size = 0
7929 
7930             disk_info.append({'type': driver_type,
7931                               'path': path,
7932                               'virt_disk_size': virt_size,
7933                               'backing_file': backing_file,
7934                               'disk_size': dk_size,
7935                               'over_committed_disk_size': over_commit_size})
7936         return disk_info
7937 
7938     def _get_instance_disk_info(self, instance, block_device_info):
7939         try:
7940             guest = self._host.get_guest(instance)
7941             config = guest.get_config()
7942         except libvirt.libvirtError as ex:
7943             error_code = ex.get_error_code()
7944             LOG.warning('Error from libvirt while getting description of '
7945                         '%(instance_name)s: [Error Code %(error_code)s] '
7946                         '%(ex)s',
7947                         {'instance_name': instance.name,
7948                          'error_code': error_code,
7949                          'ex': encodeutils.exception_to_unicode(ex)},
7950                         instance=instance)
7951             raise exception.InstanceNotFound(instance_id=instance.uuid)
7952 
7953         return self._get_instance_disk_info_from_config(config,
7954                                                         block_device_info)
7955 
7956     def get_instance_disk_info(self, instance,
7957                                block_device_info=None):
7958         return jsonutils.dumps(
7959             self._get_instance_disk_info(instance, block_device_info))
7960 
7961     def _get_disk_over_committed_size_total(self):
7962         """Return total over committed disk size for all instances."""
7963         # Disk size that all instance uses : virtual_size - disk_size
7964         disk_over_committed_size = 0
7965         instance_domains = self._host.list_instance_domains(only_running=False)
7966         if not instance_domains:
7967             return disk_over_committed_size
7968 
7969         # Get all instance uuids
7970         instance_uuids = [dom.UUIDString() for dom in instance_domains]
7971         ctx = nova_context.get_admin_context()
7972         # Get instance object list by uuid filter
7973         filters = {'uuid': instance_uuids}
7974         # NOTE(ankit): objects.InstanceList.get_by_filters method is
7975         # getting called twice one is here and another in the
7976         # _update_available_resource method of resource_tracker. Since
7977         # _update_available_resource method is synchronized, there is a
7978         # possibility the instances list retrieved here to calculate
7979         # disk_over_committed_size would differ to the list you would get
7980         # in _update_available_resource method for calculating usages based
7981         # on instance utilization.
7982         local_instance_list = objects.InstanceList.get_by_filters(
7983             ctx, filters, use_slave=True)
7984         # Convert instance list to dictionary with instance uuid as key.
7985         local_instances = {inst.uuid: inst for inst in local_instance_list}
7986 
7987         # Get bdms by instance uuids
7988         bdms = objects.BlockDeviceMappingList.bdms_by_instance_uuid(
7989             ctx, instance_uuids)
7990 
7991         for dom in instance_domains:
7992             try:
7993                 guest = libvirt_guest.Guest(dom)
7994                 config = guest.get_config()
7995 
7996                 block_device_info = None
7997                 if guest.uuid in local_instances \
7998                         and (bdms and guest.uuid in bdms):
7999                     # Get block device info for instance
8000                     block_device_info = driver.get_block_device_info(
8001                         local_instances[guest.uuid], bdms[guest.uuid])
8002 
8003                 disk_infos = self._get_instance_disk_info_from_config(
8004                     config, block_device_info)
8005                 if not disk_infos:
8006                     continue
8007 
8008                 for info in disk_infos:
8009                     disk_over_committed_size += int(
8010                         info['over_committed_disk_size'])
8011             except libvirt.libvirtError as ex:
8012                 error_code = ex.get_error_code()
8013                 LOG.warning(
8014                     'Error from libvirt while getting description of '
8015                     '%(instance_name)s: [Error Code %(error_code)s] %(ex)s',
8016                     {'instance_name': guest.name,
8017                      'error_code': error_code,
8018                      'ex': encodeutils.exception_to_unicode(ex)})
8019             except OSError as e:
8020                 if e.errno in (errno.ENOENT, errno.ESTALE):
8021                     LOG.warning('Periodic task is updating the host stat, '
8022                                 'it is trying to get disk %(i_name)s, '
8023                                 'but disk file was removed by concurrent '
8024                                 'operations such as resize.',
8025                                 {'i_name': guest.name})
8026                 elif e.errno == errno.EACCES:
8027                     LOG.warning('Periodic task is updating the host stat, '
8028                                 'it is trying to get disk %(i_name)s, '
8029                                 'but access is denied. It is most likely '
8030                                 'due to a VM that exists on the compute '
8031                                 'node but is not managed by Nova.',
8032                                 {'i_name': guest.name})
8033                 else:
8034                     raise
8035             except exception.VolumeBDMPathNotFound as e:
8036                 LOG.warning('Periodic task is updating the host stats, '
8037                             'it is trying to get disk info for %(i_name)s, '
8038                             'but the backing volume block device was removed '
8039                             'by concurrent operations such as resize. '
8040                             'Error: %(error)s',
8041                             {'i_name': guest.name, 'error': e})
8042             except exception.DiskNotFound:
8043                 with excutils.save_and_reraise_exception() as err_ctxt:
8044                     # If the instance is undergoing a task state transition,
8045                     # like moving to another host or is being deleted, we
8046                     # should ignore this instance and move on.
8047                     if guest.uuid in local_instances:
8048                         inst = local_instances[guest.uuid]
8049                         if inst.task_state is not None:
8050                             LOG.info('Periodic task is updating the host '
8051                                      'stats; it is trying to get disk info '
8052                                      'for %(i_name)s, but the backing disk '
8053                                      'was removed by a concurrent operation '
8054                                      '(task_state=%(task_state)s)',
8055                                      {'i_name': guest.name,
8056                                       'task_state': inst.task_state},
8057                                      instance=inst)
8058                             err_ctxt.reraise = False
8059 
8060             # NOTE(gtt116): give other tasks a chance.
8061             greenthread.sleep(0)
8062         return disk_over_committed_size
8063 
8064     def unfilter_instance(self, instance, network_info):
8065         """See comments of same method in firewall_driver."""
8066         self.firewall_driver.unfilter_instance(instance,
8067                                                network_info=network_info)
8068 
8069     def get_available_nodes(self, refresh=False):
8070         return [self._host.get_hostname()]
8071 
8072     def get_host_cpu_stats(self):
8073         """Return the current CPU state of the host."""
8074         return self._host.get_cpu_stats()
8075 
8076     def get_host_uptime(self):
8077         """Returns the result of calling "uptime"."""
8078         out, err = utils.execute('env', 'LANG=C', 'uptime')
8079         return out
8080 
8081     def manage_image_cache(self, context, all_instances):
8082         """Manage the local cache of images."""
8083         self.image_cache_manager.update(context, all_instances)
8084 
8085     def _cleanup_remote_migration(self, dest, inst_base, inst_base_resize,
8086                                   shared_storage=False):
8087         """Used only for cleanup in case migrate_disk_and_power_off fails."""
8088         try:
8089             if os.path.exists(inst_base_resize):
8090                 shutil.rmtree(inst_base, ignore_errors=True)
8091                 os.rename(inst_base_resize, inst_base)
8092                 if not shared_storage:
8093                     self._remotefs.remove_dir(dest, inst_base)
8094         except Exception:
8095             pass
8096 
8097     def _is_storage_shared_with(self, dest, inst_base):
8098         # NOTE (rmk): There are two methods of determining whether we are
8099         #             on the same filesystem: the source and dest IP are the
8100         #             same, or we create a file on the dest system via SSH
8101         #             and check whether the source system can also see it.
8102         # NOTE (drwahl): Actually, there is a 3rd way: if images_type is rbd,
8103         #                it will always be shared storage
8104         if CONF.libvirt.images_type == 'rbd':
8105             return True
8106         shared_storage = (dest == self.get_host_ip_addr())
8107         if not shared_storage:
8108             tmp_file = uuidutils.generate_uuid(dashed=False) + '.tmp'
8109             tmp_path = os.path.join(inst_base, tmp_file)
8110 
8111             try:
8112                 self._remotefs.create_file(dest, tmp_path)
8113                 if os.path.exists(tmp_path):
8114                     shared_storage = True
8115                     os.unlink(tmp_path)
8116                 else:
8117                     self._remotefs.remove_file(dest, tmp_path)
8118             except Exception:
8119                 pass
8120         return shared_storage
8121 
8122     def migrate_disk_and_power_off(self, context, instance, dest,
8123                                    flavor, network_info,
8124                                    block_device_info=None,
8125                                    timeout=0, retry_interval=0):
8126         LOG.debug("Starting migrate_disk_and_power_off",
8127                    instance=instance)
8128 
8129         ephemerals = driver.block_device_info_get_ephemerals(block_device_info)
8130 
8131         # get_bdm_ephemeral_disk_size() will return 0 if the new
8132         # instance's requested block device mapping contain no
8133         # ephemeral devices. However, we still want to check if
8134         # the original instance's ephemeral_gb property was set and
8135         # ensure that the new requested flavor ephemeral size is greater
8136         eph_size = (block_device.get_bdm_ephemeral_disk_size(ephemerals) or
8137                     instance.flavor.ephemeral_gb)
8138 
8139         # Checks if the migration needs a disk resize down.
8140         root_down = flavor.root_gb < instance.flavor.root_gb
8141         ephemeral_down = flavor.ephemeral_gb < eph_size
8142         booted_from_volume = self._is_booted_from_volume(block_device_info)
8143 
8144         if (root_down and not booted_from_volume) or ephemeral_down:
8145             reason = _("Unable to resize disk down.")
8146             raise exception.InstanceFaultRollback(
8147                 exception.ResizeError(reason=reason))
8148 
8149         # NOTE(dgenin): Migration is not implemented for LVM backed instances.
8150         if CONF.libvirt.images_type == 'lvm' and not booted_from_volume:
8151             reason = _("Migration is not supported for LVM backed instances")
8152             raise exception.InstanceFaultRollback(
8153                 exception.MigrationPreCheckError(reason=reason))
8154 
8155         # copy disks to destination
8156         # rename instance dir to +_resize at first for using
8157         # shared storage for instance dir (eg. NFS).
8158         inst_base = libvirt_utils.get_instance_path(instance)
8159         inst_base_resize = inst_base + "_resize"
8160         shared_storage = self._is_storage_shared_with(dest, inst_base)
8161 
8162         # try to create the directory on the remote compute node
8163         # if this fails we pass the exception up the stack so we can catch
8164         # failures here earlier
8165         if not shared_storage:
8166             try:
8167                 self._remotefs.create_dir(dest, inst_base)
8168             except processutils.ProcessExecutionError as e:
8169                 reason = _("not able to execute ssh command: %s") % e
8170                 raise exception.InstanceFaultRollback(
8171                     exception.ResizeError(reason=reason))
8172 
8173         self.power_off(instance, timeout, retry_interval)
8174 
8175         block_device_mapping = driver.block_device_info_get_mapping(
8176             block_device_info)
8177         for vol in block_device_mapping:
8178             connection_info = vol['connection_info']
8179             self._disconnect_volume(context, connection_info, instance)
8180 
8181         disk_info = self._get_instance_disk_info(instance, block_device_info)
8182 
8183         try:
8184             os.rename(inst_base, inst_base_resize)
8185             # if we are migrating the instance with shared storage then
8186             # create the directory.  If it is a remote node the directory
8187             # has already been created
8188             if shared_storage:
8189                 dest = None
8190                 fileutils.ensure_tree(inst_base)
8191 
8192             on_execute = lambda process: \
8193                 self.job_tracker.add_job(instance, process.pid)
8194             on_completion = lambda process: \
8195                 self.job_tracker.remove_job(instance, process.pid)
8196 
8197             for info in disk_info:
8198                 # assume inst_base == dirname(info['path'])
8199                 img_path = info['path']
8200                 fname = os.path.basename(img_path)
8201                 from_path = os.path.join(inst_base_resize, fname)
8202 
8203                 # We will not copy over the swap disk here, and rely on
8204                 # finish_migration to re-create it for us. This is ok because
8205                 # the OS is shut down, and as recreating a swap disk is very
8206                 # cheap it is more efficient than copying either locally or
8207                 # over the network. This also means we don't have to resize it.
8208                 if fname == 'disk.swap':
8209                     continue
8210 
8211                 compression = info['type'] not in NO_COMPRESSION_TYPES
8212                 libvirt_utils.copy_image(from_path, img_path, host=dest,
8213                                          on_execute=on_execute,
8214                                          on_completion=on_completion,
8215                                          compression=compression)
8216 
8217             # Ensure disk.info is written to the new path to avoid disks being
8218             # reinspected and potentially changing format.
8219             src_disk_info_path = os.path.join(inst_base_resize, 'disk.info')
8220             if os.path.exists(src_disk_info_path):
8221                 dst_disk_info_path = os.path.join(inst_base, 'disk.info')
8222                 libvirt_utils.copy_image(src_disk_info_path,
8223                                          dst_disk_info_path,
8224                                          host=dest, on_execute=on_execute,
8225                                          on_completion=on_completion)
8226         except Exception:
8227             with excutils.save_and_reraise_exception():
8228                 self._cleanup_remote_migration(dest, inst_base,
8229                                                inst_base_resize,
8230                                                shared_storage)
8231 
8232         return jsonutils.dumps(disk_info)
8233 
8234     def _wait_for_running(self, instance):
8235         state = self.get_info(instance).state
8236 
8237         if state == power_state.RUNNING:
8238             LOG.info("Instance running successfully.", instance=instance)
8239             raise loopingcall.LoopingCallDone()
8240 
8241     @staticmethod
8242     def _disk_raw_to_qcow2(path):
8243         """Converts a raw disk to qcow2."""
8244         path_qcow = path + '_qcow'
8245         utils.execute('qemu-img', 'convert', '-f', 'raw',
8246                       '-O', 'qcow2', path, path_qcow)
8247         os.rename(path_qcow, path)
8248 
8249     def finish_migration(self, context, migration, instance, disk_info,
8250                          network_info, image_meta, resize_instance,
8251                          block_device_info=None, power_on=True):
8252         LOG.debug("Starting finish_migration", instance=instance)
8253 
8254         block_disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
8255                                                   instance,
8256                                                   image_meta,
8257                                                   block_device_info)
8258         # assume _create_image does nothing if a target file exists.
8259         # NOTE: This has the intended side-effect of fetching a missing
8260         # backing file.
8261         self._create_image(context, instance, block_disk_info['mapping'],
8262                            block_device_info=block_device_info,
8263                            ignore_bdi_for_swap=True,
8264                            fallback_from_host=migration.source_compute)
8265 
8266         # Required by Quobyte CI
8267         self._ensure_console_log_for_instance(instance)
8268 
8269         gen_confdrive = functools.partial(
8270             self._create_configdrive, context, instance,
8271             InjectionInfo(admin_pass=None, network_info=network_info,
8272                           files=None))
8273 
8274         # Convert raw disks to qcow2 if migrating to host which uses
8275         # qcow2 from host which uses raw.
8276         disk_info = jsonutils.loads(disk_info)
8277         for info in disk_info:
8278             path = info['path']
8279             disk_name = os.path.basename(path)
8280 
8281             # NOTE(mdbooth): The code below looks wrong, but is actually
8282             # required to prevent a security hole when migrating from a host
8283             # with use_cow_images=False to one with use_cow_images=True.
8284             # Imagebackend uses use_cow_images to select between the
8285             # atrociously-named-Raw and Qcow2 backends. The Qcow2 backend
8286             # writes to disk.info, but does not read it as it assumes qcow2.
8287             # Therefore if we don't convert raw to qcow2 here, a raw disk will
8288             # be incorrectly assumed to be qcow2, which is a severe security
8289             # flaw. The reverse is not true, because the atrociously-named-Raw
8290             # backend supports both qcow2 and raw disks, and will choose
8291             # appropriately between them as long as disk.info exists and is
8292             # correctly populated, which it is because Qcow2 writes to
8293             # disk.info.
8294             #
8295             # In general, we do not yet support format conversion during
8296             # migration. For example:
8297             #   * Converting from use_cow_images=True to use_cow_images=False
8298             #     isn't handled. This isn't a security bug, but is almost
8299             #     certainly buggy in other cases, as the 'Raw' backend doesn't
8300             #     expect a backing file.
8301             #   * Converting to/from lvm and rbd backends is not supported.
8302             #
8303             # This behaviour is inconsistent, and therefore undesirable for
8304             # users. It is tightly-coupled to implementation quirks of 2
8305             # out of 5 backends in imagebackend and defends against a severe
8306             # security flaw which is not at all obvious without deep analysis,
8307             # and is therefore undesirable to developers. We should aim to
8308             # remove it. This will not be possible, though, until we can
8309             # represent the storage layout of a specific instance
8310             # independent of the default configuration of the local compute
8311             # host.
8312 
8313             # Config disks are hard-coded to be raw even when
8314             # use_cow_images=True (see _get_disk_config_image_type),so don't
8315             # need to be converted.
8316             if (disk_name != 'disk.config' and
8317                         info['type'] == 'raw' and CONF.use_cow_images):
8318                 self._disk_raw_to_qcow2(info['path'])
8319 
8320         xml = self._get_guest_xml(context, instance, network_info,
8321                                   block_disk_info, image_meta,
8322                                   block_device_info=block_device_info)
8323         # NOTE(mriedem): vifs_already_plugged=True here, regardless of whether
8324         # or not we've migrated to another host, because we unplug VIFs locally
8325         # and the status change in the port might go undetected by the neutron
8326         # L2 agent (or neutron server) so neutron may not know that the VIF was
8327         # unplugged in the first place and never send an event.
8328         guest = self._create_domain_and_network(context, xml, instance,
8329                                         network_info,
8330                                         block_device_info=block_device_info,
8331                                         power_on=power_on,
8332                                         vifs_already_plugged=True,
8333                                         post_xml_callback=gen_confdrive)
8334         if power_on:
8335             timer = loopingcall.FixedIntervalLoopingCall(
8336                                                     self._wait_for_running,
8337                                                     instance)
8338             timer.start(interval=0.5).wait()
8339 
8340             # Sync guest time after migration.
8341             guest.sync_guest_time()
8342 
8343         LOG.debug("finish_migration finished successfully.", instance=instance)
8344 
8345     def _cleanup_failed_migration(self, inst_base):
8346         """Make sure that a failed migrate doesn't prevent us from rolling
8347         back in a revert.
8348         """
8349         try:
8350             shutil.rmtree(inst_base)
8351         except OSError as e:
8352             if e.errno != errno.ENOENT:
8353                 raise
8354 
8355     def finish_revert_migration(self, context, instance, network_info,
8356                                 block_device_info=None, power_on=True):
8357         LOG.debug("Starting finish_revert_migration",
8358                   instance=instance)
8359 
8360         inst_base = libvirt_utils.get_instance_path(instance)
8361         inst_base_resize = inst_base + "_resize"
8362 
8363         # NOTE(danms): if we're recovering from a failed migration,
8364         # make sure we don't have a left-over same-host base directory
8365         # that would conflict. Also, don't fail on the rename if the
8366         # failure happened early.
8367         if os.path.exists(inst_base_resize):
8368             self._cleanup_failed_migration(inst_base)
8369             os.rename(inst_base_resize, inst_base)
8370 
8371         root_disk = self.image_backend.by_name(instance, 'disk')
8372         # Once we rollback, the snapshot is no longer needed, so remove it
8373         # TODO(nic): Remove the try/except/finally in a future release
8374         # To avoid any upgrade issues surrounding instances being in pending
8375         # resize state when the software is updated, this portion of the
8376         # method logs exceptions rather than failing on them.  Once it can be
8377         # reasonably assumed that no such instances exist in the wild
8378         # anymore, the try/except/finally should be removed,
8379         # and ignore_errors should be set back to False (the default) so
8380         # that problems throw errors, like they should.
8381         if root_disk.exists():
8382             try:
8383                 root_disk.rollback_to_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME)
8384             except exception.SnapshotNotFound:
8385                 LOG.warning("Failed to rollback snapshot (%s)",
8386                             libvirt_utils.RESIZE_SNAPSHOT_NAME)
8387             finally:
8388                 root_disk.remove_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME,
8389                                       ignore_errors=True)
8390 
8391         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
8392                                             instance,
8393                                             instance.image_meta,
8394                                             block_device_info)
8395         xml = self._get_guest_xml(context, instance, network_info, disk_info,
8396                                   instance.image_meta,
8397                                   block_device_info=block_device_info)
8398         self._create_domain_and_network(context, xml, instance, network_info,
8399                                         block_device_info=block_device_info,
8400                                         power_on=power_on,
8401                                         vifs_already_plugged=True)
8402 
8403         if power_on:
8404             timer = loopingcall.FixedIntervalLoopingCall(
8405                                                     self._wait_for_running,
8406                                                     instance)
8407             timer.start(interval=0.5).wait()
8408 
8409         LOG.debug("finish_revert_migration finished successfully.",
8410                   instance=instance)
8411 
8412     def confirm_migration(self, context, migration, instance, network_info):
8413         """Confirms a resize, destroying the source VM."""
8414         self._cleanup_resize(context, instance, network_info)
8415 
8416     @staticmethod
8417     def _get_io_devices(xml_doc):
8418         """get the list of io devices from the xml document."""
8419         result = {"volumes": [], "ifaces": []}
8420         try:
8421             doc = etree.fromstring(xml_doc)
8422         except Exception:
8423             return result
8424         blocks = [('./devices/disk', 'volumes'),
8425             ('./devices/interface', 'ifaces')]
8426         for block, key in blocks:
8427             section = doc.findall(block)
8428             for node in section:
8429                 for child in node.getchildren():
8430                     if child.tag == 'target' and child.get('dev'):
8431                         result[key].append(child.get('dev'))
8432         return result
8433 
8434     def get_diagnostics(self, instance):
8435         guest = self._host.get_guest(instance)
8436 
8437         # TODO(sahid): We are converting all calls from a
8438         # virDomain object to use nova.virt.libvirt.Guest.
8439         # We should be able to remove domain at the end.
8440         domain = guest._domain
8441         output = {}
8442         # get cpu time, might launch an exception if the method
8443         # is not supported by the underlying hypervisor being
8444         # used by libvirt
8445         try:
8446             for vcpu in guest.get_vcpus_info():
8447                 output["cpu" + str(vcpu.id) + "_time"] = vcpu.time
8448         except libvirt.libvirtError:
8449             pass
8450         # get io status
8451         xml = guest.get_xml_desc()
8452         dom_io = LibvirtDriver._get_io_devices(xml)
8453         for guest_disk in dom_io["volumes"]:
8454             try:
8455                 # blockStats might launch an exception if the method
8456                 # is not supported by the underlying hypervisor being
8457                 # used by libvirt
8458                 stats = domain.blockStats(guest_disk)
8459                 output[guest_disk + "_read_req"] = stats[0]
8460                 output[guest_disk + "_read"] = stats[1]
8461                 output[guest_disk + "_write_req"] = stats[2]
8462                 output[guest_disk + "_write"] = stats[3]
8463                 output[guest_disk + "_errors"] = stats[4]
8464             except libvirt.libvirtError:
8465                 pass
8466         for interface in dom_io["ifaces"]:
8467             try:
8468                 # interfaceStats might launch an exception if the method
8469                 # is not supported by the underlying hypervisor being
8470                 # used by libvirt
8471                 stats = domain.interfaceStats(interface)
8472                 output[interface + "_rx"] = stats[0]
8473                 output[interface + "_rx_packets"] = stats[1]
8474                 output[interface + "_rx_errors"] = stats[2]
8475                 output[interface + "_rx_drop"] = stats[3]
8476                 output[interface + "_tx"] = stats[4]
8477                 output[interface + "_tx_packets"] = stats[5]
8478                 output[interface + "_tx_errors"] = stats[6]
8479                 output[interface + "_tx_drop"] = stats[7]
8480             except libvirt.libvirtError:
8481                 pass
8482         output["memory"] = domain.maxMemory()
8483         # memoryStats might launch an exception if the method
8484         # is not supported by the underlying hypervisor being
8485         # used by libvirt
8486         try:
8487             mem = domain.memoryStats()
8488             for key in mem.keys():
8489                 output["memory-" + key] = mem[key]
8490         except (libvirt.libvirtError, AttributeError):
8491             pass
8492         return output
8493 
8494     def get_instance_diagnostics(self, instance):
8495         guest = self._host.get_guest(instance)
8496 
8497         # TODO(sahid): We are converting all calls from a
8498         # virDomain object to use nova.virt.libvirt.Guest.
8499         # We should be able to remove domain at the end.
8500         domain = guest._domain
8501 
8502         xml = guest.get_xml_desc()
8503         xml_doc = etree.fromstring(xml)
8504 
8505         # TODO(sahid): Needs to use get_info but more changes have to
8506         # be done since a mapping STATE_MAP LIBVIRT_POWER_STATE is
8507         # needed.
8508         (state, max_mem, mem, num_cpu, cpu_time) = \
8509             guest._get_domain_info(self._host)
8510         config_drive = configdrive.required_by(instance)
8511         launched_at = timeutils.normalize_time(instance.launched_at)
8512         uptime = timeutils.delta_seconds(launched_at,
8513                                          timeutils.utcnow())
8514         diags = diagnostics_obj.Diagnostics(state=power_state.STATE_MAP[state],
8515                                         driver='libvirt',
8516                                         config_drive=config_drive,
8517                                         hypervisor=CONF.libvirt.virt_type,
8518                                         hypervisor_os='linux',
8519                                         uptime=uptime)
8520         diags.memory_details = diagnostics_obj.MemoryDiagnostics(
8521             maximum=max_mem / units.Mi,
8522             used=mem / units.Mi)
8523 
8524         # get cpu time, might launch an exception if the method
8525         # is not supported by the underlying hypervisor being
8526         # used by libvirt
8527         try:
8528             for vcpu in guest.get_vcpus_info():
8529                 diags.add_cpu(id=vcpu.id, time=vcpu.time)
8530         except libvirt.libvirtError:
8531             pass
8532         # get io status
8533         dom_io = LibvirtDriver._get_io_devices(xml)
8534         for guest_disk in dom_io["volumes"]:
8535             try:
8536                 # blockStats might launch an exception if the method
8537                 # is not supported by the underlying hypervisor being
8538                 # used by libvirt
8539                 stats = domain.blockStats(guest_disk)
8540                 diags.add_disk(read_bytes=stats[1],
8541                                read_requests=stats[0],
8542                                write_bytes=stats[3],
8543                                write_requests=stats[2],
8544                                errors_count=stats[4])
8545             except libvirt.libvirtError:
8546                 pass
8547         for interface in dom_io["ifaces"]:
8548             try:
8549                 # interfaceStats might launch an exception if the method
8550                 # is not supported by the underlying hypervisor being
8551                 # used by libvirt
8552                 stats = domain.interfaceStats(interface)
8553                 diags.add_nic(rx_octets=stats[0],
8554                               rx_errors=stats[2],
8555                               rx_drop=stats[3],
8556                               rx_packets=stats[1],
8557                               tx_octets=stats[4],
8558                               tx_errors=stats[6],
8559                               tx_drop=stats[7],
8560                               tx_packets=stats[5])
8561             except libvirt.libvirtError:
8562                 pass
8563 
8564         # Update mac addresses of interface if stats have been reported
8565         if diags.nic_details:
8566             nodes = xml_doc.findall('./devices/interface/mac')
8567             for index, node in enumerate(nodes):
8568                 diags.nic_details[index].mac_address = node.get('address')
8569         return diags
8570 
8571     @staticmethod
8572     def _prepare_device_bus(dev):
8573         """Determines the device bus and its hypervisor assigned address
8574         """
8575         bus = None
8576         address = (dev.device_addr.format_address() if
8577                    dev.device_addr else None)
8578         if isinstance(dev.device_addr,
8579                       vconfig.LibvirtConfigGuestDeviceAddressPCI):
8580             bus = objects.PCIDeviceBus()
8581         elif isinstance(dev, vconfig.LibvirtConfigGuestDisk):
8582             if dev.target_bus == 'scsi':
8583                 bus = objects.SCSIDeviceBus()
8584             elif dev.target_bus == 'ide':
8585                 bus = objects.IDEDeviceBus()
8586             elif dev.target_bus == 'usb':
8587                 bus = objects.USBDeviceBus()
8588         if address is not None and bus is not None:
8589             bus.address = address
8590         return bus
8591 
8592     def _build_device_metadata(self, context, instance):
8593         """Builds a metadata object for instance devices, that maps the user
8594            provided tag to the hypervisor assigned device address.
8595         """
8596         def _get_device_name(bdm):
8597             return block_device.strip_dev(bdm.device_name)
8598 
8599         network_info = instance.info_cache.network_info
8600         vlans_by_mac = netutils.get_cached_vifs_with_vlan(network_info)
8601         trusted_by_mac = netutils.get_cached_vifs_with_trusted(network_info)
8602         vifs = objects.VirtualInterfaceList.get_by_instance_uuid(context,
8603                                                                  instance.uuid)
8604         vifs_to_expose = {vif.address: vif for vif in vifs
8605                           if ('tag' in vif and vif.tag) or
8606                              vlans_by_mac.get(vif.address)}
8607         # TODO(mriedem): We should be able to avoid the DB query here by using
8608         # block_device_info['block_device_mapping'] which is passed into most
8609         # methods that call this function.
8610         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
8611             context, instance.uuid)
8612         tagged_bdms = {_get_device_name(bdm): bdm for bdm in bdms if bdm.tag}
8613 
8614         devices = []
8615         guest = self._host.get_guest(instance)
8616         xml = guest.get_xml_desc()
8617         xml_dom = etree.fromstring(xml)
8618         guest_config = vconfig.LibvirtConfigGuest()
8619         guest_config.parse_dom(xml_dom)
8620 
8621         for dev in guest_config.devices:
8622             # Build network interfaces related metadata
8623             if isinstance(dev, vconfig.LibvirtConfigGuestInterface):
8624                 vif = vifs_to_expose.get(dev.mac_addr)
8625                 if not vif:
8626                     continue
8627                 bus = self._prepare_device_bus(dev)
8628                 device = objects.NetworkInterfaceMetadata(mac=vif.address)
8629                 if 'tag' in vif and vif.tag:
8630                     device.tags = [vif.tag]
8631                 if bus:
8632                     device.bus = bus
8633                 vlan = vlans_by_mac.get(vif.address)
8634                 if vlan:
8635                     device.vlan = int(vlan)
8636                 device.vf_trusted = trusted_by_mac.get(vif.address, False)
8637                 devices.append(device)
8638 
8639             # Build disks related metadata
8640             if isinstance(dev, vconfig.LibvirtConfigGuestDisk):
8641                 bdm = tagged_bdms.get(dev.target_dev)
8642                 if not bdm:
8643                     continue
8644                 bus = self._prepare_device_bus(dev)
8645                 device = objects.DiskMetadata(tags=[bdm.tag])
8646                 # NOTE(artom) Setting the serial (which corresponds to
8647                 # volume_id in BlockDeviceMapping) in DiskMetadata allows us to
8648                 # find the disks's BlockDeviceMapping object when we detach the
8649                 # volume and want to clean up its metadata.
8650                 device.serial = bdm.volume_id
8651                 if bus:
8652                     device.bus = bus
8653                 devices.append(device)
8654         if devices:
8655             dev_meta = objects.InstanceDeviceMetadata(devices=devices)
8656             return dev_meta
8657 
8658     def instance_on_disk(self, instance):
8659         # ensure directories exist and are writable
8660         instance_path = libvirt_utils.get_instance_path(instance)
8661         LOG.debug('Checking instance files accessibility %s', instance_path,
8662                   instance=instance)
8663         shared_instance_path = os.access(instance_path, os.W_OK)
8664         # NOTE(flwang): For shared block storage scenario, the file system is
8665         # not really shared by the two hosts, but the volume of evacuated
8666         # instance is reachable.
8667         shared_block_storage = (self.image_backend.backend().
8668                                 is_shared_block_storage())
8669         return shared_instance_path or shared_block_storage
8670 
8671     def inject_network_info(self, instance, nw_info):
8672         self.firewall_driver.setup_basic_filtering(instance, nw_info)
8673 
8674     def delete_instance_files(self, instance):
8675         target = libvirt_utils.get_instance_path(instance)
8676         # A resize may be in progress
8677         target_resize = target + '_resize'
8678         # Other threads may attempt to rename the path, so renaming the path
8679         # to target + '_del' (because it is atomic) and iterating through
8680         # twice in the unlikely event that a concurrent rename occurs between
8681         # the two rename attempts in this method. In general this method
8682         # should be fairly thread-safe without these additional checks, since
8683         # other operations involving renames are not permitted when the task
8684         # state is not None and the task state should be set to something
8685         # other than None by the time this method is invoked.
8686         target_del = target + '_del'
8687         for i in range(2):
8688             try:
8689                 os.rename(target, target_del)
8690                 break
8691             except Exception:
8692                 pass
8693             try:
8694                 os.rename(target_resize, target_del)
8695                 break
8696             except Exception:
8697                 pass
8698         # Either the target or target_resize path may still exist if all
8699         # rename attempts failed.
8700         remaining_path = None
8701         for p in (target, target_resize):
8702             if os.path.exists(p):
8703                 remaining_path = p
8704                 break
8705 
8706         # A previous delete attempt may have been interrupted, so target_del
8707         # may exist even if all rename attempts during the present method
8708         # invocation failed due to the absence of both target and
8709         # target_resize.
8710         if not remaining_path and os.path.exists(target_del):
8711             self.job_tracker.terminate_jobs(instance)
8712 
8713             LOG.info('Deleting instance files %s', target_del,
8714                      instance=instance)
8715             remaining_path = target_del
8716             try:
8717                 shutil.rmtree(target_del)
8718             except OSError as e:
8719                 LOG.error('Failed to cleanup directory %(target)s: %(e)s',
8720                           {'target': target_del, 'e': e}, instance=instance)
8721 
8722         # It is possible that the delete failed, if so don't mark the instance
8723         # as cleaned.
8724         if remaining_path and os.path.exists(remaining_path):
8725             LOG.info('Deletion of %s failed', remaining_path,
8726                      instance=instance)
8727             return False
8728 
8729         LOG.info('Deletion of %s complete', target_del, instance=instance)
8730         return True
8731 
8732     @property
8733     def need_legacy_block_device_info(self):
8734         return False
8735 
8736     def default_root_device_name(self, instance, image_meta, root_bdm):
8737         disk_bus = blockinfo.get_disk_bus_for_device_type(
8738             instance, CONF.libvirt.virt_type, image_meta, "disk")
8739         cdrom_bus = blockinfo.get_disk_bus_for_device_type(
8740             instance, CONF.libvirt.virt_type, image_meta, "cdrom")
8741         root_info = blockinfo.get_root_info(
8742             instance, CONF.libvirt.virt_type, image_meta,
8743             root_bdm, disk_bus, cdrom_bus)
8744         return block_device.prepend_dev(root_info['dev'])
8745 
8746     def default_device_names_for_instance(self, instance, root_device_name,
8747                                           *block_device_lists):
8748         block_device_mapping = list(itertools.chain(*block_device_lists))
8749         # NOTE(ndipanov): Null out the device names so that blockinfo code
8750         #                 will assign them
8751         for bdm in block_device_mapping:
8752             if bdm.device_name is not None:
8753                 LOG.warning(
8754                     "Ignoring supplied device name: %(device_name)s. "
8755                     "Libvirt can't honour user-supplied dev names",
8756                     {'device_name': bdm.device_name}, instance=instance)
8757                 bdm.device_name = None
8758         block_device_info = driver.get_block_device_info(instance,
8759                                                          block_device_mapping)
8760 
8761         blockinfo.default_device_names(CONF.libvirt.virt_type,
8762                                        nova_context.get_admin_context(),
8763                                        instance,
8764                                        block_device_info,
8765                                        instance.image_meta)
8766 
8767     def get_device_name_for_instance(self, instance, bdms, block_device_obj):
8768         block_device_info = driver.get_block_device_info(instance, bdms)
8769         instance_info = blockinfo.get_disk_info(
8770                 CONF.libvirt.virt_type, instance,
8771                 instance.image_meta, block_device_info=block_device_info)
8772 
8773         suggested_dev_name = block_device_obj.device_name
8774         if suggested_dev_name is not None:
8775             LOG.warning(
8776                 'Ignoring supplied device name: %(suggested_dev)s',
8777                 {'suggested_dev': suggested_dev_name}, instance=instance)
8778 
8779         # NOTE(ndipanov): get_info_from_bdm will generate the new device name
8780         #                 only when it's actually not set on the bd object
8781         block_device_obj.device_name = None
8782         disk_info = blockinfo.get_info_from_bdm(
8783             instance, CONF.libvirt.virt_type, instance.image_meta,
8784             block_device_obj, mapping=instance_info['mapping'])
8785         return block_device.prepend_dev(disk_info['dev'])
8786 
8787     def is_supported_fs_format(self, fs_type):
8788         return fs_type in [nova.privsep.fs.FS_FORMAT_EXT2,
8789                            nova.privsep.fs.FS_FORMAT_EXT3,
8790                            nova.privsep.fs.FS_FORMAT_EXT4,
8791                            nova.privsep.fs.FS_FORMAT_XFS]
