Please review the code below for security defects using the CWE (Common Weakness Enumeration) as a reference standard. If any are found, please describe the security defect in detail and indicate the corresponding line number of code and solution. If none are detected, state: 'No security defects are detected in the code'.

1 #    Copyright 2015 Red Hat, Inc.
2 #
3 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
4 #    not use this file except in compliance with the License. You may obtain
5 #    a copy of the License at
6 #
7 #         http://www.apache.org/licenses/LICENSE-2.0
8 #
9 #    Unless required by applicable law or agreed to in writing, software
10 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
11 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
12 #    License for the specific language governing permissions and limitations
13 #    under the License.
14 import copy
15 import itertools
16 
17 from oslo_log import log as logging
18 from oslo_serialization import jsonutils
19 from oslo_utils import versionutils
20 
21 
22 from nova.db.sqlalchemy import api as db
23 from nova.db.sqlalchemy import api_models
24 from nova import exception
25 from nova import objects
26 from nova.objects import base
27 from nova.objects import fields
28 from nova.objects import instance as obj_instance
29 from nova.virt import hardware
30 
31 LOG = logging.getLogger(__name__)
32 
33 REQUEST_SPEC_OPTIONAL_ATTRS = ['requested_destination',
34                                'security_groups',
35                                'network_metadata',
36                                'requested_resources']
37 
38 
39 @base.NovaObjectRegistry.register
40 class RequestSpec(base.NovaObject):
41     # Version 1.0: Initial version
42     # Version 1.1: ImageMeta version 1.6
43     # Version 1.2: SchedulerRetries version 1.1
44     # Version 1.3: InstanceGroup version 1.10
45     # Version 1.4: ImageMeta version 1.7
46     # Version 1.5: Added get_by_instance_uuid(), create(), save()
47     # Version 1.6: Added requested_destination
48     # Version 1.7: Added destroy()
49     # Version 1.8: Added security_groups
50     # Version 1.9: Added user_id
51     # Version 1.10: Added network_metadata
52     # Version 1.11: Added is_bfv
53     # Version 1.12: Added requested_resources
54     VERSION = '1.12'
55 
56     fields = {
57         'id': fields.IntegerField(),
58         'image': fields.ObjectField('ImageMeta', nullable=True),
59         'numa_topology': fields.ObjectField('InstanceNUMATopology',
60                                             nullable=True),
61         'pci_requests': fields.ObjectField('InstancePCIRequests',
62                                            nullable=True),
63         # TODO(mriedem): The project_id shouldn't be nullable since the
64         # scheduler relies on it being set.
65         'project_id': fields.StringField(nullable=True),
66         'user_id': fields.StringField(nullable=True),
67         'availability_zone': fields.StringField(nullable=True),
68         'flavor': fields.ObjectField('Flavor', nullable=False),
69         'num_instances': fields.IntegerField(default=1),
70         'ignore_hosts': fields.ListOfStringsField(nullable=True, default=None),
71         # NOTE(mriedem): In reality, you can only ever have one
72         # host in the force_hosts list. The fact this is a list
73         # is a mistake perpetuated over time.
74         'force_hosts': fields.ListOfStringsField(nullable=True),
75         # NOTE(mriedem): In reality, you can only ever have one
76         # node in the force_nodes list. The fact this is a list
77         # is a mistake perpetuated over time.
78         'force_nodes': fields.ListOfStringsField(nullable=True),
79         'requested_destination': fields.ObjectField('Destination',
80                                                     nullable=True,
81                                                     default=None),
82         'retry': fields.ObjectField('SchedulerRetries', nullable=True,
83                                     default=None),
84         'limits': fields.ObjectField('SchedulerLimits', nullable=True),
85         'instance_group': fields.ObjectField('InstanceGroup', nullable=True),
86         # NOTE(sbauza): Since hints are depending on running filters, we prefer
87         # to leave the API correctly validating the hints per the filters and
88         # just provide to the RequestSpec object a free-form dictionary
89         'scheduler_hints': fields.DictOfListOfStringsField(nullable=True),
90         'instance_uuid': fields.UUIDField(),
91         'security_groups': fields.ObjectField('SecurityGroupList'),
92         'network_metadata': fields.ObjectField('NetworkMetadata',
93                                                default=objects.NetworkMetadata(
94                                                    physnets=set(),
95                                                    tunneled=False)),
96         'is_bfv': fields.BooleanField(),
97         # NOTE(gibi): Eventually we want to store every resource request as
98         # RequestGroup objects here. However currently the flavor based
99         # resources like vcpu, ram, disk, and flavor.extra_spec based resources
100         # are not handled this way. See the Todo in from_components() where
101         # requested_resources are set.
102         'requested_resources': fields.ListOfObjectsField('RequestGroup',
103                                                          nullable=True,
104                                                          default=None)
105     }
106 
107     def obj_make_compatible(self, primitive, target_version):
108         super(RequestSpec, self).obj_make_compatible(primitive, target_version)
109         target_version = versionutils.convert_version_to_tuple(target_version)
110         if target_version < (1, 12):
111             if 'requested_resources' in primitive:
112                 del primitive['requested_resources']
113         if target_version < (1, 11) and 'is_bfv' in primitive:
114             del primitive['is_bfv']
115         if target_version < (1, 10):
116             if 'network_metadata' in primitive:
117                 del primitive['network_metadata']
118         if target_version < (1, 9):
119             if 'user_id' in primitive:
120                 del primitive['user_id']
121         if target_version < (1, 8):
122             if 'security_groups' in primitive:
123                 del primitive['security_groups']
124         if target_version < (1, 6):
125             if 'requested_destination' in primitive:
126                 del primitive['requested_destination']
127 
128     def obj_load_attr(self, attrname):
129         if attrname not in REQUEST_SPEC_OPTIONAL_ATTRS:
130             raise exception.ObjectActionError(
131                 action='obj_load_attr',
132                 reason='attribute %s not lazy-loadable' % attrname)
133 
134         if attrname == 'security_groups':
135             self.security_groups = objects.SecurityGroupList(objects=[])
136             return
137 
138         # NOTE(sbauza): In case the primitive was not providing that field
139         # because of a previous RequestSpec version, we want to default
140         # that field in order to have the same behaviour.
141         self.obj_set_defaults(attrname)
142 
143     @property
144     def vcpus(self):
145         return self.flavor.vcpus
146 
147     @property
148     def memory_mb(self):
149         return self.flavor.memory_mb
150 
151     @property
152     def root_gb(self):
153         return self.flavor.root_gb
154 
155     @property
156     def ephemeral_gb(self):
157         return self.flavor.ephemeral_gb
158 
159     @property
160     def swap(self):
161         return self.flavor.swap
162 
163     def _image_meta_from_image(self, image):
164         if isinstance(image, objects.ImageMeta):
165             self.image = image
166         elif isinstance(image, dict):
167             # NOTE(sbauza): Until Nova is fully providing an ImageMeta object
168             # for getting properties, we still need to hydrate it here
169             # TODO(sbauza): To be removed once all RequestSpec hydrations are
170             # done on the conductor side and if the image is an ImageMeta
171             self.image = objects.ImageMeta.from_dict(image)
172         else:
173             self.image = None
174 
175     def _from_instance(self, instance):
176         if isinstance(instance, obj_instance.Instance):
177             # NOTE(sbauza): Instance should normally be a NovaObject...
178             getter = getattr
179         elif isinstance(instance, dict):
180             # NOTE(sbauza): ... but there are some cases where request_spec
181             # has an instance key as a dictionary, just because
182             # select_destinations() is getting a request_spec dict made by
183             # sched_utils.build_request_spec()
184             # TODO(sbauza): To be removed once all RequestSpec hydrations are
185             # done on the conductor side
186             getter = lambda x, y: x.get(y)
187         else:
188             # If the instance is None, there is no reason to set the fields
189             return
190 
191         instance_fields = ['numa_topology', 'pci_requests', 'uuid',
192                            'project_id', 'user_id', 'availability_zone']
193         for field in instance_fields:
194             if field == 'uuid':
195                 setattr(self, 'instance_uuid', getter(instance, field))
196             elif field == 'pci_requests':
197                 self._from_instance_pci_requests(getter(instance, field))
198             elif field == 'numa_topology':
199                 self._from_instance_numa_topology(getter(instance, field))
200             else:
201                 setattr(self, field, getter(instance, field))
202 
203     def _from_instance_pci_requests(self, pci_requests):
204         if isinstance(pci_requests, dict):
205             pci_req_cls = objects.InstancePCIRequests
206             self.pci_requests = pci_req_cls.from_request_spec_instance_props(
207                 pci_requests)
208         else:
209             self.pci_requests = pci_requests
210 
211     def _from_instance_numa_topology(self, numa_topology):
212         if isinstance(numa_topology, dict):
213             self.numa_topology = hardware.instance_topology_from_instance(
214                 dict(numa_topology=numa_topology))
215         else:
216             self.numa_topology = numa_topology
217 
218     def _from_flavor(self, flavor):
219         if isinstance(flavor, objects.Flavor):
220             self.flavor = flavor
221         elif isinstance(flavor, dict):
222             # NOTE(sbauza): Again, request_spec is primitived by
223             # sched_utils.build_request_spec() and passed to
224             # select_destinations() like this
225             # TODO(sbauza): To be removed once all RequestSpec hydrations are
226             # done on the conductor side
227             self.flavor = objects.Flavor(**flavor)
228 
229     def _from_retry(self, retry_dict):
230         self.retry = (SchedulerRetries.from_dict(self._context, retry_dict)
231                       if retry_dict else None)
232 
233     def _populate_group_info(self, filter_properties):
234         if filter_properties.get('instance_group'):
235             # New-style group information as a NovaObject, we can directly set
236             # the field
237             self.instance_group = filter_properties.get('instance_group')
238         elif filter_properties.get('group_updated') is True:
239             # Old-style group information having ugly dict keys containing sets
240             # NOTE(sbauza): Can be dropped once select_destinations is removed
241             policies = list(filter_properties.get('group_policies'))
242             hosts = list(filter_properties.get('group_hosts'))
243             members = list(filter_properties.get('group_members'))
244             self.instance_group = objects.InstanceGroup(policy=policies[0],
245                                                         hosts=hosts,
246                                                         members=members)
247             # InstanceGroup.uuid is not nullable so only set it if we got it.
248             group_uuid = filter_properties.get('group_uuid')
249             if group_uuid:
250                 self.instance_group.uuid = group_uuid
251             # hosts has to be not part of the updates for saving the object
252             self.instance_group.obj_reset_changes(['hosts'])
253         else:
254             # Set the value anyway to avoid any call to obj_attr_is_set for it
255             self.instance_group = None
256 
257     def _from_limits(self, limits):
258         if isinstance(limits, dict):
259             self.limits = SchedulerLimits.from_dict(limits)
260         else:
261             # Already a SchedulerLimits object.
262             self.limits = limits
263 
264     def _from_hints(self, hints_dict):
265         if hints_dict is None:
266             self.scheduler_hints = None
267             return
268         self.scheduler_hints = {
269             hint: value if isinstance(value, list) else [value]
270             for hint, value in hints_dict.items()}
271 
272     @classmethod
273     def from_primitives(cls, context, request_spec, filter_properties):
274         """Returns a new RequestSpec object by hydrating it from legacy dicts.
275 
276         Deprecated.  A RequestSpec object is created early in the boot process
277         using the from_components method.  That object will either be passed to
278         places that require it, or it can be looked up with
279         get_by_instance_uuid.  This method can be removed when there are no
280         longer any callers.  Because the method is not remotable it is not tied
281         to object versioning.
282 
283         That helper is not intended to leave the legacy dicts kept in the nova
284         codebase, but is rather just for giving a temporary solution for
285         populating the Spec object until we get rid of scheduler_utils'
286         build_request_spec() and the filter_properties hydratation in the
287         conductor.
288 
289         :param context: a context object
290         :param request_spec: An old-style request_spec dictionary
291         :param filter_properties: An old-style filter_properties dictionary
292         """
293         num_instances = request_spec.get('num_instances', 1)
294         spec = cls(context, num_instances=num_instances)
295         # Hydrate from request_spec first
296         image = request_spec.get('image')
297         spec._image_meta_from_image(image)
298         instance = request_spec.get('instance_properties')
299         spec._from_instance(instance)
300         flavor = request_spec.get('instance_type')
301         spec._from_flavor(flavor)
302         # Hydrate now from filter_properties
303         spec.ignore_hosts = filter_properties.get('ignore_hosts')
304         spec.force_hosts = filter_properties.get('force_hosts')
305         spec.force_nodes = filter_properties.get('force_nodes')
306         retry = filter_properties.get('retry', {})
307         spec._from_retry(retry)
308         limits = filter_properties.get('limits', {})
309         spec._from_limits(limits)
310         spec._populate_group_info(filter_properties)
311         scheduler_hints = filter_properties.get('scheduler_hints', {})
312         spec._from_hints(scheduler_hints)
313         spec.requested_destination = filter_properties.get(
314             'requested_destination')
315 
316         # NOTE(sbauza): Default the other fields that are not part of the
317         # original contract
318         spec.obj_set_defaults()
319 
320         return spec
321 
322     def get_scheduler_hint(self, hint_name, default=None):
323         """Convenient helper for accessing a particular scheduler hint since
324         it is hydrated by putting a single item into a list.
325 
326         In order to reduce the complexity, that helper returns a string if the
327         requested hint is a list of only one value, and if not, returns the
328         value directly (ie. the list). If the hint is not existing (or
329         scheduler_hints is None), then it returns the default value.
330 
331         :param hint_name: name of the hint
332         :param default: the default value if the hint is not there
333         """
334         if (not self.obj_attr_is_set('scheduler_hints')
335                 or self.scheduler_hints is None):
336             return default
337         hint_val = self.scheduler_hints.get(hint_name, default)
338         return (hint_val[0] if isinstance(hint_val, list)
339                 and len(hint_val) == 1 else hint_val)
340 
341     def _to_legacy_image(self):
342         return base.obj_to_primitive(self.image) if (
343             self.obj_attr_is_set('image') and self.image) else {}
344 
345     def _to_legacy_instance(self):
346         # NOTE(sbauza): Since the RequestSpec only persists a few Instance
347         # fields, we can only return a dict.
348         instance = {}
349         instance_fields = ['numa_topology', 'pci_requests',
350                            'project_id', 'user_id', 'availability_zone',
351                            'instance_uuid']
352         for field in instance_fields:
353             if not self.obj_attr_is_set(field):
354                 continue
355             if field == 'instance_uuid':
356                 instance['uuid'] = getattr(self, field)
357             else:
358                 instance[field] = getattr(self, field)
359         flavor_fields = ['root_gb', 'ephemeral_gb', 'memory_mb', 'vcpus']
360         if not self.obj_attr_is_set('flavor'):
361             return instance
362         for field in flavor_fields:
363             instance[field] = getattr(self.flavor, field)
364         return instance
365 
366     def _to_legacy_group_info(self):
367         # NOTE(sbauza): Since this is only needed until the AffinityFilters are
368         # modified by using directly the RequestSpec object, we need to keep
369         # the existing dictionary as a primitive.
370         return {'group_updated': True,
371                 'group_hosts': set(self.instance_group.hosts),
372                 'group_policies': set([self.instance_group.policy]),
373                 'group_members': set(self.instance_group.members),
374                 'group_uuid': self.instance_group.uuid}
375 
376     def to_legacy_request_spec_dict(self):
377         """Returns a legacy request_spec dict from the RequestSpec object.
378 
379         Since we need to manage backwards compatibility and rolling upgrades
380         within our RPC API, we need to accept to provide an helper for
381         primitiving the right RequestSpec object into a legacy dict until we
382         drop support for old Scheduler RPC API versions.
383         If you don't understand why this method is needed, please don't use it.
384         """
385         req_spec = {}
386         if not self.obj_attr_is_set('num_instances'):
387             req_spec['num_instances'] = self.fields['num_instances'].default
388         else:
389             req_spec['num_instances'] = self.num_instances
390         req_spec['image'] = self._to_legacy_image()
391         req_spec['instance_properties'] = self._to_legacy_instance()
392         if self.obj_attr_is_set('flavor'):
393             req_spec['instance_type'] = self.flavor
394         else:
395             req_spec['instance_type'] = {}
396         return req_spec
397 
398     def to_legacy_filter_properties_dict(self):
399         """Returns a legacy filter_properties dict from the RequestSpec object.
400 
401         Since we need to manage backwards compatibility and rolling upgrades
402         within our RPC API, we need to accept to provide an helper for
403         primitiving the right RequestSpec object into a legacy dict until we
404         drop support for old Scheduler RPC API versions.
405         If you don't understand why this method is needed, please don't use it.
406         """
407         filt_props = {}
408         if self.obj_attr_is_set('ignore_hosts') and self.ignore_hosts:
409             filt_props['ignore_hosts'] = self.ignore_hosts
410         if self.obj_attr_is_set('force_hosts') and self.force_hosts:
411             filt_props['force_hosts'] = self.force_hosts
412         if self.obj_attr_is_set('force_nodes') and self.force_nodes:
413             filt_props['force_nodes'] = self.force_nodes
414         if self.obj_attr_is_set('retry') and self.retry:
415             filt_props['retry'] = self.retry.to_dict()
416         if self.obj_attr_is_set('limits') and self.limits:
417             filt_props['limits'] = self.limits.to_dict()
418         if self.obj_attr_is_set('instance_group') and self.instance_group:
419             filt_props.update(self._to_legacy_group_info())
420         if self.obj_attr_is_set('scheduler_hints') and self.scheduler_hints:
421             # NOTE(sbauza): We need to backport all the hints correctly since
422             # we had to hydrate the field by putting a single item into a list.
423             filt_props['scheduler_hints'] = {hint: self.get_scheduler_hint(
424                 hint) for hint in self.scheduler_hints}
425         if self.obj_attr_is_set('requested_destination'
426                                 ) and self.requested_destination:
427             filt_props['requested_destination'] = self.requested_destination
428         return filt_props
429 
430     @classmethod
431     def from_components(cls, context, instance_uuid, image, flavor,
432             numa_topology, pci_requests, filter_properties, instance_group,
433             availability_zone, security_groups=None, project_id=None,
434             user_id=None, port_resource_requests=None):
435         """Returns a new RequestSpec object hydrated by various components.
436 
437         This helper is useful in creating the RequestSpec from the various
438         objects that are assembled early in the boot process.  This method
439         creates a complete RequestSpec object with all properties set or
440         intentionally left blank.
441 
442         :param context: a context object
443         :param instance_uuid: the uuid of the instance to schedule
444         :param image: a dict of properties for an image or volume
445         :param flavor: a flavor NovaObject
446         :param numa_topology: InstanceNUMATopology or None
447         :param pci_requests: InstancePCIRequests
448         :param filter_properties: a dict of properties for scheduling
449         :param instance_group: None or an instance group NovaObject
450         :param availability_zone: an availability_zone string
451         :param security_groups: A SecurityGroupList object. If None, don't
452                                 set security_groups on the resulting object.
453         :param project_id: The project_id for the requestspec (should match
454                            the instance project_id).
455         :param user_id: The user_id for the requestspec (should match
456                            the instance user_id).
457         :param port_resource_requests: a list of RequestGroup objects
458                                        representing the resource needs of the
459                                        neutron ports
460         """
461         spec_obj = cls(context)
462         spec_obj.num_instances = 1
463         spec_obj.instance_uuid = instance_uuid
464         spec_obj.instance_group = instance_group
465         if spec_obj.instance_group is None and filter_properties:
466             spec_obj._populate_group_info(filter_properties)
467         spec_obj.project_id = project_id or context.project_id
468         spec_obj.user_id = user_id or context.user_id
469         spec_obj._image_meta_from_image(image)
470         spec_obj._from_flavor(flavor)
471         spec_obj._from_instance_pci_requests(pci_requests)
472         spec_obj._from_instance_numa_topology(numa_topology)
473         spec_obj.ignore_hosts = filter_properties.get('ignore_hosts')
474         spec_obj.force_hosts = filter_properties.get('force_hosts')
475         spec_obj.force_nodes = filter_properties.get('force_nodes')
476         spec_obj._from_retry(filter_properties.get('retry', {}))
477         spec_obj._from_limits(filter_properties.get('limits', {}))
478         spec_obj._from_hints(filter_properties.get('scheduler_hints', {}))
479         spec_obj.availability_zone = availability_zone
480         if security_groups is not None:
481             spec_obj.security_groups = security_groups
482         spec_obj.requested_destination = filter_properties.get(
483             'requested_destination')
484 
485         # TODO(gibi): do the creation of the unnumbered group and any
486         # numbered group from the flavor by moving the logic from
487         # nova.scheduler.utils.resources_from_request_spec() here. See also
488         # the comment in the definition of requested_resources field.
489         spec_obj.requested_resources = []
490         if port_resource_requests:
491             spec_obj.requested_resources.extend(port_resource_requests)
492 
493         # NOTE(sbauza): Default the other fields that are not part of the
494         # original contract
495         spec_obj.obj_set_defaults()
496         return spec_obj
497 
498     def ensure_project_and_user_id(self, instance):
499         if 'project_id' not in self or self.project_id is None:
500             self.project_id = instance.project_id
501         if 'user_id' not in self or self.user_id is None:
502             self.user_id = instance.user_id
503 
504     def ensure_network_metadata(self, instance):
505         if not (instance.info_cache and instance.info_cache.network_info):
506             return
507 
508         physnets = set([])
509         tunneled = True
510 
511         # physical_network and tunneled might not be in the cache for old
512         # instances that haven't had their info_cache healed yet
513         for vif in instance.info_cache.network_info:
514             physnet = vif.get('network', {}).get('meta', {}).get(
515                 'physical_network', None)
516             if physnet:
517                 physnets.add(physnet)
518             tunneled |= vif.get('network', {}).get('meta', {}).get(
519                 'tunneled', False)
520 
521         self.network_metadata = objects.NetworkMetadata(
522             physnets=physnets, tunneled=tunneled)
523 
524     @staticmethod
525     def _from_db_object(context, spec, db_spec):
526         spec_obj = spec.obj_from_primitive(jsonutils.loads(db_spec['spec']))
527         for key in spec.fields:
528             # Load these from the db model not the serialized object within,
529             # though they should match.
530             if key in ['id', 'instance_uuid']:
531                 setattr(spec, key, db_spec[key])
532             elif key in ('requested_destination', 'requested_resources',
533                          'network_metadata', 'retry', 'ignore_hosts'):
534                 # Do not override what we already have in the object as this
535                 # field is not persisted. If save() is called after
536                 # requested_resources, requested_destination
537                 # network_metadata, retry or ignore_hosts is populated,
538                 # it will reset the field to None and we'll lose what is set
539                 # (but not persisted) on the object.
540                 spec.obj_set_defaults(key)
541             elif key in spec_obj:
542                 setattr(spec, key, getattr(spec_obj, key))
543         spec._context = context
544 
545         if 'instance_group' in spec and spec.instance_group:
546             # NOTE(mriedem): We could have a half-baked instance group with no
547             # uuid if some legacy translation was performed on this spec in the
548             # past. In that case, try to workaround the issue by getting the
549             # group uuid from the scheduler hint.
550             if 'uuid' not in spec.instance_group:
551                 spec.instance_group.uuid = spec.get_scheduler_hint('group')
552             # NOTE(danms): We don't store the full instance group in
553             # the reqspec since it would be stale almost immediately.
554             # Instead, load it by uuid here so it's up-to-date.
555             try:
556                 spec.instance_group = objects.InstanceGroup.get_by_uuid(
557                     context, spec.instance_group.uuid)
558             except exception.InstanceGroupNotFound:
559                 # NOTE(danms): Instance group may have been deleted
560                 spec.instance_group = None
561 
562         spec.obj_reset_changes()
563         return spec
564 
565     @staticmethod
566     @db.api_context_manager.reader
567     def _get_by_instance_uuid_from_db(context, instance_uuid):
568         db_spec = context.session.query(api_models.RequestSpec).filter_by(
569             instance_uuid=instance_uuid).first()
570         if not db_spec:
571             raise exception.RequestSpecNotFound(
572                     instance_uuid=instance_uuid)
573         return db_spec
574 
575     @base.remotable_classmethod
576     def get_by_instance_uuid(cls, context, instance_uuid):
577         db_spec = cls._get_by_instance_uuid_from_db(context, instance_uuid)
578         return cls._from_db_object(context, cls(), db_spec)
579 
580     @staticmethod
581     @db.api_context_manager.writer
582     def _create_in_db(context, updates):
583         db_spec = api_models.RequestSpec()
584         db_spec.update(updates)
585         db_spec.save(context.session)
586         return db_spec
587 
588     def _get_update_primitives(self):
589         """Serialize object to match the db model.
590 
591         We store copies of embedded objects rather than
592         references to these objects because we want a snapshot of the request
593         at this point.  If the references changed or were deleted we would
594         not be able to reschedule this instance under the same conditions as
595         it was originally scheduled with.
596         """
597         updates = self.obj_get_changes()
598         db_updates = None
599         # NOTE(alaski): The db schema is the full serialized object in a
600         # 'spec' column.  If anything has changed we rewrite the full thing.
601         if updates:
602             # NOTE(danms): Don't persist the could-be-large and could-be-stale
603             # properties of InstanceGroup
604             spec = self.obj_clone()
605             if 'instance_group' in spec and spec.instance_group:
606                 spec.instance_group.members = None
607                 spec.instance_group.hosts = None
608             # NOTE(mriedem): Don't persist retries, requested_destination,
609             # requested_resources or ignored hosts since those are per-request
610             for excluded in ('retry', 'requested_destination',
611                              'requested_resources', 'ignore_hosts'):
612                 if excluded in spec and getattr(spec, excluded):
613                     setattr(spec, excluded, None)
614             # NOTE(stephenfin): Don't persist network metadata since we have
615             # no need for it after scheduling
616             if 'network_metadata' in spec and spec.network_metadata:
617                 del spec.network_metadata
618 
619             db_updates = {'spec': jsonutils.dumps(spec.obj_to_primitive())}
620             if 'instance_uuid' in updates:
621                 db_updates['instance_uuid'] = updates['instance_uuid']
622         return db_updates
623 
624     @base.remotable
625     def create(self):
626         if self.obj_attr_is_set('id'):
627             raise exception.ObjectActionError(action='create',
628                                               reason='already created')
629 
630         updates = self._get_update_primitives()
631         if not updates:
632             raise exception.ObjectActionError(action='create',
633                                               reason='no fields are set')
634         db_spec = self._create_in_db(self._context, updates)
635         self._from_db_object(self._context, self, db_spec)
636 
637     @staticmethod
638     @db.api_context_manager.writer
639     def _save_in_db(context, instance_uuid, updates):
640         # FIXME(sbauza): Provide a classmethod when oslo.db bug #1520195 is
641         # fixed and released
642         db_spec = RequestSpec._get_by_instance_uuid_from_db(context,
643                                                             instance_uuid)
644         db_spec.update(updates)
645         db_spec.save(context.session)
646         return db_spec
647 
648     @base.remotable
649     def save(self):
650         updates = self._get_update_primitives()
651         if updates:
652             db_spec = self._save_in_db(self._context, self.instance_uuid,
653                                        updates)
654             self._from_db_object(self._context, self, db_spec)
655             self.obj_reset_changes()
656 
657     @staticmethod
658     @db.api_context_manager.writer
659     def _destroy_in_db(context, instance_uuid):
660         result = context.session.query(api_models.RequestSpec).filter_by(
661             instance_uuid=instance_uuid).delete()
662         if not result:
663             raise exception.RequestSpecNotFound(instance_uuid=instance_uuid)
664 
665     @base.remotable
666     def destroy(self):
667         self._destroy_in_db(self._context, self.instance_uuid)
668 
669     @staticmethod
670     @db.api_context_manager.writer
671     def _destroy_bulk_in_db(context, instance_uuids):
672         return context.session.query(api_models.RequestSpec).filter(
673                 api_models.RequestSpec.instance_uuid.in_(instance_uuids)).\
674                 delete(synchronize_session=False)
675 
676     @classmethod
677     def destroy_bulk(cls, context, instance_uuids):
678         return cls._destroy_bulk_in_db(context, instance_uuids)
679 
680     def reset_forced_destinations(self):
681         """Clears the forced destination fields from the RequestSpec object.
682 
683         This method is for making sure we don't ask the scheduler to give us
684         again the same destination(s) without persisting the modifications.
685         """
686         self.force_hosts = None
687         self.force_nodes = None
688         # NOTE(sbauza): Make sure we don't persist this, we need to keep the
689         # original request for the forced hosts
690         self.obj_reset_changes(['force_hosts', 'force_nodes'])
691 
692     @property
693     def maps_requested_resources(self):
694         """Returns True if this RequestSpec needs to map requested_resources
695         to resource providers, False otherwise.
696         """
697         return 'requested_resources' in self and self.requested_resources
698 
699     def _is_valid_group_rp_mapping(
700             self, group_rp_mapping, placement_allocations, provider_traits):
701         """Decides if the mapping is valid from resources and traits
702         perspective.
703 
704         :param group_rp_mapping: A list of RequestGroup - RP UUID two tuples
705                                 representing a mapping between request groups
706                                 in this RequestSpec and RPs from the
707                                 allocation. It contains every RequestGroup in
708                                 this RequestSpec but the mapping might not be
709                                 valid from resources and traits perspective.
710         :param placement_allocations: The overall allocation made by the
711                                       scheduler for this RequestSpec
712         :param provider_traits: A dict keyed by resource provider uuids
713                                 containing the list of traits the given RP has.
714                                 This dict contains info only about RPs
715                                 appearing in the placement_allocations param.
716         :return: True if each group's resource and trait request can be
717                  fulfilled from the RP it is mapped to. False otherwise.
718         """
719 
720         # Check that traits are matching for each group - rp pair in
721         # this mapping
722         for group, rp_uuid in group_rp_mapping:
723             if not group.required_traits.issubset(provider_traits[rp_uuid]):
724                 return False
725 
726         # TODO(gibi): add support for groups with forbidden_traits and
727         # aggregates
728 
729         # Check that each group can consume the requested resources from the rp
730         # that it is mapped to in the current mapping. Consume each group's
731         # request from the allocation, if anything drops below zero, then this
732         # is not a solution
733         rcs = set()
734         allocs = copy.deepcopy(placement_allocations)
735         for group, rp_uuid in group_rp_mapping:
736             rp_allocs = allocs[rp_uuid]['resources']
737             for rc, amount in group.resources.items():
738                 rcs.add(rc)
739                 if rc in rp_allocs:
740                     rp_allocs[rc] -= amount
741                     if rp_allocs[rc] < 0:
742                         return False
743                 else:
744                     return False
745 
746         # Check that all the allocations are consumed from the resource
747         # classes that appear in the request groups. It should never happen
748         # that we have a match but also have some leftover if placement returns
749         # valid allocation candidates. Except if the leftover in the allocation
750         # are due to the RC requested in the unnumbered group.
751         for rp_uuid in allocs:
752             rp_allocs = allocs[rp_uuid]['resources']
753             for rc, amount in group.resources.items():
754                 if rc in rcs and rc in rp_allocs:
755                     if rp_allocs[rc] != 0:
756                         LOG.debug(
757                             'Found valid group - RP mapping %s but there are '
758                             'allocations leftover in %s from resource class '
759                             '%s', group_rp_mapping, allocs, rc)
760                         return False
761 
762         # If both the traits and the allocations are OK then mapping is valid
763         return True
764 
765     def map_requested_resources_to_providers(
766             self, placement_allocations, provider_traits):
767         """Fill the provider_uuids field in each RequestGroup objects in the
768         requested_resources field.
769 
770         The mapping is generated based on the overall allocation made for this
771         RequestSpec, the request in each RequestGroup, and the traits of the
772         RPs in the allocation.
773 
774         Limitations:
775         * only groups with use_same_provider = True is mapped, the un-numbered
776           group are not supported.
777         * mapping is generated only based on the resource request and the
778           required traits, aggregate membership and forbidden traits are not
779           supported.
780         * requesting the same resource class in numbered and un-numbered group
781           is not supported
782 
783         We can live with these limitations today as Neutron does not use
784         forbidden traits and aggregates in the request and each Neutron port is
785         mapped to a numbered group and the resources class used by neutron
786         ports are never requested through the flavor extra_spec.
787 
788         This is a workaround as placement does not return which RP fulfills
789         which granular request group in the allocation candidate request. There
790         is a spec proposing a solution in placement:
791         https://review.opendev.org/#/c/597601/
792 
793         :param placement_allocations: The overall allocation made by the
794                                       scheduler for this RequestSpec
795         :param provider_traits: A dict keyed by resource provider uuids
796                                 containing the list of traits the given RP has.
797                                 This dict contains info only about RPs
798                                 appearing in the placement_allocations param.
799         """
800         if not self.maps_requested_resources:
801             # Nothing to do, so let's return early
802             return
803 
804         for group in self.requested_resources:
805             # See the limitations in the func doc above
806             if (not group.use_same_provider
807                     or group.aggregates
808                     or group.forbidden_traits):
809                 raise NotImplementedError()
810 
811         # Iterate through every possible group - RP mappings and try to find a
812         # valid one. If there are more than one possible solution then it is
813         # enough to find one as these solutions are interchangeable from
814         # backend (e.g. Neutron) perspective.
815         LOG.debug('Trying to find a valid group - RP mapping for groups %s to '
816                   'allocations %s with traits %s', self.requested_resources,
817                   placement_allocations, provider_traits)
818 
819         # This generator first creates permutations with repetition of the RPs
820         # with length of the number of groups we have. So if there is
821         #   2 RPs (rp1, rp2) and
822         #   3 groups (g1, g2, g3).
823         # Then the itertools.product(('rp1', 'rp2'), repeat=3)) will be:
824         #  (rp1, rp1, rp1)
825         #  (rp1, rp1, rp2)
826         #  (rp1, rp2, rp1)
827         #  ...
828         #  (rp2, rp2, rp2)
829         # Then we zip each of this permutations to our group list resulting in
830         # a list of list of group - rp pairs:
831         # [[('g1', 'rp1'), ('g2', 'rp1'), ('g3', 'rp1')],
832         #  [('g1', 'rp1'), ('g2', 'rp1'), ('g3', 'rp2')],
833         #  [('g1', 'rp1'), ('g2', 'rp2'), ('g3', 'rp1')],
834         #  ...
835         #  [('g1', 'rp2'), ('g2', 'rp2'), ('g3', 'rp2')]]
836         # NOTE(gibi): the list() around the zip() below is needed as the
837         # algorithm looks into the mapping more than once and zip returns an
838         # iterator in py3.x. Still we need to generate a mapping once hence the
839         # generator expression.
840         every_possible_mapping = (list(zip(self.requested_resources, rps))
841                                   for rps in itertools.product(
842                                       placement_allocations.keys(),
843                                       repeat=len(self.requested_resources)))
844         for mapping in every_possible_mapping:
845             if self._is_valid_group_rp_mapping(
846                     mapping, placement_allocations, provider_traits):
847                 for group, rp in mapping:
848                     # NOTE(gibi): un-numbered group might be mapped to more
849                     # than one RP but we do not support that yet here.
850                     group.provider_uuids = [rp]
851                 LOG.debug('Found valid group - RP mapping %s', mapping)
852                 return
853 
854         # if we reached this point then none of the possible mappings was
855         # valid. This should never happen as Placement returns allocation
856         # candidates based on the overall resource request of the server
857         # including the request of the groups.
858         raise ValueError('No valid group - RP mapping is found for '
859                          'groups %s, allocation %s and provider traits %s' %
860                          (self.requested_resources, placement_allocations,
861                           provider_traits))
862 
863 
864 @base.NovaObjectRegistry.register
865 class Destination(base.NovaObject):
866     # Version 1.0: Initial version
867     # Version 1.1: Add cell field
868     # Version 1.2: Add aggregates field
869     VERSION = '1.2'
870 
871     fields = {
872         'host': fields.StringField(),
873         # NOTE(sbauza): Given we want to split the host/node relationship later
874         # and also remove the possibility to have multiple nodes per service,
875         # let's provide a possible nullable node here.
876         'node': fields.StringField(nullable=True),
877         'cell': fields.ObjectField('CellMapping', nullable=True),
878 
879         # NOTE(dansmith): These are required aggregates (or sets) and
880         # are passed to placement.  See require_aggregates() below.
881         'aggregates': fields.ListOfStringsField(nullable=True,
882                                                 default=None),
883     }
884 
885     def obj_make_compatible(self, primitive, target_version):
886         super(Destination, self).obj_make_compatible(primitive, target_version)
887         target_version = versionutils.convert_version_to_tuple(target_version)
888         if target_version < (1, 2):
889             if 'aggregates' in primitive:
890                 del primitive['aggregates']
891         if target_version < (1, 1):
892             if 'cell' in primitive:
893                 del primitive['cell']
894 
895     def obj_load_attr(self, attrname):
896         self.obj_set_defaults(attrname)
897 
898     def require_aggregates(self, aggregates):
899         """Add a set of aggregates to the list of required aggregates.
900 
901         This will take a list of aggregates, which are to be logically OR'd
902         together and add them to the list of required aggregates that will
903         be used to query placement. Aggregate sets provided in sequential calls
904         to this method will be AND'd together.
905 
906         For example, the following set of calls:
907             dest.require_aggregates(['foo', 'bar'])
908             dest.require_aggregates(['baz'])
909         will generate the following logical query to placement:
910             "Candidates should be in 'foo' OR 'bar', but definitely in 'baz'"
911 
912         :param aggregates: A list of aggregates, at least one of which
913                            must contain the destination host.
914 
915         """
916         if self.aggregates is None:
917             self.aggregates = []
918         self.aggregates.append(','.join(aggregates))
919 
920 
921 @base.NovaObjectRegistry.register
922 class SchedulerRetries(base.NovaObject):
923     # Version 1.0: Initial version
924     # Version 1.1: ComputeNodeList version 1.14
925     VERSION = '1.1'
926 
927     fields = {
928         'num_attempts': fields.IntegerField(),
929         # NOTE(sbauza): Even if we are only using host/node strings, we need to
930         # know which compute nodes were tried
931         'hosts': fields.ObjectField('ComputeNodeList'),
932     }
933 
934     @classmethod
935     def from_dict(cls, context, retry_dict):
936         # NOTE(sbauza): We are not persisting the user context since it's only
937         # needed for hydrating the Retry object
938         retry_obj = cls()
939         if not ('num_attempts' and 'hosts') in retry_dict:
940             # NOTE(sbauza): We prefer to return an empty object if the
941             # primitive is not good enough
942             return retry_obj
943         retry_obj.num_attempts = retry_dict.get('num_attempts')
944         # NOTE(sbauza): each retry_dict['hosts'] item is a list of [host, node]
945         computes = [objects.ComputeNode(context=context, host=host,
946                                         hypervisor_hostname=node)
947                     for host, node in retry_dict.get('hosts')]
948         retry_obj.hosts = objects.ComputeNodeList(objects=computes)
949         return retry_obj
950 
951     def to_dict(self):
952         legacy_hosts = [[cn.host, cn.hypervisor_hostname] for cn in self.hosts]
953         return {'num_attempts': self.num_attempts,
954                 'hosts': legacy_hosts}
955 
956 
957 @base.NovaObjectRegistry.register
958 class SchedulerLimits(base.NovaObject):
959     # Version 1.0: Initial version
960     VERSION = '1.0'
961 
962     fields = {
963         'numa_topology': fields.ObjectField('NUMATopologyLimits',
964                                             nullable=True,
965                                             default=None),
966         'vcpu': fields.IntegerField(nullable=True, default=None),
967         'disk_gb': fields.IntegerField(nullable=True, default=None),
968         'memory_mb': fields.IntegerField(nullable=True, default=None),
969     }
970 
971     @classmethod
972     def from_dict(cls, limits_dict):
973         limits = cls(**limits_dict)
974         # NOTE(sbauza): Since the limits can be set for each field or not, we
975         # prefer to have the fields nullable, but default the value to None.
976         # Here we accept that the object is always generated from a primitive
977         # hence the use of obj_set_defaults exceptionally.
978         limits.obj_set_defaults()
979         return limits
980 
981     def to_dict(self):
982         limits = {}
983         for field in self.fields:
984             if getattr(self, field) is not None:
985                 limits[field] = getattr(self, field)
986         return limits
987 
988 
989 @base.NovaObjectRegistry.register
990 class RequestGroup(base.NovaObject):
991     """Versioned object based on the unversioned
992     nova.api.openstack.placement.lib.RequestGroup object.
993     """
994     # Version 1.0: Initial version
995     # Version 1.1: add requester_id and provider_uuids fields
996     # Version 1.2: add in_tree field
997     VERSION = '1.2'
998 
999     fields = {
1000         'use_same_provider': fields.BooleanField(default=True),
1001         'resources': fields.DictOfIntegersField(default={}),
1002         'required_traits': fields.SetOfStringsField(default=set()),
1003         'forbidden_traits': fields.SetOfStringsField(default=set()),
1004         # The aggregates field has a form of
1005         #     [[aggregate_UUID1],
1006         #      [aggregate_UUID2, aggregate_UUID3]]
1007         # meaning that the request should be fulfilled from an RP that is a
1008         # member of the aggregate aggregate_UUID1 and member of the aggregate
1009         # aggregate_UUID2 or aggregate_UUID3 .
1010         'aggregates': fields.ListOfListsOfStringsField(default=[]),
1011         # The entity the request is coming from (e.g. the Neutron port uuid)
1012         # which may not always be a UUID.
1013         'requester_id': fields.StringField(nullable=True, default=None),
1014         # The resource provider UUIDs that together fulfill the request
1015         # NOTE(gibi): this can be more than one if this is the unnumbered
1016         # request group (i.e. use_same_provider=False)
1017         'provider_uuids': fields.ListOfUUIDField(default=[]),
1018         'in_tree': fields.UUIDField(nullable=True, default=None),
1019     }
1020 
1021     def __init__(self, context=None, **kwargs):
1022         super(RequestGroup, self).__init__(context=context, **kwargs)
1023         self.obj_set_defaults()
1024 
1025     @classmethod
1026     def from_port_request(cls, context, port_uuid, port_resource_request):
1027         """Init the group from the resource request of a neutron port
1028 
1029         :param context: the request context
1030         :param port_uuid: the port requesting the resources
1031         :param port_resource_request: the resource_request attribute of the
1032                                       neutron port
1033         For example:
1034 
1035             port_resource_request = {
1036                 "resources": {
1037                     "NET_BW_IGR_KILOBIT_PER_SEC": 1000,
1038                     "NET_BW_EGR_KILOBIT_PER_SEC": 1000},
1039                 "required": ["CUSTOM_PHYSNET_2",
1040                              "CUSTOM_VNIC_TYPE_NORMAL"]
1041             }
1042         """
1043 
1044         # NOTE(gibi): Assumptions:
1045         # * a port requests resource from a single provider.
1046         # * a port only specifies resources and required traits
1047         # NOTE(gibi): Placement rejects allocation candidates where a request
1048         # group has traits but no resources specified. This is why resources
1049         # are handled as mandatory below but not traits.
1050         obj = cls(context=context,
1051                   use_same_provider=True,
1052                   resources=port_resource_request['resources'],
1053                   required_traits=set(port_resource_request.get(
1054                       'required', [])),
1055                   requester_id=port_uuid)
1056         obj.obj_set_defaults()
1057         return obj
1058 
1059     def obj_make_compatible(self, primitive, target_version):
1060         super(RequestGroup, self).obj_make_compatible(
1061             primitive, target_version)
1062         target_version = versionutils.convert_version_to_tuple(target_version)
1063         if target_version < (1, 2):
1064             if 'in_tree' in primitive:
1065                 del primitive['in_tree']
1066         if target_version < (1, 1):
1067             if 'requester_id' in primitive:
1068                 del primitive['requester_id']
1069             if 'provider_uuids' in primitive:
1070                 del primitive['provider_uuids']
